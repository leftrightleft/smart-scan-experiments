diff --git a/config/logstash.yml b/config/logstash.yml
index cb41b696f0d..7e2eb53965a 100644
--- a/config/logstash.yml
+++ b/config/logstash.yml
@@ -278,6 +278,15 @@
 #
 # dead_letter_queue.storage_policy: drop_newer
 
+# If using dead_letter_queue.enable: true, the interval that events have to be considered valid. After the interval has
+# expired the events could be automatically deleted from the DLQ.
+# The interval could be expressed in days, hours, minutes or seconds, using as postfix notation like 5d,
+# to represent a five days interval.
+# The available units are respectively d, h, m, s for day, hours, minutes and seconds.
+# If not specified then the DLQ doesn't use any age policy for cleaning events.
+#
+# dead_letter_queue.retain.age: 1d
+
 # If using dead_letter_queue.enable: true, defines the action to take when the dead_letter_queue.max_bytes is reached,
 # could be "drop_newer" or "drop_older".
 # With drop_newer, messages that were inserted most recently are dropped, logging an error line.
diff --git a/docker/data/logstash/env2yaml/env2yaml.go b/docker/data/logstash/env2yaml/env2yaml.go
index 575826633c9..ebd015df23e 100644
--- a/docker/data/logstash/env2yaml/env2yaml.go
+++ b/docker/data/logstash/env2yaml/env2yaml.go
@@ -84,6 +84,7 @@ func normalizeSetting(setting string) (string, error) {
 		"dead_letter_queue.max_bytes",
 		"dead_letter_queue.flush_interval",
 		"dead_letter_queue.storage_policy",
+		"dead_letter_queue.retain.age",
 		"path.dead_letter_queue",
 		"http.enabled",     // DEPRECATED: prefer `api.enabled`
 		"http.environment", // DEPRECATED: prefer `api.environment`
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 4c03ff22460..48306cdf86b 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -100,6 +100,7 @@ module Environment
              Setting::Bytes.new("dead_letter_queue.max_bytes", "1024mb"),
            Setting::Numeric.new("dead_letter_queue.flush_interval", 5000),
             Setting::String.new("dead_letter_queue.storage_policy", "drop_newer", true, ["drop_newer", "drop_older"]),
+    Setting::NullableString.new("dead_letter_queue.retain.age"), # example 5d
          Setting::TimeValue.new("slowlog.threshold.warn", "-1"),
          Setting::TimeValue.new("slowlog.threshold.info", "-1"),
          Setting::TimeValue.new("slowlog.threshold.debug", "-1"),
diff --git a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
index 9c16faacdd5..16fbc0c5b2d 100644
--- a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
+++ b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
@@ -77,12 +77,30 @@ private DeadLetterQueueFactory() {
      *                     that would make the size of this dlq greater than this value
      * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
      * @param storageType overwriting type in case of queue full: drop_older or drop_newer.
-     * @return The write manager for the specific id's dead-letter-queue context
+     * @return write manager for the specific id's dead-letter-queue context
      */
     public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval, QueueStorageType storageType) {
         return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize, flushInterval, storageType));
     }
 
+    /**
+     * Like {@link #getWriter(String, String, long, Duration, QueueStorageType)} but also setting the age duration
+     * of the segments.
+     *
+     * @param id The identifier context for this dlq manager
+     * @param dlqPath The path to use for the queue's backing data directory. contains sub-directories
+     *                for each id
+     * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written
+     *                     that would make the size of this dlq greater than this value
+     * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
+     * @param storageType overwriting type in case of queue full: drop_older or drop_newer.
+     * @param age the period that DLQ events should be considered as valid, before automatic removal.
+     * @return write manager for the specific id's dead-letter-queue context
+     * */
+    public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval, QueueStorageType storageType, Duration age) {
+        return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize, flushInterval, storageType, age));
+    }
+
     public static DeadLetterQueueWriter release(String id) {
         return REGISTRY.remove(id);
     }
@@ -90,10 +108,28 @@ public static DeadLetterQueueWriter release(String id) {
     private static DeadLetterQueueWriter newWriter(final String id, final String dlqPath, final long maxQueueSize,
                                                    final Duration flushInterval, final QueueStorageType storageType) {
         try {
-            return new DeadLetterQueueWriter(Paths.get(dlqPath, id), MAX_SEGMENT_SIZE_BYTES, maxQueueSize, flushInterval, storageType);
+            return DeadLetterQueueWriter
+                    .newBuilder(Paths.get(dlqPath, id), MAX_SEGMENT_SIZE_BYTES, maxQueueSize, flushInterval)
+                    .storageType(storageType)
+                    .build();
+        } catch (IOException e) {
+            logger.error("unable to create dead letter queue writer", e);
+            return null;
+        }
+    }
+
+    private static DeadLetterQueueWriter newWriter(final String id, final String dlqPath, final long maxQueueSize,
+                                                   final Duration flushInterval, final QueueStorageType storageType,
+                                                   final Duration age) {
+        try {
+            return DeadLetterQueueWriter
+                    .newBuilder(Paths.get(dlqPath, id), MAX_SEGMENT_SIZE_BYTES, maxQueueSize, flushInterval)
+                    .storageType(storageType)
+                    .retentionTime(age)
+                    .build();
         } catch (IOException e) {
             logger.error("unable to create dead letter queue writer", e);
+            return null;
         }
-        return null;
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
index 5ebaf129166..0e27ec93f99 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
@@ -42,10 +42,13 @@
 import java.io.IOException;
 import java.nio.channels.FileLock;
 import java.nio.file.Files;
+import java.nio.file.NoSuchFileException;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
+import java.time.Clock;
 import java.time.Duration;
 import java.time.Instant;
+import java.time.temporal.TemporalAmount;
 import java.util.Comparator;
 import java.util.Locale;
 import java.util.Optional;
@@ -53,6 +56,7 @@
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.AtomicLong;
 import java.util.concurrent.atomic.LongAdder;
 import java.util.concurrent.locks.ReentrantLock;
 
@@ -68,7 +72,9 @@
 import static org.logstash.common.io.DeadLetterQueueUtils.listFiles;
 import static org.logstash.common.io.DeadLetterQueueUtils.listSegmentPaths;
 import static org.logstash.common.io.RecordIOReader.SegmentStatus;
+import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
+import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
 
 public final class DeadLetterQueueWriter implements Closeable {
 
@@ -84,7 +90,7 @@ private enum FinalizeWhen {ALWAYS, ONLY_IF_STALE};
     private final long maxSegmentSize;
     private final long maxQueueSize;
     private final QueueStorageType storageType;
-    private LongAdder currentQueueSize;
+    private AtomicLong currentQueueSize;
     private final Path queuePath;
     private final FileLock fileLock;
     private volatile RecordIOWriter currentWriter;
@@ -96,24 +102,70 @@ private enum FinalizeWhen {ALWAYS, ONLY_IF_STALE};
     private ScheduledExecutorService flushScheduler;
     private final LongAdder droppedEvents = new LongAdder();
     private String lastError = "no errors";
+    private final Clock clock;
+    private Optional<Timestamp> oldestSegmentTimestamp;
+    private Optional<Path> oldestSegmentPath;
+    private final TemporalAmount retentionTime;
+
+    public static final class Builder {
+
+        private final Path queuePath;
+        private final long maxSegmentSize;
+        private final long maxQueueSize;
+        private final Duration flushInterval;
+        private QueueStorageType storageType = QueueStorageType.DROP_NEWER;
+        private Duration retentionTime = null;
+        private Clock clock = Clock.systemDefaultZone();
+
+        private Builder(Path queuePath, long maxSegmentSize, long maxQueueSize, Duration flushInterval) {
+            this.queuePath = queuePath;
+            this.maxSegmentSize = maxSegmentSize;
+            this.maxQueueSize = maxQueueSize;
+            this.flushInterval = flushInterval;
+        }
+
+        public Builder storageType(QueueStorageType storageType) {
+            this.storageType = storageType;
+            return this;
+        }
 
-    public DeadLetterQueueWriter(final Path queuePath, final long maxSegmentSize, final long maxQueueSize,
-                                 final Duration flushInterval) throws IOException {
-        this(queuePath, maxSegmentSize, maxQueueSize, flushInterval, QueueStorageType.DROP_NEWER);
+        public Builder retentionTime(Duration retentionTime) {
+            this.retentionTime = retentionTime;
+            return this;
+        }
+
+        @VisibleForTesting
+        Builder clock(Clock clock) {
+            this.clock = clock;
+            return this;
+        }
+
+        public DeadLetterQueueWriter build() throws IOException {
+            return new DeadLetterQueueWriter(queuePath, maxSegmentSize, maxQueueSize, flushInterval, storageType, retentionTime, clock);
+        }
     }
 
-    public DeadLetterQueueWriter(final Path queuePath, final long maxSegmentSize, final long maxQueueSize,
-                                 final Duration flushInterval, final QueueStorageType storageType) throws IOException {
+    public static Builder newBuilder(final Path queuePath, final long maxSegmentSize, final long maxQueueSize,
+                                     final Duration flushInterval) {
+        return new Builder(queuePath, maxSegmentSize, maxQueueSize, flushInterval);
+    }
+
+    private DeadLetterQueueWriter(final Path queuePath, final long maxSegmentSize, final long maxQueueSize,
+                          final Duration flushInterval, final QueueStorageType storageType, final Duration retentionTime,
+                          final Clock clock) throws IOException {
+        this.clock = clock;
+
         this.fileLock = FileLockFactory.obtainLock(queuePath, LOCK_FILE);
         this.queuePath = queuePath;
         this.maxSegmentSize = maxSegmentSize;
         this.maxQueueSize = maxQueueSize;
         this.storageType = storageType;
         this.flushInterval = flushInterval;
-        this.currentQueueSize = new LongAdder();
-        this.currentQueueSize.add(getStartupQueueSize());
+        this.currentQueueSize = new AtomicLong(computeQueueSize());
+        this.retentionTime = retentionTime;
 
         cleanupTempFiles();
+        updateOldestSegmentReference();
         currentSegmentIndex = listSegmentPaths(queuePath)
                 .map(s -> s.getFileName().toString().split("\\.")[0])
                 .mapToInt(Integer::parseInt)
@@ -197,6 +249,8 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {
         }
         byte[] record = entry.serialize();
         int eventPayloadSize = RECORD_HEADER_SIZE + record.length;
+        executeAgeRetentionPolicy();
+
         if (currentQueueSize.longValue() + eventPayloadSize > maxQueueSize) {
             if (storageType == QueueStorageType.DROP_NEWER) {
                 lastError = String.format("Cannot write event to DLQ(path: %s): reached maxQueueSize of %d", queuePath, maxQueueSize);
@@ -212,23 +266,104 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {
         if (currentWriter.getPosition() + eventPayloadSize > maxSegmentSize) {
             finalizeSegment(FinalizeWhen.ALWAYS);
         }
-        currentQueueSize.add(currentWriter.writeEvent(record));
+        currentQueueSize.getAndAdd(currentWriter.writeEvent(record));
         lastWrite = Instant.now();
     }
 
+    private void executeAgeRetentionPolicy() throws IOException {
+        if (isOldestSegmentExpired()) {
+            deleteExpiredSegments();
+        }
+    }
+
+    private boolean isOldestSegmentExpired() {
+        if (retentionTime == null) {
+            return false;
+        }
+        final Instant now = clock.instant();
+        return oldestSegmentTimestamp
+                .map(t -> t.toInstant().isBefore(now.minus(retentionTime)))
+                .orElse(false);
+    }
+
+    private void deleteExpiredSegments() throws IOException {
+        // remove all the old segments that verifies the age retention condition
+        boolean cleanNextSegment;
+        do {
+            if (oldestSegmentPath.isPresent()) {
+                Path beheadedSegment = oldestSegmentPath.get();
+                deleteTailSegment(beheadedSegment, "age retention policy");
+            }
+            updateOldestSegmentReference();
+            cleanNextSegment = isOldestSegmentExpired();
+        } while (cleanNextSegment);
+
+        this.currentQueueSize.set(computeQueueSize());
+    }
+
+    private void deleteTailSegment(Path segment, String motivation) throws IOException {
+        try {
+            Files.delete(segment);
+            logger.debug("Removed segment file {} due to {}", motivation, segment);
+        } catch (NoSuchFileException nsfex) {
+            // the last segment was deleted by another process, maybe the reader that's cleaning consumed segments
+            logger.debug("File not found {}, maybe removed by the reader pipeline", segment);
+        }
+    }
+
+    private void updateOldestSegmentReference() throws IOException {
+        oldestSegmentPath = listSegmentPaths(this.queuePath).sorted().findFirst();
+        if (!oldestSegmentPath.isPresent()) {
+            oldestSegmentTimestamp = Optional.empty();
+            return;
+        }
+        // extract the newest timestamp from the oldest segment
+        Optional<Timestamp> foundTimestamp = readTimestampOfLastEventInSegment(oldestSegmentPath.get());
+        if (!foundTimestamp.isPresent()) {
+            // clean also the last segment, because doesn't contain a timestamp (corrupted maybe)
+            // or is not present anymore
+            oldestSegmentPath = Optional.empty();
+        }
+        oldestSegmentTimestamp = foundTimestamp;
+    }
+
+    /**
+     * Extract the timestamp from the last DLQEntry it finds in the given segment.
+     * Start from the end of the latest block, and going backward try to read the next event from its start.
+     * */
+    private static Optional<Timestamp> readTimestampOfLastEventInSegment(Path segmentPath) throws IOException {
+        final int lastBlockId = (int) Math.ceil(((Files.size(segmentPath) - VERSION_SIZE) / (double) BLOCK_SIZE)) - 1;
+        byte[] eventBytes;
+        try (RecordIOReader recordReader = new RecordIOReader(segmentPath)) {
+            int blockId = lastBlockId;
+            do {
+                recordReader.seekToBlock(blockId);
+                eventBytes = recordReader.readEvent();
+                blockId--;
+            } while (eventBytes == null && blockId >= 0); // no event present in last block, try with the one before
+        } catch (NoSuchFileException nsfex) {
+            // the segment file may have been removed by the clean consumed feature on the reader side
+            return Optional.empty();
+        }
+        if (eventBytes == null) {
+            logger.warn("Cannot find a complete event into the segment file [{}], this is a DLQ segment corruption", segmentPath);
+            return Optional.empty();
+        }
+        return Optional.of(DLQEntry.deserialize(eventBytes).getEntryTime());
+    }
+
     // package-private for testing
     void dropTailSegment() throws IOException {
         // remove oldest segment
         final Optional<Path> oldestSegment = listSegmentPaths(queuePath)
                 .min(Comparator.comparingInt(DeadLetterQueueUtils::extractSegmentId));
-        if (!oldestSegment.isPresent()) {
-            throw new IllegalStateException("Listing of DLQ segments resulted in empty set during storage policy size(" + maxQueueSize + ") check");
+        if (oldestSegment.isPresent()) {
+            final Path beheadedSegment = oldestSegment.get();
+            deleteTailSegment(beheadedSegment, "dead letter queue size exceeded dead_letter_queue.max_bytes size(" + maxQueueSize + ")");
+        } else {
+            logger.info("Queue size {} exceeded, but no complete DLQ segments found", maxQueueSize);
         }
-        final Path beheadedSegment = oldestSegment.get();
-        final long segmentSize = Files.size(beheadedSegment);
-        currentQueueSize.add(-segmentSize);
-        Files.delete(beheadedSegment);
-        logger.debug("Deleted exceeded retained size segment file {}", beheadedSegment);
+        this.currentQueueSize.set(computeQueueSize());
     }
 
     /**
@@ -271,6 +406,8 @@ private void finalizeSegment(final FinalizeWhen finalizeWhen) throws IOException
                 Files.move(queuePath.resolve(String.format(TEMP_FILE_PATTERN, currentSegmentIndex)),
                         queuePath.resolve(String.format(SEGMENT_FILE_PATTERN, currentSegmentIndex)),
                         StandardCopyOption.ATOMIC_MOVE);
+                updateOldestSegmentReference();
+                executeAgeRetentionPolicy();
                 if (isOpen()) {
                     nextWriter();
                 }
@@ -292,18 +429,21 @@ private void createFlushScheduler() {
         flushScheduler.scheduleAtFixedRate(this::flushCheck, 1L, 1L, TimeUnit.SECONDS);
     }
 
-    private long getStartupQueueSize() throws IOException {
-        return listSegmentPaths(queuePath)
-                .mapToLong((p) -> {
-                    try {
-                        return Files.size(p);
-                    } catch (IOException e) {
-                        throw new IllegalStateException(e);
-                    }
-                } )
+
+    private long computeQueueSize() throws IOException {
+        return listSegmentPaths(this.queuePath)
+                .mapToLong(DeadLetterQueueWriter::safeFileSize)
                 .sum();
     }
 
+    private static long safeFileSize(Path p) {
+        try {
+            return Files.size(p);
+        } catch (IOException e) {
+            return 0L;
+        }
+    }
+
     private void releaseFileLock() {
         try {
             FileLockFactory.releaseLock(fileLock);
@@ -319,7 +459,7 @@ private void releaseFileLock() {
 
     private void nextWriter() throws IOException {
         currentWriter = new RecordIOWriter(queuePath.resolve(String.format(TEMP_FILE_PATTERN, ++currentSegmentIndex)));
-        currentQueueSize.increment();
+        currentQueueSize.incrementAndGet();
     }
 
     // Clean up existing temp files - files with an extension of .log.tmp. Either delete them if an existing
diff --git a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
index 84413fb7fdf..14fb03aa588 100644
--- a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
@@ -26,11 +26,17 @@
 import java.nio.file.Paths;
 import java.security.MessageDigest;
 import java.security.NoSuchAlgorithmException;
+import java.time.temporal.ChronoUnit;
+import java.time.temporal.TemporalUnit;
 import java.util.ArrayList;
 import java.time.Duration;
 import java.util.Arrays;
 import java.util.List;
 import java.util.UUID;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+import com.google.common.annotations.VisibleForTesting;
 import org.apache.commons.codec.binary.Hex;
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
@@ -53,6 +59,7 @@
 import org.logstash.common.DeadLetterQueueFactory;
 import org.logstash.common.EnvironmentVariableProvider;
 import org.logstash.common.SourceWithMetadata;
+import org.logstash.common.io.DeadLetterQueueWriter;
 import org.logstash.common.io.QueueStorageType;
 import org.logstash.config.ir.ConfigCompiler;
 import org.logstash.config.ir.InvalidIRException;
@@ -289,17 +296,8 @@ public final IRubyObject lir(final ThreadContext context) {
     public final IRubyObject dlqWriter(final ThreadContext context) {
         if (dlqWriter == null) {
             if (dlqEnabled(context).isTrue()) {
-                final QueueStorageType storageType = QueueStorageType.parse(getSetting(context, "dead_letter_queue.storage_policy").asJavaString());
-
-                dlqWriter = JavaUtil.convertJavaToUsableRubyObject(
-                    context.runtime,
-                    DeadLetterQueueFactory.getWriter(
-                        pipelineId.asJavaString(),
-                        getSetting(context, "path.dead_letter_queue").asJavaString(),
-                        getSetting(context, "dead_letter_queue.max_bytes").convertToInteger().getLongValue(),
-                        Duration.ofMillis(getSetting(context, "dead_letter_queue.flush_interval").convertToInteger().getLongValue()),
-                        storageType)
-                    );
+                final DeadLetterQueueWriter javaDlqWriter = createDeadLetterQueueWriterFromSettings(context);
+                dlqWriter = JavaUtil.convertJavaToUsableRubyObject(context.runtime, javaDlqWriter);
             } else {
                 dlqWriter = RubyUtil.DUMMY_DLQ_WRITER_CLASS.callMethod(context, "new");
             }
@@ -307,6 +305,45 @@ public final IRubyObject dlqWriter(final ThreadContext context) {
         return dlqWriter;
     }
 
+    private DeadLetterQueueWriter createDeadLetterQueueWriterFromSettings(ThreadContext context) {
+        final QueueStorageType storageType = QueueStorageType.parse(getSetting(context, "dead_letter_queue.storage_policy").asJavaString());
+
+        String dlqPath = getSetting(context, "path.dead_letter_queue").asJavaString();
+        long dlqMaxBytes = getSetting(context, "dead_letter_queue.max_bytes").convertToInteger().getLongValue();
+        Duration dlqFlushInterval = Duration.ofMillis(getSetting(context, "dead_letter_queue.flush_interval").convertToInteger().getLongValue());
+
+        if (hasSetting(context, "dead_letter_queue.retain.age") && !getSetting(context, "dead_letter_queue.retain.age").isNil()) {
+            // convert to Duration
+            final Duration age = parseToDuration(getSetting(context, "dead_letter_queue.retain.age").convertToString().toString());
+            return DeadLetterQueueFactory.getWriter(pipelineId.asJavaString(), dlqPath, dlqMaxBytes,
+                    dlqFlushInterval, storageType, age);
+        }
+
+        return DeadLetterQueueFactory.getWriter(pipelineId.asJavaString(), dlqPath, dlqMaxBytes, dlqFlushInterval, storageType);
+    }
+
+    /**
+     * Convert time strings like 3d or 4h or 5m to a duration
+     * */
+    @VisibleForTesting
+    static Duration parseToDuration(String timeStr) {
+        final Matcher matcher = Pattern.compile("(?<value>\\d+)\\s*(?<time>[dhms])").matcher(timeStr);
+        if (!matcher.matches()) {
+            throw new IllegalArgumentException("Expected a time specification in the form <number>[d,h,m,s], e.g. 3m, but found [" + timeStr + "]");
+        }
+        final int value = Integer.parseInt(matcher.group("value"));
+        final String timeSpecifier = matcher.group("time");
+        final TemporalUnit unit;
+        switch (timeSpecifier) {
+            case "d": unit = ChronoUnit.DAYS; break;
+            case "h": unit = ChronoUnit.HOURS; break;
+            case "m": unit = ChronoUnit.MINUTES; break;
+            case "s": unit = ChronoUnit.SECONDS; break;
+            default: throw new IllegalStateException("Expected a time unit specification from d,h,m,s but found: [" + timeSpecifier + "]");
+        }
+        return Duration.of(value, unit);
+    }
+
     @JRubyMethod(name = "dlq_enabled?")
     public final IRubyObject dlqEnabled(final ThreadContext context) {
         return getSetting(context, "dead_letter_queue.enable");
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
index b718cf63b87..183fa4eae56 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
@@ -60,6 +60,7 @@
 import static org.junit.Assert.assertNotNull;
 import static org.junit.Assert.assertNull;
 import static org.junit.Assert.assertTrue;
+import static org.logstash.common.io.DeadLetterQueueTestUtils.GB;
 import static org.logstash.common.io.DeadLetterQueueTestUtils.MB;
 import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
@@ -154,7 +155,9 @@ public void testRereadFinalBlock() throws Exception {
         event.setField("message", generateMessageContent(32495));
         long startTime = System.currentTimeMillis();
         int messageSize = 0;
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try(DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = 0; i < 2; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), constantSerializationLengthTimestamp(startTime++));
                 final int serializationLength = entry.serialize().length;
@@ -209,7 +212,9 @@ private void writeSegmentSizeEntries(int count) throws IOException {
         long startTime = System.currentTimeMillis();
         DLQEntry templateEntry = new DLQEntry(event, "1", "1", String.valueOf(0), constantSerializationLengthTimestamp(startTime));
         int size = templateEntry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE + VERSION_SIZE;
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, size, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, size, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = 1; i <= count; i++) {
                 writeManager.writeEntry(new DLQEntry(event, "1", "1", String.valueOf(i), constantSerializationLengthTimestamp(startTime++)));
             }
@@ -241,7 +246,9 @@ public void testBlockBoundary() throws Exception {
         event.setField("T", new String(field));
         Timestamp timestamp = constantSerializationLengthTimestamp();
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = 0; i < 2; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", "", timestamp);
                 assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
@@ -264,7 +271,9 @@ public void testBlockBoundaryMultiple() throws Exception {
         event.setField("message", new String(field));
         long startTime = System.currentTimeMillis();
         int messageSize = 0;
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = 1; i <= 5; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", "", constantSerializationLengthTimestamp(startTime++));
                 messageSize += entry.serialize().length;
@@ -287,7 +296,9 @@ public void testFlushAfterWriterClose() throws Exception {
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT/8));
         Timestamp timestamp = new Timestamp();
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = 0; i < 6; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -307,8 +318,9 @@ public void testFlushAfterSegmentComplete() throws Exception {
         final int EVENTS_BEFORE_FLUSH = randomBetween(1, 32);
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = new Timestamp();
-
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * EVENTS_BEFORE_FLUSH, defaultDlqSize, Duration.ofHours(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, BLOCK_SIZE * EVENTS_BEFORE_FLUSH, defaultDlqSize, Duration.ofHours(1))
+                .build()) {
             for (int i = 1; i < EVENTS_BEFORE_FLUSH; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -341,7 +353,9 @@ public void testMultiFlushAfterSegmentComplete() throws Exception {
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = new Timestamp();
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * eventsInSegment, defaultDlqSize, Duration.ofHours(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, BLOCK_SIZE * eventsInSegment, defaultDlqSize, Duration.ofHours(1))
+                .build()) {
             for (int i = 1; i < totalEventsToWrite; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -382,7 +396,9 @@ public void testFlushAfterDelay() throws Exception {
 
         System.out.println("events per block= " + eventsPerBlock);
 
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(2))) {
+        try(DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(2))
+                .build()) {
             for (int i = 1; i < eventsToWrite; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -414,7 +430,9 @@ public void testBlockAndSegmentBoundary() throws Exception {
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = constantSerializationLengthTimestamp();
 
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))) {
+        try(DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = 0; i < 2; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", "", timestamp);
                 assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
@@ -435,7 +453,9 @@ public void testWriteReadRandomEventSize() throws Exception {
         int eventCount = 1024; // max = 1000 * 64kb = 64mb
         long startTime = System.currentTimeMillis();
 
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try(DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = 0; i < eventCount; i++) {
                 event.setField("message", generateMessageContent((int)(Math.random() * (maxEventSize))));
                 DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), new Timestamp(startTime++));
@@ -536,7 +556,9 @@ public void testConcurrentWriteReadRandomEventSize() throws Exception {
             exec.submit(() -> {
                 final Event event = new Event();
                 long startTime = System.currentTimeMillis();
-                try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(10))) {
+                try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                        .newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(10))
+                        .build()) {
                     for (int i = 0; i < eventCount; i++) {
                         event.setField(
                                 "message",
@@ -875,7 +897,9 @@ private void seekReadAndVerify(final Timestamp seekTarget, final String expected
     }
 
     private void writeEntries(final Event event, int offset, final int numberOfEvents, long startTime) throws IOException {
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             for (int i = offset; i <= offset + numberOfEvents; i++) {
                 DLQEntry entry = new DLQEntry(event, "foo", "bar", String.valueOf(i), new Timestamp(startTime++));
                 writeManager.writeEntry(entry);
@@ -896,7 +920,9 @@ private int prepareFilledSegmentFiles(int segments, long start) throws IOExcepti
 
         final int maxSegmentSize = 10 * MB;
         final int loopPerSegment = (int) Math.floor((maxSegmentSize - 1.0) / BLOCK_SIZE);
-        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, maxSegmentSize, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, maxSegmentSize, defaultDlqSize, Duration.ofSeconds(1))
+                .build()) {
             final int loops = loopPerSegment * segments;
             for (int i = 0; i < loops; i++) {
                 entry = new DLQEntry(event, "", "", String.format("%05d", i), constantSerializationLengthTimestamp(start++));
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
index 6d898909894..36b284d7a4f 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
@@ -19,9 +19,10 @@
 package org.logstash.common.io;
 
 import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
+import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
 
 public class DeadLetterQueueTestUtils {
     public static final int MB = 1024 * 1024;
     public static final int GB = 1024 * 1024 * 1024;
-    public static final int FULL_SEGMENT_FILE_SIZE = 319 * BLOCK_SIZE + 1; // 319 records that fills completely a block plus the 1 byte header of the segment file
+    public static final int FULL_SEGMENT_FILE_SIZE = 319 * BLOCK_SIZE + VERSION_SIZE; // 319 records that fills completely a block plus the 1 byte header of the segment file
 }
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java
new file mode 100644
index 00000000000..c03ceba5719
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterAgeRetentionTest.java
@@ -0,0 +1,215 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.nio.file.Path;
+import java.time.Clock;
+import java.time.Duration;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.util.Collections;
+
+import org.hamcrest.Matchers;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.DLQEntry;
+import org.logstash.Event;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.lessThan;
+import static org.junit.Assert.assertEquals;
+import static org.logstash.common.io.DeadLetterQueueTestUtils.FULL_SEGMENT_FILE_SIZE;
+import static org.logstash.common.io.DeadLetterQueueTestUtils.GB;
+import static org.logstash.common.io.DeadLetterQueueTestUtils.MB;
+import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
+import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
+
+public class DeadLetterQueueWriterAgeRetentionTest {
+
+    static class ForwardableClock extends Clock {
+
+        private Clock currentClock;
+
+        ForwardableClock(Clock clock) {
+            this.currentClock = clock;
+        }
+
+        void forward(Duration period) {
+            currentClock = Clock.offset(currentClock, period);
+        }
+
+        @Override
+        public ZoneId getZone() {
+            return currentClock.getZone();
+        }
+
+        @Override
+        public Clock withZone(ZoneId zone) {
+            return currentClock.withZone(zone);
+        }
+
+        @Override
+        public Instant instant() {
+            return currentClock.instant();
+        }
+    }
+
+    private Path dir;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        dir = temporaryFolder.newFolder().toPath();
+    }
+
+    @Test
+    public void testRemovesOlderSegmentsWhenWriteOnReopenedDLQContainingExpiredSegments() throws IOException {
+        final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueReaderTest.generateMessageContent(32479));
+
+        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
+        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
+        // given DLQ with first segment filled of expired events
+        prepareDLQWithFirstSegmentOlderThanRetainPeriod(event, fakeClock, Duration.ofDays(2));
+
+        // Exercise
+        final long prevQueueSize;
+        final long beheadedQueueSize;
+
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1))
+                .retentionTime(Duration.ofDays(2))
+                .clock(fakeClock)
+                .build()) {
+            prevQueueSize = writeManager.getCurrentQueueSize();
+            assertEquals("Queue size is composed of one just one empty file with version byte", VERSION_SIZE, prevQueueSize);
+
+            // write new entry that trigger clean of age retained segment
+            DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", 320), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
+            // when a new write happens in a reopened queue
+            writeManager.writeEntry(entry);
+            beheadedQueueSize = writeManager.getCurrentQueueSize();
+        }
+
+        // then the age policy must remove the expired segments
+        assertEquals("Write should push off the age expired segments", VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+    }
+
+    private void prepareDLQWithFirstSegmentOlderThanRetainPeriod(Event event, ForwardableClock fakeClock, Duration retainedPeriod) throws IOException {
+        final Duration littleMoreThanRetainedPeriod = retainedPeriod.plusMinutes(1);
+        long startTime = fakeClock.instant().minus(littleMoreThanRetainedPeriod).toEpochMilli();
+        int messageSize = 0;
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1))
+                .retentionTime(retainedPeriod)
+                .clock(fakeClock)
+                .build()) {
+
+            // 320 generates 10 Mb of data
+            for (int i = 0; i < 320 - 1; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
+                final int serializationLength = entry.serialize().length;
+                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                messageSize += serializationLength;
+                writeManager.writeEntry(entry);
+            }
+            assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
+        }
+    }
+
+    @Test
+    public void testRemovesOlderSegmentsWhenWritesIntoDLQContainingExpiredSegments() throws IOException {
+        final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueReaderTest.generateMessageContent(32479));
+        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
+        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
+
+        long startTime = fakeClock.instant().toEpochMilli();
+        int messageSize = 0;
+
+        final Duration retention = Duration.ofDays(2);
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1))
+                .retentionTime(retention)
+                .clock(fakeClock)
+                .build()) {
+
+            // 320 generates 10 Mb of data
+            for (int i = 0; i < 320 - 1; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
+                final int serializationLength = entry.serialize().length;
+                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                messageSize += serializationLength;
+                writeManager.writeEntry(entry);
+            }
+            assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
+
+            // Exercise
+            // write an event that goes in second segment
+            fakeClock.forward(retention.plusSeconds(1));
+            final long prevQueueSize = writeManager.getCurrentQueueSize();
+            assertEquals("Queue size is composed of one full segment files", FULL_SEGMENT_FILE_SIZE, prevQueueSize);
+
+            // write new entry that trigger clean of age retained segment
+            DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", 320), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
+            // when a new write happens in the same writer
+            writeManager.writeEntry(entry);
+            final long beheadedQueueSize = writeManager.getCurrentQueueSize();
+
+            // then the age policy must remove the expired segments
+            assertEquals("Write should push off the age expired segments",VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+        }
+    }
+
+    @Test
+    public void testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded() throws IOException {
+        final Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueReaderTest.generateMessageContent(32479));
+
+        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
+        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
+
+        long startTime = fakeClock.instant().toEpochMilli();
+        int messageSize = 0;
+
+        final Duration retention = Duration.ofDays(2);
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 1 * GB, Duration.ofSeconds(1))
+                .retentionTime(retention)
+                .clock(fakeClock)
+                .build()) {
+
+            // given DLQ with a couple of segments filled of expired events
+            // 320 generates 10 Mb of data
+            for (int i = 0; i < 319 * 2; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(startTime++));
+                final int serializationLength = entry.serialize().length;
+                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                messageSize += serializationLength;
+                writeManager.writeEntry(entry);
+            }
+            assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
+
+            // when the age expires the retention and a write is done
+            // make the retention age to pass for the first 2 full segments
+            fakeClock.forward(retention.plusSeconds(1));
+
+            // Exercise
+            // write an event that goes in second segment
+            final long prevQueueSize = writeManager.getCurrentQueueSize();
+            assertEquals("Queue size is composed of 2 full segment files", 2 * FULL_SEGMENT_FILE_SIZE, prevQueueSize);
+
+            // write new entry that trigger clean of age retained segment
+            DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", 320), DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
+            writeManager.writeEntry(entry);
+            final long beheadedQueueSize = writeManager.getCurrentQueueSize();
+
+            // then the age policy must remove the expired segments
+            assertEquals("Write should push off the age expired segments",VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+        }
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
index 816d0627d7c..b92600d9484 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
@@ -51,10 +51,11 @@
 import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
+import static org.logstash.common.io.DeadLetterQueueTestUtils.FULL_SEGMENT_FILE_SIZE;
+import static org.logstash.common.io.DeadLetterQueueTestUtils.MB;
 import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
 import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
-import static org.logstash.common.io.DeadLetterQueueTestUtils.*;
 
 public class DeadLetterQueueWriterTest {
 
@@ -73,7 +74,9 @@ public void setUp() throws Exception {
     @Test
     public void testLockFileManagement() throws Exception {
         Path lockFile = dir.resolve(".lock");
-        DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 100000, Duration.ofSeconds(1));
+        DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, 1_000, 100_000, Duration.ofSeconds(1))
+                .build();
         assertTrue(Files.exists(lockFile));
         writer.close();
         assertFalse(Files.exists(lockFile));
@@ -81,9 +84,13 @@ public void testLockFileManagement() throws Exception {
 
     @Test
     public void testFileLocking() throws Exception {
-        DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 100000, Duration.ofSeconds(1));
+        DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, 1_000, 100_000, Duration.ofSeconds(1))
+                .build();
         try {
-            new DeadLetterQueueWriter(dir, 100, 1000, Duration.ofSeconds(1));
+            DeadLetterQueueWriter
+                    .newBuilder(dir, 100, 1_000, Duration.ofSeconds(1))
+                    .build();
             fail();
         } catch (LockException e) {
         } finally {
@@ -95,7 +102,9 @@ public void testFileLocking() throws Exception {
     public void testUncleanCloseOfPreviousWriter() throws Exception {
         Path lockFilePath = dir.resolve(".lock");
         boolean created = lockFilePath.toFile().createNewFile();
-        DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 100000, Duration.ofSeconds(1));
+        DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, 1_000, 100_000, Duration.ofSeconds(1))
+                .build();
 
         FileChannel channel = FileChannel.open(lockFilePath, StandardOpenOption.WRITE);
         try {
@@ -110,7 +119,9 @@ public void testUncleanCloseOfPreviousWriter() throws Exception {
 
     @Test
     public void testWrite() throws Exception {
-        DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 100000, Duration.ofSeconds(1));
+        DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, 1_000, 100_000, Duration.ofSeconds(1))
+                .build();
         DLQEntry entry = new DLQEntry(new Event(), "type", "id", "reason");
         writer.writeEntry(entry);
         writer.close();
@@ -123,7 +134,9 @@ public void testDoesNotWriteMessagesAlreadyRoutedThroughDLQ() throws Exception {
         DLQEntry entry = new DLQEntry(new Event(), "type", "id", "reason");
         DLQEntry dlqEntry = new DLQEntry(dlqEvent, "type", "id", "reason");
 
-        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 100000, Duration.ofSeconds(1));) {
+        try (DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, 1_000, 100_000, Duration.ofSeconds(1))
+                .build()) {
             writer.writeEntry(entry);
             long dlqLengthAfterEvent = dlqLength();
 
@@ -142,7 +155,9 @@ public void testDoesNotWriteBeyondLimit() throws Exception {
         long MAX_QUEUE_LENGTH = payloadLength * MESSAGE_COUNT;
 
 
-        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, payloadLength, MAX_QUEUE_LENGTH, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, payloadLength, MAX_QUEUE_LENGTH, Duration.ofSeconds(1))
+                .build()) {
 
             for (int i = 0; i < MESSAGE_COUNT; i++)
                 writer.writeEntry(entry);
@@ -158,7 +173,9 @@ public void testDoesNotWriteBeyondLimit() throws Exception {
 
     @Test
     public void testSlowFlush() throws Exception {
-        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 1_000_000, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, 1_000, 1_000_000, Duration.ofSeconds(1))
+                .build()) {
             DLQEntry entry = new DLQEntry(new Event(), "type", "id", "1");
             writer.writeEntry(entry);
             entry = new DLQEntry(new Event(), "type", "id", "2");
@@ -178,7 +195,9 @@ public void testSlowFlush() throws Exception {
 
     @Test
     public void testNotFlushed() throws Exception {
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, 1_000_000_000, Duration.ofSeconds(5))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, BLOCK_SIZE, 1_000_000_000, Duration.ofSeconds(5))
+                .build()) {
             for (int i = 0; i < 4; i++) {
                 DLQEntry entry = new DLQEntry(new Event(), "type", "id", "1");
                 writeManager.writeEntry(entry);
@@ -199,7 +218,9 @@ public void testNotFlushed() throws Exception {
 
     @Test
     public void testCloseFlush() throws Exception {
-        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, 1000, 1_000_000, Duration.ofHours(1))) {
+        try (DeadLetterQueueWriter writer = DeadLetterQueueWriter
+                .newBuilder(dir, 1_000, 1_000_000, Duration.ofHours(1))
+                .build()) {
             DLQEntry entry = new DLQEntry(new Event(), "type", "id", "1");
             writer.writeEntry(entry);
         }
@@ -233,7 +254,9 @@ public void testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsE
         long startTime = System.currentTimeMillis();
 
         int messageSize = 0;
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))
+                .build()) {
 
             // 320 generates 10 Mb of data
             for (int i = 0; i < (320 * 2) - 1; i++) {
@@ -263,8 +286,10 @@ public void testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsE
         final long prevQueueSize;
         final long beheadedQueueSize;
         long droppedEvent;
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 20 * MB,
-                Duration.ofSeconds(1), QueueStorageType.DROP_OLDER)) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))
+                .storageType(QueueStorageType.DROP_OLDER)
+                .build()) {
             prevQueueSize = writeManager.getCurrentQueueSize();
             final int expectedQueueSize = 2 * // number of full segment files
                     FULL_SEGMENT_FILE_SIZE  + // size of a segment file
@@ -289,7 +314,8 @@ public void testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsE
         assertThat(afterBeheadSegmentFileNames, Matchers.not(Matchers.contains(fileToBeRemoved)));
         final long expectedQueueSize = prevQueueSize +
                 BLOCK_SIZE - // the space of the newly inserted message
-                FULL_SEGMENT_FILE_SIZE; //the size of the removed segment file
+                FULL_SEGMENT_FILE_SIZE - //the size of the removed segment file
+                VERSION_SIZE; // the size of a previous head file (n.log.tmp) that doesn't exist anymore.
         assertEquals("Total queue size must be decremented by the size of the first segment file",
                 expectedQueueSize, beheadedQueueSize);
         assertEquals("Last segment removal doesn't increment dropped events counter",
@@ -298,7 +324,9 @@ public void testRemoveOldestSegmentWhenRetainedSizeIsExceededAndDropOlderModeIsE
 
     @Test
     public void testRemoveSegmentsOrder() throws IOException {
-        try (DeadLetterQueueWriter sut = new DeadLetterQueueWriter(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter sut = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))
+                .build()) {
             // create some segments files
             Files.createFile(dir.resolve("9.log"));
             Files.createFile(dir.resolve("10.log"));
@@ -327,7 +355,9 @@ public void testDropEventCountCorrectlyNotEnqueuedEvents() throws IOException {
         Event bigEvent = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
         bigEvent.setField("message", DeadLetterQueueReaderTest.generateMessageContent(2 * BLOCK_SIZE));
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))
+                .build()) {
             // enqueue a record with size smaller than BLOCK_SIZE
             DLQEntry entry = new DLQEntry(blockAlmostFullEvent, "", "", "00001", DeadLetterQueueReaderTest.constantSerializationLengthTimestamp(System.currentTimeMillis()));
             assertEquals("Serialized plus header must not leave enough space for another record header ",
@@ -343,8 +373,10 @@ public void testDropEventCountCorrectlyNotEnqueuedEvents() throws IOException {
         // fill the queue to push out the segment with the 2 previous events
         Event event = DeadLetterQueueReaderTest.createEventWithConstantSerializationOverhead(Collections.emptyMap());
         event.setField("message", DeadLetterQueueReaderTest.generateMessageContent(32479));
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 20 * MB,
-                Duration.ofSeconds(1), QueueStorageType.DROP_NEWER)) {
+        try (DeadLetterQueueWriter writeManager = DeadLetterQueueWriter
+                .newBuilder(dir, 10 * MB, 20 * MB, Duration.ofSeconds(1))
+                .storageType(QueueStorageType.DROP_NEWER)
+                .build()) {
 
             long startTime = System.currentTimeMillis();
             // 319 events of 32K generates almost 2 segments of 10 Mb of data
diff --git a/logstash-core/src/test/java/org/logstash/execution/AbstractPipelineExtTest.java b/logstash-core/src/test/java/org/logstash/execution/AbstractPipelineExtTest.java
new file mode 100644
index 00000000000..5f38649b105
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/execution/AbstractPipelineExtTest.java
@@ -0,0 +1,34 @@
+package org.logstash.execution;
+
+import org.junit.Test;
+
+import java.time.Duration;
+import java.time.temporal.ChronoUnit;
+
+import static org.junit.Assert.assertEquals;
+
+public class AbstractPipelineExtTest {
+
+    @Test(expected = IllegalArgumentException.class)
+    public void testParseToDurationWithBadDurationFormatThrowsAnError() {
+        AbstractPipelineExt.parseToDuration("+3m");
+    }
+
+    @Test(expected = IllegalArgumentException.class)
+    public void testParseToDurationWithUnrecognizedTimeUnitThrowsAnError() {
+        AbstractPipelineExt.parseToDuration("3y");
+    }
+
+    @Test(expected = IllegalArgumentException.class)
+    public void testParseToDurationWithUnspecifiedTimeUnitThrowsAnError() {
+        AbstractPipelineExt.parseToDuration("3");
+    }
+
+    @Test
+    public void testParseToDurationSuccessfullyParseExpectedFormats() {
+        assertEquals(Duration.of(4, ChronoUnit.DAYS), AbstractPipelineExt.parseToDuration("4d"));
+        assertEquals(Duration.of(3, ChronoUnit.HOURS), AbstractPipelineExt.parseToDuration("3h"));
+        assertEquals(Duration.of(2, ChronoUnit.MINUTES), AbstractPipelineExt.parseToDuration("2m"));
+        assertEquals(Duration.of(1, ChronoUnit.SECONDS), AbstractPipelineExt.parseToDuration("1s"));
+    }
+}
\ No newline at end of file
