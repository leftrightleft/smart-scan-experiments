diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index b23af958b87..2d3b47b27a8 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -10,8 +10,11 @@
 require "logstash/pipeline"
 require "logstash/webserver"
 require "logstash/event_dispatcher"
+require "logstash/config/source_loader"
+require "logstash/pipeline_action"
+require "logstash/converge_result"
+require "logstash/state_resolver"
 require "stud/trap"
-require "logstash/config/loader"
 require "uri"
 require "socket"
 require "securerandom"
@@ -30,7 +33,7 @@ class LogStash::Agent
   #   :name [String] - identifier for the agent
   #   :auto_reload [Boolean] - enable reloading of pipelines
   #   :reload_interval [Integer] - reload pipelines every X seconds
-  def initialize(settings = LogStash::SETTINGS)
+  def initialize(settings = LogStash::SETTINGS, source_loader = nil)
     @logger = self.class.logger
     @settings = settings
     @auto_reload = setting("config.reload.automatic")
@@ -43,37 +46,63 @@ def initialize(settings = LogStash::SETTINGS)
     # Generate / load the persistent uuid
     id
 
-    @config_loader = LogStash::Config::Loader.new(@logger)
+    # This is for backward compatibility in the tests
+    if source_loader.nil?
+      @source_loader = LogStash::Config::SOURCE_LOADER
+      @source_loader.add_source(LogStash::Config::Source::Local.new(@settings))
+    else
+      @source_loader = source_loader
+    end
+
     @reload_interval = setting("config.reload.interval")
-    @upgrade_mutex = Mutex.new
+    @pipelines_mutex = Mutex.new
 
     @collect_metric = setting("metric.collect")
 
     # Create the collectors and configured it with the library
     configure_metrics_collectors
 
+    @state_resolver = LogStash::StateResolver.new(metric)
+
     @pipeline_reload_metric = metric.namespace([:stats, :pipelines])
     @instance_reload_metric = metric.namespace([:stats, :reloads])
+    initialize_agent_metrics
 
     @dispatcher = LogStash::EventDispatcher.new(self)
     LogStash::PLUGIN_REGISTRY.hooks.register_emitter(self.class, dispatcher)
     dispatcher.fire(:after_initialize)
+
+    @running = Concurrent::AtomicBoolean.new(false)
   end
 
   def execute
-    @thread = Thread.current # this var is implicitly used by Stud.stop?
-    @logger.debug("starting agent")
+    @thread = Thread.current # this var is implicilty used by Stud.stop?
+    logger.debug("starting agent")
 
-    start_pipelines
     start_webserver
 
-    return 1 if clean_state?
-
-    Stud.stoppable_sleep(@reload_interval) # sleep before looping
-
-    if @auto_reload
-      Stud.interval(@reload_interval) { reload_state! }
+    transition_to_running
+
+    converge_state_and_update
+
+    if auto_reload?
+      # `sleep_then_run` instead of firing the interval right away
+      Stud.interval(@reload_interval, :sleep_then_run => true) do
+        # TODO(ph) OK, in reality, we should get out of the loop, but I am
+        # worried about the implication of that change so instead when we are stopped
+        # we don't converge.
+        #
+        # Logstash currently expect to be block here, the signal will force a kill on the agent making
+        # the agent thread unblock
+        #
+        # Actually what we really need is one more state:
+        #
+        # init => running => stopping => stopped
+        converge_state_and_update unless stopped?
+      end
     else
+      return 1 if clean_state?
+
       while !Stud.stop?
         if clean_state? || running_pipelines?
           sleep 0.5
@@ -82,44 +111,52 @@ def execute
         end
       end
     end
+
+    return 0
+  ensure
+    transition_to_stopped
   end
 
-  # register_pipeline - adds a pipeline to the agent's state
-  # @param pipeline_id [String] pipeline string identifier
-  # @param settings [Hash] settings that will be passed when creating the pipeline.
-  #   keys should be symbols such as :pipeline_workers and :pipeline_batch_delay
-  def register_pipeline(settings)
-    pipeline_settings = settings.clone
-    pipeline_id = pipeline_settings.get("pipeline.id")
-
-    pipeline = create_pipeline(pipeline_settings)
-    return unless pipeline.is_a?(LogStash::Pipeline)
-    if @auto_reload && !pipeline.reloadable?
-      @logger.error(I18n.t("logstash.agent.non_reloadable_config_register"),
-                    :pipeline_id => pipeline_id,
-                    :plugins => pipeline.non_reloadable_plugins.map(&:class))
-      return
-    end
-    @pipelines[pipeline_id] = pipeline
-  end
-
-  def reload_state!
-    @upgrade_mutex.synchronize do
-      @pipelines.each do |pipeline_id, pipeline|
-        next if pipeline.settings.get("config.reload.automatic") == false
-        begin
-          reload_pipeline!(pipeline_id)
-        rescue => e
-          @instance_reload_metric.increment(:failures)
-          @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
-            n.increment(:failures)
-            n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
-            n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
-          end
-          @logger.error(I18n.t("oops"), :message => e.message, :class => e.class.name, :backtrace => e.backtrace)
-        end
+  def auto_reload?
+    @auto_reload
+  end
+
+  def running?
+    @running.value
+  end
+
+  def stopped?
+    !@running.value
+  end
+
+  def converge_state_and_update
+    results = @source_loader.fetch
+
+    unless results.success?
+      if auto_reload?
+        logger.debug("Count not fetch the configuration to converge, will retry", :message => results.error, :retrying_in => @reload_interval)
+        return
+      else
+        raise "Count not fetch the configuration, message: #{results.error}"
       end
     end
+
+    # We Lock any access on the pipelines, since the actions will modify the
+    # content of it.
+    converge_result = nil
+
+    @pipelines_mutex.synchronize do
+      pipeline_actions = resolve_actions(results.response)
+      converge_result = converge_state(pipeline_actions)
+    end
+
+    report_currently_running_pipelines(converge_result)
+    update_metrics(converge_result)
+    dispatch_events(converge_result)
+
+    converge_result
+  rescue => e
+    logger.error("An exception happened when converging configuration", :exception => e.class, :message => e.message, :backtrace => e.backtrace)
   end
 
   # Calculate the Logstash uptime in milliseconds
@@ -129,14 +166,19 @@ def uptime
     ((Time.now.to_f - STARTED_AT.to_f) * 1000.0).to_i
   end
 
-  def stop_collecting_metrics
-    @periodic_pollers.stop
+  def shutdown
+    stop_collecting_metrics
+    stop_webserver
+    transition_to_stopped
+    converge_result = shutdown_pipelines
+    converge_result
   end
 
-  def shutdown
+  def force_shutdown!
     stop_collecting_metrics
     stop_webserver
-    shutdown_pipelines
+    transition_to_stopped
+    force_shutdown_pipelines!
   end
 
   def id
@@ -177,14 +219,26 @@ def id_path
     @id_path ||= ::File.join(settings.get("path.data"), "uuid")
   end
 
+  def get_pipeline(pipeline_id)
+    @pipelines_mutex.synchronize do
+      @pipelines[pipeline_id]
+    end
+  end
+
+  def pipelines_count
+    @pipelines_mutex.synchronize do
+      pipelines.size
+    end
+  end
+
   def running_pipelines
-    @upgrade_mutex.synchronize do
+    @pipelines_mutex.synchronize do
       @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }
     end
   end
 
   def running_pipelines?
-    @upgrade_mutex.synchronize do
+    @pipelines_mutex.synchronize do
       @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }.any?
     end
   end
@@ -204,6 +258,93 @@ def close_pipelines
   end
 
   private
+  def transition_to_stopped
+    @running.make_false
+  end
+
+  def transition_to_running
+    @running.make_true
+  end
+
+  # We depends on a series of task derived from the internal state and what
+  # need to be run, theses actions are applied to the current pipelines to converge to
+  # the desired state.
+  #
+  # The current actions are simple and favor composition, allowing us to experiment with different
+  # way to making them and also test them in isolation with the current running agent.
+  #
+  # Currently only action related to pipeline exist, but nothing prevent us to use the same logic
+  # for other tasks.
+  #
+  def converge_state(pipeline_actions)
+    logger.debug("Converging pipelines")
+
+    converge_result = LogStash::ConvergeResult.new(pipeline_actions.size)
+
+    logger.debug("Needed actions to converge", :actions_count => pipeline_actions.size) unless pipeline_actions.empty?
+
+    pipeline_actions.each do |action|
+      # We execute every task we need to converge the current state of pipelines
+      # for every task we will record the action result, that will help us
+      # the results of all the task will determine if the converge was successful or not
+      #
+      # The ConvergeResult#add, will accept the following values
+      #  - boolean
+      #  - FailedAction
+      #  - SuccessfulAction
+      #  - Exception
+      #
+      # This give us a bit more extensibility with the current startup/validation model
+      # that we currently have.
+      begin
+        logger.debug("Executing action", :action => action)
+        action_result = action.execute(@pipelines)
+        converge_result.add(action, action_result)
+
+        unless action_result.successful?
+          logger.error("Failed to execute action", :id => action.pipeline_id,
+                       :action_type => action_result.class, :message => action_result.message)
+        end
+      rescue SystemExit => e
+        converge_result.add(action, e)
+      rescue Exception => e
+        logger.error("Failed to execute action", :action => action, :exception => e.class.name, :message => e.message)
+        converge_result.add(action, e)
+      end
+    end
+
+    if logger.trace?
+      logger.trace("Converge results", :success => converge_result.success?,
+                   :failed_actions => converge_result.failed_actions.collect { |a, r| "id: #{a.pipeline_id}, action_type: #{a.class}, message: #{r.message}" },
+                   :successful_actions => converge_result.successful_actions.collect { |a, r| "id: #{a.pipeline_id}, action_type: #{a.class}" })
+    end
+
+    converge_result
+  end
+
+  def resolve_actions(pipeline_configs)
+    @state_resolver.resolve(@pipelines, pipeline_configs)
+  end
+
+  def report_currently_running_pipelines(converge_result)
+    if converge_result.success? && converge_result.total > 0
+      number_of_running_pipeline = running_pipelines.size
+      logger.info("Pipelines running", :count => number_of_running_pipeline, :pipelines => running_pipelines.values.collect(&:pipeline_id) )
+    end
+  end
+
+  def dispatch_events(converge_results)
+    converge_results.successful_actions.each do |action, _|
+      case action
+      when LogStash::PipelineAction::Create
+        dispatcher.fire(:pipeline_started, get_pipeline(action.pipeline_id))
+      when LogStash::PipelineAction::Reload
+        dispatcher.fire(:pipeline_stopped, get_pipeline(action.pipeline_id))
+      when LogStash::PipelineAction::Stop
+        dispatcher.fire(:pipeline_started, get_pipeline(action.pipeline_id))
+      end
+    end
+  end
 
   def start_webserver
     options = {:http_host => @http_host, :http_ports => @http_port, :http_environment => @http_environment }
@@ -232,224 +373,94 @@ def configure_metrics_collectors
     @periodic_pollers.start
   end
 
-  def reset_pipeline_metrics(id)
-    # selectively reset metrics we don't wish to keep after reloading
-    # these include metrics about the plugins and number of processed events
-    # we want to keep other metrics like reload counts and error messages
-    @collector.clear("stats/pipelines/#{id}/plugins")
-    @collector.clear("stats/pipelines/#{id}/events")
+  def stop_collecting_metrics
+    @periodic_pollers.stop
   end
 
   def collect_metrics?
     @collect_metric
   end
 
-  def increment_reload_failures_metrics(id, message, backtrace = nil)
-    @instance_reload_metric.increment(:failures)
-    @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
-      n.increment(:failures)
-      n.gauge(:last_error, { :message => message, :backtrace =>backtrace})
-      n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
-    end
-    if @logger.debug?
-      @logger.error("Cannot load an invalid configuration", :reason => message, :backtrace => backtrace)
-    else
-      @logger.error("Cannot load an invalid configuration", :reason => message)
+  def force_shutdown_pipelines!
+    @pipelines.each do |_, pipeline|
+      # TODO(ph): should it be his own action?
+      pipeline.force_shutdown!
     end
   end
 
-  # create a new pipeline with the given settings and config, if the pipeline initialization failed
-  # increment the failures metrics
-  # @param settings [Settings] the setting for the new pipelines
-  # @param config [String] the configuration string or nil to fetch the configuration per settings
-  # @return [Pipeline] the new pipeline or nil if it failed
-  def create_pipeline(settings, config = nil)
-    if config.nil?
-      begin
-        config = fetch_config(settings)
-      rescue => e
-        @logger.error("failed to fetch pipeline configuration", :message => e.message)
-        return nil
-      end
-    end
-
-    begin
-      LogStash::Pipeline.new(config, settings, metric)
-    rescue => e
-      increment_reload_failures_metrics(settings.get("pipeline.id"), e.message, e.backtrace)
-      return nil
+  def shutdown_pipelines
+    logger.debug("Shutting down all pipelines", :pipelines_count => pipelines_count)
+
+    # In this context I could just call shutdown, but I've decided to
+    # use the stop action implementation for that so we have the same code.
+    # This also give us some context into why a shutdown is failing
+    @pipelines_mutex.synchronize do
+      pipeline_actions = resolve_actions([]) # We stop all the pipeline, so we converge to a empty state
+      converge_state(pipeline_actions)
     end
   end
 
-  def fetch_config(settings)
-    @config_loader.format_config(settings.get("path.config"), settings.get("config.string"))
+  def running_pipeline?(pipeline_id)
+    thread = @pipelines[pipeline_id].thread
+    thread.is_a?(Thread) && thread.alive?
   end
 
-  # reload_pipeline trys to reloads the pipeline with id using a potential new configuration if it changed
-  # since this method modifies the @pipelines hash it is wrapped in @upgrade_mutex in the parent call `reload_state!`
-  # @param id [String] the pipeline id to reload
-  def reload_pipeline!(id)
-    old_pipeline = @pipelines[id]
-    new_config = fetch_config(old_pipeline.settings)
-
-    if old_pipeline.config_str == new_config
-      @logger.debug("no configuration change for pipeline", :pipeline => id)
-      return
-    end
-
-    # check if this pipeline is not reloadable. it should not happen as per the check below
-    # but keep it here as a safety net if a reloadable pipeline was reloaded with a non reloadable pipeline
-    if !old_pipeline.reloadable?
-      @logger.error("pipeline is not reloadable", :pipeline => id)
-      return
-    end
-
-    # BasePipeline#initialize will compile the config, and load all plugins and raise an exception
-    # on an invalid configuration
-    begin
-      pipeline_validator = LogStash::BasePipeline.new(new_config, old_pipeline.settings)
-    rescue => e
-      increment_reload_failures_metrics(id, e.message, e.backtrace)
-      return
-    end
-
-    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort
-    if !pipeline_validator.reloadable?
-      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => id, :plugins => pipeline_validator.non_reloadable_plugins.map(&:class))
-      increment_reload_failures_metrics(id, "non reloadable pipeline")
-      return
-    end
-
-    # we know configis valid so we are fairly comfortable to first stop old pipeline and then start new one
-    upgrade_pipeline(id, old_pipeline.settings, new_config)
+  def clean_state?
+    @pipelines.empty?
   end
 
-  # upgrade_pipeline first stops the old pipeline and starts the new one
-  # this method exists only for specs to be able to expects this to be executed
-  # @params pipeline_id [String] the pipeline id to upgrade
-  # @params settings [Settings] the settings for the new pipeline
-  # @params new_config [String] the new pipeline config
-  def upgrade_pipeline(pipeline_id, settings, new_config)
-    @logger.warn("fetched new config for pipeline. upgrading..", :pipeline => pipeline_id, :config => new_config)
-
-    # first step: stop the old pipeline.
-    # IMPORTANT: a new pipeline with same settings should not be instantiated before the previous one is shutdown
-
-    stop_pipeline(pipeline_id)
-    reset_pipeline_metrics(pipeline_id)
-
-    # second step create and start a new pipeline now that the old one is shutdown
+  def setting(key)
+    @settings.get(key)
+  end
 
-    new_pipeline = create_pipeline(settings, new_config)
-    if new_pipeline.nil?
-      # this is a scenario where the configuration is valid (compilable) but the new pipeline refused to start
-      # and at this point NO pipeline is running
-      @logger.error("failed to create the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
-      increment_reload_failures_metrics(pipeline_id, "failed to create the reloaded pipeline")
-      return
+  # Methods related to the creation of all metrics
+  # related to states changes and failures
+  #
+  # I think we could use an observer here to decouple the metrics, but moving the code
+  # into separate function is the first step we take.
+  def update_metrics(converge_result)
+    converge_result.failed_actions.each do |action, action_result|
+      update_failures_metrics(action, action_result)
     end
 
-    ### at this point pipeline#close must be called if upgrade_pipeline does not succeed
-
-    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort. this should normally not
-    # happen since the check should be done in reload_pipeline! prior to get here.
-    if !new_pipeline.reloadable?
-      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => pipeline_id, :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
-      increment_reload_failures_metrics(pipeline_id, "non reloadable pipeline")
-      new_pipeline.close
-      return
+    converge_result.successful_actions.each do |action, action_result|
+      update_success_metrics(action, action_result)
     end
+  end
 
-    # @pipelines[pipeline_id] must be initialized before #start_pipeline below which uses it
-    @pipelines[pipeline_id] = new_pipeline
-
-    if !start_pipeline(pipeline_id)
-      @logger.error("failed to start the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
-      # do not call increment_reload_failures_metrics here since #start_pipeline already does it on failure
-      new_pipeline.close
-      return
+  def update_success_metrics(action, action_result)
+    case action
+      when LogStash::PipelineAction::Create
+        # When a pipeline is successfully created we create the metric
+        # place holder related to the lifecycle of the pipeline
+        initialize_pipeline_metrics(action)
+      when LogStash::PipelineAction::Reload
+        update_successful_reload_metrics(action, action_result)
     end
+  end
 
-    # pipeline started successfully, update reload success metrics
-    @instance_reload_metric.increment(:successes)
-    @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
-      n.increment(:successes)
-      n.gauge(:last_success_timestamp, LogStash::Timestamp.now)
+  def update_failures_metrics(action, action_result)
+    if action.is_a?(LogStash::PipelineAction::Create)
+      # force to create the metric fields
+      initialize_pipeline_metrics(action)
     end
-  end
 
-  def start_pipeline(id)
-    pipeline = @pipelines[id]
-    return unless pipeline.is_a?(LogStash::Pipeline)
-    return if pipeline.ready?
-    @logger.debug("starting pipeline", :id => id)
-    t = Thread.new do
-      LogStash::Util.set_thread_name("pipeline.#{id}")
-      begin
-        pipeline.run
-      rescue => e
-        @instance_reload_metric.increment(:failures)
-        @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
-          n.increment(:failures)
-          n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
-          n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
-        end
-        @logger.error("Pipeline aborted due to error", :exception => e, :backtrace => e.backtrace)
+    @instance_reload_metric.increment(:failures)
 
-        # TODO: this is weird, why dont we return directly here? any reason we need to enter the while true loop below?!
-      end
-    end
-    while true do
-      if !t.alive?
-        return false
-      elsif pipeline.running?
-        dispatcher.fire(:pipeline_started, pipeline)
-        return true
-      else
-        sleep 0.01
-      end
+    @pipeline_reload_metric.namespace([action.pipeline_id, :reloads]).tap do |n|
+      n.increment(:failures)
+      n.gauge(:last_error, { :message => action_result.message, :backtrace => action_result.backtrace})
+      n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
     end
   end
 
-  def stop_pipeline(id)
-    pipeline = @pipelines[id]
-    return unless pipeline
-    @logger.warn("stopping pipeline", :id => id)
-    pipeline.shutdown { LogStash::ShutdownWatcher.start(pipeline) }
-    @pipelines[id].thread.join
-    dispatcher.fire(:pipeline_stopped, pipeline)
-  end
-
-  def start_pipelines
+  def initialize_agent_metrics
     @instance_reload_metric.increment(:successes, 0)
     @instance_reload_metric.increment(:failures, 0)
-    @pipelines.each do |id, pipeline|
-      start_pipeline(id)
-      pipeline.collect_stats
-      # no reloads yet, initialize all the reload metrics
-      init_pipeline_reload_metrics(id)
-    end
   end
 
-  def shutdown_pipelines
-    @pipelines.each { |id, _| stop_pipeline(id) }
-  end
-
-  def running_pipeline?(pipeline_id)
-    thread = @pipelines[pipeline_id].thread
-    thread.is_a?(Thread) && thread.alive?
-  end
-
-  def clean_state?
-    @pipelines.empty?
-  end
-
-  def setting(key)
-    @settings.get(key)
-  end
-
-  def init_pipeline_reload_metrics(id)
-    @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+  def initialize_pipeline_metrics(action)
+    @pipeline_reload_metric.namespace([action.pipeline_id, :reloads]).tap do |n|
       n.increment(:successes, 0)
       n.increment(:failures, 0)
       n.gauge(:last_error, nil)
@@ -457,4 +468,13 @@ def init_pipeline_reload_metrics(id)
       n.gauge(:last_failure_timestamp, nil)
     end
   end
+
+  def update_successful_reload_metrics(action, action_result)
+    @instance_reload_metric.increment(:successes)
+
+    @pipeline_reload_metric.namespace([action.pipeline_id, :reloads]).tap do |n|
+      n.increment(:successes)
+      n.gauge(:last_success_timestamp, action_result.executed_at)
+    end
+  end
 end # class LogStash::Agent
diff --git a/logstash-core/lib/logstash/bootstrap_check/bad_java.rb b/logstash-core/lib/logstash/bootstrap_check/bad_java.rb
new file mode 100644
index 00000000000..5bbcd1f47fe
--- /dev/null
+++ b/logstash-core/lib/logstash/bootstrap_check/bad_java.rb
@@ -0,0 +1,16 @@
+# encoding: utf-8
+require "logstash/util"
+require "logstash/util/java_version"
+require "logstash/errors"
+
+module LogStash module BootstrapCheck
+  class BadJava
+    def self.check(settings)
+      # Exit on bad java versions
+      LogStash::Util::JavaVersion.validate_java_version!
+    rescue => e
+      # Just rewrap the original exception
+      raise LogStash::BootstrapCheckError, e.message
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/bootstrap_check/bad_ruby.rb b/logstash-core/lib/logstash/bootstrap_check/bad_ruby.rb
new file mode 100644
index 00000000000..3c090bf0eef
--- /dev/null
+++ b/logstash-core/lib/logstash/bootstrap_check/bad_ruby.rb
@@ -0,0 +1,12 @@
+# encoding: utf-8
+require "logstash/errors"
+
+module LogStash module BootstrapCheck
+  class BadRuby
+    def self.check(settings)
+      if RUBY_VERSION < "1.9.2"
+        raise LogStash::BootstrapCheckError, "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
+      end
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/bootstrap_check/default_config.rb b/logstash-core/lib/logstash/bootstrap_check/default_config.rb
new file mode 100644
index 00000000000..04e9039826b
--- /dev/null
+++ b/logstash-core/lib/logstash/bootstrap_check/default_config.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "logstash/errors"
+
+module LogStash module BootstrapCheck
+  class DefaultConfig
+    def self.check(settings)
+      if settings.get("config.string").nil? && settings.get("path.config").nil?
+        raise LogStash::BootstrapCheckError, I18n.t("logstash.runner.missing-configuration")
+      end
+
+      if settings.get("config.reload.automatic") && settings.get("path.config").nil?
+        # there's nothing to reload
+        raise LogStash::BootstrapCheckError, I18n.t("logstash.runner.reload-without-config-path")
+      end
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/config/config_part.rb b/logstash-core/lib/logstash/config/config_part.rb
new file mode 100644
index 00000000000..7f04254d3ea
--- /dev/null
+++ b/logstash-core/lib/logstash/config/config_part.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+
+module LogStash module Config
+ class ConfigPart
+   attr_reader :reader, :source_id, :config_string
+
+   def initialize(reader, source_id, config_string)
+     @reader = reader
+     @source_id = source_id
+     @config_string = config_string
+   end
+ end
+end end
diff --git a/logstash-core/lib/logstash/config/loader.rb b/logstash-core/lib/logstash/config/loader.rb
deleted file mode 100644
index d894bd71bee..00000000000
--- a/logstash-core/lib/logstash/config/loader.rb
+++ /dev/null
@@ -1,107 +0,0 @@
-require "logstash/config/defaults"
-
-module LogStash; module Config; class Loader
-  def initialize(logger)
-    @logger = logger
-    @config_debug = LogStash::SETTINGS.get_value("config.debug")
-  end
-
-  def format_config(config_path, config_string)
-    config_string = config_string.to_s
-    if config_path
-      # Append the config string.
-      # This allows users to provide both -f and -e flags. The combination
-      # is rare, but useful for debugging.
-      loaded_config = load_config(config_path)
-      if loaded_config.empty? && config_string.empty?
-        # If loaded config from `-f` is empty *and* if config string is empty we raise an error
-        fail(I18n.t("logstash.runner.configuration.file-not-found", :path => config_path))
-      end
-
-      # tell the user we are merging, otherwise it is very confusing
-      if !loaded_config.empty? && !config_string.empty?
-        @logger.info("Created final config by merging config string and config path", :path => config_path)
-      end
-
-      config_string = config_string + loaded_config
-    else
-      # include a default stdin input if no inputs given
-      if config_string !~ /input *{/
-        config_string += LogStash::Config::Defaults.input
-      end
-      # include a default stdout output if no outputs given
-      if config_string !~ /output *{/
-        config_string += LogStash::Config::Defaults.output
-      end
-    end
-    config_string
-  end
-
-  def load_config(path)
-    begin
-      uri = URI.parse(path)
-
-      case uri.scheme
-      when nil then
-        local_config(path)
-      when /http/ then
-        fetch_config(uri)
-      when "file" then
-        local_config(uri.path)
-      else
-        fail(I18n.t("logstash.runner.configuration.scheme-not-supported", :path => path))
-      end
-    rescue URI::InvalidURIError
-      # fallback for windows.
-      # if the parsing of the file failed we assume we can reach it locally.
-      # some relative path on windows arent parsed correctly (.\logstash.conf)
-      local_config(path)
-    end
-  end
-
-  def local_config(path)
-    path = ::File.expand_path(path)
-    path = ::File.join(path, "*") if ::File.directory?(path)
-
-    config = ""
-    if Dir.glob(path).length == 0
-      @logger.info("No config files found in path", :path => path)
-      return config
-    end
-
-    encoding_issue_files = []
-    Dir.glob(path).sort.each do |file|
-      next unless ::File.file?(file)
-      if file.match(/~$/)
-        @logger.debug("NOT reading config file because it is a temp file", :config_file => file)
-        next
-      end
-      @logger.debug("Reading config file", :config_file => file)
-      cfg = ::File.read(file)
-      if !cfg.ascii_only? && !cfg.valid_encoding?
-        encoding_issue_files << file
-      end
-      config << cfg + "\n"
-      if @config_debug
-        @logger.debug? && @logger.debug("\nThe following is the content of a file", :config_file => file.to_s)
-        @logger.debug? && @logger.debug("\n" + cfg + "\n\n")
-      end
-    end
-    if encoding_issue_files.any?
-      fail("The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}")
-    end
-    if @config_debug
-      @logger.debug? && @logger.debug("\nThe following is the merged configuration")
-      @logger.debug? && @logger.debug("\n" + config + "\n\n")
-    end
-    return config
-  end # def load_config
-
-  def fetch_config(uri)
-    begin
-      Net::HTTP.get(uri) + "\n"
-    rescue Exception => e
-      fail(I18n.t("logstash.runner.configuration.fetch-failed", :path => uri.to_s, :message => e.message))
-    end
-  end
-end end end
diff --git a/logstash-core/lib/logstash/config/pipeline_config.rb b/logstash-core/lib/logstash/config/pipeline_config.rb
new file mode 100644
index 00000000000..e4bbcd53cfc
--- /dev/null
+++ b/logstash-core/lib/logstash/config/pipeline_config.rb
@@ -0,0 +1,42 @@
+# encoding: utf-8
+require "digest"
+
+module LogStash module Config
+  class PipelineConfig
+    include LogStash::Util::Loggable
+
+    attr_reader :source, :pipeline_id, :config_parts, :settings, :read_at
+
+    def initialize(source, pipeline_id, config_parts, settings)
+      @source = source
+      @pipeline_id = pipeline_id
+      @config_parts = Array(config_parts).sort_by { |config_part| [config_part.reader.to_s, config_part.source_id] }
+      @settings = settings
+      @read_at = Time.now
+    end
+
+    def config_hash
+      @config_hash ||= Digest::SHA1.hexdigest(config_string)
+    end
+
+    def config_string
+      @config_string = config_parts.collect(&:config_string).join("\n")
+    end
+
+    def ==(other)
+      config_hash == other.config_hash && pipeline_id == other.pipeline_id
+    end
+
+    def display_debug_information
+      logger.debug("-------- Logstash Config ---------")
+      logger.debug("Config from source", :source => source, :pipeline_id => pipeline_id)
+
+      config_parts.each do |config_part|
+        logger.debug("Config string", :reader => config_part.reader, :source_id => config_part.source_id)
+        logger.debug("\n\n#{config_part.config_string}")
+      end
+      logger.debug("Merged config")
+      logger.debug("\n\n#{config_string}")
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/config/source/base.rb b/logstash-core/lib/logstash/config/source/base.rb
new file mode 100644
index 00000000000..db48f3e3837
--- /dev/null
+++ b/logstash-core/lib/logstash/config/source/base.rb
@@ -0,0 +1,16 @@
+# encoding: utf-8
+module LogStash module Config module Source
+  class Base
+    def initialize(settings)
+      @settings = settings
+    end
+
+    def pipeline_configs
+      raise NotImplementedError, "`#pipeline_configs` must be implemented!"
+    end
+
+    def match?
+      raise NotImplementedError, "`match?` must be implemented!"
+    end
+  end
+end end end
diff --git a/logstash-core/lib/logstash/config/source/local.rb b/logstash-core/lib/logstash/config/source/local.rb
new file mode 100644
index 00000000000..34ffcc26a41
--- /dev/null
+++ b/logstash-core/lib/logstash/config/source/local.rb
@@ -0,0 +1,215 @@
+# encoding: utf-8
+require "logstash/config/source/base"
+require "logstash/config/config_part"
+require "logstash/config/pipeline_config"
+require "logstash/util/loggable"
+require "logstash/errors"
+require "uri"
+
+module LogStash module Config module Source
+  # A locally defined configuration source
+  #
+  # Which can aggregate the following config options:
+  #  - settings.config_string: "input { stdin {} }"
+  #  - settings.config_path: /tmp/logstash/*.conf
+  #  - settings.config_path: http://localhost/myconfig.conf
+  #
+  #  All theses option will create a unique pipeline, generated parts will be
+  #  sorted alphabetically. Se `PipelineConfig` class for the sorting algorithm.
+  #
+  class Local < Base
+    class ConfigStringLoader
+      def self.read(config_string)
+        [ConfigPart.new(self.name, "config_string", config_string)]
+      end
+    end
+
+    class ConfigPathLoader
+      include LogStash::Util::Loggable
+
+      TEMPORARY_FILE_RE = /~$/
+      LOCAL_FILE_URI = /^file:\/\//i
+
+      def initialize(path)
+        @path = normalize_path(path)
+      end
+
+      def read
+        config_parts = []
+        encoding_issue_files = []
+
+        get_files.each do |file|
+          next unless ::File.file?(file) # skip directory
+
+          logger.debug("Reading config file", :config_file => file)
+
+          if temporary_file?(file)
+            logger.warn("NOT reading config file because it is a temp file", :config_file => file)
+            next
+          end
+
+          config_string = ::File.read(file)
+
+          if valid_encoding?(config_string)
+            config_parts << ConfigPart.new(self.class.name, file, config_string)
+          else
+            encoding_issue_files << file
+          end
+        end
+
+        if encoding_issue_files.any?
+          raise LogStash::ConfigLoadingError, "The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}"
+        end
+
+        if config_parts.empty?
+          logger.info("No config files found in path", :path => path)
+        end
+
+        config_parts
+      end
+
+      def self.read(path)
+        ConfigPathLoader.new(path).read
+      end
+
+      private
+      def normalize_path(path)
+        path.gsub!(LOCAL_FILE_URI, "")
+        ::File.expand_path(path)
+      end
+
+      def get_files
+        Dir.glob(path).sort
+      end
+
+      def path
+        if ::File.directory?(@path)
+          ::File.join(@path, "*")
+        else
+          @path
+        end
+      end
+
+      def valid_encoding?(content)
+        content.ascii_only? && content.valid_encoding?
+      end
+
+      def temporary_file?(filepath)
+        filepath.match(TEMPORARY_FILE_RE)
+      end
+    end
+
+    class ConfigRemoteLoader
+      def self.read(uri)
+        uri = URI.parse(uri)
+
+        Net::HTTP.start(uri.host, uri.port, :use_ssl => uri.scheme == "https") do |http|
+          request = Net::HTTP::Get.new(uri.path)
+          response = http.request(request)
+
+          # since we have fetching config we wont follow any redirection.
+          case response.code.to_i
+          when 200
+            [ConfigPart.new(self.name, uri.to_s, response.body)]
+          when 302
+            raise LogStash::ConfigLoadingError, I18n.t("logstash.runner.configuration.fetch-failed", :path => uri.to_s, :message => "We don't follow redirection for remote configuration")
+          when 404
+            raise LogStash::ConfigLoadingError, I18n.t("logstash.runner.configuration.fetch-failed", :path => uri.to_s, :message => "File not found")
+          when 403
+            raise LogStash::ConfigLoadingError, I18n.t("logstash.runner.configuration.fetch-failed", :path => uri.to_s, :message => "Permission denied")
+          when 500
+            raise LogStash::ConfigLoadingError, I18n.t("logstash.runner.configuration.fetch-failed", :path => uri.to_s, :message => "500 error on remote host")
+          else
+            raise LogStash::ConfigLoadingError, I18n.t("logstash.runner.configuration.fetch-failed", :path => uri.to_s, :message => "code: #{response.code}, message: #{response.class.to_s}")
+          end
+        end
+      end
+    end
+
+    PIPELINE_ID = LogStash::SETTINGS.get("pipeline.id").to_sym
+    HTTP_RE = /^http(s)?/
+    INPUT_BLOCK_RE = /input *{/
+    OUTPUT_BLOCK_RE = /output *{/
+
+    def pipeline_configs
+      config_parts = []
+
+      config_parts.concat(ConfigStringLoader.read(config_string)) if config_string?
+      if local_config?
+        local_config_parts = ConfigPathLoader.read(config_path)
+        config_parts.concat(local_config_parts)
+      else
+        local_config_parts = []
+      end
+
+      config_parts.concat(ConfigRemoteLoader.read(config_path)) if remote_config?
+
+      return if config_parts.empty?
+      return if config_string? && config_string.strip.empty? && local_config? && local_config_parts.empty?
+
+      add_missing_default_inputs_or_outputs(config_parts)
+
+      PipelineConfig.new(self.class, PIPELINE_ID, config_parts, @settings)
+    end
+
+    def match?
+      config_string? || config_path?
+    end
+
+    private
+    # Make sure we have an input and at least 1 output
+    # if its not the case we will add stdin and stdout
+    # this is for backward compatibility reason
+    def add_missing_default_inputs_or_outputs(config_parts)
+      if !config_parts.any? { |part| INPUT_BLOCK_RE.match(part.config_string) }
+        config_parts << LogStash::Config::ConfigPart.new(self.class.name, "default input", LogStash::Config::Defaults.input)
+      end
+
+      # include a default stdout output if no outputs given
+      if !config_parts.any? { |part| OUTPUT_BLOCK_RE.match(part.config_string) }
+        config_parts << LogStash::Config::ConfigPart.new(self.class.name, "default output", LogStash::Config::Defaults.output)
+      end
+    end
+
+    def config_string
+      @settings.get("config.string")
+    end
+
+    def config_string?
+      !config_string.nil?
+    end
+
+    def config_path
+      @settings.get("path.config")
+    end
+
+    def config_path?
+      !config_path.nil? && !config_path.empty?
+    end
+
+    def local_config?
+      return false unless config_path?
+
+      begin
+        uri = URI.parse(config_path)
+        uri.scheme == "file" || uri.scheme.nil?
+      rescue URI::InvalidURIError
+        # fallback for windows.
+        # if the parsing of the file failed we assume we can reach it locally.
+        # some relative path on windows arent parsed correctly (.\logstash.conf)
+        true
+      end
+    end
+
+    def remote_config?
+      return false unless config_path?
+
+      begin
+        uri = URI.parse(config_path)
+        uri.scheme =~ HTTP_RE
+      rescue URI::InvalidURIError
+        false
+      end
+    end
+  end
+end end end
diff --git a/logstash-core/lib/logstash/config/source_loader.rb b/logstash-core/lib/logstash/config/source_loader.rb
new file mode 100644
index 00000000000..06310f55d32
--- /dev/null
+++ b/logstash-core/lib/logstash/config/source_loader.rb
@@ -0,0 +1,113 @@
+# encoding: utf-8
+require "logstash/config/source/local"
+require "logstash/errors"
+require "thread"
+require "set"
+
+module LogStash module Config
+  class SourceLoader
+    class SuccessfulFetch
+      attr_reader :response
+
+      def initialize(response)
+        @response = response
+      end
+
+      def success?
+        true
+      end
+    end
+
+    class FailedFetch
+      attr_reader :error
+
+      def initialize(error)
+        @error = error
+      end
+
+      def success?
+        false
+      end
+    end
+
+    include LogStash::Util::Loggable
+
+    def initialize(settings = LogStash::SETTINGS)
+      @sources_lock = Mutex.new
+      @sources = Set.new
+      @settings = settings
+    end
+
+    # This return a ConfigLoader object that will
+    # abstract the call to the different sources and will return multiples pipeline
+    def fetch
+      sources_loaders = []
+
+      sources do |source|
+        sources_loaders << source if source.match?
+      end
+
+      if sources_loaders.empty?
+        # This shouldn't happen with the settings object or with any external plugins.
+        # but lets add a guard so we fail fast.
+        raise LogStash::InvalidSourceLoaderSettingError, "Can't find an appropriate config loader with current settings"
+      else
+        begin
+          pipeline_configs = sources_loaders
+            .collect { |source| Array(source.pipeline_configs) }
+            .compact
+            .flatten
+
+          if config_debug?
+            pipeline_configs.each { |pipeline_config| pipeline_config.display_debug_information }
+          end
+
+          if pipeline_configs.empty?
+            logger.error("No configuration found in the configured sources.")
+          end
+
+          SuccessfulFetch.new(pipeline_configs)
+        rescue => e
+          logger.error("Could not fetch all the sources", :exception => e.class, :message => e.message)
+          FailedFetch.new(e.message)
+        end
+      end
+    end
+
+    def sources
+      @sources_lock.synchronize do
+        if block_given?
+          @sources.each do |source|
+            yield source
+          end
+        else
+          @sources
+        end
+      end
+    end
+
+    def remove_source(klass)
+      @sources_lock.synchronize do
+        @sources.delete_if { |source| source == klass || source.is_a?(klass) }
+      end
+    end
+
+    def configure_sources(new_sources)
+      new_sources = Array(new_sources).to_set
+      logger.debug("Configure sources", :sources => new_sources.collect(&:to_s))
+      @sources_lock.synchronize { @sources = new_sources }
+    end
+
+    def add_source(new_source)
+      logger.debug("Adding source", :source => new_source.to_s)
+      @sources_lock.synchronize { @sources << new_source}
+    end
+
+    private
+    def config_debug?
+      @settings.get_value("config.debug") && logger.debug?
+    end
+  end
+
+  SOURCE_LOADER = SourceLoader.new
+end end
diff --git a/logstash-core/lib/logstash/converge_result.rb b/logstash-core/lib/logstash/converge_result.rb
new file mode 100644
index 00000000000..099749cb63d
--- /dev/null
+++ b/logstash-core/lib/logstash/converge_result.rb
@@ -0,0 +1,103 @@
+# encoding: utf-8
+require "logstash/errors"
+
+module LogStash
+  # This class allow us to keep track and uniform all the return values from the
+  # action task
+  class ConvergeResult
+    class ActionResult
+      attr_reader :executed_at
+
+      def initialize
+        @executed_at = LogStash::Timestamp.now
+      end
+
+      # Until all the action have more granularity in the validation
+      # or execution we make the ConvergeResult works with primitives and exceptions
+      def self.create(action, action_result)
+        if action_result.is_a?(ActionResult)
+          action_result
+        elsif action_result.is_a?(Exception)
+          FailedAction.from_exception(action_result)
+        elsif action_result == true
+          SuccessfulAction.new
+        elsif action_result == false
+          FailedAction.from_action(action)
+        else
+          raise LogStash::Error, "Don't know how to handle `#{action_result.class}` for `#{action}`"
+        end
+      end
+    end
+
+    class FailedAction < ActionResult
+      attr_reader :message, :backtrace
+
+      def initialize(message, backtrace = nil)
+        super()
+
+        @message = message
+        @backtrace = backtrace
+      end
+
+      def self.from_exception(exception)
+        FailedAction.new(exception.message, exception.backtrace)
+      end
+
+      def self.from_action(action)
+        FailedAction.new("Could not execute action: #{action}")
+      end
+
+      def successful?
+        false
+      end
+    end
+
+    class SuccessfulAction < ActionResult
+      def successful?
+        true
+      end
+    end
+
+    def initialize(expected_actions_count)
+      @expected_actions_count = expected_actions_count
+      @actions = {}
+    end
+
+    def add(action, action_result)
+      @actions[action] = ActionResult.create(action, action_result)
+    end
+
+    def failed_actions
+      filter_by_successful_state(false)
+    end
+
+    def successful_actions
+      filter_by_successful_state(true)
+    end
+
+    def complete?
+      total == @expected_actions_count
+    end
+
+    def success?
+      failed_actions.empty? && complete?
+    end
+
+    def fails_count
+      failed_actions.size
+    end
+
+    def success_count
+      successful_actions.size
+    end
+
+    def total
+      @actions.size
+    end
+
+    private
+    def filter_by_successful_state(predicate)
+      @actions.select { |action, action_result| action_result.successful? == predicate }
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 8f5f9d33380..481ecaf150c 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -30,6 +30,7 @@ module Environment
    Setting::PositiveInteger.new("pipeline.batch.size", 125),
            Setting::Numeric.new("pipeline.batch.delay", 5), # in milliseconds
            Setting::Boolean.new("pipeline.unsafe_shutdown", false),
+           Setting::Boolean.new("pipeline.reloadable", true),
                     Setting.new("path.plugins", Array, []),
     Setting::NullableString.new("interactive", nil, false),
            Setting::Boolean.new("config.debug", false),
diff --git a/logstash-core/lib/logstash/errors.rb b/logstash-core/lib/logstash/errors.rb
index 8960a7f12bc..d684087b764 100644
--- a/logstash-core/lib/logstash/errors.rb
+++ b/logstash-core/lib/logstash/errors.rb
@@ -6,7 +6,10 @@ class ConfigurationError < Error; end
   class PluginLoadingError < Error; end
   class ShutdownSignal < StandardError; end
   class PluginNoVersionError < Error; end
+  class BootstrapCheckError < Error; end
 
   class Bug < Error; end
   class ThisMethodWasRemoved < Bug; end
+  class ConfigLoadingError < Error; end
+  class InvalidSourceLoaderSettingError < Error; end
 end
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 8bbb9207e9d..760f791adab 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -27,15 +27,17 @@
 module LogStash; class BasePipeline
   include LogStash::Util::Loggable
 
-  attr_reader :config_str, :config_hash, :inputs, :filters, :outputs, :pipeline_id, :lir
-  
-  def initialize(config_str, settings = SETTINGS)
+  attr_reader :settings, :config_str, :config_hash, :inputs, :filters, :outputs, :pipeline_id, :lir
+
+  def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     @logger = self.logger
+
     @config_str = config_str
+    @settings = settings
     @config_hash = Digest::SHA1.hexdigest(@config_str)
-    
+
     @lir = compile_lir
-    
+
     # Every time #plugin is invoked this is incremented to give each plugin
     # a unique id when auto-generating plugin ids
     @plugin_counter ||= 0
@@ -58,7 +60,7 @@ def initialize(config_str, settings = SETTINGS)
     # config_code = BasePipeline.compileConfig(config_str)
 
     if settings.get_value("config.debug") && @logger.debug?
-      @logger.debug("Compiled pipeline code", :code => config_code)
+      @logger.debug("Compiled pipeline code", default_logging_keys(:code => config_code))
     end
 
     # Evaluate the config compiled code that will initialize all the plugins and define the
@@ -104,13 +106,23 @@ def plugin(plugin_type, name, *args)
       FilterDelegator.new(@logger, klass, type_scoped_metric, @execution_context, args)
     else # input
       input_plugin = klass.new(args)
-      input_plugin.metric = type_scoped_metric.namespace(id)
+      scoped_metric = type_scoped_metric.namespace(id.to_sym)
+      scoped_metric.gauge(:name, input_plugin.config_name)
+      input_plugin.metric = scoped_metric
       input_plugin.execution_context = @execution_context
       input_plugin
     end
   end
 
   def reloadable?
+    configured_as_reloadable? && reloadable_plugins?
+  end
+
+  def configured_as_reloadable?
+    settings.get("pipeline.reloadable")
+  end
+
+  def reloadable_plugins?
     non_reloadable_plugins.empty?
   end
 
@@ -153,7 +165,7 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     begin
       @queue = LogStash::QueueFactory.create(settings)
     rescue => e
-      @logger.error("Logstash failed to create queue", "exception" => e.message, "backtrace" => e.backtrace)
+      @logger.error("Logstash failed to create queue", default_logging_keys("exception" => e.message, "backtrace" => e.backtrace))
       raise e
     end
 
@@ -176,6 +188,7 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     @ready = Concurrent::AtomicBoolean.new(false)
     @running = Concurrent::AtomicBoolean.new(false)
     @flushing = Concurrent::AtomicReference.new(false)
+    @force_shutdown = Concurrent::AtomicBoolean.new(false)
   end # def initialize
   
   
@@ -194,15 +207,14 @@ def safe_pipeline_worker_count
 
     if @settings.set?("pipeline.workers")
       if pipeline_workers > 1
-        @logger.warn("Warning: Manual override - there are filters that might not work with multiple worker threads",
-                     :worker_threads => pipeline_workers, :filters => plugins)
+        @logger.warn("Warning: Manual override - there are filters that might not work with multiple worker threads", default_logging_keys(:worker_threads => pipeline_workers, :filters => plugins))
       end
     else
       # user did not specify a worker thread count
       # warn if the default is multiple
       if default > 1
         @logger.warn("Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads",
-                     :count_was => default, :filters => plugins)
+                     default_logging_keys(:count_was => default, :filters => plugins))
         return 1 # can't allow the default value to propagate if there are unsafe filters
       end
     end
@@ -213,15 +225,61 @@ def filters?
     return @filters.any?
   end
 
+  def start
+    # Since we start lets assume that the metric namespace is cleared
+    # this is useful in the context of pipeline reloading
+    collect_stats
+
+    logger.debug("Starting pipeline", default_logging_keys)
+
+    @finished_execution = Concurrent::AtomicBoolean.new(false)
+
+    @thread = Thread.new do
+      begin
+        LogStash::Util.set_thread_name("pipeline.#{pipeline_id}")
+        run
+        @finished_execution.make_true
+      rescue => e
+        close
+        logger.error("Pipeline aborted due to error", default_logging_keys(:exception => e, :backtrace => e.backtrace))
+      end
+    end
+
+    status = wait_until_started
+
+    if status
+      logger.debug("Pipeline started succesfully", default_logging_keys(:pipeline_id => pipeline_id))
+    end
+
+    status
+  end
+
+  def wait_until_started
+    while true do
+      # This should be changed with an appropriate FSM
+      # It's an edge case, if we have a pipeline with
+      # a generator { count => 1 } its possible that `Thread#alive?` doesn't return true
+      # because the execution of the thread was successful and complete
+      if @finished_execution.true?
+        return true
+      elsif !thread.alive?
+        return false
+      elsif running?
+        return true
+      else
+        sleep 0.01
+      end
+    end
+  end
+
   def run
     @started_at = Time.now
-
     @thread = Thread.current
     Util.set_thread_name("[#{pipeline_id}]-pipeline-manager")
 
     start_workers
 
-    @logger.info("Pipeline #{@pipeline_id} started")
+    @logger.info("Pipeline started", default_logging_keys)
 
     # Block until all inputs have stopped
     # Generally this happens if SIGINT is sent and `shutdown` is called from an external thread
@@ -231,14 +289,14 @@ def run
     wait_inputs
     transition_to_stopped
 
-    @logger.debug("Input plugins stopped! Will shutdown filter/output workers.")
+    @logger.debug("Input plugins stopped! Will shutdown filter/output workers.", default_logging_keys)
 
     shutdown_flusher
     shutdown_workers
 
     close
 
-    @logger.debug("Pipeline #{@pipeline_id} has been shutdown")
+    @logger.debug("Pipeline has been shutdown", default_logging_keys)
 
     # exit code
     return 0
@@ -272,7 +330,7 @@ def register_plugin(plugin)
     plugin.register
     plugin
   rescue => e
-    @logger.error("Error registering plugin", :plugin => plugin.inspect, :error => e.message)
+    @logger.error("Error registering plugin", default_logging_keys(:plugin => plugin.inspect, :error => e.message))
     raise e
   end
 
@@ -305,14 +363,13 @@ def start_workers
       config_metric.gauge(:config_reload_automatic, @settings.get("config.reload.automatic"))
       config_metric.gauge(:config_reload_interval, @settings.get("config.reload.interval"))
 
-      @logger.info("Starting pipeline",
-                   "id" => self.pipeline_id,
-                   "pipeline.workers" => pipeline_workers,
-                   "pipeline.batch.size" => batch_size,
-                   "pipeline.batch.delay" => batch_delay,
-                   "pipeline.max_inflight" => max_inflight)
+      @logger.info("Starting pipeline", default_logging_keys(
+        "pipeline.workers" => pipeline_workers,
+        "pipeline.batch.size" => batch_size,
+        "pipeline.batch.delay" => batch_delay,
+        "pipeline.max_inflight" => max_inflight))
       if max_inflight > MAX_INFLIGHT_WARN_THRESHOLD
-        @logger.warn "CAUTION: Recommended inflight events max exceeded! Logstash will run with up to #{max_inflight} events in memory in your current configuration. If your message sizes are large this may cause instability with the default heap size. Please consider setting a non-standard heap size, changing the batch size (currently #{batch_size}), or changing the number of pipeline workers (currently #{pipeline_workers})"
+        @logger.warn("CAUTION: Recommended inflight events max exceeded! Logstash will run with up to #{max_inflight} events in memory in your current configuration. If your message sizes are large this may cause instability with the default heap size. Please consider setting a non-standard heap size, changing the batch size (currently #{batch_size}), or changing the number of pipeline workers (currently #{pipeline_workers})", default_logging_keys)
       end
 
       pipeline_workers.times do |t|
@@ -354,6 +411,7 @@ def worker_loop(batch_size, batch_delay)
       filter_batch(batch)
       flush_filters_to_batch(batch, :final => false) if signal.flush?
       output_batch(batch)
+      break if @force_shutdown.true? # Do not ack the current batch
       @filter_queue_client.close_batch(batch)
 
       # keep break at end of loop, after the read_batch operation, some pipeline specs rely on this "final read_batch" before shutdown.
@@ -365,12 +423,15 @@ def worker_loop(batch_size, batch_delay)
     batch = @filter_queue_client.new_batch
     @filter_queue_client.start_metrics(batch) # explicitly call start_metrics since we dont do a read_batch here
     flush_filters_to_batch(batch, :final => true)
+    return if @force_shutdown.true? # Do not ack the current batch
     output_batch(batch)
     @filter_queue_client.close_batch(batch)
   end
 
   def filter_batch(batch)
     batch.each do |event|
+      return if @force_shutdown.true?
+
       filter_func(event).each do |e|
         #these are both original and generated events
         batch.merge(e) unless e.cancelled?
@@ -386,7 +447,7 @@ def filter_batch(batch)
     # Users need to check their configuration or see if there is a bug in the
     # plugin.
     @logger.error("Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
-                  "exception" => e.message, "backtrace" => e.backtrace)
+                  default_logging_keys("exception" => e.message, "backtrace" => e.backtrace))
 
     raise e
   end
@@ -408,9 +469,10 @@ def output_batch(batch)
     # Now that we have our output to event mapping we can just invoke each output
     # once with its list of events
     output_events_map.each do |output, events|
+      return if @force_shutdown.true?
       output.multi_receive(events)
     end
-    
+
     @filter_queue_client.add_output_metrics(batch)
   end
 
@@ -448,20 +510,21 @@ def inputworker(plugin)
     rescue => e
       if plugin.stop?
         @logger.debug("Input plugin raised exception during shutdown, ignoring it.",
-                      :plugin => plugin.class.config_name, :exception => e.message,
-                      :backtrace => e.backtrace)
+                      default_logging_keys(:plugin => plugin.class.config_name, :exception => e.message, :backtrace => e.backtrace))
         return
       end
 
       # otherwise, report error and restart
       if @logger.debug?
         @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
-                             :plugin => plugin.inspect, :error => e.message,
-                             :exception => e.class,
-                             :stacktrace => e.backtrace.join("\n")))
+                             default_logging_keys(
+                               :plugin => plugin.inspect,
+                               :error => e.message,
+                               :exception => e.class,
+                               :stacktrace => e.backtrace.join("\n"))))
       else
         @logger.error(I18n.t("logstash.pipeline.worker-error",
-                             :plugin => plugin.inspect, :error => e.message))
+                             default_logging_keys(:plugin => plugin.inspect, :error => e.message)))
       end
 
       # Assuming the failure that caused this exception is transient,
@@ -486,23 +549,42 @@ def shutdown(&before_stop)
 
     before_stop.call if block_given?
 
-    @logger.debug "Closing inputs"
-    @inputs.each(&:do_stop)
-    @logger.debug "Closed inputs"
+    stop_inputs
+
+    # We make this call blocking, so we know for sure when the method return the shtudown is
+    # stopped
+    wait_for_workers
+    clear_pipeline_metrics
   end # def shutdown
 
+  def force_shutdown!
+    @force_shutdown.make_true
+  end
+
+  def wait_for_workers
+    @logger.debug("Closing inputs", default_logging_keys)
+    @worker_threads.map(&:join)
+    @logger.debug("Worker closed", default_logging_keys)
+  end
+
+  def stop_inputs
+    @logger.debug("Closing inputs", default_logging_keys)
+    @inputs.each(&:do_stop)
+    @logger.debug("Closed inputs", default_logging_keys)
+  end
+
   # After `shutdown` is called from an external thread this is called from the main thread to
   # tell the worker threads to stop and then block until they've fully stopped
   # This also stops all filter and output plugins
   def shutdown_workers
     # Each worker thread will receive this exactly once!
     @worker_threads.each do |t|
-      @logger.debug("Pushing shutdown", :thread => t.inspect)
+      @logger.debug("Pushing shutdown", default_logging_keys(:thread => t.inspect))
       @signal_queue.push(SHUTDOWN)
     end
 
     @worker_threads.each do |t|
-      @logger.debug("Shutdown waiting for worker thread #{t}")
+      @logger.debug("Shutdown waiting for worker thread" , default_logging_keys(:thread => t.inspect))
       t.join
     end
 
@@ -525,6 +607,7 @@ def flush_filters(options = {}, &block)
     flushers = options[:final] ? @shutdown_flushers : @periodic_flushers
 
     flushers.each do |flusher|
+      return if @force_shutdown.true?
       flusher.call(options, &block)
     end
   end
@@ -547,7 +630,7 @@ def shutdown_flusher
 
   def flush
     if @flushing.compare_and_set(false, true)
-      @logger.debug? && @logger.debug("Pushing flush onto pipeline")
+      @logger.debug? && @logger.debug("Pushing flush onto pipeline", default_logging_keys)
       @signal_queue.push(FLUSH)
     end
   end
@@ -566,8 +649,10 @@ def uptime
   # @param options [Hash]
   def flush_filters_to_batch(batch, options = {})
     flush_filters(options) do |event|
+      return if @force_shutdown.true?
+
       unless event.cancelled?
-        @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
+        @logger.debug? and @logger.debug("Pushing flushed events", default_logging_keys(:event => event))
         batch.merge(event)
       end
     end
@@ -613,6 +698,20 @@ def collect_stats
     end
   end
 
+  def clear_pipeline_metrics
+    # TODO(ph): I think the metric should also proxy that call correctly to the collector
+    # this will simplify everything since the null metric would simply just do a noop
+    collector = @metric.collector
+
+    unless collector.nil?
+      # selectively reset metrics we don't wish to keep after reloading
+      # these include metrics about the plugins and number of processed events
+      # we want to keep other metrics like reload counts and error messages
+      collector.clear("stats/pipelines/#{pipeline_id}/plugins")
+      collector.clear("stats/pipelines/#{pipeline_id}/events")
+    end
+  end
+
   # Sometimes we log stuff that will dump the pipeline which may contain
   # sensitive information (like the raw syntax tree which can contain passwords)
   # We want to hide most of what's in here
@@ -628,6 +727,15 @@ def inspect
 
   private
 
+  def default_logging_keys(other_keys = {})
+    default_options = if thread
+                        { :pipeline_id => pipeline_id, :thread => thread.inspect }
+                      else
+                        { :pipeline_id => pipeline_id }
+                      end
+    default_options.merge(other_keys)
+  end
+
   def draining_queue?
     @drain_queue ? !@filter_queue_client.empty? : false
   end
diff --git a/logstash-core/lib/logstash/pipeline_action.rb b/logstash-core/lib/logstash/pipeline_action.rb
new file mode 100644
index 00000000000..2cc6f2dc4ee
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline_action.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "logstash/pipeline_action/base"
+require "logstash/pipeline_action/create"
+require "logstash/pipeline_action/stop"
+require "logstash/pipeline_action/reload"
+
+module LogStash module PipelineAction
+  ORDERING = [
+    LogStash::PipelineAction::Create,
+    LogStash::PipelineAction::Reload,
+    LogStash::PipelineAction::Stop
+  ]
+end end
diff --git a/logstash-core/lib/logstash/pipeline_action/base.rb b/logstash-core/lib/logstash/pipeline_action/base.rb
new file mode 100644
index 00000000000..bc91b19ee98
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline_action/base.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+# I've decided to take the action strategy, I think this make the code a bit easier to understand.
+# maybe in the context of config management we will want to have force kill on the
+# threads instead of waiting forever or sending feedback to the host
+#
+# Some actions could be retryable, or have a delay or timeout.
+module LogStash module PipelineAction
+  class Base
+
+    # Only used for debugging purpose and in the logger statement.
+    def inspect
+      "#{self.class.name}/pipeline_id:#{pipeline_id}"
+    end
+    alias_method :to_s, :inspect
+
+    def execute(pipelines)
+      raise "`#execute` Not implemented!"
+    end
+
+    def <=>(other)
+      order = ORDERING.index(self.class) <=> ORDERING.index(other.class)
+      order.nonzero? ? order : self.pipeline_id <=> other.pipeline_id
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/pipeline_action/create.rb b/logstash-core/lib/logstash/pipeline_action/create.rb
new file mode 100644
index 00000000000..c0beca886a2
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline_action/create.rb
@@ -0,0 +1,42 @@
+# encoding: utf-8
+require "logstash/pipeline_action/base"
+require "logstash/pipeline"
+require "logstash/converge_result"
+require "logstash/util/loggable"
+
+module LogStash module PipelineAction
+  class Create < Base
+    include LogStash::Util::Loggable
+
+    # We currently pass around the metric object again this
+    # is needed to correctly create a pipeline, in a future
+    # PR we could pass a factory to create the pipeline so we pass the logic
+    # to create the pipeline instead.
+    def initialize(pipeline_config, metric)
+      @pipeline_config = pipeline_config
+      @metric = metric
+    end
+
+    def pipeline_id
+      @pipeline_config.pipeline_id
+    end
+
+    # The execute assume that the thread safety access of the pipeline
+    # is managed by the caller.
+    def execute(pipelines)
+      pipeline = create_pipeline
+
+      status = pipeline.start # block until the pipeline is correctly started or crashed
+
+      if status
+        pipelines[pipeline_id] = pipeline # The pipeline is successfully started we can add it to the hash
+      end
+
+      LogStash::ConvergeResult::ActionResult.create(self, status)
+    end
+
+    def create_pipeline
+      LogStash::Pipeline.new(@pipeline_config.config_string, @pipeline_config.settings, @metric)
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/pipeline_action/reload.rb b/logstash-core/lib/logstash/pipeline_action/reload.rb
new file mode 100644
index 00000000000..c66f40d979c
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline_action/reload.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require "logstash/pipeline_action/base"
+require "logstash/pipeline_action/create"
+require "logstash/pipeline_action/stop"
+require "logstash/errors"
+require "logstash/util/loggable"
+require "logstash/converge_result"
+
+module LogStash module PipelineAction
+  class Reload < Base
+    include LogStash::Util::Loggable
+
+    def initialize(pipeline_config, metric)
+      @pipeline_config = pipeline_config
+      @metric = metric
+    end
+
+    def pipeline_id
+      @pipeline_config.pipeline_id
+    end
+
+    def execute(pipelines)
+      old_pipeline = pipelines[pipeline_id]
+
+      if !old_pipeline.reloadable?
+        return LogStash::ConvergeResult::FailedAction.new("Cannot reload pipeline, because the existing pipeline is not reloadable")
+      end
+
+      begin
+        pipeline_validator = LogStash::BasePipeline.new(@pipeline_config.config_string, @pipeline_config.settings)
+      rescue => e
+        return LogStash::ConvergeResult::FailedAction.from_exception(e)
+      end
+
+      if !pipeline_validator.reloadable?
+        return LogStash::ConvergeResult::FailedAction.new("Cannot reload pipeline, because the new pipeline is not reloadable")
+      end
+
+      status = Stop.new(pipeline_id).execute(pipelines)
+
+      if status
+        return Create.new(@pipeline_config, @metric).execute(pipelines)
+      else
+        return status
+      end
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/pipeline_action/stop.rb b/logstash-core/lib/logstash/pipeline_action/stop.rb
new file mode 100644
index 00000000000..3fc389b3532
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline_action/stop.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "logstash/pipeline_action/base"
+require "logstash/shutdown_watcher"
+require "logstash/converge_result"
+
+module LogStash module PipelineAction
+  class Stop < Base
+    attr_reader :pipeline_id
+
+    def initialize(pipeline_id)
+      @pipeline_id = pipeline_id
+    end
+
+    def execute(pipelines)
+      pipeline = pipelines[pipeline_id]
+      pipeline.shutdown { LogStash::ShutdownWatcher.start(pipeline) }
+      pipelines.delete(pipeline_id)
+      # If we reach this part of the code we have succeeded because
+      # the shutdown call will block.
+      return LogStash::ConvergeResult::SuccessfulAction.new
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/plugins/hooks_registry.rb b/logstash-core/lib/logstash/plugins/hooks_registry.rb
index 93ba026a263..ed3e1d3f86f 100644
--- a/logstash-core/lib/logstash/plugins/hooks_registry.rb
+++ b/logstash-core/lib/logstash/plugins/hooks_registry.rb
@@ -41,6 +41,12 @@ def hooks_count(emitter_scope = nil)
       end
     end
 
+    def registered_hook?(emitter_scope, klass)
+      callbacks = @registered_hooks[emitter_scope]
+      return false if callbacks.nil?
+      callbacks.collect(&:class).include?(klass)
+    end
+
     private
     def sync_hooks
       @registered_emitters.each do |emitter, dispatcher|
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index a3b052eaef0..d44c55f0d23 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -19,6 +19,10 @@
 require "logstash/settings"
 require "logstash/version"
 require "logstash/plugins/registry"
+require "logstash/bootstrap_check/default_config"
+require "logstash/bootstrap_check/bad_java"
+require "logstash/bootstrap_check/bad_ruby"
+require "set"
 
 java_import 'org.logstash.FileLockFactory'
 
@@ -31,6 +35,14 @@ class LogStash::Runner < Clamp::StrictCommand
   LogStash::SETTINGS.register(LogStash::Setting::String.new("path.settings", ::File.join(LogStash::Environment::LOGSTASH_HOME, "config")))
   LogStash::SETTINGS.register(LogStash::Setting::String.new("path.logs", ::File.join(LogStash::Environment::LOGSTASH_HOME, "logs")))
 
+  # Ordered list of check to run before starting logstash
+  # theses checks can be changed by a plugin loaded into memory.
+  DEFAULT_BOOTSTRAP_CHECKS = [
+      LogStash::BootstrapCheck::BadRuby,
+      LogStash::BootstrapCheck::BadJava,
+      LogStash::BootstrapCheck::DefaultConfig
+  ]
+
   # Node Settings
   option ["-n", "--node.name"], "NAME",
     I18n.t("logstash.runner.flag.name"),
@@ -153,10 +165,16 @@ class LogStash::Runner < Clamp::StrictCommand
     I18n.t("logstash.runner.flag.quiet"),
     :new_flag => "log.level", :new_value => "error"
 
-  attr_reader :agent
+  attr_reader :agent, :settings
+  attr_accessor :bootstrap_checks
 
   def initialize(*args)
     @settings = LogStash::SETTINGS
+    @bootstrap_checks = DEFAULT_BOOTSTRAP_CHECKS.dup
+
+    # Default we check local sources: `-e`, `-f` and the logstash.yml options.
+    LogStash::Config::SOURCE_LOADER.add_source(LogStash::Config::Source::Local.new(@settings))
+
     super(*args)
   end
 
@@ -190,7 +208,7 @@ def execute
     # We invoke post_process to apply extra logic to them.
     # The post_process callbacks have been added in environment.rb
     @settings.post_process
-    
+
     require "logstash/util"
     require "logstash/util/java_version"
     require "stud/task"
@@ -211,54 +229,55 @@ def execute
       logger.warn("--config.debug was specified, but log.level was not set to \'debug\'! No config info will be logged.")
     end
 
+    # Skip any validation and just return the version
+    if version?
+      show_version
+      return 0
+    end
+
     # We configure the registry and load any plugin that can register hooks
     # with logstash, this need to be done before any operation.
     LogStash::PLUGIN_REGISTRY.setup!
+
+    @dispatcher = LogStash::EventDispatcher.new(self)
+    LogStash::PLUGIN_REGISTRY.hooks.register_emitter(self.class, @dispatcher)
+
     @settings.validate_all
+    @dispatcher.fire(:before_bootstrap_checks)
 
-    LogStash::Util::set_thread_name(self.class.name)
+    return start_shell(setting("interactive"), binding) if setting("interactive")
 
-    if RUBY_VERSION < "1.9.2"
-      logger.fatal "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
+    begin
+      @bootstrap_checks.each { |bootstrap| bootstrap.check(@settings) }
+    rescue LogStash::BootstrapCheckError => e
+      signal_usage_error(e.message)
       return 1
     end
+    @dispatcher.fire(:after_bootstrap_checks)
 
-    # Exit on bad java versions
-    java_version = LogStash::Util::JavaVersion.version
-    if LogStash::Util::JavaVersion.bad_java_version?(java_version)
-      logger.fatal "Java version 1.8.0 or later is required. (You are running: #{java_version})"
-      return 1
-    end
+    LogStash::Util::set_thread_name(self.class.name)
 
     LogStash::ShutdownWatcher.unsafe_shutdown = setting("pipeline.unsafe_shutdown")
 
     configure_plugin_paths(setting("path.plugins"))
 
-    if version?
-      show_version
-      return 0
-    end
-
-    return start_shell(setting("interactive"), binding) if setting("interactive")
 
     @settings.format_settings.each {|line| logger.debug(line) }
 
-    if setting("config.string").nil? && setting("path.config").nil?
-      fail(I18n.t("logstash.runner.missing-configuration"))
-    end
-
-    if setting("config.reload.automatic") && setting("path.config").nil?
-      # there's nothing to reload
-      signal_usage_error(I18n.t("logstash.runner.reload-without-config-path"))
-    end
-
     if setting("config.test_and_exit")
-      config_loader = LogStash::Config::Loader.new(logger)
-      config_str = config_loader.format_config(setting("path.config"), setting("config.string"))
       begin
-        LogStash::BasePipeline.new(config_str)
-        puts "Configuration OK"
-        logger.info "Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash"
+        results = LogStash::Config::SOURCE_LOADER.fetch
+
+        # TODO(ph): make it better for multiple pipeline
+        if results.success?
+          results.response.each do |pipeline_config|
+            LogStash::BasePipeline.new(pipeline_config.config_string)
+          end
+          puts "Configuration OK"
+          logger.info "Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash"
+        else
+          raise "Could not load the configuration file"
+        end
         return 0
       rescue => e
         logger.fatal I18n.t("logstash.runner.invalid-configuration", :error => e.message)
@@ -269,9 +288,9 @@ def execute
     # lock path.data before starting the agent
     @data_path_lock = FileLockFactory.getDefault().obtainLock(setting("path.data"), ".lock");
 
-    @agent = create_agent(@settings)
-
-    @agent.register_pipeline(@settings)
+    @dispatcher.fire(:before_agent)
+    @agent = create_agent(@settings, LogStash::Config::SOURCE_LOADER)
+    @dispatcher.fire(:after_agent)
 
     # enable sigint/sigterm before starting the agent
     # to properly handle a stalled agent
@@ -388,7 +407,7 @@ def start_shell(shell, start_binding)
   def trap_sighup
     Stud::trap("HUP") do
       logger.warn(I18n.t("logstash.agent.sighup"))
-      @agent.reload_state!
+      @agent.converge_state_and_update
     end
   end
 
@@ -403,6 +422,7 @@ def trap_sigint
     Stud::trap("INT") do
       if @interrupted_once
         logger.fatal(I18n.t("logstash.agent.forced_sigint"))
+        @agent.force_shutdown!
         exit
       else
         logger.warn(I18n.t("logstash.agent.sigint"))
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
index 3d9ddcef3de..1d7109adff9 100644
--- a/logstash-core/lib/logstash/settings.rb
+++ b/logstash-core/lib/logstash/settings.rb
@@ -47,6 +47,7 @@ def set?(setting_name)
     def clone
       get_subset(".*")
     end
+    alias_method :dup, :clone
 
     def get_default(setting_name)
       get_setting(setting_name).default
diff --git a/logstash-core/lib/logstash/state_resolver.rb b/logstash-core/lib/logstash/state_resolver.rb
new file mode 100644
index 00000000000..541f95c28cf
--- /dev/null
+++ b/logstash-core/lib/logstash/state_resolver.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+module LogStash
+  # In the beginning I was using this code as a method in the Agent class directly
+  # But with the plugins system I think we should be able to swap what kind of action would be run.
+  #
+  # Lets take the example of dynamic source, where the pipeline config and settings are located and
+  # managed outside of the machine.
+  class StateResolver
+    def initialize(metric)
+      @metric = metric
+    end
+
+    def resolve(pipelines, pipeline_configs)
+      actions = []
+
+      pipeline_configs.each do |pipeline_config|
+        pipeline = pipelines[pipeline_config.pipeline_id]
+
+        if pipeline.nil?
+          actions << LogStash::PipelineAction::Create.new(pipeline_config, @metric)
+        else
+          # TODO(ph): The pipeline should keep a reference to the original PipelineConfig
+          # and we could use straight comparison.
+          if pipeline_config.config_hash != pipeline.config_hash
+            actions << LogStash::PipelineAction::Reload.new(pipeline_config, @metric)
+          end
+        end
+      end
+
+      running_pipelines = pipeline_configs.collect(&:pipeline_id)
+
+      # If one of the running pipeline is not in the pipeline_configs, we assume that we need to
+      # stop it.
+      pipelines.keys
+        .select { |pipeline_id| !running_pipelines.include?(pipeline_id) }
+        .each { |pipeline_id| actions << LogStash::PipelineAction::Stop.new(pipeline_id) }
+
+      actions.sort # See logstash/pipeline_action.rb
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/util/java_version.rb b/logstash-core/lib/logstash/util/java_version.rb
index f73d09de10d..33efeb50489 100644
--- a/logstash-core/lib/logstash/util/java_version.rb
+++ b/logstash-core/lib/logstash/util/java_version.rb
@@ -32,6 +32,12 @@ def self.parse_java_version(version_string)
     }
   end
 
+  def self.validate_java_version!
+    if bad_java_version?(version)
+      raise "Java version 1.8.0 or later is required. (You are running: #{version})"
+    end
+  end
+
   # Determine if the given java version string is a bad version of java
   # If it is, return true, if it isn't return false.
   # Accepts nil, returning nil.
diff --git a/logstash-core/locales/en.yml b/logstash-core/locales/en.yml
index a00ce406a99..a5550b6ccb2 100644
--- a/logstash-core/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -16,10 +16,12 @@ en:
     pipeline:
       worker-error: |-
         A plugin had an unrecoverable error. Will restart this plugin.
+          Pipeline_id:%{pipeline_id}
           Plugin: %{plugin}
           Error: %{error}
       worker-error-debug: |-
         A plugin had an unrecoverable error. Will restart this plugin.
+          Pipeline_id:%{pipeline_id}
           Plugin: %{plugin}
           Error: %{error}
           Exception: %{exception}
diff --git a/logstash-core/spec/api/lib/api/node_plugins_spec.rb b/logstash-core/spec/api/lib/api/node_plugins_spec.rb
index 5389e10c418..79094ed4707 100644
--- a/logstash-core/spec/api/lib/api/node_plugins_spec.rb
+++ b/logstash-core/spec/api/lib/api/node_plugins_spec.rb
@@ -12,7 +12,7 @@
   extend ResourceDSLMethods
 
   before(:each) do
-    do_request { get "/" }
+    get "/"
   end
 
   let(:payload) { LogStash::Json.load(last_response.body) }
diff --git a/logstash-core/spec/api/lib/api/node_spec.rb b/logstash-core/spec/api/lib/api/node_spec.rb
index a8f8b009f5b..e818e0f42f8 100644
--- a/logstash-core/spec/api/lib/api/node_spec.rb
+++ b/logstash-core/spec/api/lib/api/node_spec.rb
@@ -12,7 +12,7 @@
   describe "#hot threads" do
 
     before(:all) do
-      do_request { get "/hot_threads" }
+      get "/hot_threads"
     end
 
     it "respond OK" do
@@ -26,7 +26,7 @@
     context "#threads count" do
 
       before(:all) do
-        do_request { get "/hot_threads?threads=5" }
+        get "/hot_threads?threads=5"
       end
 
       let(:payload) { LogStash::Json.load(last_response.body) }
@@ -49,7 +49,7 @@
       ].each do |path|
 
         before(:all) do
-          do_request { get path }
+          get path
         end
 
         let(:payload) { last_response.body }
@@ -70,7 +70,7 @@
         @threads = []
         5.times { @threads << Thread.new { loop {} } }
 
-        do_request { get "/hot_threads?human=t&threads=2"}
+        get "/hot_threads?human=t&threads=2"
       end
 
       after(:all) do
@@ -91,7 +91,7 @@
         "/hot_threads?human=f",
       ].each do |path|
         before(:all) do
-          do_request { get path }
+          get path
         end
 
         it "should return a json payload content type" do
diff --git a/logstash-core/spec/api/lib/api/root_spec.rb b/logstash-core/spec/api/lib/api/root_spec.rb
index ad9dc08381a..696ce6dc693 100644
--- a/logstash-core/spec/api/lib/api/root_spec.rb
+++ b/logstash-core/spec/api/lib/api/root_spec.rb
@@ -9,7 +9,7 @@
   include_context "api setup"
 
   it "should respond to root resource" do
-    do_request { get "/" }
+    get "/"
     expect(last_response).to be_ok
   end
 
diff --git a/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
index 0800e731e97..c5ec25d5aa0 100644
--- a/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
+++ b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
@@ -27,7 +27,7 @@ def test_api(expected, path)
       let(:payload) { LogStash::Json.load(last_response.body) }
 
       before(:all) do
-        do_request { get path }
+        get path
       end
 
       it "should respond OK" do
diff --git a/logstash-core/spec/api/lib/commands/stats.rb b/logstash-core/spec/api/lib/commands/stats.rb
index 3059e1460f3..2efdb6c4241 100644
--- a/logstash-core/spec/api/lib/commands/stats.rb
+++ b/logstash-core/spec/api/lib/commands/stats.rb
@@ -4,7 +4,7 @@
 describe LogStash::Api::Commands::Stats do
 
   let(:report_method) { :run }
-  subject(:report) { do_request { report_class.new.send(report_method) } }
+  subject(:report) { report_class.new.send(report_method) }
 
   let(:report_class) { described_class }
 
diff --git a/logstash-core/spec/api/spec_helper.rb b/logstash-core/spec/api/spec_helper.rb
index 193eff2d916..6c5d4f99425 100644
--- a/logstash-core/spec/api/spec_helper.rb
+++ b/logstash-core/spec/api/spec_helper.rb
@@ -6,6 +6,7 @@
 $LOAD_PATH.unshift(File.expand_path(File.dirname(__FILE__)))
 require "lib/api/support/resource_dsl_methods"
 require_relative "../support/mocks_classes"
+require_relative "../support/helpers"
 require 'rspec/expectations'
 require "logstash/settings"
 require 'rack/test'
@@ -24,6 +25,7 @@ def start_webserver
       @webserver = Struct.new(:address).new(http_address)
       self.metric.gauge([], :http_address, http_address)
     end
+
     def stop_webserver; end
   end
 end
@@ -38,7 +40,7 @@ class LogStashRunner
   attr_reader :config_str, :agent, :pipeline_settings
 
   def initialize
-    @config_str   = "input { generator {count => 100 } } output { dummyoutput {} }"
+    @config_str   = "input { generator {id => 'api-generator-pipeline' count => 100 } } output { dummyoutput {} }"
 
     args = {
       "config.reload.automatic" => false,
@@ -51,15 +53,14 @@ def initialize
       "pipeline.batch.size" => 1,
       "pipeline.workers" => 1
     }
-    @settings = ::LogStash::SETTINGS.clone.merge(args)
 
+    @settings = ::LogStash::SETTINGS.clone.merge(args)
     @agent = LogStash::DummyAgent.new(@settings)
   end
 
   def start
     # We start a pipeline that will generate a finite number of events
     # before starting the expectations
-    agent.register_pipeline(@settings)
     @agent_task = Stud::Task.new { agent.execute }
     @agent_task.wait
   end
@@ -69,19 +70,6 @@ def stop
   end
 end
 
-##
-# Method used to wrap up a request in between of a running
-# pipeline, this makes the whole execution model easier and
-# more contained as some threads might go wild.
-##
-def do_request(&block)
-  runner = LogStashRunner.new
-  runner.start
-  ret_val = block.call
-  runner.stop
-  ret_val
-end
-
 RSpec::Matchers.define :be_available? do
   match do |plugin|
     begin
@@ -95,10 +83,11 @@ def do_request(&block)
 
 shared_context "api setup" do
   before :all do
+    clear_data_dir
     @runner = LogStashRunner.new
     @runner.start
   end
-  
+
   after :all do
     @runner.stop
   end
diff --git a/logstash-core/spec/logstash/agent/converge_spec.rb b/logstash-core/spec/logstash/agent/converge_spec.rb
new file mode 100644
index 00000000000..f45fcb90b35
--- /dev/null
+++ b/logstash-core/spec/logstash/agent/converge_spec.rb
@@ -0,0 +1,237 @@
+# encoding: utf-8
+require "logstash/agent"
+require_relative "../../support/helpers"
+require_relative "../../support/matchers"
+require_relative "../../support/mocks_classes"
+require "spec_helper"
+
+describe LogStash::Agent do
+  # by default no tests uses the auto reload logic
+  let(:agent_settings) { mock_settings("config.reload.automatic" => false, "queue.type" => "persisted") }
+
+  subject { described_class.new(agent_settings, source_loader) }
+
+  before do
+    clear_data_dir
+
+    # until we decouple the webserver from the agent
+    allow(subject).to receive(:start_webserver).and_return(false)
+    allow(subject).to receive(:stop_webserver).and_return(false)
+  end
+
+  # Make sure that we close any running pipeline to release any pending locks
+  # on the queues
+  after do
+    converge_result = subject.shutdown
+    expect(converge_result).to be_a_successful_converge
+  end
+
+  context "Agent execute options" do
+    let(:source_loader) do
+      TestSourceLoader.new(finite_pipeline_config)
+    end
+
+    context "when the pipeline is execution limite (finite)" do
+      let(:finite_pipeline_config) { mock_pipeline_config(:main, "input { generator { count => 1000 } } output { null {} }") }
+
+      it "execute the pipeline and stop execution" do
+        expect(subject.execute).to eq(0)
+      end
+    end
+
+    context "when the config is short lived (generator { count => 1 })" do
+      let(:finite_pipeline_config) { mock_pipeline_config(:main, "input { generator { count => 1 } } output { null {} }") }
+
+      it "execute the pipeline and stop execution" do
+        expect(subject.execute).to eq(0)
+      end
+    end
+
+    context "when `config.reload.automatic`" do
+      let(:pipeline_config) { mock_pipeline_config(:main, "input { generator {} } output { null {} }") }
+
+      let(:source_loader) do
+        TestSourceLoader.new(pipeline_config)
+      end
+
+      context "is set to`FALSE`" do
+        context "and succesfully load the config" do
+          let(:agent_settings) { mock_settings("config.reload.automatic" => false) }
+
+          it "converge only once" do
+            agent_task = start_agent(subject)
+
+            expect(source_loader.fetch_count).to eq(1)
+            expect(subject).to have_running_pipeline?(pipeline_config)
+
+            subject.shutdown
+            agent_task.stop!
+          end
+        end
+
+        context "and it fails to load the config" do
+          let(:source_loader) do
+            TestSourceLoader.new(TestSourceLoader::FailedFetch.new("can't load the file"))
+          end
+
+          it "doesn't execute any pipeline" do
+            expect { subject.execute }.not_to raise_error # errors is logged
+
+            expect(source_loader.fetch_count).to eq(1)
+            expect(subject.pipelines_count).to eq(0)
+
+            subject.shutdown
+          end
+        end
+      end
+
+      context "is set to `TRUE`" do
+        let(:interval) { 0.01 }
+        let(:agent_settings) do
+          mock_settings(
+            "config.reload.automatic" => true,
+            "config.reload.interval" =>  interval
+          )
+        end
+
+        context "and succesfully load the config" do
+          it "converges periodically the pipelines from the configs source" do
+            agent_task = start_agent(subject)
+
+            sleep(interval * 10) # let the interval reload a few times
+            expect(subject).to have_running_pipeline?(pipeline_config)
+            expect(source_loader.fetch_count).to be > 1
+
+            subject.shutdown
+            agent_task.stop!
+          end
+        end
+
+        context "and it fails to load the config" do
+          let(:source_loader) do
+            TestSourceLoader.new(TestSourceLoader::FailedFetch.new("can't load the file"))
+          end
+
+          it "it will keep trying to converge" do
+            agent_task = start_agent(subject)
+
+            sleep(interval * 20) # let the interval reload a few times
+            expect(subject.pipelines_count).to eq(0)
+            expect(source_loader.fetch_count).to be > 1
+
+            subject.shutdown
+            agent_task.stop!
+          end
+        end
+      end
+    end
+  end
+
+  context "when shutting down the agent" do
+    let(:pipeline_config) { mock_pipeline_config(:main, "input { generator {} } output { null {} }") }
+    let(:new_pipeline_config) { mock_pipeline_config(:new, "input { generator { id => 'new' } } output { null {} }") }
+
+    let(:source_loader) do
+      TestSourceLoader.new([pipeline_config, new_pipeline_config])
+    end
+
+    it "stops the running pipelines" do
+      expect(subject.converge_state_and_update).to be_a_successful_converge
+      expect { subject.shutdown }.to change { subject.running_pipelines.size }.from(2).to(0)
+    end
+  end
+
+  context "Configuration converge scenario" do
+    let(:pipeline_config) { mock_pipeline_config(:main, "input { generator {} } output { null {} }", { "pipeline.reloadable" => true }) }
+    let(:new_pipeline_config) { mock_pipeline_config(:new, "input { generator {} } output { null {} }", { "pipeline.reloadable" => true }) }
+
+    before do
+      # Set the Agent to an initial state of pipelines
+      expect(subject.converge_state_and_update).to be_a_successful_converge
+    end
+
+    context "no pipelines is running" do
+      let(:source_loader) do
+        TestSequenceSourceLoader.new([], pipeline_config)
+      end
+
+      it "creates and starts the new pipeline" do
+        expect {
+          expect(subject.converge_state_and_update).to be_a_successful_converge
+        }.to change { subject.running_pipelines.count }.from(0).to(1)
+        expect(subject).to have_running_pipeline?(pipeline_config)
+      end
+    end
+
+    context "when a pipeline is running" do
+      context "when the source returns the current pipeline and a new one" do
+        let(:source_loader) do
+          TestSequenceSourceLoader.new(
+            pipeline_config,
+            [pipeline_config, new_pipeline_config]
+          )
+        end
+
+        it "start a new pipeline and keep the original" do
+          expect {
+            expect(subject.converge_state_and_update).to be_a_successful_converge
+          }.to change { subject.running_pipelines.count }.from(1).to(2)
+          expect(subject).to have_running_pipeline?(pipeline_config)
+          expect(subject).to have_running_pipeline?(new_pipeline_config)
+        end
+      end
+
+      context "when the source returns a new pipeline but not the old one" do
+        let(:source_loader) do
+          TestSequenceSourceLoader.new(
+            pipeline_config,
+            new_pipeline_config
+          )
+        end
+
+        it "stops the missing pipeline and start the new one" do
+          expect {
+            expect(subject.converge_state_and_update).to be_a_successful_converge
+          }.not_to change { subject.running_pipelines.count }
+          expect(subject).not_to have_pipeline?(pipeline_config)
+          expect(subject).to have_running_pipeline?(new_pipeline_config)
+        end
+      end
+    end
+
+    context "when the source return a modified pipeline" do
+      let(:modified_pipeline_config) { mock_pipeline_config(:main, "input { generator { id => 'new-and-modified' } } output { null {} }", { "pipeline.reloadable" => true }) }
+
+      let(:source_loader) do
+        TestSequenceSourceLoader.new(
+          [pipeline_config],
+          [modified_pipeline_config]
+        )
+      end
+
+      it "reloads the modified pipeline" do
+        expect {
+          expect(subject.converge_state_and_update).to be_a_successful_converge
+        }.not_to change { subject.running_pipelines.count }
+        expect(subject).to have_running_pipeline?(modified_pipeline_config)
+        expect(subject).not_to have_pipeline?(pipeline_config)
+      end
+    end
+
+    context "when the source return no pipelines" do
+      let(:source_loader) do
+        TestSequenceSourceLoader.new(
+          [pipeline_config, new_pipeline_config],
+          []
+        )
+      end
+
+      it "stops all the pipelines" do
+        expect {
+          expect(subject.converge_state_and_update).to be_a_successful_converge
+        }.to change { subject.running_pipelines.count }.from(2).to(0)
+        expect(subject).not_to have_pipeline?(pipeline_config)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/agent/metrics_spec.rb b/logstash-core/spec/logstash/agent/metrics_spec.rb
new file mode 100644
index 00000000000..a66aac1067c
--- /dev/null
+++ b/logstash-core/spec/logstash/agent/metrics_spec.rb
@@ -0,0 +1,244 @@
+# encoding: utf-8
+#
+require "logstash/agent"
+require_relative "../../support/helpers"
+require_relative "../../support/matchers"
+require_relative "../../support/mocks_classes"
+require "spec_helper"
+
+# Just make the tests a bit shorter to write and
+# assert, I will keep theses methods here for easier understanding.
+def mval(*path_elements)
+  mhash(*path_elements).value
+end
+
+def mhash(*path_elements)
+  metric.get_shallow(*path_elements)
+end
+
+describe LogStash::Agent do
+  # by default no tests uses the auto reload logic
+  let(:agent_settings) { mock_settings("config.reload.automatic" => false) }
+  let(:pipeline_settings) { { "pipeline.reloadable" => true } }
+
+  let(:pipeline_config) { mock_pipeline_config(:main, "input { generator {} } filter { mutate { add_tag => 'hello world' }} output { null {} }", pipeline_settings) }
+  let(:update_pipeline_config) { mock_pipeline_config(:main, "input { generator { id => 'new' } } output { null {} }", pipeline_settings) }
+  let(:bad_update_pipeline_config) { mock_pipeline_config(:main, "hooo }", pipeline_settings) }
+
+  let(:new_pipeline_config) { mock_pipeline_config(:new, "input { generator {} } output { null {} }", pipeline_settings) }
+  let(:bad_pipeline_config) { mock_pipeline_config(:bad, "hooo }", pipeline_settings) }
+  let(:second_bad_pipeline_config) { mock_pipeline_config(:second_bad, "hooo }", pipeline_settings) }
+
+  let(:source_loader) do
+    TestSourceLoader.new([])
+  end
+
+  subject { described_class.new(agent_settings, source_loader) }
+
+  before :each do
+    # This MUST run first, before `subject` is invoked to ensure clean state
+    clear_data_dir
+
+    # TODO(ph) until we decouple the webserver from the agent
+    # we just disable these calls
+    allow(subject).to receive(:start_webserver).and_return(false)
+    allow(subject).to receive(:stop_webserver).and_return(false)
+  end
+
+  # Lets make sure we stop all the running pipeline for every example
+  # so we don't have any rogue pipeline in the background
+  after :each do
+    subject.shutdown
+  end
+
+  let(:metric) { subject.metric.collector.snapshot_metric.metric_store }
+
+  context "when starting the agent" do
+    it "initialize the instance reload metrics" do
+      expect(mval(:stats, :reloads, :successes)).to eq(0)
+      expect(mval(:stats, :reloads, :failures)).to eq(0)
+    end
+  end
+
+  context "when we try to start one pipeline" do
+    context "and it succeed" do
+      let(:source_loader) do
+        TestSourceLoader.new(pipeline_config)
+      end
+
+      let(:pipeline_name) { :main }
+
+      it "doesnt changes the global successes" do
+        expect { subject.converge_state_and_update }.not_to change { mval(:stats, :reloads, :successes) }
+      end
+
+      it "doesn't change the failures" do
+        expect { subject.converge_state_and_update }.not_to change { mval(:stats, :reloads, :failures) }
+      end
+
+      it "sets the failures to 0" do
+        subject.converge_state_and_update
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :failures)).to eq(0)
+      end
+
+      it "sets the successes to 0" do
+        subject.converge_state_and_update
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :successes)).to eq(0)
+      end
+
+      it "sets the `last_error` to nil" do
+        subject.converge_state_and_update
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_error)).to be_nil
+      end
+
+      it "sets the `last_failure_timestamp` to nil" do
+        subject.converge_state_and_update
+        expect(mval(:stats, :pipelines, :main, :reloads, :last_failure_timestamp)).to be_nil
+      end
+
+      it "sets the `last_success_timestamp` to nil" do
+        subject.converge_state_and_update
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_success_timestamp)).to be_nil
+      end
+    end
+
+    context "and it fails" do
+      let(:source_loader) do
+        TestSourceLoader.new(bad_pipeline_config)
+      end
+
+      let(:pipeline_name) { :bad }
+
+      before do
+        subject.converge_state_and_update
+      end
+
+      it "doesnt changes the global successes" do
+        expect { subject.converge_state_and_update }.not_to change { mval(:stats, :reloads, :successes)}
+      end
+
+      it "doesn't change the failures" do
+        expect { subject.converge_state_and_update }.to change { mval(:stats, :reloads, :failures) }.by(1)
+      end
+
+      it "increments the pipeline failures" do
+        expect { subject.converge_state_and_update }.to change { mval(:stats, :pipelines, pipeline_name, :reloads, :failures) }.by(1)
+      end
+
+      it "sets the successes to 0" do
+        subject.converge_state_and_update
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :successes)).to eq(0)
+      end
+
+      it "increase the global failures" do
+        expect { subject.converge_state_and_update }.to change { mval(:stats, :reloads, :failures) }
+      end
+
+      it "records the `last_error`" do
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_error)).to_not be_nil
+      end
+
+      it "records the `message` and the `backtrace`" do
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_error)[:message]).to_not be_nil
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_error)[:backtrace]).to_not be_nil
+      end
+
+      it "records the time of the last failure" do
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_failure_timestamp)).to_not be_nil
+      end
+
+      it "initializes the `last_success_timestamp`" do
+        expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_success_timestamp)).to be_nil
+      end
+    end
+
+    context "when we try to reload a pipeline" do
+      before do
+        # Running Pipeline to -> reload pipeline
+        expect(subject.converge_state_and_update.success?).to be_truthy
+      end
+
+      let(:pipeline_name) { :main }
+
+      context "and it succeed" do
+        let(:source_loader) do
+          TestSequenceSourceLoader.new(pipeline_config, update_pipeline_config)
+        end
+
+        it "increments the global successes" do
+          expect { subject.converge_state_and_update }.to change { mval(:stats, :reloads, :successes) }.by(1)
+        end
+
+        it "increment the pipeline successes" do
+          expect{ subject.converge_state_and_update }.to change { mval(:stats, :pipelines, pipeline_name, :reloads, :successes) }.by(1)
+        end
+
+        it "record the `last_success_timestamp`" do
+          expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_success_timestamp)).to be_nil
+          subject.converge_state_and_update
+          expect(mval(:stats, :pipelines, pipeline_name, :reloads, :last_success_timestamp)).not_to be_nil
+        end
+      end
+
+      context "and it fails" do
+        let(:source_loader) do
+          TestSequenceSourceLoader.new(pipeline_config, bad_update_pipeline_config)
+        end
+
+        it "increments the global failures" do
+          expect { subject.converge_state_and_update }.to change { mval(:stats, :reloads, :failures) }.by(1)
+        end
+
+        it "increment the pipeline failures" do
+          expect{ subject.converge_state_and_update }.to change { mval(:stats, :pipelines, pipeline_name, :reloads, :failures) }.by(1)
+        end
+      end
+    end
+
+    context "when we successfully reload a pipeline" do
+      let(:source_loader) do
+        TestSequenceSourceLoader.new(pipeline_config, update_pipeline_config)
+      end
+
+      before do
+        expect(subject.converge_state_and_update.success?).to be_truthy
+      end
+
+      it "it clear previous metrics" do
+        try(20) do
+          expect { mhash(:stats, :pipelines, :main, :plugins, :filters) }.not_to raise_error, "Filters stats should exist"
+        end
+        expect(subject.converge_state_and_update.success?).to be_truthy
+
+        # We do have to retry here, since stopping a pipeline is a blocking operation
+        expect { mhash(:stats, :pipelines, :main, :plugins, :filters) }.to raise_error
+      end
+    end
+
+    context "when we stop a pipeline" do
+      let(:source_loader) do
+        TestSequenceSourceLoader.new(pipeline_config, [])
+      end
+
+      before do
+        # Running Pipeline to -> reload pipeline
+        expect(subject.converge_state_and_update.success?).to be_truthy
+      end
+
+      it "clear pipeline specific metric" do
+        # since the pipeline is async, it can actually take some time to have metrics recordings
+        # so we try a few times
+        try(20) do
+          expect { mhash(:stats, :pipelines, :main, :events) }.not_to raise_error , "Events pipelien stats should exist"
+          expect { mhash(:stats, :pipelines, :main, :plugins) }.not_to raise_error, "Plugins pipeline stats should exist"
+        end
+
+        expect(subject.converge_state_and_update.success?).to be_truthy
+
+        # We do have to retry here, since stopping a pipeline is a blocking operation
+        expect { mhash(:stats, :pipelines, :main, :plugins) }.to raise_error
+        expect { mhash(:stats, :pipelines, :main, :events) }.to raise_error
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index 7c40baab865..1a2b0cbaa1d 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -2,27 +2,36 @@
 require "spec_helper"
 require "stud/temporary"
 require "logstash/inputs/generator"
+require "logstash/config/pipeline_config"
+require "logstash/config/config_part"
+require "logstash/config/source/local"
 require_relative "../support/mocks_classes"
 require "fileutils"
 require_relative "../support/helpers"
+require_relative "../support/matchers"
 
 describe LogStash::Agent do
-
-  let(:agent_settings) { LogStash::SETTINGS }
+  let(:agent_settings) { mock_settings("config.string" => "input {}") }
   let(:default_pipeline_id) { LogStash::SETTINGS.get("pipeline.id") }
   let(:agent_args) { {} }
   let(:pipeline_settings) { agent_settings.clone }
   let(:pipeline_args) { {} }
   let(:config_file) { Stud::Temporary.pathname }
-  let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
+  let(:config_file_txt) { "input { generator { id => 'initial' } } output { }" }
+  let(:default_source_loader) do
+    sl = LogStash::Config::SourceLoader.new
+    sl.add_source(LogStash::Config::Source::Local.new(agent_settings))
+    sl
+  end
 
-  subject { LogStash::Agent.new(agent_settings) }
+  subject { LogStash::Agent.new(agent_settings, default_source_loader) }
 
   before :each do
     # This MUST run first, before `subject` is invoked to ensure clean state
     clear_data_dir
 
     File.open(config_file, "w") { |f| f.puts config_file_txt }
+
     agent_args.each do |key, value|
       agent_settings.set(key, value)
       pipeline_settings.set(key, value)
@@ -38,10 +47,14 @@
   end
 
   it "fallback to hostname when no name is provided" do
-    expect(LogStash::Agent.new.name).to eq(Socket.gethostname)
+    expect(LogStash::Agent.new(agent_settings, default_source_loader).name).to eq(Socket.gethostname)
   end
 
-  describe "register_pipeline" do
+  after(:each) do
+    subject.shutdown # shutdown/close the pipelines
+  end
+
+  describe "adding a new pipeline" do
     let(:config_string) { "input { } filter { } output { }" }
     let(:agent_args) do
       {
@@ -52,16 +65,12 @@
       }
     end
 
-    after(:each) do
-      subject.close_pipelines
-    end
-
     it "should delegate settings to new pipeline" do
       expect(LogStash::Pipeline).to receive(:new) do |arg1, arg2|
         expect(arg1).to eq(config_string)
         expect(arg2.to_hash).to include(agent_args)
       end
-      subject.register_pipeline(agent_settings)
+      subject.converge_state_and_update
     end
   end
 
@@ -80,7 +89,11 @@
   end
 
   describe "#execute" do
-    let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
+    let(:config_file_txt) { "input { generator { id => 'old'} } output { }" }
+    let(:mock_config_pipeline) { mock_pipeline_config(:main, config_file_txt, pipeline_settings) }
+
+    let(:source_loader) { TestSourceLoader.new(mock_config_pipeline) }
+    subject { described_class.new(agent_settings, source_loader) }
 
     before :each do
       allow(subject).to receive(:start_webserver).and_return(false)
@@ -95,19 +108,15 @@
         }
       end
 
-      before(:each) do
-        subject.register_pipeline(pipeline_settings)
-      end
 
       context "if state is clean" do
         before :each do
           allow(subject).to receive(:running_pipelines?).and_return(true)
-          allow(subject).to receive(:sleep)
           allow(subject).to receive(:clean_state?).and_return(false)
         end
 
-        it "should not reload_state!" do
-          expect(subject).to_not receive(:reload_state!)
+        it "should not converge state more than once" do
+          expect(subject).to receive(:converge_state_and_update).once
           t = Thread.new { subject.execute }
 
           # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
@@ -119,16 +128,19 @@
         end
       end
 
-      context "when calling reload_pipeline!" do
+      context "when calling reloading a pipeline" do
         context "with a config that contains reload incompatible plugins" do
           let(:second_pipeline_config) { "input { stdin {} } filter { } output { }" }
+          let(:mock_second_pipeline_config) { mock_pipeline_config(:main, second_pipeline_config, pipeline_settings) }
+
+          let(:source_loader) { TestSequenceSourceLoader.new(mock_config_pipeline, mock_second_pipeline_config)}
 
           it "does not upgrade the new config" do
             t = Thread.new { subject.execute }
             sleep(0.1) until subject.running_pipelines? && subject.pipelines.values.first.ready?
-            expect(subject).to_not receive(:upgrade_pipeline)
-            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
-            subject.send(:"reload_pipeline!", "main")
+
+            expect(subject.converge_state_and_update).not_to be_a_successful_converge
+            expect(subject).to have_running_pipeline?(mock_config_pipeline)
 
             # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
             # a bad test design or missing class functionality.
@@ -141,13 +153,16 @@
 
         context "with a config that does not contain reload incompatible plugins" do
           let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+          let(:mock_second_pipeline_config) { mock_pipeline_config(:main, second_pipeline_config, pipeline_settings) }
+
+          let(:source_loader) { TestSequenceSourceLoader.new(mock_config_pipeline, mock_second_pipeline_config)}
 
           it "does upgrade the new config" do
             t = Thread.new { subject.execute }
-            sleep(0.1) until subject.running_pipelines? && subject.pipelines.values.first.ready?
-            expect(subject).to receive(:upgrade_pipeline).once.and_call_original
-            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
-            subject.send(:"reload_pipeline!", "main")
+            sleep(0.1) until subject.pipelines_count > 0 && subject.pipelines.values.first.ready?
+
+            expect(subject.converge_state_and_update).to be_a_successful_converge
+            expect(subject).to have_running_pipeline?(mock_second_pipeline_config)
 
             # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
             # a bad test design or missing class functionality.
@@ -162,14 +177,17 @@
       context "when calling reload_state!" do
         context "with a pipeline with auto reloading turned off" do
           let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
-          let(:pipeline_args) { { "config.reload.automatic" => false } }
+          let(:pipeline_args) { { "pipeline.reloadable" => false } }
+          let(:mock_second_pipeline_config) { mock_pipeline_config(:main, second_pipeline_config, mock_settings(pipeline_args)) }
+
+          let(:source_loader) { TestSequenceSourceLoader.new(mock_config_pipeline, mock_second_pipeline_config)}
 
           it "does not try to reload the pipeline" do
             t = Thread.new { subject.execute }
             sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
-            expect(subject).to_not receive(:reload_pipeline!)
-            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
-            subject.reload_state!
+
+            expect(subject.converge_state_and_update).not_to be_a_successful_converge
+            expect(subject).to have_running_pipeline?(mock_config_pipeline)
 
             # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
             # a bad test design or missing class functionality.
@@ -181,15 +199,17 @@
         end
 
         context "with a pipeline with auto reloading turned on" do
-          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
-          let(:pipeline_args) { { "config.reload.automatic" => true } }
+          let(:second_pipeline_config) { "input { generator { id => 'second' } } filter { } output { }" }
+          let(:pipeline_args) { { "pipeline.reloadable" => true } }
+          let(:mock_second_pipeline_config) { mock_pipeline_config(:main, second_pipeline_config, mock_settings(pipeline_args)) }
+          let(:source_loader) { TestSequenceSourceLoader.new(mock_config_pipeline, mock_second_pipeline_config)}
 
           it "tries to reload the pipeline" do
             t = Thread.new { subject.execute }
             sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
-            expect(subject).to receive(:reload_pipeline!).once.and_call_original
-            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
-            subject.reload_state!
+
+            expect(subject.converge_state_and_update).to be_a_successful_converge
+            expect(subject).to have_running_pipeline?(mock_second_pipeline_config)
 
             # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
             # a bad test design or missing class functionality.
@@ -203,24 +223,23 @@
     end
 
     context "when auto_reload is true" do
+      subject { described_class.new(agent_settings, default_source_loader) }
+
       let(:agent_args) do
         {
+          "config.string" => "",
           "config.reload.automatic" => true,
           "config.reload.interval" => 0.01,
-          "path.config" => config_file,
+          "path.config" => config_file
         }
       end
 
-      before(:each) do
-        subject.register_pipeline(pipeline_settings)
-      end
-
       context "if state is clean" do
         it "should periodically reload_state" do
           allow(subject).to receive(:clean_state?).and_return(false)
           t = Thread.new { subject.execute }
-          sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
-          expect(subject).to receive(:reload_state!).at_least(2).times
+          sleep(0.05) until subject.running_pipelines? && subject.pipelines.values.first.running?
+          expect(subject).to receive(:converge_state_and_update).at_least(2).times
 
           # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
           # a bad test design or missing class functionality.
@@ -232,18 +251,19 @@
       end
 
       context "when calling reload_state!" do
-        context "with a config that contains reload incompatible plugins" do
-          let(:second_pipeline_config) { "input { stdin {} } filter { } output { }" }
+        xcontext "with a config that contains reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { stdin { id => '123' } } filter { } output { }" }
 
           it "does not upgrade the new config" do
             t = Thread.new { subject.execute }
-            sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
-            expect(subject).to_not receive(:upgrade_pipeline)
+            sleep(0.05) until subject.running_pipelines? && subject.pipelines.values.first.running?
             File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            sleep(0.2) # lets us catch the new file
+
+            try do
+              expect(subject.pipelines[default_pipeline_id.to_sym].config_str).not_to eq(second_pipeline_config)
+            end
 
-            # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
-            # a bad test design or missing class functionality.
-            sleep(0.1)
             Stud.stop!(t)
             t.join
             subject.shutdown
@@ -251,19 +271,27 @@
         end
 
         context "with a config that does not contain reload incompatible plugins" do
-          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+          let(:second_pipeline_config) { "input { generator { id => 'new' } } filter { } output { }" }
 
           it "does upgrade the new config" do
             t = Thread.new { subject.execute }
-            sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
-            expect(subject).to receive(:upgrade_pipeline).once.and_call_original
+
+            sleep(0.05) until subject.running_pipelines? && subject.pipelines.values.first.running?
+
             File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            sleep(0.2) # lets us catch the new file
+
+            try do
+              expect(subject.pipelines[default_pipeline_id.to_sym]).not_to be_nil
+              expect(subject.pipelines[default_pipeline_id.to_sym].config_str).to match(second_pipeline_config)
+            end
 
             # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
             # a bad test design or missing class functionality.
             sleep(0.1)
             Stud.stop!(t)
             t.join
+            expect(subject.get_pipeline(:main).config_str).to match(second_pipeline_config)
             subject.shutdown
           end
         end
@@ -271,47 +299,26 @@
     end
   end
 
-  describe "#reload_state!" do
-    let(:first_pipeline_config) { "input { } filter { } output { }" }
-    let(:second_pipeline_config) { "input { generator {} } filter { } output { }" }
-    let(:pipeline_args) { {
-      "config.string" => first_pipeline_config,
-      "pipeline.workers" => 4,
-      "config.reload.automatic" => true
-    } }
-
-    before(:each) do
-      subject.register_pipeline(pipeline_settings)
-    end
-
-    after(:each) do
-      subject.close_pipelines
-    end
-
-    context "when fetching a new state" do
-      it "upgrades the state" do
-        expect(subject).to receive(:fetch_config).and_return(second_pipeline_config)
-        expect(subject).to receive(:upgrade_pipeline).with(default_pipeline_id, kind_of(LogStash::Settings), second_pipeline_config)
-        subject.reload_state!
-      end
-    end
-    context "when fetching the same state" do
-      it "doesn't upgrade the state" do
-        expect(subject).to receive(:fetch_config).and_return(first_pipeline_config)
-        expect(subject).to_not receive(:upgrade_pipeline)
-        subject.reload_state!
-      end
-    end
-  end
-
   describe "Environment Variables In Configs" do
-    let(:pipeline_config) { "input { generator { message => '${FOO}-bar' } } filter { } output { }" }
+    let(:temporary_file) { Stud::Temporary.file.path }
+
+    let(:pipeline_config) { "input { generator { message => '${FOO}-bar' count => 1 } } filter { } output { file { path => '#{temporary_file}' } }" }
     let(:agent_args) { {
       "config.reload.automatic" => false,
       "config.reload.interval" => 0.01,
       "config.string" => pipeline_config
     } }
 
+    let(:source_loader) {
+      TestSourceLoader.new(mock_pipeline_config(default_pipeline_id, pipeline_config))
+    }
+
+    subject { described_class.new(mock_settings(agent_args), source_loader) }
+
+    after do
+      subject.shutdown
+    end
+
     context "environment variable templating" do
       before :each do
         @foo_content = ENV["FOO"]
@@ -320,52 +327,50 @@
 
       after :each do
         ENV["FOO"] = @foo_content
-        subject.close_pipelines
       end
 
-      it "doesn't upgrade the state" do
-        allow(subject).to receive(:fetch_config).and_return(pipeline_config)
-        subject.register_pipeline(pipeline_settings)
-        expect(subject.pipelines[default_pipeline_id].inputs.first.message).to eq("foo-bar")
+      it "are evaluated at plugins creation" do
+        expect(subject.converge_state_and_update).to be_a_successful_converge
+
+        # Since the pipeline is running in another threads
+        # the content of the file wont be instant.
+        sleep(0.1) until ::File.size(temporary_file) > 0
+        json_document = LogStash::Json.load(File.read(temporary_file).chomp)
+        expect(json_document["message"]).to eq("foo-bar")
       end
     end
   end
 
   describe "#upgrade_pipeline" do
-    let(:pipeline_config) { "input { } filter { } output { }" }
-    let(:pipeline_args) { {
-      "config.string" => pipeline_config,
-      "pipeline.workers" => 4
-    } }
-    let(:new_pipeline_config) { "input { generator {} } output { }" }
+    let(:pipeline_config) { "input { generator {} } filter { } output { }" }
+    let(:pipeline_args) { { "pipeline.workers" => 4 } }
+    let(:mocked_pipeline_config) { mock_pipeline_config(default_pipeline_id, pipeline_config, mock_settings(pipeline_args))}
+
+    let(:new_pipeline_config) { "input generator {} } output { }" }
+    let(:mocked_new_pipeline_config) { mock_pipeline_config(default_pipeline_id, new_pipeline_config, mock_settings(pipeline_args))}
+    let(:source_loader) { TestSequenceSourceLoader.new(mocked_pipeline_config, mocked_new_pipeline_config)}
+
+    subject { described_class.new(agent_settings, source_loader) }
 
     before(:each) do
-      subject.register_pipeline(pipeline_settings)
+      # Run the initial config
+      expect(subject.converge_state_and_update).to be_a_successful_converge
     end
 
     after(:each) do
       # new pipelines will be created part of the upgrade process so we need
       # to close any initialized pipelines
-      subject.close_pipelines
+      subject.shutdown
     end
 
     context "when the upgrade fails" do
-      before :each do
-        allow(subject).to receive(:fetch_config).and_return(new_pipeline_config)
-        allow(subject).to receive(:create_pipeline).and_return(nil)
-        allow(subject).to receive(:stop_pipeline) do |id|
-          # we register_pipeline but we never execute them so we have to mock #stop_pipeline to
-          # not call Pipeline#shutdown but Pipeline#close
-          subject.close_pipeline(id)
-        end
-      end
-
       it "leaves the state untouched" do
-        subject.send(:"reload_pipeline!", default_pipeline_id)
+        expect(subject.converge_state_and_update).not_to be_a_successful_converge
         expect(subject.pipelines[default_pipeline_id].config_str).to eq(pipeline_config)
       end
 
-      context "and current state is empty" do
+      # TODO(ph): This valid?
+      xcontext "and current state is empty" do
         it "should not start a pipeline" do
           expect(subject).to_not receive(:start_pipeline)
           subject.send(:"reload_pipeline!", default_pipeline_id)
@@ -374,45 +379,21 @@
     end
 
     context "when the upgrade succeeds" do
-      let(:new_config) { "input { generator { count => 1 } } output { }" }
-
-      before :each do
-        allow(subject).to receive(:fetch_config).and_return(new_config)
-        allow(subject).to receive(:start_pipeline).and_return(true)
-        allow(subject).to receive(:stop_pipeline) do |id|
-          # we register_pipeline but we never execute them so we have to mock #stop_pipeline to
-          # not call Pipeline#shutdown but Pipeline#close
-          subject.close_pipeline(id)
-        end
-      end
+      let(:new_config) { "input { generator { id => 'abc' count => 1000000 } } output { }" }
+      let(:mocked_new_pipeline_config) { mock_pipeline_config(default_pipeline_id, new_config, mock_settings(pipeline_args)) }
 
       it "updates the state" do
-        subject.send(:"reload_pipeline!", default_pipeline_id)
+        expect(subject.converge_state_and_update).to be_a_successful_converge
         expect(subject.pipelines[default_pipeline_id].config_str).to eq(new_config)
       end
 
       it "starts the pipeline" do
-        expect(subject).to receive(:start_pipeline).and_return(true)
-        expect(subject).to receive(:stop_pipeline) do |id|
-          # we register_pipeline but we never execute them so we have to mock #stop_pipeline to
-          # not call Pipeline#shutdown but Pipeline#close
-          subject.close_pipeline(id)
-        end
-        subject.send(:"reload_pipeline!", default_pipeline_id)
+        expect(subject.converge_state_and_update).to be_a_successful_converge
+        expect(subject.pipelines[default_pipeline_id].running?).to be_truthy
       end
     end
   end
 
-  describe "#fetch_config" do
-    let(:cli_config) { "filter { drop { } } " }
-    let(:agent_args) { { "config.string" => cli_config, "path.config" => config_file } }
-
-    it "should join the config string and config path content" do
-      fetched_config = subject.send(:fetch_config, agent_settings)
-      expect(fetched_config.strip).to eq(cli_config + IO.read(config_file).strip)
-    end
-  end
-
   context "#started_at" do
     it "return the start time when the agent is started" do
       expect(described_class::STARTED_AT).to be_kind_of(Time)
@@ -436,18 +417,13 @@
       f.path
     end
 
-    let(:pipeline_args) do
-      {
-        "pipeline.workers" => 2,
-        "path.config" => config_path
-      }
-    end
-
     let(:agent_args) do
       {
-        "config.reload.automatic" => false,
+        "config.reload.automatic" => true,
+        "config.reload.interval" => 0.01,
         "pipeline.batch.size" => 1,
-        "metric.collect" => true
+        "metric.collect" => true,
+        "path.config" => config_path
       }
     end
 
@@ -461,11 +437,11 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
     let(:initial_generator_threshold) { 1000 }
     let(:pipeline_thread) do
       Thread.new do
-        subject.register_pipeline(pipeline_settings)
         subject.execute
       end
     end
 
+    subject { described_class.new(agent_settings, default_source_loader) }
 
     before :each do
       allow(LogStash::Outputs::DroppingDummyOutput).to receive(:new).at_least(:once).with(anything).and_return(dummy_output)
@@ -479,7 +455,9 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
       @abort_on_exception = Thread.abort_on_exception
       Thread.abort_on_exception = true
 
-      pipeline_thread
+      @t = Thread.new do
+        subject.execute
+      end
 
       # wait for some events to reach the dummy_output
       sleep(0.1) until dummy_output.events_received > initial_generator_threshold
@@ -505,10 +483,8 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
           f.fsync
         end
 
-        subject.send(:"reload_pipeline!", "main")
-
         # wait until pipeline restarts
-        sleep(0.01) until dummy_output2.events_received > 0
+        sleep(0.2) until dummy_output2.events_received > 0
       end
 
       it "resets the pipeline metric collector" do
@@ -527,8 +503,8 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
         snapshot = subject.metric.collector.snapshot_metric
         value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
         instance_value = snapshot.metric_store.get_with_path("/stats")[:stats][:reloads][:successes].value
-        expect(value).to eq(1)
         expect(instance_value).to eq(1)
+        expect(value).to eq(1)
       end
 
       it "does not set the failure reload timestamp" do
@@ -559,7 +535,7 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
           f.fsync
         end
 
-        subject.send(:"reload_pipeline!", "main")
+        sleep(0.1)
       end
 
       it "does not increase the successful reload count" do
@@ -596,6 +572,14 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
 
     context "when reloading a config that raises exception on pipeline.run" do
       let(:new_config) { "input { generator { count => 10000 } }" }
+      let(:agent_args) do
+        {
+          "config.reload.automatic" => false,
+          "pipeline.batch.size" => 1,
+          "metric.collect" => true,
+          "path.config" => config_path
+        }
+      end
 
       class BrokenGenerator < LogStash::Inputs::Generator
         def register
@@ -605,6 +589,7 @@ def register
 
       before :each do
         allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(BrokenGenerator)
+        allow(LogStash::Plugin).to receive(:lookup).with("output", "stdout").and_return(DummyOutput2)
 
         File.open(config_path, "w") do |f|
           f.write(new_config)
@@ -613,7 +598,7 @@ def register
       end
 
       it "does not increase the successful reload count" do
-        expect { subject.send(:"reload_pipeline!", "main") }.to_not change {
+        expect { subject.converge_state_and_update }.to_not change {
           snapshot = subject.metric.collector.snapshot_metric
           reload_metrics = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads]
           reload_metrics[:successes].value
@@ -621,7 +606,7 @@ def register
       end
 
       it "increases the failures reload count" do
-        expect { subject.send(:"reload_pipeline!", "main") }.to change {
+        expect { subject.converge_state_and_update }.to change {
           snapshot = subject.metric.collector.snapshot_metric
           reload_metrics = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads]
           reload_metrics[:failures].value
diff --git a/logstash-core/spec/logstash/config/config_part_spec.rb b/logstash-core/spec/logstash/config/config_part_spec.rb
new file mode 100644
index 00000000000..265f44a27d2
--- /dev/null
+++ b/logstash-core/spec/logstash/config/config_part_spec.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "logstash/config/config_part"
+require "logstash/config/source/local"
+
+describe LogStash::Config::ConfigPart do
+  let(:reader) { LogStash::Config::Source::Local::ConfigStringLoader.to_s }
+  let(:source_id) { "config_string" }
+  let(:config_string) { "input { generator {}}  output { stdout {} }"}
+
+  subject { described_class.new(reader, source_id, config_string) }
+
+  it "expose reader, source_id, source as instance methods" do
+    expect(subject.reader).to eq(reader)
+    expect(subject.source_id).to eq(source_id)
+    expect(subject.config_string).to eq(config_string)
+  end
+end
diff --git a/logstash-core/spec/logstash/config/loader_spec.rb b/logstash-core/spec/logstash/config/loader_spec.rb
deleted file mode 100644
index 955feb2f615..00000000000
--- a/logstash-core/spec/logstash/config/loader_spec.rb
+++ /dev/null
@@ -1,38 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/config/loader"
-
-describe LogStash::Config::Loader do
-  let(:logger) { double("logger") }
-  subject { described_class.new(logger) }
-
-  context "when local" do
-    before { expect(subject).to receive(:local_config).with(path) }
-
-    context "unix" do
-      let(:path) { './test.conf' }
-      it 'works with relative path' do
-        subject.load_config(path)
-      end
-    end
-
-    context "windows" do
-      let(:path) { '.\test.conf' }
-      it 'work with relative windows path' do
-        subject.load_config(path)
-      end
-    end
-  end
-
-  context "when remote" do
-    context 'supported scheme' do
-      let(:path) { "http://test.local/superconfig.conf" }
-      let(:dummy_config) { 'input {}' }
-
-      before { expect(Net::HTTP).to receive(:get) { dummy_config } }
-      it 'works with http' do
-        expect(subject.load_config(path)).to eq("#{dummy_config}\n")
-      end
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/config/pipeline_config_spec.rb b/logstash-core/spec/logstash/config/pipeline_config_spec.rb
new file mode 100644
index 00000000000..abfa327b480
--- /dev/null
+++ b/logstash-core/spec/logstash/config/pipeline_config_spec.rb
@@ -0,0 +1,60 @@
+# encoding: utf-8
+require "logstash/config/pipeline_config"
+require "logstash/config/config_part"
+require "logstash/config/source/local"
+
+describe LogStash::Config::PipelineConfig do
+  let(:source) { LogStash::Config::Source::Local }
+  let(:pipeline_id) { :main }
+  let(:ordered_config_parts) do
+    [
+      LogStash::Config::ConfigPart.new(LogStash::Config::Source::Local::ConfigPathLoader, "/tmp/1", "input { generator1 }"),
+      LogStash::Config::ConfigPart.new(LogStash::Config::Source::Local::ConfigPathLoader, "/tmp/2", "input { generator2 }"),
+      LogStash::Config::ConfigPart.new(LogStash::Config::Source::Local::ConfigPathLoader, "/tmp/3", "input { generator3 }"),
+      LogStash::Config::ConfigPart.new(LogStash::Config::Source::Local::ConfigPathLoader, "/tmp/4", "input { generator4 }"),
+      LogStash::Config::ConfigPart.new(LogStash::Config::Source::Local::ConfigPathLoader, "/tmp/5", "input { generator5 }"),
+      LogStash::Config::ConfigPart.new(LogStash::Config::Source::Local::ConfigPathLoader, "/tmp/6", "input { generator6 }"),
+      LogStash::Config::ConfigPart.new(LogStash::Config::Source::Local::ConfigStringLoader, "config_string", "input { generator1 }"),
+    ]
+  end
+
+  let(:unordered_config_parts) { ordered_config_parts.shuffle }
+  let(:settings) { LogStash::SETTINGS }
+
+  subject { described_class.new(source, pipeline_id, unordered_config_parts, settings) }
+
+  it "returns the source" do
+    expect(subject.source).to eq(source)
+  end
+
+  it "returns the pipeline id" do
+    expect(subject.pipeline_id).to eq(pipeline_id)
+  end
+
+  it "returns the sorted config parts" do
+    expect(subject.config_parts).to eq(ordered_config_parts)
+  end
+
+  it "returns the config_hash" do
+    expect(subject.config_hash).not_to be_nil
+  end
+
+  it "returns the merged `ConfigPart#config_string`" do
+    expect(subject.config_string).to eq(ordered_config_parts.collect(&:config_string).join("\n"))
+  end
+
+  it "records when the config was read" do
+    expect(subject.read_at).to be <= Time.now
+  end
+
+  it "does object equality on config_hash and pipeline_id" do
+    another_exact_pipeline = described_class.new(source, pipeline_id, ordered_config_parts, settings)
+    expect(subject).to eq(another_exact_pipeline)
+
+    not_matching_pipeline = described_class.new(source, pipeline_id, [], settings)
+    expect(subject).not_to eq(not_matching_pipeline)
+
+    not_same_pipeline_id = described_class.new(source, :another_pipeline, unordered_config_parts, settings)
+    expect(subject).not_to eq(not_same_pipeline_id)
+  end
+end
diff --git a/logstash-core/spec/logstash/config/source/local_spec.rb b/logstash-core/spec/logstash/config/source/local_spec.rb
new file mode 100644
index 00000000000..7f23f7c893f
--- /dev/null
+++ b/logstash-core/spec/logstash/config/source/local_spec.rb
@@ -0,0 +1,393 @@
+# encoding: utf-8
+require "logstash/config/source/local"
+require "rspec/expectations"
+require "stud/temporary"
+require "fileutils"
+require "pathname"
+require_relative "../../../support/helpers"
+require_relative "../../../support/matchers"
+require "spec_helper"
+require "webmock/rspec"
+
+describe LogStash::Config::Source::Local::ConfigStringLoader do
+  subject { described_class }
+  let(:config_string) { "input { generator {} } output { stdout {} }"}
+
+  it "returns one config_parts" do
+    expect(subject.read(config_string).size).to eq(1)
+  end
+
+  it "returns a valid config part" do
+    config_part = subject.read(config_string).first
+    expect(config_part).to be_a_config_part(described_class.to_s, "config_string", config_string)
+  end
+end
+
+describe LogStash::Config::Source::Local::ConfigPathLoader do
+  subject { described_class }
+
+  context "no configs" do
+    context "in the directory" do
+      let(:directory) do
+        p =  Stud::Temporary.pathname
+        FileUtils.mkdir_p(p)
+        p
+      end
+
+      it "returns an empty array" do
+        expect(subject.read(directory)).to be_empty
+      end
+    end
+
+    context "target file doesn't exist" do
+      let(:directory) do
+        p =  Stud::Temporary.pathname
+        FileUtils.mkdir_p(p)
+        ::File.join(p, "ls.conf")
+      end
+
+      it "returns an empty array" do
+        expect(subject.read(directory)).to be_empty
+      end
+    end
+  end
+
+  context "when it exist" do
+    shared_examples "read config from files" do
+      let(:directory) { Stud::Temporary.pathname }
+
+      before do
+        files.keys.shuffle.each do |file|
+          content = files[file]
+          temporary_file(content, file, directory)
+        end
+      end
+
+      it "returns a `config_parts` per file" do
+        expect(subject.read(reader_config).size).to eq(files.size)
+      end
+
+      it "returns alphabetically sorted parts" do
+        parts = subject.read(reader_config)
+        expect(parts.collect { |part| ::File.basename(part.source_id) }).to eq(files.keys.sort)
+      end
+
+      it "returns valid `config_parts`" do
+        parts = subject.read(reader_config)
+
+        parts.each do |part|
+          basename = ::File.basename(part.source_id)
+          file_path = ::File.join(directory, basename)
+          content = files[basename]
+          expect(part).to be_a_config_part(described_class.to_s, file_path, content)
+        end
+      end
+    end
+
+    context "when the files have invalid encoding" do
+      let(:config_string) { "\x80" }
+      let(:file_path) { Stud::Temporary.pathname }
+      let(:file) { ::File.join(file_path, "wrong_encoding.conf") }
+
+      before do
+        FileUtils.mkdir_p(file_path)
+        f = File.open(file, "wb") do |file|
+          file.write(config_string)
+        end
+      end
+
+      it "raises an exception" do
+        expect { subject.read(file_path) }.to raise_error LogStash::ConfigLoadingError, /#{file_path}/
+      end
+    end
+
+    context "when we target one file" do
+      let(:reader_config) { ::File.join(directory, files.keys.first) }
+      let(:files) {
+        {
+          "config1.conf" => "input1",
+        }
+      }
+
+      include_examples "read config from files"
+    end
+
+    context "when we target a path with multiples files" do
+      let(:reader_config) { directory }
+
+      let(:files) {
+        {
+          "config1.conf" => "input1",
+          "config2.conf" => "input2",
+          "config3.conf" => "input3",
+          "config4.conf" => "input4"
+        }
+      }
+
+      include_examples "read config from files"
+    end
+
+    context "when there temporary files in the directory" do
+      let(:reader_config) { ::File.join(directory, "conf*.conf") }
+
+      let(:files) {
+        {
+          "config1.conf" => "input1",
+          "config2.conf" => "input2",
+          "config3.conf" => "input3",
+          "config4.conf" => "input4"
+        }
+      }
+
+      let(:other_files) do
+        {
+          "config1.conf~" => "input1",
+          "config2.conf~" => "input2",
+          "config3.conf~" => "input3",
+          "config4.conf~" => "input4"
+        }
+      end
+
+      include_examples "read config from files" do
+        before do
+          other_files.keys.shuffle.each do |file|
+            content = files[file]
+            temporary_file(content, file, directory)
+          end
+
+          # make sure we actually do some filtering
+          expect(Dir.glob(::File.join(directory, "*")).size).to eq(other_files.size + files.size)
+        end
+      end
+    end
+
+    context "when the path is a wildcard" do
+      let(:reader_config) { ::File.join(directory, "conf*.conf") }
+
+      let(:files) {
+        {
+          "config1.conf" => "input1",
+          "config2.conf" => "input2",
+          "config3.conf" => "input3",
+          "config4.conf" => "input4"
+        }
+      }
+
+      let(:other_files) do
+        {
+          "bad1.conf" => "input1",
+          "bad2.conf" => "input2",
+          "bad3.conf" => "input3",
+          "bad4.conf" => "input4"
+        }
+      end
+
+      include_examples "read config from files" do
+        before do
+          other_files.keys.shuffle.each do |file|
+            content = files[file]
+            temporary_file(content, file, directory)
+          end
+
+          # make sure we actually do some filtering
+          expect(Dir.glob(::File.join(directory, "*")).size).to eq(other_files.size + files.size)
+        end
+      end
+    end
+
+    context "URI defined path (file://..)" do
+      let(:reader_config) { "file://#{::File.join(directory, files.keys.first)}" }
+      let(:files) {
+        {
+          "config1.conf" => "input1",
+        }
+      }
+
+      include_examples "read config from files"
+    end
+
+    context "relative path" do
+      let(:reader_config) do
+        current = Pathname.new(::File.dirname(__FILE__))
+        target = Pathname.new(::File.join(directory, files.keys.first))
+        target.relative_path_from(current).to_s
+      end
+
+      let(:files) {
+        {
+          "config1.conf" => "input1",
+        }
+      }
+
+      include_examples "read config from files"
+    end
+  end
+end
+
+describe LogStash::Config::Source::Local::ConfigRemoteLoader do
+  before :all do
+    WebMock.disable_net_connect!
+  end
+
+  after :all do
+    WebMock.allow_net_connect!
+  end
+
+  subject { described_class }
+
+  let(:remote_url) { "http://test.dev/superconfig.conf" }
+
+  context "when the remote configuration exist" do
+    let(:config_string) { "input { generator {} } output { stdout {} }"}
+
+    before do
+      stub_request(:get, remote_url)
+        .to_return({
+        :body => config_string,
+        :status => 200
+      })
+    end
+
+    it "returns one config_parts" do
+      expect(subject.read(remote_url).size).to eq(1)
+    end
+
+    it "returns a valid config part" do
+      config_part = subject.read(remote_url).first
+      expect(config_part).to be_a_config_part(described_class.to_s, remote_url, config_string)
+    end
+  end
+
+  # I am aware that 656 http doesn't exist I am just testing the
+  # catch all block
+  [302, 404, 500, 403, 656].each do |code|
+    context "when the remote return an error code: #{code}" do
+      before do
+        stub_request(:get, remote_url)
+          .to_return({ :status => code })
+      end
+
+      it "raises the exception up" do
+        expect { subject.read(remote_url) }.to raise_error LogStash::ConfigLoadingError
+      end
+    end
+  end
+end
+
+describe LogStash::Config::Source::Local do
+  let(:input_block) { "input { generator {} }" }
+  let(:filter_block) { "filter { mutate {} } " }
+  let(:output_block) { "output { elasticsearch {}}" }
+  subject { described_class.new(settings) }
+
+  context "when `config.string` and `config.path` are set`" do
+    let(:config_file) { temporary_file(input_block) }
+
+    let(:settings) do
+      mock_settings(
+        "config.string" => "#{filter_block} #{output_block}",
+        "path.config" => config_file
+      )
+    end
+
+    it "returns a merged config" do
+      expect(subject.pipeline_configs.config_string).to include(input_block, output_block, filter_block)
+    end
+  end
+
+  context "when only the `config.string` is set" do
+    let(:settings) do
+      mock_settings( "config.string" => filter_block)
+    end
+
+    it "returns a config" do
+      expect(subject.pipeline_configs.config_string).to include(filter_block)
+    end
+  end
+
+  context "when only the `path.config` is set" do
+    let(:config_file) { temporary_file(input_block) }
+    let(:settings) do
+      mock_settings( "path.config" => config_file)
+    end
+
+    it "returns a config" do
+      expect(subject.pipeline_configs.config_string).to include(input_block)
+    end
+  end
+
+  context "when the `path.config` is an url" do
+    let(:remote_url) { "http://test.dev/superconfig.conf" }
+
+    before :all do
+      WebMock.disable_net_connect!
+    end
+
+    after :all do
+      WebMock.allow_net_connect!
+    end
+
+    before do
+      stub_request(:get, remote_url)
+        .to_return({
+        :body => input_block,
+        :status => 200
+      })
+    end
+
+    let(:settings) do
+      mock_settings( "path.config" => remote_url)
+    end
+
+    it "returns a config" do
+      expect(subject.pipeline_configs.config_string).to include(input_block)
+    end
+
+    context "when `config.string` is set" do
+      let(:settings) do
+        mock_settings(
+          "path.config" => remote_url,
+          "config.string" => filter_block
+        )
+      end
+
+      it "returns a merged config" do
+        expect(subject.pipeline_configs.config_string).to include(input_block, filter_block)
+      end
+    end
+  end
+
+  context "incomplete configuration" do
+    context "when the input block is missing" do
+      let(:settings) { mock_settings( "config.string" => "#{filter_block} #{output_block}") }
+
+      it "add stdin input" do
+        expect(subject.pipeline_configs.config_string).to include(LogStash::Config::Defaults.input)
+      end
+    end
+
+    context "when the output block is missing" do
+      let(:settings) { mock_settings( "config.string" => "#{input_block} #{filter_block}") }
+
+      it "add stdout output" do
+        expect(subject.pipeline_configs.config_string).to include(LogStash::Config::Defaults.output)
+      end
+    end
+
+    context "when both the output block and input block are missing" do
+      let(:settings) { mock_settings( "config.string" => "#{filter_block}") }
+
+      it "add stdin and output" do
+        expect(subject.pipeline_configs.config_string).to include(LogStash::Config::Defaults.output, LogStash::Config::Defaults.input)
+      end
+    end
+
+    context "when it has an input and an output" do
+      let(:settings) { mock_settings( "config.string" => "#{input_block} #{filter_block} #{output_block}") }
+
+      it "doesn't add anything" do
+        expect(subject.pipeline_configs.config_string).not_to include(LogStash::Config::Defaults.output, LogStash::Config::Defaults.input)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/config/source_loader_spec.rb b/logstash-core/spec/logstash/config/source_loader_spec.rb
new file mode 100644
index 00000000000..568ca660a1a
--- /dev/null
+++ b/logstash-core/spec/logstash/config/source_loader_spec.rb
@@ -0,0 +1,109 @@
+# encoding: utf-8
+require "logstash/config/source_loader"
+require "logstash/config/source/base"
+require_relative "../../support/helpers"
+
+class DummySource < LogStash::Config::Source::Base
+  def pipeline_configs
+    [self.class]
+  end
+
+  def match?
+    @settings.get("path.config") =~ /dummy/
+  end
+end
+
+class AnotherDummySource < LogStash::Config::Source::Base
+  def pipeline_configs
+    [self.class]
+  end
+
+  def match?
+    @settings.get("path.config") =~ /another/
+  end
+end
+
+class FailingSource < LogStash::Config::Source::Base
+  def pipeline_configs
+    raise "Something went wrong"
+  end
+
+  def match?
+    @settings.get("path.config") =~ /fail/
+  end
+end
+
+describe LogStash::Config::SourceLoader do
+  subject { described_class.new }
+
+  it "default to local source" do
+    expect(subject.sources.size).to eq(0)
+  end
+
+  it "allows to override the available source loaders" do
+    subject.configure_sources(DummySource)
+    expect(subject.sources.size).to eq(1)
+    expect(subject.sources).to include(DummySource)
+  end
+
+  it "allows to add a new sources" do
+    subject.add_source(DummySource)
+    subject.add_source(LogStash::Config::Source::Local)
+
+    expect(subject.sources.size).to eq(2)
+    expect(subject.sources).to include(DummySource, LogStash::Config::Source::Local)
+  end
+
+  context "when no source match" do
+    let(:settings) { mock_settings("path.config" => "make it not match") } # match both regex
+
+    it "raises an exception" do
+      subject.configure_sources([DummySource.new(settings), AnotherDummySource.new(settings)])
+
+      expect { subject.fetch }.to raise_error
+    end
+  end
+
+  context "when source loader match" do
+    context "when an happen in the source" do
+      let(:settings) { mock_settings("path.config" => "dummy fail") }
+
+      it "wraps the error in a failed result" do
+        subject.configure_sources([DummySource.new(settings), FailingSource.new(settings)])
+
+        result = subject.fetch
+
+        expect(result.success?).to be_falsey
+        expect(result.error).not_to be_nil
+      end
+    end
+
+    context "when multiple match" do
+      let(:settings) { mock_settings("path.config" => "another dummy") } # match both regex
+
+      it "return the loaders with the matched sources" do
+        subject.configure_sources([DummySource.new(settings), AnotherDummySource.new(settings)])
+
+        result = subject.fetch
+
+        expect(result.success?).to be_truthy
+        expect(result.response.size).to eq(2)
+        expect(result.response).to include(DummySource, AnotherDummySource)
+      end
+    end
+
+    context "when multiple match" do
+      let(:settings) { mock_settings("path.config" => "another") } # match both regex
+
+      it "return the loaders with the matched sources" do
+        subject.configure_sources([DummySource.new(settings), AnotherDummySource.new(settings)])
+
+        result = subject.fetch
+
+        expect(result.success?).to be_truthy
+        expect(result.response.size).to eq(1)
+        expect(result.response).to include(AnotherDummySource)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/converge_result_spec.rb b/logstash-core/spec/logstash/converge_result_spec.rb
new file mode 100644
index 00000000000..37276f63a06
--- /dev/null
+++ b/logstash-core/spec/logstash/converge_result_spec.rb
@@ -0,0 +1,179 @@
+# encoding: utf-8
+require "logstash/converge_result"
+require "logstash/pipeline_action/stop"
+require "spec_helper"
+
+describe LogStash::ConvergeResult do
+  let(:expected_actions_count) { 2 }
+  let(:action) { LogStash::PipelineAction::Stop.new(:main) }
+
+  subject { described_class.new(expected_actions_count) }
+
+
+  context "When the action was executed" do
+    it "returns the time of execution" do
+      expect(LogStash::ConvergeResult::FailedAction.new("testing").executed_at.class).to eq(LogStash::Timestamp)
+      expect(LogStash::ConvergeResult::SuccessfulAction.new.executed_at.class).to eq(LogStash::Timestamp)
+    end
+  end
+
+  context "conversion of action result" do
+    let(:action) { LogStash::PipelineAction::Stop.new(:an_action) }
+
+    context "booleans" do
+      context "True" do
+        it "converts to a `SuccessfulAction`" do
+          subject.add(action, true)
+          expect(subject.successful_actions.keys).to include(action)
+        end
+      end
+
+      context "False" do
+        it "converts to a `FailedAction`" do
+          subject.add(action, false)
+          expect(subject.failed_actions.keys).to include(action)
+          expect(subject.failed_actions.values.pop.message).to match(/Could not execute action: #{action}/)
+        end
+      end
+    end
+
+    context "`ActionResult` classes" do
+      context "SuccessfulAction" do
+        let(:result) { LogStash::ConvergeResult::SuccessfulAction.new }
+
+        it "doesn't convert the class" do
+          subject.add(action, result)
+          expect(subject.successful_actions.keys).to include(action)
+          expect(subject.successful_actions.values).to include(result)
+        end
+      end
+
+      context "FailedAction" do
+        let(:result) { LogStash::ConvergeResult::FailedAction.new("could be worse") }
+
+        it "doesn't convert the class" do
+          subject.add(action, result)
+          expect(subject.failed_actions.keys).to include(action)
+          expect(subject.failed_actions.values).to include(result)
+        end
+      end
+    end
+
+    context "Exception" do
+      it "converts to a `FailedAction" do
+        begin
+          raise ArgumentError, "hello world"
+        rescue => e
+          subject.add(action, e)
+
+          expect(subject.failed_actions.keys).to include(action)
+          failed_action = subject.failed_actions.values.pop
+
+          expect(failed_action.message).to eq("hello world")
+          expect(failed_action.backtrace).not_to be_nil
+        end
+      end
+    end
+  end
+
+  context "when not all the actions are executed" do
+    context "#complete?" do
+      it "returns false" do
+        expect(subject.complete?).to be_falsey
+      end
+    end
+
+    context "#success?" do
+      it "returns false" do
+        expect(subject.success?).to be_falsey
+      end
+    end
+  end
+
+  context "when all the actions are executed" do
+    context "all succesfull" do
+      let(:success_action) { LogStash::PipelineAction::Stop.new(:success) }
+      let(:success_action_2) { LogStash::PipelineAction::Stop.new(:success_2) }
+
+      before do
+        subject.add(success_action, true)
+        subject.add(success_action_2, true)
+      end
+
+      context "#success?" do
+        it "returns true" do
+          expect(subject.success?).to be_truthy
+        end
+      end
+
+      context "#complete?" do
+        it "returns true" do
+          expect(subject.complete?).to be_truthy
+        end
+      end
+
+      context "filtering on the actions result" do
+        it "returns the successful actions" do
+          expect(subject.successful_actions.size).to eq(2)
+          expect(subject.successful_actions.keys).to include(success_action, success_action_2)
+        end
+
+        it "returns the failed actions" do
+          expect(subject.failed_actions.size).to eq(0)
+        end
+      end
+    end
+
+    context "not successfully" do
+      let(:success_action) { LogStash::PipelineAction::Stop.new(:success) }
+      let(:failed_action) { LogStash::PipelineAction::Stop.new(:failed) }
+
+      before do
+        subject.add(failed_action, false)
+        subject.add(success_action, true)
+      end
+
+      context "#success?" do
+        it "returns false" do
+          expect(subject.success?).to be_falsey
+        end
+      end
+
+      context "#complete?" do
+        it "returns true" do
+          expect(subject.complete?).to be_truthy
+        end
+      end
+
+      context "#total" do
+        it "returns the number of actions" do
+          expect(subject.total).to eq(2)
+        end
+      end
+
+      context "#fails_count" do
+        it "returns the number of actions" do
+          expect(subject.fails_count).to eq(1)
+        end
+      end
+
+      context "#success_count" do
+        it "returns the number of actions" do
+          expect(subject.success_count).to eq(1)
+        end
+      end
+
+      context "filtering on the actions result" do
+        it "returns the successful actions" do
+          expect(subject.successful_actions.size).to eq(1)
+          expect(subject.successful_actions.keys).to include(success_action)
+        end
+
+        it "returns the failed actions" do
+          expect(subject.failed_actions.size).to eq(1)
+          expect(subject.failed_actions.keys).to include(failed_action)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_action/create_spec.rb b/logstash-core/spec/logstash/pipeline_action/create_spec.rb
new file mode 100644
index 00000000000..cfbbc27fc24
--- /dev/null
+++ b/logstash-core/spec/logstash/pipeline_action/create_spec.rb
@@ -0,0 +1,68 @@
+# encoding: utf-8
+require "spec_helper"
+require_relative "../../support/helpers"
+require "logstash/pipeline_action/create"
+require "logstash/instrument/null_metric"
+require "logstash/inputs/generator"
+
+describe LogStash::PipelineAction::Create do
+  let(:metric) { LogStash::Instrument::NullMetric.new(LogStash::Instrument::Collector.new) }
+  let(:pipeline_config) { mock_pipeline_config(:main, "input { generator { id => '123' } } output { null {} }") }
+  let(:pipelines) {  Hash.new }
+
+  before do
+    clear_data_dir
+  end
+
+  subject { described_class.new(pipeline_config, metric) }
+
+  after do
+    pipelines.each { |_, pipeline| pipeline.shutdown }
+  end
+
+  it "returns the pipeline_id" do
+    expect(subject.pipeline_id).to eq(:main)
+  end
+
+
+  context "when we have really short lived pipeline" do
+    let(:pipeline_config) { mock_pipeline_config(:main, "input { generator { count => 1 } } output { null {} }") }
+
+    it "returns a successful execution status" do
+      expect(subject.execute(pipelines)).to be_truthy
+    end
+  end
+
+  context "when the pipeline succesfully start" do
+    it "adds the pipeline to the current pipelines" do
+      expect { subject.execute(pipelines) }.to change(pipelines, :size).by(1)
+    end
+
+    it "starts the pipeline" do
+      subject.execute(pipelines)
+      expect(pipelines[:main].running?).to be_truthy
+    end
+
+    it "returns a successful execution status" do
+      expect(subject.execute(pipelines)).to be_truthy
+    end
+  end
+
+  context  "when the pipeline doesn't start" do
+    context "with a syntax error" do
+      let(:pipeline_config) { mock_pipeline_config(:main, "input { generator { id => '123' } } output { stdout ") } # bad syntax
+
+      it "raises the exception upstream" do
+        expect { subject.execute(pipelines) }.to raise_error
+      end
+    end
+
+    context "with an error raised during `#register`" do
+      let(:pipeline_config) { mock_pipeline_config(:main, "input { generator { id => '123' } } filter { ruby { init => '1/0' code => '1+2' } } output { null {} }") }
+
+      it "returns false" do
+        expect(subject.execute(pipelines)).not_to be_a_successful_action
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_action/reload_spec.rb b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
new file mode 100644
index 00000000000..17cd52acae4
--- /dev/null
+++ b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
@@ -0,0 +1,82 @@
+# encoding: utf-8
+require "spec_helper"
+require_relative "../../support/helpers"
+require_relative "../../support/matchers"
+require "logstash/pipeline_action/reload"
+require "logstash/instrument/null_metric"
+
+describe LogStash::PipelineAction::Reload do
+  let(:metric) { LogStash::Instrument::NullMetric.new(LogStash::Instrument::Collector.new) }
+  let(:pipeline_id) { :main }
+  let(:new_pipeline_config) { mock_pipeline_config(pipeline_id, "input { generator { id => 'new' } } output { null {} }", { "pipeline.reloadable" => true}) }
+  let(:pipeline_config) { "input { generator {} } output { null {} }" }
+  let(:pipeline) { LogStash::Pipeline.new(pipeline_config, mock_settings("pipeline.reloadable" => true)) }
+  let(:pipelines) { { pipeline_id => pipeline } }
+
+  subject { described_class.new(new_pipeline_config, metric) }
+
+  before do
+    clear_data_dir
+    pipeline.start
+  end
+
+  after do
+    pipelines.each { |_, pipeline| pipeline.shutdown }
+  end
+
+  it "returns the pipeline_id" do
+    expect(subject.pipeline_id).to eq(pipeline_id)
+  end
+
+  context "when existing pipeline and new pipeline are both reloadable" do
+    it "stop the previous pipeline" do
+      expect { subject.execute(pipelines) }.to change(pipeline, :running?).from(true).to(false)
+    end
+
+    it "start the new pipeline" do
+      subject.execute(pipelines)
+      expect(pipelines[pipeline_id].running?).to be_truthy
+    end
+
+    it "run the new pipeline code" do
+      subject.execute(pipelines)
+      expect(pipelines[pipeline_id].config_hash).to eq(new_pipeline_config.config_hash)
+    end
+  end
+
+  context "when the existing pipeline is not reloadable" do
+    before do
+      allow(pipeline).to receive(:reloadable?).and_return(false)
+    end
+
+    it "cannot successfully execute the action" do
+      expect(subject.execute(pipelines)).not_to be_a_successful_action
+    end
+  end
+
+  context "when the new pipeline is not reloadable" do
+    let(:new_pipeline_config) { mock_pipeline_config(pipeline_id, "input { generator { id => 'new' } } output { null {} }", { "pipeline.reloadable" => false}) }
+
+    it "cannot successfully execute the action" do
+      expect(subject.execute(pipelines)).not_to be_a_successful_action
+    end
+  end
+
+  context "when the new pipeline has syntax errors" do
+    let(:new_pipeline_config) { mock_pipeline_config(pipeline_id, "input generator { id => 'new' } } output { null {} }", { "pipeline.reloadable" => false}) }
+
+    it "cannot successfully execute the action" do
+      expect(subject.execute(pipelines)).not_to be_a_successful_action
+    end
+  end
+
+  context "when there is an error in the register" do
+    before do
+      allow_any_instance_of(LogStash::Inputs::Generator).to receive(:register).and_raise("Bad value")
+    end
+
+    it "cannot successfully execute the action" do
+      expect(subject.execute(pipelines)).not_to be_a_successful_action
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_action/stop_spec.rb b/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
new file mode 100644
index 00000000000..b0caaf61cea
--- /dev/null
+++ b/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
@@ -0,0 +1,36 @@
+# encoding: utf-8
+require "spec_helper"
+require_relative "../../support/helpers"
+require "logstash/pipeline_action/stop"
+require "logstash/pipeline"
+require "logstash/instrument/null_metric"
+
+describe LogStash::PipelineAction::Stop do
+  let(:pipeline_config) { "input { generator {} } output { null {} }" }
+  let(:pipeline_id) { :main }
+  let(:pipeline) { LogStash::Pipeline.new(pipeline_config) }
+  let(:pipelines) { { :main => pipeline } }
+
+  subject { described_class.new(pipeline_id) }
+
+  before do
+    clear_data_dir
+    pipeline.start
+  end
+
+  after do
+    pipeline.shutdown
+  end
+
+  it "returns the pipeline_id" do
+    expect(subject.pipeline_id).to eq(:main)
+  end
+
+  it "shutdown the running pipeline" do
+    expect { subject.execute(pipelines) }.to change(pipeline, :running?).from(true).to(false)
+  end
+
+  it "removes the pipeline from the running pipelines" do
+    expect { subject.execute(pipelines) }.to change { pipelines.include?(pipeline_id) }.from(true).to(false)
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 552b62618dd..3dc774a3ae0 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -3,6 +3,7 @@
 require "logstash/inputs/generator"
 require "logstash/filters/multiline"
 require_relative "../support/mocks_classes"
+require_relative "../support/helpers"
 require_relative "../logstash/pipeline_reporter_spec" # for DummyOutput class
 
 class DummyInput < LogStash::Inputs::Base
@@ -210,7 +211,7 @@ class TestPipeline < LogStash::Pipeline
           msg = "Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads"
           pipeline = TestPipeline.new(test_config_with_filters)
           expect(pipeline.logger).to receive(:warn).with(msg,
-            {:count_was=>worker_thread_count, :filters=>["dummyfilter"]})
+            hash_including({:count_was=>worker_thread_count, :filters=>["dummyfilter"]}))
           pipeline.run
           expect(pipeline.worker_threads.size).to eq(safe_thread_count)
           pipeline.shutdown
@@ -223,8 +224,7 @@ class TestPipeline < LogStash::Pipeline
           msg = "Warning: Manual override - there are filters that might" +
                 " not work with multiple worker threads"
           pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
-          expect(pipeline.logger).to receive(:warn).with(msg,
-            {:worker_threads=> override_thread_count, :filters=>["dummyfilter"]})
+          expect(pipeline.logger).to receive(:warn).with(msg, hash_including({:worker_threads=> override_thread_count, :filters=>["dummyfilter"]}))
           pipeline.run
           expect(pipeline.worker_threads.size).to eq(override_thread_count)
           pipeline.shutdown
@@ -406,7 +406,7 @@ class TestPipeline < LogStash::Pipeline
       let(:batch_size) { LogStash::Pipeline::MAX_INFLIGHT_WARN_THRESHOLD + 1 }
 
       it "should raise a max inflight warning if the max_inflight count is exceeded" do
-        expect(logger).to have_received(:warn).with(warning_prefix)
+        expect(logger).to have_received(:warn).with(warning_prefix, hash_including(:pipeline_id => anything))
       end
     end
   end
@@ -854,4 +854,34 @@ class TestPipeline < LogStash::Pipeline
        expect(pipeline1.instance_variables).to eq(pipeline2.instance_variables)
     end
   end
+
+  context "#reloadable?" do
+    after do
+      pipeline.close # close the queue
+    end
+
+    context "when all plugins are reloadable and pipeline is configured as reloadable" do
+      let(:pipeline) { LogStash::Pipeline.new("input { generator {} } output { null {} }", mock_settings("pipeline.reloadable" => true)) }
+
+      it "returns true" do
+        expect(pipeline.reloadable?).to be_truthy
+      end
+    end
+
+    context "when the plugins are not reloadable and pipeline is configured as reloadable" do
+      let(:pipeline) { LogStash::Pipeline.new("input { stdin {} } output { null {} }", mock_settings("pipeline.reloadable" => true)) }
+
+      it "returns true" do
+        expect(pipeline.reloadable?).to be_falsey
+      end
+    end
+
+    context "when all plugins are reloadable and pipeline is configured as non-reloadable" do
+      let(:pipeline) { LogStash::Pipeline.new("input { generator {} } output { null {} }", mock_settings("pipeline.reloadable" => false)) }
+
+      it "returns true" do
+        expect(pipeline.reloadable?).to be_falsey
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb b/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
index 7a20e16945e..d8fb0e2b909 100644
--- a/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
+++ b/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
@@ -48,6 +48,12 @@ def work?
     expect { subject.register_hooks(emitter.class, listener) }.to change { subject.hooks_count(emitter.class) }.by(1)
   end
 
+  it "verifies if a hook is registered to a specific emitter scope" do
+    subject.register_hooks(emitter.class, listener)
+    expect(subject.registered_hook?(emitter.class, listener.class)).to be_truthy
+    expect(subject.registered_hook?(Class.new, listener.class)).to be_falsey
+  end
+
   it "link the emitter class to the listener" do
     subject.register_emitter(emitter.class, emitter.dispatcher)
     subject.register_hooks(emitter.class, listener)
diff --git a/logstash-core/spec/logstash/runner_spec.rb b/logstash-core/spec/logstash/runner_spec.rb
index e64412ebdc1..9efeef0bdf7 100644
--- a/logstash-core/spec/logstash/runner_spec.rb
+++ b/logstash-core/spec/logstash/runner_spec.rb
@@ -6,7 +6,9 @@
 require "stud/temporary"
 require "logstash/util/java_version"
 require "logstash/logging/json"
+require "logstash/config/source_loader"
 require "json"
+require_relative "../support/helpers"
 
 class NullRunner
   def run(args); end
@@ -18,6 +20,8 @@ def run(args); end
   let(:logger) { double("logger") }
 
   before :each do
+    clear_data_dir
+
     allow(LogStash::Runner).to receive(:logger).and_return(logger)
     allow(logger).to receive(:debug?).and_return(true)
     allow(logger).to receive(:subscribe).with(any_args)
@@ -36,9 +40,6 @@ def run(args); end
     LogStash::SETTINGS.reset
   end
 
-  after :all do
-  end
-
   describe "argument precedence" do
     let(:config) { "input {} output {}" }
     let(:cli_args) { ["-e", config, "-w", "20"] }
@@ -136,6 +137,10 @@ def run(args); end
   end
 
   describe "--config.test_and_exit" do
+    before do
+      # Reset the source in a clean state before any asserts
+      LogStash::Config::SOURCE_LOADER.configure_sources([])
+    end
     subject { LogStash::Runner.new("") }
     let(:args) { ["-t", "-e", pipeline_string] }
 
@@ -382,10 +387,10 @@ def run(args); end
   describe "path.settings" do
     subject { LogStash::Runner.new("") }
     context "if does not exist" do
-      let(:args) { ["--path.settings", "/tmp/a/a/a/a", "-e", "input {} output {}"] }
+      let(:args) { ["--path.settings", "/tmp/a/a/a/a", "-e", "input { generator { count => 1000 }} output {}"] }
 
       it "should not terminate logstash" do
-        expect(subject.run(args)).to eq(nil)
+        expect(subject.run(args)).to eq(0)
       end
 
       context "but if --help is passed" do
diff --git a/logstash-core/spec/logstash/state_resolver_spec.rb b/logstash-core/spec/logstash/state_resolver_spec.rb
new file mode 100644
index 00000000000..a6103d4b55a
--- /dev/null
+++ b/logstash-core/spec/logstash/state_resolver_spec.rb
@@ -0,0 +1,130 @@
+# encoding: utf-8
+require "spec_helper"
+require_relative "../support/helpers"
+require_relative "../support/matchers"
+require "logstash/state_resolver"
+require "logstash/config/config_part"
+require "logstash/config/pipeline_config"
+require "logstash/instrument/null_metric"
+require "logstash/pipeline"
+require "ostruct"
+require "digest"
+
+describe LogStash::StateResolver do
+  subject { described_class.new(metric) }
+  let(:metric) { LogStash::Instrument::NullMetric.new }
+
+  before do
+    clear_data_dir
+  end
+
+  context "when no pipeline is running" do
+    let(:running_pipelines) { {} }
+
+    context "no pipeline configs is received" do
+      let(:pipeline_configs) { [] }
+
+      it "returns no action" do
+        expect(subject.resolve(running_pipelines, pipeline_configs).size).to eq(0)
+      end
+    end
+
+    context "we receive some pipeline configs" do
+      let(:pipeline_configs) { [mock_pipeline_config(:hello_world)] }
+
+      it "returns some actions" do
+        expect(subject.resolve(running_pipelines, pipeline_configs)).to have_actions(
+          [:create, :hello_world],
+        )
+      end
+    end
+  end
+
+  context "when some pipeline are running" do
+    context "when a pipeline is running" do
+      let(:running_pipelines) { { :main => mock_pipeline(:main) } }
+
+
+      after do
+        # ensure that the the created pipeline are closed
+        running_pipelines.each { |_, pipeline| pipeline.close }
+      end
+
+      context "when the pipeline config contains a new one and the existing" do
+        let(:pipeline_configs) { [mock_pipeline_config(:hello_world), mock_pipeline_config(:main)] }
+
+        it "creates the new one and keep the other one" do
+          expect(subject.resolve(running_pipelines, pipeline_configs)).to have_actions(
+            [:create, :hello_world],
+          )
+        end
+
+        context "when the pipeline config contains only the new one" do
+          let(:pipeline_configs) { [mock_pipeline_config(:hello_world)] }
+
+          it "creates the new one and stop the old one one" do
+            expect(subject.resolve(running_pipelines, pipeline_configs)).to have_actions(
+              [:create, :hello_world],
+              [:stop, :main]
+            )
+          end
+        end
+
+        context "when the pipeline config contains no pipeline" do
+          let(:pipeline_configs) { [] }
+
+          it "stops the old one one" do
+            expect(subject.resolve(running_pipelines, pipeline_configs)).to have_actions(
+              [:stop, :main]
+            )
+          end
+        end
+
+        context "when pipeline config contains an updated pipeline" do
+          let(:pipeline_configs) { [mock_pipeline_config(:main, "input { generator {}}")] }
+
+          it "reloads the old one one" do
+            expect(subject.resolve(running_pipelines, pipeline_configs)).to have_actions(
+              [:reload, :main]
+            )
+          end
+        end
+      end
+    end
+
+    context "when we have a lot of pipeline running" do
+      let(:running_pipelines) do
+        {
+          :main1 => mock_pipeline(:main1),
+          :main2 => mock_pipeline(:main2),
+          :main3 => mock_pipeline(:main3),
+          :main4 => mock_pipeline(:main4),
+          :main5 => mock_pipeline(:main5),
+          :main6 => mock_pipeline(:main6),
+        }
+      end
+
+      let(:pipeline_configs) do
+        [
+          mock_pipeline_config(:main1),
+          mock_pipeline_config(:main9),
+          mock_pipeline_config(:main5, "input { generator {}}"),
+          mock_pipeline_config(:main3, "input { generator {}}"),
+          mock_pipeline_config(:main7)
+        ]
+      end
+
+      it "generates actions required to converge" do
+        expect(subject.resolve(running_pipelines, pipeline_configs)).to have_actions(
+          [:create, :main7],
+          [:create, :main9],
+          [:reload, :main3],
+          [:reload, :main5],
+          [:stop, :main2],
+          [:stop, :main4],
+          [:stop, :main6]
+        )
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/util/java_version_spec.rb b/logstash-core/spec/logstash/util/java_version_spec.rb
index 930ce064d5c..6e43c21a934 100644
--- a/logstash-core/spec/logstash/util/java_version_spec.rb
+++ b/logstash-core/spec/logstash/util/java_version_spec.rb
@@ -39,6 +39,28 @@
     expect(mod.bad_java_version?("pwi3270sr9fp10-20150708_01 (SR9 FP10)")).to be_falsey
   end
 
+  context ".validate_java_version!" do
+    context "with good version" do
+      before do
+        expect(mod).to receive(:version).and_return("1.8.0")
+      end
+
+      it "doesn't raise an error" do
+        expect { mod.validate_java_version! }.not_to raise_error
+      end
+    end
+
+    context "with a bad version" do
+      before do
+        expect(mod).to receive(:version).and_return("1.7.0").twice
+      end
+
+      it "raises an error" do
+        expect { mod.validate_java_version! }.to raise_error RuntimeError, /Java version 1.8.0 or later/
+      end
+    end
+  end
+
   describe "parsing java versions" do
     it "should return nil on a nil version" do
       expect(mod.parse_java_version(nil)).to be_nil
diff --git a/logstash-core/spec/support/helpers.rb b/logstash-core/spec/support/helpers.rb
index 98ac9e0b82d..6a9a78ec5c6 100644
--- a/logstash-core/spec/support/helpers.rb
+++ b/logstash-core/spec/support/helpers.rb
@@ -1,4 +1,6 @@
 # encoding: utf-8
+require "stud/task"
+
 def silence_warnings
   warn_level = $VERBOSE
   $VERBOSE = nil
@@ -8,20 +10,81 @@ def silence_warnings
 end
 
 def clear_data_dir
+  if defined?(agent_settings)
     data_path = agent_settings.get("path.data")
-    Dir.foreach(data_path) do |f|
+  else
+    data_path = LogStash::SETTINGS.get("path.data")
+  end
+
+  Dir.foreach(data_path) do |f|
     next if f == "." || f == ".." || f == ".gitkeep"
     FileUtils.rm_rf(File.join(data_path, f))
   end
 end
 
+def mock_settings(settings_values)
+  settings = LogStash::SETTINGS.clone
+
+  settings_values.each do |key, value|
+    settings.set(key, value)
+  end
+
+  settings
+end
+
+def mock_pipeline(pipeline_id, reloadable = true, config_hash = nil)
+  config_string = "input { stdin { id => '#{pipeline_id}' }}"
+  settings = mock_settings("pipeline.id" => pipeline_id.to_s,
+                           "config.string" => config_string,
+                           "config.reload.automatic" => reloadable)
+  pipeline = LogStash::Pipeline.new(config_string, settings)
+  pipeline
+end
+
+def mock_pipeline_config(pipeline_id, config_string = nil, settings = {})
+  config_string = "input { stdin { id => '#{pipeline_id}' }}" if config_string.nil?
+
+  # This is for older tests when we already have a config
+  unless settings.is_a?(LogStash::Settings)
+    settings.merge!({ "pipeline.id" => pipeline_id.to_s })
+    settings = mock_settings(settings)
+  end
+
+  config_part = LogStash::Config::ConfigPart.new(:config_string, "config_string", config_string)
+
+  LogStash::Config::PipelineConfig.new(LogStash::Config::Source::Local, pipeline_id, config_part, settings)
+end
+
+def start_agent(agent)
+  agent_task = Stud::Task.new do
+    begin
+      agent.execute
+    rescue => e
+      raise "Start Agent exception: #{e}"
+    end
+  end
+
+  sleep(0.1) unless subject.running?
+  agent_task
+end
+
+def temporary_file(content, file_name = Time.now.to_i.to_s, directory = Stud::Temporary.pathname)
+  FileUtils.mkdir_p(directory)
+  target = ::File.join(directory, file_name)
+
+  File.open(target, "w+") do |f|
+    f.write(content)
+  end
+  target
+end
+
 RSpec::Matchers.define :ir_eql do |expected|
   match do |actual|
     next unless expected.java_kind_of?(org.logstash.config.ir.SourceComponent) && actual.java_kind_of?(org.logstash.config.ir.SourceComponent)
-    
+
     expected.sourceComponentEquals(actual)
   end
-  
+
   failure_message do |actual|
     "actual value \n#{actual.to_s}\nis not .sourceComponentEquals to the expected value: \n#{expected.to_s}\n"
   end
diff --git a/logstash-core/spec/support/matchers.rb b/logstash-core/spec/support/matchers.rb
index 88ea508b02d..88f3833dee2 100644
--- a/logstash-core/spec/support/matchers.rb
+++ b/logstash-core/spec/support/matchers.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "rspec"
 require "rspec/expectations"
+require "logstash/config/pipeline_config"
 
 RSpec::Matchers.define :be_a_metric_event do |namespace, type, *args|
   match do
@@ -28,3 +29,105 @@ def all_instance_methods_implemented?
     "Expecting `#{expected}` to implements instance methods of `#{actual}`, missing methods: #{missing_methods.join(",")}"
   end
 end
+
+RSpec::Matchers.define :have_actions do |*expected|
+  match do |actual|
+    expect(actual.size).to eq(expected.size)
+
+    expected_values = expected.each_with_object([]) do |i, obj|
+      klass_name = "LogStash::PipelineAction::#{i.first.capitalize}"
+      obj << [klass_name, i.last]
+    end
+
+    actual_values = actual.each_with_object([]) do |i, obj|
+      klass_name = i.class.name
+      obj << [klass_name, i.pipeline_id]
+    end
+
+    values_match? expected_values, actual_values
+  end
+end
+
+RSpec::Matchers.define :have_pipeline? do |pipeline_config|
+  match do |agent|
+    pipeline = agent.get_pipeline(pipeline_config.pipeline_id)
+    expect(pipeline).to_not be_nil
+    expect(pipeline.config_str).to eq(pipeline_config.config_string)
+  end
+
+  match_when_negated do |agent|
+    pipeline = agent.get_pipeline(pipeline_config.pipeline_id)
+    pipeline.nil? || pipeline.config_str != pipeline_config.config_string
+  end
+end
+
+RSpec::Matchers.define :have_running_pipeline? do |pipeline_config|
+  match do |agent|
+    pipeline = agent.get_pipeline(pipeline_config.pipeline_id)
+    expect(pipeline).to_not be_nil
+    expect(pipeline.config_str).to eq(pipeline_config.config_string)
+    expect(pipeline.running?).to be_truthy
+  end
+
+  failure_message do |agent|
+    pipeline = agent.get_pipeline(pipeline_config.pipeline_id)
+
+    if pipeline.nil?
+      "Expected pipeline to exist and running, be we cannot find `#{pipeline_config.pipeline_id}` in the running pipelines `#{agent.pipelines.keys.join(",")}`"
+      else
+        if pipeline.running? == false
+          "Found `#{pipeline_config.pipeline_id}` in the list of pipelines but its not running"
+        elsif pipeline.config_str != pipeline_config.config_string
+          "Found `#{pipeline_config.pipeline_id}` in the list of pipelines and running, but the config_string doesn't match,
+Expected:
+#{pipeline_config.config_string}
+
+got:
+#{pipeline.config_str}"
+        end
+      end
+  end
+
+  match_when_negated do
+    raise "Not implemented"
+  end
+end
+
+RSpec::Matchers.define :be_a_successful_converge do
+  match do |converge_results|
+    converge_results.success?
+  end
+
+  failure_message do |converge_results|
+    "Expected all actions to be successful:
+    #{converge_results.failed_actions.collect { |action, result| "pipeline_id: #{action.pipeline_id}, message: #{result.message}"}.join("\n")}"
+  end
+
+  failure_message_when_negated do |converge_results|
+    "Expected all actions to failed:
+    #{converge_results.successful_actions.collect { |action, result| "pipeline_id: #{action.pipeline_id}"}.join("\n")}"
+  end
+end
+
+RSpec::Matchers.define :be_a_successful_action do
+  match do |pipeline_action|
+    case pipeline_action
+    when LogStash::ConvergeResult::ActionResult
+      return pipeline_action.successful?
+    when TrueClass
+      return true
+    when FalseClass
+      return false
+    else
+      raise "Incompatible class type of: #{pipeline_action.class}, Expected `Boolean` or `LogStash::ConvergeResult::ActionResult`"
+    end
+  end
+end
+
+RSpec::Matchers.define :be_a_config_part do |reader, source_id, config_string = nil|
+  match do |actual|
+   expect(actual.reader).to eq(reader)
+   expect(actual.source_id).to eq(source_id)
+   expect(actual.config_string).to match(config_string) unless config_string.nil?
+  end
+end
diff --git a/logstash-core/spec/support/mocks_classes.rb b/logstash-core/spec/support/mocks_classes.rb
index a69b89bc821..4b69cd534ae 100644
--- a/logstash-core/spec/support/mocks_classes.rb
+++ b/logstash-core/spec/support/mocks_classes.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require "logstash/outputs/base"
+require "logstash/config/source_loader"
 require "logstash/inputs/base"
 require "thread"
 
@@ -87,5 +88,84 @@ def close
         @num_closes = 1
       end
     end
+end end
+
+
+# A Test Source loader will return the same configuration on every fetch call
+class TestSourceLoader
+  FailedFetch = LogStash::Config::SourceLoader::FailedFetch
+  SuccessfulFetch = LogStash::Config::SourceLoader::SuccessfulFetch
+
+  def initialize(*responses)
+    @count = Concurrent::AtomicFixnum.new(0)
+    @responses_mutex = Mutex.new
+    @responses = coerce_responses(responses)
+  end
+
+  def fetch
+    @count.increment
+    @responses
+end
+
+  def fetch_count
+    @count.value
+  end
+
+  private
+  def coerce_responses(responses)
+    if responses.size == 1
+      response = responses.first
+
+      case response
+      when LogStash::Config::SourceLoader::SuccessfulFetch
+        response
+      when LogStash::Config::SourceLoader::FailedFetch
+        response
+      else
+        LogStash::Config::SourceLoader::SuccessfulFetch.new(Array(response))
+      end
+
+    else
+      LogStash::Config::SourceLoader::SuccessfulFetch.new(responses)
+    end
+  end
+end
+
+# This source loader will return a new configuration on very call until we ran out.
+class TestSequenceSourceLoader
+  FailedFetch = LogStash::Config::SourceLoader::FailedFetch
+  SuccessfulFetch = LogStash::Config::SourceLoader::SuccessfulFetch
+
+  attr_reader :original_responses
+
+  def initialize(*responses)
+    @count = Concurrent::AtomicFixnum.new(0)
+    @responses_mutex = Mutex.new
+    @responses = responses.collect(&method(:coerce_response))
+
+    @original_responses = @responses.dup
+  end
+
+  def fetch
+    @count.increment
+    response  = @responses_mutex.synchronize { @responses.shift }
+    raise "TestSequenceSourceLoader runs out of response" if response.nil?
+    response
+  end
+
+  def fetch_count
+    @count.value
+  end
+
+  private
+  def coerce_response(response)
+    case response
+    when LogStash::Config::SourceLoader::SuccessfulFetch
+      response
+    when LogStash::Config::SourceLoader::FailedFetch
+      response
+    else
+      LogStash::Config::SourceLoader::SuccessfulFetch.new(Array(response))
+    end
   end
 end
diff --git a/logstash-core/spec/support/shared_examples.rb b/logstash-core/spec/support/shared_examples.rb
index 0218bebb53c..04a409d2b21 100644
--- a/logstash-core/spec/support/shared_examples.rb
+++ b/logstash-core/spec/support/shared_examples.rb
@@ -97,7 +97,7 @@
 
 shared_examples "not found" do
   it "should return a 404 to unknown request" do
-    do_request { get "/i_want_to_believe-#{Time.now.to_i}" }
+    get "/i_want_to_believe-#{Time.now.to_i}"
     expect(last_response.content_type).to eq("application/json")
     expect(last_response).not_to be_ok
     expect(last_response.status).to eq(404)
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index e1e3ac421b9..28843f00c86 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -22,6 +22,8 @@ def puts(payload)
 
 RSpec.configure do |c|
   Flores::RSpec.configure(c)
+  c.include LogStashHelper
+  c.extend LogStashHelper
   c.before do
     # TODO: commented out on post-merged in master - the logger has moved to log4j
     #
