diff --git a/.fossa.yml b/.fossa.yml
new file mode 100755
index 00000000000..55e69801bbc
--- /dev/null
+++ b/.fossa.yml
@@ -0,0 +1,59 @@
+# Generated by FOSSA CLI (https://github.com/fossas/fossa-cli)
+# Visit https://fossa.com to learn more
+
+version: 2
+cli:
+  server: https://app.fossa.com
+  fetcher: custom
+  project: git@github.com:elastic/logstash.git
+analyze:
+  modules:
+    - name: Logstash gems
+      type: bundler
+      strategy: lockfile
+      target: .
+      path: .
+    - name: benchmark-cli
+      type: gradle
+      target: 'benchmark-cli:'
+      path: .
+    - name: dependencies-report
+      type: gradle
+      target: 'dependencies-report:'
+      path: .
+    - name: ingest-converter
+      type: gradle
+      target: 'ingest-converter:'
+      path: .
+    - name: logstash-core
+      type: gradle
+      target: 'logstash-core:'
+      path: .
+    - name: logstash-core-benchmarks
+      type: gradle
+      target: 'logstash-core-benchmarks:'
+      path: .
+    - name: logstash-integration-tests
+      type: gradle
+      target: 'logstash-integration-tests:'
+      path: .
+    - name: logstash-xpack
+      type: gradle
+      target: 'logstash-xpack:'
+    #   path: .
+    # - name: docker
+    #   type: pip
+    #   target: docker
+    #   path: docker
+    # - name: Gemfile
+    #   type: gem
+    #   target: qa
+    #   path: qa
+    # - name: Gemfile
+    #   type: gem
+    #   target: qa/integration
+    #   path: qa/integration
+    # - name: Gemfile
+    #   type: gem
+    #   target: tools/paquet
+    #   path: tools/paquet
diff --git a/bin/system-install b/bin/system-install
index 5ac9a2ce2d6..d81c5e87b49 100755
--- a/bin/system-install
+++ b/bin/system-install
@@ -17,8 +17,8 @@ elif [ "$1" == "-h" ] || [ "$1" == "--help" ]; then
   echo
   echo "OPTIONSFILE: Full path to a startup.options file"
   echo "OPTIONSFILE is required if STARTUPTYPE is specified, but otherwise looks first"
-  echo "in $LOGSTASH_HOME/config/startup.options and then /etc/logstash/startup.options"
-  echo "Last match wins"
+  echo "in /etc/logstash/startup.options and then "
+  echo "in $LOGSTASH_HOME/config/startup.options "
   echo
   echo "STARTUPTYPE: e.g. sysv, upstart, systemd, etc."
   echo "OPTIONSFILE is required to specify a STARTUPTYPE."
diff --git a/build.gradle b/build.gradle
index 7674003f42e..6f3fdd25e47 100644
--- a/build.gradle
+++ b/build.gradle
@@ -131,7 +131,7 @@ tasks.register("configureArchitecture") {
     String esArch = arch
 
     // For aarch64 architectures, beats and elasticsearch name their artifacts differently
-    if (arch == "aarch64") {i
+    if (arch == "aarch64") {
         beatsArch="arm64"
         esArch="aarch64"
     } else if (arch == "amd64") {
@@ -450,12 +450,9 @@ tasks.register("runIntegrationTests"){
     dependsOn tasks.getByPath(":logstash-integration-tests:integrationTests")
     dependsOn copyEs
     dependsOn copyFilebeat
+    shouldRunAfter ":logstash-core:test"
 }
 
-bootstrap.dependsOn assemblyDeps
-
-runIntegrationTests.shouldRunAfter tasks.getByPath(":logstash-core:test")
-check.dependsOn runIntegrationTests
 
 
 tasks.register("generateLicenseReport", JavaExec) {
@@ -506,10 +503,13 @@ tasks.register("generatePluginsVersion") {
 }
 
 bootstrap.dependsOn assemblyDeps
-
-runIntegrationTests.shouldRunAfter tasks.getByPath(":logstash-core:test")
-check.dependsOn runIntegrationTests
-
+// FIXME: adding the integration tests task to check will mean
+// that any registered task will be evaluated. This creates an issue
+// where the downloadES task may throw an error on versions where
+// Elasticsearch doesn't yet have a build we can fetch
+// So for now we'll remove this to unblock builds, but finding a way
+// to compartimentalize failures is needed going forward
+//check.dependsOn runIntegrationTests
 
 Boolean oss = System.getenv('OSS').equals('true')
 
@@ -520,11 +520,12 @@ if (!oss) {
         dependsOn installTestGems
       }
     }
-    tasks.getByPath(":logstash-xpack:rubyIntegrationTests").configure {
-      dependsOn copyEs
-    }
   }
-
-  task runXPackUnitTests(dependsOn: [tasks.getByPath(":logstash-xpack:rubyTests")]) {}
-  task runXPackIntegrationTests(dependsOn: [tasks.getByPath(":logstash-xpack:rubyIntegrationTests")]) {}
 }
+
+ tasks.register("runXPackUnitTests"){
+   dependsOn ":logstash-xpack:rubyTests"
+ }
+ tasks.register("runXPackIntegrationTests"){
+   dependsOn ":logstash-xpack:rubyIntegrationTests"
+ }
diff --git a/ci/bootstrap_dependencies.sh b/ci/bootstrap_dependencies.sh
new file mode 100644
index 00000000000..9b4e972e039
--- /dev/null
+++ b/ci/bootstrap_dependencies.sh
@@ -0,0 +1,3 @@
+#!/bin/bash
+
+./gradlew installDefaultGems
diff --git a/docker/Makefile b/docker/Makefile
index 673043f8b4e..7bb782cab2f 100644
--- a/docker/Makefile
+++ b/docker/Makefile
@@ -21,6 +21,7 @@ HTTPD ?= logstash-docker-artifact-server
 FIGLET := pyfiglet -w 160 -f puffy
 
 all: build-from-local-artifacts build-from-local-oss-artifacts public-dockerfiles
+DATE:= $(shell date -u +'%Y-%m-%dT%H:%M:%S.%sZ')
 
 lint: venv
 	flake8 tests
@@ -70,6 +71,7 @@ docker_paths:
 
 public-dockerfiles: venv templates/Dockerfile.j2 docker_paths $(COPY_FILES)
 	jinja2 \
+	  -D created_date='$(DATE)' \
 	  -D elastic_version='$(ELASTIC_VERSION)' \
 	  -D version_tag='$(VERSION_TAG)' \
 	  -D image_flavor='full' \
@@ -77,6 +79,7 @@ public-dockerfiles: venv templates/Dockerfile.j2 docker_paths $(COPY_FILES)
 	  -D release='$(RELEASE)' \
 	  templates/Dockerfile.j2 > $(ARTIFACTS_DIR)/Dockerfile-full && \
 	jinja2 \
+	  -D created_date='$(DATE)' \
 	  -D elastic_version='$(ELASTIC_VERSION)' \
 	  -D version_tag='$(VERSION_TAG)' \
 	  -D image_flavor='oss' \
@@ -133,6 +136,7 @@ env2yaml: golang
 dockerfile: venv templates/Dockerfile.j2
 	$(foreach FLAVOR, $(IMAGE_FLAVORS), \
 	  jinja2 \
+	    -D created_date='$(DATE)' \
 	    -D elastic_version='$(ELASTIC_VERSION)' \
 	    -D version_tag='$(VERSION_TAG)' \
 	    -D image_flavor='$(FLAVOR)' \
diff --git a/docker/templates/Dockerfile.j2 b/docker/templates/Dockerfile.j2
index 44e782d2e2e..59ac10cdbe7 100644
--- a/docker/templates/Dockerfile.j2
+++ b/docker/templates/Dockerfile.j2
@@ -7,8 +7,10 @@
 
 {% if image_flavor == 'oss' -%}
   {% set tarball = 'logstash-oss-%s.tar.gz' % elastic_version -%}
+  {% set license = 'Apache 2.0' -%}
 {% else -%}
   {% set tarball = 'logstash-%s.tar.gz' % elastic_version -%}
+  {% set license = 'Elastic License' -%}
 {% endif -%}
 
 
@@ -62,17 +64,20 @@ ADD env2yaml/env2yaml /usr/local/bin/
 EXPOSE 9600 5044
 
 
-LABEL org.label-schema.schema-version="1.0" \
+LABEL  org.label-schema.schema-version="1.0" \
   org.label-schema.vendor="Elastic" \
+  org.opencontainers.image.vendor="Elastic" \
   org.label-schema.name="logstash" \
+  org.opencontainers.image.title="logstash" \
   org.label-schema.version="{{ elastic_version }}" \
+  org.opencontainers.image.version="{{ elastic_version }}" \
   org.label-schema.url="https://www.elastic.co/products/logstash" \
   org.label-schema.vcs-url="https://github.com/elastic/logstash" \
-{% if image_flavor == 'oss' -%}
-  license="Apache-2.0"
-{% else -%}
-  license="Elastic License"
-{% endif -%}
+  license="{{ license }}" \
+  org.label-schema.license="{{ license }}" \
+  org.opencontainers.image.licenses="{{ license }}" \
+  org.label-schema.build-date={{ created_date }} \
+  org.opencontainers.image.created={{ created_date }}
 
 
 ENTRYPOINT ["/usr/local/bin/docker-entrypoint"]
diff --git a/docs/include/plugin_header-integration.asciidoc b/docs/include/plugin_header-integration.asciidoc
new file mode 100644
index 00000000000..c12cbbb23ad
--- /dev/null
+++ b/docs/include/plugin_header-integration.asciidoc
@@ -0,0 +1,39 @@
+ifeval::["{versioned_docs}"!="true"]
+[subs="attributes"]
+++++
+<titleabbrev>{plugin}</titleabbrev>
+++++
+endif::[]
+ifeval::["{versioned_docs}"=="true"]
+[subs="attributes"]
+++++
+<titleabbrev>{version}</titleabbrev>
+++++
+endif::[]
+
+* A component of the <<plugins-integrations-{plugin},{plugin} integration plugin>> 
+* Integration version: {version}
+* Released on: {release_date}
+* {changelog_url}[Changelog]
+
+ifeval::["{versioned_docs}"!="true"]
+
+For other versions, see the
+{lsplugindocs}/{type}-{plugin}-index.html[Versioned plugin docs].
+
+endif::[]
+
+ifeval::["{versioned_docs}"=="true"]
+
+For other versions, see the <<integration-{plugin}-index,overview list>>.
+
+To learn more about Logstash, see the {logstash-ref}/index.html[Logstash Reference].
+
+endif::[]
+
+==== Getting Help
+
+For questions about the plugin, open a topic in the http://discuss.elastic.co[Discuss] forums. 
+For bugs or feature requests, open an issue in https://github.com/logstash-plugins/logstash-integration-{plugin}[Github].
+For the list of Elastic supported plugins, please consult the https://www.elastic.co/support/matrix#matrix_logstash_plugins[Elastic Support Matrix].
+
diff --git a/docs/static/advanced-pipeline.asciidoc b/docs/static/advanced-pipeline.asciidoc
index 63668aa0ec5..1e3e274ee98 100644
--- a/docs/static/advanced-pipeline.asciidoc
+++ b/docs/static/advanced-pipeline.asciidoc
@@ -31,7 +31,7 @@ input plugin enables Logstash to receive events from the Elastic Beats framework
 to work with the Beats framework, such as Packetbeat and Metricbeat, can also send event data to Logstash.
 
 To install Filebeat on your data source machine, download the appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page]. You can also refer to
-{filebeat-ref}/filebeat-getting-started.html[Getting Started with Filebeat] in the Beats documentation for additional
+{filebeat-ref}/filebeat-installation-configuration.html[Filebeat quick start] for additional
 installation instructions.
 
 After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
@@ -654,7 +654,7 @@ If you are using Kibana to visualize your data, you can also explore the Filebea
 
 image::static/images/kibana-filebeat-data.png[Discovering Filebeat data in Kibana]
 
-See the {filebeat-ref}/filebeat-getting-started.html[Filebeat getting started docs] for info about loading the Kibana
+See the {filebeat-ref}/filebeat-installation-configuration.html[Filebeat quick start docs] for info about loading the Kibana
 index pattern for Filebeat.
 
 You've successfully created a pipeline that uses Filebeat to take Apache web logs as input, parses those logs to
diff --git a/docs/static/fb-ls-kafka-example.asciidoc b/docs/static/fb-ls-kafka-example.asciidoc
index e9fef351957..de2280ffb62 100644
--- a/docs/static/fb-ls-kafka-example.asciidoc
+++ b/docs/static/fb-ls-kafka-example.asciidoc
@@ -28,8 +28,8 @@ The `-e` flag is optional and sends output to standard error instead of syslog.
 A connection to {es} and {kib} is required for this one-time setup
 step because {filebeat} needs to create the index template in {es} and
 load the sample dashboards into {kib}. For more information about configuring
-the connection to {es}, see the Filebeat modules
-{filebeat-ref}/filebeat-modules-quickstart.html[quick start].
+the connection to {es}, see the Filebeat
+{filebeat-ref}/filebeat-installation-configuration.html[quick start].
 +
 After the template and dashboards are loaded, you'll see the message `INFO
 {kib} dashboards successfully loaded. Loaded dashboards`.
diff --git a/docs/static/monitoring/images/pipeline-tree.png b/docs/static/monitoring/images/pipeline-tree.png
index 2643c755a4f..5df4d3545e9 100644
Binary files a/docs/static/monitoring/images/pipeline-tree.png and b/docs/static/monitoring/images/pipeline-tree.png differ
diff --git a/docs/static/monitoring/monitoring-mb.asciidoc b/docs/static/monitoring/monitoring-mb.asciidoc
index a6bd60edc99..116a11304d2 100644
--- a/docs/static/monitoring/monitoring-mb.asciidoc
+++ b/docs/static/monitoring/monitoring-mb.asciidoc
@@ -52,7 +52,7 @@ monitoring.cluster_uuid: PRODUCTION_ES_CLUSTER_UUID
 [[configure-metricbeat]]
 ==== Install and configure {metricbeat}
 
-. {metricbeat-ref}/metricbeat-installation.html[Install {metricbeat}] on the
+. {metricbeat-ref}/metricbeat-installation-configuration.html[Install {metricbeat}] on the
 same server as {ls}. 
 
 . Enable the `logstash-xpack` module in {metricbeat}. +
diff --git a/docs/static/processing-info.asciidoc b/docs/static/processing-info.asciidoc
index 6466c9af094..fe9904109b8 100644
--- a/docs/static/processing-info.asciidoc
+++ b/docs/static/processing-info.asciidoc
@@ -44,3 +44,5 @@ processing cost required to preserve order.
 The Java pipeline initialization time appears in the startup logs at INFO level.
 Initialization time is the time it takes to compile the pipeline config and
 instantiate the compiled execution for all workers.
+
+include::reserved-fields.asciidoc[]
diff --git a/docs/static/reserved-fields.asciidoc b/docs/static/reserved-fields.asciidoc
new file mode 100644
index 00000000000..d05fe9b7337
--- /dev/null
+++ b/docs/static/reserved-fields.asciidoc
@@ -0,0 +1,39 @@
+[float]
+[[reserved-fields]] 
+==== Reserved fields in {ls} events 
+
+Some fields in {ls} events are reserved, or are required to adhere to a certain
+shape. Using these fields can cause runtime exceptions when the event API or
+plugins encounter incompatible values.
+
+[cols="<,<",options="header",]
+|=======================================================================
+| | 
+| <<metadata,`@metadata`>> |A key/value map. 
+
+Ruby-based Plugin API: value is an 
+https://javadoc.io/static/org.jruby/jruby-core/9.2.5.0/org/jruby/RubyHash.html[org.jruby.RubyHash]. 
+
+Java-based Plugin API: value is an 
+https://github.com/elastic/logstash/blob/master/logstash-core/src/main/java/org/logstash/ConvertedMap.java[org.logstash.ConvertedMap].
+ 
+In serialized form (such as JSON): a key/value map where the keys must be
+strings and the values are not constrained to a particular type.
+
+| `@timestamp` |An object holding representation of a specific moment in time.
+
+Ruby-based Plugin API: value is an
+https://javadoc.io/static/org.jruby/jruby-core/9.2.5.0/org/jruby/RubyTime.html[org.jruby.RubyTime].
+
+Java-based Plugin API: value is a
+https://docs.oracle.com/en/java/javase/11/docs/api/java.base/java/time/Instant.html[java.time.Instant].
+
+In serialized form (such as JSON) or when setting with Event#set: an
+ISO8601-compliant String value is acceptable.
+
+| `@version` |A string, holding an integer value.
+| `tags` |An array of distinct strings
+|=======================================================================
+
+
+
diff --git a/docs/static/security/api-keys.asciidoc b/docs/static/security/api-keys.asciidoc
index b5718dce7d6..e95dd2bf49b 100644
--- a/docs/static/security/api-keys.asciidoc
+++ b/docs/static/security/api-keys.asciidoc
@@ -176,6 +176,27 @@ filter {
 <1> Format is `id:api_key` (as returned by {ref}/security-api-create-api-key.html[Create API key])
 
 
+[float]
+[[ls-api-key-mon]]
+====== Create an API key for monitoring and management
+
+
+
+/////
+API keys are tied to the cluster they were created in.
+
+In a single cluster a key can be shared for
+ingestion plus monitoring purposes, while a production cluster and monitoring
+cluster setup will require separate keys.
+
+{ls} can send both collected data and monitoring information to {es}. If you are
+sending both to the same cluster, you can use the same API key. For different
+clusters, you need to use an API key per cluster.
+/////
+
+
+
+
 [float]
 [[learn-more-api-keys]]
 ===== Learn more about API keys
diff --git a/docs/static/settings-file.asciidoc b/docs/static/settings-file.asciidoc
index 57379e04c8d..5075e84a62a 100644
--- a/docs/static/settings-file.asciidoc
+++ b/docs/static/settings-file.asciidoc
@@ -27,8 +27,8 @@ pipeline.batch.size: 125
 pipeline.batch.delay: 50
 -------------------------------------------------------------------------------------
 
-The `logstash.yml` file also supports bash-style interpolation of environment variables in
-setting values.
+The `logstash.yml` file also supports bash-style interpolation of environment variables and
+keystore secrets in setting values.
 
 [source,yaml]
 -------------------------------------------------------------------------------------
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 3042f58a8b0..f05a3b9f7f6 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -200,7 +200,9 @@ def converge_state_and_update
 
     converge_result
   rescue => e
-    logger.error("An exception happened when converging configuration", :exception => e.class, :message => e.message, :backtrace => e.backtrace)
+    attributes = {:exception => e.class, :message => e.message}
+    attributes.merge!({:backtrace => e.backtrace}) if logger.debug?
+    logger.error("An exception happened when converging configuration", attributes)
   end
 
   # Calculate the Logstash uptime in milliseconds
diff --git a/logstash-core/lib/logstash/java_pipeline.rb b/logstash-core/lib/logstash/java_pipeline.rb
index c44a81f7284..7e7ba301680 100644
--- a/logstash-core/lib/logstash/java_pipeline.rb
+++ b/logstash-core/lib/logstash/java_pipeline.rb
@@ -17,6 +17,7 @@
 
 require "thread"
 require "concurrent"
+require "thwait"
 require "logstash/filters/base"
 require "logstash/inputs/base"
 require "logstash/outputs/base"
@@ -119,19 +120,31 @@ def start
     @finished_run.make_false
 
     @thread = Thread.new do
+      error_log_params = ->(e) {
+        default_logging_keys(
+          :exception => e,
+          :backtrace => e.backtrace,
+          "pipeline.sources" => pipeline_source_details
+        )
+      }
+
       begin
         LogStash::Util.set_thread_name("pipeline.#{pipeline_id}")
         ThreadContext.put("pipeline.id", pipeline_id)
         run
         @finished_run.make_true
       rescue => e
-        close
-        pipeline_log_params = default_logging_keys(
-          :exception => e,
-          :backtrace => e.backtrace,
-          "pipeline.sources" => pipeline_source_details)
-        logger.error("Pipeline aborted due to error", pipeline_log_params)
+        # no need to log at ERROR level since this log will be redundant to the log in
+        # the worker loop thread global rescue clause
+        logger.debug("Pipeline terminated by worker error", error_log_params.call(e))
       ensure
+        # we must trap any exception here to make sure the following @finished_execution
+        # is always set to true regardless of any exception before in the close method call
+        begin
+          close
+        rescue => e
+          logger.error("Pipeline close error, ignoring", error_log_params.call(e))
+        end
         @finished_execution.make_true
       end
     end
@@ -176,21 +189,18 @@ def run
 
     transition_to_running
     start_flusher # Launches a non-blocking thread for flush events
-    wait_inputs
-    transition_to_stopped
-
-    @logger.debug("Input plugins stopped! Will shutdown filter/output workers.", default_logging_keys)
-
-    shutdown_flusher
-    shutdown_workers
+    begin
+      monitor_inputs_and_workers
+    ensure
+      transition_to_stopped
 
-    close
+      shutdown_flusher
+      shutdown_workers
 
+      close
+    end
     @logger.debug("Pipeline has been shutdown", default_logging_keys)
-
-    # exit code
-    return 0
-  end # def run
+  end
 
   def transition_to_running
     @running.make_true
@@ -279,7 +289,16 @@ def start_workers
         thread = Thread.new do
           Util.set_thread_name("[#{pipeline_id}]>worker#{t}")
           ThreadContext.put("pipeline.id", pipeline_id)
-          worker_loop.run
+          begin
+            worker_loop.run
+          rescue => e
+            # WorkerLoop.run() catches all Java Exception class and re-throws as IllegalStateException with the
+            # original exception as the cause
+            @logger.error(
+              "Pipeline worker error, the pipeline will be stopped",
+              default_logging_keys(:error => e.cause.message, :exception => e.cause.class, :backtrace => e.cause.backtrace)
+            )
+          end
         end
         @worker_threads << thread
       end
@@ -309,10 +328,20 @@ def resolve_cluster_uuids
     end.to_a.compact
   end
 
-  def wait_inputs
-    @input_threads.each do |thread|
-      thread.join # Thread or java.lang.Thread (both have #join)
+  def monitor_inputs_and_workers
+    twait = ThreadsWait.new(*(@input_threads + @worker_threads))
+
+    while !@input_threads.empty?
+      terminated_thread = twait.next_wait
+      if @input_threads.delete(terminated_thread).nil?
+        # this is a worker thread termination
+        # delete it from @worker_threads so that wait_for_workers does not wait for it
+        @worker_threads.delete(terminated_thread)
+        raise("Worker thread terminated in pipeline.id: #{pipeline_id}")
+      end
     end
+
+    @logger.debug("Input plugins stopped! Will shutdown filter/output workers.", default_logging_keys)
   end
 
   def start_inputs
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 30d1663d1a2..ccec00bab8b 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -402,8 +402,10 @@ def filter_batch(events)
     #
     # Users need to check their configuration or see if there is a bug in the
     # plugin.
-    @logger.error("Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
-                  default_logging_keys("exception" => e.message, "backtrace" => e.backtrace))
+    @logger.error(
+      "Pipeline worker error, the pipeline will be stopped",
+      default_logging_keys("exception" => e.message, "backtrace" => e.backtrace)
+    )
 
     raise e
   end
diff --git a/logstash-core/lib/logstash/pipelines_registry.rb b/logstash-core/lib/logstash/pipelines_registry.rb
index cb9e3752779..6143f3ee356 100644
--- a/logstash-core/lib/logstash/pipelines_registry.rb
+++ b/logstash-core/lib/logstash/pipelines_registry.rb
@@ -17,40 +17,100 @@
 
 module LogStash
   class PipelineState
-    attr_reader :pipeline_id, :pipeline
+    attr_reader :pipeline_id
 
     def initialize(pipeline_id, pipeline)
       @pipeline_id = pipeline_id
       @pipeline = pipeline
-      @reloading = Concurrent::AtomicBoolean.new(false)
+      @loading = Concurrent::AtomicBoolean.new(false)
+
+      # this class uses a lock to ensure thread safe visibility.
+      @lock = Mutex.new
     end
 
     def terminated?
-      # a reloading pipeline is never considered terminated
-      @reloading.false? && @pipeline.finished_execution?
+      @lock.synchronize do
+        # a loading pipeline is never considered terminated
+        @loading.false? && @pipeline.finished_execution?
+      end
     end
 
-    def set_reloading(is_reloading)
-      @reloading.value = is_reloading
+    def set_loading(is_loading)
+      @lock.synchronize do
+        @loading.value = is_loading
+      end
     end
 
     def set_pipeline(pipeline)
-      raise(ArgumentError, "invalid nil pipeline") if pipeline.nil?
-      @pipeline = pipeline
+      @lock.synchronize do
+        raise(ArgumentError, "invalid nil pipeline") if pipeline.nil?
+        @pipeline = pipeline
+      end
+    end
+
+    def pipeline
+      @lock.synchronize { @pipeline }
     end
   end
 
+  class PipelineStates
+
+    def initialize
+      @states = {}
+      @locks = {}
+      @lock = Mutex.new
+    end
+
+    def get(pipeline_id)
+      @lock.synchronize do
+        @states[pipeline_id]
+      end
+    end
+
+    def put(pipeline_id, state)
+      @lock.synchronize do
+        @states[pipeline_id] = state
+      end
+    end
+
+    def remove(pipeline_id)
+      @lock.synchronize do
+        @states.delete(pipeline_id)
+        @locks.delete(pipeline_id)
+      end
+    end
+
+    def size
+      @lock.synchronize do
+        @states.size
+      end
+    end
+
+    def empty?
+      @lock.synchronize do
+        @states.empty?
+      end
+    end
+
+    def each_with_object(init, &block)
+      states = @lock.synchronize { @states.dup }
+      states.each_with_object(init, &block)
+    end
+
+    def get_lock(pipeline_id)
+      @lock.synchronize do
+        @locks[pipeline_id] ||= Mutex.new
+      end
+    end
+  end
+
+
   class PipelinesRegistry
     attr_reader :states
     include LogStash::Util::Loggable
 
     def initialize
-      # we leverage the semantic of the Java ConcurrentHashMap for the
-      # compute() method which is atomic; calling compute() concurrently
-      # will block until the other compute finishes so no mutex is necessary
-      # for synchronizing compute calls
-      @states = java.util.concurrent.ConcurrentHashMap.new
-      @locks = java.util.concurrent.ConcurrentHashMap.new
+      @states = PipelineStates.new
     end
 
     # Execute the passed creation logic block and create a new state upon success
@@ -62,23 +122,35 @@ def initialize
     #
     # @return [Boolean] new pipeline creation success
     def create_pipeline(pipeline_id, pipeline, &create_block)
-      lock = get_lock(pipeline_id)
+      lock = @states.get_lock(pipeline_id)
       lock.lock
-
       success = false
 
       state = @states.get(pipeline_id)
-      if state
-        if state.terminated?
+
+      if state && !state.terminated?
+        logger.error("Attempted to create a pipeline that already exists", :pipeline_id => pipeline_id)
+        return false
+      end
+
+      if state.nil?
+        state = PipelineState.new(pipeline_id, pipeline)
+        state.set_loading(true)
+        @states.put(pipeline_id, state)
+        begin
           success = yield
-          state.set_pipeline(pipeline)
-        else
-          logger.error("Attempted to create a pipeline that already exists", :pipeline_id => pipeline_id)
+        ensure
+          state.set_loading(false)
+          @states.remove(pipeline_id) unless success
         end
-        @states.put(pipeline_id, state)
       else
-        success = yield
-        @states.put(pipeline_id, PipelineState.new(pipeline_id, pipeline)) if success
+        state.set_loading(true)
+        state.set_pipeline(pipeline)
+        begin
+          success = yield
+        ensure
+          state.set_loading(false)
+        end
       end
 
       success
@@ -92,22 +164,20 @@ def create_pipeline(pipeline_id, pipeline, &create_block)
     #
     # @yieldparam [Pipeline] the pipeline to terminate
     def terminate_pipeline(pipeline_id, &stop_block)
-      lock = get_lock(pipeline_id)
+      lock = @states.get_lock(pipeline_id)
       lock.lock
 
       state = @states.get(pipeline_id)
       if state.nil?
         logger.error("Attempted to terminate a pipeline that does not exists", :pipeline_id => pipeline_id)
-        @states.remove(pipeline_id)
       else
         yield(state.pipeline)
-        @states.put(pipeline_id, state)
       end
     ensure
       lock.unlock
     end
 
-    # Execute the passed reloading logic block in the context of the reloading state and set new pipeline in state
+    # Execute the passed reloading logic block in the context of the loading state and set new pipeline in state
     # @param pipeline_id [String, Symbol] the pipeline id
     # @param reload_block [Block] the reloading execution logic
     #
@@ -115,26 +185,26 @@ def terminate_pipeline(pipeline_id, &stop_block)
     #
     # @return [Boolean] new pipeline creation success
     def reload_pipeline(pipeline_id, &reload_block)
-      lock = get_lock(pipeline_id)
+      lock = @states.get_lock(pipeline_id)
       lock.lock
       success = false
 
       state = @states.get(pipeline_id)
+
       if state.nil?
         logger.error("Attempted to reload a pipeline that does not exists", :pipeline_id => pipeline_id)
-        @states.remove(pipeline_id)
-      else
-        state.set_reloading(true)
-        begin
-          success, new_pipeline = yield
-          state.set_pipeline(new_pipeline)
-        ensure
-          state.set_reloading(false)
-        end
-        @states.put(pipeline_id, state)
+        return false
       end
 
-    success
+      state.set_loading(true)
+      begin
+        success, new_pipeline = yield
+        state.set_pipeline(new_pipeline)
+      ensure
+        state.set_loading(false)
+      end
+
+      success
     ensure
       lock.unlock
     end
@@ -153,7 +223,7 @@ def size
 
     # @return [Boolean] true if the states collection is empty.
     def empty?
-      @states.isEmpty
+      @states.empty?
     end
 
     # @return [Hash{String=>Pipeline}]
@@ -189,11 +259,5 @@ def select_pipelines(&optional_state_filter)
         end
       end
     end
-
-    def get_lock(pipeline_id)
-      @locks.compute_if_absent(pipeline_id) do |k|
-        java.util.concurrent.locks.ReentrantLock.new
-      end
-    end
   end
 end
diff --git a/logstash-core/spec/logstash/java_pipeline_spec.rb b/logstash-core/spec/logstash/java_pipeline_spec.rb
index 68ad5c91abc..69f5ceab9a2 100644
--- a/logstash-core/spec/logstash/java_pipeline_spec.rb
+++ b/logstash-core/spec/logstash/java_pipeline_spec.rb
@@ -88,6 +88,17 @@ def threadsafe?() false; end
   def close() end
 end
 
+class DummyCrashingFilter < LogStash::Filters::Base
+  config_name "dummycrashingfilter"
+  milestone 2
+
+  def register; end
+
+  def filter(event)
+    raise("crashing filter")
+  end
+end
+
 class DummySafeFilter < LogStash::Filters::Base
   config_name "dummysafefilter"
   milestone 2
@@ -226,6 +237,37 @@ def flush(options)
     end
   end
 
+  context "a crashing worker" do
+    subject { mock_java_pipeline_from_string(config, pipeline_settings_obj) }
+
+    let(:pipeline_settings) { { "pipeline.batch.size" => 1, "pipeline.workers" => 1 } }
+    let(:config) do
+      <<-EOS
+      input { generator {} }
+      filter { dummycrashingfilter {} }
+      output { dummyoutput {} }
+      EOS
+    end
+    let(:dummyoutput) { ::LogStash::Outputs::DummyOutput.new }
+
+    before :each do
+      allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummycrashingfilter").and_return(DummyCrashingFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
+    end
+
+    after :each do
+      subject.shutdown
+    end
+
+    it "does not raise in the main thread, terminates the run thread and finishes execution" do
+      expect { subject.start && subject.thread.join }.to_not raise_error
+      expect(subject.finished_execution?).to be_truthy
+    end
+  end
+
   describe "defaulting the pipeline workers based on thread safety" do
     before(:each) do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
diff --git a/logstash-core/spec/logstash/pipelines_registry_spec.rb b/logstash-core/spec/logstash/pipelines_registry_spec.rb
index 5a36b633225..3af1ff78a4f 100644
--- a/logstash-core/spec/logstash/pipelines_registry_spec.rb
+++ b/logstash-core/spec/logstash/pipelines_registry_spec.rb
@@ -101,6 +101,47 @@
         end
       end
     end
+
+    context "when pipeline is initializing" do
+      let (:wait_start_create_block) { Queue.new }
+      let (:wait_before_exiting_create_block) { Queue.new }
+      let (:slow_initializing_pipeline) { double("slow_initializing_pipeline") }
+      let (:pipeline2) { double("pipeline2") }
+
+      it "should create a loading state before calling the create block" do
+
+        # create a thread which calls create_pipeline and wait in the create
+        # block so we can controle the pipeline initialization phase
+        t = Thread.new do
+          subject.create_pipeline(pipeline_id, slow_initializing_pipeline) do
+            # signal that we entered the create block
+            wait_start_create_block << "ping"
+
+            # stall here until wait_before_exiting_create_block receives a message
+            wait_before_exiting_create_block.pop
+
+            true
+          end
+        end
+
+        # stall here until subject.create_pipeline has been called in the above thread
+        # and it entered the create block
+        wait_start_create_block.pop
+
+        # finished_execution? should not be called in the below tests using terminated?
+        # because the loading state is true. This is to make sure the state is used and not
+        # the pipeline termination status
+        expect(slow_initializing_pipeline).not_to receive(:finished_execution?)
+
+        expect(subject.states.get(pipeline_id).terminated?).to be_falsey
+        expect(subject.get_pipeline(pipeline_id)).to eq(slow_initializing_pipeline)
+        expect(subject.empty?).to be_falsey
+
+        # signal termination of create block
+        wait_before_exiting_create_block << "ping"
+        t.join
+      end
+    end
   end
 
   context "terminating a pipeline" do
diff --git a/logstash-core/spec/support/matchers.rb b/logstash-core/spec/support/matchers.rb
index 71a37cd0060..0a6c08f383b 100644
--- a/logstash-core/spec/support/matchers.rb
+++ b/logstash-core/spec/support/matchers.rb
@@ -92,9 +92,9 @@ def all_instance_methods_implemented?
     try(30) do
       pipeline = agent.get_pipeline(pipeline_config.pipeline_id)
       expect(pipeline).to_not be_nil
+      expect(pipeline.running?).to be_truthy
     end
     expect(pipeline.config_str).to eq(pipeline_config.config_string)
-    expect(pipeline.running?).to be_truthy
     expect(agent.running_pipelines.keys.map(&:to_s)).to include(pipeline_config.pipeline_id.to_s)
   end
 
diff --git a/logstash-core/src/main/java/org/logstash/execution/WorkerLoop.java b/logstash-core/src/main/java/org/logstash/execution/WorkerLoop.java
index 843f67fe7a0..b0e974d3d54 100644
--- a/logstash-core/src/main/java/org/logstash/execution/WorkerLoop.java
+++ b/logstash-core/src/main/java/org/logstash/execution/WorkerLoop.java
@@ -98,10 +98,6 @@ public void run() {
             execution.compute(batch, true, true);
             readClient.closeBatch(batch);
         } catch (final Exception ex) {
-            LOGGER.error(
-                "Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
-                ex
-            );
             throw new IllegalStateException(ex);
         }
     }
diff --git a/qa/integration/services/kafka_setup.sh b/qa/integration/services/kafka_setup.sh
index e874883af03..08d36e357d8 100755
--- a/qa/integration/services/kafka_setup.sh
+++ b/qa/integration/services/kafka_setup.sh
@@ -17,6 +17,7 @@ KAFKA_HOME=$INSTALL_DIR/kafka
 KAFKA_TOPIC=logstash_topic_plain
 KAFKA_MESSAGES=37
 KAFKA_LOGS_DIR=/tmp/ls_integration/kafka-logs
+ZOOKEEPER_DATA_DIR=/tmp/ls_integration/zookeeper
 
 setup_kafka() {
     local version=$1
@@ -25,17 +26,19 @@ setup_kafka() {
         curl -s -o $INSTALL_DIR/kafka.tgz "https://mirrors.ocf.berkeley.edu/apache/kafka/$version/kafka_2.11-$version.tgz"
         mkdir $KAFKA_HOME && tar xzf $INSTALL_DIR/kafka.tgz -C $KAFKA_HOME --strip-components 1
         rm $INSTALL_DIR/kafka.tgz
+        echo "dataDir=$ZOOKEEPER_DATA_DIR" >> $KAFKA_HOME/config/zookeeper.properties
     fi
 }
 
 start_kafka() {
     echo "Starting ZooKeeper"
-    $KAFKA_HOME/bin/zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
-    wait_for_port 2181
-    echo "Starting Kafka broker"
     rm -rf ${KAFKA_LOGS_DIR}
     mkdir -p ${KAFKA_LOGS_DIR}
-    $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties --override delete.topic.enable=true --override advertised.host.name=127.0.0.1 --override log.dir=${KAFKA_LOGS_DIR} --override log.flush.interval.ms=200
+    rm -rf ${ZOOKEEPER_DATA_DIR}
+    mkdir -p ${ZOOKEEPER_DATA_DIR} 
+    $KAFKA_HOME/bin/zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
+    wait_for_port 2181
+    $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties --override delete.topic.enable=true --override advertised.host.name=127.0.0.1 --override log.dir=${KAFKA_LOGS_DIR} --override log.dirs=${KAFKA_LOGS_DIR} --override log.flush.interval.ms=200 
     wait_for_port 9092
 }
 
diff --git a/qa/integration/specs/kafka_input_spec.rb b/qa/integration/specs/kafka_input_spec.rb
index 6e9ac5a0f95..315b45efabc 100644
--- a/qa/integration/specs/kafka_input_spec.rb
+++ b/qa/integration/specs/kafka_input_spec.rb
@@ -30,21 +30,22 @@
   }
 
   after(:all) {
-    @fixture.teardown
+    @fixture.teardown unless @fixture.nil?
   }
 
   it "can ingest 37 apache log lines from Kafka broker" do
-    logstash_service = @fixture.get_service("logstash")
-    logstash_service.start_background(@fixture.config)
-
-    try(num_retries) do
-      expect(@fixture.output_exists?).to be true
-    end
-
-    try(num_retries) do
-      count = File.foreach(@fixture.actual_output).inject(0) {|c, _| c+1}
-      expect(count).to eq(num_events)
+    unless @fixture.nil?
+      logstash_service = @fixture.get_service("logstash")
+      logstash_service.start_background(@fixture.config)
+
+      try(num_retries) do
+        expect(@fixture.output_exists?).to be true
+      end
+
+      try(num_retries) do
+        count = File.foreach(@fixture.actual_output).inject(0) {|c, _| c+1}
+        expect(count).to eq(num_events)
+      end
     end
   end
-
 end
diff --git a/tools/dependencies-report/src/main/resources/licenseMapping.csv b/tools/dependencies-report/src/main/resources/licenseMapping.csv
index ead734cb6a8..b1692eadf67 100644
--- a/tools/dependencies-report/src/main/resources/licenseMapping.csv
+++ b/tools/dependencies-report/src/main/resources/licenseMapping.csv
@@ -3,6 +3,7 @@ dependency,dependencyUrl,licenseOverride
 "atomic:",http://github.com/ruby-concurrency/atomic,Apache-2.0
 "avl_tree:",https://github.com/nahi/avl_tree,BSD-2-Clause-FreeBSD
 "avro:",https://github.com/apache/avro/tree/master/lang/ruby,Apache-2.0
+"amazing_print:",https://github.com/amazing-print/amazing_print,MIT
 "awesome_print:",https://github.com/awesome-print/awesome_print,MIT
 "aws-eventstream:",https://github.com/aws/aws-sdk-ruby/tree/master/gems/aws-eventstream,Apache-2.0
 "aws-sdk-core:",http://github.com/aws/aws-sdk-ruby,Apache-2.0
diff --git a/tools/dependencies-report/src/main/resources/notices/amazing_print-NOTICE.txt b/tools/dependencies-report/src/main/resources/notices/amazing_print-NOTICE.txt
new file mode 100644
index 00000000000..d5ab3a384f0
--- /dev/null
+++ b/tools/dependencies-report/src/main/resources/notices/amazing_print-NOTICE.txt
@@ -0,0 +1,22 @@
+MIT License
+
+Copyright (c) 2010-2019 Michael Dvorkin
+Copyright (c) 2020 AmazingPrint
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+SOFTWARE.
diff --git a/x-pack/build.gradle b/x-pack/build.gradle
index 5f78f038833..5c978f55642 100644
--- a/x-pack/build.gradle
+++ b/x-pack/build.gradle
@@ -35,6 +35,7 @@ tasks.register("rubyTests", Test) {
 }
 
 tasks.register("rubyIntegrationTests", Test) {
+  dependsOn (":copyEs")
   inputs.files fileTree("${projectDir}/qa")
   inputs.files fileTree("${projectDir}/lib")
   inputs.files fileTree("${projectDir}/modules")
diff --git a/x-pack/lib/helpers/elasticsearch_options.rb b/x-pack/lib/helpers/elasticsearch_options.rb
index 26bd6d82a7f..397632a069f 100644
--- a/x-pack/lib/helpers/elasticsearch_options.rb
+++ b/x-pack/lib/helpers/elasticsearch_options.rb
@@ -67,9 +67,12 @@ def es_options_from_settings(feature, settings)
         opts['ssl'] = true
       end
 
-      # the username setting has a default value and should not be included when using another authentication
+      # the username setting has a default value and should not be included when using another authentication such as cloud_auth or api_key.
+      # it should also not be included when no password is set.
       # it is safe to silently remove here since all authentication verifications have been validated at this point.
-      if settings.set?("#{prefix}#{feature}.elasticsearch.cloud_auth") || settings.set?("#{prefix}#{feature}.elasticsearch.api_key")
+      if settings.set?("#{prefix}#{feature}.elasticsearch.cloud_auth") ||
+         settings.set?("#{prefix}#{feature}.elasticsearch.api_key") ||
+         (!settings.set?("#{prefix}#{feature}.elasticsearch.password") && !settings.set?("#{prefix}#{feature}.elasticsearch.username"))
         opts.delete('user')
       end
 
@@ -201,15 +204,6 @@ def validate_authentication!(feature, settings, prefix)
       authentication_count += 1 if provided_password
       authentication_count += 1 if provided_api_key
 
-      if authentication_count == 0
-        # when no explicit authentication is set it is relying on default username
-        # but without and explicit password set
-        raise(ArgumentError,
-          "With the default #{prefix}#{feature}.elasticsearch.username, " +
-          "#{prefix}#{feature}.elasticsearch.password must be set"
-        )
-      end
-
       if authentication_count > 1
         raise(ArgumentError, "Multiple authentication options are specified, please only use one of #{prefix}#{feature}.elasticsearch.username/password, #{prefix}#{feature}.elasticsearch.cloud_auth or #{prefix}#{feature}.elasticsearch.api_key")
       end
diff --git a/x-pack/spec/config_management/elasticsearch_source_spec.rb b/x-pack/spec/config_management/elasticsearch_source_spec.rb
index 7eb385dd162..927d853b241 100644
--- a/x-pack/spec/config_management/elasticsearch_source_spec.rb
+++ b/x-pack/spec/config_management/elasticsearch_source_spec.rb
@@ -151,8 +151,12 @@
           }
         end
 
-        it "should raise an ArgumentError" do
-          expect { described_class.new(system_settings) }.to raise_error(ArgumentError)
+        it "will rely on username and password settings" do
+          # since cloud_id and cloud_auth are simply containers for host and username/password
+          # both could be set independently and if cloud_auth is not set then authn will be done
+          # using the provided username/password settings, which can be set or not if not auth is
+          # required.
+          expect { described_class.new(system_settings) }.to_not raise_error
         end
       end
     end
diff --git a/x-pack/spec/helpers/elasticsearch_options_spec.rb b/x-pack/spec/helpers/elasticsearch_options_spec.rb
index 3fb5b492299..edc0d7eeab5 100644
--- a/x-pack/spec/helpers/elasticsearch_options_spec.rb
+++ b/x-pack/spec/helpers/elasticsearch_options_spec.rb
@@ -107,10 +107,11 @@
         }
       end
 
-      it "fails without password" do
-        expect {
-          test_class.es_options_from_settings_or_modules('monitoring', system_settings)
-        }.to raise_error(ArgumentError, /password must be set/)
+      it "ignores the implicit default username when no password is set" do
+        # when no explicit password is set then the default/implicit username should be ignored
+        es_options = test_class.es_options_from_settings_or_modules('monitoring', system_settings)
+        expect(es_options).to_not include("user")
+        expect(es_options).to_not include("password")
       end
 
       context "with cloud_auth" do
@@ -296,13 +297,10 @@
       end
 
       context "when cloud_auth is not set" do
-
-        it "raises for invalid configuration" do
-          # if not other authn is provided it will assume basic auth using the default username
-          # but the password is missing.
-          expect {
-            test_class.es_options_from_settings_or_modules('monitoring', system_settings)
-          }.to raise_error(ArgumentError, /With the default.*?username,.*?password must be set/)
+        it "does not use authentication and ignores the default username" do
+          es_options = test_class.es_options_from_settings_or_modules('monitoring', system_settings)
+          expect(es_options).to include("cloud_id")
+          expect(es_options.keys).to_not include("hosts", "user", "password")
         end
 
         context 'username and password set' do
