diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index a76d643f8d5..a27ba439b19 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -19,7 +19,6 @@
 require "logstash/config/cpu_core_strategy"
 require "logstash/instrument/collector"
 require "logstash/instrument/periodic_pollers"
-require "logstash/pipeline"
 require "logstash/webserver"
 require "logstash/config/source_loader"
 require "logstash/config/pipeline_config"
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 6b3ec1733eb..903a7c0c662 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -15,681 +15,24 @@
 # specific language governing permissions and limitations
 # under the License.
 
-require "thread"
-require "stud/interval"
-require "concurrent"
-require "logstash-core/logstash-core"
-require "logstash/event"
-require "logstash/config/file"
-require "logstash/filters/base"
-require "logstash/inputs/base"
-require "logstash/outputs/base"
-require "logstash/instrument/collector"
-require "logstash/filter_delegator"
-require "logstash/compiler"
+# for backward compatibility
+# logstash-devutils-1.3.6 logstash_helpers has dependency on this class
+module LogStash
+  class Pipeline
 
-module LogStash; class BasePipeline < AbstractPipeline
-  include LogStash::Util::Loggable
-
-  java_import org.apache.logging.log4j.ThreadContext
-
-  attr_reader :inputs, :filters, :outputs
-
-  def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
-    @logger = self.logger
-    super pipeline_config, namespaced_metric, @logger
-
-    @inputs = nil
-    @filters = nil
-    @outputs = nil
-    @agent = agent
-
-    @plugin_factory = LogStash::Plugins::PluginFactory.new(
-      # use NullMetric if called in the BasePipeline context otherwise use the @metric value
-      lir, LogStash::Plugins::PluginMetricsFactory.new(pipeline_id, metric),
-      LogStash::Plugins::ExecutionContextFactory.new(@agent, self, dlq_writer),
-      FilterDelegator
-    )
-    grammar = LogStashConfigParser.new
-    parsed_config = grammar.parse(config_str)
-    raise(ConfigurationError, grammar.failure_reason) if parsed_config.nil?
-
-    parsed_config.process_escape_sequences = settings.get_value("config.support_escapes")
-    config_code = parsed_config.compile
-
-    if settings.get_value("config.debug")
-      @logger.debug("Compiled pipeline code", default_logging_keys(:code => config_code))
-    end
-
-    # Evaluate the config compiled code that will initialize all the plugins and define the
-    # filter and output methods.
-    begin
-      eval(config_code)
-    rescue => e
-      raise e
-    end
-  end
-
-  def line_to_source(line, column)
-    pipeline_config.lookup_source(line, column)
-  end
-
-  def reloadable?
-    configured_as_reloadable? && reloadable_plugins?
-  end
-
-  def reloadable_plugins?
-    non_reloadable_plugins.empty?
-  end
-
-  def non_reloadable_plugins
-    (inputs + filters + outputs).select { |plugin| !plugin.reloadable? }
-  end
-
-  private
-
-
-  def plugin(plugin_type, name, args, source)
-    @plugin_factory.plugin(plugin_type, name, args, source)
-  end
-
-  def default_logging_keys(other_keys = {})
-    { :pipeline_id => pipeline_id }.merge(other_keys)
-  end
-end; end
-
-module LogStash; class Pipeline < BasePipeline
-  attr_reader \
-    :worker_threads,
-    :events_consumed,
-    :events_filtered,
-    :started_at,
-    :thread
-
-  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
-
-  def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
-    super
-    open_queue
-
-    @worker_threads = []
-
-    @signal_queue = java.util.concurrent.LinkedBlockingQueue.new
-
-    @drain_queue =  settings.get_value("queue.drain") || settings.get("queue.type") == "memory"
-
-
-    @events_filtered = java.util.concurrent.atomic.LongAdder.new
-    @events_consumed = java.util.concurrent.atomic.LongAdder.new
-
-    @input_threads = []
-    # @ready requires thread safety since it is typically polled from outside the pipeline thread
-    @ready = Concurrent::AtomicBoolean.new(false)
-    @running = Concurrent::AtomicBoolean.new(false)
-    @flushing = Concurrent::AtomicReference.new(false)
-    @outputs_registered = Concurrent::AtomicBoolean.new(false)
-    @worker_shutdown = java.util.concurrent.atomic.AtomicBoolean.new(false)
-
-    # @finished_execution signals that the pipeline thread has finished its execution
-    # regardless of any exceptions; it will always be true when the thread completes
-    @finished_execution = Concurrent::AtomicBoolean.new(false)
-
-    # @finished_run signals that the run methods called in the pipeline thread was completed
-    # without errors and it will NOT be set if the run method exits from an exception; this
-    # is by design and necessary for the wait_until_started semantic
-    @finished_run = Concurrent::AtomicBoolean.new(false)
-
-    @thread = nil
-  end # def initialize
-
-  def finished_execution?
-    @finished_execution.true?
-  end
-
-  def ready?
-    @ready.value
-  end
-
-  def safe_pipeline_worker_count
-    default = settings.get_default("pipeline.workers")
-    pipeline_workers = settings.get("pipeline.workers") #override from args "-w 8" or config
-    safe_filters, unsafe_filters = @filters.partition(&:threadsafe?)
-    plugins = unsafe_filters.collect { |f| f.config_name }
-
-    return pipeline_workers if unsafe_filters.empty?
-
-    if settings.set?("pipeline.workers")
-      if pipeline_workers > 1
-        @logger.warn("Warning: Manual override - there are filters that might not work with multiple worker threads", default_logging_keys(:worker_threads => pipeline_workers, :filters => plugins))
-      end
-    else
-      # user did not specify a worker thread count
-      # warn if the default is multiple
-      if default > 1
-        @logger.warn("Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads",
-                     default_logging_keys(:count_was => default, :filters => plugins))
-        return 1 # can't allow the default value to propagate if there are unsafe filters
-      end
-    end
-    pipeline_workers
-  end
-
-  def filters?
-    return @filters.any?
-  end
-
-  def start
-    # Since we start lets assume that the metric namespace is cleared
-    # this is useful in the context of pipeline reloading
-    collect_stats
-    collect_dlq_stats
-
-    pipeline_log_params = default_logging_keys(
-        "pipeline.workers" => settings.get("pipeline.workers"),
-        "pipeline.batch.size" => settings.get("pipeline.batch.size"),
-        "pipeline.batch.delay" => settings.get("pipeline.batch.delay"),
-        "pipeline.sources" => pipeline_source_details)
-    @logger.info("Starting pipeline", pipeline_log_params)
-
-    @finished_execution.make_false
-    @finished_run.make_false
-
-    @thread = Thread.new do
-      begin
-        LogStash::Util.set_thread_name("[#{pipeline_id}]-manager")
-        ThreadContext.put("pipeline.id", pipeline_id)
-        run
-        @finished_run.make_true
-      rescue => e
-        close
-        pipeline_log_params = default_logging_keys(
-          :exception => e,
-          :backtrace => e.backtrace,
-          "pipeline.sources" => pipeline_source_details)
-        @logger.error("Pipeline aborted due to error", pipeline_log_params)
-      ensure
-        @finished_execution.make_true
-      end
-    end
-
-    status = wait_until_started
-
-    if status
-      @logger.info("Pipeline started successfully", default_logging_keys)
-    end
-
-    status
-  end
-
-  def wait_until_started
-    while true do
-      if @finished_run.true?
-        # it completed run without exception
-        return true
-      elsif thread.nil? || !thread.alive?
-        # some exception occured and the thread is dead
-        return false
-      elsif running?
-        # fully initialized and running
-        return true
-      else
-        sleep 0.01
-      end
+    # for backward compatibility in devutils for the logstash helpers, this method is not used
+    # in the pipeline anymore.
+    def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
     end
-  end
-
-  def run
-    @started_at = Time.now
-    @thread = Thread.current
-    Util.set_thread_name("[#{pipeline_id}]-pipeline-manager")
-
-    start_workers
-
-    # Block until all inputs have stopped
-    # Generally this happens if SIGINT is sent and `shutdown` is called from an external thread
-
-    transition_to_running
-    start_flusher # Launches a non-blocking thread for flush events
-    wait_inputs
-    transition_to_stopped
-
-    shutdown_flusher
-    @logger.debug("Shutting down filter/output workers", default_logging_keys)
-    shutdown_workers
-
-    close
-
-    @logger.info("Pipeline has terminated", default_logging_keys)
-
-    # exit code
-    return 0
-  end # def run
-
-  def transition_to_running
-    @running.make_true
-  end
-
-  def transition_to_stopped
-    @running.make_false
-  end
-
-  def running?
-    @running.true?
-  end
-
-  def stopped?
-    @running.false?
-  end
-
-  # register_plugin simply calls the plugin #register method and catches & logs any error
-  # @param plugin [Plugin] the plugin to register
-  # @return [Plugin] the registered plugin
-  def register_plugin(plugin)
-    plugin.register
-    plugin
-  rescue => e
-    @logger.error("Error registering plugin", default_logging_keys(:plugin => plugin.inspect, :error => e.message))
-    raise e
-  end
-
-  # register_plugins calls #register_plugin on the plugins list and upon exception will call Plugin#do_close on all registered plugins
-  # @param plugins [Array[Plugin]] the list of plugins to register
-  def register_plugins(plugins)
-    registered = []
-    plugins.each { |plugin| registered << register_plugin(plugin) }
-  rescue => e
-    registered.each(&:do_close)
-    raise e
-  end
-
-  def start_workers
-    @worker_threads.clear # In case we're restarting the pipeline
-    @outputs_registered.make_false
-    begin
-      maybe_setup_out_plugins
-
-      pipeline_workers = safe_pipeline_worker_count
-      verify_event_ordering!(pipeline_workers)
-      batch_size = settings.get("pipeline.batch.size")
-      batch_delay = settings.get("pipeline.batch.delay")
-
-      max_inflight = batch_size * pipeline_workers
-
-      config_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :config])
-      config_metric.gauge(:workers, pipeline_workers)
-      config_metric.gauge(:batch_size, batch_size)
-      config_metric.gauge(:batch_delay, batch_delay)
-      config_metric.gauge(:config_reload_automatic, settings.get("config.reload.automatic"))
-      config_metric.gauge(:config_reload_interval, settings.get("config.reload.interval"))
-      config_metric.gauge(:dead_letter_queue_enabled, dlq_enabled?)
-      config_metric.gauge(:dead_letter_queue_path, dlq_writer.get_path.to_absolute_path.to_s) if dlq_enabled?
-
-      if max_inflight > MAX_INFLIGHT_WARN_THRESHOLD
-        @logger.warn("CAUTION: Recommended inflight events max exceeded! Logstash will run with up to #{max_inflight} events in memory in your current configuration. If your message sizes are large this may cause instability with the default heap size. Please consider setting a non-standard heap size, changing the batch size (currently #{batch_size}), or changing the number of pipeline workers (currently #{pipeline_workers})", default_logging_keys)
-      end
-
-      pipeline_workers.times do |t|
-        thread = Thread.new(batch_size, batch_delay, self) do |_b_size, _b_delay, _pipeline|
-          LogStash::Util::set_thread_name("[#{pipeline_id}]>worker#{t}")
-          ThreadContext.put("pipeline.id", pipeline_id)
-          _pipeline.worker_loop(_b_size, _b_delay)
-        end
-        @worker_threads << thread
-      end
-
-      # inputs should be started last, after all workers
-      begin
-        start_inputs
-      rescue => e
-        # if there is any exception in starting inputs, make sure we shutdown workers.
-        # exception will already by logged in start_inputs
-        shutdown_workers
-        raise e
-      end
-    ensure
-      # it is important to guarantee @ready to be true after the startup sequence has been completed
-      # to potentially unblock the shutdown method which may be waiting on @ready to proceed
-      @ready.make_true
-    end
-  end
-
-  # Main body of what a worker thread does
-  # Repeatedly takes batches off the queue, filters, then outputs them
-  def worker_loop(batch_size, batch_delay)
-    filter_queue_client.set_batch_dimensions(batch_size, batch_delay)
-    output_events_map = Hash.new { |h, k| h[k] = [] }
-    while true
-      signal = @signal_queue.poll || NO_SIGNAL
-
-      batch = filter_queue_client.read_batch.to_java # metrics are started in read_batch
-      batch_size = batch.filteredSize
-      events = batch.to_a
-      if batch_size > 0
-        @events_consumed.add(batch_size)
-        events = filter_batch(events)
-      end
-
-      if signal.flush?
-        events = flush_filters_to_batch(events, :final => false)
-      end
-      if events.size > 0
-        output_batch(events, output_events_map)
-        filter_queue_client.close_batch(batch)
-      end
-      # keep break at end of loop, after the read_batch operation, some pipeline specs rely on this "final read_batch" before shutdown.
-      break if (@worker_shutdown.get && !draining_queue?)
-    end
-
-    # we are shutting down, queue is drained if it was required, now  perform a final flush.
-    # for this we need to create a new empty batch to contain the final flushed events
-    batch = filter_queue_client.to_java.newBatch
-    filter_queue_client.start_metrics(batch) # explicitly call start_metrics since we dont do a read_batch here
-    events = batch.to_a
-    events = flush_filters_to_batch(events, :final => true)
-    output_batch(events, output_events_map)
-    filter_queue_client.close_batch(batch)
-  end
-
-  def filter_batch(events)
-    result = filter_func(events)
-    filter_queue_client.add_filtered_metrics(result.size)
-    @events_filtered.add(result.size)
-    result
-  rescue Exception => e
-    # Plugins authors should manage their own exceptions in the plugin code
-    # but if an exception is raised up to the worker thread they are considered
-    # fatal and logstash will not recover from this situation.
     #
-    # Users need to check their configuration or see if there is a bug in the
-    # plugin.
-    @logger.error(
-      "Pipeline worker error, the pipeline will be stopped",
-      default_logging_keys("exception" => e.message, "backtrace" => e.backtrace)
-    )
-
-    raise e
-  end
-
-  # Take an array of events and send them to the correct output
-  def output_batch(events, output_events_map)
-    # Build a mapping of { output_plugin => [events...]}
-    events.each do |event|
-      unless event.cancelled?
-        # We ask the AST to tell us which outputs to send each event to
-        # Then, we stick it in the correct bin
-        output_func(event).each do |output|
-          output_events_map[output].push(event)
-        end
-      end
-    end
-    # Now that we have our output to event mapping we can just invoke each output
-    # once with its list of events
-    output_events_map.each do |output, events|
-      output.multi_receive(events)
-      events.clear
+    # for backward compatibility in devutils for the rspec helpers, this method is not used
+    # in the pipeline anymore.
+    def filter(event, &block)
     end
 
-    filter_queue_client.add_output_metrics(events.size)
-  end
-
-  def resolve_cluster_uuids
-    outputs.each_with_object(Set.new) do |output, cluster_uuids|
-      if LogStash::PluginMetadata.exists?(output.id)
-        cluster_uuids << LogStash::PluginMetadata.for_plugin(output.id).get(:cluster_uuid)
-      end
-    end.to_a.compact
-  end
-
-  def wait_inputs
-    @input_threads.each(&:join)
-  end
-
-  def start_inputs
-    moreinputs = []
-    @inputs.each do |input|
-      if input.threadable && input.threads > 1
-        (input.threads - 1).times do |i|
-          moreinputs << input.clone
-        end
-      end
+    # for backward compatibility in devutils for the rspec helpers, this method is not used
+    # in the pipeline anymore.
+    def flush_filters(options = {}, &block)
     end
-    @inputs += moreinputs
-
-    # first make sure we can register all input plugins
-    register_plugins(@inputs)
-
-    # then after all input plugins are successfully registered, start them
-    @inputs.each { |input| start_input(input) }
-  end
-
-  def start_input(plugin)
-    @input_threads << Thread.new { inputworker(plugin) }
-  end
-
-  def inputworker(plugin)
-    Util::set_thread_name("[#{pipeline_id}]<#{plugin.class.config_name}")
-    ThreadContext.put("pipeline.id", pipeline_id)
-    ThreadContext.put("plugin.id", plugin.id)
-    begin
-      plugin.run(wrapped_write_client(plugin.id.to_sym))
-    rescue => e
-      if plugin.stop?
-        @logger.debug(
-          "Input plugin raised exception during shutdown, ignoring it.",
-          default_logging_keys(
-            :plugin => plugin.class.config_name,
-            :exception => e.message,
-            :backtrace => e.backtrace))
-        return
-      end
-
-      # otherwise, report error and restart
-      @logger.error(I18n.t(
-        "logstash.pipeline.worker-error-debug",
-        default_logging_keys(
-          :plugin => plugin.inspect,
-          :error => e.message,
-          :exception => e.class,
-          :stacktrace => e.backtrace.join("\n"))))
-
-      # Assuming the failure that caused this exception is transient,
-      # let's sleep for a bit and execute #run again
-      sleep(1)
-      close_plugin_and_ignore(plugin)
-      retry
-    ensure
-      close_plugin_and_ignore(plugin)
-    end
-  end
-
-  # initiate the pipeline shutdown sequence
-  # this method is intended to be called from outside the pipeline thread
-  # and will block until the pipeline has successfully shut down.
-  def shutdown
-    return if finished_execution?
-
-    # shutdown can only start once the pipeline has completed its startup.
-    # avoid potential race condition between the startup sequence and this
-    # shutdown method which can be called from another thread at any time
-    sleep(0.1) while !ready?
-
-    stop_inputs
-    wait_for_shutdown
-    clear_pipeline_metrics
-  end # def shutdown
-
-  def wait_for_shutdown
-    ShutdownWatcher.new(self).start
-  end
-
-  def stop_inputs
-    @logger.debug("Stopping inputs", default_logging_keys)
-    @inputs.each(&:do_stop)
-    @logger.debug("Stopped inputs", default_logging_keys)
-  end
-
-  # After `shutdown` is called from an external thread this is called from the main thread to
-  # tell the worker threads to stop and then block until they've fully stopped
-  # This also stops all filter and output plugins
-  def shutdown_workers
-    @logger.debug("Setting shutdown", default_logging_keys)
-    @worker_shutdown.set(true)
-
-    @worker_threads.each do |t|
-      @logger.debug("Shutdown waiting for worker thread" , default_logging_keys(:thread => t.inspect))
-      t.join
-    end
-
-    @filters.each(&:do_close)
-    @outputs.each(&:do_close)
-  end
-
-  # for backward compatibility in devutils for the rspec helpers, this method is not used
-  # in the pipeline anymore.
-  def filter(event, &block)
-    maybe_setup_out_plugins
-    # filter_func returns all filtered events, including cancelled ones
-    filter_func([event]).each {|e| block.call(e)}
   end
-
-  # perform filters flush and yield flushed event to the passed block
-  # @param options [Hash]
-  # @option options [Boolean] :final => true to signal a final shutdown flush
-  def flush_filters(options = {}, &block)
-    flushers = options[:final] ? @shutdown_flushers : @periodic_flushers
-
-    flushers.each do |flusher|
-      flusher.call(options, &block)
-    end
-  end
-
-  def start_flusher
-    # Invariant to help detect improper initialization
-    raise "Attempted to start flusher on a stopped pipeline!" if stopped?
-
-    @flusher_thread = Thread.new do
-      LogStash::Util.set_thread_name("[#{pipeline_id}]-flusher-thread")
-      ThreadContext.put("pipeline.id", pipeline_id)
-      while Stud.stoppable_sleep(5, 0.1) { stopped? }
-        flush
-        break if stopped?
-      end
-    end
-  end
-
-  def shutdown_flusher
-    @flusher_thread.join
-  end
-
-  def flush
-    if @flushing.compare_and_set(false, true)
-      @logger.debug? && @logger.debug("Pushing flush onto pipeline", default_logging_keys)
-      @signal_queue.put(FLUSH)
-    end
-  end
-
-  # Calculate the uptime in milliseconds
-  #
-  # @return [Integer] Uptime in milliseconds, 0 if the pipeline is not started
-  def uptime
-    return 0 if started_at.nil?
-    ((Time.now.to_f - started_at.to_f) * 1000.0).to_i
-  end
-
-  # perform filters flush into the output queue
-  #
-  # @param batch [ReadClient::ReadBatch]
-  # @param options [Hash]
-  def flush_filters_to_batch(events, options = {})
-    result = events
-    flush_filters(options) do |event|
-      unless event.cancelled?
-        @logger.debug? and @logger.debug("Pushing flushed events", default_logging_keys(:event => event))
-        result << event
-      end
-    end
-    @flushing.set(false)
-    result
-  end # flush_filters_to_batch
-
-  def plugin_threads_info
-    input_threads = @input_threads.select {|t| t.alive? }
-    worker_threads = @worker_threads.select {|t| t.alive? }
-    (input_threads + worker_threads).map {|t| Util.thread_info(t) }
-  end
-
-  def stalling_threads_info
-    plugin_threads_info
-      .reject {|t| t["blocked_on"] } # known benign blocking statuses
-      .each {|t| t.delete("backtrace") }
-      .each {|t| t.delete("blocked_on") }
-      .each {|t| t.delete("status") }
-  end
-
-  def clear_pipeline_metrics
-    # TODO(ph): I think the metric should also proxy that call correctly to the collector
-    # this will simplify everything since the null metric would simply just do a noop
-    collector = metric.collector
-
-    unless collector.nil?
-      # selectively reset metrics we don't wish to keep after reloading
-      # these include metrics about the plugins and number of processed events
-      # we want to keep other metrics like reload counts and error messages
-      collector.clear("stats/pipelines/#{pipeline_id}/plugins")
-      collector.clear("stats/pipelines/#{pipeline_id}/events")
-    end
-  end
-
-  # Sometimes we log stuff that will dump the pipeline which may contain
-  # sensitive information (like the raw syntax tree which can contain passwords)
-  # We want to hide most of what's in here
-  def inspect
-    {
-      :pipeline_id => pipeline_id,
-      :settings => settings.inspect,
-      :ready => @ready,
-      :running => @running,
-      :flushing => @flushing
-    }
-  end
-
-  private
-
-  def close_plugin_and_ignore(plugin)
-    begin
-      plugin.do_close
-    rescue => e
-      @logger.warn(
-        "plugin raised exception while closing, ignoring",
-        default_logging_keys(
-          :plugin => plugin.class.config_name,
-          :exception => e.message,
-          :backtrace => e.backtrace))
-    end
-  end
-
-  def maybe_setup_out_plugins
-    if @outputs_registered.make_true
-      register_plugins(@outputs)
-      register_plugins(@filters)
-    end
-  end
-
-  def default_logging_keys(other_keys = {})
-    keys = super
-    keys[:thread] ||= thread.inspect if thread
-    keys
-  end
-
-  def draining_queue?
-    @drain_queue ? !filter_queue_client.empty? : false
-  end
-
-  def verify_event_ordering!(pipeline_workers)
-    # the Ruby execution keep event order by design but when using a single worker only
-    if settings.get("pipeline.ordered") == "true" && pipeline_workers > 1
-      fail("enabling the 'pipeline.ordered' setting requires the use of a single pipeline worker")
-    end
-  end
-
-end; end
+end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/pipeline_action/create.rb b/logstash-core/lib/logstash/pipeline_action/create.rb
index 3a3531e1560..d6ae384ea57 100644
--- a/logstash-core/lib/logstash/pipeline_action/create.rb
+++ b/logstash-core/lib/logstash/pipeline_action/create.rb
@@ -16,7 +16,6 @@
 # under the License.
 
 require "logstash/pipeline_action/base"
-require "logstash/pipeline"
 require "logstash/java_pipeline"
 
 
@@ -48,8 +47,7 @@ def execution_priority
     # The execute assume that the thread safety access of the pipeline
     # is managed by the caller.
     def execute(agent, pipelines_registry)
-      pipeline_class = @pipeline_config.settings.get_value("pipeline.java_execution") ? LogStash::JavaPipeline : LogStash::Pipeline
-      new_pipeline = pipeline_class.new(@pipeline_config, @metric, agent)
+      new_pipeline = LogStash::JavaPipeline.new(@pipeline_config, @metric, agent)
       success = pipelines_registry.create_pipeline(pipeline_id, new_pipeline) do
         new_pipeline.start # block until the pipeline is correctly started or crashed
       end
diff --git a/logstash-core/lib/logstash/pipeline_action/reload.rb b/logstash-core/lib/logstash/pipeline_action/reload.rb
index 56e4309145d..9b7c66b5999 100644
--- a/logstash-core/lib/logstash/pipeline_action/reload.rb
+++ b/logstash-core/lib/logstash/pipeline_action/reload.rb
@@ -47,10 +47,8 @@ def execute(agent, pipelines_registry)
         return LogStash::ConvergeResult::FailedAction.new("Cannot reload pipeline, because the existing pipeline is not reloadable")
       end
 
-      java_exec = @pipeline_config.settings.get_value("pipeline.java_execution")
-
       begin
-        pipeline_validator = java_exec ? LogStash::JavaBasePipeline.new(@pipeline_config, nil, logger, nil) : LogStash::BasePipeline.new(@pipeline_config)
+        pipeline_validator = LogStash::JavaBasePipeline.new(@pipeline_config, nil, logger, nil)
       rescue => e
         return LogStash::ConvergeResult::FailedAction.from_exception(e)
       end
@@ -69,7 +67,7 @@ def execute(agent, pipelines_registry)
         old_pipeline.shutdown
 
         # Then create a new pipeline
-        new_pipeline = java_exec ? LogStash::JavaPipeline.new(@pipeline_config, @metric, agent) : LogStash::Pipeline.new(@pipeline_config, @metric, agent)
+        new_pipeline = LogStash::JavaPipeline.new(@pipeline_config, @metric, agent)
         success = new_pipeline.start # block until the pipeline is correctly started or crashed
 
         # return success and new_pipeline to registry reload_pipeline
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index 6d9d974be11..f1d61bf562b 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -364,10 +364,7 @@ def execute
 
         # TODO(ph): make it better for multiple pipeline
         if results.success?
-          results.response.each do |pipeline_config|
-            pipeline_class = pipeline_config.settings.get_value("pipeline.java_execution") ? LogStash::JavaPipeline : LogStash::BasePipeline
-            pipeline_class.new(pipeline_config)
-          end
+          results.response.each { |pipeline_config| LogStash::JavaPipeline.new(pipeline_config) }
           puts "Configuration OK"
           logger.info "Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash"
         else
diff --git a/logstash-core/spec/conditionals_spec.rb b/logstash-core/spec/conditionals_spec.rb
index c7507ed1152..59169f988ae 100644
--- a/logstash-core/spec/conditionals_spec.rb
+++ b/logstash-core/spec/conditionals_spec.rb
@@ -51,27 +51,40 @@ def multi_receive(events)
     end
   end
 
-  before do
-    LogStash::PLUGIN_REGISTRY.add(:output, "dummynull", DummyNullOutput)
-  end
-
   describe "simple" do
-    config <<-CONFIG
-      input {
-        generator {
-          message => '{"foo":{"bar"},"baz": "quux"}'
-          count => 1
-        }
+    let(:config) do <<-CONFIG
+    input {
+      generator {
+        message => '{"foo":{"bar"},"baz": "quux"}'
+        count => 1
       }
-      output {
-        if [foo] == "bar" {
-          dummynull { }
-        }
+    }
+    output {
+      if [foo] == "bar" {
+        dummynull { }
       }
+    }
     CONFIG
+    end
+
+    let(:pipeline) do
+      settings = ::LogStash::SETTINGS.clone
+      config_part = org.logstash.common.SourceWithMetadata.new("config_string", "config_string", config)
+      pipeline_config = LogStash::Config::PipelineConfig.new(LogStash::Config::Source::Local, :main, config_part, settings)
+      LogStash::JavaPipeline.new(pipeline_config)
+    end
+
+    before do
+      LogStash::PLUGIN_REGISTRY.add(:output, "dummynull", DummyNullOutput)
+    end
+
+    after do
+      pipeline.close
+    end
 
-    agent do
+    it "should not fail in pipeline run" do
       #LOGSTASH-2288, should not fail raising an exception
+      pipeline.run
     end
   end
 end
diff --git a/logstash-core/spec/logstash/pipeline_action/reload_spec.rb b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
index d51c5a9d716..0ce5ec0f5d6 100644
--- a/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
@@ -26,7 +26,7 @@
   let(:pipeline_id) { :main }
   let(:new_pipeline_config) { mock_pipeline_config(pipeline_id, "input { dummyblockinginput { id => 'new' } } output { null {} }", { "pipeline.reloadable" => true}) }
   let(:pipeline_config) { "input { dummyblockinginput {} } output { null {} }" }
-  let(:pipeline) { mock_pipeline_from_string(pipeline_config, mock_settings("pipeline.reloadable" => true)) }
+  let(:pipeline) { mock_java_pipeline_from_string(pipeline_config, mock_settings("pipeline.reloadable" => true)) }
   let(:pipelines) { r = LogStash::PipelinesRegistry.new; r.create_pipeline(pipeline_id, pipeline) { true }; r }
   let(:agent) { double("agent") }
 
diff --git a/logstash-core/spec/logstash/pipeline_action/stop_spec.rb b/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
index 5e1613c666f..8355abb8cfd 100644
--- a/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
@@ -19,12 +19,11 @@
 require_relative "../../support/helpers"
 require "logstash/pipelines_registry"
 require "logstash/pipeline_action/stop"
-require "logstash/pipeline"
 
 describe LogStash::PipelineAction::Stop do
   let(:pipeline_config) { "input { dummyblockinginput {} } output { null {} }" }
   let(:pipeline_id) { :main }
-  let(:pipeline) { mock_pipeline_from_string(pipeline_config) }
+  let(:pipeline) { mock_java_pipeline_from_string(pipeline_config) }
   let(:pipelines) { chm = LogStash::PipelinesRegistry.new; chm.create_pipeline(pipeline_id, pipeline) { true }; chm }
   let(:agent) { double("agent") }
 
diff --git a/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb b/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
index 15ae5459b36..3ef961afb7e 100644
--- a/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
@@ -51,7 +51,7 @@ def threadsafe?() true; end
   def close() end
 end
 
-describe LogStash::Pipeline do
+describe LogStash::JavaPipeline do
   let(:pipeline_settings_obj) { LogStash::SETTINGS.clone }
   let(:pipeline_settings) do
     {
@@ -72,7 +72,7 @@ def close() end
     eos
   }
 
-  subject { mock_pipeline_from_string(test_config, pipeline_settings_obj, metric) }
+  subject { mock_java_pipeline_from_string(test_config, pipeline_settings_obj, metric) }
 
   before(:each) do
     pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
@@ -91,7 +91,7 @@ def close() end
     let(:pipeline_id) { "test-dlq" }
 
     it "retrieves proper pipeline-level DLQ writer" do
-      expect_any_instance_of(org.logstash.common.io.DeadLetterQueueWriter).to receive(:close).and_call_original
+      expect_any_instance_of(org.logstash.common.io.DeadLetterQueueWriter).to receive(:close).at_least(:once).and_call_original
       subject.start
       subject.shutdown
       dlq_path = java.nio.file.Paths.get(pipeline_settings_obj.get("path.dead_letter_queue"), pipeline_id)
@@ -108,7 +108,7 @@ def close() end
 
     it "does not write to the DLQ" do
       expect(LogStash::Util::DummyDeadLetterQueueWriter).to receive(:new).and_call_original
-      expect_any_instance_of(LogStash::Util::DummyDeadLetterQueueWriter).to receive(:close).and_call_original
+      expect_any_instance_of(LogStash::Util::DummyDeadLetterQueueWriter).to receive(:close).at_least(:once).and_call_original
       subject.start
       dlq_path = java.nio.file.Paths.get(pipeline_settings_obj.get("path.dead_letter_queue"), pipeline_id)
       expect(java.nio.file.Files.exists(dlq_path)).to eq(false)
diff --git a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
index df0b313acbe..69f5fc63bff 100644
--- a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
@@ -52,7 +52,7 @@ def close
   end
 end
 
-describe LogStash::Pipeline do
+describe LogStash::JavaPipeline do
   let(:pipeline_settings_obj) { LogStash::SETTINGS.clone }
   let(:pipeline_id) { "main" }
 
@@ -154,10 +154,6 @@ def close
 
   context "using PQ" do
     let(:queue_type) { "persisted" } #  "memory", "persisted"
-    context "with Ruby execution" do
-      subject { LogStash::Pipeline.new(pipeline_config, metric) }
-      it_behaves_like "a well behaved pipeline"
-    end
     context "with Java execution" do
       subject { LogStash::JavaPipeline.new(pipeline_config, metric) }
       it_behaves_like "a well behaved pipeline"
@@ -165,10 +161,6 @@ def close
   end
   context "using MQ" do
     let(:queue_type) { "memory" } #  "memory", "persisted"
-    context "with Ruby execution" do
-      subject { LogStash::Pipeline.new(pipeline_config, metric) }
-      it_behaves_like "a well behaved pipeline"
-    end
     context "with Java execution" do
       subject { LogStash::JavaPipeline.new(pipeline_config, metric) }
       it_behaves_like "a well behaved pipeline"
diff --git a/logstash-core/spec/logstash/pipeline_reporter_spec.rb b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
index e2590a94d66..082b06a6859 100644
--- a/logstash-core/spec/logstash/pipeline_reporter_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
@@ -16,7 +16,6 @@
 # under the License.
 
 require "spec_helper"
-require "logstash/pipeline"
 require_relative "../support/helpers"
 require_relative "../support/mocks_classes"
 
@@ -86,9 +85,6 @@
 end
 
 describe LogStash::PipelineReporter do
-  context "with ruby execution" do
-    it_behaves_like "a pipeline reporter", :mock_pipeline_from_string
-  end
   context "with java execution" do
     it_behaves_like "a pipeline reporter", :mock_java_pipeline_from_string
   end
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
deleted file mode 100644
index 01f8d411f57..00000000000
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ /dev/null
@@ -1,1019 +0,0 @@
-# Licensed to Elasticsearch B.V. under one or more contributor
-# license agreements. See the NOTICE file distributed with
-# this work for additional information regarding copyright
-# ownership. Elasticsearch B.V. licenses this file to you under
-# the Apache License, Version 2.0 (the "License"); you may
-# not use this file except in compliance with the License.
-# You may obtain a copy of the License at
-#
-#  http://www.apache.org/licenses/LICENSE-2.0
-#
-# Unless required by applicable law or agreed to in writing,
-# software distributed under the License is distributed on an
-# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
-# KIND, either express or implied.  See the License for the
-# specific language governing permissions and limitations
-# under the License.
-
-require "spec_helper"
-require "logstash/inputs/generator"
-require "logstash/filters/drop"
-require_relative "../support/mocks_classes"
-require_relative "../support/helpers"
-require "stud/try"
-require 'timeout'
-require 'logstash/config/pipeline_config'
-
-class DummyInput < LogStash::Inputs::Base
-  config_name "dummyinput"
-  milestone 2
-
-  def register
-  end
-
-  def run(queue)
-    @logger.debug("check log4j fish tagging input plugin")
-  end
-
-  def close
-  end
-end
-
-# This input runs long enough that a flush should occur
-class DummyFlushEnablingInput < DummyInput
-  def run(queue)
-    while !stop?
-      sleep 1
-    end
-  end
-end
-
-class DummyInputGenerator < LogStash::Inputs::Base
-  config_name "dummyinputgenerator"
-  milestone 2
-
-  def register
-  end
-
-  def run(queue)
-    queue << Logstash::Event.new while !stop?
-  end
-
-  def close
-  end
-end
-
-class DummyCodec < LogStash::Codecs::Base
-  config_name "dummycodec"
-  milestone 2
-
-  config :format, :validate => :string
-
-  def decode(data)
-    data
-  end
-
-  def encode(event)
-    event
-  end
-
-  def close
-  end
-end
-
-class DummyOutputMore < ::LogStash::Outputs::DummyOutput
-  config_name "dummyoutputmore"
-end
-
-class DummyFilter < LogStash::Filters::Base
-  config_name "dummyfilter"
-  milestone 2
-
-  def register() end
-
-  def filter(event)
-    @logger.debug("check log4j fish tagging filter plugin")
-  end
-
-  def threadsafe?() false; end
-
-  def close() end
-end
-
-class DummySafeFilter < LogStash::Filters::Base
-  config_name "dummysafefilter"
-  milestone 2
-
-  def register() end
-
-  def filter(event) end
-
-  def threadsafe?() true; end
-
-  def close() end
-end
-
-class DummyFlushingFilter < LogStash::Filters::Base
-  config_name "dummyflushingfilter"
-  milestone 2
-
-  def register() end
-  def filter(event) end
-  def periodic_flush
-    true
-  end
-  def flush(options)
-    [::LogStash::Event.new("message" => "dummy_flush")]
-  end
-  def close() end
-end
-
-class DummyFlushingFilterPeriodic < DummyFlushingFilter
-  config_name "dummyflushingfilterperiodic"
-
-  def flush(options)
-    # Don't generate events on the shutdown flush to make sure we actually test the
-    # periodic flush.
-    options[:final] ? [] : [::LogStash::Event.new("message" => "dummy_flush")]
-  end
-end
-
-describe LogStash::Pipeline do
-  let(:worker_thread_count)     { 5 }
-  let(:safe_thread_count)       { 1 }
-  let(:override_thread_count)   { 42 }
-  let(:dead_letter_queue_enabled) { false }
-  let(:dead_letter_queue_path) { }
-  let(:pipeline_settings_obj) { LogStash::SETTINGS.clone }
-  let(:pipeline_settings) { {} }
-  let(:max_retry) {10} #times
-  let(:timeout) {120} #seconds
-
-  before :each do
-    pipeline_workers_setting = LogStash::SETTINGS.get_setting("pipeline.workers")
-    allow(pipeline_workers_setting).to receive(:default).and_return(worker_thread_count)
-    dlq_enabled_setting = LogStash::SETTINGS.get_setting("dead_letter_queue.enable")
-    allow(dlq_enabled_setting).to receive(:value).and_return(dead_letter_queue_enabled)
-    dlq_path_setting = LogStash::SETTINGS.get_setting("path.dead_letter_queue")
-    allow(dlq_path_setting).to receive(:value).and_return(dead_letter_queue_path)
-
-    pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
-  end
-
-  describe "#ephemeral_id" do
-    it "creates an ephemeral_id at creation time" do
-      pipeline = mock_pipeline_from_string("input { generator { count =>  1 } } output { null {} }")
-      expect(pipeline.ephemeral_id).to_not be_nil
-      pipeline.close
-
-      second_pipeline = mock_pipeline_from_string("input { generator { count => 1 } } output { null {} }")
-      expect(second_pipeline.ephemeral_id).not_to eq(pipeline.ephemeral_id)
-      second_pipeline.close
-    end
-  end
-
-
-  describe "event cancellation" do
-    # test harness for https://github.com/elastic/logstash/issues/6055
-
-    let(:output) { LogStash::Outputs::DummyOutputWithEventsArray.new }
-
-    before do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputwitheventsarray").and_return(LogStash::Outputs::DummyOutputWithEventsArray)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "drop").and_call_original
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "mutate").and_call_original
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
-      allow(LogStash::Outputs::DummyOutputWithEventsArray).to receive(:new).with(any_args).and_return(output)
-    end
-
-    let(:config) do
-      <<-CONFIG
-        input {
-          generator {
-            lines => ["1", "2", "END"]
-            count => 1
-          }
-        }
-        filter {
-          if [message] == "1" {
-            drop {}
-          }
-          mutate { add_tag => ["notdropped"] }
-        }
-        output { dummyoutputwitheventsarray {} }
-      CONFIG
-    end
-
-    it "should not propagate cancelled events from filter to output" do
-      abort_on_exception_state = Thread.abort_on_exception
-      Thread.abort_on_exception = true
-
-      pipeline = mock_pipeline_from_string(config, pipeline_settings_obj)
-      pipeline.start
-      Timeout.timeout(timeout) do
-        sleep(0.1) until pipeline.ready?
-      end
-      Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
-        wait(3).for do
-          # give us a bit of time to flush the events
-          # puts("*****" + output.events.map{|e| e.message}.to_s)
-          output.events.map{|e| e.get("message")}.include?("END")
-        end.to be_truthy
-      end
-      expect(output.events.size).to eq(2)
-      expect(output.events[0].get("tags")).to eq(["notdropped"])
-      expect(output.events[1].get("tags")).to eq(["notdropped"])
-      pipeline.shutdown
-
-      Thread.abort_on_exception = abort_on_exception_state
-    end
-  end
-
-  describe "defaulting the pipeline workers based on thread safety" do
-    before(:each) do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummysafefilter").and_return(DummySafeFilter)
-    end
-
-    context "when there are some not threadsafe filters" do
-      let(:test_config_with_filters) {
-        <<-eos
-        input {
-          dummyinput {}
-        }
-
-        filter {
-          dummyfilter {}
-        }
-
-        output {
-          dummyoutput {}
-        }
-        eos
-      }
-
-      describe "debug compiled" do
-        let(:logger) { double("pipeline logger").as_null_object }
-
-        before do
-          expect(::LogStash::Pipeline).to receive(:logger).and_return(logger)
-          allow(logger).to receive(:debug?).and_return(true)
-          allow_any_instance_of(DummyFilter).to receive(:logger).and_return(logger)
-        end
-
-        it "should not receive a debug message with the compiled code" do
-          pipeline_settings_obj.set("config.debug", false)
-          expect(logger).not_to receive(:debug).with(/Compiled pipeline/, anything)
-          pipeline = mock_pipeline_from_string(test_config_with_filters)
-          pipeline.close
-        end
-
-        it "should print the compiled code if config.debug is set to true" do
-          pipeline_settings_obj.set("config.debug", true)
-          expect(logger).to receive(:debug).with(/Compiled pipeline/, anything)
-          pipeline = mock_pipeline_from_string(test_config_with_filters, pipeline_settings_obj)
-          pipeline.close
-        end
-
-        it "should log each filtered event if config.debug is set to true" do
-          pipeline_settings_obj.set("config.debug", true)
-          pipeline = mock_pipeline_from_string(test_config_with_filters, pipeline_settings_obj)
-          expect(logger).to receive(:debug).with(/filter received/, anything)
-          pipeline.filter_func([LogStash::Event.new])
-          pipeline.close
-        end
-
-        it "should log fish tagging of plugins" do
-          pipeline_settings_obj.set("config.debug", true)
-          pipeline = mock_pipeline_from_string(test_config_with_filters, pipeline_settings_obj)
-          expect(logger).to receive(:debug).with(/filter received/, anything)
-          expect(logger).to receive(:debug).with(/[dummyfilter]/)
-          pipeline.filter_func([LogStash::Event.new])
-          pipeline.close
-        end
-      end
-
-
-      context "when there is no command line -w N set" do
-        it "starts one filter thread" do
-          msg = "Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads"
-          pipeline = mock_pipeline_from_string(test_config_with_filters)
-          expect(pipeline.logger).to receive(:warn).with(msg,
-            hash_including({:count_was=>worker_thread_count, :filters=>["dummyfilter"]}))
-          pipeline.start
-          expect(pipeline.worker_threads.size).to eq(safe_thread_count)
-          pipeline.shutdown
-        end
-      end
-
-      context "when there is command line -w N set" do
-        let(:pipeline_settings) { {"pipeline.workers" => override_thread_count } }
-        it "starts multiple filter thread" do
-          msg = "Warning: Manual override - there are filters that might" +
-                " not work with multiple worker threads"
-          pipeline = mock_pipeline_from_string(test_config_with_filters, pipeline_settings_obj)
-          expect(pipeline.logger).to receive(:warn).with(msg, hash_including({:worker_threads=> override_thread_count, :filters=>["dummyfilter"]}))
-          pipeline.start
-          expect(pipeline.worker_threads.size).to eq(override_thread_count)
-          pipeline.shutdown
-        end
-      end
-    end
-
-    context "when there are threadsafe filters only" do
-      let(:test_config_with_filters) {
-        <<-eos
-        input {
-          dummyinput {}
-        }
-
-        filter {
-          dummysafefilter {}
-        }
-
-        output {
-          dummyoutput {}
-        }
-        eos
-      }
-
-      it "starts multiple filter threads" do
-        skip("This test has been failing periodically since November 2016. Tracked as https://github.com/elastic/logstash/issues/6245")
-        pipeline = mock_pipeline_from_string(test_config_with_filters)
-        pipeline.start
-        expect(pipeline.worker_threads.size).to eq(worker_thread_count)
-        pipeline.shutdown
-      end
-    end
-  end
-
-  context "close" do
-    before(:each) do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-    end
-
-
-    let(:test_config_without_output_workers) {
-      <<-eos
-      input {
-        dummyinput {}
-      }
-
-      output {
-        dummyoutput {}
-      }
-      eos
-    }
-
-    context "inputs and output close" do
-      let(:pipeline) { mock_pipeline_from_string(test_config_without_output_workers) }
-      let(:output) { pipeline.outputs.first }
-      let(:input) { pipeline.inputs.first }
-
-      before do
-        allow(output).to receive(:do_close)
-        allow(input).to receive(:do_close)
-      end
-
-      it "should call close of output without output-workers" do
-        pipeline.start
-        pipeline.shutdown
-        expect(output).to have_received(:do_close).once
-        expect(input).to have_received(:do_close).once
-      end
-    end
-  end
-
-  context "with no explicit ids declared" do
-    before(:each) do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-    end
-
-    let(:config) { "input { dummyinput { codec => plain { format => 'something'  } } } filter { dummyfilter {} } output { dummyoutput {} }"}
-    let(:pipeline) { mock_pipeline_from_string(config) }
-
-    after do
-      # If you don't start/stop the pipeline it won't release the queue lock and will
-      # cause the suite to fail :(
-      pipeline.close
-    end
-
-    it "should use LIR provided IDs" do
-      expect(pipeline.inputs.first.id).to eq(pipeline.lir.input_plugin_vertices.first.id)
-      expect(pipeline.filters.first.id).to eq(pipeline.lir.filter_plugin_vertices.first.id)
-      expect(pipeline.outputs.first.id).to eq(pipeline.lir.output_plugin_vertices.first.id)
-    end
-  end
-
-  context "compiled flush function" do
-    describe "flusher thread" do
-      before(:each) do
-        allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
-        allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-        allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-      end
-
-      let(:config) { "input { dummyinput {} } output { dummyoutput {} }"}
-
-      it "should start the flusher thread only after the pipeline is running" do
-        pipeline = mock_pipeline_from_string(config)
-
-        expect(pipeline).to receive(:transition_to_running).ordered.and_call_original
-        expect(pipeline).to receive(:start_flusher).ordered.and_call_original
-
-        pipeline.start
-        pipeline.shutdown
-      end
-    end
-
-    context "cancelled events should not propagate down the filters" do
-      config <<-CONFIG
-        filter {
-          drop {}
-        }
-      CONFIG
-
-      sample("hello") do
-        expect(subject).to eq(nil)
-      end
-    end
-
-    context "new events should propagate down the filters" do
-      config <<-CONFIG
-        filter {
-          clone {
-            clones => ["clone1"]
-          }
-        }
-      CONFIG
-      sample(["foo", "bar"]) do
-        expect(subject.size).to eq(4)
-      end
-    end
-  end
-
-  describe "max inflight warning" do
-    let(:config) { "input { dummyinput {} } output { dummyoutput {} }" }
-    let(:batch_size) { 1 }
-    let(:pipeline_settings) { { "pipeline.batch.size" => batch_size, "pipeline.workers" => 1 } }
-    let(:pipeline) { mock_pipeline_from_string(config, pipeline_settings_obj) }
-    let(:logger) { pipeline.logger }
-    let(:warning_prefix) { Regexp.new("CAUTION: Recommended inflight events max exceeded!") }
-
-    before(:each) do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-      allow(logger).to receive(:warn)
-
-      # pipeline must be first called outside the thread context because it lazily initialize and will create a
-      # race condition if called in the thread
-      pipeline.start
-      Timeout.timeout(timeout) do
-        sleep(0.1) until pipeline.ready?
-      end
-      pipeline.shutdown
-    end
-
-    it "should not raise a max inflight warning if the max_inflight count isn't exceeded" do
-      expect(logger).not_to have_received(:warn).with(warning_prefix)
-    end
-
-    context "with a too large inflight count" do
-      let(:batch_size) { LogStash::Pipeline::MAX_INFLIGHT_WARN_THRESHOLD + 1 }
-
-      it "should raise a max inflight warning if the max_inflight count is exceeded" do
-        expect(logger).to have_received(:warn).with(warning_prefix, hash_including(:pipeline_id => anything))
-      end
-    end
-  end
-
-  context "compiled filter functions" do
-    context "new events should propagate down the filters" do
-      config <<-CONFIG
-        filter {
-          clone {
-            clones => ["clone1", "clone2"]
-          }
-          mutate {
-            add_field => {"foo" => "bar"}
-          }
-        }
-      CONFIG
-
-      sample("hello") do
-        expect(subject.size).to eq(3)
-
-        expect(subject[0].get("message")).to eq("hello")
-        expect(subject[0].get("type")).to be_nil
-        expect(subject[0].get("foo")).to eq("bar")
-
-        expect(subject[1].get("message")).to eq("hello")
-        expect(subject[1].get("type")).to eq("clone1")
-        expect(subject[1].get("foo")).to eq("bar")
-
-        expect(subject[2].get("message")).to eq("hello")
-        expect(subject[2].get("type")).to eq("clone2")
-        expect(subject[2].get("foo")).to eq("bar")
-      end
-    end
-  end
-
-  context "metrics" do
-    config = "input { } filter { } output { }"
-
-    let(:settings) { LogStash::SETTINGS.clone }
-    subject { mock_pipeline_from_string(config, settings, metric) }
-
-    after :each do
-      subject.close
-    end
-
-    context "when metric.collect is disabled" do
-      before :each do
-        settings.set("metric.collect", false)
-      end
-
-      context "if namespaced_metric is nil" do
-        let(:metric) { nil }
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-      end
-
-      context "if namespaced_metric is a Metric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
-
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-
-      context "if namespaced_metric is a NullMetric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
-
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(::LogStash::Instrument::NullMetric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-    end
-
-    context "when metric.collect is enabled" do
-      before :each do
-        settings.set("metric.collect", true)
-      end
-
-      context "if namespaced_metric is nil" do
-        let(:metric) { nil }
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-      end
-
-      context "if namespaced_metric is a Metric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
-
-        it "uses a `Metric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::Metric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-
-      context "if namespaced_metric is a NullMetric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
-
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-    end
-  end
-
-  context "Multiples pipelines" do
-    before do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinputgenerator").and_return(DummyInputGenerator)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputmore").and_return(DummyOutputMore)
-    end
-
-    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
-    before :each do
-      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
-      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
-      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
-    end
-
-    let(:pipeline1) { mock_pipeline_from_string("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
-    let(:pipeline2) { mock_pipeline_from_string("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutputmore {}}") }
-
-    after  do
-      pipeline1.close
-      pipeline2.close
-    end
-
-    it "should handle evaluating different config" do
-      expect(pipeline1.output_func(LogStash::Event.new)).not_to include(nil)
-      expect(pipeline1.filter_func([LogStash::Event.new])).not_to include(nil)
-      expect(pipeline2.output_func(LogStash::Event.new)).not_to include(nil)
-      expect(pipeline1.filter_func([LogStash::Event.new])).not_to include(nil)
-    end
-  end
-
-  context "Periodic Flush" do
-    let(:config) do
-      <<-EOS
-      input {
-        dummy_input {}
-      }
-      filter {
-        dummy_flushing_filter {}
-      }
-      output {
-        dummy_output {}
-      }
-      EOS
-    end
-    let(:output) { ::LogStash::Outputs::DummyOutput.new }
-
-    before do
-      allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(output)
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummy_input").and_return(DummyFlushEnablingInput)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummy_flushing_filter").and_return(DummyFlushingFilterPeriodic)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummy_output").and_return(::LogStash::Outputs::DummyOutput)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
-    end
-
-    it "flush periodically" do
-      Thread.abort_on_exception = true
-      pipeline = mock_pipeline_from_string(config, pipeline_settings_obj)
-      pipeline.start
-      Timeout.timeout(timeout) do
-        sleep(0.1) until pipeline.ready?
-      end
-      Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
-        wait(10).for do
-          # give us a bit of time to flush the events
-          output.events.empty?
-        end.to be_falsey
-      end
-
-      expect(output.events.any? {|e| e.get("message") == "dummy_flush"}).to eq(true)
-
-      pipeline.shutdown
-    end
-  end
-
-  context "Multiple pipelines" do
-    before do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-    end
-
-    let(:pipeline1) { mock_pipeline_from_string("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
-    let(:pipeline2) { mock_pipeline_from_string("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
-
-    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
-    before :each do
-      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
-      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
-      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
-    end
-
-    it "should handle evaluating different config" do
-      # When the functions are compiled from the AST it will generate instance
-      # variables that are unique to the actual config, the instances are pointing
-      # to conditionals and/or plugins.
-      #
-      # Before the `defined_singleton_method`, the definition of the method was
-      # not unique per class, but the `instance variables` were unique per class.
-      #
-      # So the methods were trying to access instance variables that did not exist
-      # in the current instance and was returning an array containing nil values for
-      # the match.
-      expect(pipeline1.output_func(LogStash::Event.new)).not_to include(nil)
-      expect(pipeline1.filter_func([LogStash::Event.new])).not_to include(nil)
-      expect(pipeline2.output_func(LogStash::Event.new)).not_to include(nil)
-      expect(pipeline1.filter_func([LogStash::Event.new])).not_to include(nil)
-    end
-  end
-
-  context "#started_at" do
-    # use a run limiting count to shutdown the pipeline automatically
-    let(:config) do
-      <<-EOS
-      input {
-        generator { count => 10 }
-      }
-      EOS
-    end
-
-    subject { mock_pipeline_from_string(config) }
-
-    context "when the pipeline is not started" do
-      after :each do
-        subject.close
-      end
-
-      it "returns nil when the pipeline isnt started" do
-        expect(subject.started_at).to be_nil
-      end
-    end
-
-    it "return when the pipeline started working" do
-      subject.start
-      expect(subject.started_at).to be < Time.now
-      subject.shutdown
-    end
-  end
-
-  context "#uptime" do
-    let(:config) do
-      <<-EOS
-      input {
-        generator {}
-      }
-      EOS
-    end
-    subject { mock_pipeline_from_string(config) }
-
-    context "when the pipeline is not started" do
-      after :each do
-        subject.close
-      end
-
-      it "returns 0" do
-        expect(subject.uptime).to eq(0)
-      end
-    end
-
-    context "when the pipeline is started" do
-      it "return the duration in milliseconds" do
-        subject.start
-        Timeout.timeout(timeout) do
-          sleep(0.1) until subject.ready?
-        end
-        Timeout.timeout(timeout) do
-          sleep(0.1)
-        end
-        expect(subject.uptime).to be > 0
-        subject.shutdown
-      end
-    end
-  end
-
-  context "when collecting metrics in the pipeline" do
-    let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
-
-    subject { mock_pipeline_from_string(config, pipeline_settings_obj, metric) }
-
-    let(:pipeline_settings) { { "pipeline.id" => pipeline_id } }
-    let(:pipeline_id) { "main" }
-    let(:number_of_events) { 420 }
-    let(:dummy_id) { "my-multiline" }
-    let(:dummy_id_other) { "my-multiline_other" }
-    let(:dummy_output_id) { "my-dummyoutput" }
-    let(:generator_id) { "my-generator" }
-    let(:config) do
-      <<-EOS
-      input {
-        generator {
-           count => #{number_of_events}
-           id => "#{generator_id}"
-        }
-      }
-      filter {
-          dummyfilter {
-              id => "#{dummy_id}"
-          }
-          dummyfilter {
-               id => "#{dummy_id_other}"
-           }
-      }
-      output {
-        dummyoutput {
-          id => "#{dummy_output_id}"
-        }
-      }
-      EOS
-    end
-    let(:dummyoutput) { ::LogStash::Outputs::DummyOutput.new({ "id" => dummy_output_id }) }
-    let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
-
-    before :each do
-      allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(LogStash::Filters::DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-
-      subject.start
-      Timeout.timeout(timeout) do
-        sleep(0.1) until subject.ready?
-      end
-
-      # make sure we have received all the generated events
-      Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
-        wait(3).for do
-          # give us a bit of time to flush the events
-          dummyoutput.events.size >= number_of_events
-        end.to be_truthy
-      end
-    end
-
-    after :each do
-      subject.shutdown
-    end
-
-    context "global metric" do
-      let(:collected_metric) { metric_store.get_with_path("stats/events") }
-
-      it "populates the different metrics" do
-        expect(collected_metric[:stats][:events][:duration_in_millis].value).not_to be_nil
-        expect(collected_metric[:stats][:events][:in].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:events][:filtered].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:events][:out].value).to eq(number_of_events)
-      end
-    end
-
-    context "pipelines" do
-      let(:collected_metric) { metric_store.get_with_path("stats/pipelines/") }
-
-      it "populates the pipelines core metrics" do
-        expect(collected_metric[:stats][:pipelines][:main][:events][:duration_in_millis].value).not_to be_nil
-        expect(collected_metric[:stats][:pipelines][:main][:events][:in].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:events][:filtered].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:events][:out].value).to eq(number_of_events)
-      end
-
-      it "populates the filter metrics" do
-        [dummy_id, dummy_id_other].map(&:to_sym).each do |id|
-          [:in, :out].each do |metric_key|
-            plugin_name = id.to_sym
-            expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:events][metric_key].value).to eq(number_of_events)
-          end
-        end
-      end
-
-      it "populates the output metrics" do
-        plugin_name = dummy_output_id.to_sym
-
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:in].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:out].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:duration_in_millis].value).not_to be_nil
-      end
-
-      it "populates the name of the output plugin" do
-        plugin_name = dummy_output_id.to_sym
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:name].value).to eq(::LogStash::Outputs::DummyOutput.config_name)
-      end
-
-      it "populates the name of the filter plugin" do
-        [dummy_id, dummy_id_other].map(&:to_sym).each do |id|
-          plugin_name = id.to_sym
-          expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:name].value).to eq(LogStash::Filters::DummyFilter.config_name)
-        end
-      end
-
-      context 'when dlq is disabled' do
-        let (:collect_stats) { subject.collect_dlq_stats}
-        let (:collected_stats) { collected_metric[:stats][:pipelines][:main][:dlq]}
-        let (:available_stats) {[:path, :queue_size_in_bytes]}
-
-        it 'should show not show any dlq stats' do
-          collect_stats
-          expect(collected_stats).to be_nil
-        end
-
-      end
-
-      context 'when dlq is enabled' do
-        let (:dead_letter_queue_enabled) { true }
-        let (:dead_letter_queue_path) { Stud::Temporary.directory }
-        let (:pipeline_dlq_path) { "#{dead_letter_queue_path}/#{pipeline_id}"}
-
-        let (:collect_stats) { subject.collect_dlq_stats }
-        let (:collected_stats) { collected_metric[:stats][:pipelines][:main][:dlq]}
-
-        it 'should show dlq stats' do
-          collect_stats
-          # A newly created dead letter queue with no entries will have a size of 1 (the version 'header')
-          expect(collected_stats[:queue_size_in_bytes].value).to eq(1)
-        end
-      end
-    end
-  end
-
-  context "Pipeline object" do
-    before do
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
-      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
-    end
-
-    let(:pipeline1) { mock_pipeline_from_string("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
-    let(:pipeline2) { mock_pipeline_from_string("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
-
-    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
-    before :each do
-      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
-      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
-      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
-    end
-
-    it "should not add ivars" do
-       expect(pipeline1.instance_variables).to eq(pipeline2.instance_variables)
-    end
-  end
-
-  context "#system" do
-    after do
-      pipeline.close # close the queue
-    end
-
-    context "when the pipeline is a system pipeline" do
-      let(:pipeline) { mock_pipeline_from_string("input { generator {} } output { null {} }", mock_settings("pipeline.system" => true)) }
-      it "returns true" do
-        expect(pipeline.system?).to be_truthy
-      end
-    end
-
-    context "when the pipeline is not a system pipeline" do
-      let(:pipeline) { mock_pipeline_from_string("input { generator {} } output { null {} }", mock_settings("pipeline.system" => false)) }
-      it "returns true" do
-        expect(pipeline.system?).to be_falsey
-      end
-    end
-  end
-
-  context "#reloadable?" do
-    after do
-      pipeline.close # close the queue
-    end
-
-    context "when all plugins are reloadable and pipeline is configured as reloadable" do
-      let(:pipeline) { mock_pipeline_from_string("input { generator {} } output { null {} }", mock_settings("pipeline.reloadable" => true)) }
-
-      it "returns true" do
-        expect(pipeline.reloadable?).to be_truthy
-      end
-    end
-
-    context "when the plugins are not reloadable and pipeline is configured as reloadable" do
-      let(:pipeline) { mock_pipeline_from_string("input { stdin {} } output { null {} }", mock_settings("pipeline.reloadable" => true)) }
-
-      it "returns true" do
-        expect(pipeline.reloadable?).to be_falsey
-      end
-    end
-
-    context "when all plugins are reloadable and pipeline is configured as non-reloadable" do
-      let(:pipeline) { mock_pipeline_from_string("input { generator {} } output { null {} }", mock_settings("pipeline.reloadable" => false)) }
-
-      it "returns true" do
-        expect(pipeline.reloadable?).to be_falsey
-      end
-    end
-  end
-
-  context "event ordering" do
-    let(:pipeline) { mock_pipeline_from_string("input { } output { }", mock_settings("pipeline.ordered" => true, "pipeline.workers" => 2)) }
-
-    it "fail running when ordering is set to true and there are multiple workers" do
-      expect{pipeline.run}.to raise_error(RuntimeError, /pipeline\.ordered/)
-      pipeline.close
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/state_resolver_spec.rb b/logstash-core/spec/logstash/state_resolver_spec.rb
index 35083279751..741afe967f1 100644
--- a/logstash-core/spec/logstash/state_resolver_spec.rb
+++ b/logstash-core/spec/logstash/state_resolver_spec.rb
@@ -19,7 +19,6 @@
 require_relative "../support/helpers"
 require_relative "../support/matchers"
 require "logstash/state_resolver"
-require "logstash/pipeline"
 require "ostruct"
 require "digest"
 
diff --git a/logstash-core/spec/support/helpers.rb b/logstash-core/spec/support/helpers.rb
index 52f67964de6..18bda9b2edb 100644
--- a/logstash-core/spec/support/helpers.rb
+++ b/logstash-core/spec/support/helpers.rb
@@ -62,12 +62,7 @@ def mock_pipeline(pipeline_id, reloadable = true, config_hash = nil)
                            "config.string" => config_string,
                            "config.reload.automatic" => reloadable)
   pipeline_config = mock_pipeline_config(pipeline_id, config_string, settings)
-  LogStash::Pipeline.new(pipeline_config)
-end
-
-def mock_pipeline_from_string(config_string, settings = LogStash::SETTINGS, metric = nil)
-  pipeline_config = mock_pipeline_config(settings.get("pipeline.id"), config_string, settings)
-  LogStash::Pipeline.new(pipeline_config, metric)
+  LogStash::JavaPipeline.new(pipeline_config)
 end
 
 def mock_java_pipeline_from_string(config_string, settings = LogStash::SETTINGS, metric = nil)
diff --git a/logstash-core/src/main/java/org/logstash/execution/Engine.java b/logstash-core/src/main/java/org/logstash/execution/Engine.java
deleted file mode 100644
index 112b6bf2d84..00000000000
--- a/logstash-core/src/main/java/org/logstash/execution/Engine.java
+++ /dev/null
@@ -1,7 +0,0 @@
-package org.logstash.execution;
-
-public enum Engine {
-    RUBY,
-    JAVA,
-    ;
-}
diff --git a/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java
index 7d58f2c1a15..aea003dde43 100644
--- a/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java
@@ -77,8 +77,7 @@ public JavaBasePipelineExt initialize(final ThreadContext context, final IRubyOb
                 new ExecutionContextFactoryExt(
                     context.runtime, RubyUtil.EXECUTION_CONTEXT_FACTORY_CLASS
                 ).initialize(context, args[3], this, dlqWriter(context)),
-                RubyUtil.FILTER_DELEGATOR_CLASS,
-                Engine.JAVA
+                RubyUtil.FILTER_DELEGATOR_CLASS
             ),
             getSecretStore(context)
         );
diff --git a/logstash-core/src/main/java/org/logstash/plugins/factory/PluginFactoryExt.java b/logstash-core/src/main/java/org/logstash/plugins/factory/PluginFactoryExt.java
index 0d3226e0755..4a0bdfe7433 100644
--- a/logstash-core/src/main/java/org/logstash/plugins/factory/PluginFactoryExt.java
+++ b/logstash-core/src/main/java/org/logstash/plugins/factory/PluginFactoryExt.java
@@ -14,7 +14,6 @@
 import org.logstash.config.ir.PipelineIR;
 import org.logstash.config.ir.compiler.*;
 import org.logstash.config.ir.graph.Vertex;
-import org.logstash.execution.Engine;
 import org.logstash.execution.ExecutionContextExt;
 import org.logstash.instrument.metrics.AbstractMetricExt;
 import org.logstash.instrument.metrics.AbstractNamespacedMetricExt;
@@ -40,8 +39,6 @@ public interface PluginResolver {
 
     private final Collection<String> pluginsById = ConcurrentHashMap.newKeySet();
 
-    private Engine engine;
-
     private PipelineIR lir;
 
     private ExecutionContextFactoryExt executionContextFactory;
@@ -86,38 +83,22 @@ public PluginFactoryExt(final Ruby runtime, final RubyClass metaClass) {
         this.pluginResolver = pluginResolver;
     }
 
-    @JRubyMethod(required = 4)
-    public PluginFactoryExt initialize(final ThreadContext context,
-                                       final IRubyObject[] args) {
-        return init(
-                args[0].toJava(PipelineIR.class),
-                (PluginMetricsFactoryExt) args[1],
-                (ExecutionContextFactoryExt) args[2],
-                (RubyClass) args[3],
-                EnvironmentVariableProvider.defaultProvider(),
-                Engine.RUBY
-        );
-    }
-
     public PluginFactoryExt init(final PipelineIR lir,
                                      final PluginMetricsFactoryExt metrics,
                                      final ExecutionContextFactoryExt executionContextFactoryExt,
-                                     final RubyClass filterClass,
-                                     final Engine engine) {
-        return this.init(lir, metrics, executionContextFactoryExt, filterClass, EnvironmentVariableProvider.defaultProvider(), engine);
+                                     final RubyClass filterClass) {
+        return this.init(lir, metrics, executionContextFactoryExt, filterClass, EnvironmentVariableProvider.defaultProvider());
     }
 
     PluginFactoryExt init(final PipelineIR lir,
                           final PluginMetricsFactoryExt metrics,
                           final ExecutionContextFactoryExt executionContextFactoryExt,
                           final RubyClass filterClass,
-                          final EnvironmentVariableProvider envVars,
-                          final Engine engine) {
+                          final EnvironmentVariableProvider envVars) {
         this.lir = lir;
         this.metrics = metrics;
         this.executionContextFactory = executionContextFactoryExt;
         this.filterDelegatorClass = filterClass;
-        this.engine = engine;
         this.pluginCreatorsRegistry.put(PluginLookup.PluginType.INPUT, new InputPluginCreator(this));
         this.pluginCreatorsRegistry.put(PluginLookup.PluginType.CODEC, new CodecPluginCreator());
         this.pluginCreatorsRegistry.put(PluginLookup.PluginType.FILTER, new FilterPluginCreator());
@@ -259,12 +240,6 @@ private IRubyObject plugin(final ThreadContext context,
                 return pluginInstance;
             }
         } else {
-            if (engine != Engine.JAVA) {
-                String err = String.format("Cannot start the Java plugin '%s' in the %s execution engine." +
-                        " The Java execution engine is required to run Java plugins.", name, engine);
-                throw new IllegalStateException(err);
-            }
-
             AbstractPluginCreator<? extends Plugin> pluginCreator = pluginCreatorsRegistry.get(type);
             if (pluginCreator == null) {
                 throw new IllegalStateException("Unable to create plugin: " + pluginClass.toReadableString());
diff --git a/logstash-core/src/test/java/org/logstash/plugins/factory/PluginFactoryExtTest.java b/logstash-core/src/test/java/org/logstash/plugins/factory/PluginFactoryExtTest.java
index 1f621b90b85..e1137f5654e 100644
--- a/logstash-core/src/test/java/org/logstash/plugins/factory/PluginFactoryExtTest.java
+++ b/logstash-core/src/test/java/org/logstash/plugins/factory/PluginFactoryExtTest.java
@@ -30,7 +30,6 @@
 import org.logstash.config.ir.InvalidIRException;
 import org.logstash.config.ir.PipelineIR;
 import org.logstash.config.ir.RubyEnvTestCase;
-import org.logstash.execution.Engine;
 import org.logstash.instrument.metrics.NamespacedMetricExt;
 import org.logstash.plugins.MetricTestCase;
 import org.logstash.plugins.PluginLookup;
@@ -94,7 +93,7 @@ public void testPluginIdResolvedWithEnvironmentVariables() throws InvalidIRExcep
         envVars.put("CUSTOM", "test");
         PluginFactoryExt sut = new PluginFactoryExt(RubyUtil.RUBY, RubyUtil.PLUGIN_FACTORY_CLASS,
                 mockPluginResolver);
-        sut.init(pipelineIR, metricsFactory, execContextFactory, RubyUtil.FILTER_DELEGATOR_CLASS, envVars::get, Engine.JAVA);
+        sut.init(pipelineIR, metricsFactory, execContextFactory, RubyUtil.FILTER_DELEGATOR_CLASS, envVars::get);
 
         RubyString pluginName = RubyUtil.RUBY.newString("mockinput");
 
diff --git a/x-pack/lib/monitoring/inputs/metrics/state_event_factory.rb b/x-pack/lib/monitoring/inputs/metrics/state_event_factory.rb
index a8f9b2afb08..2904be00f1d 100644
--- a/x-pack/lib/monitoring/inputs/metrics/state_event_factory.rb
+++ b/x-pack/lib/monitoring/inputs/metrics/state_event_factory.rb
@@ -7,7 +7,7 @@ class StateEventFactory
     require "logstash/config/lir_serializer"
 
     def initialize(pipeline, cluster_uuid, collection_interval = 10)
-      raise ArgumentError, "No pipeline passed in!" unless pipeline.is_a?(LogStash::Pipeline) || pipeline.is_a?(LogStash::JavaPipeline)
+      raise ArgumentError, "No pipeline passed in!" unless pipeline.is_a?(LogStash::JavaPipeline)
 
       pipeline_doc = {"pipeline" => pipeline_data(pipeline)}
 
diff --git a/x-pack/spec/modules/azure/filters/azure_event_spec.rb b/x-pack/spec/modules/azure/filters/azure_event_spec.rb
index a1267119317..2389bacc790 100644
--- a/x-pack/spec/modules/azure/filters/azure_event_spec.rb
+++ b/x-pack/spec/modules/azure/filters/azure_event_spec.rb
@@ -7,9 +7,16 @@
 require 'logstash/json'
 require 'filters/azure_event'
 require 'logstash/config/pipeline_config'
-
+require_relative '../../../../../logstash-core/spec/support/pipeline/pipeline_helpers'
 
 describe LogStash::Filters::AzureEvent do
+  extend PipelineHelpers
+
+  let(:settings) do
+    s = LogStash::SETTINGS.clone
+    s.set_value("pipeline.workers", 1)
+    s
+  end
 
   describe "Parses the admin activity log" do
     let(:config) do
@@ -25,7 +32,7 @@
 
     # as documented
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/administrative1.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("DD042F02-6B3E-4F79-939A-6A381FFED3C0")
       expect(subject.get("[azure][resource_group]")).to eq("MYRESOURCEGROUP")
@@ -51,7 +58,7 @@
       end
       # as observed
       file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/administrative2.json'))
-      sample(LogStash::Json.load(file)) do
+      sample_one(LogStash::Json.load(file)) do
         expect(subject).to include("resourceId")
         expect(subject.get("[azure][subscription]")).to eq("9103C2E0-A392-4CE3-BADD-E50F19378DEB")
         expect(subject.get("[azure][resource_group]")).to eq("MYLINUXVMRG")
@@ -67,7 +74,7 @@
 
       # as observed, missing the resource group, type, and name
       file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/administrative3.json'))
-      sample(LogStash::Json.load(file)) do
+      sample_one(LogStash::Json.load(file)) do
         expect(subject).to include("resourceId")
         expect(subject.get("[azure][subscription]")).to eq("872F2E12-6CCC-4EAD-8D3C-AC833009C1A4")
         expect(subject.get("[azure][resource_group]")).to be_nil
@@ -97,7 +104,7 @@
 
     # as documented
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/service_health1.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("MYSUBSCRIPTIONID")
       expect(subject.get("[azure][resource_group]")).to be_nil
@@ -112,7 +119,7 @@
 
     # as observed
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/service_health2.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("13100E9F-5DCE-4686-B4F4-FF997D407A75")
       expect(subject.get("[azure][resource_group]")).to be_nil
@@ -140,7 +147,7 @@
 
     # as documented
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/security1.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("D4742BB8-C279-4903-9653-9858B17D0C2E")
       expect(subject.get("[azure][resource_group]")).to be_nil
@@ -155,7 +162,7 @@
 
     # as observed
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/security2.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("F628BA4C-F07B-4AEB-86CB-C89784BBD9B3")
       expect(subject.get("[azure][resource_group]")).to be_nil
@@ -183,13 +190,13 @@
 
     # as documented
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/auto_scale1.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       assert
     end
 
     # as observed TODO: actually observe this ! (I don't have any examples for autoscale)
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/auto_scale2.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       assert
     end
 
@@ -222,7 +229,7 @@ def assert
 
     # as documented
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/alert1.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("MYSUBSCRIPTIONID")
       expect(subject.get("[azure][resource_group]")).to eq("MYRESOURCEGROUP")
@@ -237,7 +244,7 @@ def assert
 
     # as observed
     file = File.read(File.join(File.dirname(__FILE__), '../samples/activity_log/alert2.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("C27F13D5-E19A-4A4C-8415-4056C7C752BC")
       expect(subject.get("[azure][resource_group]")).to eq("DEFAULT-ACTIVITYLOGALERTS")
@@ -265,7 +272,7 @@ def assert
 
     # sql diagnostic
     file = File.read(File.join(File.dirname(__FILE__), '../samples/sql_diagnostics/database_wait_statistics.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("741FD6F5-9FB8-462C-97C3-3HF4CH23HC2A")
       expect(subject.get("[azure][resource_group]")).to eq("GO5RG")
@@ -298,7 +305,7 @@ def assert
     end
 
     file = File.read(File.join(File.dirname(__FILE__), '../samples/sql_diagnostics/blocks.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("851FD0F5-9GB8-552C-41C3-3TH4FA231C3A")
       expect(subject.get("[azure][resource_group]")).to eq("DEMTP789")
@@ -330,7 +337,7 @@ def assert
     end
 
     file = File.read(File.join(File.dirname(__FILE__), '../samples/sql_diagnostics/errors.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("846FY0F5-9C8H-452C-95C3-555555666C2A")
       expect(subject.get("[azure][resource_group]")).to eq("OPT489")
@@ -362,7 +369,7 @@ def assert
     end
 
     file = File.read(File.join(File.dirname(__FILE__), '../samples/sql_diagnostics/timeouts.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("E3E7F07F-161E-4591-BB76-711473D4940C")
       expect(subject.get("[azure][resource_group]")).to eq("ST76")
@@ -394,7 +401,7 @@ def assert
     end
 
     file = File.read(File.join(File.dirname(__FILE__), '../samples/sql_diagnostics/querystore_runtime_statistics.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("CBB8D135-BF4F-4792-B835-5872F7EAC917")
       expect(subject.get("[azure][resource_group]")).to eq("WWD66")
@@ -426,7 +433,7 @@ def assert
     end
 
     file = File.read(File.join(File.dirname(__FILE__), '../samples/sql_diagnostics/querystore_wait_statistics.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("70E322AB-5B33-40C7-AD1E-88B2B7CFA236")
       expect(subject.get("[azure][resource_group]")).to eq("W66PT")
@@ -458,7 +465,7 @@ def assert
     end
 
     file = File.read(File.join(File.dirname(__FILE__), '../samples/sql_diagnostics/metric.json'))
-    sample(LogStash::Json.load(file)) do
+    sample_one(LogStash::Json.load(file)) do
       expect(subject).to include("resourceId")
       expect(subject.get("[azure][subscription]")).to eq("D427EE51-17CB-441A-9363-07390D5DC79E")
       expect(subject.get("[azure][resource_group]")).to eq("RG5")
@@ -488,7 +495,7 @@ def assert
       CONFIG
     end
 
-    sample("{'a': 'b'}") do
+    sample_one("{'a': 'b'}") do
       expect(subject).to include("tags")
       expect(subject.get("[tags][0]")).to eq("_azure_event_failure")
     end
diff --git a/x-pack/spec/monitoring/inputs/metrics/state_event_factory_spec.rb b/x-pack/spec/monitoring/inputs/metrics/state_event_factory_spec.rb
index 5c6e0cc74d5..98399fc6b09 100644
--- a/x-pack/spec/monitoring/inputs/metrics/state_event_factory_spec.rb
+++ b/x-pack/spec/monitoring/inputs/metrics/state_event_factory_spec.rb
@@ -47,7 +47,7 @@
     LogStash::SETTINGS.set_value("monitoring.enabled", false)
   end
 
-  let(:pipeline) { LogStash::Pipeline.new(config) }
+  let(:pipeline) { LogStash::JavaPipeline.new(config) }
 
   subject(:sut) { described_class.new(pipeline, "funky_cluster_uuid") }
 
