diff --git a/.gitignore b/.gitignore
index fcc2c1133b7..7121c2debbd 100644
--- a/.gitignore
+++ b/.gitignore
@@ -4,7 +4,6 @@ pkg/*.deb
 pkg/*.rpm
 *.class
 .rbx
-Gemfile.lock
 .rbx
 *.tar.gz
 *.jar
@@ -22,3 +21,7 @@ data
 etc/jira-output.conf
 coverage/*
 .VERSION.mk
+.idea/*
+spec/reports
+rspec.xml
+.install-done
diff --git a/.travis.yml b/.travis.yml
deleted file mode 100644
index 72700b92444..00000000000
--- a/.travis.yml
+++ /dev/null
@@ -1,12 +0,0 @@
-language: ruby
-rvm:
-  - jruby-19mode
-jdk:
-  - oraclejdk7
-  - openjdk7
-
-script:
-  - JRUBY_OPTS=--debug COVERAGE=true GEM_HOME=./vendor/bundle/jruby/1.9 GEM_PATH= ./vendor/bundle/jruby/1.9/bin/rspec spec/support/*.rb spec/filters/*.rb spec/examples/*.rb spec/codecs/*.rb spec/conditionals/*.rb spec/event.rb 
-install: 
-  - ruby gembag.rb
-  - make vendor-geoip
diff --git a/CHANGELOG b/CHANGELOG
index f050f172abf..22c1ab03714 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -1,3 +1,94 @@
+1.5.0.beta1
+  # general
+  - Performance improvements: Logstash 1.5.0 is much faster -- we have improved the throughput 
+    of grok filter in some cases by 100%. In our benchmark testing, using only grok filter and
+    ingesting apache logs, throughput increased from 34K eps to 50K eps. 
+    JSON serialization/deserialization are now implemented using JrJackson library which 
+    improved performance significantly. Ingesting JSON events 1.3KB in size measured a throughput
+    increase from 16Keps to 30K eps. With events 45KB in size, throughput increased from 
+    850 eps to 3.5K eps
+  - Allow spaces in field references like [hello world] (#1513)
+  - Add Plugin manager functionality to Logstash which allows to install, delete and 
+    update Logstash plugins
+  - Remove ability to run multiple subcommands from bin/logstash like 
+    bin/logstash agent -f something.conf -- web (#1747)
+  - Fixed Logstash crashing on converting from ASCII to UTF-8. This was caused by charset
+    conversion issues in input codec (LOGSTASH-1789)
+  - Allow storing 'metadata' to an event which is not sent/encoded on output. This eliminates
+    the need for intermediate fields for example, while using date filter. (#1834)
+  - Accept file and http uri in -f command line option for specifying config files (#1873)
+  - Filters that generated events (multiline, clone, split, metrics) now propagate those events 
+    correctly to future conditionals (#1431)
+  - Bump Kibana version to 3.1.2
+  - Fixed file descriptor leaks when using HTTP. The fix prevents Logstash from stalling, and
+    in some cases crashing from out-of-memory errors (#1604, LOGSTASH-892)
+
+  # input
+  - Lumberjack: fixed Logstash crashes with Java Out Of Memory because of TCP 
+    thread leaks (#LOGSTASH-2168)
+  - TCP: fixed connection threads leak (#1509)
+  - Stdin: prevent overwrite of host field if already present in Event (#1668)
+  - S3: AWS credentials can be specfied through environment variables (#1619)
+  - Kafka: merge @joekiller's plugin to Logstash to get events from Kafka (#1472)
+  - RabbitMQ: fixed march_hare client uses incorrect connection url (LOGSTASH-2276)
+  - RabbitMQ: use Bunny 1.5.0+ (#1894)
+  - Twitter: added improvements, robustness, fixes. full_tweet option now works, we handle 
+    Twitter rate limiting errors (#1471)
+  - TCP: fixed input host field also contains source port (LOGSTASH-1849)
+  - Syslog: if input does not match syslog format, add tag _grokparsefailure_sysloginputplugin
+    which can be used to debug (#1593)
+
+  # filter
+  - Mutate: gsub evaluates variables like %{format} in the replacement text (#1529)
+  - Grok: "break_on_match => false" option now works correctly (#1547)
+  - Grok: allow user@hostname in commonapache log pattern (#1500 #1736)
+  - Grok: use optimized ruby-grok library which improves throughput in some cases by 50% (#1657)
+  - Date: fixed match defaults to 1970-01-01 when none of the formats matches and 
+    UNIX format is present in the list (#1236, LOGSTASH-1597)
+  - Mutate: fixed confusing error message for invalid type conversion (#1656, LOGSTASH-2003)
+  - Date: support parsing almost-ISO8601 patterns like 2001-11-06 20:45:45.123-0000 (without a T)
+    which does not match %{TIMESTAMP_ISO8601}
+  - KV: allows dynamic include/exclude keys. For example, if an event has a key field and the user 
+    wants to parse out a value using the kv filter, the user should be able to 
+    include_keys: [ "%{key}" ]
+  - DNS: fixed add_tag adds tags even if filter was unsuccessful (#1785)
+  - XML: fixed UndefinedConversionError with UTF-8 encoding (LOGSTASH-2246)
+  - Mutate: fixed nested field notation for convert option like 
+    'convert => [ "[a][0]", "float" ]' (#1401)
+
+  # output
+  - RabbitMQ: fixed crash while running Logstash for longer periods, typically when there's no
+    traffic on the logstash<->rabbitmq socket (LOGSTASH-1886)
+  - Statsd: fixed issue of converting very small float numbers to scientific notation 
+    like 9.3e-05 (#1670)
+  - Fixed undefined method error when conditional on an output (#LOGSTASH-2288)
+  - Elasticsearch output: Added support for multiple hosts in configuration and enhanced stability
+  - Elasticsearch output: Logstash will not create a message.raw field by default now. Message field 
+    is not_analyzed by Elasticsearch and adding a multi-field was essentially doubling the disk space
+    required, with no benefit
+  - Elasticsearch output: We have improved the security of the Elasticsearch output, input, and filter by
+    adding authentication and transport encryption support. In http protocol you can configure SSL/TLS to 
+    enable encryption and HTTP basic authentication to provide a username and password while making
+    requests (#1453)
+  - Kafka: merge @joekiller's plugin into Logstash to produce events to Kafka (#1472)
+  - File: Added enhancements and validations for destination path. Absolute path cannot start with a dynamic
+    string like /%{myfield}/, /test-%{myfield}/
+
+1.4.2 (June 24, 2014)
+  # general
+  - fixed path issues when invoking bin/logstash outside its home directory
+
+  # input
+  - bugfix: generator: fixed stdin option support
+  - bugfix: file: fixed debian 7 path issue
+
+  # codecs
+  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
+  - improvement: collectd: add support for NaN values
+
+  # outputs
+  - improvement: nagios_nsca: fix external command invocation to avoid shell escaping
+
 1.4.1 (May 6, 2014)
   # General
   - bumped Elasticsearch to 1.1.1 and Kibana to 3.0.1
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index 99e6f382f82..0249977e1e6 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -26,7 +26,7 @@ irc.freenode.org and ask for help there!
 
 ## Have an Idea or Feature Request?
 
-* File a ticket on [jira](https://logstash.jira.com/secure/Dashboard.jspa), or email the
+* File a ticket on [github](https://github.com/elasticsearch/logstash/issues), or email the
   [mailing list](http://groups.google.com/group/logstash-users), or email
   me personally (jls@semicomplete.com) if that is more comfortable.
 
@@ -34,14 +34,14 @@ irc.freenode.org and ask for help there!
 
 If you think you found a bug, it probably is a bug.
 
-* File it on [jira](https://logstash.jira.com/secure/Dashboard.jspa)
+* File it on [github](https://github.com/elasticsearch/logstash/issues)
 * or the [mailing list](http://groups.google.com/group/logstash-users).
 
 # Contributing Documentation and Code Changes
 
 If you have a bugfix or new feature that you would like to contribute to
 logstash, and you think it will take more than a few minutes to produce the fix
-(ie; write code), it is worth discussing the change with the logstash users and developers first! You can reach us via [jira](https://logstash.jira.com/secure/Dashboard.jspa), the [mailing list](http://groups.google.com/group/logstash-users), or via IRC (#logstash on freenode irc)
+(ie; write code), it is worth discussing the change with the logstash users and developers first! You can reach us via [github](https://github.com/elasticsearch/logstash/issues), the [mailing list](http://groups.google.com/group/logstash-users), or via IRC (#logstash on freenode irc)
 
 ## Contribution Steps
 
@@ -55,7 +55,7 @@ logstash, and you think it will take more than a few minutes to produce the fix
 3. Send a pull request! Push your changes to your fork of the repository and
    [submit a pull
    request](https://help.github.com/articles/using-pull-requests). In the pull
-   request, describe what your changes do and mention any jira issues related
+   request, describe what your changes do and mention any bugs/issues related
    to the pull request.
 
 
diff --git a/CONTRIBUTORS b/CONTRIBUTORS
index 4bf4e5194ea..e14761e28fe 100644
--- a/CONTRIBUTORS
+++ b/CONTRIBUTORS
@@ -73,6 +73,7 @@ Contributors:
 * Bernd Ahlers (bernd)
 * Andrea Forni (andreaforni)
 * Leandro Moreira (leandromoreira)
+* Hao Chen (haoch)
 
 Note: If you've sent me patches, bug reports, or otherwise contributed to
 logstash, and you aren't on the list above and want to be, please let me know
diff --git a/LICENSE b/LICENSE
index b3e30706bd8..f8b711d55df 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,14 +1,13 @@
-Copyright 2009-2013 Jordan Sissel, Pete Fritchman, and contributors.
+Copyright (c) 2012-2014 Elasticsearch <http://www.elasticsearch.org>
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
 You may obtain a copy of the License at
 
-http://www.apache.org/licenses/LICENSE-2.0
+    http://www.apache.org/licenses/LICENSE-2.0
 
 Unless required by applicable law or agreed to in writing, software
 distributed under the License is distributed on an "AS IS" BASIS,
 WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 See the License for the specific language governing permissions and
 limitations under the License.
-
diff --git a/Makefile b/Makefile
index f8b63897d66..db263b80aff 100644
--- a/Makefile
+++ b/Makefile
@@ -1,385 +1,2 @@
-# Requirements to build:
-#   rsync
-#   wget or curl
-#
-JRUBY_VERSION=1.7.11
-ELASTICSEARCH_VERSION=1.1.1
-
-WITH_JRUBY=java -jar $(shell pwd)/$(JRUBY) -S
-JRUBY=vendor/jar/jruby-complete-$(JRUBY_VERSION).jar
-JRUBY_URL=http://jruby.org.s3.amazonaws.com/downloads/$(JRUBY_VERSION)/jruby-complete-$(JRUBY_VERSION).jar
-JRUBY_CMD=bin/logstash env java -jar $(JRUBY)
-
-ELASTICSEARCH_URL=http://download.elasticsearch.org/elasticsearch/elasticsearch
-ELASTICSEARCH=vendor/jar/elasticsearch-$(ELASTICSEARCH_VERSION)
-TYPESDB=vendor/collectd/types.db
-COLLECTD_VERSION=5.4.0
-TYPESDB_URL=https://collectd.org/files/collectd-$(COLLECTD_VERSION).tar.gz
-GEOIP=vendor/geoip/GeoLiteCity.dat
-GEOIP_URL=http://logstash.objects.dreamhost.com/maxmind/GeoLiteCity-2013-01-18.dat.gz
-GEOIP_ASN=vendor/geoip/GeoIPASNum.dat
-GEOIP_ASN_URL=http://logstash.objects.dreamhost.com/maxmind/GeoIPASNum-2014-02-12.dat.gz
-KIBANA_URL=https://download.elasticsearch.org/kibana/kibana/kibana-3.0.1.tar.gz
-PLUGIN_FILES=$(shell find lib -type f| egrep '^lib/logstash/(inputs|outputs|filters|codecs)/[^/]+$$' | egrep -v '/(base|threadable).rb$$|/inputs/ganglia/')
-QUIET=@
-ifeq (@,$(QUIET))
-	QUIET_OUTPUT=> /dev/null 2>&1
-endif
-
-WGET=$(shell which wget 2>/dev/null)
-CURL=$(shell which curl 2>/dev/null)
-
-# OS-specific options
-TARCHECK=$(shell tar --help|grep wildcard|wc -l|tr -d ' ')
-ifeq (0, $(TARCHECK))
-TAR_OPTS=
-else
-TAR_OPTS=--wildcards
-endif
-
-#spec/outputs/graphite.rb spec/outputs/email.rb)
-default:
-	@echo "Make targets you might be interested in:"
-	@echo "  tarball -- builds the tarball package"
-	@echo "  tarball-test -- runs the test suite against the tarball package"
-
-TESTS=$(wildcard spec/*.rb spec/**/*.rb spec/**/**/*.rb)
-
-# The 'version' is generated based on the logstash version, git revision, etc.
-.VERSION.mk: REVISION=$(shell git rev-parse --short HEAD | tr -d ' ')
-.VERSION.mk: RELEASE=$(shell awk -F\" '/LOGSTASH_VERSION/ {print $$2}' lib/logstash/version.rb | tr -d ' ')
-#.VERSION.mk: TAGGED=$(shell git tag --points-at HEAD | egrep '^v[0-9]')
-.VERSION.mk: DEV=$(shell echo $RELEASE | egrep '\.dev$$')
-.VERSION.mk: MODIFIED=$(shell git diff --shortstat --exit-code > /dev/null ; echo $$?)
-.VERSION.mk:
-	$(QUIET)echo "RELEASE=${RELEASE}" > $@
-	$(QUIET)echo "REVISION=${REVISION}" >> $@
-	$(QUIET)echo "DEV=${DEV}" >> $@
-	$(QUIET)echo "MODIFIED=${MODIFIED}" >> $@
-	$(QUIET)if [ -z "${DEV}" ] ; then \
-		if [ "${MODIFIED}" -eq 1 ] ; then \
-			echo "VERSION=${RELEASE}-modified" ; \
-		else \
-			echo "VERSION=${RELEASE}" ; \
-		fi ; \
-	else \
-		if [ "${MODIFIED}" -eq 1 ] ; then \
-			echo "VERSION=${RELEASE}-${REVISION}-modified" ; \
-		else \
-			echo "VERSION=${RELEASE}-${REVISION}" ; \
-		fi ; \
-	fi >> $@
-
--include .VERSION.mk
-
-version:
-	@echo "Version: $(VERSION)"
-
-# Figure out if we're using wget or curl
-.PHONY: wget-or-curl
-wget-or-curl:
-ifeq ($(CURL),)
-ifeq ($(WGET),)
-	@echo "wget or curl are required."
-	exit 1
-else
-DOWNLOAD_COMMAND=wget -q --no-check-certificate -O
-endif
-else
-DOWNLOAD_COMMAND=curl -s -L -k -o
-endif
-
-# Compile config grammar (ragel -> ruby)
-.PHONY: compile-grammar
-compile-grammar: lib/logstash/config/grammar.rb
-lib/logstash/config/grammar.rb: lib/logstash/config/grammar.treetop
-	$(QUIET)$(MAKE) -C lib/logstash/config grammar.rb
-
-.PHONY: clean
-clean:
-	@echo "=> Cleaning up"
-	-$(QUIET)rm -rf .bundle
-	-$(QUIET)rm -rf build
-	-$(QUIET)rm -f pkg/*.deb
-	-$(QUIET)rm .VERSION.mk
-
-.PHONY: vendor-clean
-vendor-clean:
-	-$(QUIET)rm -rf vendor/kibana vendor/geoip vendor/collectd
-	-$(QUIET)rm -rf vendor/jar vendor/ua-parser
-
-.PHONY: clean-vendor
-clean-vendor:
-	-$(QUIET)rm -rf vendor
-
-.PHONY: compile
-compile: compile-grammar compile-runner | build/ruby
-
-.PHONY: compile-runner
-compile-runner: build/ruby/logstash/runner.class
-build/ruby/logstash/runner.class: lib/logstash/runner.rb | build/ruby $(JRUBY)
-	$(QUIET)(cd lib; java -jar ../$(JRUBY) -rjruby/jrubyc -e 'exit JRuby::Compiler::compile_argv(ARGV)' -- -t ../build/ruby logstash/runner.rb)
-
-.PHONY: copy-ruby-files
-copy-ruby-files: | build/ruby
-	@# Copy lib/ and test/ files to the root
-	$(QUIET)rsync -a --include "*/" --include "*.rb" --include "*.yaml" --exclude "*" ./lib/ ./test/ ./build/ruby
-	$(QUIET)rsync -a ./spec ./build/ruby
-	$(QUIET)rsync -a ./locales ./build/ruby
-	@# Delete any empty directories copied by rsync.
-	$(QUIET)find ./build/ruby -type d -empty -delete
-
-vendor:
-	$(QUIET)mkdir $@
-
-vendor/jar: | vendor
-	$(QUIET)mkdir $@
-
-vendor-jruby: $(JRUBY)
-
-$(JRUBY): | vendor/jar
-	$(QUIET)echo "=> Downloading jruby $(JRUBY_VERSION)"
-	$(QUIET)$(DOWNLOAD_COMMAND) $@ $(JRUBY_URL)
-
-vendor/jar/elasticsearch-$(ELASTICSEARCH_VERSION).tar.gz: | wget-or-curl vendor/jar
-	@echo "=> Fetching elasticsearch"
-	$(QUIET)$(DOWNLOAD_COMMAND) $@ $(ELASTICSEARCH_URL)/elasticsearch-$(ELASTICSEARCH_VERSION).tar.gz
-
-vendor/jar/graphtastic-rmiclient.jar: | wget-or-curl vendor/jar
-	@echo "=> Fetching graphtastic rmi client jar"
-	$(QUIET)$(DOWNLOAD_COMMAND) $@ http://cloud.github.com/downloads/NickPadilla/GraphTastic/graphtastic-rmiclient.jar
-
-.PHONY: vendor-elasticsearch
-vendor-elasticsearch: $(ELASTICSEARCH)
-$(ELASTICSEARCH): $(ELASTICSEARCH).tar.gz | vendor/jar
-	@echo "=> Pulling the jars out of $<"
-	$(QUIET)tar -C $(shell dirname $@) -xf $< $(TAR_OPTS) --exclude '*sigar*' \
-		'elasticsearch-$(ELASTICSEARCH_VERSION)/lib/*.jar'
-
-vendor/geoip: | vendor
-	$(QUIET)mkdir $@
-
-.PHONY: vendor-geoip
-vendor-geoip: $(GEOIP) $(GEOIP_ASN)
-$(GEOIP): | vendor/geoip
-	$(QUIET)$(DOWNLOAD_COMMAND) $@.tmp.gz $(GEOIP_URL)
-	$(QUIET)gzip -dc $@.tmp.gz > $@.tmp
-	$(QUIET)rm "$@.tmp.gz"
-	$(QUIET)mv $@.tmp $@
-
-$(GEOIP_ASN): | vendor/geoip
-	$(QUIET)$(DOWNLOAD_COMMAND) $@.tmp.gz $(GEOIP_ASN_URL)
-	$(QUIET)gzip -dc $@.tmp.gz > $@.tmp
-	$(QUIET)rm "$@.tmp.gz"
-	$(QUIET)mv $@.tmp $@
-
-vendor/collectd: | vendor
-	$(QUIET)mkdir $@
-
-.PHONY: vendor-collectd
-vendor-collectd: $(TYPESDB)
-$(TYPESDB): | vendor/collectd
-	$(QUIET)$(DOWNLOAD_COMMAND) $@.tar.gz $(TYPESDB_URL)
-	$(QUIET)tar zxf $@.tar.gz -O "collectd-$(COLLECTD_VERSION)/src/types.db" > $@
-	$(QUIET)rm $@.tar.gz
-
-# Always run vendor/bundle
-.PHONY: fix-bundler
-fix-bundler:
-	-$(QUIET)rm -rf .bundle
-
-.PHONY: vendor-gems
-vendor-gems: | vendor/bundle
-
-.PHONY: vendor/bundle
-vendor/bundle: | vendor $(JRUBY)
-	@echo "=> Ensuring ruby gems dependencies are in $@..."
-	$(QUIET)USE_JRUBY=1 bin/logstash deps $(QUIET_OUTPUT)
-	@# Purge any junk that fattens our jar without need!
-	@# The riak gem includes previous gems in the 'pkg' dir. :(
-	-$(QUIET)rm -rf $@/jruby/1.9/gems/riak-client-1.0.3/pkg
-	@# Purge any rspec or test directories
-	-$(QUIET)rm -rf $@/jruby/1.9/gems/*/spec $@/jruby/1.9/gems/*/test
-	@# Purge any comments in ruby code.
-	@#-find $@/jruby/1.9/gems/ -name '*.rb' | xargs -n1 sed -i -e '/^[ \t]*#/d; /^[ \t]*$$/d'
-
-.PHONY: build
-build:
-	-$(QUIET)mkdir -p $@
-
-build/ruby: | build
-	-$(QUIET)mkdir -p $@
-
-vendor/ua-parser/: | build
-	$(QUIET)mkdir $@
-
-vendor/ua-parser/regexes.yaml: | vendor/ua-parser/
-	@echo "=> Fetching ua-parser regexes.yaml"
-	$(QUIET)$(DOWNLOAD_COMMAND) $@ https://raw.github.com/tobie/ua-parser/master/regexes.yaml
-
-.PHONY: test
-test: QUIET_OUTPUT=
-test: | $(JRUBY) vendor-elasticsearch vendor-geoip vendor-collectd vendor-gems
-	$(SPEC_ENV) USE_JRUBY=1 bin/logstash rspec $(SPEC_OPTS) --order rand --fail-fast $(TESTS)
-
-.PHONY: reporting-test
-reporting-test: SPEC_ENV=JRUBY_OPTS=--debug COVERAGE=TRUE
-reporting-test: SPEC_OPTS=--format CI::Reporter::RSpec
-reporting-test: | test
-
-.PHONY: docs
-docs: docgen doccopy docindex
-
-doccopy: $(addprefix build/,$(shell find docs -type f | grep '^docs/')) | build/docs
-docindex: build/docs/index.html
-
-docgen: $(addprefix build/docs/,$(subst lib/logstash/,,$(subst .rb,.html,$(PLUGIN_FILES))))
-docgen: build/docs/tutorials/getting-started-with-logstash.md
-
-build/docs: build
-	$(QUIET)-mkdir $@
-
-build/docs/tutorials: build/docs
-	$(QUIET)-mkdir $@
-
-
-build/docs/inputs build/docs/filters build/docs/outputs build/docs/codecs: | build/docs
-	$(QUIET)-mkdir $@
-
-build/docs/tutorials/getting-started-with-logstash.md: build/docs/tutorials/getting-started-with-logstash.xml | build/docs/tutorials
-	$(QUIET)( \
-		echo "---"; \
-		echo "title: Metrics from Logs - logstash"; \
-		echo "layout: content_right"; \
-		echo "---"; \
-		pandoc -f docbook -t markdown $< \
-	) \
-	| sed -e 's/%VERSION%/$(VERSION)/g' \
-	| sed -e 's/%ELASTICSEARCH_VERSION%/$(ELASTICSEARCH_VERSION)/g' > $@
-
-build/docs/tutorials/getting-started-with-logstash.xml: docs/tutorials/getting-started-with-logstash.asciidoc | build/docs/tutorials
-	$(QUIET)asciidoc -b docbook -o $@ $<
-
-build/docs/inputs/%.html: lib/logstash/inputs/%.rb docs/docgen.rb docs/plugin-doc.html.erb | build/docs/inputs
-	$(QUIET)$(JRUBY_CMD) docs/docgen.rb -o build/docs $<
-	$(QUIET)sed -i -e 's/%VERSION%/$(VERSION)/g' $@
-	$(QUIET)sed -i -e 's/%ELASTICSEARCH_VERSION%/$(ELASTICSEARCH_VERSION)/g' $@
-build/docs/filters/%.html: lib/logstash/filters/%.rb docs/docgen.rb docs/plugin-doc.html.erb | build/docs/filters
-	$(QUIET)$(JRUBY_CMD) docs/docgen.rb -o build/docs $<
-	$(QUIET)sed -i -e 's/%VERSION%/$(VERSION)/g' $@
-	$(QUIET)sed -i -e 's/%ELASTICSEARCH_VERSION%/$(ELASTICSEARCH_VERSION)/g' $@
-build/docs/outputs/%.html: lib/logstash/outputs/%.rb docs/docgen.rb docs/plugin-doc.html.erb | build/docs/outputs
-	$(QUIET)$(JRUBY_CMD) docs/docgen.rb -o build/docs $<
-	$(QUIET)sed -i -e 's/%VERSION%/$(VERSION)/g' $@
-	$(QUIET)sed -i -e 's/%ELASTICSEARCH_VERSION%/$(ELASTICSEARCH_VERSION)/g' $@
-build/docs/codecs/%.html: lib/logstash/codecs/%.rb docs/docgen.rb docs/plugin-doc.html.erb | build/docs/codecs
-	$(QUIET)$(JRUBY_CMD) docs/docgen.rb -o build/docs $<
-	$(QUIET)sed -i -e 's/%VERSION%/$(VERSION)/g' $@
-
-build/docs/%: docs/% lib/logstash/version.rb Makefile
-	@echo "Copying $< (to $@)"
-	-$(QUIET)mkdir -p $(shell dirname $@)
-	$(QUIET)cp $< $@
-	$(QUIET)case "$(suffix $<)" in \
-		.gz|.bz2|.png|.jpg) ;; \
-		*) \
-			sed -i -e 's/%VERSION%/$(VERSION)/g' $@ ; \
-			sed -i -e 's/%ELASTICSEARCH_VERSION%/$(ELASTICSEARCH_VERSION)/g' $@ ; \
-			;; \
-	esac
-
-build/docs/index.html: $(addprefix build/docs/,$(subst lib/logstash/,,$(subst .rb,.html,$(PLUGIN_FILES))))
-build/docs/index.html: docs/generate_index.rb lib/logstash/version.rb docs/index.html.erb Makefile
-	@echo "Building documentation index.html"
-	$(QUIET)$(JRUBY_CMD) $< build/docs > $@
-	$(QUIET)sed -i -e 's/%VERSION%/$(VERSION)/g' $@
-	$(QUIET)sed -i -e 's/%ELASTICSEARCH_VERSION%/$(ELASTICSEARCH_VERSION)/g' $@
-
-.PHONY: patterns
-patterns:
-	curl https://nodeload.github.com/logstash/grok-patterns/tarball/master | tar zx
-	mv logstash-grok-patterns*/* patterns/
-	rm -rf logstash-grok-patterns*
-
-## JIRA Interaction section
-JIRACLI=/path/to/your/jira-cli-3.1.0/jira.sh
-
-sync-jira-components: $(addprefix create/jiracomponent/,$(subst lib/logstash/,,$(subst .rb,,$(PLUGIN_FILES))))
-	-$(QUIET)$(JIRACLI) --action run --file tmp_jira_action_list --continue > /dev/null 2>&1
-	$(QUIET)rm tmp_jira_action_list
-
-create/jiracomponent/%:
-	$(QUIET)echo "--action addComponent --project LOGSTASH --name $(subst create/jiracomponent/,,$@)" >> tmp_jira_action_list
-
-## Release note section (up to you if/how/when to integrate in docs)
-# Collect the details of:
-#  - merged pull request from GitHub since last release
-#  - issues for FixVersion from JIRA
-
-# Note on used Github logic
-# We parse the commit between the last tag (should be the last release) and HEAD
-# to extract all the notice about merged pull requests.
-
-# Note on used JIRA release note URL
-# The JIRA Release note list all issues (even open ones)
-# with Fix Version assigned to target version
-# So one must verify manually that there is no open issue left (TODO use JIRACLI)
-
-# This is the ID for a version item in jira, can be obtained by CLI
-# or through the Version URL https://logstash.jira.com/browse/LOGSTASH/fixforversion/xxx
-JIRA_VERSION_ID=10820
-
-releaseNote:
-	-$(QUIET)rm releaseNote.html
-	$(QUIET)curl -si "https://logstash.jira.com/secure/ReleaseNote.jspa?version=$(JIRA_VERSION_ID)&projectId=10020" | sed -n '/<textarea.*>/,/<\/textarea>/p' | grep textarea -v >> releaseNote.html
-	$(QUIET)$(JRUBY_CMD) pull_release_note.rb
-
-package: build/logstash-$(VERSION).tar.gz
-	(cd pkg; \
-		./build.sh ubuntu 12.04; \
-		./build.sh centos 6 \
-	)
-
-vendor/kibana: | vendor
-	@echo "=> Fetching kibana"
-	$(QUIET)mkdir vendor/kibana || true
-	$(DOWNLOAD_COMMAND) - $(KIBANA_URL) | tar -C $@ -zx --strip-components=1
-
-build/tarball: | build
-	mkdir $@
-build/tarball/logstash-%: | build/tarball
-	mkdir $@
-
-show:
-	echo $(VERSION)
-
-.PHONY: prepare-tarball
-prepare-tarball tarball zip: WORKDIR=build/tarball/logstash-$(VERSION)
-prepare-tarball: vendor/kibana $(ELASTICSEARCH) $(JRUBY) $(GEOIP) $(TYPESDB) vendor-gems
-prepare-tarball: vendor/ua-parser/regexes.yaml
-prepare-tarball:
-	@echo "=> Preparing tarball"
-	$(QUIET)$(MAKE) $(WORKDIR)
-	$(QUIET)rsync -a --relative bin lib spec locales patterns vendor/bundle/jruby vendor/geoip vendor/jar vendor/kibana vendor/ua-parser vendor/collectd LICENSE README.md --exclude 'vendor/bundle/jruby/1.9/cache' --exclude 'vendor/bundle/jruby/1.9/gems/*/doc' --exclude 'vendor/jar/elasticsearch-$(ELASTICSEARCH_VERSION).tar.gz'  $(WORKDIR)
-	$(QUIET)sed -i -e 's/^LOGSTASH_VERSION = .*/LOGSTASH_VERSION = "$(VERSION)"/' $(WORKDIR)/lib/logstash/version.rb
-	$(QUIET)sed -i -e 's/%JRUBY_VERSION%/$(JRUBY_VERSION)/' $(WORKDIR)/bin/logstash.bat
-
-.PHONY: tarball
-tarball: | build/logstash-$(VERSION).tar.gz
-build/logstash-$(VERSION).tar.gz: | prepare-tarball
-	$(QUIET)tar -C $$(dirname $(WORKDIR)) -c $$(basename $(WORKDIR)) \
-		| gzip -9c > $@
-	@echo "=> tarball ready: $@"
-
-.PHONY: zip
-zip: | build/logstash-$(VERSION).zip
-build/logstash-$(VERSION).zip: | prepare-tarball
-	$(QUIET)(cd $$(dirname $(WORKDIR)); find $$(basename $(WORKDIR)) | zip $(PWD)/$@ -@ -9)$(QUIET_OUTPUT)
-	@echo "=> zip ready: $@"
-
-.PHONY: tarball-test
-tarball-test: #build/logstash-$(VERSION).tar.gz
-	$(QUIET)-rm -rf build/test-tarball/
-	$(QUIET)mkdir -p build/test-tarball/
-	tar -C build/test-tarball --strip-components 1 -xf build/logstash-$(VERSION).tar.gz
-	(cd build/test-tarball; USE_JRUBY=1 bin/logstash rspec $(TESTS) --fail-fast)
+%: 
+	rake $@
diff --git a/README.md b/README.md
index 347478df719..4d33f674f97 100644
--- a/README.md
+++ b/README.md
@@ -1,4 +1,4 @@
-# Logstash
+# Logstash [![Code Climate](https://codeclimate.com/github/elasticsearch/logstash/badges/gpa.svg)](https://codeclimate.com/github/elasticsearch/logstash)
 
 Logstash is a tool for managing events and logs. You can use it to collect
 logs, parse them, and store them for later use (like, for searching). Speaking
@@ -29,21 +29,12 @@ You can also find documentation on the <http://logstash.net> site.
 
 ## Developing
 
-If you don't have JRuby already (or don't use rvm, rbenv, etc), you can have `bin/logstash` fetch it for you by setting `USE_JRUBY`:
+To get started, you'll need *any* ruby available and it should come with the `rake` tool.
 
-    USE_JRUBY=1 bin/logstash ...
-
-Otherwise, here's how to get started with rvm:
-
-    # Install JRuby with rvm
-    rvm install jruby-1.7.11
-    rvm use jruby-1.7.11
-
-Now install dependencies:
-
-    # Install logstash ruby dependencies
-    bin/logstash deps
+Here's how to get started with Logstash development:
 
+    rake bootstrap
+    
 Other commands:
 
     # to use Logstash gems or libraries in irb, use the following
@@ -53,9 +44,12 @@ Other commands:
     # Run Logstash
     bin/logstash agent [options]
 
-    # If running bin/logstash agent yields complaints about log4j/other things
-    # This will download the elasticsearch jars so Logstash can use them.
-    make vendor-elasticsearch
+Notes about using other rubies. If you don't use rvm, you can probably skip
+this paragraph. Logstash works with other rubies, and if you wish to use your
+own ruby you must set `USE_RUBY=1` in your environment.
+
+We recommend using flatland/drip for faster startup times during development. To
+tell Logstash to use drip, set `USE_DRIP=1` in your environment.
 
 ## Testing
 
@@ -68,14 +62,9 @@ rspec <some spec>` will suffice:
     Finished in 0.123 seconds
     19 examples, 0 failures
 
-Alternately, if you have just built the tarball, you can run the tests
-specifically on those like so:
-
-    make tarball-test
-
 If you want to run all the tests from source, do:
 
-    make test
+    rake test
 
 ## Building
 
@@ -84,11 +73,15 @@ we provide from the Logstash site!
 
 If you want to build the release tarball yourself, run:
 
-    make tarball
+    rake artifact:tar
 
 You can build rpms and debs, if you need those. Building rpms requires you have [fpm](https://github.com/jordansissel/fpm), then do this:
 
-    make package
+    # Build an RPM
+    rake artifact:rpm
+
+    # Build a Debian/Ubuntu package
+    rake artifact:deb
 
 ## Project Principles
 
diff --git a/Rakefile b/Rakefile
new file mode 100644
index 00000000000..d77311081b3
--- /dev/null
+++ b/Rakefile
@@ -0,0 +1,19 @@
+
+$: << File.join(File.dirname(__FILE__), "lib")
+
+task "default" => "help"
+
+task "help" do
+  puts <<HELP
+What do you want to do?
+
+Packaging?
+  `rake artifact:tar`  to build a deployable .tar.gz
+  `rake artifact:rpm`  to build an rpm
+  `rake artifact:deb`  to build an deb
+
+Developing?
+  `rake bootstrap`     installs any dependencies for doing Logstash development
+  `rake vendor:clean`  clean vendored dependencies used for Logstash development
+HELP
+end
diff --git a/acceptance_spec/acceptance/install_spec.rb b/acceptance_spec/acceptance/install_spec.rb
index 2492737d53d..ac01470822a 100644
--- a/acceptance_spec/acceptance/install_spec.rb
+++ b/acceptance_spec/acceptance/install_spec.rb
@@ -5,17 +5,13 @@
   case fact('osfamily')
   when 'RedHat'
     core_package_name    = 'logstash'
-    contrib_package_name = 'logstash-contrib'
     service_name         = 'logstash'
-    core_url             = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-latest.rpm'
-    contrib_url          = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-contrib-latest.rpm'
+    core_url             = 'https://s3-eu-west-1.amazonaws.com/users.eu.elasticsearch.org/electrical/logstash-2.0.0.dev-1.noarch.rpm'
     pid_file             = '/var/run/logstash.pid'
   when 'Debian'
     core_package_name    = 'logstash'
-    contrib_package_name = 'logstash-contrib'
     service_name         = 'logstash'
-    core_url             = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-latest.deb'
-    contrib_url          = 'https://s3-us-west-2.amazonaws.com/build.elasticsearch.org/logstash/master/nightly/logstash-contrib-latest.deb'
+    core_url             = 'https://s3-eu-west-1.amazonaws.com/users.eu.elasticsearch.org/electrical/logstash_2.0.0.dev-1_all.deb'
     pid_file             = '/var/run/logstash.pid'
   end
 
@@ -92,61 +88,5 @@
 
   end
 
-  context "Install Nightly core + contrib packages" do
-
-    it 'should run successfully' do
-      pp = "class { 'logstash': package_url => '#{core_url}', java_install => true, contrib_package_url => '#{contrib_url}', install_contrib => true }
-            logstash::configfile { 'basic_config': content => 'input { tcp { port => 2000 } } output { stdout { } } ' }
-           "
-
-      # Run it twice and test for idempotency
-      apply_manifest(pp, :catch_failures => true)
-      sleep 20
-      expect(apply_manifest(pp, :catch_failures => true).exit_code).to be_zero
-
-    end
-
-    describe package(core_package_name) do
-      it { should be_installed }
-    end
-
-    describe package(contrib_package_name) do
-      it { should be_installed }
-    end
-
-    describe service(service_name) do
-      it { should be_enabled }
-      it { should be_running }
-    end
-
-    describe file(pid_file) do
-      it { should be_file }
-      its(:content) { should match /[0-9]+/ }
-    end
-
-    describe port(2000) do
-      it {
-        sleep 30
-        should be_listening
-      }
-    end
-
-  end
-
-  context "ensure we are still running" do
-
-    describe service(service_name) do
-      it {
-        sleep 30
-        should be_running
-      }
-    end
-
-    describe port(2000) do
-      it { should be_listening }
-    end
-
-  end
-
 end
 
diff --git a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml b/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
index 6429c65c7a5..430325afe9d 100644
--- a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
+++ b/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
@@ -5,7 +5,7 @@ HOSTS:
       - database
       - dashboard
     platform: el-6-x86_64
-    image: jordansissel/system:centos-6.4
+    image: electrical/centos:6.4-1
     hypervisor: docker
     docker_cmd: '["/sbin/init"]'
     docker_image_commands:
diff --git a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml b/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
index 6468721375a..aeca5651929 100644
--- a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
+++ b/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
@@ -9,7 +9,7 @@ HOSTS:
     hypervisor: docker
     docker_cmd: '["/sbin/init"]'
     docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all'
+      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
       - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
 CONFIG:
   type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml b/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
index c90dafaefd8..349a4cb497c 100644
--- a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
+++ b/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
@@ -9,7 +9,7 @@ HOSTS:
     hypervisor: docker
     docker_cmd: '["/sbin/init"]'
     docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all'
+      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
       - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
 CONFIG:
   type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
index e53d08047a3..d5ec812859e 100644
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
+++ b/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
@@ -9,6 +9,6 @@ HOSTS:
     hypervisor: docker
     docker_cmd: '["/sbin/init"]'
     docker_image_commands:
-      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl'
+      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl logrotate'
 CONFIG:
   type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1304-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1304-x64.yml
deleted file mode 100644
index 5735afc4669..00000000000
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1304-x64.yml
+++ /dev/null
@@ -1,14 +0,0 @@
-HOSTS:
-  ubuntu-13-04:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: ubuntu-13.04-amd64
-    image: jordansissel/system:ubuntu-13.04
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl'
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
new file mode 100644
index 00000000000..c80ed90f746
--- /dev/null
+++ b/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
@@ -0,0 +1,14 @@
+HOSTS:
+  ubuntu-14-04:
+    roles:
+      - master
+      - database
+      - dashboard
+    platform: ubuntu-14.04-amd64
+    image: electrical/ubuntu:14.04
+    hypervisor: docker
+    docker_cmd: '["/sbin/init"]'
+    docker_image_commands:
+      - 'apt-get install -yq ruby ruby1.9.1-dev libaugeas-dev libaugeas-ruby lsb-release wget net-tools curl'
+CONFIG:
+  type: foss
diff --git a/acceptance_spec/spec_helper_acceptance.rb b/acceptance_spec/spec_helper_acceptance.rb
index a37cbf9e35f..ffbb671c61b 100644
--- a/acceptance_spec/spec_helper_acceptance.rb
+++ b/acceptance_spec/spec_helper_acceptance.rb
@@ -4,34 +4,59 @@
 
 files_dir = ENV['files_dir'] || '/home/jenkins/puppet'
 
-proxy_host = ENV['proxy_host'] || ''
+proxy_host = ENV['BEAKER_PACKAGE_PROXY'] || ''
 
-gem_proxy = ''
-gem_proxy = "http_proxy=http://#{proxy_host}" unless proxy_host.empty?
+if !proxy_host.empty?
+  gem_proxy = "http_proxy=#{proxy_host}" unless proxy_host.empty?
+
+  hosts.each do |host|
+    on host, "echo 'export http_proxy='#{proxy_host}'' >> /root/.bashrc"
+    on host, "echo 'export https_proxy='#{proxy_host}'' >> /root/.bashrc"
+    on host, "echo 'export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,#{host.name}\"' >> /root/.bashrc"
+  end
+else
+  gem_proxy = ''
+end
 
 hosts.each do |host|
   # Install Puppet
   if host.is_pe?
     install_pe
   else
-    puppetversion = ENV['VM_PUPPET_VERSION'] || '3.4.0'
-    install_package host, 'rubygems'
+    puppetversion = ENV['VM_PUPPET_VERSION']
     on host, "#{gem_proxy} gem install puppet --no-ri --no-rdoc --version '~> #{puppetversion}'"
     on host, "mkdir -p #{host['distmoduledir']}"
 
     if fact('osfamily') == 'Suse'
-      install_package host, 'ruby-devel augeas-devel libxml2-devel'
-      on host, 'gem install ruby-augeas --no-ri --no-rdoc'
+      install_package host, 'rubygems ruby-devel augeas-devel libxml2-devel'
+      on host, "#{gem_proxy} gem install ruby-augeas --no-ri --no-rdoc"
     end
 
   end
 
-  # Setup proxy if its enabled
-  if fact('osfamily') == 'Debian'
-          on host, "echo 'Acquire::http::Proxy \"http://#{proxy_host}/\";' >> /etc/apt/apt.conf.d/10proxy" unless proxy_host.empty?
+  case fact('osfamily')
+    when 'RedHat'
+      scp_to(host, "#{files_dir}/elasticsearch-1.3.1.noarch.rpm", '/tmp/elasticsearch-1.3.1.noarch.rpm')
+    when 'Debian'
+      case fact('lsbmajdistrelease')
+        when '6'
+          scp_to(host, "#{files_dir}/elasticsearch-1.1.0.deb", '/tmp/elasticsearch-1.1.0.deb')
+        else
+          scp_to(host, "#{files_dir}/elasticsearch-1.3.1.deb", '/tmp/elasticsearch-1.3.1.deb')
+      end
+    when 'Suse'
+      case fact('operatingsystem')
+        when 'OpenSuSE'
+          scp_to(host, "#{files_dir}/elasticsearch-1.1.0.noarch.rpm", '/tmp/elasticsearch-1.1.0.noarch.rpm')
+        else
+          scp_to(host, "#{files_dir}/elasticsearch-1.3.1.noarch.rpm", '/tmp/elasticsearch-1.3.1.noarch.rpm')
+      end
   end
-  if fact('osfamily') == 'RedHat'
-    on host, "echo 'proxy=http://#{proxy_host}/' >> /etc/yum.conf" unless proxy_host.empty?
+
+  # on debian/ubuntu nodes ensure we get the latest info
+  # Can happen we have stalled data in the images
+  if fact('osfamily') == 'Debian'
+    on host, "apt-get update"
   end
 
 end
diff --git a/bin/logstash b/bin/logstash
index b4b28015a41..21e26d9ddf5 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -8,13 +8,6 @@
 #
 # See 'bin/logstash help' for a list of commands.
 #
-# NOTE: One extra command is available 'deps'
-# The 'deps' command will install dependencies for logstash.
-#
-# If you do not have ruby installed, you can set "USE_JRUBY=1"
-# in your environment and this script will download and use
-# a release of JRuby for you.
-
 # Defaults you can override with environment variables
 LS_HEAP_SIZE="${LS_HEAP_SIZE:=500m}"
 
@@ -28,22 +21,18 @@ setup
 export HOME SINCEDB_DIR
 
 case $1 in
-  deps) install_deps ;;
-  env) env "$@" ;;
   -*)
     if [ -z "$VENDORED_JRUBY" ] ; then
-      exec "${RUBYCMD}" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
+      exec "${RUBYCMD}" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
     else
-      exec "${JAVACMD}" $JAVA_OPTS "-jar" "$JRUBY_JAR" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "agent" "$@"
+      exec "$JRUBY_BIN" $(jruby_opts) "${basedir}/lib/logstash/runner.rb" "agent" "$@"
     fi
     ;;
   *)
     if [ -z "$VENDORED_JRUBY" ] ; then
-      exec "${RUBYCMD}" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "$@"
+      exec "${RUBYCMD}" "${basedir}/lib/logstash/runner.rb" "$@"
     else
-      exec "${JAVACMD}" $JAVA_OPTS "-jar" "$JRUBY_JAR" "-I${RUBYLIB}" "${basedir}/lib/logstash/runner.rb" "$@"
+      exec "$JRUBY_BIN" $(jruby_opts) "${basedir}/lib/logstash/runner.rb" "$@"
     fi
     ;;
 esac
-
-
diff --git a/bin/logstash.bat b/bin/logstash.bat
index f90ef3673ab..eb8f52b09e3 100644
--- a/bin/logstash.bat
+++ b/bin/logstash.bat
@@ -2,12 +2,23 @@
 
 SETLOCAL
 
-if not defined JAVA_HOME goto missing_java_home
-
 set SCRIPT_DIR=%~dp0
 for %%I in ("%SCRIPT_DIR%..") do set LS_HOME=%%~dpfI
 
+if "%USE_RUBY%" == "1" (
+goto setup_ruby
+) else (
+goto setup_jruby
+)
 
+:setup_ruby
+set RUBYCMD=ruby
+set VENDORED_JRUBY=
+goto EXEC
+
+:setup_jruby
+REM setup_java()
+if not defined JAVA_HOME goto missing_java_home
 REM ***** JAVA options *****
 
 if "%LS_MIN_MEM%" == "" (
@@ -46,26 +57,34 @@ REM The path to the heap dump location, note directory must exists and have enou
 REM space for a full heap dump.
 REM JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath=$LS_HOME/logs/heapdump.hprof
 
-set RUBYLIB=%LS_HOME%\lib
-set GEM_HOME=%LS_HOME%\vendor\bundle\jruby\1.9\
-set GEM_PATH=%GEM_HOME%
-
-for %%I in ("%LS_HOME%\vendor\jar\jruby-complete-*.jar") do set JRUBY_JAR_FILE=%%I
-if not defined JRUBY_JAR_FILE goto missing_jruby_jar
-
-set RUBY_CMD="%JAVA_HOME%\bin\java" %JAVA_OPTS% %LS_JAVA_OPTS% -jar "%JRUBY_JAR_FILE%"
-
-if "%*"=="deps" goto install_deps
-goto run_logstash
-
-:install_deps
-if not exist "%LS_HOME%\logstash.gemspec" goto missing_gemspec
-echo Installing gem dependencies. This will probably take a while the first time.
-%RUBY_CMD% "%LS_HOME%\gembag.rb"
-goto finally
+REM setup_vendored_jruby()
+set JRUBY_BIN="%LS_HOME%\vendor\jruby\bin\jruby"
+if exist "%JRUBY_BIN%" (
+set VENDORED_JRUBY=1
+goto EXEC
+) else (
+goto missing_jruby
+)
 
-:run_logstash
-%RUBY_CMD% "%LS_HOME%\lib\logstash\runner.rb" %*
+:EXEC
+REM run logstash
+set RUBYLIB=%LS_HOME%\lib
+REM is the first argument a flag? If so, assume 'agent'
+set first_arg=%1
+setlocal EnableDelayedExpansion
+if "!first_arg:~0,1!" equ "-" (
+  if "%VENDORED_JRUBY%" == "" (
+    %RUBYCMD% "%LS_HOME%\lib\logstash\runner.rb" agent %*
+  ) else (
+    %JRUBY_BIN% %jruby_opts% "%LS_HOME%\lib\logstash\runner.rb" agent %*
+  )
+) else (
+  if "%VENDORED_JRUBY%" == "" (
+    %RUBYCMD% "%LS_HOME%\lib\logstash\runner.rb" %*
+  ) else (
+    %JRUBY_BIN% %jruby_opts% "%LS_HOME%\lib\logstash\runner.rb" %*
+  )
+)
 goto finally
 
 :missing_java_home
@@ -73,15 +92,10 @@ echo JAVA_HOME environment variable must be set!
 pause
 goto finally
 
-:missing_jruby_jar
-md "%LS_HOME%\vendor\jar\"
-echo Please download the JRuby Complete .jar from http://jruby.org/download to %LS_HOME%\vendor\jar\ and re-run this command.
-pause
-goto finally
-
-:missing_gemspec
-echo Cannot install dependencies; missing logstash.gemspec. This 'deps' command only works from a logstash git clone.
-pause
+:missing_jruby
+echo Unable to find JRuby.
+echo If you are a user, this is a bug.
+echo If you are a developer, please run 'rake bootstrap'. Running 'rake' requires the 'ruby' program be available.
 goto finally
 
 :finally
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index fe49c8f6009..1b1fc46ea15 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -1,24 +1,5 @@
 basedir=$(cd `dirname $0`/..; pwd)
 
-setup_ruby() {
-  export RUBYLIB="${basedir}/lib"
-
-  # Verify ruby works
-  if ! ruby -e 'puts "HURRAY"' 2> /dev/null | grep -q "HURRAY" ; then
-    echo "No ruby program found. Cannot start."
-    exit 1
-  fi
-
-  # set $RUBY and $RUBYVER
-  eval $(ruby -rrbconfig -e 'puts "RUBYVER=#{RbConfig::CONFIG["ruby_version"]}"; puts "RUBY=#{RUBY_ENGINE}"')
-
-  RUBYCMD="ruby"
-  VENDORED_JRUBY=
-
-  export GEM_HOME="${basedir}/vendor/bundle/${RUBY}/${RUBYVER}"
-  export GEM_PATH=
-}
-
 setup_java() {
   if [ -z "$JAVACMD" ] ; then
     if [ -n "$JAVA_HOME" ] ; then
@@ -38,11 +19,6 @@ setup_java() {
     exit 1
   fi
 
-  if [ "$(basename $JAVACMD)" = "drip" ] ; then
-    export DRIP_INIT_CLASS="org.jruby.main.DripMain"
-    export DRIP_INIT=
-  fi
-
   JAVA_OPTS="$JAVA_OPTS -Xmx${LS_HEAP_SIZE}"
   JAVA_OPTS="$JAVA_OPTS -XX:+UseParNewGC"
   JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
@@ -65,35 +41,77 @@ setup_java() {
   export JAVA_OPTS
 }
 
+setup_drip() {
+  if [ -z $DRIP_JAVACMD ] ; then
+    JAVACMD="drip"
+  fi
+
+  # resolve full path to the drip command.
+  if [ ! -f "$JAVACMD" ] ; then
+    JAVACMD=$(which $JAVACMD 2>/dev/null)
+  fi
+
+  if [ ! -x "$JAVACMD" ] ; then
+    echo "Could not find executable drip binary. Please install drip in your PATH"
+    exit 1
+  fi
+
+  # faster JRuby startup options https://github.com/jruby/jruby/wiki/Improving-startup-time
+  # since we are using drip to speed up, we may as well throw these in also
+  if [ "$USE_RUBY" = "1" ] ; then
+    export JRUBY_OPTS="-J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify"
+  else
+    JAVA_OPTS="$JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+  fi
+  export JAVACMD
+  export DRIP_INIT_CLASS="org.jruby.main.DripMain"
+  export DRIP_INIT=""
+}
+
 setup_vendored_jruby() {
-  RUBYVER=1.9
-  RUBY=jruby
+  #JRUBY_JAR=$(ls "${basedir}"/vendor/jruby/jruby-complete-*.jar)
+  JRUBY_BIN="${basedir}/vendor/jruby/bin/jruby"
 
-  JRUBY_JAR=$(ls "${basedir}"/vendor/jar/jruby-complete-*.jar)
+  if [ ! -f "${JRUBY_BIN}" ] ; then
+    echo "Unable to find JRuby."
+    echo "If you are a user, this is a bug."
+    echo "If you are a developer, please run 'rake bootstrap'. Running 'rake' requires the 'ruby' program be available."
+    exit 1
+  fi
   VENDORED_JRUBY=1
+}
 
-  export RUBYLIB="${basedir}/lib"
-  export GEM_HOME="${basedir}/vendor/bundle/${RUBY}/${RUBYVER}"
-  export GEM_PATH=
+setup_ruby() {
+  RUBYCMD="ruby"
+  VENDORED_JRUBY=
+}
+
+jruby_opts() {
+  echo "--1.9"
+  for i in $JAVA_OPTS ; do
+    echo "-J$i"
+  done
 }
 
 setup() {
-  setup_java
-  if [ -z "$USE_JRUBY" -a \( -d "$basedir/.git" -o ! -z "$USE_RUBY" \) ] ; then
+  # first check if we want to use drip, which can be used in vendored jruby mode
+  # and also when setting USE_RUBY=1 if the ruby interpretor is in fact jruby
+  if [ ! -z "$JAVACMD" ] ; then
+    if [ "$(basename $JAVACMD)" = "drip" ] ; then
+      DRIP_JAVACMD=1
+      USE_DRIP=1
+    fi
+  fi
+  if [ "$USE_DRIP" = "1" ] ; then
+    setup_drip
+  fi
+
+  if [ "$USE_RUBY" = "1" ] ; then
     setup_ruby
   else
+    setup_java
     setup_vendored_jruby
   fi
-}
 
-install_deps() {
-  if [ -f "$basedir/logstash.gemspec" ] ; then
-    if [ -z "$VENDORED_JRUBY" ] ; then
-      exec "${RUBYCMD}" "${basedir}/gembag.rb" "${basedir}/logstash.gemspec" "$@"
-    else
-      exec "${JAVACMD}" $JAVA_OPTS "-jar" "$JRUBY_JAR" "${basedir}/gembag.rb" "${basedir}/logstash.gemspec" "$@"
-    fi
-  else
-    echo "Cannot install dependencies; missing logstash.gemspec. This 'deps' command only works from a logstash git clone."
-  fi
-}
\ No newline at end of file
+  export RUBYLIB="${basedir}/lib"
+}
diff --git a/bin/plugin b/bin/plugin
index 4e2601fb911..bfd9b78f551 100755
--- a/bin/plugin
+++ b/bin/plugin
@@ -1,61 +1,8 @@
 #!/bin/sh
-# Install contrib plugins.
+# Install plugins.
 #
 # Usage:
-#     bin/plugin install contrib
-#
-# Figure out if we're using wget or curl
+#     bin/plugin --help
 
 basedir=$(cd `dirname $0`/..; pwd)
-. ${basedir}/bin/logstash.lib.sh
-
-WGET=$(which wget 2>/dev/null)
-CURL=$(which curl 2>/dev/null)
-
-URLSTUB="http://download.elasticsearch.org/logstash/logstash/"
-
-if [ "x$WGET" != "x" ]; then
-	DOWNLOAD_COMMAND="wget -q --no-check-certificate -O"
-elif [ "x$CURL" != "x" ]; then
-    DOWNLOAD_COMMAND="curl -s -L -k -o"
-else
-	echo "wget or curl are required."
-	exit 1
-fi
-
-
-if [ -f "$basedir/lib/logstash/version.rb" ] ; then
-	VERSION=$(cat "$basedir/lib/logstash/version.rb" | grep LOGSTASH_VERSION | awk -F\" '{print $2}') 
-else
-	echo "ERROR: Cannot determine Logstash version.  Exiting."
-	exit 1
-fi
-
-# Placeholder for now, if other installs ever become available.
-if [ "x$2" != "xcontrib" ]; then
-	echo "Can only install contrib at this time... Exiting."
-	exit 1
-fi
-
-TARGETDIR="$basedir/vendor/logstash"
-mkdir -p $TARGETDIR
-SUFFIX=".tar.gz"
-FILEPATH="logstash-contrib-${VERSION}"
-FILENAME=${FILEPATH}${SUFFIX}
-TARGET="${TARGETDIR}/${FILENAME}"
-
-case $1 in
-  install)
-  	$DOWNLOAD_COMMAND ${TARGET} ${URLSTUB}${FILENAME}
-  	if [ ! -f "${TARGET}" ]; then
-	  	echo "ERROR: Unable to download ${URLSTUB}${FILENAME}"
-	  	echo "Exiting."
-	  	exit 1
-	fi
-  	gzip -dc ${TARGET} | tar -xC $TARGETDIR
-  	cp -R ${TARGETDIR}/$FILEPATH/* $basedir  ;; # Copy contents to local directory, adding on top of existing install
-  *) 
-  	echo "Usage: bin/plugin install contrib"
-	exit 0
-	;;
-esac
+exec ${basedir}/bin/logstash plugin "$@"
diff --git a/docs/asciidoc/static/command-line-flags.asciidoc b/docs/asciidoc/static/command-line-flags.asciidoc
new file mode 100644
index 00000000000..797fd67c52d
--- /dev/null
+++ b/docs/asciidoc/static/command-line-flags.asciidoc
@@ -0,0 +1,50 @@
+== Command-line flags
+
+[float]
+=== Agent
+
+The Logstash agent has the following flags (also try using the '--help' flag)
+
+[source,js]
+----------------------------------
+-f, --config CONFIGFILE
+ Load the Logstash config from a specific file, directory, or a wildcard. If given a directory or wildcard, config files will be read from the directory in alphabetical order.
+
+-e CONFIGSTRING
+ Use the given string as the configuration data. Same syntax as the config file. If not input is specified, 'stdin { type => stdin }' is default. If no output is specified, 'stdout { codec => rubydebug }}' is default.
+
+-w, --filterworkers COUNT
+ Run COUNT filter workers (default: 1)
+
+--watchdog-timeout TIMEOUT
+ Set watchdog timeout value in seconds. Default is 10.
+
+-l, --log FILE 
+ Log to a given path. Default is to log to stdout 
+
+--verbose 
+ Increase verbosity to the first level, less verbose.
+
+--debug 
+ Increase verbosity to the last level, more verbose.
+
+-v  
+ *DEPRECATED: see --verbose/debug* Increase verbosity. There are multiple levels of verbosity available with
+'-vv' currently being the highest 
+
+--pluginpath PLUGIN_PATH 
+ A colon-delimited path to find other Logstash plugins in 
+----------------------------------
+
+[float]
+=== Web
+
+[source,js]
+----------------------------------
+-a, --address ADDRESS 
+ Address on which to start webserver. Default is 0.0.0.0.
+
+-p, --port PORT
+ Port on which to start webserver. Default is 9292.
+----------------------------------
+
diff --git a/docs/asciidoc/static/configuration.asciidoc b/docs/asciidoc/static/configuration.asciidoc
new file mode 100644
index 00000000000..748c26ff518
--- /dev/null
+++ b/docs/asciidoc/static/configuration.asciidoc
@@ -0,0 +1,393 @@
+== Logstash Config Language
+[float]
+=== Basic Layout
+
+The Logstash config language aims to be simple.
+
+There are 3 main sections: inputs, filters, outputs. Each section has configurations for each plugin available in that section.
+
+Example:
+
+[source,js]
+----------------------------------
+# This is a comment. You should use comments to describe
+# parts of your configuration.
+input {
+  ...
+}
+
+filter {
+  ...
+}
+
+output {
+  ...
+}
+----------------------------------
+[float]
+=== Filters and Ordering
+
+For a given event, filters are applied in the order of appearance in the configuration file.
+[float]
+=== Comments
+
+Comments are the same as in perl, ruby, and python. A comment starts with a '#' character, and does not need to be at the beginning of a line. For example:
+
+[source,js]
+----------------------------------
+# this is a comment
+
+input { # comments can appear at the end of a line, too
+  # ...
+}
+----------------------------------
+[float]
+[[plugin_configuration]]
+=== Plugins
+
+The input, filter and output sections all let you configure plugins. Plugin
+configuration consists of the plugin name followed by a block of settings for
+that plugin. For example, how about two file inputs:
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/var/log/messages"
+    type => "syslog"
+  }
+
+  file {
+    path => "/var/log/apache/access.log"
+    type => "apache"
+  }
+}
+----------------------------------
+
+The above configures two file separate inputs. Both set two configuration settings each: 'path' and 'type'. Each plugin has different settings for configuring it; seek the documentation for your plugin to learn what settings are available and what they mean. For example, the [file input][fileinput] documentation will explain the meanings of the path and type settings.
+
+[float]
+=== Value Types
+
+The documentation for a plugin may enforce a configuration field having a
+certain type.  Examples include boolean, string, array, number, hash,
+etc.
+[[boolean]]
+[float]
+==== Boolean
+
+A boolean must be either `true` or `false`. Note the lack of quotes around `true` and `false`.
+
+Examples:
+
+[source,js]
+----------------------------------
+  ssl_enable => true
+----------------------------------
+[[string]]
+[float]
+==== String
+
+A string must be a single value.
+
+Example:
+
+[source,js]
+----------------------------------
+  name => "Hello world"
+----------------------------------
+
+You should use quotes around string values.
+[[number]]
+[float]
+==== Number
+
+Numbers must be valid numerics (floating point or integer are OK).
+
+Example:
+
+[source,js]
+----------------------------------
+  port => 33
+----------------------------------
+[[array]]
+[float]
+==== Array
+
+An array can be a single string value or multiple. If you specify the same
+field multiple times, it appends to the array.
+
+Examples:
+
+[source,js]
+----------------------------------
+  path => [ "/var/log/messages", "/var/log/*.log" ]
+  path => "/data/mysql/mysql.log"
+----------------------------------
+
+The above makes 'path' a 3-element array including all 3 strings.
+[[hash]]
+[float]
+==== Hash
+
+A hash is basically the same syntax as Ruby hashes.
+The key and value are simply pairs, such as:
+
+[source,js]
+----------------------------------
+match => {
+  "field1" => "value1"
+  "field2" => "value2"
+  ...
+}
+----------------------------------
+
+[[password]]
+[float]
+==== Password
+
+A password field is basically a String field with a single value, but it will
+not be logged or printed
+
+Example:
+
+[source,js]
+----------------------------------
+  my_password => "password"
+----------------------------------
+
+
+[[path]]
+[float]
+==== Path
+
+A path field is a String field which represents a valid operating system path
+
+Example:
+
+[source,js]
+----------------------------------
+  my_path => "/tmp/logstash"
+----------------------------------
+
+[[codec]]
+[float]
+==== Codec
+
+A codec is the name of Logstash codec used to represent the data. Codec can be
+used in both inputs and outputs.
+Input codecs are a convenient method for decoding your data before it enters the input,
+without needing a separate filter in your Logstash pipeline.
+Output codecs are a convenient method for encoding your data before it leaves the output,
+without needing a separate filter in your Logstash pipeline.
+
+Example:
+
+[source,js]
+----------------------------------
+  codec => "json"
+----------------------------------
+
+
+[float]
+=== Field References
+
+All events have properties. For example, an apache access log would have things
+like status code (200, 404), request path ("/", "index.html"), HTTP verb (GET, POST),
+client IP address, etc. Logstash calls these properties "fields."
+
+In many cases, it is useful to be able to refer to a field by name. To do this,
+you can use the Logstash field reference syntax.
+
+By way of example, let us suppose we have this event:
+
+[source,js]
+----------------------------------
+{
+  "agent": "Mozilla/5.0 (compatible; MSIE 9.0)",
+  "ip": "192.168.24.44",
+  "request": "/index.html"
+  "response": {
+    "status": 200,
+    "bytes": 52353
+  },
+  "ua": {
+    "os": "Windows 7"
+  }
+}
+
+----------------------------------
+
+- the syntax to access fields is `[fieldname]`.
+- if you are only referring to a **top-level field**, you can omit the `[]` and
+simply say `fieldname`.
+- in the case of **nested fields**, like the "os" field above, you need
+the full path to that field: `[ua][os]`.
+
+[float]
+==== sprintf format
+
+This syntax is also used in what Logstash calls 'sprintf format'. This format
+allows you to refer to field values from within other strings. For example, the
+statsd output has an 'increment' setting, to allow you to keep a count of
+apache logs by status code:
+
+[source,js]
+----------------------------------
+output {
+  statsd {
+    increment => "apache.%{[response][status]}"
+  }
+}
+----------------------------------
+
+You can also do time formatting in this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a [time format](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html).
+
+For example, if you want to use the file output to write to logs based on the
+hour and the 'type' field:
+
+[source,js]
+----------------------------------
+output {
+  file {
+    path => "/var/log/%{type}.%{+yyyy.MM.dd.HH}"
+  }
+}
+----------------------------------
+
+[float]
+=== Conditionals
+
+Sometimes you only want a filter or output to process an event under
+certain conditions. For that, you'll want to use a conditional!
+
+Conditionals in Logstash look and act the same way they do in programming
+languages. You have `if`, `else if` and `else` statements. Conditionals may be
+nested if you need that.
+
+The syntax is follows:
+
+[source,js]
+----------------------------------
+if EXPRESSION {
+  ...
+} else if EXPRESSION {
+  ...
+} else {
+  ...
+}
+----------------------------------
+
+What's an expression? Comparison tests, boolean logic, etc!
+
+The following comparison operators  are supported:
+
+* equality, etc: ==,  !=,  <,  >,  <=,  >=
+* regexp: =~, !~
+* inclusion: in, not in
+
+The following boolean operators are supported:
+
+* and, or, nand, xor
+
+The following unary operators are supported:
+
+* !
+
+Expressions may contain expressions. Expressions may be negated with `!`.
+Expressions may be grouped with parentheses `(...)`. Expressions can be long
+and complex.
+
+For example, if we want to remove the field `secret` if the field
+`action` has a value of `login`:
+
+[source,js]
+----------------------------------
+filter {
+  if [action] == "login" {
+    mutate { remove => "secret" }
+  }
+}
+----------------------------------
+
+The above uses the field reference syntax to get the value of the
+`action` field. It is compared against the text `login` and, if equal,
+allows the mutate filter to delete the field named `secret`.
+
+How about a more complex example?
+
+* alert nagios of any apache events with status 5xx
+* record any 4xx status to elasticsearch
+* record all status code hits via statsd
+
+How about telling nagios of any http event that has a status code of 5xx?
+
+[source,js]
+----------------------------------
+output {
+  if [type] == "apache" {
+    if [status] =~ /^5\d\d/ {
+      nagios { ...  }
+    } else if [status] =~ /^4\d\d/ {
+      elasticsearch { ... }
+    }
+    statsd { increment => "apache.%{status}" }
+  }
+}
+----------------------------------
+
+You can also do multiple expressions in a single condition:
+
+[source,js]
+----------------------------------
+output {
+  # Send production errors to pagerduty
+  if [loglevel] == "ERROR" and [deployment] == "production" {
+    pagerduty {
+    ...
+    }
+  }
+}
+----------------------------------
+
+Here are some examples for testing with the in conditional:
+
+[source,js]
+----------------------------------
+filter {
+  if [foo] in [foobar] {
+    mutate { add_tag => "field in field" }
+  }
+  if [foo] in "foo" {
+    mutate { add_tag => "field in string" }
+  }
+  if "hello" in [greeting] {
+    mutate { add_tag => "string in field" }
+  }
+  if [foo] in ["hello", "world", "foo"] {
+    mutate { add_tag => "field in list" }
+  }
+  if [missing] in [alsomissing] {
+    mutate { add_tag => "shouldnotexist" }
+  }
+  if !("foo" in ["hello", "world"]) {
+    mutate { add_tag => "shouldexist" }
+  }
+}
+----------------------------------
+
+Or, to test if grok was successful:
+
+[source,js]
+----------------------------------
+output {
+  if "_grokparsefailure" not in [tags] {
+    elasticsearch { ... }
+  }
+}
+----------------------------------
+
+[float]
+=== Further Reading
+
+For more information, see [the plugin docs index](index)
diff --git a/docs/asciidoc/static/contrib-plugins.asciidoc b/docs/asciidoc/static/contrib-plugins.asciidoc
new file mode 100644
index 00000000000..59ec04ca765
--- /dev/null
+++ b/docs/asciidoc/static/contrib-plugins.asciidoc
@@ -0,0 +1,59 @@
+== contrib plugins
+[float]
+=== Why contrib?
+As Logstash has grown, we've accumulated a massive repository of plugins. Well over 100 plugins, it became difficult for the project maintainers to adequately support everything effectively.
+
+In order to improve the quality of popular plugins, we've moved the less-commonly-used plugins to a separate repository we're calling "contrib". Concentrating common plugin usage into core solves a few problems, most notably user complaints about the size of Logstash releases, support/maintenance costs, etc.
+
+It is our intent that this separation will improve life for users. If it doesn't, please file a bug so we can work to address it!
+
+If a plugin is available in the 'contrib' package, the documentation for that plugin will note this boldly at the top of that plugin's documentation.
+
+Contrib plugins reside in a [separate github project](https://github.com/elasticsearch/logstash-contrib).
+
+[float]
+=== Packaging
+
+At present, the contrib modules are available as a tarball.
+
+[float]
+=== Automated Installation
+
+The `bin/plugin` script will handle the installation for you:
+
+[source,js]
+----------------------------------
+cd /path/to/logstash
+bin/plugin install contrib
+----------------------------------
+[float]
+=== Manual Installation
+
+The contrib plugins can be extracted on top of an existing Logstash installation. 
+
+For example, if I've extracted `logstash-%VERSION%.tar.gz` into `/path`, e.g.
+
+[source,js]
+----------------------------------
+cd /path
+tar zxf ~/logstash-%VERSION%.tar.gz
+----------------------------------
+
+It will have a `/path/logstash-%VERSION%` directory, e.g.
+
+[source,js]
+----------------------------------
+$ ls
+logstash-%VERSION%
+----------------------------------
+
+The method to install the contrib tarball is identical.
+[source,js]
+----------------------------------
+cd /path
+wget http://download.elasticsearch.org/logstash/logstash/logstash-contrib-%VERSION%.tar.gz
+tar zxf ~/logstash-contrib-%VERSION%.tar.gz
+----------------------------------
+This will install the contrib plugins in the same directory as the core
+install. These plugins will be available to Logstash the next time it starts.
+
diff --git a/docs/asciidoc/static/contributing-to-logstash.asciidoc b/docs/asciidoc/static/contributing-to-logstash.asciidoc
new file mode 100644
index 00000000000..bbb928fcdf8
--- /dev/null
+++ b/docs/asciidoc/static/contributing-to-logstash.asciidoc
@@ -0,0 +1,125 @@
+[[contributing-to-logstash]]
+
+== Extending Logstash
+
+You can add your own input, output, or filter plugins to Logstash.
+
+If you're looking to extend Logstash today, the best way is to look at how some existing plugins are written.
+
+[float]
+=== Good examples of plugins
+
+* https://github.com/logstash/logstash/blob/master/lib/logstash/inputs/tcp.rb[inputs/tcp]
+* https://github.com/logstash/logstash/blob/master/lib/logstash/filters/multiline.rb[filters/multiline]
+* https://github.com/elasticsearch/logstash-contrib/blob/master/lib/logstash/outputs/mongodb.rb[outputs/mongodb]
+
+[float]
+=== Common concepts
+
+* The `config_name` sets the name used in the config file.
+* The `milestone` sets the milestone number of the plugin. See link:plugin-milestones[Plugin Milestones] for more info.
+* The `config` lines define this plugin's configuration options.
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+
+[float]
+==== Required modules
+
+All plugins should require the Logstash module.
+
+[source,js]
+----------------------------------
+require 'logstash/namespace'
+----------------------------------
+
+[float]
+==== Plugin name
+
+Every plugin must have a name set with the `config_name` method. If this
+is not specified plugins will fail to load with an error.
+
+[float]
+==== Milestones
+
+Every plugin needs a milestone set using `milestone`. See
+<../plugin-milestones> for more info.
+
+[float]
+==== Config lines
+
+The `config` lines define configuration options and are constructed like
+so:
+
+[source,js]
+----------------------------------
+config :host, :validate => :string, :default => "0.0.0.0"
+----------------------------------
+
+The name of the option is specified, here `:host` and then the
+attributes of the option. They can include `:validate`, `:default`,
+`:required` (a Boolean `true` or `false`), and `:deprecated` (also a
+Boolean).  
+ 
+[float]
+=== Inputs
+
+All inputs require and extend the LogStash::Inputs::Base class, like so:
+
+[source,js]
+----------------------------------
+require 'logstash/inputs/base'
+...
+
+class LogStash::Inputs::YourPlugin < LogStash::Inputs::Base
+...
+----------------------------------
+ 
+Inputs have two methods: `register` and `run`.
+
+* Each input runs as its own thread.
+* The `run` method is expected to run-forever.
+
+[float]
+=== Filters
+
+All filters require and extend the LogStash::Filters::Base class, like so:
+
+[source,js]
+----------------------------------
+require 'logstash/filters/base'
+...
+
+class LogStash::Filters::YourPlugin < LogStash::Filters::Base
+...
+----------------------------------
+ 
+Filters have two methods: `register` and `filter`.
+
+* The `filter` method gets an event. 
+* Call `event.cancel` to drop the event.
+* To modify an event, simply make changes to the event you are given.
+* The return value is ignored.
+
+[float]
+=== Outputs
+
+All outputs require and extend the LogStash::Outputs::Base class, like so:
+
+[source,js]
+----------------------------------
+require 'logstash/outputs/base'
+...
+
+class LogStash::Outputs::YourPlugin < LogStash::Outputs::Base
+...
+----------------------------------
+
+Outputs have two methods: `register` and `receive`.
+
+* The `receive` method is called when an event gets pushed to your output
+
+[float]
+=== Example: a new filter
+
+Learn by example how to [add a new filter to Logstash](example-add-a-new-filter)
+
+
diff --git a/docs/asciidoc/static/example-add-a-new-filter.asciidoc b/docs/asciidoc/static/example-add-a-new-filter.asciidoc
new file mode 100644
index 00000000000..267c61e3aaf
--- /dev/null
+++ b/docs/asciidoc/static/example-add-a-new-filter.asciidoc
@@ -0,0 +1,119 @@
+== Adding a sample filter to Logstash
+
+This document shows you how to add a new filter to Logstash.
+
+For a general overview of how to add a new plugin, see [the extending Logstash](.) overview.
+
+[float]
+=== Write code.
+
+Let's write a 'hello world' filter. This filter will replace the 'message' in the event with "Hello world!"
+
+First, Logstash expects plugins in a certain directory structure: `logstash/TYPE/PLUGIN_NAME.rb`
+
+Since we're creating a filter, let's mkdir this:
+
+[source,js]
+----------------------------------
+mkdir -p logstash/filters/
+cd logstash/filters
+----------------------------------
+
+Now add the code:
+
+[source,js]
+----------------------------------
+# Call this file 'foo.rb' (in logstash/filters, as above)
+
+require "logstash/filters/base"
+require "logstash/namespace"
+
+class LogStash::Filters::Foo < LogStash::Filters::Base
+
+  # Setting the config_name here is required. This is how you
+  # configure this filter from your Logstash config.
+  #
+  # filter {
+  #   foo { ... }
+  # }
+  config_name "foo"
+
+  # New plugins should start life at milestone 1.
+  milestone 1
+
+  # Replace the message with this value.
+  config :message, :validate => :string
+
+  public
+    def register
+    # nothing to do
+  end # def register
+
+  public
+    def filter(event)
+
+      if @message
+        # Replace the event message with our message as configured in the
+        # config file.
+        event["message"] = @message
+      end
+
+      # filter_matched should go in the last line of our successful code 
+      filter_matched(event)
+    end # def filter
+  end # class LogStash::Filters::Foo
+
+  ## Add it to your configuration
+----------------------------------
+
+For this simple example, let's just use stdin input and stdout output.
+The config file looks like this:
+
+[source,js]
+----------------------------------
+input { 
+  stdin { type => "foo" } 
+}
+filter {
+  if [type] == "foo" {
+    foo {
+      message => "Hello world!"
+    }
+  }
+}
+output {
+  stdout { }
+}
+----------------------------------
+
+Call this file 'example.conf'
+
+[float]
+=== Tell Logstash about it.
+
+Depending on how you installed Logstash, you have a few ways of including this
+plugin.
+
+You can use the agent flag --pluginpath flag to specify where the root of your
+plugin tree is. In our case, it's the current directory.
+
+[source,js]
+----------------------------------
+% bin/logstash --pluginpath your/plugin/root -f example.conf
+----------------------------------
+
+## Example running
+
+In the example below, I typed in "the quick brown fox" after running the java
+command.
+
+[source,js]
+----------------------------------
+% bin/logstash --pluginpath your/plugin/root -f example.conf
+the quick brown fox   
+2011-05-12T01:05:09.495000Z mylocalhost: Hello world!
+----------------------------------
+
+The output is the standard Logstash stdout output, but in this case our "the quick brown fox" message was replaced with "Hello world!"
+
+All done! :)
diff --git a/docs/asciidoc/static/extending-logstash.asciidoc b/docs/asciidoc/static/extending-logstash.asciidoc
new file mode 100644
index 00000000000..ce28ca4d7e1
--- /dev/null
+++ b/docs/asciidoc/static/extending-logstash.asciidoc
@@ -0,0 +1,104 @@
+= Extending Logstash
+
+== Extending Logstash
+
+You can add your own input, output, or filter plugins to Logstash.
+
+If you're looking to extend Logstash today, please look at the existing plugins.
+
+== Good examples of plugins
+
+* https://github.com/logstash-plugins/logstash-input-tcp/blob/master/lib/logstash/inputs/tcp.rb[inputs/tcp]
+* https://github.com/logstash-plugins/logstash-filter-multiline/blob/master/lib/logstash/filters/multiline.rb[filters/multiline]
+* https://github.com/logstash-plugins/logstash-output-mongodb/blob/master/lib/logstash/outputs/mongodb.rb[outputs/mongodb]
+
+== Common concepts
+
+* The `config_name` sets the name used in the config file.
+* The `milestone` sets the milestone number of the plugin. See <../plugin-milestones> for more info.
+* The `config` lines define config options.
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+
+=== Required modules
+
+All plugins should require the Logstash module.
+
+[source,js]
+----------------------------------
+require 'logstash/namespace'
+----------------------------------
+
+=== Plugin name
+
+Every plugin must have a name set with the `config_name` method. If this
+is not specified plugins will fail to load with an error.
+
+=== Milestones
+
+Every plugin needs a milestone set using `milestone`. See
+<../plugin-milestones> for more info.
+
+=== Config lines
+
+The `config` lines define configuration options and are constructed like
+so:
+
+[source,js]
+----------------------------------
+config :host, :validate => :string, :default => "0.0.0.0"
+----------------------------------
+
+The name of the option is specified, here `:host` and then the
+attributes of the option. They can include `:validate`, `:default`,
+`:required` (a Boolean `true` or `false`), and `:deprecated` (also a
+Boolean).  
+ 
+== Inputs
+
+All inputs require the LogStash::Inputs::Base class:
+
+[source,js]
+----------------------------------
+require 'logstash/inputs/base'
+---------------------------------- 
+
+Inputs have two methods: `register` and `run`.
+
+* Each input runs as its own thread.
+* The `run` method is expected to run-forever.
+
+== Filters
+
+All filters require the LogStash::Filters::Base class:
+
+[source,js]
+----------------------------------
+require 'logstash/filters/base'
+----------------------------------
+
+Filters have two methods: `register` and `filter`.
+
+* The `filter` method gets an event. 
+* Call `event.cancel` to drop the event.
+* To modify an event, simply make changes to the event you are given.
+* The return value is ignored.
+
+== Outputs
+
+All outputs require the LogStash::Outputs::Base class:
+
+[source,js]
+----------------------------------
+require 'logstash/outputs/base'
+----------------------------------
+
+Outputs have two methods: `register` and `receive`.
+
+* The `register` method is called per plugin instantiation. Do any of your initialization here.
+* The `receive` method is called when an event gets pushed to your output
+
+== Example: a new filter
+
+Learn by example how to http://foo.com/example-add-a-new-filter[add a new filter to Logstash]
+
+
diff --git a/docs/asciidoc/static/getting-started-with-logstash.asciidoc b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
new file mode 100644
index 00000000000..46b06a8cb24
--- /dev/null
+++ b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
@@ -0,0 +1,490 @@
+== Getting Started with Logstash
+
+[float]
+=== Introduction
+Logstash is a tool for receiving, processing and outputting logs. All kinds of logs. System logs, webserver logs, error logs, application logs and just about anything you can throw at it. Sounds great, eh?
+
+Using Elasticsearch as a backend datastore, and kibana as a frontend reporting tool, Logstash acts as the workhorse, creating a powerful pipeline for storing, querying and analyzing your logs. With an arsenal of built-in inputs, filters, codecs and outputs, you can harness some powerful functionality with a small amount of effort. So, let's get started!
+
+[float]
+==== Prerequisite: Java
+The only prerequisite required by Logstash is a Java runtime. You can check that you have it installed by running the  command `java -version` in your shell. Here's something similar to what you might see:
+[source,java]
+----------------------------------
+> java -version
+java version "1.7.0_45"
+Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
+Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
+----------------------------------
+It is recommended to run a recent version of Java in order to ensure the greatest success in running Logstash.
+
+It's fine to run an open-source version such as OpenJDK: +
+http://openjdk.java.net/
+
+Or you can use the official Oracle version: +
+http://www.oracle.com/technetwork/java/index.html
+
+Once you have verified the existence of Java on your system, we can move on!
+
+[float]
+=== Up and Running!
+
+[float]
+==== Logstash in two commands
+First, we're going to download the 'logstash' binary and run it with a very simple configuration.
+["source","sh",subs="attributes,callouts"]
+----------------------------------
+curl -O https://download.elasticsearch.org/logstash/logstash/logstash-{logstash_version}.tar.gz
+----------------------------------
+Now you should have the file named 'logstash-{logstash_version}.tar.gz' on your local filesystem. Let's unpack it:
+["source","sh",subs="attributes,callouts"]
+----------------------------------
+tar zxvf logstash-{logstash_version}.tar.gz
+cd logstash-{logstash_version}
+----------------------------------
+Now let's run it:
+[source,js]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { stdout {} }'
+----------------------------------
+
+Now type something into your command prompt, and you will see it output by Logstash:
+[source,js]
+----------------------------------
+hello world
+2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
+----------------------------------
+
+OK, that's interesting... We ran Logstash with an input called `stdin`, and an output named `stdout`, and Logstash basically echoed back whatever we typed in some sort of structured format. Note that specifying the `-e` command line flag allows Logstash to accept a configuration directly from the command line. This is especially useful for quickly testing configurations without having to edit a file between iterations.
+
+Let's try a slightly fancier example. First, you should exit Logstash by issuing a `CTRL-C` command in the shell in which it is running. Now run Logstash again with the following command:
+[source,ruby]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { stdout { codec => rubydebug } }'
+----------------------------------
+
+And then try another test input, typing the text "goodnight moon":
+[source,ruby]
+----------------------------------
+goodnight moon
+{
+  "message" => "goodnight moon",
+  "@timestamp" => "2013-11-20T23:48:05.335Z",
+  "@version" => "1",
+  "host" => "my-laptop"
+}
+----------------------------------
+
+So, by re-configuring the `stdout` output (adding a "codec"), we can change the output of Logstash. By adding inputs, outputs and filters to your configuration, it's possible to massage the log data in many ways, in order to maximize flexibility of the stored data when you are querying it.
+
+[float]
+=== Storing logs with Elasticsearch
+Now, you're probably saying, "that's all fine and dandy, but typing all my logs into Logstash isn't really an option, and merely seeing them spit to STDOUT isn't very useful." Good point. First, let's set up Elasticsearch to store the messages we send into Logstash. If you don't have Elasticearch already installed, you can http://www.elasticsearch.org/download/[download the RPM or DEB package], or install manually by downloading the current release tarball, by issuing the following four commands:
+
+["source","sh",subs="attributes,callouts"]
+----------------------------------
+curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-{elasticsearch_version}.tar.gz
+tar zxvf elasticsearch-{elasticsearch_version}.tar.gz
+cd elasticsearch-{elasticsearch_version}/
+./bin/elasticsearch
+----------------------------------
+
+NOTE: This tutorial is running running Logstash {logstash_version} with Elasticsearch {elasticsearch_version}, although you can use it with a cluster running 1.0.0 or later. Each release of Logstash has a *recommended* version of Elasticsearch to pair with. Make sure they match based on the http://www.elasticsearch.org/overview/logstash[Logstash version] you're running!
+
+More detailed information on installing and configuring Elasticsearch can be found on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html[The Elasticsearch reference pages]. However, for the purposes of Getting Started with Logstash, the default installation and configuration of Elasticsearch should be sufficient.
+
+Now that we have Elasticsearch running on port 9200 (we do, right?), Logstash can be simply configured to use Elasticsearch as its backend. The defaults for both Logstash and Elasticsearch are fairly sane and well thought out, so we can omit the optional configurations within the elasticsearch output:
+
+[source,js]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } }'
+----------------------------------
+
+Type something, and Logstash will process it as before (this time you won't see any output, since we don't have the stdout output configured)
+
+[source,js]
+----------------------------------
+you know, for logs
+----------------------------------
+
+You can confirm that ES actually received the data by making a curl request and inspecting the return:
+
+[source,js]
+----------------------------------
+curl 'http://localhost:9200/_search?pretty'
+----------------------------------
+
+which should return something like this:
+
+[source,js]
+----------------------------------
+{
+  "took" : 2,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 1,
+    "max_score" : 1.0,
+    "hits" : [ {
+      "_index" : "logstash-2013.11.21",
+      "_type" : "logs",
+      "_id" : "2ijaoKqARqGvbMgP3BspJA",
+      "_score" : 1.0, "_source" : {"message":"you know, for logs","@timestamp":"2013-11-21T18:45:09.862Z","@version":"1","host":"my-laptop"}
+    } ]
+  }
+}
+----------------------------------
+
+Congratulations! You've successfully stashed logs in Elasticsearch via Logstash.
+
+[float]
+==== Elasticsearch Plugins (an aside)
+Another very useful tool for querying your Logstash data (and Elasticsearch in general) is the Elasticearch-kopf plugin. Here is more information on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html[Elasticsearch plugins]. To install elasticsearch-kopf, simply issue the following command in your Elasticsearch directory (the same one in which you ran Elasticsearch earlier):
+
+[source,js]
+----------------------------------
+bin/plugin -install lmenezes/elasticsearch-kopf
+----------------------------------
+Now you can browse to http://localhost:9200/_plugin/kopf/[http://localhost:9200/_plugin/kopf/] to browse your Elasticsearch data, settings and mappings!
+
+[float]
+==== Multiple Outputs
+As a quick exercise in configuring multiple Logstash outputs, let's invoke Logstash again, using both the 'stdout' as well as the 'elasticsearch' output:
+
+[source,js]
+----------------------------------
+bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } stdout { } }'
+----------------------------------
+Typing a phrase will now echo back to your terminal, as well as save in Elasticsearch! (Feel free to verify this using curl or elasticsearch-kopf).
+
+[float]
+==== Default - Daily Indices
+You might notice that Logstash was smart enough to create a new index in Elasticsearch... The default index name is in the form of `logstash-YYYY.MM.DD`, which essentially creates one index per day. At midnight (UTC), Logstash will automagically rotate the index to a fresh new one, with the new current day's timestamp. This allows you to keep windows of data, based on how far retroactively you'd like to query your log data. Of course, you can always archive (or re-index) your data to an alternate location, where you are able to query further into the past. If you'd like to simply delete old indices after a certain time period, you can use the https://github.com/elasticsearch/curator[Elasticsearch Curator tool].
+
+[float]
+=== Moving On
+Now you're ready for more advanced configurations. At this point, it makes sense for a quick discussion of some of the core features of Logstash, and how they interact with the Logstash engine.
+[float]
+==== The Life of an Event
+
+Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configuration. By creating a pipeline of event processing, Logstash is able to extract the relevant data from your logs and make it available to elasticsearch, in order to efficiently query your data. To get you thinking about the various options available in Logstash, let's discuss some of the more common configurations currently in use. For more details, read about http://logstash.net/docs/latest/life-of-an-event[the Logstash event pipeline].
+
+[float]
+===== Inputs
+Inputs are the mechanism for passing log data to Logstash. Some of the more useful, commonly-used ones are:
+
+* *file*: reads from a file on the filesystem, much like the UNIX command `tail -0a`
+* *syslog*: listens on the well-known port 514 for syslog messages and parses according to RFC3164 format
+* *redis*: reads from a redis server, using both redis channels and also redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
+* *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
+
+[float]
+===== Filters
+Filters are used as intermediary processing devices in the Logstash chain. They are often combined with conditionals in order to perform a certain action on an event, if it matches particular criteria. Some useful filters:
+
+* *grok*: parses arbitrary text and structure it. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns shipped built-in to Logstash, it's more than likely you'll find one that meets your needs!
+* *mutate*: The mutate filter allows you to do general mutations to fields. You can rename, remove, replace, and modify fields in your events.
+* *drop*: drop an event completely, for example, 'debug' events.
+* *clone*: make a copy of an event, possibly adding or removing fields.
+* *geoip*: adds information about geographical location of IP addresses (and displays amazing charts in kibana)
+[float]
+===== Outputs
+Outputs are the final phase of the Logstash pipeline. An event may pass through multiple outputs during processing, but once all outputs are complete, the event has finished its execution. Some commonly used outputs include:
+
+* *elasticsearch*: If you're planning to save your data in an efficient, convenient and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
+* *file*: writes event data to a file on disk.
+* *graphite*: sends event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
+* *statsd*: a service which "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
+[float]
+===== Codecs
+Codecs are basically stream filters which can operate as part of an input, or an output. Codecs allow you to easily separate the transport of your messages from the serialization process. Popular codecs include `json`, `msgpack` and `plain` (text).
+
+* *json*: encode / decode data in JSON format
+* *multiline*: Takes multiple-line text events and merge them into a single event, e.g. java exception and stacktrace messages
+
+For the complete list of (current) configurations, visit the Logstash <<plugin_configuration, plugin configuration>> section of the http://www.elasticsearch.org/guide/en/logstash/current/[Logstash documentation page].
+
+
+[float]
+=== More fun with Logstash
+[float]
+==== Persistent Configuration files
+
+Specifying configurations on the command line using '-e' is only so helpful, and more advanced setups will require more lengthy, long-lived configurations. First, let's create a simple configuration file, and invoke Logstash using it. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
+
+[source,ruby]
+----------------------------------
+input { stdin { } }
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+Then, run this command:
+
+[source,ruby]
+----------------------------------
+bin/logstash -f logstash-simple.conf
+----------------------------------
+
+Et voil! Logstash will read in the configuration file you just created and run as in the example we saw earlier. Note that we used the '-f' to read in the file, rather than the '-e' to read the configuration from the command line. This is a very simple case, of course, so let's move on to some more complex examples.
+
+[float]
+==== Filters
+Filters are an in-line processing mechanism which provide the flexibility to slice and dice your data to fit your needs. Let's see one in action, namely the *grok filter*.
+
+[source,ruby]
+----------------------------------
+input { stdin { } }
+
+filter {
+  grok {
+    match => { "message" => "%{COMBINEDAPACHELOG}" }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+Run Logstash with this configuration:
+
+[source,ruby]
+----------------------------------
+bin/logstash -f logstash-filter.conf
+----------------------------------
+
+Now paste this line into the terminal (so it will be processed by the stdin input):
+[source,ruby]
+----------------------------------
+127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
+----------------------------------
+
+You should see something returned to STDOUT which looks like this:
+
+[source,ruby]
+----------------------------------
+{
+        "message" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
+     "@timestamp" => "2013-12-11T08:01:45.000Z",
+       "@version" => "1",
+           "host" => "cadenza",
+       "clientip" => "127.0.0.1",
+          "ident" => "-",
+           "auth" => "-",
+      "timestamp" => "11/Dec/2013:00:01:45 -0800",
+           "verb" => "GET",
+        "request" => "/xampp/status.php",
+    "httpversion" => "1.1",
+       "response" => "200",
+          "bytes" => "3891",
+       "referrer" => "\"http://cadenza/xampp/navi.php\"",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
+}
+----------------------------------
+
+As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This will be extremely useful later when we start querying and analyzing our log data... for example, we'll be able to run reports on HTTP response codes, IP addresses, referrers, etc. very easily. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you're attempting to parse a fairly common log format, someone has already done the work for you. For more details, see the list of https://github.com/logstash/logstash/blob/master/patterns/[logstash grok patterns] on github.
+
+The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs, for example... the ability to tell Logstash "use this value as the timestamp for this event".
+
+[float]
+=== Useful Examples
+
+[float]
+==== Apache logs (from files)
+Now, let's configure something actually *useful*... apache2 access log files! We are going to read the input from a file on the localhost, and use a *conditional* to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you'll need to change the log's file path to suit your needs):
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/tmp/access_log"
+    start_position => "beginning"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { "type" => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch {
+    host => localhost
+  }
+  stdout { codec => rubydebug }
+}
+
+----------------------------------
+
+Then, create the file you configured above (in this example, "/tmp/access_log") with the following log lines as contents (or use some from your own webserver):
+
+[source,js]
+----------------------------------
+71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
+134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
+98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
+----------------------------------
+
+Now run it with the -f flag as in the last example:
+
+[source,js]
+----------------------------------
+bin/logstash -f logstash-apache.conf
+----------------------------------
+
+You should be able to see your apache log data in Elasticsearch now! You'll notice that Logstash opened the file you configured, and read through it, processing any events it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events and stored in Elasticsearch. As an added bonus, they will be stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
+
+In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching '*log'), by changing one line in the above configuration, like this:
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/tmp/*_log"
+...
+----------------------------------
+
+Now, rerun Logstash, and you will see both the error and access logs processed via Logstash. However, if you inspect your data (using elasticsearch-kopf, perhaps), you will see that the access_log was broken up into discrete fields, but not the error_log. That's because we used a "grok" filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
+
+Also, you might have noticed that Logstash did not reprocess the events which were already seen in the access_log file. Logstash is able to save its position in files, only processing new lines as they are added to the file. Neat!
+
+[float]
+==== Conditionals
+Now we can build on the previous example, where we introduced the concept of a *conditional*. A conditional should be familiar to most Logstash users, in the general sense. You may use 'if', 'else if' and 'else' statements, as in many other programming languages. Let's label each event according to which file it appeared in (access_log, error_log and other random files which end with "log").
+
+[source,ruby]
+----------------------------------
+input {
+  file {
+    path => "/tmp/*_log"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { type => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+    date {
+      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+    }
+  } else if [path] =~ "error" {
+    mutate { replace => { type => "apache_error" } }
+  } else {
+    mutate { replace => { type => "random_logs" } }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+You'll notice we've labeled all events using the "type" field, but we didn't actually parse the "error" or "random" files... There are so many types of error logs that it's better left as an exercise for you, depending on the logs you're seeing.
+
+[float]
+==== Syslog
+OK, now we can move on to another incredibly useful example: *syslog*. Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164 :). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line, so you can get a feel for what happens.
+
+First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
+
+[source,ruby]
+----------------------------------
+input {
+  tcp {
+    port => 5000
+    type => syslog
+  }
+  udp {
+    port => 5000
+    type => syslog
+  }
+}
+
+filter {
+  if [type] == "syslog" {
+    grok {
+      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
+      add_field => [ "received_at", "%{@timestamp}" ]
+      add_field => [ "received_from", "%{host}" ]
+    }
+    syslog_pri { }
+    date {
+      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+    }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+Run it as normal:
+
+[source,ruby]
+----------------------------------
+bin/logstash -f logstash-syslog.conf
+----------------------------------
+
+Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. In this simplified case, we're simply going to telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). First, open another shell window to interact with the Logstash syslog input and type the following command:
+
+[source,ruby]
+----------------------------------
+telnet localhost 5000
+----------------------------------
+
+You can copy and paste the following lines as samples (feel free to try some of your own, but keep in mind they might not parse if the grok filter is not correct for your data):
+
+[source,ruby]
+----------------------------------
+Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
+Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
+Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)
+Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
+----------------------------------
+
+Now you should see the output of Logstash in your original shell as it processes and parses messages!
+
+[source,ruby]
+----------------------------------
+{
+                 "message" => "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+              "@timestamp" => "2013-12-23T22:30:01.000Z",
+                "@version" => "1",
+                    "type" => "syslog",
+                    "host" => "0:0:0:0:0:0:0:1:52617",
+        "syslog_timestamp" => "Dec 23 14:30:01",
+         "syslog_hostname" => "louis",
+          "syslog_program" => "CRON",
+              "syslog_pid" => "619",
+          "syslog_message" => "(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+             "received_at" => "2013-12-23 22:49:22 UTC",
+           "received_from" => "0:0:0:0:0:0:0:1:52617",
+    "syslog_severity_code" => 5,
+    "syslog_facility_code" => 1,
+         "syslog_facility" => "user-level",
+         "syslog_severity" => "notice"
+}
+----------------------------------
+
+Congratulations! You're well on your way to being a real Logstash power user. You should be comfortable configuring, running and sending events to Logstash, but there's much more to explore.
diff --git a/docs/asciidoc/static/howtos-and-tutorials.asciidoc b/docs/asciidoc/static/howtos-and-tutorials.asciidoc
new file mode 100644
index 00000000000..fe98a8e0cb9
--- /dev/null
+++ b/docs/asciidoc/static/howtos-and-tutorials.asciidoc
@@ -0,0 +1,16 @@
+[[howtos-and-tutorials]]
+== Logstash HOWTOs and Tutorials
+Pretty self-explanatory, really
+
+=== Downloads and Releases
+* http://elasticsearch.org/#[Getting Started with Logstash]
+* http://elasticsearch.org/#[Configuration file overview]
+* http://elasticsearch.org/#[Command-line flags]
+* http://elasticsearch.org/#[The life of an event in Logstash]
+* http://elasticsearch.org/#[Using conditional logic]
+* http://elasticsearch.org/#[Glossary]
+* http://elasticsearch.org/#[referring to fields `[like][this]`]
+* http://elasticsearch.org/#[using the `%{fieldname}` syntax]
+* http://elasticsearch.org/#[Metrics from Logs]
+* http://elasticsearch.org/#[Using RabbitMQ]
+* http://elasticsearch.org/#[Contributing to Logstash]
diff --git a/docs/asciidoc/static/life-of-an-event.asciidoc b/docs/asciidoc/static/life-of-an-event.asciidoc
new file mode 100644
index 00000000000..45b0eb34431
--- /dev/null
+++ b/docs/asciidoc/static/life-of-an-event.asciidoc
@@ -0,0 +1,69 @@
+== the life of an event
+
+The Logstash agent is an event pipeline.
+
+[float]
+=== The Pipeline
+
+The Logstash agent is a processing pipeline with 3 stages: inputs -> filters -> outputs. Inputs generate events, filters modify them, outputs ship them elsewhere.
+
+Internal to Logstash, events are passed from each phase using internal queues. It is implemented with a 'SizedQueue' in Ruby. SizedQueue allows a bounded maximum of items in the queue such that any writes to the queue will block if the queue is full at maximum capacity.
+
+Logstash sets each queue size to 20. This means only 20 events can be pending into the next phase - this helps reduce any data loss and in general avoids Logstash trying to act as a data storage system. These internal queues are not for storing messages long-term.
+
+[float]
+=== Fault Tolerance
+
+Starting at outputs, here's what happens when things break.
+
+An output can fail or have problems because of some downstream cause, such as full disk, permissions problems, temporary network failures, or service outages. Most outputs should keep retrying to ship any events that were involved in the failure.
+
+If an output is failing, the output thread will wait until this output is healthy again and able to successfully send the message. Therefore, the output queue will stop being read from by this output and will eventually fill up with events and block new events from being written to this queue.
+
+A full output queue means filters will block trying to write to the output queue. Because filters will be stuck, blocked writing to the output queue, they will stop reading from the filter queue which will eventually cause the filter queue (input -> filter) to fill up.
+
+A full filter queue will cause inputs to block when writing to the filters. This will cause each input to block, causing each input to stop processing new data from wherever that input is getting new events.
+
+In ideal circumstances, this will behave similarly to when the tcp window closes to 0, no new data is sent because the receiver hasn't finished processing the current queue of data, but as soon as the downstream (output) problem is resolved, messages will begin flowing again..
+
+[float]
+=== Thread Model
+
+The thread model in Logstash is currently:
+
+[source,js]
+----------------------------------
+input threads | filter worker threads | output worker
+----------------------------------
+
+Filters are optional, so you will have this model if you have no filters defined:
+
+[source,js]
+----------------------------------
+input threads | output worker
+----------------------------------
+
+Each input runs in a thread by itself. This allows busier inputs to not be blocked by slower ones, etc. It also allows for easier containment of scope because each input has a thread.
+
+The filter thread model is a 'worker' model where each worker receives an event and applies all filters, in order, before emitting that to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety). 
+
+The default number of filter workers is 1, but you can increase this number with the '-w' flag on the agent.
+
+The output worker model is currently a single thread. Outputs will receive events in the order they are defined in the config file. 
+
+Outputs may decide to buffer events temporarily before publishing them, possibly in a separate thread. One example of this is the elasticsearch output
+which will buffer events and flush them all at once, in a separate thread. This mechanism (buffering many events + writing in a separate thread) can improve performance so the Logstash pipeline isn't stalled waiting for a response from elasticsearch.
+
+[float]
+=== Consequences and Expectations
+
+Small queue sizes mean that Logstash simply blocks and stalls safely during times of load or other temporary pipeline problems. There are two alternatives to this - unlimited queue length and dropping messages. Unlimited queues grow grow unbounded and eventually exceed memory causing a crash which loses all of those messages. Dropping messages is also an undesirable behavior in most cases.
+
+At a minimum, Logstash will have probably 3 threads (2 if you have no filters). One input, one filter worker, and one output thread each.
+
+If you see Logstash using multiple CPUs, this is likely why. If you want to know more about what each thread is doing, you should read this: <http://www.semicomplete.com/blog/geekery/debugging-java-performance.html>.
+
+Threads in java have names, and you can use jstack and top to figure out who is using what resources. The URL above will help you learn how to do this.
+
+On Linux platforms, Logstash will label all the threads it can with something descriptive. Inputs will show up as "<inputname" and filter workers as "|worker" and outputs as ">outputworker" (or something similar).  Other threads may be labeled as well, and are intended to help you identify their purpose should you wonder why they are consuming resources!
+
diff --git a/docs/asciidoc/static/logstash-docs-home.asciidoc b/docs/asciidoc/static/logstash-docs-home.asciidoc
new file mode 100644
index 00000000000..19bd3281184
--- /dev/null
+++ b/docs/asciidoc/static/logstash-docs-home.asciidoc
@@ -0,0 +1,30 @@
+[[logstash-docs-home]]
+== Logstash Documentation
+Pretty self-explanatory, really
+
+=== Downloads and Releases
+* http://www.elasticsearch.org/overview/logstash/download/[Download Logstash 1.4.2]
+* http://www.elasticsearch.org/blog/apt-and-yum-repositories/[package repositories]
+* http://www.elasticsearch.org/blog/logstash-1-4-2/[release notes]
+* https://github.com/elasticsearch/logstash/blob/master/CHANGELOG[view changelog]
+* https://github.com/elasticsearch/puppet-logstash[Puppet Module]
+
+=== Plugins
+* http://elasticsearch.org/#[contrib plugins]
+* http://elasticsearch.org/#[writing your own plugins]
+* http://elasticsearch.org/#[Inputs] / http://elasticsearch.org/#[Filters] / http://elasticsearch.org/#[Outputs]
+* http://elasticsearch.org/#[Codecs]
+* http://elasticsearch.org/#[(more)]
+
+=== HOWTOs, References, Information
+* http://elasticsearch.org/#[Getting Started with Logstash]
+* http://elasticsearch.org/#[Configuration file overview]
+* http://elasticsearch.org/#[Command-line flags]
+* http://elasticsearch.org/#[The life of an event in Logstash]
+* http://elasticsearch.org/#[Using conditional logic]
+* http://elasticsearch.org/#[Glossary]
+* http://elasticsearch.org/#[(more)]
+
+=== About / Videos / Blogs
+* http://elasticsearch.org/#[Videos]
+* http://elasticsearch.org/#[Blogs]
diff --git a/docs/asciidoc/static/logstash-glossary.asciidoc b/docs/asciidoc/static/logstash-glossary.asciidoc
new file mode 100644
index 00000000000..9715ffa6c76
--- /dev/null
+++ b/docs/asciidoc/static/logstash-glossary.asciidoc
@@ -0,0 +1,132 @@
+== Glossary 
+Logstash Glossary
+
+apache ::
+	A very common open source web server application, which produces logs easily consumed by Logstash (Apache Common/Combined Log Format).
+
+agent ::
+	An invocation of Logstash with a particular configuration, allowing it to operate as a "shipper", a "collector", or a combination of functionalities.
+
+
+broker ::
+	An intermediary used in a multi-tiered Logstash deployment which allows a queueing mechanism to be used. Examples of brokers are Redis, RabbitMQ, and Apache Kafka. This pattern is a common method of building fault-tolerance into a Logstash architecture. 
+
+buffer::
+	Within Logstash, a temporary storage area where events can queue up, waiting to be processed. The default queue size is 20 events, but it is not recommended to increase this, as Logstash is not designed to operate as a queueing mechanism.
+
+centralized::
+	A configuration of Logstash in which the Logstash agent, input and output sources live on multiple machines, and the pipeline passes through these tiers.
+
+codec::
+	A Logstash plugin which works within an input or output plugin, and usually aims to serialize or deserialize data flowing through the Logstash pipeline. A common example is the JSON codec, which allows Logstash inputs to receive data which arrives in JSON format, or output event data in JSON format.
+
+collector::
+	An instance of Logstash which receives external events from another instance of Logstash, or perhaps some other client, either remote or local.
+
+conditional::
+	In a computer programming context, a control flow which executes certain actions based on true/false values of a statement (called the condition). Often expressed in the form of "if ... then ... (elseif ...) else". Logstash has built-in conditionals to allow users control of the plugin pipeline.
+
+elasticsearch::
+	An open-source, Lucene-based, RESTful search and analytics engine written in Java, with supported clients in various languages such as Perl, Python, Ruby, Java, etc. 
+
+event::
+	In Logstash parlance, a single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the Logstash pipeline.
+
+field::
+	A data point (often a key-value pair) within a full Logstash event, e.g. "timestamp", "message", "hostname", "ipaddress". Also used to describe a key-value pair in Elasticsearch.
+
+file::
+	A resource storing binary data (which might be text, image, application, etc.) on a physical storage media. In the Logstash context, a common input source which monitors a growing collection of text-based log lines.
+
+filter:
+	An intermediary processing mechanism in the Lostash pipeline. Typically, filters act upon event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to configuration rules. The second phase of the typical Logstash pipeline (inputs->filters->outputs). 
+
+fluentd::
+	Like Logstash, another open-source tool for collecting logs and events, with plugins to extend functionality.
+
+ganglia::
+	A scalable, distributed monitoring system suitable for large clusters. Logstash features both an input and an output to enable reading from, and writing to Ganglia.
+
+graphite::
+	A highly-scalable realtime graphing application, which presents graphs through web interfaces. Logstash provides an output which ships event data to Graphite for visualization.
+
+heka::
+	An open-source event processing system developed by Mozilla and often compared to Logstash.
+
+index::
+	An index can be seen as a named collection of documents in Elasticsearch which are available for searching and querying. It is a logical namespace which maps to one or more primary shards and can have zero or more replica shards.
+
+indexer::
+	Refers to a Logstash instance which is tasked with interfacing with an Elasticsearch cluster in order to index event data.
+
+input::
+	The means for ingesting data into Logstash. Inputs allow users to pull data from files, network sockets, other applications, etc. The initial phase of the typical Logstash pipeline (inputs->filters->outputs). 
+
+jar / jarfile::
+	A packaging method for Java libraries. Since Logstash runs on the JRuby runtime environment, it is possible to use these Java libraries to provide extra functionality to Logstash.
+
+java::
+	An object-oriented programming language popular for its flexibility, extendability and portability.
+
+jRuby:
+	JRuby is a 100% Java implementation of the Ruby programming language, which allows Ruby to run in the JVM. Logstash typically runs in JRuby, which provides it with a fast, extensible runtime environment. 
+
+kibana::
+	A visual tool for viewing time-based data which has been stored in Elasticsearch. Kibana features a powerful set of functionality based on panels which query Elasticsearch in different ways.
+
+log::
+	A snippet of textual information emitted by a device, ostensibly with some pertinent information about the status of said device.
+
+log4j::
+	A very common Java-based logging utility.
+
+Logstash::
+	An application which offers a powerful data processing pipeline, allowing users to consume information from various sources, enrich the data, and output it to any number of other sources.
+
+lumberjack::
+	A protocol for shipping logs from one location to another, in a secure and optimized manner. Also the (deprecated) name of a software application, now known as Logstash Forwarder (LSF).
+
+output::
+	The means for passing event data out of Logstash into other applications, network endpoints, files, etc. The last phase of the typical Logstash pipeline (inputs->filters->outputs). 
+
+pipeline::
+	A term used to describe the flow of events through the Logstash workflow. The pipeline typically consists of a series of inputs, filters, and outputs.
+
+plugin::
+	A generic term referring to an input, codec, filter, or output which extends basic Logstash functionality.
+
+redis::
+	An open-source key-value store and cache which is often used in conjunction with Logstash as a message broker.
+
+ruby::
+	A popular, open-source, object-oriented programming language in which Logstash is implemented.
+
+shell::
+	A command-line interface to an operating system.
+
+shipper::
+	An instance of Logstash which send events to another instance of Logstash, or some other application.
+
+statsd::
+	A network daemon for aggregating statistics, such as counters and timers, and shipping over UDP to backend services, such as Graphite or Datadog. Logstash provides an output to statsd.
+
+stdin::
+	An I/O stream providing input to a software application. In Logstash, an input which receives data from this stream.
+
+stdout::
+	An I/O stream producing output from a software application. In Logstash, an output which produces data from this stream.
+
+syslog::
+	A popular method for logging messages from a computer. The standard is somewhat loose, but Logstash has tools (input, grok patterns) to make this simpler.
+
+standalone::
+	A configuration of Logstash in which the Logstash agent, input and output sources typically live on the same host machine.
+
+thread::
+	Parallel sequences of execution within a process which allow a computer to perform several tasks simultaneously, in a multi-processor environment. Logstash takes advantage of this functionality, by specifying the "-w" flag
+
+type::
+	In Elasticsearch type, a type can be compared to a table in a relational database. Each type has a list of fields that can be specified for documents of that type. The mapping defines how each field in the document is analyzed. To index documents, it is required to specify both an index and a type.
+
+worker::
+	The filter thread model used by Logstash, where each worker receives an event and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety). 
diff --git a/docs/asciidoc/static/plugin-milestones.asciidoc b/docs/asciidoc/static/plugin-milestones.asciidoc
new file mode 100644
index 00000000000..5f767d4fe03
--- /dev/null
+++ b/docs/asciidoc/static/plugin-milestones.asciidoc
@@ -0,0 +1,29 @@
+== Plugin Milestones
+
+[float]
+=== Why Milestones?
+Plugins (inputs/outputs/filters/codecs) have a milestone label in Logstash. This is to provide an indicator to the end-user as to the kinds of changes a given plugin could have between Logstash releases.
+
+The desire here is to allow plugin developers to quickly iterate on possible new plugins while conveying to the end-user a set of expectations about that plugin.
+
+[float]
+=== Milestone 1
+
+Plugins at this milestone need your feedback to improve! Plugins at this milestone may change between releases as the community figures out the best way for the plugin to behave and be configured.
+
+[float]
+=== Milestone 2
+
+Plugins at this milestone are more likely to have backwards-compatibility to previous releases than do Milestone 1 plugins. This milestone also indicates a greater level of in-the-wild usage by the community than the previous milestone.
+
+[float]
+=== Milestone 3
+
+Plugins at this milestone have strong promises towards backwards-compatibility. This is enforced with automated tests to ensure behavior and configuration are consistent across releases.
+
+[float]
+=== Milestone 0
+
+This milestone appears at the bottom of the page because it is very infrequently used.
+
+This milestone marker is used to generally indicate that a plugin has no active code maintainer nor does it have support from the community in terms of getting help.
diff --git a/docs/asciidoc_index.rb b/docs/asciidoc_index.rb
new file mode 100644
index 00000000000..be1b94403f8
--- /dev/null
+++ b/docs/asciidoc_index.rb
@@ -0,0 +1,35 @@
+#!/usr/bin/env ruby
+
+require "erb"
+
+if ARGV.size != 2
+  $stderr.puts "No path given to search for plugin docs"
+  $stderr.puts "Usage: #{$0} plugin_doc_dir type"
+  exit 1
+end
+
+
+def plugins(glob)
+  plugins=Hash.new []
+  files = Dir.glob(glob)
+  files.collect { |f| File.basename(f).gsub(".asciidoc", "") }.each {|plugin|
+    first_letter = plugin[0,1]
+    plugins[first_letter] += [plugin]
+  }
+  return Hash[plugins.sort]
+end # def plugins
+
+basedir = ARGV[0]
+type = ARGV[1]
+
+docs = plugins(File.join(basedir, "#{type}/*.asciidoc"))
+template_path = File.join(File.dirname(__FILE__), "index-#{type}.asciidoc.erb")
+template = File.new(template_path).read
+erb = ERB.new(template, nil, "-")
+
+path = "#{basedir}/#{type}.asciidoc"
+
+File.open(path, "w") do |out|
+  html = erb.result(binding)
+  out.puts(html)
+end
diff --git a/docs/asciidocgen.rb b/docs/asciidocgen.rb
new file mode 100644
index 00000000000..b0835722e2d
--- /dev/null
+++ b/docs/asciidocgen.rb
@@ -0,0 +1,265 @@
+require "rubygems"
+require "erb"
+require "optparse"
+
+$: << Dir.pwd
+$: << File.join(File.dirname(__FILE__), "..", "lib")
+
+require "logstash/config/mixin"
+require "logstash/inputs/base"
+require "logstash/codecs/base"
+require "logstash/filters/base"
+require "logstash/outputs/base"
+require "logstash/version"
+
+class LogStashConfigAsciiDocGenerator
+  COMMENT_RE = /^ *#(?: (.*)| *$)/
+
+  def initialize
+    @rules = {
+      COMMENT_RE => lambda { |m| add_comment(m[1]) },
+      /^ *class.*< *LogStash::(Outputs|Filters|Inputs|Codecs)::(Base|Threadable)/ => \
+        lambda { |m| set_class_description },
+      /^ *config +[^=].*/ => lambda { |m| add_config(m[0]) },
+      /^ *milestone .*/ => lambda { |m| set_milestone(m[0]) },
+      /^ *config_name .*/ => lambda { |m| set_config_name(m[0]) },
+      /^ *flag[( ].*/ => lambda { |m| add_flag(m[0]) },
+      /^ *(class|def|module) / => lambda { |m| clear_comments },
+    }
+
+    if File.exists?("build/contrib_plugins")
+      @contrib_list = File.read("build/contrib_plugins").split("\n")
+    else
+      @contrib_list = []
+    end
+  end
+
+  def parse(string)
+    clear_comments
+    buffer = ""
+    string.split(/\r\n|\n/).each do |line|
+      # Join long lines
+      if line =~ COMMENT_RE
+        # nothing
+      else
+        # Join extended lines
+        if line =~ /(, *$)|(\\$)|(\[ *$)/
+          buffer += line.gsub(/\\$/, "")
+          next
+        end
+      end
+
+      line = buffer + line
+      buffer = ""
+
+      @rules.each do |re, action|
+        m = re.match(line)
+        if m
+          action.call(m)
+        end
+      end # RULES.each
+    end # string.split("\n").each
+  end # def parse
+
+  def set_class_description
+    @class_description = @comments.join("\n")
+    clear_comments
+  end # def set_class_description
+
+  def add_comment(comment)
+    return if comment == "encoding: utf-8"
+    @comments << comment
+  end # def add_comment
+
+  def add_config(code)
+    # I just care about the 'config :name' part
+    code = code.sub(/,.*/, "")
+
+    # call the code, which calls 'config' in this class.
+    # This will let us align comments with config options.
+    name, opts = eval(code)
+
+    # TODO(sissel): This hack is only required until regexp configs
+    # are gone from logstash.
+    name = name.to_s unless name.is_a?(Regexp)
+
+    description = @comments.join("\n")
+    @attributes[name][:description] = description
+    clear_comments
+  end # def add_config
+
+  def add_flag(code)
+    # call the code, which calls 'config' in this class.
+    # This will let us align comments with config options.
+    #p :code => code
+    fixed_code = code.gsub(/ do .*/, "")
+    #p :fixedcode => fixed_code
+    name, description = eval(fixed_code)
+    @flags[name] = description
+    clear_comments
+  end # def add_flag
+
+  def set_config_name(code)
+    name = eval(code)
+    @name = name
+  end # def set_config_name
+
+  def set_milestone(code)
+    @milestone = eval(code)
+  end
+
+  # pretend to be the config DSL and just get the name
+  def config(name, opts={})
+    return name, opts
+  end # def config
+
+  # Pretend to support the flag DSL
+  def flag(*args, &block)
+    name = args.first
+    description = args.last
+    return name, description
+  end # def config
+
+  # pretend to be the config dsl's 'config_name' method
+  def config_name(name)
+    return name
+  end # def config_name
+
+  # pretend to be the config dsl's 'milestone' method
+  def milestone(m)
+    return m
+  end # def milestone
+
+  def clear_comments
+    @comments.clear
+  end # def clear_comments
+
+  def generate(file, settings)
+    @class_description = ""
+    @milestone = ""
+    @comments = []
+    @attributes = Hash.new { |h,k| h[k] = {} }
+    @flags = {}
+
+    # local scoping for the monkeypatch belowg
+    attributes = @attributes
+    # Monkeypatch the 'config' method to capture
+    # Note, this monkeypatch requires us do the config processing
+    # one at a time.
+    #LogStash::Config::Mixin::DSL.instance_eval do
+      #define_method(:config) do |name, opts={}|
+        #p name => opts
+        #attributes[name].merge!(opts)
+      #end
+    #end
+
+    # Loading the file will trigger the config dsl which should
+    # collect all the config settings.
+
+    # include the plugin lib dir for loading specific files
+
+    $: << File.join(File.dirname(file), "..", "..")
+    # include the lib dir of the plugin it self for any local dependencies
+    load file
+
+    # Get the correct base path
+    base = File.join(LogStash::Environment::LOGSTASH_HOME,'lib/logstash', file.split("/")[-2])
+
+    # parse base first
+    parse(File.new(File.join(base, "base.rb"), "r").read)
+
+    # Now parse the real library
+    code = File.new(file).read
+
+    # inputs either inherit from Base or Threadable.
+    if code =~ /\< LogStash::Inputs::Threadable/
+      parse(File.new(File.join(base, "threadable.rb"), "r").read)
+    end
+
+    if code =~ /include LogStash::PluginMixins/
+      mixin = code.gsub(/.*include LogStash::PluginMixins::(\w+)\s.*/m, '\1')
+      mixin.gsub!(/(.)([A-Z])/, '\1_\2')
+      mixin.downcase!
+      #parse(File.new(File.join(base, "..", "plugin_mixins", "#{mixin}.rb")).read)
+      #TODO: RP make this work better with the naming
+      mixinfile = Dir.glob(File.join(LogStash::Environment.logstash_gem_home,'gems',"logstash-mixin-#{mixin.split('_').first}-*",'lib/logstash/plugin_mixins', "#{mixin}.rb")).first
+      parse(File.new(mixinfile).read)
+  
+    end
+
+    parse(code)
+
+    puts "Generating docs for #{file}"
+
+    if @name.nil?
+      $stderr.puts "Missing 'config_name' setting in #{file}?"
+      return nil
+    end
+
+    klass = LogStash::Config::Registry.registry[@name]
+    if klass.ancestors.include?(LogStash::Inputs::Base)
+      section = "input"
+    elsif klass.ancestors.include?(LogStash::Filters::Base)
+      section = "filter"
+    elsif klass.ancestors.include?(LogStash::Outputs::Base)
+      section = "output"
+    elsif klass.ancestors.include?(LogStash::Codecs::Base)
+      section = "codec"
+    end
+
+    template_file = File.join(File.dirname(__FILE__), "plugin-doc.asciidoc.erb")
+    template = ERB.new(File.new(template_file).read, nil, "-")
+
+    is_contrib_plugin = @contrib_list.include?(file)
+
+    # descriptions are assumed to be markdown
+    description = @class_description
+
+    klass.get_config.each do |name, settings|
+      @attributes[name].merge!(settings)
+      default = klass.get_default(name)
+      unless default.nil?
+        @attributes[name][:default] = default
+      end
+    end
+    sorted_attributes = @attributes.sort { |a,b| a.first.to_s <=> b.first.to_s }
+    klassname = LogStash::Config::Registry.registry[@name].to_s
+    name = @name
+
+    synopsis_file = File.join(File.dirname(__FILE__), "plugin-synopsis.asciidoc.erb")
+    synopsis = ERB.new(File.new(synopsis_file).read, nil, "-").result(binding)
+
+    if settings[:output]
+      dir = File.join(settings[:output], section + "s")
+      path = File.join(dir, "#{name}.asciidoc")
+      Dir.mkdir(settings[:output]) if !File.directory?(settings[:output])
+      Dir.mkdir(dir) if !File.directory?(dir)
+      File.open(path, "w") do |out|
+        html = template.result(binding)
+        html.gsub!("%VERSION%", LOGSTASH_VERSION)
+        html.gsub!("%PLUGIN%", @name)
+        out.puts(html)
+      end
+    else
+      puts template.result(binding)
+    end
+  end # def generate
+
+end # class LogStashConfigDocGenerator
+
+if __FILE__ == $0
+  opts = OptionParser.new
+  settings = {}
+  opts.on("-o DIR", "--output DIR",
+          "Directory to output to; optional. If not specified,"\
+          "we write to stdout.") do |val|
+    settings[:output] = val
+  end
+
+  args = opts.parse(ARGV)
+
+  args.each do |arg|
+    gen = LogStashConfigAsciiDocGenerator.new
+    gen.generate(arg, settings)
+  end
+end
diff --git a/docs/index-codecs.asciidoc.erb b/docs/index-codecs.asciidoc.erb
new file mode 100644
index 00000000000..c87562f4218
--- /dev/null
+++ b/docs/index-codecs.asciidoc.erb
@@ -0,0 +1,43 @@
+[[codec-plugins]]
+== Codec plugins
+
+The plugins in this section change the data representation of an input or output in Logstash.
+
+The following codec plugins are available:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-codecs-letters-<%= letter %>, <%=letter %>>>
+<%- end -%>
+
+<%-
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-codecs-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-codecs-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::codecs/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/index-filters.asciidoc.erb b/docs/index-filters.asciidoc.erb
new file mode 100644
index 00000000000..5f9e5363c9e
--- /dev/null
+++ b/docs/index-filters.asciidoc.erb
@@ -0,0 +1,43 @@
+[[filter-plugins]]
+== Filter plugins
+
+The plugins in this section apply intermediary processing to the information from a given source in Logstash.
+
+The following filter plugins are available:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-filters-letters-<%= letter %>, <%=letter %>>>
+<%- end -%>
+
+<%-
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-filters-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-filters-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::filters/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/index-inputs.asciidoc.erb b/docs/index-inputs.asciidoc.erb
new file mode 100644
index 00000000000..ade4fc9b249
--- /dev/null
+++ b/docs/index-inputs.asciidoc.erb
@@ -0,0 +1,43 @@
+[[input-plugins]]
+== Input plugins
+
+An input plugin enables a specific source of events to be read by Logstash.
+
+The following input plugins are available:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-inputs-letters-<%= letter %>, <%=letter %>>>
+<%- 
+end
+
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-inputs-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-inputs-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::inputs/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/index-outputs.asciidoc.erb b/docs/index-outputs.asciidoc.erb
new file mode 100644
index 00000000000..60744157fda
--- /dev/null
+++ b/docs/index-outputs.asciidoc.erb
@@ -0,0 +1,43 @@
+[[output-plugins]]
+== Output plugins
+
+Output plugins manage the final disposition of event data.
+
+The following output plugins are available:
+
+<%-
+full_list=[]
+letters=[]
+docs.each do |doc|
+letter = doc[0]
+letters << letter
+-%>
+<<plugins-outputs-letters-<%= letter %>, <%=letter %>>>
+<%- end -%>
+
+<%-
+cols=3
+rows=(docs.count/cols)+1
+item=0
+r=0
+-%>
+[cols="asciidoc,asciidoc,asciidoc"]
+|=======================================================================
+<%- while r < rows do -%>
+<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-outputs-letters-<%=letters[item] %>]] <% end %>
+<%- letter = letters[item];
+arr = docs[letter]
+if ! arr.nil?
+arr.each do |plugin_item|
+full_list << plugin_item
+%>* <<plugins-outputs-<%=plugin_item -%>,<%=plugin_item -%>>>
+<%- end 
+end -%>
+<%- item+=1; c+=1; end; r+=1 end -%>
+|=======================================================================
+
+<%-
+full_list.each do |plugin|
+-%>
+include::outputs/<%=plugin %>.asciidoc[]
+<%- end -%>
diff --git a/docs/learn.md b/docs/learn.md
index a2fa5412df7..e26bf320fa8 100644
--- a/docs/learn.md
+++ b/docs/learn.md
@@ -33,7 +33,7 @@ email the mailing list (logstash-users@googlegroups.com). Further, there is also
 an IRC channel - #logstash on irc.freenode.org.
 
 If you find a bug or have a feature request, file them
-on <http://logstash.jira.com/>. (Honestly though, if you prefer email or irc
+on [github](https://github.com/elasticsearch/logstas/issues). (Honestly though, if you prefer email or irc
 for such things, that works for me, too.)
 
 ## Download It
diff --git a/docs/plugin-doc.asciidoc.erb b/docs/plugin-doc.asciidoc.erb
new file mode 100644
index 00000000000..36e617ad673
--- /dev/null
+++ b/docs/plugin-doc.asciidoc.erb
@@ -0,0 +1,55 @@
+<%- plugin_name = name -%>
+[[plugins-<%= section %>s-<%= name %>]]
+=== <%= name %>
+
+<%= description %>
+
+&nbsp;
+
+==== Synopsis
+
+This plugin supports the following configuration options:
+
+<%= synopsis -%>
+
+==== Details
+
+&nbsp;
+
+<% sorted_attributes.each do |name, config| -%>
+<%
+     if name.is_a?(Regexp)
+       name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+       is_regexp = true
+     else
+       is_regexp = false
+     end
+-%>
+[[plugins-<%= section%>s-<%= plugin_name%>-<%= name%>]]
+===== `<%= name %>` <%= " (DEPRECATED)" if config[:deprecated] %>
+
+<% if config[:required] -%>
+  * This is a required setting.
+<% end -%>
+<% if config[:deprecated] -%>
+  * DEPRECATED WARNING: This configuration item is deprecated and may not be available in future versions.
+<% end -%>
+<% if is_regexp -%>
+  * The configuration attribute name here is anything that matches the above regular expression.
+<% end -%>
+<% if config[:validate].is_a?(Symbol) -%>
+  * Value type is <<<%= config[:validate] %>,<%= config[:validate] %>>>
+<% elsif config[:validate].nil? -%>
+  <li> Value type is <<string,string>>
+<% elsif config[:validate].is_a?(Array) -%>
+  * Value can be any of: `<%= config[:validate].join('`, `') %>`
+<% end -%>
+<% if config.include?(:default) -%>
+  * Default value is `<%= config[:default].inspect %>`
+<% else -%>
+  * There is no default value for this setting.
+<% end -%>
+
+<%= config[:description] %>
+
+<% end -%>
diff --git a/docs/plugin-doc.html.erb b/docs/plugin-doc.html.erb
index 5b9733afa52..c236314e0af 100644
--- a/docs/plugin-doc.html.erb
+++ b/docs/plugin-doc.html.erb
@@ -5,17 +5,13 @@ layout: content_right
 <h2><%= name %></h2>
 <h3>Milestone: <a href="../plugin-milestones"><%= @milestone %></a></h3>
 <% if is_contrib_plugin -%>
-<div class="community-plugin-notice">
-  <strong>This is a community-contributed plugin!</strong> It does not ship with logstash by default, but it is easy to install!
-  To use this, you must have <a href="../contrib-plugins">installed the contrib plugins package</a>.
-</div>
 <% end -%>
 
 <%= description %>
 
 <h3> Synopsis </h3>
 
-This is what it might look like in your config file:
+A sample configuration file is shown here:
 
 <pre><code><% if section == "codec" -%>
 # with an input plugin:
@@ -56,7 +52,7 @@ input {
 
 <ul>
 <% if config[:deprecated] -%>
-  <li> DEPRECATED WARNING: This config item is deprecated. It may be removed in a further version. </li>
+  <li> DEPRECATED WARNING: This configuration item is deprecated and may not be included in later versions.</li>
 <% end -%>
 <% if is_regexp -%>
   <li> The configuration attribute name here is anything that matches the above regular expression. </li>
diff --git a/docs/plugin-synopsis.asciidoc.erb b/docs/plugin-synopsis.asciidoc.erb
new file mode 100644
index 00000000000..87886f54aba
--- /dev/null
+++ b/docs/plugin-synopsis.asciidoc.erb
@@ -0,0 +1,51 @@
+<%- plugin_name = name -%>
+
+Required configuration options:
+
+[source,json]
+--------------------------
+<%= name %> {
+<% sorted_attributes.each do |name, config|
+   next if config[:deprecated]
+   next if !config[:required]
+-%>
+<%= "  " if section == "codec" %>    <%= name %> => ... 
+<% end -%>
+<%= "  " if section == "codec" %>}
+--------------------------
+
+<% if sorted_attributes.count > 0 %>
+
+Available configuration options:
+
+[cols="<,<,<,<m",options="header",]
+|=======================================================================
+|Setting |Input type|Required|Default value
+<% sorted_attributes.each do |name, config|
+   next if config[:deprecated]
+   if config[:validate].is_a?(Array) 
+     annotation = "|<<string,string>>, one of `#{config[:validate].inspect}`"
+   elsif config[:validate] == :path
+     annotation = "|a valid filesystem path"
+   else
+     annotation = "|<<#{config[:validate]},#{config[:validate]}>>"
+   end
+
+   if name.is_a?(Regexp)
+     name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+   end
+   if config[:required]
+     annotation += "|Yes"
+   else
+     annotation += "|No"
+   end
+   if config.include?(:default)
+     annotation += "|`#{config[:default].inspect}`"
+   else 
+     annotation += "|"
+   end
+-%>
+| <<plugins-<%= section %>s-<%=plugin_name%>-<%= name %>>> <%= annotation %>
+<% end -%>
+|=======================================================================
+<% end %>
diff --git a/docs/release-notes.md b/docs/release-notes.md
index e8b3324dd7d..254fee6b6c7 100644
--- a/docs/release-notes.md
+++ b/docs/release-notes.md
@@ -42,12 +42,12 @@ The old way to run logstash of `java -jar logstash.jar` is now replaced with
 For example:
 
     # Old way:
-    % java -jar logstash-1.3.3-flatjar.jar agent -f logstash.conf
+    `% java -jar logstash-1.3.3-flatjar.jar agent -f logstash.conf`
 
     # New way:
-    % bin/logstash agent -f logstash.conf
+    `% bin/logstash agent -f logstash.conf`
 
-### contrib plugins
+### plugins
 
 Logstash has grown brilliantly over the past few years with great contributions
 from the community. Now having 165 plugins, it became hard for us (the Logstash
@@ -56,15 +56,9 @@ contributed plugin. We combed through all the plugins and picked the ones we
 felt strongly we could support, and those now ship by default with Logstash.
 
 All the other plugins are now available in a contrib package. All plugins
-continue to be open source and free, of course! Installing plugins from the
-contrib package is very easy:
+continue to be open source and free, of course! Installing plugins is very easy:
 
+....
     % cd /path/to/logstash-%VERSION%/
-    % bin/plugin install contrib
-
-A bonus effect of this decision is that the default Logstash download size
-shrank by 19MB compared to the previous release because we were able to shed
-some lesser-used dependencies.
-
-You can learn more about contrib plugins on the [contrib plugins
-page](http://logstash.net/docs/%VERSION%/contrib-plugins)
+    % bin/plugin install [PLUGIN_NAME]
+....
diff --git a/docs/repositories.md b/docs/repositories.md
index 7eb163bde9b..320059e5e9b 100644
--- a/docs/repositories.md
+++ b/docs/repositories.md
@@ -6,13 +6,13 @@ layout: content_right
 
 We also have Logstash available as APT and YUM repositories.
 
-Our public signing key can be found on the [Elasticsearch packages apt GPG signing key page](http://packages.elasticsearch.org/GPG-KEY-elasticsearch)
+Our public signing key can be found on the [Elasticsearch packages apt GPG signing key page](https://packages.elasticsearch.org/GPG-KEY-elasticsearch)
 
 ## Apt based distributions
 
 Add the key:
 
-     wget -O - http://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
+     wget -O - https://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
 
 Add the repo to /etc/apt/sources.list
 
@@ -23,13 +23,13 @@ Add the repo to /etc/apt/sources.list
 
 Add the key:
 
-     rpm --import http://packages.elasticsearch.org/GPG-KEY-elasticsearch
+     rpm --import https://packages.elasticsearch.org/GPG-KEY-elasticsearch
 
 Add the repo to /etc/yum.repos.d/ directory
 
      [logstash-1.4]
      name=logstash repository for 1.4.x packages
-     baseurl=http://packages.elasticsearch.org/logstash/1.4/centos
+     baseurl=https://packages.elasticsearch.org/logstash/1.4/centos
      gpgcheck=1
-     gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearch
+     gpgkey=https://packages.elasticsearch.org/GPG-KEY-elasticsearch
      enabled=1
diff --git a/docs/tutorials/getting-started-with-logstash.asciidoc b/docs/tutorials/getting-started-with-logstash.asciidoc
index 3bc39f7d9b9..aa60074c79c 100644
--- a/docs/tutorials/getting-started-with-logstash.asciidoc
+++ b/docs/tutorials/getting-started-with-logstash.asciidoc
@@ -48,7 +48,7 @@ hello world
 
 OK, that's interesting... We ran Logstash with an input called "stdin", and an output named "stdout", and Logstash basically echoed back whatever we typed in some sort of structured format. Note that specifying the *-e* command line flag allows Logstash to accept a configuration directly from the command line. This is especially useful for quickly testing configurations without having to edit a file between iterations.
 
-Let's try a slightly fancier example. First, you should exit Logstash by issuing a 'CTRL-C' command in the shell in which it is running. Now run Logstash again with the following command:
+Let's try a slightly fancier example. First, you should exit Logstash by issuing a 'CTRL-D' command (or 'CTRL-C Enter') in the shell in which it is running. Now run Logstash again with the following command:
 ----
 bin/logstash -e 'input { stdin { } } output { stdout { codec => rubydebug } }'
 ----
@@ -75,7 +75,7 @@ cd elasticsearch-%ELASTICSEARCH_VERSION%/
 ./bin/elasticsearch
 ----
 
-NOTE: This tutorial specifies running Logstash %VERSION% with Elasticsearch %ELASTICSEARCH_VERSION%. Each release of Logstash has a *recommended* version of Elasticsearch to pair with. Make sure the versions match based on the http://logstash.net/docs/latest[Logstash version] you're running!
+NOTE: This tutorial specifies running Logstash %VERSION% with Elasticsearch %ELASTICSEARCH_VERSION%. Each release of Logstash has a *recommended* version of Elasticsearch to pair with. Make sure the versions match based on the http://www.elasticsearch.org/overview/logstash[Logstash version] you're running!
 
 More detailed information on installing and configuring Elasticsearch can be found on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html[The Elasticsearch reference pages]. However, for the purposes of Getting Started with Logstash, the default installation and configuration of Elasticsearch should be sufficient.
 
@@ -146,7 +146,7 @@ Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configurati
 ==== Inputs
 Inputs are the mechanism for passing log data to Logstash. Some of the more useful, commonly-used ones are:
 
-* *file*: reads from a file on the filesystem, much like the UNIX command "tail -0a"
+* *file*: reads from a file on the filesystem, much like the UNIX command "tail -0F"
 * *syslog*: listens on the well-known port 514 for syslog messages and parses according to RFC3164 format
 * *redis*: reads from a redis server, using both redis channels and also redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
 * *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
@@ -174,7 +174,7 @@ Codecs are basically stream filters which can operate as part of an input, or an
 * *json*: encode / decode data in JSON format
 * *multiline*: Takes multiple-line text events and merge them into a single event, e.g. java exception and stacktrace messages
 
-For the complete list of (current) configurations, visit the Logstash "plugin configuration" section of the http://logstash.net/docs/latest/[Logstash documentation page].
+For the complete list of (current) configurations, visit the Logstash "plugin configuration" section of the http://www.elasticsearch.org/overview/logstash[Logstash documentation page].
 
 
 == More fun with Logstash
@@ -250,7 +250,7 @@ You should see something returned to STDOUT which looks like this:
 ----
 As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This will be extremely useful later when we start querying and analyzing our log data... for example, we'll be able to run reports on HTTP response codes, IP addresses, referrers, etc. very easily. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you're attempting to parse a fairly common log format, someone has already done the work for you. For more details, see the list of https://github.com/logstash/logstash/blob/master/patterns/grok-patterns[logstash grok patterns] on github.
 
-The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs, for example... the ability to tell Logstash "use this value as the timestamp for this event".
+The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs, for example... the ability to tell Logstash "use this value as the timestamp for this event". For non-english installation you may have to precise the locale in date filter (locale => en).
 
 == Useful Examples
 
@@ -261,7 +261,7 @@ Now, let's configure something actually *useful*... apache2 access log files! We
 input {
   file {
     path => "/tmp/access_log"
-    start_position => beginning
+    start_position => "beginning"
   }
 }
 
diff --git a/dripmain.rb b/dripmain.rb
new file mode 100644
index 00000000000..23426a5b063
--- /dev/null
+++ b/dripmain.rb
@@ -0,0 +1,29 @@
+# dripmain.rb is called by org.jruby.main.DripMain to further warm the JVM with any preloading
+# that we can do to speedup future startup using drip.
+
+# we are out of the application context here so setup the load path and gem paths
+lib_path = File.expand_path(File.join(File.dirname(__FILE__), "./lib"))
+$:.unshift(lib_path)
+
+require "logstash/environment"
+LogStash::Environment.set_gem_paths!
+
+# typical required gems and libs
+require "i18n"
+I18n.enforce_available_locales = true
+I18n.load_path << LogStash::Environment.locales_path("en.yml")
+require "cabin"
+require "stud/trap"
+require "stud/task"
+require "clamp"
+require "rspec"
+require "rspec/core/runner"
+
+require "logstash/namespace"
+require "logstash/program"
+require "logstash/agent"
+require "logstash/kibana"
+require "logstash/util"
+require "logstash/errors"
+require "logstash/pipeline"
+require "logstash/plugin"
diff --git a/extract_services.rb b/extract_services.rb
deleted file mode 100644
index fa6b71beb12..00000000000
--- a/extract_services.rb
+++ /dev/null
@@ -1,29 +0,0 @@
-# Extract META-INFO/services/* files from jars
-#
-require "optparse"
-
-output = nil
-
-flags = OptionParser.new do |opts|
-  opts.on("-o", "--output DIR",
-          "Where to write the merged META-INF/services/* files") do |dir|
-    output = dir
-  end
-end
-
-flags.parse!(ARGV)
-
-ARGV.each do |jar|
-  # Find any files matching /META-INF/services/* in any jar given on the
-  # command line.
-  # Append all file content to the output directory with the same file name
-  # as is in the jar.
-  glob = "file:///#{File.expand_path(jar)}!/META-INF/services/*"
-  Dir.glob(glob).each do |service|
-    name = File.basename(service)
-    File.open(File.join(output, name), "a") do |fd|
-      puts "Adding #{name} from #{File.basename(jar)}"
-      fd.write(File.read(service))
-    end
-  end
-end
diff --git a/gembag.rb b/gembag.rb
deleted file mode 100644
index 9f03c8c9bfd..00000000000
--- a/gembag.rb
+++ /dev/null
@@ -1,75 +0,0 @@
-#!/usr/bin/env ruby
-
-require "rbconfig"
-
-rubyabi = RbConfig::CONFIG["ruby_version"]
-target = "#{Dir.pwd}/vendor/bundle"
-gemdir = "#{target}/#{RUBY_ENGINE}/#{rubyabi}/"
-ENV["GEM_HOME"] = gemdir
-ENV["GEM_PATH"] = ""
-
-require "rubygems/specification"
-require "rubygems/commands/install_command"
-require "logstash/JRUBY-PR1448" if RUBY_PLATFORM == "java" && Gem.win_platform?
-
-def install_gem(name, requirement, target)
-  puts "Fetching and installing gem: #{name} (#{requirement})"
-
-  installer = Gem::Commands::InstallCommand.new
-  installer.options[:generate_rdoc] = false
-  installer.options[:generate_ri] = false
-  installer.options[:version] = requirement
-  installer.options[:args] = [name]
-  installer.options[:install_dir] = target
-
-  # ruby 2.0.0 / rubygems 2.x; disable documentation generation
-  installer.options[:document] = []
-  begin
-    installer.execute
-  rescue Gem::SystemExitException => e
-    if e.exit_code != 0
-      puts "Installation of #{name} failed"
-      raise
-    end
-  end
-end # def install_gem
-
-# Ensure bundler is available.
-begin
-  gem("bundler", ">=1.3.5")
-rescue Gem::LoadError => e
-  install_gem("bundler", ">= 1.3.5", ENV["GEM_HOME"])
-end
-
-require "bundler/cli"
-
-# Monkeypatch bundler to write a .lock file specific to the version of ruby.
-# This keeps MRI/JRuby/RBX from conflicting over the Gemfile.lock updates
-module Bundler
-  module SharedHelpers
-    def default_lockfile
-      ruby = "#{RUBY_ENGINE}-#{RbConfig::CONFIG["ruby_version"]}"
-      return Pathname.new("#{default_gemfile}.#{ruby}.lock")
-    end
-  end
-end
-
-if RUBY_ENGINE == "rbx"
-  begin
-    gem("rubysl")
-  rescue Gem::LoadError => e
-    install_gem("rubysl", ">= 0", ENV["GEM_HOME"])
-  end
-end
-
-# Try installing a few times in case we hit the "bad_record_mac" ssl error during installation.
-10.times do
-  begin
-    Bundler::CLI.start(["install", "--gemfile=tools/Gemfile", "--path", target, "--clean"])
-    break
-  rescue Gem::RemoteFetcher::FetchError => e
-    puts e.message
-    puts e.backtrace.inspect
-    sleep 5 #slow down a bit before retry
-  end
-end
diff --git a/lib/logstash/agent.rb b/lib/logstash/agent.rb
index 000d1ba4eb0..1ab88556da0 100644
--- a/lib/logstash/agent.rb
+++ b/lib/logstash/agent.rb
@@ -2,7 +2,9 @@
 require "clamp" # gem 'clamp'
 require "logstash/environment"
 require "logstash/errors"
-require "i18n"
+require "uri"
+require "net/http"
+LogStash::Environment.load_locale!
 
 class LogStash::Agent < Clamp::Command
   option ["-f", "--config"], "CONFIG_PATH",
@@ -285,6 +287,21 @@ def configure_plugin_path(paths)
   end # def configure_plugin_path
 
   def load_config(path)
+
+    uri = URI.parse(path)
+    case uri.scheme
+    when nil then
+      local_config(path)
+    when /http/ then
+      fetch_config(uri)
+    when "file" then
+      local_config(uri.path)
+    else
+      fail(I18n.t("logstash.agent.configuration.scheme-not-supported", :path => path))
+    end
+  end
+
+  def local_config(path)
     path = File.join(path, "*") if File.directory?(path)
 
     if Dir.glob(path).length == 0
@@ -292,6 +309,7 @@ def load_config(path)
     end
 
     config = ""
+    encoding_issue_files = []
     Dir.glob(path).sort.each do |file|
       next unless File.file?(file)
       if file.match(/~$/)
@@ -299,9 +317,24 @@ def load_config(path)
         next
       end
       @logger.debug("Reading config file", :file => file)
-      config << File.read(file) + "\n"
+      cfg = File.read(file)
+      if !cfg.ascii_only? && !cfg.valid_encoding?
+        encoding_issue_files << file
+      end
+      config << cfg + "\n"
+    end
+    if (encoding_issue_files.any?)
+      fail("The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}")
     end
     return config
   end # def load_config
 
+  def fetch_config(uri)
+    begin
+      Net::HTTP.get(uri) + "\n"
+    rescue Exception => e
+      fail(I18n.t("logstash.agent.configuration.fetch-failed", :path => uri.to_s, :message => e.message))
+    end
+  end
+
 end # class LogStash::Agent
diff --git a/lib/logstash/codecs/base.rb b/lib/logstash/codecs/base.rb
index e5041704d52..662f054dfde 100644
--- a/lib/logstash/codecs/base.rb
+++ b/lib/logstash/codecs/base.rb
@@ -23,13 +23,14 @@ def decode(data)
   alias_method :<<, :decode
 
   public
-  def encode(data)
+  def encode(event)
     raise "#{self.class}#encode must be overidden"
   end # def encode
 
   public 
   def teardown; end;
 
+  # @param block [Proc(event, data)] the callback proc passing the original event and the encoded event
   public
   def on_event(&block)
     @on_event = block
diff --git a/lib/logstash/codecs/collectd.rb b/lib/logstash/codecs/collectd.rb
deleted file mode 100644
index 2e7bf9afdf3..00000000000
--- a/lib/logstash/codecs/collectd.rb
+++ /dev/null
@@ -1,441 +0,0 @@
-# encoding utf-8
-require "date"
-require "logstash/codecs/base"
-require "logstash/namespace"
-require "tempfile"
-require "time"
-
-# Read events from the connectd binary protocol over the network via udp.
-# See https://collectd.org/wiki/index.php/Binary_protocol
-#
-# Configuration in your Logstash configuration file can be as simple as:
-#     input {
-#       udp {
-#         port => 28526
-#         buffer_size => 1452
-#         codec => collectd { }
-#       }
-#     }
-#
-# A sample collectd.conf to send to Logstash might be:
-#
-#     Hostname    "host.example.com"
-#     LoadPlugin interface
-#     LoadPlugin load
-#     LoadPlugin memory
-#     LoadPlugin network
-#     <Plugin interface>
-#         Interface "eth0"
-#         IgnoreSelected false
-#     </Plugin>
-#     <Plugin network>
-#         <Server "10.0.0.1" "25826">
-#         </Server>
-#     </Plugin>
-#
-# Be sure to replace "10.0.0.1" with the IP of your Logstash instance.
-#
-
-class ProtocolError < LogStash::Error; end
-class HeaderError < LogStash::Error; end
-class EncryptionError < LogStash::Error; end
-
-class LogStash::Codecs::Collectd < LogStash::Codecs::Base
-  config_name "collectd"
-  milestone 1
-
-  AUTHFILEREGEX = /([^:]+): (.+)/
-
-  PLUGIN_TYPE = 2
-  COLLECTD_TYPE = 4
-  SIGNATURE_TYPE = 512
-  ENCRYPTION_TYPE = 528
-
-  TYPEMAP = {
-    0               => "host",
-    1               => "@timestamp",
-    PLUGIN_TYPE     => "plugin",
-    3               => "plugin_instance",
-    COLLECTD_TYPE   => "collectd_type",
-    5               => "type_instance",
-    6               => "values",
-    7               => "interval",
-    8               => "@timestamp",
-    9               => "interval",
-    256             => "message",
-    257             => "severity",
-    SIGNATURE_TYPE  => "signature",
-    ENCRYPTION_TYPE => "encryption"
-  }
-
-  PLUGIN_TYPE_FIELDS = {
-    'host' => true,
-    '@timestamp' => true,
-  }
-
-  COLLECTD_TYPE_FIELDS = {
-    'host' => true,
-    '@timestamp' => true, 
-    'plugin' => true, 
-    'plugin_instance' => true,
-  }
-
-  INTERVAL_VALUES_FIELDS = {
-    "interval" => true, 
-    "values" => true,
-  }
-
-  INTERVAL_BASE_FIELDS = {
-    'host' => true,
-    'collectd_type' => true,
-    'plugin' => true, 
-    'plugin_instance' => true,
-    '@timestamp' => true,
-    'type_instance' => true,
-  }
-
-  INTERVAL_TYPES = {
-    7 => true,
-    9 => true,
-  }
-
-  SECURITY_NONE = "None"
-  SECURITY_SIGN = "Sign"
-  SECURITY_ENCR = "Encrypt"
-
-  # File path(s) to collectd types.db to use.
-  # The last matching pattern wins if you have identical pattern names in multiple files.
-  # If no types.db is provided the included types.db will be used (currently 5.4.0).
-  config :typesdb, :validate => :array
-
-  # Prune interval records.  Defaults to true.
-  config :prune_intervals, :validate => :boolean, :default => true
-
-  # Security Level. Default is "None". This setting mirrors the setting from the
-  # collectd [Network plugin](https://collectd.org/wiki/index.php/Plugin:Network)
-  config :security_level, :validate => [SECURITY_NONE, SECURITY_SIGN, SECURITY_ENCR],
-    :default => "None"
-
-  # Path to the authentication file. This file should have the same format as
-  # the [AuthFile](http://collectd.org/documentation/manpages/collectd.conf.5.shtml#authfile_filename)
-  # in collectd. You only need to set this option if the security_level is set to
-  # "Sign" or "Encrypt"
-  config :authfile, :validate => :string
-
-  public
-  def register
-    @logger.info("Starting Collectd codec...")
-    if @typesdb.nil?
-      @typesdb = LogStash::Environment.vendor_path("collectd/types.db")
-      if !File.exists?(@typesdb)
-        raise "You must specify 'typesdb => ...' in your collectd input (I looked for '#{@typesdb}')"
-      end
-      @logger.info("Using types.db", :typesdb => @typesdb.to_s)
-    end
-    @types = get_types(@typesdb)
-
-    if ([SECURITY_SIGN, SECURITY_ENCR].include?(@security_level))
-      if @authfile.nil?
-        raise "Security level is set to #{@security_level}, but no authfile was configured"
-      else
-        # Load OpenSSL and instantiate Digest and Crypto functions
-        require 'openssl'
-        @sha256 = OpenSSL::Digest::Digest.new('sha256')
-        @sha1 = OpenSSL::Digest::Digest.new('sha1')
-        @cipher = OpenSSL::Cipher.new('AES-256-OFB')
-        @auth = {}
-        parse_authfile
-      end
-    end
-  end # def register
-
-  public
-  def get_types(paths)
-    types = {}
-    # Get the typesdb
-    paths = Array(paths) # Make sure a single path is still forced into an array type
-    paths.each do |path|
-      @logger.info("Getting Collectd typesdb info", :typesdb => path.to_s)
-      File.open(path, 'r').each_line do |line|
-        typename, *line = line.strip.split
-        @logger.debug("typename", :typename => typename.to_s)
-        next if typename.nil? || typename[0,1] == '#'
-        types[typename] = line.collect { |l| l.strip.split(":")[0] }
-      end
-    end
-    @logger.debug("Collectd Types", :types => types.to_s)
-    return types
-  end # def get_types
-
-  # Lambdas for hash + closure methodology
-  # This replaces when statements for fixed values and is much faster
-  string_decoder = lambda { |body| body.pack("C*")[0..-2] }
-  numeric_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
-  counter_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("Q>")[0] }
-  gauge_decoder   = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
-  derive_decoder  = lambda { |body| body.slice!(0..7).pack("C*").unpack("q>")[0] }
-  # For Low-Resolution time
-  time_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2))).utc
-  end
-  # Hi-Resolution time
-  hirestime_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
-  end
-  # Hi resolution intervals
-  hiresinterval_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).to_i
-  end
-  # Values decoder
-  values_decoder = lambda do |body|
-    body.slice!(0..1)       # Prune the header
-    if body.length % 9 == 0 # Should be 9 fields
-      count = 0
-      retval = []
-      # Iterate through and take a slice each time
-      types = body.slice!(0..((body.length/9)-1))
-      while body.length > 0
-        # Use another hash + closure here...
-        retval << VALUES_DECODER[types[count]].call(body)
-        count += 1
-      end
-    else
-      @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
-    end
-    return retval
-  end
-  # Signature
-  signature_decoder = lambda do |body|
-    if body.length < 32
-      @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
-    elsif body.length < 33
-      @logger.warning("Received signature without username")
-    else
-      retval = []
-      # Byte 32 till the end contains the username as chars (=unsigned ints)
-      retval << body[32..-1].pack('C*')
-      # Byte 0 till 31 contain the signature
-      retval << body[0..31].pack('C*')
-    end
-    return retval
-  end
-  # Encryption
-  encryption_decoder = lambda do |body|
-    retval = []
-    user_length = (body.slice!(0) << 8) + body.slice!(0)
-    retval << body.slice!(0..user_length-1).pack('C*') # Username
-    retval << body.slice!(0..15).pack('C*')            # IV
-    retval << body.pack('C*')
-    return retval
-  end
-  # Lambda Hashes
-  ID_DECODER = {
-    0 => string_decoder,
-    1 => time_decoder,
-    2 => string_decoder,
-    3 => string_decoder,
-    4 => string_decoder,
-    5 => string_decoder,
-    6 => values_decoder,
-    7 => numeric_decoder,
-    8 => hirestime_decoder,
-    9 => hiresinterval_decoder,
-    256 => string_decoder,
-    257 => numeric_decoder,
-    512 => signature_decoder,
-    528 => encryption_decoder
-  }
-  # TYPE VALUES:
-  # 0: COUNTER
-  # 1: GAUGE
-  # 2: DERIVE
-  # 3: ABSOLUTE
-  VALUES_DECODER = {
-    0 => counter_decoder,
-    1 => gauge_decoder,
-    2 => derive_decoder,
-    3 => counter_decoder
-  }
-
-  public
-  def get_values(id, body)
-    # Use hash + closure/lambda to speed operations
-    ID_DECODER[id].call(body)
-  end
-
-  private
-  def parse_authfile
-    # We keep the authfile parsed in memory so we don't have to open the file
-    # for every event.
-    @logger.debug("Parsing authfile #{@authfile}")
-    if !File.exist?(@authfile)
-      raise LogStash::ConfigurationError, "The file #{@authfile} was not found"
-    end
-    @auth.clear
-    @authmtime = File.stat(@authfile).mtime
-    File.readlines(@authfile).each do |line|
-      #line.chomp!
-      k,v = line.scan(AUTHFILEREGEX).flatten
-      if k && v
-        @logger.debug("Added authfile entry '#{k}' with key '#{v}'")
-        @auth[k] = v
-      else
-        @logger.info("Ignoring malformed authfile line '#{line.chomp}'")
-      end
-    end
-  end # def parse_authfile
-
-  private
-  def get_key(user)
-    return if @authmtime.nil? or @authfile.nil?
-    # Validate that our auth data is still up-to-date
-    parse_authfile if @authmtime < File.stat(@authfile).mtime
-    key = @auth[user]
-    @logger.warn("User #{user} is not found in the authfile #{@authfile}") if key.nil?
-    return key
-  end # def get_key
-
-  private
-  def verify_signature(user, signature, payload)
-    # The user doesn't care about the security
-    return true if @security_level == SECURITY_NONE
-
-    # We probably got and array of ints, pack it!
-    payload = payload.pack('C*') if payload.is_a?(Array)
-
-    key = get_key(user)
-    return false if key.nil?
-
-    return OpenSSL::HMAC.digest(@sha256, key, user+payload) == signature
-  end # def verify_signature
-
-  private
-  def decrypt_packet(user, iv, content)
-    # Content has to have at least a SHA1 hash (20 bytes), a header (4 bytes) and
-    # one byte of data
-    return [] if content.length < 26
-    content = content.pack('C*') if content.is_a?(Array)
-    key = get_key(user)
-    if key.nil?
-      @logger.debug("Key was nil")
-      return []
-    end
-
-    # Set the correct state of the cipher instance
-    @cipher.decrypt
-    @cipher.padding = 0
-    @cipher.iv = iv
-    @cipher.key = @sha256.digest(key);
-    # Decrypt the content
-    plaintext = @cipher.update(content) + @cipher.final
-    # Reset the state, as adding a new key to an already instantiated state
-    # results in an exception
-    @cipher.reset
-
-    # The plaintext contains a SHA1 hash as checksum in the first 160 bits
-    # (20 octets) of the rest of the data
-    hash = plaintext.slice!(0..19)
-
-    if @sha1.digest(plaintext) != hash
-      @logger.warn("Unable to decrypt packet, checksum mismatch")
-      return []
-    end
-    return plaintext.unpack('C*')
-  end # def decrypt_packet
-
-  public
-  def decode(payload)
-    payload = payload.bytes.to_a
-
-    collectd = {}
-    was_encrypted = false
-
-    while payload.length > 0 do
-      typenum = (payload.slice!(0) << 8) + payload.slice!(0)
-      # Get the length of the data in this part, but take into account that
-      # the header is 4 bytes
-      length  = ((payload.slice!(0) << 8) + payload.slice!(0)) - 4
-      # Validate that the part length is correct
-      raise(HeaderError) if length > payload.length
-      
-      body = payload.slice!(0..length-1)
-
-      field = TYPEMAP[typenum]
-      if field.nil?
-        @logger.warn("Unknown typenumber: #{typenum}")
-        next
-      end
-
-      values = get_values(typenum, body)
-
-      case typenum
-      when SIGNATURE_TYPE
-        raise(EncryptionError) unless verify_signature(values[0], values[1], payload)
-        next
-      when ENCRYPTION_TYPE
-        payload = decrypt_packet(values[0], values[1], values[2])
-        raise(EncryptionError) if payload.empty?
-        was_encrypted = true
-        next
-      when PLUGIN_TYPE
-        # We've reached a new plugin, delete everything except for the the host
-        # field, because there's only one per packet and the timestamp field,
-        # because that one goes in front of the plugin
-        collectd.each_key do |k|
-          collectd.delete(k) unless PLUGIN_TYPE_FIELDS.has_key?(k)
-        end
-      when COLLECTD_TYPE
-        # We've reached a new type within the plugin section, delete all fields
-        # that could have something to do with the previous type (if any)
-        collectd.each_key do |k|
-          collectd.delete(k) unless COLLECTD_TYPE_FIELDS.has_key?(k)
-        end
-      end
-
-      raise(EncryptionError) if !was_encrypted and @security_level == SECURITY_ENCR
-
-      # Fill in the fields.
-      if values.is_a?(Array)
-        if values.length > 1              # Only do this iteration on multi-value arrays
-          values.each_with_index do |value, x|
-            begin
-              type = collectd['collectd_type']
-              key = @types[type]
-              key_x = key[x]
-              # assign
-              collectd[key_x] = value
-            rescue
-              @logger.error("Invalid value for type=#{type.inspect}, key=#{@types[type].inspect}, index=#{x}")
-            end
-          end
-        else                              # Otherwise it's a single value
-          collectd['value'] = values[0]      # So name it 'value' accordingly
-        end
-      elsif field != nil                  # Not an array, make sure it's non-empty
-        collectd[field] = values            # Append values to collectd under key field
-      end
-
-      if INTERVAL_VALUES_FIELDS.has_key?(field)
-        if ((@prune_intervals && !INTERVAL_TYPES.has_key?(typenum)) || !@prune_intervals)
-          # Prune these *specific* keys if they exist and are empty.
-          # This is better than looping over all keys every time.
-          collectd.delete('type_instance') if collectd['type_instance'] == ""
-          collectd.delete('plugin_instance') if collectd['plugin_instance'] == ""
-          # This ugly little shallow-copy hack keeps the new event from getting munged by the cleanup
-          # With pass-by-reference we get hosed (if we pass collectd, then clean it up rapidly, values can disappear)
-          yield LogStash::Event.new(collectd.dup)
-        end
-        # Clean up the event
-        collectd.each_key do |k|
-          collectd.delete(k) if !INTERVAL_BASE_FIELDS.has_key?(k)
-        end
-      end
-    end # while payload.length > 0 do
-  rescue EncryptionError, ProtocolError, HeaderError
-    # basically do nothing, we just want out
-  end # def decode
-
-end # class LogStash::Codecs::Collectd
diff --git a/lib/logstash/codecs/dots.rb b/lib/logstash/codecs/dots.rb
deleted file mode 100644
index 471e60dfbea..00000000000
--- a/lib/logstash/codecs/dots.rb
+++ /dev/null
@@ -1,18 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-
-class LogStash::Codecs::Dots < LogStash::Codecs::Base
-  config_name "dots"
-  milestone 1
-
-  public
-  def decode(data)
-    raise "Not implemented"
-  end # def decode
-
-  public
-  def encode(data)
-    @on_event.call(".")
-  end # def encode
-
-end # class LogStash::Codecs::Dots
diff --git a/lib/logstash/codecs/edn.rb b/lib/logstash/codecs/edn.rb
deleted file mode 100644
index f5686db0bf1..00000000000
--- a/lib/logstash/codecs/edn.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-require "logstash/codecs/base"
-require "logstash/codecs/line"
-
-class LogStash::Codecs::EDN < LogStash::Codecs::Base
-  config_name "edn"
-
-  milestone 1
-
-  def register
-    require "edn"
-  end
-
-  public
-  def decode(data)
-    begin
-      yield LogStash::Event.new(EDN.read(data))
-    rescue
-      @logger.info("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
-      yield LogStash::Event.new("message" => data)
-    end
-  end
-
-  public
-  def encode(data)
-    @on_event.call(data.to_hash.to_edn)
-  end
-
-end
diff --git a/lib/logstash/codecs/edn_lines.rb b/lib/logstash/codecs/edn_lines.rb
deleted file mode 100644
index 8b6b490c239..00000000000
--- a/lib/logstash/codecs/edn_lines.rb
+++ /dev/null
@@ -1,36 +0,0 @@
-require "logstash/codecs/base"
-require "logstash/codecs/line"
-
-class LogStash::Codecs::EDNLines < LogStash::Codecs::Base
-  config_name "edn_lines"
-
-  milestone 1
-
-  def register
-    require "edn"
-  end
-
-  public
-  def initialize(params={})
-    super(params)
-    @lines = LogStash::Codecs::Line.new
-  end
-
-  public
-  def decode(data)
-    @lines.decode(data) do |event|
-      begin
-        yield LogStash::Event.new(EDN.read(event["message"]))
-      rescue => e
-        @logger.info("EDN parse failure. Falling back to plain-text", :error => e, :data => data)
-        yield LogStash::Event.new("message" => data)
-      end
-    end
-  end
-
-  public
-  def encode(data)
-    @on_event.call(data.to_hash.to_edn + "\n")
-  end
-
-end
diff --git a/lib/logstash/codecs/fluent.rb b/lib/logstash/codecs/fluent.rb
deleted file mode 100644
index d1e6acd336e..00000000000
--- a/lib/logstash/codecs/fluent.rb
+++ /dev/null
@@ -1,55 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/util/charset"
-
-# This codec handles fluentd's msgpack schema.
-#
-# For example, you can receive logs from fluent-logger-ruby with:
-#
-#     input {
-#       tcp {
-#         codec => fluent
-#         port => 4000
-#       }
-#     }
-#
-# And from your ruby code in your own application:
-#
-#     logger = Fluent::Logger::FluentLogger.new(nil, :host => "example.log", :port => 4000)
-#     logger.post("some_tag", { "your" => "data", "here" => "yay!" })
-#
-# Notes:
-#
-# * the fluent uses a second-precision time for events, so you will never see
-#   subsecond precision on events processed by this codec.
-#
-class LogStash::Codecs::Fluent < LogStash::Codecs::Base
-  config_name "fluent"
-  milestone 1
-
-  public
-  def register
-    require "msgpack"
-    @decoder = MessagePack::Unpacker.new
-  end
-
-  public
-  def decode(data)
-    @decoder.feed(data)
-    @decoder.each do |tag, epochtime, map|
-      event = LogStash::Event.new(map.merge(
-        "@timestamp" => Time.at(epochtime),
-        "tags" => tag
-      ))
-      yield event
-    end
-  end # def decode
-
-  public
-  def encode(event)
-    tag = event["tags"] || "log"
-    epochtime = event["@timestamp"].to_i
-    @on_event.call(MessagePack.pack([ tag, epochtime, event.to_hash ]))
-  end # def encode
-
-end # class LogStash::Codecs::Fluent
diff --git a/lib/logstash/codecs/graphite.rb b/lib/logstash/codecs/graphite.rb
deleted file mode 100644
index e3510f3b65e..00000000000
--- a/lib/logstash/codecs/graphite.rb
+++ /dev/null
@@ -1,103 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/codecs/line"
-require "json"
-
-# This codec will encode and decode Graphite formated lines.
-class LogStash::Codecs::Graphite < LogStash::Codecs::Base
-  config_name "graphite"
-
-  milestone 2
-
-  EXCLUDE_ALWAYS = [ "@timestamp", "@version" ]
-
-  DEFAULT_METRICS_FORMAT = "*"
-  METRIC_PLACEHOLDER = "*"
-
-  # The metric(s) to use. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key
-  # of the metric name, value of the metric value. Example:
-  #
-  #     [ "%{host}/uptime", "%{uptime_1m}" ]
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :metrics, :validate => :hash, :default => {}
-
-  # Indicate that the event @fields should be treated as metrics and will be sent as is to graphite
-  config :fields_are_metrics, :validate => :boolean, :default => false
-
-  # Include only regex matched metric names
-  config :include_metrics, :validate => :array, :default => [ ".*" ]
-
-  # Exclude regex matched metric names, by default exclude unresolved %{field} strings
-  config :exclude_metrics, :validate => :array, :default => [ "%\{[^}]+\}" ]
-
-  # Defines format of the metric string. The placeholder '*' will be
-  # replaced with the name of the actual metric.
-  #
-  #     metrics_format => "foo.bar.*.sum"
-  #
-  # NOTE: If no metrics_format is defined the name of the metric will be used as fallback.
-  config :metrics_format, :validate => :string, :default => DEFAULT_METRICS_FORMAT
-
-
-  public
-  def initialize(params={})
-    super(params)
-    @lines = LogStash::Codecs::Line.new
-  end
-
-  public
-  def decode(data)
-    @lines.decode(data) do |event|
-      name, value, time = event["message"].split(" ")
-      yield LogStash::Event.new(name => value.to_f, "@timestamp" => Time.at(time.to_i).gmtime)
-    end # @lines.decode
-  end # def decode
-
-  private
-  def construct_metric_name(metric)
-    if @metrics_format
-      return @metrics_format.gsub(METRIC_PLACEHOLDER, metric)
-    end
-
-    return metric
-  end
-
-  public
-  def encode(event)
-    # Graphite message format: metric value timestamp\n
-
-    messages = []
-    timestamp = event.sprintf("%{+%s}")
-
-    if @fields_are_metrics
-      @logger.debug("got metrics event", :metrics => event.to_hash)
-      event.to_hash.each do |metric,value|
-        next if EXCLUDE_ALWAYS.include?(metric)
-        next unless @include_metrics.empty? || @include_metrics.any? { |regexp| metric.match(regexp) }
-        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
-        messages << "#{construct_metric_name(metric)} #{event.sprintf(value.to_s).to_f} #{timestamp}"
-      end # data.to_hash.each
-    else
-      @metrics.each do |metric, value|
-        @logger.debug("processing", :metric => metric, :value => value)
-        metric = event.sprintf(metric)
-        next unless @include_metrics.any? {|regexp| metric.match(regexp)}
-        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
-        messages << "#{construct_metric_name(event.sprintf(metric))} #{event.sprintf(value).to_f} #{timestamp}"
-      end # @metrics.each
-    end # if @fields_are_metrics
-
-    if messages.empty?
-      @logger.debug("Message is empty, not emiting anything.", :messages => messages)
-    else
-      message = messages.join("\n") + "\n"
-      @logger.debug("Emiting carbon messages", :messages => messages)
-
-      @on_event.call(message)
-    end # if messages.empty?
-  end # def encode
-
-end # class LogStash::Codecs::Graphite
diff --git a/lib/logstash/codecs/json.rb b/lib/logstash/codecs/json.rb
deleted file mode 100644
index 718498cad0b..00000000000
--- a/lib/logstash/codecs/json.rb
+++ /dev/null
@@ -1,48 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/codecs/line"
-require "json"
-
-# This codec may be used to decode (via inputs) and encode (via outputs) 
-# full JSON messages.  If you are streaming JSON messages delimited
-# by '\n' then see the `json_lines` codec.
-# Encoding will result in a single JSON string.
-class LogStash::Codecs::JSON < LogStash::Codecs::Base
-  config_name "json"
-
-  milestone 3
-
-  # The character encoding used in this codec. Examples include "UTF-8" and
-  # "CP1252".
-  #
-  # JSON requires valid UTF-8 strings, but in some cases, software that
-  # emits JSON does so in another encoding (nxlog, for example). In
-  # weird cases like this, you can set the `charset` setting to the
-  # actual encoding of the text and Logstash will convert it for you.
-  #
-  # For nxlog users, you'll want to set this to "CP1252".
-  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
-
-  public
-  def register
-    @converter = LogStash::Util::Charset.new(@charset)
-    @converter.logger = @logger
-  end
-  
-  public
-  def decode(data)
-    data = @converter.convert(data)
-    begin
-      yield LogStash::Event.new(JSON.parse(data))
-    rescue JSON::ParserError => e
-      @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
-      yield LogStash::Event.new("message" => data)
-    end
-  end # def decode
-
-  public
-  def encode(data)
-    @on_event.call(data.to_json)
-  end # def encode
-
-end # class LogStash::Codecs::JSON
diff --git a/lib/logstash/codecs/json_lines.rb b/lib/logstash/codecs/json_lines.rb
deleted file mode 100644
index e3dac771d17..00000000000
--- a/lib/logstash/codecs/json_lines.rb
+++ /dev/null
@@ -1,53 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/codecs/line"
-require "json"
-
-# This codec will decode streamed JSON that is newline delimited.
-# For decoding line-oriented JSON payload in the redis or file inputs,
-# for example, use the json codec instead.
-# Encoding will emit a single JSON string ending in a '\n'
-class LogStash::Codecs::JSONLines < LogStash::Codecs::Base
-  config_name "json_lines"
-
-  milestone 3
-
-  # The character encoding used in this codec. Examples include "UTF-8" and
-  # "CP1252"
-  #
-  # JSON requires valid UTF-8 strings, but in some cases, software that
-  # emits JSON does so in another encoding (nxlog, for example). In
-  # weird cases like this, you can set the charset setting to the
-  # actual encoding of the text and logstash will convert it for you.
-  #
-  # For nxlog users, you'll want to set this to "CP1252"
-  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
-
-  public
-  def initialize(params={})
-    super(params)
-    @lines = LogStash::Codecs::Line.new
-    @lines.charset = @charset
-  end
-  
-  public
-  def decode(data)
-
-    @lines.decode(data) do |event|
-      begin
-        yield LogStash::Event.new(JSON.parse(event["message"]))
-      rescue JSON::ParserError => e
-        @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
-        yield LogStash::Event.new("message" => event["message"])
-      end
-    end
-  end # def decode
-
-  public
-  def encode(data)
-    # Tack on a \n for now because previously most of logstash's JSON
-    # outputs emitted one per line, and whitespace is OK in json.
-    @on_event.call(data.to_json + "\n")
-  end # def encode
-
-end # class LogStash::Codecs::JSON
diff --git a/lib/logstash/codecs/json_spooler.rb b/lib/logstash/codecs/json_spooler.rb
deleted file mode 100644
index a143971eeac..00000000000
--- a/lib/logstash/codecs/json_spooler.rb
+++ /dev/null
@@ -1,27 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/codecs/spool"
-
-# This is the base class for logstash codecs.
-class LogStash::Codecs::JsonSpooler < LogStash::Codecs::Spool
-  config_name "json_spooler"
-  milestone 0
-
-  public
-  def register
-    @logger.error("the json_spooler codec is deprecated and will be removed in a future release")
-  end
-
-  public
-  def decode(data)
-    super(JSON.parse(data.force_encoding(Encoding::UTF_8))) do |event|
-      yield event
-    end
-  end # def decode
-
-  public
-  def encode(data)
-    super(data)
-  end # def encode
-
-end # class LogStash::Codecs::Json
diff --git a/lib/logstash/codecs/line.rb b/lib/logstash/codecs/line.rb
deleted file mode 100644
index 21ae47a892b..00000000000
--- a/lib/logstash/codecs/line.rb
+++ /dev/null
@@ -1,58 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/util/charset"
-
-# Line-oriented text data.
-#
-# Decoding behavior: Only whole line events will be emitted.
-#
-# Encoding behavior: Each event will be emitted with a trailing newline.
-class LogStash::Codecs::Line < LogStash::Codecs::Base
-  config_name "line"
-  milestone 3
-
-  # Set the desired text format for encoding.
-  config :format, :validate => :string
-
-  # The character encoding used in this input. Examples include "UTF-8"
-  # and "cp1252"
-  #
-  # This setting is useful if your log files are in Latin-1 (aka cp1252)
-  # or in another character set other than UTF-8.
-  #
-  # This only affects "plain" format logs since json is UTF-8 already.
-  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
-
-  public
-  def register
-    require "logstash/util/buftok"
-    @buffer = FileWatch::BufferedTokenizer.new
-    @converter = LogStash::Util::Charset.new(@charset)
-    @converter.logger = @logger
-  end
-  
-  public
-  def decode(data)
-    @buffer.extract(data).each do |line|
-      yield LogStash::Event.new("message" => @converter.convert(line))
-    end
-  end # def decode
-
-  public
-  def flush(&block)
-    remainder = @buffer.flush
-    if !remainder.empty?
-      block.call(LogStash::Event.new({"message" => remainder}))
-    end
-  end
-
-  public
-  def encode(data)
-    if data.is_a? LogStash::Event and @format
-      @on_event.call(data.sprintf(@format) + "\n")
-    else
-      @on_event.call(data.to_s + "\n")
-    end
-  end # def encode
-
-end # class LogStash::Codecs::Plain
diff --git a/lib/logstash/codecs/msgpack.rb b/lib/logstash/codecs/msgpack.rb
deleted file mode 100644
index 05dedf449c5..00000000000
--- a/lib/logstash/codecs/msgpack.rb
+++ /dev/null
@@ -1,43 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-
-class LogStash::Codecs::Msgpack < LogStash::Codecs::Base
-  config_name "msgpack"
-
-  milestone 1
-
-  config :format, :validate => :string, :default => nil
-
-  public
-  def register
-    require "msgpack"
-  end
-
-  public
-  def decode(data)
-    begin
-      # Msgpack does not care about UTF-8
-      event = LogStash::Event.new(MessagePack.unpack(data))
-      event["@timestamp"] = Time.at(event["@timestamp"]).utc if event["@timestamp"].is_a? Float
-      event["tags"] ||= []
-      if @format
-        event["message"] ||= event.sprintf(@format)
-      end
-    rescue => e
-      # Treat as plain text and try to do the best we can with it?
-      @logger.warn("Trouble parsing msgpack input, falling back to plain text",
-                   :input => data, :exception => e)
-      event["message"] = data
-      event["tags"] ||= []
-      event["tags"] << "_msgpackparsefailure"
-    end
-    yield event
-  end # def decode
-
-  public
-  def encode(event)
-    event["@timestamp"] = event["@timestamp"].to_f
-    @on_event.call event.to_hash.to_msgpack
-  end # def encode
-
-end # class LogStash::Codecs::Msgpack
diff --git a/lib/logstash/codecs/multiline.rb b/lib/logstash/codecs/multiline.rb
deleted file mode 100644
index 1bbdecb0d43..00000000000
--- a/lib/logstash/codecs/multiline.rb
+++ /dev/null
@@ -1,193 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/util/charset"
-
-# The multiline codec will collapse multiline messages and merge them into a
-# single event.
-#
-# The original goal of this codec was to allow joining of multiline messages
-# from files into a single event. For example, joining Java exception and
-# stacktrace messages into a single event.
-#
-# The config looks like this:
-#
-#     input {
-#       stdin {
-#         codec => multiline {
-#           pattern => "pattern, a regexp"
-#           negate => "true" or "false"
-#           what => "previous" or "next"
-#         }
-#       }
-#     }
-#
-# The `pattern` should match what you believe to be an indicator that the field
-# is part of a multi-line event.
-#
-# The `what` must be "previous" or "next" and indicates the relation
-# to the multi-line event.
-#
-# The `negate` can be "true" or "false" (defaults to "false"). If "true", a
-# message not matching the pattern will constitute a match of the multiline
-# filter and the `what` will be applied. (vice-versa is also true)
-#
-# For example, Java stack traces are multiline and usually have the message
-# starting at the far-left, with each subsequent line indented. Do this:
-#
-#     input {
-#       stdin {
-#         codec => multiline {
-#           pattern => "^\s"
-#           what => "previous"
-#         }
-#       }
-#     }
-#
-# This says that any line starting with whitespace belongs to the previous line.
-#
-# Another example is to merge lines not starting with a date up to the previous
-# line..
-#
-#     input {
-#       file {
-#         path => "/var/log/someapp.log"
-#         codec => multiline {
-#           # Grok pattern names are valid! :)
-#           pattern => "^%{TIMESTAMP_ISO8601} "
-#           negate => true
-#           what => previous
-#         }
-#       }
-#     }
-#
-# This says that any line not starting with a timestamp should be merged with the previous line.
-#
-# One more common example is C line continuations (backslash). Here's how to do that:
-#
-#     filter {
-#       multiline {
-#         type => "somefiletype"
-#         pattern => "\\$"
-#         what => "next"
-#       }
-#     }
-#
-# This says that any line ending with a backslash should be combined with the
-# following line.
-#
-class LogStash::Codecs::Multiline < LogStash::Codecs::Base
-  config_name "multiline"
-  milestone 3
-
-  # The regular expression to match.
-  config :pattern, :validate => :string, :required => true
-
-  # If the pattern matched, does event belong to the next or previous event?
-  config :what, :validate => ["previous", "next"], :required => true
-
-  # Negate the regexp pattern ('if not matched').
-  config :negate, :validate => :boolean, :default => false
-
-  # Logstash ships by default with a bunch of patterns, so you don't
-  # necessarily need to define this yourself unless you are adding additional
-  # patterns.
-  #
-  # Pattern files are plain text with format:
-  #
-  #     NAME PATTERN
-  #
-  # For example:
-  #
-  #     NUMBER \d+
-  config :patterns_dir, :validate => :array, :default => []
-
-  # The character encoding used in this input. Examples include "UTF-8"
-  # and "cp1252"
-  #
-  # This setting is useful if your log files are in Latin-1 (aka cp1252)
-  # or in another character set other than UTF-8.
-  #
-  # This only affects "plain" format logs since JSON is UTF-8 already.
-  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
-
-  # Tag multiline events with a given tag. This tag will only be added
-  # to events that actually have multiple lines in them.
-  config :multiline_tag, :validate => :string, :default => "multiline"
-
-  public
-  def register
-    require "grok-pure" # rubygem 'jls-grok'
-    # Detect if we are running from a jarfile, pick the right path.
-    patterns_path = []
-    patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
-
-    @grok = Grok.new
-
-    @patterns_dir = patterns_path.to_a + @patterns_dir
-    @patterns_dir.each do |path|
-      if File.directory?(path)
-        path = File.join(path, "*")
-      end
-
-      Dir.glob(path).each do |file|
-        @logger.info("Grok loading patterns from file", :path => file)
-        @grok.add_patterns_from_file(file)
-      end
-    end
-
-    @grok.compile(@pattern)
-    @logger.debug("Registered multiline plugin", :type => @type, :config => @config)
-
-    @buffer = []
-    @handler = method("do_#{@what}".to_sym)
-
-    @converter = LogStash::Util::Charset.new(@charset)
-    @converter.logger = @logger
-  end # def register
-
-  public
-  def decode(text, &block)
-    text = @converter.convert(text)
-
-    match = @grok.match(text)
-    @logger.debug("Multiline", :pattern => @pattern, :text => text,
-                  :match => !match.nil?, :negate => @negate)
-
-    # Add negate option
-    match = (match and !@negate) || (!match and @negate)
-    @handler.call(text, match, &block)
-  end # def decode
-
-  def buffer(text)
-    @time = Time.now.utc if @buffer.empty?
-    @buffer << text
-  end
-
-  def flush(&block)
-    if @buffer.any?
-      event = LogStash::Event.new("@timestamp" => @time, "message" => @buffer.join("\n"))
-      # Tag multiline events
-      event.tag @multiline_tag if @multiline_tag && @buffer.size > 1
-
-      yield event
-      @buffer = []
-    end
-  end
-
-  def do_next(text, matched, &block)
-    buffer(text)
-    flush(&block) if !matched
-  end
-
-  def do_previous(text, matched, &block)
-    flush(&block) if !matched
-    buffer(text)
-  end
-
-  public
-  def encode(data)
-    # Nothing to do.
-    @on_event.call(data)
-  end # def encode
-
-end # class LogStash::Codecs::Plain
diff --git a/lib/logstash/codecs/netflow.rb b/lib/logstash/codecs/netflow.rb
deleted file mode 100644
index 9e2d99de1c2..00000000000
--- a/lib/logstash/codecs/netflow.rb
+++ /dev/null
@@ -1,262 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The "netflow" codec is for decoding Netflow v5/v9 flows.
-class LogStash::Codecs::Netflow < LogStash::Codecs::Base
-  config_name "netflow"
-  milestone 1
-
-  # Netflow v9 template cache TTL (minutes)
-  config :cache_ttl, :validate => :number, :default => 4000
-
-  # Specify into what field you want the Netflow data.
-  config :target, :validate => :string, :default => "netflow"
-
-  # Specify which Netflow versions you will accept.
-  config :versions, :validate => :array, :default => [5, 9]
-
-  # Override YAML file containing Netflow field definitions
-  #
-  # Each Netflow field is defined like so:
-  #
-  #    ---
-  #    id:
-  #    - default length in bytes
-  #    - :name
-  #    id:
-  #    - :uintN or :ip4_addr or :ip6_addr or :mac_addr or :string
-  #    - :name
-  #    id:
-  #    - :skip
-  #
-  # See <https://github.com/logstash/logstash/tree/v%VERSION%/lib/logstash/codecs/netflow/netflow.yaml> for the base set.
-  config :definitions, :validate => :path
-
-  public
-  def initialize(params={})
-    super(params)
-    @threadsafe = false
-  end
-
-  public
-  def register
-    require "logstash/codecs/netflow/util"
-    @templates = Vash.new()
-
-    # Path to default Netflow v9 field definitions
-    filename = File.join(File.dirname(__FILE__), "netflow/netflow.yaml")
-
-    begin
-      @fields = YAML.load_file(filename)
-    rescue Exception => e
-      raise "#{self.class.name}: Bad syntax in definitions file #{filename}"
-    end
-
-    # Allow the user to augment/override/rename the supported Netflow fields
-    if @definitions
-      raise "#{self.class.name}: definitions file #{@definitions} does not exists" unless File.exists?(@definitions)
-      begin
-        @fields.merge!(YAML.load_file(@definitions))
-      rescue Exception => e
-        raise "#{self.class.name}: Bad syntax in definitions file #{@definitions}"
-      end
-    end
-  end # def register
-
-  public
-  def decode(payload, &block)
-    header = Header.read(payload)
-
-    unless @versions.include?(header.version)
-      @logger.warn("Ignoring Netflow version v#{header.version}")
-      return
-    end
-
-    if header.version == 5
-      flowset = Netflow5PDU.read(payload)
-    elsif header.version == 9
-      flowset = Netflow9PDU.read(payload)
-    else
-      @logger.warn("Unsupported Netflow version v#{header.version}")
-      return
-    end
-
-    flowset.records.each do |record|
-      if flowset.version == 5
-        event = LogStash::Event.new
-
-        # FIXME Probably not doing this right WRT JRuby?
-        #
-        # The flowset header gives us the UTC epoch seconds along with
-        # residual nanoseconds so we can set @timestamp to that easily
-        event["@timestamp"] = Time.at(flowset.unix_sec, flowset.unix_nsec / 1000).utc
-        event[@target] = {}
-
-        # Copy some of the pertinent fields in the header to the event
-        ['version', 'flow_seq_num', 'engine_type', 'engine_id', 'sampling_algorithm', 'sampling_interval', 'flow_records'].each do |f|
-          event[@target][f] = flowset[f]
-        end
-
-        # Create fields in the event from each field in the flow record
-        record.each_pair do |k,v|
-          case k.to_s
-          when /_switched$/
-            # The flow record sets the first and last times to the device
-            # uptime in milliseconds. Given the actual uptime is provided
-            # in the flowset header along with the epoch seconds we can
-            # convert these into absolute times
-            millis = flowset.uptime - v
-            seconds = flowset.unix_sec - (millis / 1000)
-            micros = (flowset.unix_nsec / 1000) - (millis % 1000)
-            if micros < 0
-              seconds--
-              micros += 1000000
-            end
-            # FIXME Again, probably doing this wrong WRT JRuby?
-            event[@target][k.to_s] = Time.at(seconds, micros).utc.strftime("%Y-%m-%dT%H:%M:%S.%3NZ")
-          else
-            event[@target][k.to_s] = v
-          end
-        end
-
-        yield event
-      elsif flowset.version == 9
-        case record.flowset_id
-        when 0
-          # Template flowset
-          record.flowset_data.templates.each do |template|
-            catch (:field) do
-              fields = []
-              template.fields.each do |field|
-                entry = netflow_field_for(field.field_type, field.field_length)
-                if ! entry
-                  throw :field
-                end
-                fields += entry
-              end
-              # We get this far, we have a list of fields
-              #key = "#{flowset.source_id}|#{event["source"]}|#{template.template_id}"
-              key = "#{flowset.source_id}|#{template.template_id}"
-              @templates[key, @cache_ttl] = BinData::Struct.new(:endian => :big, :fields => fields)
-              # Purge any expired templates
-              @templates.cleanup!
-            end
-          end
-        when 1
-          # Options template flowset
-          record.flowset_data.templates.each do |template|
-            catch (:field) do
-              fields = []
-              template.option_fields.each do |field|
-                entry = netflow_field_for(field.field_type, field.field_length)
-                if ! entry
-                  throw :field
-                end
-                fields += entry
-              end
-              # We get this far, we have a list of fields
-              #key = "#{flowset.source_id}|#{event["source"]}|#{template.template_id}"
-              key = "#{flowset.source_id}|#{template.template_id}"
-              @templates[key, @cache_ttl] = BinData::Struct.new(:endian => :big, :fields => fields)
-              # Purge any expired templates
-              @templates.cleanup!
-            end
-          end 
-        when 256..65535
-          # Data flowset
-          #key = "#{flowset.source_id}|#{event["source"]}|#{record.flowset_id}"
-          key = "#{flowset.source_id}|#{record.flowset_id}"
-          template = @templates[key]
-
-          if ! template
-            #@logger.warn("No matching template for flow id #{record.flowset_id} from #{event["source"]}")
-            @logger.warn("No matching template for flow id #{record.flowset_id}")
-            next
-          end
-
-          length = record.flowset_length - 4
-
-          # Template shouldn't be longer than the record and there should
-          # be at most 3 padding bytes
-          if template.num_bytes > length or ! (length % template.num_bytes).between?(0, 3)
-            @logger.warn("Template length doesn't fit cleanly into flowset", :template_id => record.flowset_id, :template_length => template.num_bytes, :record_length => length) 
-            next
-          end
-
-          array = BinData::Array.new(:type => template, :initial_length => length / template.num_bytes)
-
-          records = array.read(record.flowset_data)
-
-          records.each do |r|
-            event = LogStash::Event.new(
-              "@timestamp" => Time.at(flowset.unix_sec).utc,
-              @target => {}
-            )
-
-            # Fewer fields in the v9 header
-            ['version', 'flow_seq_num'].each do |f|
-              event[@target][f] = flowset[f]
-            end
-
-            event[@target]['flowset_id'] = record.flowset_id
-
-            r.each_pair do |k,v|
-              case k.to_s
-              when /_switched$/
-                millis = flowset.uptime - v
-                seconds = flowset.unix_sec - (millis / 1000)
-                # v9 did away with the nanosecs field
-                micros = 1000000 - (millis % 1000)
-                event[@target][k.to_s] = Time.at(seconds, micros).utc.strftime("%Y-%m-%dT%H:%M:%S.%3NZ")
-              else
-                event[@target][k.to_s] = v
-              end
-            end
-
-            yield event
-          end
-        else
-          @logger.warn("Unsupported flowset id #{record.flowset_id}")
-        end
-      end
-    end
-  end # def filter
-
-  private
-  def uint_field(length, default)
-    # If length is 4, return :uint32, etc. and use default if length is 0
-    ("uint" + (((length > 0) ? length : default) * 8).to_s).to_sym
-  end # def uint_field
-
-  private
-  def netflow_field_for(type, length)
-    if @fields.include?(type)
-      field = @fields[type]
-      if field.is_a?(Array)
-
-        if field[0].is_a?(Integer)
-          field[0] = uint_field(length, field[0])
-        end
-
-        # Small bit of fixup for skip or string field types where the length
-        # is dynamic
-        case field[0]
-        when :skip
-          field += [nil, {:length => length}]
-        when :string
-          field += [{:length => length, :trim_padding => true}]
-        end
-
-        @logger.debug("Definition complete", :field => field)
-        [field]
-      else
-        @logger.warn("Definition should be an array", :field => field)
-        nil
-      end
-    else
-      @logger.warn("Unsupported field", :type => type, :length => length)
-      nil
-    end
-  end # def netflow_field_for
-end # class LogStash::Filters::Netflow
diff --git a/lib/logstash/codecs/netflow/netflow.yaml b/lib/logstash/codecs/netflow/netflow.yaml
deleted file mode 100644
index 9f823dcf449..00000000000
--- a/lib/logstash/codecs/netflow/netflow.yaml
+++ /dev/null
@@ -1,215 +0,0 @@
----
-1:
-- 4
-- :in_bytes
-2:
-- 4
-- :in_pkts
-3:
-- 4
-- :flows
-4:
-- :uint8
-- :protocol
-5:
-- :uint8
-- :src_tos
-6:
-- :uint8
-- :tcp_flags
-7:
-- :uint16
-- :l4_src_port
-8:
-- :ip4_addr
-- :ipv4_src_addr
-9:
-- :uint8
-- :src_mask
-10:
-- 2
-- :input_snmp
-11:
-- :uint16
-- :l4_dst_port
-12:
-- :ip4_addr
-- :ipv4_dst_addr
-13:
-- :uint8
-- :dst_mask
-14:
-- 2
-- :output_snmp
-15:
-- :ip4_addr
-- :ipv4_next_hop
-16:
-- 2
-- :src_as
-17:
-- 2
-- :dst_as
-18:
-- :ip4_addr
-- :bgp_ipv4_next_hop
-19:
-- 4
-- :mul_dst_pkts
-20:
-- 4
-- :mul_dst_bytes
-21:
-- :uint32
-- :last_switched
-22:
-- :uint32
-- :first_switched
-23:
-- 4
-- :out_bytes
-24:
-- 4
-- :out_pkts
-25:
-- :uint16
-- :min_pkt_length
-26:
-- :uint16
-- :max_pkt_length
-27:
-- :ip6_addr
-- :ipv6_src_addr
-28:
-- :ip6_addr
-- :ipv6_dst_addr
-29:
-- :uint8
-- :ipv6_src_mask
-30:
-- :uint8
-- :ipv6_dst_mask
-31:
-- :uint24
-- :ipv6_flow_label
-32:
-- :uint16
-- :icmp_type
-33:
-- :uint8
-- :mul_igmp_type
-34:
-- :uint32
-- :sampling_interval
-35:
-- :uint8
-- :sampling_algorithm
-36:
-- :uint16
-- :flow_active_timeout
-37:
-- :uint16
-- :flow_inactive_timeout
-38:
-- :uint8
-- :engine_type
-39:
-- :uint8
-- :engine_id
-40:
-- 4
-- :total_bytes_exp
-41:
-- 4
-- :total_pkts_exp
-42:
-- 4
-- :total_flows_exp
-43:
-- :skip
-44:
-- :ip4_addr
-- :ipv4_src_prefix
-45:
-- :ip4_addr
-- :ipv4_dst_prefix
-46:
-- :uint8
-- :mpls_top_label_type
-47:
-- :uint32
-- :mpls_top_label_ip_addr
-48:
-- 4
-- :flow_sampler_id
-49:
-- :uint8
-- :flow_sampler_mode
-50:
-- :uint32
-- :flow_sampler_random_interval
-51:
-- :skip
-52:
-- :uint8
-- :min_ttl
-53:
-- :uint8
-- :max_ttl
-54:
-- :uint16
-- :ipv4_ident
-55:
-- :uint8
-- :dst_tos
-56:
-- :mac_addr
-- :in_src_max
-57:
-- :mac_addr
-- :out_dst_max
-58:
-- :uint16
-- :src_vlan
-59:
-- :uint16
-- :dst_vlan
-60:
-- :uint8
-- :ip_protocol_version
-61:
-- :uint8
-- :direction
-62:
-- :ip6_addr
-- :ipv6_next_hop
-63:
-- :ip6_addr
-- :bgp_ipv6_next_hop
-64:
-- :uint32
-- :ipv6_option_headers
-64:
-- :skip
-65:
-- :skip
-66:
-- :skip
-67:
-- :skip
-68:
-- :skip
-69:
-- :skip
-80:
-- :mac_addr
-- :in_dst_mac
-81:
-- :mac_addr
-- :out_src_mac
-82:
-- :string
-- :if_name
-83:
-- :string
-- :if_desc
diff --git a/lib/logstash/codecs/netflow/util.rb b/lib/logstash/codecs/netflow/util.rb
deleted file mode 100644
index 627fb1265f0..00000000000
--- a/lib/logstash/codecs/netflow/util.rb
+++ /dev/null
@@ -1,212 +0,0 @@
-# encoding: utf-8
-require "bindata"
-require "ipaddr"
-
-class IP4Addr < BinData::Primitive
-  endian :big
-  uint32 :storage
-
-  def set(val)
-    ip = IPAddr.new(val)
-    if ! ip.ipv4?
-      raise ArgumentError, "invalid IPv4 address '#{val}'"
-    end
-    self.storage = ip.to_i
-  end
-
-  def get
-    IPAddr.new_ntoh([self.storage].pack('N')).to_s
-  end
-end
-
-class IP6Addr < BinData::Primitive
-  endian  :big
-  uint128 :storage
-
-  def set(val)
-    ip = IPAddr.new(val)
-    if ! ip.ipv6?
-      raise ArgumentError, "invalid IPv6 address `#{val}'"
-    end
-    self.storage = ip.to_i
-  end
-
-  def get
-    IPAddr.new_ntoh((0..7).map { |i|
-      (self.storage >> (112 - 16 * i)) & 0xffff
-    }.pack('n8')).to_s
-  end
-end
-
-class MacAddr < BinData::Primitive
-  array :bytes, :type => :uint8, :initial_length => 6
-
-  def set(val)
-    ints = val.split(/:/).collect { |int| int.to_i(16) }
-    self.bytes = ints
-  end
-
-  def get
-    self.bytes.collect { |byte| byte.to_s(16) }.join(":")
-  end
-end
-
-class Header < BinData::Record
-  endian :big
-  uint16 :version
-end
-
-class Netflow5PDU < BinData::Record
-  endian :big
-  uint16 :version
-  uint16 :flow_records
-  uint32 :uptime
-  uint32 :unix_sec
-  uint32 :unix_nsec
-  uint32 :flow_seq_num
-  uint8  :engine_type
-  uint8  :engine_id
-  bit2   :sampling_algorithm
-  bit14  :sampling_interval
-  array  :records, :initial_length => :flow_records do
-    ip4_addr :ipv4_src_addr
-    ip4_addr :ipv4_dst_addr
-    ip4_addr :ipv4_next_hop
-    uint16   :input_snmp
-    uint16   :output_snmp
-    uint32   :in_pkts
-    uint32   :in_bytes
-    uint32   :first_switched
-    uint32   :last_switched
-    uint16   :l4_src_port
-    uint16   :l4_dst_port
-    skip     :length => 1
-    uint8    :tcp_flags # Split up the TCP flags maybe?
-    uint8    :protocol
-    uint8    :src_tos
-    uint16   :src_as
-    uint16   :dst_as
-    uint8    :src_mask
-    uint8    :dst_mask
-    skip     :length => 2
-  end
-end
-
-class TemplateFlowset < BinData::Record
-  endian :big
-  array  :templates, :read_until => lambda { array.num_bytes == flowset_length - 4 } do
-    uint16 :template_id
-    uint16 :field_count
-    array  :fields, :initial_length => :field_count do
-      uint16 :field_type
-      uint16 :field_length
-    end
-  end
-end
-
-class OptionFlowset < BinData::Record
-  endian :big
-  array  :templates, :read_until => lambda { flowset_length - 4 - array.num_bytes <= 2 } do
-    uint16 :template_id
-    uint16 :scope_length
-    uint16 :option_length
-    array  :scope_fields, :initial_length => lambda { scope_length / 4 } do
-      uint16 :field_type
-      uint16 :field_length
-    end
-    array  :option_fields, :initial_length => lambda { option_length / 4 } do
-      uint16 :field_type
-      uint16 :field_length
-    end
-  end
-  skip   :length => lambda { templates.length.odd? ? 2 : 0 }
-end
-
-class Netflow9PDU < BinData::Record
-  endian :big
-  uint16 :version
-  uint16 :flow_records
-  uint32 :uptime
-  uint32 :unix_sec
-  uint32 :flow_seq_num
-  uint32 :source_id
-  array  :records, :read_until => :eof do
-    uint16 :flowset_id
-    uint16 :flowset_length
-    choice :flowset_data, :selection => :flowset_id do
-      template_flowset 0
-      option_flowset   1
-      string           :default, :read_length => lambda { flowset_length - 4 }
-    end
-  end
-end
-
-# https://gist.github.com/joshaven/184837
-class Vash < Hash
-  def initialize(constructor = {})
-    @register ||= {}
-    if constructor.is_a?(Hash)
-      super()
-      merge(constructor)
-    else
-      super(constructor)
-    end
-  end
-
-  alias_method :regular_writer, :[]= unless method_defined?(:regular_writer)
-  alias_method :regular_reader, :[] unless method_defined?(:regular_reader)
-
-  def [](key)
-    sterilize(key)
-    clear(key) if expired?(key)
-    regular_reader(key)
-  end
-
-  def []=(key, *args)
-    if args.length == 2
-      value, ttl = args[1], args[0]
-    elsif args.length == 1
-      value, ttl = args[0], 60
-    else
-      raise ArgumentError, "Wrong number of arguments, expected 2 or 3, received: #{args.length+1}\n"+
-                           "Example Usage:  volatile_hash[:key]=value OR volatile_hash[:key, ttl]=value"
-    end
-    sterilize(key)
-    ttl(key, ttl)
-    regular_writer(key, value)
-  end
-
-  def merge(hsh)
-    hsh.map {|key,value| self[sterile(key)] = hsh[key]}
-    self
-  end
-
-  def cleanup!
-    now = Time.now.to_i
-    @register.map {|k,v| clear(k) if v < now}
-  end
-
-  def clear(key)
-    sterilize(key)
-    @register.delete key
-    self.delete key
-  end
-
-  private
-  def expired?(key)
-    Time.now.to_i > @register[key].to_i
-  end
-
-  def ttl(key, secs=60)
-    @register[key] = Time.now.to_i + secs.to_i
-  end
-
-  def sterile(key)
-    String === key ? key.chomp('!').chomp('=') : key.to_s.chomp('!').chomp('=').to_sym
-  end
-
-  def sterilize(key)
-    key = sterile(key)
-  end
-end
-
diff --git a/lib/logstash/codecs/noop.rb b/lib/logstash/codecs/noop.rb
deleted file mode 100644
index 8a7a0d7213a..00000000000
--- a/lib/logstash/codecs/noop.rb
+++ /dev/null
@@ -1,19 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-
-class LogStash::Codecs::Noop < LogStash::Codecs::Base
-  config_name "noop"
-
-  milestone 1
-  
-  public
-  def decode(data)
-    yield data
-  end # def decode
-
-  public
-  def encode(data)
-    @on_event.call data
-  end # def encode
-
-end # class LogStash::Codecs::Noop
diff --git a/lib/logstash/codecs/oldlogstashjson.rb b/lib/logstash/codecs/oldlogstashjson.rb
deleted file mode 100644
index d815ece335e..00000000000
--- a/lib/logstash/codecs/oldlogstashjson.rb
+++ /dev/null
@@ -1,56 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-
-class LogStash::Codecs::OldLogStashJSON < LogStash::Codecs::Base
-  config_name "oldlogstashjson"
-  milestone 2
-
-  # Map from v0 name to v1 name.
-  # Note: @source is gone and has no similar field.
-  V0_TO_V1 = {"@timestamp" => "@timestamp", "@message" => "message",
-              "@tags" => "tags", "@type" => "type",
-              "@source_host" => "host", "@source_path" => "path"}
-
-  public
-  def decode(data)
-    begin
-      obj = JSON.parse(data.force_encoding(Encoding::UTF_8))
-    rescue JSON::ParserError => e
-      @logger.info("JSON parse failure. Falling back to plain-text", :error => e, :data => data)
-      yield LogStash::Event.new("message" => data)
-      return
-    end
-
-    h  = {}
-
-    # Convert the old logstash schema to the new one.
-    V0_TO_V1.each do |key, val|
-      h[val] = obj[key] if obj.include?(key)
-    end
-
-    h.merge!(obj["@fields"]) if obj["@fields"].is_a?(Hash)
-    yield LogStash::Event.new(h)
-  end # def decode
-
-  public
-  def encode(data)
-    h  = {}
-
-    # Convert the new logstash schema to the old one.
-    V0_TO_V1.each do |key, val|
-      h[key] = data[val] if data.include?(val)
-    end
-
-    data.to_hash.each do |field, val|
-      # TODO: might be better to V1_TO_V0 = V0_TO_V1.invert during
-      # initialization than V0_TO_V1.has_value? within loop
-      next if field == "@version" or V0_TO_V1.has_value?(field)
-      h["@fields"] = {} if h["@fields"].nil?
-      h["@fields"][field] = val
-    end
-
-    # Tack on a \n because JSON outputs 1.1.x had them.
-    @on_event.call(h.to_json + "\n")
-  end # def encode
-
-end # class LogStash::Codecs::OldLogStashJSON
diff --git a/lib/logstash/codecs/plain.rb b/lib/logstash/codecs/plain.rb
deleted file mode 100644
index 40071f5addc..00000000000
--- a/lib/logstash/codecs/plain.rb
+++ /dev/null
@@ -1,48 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-require "logstash/util/charset"
-
-# The "plain" codec is for plain text with no delimiting between events.
-#
-# This is mainly useful on inputs and outputs that already have a defined
-# framing in their transport protocol (such as zeromq, rabbitmq, redis, etc)
-class LogStash::Codecs::Plain < LogStash::Codecs::Base
-  config_name "plain"
-  milestone 3
-
-  # Set the message you which to emit for each event. This supports sprintf
-  # strings.
-  #
-  # This setting only affects outputs (encoding of events).
-  config :format, :validate => :string
-
-  # The character encoding used in this input. Examples include "UTF-8"
-  # and "cp1252"
-  #
-  # This setting is useful if your log files are in Latin-1 (aka cp1252)
-  # or in another character set other than UTF-8.
-  #
-  # This only affects "plain" format logs since json is UTF-8 already.
-  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
-
-  public
-  def register
-    @converter = LogStash::Util::Charset.new(@charset)
-    @converter.logger = @logger
-  end
-
-  public
-  def decode(data)
-    yield LogStash::Event.new("message" => @converter.convert(data))
-  end # def decode
-
-  public
-  def encode(data)
-    if data.is_a? LogStash::Event and @format
-      @on_event.call(data.sprintf(@format))
-    else
-      @on_event.call(data.to_s)
-    end
-  end # def encode
-
-end # class LogStash::Codecs::Plain
diff --git a/lib/logstash/codecs/rubydebug.rb b/lib/logstash/codecs/rubydebug.rb
deleted file mode 100644
index 607131a29be..00000000000
--- a/lib/logstash/codecs/rubydebug.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-
-# The rubydebug codec will output your Logstash event data using
-# the Ruby Awesome Print library.
-#
-class LogStash::Codecs::RubyDebug < LogStash::Codecs::Base
-  config_name "rubydebug"
-  milestone 3
-
-  def register
-    require "ap"
-  end
-
-  public
-  def decode(data)
-    raise "Not implemented"
-  end # def decode
-
-  public
-  def encode(data)
-    @on_event.call(data.to_hash.awesome_inspect + "\n")
-  end # def encode
-
-end # class LogStash::Codecs::Dots
diff --git a/lib/logstash/codecs/spool.rb b/lib/logstash/codecs/spool.rb
deleted file mode 100644
index 67ff480b06d..00000000000
--- a/lib/logstash/codecs/spool.rb
+++ /dev/null
@@ -1,38 +0,0 @@
-# encoding: utf-8
-require "logstash/codecs/base"
-
-class LogStash::Codecs::Spool < LogStash::Codecs::Base
-  config_name 'spool'
-  milestone 1
-  config :spool_size, :validate => :number, :default => 50
-
-  attr_reader :buffer
-
-  public
-  def decode(data)
-    data.each do |event|
-      yield event
-    end
-  end # def decode
-
-  public
-  def encode(data)
-    @buffer = [] if @buffer.nil?
-    #buffer size is hard coded for now until a 
-    #better way to pass args into codecs is implemented
-    if @buffer.length >= @spool_size
-      @on_event.call @buffer
-      @buffer = []
-    else
-      @buffer << data
-    end
-  end # def encode
-
-  public
-  def teardown
-    if !@buffer.nil? and @buffer.length > 0
-      @on_event.call @buffer
-    end
-    @buffer = []
-  end
-end # class LogStash::Codecs::Spool
diff --git a/lib/logstash/config/config_ast.rb b/lib/logstash/config/config_ast.rb
index f5e6eaf9af7..3a31c04a3fb 100644
--- a/lib/logstash/config/config_ast.rb
+++ b/lib/logstash/config/config_ast.rb
@@ -6,6 +6,15 @@ def compile
     return elements.collect(&:compile).reject(&:empty?).join("")
   end
 
+  # Traverse the syntax tree recursively.
+  # The order should respect the order of the configuration file as it is read
+  # and written by humans (and the order in which it is parsed).
+  def recurse(e, depth=0, &block)
+    r = block.call(e, depth)
+    e.elements.each { |e| recurse(e, depth + 1, &block) } if r && e.elements
+    nil
+  end
+
   def recursive_inject(results=[], &block)
     if !elements.nil?
       elements.each do |element|
@@ -39,44 +48,45 @@ def recursive_select_parent(results=[], klass)
   end
 end
 
-module LogStash; module Config; module AST 
+module LogStash; module Config; module AST
   class Node < Treetop::Runtime::SyntaxNode; end
   class Config < Node
     def compile
-      # TODO(sissel): Move this into config/config_ast.rb
       code = []
-      code << "@inputs = []"
-      code << "@filters = []"
-      code << "@outputs = []"
+
+      code << <<-CODE
+        @inputs = []
+        @filters = []
+        @outputs = []
+        @periodic_flushers = []
+        @shutdown_flushers = []
+      CODE
+
       sections = recursive_select(LogStash::Config::AST::PluginSection)
       sections.each do |s|
         code << s.compile_initializer
       end
 
       # start inputs
-      #code << "class << self"
       definitions = []
-        
+
       ["filter", "output"].each do |type|
-        #definitions << "def #{type}(event)"
-        definitions << "@#{type}_func = lambda do |event, &block|"
-        if type == "filter"
-          definitions << "  extra_events = []"
-        end
+        # defines @filter_func and @output_func
 
+        definitions << "@#{type}_func = lambda do |event, &block|"
+        definitions << "  events = [event]"
         definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", :event => event.to_hash)"
         sections.select { |s| s.plugin_type.text_value == type }.each do |s|
           definitions << s.compile.split("\n", -1).map { |e| "  #{e}" }
         end
 
         if type == "filter"
-          definitions << "  extra_events.each(&block)"
+          definitions << "  events.flatten.each{|e| block.call(e) }"
         end
         definitions << "end"
       end
 
       code += definitions.join("\n").split("\n", -1).collect { |l| "  #{l}" }
-      #code << "end"
       return code.join("\n")
     end
   end
@@ -84,14 +94,52 @@ def compile
   class Comment < Node; end
   class Whitespace < Node; end
   class PluginSection < Node
+    # Global plugin numbering for the janky instance variable naming we use
+    # like @filter_<name>_1
     @@i = 0
+
     # Generate ruby code to initialize all the plugins.
     def compile_initializer
       generate_variables
       code = []
-      @variables.collect do |plugin, name|
-        code << "#{name} = #{plugin.compile_initializer}"
-        code << "@#{plugin.plugin_type}s << #{name}"
+      @variables.each do |plugin, name|
+
+
+        code << <<-CODE
+          #{name} = #{plugin.compile_initializer}
+          @#{plugin.plugin_type}s << #{name}
+        CODE
+
+        # The flush method for this filter.
+        if plugin.plugin_type == "filter"
+
+          code << <<-CODE
+            #{name}_flush = lambda do |options, &block|
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name})
+
+              flushed_events = #{name}.flush(options)
+
+              return if flushed_events.nil? || flushed_events.empty?
+
+              flushed_events.each do |event|
+                @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name}, :event => event)
+
+                events = [event]
+                #{plugin.compile_starting_here.gsub(/^/, "  ")}
+
+                block.call(event)
+                events.flatten.each{|e| block.call(e) if e != event}
+              end
+
+            end
+
+            if #{name}.respond_to?(:flush)
+              @periodic_flushers << #{name}_flush if #{name}.periodic_flush
+              @shutdown_flushers << #{name}_flush
+            end
+          CODE
+
+        end
       end
       return code.join("\n")
     end
@@ -151,38 +199,69 @@ def compile_initializer
 
     def compile
       case plugin_type
-        when "input"
-          return "start_input(#{variable_name})"
-        when "filter"
-          # This is some pretty stupid code, honestly.
-          # I'd prefer much if it were put into the Pipeline itself
-          # and this should simply compile to 
-          #   #{variable_name}.filter(event)
-          return [
-            "newevents = []",
-            "extra_events.each do |event|",
-            "  #{variable_name}.filter(event) do |newevent|",
-            "    newevents << newevent",
-            "  end",
-            "end",
-            "extra_events += newevents",
-
-            "#{variable_name}.filter(event) do |newevent|",
-            "  extra_events << newevent",
-            "end",
-            "if event.cancelled?",
-            "  extra_events.each(&block)",
-            "  return",
-            "end",
-          ].map { |l| "#{l}\n" }.join("")
-        when "output"
-          return "#{variable_name}.handle(event)\n"
-        when "codec"
-          settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
-          attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
-          return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})"
+      when "input"
+        return "start_input(#{variable_name})"
+      when "filter"
+        return <<-CODE
+          events = events.flat_map do |event|
+            next [] if event.cancelled?
+
+            new_events = []
+            #{variable_name}.filter(event){|new_event| new_events << new_event}
+            event.cancelled? ? new_events : new_events.unshift(event)
+          end
+        CODE
+      when "output"
+        return "#{variable_name}.handle(event)\n"
+      when "codec"
+        settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
+        attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
+        return "plugin(#{plugin_type.inspect}, #{plugin_name.inspect}, #{attributes_code})"
       end
     end
+
+    def compile_starting_here
+      return unless plugin_type == "filter" # only filter supported.
+
+      expressions = [
+        LogStash::Config::AST::Branch,
+        LogStash::Config::AST::Plugin
+      ]
+      code = []
+
+      # Find the branch we are in, if any (the 'if' statement, etc)
+      self_branch = recursive_select_parent(LogStash::Config::AST::BranchEntry).first
+
+      # Find any siblings to our branch so we can skip them later.  For example,
+      # if we are in an 'else if' we want to skip any sibling 'else if' or
+      # 'else' blocks.
+      branch_siblings = []
+      if self_branch
+        branch_siblings = recursive_select_parent(LogStash::Config::AST::Branch).first \
+          .recursive_select(LogStash::Config::AST::BranchEntry) \
+          .reject { |b| b == self_branch }
+      end
+
+      #ast = recursive_select_parent(LogStash::Config::AST::PluginSection).first
+      ast = recursive_select_parent(LogStash::Config::AST::Config).first
+
+      found = false
+      recurse(ast) do |element, depth|
+        next false if element.is_a?(LogStash::Config::AST::PluginSection) && element.plugin_type.text_value != "filter"
+        if element == self
+          found = true
+          next false
+        end
+        if found && expressions.include?(element.class)
+          code << element.compile
+          next false
+        end
+        next false if branch_siblings.include?(element)
+        next true
+      end
+
+      return code.collect { |l| "#{l}\n" }.join("")
+    end # def compile_starting_here
   end
 
   class Name < Node
@@ -200,7 +279,7 @@ class Value < RValue; end
 
   module Unicode
     def self.wrap(text)
-      return "(" + text.inspect + ".force_encoding(\"UTF-8\")" + ")"
+      return "(" + text.inspect + ".force_encoding(Encoding::UTF_8)" + ")"
     end
   end
 
@@ -245,24 +324,40 @@ class BranchOrPlugin < Node; end
 
   class Branch < Node
     def compile
-      return super + "end\n"
+
+      # this construct is non obvious. we need to loop through each event and apply the conditional.
+      # each branch of a conditional will contain a construct (a filter for example) that also loops through
+      # the events variable so we have to initialize it to [event] for the branch code.
+      # at the end, events is returned to handle the case where no branch match and no branch code is executed
+      # so we must make sure to return the current event.
+
+      return <<-CODE
+        events = events.flat_map do |event|
+          events = [event]
+          #{super}
+          end
+          events
+        end
+      CODE
     end
   end
-  class If < Node
+
+  class BranchEntry < Node; end
+  class If < BranchEntry
     def compile
       children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
-      return "if #{condition.compile}\n" \
+      return "if #{condition.compile} # if #{condition.text_value}\n" \
         << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
     end
   end
-  class Elsif < Node
+  class Elsif < BranchEntry
     def compile
       children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
-      return "elsif #{condition.compile}\n" \
+      return "elsif #{condition.compile} # else if #{condition.text_value}\n" \
         << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
     end
   end
-  class Else < Node
+  class Else < BranchEntry
     def compile
       children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
       return "else\n" \
@@ -325,7 +420,7 @@ def compile
     end
   end
 
-  module ComparisonOperator 
+  module ComparisonOperator
     def compile
       return " #{text_value} "
     end
diff --git a/lib/logstash/config/grammar.rb b/lib/logstash/config/grammar.rb
index 750c83f0102..1852c851a76 100644
--- a/lib/logstash/config/grammar.rb
+++ b/lib/logstash/config/grammar.rb
@@ -3453,7 +3453,7 @@ def _nt_selector_element
     if r1
       s2, i2 = [], index
       loop do
-        if has_terminal?('\G[^\\], ]', true, index)
+        if has_terminal?('\G[^\\],]', true, index)
           r3 = true
           @index += 1
         else
diff --git a/lib/logstash/config/grammar.treetop b/lib/logstash/config/grammar.treetop
index 42a62b21820..e46fc55307a 100644
--- a/lib/logstash/config/grammar.treetop
+++ b/lib/logstash/config/grammar.treetop
@@ -234,7 +234,7 @@ grammar LogStashConfig
   end
 
   rule selector_element
-    "[" [^\], ]+ "]"
+    "[" [^\],]+ "]"
     <LogStash::Config::AST::SelectorElement>
   end
 
diff --git a/lib/logstash/config/mixin.rb b/lib/logstash/config/mixin.rb
index c6a92347193..f4b0314db2d 100644
--- a/lib/logstash/config/mixin.rb
+++ b/lib/logstash/config/mixin.rb
@@ -5,7 +5,8 @@
 require "logstash/logging"
 require "logstash/util/password"
 require "logstash/version"
-require "i18n"
+require "logstash/environment"
+LogStash::Environment.load_locale!
 
 # This module is meant as a mixin to classes wishing to be configurable from
 # config files
diff --git a/lib/logstash/environment.rb b/lib/logstash/environment.rb
index fd234644cf1..f59ae2bc87b 100644
--- a/lib/logstash/environment.rb
+++ b/lib/logstash/environment.rb
@@ -1,11 +1,16 @@
 require "logstash/errors"
+require 'logstash/version'
 
 module LogStash
   module Environment
     extend self
 
-    LOGSTASH_HOME = ::File.expand_path(::File.join(::File.dirname(__FILE__), "/../.."))
-    JAR_DIR = ::File.join(LOGSTASH_HOME, "/vendor/jar")
+    LOGSTASH_HOME = ::File.expand_path(::File.join(::File.dirname(__FILE__), "..", ".."))
+    JAR_DIR = ::File.join(LOGSTASH_HOME, "vendor", "jar")
+    ELASTICSEARCH_DIR = ::File.join(LOGSTASH_HOME, "vendor", "elasticsearch")
+    BUNDLE_DIR = ::File.join(LOGSTASH_HOME, "vendor", "bundle")
+    GEMFILE_PATH = ::File.join(LOGSTASH_HOME, "tools", "Gemfile")
+    BOOTSTRAP_GEM_PATH = ::File.join(LOGSTASH_HOME, 'build', 'bootstrap')
 
     # loads currently embedded elasticsearch jars
     # @raise LogStash::EnvironmentError if not running under JRuby or if no jar files are found
@@ -13,10 +18,10 @@ def load_elasticsearch_jars!
       raise(LogStash::EnvironmentError, "JRuby is required") unless jruby?
 
       require "java"
-      jars_path = ::File.join(JAR_DIR, "/elasticsearch*/lib/*.jar")
+      jars_path = ::File.join(ELASTICSEARCH_DIR, "**", "*.jar")
       jar_files = Dir.glob(jars_path)
 
-      raise(LogStash::EnvironmentError, "Could not find Elasticsearch jar files under #{JAR_DIR}") if jar_files.empty?
+      raise(LogStash::EnvironmentError, "Could not find Elasticsearch jar files under #{ELASTICSEARCH_DIR}") if jar_files.empty?
 
       jar_files.each do |jar|
         loaded = require jar
@@ -24,12 +29,90 @@ def load_elasticsearch_jars!
       end
     end
 
+    def logstash_gem_home
+      ::File.join(BUNDLE_DIR, ruby_engine, gem_ruby_version)
+    end
+
+    # set GEM_PATH for logstash runtime
+    # GEM_PATH should include the logstash gems, the plugin gems and the bootstrap gems.
+    # the bootstrap gems are required specificly for bundler which is a runtime dependency
+    # of some plugins dependedant gems.
+    def set_gem_paths!
+      ENV["GEM_HOME"] = ENV["GEM_PATH"] = logstash_gem_home
+    end
+
+    def bundler_install_command(gem_file, gem_path)
+      # for now avoid multiple jobs, ex.: --jobs 4
+      # it produces erratic exceptions and hangs (with Bundler 1.7.9)
+      [
+        "install",
+          "--gemfile=#{gem_file}",
+          "--without=development",
+          "--path", gem_path,
+      ]
+    end
+
+    def ruby_bin
+      ENV["USE_RUBY"] == "1" ? "ruby" : File.join("vendor", "jruby", "bin", "jruby")
+    end
+
+    # @return [String] major.minor ruby version, ex 1.9
+    def ruby_abi_version
+      RUBY_VERSION[/(\d+\.\d+)(\.\d+)*/, 1]
+    end
+
+    # @return [String] the ruby version string bundler uses to craft its gem path
+    def gem_ruby_version
+      RbConfig::CONFIG["ruby_version"]
+    end
+
+    # @return [String] jruby, ruby, rbx, ...
+    def ruby_engine
+      RUBY_ENGINE
+    end
+
     def jruby?
-      RUBY_PLATFORM == "java"
+      @jruby ||= !!(RUBY_PLATFORM == "java")
     end
 
     def vendor_path(path)
       return ::File.join(LOGSTASH_HOME, "vendor", path)
     end
+
+    def plugin_path(path)
+      return ::File.join(LOGSTASH_HOME, "lib", "logstash", path)
+    end
+
+    def pattern_path(path)
+      return ::File.join(LOGSTASH_HOME, "patterns", path)
+    end
+
+    def locales_path(path)
+      return ::File.join(LOGSTASH_HOME, "locales", path)
+    end
+
+    def load_logstash_gemspec!
+      logstash_spec = Gem::Specification.new do |gem|
+        gem.authors       = ["Jordan Sissel", "Pete Fritchman"]
+        gem.email         = ["jls@semicomplete.com", "petef@databits.net"]
+        gem.description   = %q{scalable log and event management (search, archive, pipeline)}
+        gem.summary       = %q{logstash - log and event management}
+        gem.homepage      = "http://logstash.net/"
+        gem.license       = "Apache License (2.0)"
+
+        gem.name          = "logstash"
+        gem.version       = LOGSTASH_VERSION
+      end
+
+      Gem::Specification.add_spec logstash_spec
+    end
+
+    def load_locale!
+      require "i18n"
+      I18n.enforce_available_locales = true
+      I18n.load_path << LogStash::Environment.locales_path("en.yml")
+      I18n.reload!
+      fail "No locale? This is a bug." if I18n.available_locales.empty?
+    end
   end
 end
diff --git a/lib/logstash/event.rb b/lib/logstash/event.rb
index 1604ad60346..18b343ca281 100644
--- a/lib/logstash/event.rb
+++ b/lib/logstash/event.rb
@@ -1,23 +1,20 @@
 # encoding: utf-8
-require "json"
 require "time"
 require "date"
+require "cabin"
 require "logstash/namespace"
 require "logstash/util/fieldreference"
 require "logstash/util/accessors"
-require "logstash/time_addon"
+require "logstash/timestamp"
+require "logstash/json"
 
-# Use a custom serialization for jsonifying Time objects.
-# TODO(sissel): Put this in a separate file.
-class Time
-  def to_json(*args)
-    return iso8601(3).to_json(*args)
-  end
-
-  def inspect
-    return to_json
-  end
-end
+# transcient pipeline events for normal in-flow signaling as opposed to
+# flow altering exceptions. for now having base classes is adequate and
+# in the future it might be necessary to refactor using like a BaseEvent
+# class to have a common interface for all pileline events to support
+# eventual queueing persistence for example, TBD.
+class LogStash::ShutdownEvent; end
+class LogStash::FlushEvent; end
 
 # the logstash event object.
 #
@@ -48,23 +45,29 @@ class DeprecatedMethod < StandardError; end
   TIMESTAMP = "@timestamp"
   VERSION = "@version"
   VERSION_ONE = "1"
+  TIMESTAMP_FAILURE_TAG = "_timestampparsefailure"
+  TIMESTAMP_FAILURE_FIELD = "_@timestamp"
+
+  # Floats outside of these upper and lower bounds are forcibly converted
+  # to scientific notation by Float#to_s
+  MIN_FLOAT_BEFORE_SCI_NOT = 0.0001
+  MAX_FLOAT_BEFORE_SCI_NOT = 1000000000000000.0
 
   public
-  def initialize(data={})
+  def initialize(data = {})
+    @logger = Cabin::Channel.get(LogStash)
     @cancelled = false
-
     @data = data
     @accessors = LogStash::Util::Accessors.new(data)
+    @data[VERSION] ||= VERSION_ONE
+    @data[TIMESTAMP] = init_timestamp(@data[TIMESTAMP])
 
-    data[VERSION] = VERSION_ONE if !@data.include?(VERSION)
-    if data.include?(TIMESTAMP)
-      t = data[TIMESTAMP]
-      if t.is_a?(String)
-        data[TIMESTAMP] = LogStash::Time.parse_iso8601(t)
-      end
+    @metadata = if @data.include?("@metadata")
+      @data.delete("@metadata")
     else
-      data[TIMESTAMP] = ::Time.now.utc
+      {}
     end
+    @metadata_accessors = LogStash::Util::Accessors.new(@metadata)
   end # def initialize
 
   public
@@ -93,17 +96,10 @@ def clone
     return self.class.new(copy)
   end # def clone
 
-  if RUBY_ENGINE == "jruby"
-    public
-    def to_s
-      return self.sprintf("%{+yyyy-MM-dd'T'HH:mm:ss.SSSZ} %{host} %{message}")
-    end # def to_s
-  else
-    public
-    def to_s
-      return self.sprintf("#{self["@timestamp"].iso8601} %{host} %{message}")
-    end # def to_s
-  end
+  public
+  def to_s
+    self.sprintf("#{timestamp.to_iso8601} %{host} %{message}")
+  end # def to_s
 
   public
   def timestamp; return @data[TIMESTAMP]; end # def timestamp
@@ -118,24 +114,31 @@ def ruby_timestamp
   end # def unix_timestamp
 
   # field-related access
+  METADATA = "@metadata".freeze
+  METADATA_BRACKETS = "[#{METADATA}]".freeze
   public
-  def [](str)
-    if str[0,1] == CHAR_PLUS
-      # nothing?
+  def [](fieldref)
+    if fieldref.start_with?(METADATA_BRACKETS)
+      @metadata_accessors.get(fieldref[METADATA_BRACKETS.length .. -1])
+    elsif fieldref == METADATA
+      @metadata
     else
-      # return LogStash::Util::FieldReference.exec(str, @data)
-      @accessors.get(str)
+      @accessors.get(fieldref)
     end
   end # def []
 
   public
-  # keep []= implementation in sync with spec/test_utils.rb monkey patch
-  # which redefines []= but using @accessors.strict_set
-  def []=(str, value)
-    if str == TIMESTAMP && !value.is_a?(Time)
-      raise TypeError, "The field '@timestamp' must be a Time, not a #{value.class} (#{value})"
+  def []=(fieldref, value)
+    if fieldref == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
+      raise TypeError, "The field '@timestamp' must be a (LogStash::Timestamp, not a #{value.class} (#{value})"
+    end
+    if fieldref.start_with?(METADATA_BRACKETS)
+      @metadata_accessors.set(fieldref[METADATA_BRACKETS.length .. -1], value)
+    elsif fieldref == METADATA
+      @metadata = value
+    else
+      @accessors.set(fieldref, value)
     end
-    @accessors.set(str, value)
   end # def []=
 
   public
@@ -145,11 +148,13 @@ def fields
 
   public
   def to_json(*args)
-    return @data.to_json(*args)
+    # ignore arguments to respect accepted to_json method signature
+    LogStash::Json.dump(@data)
   end # def to_json
 
+  public
   def to_hash
-    return @data
+    @data
   end # def to_hash
 
   public
@@ -161,7 +166,7 @@ def overwrite(event)
 
     #convert timestamp if it is a String
     if @data[TIMESTAMP].is_a?(String)
-      @data[TIMESTAMP] = LogStash::Time.parse_iso8601(@data[TIMESTAMP])
+      @data[TIMESTAMP] = LogStash::Timestamp.parse_iso8601(@data[TIMESTAMP])
     end
   end
 
@@ -183,11 +188,8 @@ def append(event)
   # Remove a field or field reference. Returns the value of that field when
   # deleted
   public
-  def remove(str)
-    # return LogStash::Util::FieldReference.exec(str, @data) do |obj, key|
-    #   next obj.delete(key)
-    # end
-    @accessors.del(str)
+  def remove(fieldref)
+    @accessors.del(fieldref)
   end # def remove
 
   # sprintf. This could use a better method name.
@@ -208,7 +210,12 @@ def remove(str)
   # is an array (or hash?) should be. Join by comma? Something else?
   public
   def sprintf(format)
-    format = format.to_s
+    if format.is_a?(Float) and
+        (format < MIN_FLOAT_BEFORE_SCI_NOT or format >= MAX_FLOAT_BEFORE_SCI_NOT) then
+      format = ("%.15f" % format).sub(/0*$/,"")
+    else
+      format = format.to_s
+    end
     if format.index("%").nil?
       return format
     end
@@ -219,9 +226,9 @@ def sprintf(format)
 
       if key == "+%s"
         # Got %{+%s}, support for unix epoch time
-        next @data["@timestamp"].to_i
+        next @data[TIMESTAMP].to_i
       elsif key[0,1] == "+"
-        t = @data["@timestamp"]
+        t = @data[TIMESTAMP]
         formatter = org.joda.time.format.DateTimeFormat.forPattern(key[1 .. -1])\
           .withZone(org.joda.time.DateTimeZone::UTC)
         #next org.joda.time.Instant.new(t.tv_sec * 1000 + t.tv_usec / 1000).toDateTime.toString(formatter)
@@ -238,7 +245,7 @@ def sprintf(format)
           when Array
             value.join(",") # Join by ',' if value is an array
           when Hash
-            value.to_json # Convert hashes to json
+            LogStash::Json.dump(value) # Convert hashes to json
           else
             value # otherwise return the value
         end # case value
@@ -251,4 +258,53 @@ def tag(value)
     self["tags"] ||= []
     self["tags"] << value unless self["tags"].include?(value)
   end
+
+  private
+
+  def init_timestamp(o)
+    begin
+      timestamp = o ? LogStash::Timestamp.coerce(o) : LogStash::Timestamp.now
+      return timestamp if timestamp
+
+      @logger.warn("Unrecognized #{TIMESTAMP} value, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD}field", :value => o.inspect)
+    rescue LogStash::TimestampParserError => e
+      @logger.warn("Error parsing #{TIMESTAMP} string, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD} field", :value => o.inspect, :exception => e.message)
+    end
+
+    @data["tags"] ||= []
+    @data["tags"] << TIMESTAMP_FAILURE_TAG unless @data["tags"].include?(TIMESTAMP_FAILURE_TAG)
+    @data[TIMESTAMP_FAILURE_FIELD] = o
+
+    LogStash::Timestamp.now
+  end
+
+  public
+  def to_hash_with_metadata
+    if @metadata.nil?
+      to_hash
+    else
+      to_hash.merge("@metadata" => @metadata)
+    end
+  end
+
+  public
+  def to_json_with_metadata(*args)
+    # ignore arguments to respect accepted to_json method signature
+    LogStash::Json.dump(to_hash_with_metadata)
+  end # def to_json
+
+  def self.validate_value(value)
+    case value
+    when String
+      raise("expected UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.encoding == Encoding::UTF_8
+      raise("invalid UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.valid_encoding?
+      value
+    when Array
+      value.each{|v| validate_value(v)} # don't map, return original object
+      value
+    else
+      value
+    end
+  end
+
 end # class LogStash::Event
diff --git a/lib/logstash/filters/anonymize.rb b/lib/logstash/filters/anonymize.rb
deleted file mode 100644
index 8ec0b77031f..00000000000
--- a/lib/logstash/filters/anonymize.rb
+++ /dev/null
@@ -1,95 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Anonymize fields using by replacing values with a consistent hash.
-class LogStash::Filters::Anonymize < LogStash::Filters::Base
-  config_name "anonymize"
-  milestone 1
-
-  # The fields to be anonymized
-  config :fields, :validate => :array, :required => true
-
-  # Hashing key
-  # When using MURMUR3 the key is ignored but must still be set.
-  # When using IPV4_NETWORK key is the subnet prefix lentgh
-  config :key, :validate => :string, :required => true
-
-  # digest/hash type
-  config :algorithm, :validate => ['SHA1', 'SHA256', 'SHA384', 'SHA512', 'MD5', "MURMUR3", "IPV4_NETWORK"], :required => true, :default => 'SHA1'
-
-  public
-  def register
-    # require any library and set the anonymize function
-    case @algorithm
-    when "IPV4_NETWORK"
-      require 'ipaddr'
-      class << self; alias_method :anonymize, :anonymize_ipv4_network; end
-    when "MURMUR3"
-      require "murmurhash3"
-      class << self; alias_method :anonymize, :anonymize_murmur3; end
-    else
-      require 'openssl'
-      class << self; alias_method :anonymize, :anonymize_openssl; end
-    end
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    @fields.each do |field|
-      next unless event.include?(field)
-      if event[field].is_a?(Array)
-        event[field] = event[field].collect { |v| anonymize(v) }
-      else
-        event[field] = anonymize(event[field])
-      end
-    end
-  end # def filter
-
-  private
-  def anonymize_ipv4_network(ip_string)
-    # in JRuby 1.7.11 outputs as US-ASCII
-    IPAddr.new(ip_string).mask(@key.to_i).to_s.force_encoding(Encoding::UTF_8)
-  end
-
-  def anonymize_openssl(data)
-    digest = algorithm()
-    # in JRuby 1.7.11 outputs as ASCII-8BIT
-    OpenSSL::HMAC.hexdigest(digest, @key, data).force_encoding(Encoding::UTF_8)
-  end
-
-  def anonymize_murmur3(value)
-    case value
-    when Fixnum
-      MurmurHash3::V32.int_hash(value)
-    when String
-      MurmurHash3::V32.str_hash(value)
-    end
-  end
-
-  def algorithm
-
-   case @algorithm
-      #when 'SHA'
-        #return OpenSSL::Digest::SHA.new
-      when 'SHA1'
-        return OpenSSL::Digest::SHA1.new
-      #when 'SHA224'
-        #return OpenSSL::Digest::SHA224.new
-      when 'SHA256'
-        return OpenSSL::Digest::SHA256.new
-      when 'SHA384'
-        return OpenSSL::Digest::SHA384.new
-      when 'SHA512'
-        return OpenSSL::Digest::SHA512.new
-      #when 'MD4'
-        #return OpenSSL::Digest::MD4.new
-      when 'MD5'
-        return OpenSSL::Digest::MD5.new
-      else
-        @logger.error("Unknown algorithm")
-    end
-  end
-
-end # class LogStash::Filters::Anonymize
diff --git a/lib/logstash/filters/base.rb b/lib/logstash/filters/base.rb
index dba1848bdc1..d71d7c5d093 100644
--- a/lib/logstash/filters/base.rb
+++ b/lib/logstash/filters/base.rb
@@ -9,8 +9,8 @@ class LogStash::Filters::Base < LogStash::Plugin
 
   config_name "filter"
 
-  # Note that all of the specified routing options (type,tags.exclude\_tags,include\_fields,exclude\_fields)
-  # must be met in order for the event to be handled by the filter.
+  # Note that all of the specified routing options (`type`,`tags`,`exclude_tags`,`include_fields`,
+  # `exclude_fields`) must be met in order for the event to be handled by the filter.
 
   # The type to act on. If a type is given, then this filter will only
   # act on messages with the same type. See any input plugin's "type"
@@ -18,25 +18,27 @@ class LogStash::Filters::Base < LogStash::Plugin
   # Optional.
   config :type, :validate => :string, :default => "", :deprecated => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
 
-  # Only handle events with all/any (controlled by include\_any config option) of these tags.
+  # Only handle events with all/any (controlled by `include_any` config option) of these tags.
   # Optional.
   config :tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if \"sometag\" in [tags] { %PLUGIN% { ... } }`"
 
-  # Only handle events without all/any (controlled by exclude\_any config
+  # Only handle events without all/any (controlled by `exclude_any` config
   # option) of these tags.
   # Optional.
   config :exclude_tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if !(\"sometag\" in [tags]) { %PLUGIN% { ... } }`"
 
   # If this filter is successful, add arbitrary tags to the event.
-  # Tags can be dynamic and include parts of the event using the %{field}
-  # syntax. Example:
+  # Tags can be dynamic and include parts of the event using the `%{field}`
+  # syntax.
   #
+  # Example:
+  # [source,ruby]
   #     filter {
   #       %PLUGIN% {
   #         add_tag => [ "foo_%{somefield}" ]
   #       }
   #     }
-  #
+  # [source,ruby]
   #     # You can also add multiple tags at once:
   #     filter {
   #       %PLUGIN% {
@@ -44,83 +46,87 @@ class LogStash::Filters::Base < LogStash::Plugin
   #       }
   #     }
   #
-  # If the event has field "somefield" == "hello" this filter, on success,
-  # would add a tag "foo_hello" (and the second example would of course add a "taggedy_tag" tag).
+  # If the event has field `"somefield" == "hello"` this filter, on success,
+  # would add a tag `foo_hello` (and the second example would of course add a `taggedy_tag` tag).
   config :add_tag, :validate => :array, :default => []
 
   # If this filter is successful, remove arbitrary tags from the event.
-  # Tags can be dynamic and include parts of the event using the %{field}
-  # syntax. Example:
+  # Tags can be dynamic and include parts of the event using the `%{field}`
+  # syntax.
   #
+  # Example:
+  # [source,ruby]
   #     filter {
   #       %PLUGIN% {
   #         remove_tag => [ "foo_%{somefield}" ]
   #       }
   #     }
-  #
+  # [source,ruby]
   #     # You can also remove multiple tags at once:
-  # 
   #     filter {
   #       %PLUGIN% {
   #         remove_tag => [ "foo_%{somefield}", "sad_unwanted_tag"]
   #       }
   #     }
   #
-  # If the event has field "somefield" == "hello" this filter, on success,
-  # would remove the tag "foo_hello" if it is present. The second example
-  # would remove a sad, unwanted tag as well. 
+  # If the event has field `"somefield" == "hello"` this filter, on success,
+  # would remove the tag `foo_hello` if it is present. The second example
+  # would remove a sad, unwanted tag as well.
   config :remove_tag, :validate => :array, :default => []
 
   # If this filter is successful, add any arbitrary fields to this event.
-  # Field names can be dynamic and include parts of the event using the %{field}
-  # Example:
+  # Field names can be dynamic and include parts of the event using the `%{field}`.
   #
+  # Example:
+  # [source,ruby]
   #     filter {
   #       %PLUGIN% {
   #         add_field => { "foo_%{somefield}" => "Hello world, from %{host}" }
   #       }
   #     }
-  #
+  # [source,ruby]
   #     # You can also add multiple fields at once:
-  #
   #     filter {
   #       %PLUGIN% {
-  #         add_field => { 
+  #         add_field => {
   #           "foo_%{somefield}" => "Hello world, from %{host}"
   #           "new_field" => "new_static_value"
   #         }
   #       }
   #     }
   #
-  # If the event has field "somefield" == "hello" this filter, on success,
-  # would add field "foo_hello" if it is present, with the
-  # value above and the %{host} piece replaced with that value from the
-  # event. The second example would also add a hardcoded field. 
+  # If the event has field `"somefield" == "hello"` this filter, on success,
+  # would add field `foo_hello` if it is present, with the
+  # value above and the `%{host}` piece replaced with that value from the
+  # event. The second example would also add a hardcoded field.
   config :add_field, :validate => :hash, :default => {}
 
   # If this filter is successful, remove arbitrary fields from this event.
   # Fields names can be dynamic and include parts of the event using the %{field}
   # Example:
-  #
+  # [source,ruby]
   #     filter {
   #       %PLUGIN% {
   #         remove_field => [ "foo_%{somefield}" ]
   #       }
   #     }
-  #
+  # [source,ruby]
   #     # You can also remove multiple fields at once:
-  #
   #     filter {
   #       %PLUGIN% {
-  #         remove_field => [ "foo_%{somefield}" "my_extraneous_field" ]
+  #         remove_field => [ "foo_%{somefield}", "my_extraneous_field" ]
   #       }
   #     }
   #
-  # If the event has field "somefield" == "hello" this filter, on success,
-  # would remove the field with name "foo_hello" if it is present. The second 
+  # If the event has field `"somefield" == "hello"` this filter, on success,
+  # would remove the field with name `foo_hello` if it is present. The second
   # example would remove an additional, non-dynamic field.
   config :remove_field, :validate => :array, :default => []
 
+  # Call the filter flush method at regular interval.
+  # Optional.
+  config :periodic_flush, :validate => :boolean, :default => false
+
   RESERVED = ["type", "tags", "exclude_tags", "include_fields", "exclude_fields", "add_tag", "remove_tag", "add_field", "remove_field", "include_any", "exclude_any"]
 
   public
@@ -169,11 +175,11 @@ def filter_matched(event)
                                        :field => field, :value => value)
       end
     end
-    
+
     @remove_field.each do |field|
       field = event.sprintf(field)
       @logger.debug? and @logger.debug("filters/#{self.class.name}: removing field",
-                                       :field => field) 
+                                       :field => field)
       event.remove(field)
     end
 
@@ -223,4 +229,9 @@ def filter?(event)
 
     return true
   end
+
+  public
+  def teardown
+    # Nothing to do by default.
+  end
 end # class LogStash::Filters::Base
diff --git a/lib/logstash/filters/checksum.rb b/lib/logstash/filters/checksum.rb
deleted file mode 100644
index 348bc39d06c..00000000000
--- a/lib/logstash/filters/checksum.rb
+++ /dev/null
@@ -1,53 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "yaml"
-
-# This filter let's you create a checksum based on various parts
-# of the logstash event.
-# This can be useful for deduplication of messages or simply to provide
-# a custom unique identifier.
-#
-# This is VERY experimental and is largely a proof-of-concept
-class LogStash::Filters::Checksum < LogStash::Filters::Base
-
-  config_name "checksum"
-  milestone 1
-
-  ALGORITHMS = ["md5", "sha", "sha1", "sha256", "sha384",]
-
-  # A list of keys to use in creating the string to checksum
-  # Keys will be sorted before building the string
-  # keys and values will then be concatenated with pipe delimeters
-  # and checksummed
-  config :keys, :validate => :array, :default => ["message", "@timestamp", "type"]
-
-  config :algorithm, :validate => ALGORITHMS, :default => "sha256"
-
-  public
-  def register
-    require 'openssl'
-    @to_checksum = ""
-  end
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @logger.debug("Running checksum filter", :event => event)
-
-    @keys.sort.each do |k|
-      @logger.debug("Adding key to string", :current_key => k)
-      @to_checksum << "|#{k}|#{event[k]}"
-    end
-    @to_checksum << "|"
-    @logger.debug("Final string built", :to_checksum => @to_checksum)
-
-
-    # in JRuby 1.7.11 outputs as ASCII-8BIT
-    digested_string = OpenSSL::Digest.hexdigest(@algorithm, @to_checksum).force_encoding(Encoding::UTF_8)
-
-    @logger.debug("Digested string", :digested_string => digested_string)
-    event['logstash_checksum'] = digested_string
-  end
-end # class LogStash::Filters::Checksum
diff --git a/lib/logstash/filters/clone.rb b/lib/logstash/filters/clone.rb
deleted file mode 100644
index 162d18156e6..00000000000
--- a/lib/logstash/filters/clone.rb
+++ /dev/null
@@ -1,35 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The clone filter is for duplicating events.
-# A clone will be made for each type in the clone list.
-# The original event is left unchanged.
-class LogStash::Filters::Clone < LogStash::Filters::Base
-
-  config_name "clone"
-  milestone 2
-
-  # A new clone will be created with the given type for each type in this list.
-  config :clones, :validate => :array, :default => []
-
-  public
-  def register
-    # Nothing to do
-  end
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    @clones.each do |type|
-      clone = event.clone
-      clone["type"] = type
-      filter_matched(clone)
-      @logger.debug("Cloned event", :clone => clone, :event => event)
-
-      # Push this new event onto the stack at the LogStash::FilterWorker
-      yield clone
-    end
-  end
-
-end # class LogStash::Filters::Clone
diff --git a/lib/logstash/filters/csv.rb b/lib/logstash/filters/csv.rb
deleted file mode 100644
index cfdabf7566e..00000000000
--- a/lib/logstash/filters/csv.rb
+++ /dev/null
@@ -1,97 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-require "csv"
-
-# The CSV filter takes an event field containing CSV data, parses it,
-# and stores it as individual fields (can optionally specify the names).
-# This filter can also parse data with any separator, not just commas.
-class LogStash::Filters::CSV < LogStash::Filters::Base
-  config_name "csv"
-  milestone 2
-
-  # The CSV data in the value of the `source` field will be expanded into a
-  # data structure.
-  config :source, :validate => :string, :default => "message"
-
-  # Define a list of column names (in the order they appear in the CSV,
-  # as if it were a header line). If `columns` is not configured, or there
-  # are not enough columns specified, the default column names are
-  # "column1", "column2", etc. In the case that there are more columns
-  # in the data than specified in this column list, extra columns will be auto-numbered:
-  # (e.g. "user_defined_1", "user_defined_2", "column3", "column4", etc.)
-  config :columns, :validate => :array, :default => []
-
-  # Define the column separator value. If this is not specified, the default
-  # is a comma ','.
-  # Optional.
-  config :separator, :validate => :string, :default => ","
-
-  # Define the character used to quote CSV fields. If this is not specified
-  # the default is a double quote '"'.
-  # Optional.
-  config :quote_char, :validate => :string, :default => '"'
-
-  # Define target field for placing the data.
-  # Defaults to writing to the root of the event.
-  config :target, :validate => :string
-
-  public
-  def register
-
-    # Nothing to do here
-
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @logger.debug("Running csv filter", :event => event)
-
-    matches = 0
-
-    if event[@source]
-      if event[@source].is_a?(String)
-        event[@source] = [event[@source]]
-      end
-
-      if event[@source].length > 1
-        @logger.warn("csv filter only works on fields of length 1",
-                     :source => @source, :value => event[@source],
-                     :event => event)
-        return
-      end
-
-      raw = event[@source].first
-      begin
-        values = CSV.parse_line(raw, :col_sep => @separator, :quote_char => @quote_char)
-
-        if @target.nil?
-          # Default is to write to the root of the event.
-          dest = event
-        else
-          dest = event[@target] ||= {}
-        end
-
-        values.each_index do |i|
-          field_name = @columns[i] || "column#{i+1}"
-          dest[field_name] = values[i]
-        end
-
-        filter_matched(event)
-      rescue => e
-        event.tag "_csvparsefailure"
-        @logger.warn("Trouble parsing csv", :source => @source, :raw => raw,
-                      :exception => e)
-        return
-      end # begin
-    end # if event
-
-    @logger.debug("Event after csv filter", :event => event)
-
-  end # def filter
-
-end # class LogStash::Filters::Csv
-
diff --git a/lib/logstash/filters/date.rb b/lib/logstash/filters/date.rb
deleted file mode 100644
index 87663f4ee3e..00000000000
--- a/lib/logstash/filters/date.rb
+++ /dev/null
@@ -1,225 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The date filter is used for parsing dates from fields, and then using that
-# date or timestamp as the logstash timestamp for the event.
-#
-# For example, syslog events usually have timestamps like this:
-#
-#     "Apr 17 09:32:01"
-#
-# You would use the date format "MMM dd HH:mm:ss" to parse this.
-#
-# The date filter is especially important for sorting events and for
-# backfilling old data. If you don't get the date correct in your
-# event, then searching for them later will likely sort out of order.
-#
-# In the absence of this filter, logstash will choose a timestamp based on the
-# first time it sees the event (at input time), if the timestamp is not already
-# set in the event. For example, with file input, the timestamp is set to the
-# time of each read.
-class LogStash::Filters::Date < LogStash::Filters::Base
-  if RUBY_ENGINE == "jruby"
-    JavaException = java.lang.Exception
-    UTC = org.joda.time.DateTimeZone.forID("UTC")
-  end
-
-  config_name "date"
-  milestone 3
-
-  # Specify a time zone canonical ID to be used for date parsing.
-  # The valid IDs are listed on the [Joda.org available time zones page](http://joda-time.sourceforge.net/timezones.html).
-  # This is useful in case the time zone cannot be extracted from the value,
-  # and is not the platform default.
-  # If this is not specified the platform default will be used.
-  # Canonical ID is good as it takes care of daylight saving time for you
-  # For example, `America/Los_Angeles` or `Europe/France` are valid IDs.
-  config :timezone, :validate => :string
-
-  # Specify a locale to be used for date parsing. If this is not specified, the
-  # platform default will be used.
-  #
-  # The locale is mostly necessary to be set for parsing month names and
-  # weekday names.
-  #
-  config :locale, :validate => :string
-
-  # The date formats allowed are anything allowed by Joda-Time (java time
-  # library). You can see the docs for this format here:
-  #
-  # [joda.time.format.DateTimeFormat](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html)
-  #
-  # An array with field name first, and format patterns following, `[ field,
-  # formats... ]`
-  #
-  # If your time field has multiple possible formats, you can do this:
-  #
-  #     match => [ "logdate", "MMM dd YYY HH:mm:ss",
-  #               "MMM  d YYY HH:mm:ss", "ISO8601" ]
-  #
-  # The above will match a syslog (rfc3164) or iso8601 timestamp.
-  #
-  # There are a few special exceptions. The following format literals exist
-  # to help you save time and ensure correctness of date parsing.
-  #
-  # * "ISO8601" - should parse any valid ISO8601 timestamp, such as
-  #   2011-04-19T03:44:01.103Z
-  # * "UNIX" - will parse unix time in seconds since epoch
-  # * "UNIX_MS" - will parse unix time in milliseconds since epoch
-  # * "TAI64N" - will parse tai64n time values
-  #
-  # For example, if you have a field 'logdate', with a value that looks like
-  # 'Aug 13 2010 00:03:44', you would use this configuration:
-  #
-  #     filter {
-  #       date {
-  #         match => [ "logdate", "MMM dd YYYY HH:mm:ss" ]
-  #       }
-  #     }
-  #
-  # If your field is nested in your structure, you can use the nested
-  # syntax [foo][bar] to match its value. For more information, please refer to
-  # http://logstash.net/docs/latest/configuration#fieldreferences
-  config :match, :validate => :array, :default => []
-
-  # Store the matching timestamp into the given target field.  If not provided,
-  # default to updating the @timestamp field of the event.
-  config :target, :validate => :string, :default => "@timestamp"
-
-  # LOGSTASH-34
-  DATEPATTERNS = %w{ y d H m s S } 
-
-  public
-  def initialize(config = {})
-    super
-
-    @parsers = Hash.new { |h,k| h[k] = [] }
-  end # def initialize
-
-  private
-  def parseLocale(localeString)
-    return nil if localeString == nil
-    matches = localeString.match(/(?<lang>.+?)(?:_(?<country>.+?))?(?:_(?<variant>.+))?/)
-    lang = matches['lang'] == nil ? "" : matches['lang'].strip()
-    country = matches['country'] == nil ? "" : matches['country'].strip()
-    variant = matches['variant'] == nil ? "" : matches['variant'].strip()
-    return lang.length > 0 ? java.util.Locale.new(lang, country, variant) : nil
-  end
-
-  public
-  def register
-    require "java"
-    if @match.length < 2
-      raise LogStash::ConfigurationError, I18n.t("logstash.agent.configuration.invalid_plugin_register", 
-        :plugin => "filter", :type => "date",
-        :error => "The match setting should contains first a field name and at least one date format, current value is #{@match}")
-    end
-    # TODO(sissel): Need a way of capturing regexp configs better.
-    locale = parseLocale(@config["locale"][0]) if @config["locale"] != nil and @config["locale"][0] != nil
-    setupMatcher(@config["match"].shift, locale, @config["match"] )
-  end
-
-  def setupMatcher(field, locale, value)
-    value.each do |format|
-      case format
-        when "ISO8601"
-          joda_parser = org.joda.time.format.ISODateTimeFormat.dateTimeParser
-          if @timezone
-            joda_parser = joda_parser.withZone(org.joda.time.DateTimeZone.forID(@timezone))
-          else
-            joda_parser = joda_parser.withOffsetParsed
-          end
-          parser = lambda { |date| joda_parser.parseMillis(date) }
-        when "UNIX" # unix epoch
-          joda_instant = org.joda.time.Instant.java_class.constructor(Java::long).method(:new_instance)
-          #parser = lambda { |date| joda_instant.call((date.to_f * 1000).to_i).to_java.toDateTime }
-          parser = lambda { |date| (date.to_f * 1000).to_i }
-        when "UNIX_MS" # unix epoch in ms
-          joda_instant = org.joda.time.Instant.java_class.constructor(Java::long).method(:new_instance)
-          parser = lambda do |date| 
-            #return joda_instant.call(date.to_i).to_java.toDateTime
-            return date.to_i
-          end
-        when "TAI64N" # TAI64 with nanoseconds, -10000 accounts for leap seconds
-          joda_instant = org.joda.time.Instant.java_class.constructor(Java::long).method(:new_instance)
-          parser = lambda do |date| 
-            # Skip leading "@" if it is present (common in tai64n times)
-            date = date[1..-1] if date[0, 1] == "@"
-            #return joda_instant.call((date[1..15].hex * 1000 - 10000)+(date[16..23].hex/1000000)).to_java.toDateTime 
-            return (date[1..15].hex * 1000 - 10000)+(date[16..23].hex/1000000)
-          end
-        else
-          joda_parser = org.joda.time.format.DateTimeFormat.forPattern(format).withDefaultYear(Time.new.year)
-          if @timezone
-            joda_parser = joda_parser.withZone(org.joda.time.DateTimeZone.forID(@timezone))
-          else
-            joda_parser = joda_parser.withOffsetParsed
-          end
-          if (locale != nil)
-            joda_parser = joda_parser.withLocale(locale)
-          end
-          parser = lambda { |date| joda_parser.parseMillis(date) }
-      end
-
-      @logger.debug("Adding type with date config", :type => @type,
-                    :field => field, :format => format)
-      @parsers[field] << {
-        :parser => parser,
-        :format => format
-      }
-    end
-  end
-
-  # def register
-
-  public
-  def filter(event)
-    @logger.debug? && @logger.debug("Date filter: received event", :type => event["type"])
-    return unless filter?(event)
-    @parsers.each do |field, fieldparsers|
-      @logger.debug? && @logger.debug("Date filter looking for field",
-                                      :type => event["type"], :field => field)
-      next unless event.include?(field)
-
-      fieldvalues = event[field]
-      fieldvalues = [fieldvalues] if !fieldvalues.is_a?(Array)
-      fieldvalues.each do |value|
-        next if value.nil?
-        begin
-          epochmillis = nil
-          success = false
-          last_exception = RuntimeError.new "Unknown"
-          fieldparsers.each do |parserconfig|
-            parser = parserconfig[:parser]
-            begin
-              epochmillis = parser.call(value)
-              success = true
-              break # success
-            rescue StandardError, JavaException => e
-              last_exception = e
-            end
-          end # fieldparsers.each
-
-          raise last_exception unless success
-
-          # Convert joda DateTime to a ruby Time
-          event[@target] = Time.at(epochmillis / 1000, (epochmillis % 1000) * 1000).utc
-          #event[@target] = Time.at(epochmillis / 1000.0).utc
-
-          @logger.debug? && @logger.debug("Date parsing done", :value => value, :timestamp => event[@target])
-          filter_matched(event)
-        rescue StandardError, JavaException => e
-          @logger.warn("Failed parsing date from field", :field => field,
-                       :value => value, :exception => e)
-          # Raising here will bubble all the way up and cause an exit.
-          # TODO(sissel): Maybe we shouldn't raise?
-          # TODO(sissel): What do we do on a failure? Tag it like grok does?
-          #raise e
-        end # begin
-      end # fieldvalue.each 
-    end # @parsers.each
-
-    return event
-  end # def filter
-end # class LogStash::Filters::Date
diff --git a/lib/logstash/filters/dns.rb b/lib/logstash/filters/dns.rb
deleted file mode 100644
index 46bb0755574..00000000000
--- a/lib/logstash/filters/dns.rb
+++ /dev/null
@@ -1,203 +0,0 @@
-# encoding: utf-8
-# DNS Filter
-#
-# This filter will resolve any IP addresses from a field of your choosing.
-#
-
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The DNS filter performs a lookup (either an A record/CNAME record lookup
-# or a reverse lookup at the PTR record) on records specified under the
-# "reverse" and "resolve" arrays.
-#
-# The config should look like this:
-#
-#     filter {
-#       dns {
-#         type => 'type'
-#         reverse => [ "source_host", "field_with_address" ]
-#         resolve => [ "field_with_fqdn" ]
-#         action => "replace"
-#       }
-#     }
-#
-# Caveats: at the moment, there's no way to tune the timeout with the 'resolv'
-# core library.  It does seem to be fixed in here:
-#
-#   http://redmine.ruby-lang.org/issues/5100
-#
-# but isn't currently in JRuby.
-class LogStash::Filters::DNS < LogStash::Filters::Base
-
-  config_name "dns"
-  milestone 2
-
-  # Reverse resolve one or more fields.
-  config :reverse, :validate => :array
-
-  # Forward resolve one or more fields.
-  config :resolve, :validate => :array
-
-  # Determine what action to do: append or replace the values in the fields
-  # specified under "reverse" and "resolve."
-  config :action, :validate => [ "append", "replace" ], :default => "append"
-
-  # Use custom nameserver.
-  config :nameserver, :validate => :string
-
-  # TODO(sissel): make 'action' required? This was always the intent, but it
-  # due to a typo it was never enforced. Thus the default behavior in past
-  # versions was 'append' by accident.
-
-  # resolv calls will be wrapped in a timeout instance
-  config :timeout, :validate => :number, :default => 2
-
-  public
-  def register
-    require "resolv"
-    require "timeout"
-    if @nameserver.nil?
-      @resolv = Resolv.new
-    else
-      @resolv = Resolv.new(resolvers=[::Resolv::Hosts.new, ::Resolv::DNS.new(:nameserver => [@nameserver], :search => [], :ndots => 1)])
-    end
-
-    @ip_validator = Resolv::AddressRegex
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    if @resolve
-      begin
-        status = Timeout::timeout(@timeout) {
-          resolve(event)
-        }
-      rescue Timeout::Error
-        @logger.debug("DNS: resolve action timed out")
-        return
-      end
-    end
-
-    if @reverse
-      begin
-        status = Timeout::timeout(@timeout) {
-          reverse(event)
-        }
-      rescue Timeout::Error
-        @logger.debug("DNS: reverse action timed out")
-        return
-      end
-    end
-
-    filter_matched(event)
-  end
-
-  private
-  def resolve(event)
-    @resolve.each do |field|
-      is_array = false
-      raw = event[field]
-      if raw.is_a?(Array)
-        is_array = true
-        if raw.length > 1
-          @logger.warn("DNS: skipping resolve, can't deal with multiple values", :field => field, :value => raw)
-          return
-        end
-        raw = raw.first
-      end
-
-      begin
-        # in JRuby 1.7.11 outputs as US-ASCII
-        address = @resolv.getaddress(raw).force_encoding(Encoding::UTF_8)
-      rescue Resolv::ResolvError
-        @logger.debug("DNS: couldn't resolve the hostname.",
-                      :field => field, :value => raw)
-        return
-      rescue Resolv::ResolvTimeout
-        @logger.debug("DNS: timeout on resolving the hostname.",
-                      :field => field, :value => raw)
-        return
-      rescue SocketError => e
-        @logger.debug("DNS: Encountered SocketError.",
-                      :field => field, :value => raw)
-        return
-      rescue NoMethodError => e
-        # see JRUBY-5647
-        @logger.debug("DNS: couldn't resolve the hostname.",
-                      :field => field, :value => raw,
-                      :extra => "NameError instead of ResolvError")
-        return
-      end
-
-      if @action == "replace"
-        if is_array
-          event[field] = [address]
-        else
-          event[field] = address
-        end
-      else
-        if !is_array
-          event[field] = [event[field], address]
-        else
-          event[field] << address
-        end
-      end
-
-    end
-  end
-
-  private
-  def reverse(event)
-    @reverse.each do |field|
-      raw = event[field]
-      is_array = false
-      if raw.is_a?(Array)
-          is_array = true
-          if raw.length > 1
-            @logger.warn("DNS: skipping reverse, can't deal with multiple values", :field => field, :value => raw)
-            return
-          end
-          raw = raw.first
-      end
-
-      if ! @ip_validator.match(raw)
-        @logger.debug("DNS: not an address",
-                      :field => field, :value => event[field])
-        return
-      end
-      begin
-        # in JRuby 1.7.11 outputs as US-ASCII
-        hostname = @resolv.getname(raw).force_encoding(Encoding::UTF_8)
-      rescue Resolv::ResolvError
-        @logger.debug("DNS: couldn't resolve the address.",
-                      :field => field, :value => raw)
-        return
-      rescue Resolv::ResolvTimeout
-        @logger.debug("DNS: timeout on resolving address.",
-                      :field => field, :value => raw)
-        return
-      rescue SocketError => e
-        @logger.debug("DNS: Encountered SocketError.",
-                      :field => field, :value => raw)
-        return
-      end
-
-      if @action == "replace"
-        if is_array
-          event[field] = [hostname]
-        else
-          event[field] = hostname
-        end
-      else
-        if !is_array
-          event[field] = [event[field], hostname]
-        else
-          event[field] << hostname
-        end
-      end
-    end
-  end
-end # class LogStash::Filters::DNS
diff --git a/lib/logstash/filters/drop.rb b/lib/logstash/filters/drop.rb
deleted file mode 100644
index 9c50f8b4a68..00000000000
--- a/lib/logstash/filters/drop.rb
+++ /dev/null
@@ -1,32 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Drop filter.
-#
-# Drops everything that gets to this filter.
-#
-# This is best used in combination with conditionals, for example:
-#
-#     filter {
-#       if [loglevel] == "debug" { 
-#         drop { } 
-#       }
-#     }
-#
-# The above will only pass events to the drop filter if the loglevel field is
-# "debug". This will cause all events matching to be dropped.
-class LogStash::Filters::Drop < LogStash::Filters::Base
-  config_name "drop"
-  milestone 3
-
-  public
-  def register
-    # nothing to do.
-  end
-
-  public
-  def filter(event)
-    event.cancel
-  end # def filter
-end # class LogStash::Filters::Drop
diff --git a/lib/logstash/filters/fingerprint.rb b/lib/logstash/filters/fingerprint.rb
deleted file mode 100644
index ce39d3c69e5..00000000000
--- a/lib/logstash/filters/fingerprint.rb
+++ /dev/null
@@ -1,122 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-#  Fingerprint fields using by replacing values with a consistent hash.
-class LogStash::Filters::Fingerprint < LogStash::Filters::Base
-  config_name "fingerprint"
-  milestone 1
-
-  # Source field(s)
-  config :source, :validate => :array, :default => 'message'
-
-  # Target field.
-  # will overwrite current value of a field if it exists.
-  config :target, :validate => :string, :default => 'fingerprint'
-
-  # When used with IPV4_NETWORK method fill in the subnet prefix length
-  # Not required for MURMUR3 or UUID methods
-  # With other methods fill in the HMAC key
-  config :key, :validate => :string
-
-  # Fingerprint method
-  config :method, :validate => ['SHA1', 'SHA256', 'SHA384', 'SHA512', 'MD5', "MURMUR3", "IPV4_NETWORK", "UUID", "PUNCTUATION"], :required => true, :default => 'SHA1'
-
-  # When set to true, we concatenate the values of all fields into 1 string like the old checksum filter.
-  config :concatenate_sources, :validate => :boolean, :default => false
-
-  def register
-    # require any library and set the anonymize function
-    case @method
-      when "IPV4_NETWORK"
-        require 'ipaddr'
-        @logger.error("Key value is empty. please fill in a subnet prefix length") if @key.nil?
-        class << self; alias_method :anonymize, :anonymize_ipv4_network; end
-      when "MURMUR3"
-        require "murmurhash3"
-        class << self; alias_method :anonymize, :anonymize_murmur3; end
-      when "UUID"
-        require "securerandom"
-      when "PUNCTUATION"
-        # nothing required
-      else
-        require 'openssl'
-        @logger.error("Key value is empty. Please fill in an encryption key") if @key.nil?
-        class << self; alias_method :anonymize, :anonymize_openssl; end
-    end
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    case @method
-      when "UUID"
-        event[@target] = SecureRandom.uuid
-      when "PUNCTUATION"
-        @source.sort.each do |field|
-          next unless event.include?(field)
-          event[@target] = event[field].tr('A-Za-z0-9 \t','')
-        end
-      else
-        if @concatenate_sources
-          to_string = ''
-          @source.sort.each do |k|
-            @logger.debug("Adding key to string")
-            to_string << "|#{k}|#{event[k]}"
-          end
-          to_string << "|"
-          @logger.debug("String built", :to_checksum => to_string)
-          event[@target] = anonymize(to_string)
-        else
-          @source.each do |field|
-            next unless event.include?(field)
-            if event[field].is_a?(Array)
-              event[@target] = event[field].collect { |v| anonymize(v) }
-            else
-              event[@target] = anonymize(event[field])
-            end
-          end # @source.each
-        end # concatenate_sources
-
-    end # casse @method
-  end # def filter
-
-  private
-  def anonymize_ipv4_network(ip_string)
-    # in JRuby 1.7.11 outputs as US-ASCII
-    IPAddr.new(ip_string).mask(@key.to_i).to_s.force_encoding(Encoding::UTF_8)
-  end
-
-  def anonymize_openssl(data)
-    digest = encryption_algorithm()
-    # in JRuby 1.7.11 outputs as ASCII-8BIT
-    OpenSSL::HMAC.hexdigest(digest, @key, data).force_encoding(Encoding::UTF_8)
-  end
-
-  def anonymize_murmur3(value)
-    case value
-      when Fixnum
-        MurmurHash3::V32.int_hash(value)
-      when String
-        MurmurHash3::V32.str_hash(value)
-    end
-  end
-
-  def encryption_algorithm
-   case @method
-     when 'SHA1'
-       return OpenSSL::Digest::SHA1.new
-     when 'SHA256'
-       return OpenSSL::Digest::SHA256.new
-     when 'SHA384'
-       return OpenSSL::Digest::SHA384.new
-     when 'SHA512'
-       return OpenSSL::Digest::SHA512.new
-     when 'MD5'
-       return OpenSSL::Digest::MD5.new
-     else
-       @logger.error("Unknown algorithm")
-    end
-  end
-
-end # class LogStash::Filters::Anonymize
diff --git a/lib/logstash/filters/geoip.rb b/lib/logstash/filters/geoip.rb
deleted file mode 100644
index b70f89c9498..00000000000
--- a/lib/logstash/filters/geoip.rb
+++ /dev/null
@@ -1,147 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "tempfile"
-
-# The GeoIP filter adds information about the geographical location of IP addresses,
-# based on data from the Maxmind database.
-#
-# Starting with version 1.3.0 of Logstash, a [geoip][location] field is created if
-# the GeoIP lookup returns a latitude and longitude. The field is stored in
-# [GeoJSON](http://geojson.org/geojson-spec.html) format. Additionally,
-# the default Elasticsearch template provided with the
-# [elasticsearch output](../outputs/elasticsearch.html)
-# maps the [geoip][location] field to a
-# [geo_point](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/mapping-geo-point-type.html).
-#
-# As this field is a geo\_point _and_ it is still valid GeoJSON, you get
-# the awesomeness of Elasticsearch's geospatial query, facet and filter functions
-# and the flexibility of having GeoJSON for all other applications (like Kibana's
-# [bettermap panel](https://github.com/elasticsearch/kibana/tree/master/src/app/panels/bettermap)).
-#
-# Logstash releases ship with the GeoLiteCity database made available from
-# Maxmind with a CCA-ShareAlike 3.0 license. For more details on GeoLite, see
-# <http://www.maxmind.com/en/geolite>.
-class LogStash::Filters::GeoIP < LogStash::Filters::Base
-  config_name "geoip"
-  milestone 3
-
-  # The path to the GeoIP database file which Logstash should use. Country, City, ASN, ISP
-  # and organization databases are supported.
-  #
-  # If not specified, this will default to the GeoLiteCity database that ships
-  # with Logstash.
-  config :database, :validate => :path
-
-  # The field containing the IP address or hostname to map via geoip. If
-  # this field is an array, only the first value will be used.
-  config :source, :validate => :string, :required => true
-
-  # An array of geoip fields to be included in the event.
-  #
-  # Possible fields depend on the database type. By default, all geoip fields
-  # are included in the event.
-  #
-  # For the built-in GeoLiteCity database, the following are available:
-  # `city\_name`, `continent\_code`, `country\_code2`, `country\_code3`, `country\_name`,
-  # `dma\_code`, `ip`, `latitude`, `longitude`, `postal\_code`, `region\_name` and `timezone`.
-  config :fields, :validate => :array
-
-  # Specify the field into which Logstash should store the geoip data.
-  # This can be useful, for example, if you have `src\_ip` and `dst\_ip` fields and
-  # would like the GeoIP information of both IPs.
-  #
-  # If you save the data to a target field other than "geoip" and want to use the
-  # geo\_point related functions in Elasticsearch, you need to alter the template
-  # provided with the Elasticsearch output and configure the output to use the
-  # new template.
-  #
-  # Even if you don't use the geo\_point mapping, the [target][location] field
-  # is still valid GeoJSON.
-  config :target, :validate => :string, :default => 'geoip'
-
-  public
-  def register
-    require "geoip"
-    if @database.nil?
-      @database = LogStash::Environment.vendor_path("geoip/GeoLiteCity.dat")
-      if !File.exists?(@database)
-        raise "You must specify 'database => ...' in your geoip filter (I looked for '#{@database}'"
-      end
-    end
-    @logger.info("Using geoip database", :path => @database)
-    # For the purpose of initializing this filter, geoip is initialized here but
-    # not set as a global. The geoip module imposes a mutex, so the filter needs
-    # to re-initialize this later in the filter() thread, and save that access
-    # as a thread-local variable.
-    geoip_initialize = ::GeoIP.new(@database)
-
-    @geoip_type = case geoip_initialize.database_type
-    when GeoIP::GEOIP_CITY_EDITION_REV0, GeoIP::GEOIP_CITY_EDITION_REV1
-      :city
-    when GeoIP::GEOIP_COUNTRY_EDITION
-      :country
-    when GeoIP::GEOIP_ASNUM_EDITION
-      :asn
-    when GeoIP::GEOIP_ISP_EDITION, GeoIP::GEOIP_ORG_EDITION
-      :isp
-    else
-      raise RuntimeException.new "This GeoIP database is not currently supported"
-    end
-
-    @threadkey = "geoip-#{self.object_id}"
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    geo_data = nil
-
-    # Use thread-local access to GeoIP. The Ruby GeoIP module forces a mutex
-    # around access to the database, which can be overcome with :pread.
-    # Unfortunately, :pread requires the io-extra gem, with C extensions that
-    # aren't supported on JRuby. If / when :pread becomes available, we can stop
-    # needing thread-local access.
-    if !Thread.current.key?(@threadkey)
-      Thread.current[@threadkey] = ::GeoIP.new(@database)
-    end
-
-    begin
-      ip = event[@source]
-      ip = ip.first if ip.is_a? Array
-      geo_data = Thread.current[@threadkey].send(@geoip_type, ip)
-    rescue SocketError => e
-      @logger.error("IP Field contained invalid IP address or hostname", :field => @field, :event => event)
-    rescue Exception => e
-      @logger.error("Unknown error while looking up GeoIP data", :exception => e, :field => @field, :event => event)
-    end
-
-    return if geo_data.nil?
-
-    geo_data_hash = geo_data.to_hash
-    geo_data_hash.delete(:request)
-    event[@target] = {} if event[@target].nil?
-    geo_data_hash.each do |key, value|
-      next if value.nil? || (value.is_a?(String) && value.empty?)
-      if @fields.nil? || @fields.empty? || @fields.include?(key.to_s)
-        # convert key to string (normally a Symbol)
-        if value.is_a?(String)
-          # Some strings from GeoIP don't have the correct encoding...
-          value = case value.encoding
-            # I have found strings coming from GeoIP that are ASCII-8BIT are actually
-            # ISO-8859-1...
-            when Encoding::ASCII_8BIT; value.force_encoding(Encoding::ISO_8859_1).encode(Encoding::UTF_8)
-            when Encoding::ISO_8859_1, Encoding::US_ASCII;  value.encode(Encoding::UTF_8)
-            else; value
-          end
-        end
-        event[@target][key.to_s] = value
-      end
-    end # geo_data_hash.each
-    if event[@target].key?('latitude') && event[@target].key?('longitude')
-      # If we have latitude and longitude values, add the location field as GeoJSON array
-      event[@target]['location'] = [ event[@target]["longitude"].to_f, event[@target]["latitude"].to_f ]
-    end
-    filter_matched(event)
-  end # def filter
-end # class LogStash::Filters::GeoIP
diff --git a/lib/logstash/filters/grok.rb b/lib/logstash/filters/grok.rb
deleted file mode 100644
index e992ccd5a2d..00000000000
--- a/lib/logstash/filters/grok.rb
+++ /dev/null
@@ -1,401 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "set"
-
-# Parse arbitrary text and structure it.
-#
-# Grok is currently the best way in logstash to parse crappy unstructured log
-# data into something structured and queryable.
-#
-# This tool is perfect for syslog logs, apache and other webserver logs, mysql
-# logs, and in general, any log format that is generally written for humans
-# and not computer consumption.
-#
-# Logstash ships with about 120 patterns by default. You can find them here:
-# <https://github.com/logstash/logstash/tree/v%VERSION%/patterns>. You can add
-# your own trivially. (See the patterns_dir setting)
-#
-# If you need help building patterns to match your logs, you will find the
-# <http://grokdebug.herokuapp.com> too quite useful!
-#
-# #### Grok Basics
-#
-# Grok works by combining text patterns into something that matches your
-# logs.
-#
-# The syntax for a grok pattern is `%{SYNTAX:SEMANTIC}`
-#
-# The `SYNTAX` is the name of the pattern that will match your text. For
-# example, "3.44" will be matched by the NUMBER pattern and "55.3.244.1" will
-# be matched by the IP pattern. The syntax is how you match.
-#
-# The `SEMANTIC` is the identifier you give to the piece of text being matched.
-# For example, "3.44" could be the duration of an event, so you could call it
-# simply 'duration'. Further, a string "55.3.244.1" might identify the 'client'
-# making a request.
-#
-# For the above example, your grok filter would look something like this:
-#
-# %{NUMBER:duration} %{IP:client}
-#
-# Optionally you can add a data type conversion to your grok pattern. By default
-# all semantics are saved as strings. If you wish to convert a semantic's data type,
-# for example change a string to an integer then suffix it with the target data type.
-# For example `%{NUMBER:num:int}` which converts the 'num' semantic from a string to an
-# integer. Currently the only supported conversions are `int` and `float`.
-#
-# #### Example
-#
-# With that idea of a syntax and semantic, we can pull out useful fields from a
-# sample log like this fictional http request log:
-#
-#     55.3.244.1 GET /index.html 15824 0.043
-#
-# The pattern for this could be:
-#
-#     %{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}
-#
-# A more realistic example, let's read these logs from a file:
-#
-#     input {
-#       file {
-#         path => "/var/log/http.log"
-#       }
-#     }
-#     filter {
-#       grok {
-#         match => [ "message", "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" ]
-#       }
-#     }
-#
-# After the grok filter, the event will have a few extra fields in it:
-#
-# * client: 55.3.244.1
-# * method: GET
-# * request: /index.html
-# * bytes: 15824
-# * duration: 0.043
-#
-# #### Regular Expressions
-#
-# Grok sits on top of regular expressions, so any regular expressions are valid
-# in grok as well. The regular expression library is Oniguruma, and you can see
-# the full supported regexp syntax [on the Onigiruma
-# site](http://www.geocities.jp/kosako3/oniguruma/doc/RE.txt).
-#
-# #### Custom Patterns
-#
-# Sometimes logstash doesn't have a pattern you need. For this, you have
-# a few options.
-#
-# First, you can use the Oniguruma syntax for 'named capture' which will
-# let you match a piece of text and save it as a field:
-#
-#     (?<field_name>the pattern here)
-#
-# For example, postfix logs have a 'queue id' that is an 10 or 11-character
-# hexadecimal value. I can capture that easily like this:
-#
-#     (?<queue_id>[0-9A-F]{10,11})
-#
-# Alternately, you can create a custom patterns file. 
-#
-# * Create a directory called `patterns` with a file in it called `extra`
-#   (the file name doesn't matter, but name it meaningfully for yourself)
-# * In that file, write the pattern you need as the pattern name, a space, then
-#   the regexp for that pattern.
-#
-# For example, doing the postfix queue id example as above:
-#
-#     # contents of ./patterns/postfix:
-#     POSTFIX_QUEUEID [0-9A-F]{10,11}
-#
-# Then use the `patterns_dir` setting in this plugin to tell logstash where
-# your custom patterns directory is. Here's a full example with a sample log:
-#
-#     Jan  1 06:25:43 mailserver14 postfix/cleanup[21403]: BEF25A72965: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>
-#
-#     filter {
-#       grok {
-#         patterns_dir => "./patterns"
-#         match => [ "message", "%{SYSLOGBASE} %{POSTFIX_QUEUEID:queue_id}: %{GREEDYDATA:syslog_message}" ]
-#       }
-#     }
-#
-# The above will match and result in the following fields:
-#
-# * timestamp: Jan  1 06:25:43
-# * logsource: mailserver14
-# * program: postfix/cleanup
-# * pid: 21403
-# * queue_id: BEF25A72965
-# * syslog_message: message-id=<20130101142543.5828399CCAF@mailserver14.example.com>
-#
-# The `timestamp`, `logsource`, `program`, and `pid` fields come from the
-# SYSLOGBASE pattern which itself is defined by other patterns.
-class LogStash::Filters::Grok < LogStash::Filters::Base
-  config_name "grok"
-  milestone 3
-
-  # Specify a pattern to parse with. This will match the 'message' field.
-  #
-  # If you want to match other fields than message, use the 'match' setting.
-  # Multiple patterns is fine.
-  config :pattern, :validate => :array, :deprecated => "You should use this instead: match => { \"message\" => \"your pattern here\" }"
-
-  # A hash of matches of field => value
-  #
-  # For example:
-  #
-  #     filter {
-  #       grok {
-  #         match => [ "message", "Duration: %{NUMBER:duration}" ]
-  #       }
-  #     }
-  #
-  config :match, :validate => :hash, :default => {}
-
-  #
-  # logstash ships by default with a bunch of patterns, so you don't
-  # necessarily need to define this yourself unless you are adding additional
-  # patterns.
-  #
-  # Pattern files are plain text with format:
-  #
-  #     NAME PATTERN
-  #
-  # For example:
-  #
-  #     NUMBER \d+
-  config :patterns_dir, :validate => :array, :default => []
-
-  # Drop if matched. Note, this feature may not stay. It is preferable to combine
-  # grok + grep filters to do parsing + dropping.
-  config :drop_if_match, :validate => :boolean, :default => false
-
-  # Break on first match. The first successful match by grok will result in the
-  # filter being finished. If you want grok to try all patterns (maybe you are
-  # parsing different things), then set this to false.
-  config :break_on_match, :validate => :boolean, :default => true
-
-  # If true, only store named captures from grok.
-  config :named_captures_only, :validate => :boolean, :default => true
-
-  # If true, keep empty captures as event fields.
-  config :keep_empty_captures, :validate => :boolean, :default => false
-
-  # If true, make single-value fields simply that value, not an array
-  # containing that one value.
-  config :singles, :validate => :boolean, :default => true, :deprecated => "This behavior is the default now, you don't need to set it."
-
-  # Append values to the 'tags' field when there has been no
-  # successful match
-  config :tag_on_failure, :validate => :array, :default => ["_grokparsefailure"]
-
-  # The fields to overwrite.
-  #
-  # This allows you to overwrite a value in a field that already exists.
-  #
-  # For example, if you have a syslog line in the 'message' field, you can
-  # overwrite the 'message' field with part of the match like so:
-  #
-  #     filter {
-  #       grok {
-  #         match => [ 
-  #           "message",
-  #           "%{SYSLOGBASE} %{DATA:message}"
-  #         ]
-  #         overwrite => [ "message" ]
-  #       }
-  #     }
-  #
-  #  In this case, a line like "May 29 16:37:11 sadness logger: hello world"
-  #  will be parsed and 'hello world' will overwrite the original message.
-  config :overwrite, :validate => :array, :default => []
-
-  # Detect if we are running from a jarfile, pick the right path.
-  @@patterns_path ||= Set.new
-  @@patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
-
-  public
-  def initialize(params)
-    super(params)
-    @match["message"] ||= []
-    @match["message"] += @pattern if @pattern # the config 'pattern' value (array)
-    # a cache of capture name handler methods.
-    @handlers = {}
-  end
-
-  public
-  def register
-    require "grok-pure" # rubygem 'jls-grok'
-
-    @patternfiles = []
-
-    # Have @@patterns_path show first. Last-in pattern definitions win; this
-    # will let folks redefine built-in patterns at runtime.
-    @patterns_dir = @@patterns_path.to_a + @patterns_dir
-    @logger.info? and @logger.info("Grok patterns path", :patterns_dir => @patterns_dir)
-    @patterns_dir.each do |path|
-      if File.directory?(path)
-        path = File.join(path, "*")
-      end
-
-      Dir.glob(path).each do |file|
-        @logger.info? and @logger.info("Grok loading patterns from file", :path => file)
-        @patternfiles << file
-      end
-    end
-
-    @patterns = Hash.new { |h,k| h[k] = [] }
-
-    @logger.info? and @logger.info("Match data", :match => @match)
-
-    @match.each do |field, patterns|
-      patterns = [patterns] if patterns.is_a?(String)
-
-      if !@patterns.include?(field)
-        @patterns[field] = Grok::Pile.new
-        #@patterns[field].logger = @logger
-
-        add_patterns_from_files(@patternfiles, @patterns[field])
-      end
-      @logger.info? and @logger.info("Grok compile", :field => field, :patterns => patterns)
-      patterns.each do |pattern|
-        @logger.debug? and @logger.debug("regexp: #{@type}/#{field}", :pattern => pattern)
-        @patterns[field].compile(pattern)
-      end
-    end # @match.each
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    matched = false
-    done = false
-
-    @logger.debug? and @logger.debug("Running grok filter", :event => event);
-    @patterns.each do |field, grok|
-      if match(grok, field, event)
-        matched = true
-        break if @break_on_match
-      end
-      #break if done
-    end # @patterns.each
-
-    if matched
-      filter_matched(event)
-    else
-      # Tag this event if we can't parse it. We can use this later to
-      # reparse+reindex logs if we improve the patterns given.
-      @tag_on_failure.each do |tag|
-        event["tags"] ||= []
-        event["tags"] << tag unless event["tags"].include?(tag)
-      end
-    end
-
-    @logger.debug? and @logger.debug("Event now: ", :event => event)
-  end # def filter
-
-  private
-  def match(grok, field, event)
-    input = event[field]
-    if input.is_a?(Array)
-      success = true
-      input.each do |input|
-        grok, match = grok.match(input)
-        if match
-          match.each_capture do |capture, value|
-            handle(capture, value, event)
-          end
-        else
-          success = false
-        end
-      end
-      return success
-    #elsif input.is_a?(String)
-    else
-      # Convert anything else to string (number, hash, etc)
-      grok, match = grok.match(input.to_s)
-      return false if !match
-
-      match.each_capture do |capture, value|
-        handle(capture, value, event)
-      end
-      return true
-    end
-  rescue StandardError => e
-    @logger.warn("Grok regexp threw exception", :exception => e.message)
-  end
-
-  private
-  def handle(capture, value, event)
-    handler = @handlers[capture] ||= compile_capture_handler(capture)
-    return handler.call(value, event)
-  end
-
-  private
-  def compile_capture_handler(capture)
-    # SYNTAX:SEMANTIC:TYPE
-    syntax, semantic, coerce = capture.split(":")
-
-    # each_capture do |fullname, value|
-    #   capture_handlers[fullname].call(value, event) 
-    # end
-
-    code = []
-    code << "# for capture #{capture}"
-    code << "lambda do |value, event|"
-    #code << "  p :value => value, :event => event"
-    if semantic.nil?
-      if @named_captures_only 
-        # Abort early if we are only keeping named (semantic) captures
-        # and this capture has no semantic name.
-        code << "  return"
-      else
-        field = syntax
-      end
-    else
-      field = semantic
-    end
-    code << "  return if value.nil? || value.empty?" unless @keep_empty_captures
-    if coerce
-      case coerce
-        when "int"; code << "  value = value.to_i"
-        when "float"; code << "  value = value.to_f"
-      end
-    end
-
-    code << "  # field: #{field}"
-    if @overwrite.include?(field)
-      code << "  event[field] = value"
-    else
-      code << "  v = event[field]"
-      code << "  if v.nil?"
-      code << "    event[field] = value"
-      code << "  elsif v.is_a?(Array)"
-      code << "    event[field] << value"
-      code << "  elsif v.is_a?(String)"
-      # Promote to array since we aren't overwriting.
-      code << "    event[field] = [v, value]"
-      code << "  end"
-    end
-    code << "  return"
-    code << "end"
-
-    #puts code
-    return eval(code.join("\n"), binding, "<grok capture #{capture}>")
-  end # def compile_capture_handler
-
-  private
-  def add_patterns_from_files(paths, pile)
-    paths.each { |path| add_patterns_from_file(path, pile) }
-  end # def add_patterns_from_files
-
-  private
-  def add_patterns_from_file(path, pile)
-    pile.add_patterns_from_file(path)
-  end # def add_patterns_from_file
-end # class LogStash::Filters::Grok
diff --git a/lib/logstash/filters/grokdiscovery.rb b/lib/logstash/filters/grokdiscovery.rb
deleted file mode 100644
index 6541e2de533..00000000000
--- a/lib/logstash/filters/grokdiscovery.rb
+++ /dev/null
@@ -1,75 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# TODO(sissel): This is not supported yet. There is a bug in grok discovery
-# that causes segfaults in libgrok.
-class LogStash::Filters::Grokdiscovery < LogStash::Filters::Base
-
-  config_name "grokdiscovery"
-  milestone 1
-
-  public
-  def initialize(config = {})
-    super
-
-    @discover_fields = {}
-  end # def initialize
-
-  public
-  def register
-    gem "jls-grok", ">=0.4.3"
-    require "grok" # rubygem 'jls-grok'
-
-    # TODO(sissel): Make patterns files come from the config
-    @config.each do |type, typeconfig|
-      @logger.debug("Registering type with grok: #{type}")
-      @grok = Grok.new
-      Dir.glob("patterns/*").each do |path|
-        @grok.add_patterns_from_file(path)
-      end
-      @discover_fields[type] = typeconfig
-      @logger.debug(["Enabling discovery", { :type => type, :fields => typeconfig }])
-      @logger.warn(@discover_fields)
-    end # @config.each
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    # parse it with grok
-    message = event["message"]
-    match = false
-
-    if event.type and @discover_fields.include?(event.type)
-      discover = @discover_fields[event.type] & event.to_hash.keys
-      discover.each do |field|
-        value = event[field]
-        value = [value] if value.is_a?(String)
-
-        value.each do |v| 
-          pattern = @grok.discover(v)
-          @logger.warn("Trying #{v} => #{pattern}")
-          @grok.compile(pattern)
-          match = @grok.match(v)
-          if match
-            @logger.warn(["Match", match.captures])
-            event.to_hash.merge!(match.captures) do |key, oldval, newval|
-              @logger.warn(["Merging #{key}", oldval, newval])
-              oldval + newval # should both be arrays...
-            end
-          else
-            @logger.warn(["Discovery produced something not matchable?", { :input => v }])
-          end
-        end # value.each
-      end # discover.each
-    else
-      @logger.info("Unknown type for #{event.source} (type: #{event.type})")
-      @logger.debug(event.to_hash)
-    end
-    @logger.debug(["Event now: ", event.to_hash])
-
-    filter_matched(event) if !event.cancelled?
-  end # def filter
-end # class LogStash::Filters::Grokdiscovery
diff --git a/lib/logstash/filters/json.rb b/lib/logstash/filters/json.rb
deleted file mode 100644
index 2f6d471356b..00000000000
--- a/lib/logstash/filters/json.rb
+++ /dev/null
@@ -1,104 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# This is a JSON parsing filter. It takes an existing field which contains JSON and
-# expands it into an actual data structure within the Logstash event.
-# 
-# By default it will place the parsed JSON in the root (top level) of the Logstash event, but this
-# filter can be configured to place the JSON into any arbitrary event field, using the
-# `target` configuration.
-class LogStash::Filters::Json < LogStash::Filters::Base
-
-  config_name "json"
-  milestone 2
-
-  # The configuration for the JSON filter:
-  #
-  #     source => source_field
-  #
-  # For example, if you have JSON data in the @message field:
-  #
-  #     filter {
-  #       json {
-  #         source => "message"
-  #       }
-  #     }
-  #
-  # The above would parse the json from the @message field
-  config :source, :validate => :string, :required => true
-
-  # Define the target field for placing the parsed data. If this setting is
-  # omitted, the JSON data will be stored at the root (top level) of the event.
-  #
-  # For example, if you want the data to be put in the 'doc' field:
-  #
-  #     filter {
-  #       json {
-  #         target => "doc"
-  #       }
-  #     }
-  #
-  # JSON in the value of the `source` field will be expanded into a
-  # data structure in the `target` field.
-  #
-  # NOTE: if the `target` field already exists, it will be overwritten!
-  config :target, :validate => :string
-
-  TIMESTAMP = "@timestamp"
-
-  public
-  def register
-    # Nothing to do here
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @logger.debug("Running json filter", :event => event)
-
-    return unless event.include?(@source)
-
-    source = event[@source]
-    if @target.nil?
-      # Default is to write to the root of the event.
-      dest = event.to_hash
-    else
-      if @target == @source
-        # Overwrite source
-        dest = event[@target] = {}
-      else
-        dest = event[@target] ||= {}
-      end
-    end
-
-    begin
-      # TODO(sissel): Note, this will not successfully handle json lists
-      # like your text is '[ 1,2,3 ]' JSON.parse gives you an array (correctly)
-      # which won't merge into a hash. If someone needs this, we can fix it
-      # later.
-      dest.merge!(JSON.parse(source))
-
-      # If no target, we target the root of the event object. This can allow
-      # you to overwrite @timestamp. If so, let's parse it as a timestamp!
-      if !@target && event[TIMESTAMP].is_a?(String)
-        # This is a hack to help folks who are mucking with @timestamp during
-        # their json filter. You aren't supposed to do anything with
-        # "@timestamp" outside of the date filter, but nobody listens... ;)
-        event[TIMESTAMP] = Time.parse(event[TIMESTAMP]).utc
-      end
-
-      filter_matched(event)
-    rescue => e
-      event.tag("_jsonparsefailure")
-      @logger.warn("Trouble parsing json", :source => @source,
-                   :raw => event[@source], :exception => e)
-      return
-    end
-
-    @logger.debug("Event after json filter", :event => event)
-
-  end # def filter
-
-end # class LogStash::Filters::Json
diff --git a/lib/logstash/filters/kv.rb b/lib/logstash/filters/kv.rb
deleted file mode 100644
index 4535df04527..00000000000
--- a/lib/logstash/filters/kv.rb
+++ /dev/null
@@ -1,232 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# This filter helps automatically parse messages (or specific event fields)
-# which are of the 'foo=bar' variety.
-#
-# For example, if you have a log message which contains 'ip=1.2.3.4
-# error=REFUSED', you can parse those automatically by configuring:
-#
-#     filter {
-#       kv { }
-#     }
-#
-# The above will result in a message of "ip=1.2.3.4 error=REFUSED" having
-# the fields:
-#
-# * ip: 1.2.3.4
-# * error: REFUSED
-#
-# This is great for postfix, iptables, and other types of logs that
-# tend towards 'key=value' syntax.
-#
-# You can configure any arbitrary strings to split your data on,
-# in case your data is not structured using '=' signs and whitespace.
-# For example, this filter can also be used to parse query parameters like
-# 'foo=bar&baz=fizz' by setting the `field_split` parameter to "&".
-class LogStash::Filters::KV < LogStash::Filters::Base
-  config_name "kv"
-  milestone 2
-
-  # A string of characters to trim from the value. This is useful if your
-  # values are wrapped in brackets or are terminated with commas (like postfix
-  # logs).
-  #
-  # These characters form a regex character class and thus you must escape special regex
-  # characters like '[' or ']' using '\'.
-  #
-  # For example, to strip '<', '>', '[', ']' and ',' characters from values:
-  #
-  #     filter {
-  #       kv {
-  #         trim => "<>\[\],"
-  #       }
-  #     }
-  config :trim, :validate => :string
-
-  # A string of characters to trim from the key. This is useful if your
-  # keys are wrapped in brackets or start with space.
-  #
-  # These characters form a regex character class and thus you must escape special regex
-  # characters like '[' or ']' using '\'.
-  #
-  # For example, to strip '<' '>' '[' ']' and ',' characters from keys:
-  #
-  #     filter {
-  #       kv {
-  #         trimkey => "<>\[\],"
-  #       }
-  #     }
-  config :trimkey, :validate => :string
-
-  # A string of characters to use as delimiters for parsing out key-value pairs.
-  #
-  # These characters form a regex character class and thus you must escape special regex
-  # characters like '[' or ']' using '\'.
-  #
-  # #### Example with URL Query Strings
-  #
-  # For example, to split out the args from a url query string such as
-  # '?pin=12345~0&d=123&e=foo@bar.com&oq=bobo&ss=12345':
-  #
-  #     filter {
-  #       kv {
-  #         field_split => "&?"
-  #       }
-  #     }
-  #
-  # The above splits on both "&" and "?" characters, giving you the following
-  # fields:
-  #
-  # * pin: 12345~0
-  # * d: 123
-  # * e: foo@bar.com
-  # * oq: bobo
-  # * ss: 12345
-  config :field_split, :validate => :string, :default => ' '
-
-
-  # A string of characters to use as delimiters for identifying key-value relations.
-  #
-  # These characters form a regex character class and thus you must escape special regex
-  # characters like '[' or ']' using '\'.
-  #
-  # For example, to identify key-values such as
-  # 'key1:value1 key2:value2':
-  #
-  #     filter { kv { value_split => ":" } }
-  config :value_split, :validate => :string, :default => '='
-
-  # A string to prepend to all of the extracted keys.
-  #
-  # For example, to prepend arg_ to all keys:
-  #
-  #     filter { kv { prefix => "arg_" } }
-  config :prefix, :validate => :string, :default => ''
-
-  # The field to perform 'key=value' searching on
-  #
-  # For example, to process the `not_the_message` field:
-  #
-  #     filter { kv { source => "not_the_message" } }
-  config :source, :validate => :string, :default => "message"
-
-  # The name of the container to put all of the key-value pairs into.
-  #
-  # If this setting is omitted, fields will be written to the root of the
-  # event, as individual fields.
-  #
-  # For example, to place all keys into the event field kv:
-  #
-  #     filter { kv { target => "kv" } }
-  config :target, :validate => :string
-
-  # An array specifying the parsed keys which should be added to the event.
-  # By default all keys will be added.
-  #
-  # For example, consider a source like "Hey, from=<abc>, to=def foo=bar". 
-  # To include "from" and "to", but exclude the "foo" key, you could use this configuration:
-  #     filter {
-  #       kv {
-  #         include_keys => [ "from", "to" ]
-  #       }
-  #     }
-  config :include_keys, :validate => :array, :default => []
-
-  # An array specifying the parsed keys which should not be added to the event.
-  # By default no keys will be excluded.
-  #
-  # For example, consider a source like "Hey, from=<abc>, to=def foo=bar". 
-  # To exclude "from" and "to", but retain the "foo" key, you could use this configuration:
-  #     filter {
-  #       kv {
-  #         exclude_keys => [ "from", "to" ]
-  #       }
-  #     }
-  config :exclude_keys, :validate => :array, :default => []
-
-  # A hash specifying the default keys and their values which should be added to the event
-  # in case these keys do not exist in the source field being parsed.
-  #
-  #     filter {
-  #       kv {
-  #         default_keys => [ "from", "logstash@example.com",
-  #                          "to", "default@dev.null" ]
-  #       }
-  #     }
-  config :default_keys, :validate => :hash, :default => {}
-
-  def register
-    @trim_re = Regexp.new("[#{@trim}]") if !@trim.nil?
-    @trimkey_re = Regexp.new("[#{@trimkey}]") if !@trimkey.nil?
-    @scan_re = Regexp.new("((?:\\\\ |[^"+@field_split+@value_split+"])+)["+@value_split+"](?:\"([^\"]+)\"|'([^']+)'|((?:\\\\ |[^"+@field_split+"])+))")
-  end # def register
-
-  def filter(event)
-    return unless filter?(event)
-
-    kv = Hash.new
-
-    value = event[@source]
-
-    case value
-      when nil; # Nothing to do
-      when String; kv = parse(value, event, kv)
-      when Array; value.each { |v| kv = parse(v, event, kv) }
-      else
-        @logger.warn("kv filter has no support for this type of data",
-                     :type => value.class, :value => value)
-    end # case value
-
-    # Add default key-values for missing keys
-    kv = @default_keys.merge(kv)
-
-    # If we have any keys, create/append the hash
-    if kv.length > 0
-      if @target.nil?
-        # Default is to write to the root of the event.
-        dest = event.to_hash
-      else
-        if !event[@target].is_a?(Hash)
-          @logger.debug("Overwriting existing target field", :target => @target)
-          dest = event[@target] = {}
-        else
-          dest = event[@target]
-        end
-      end
-
-      dest.merge!(kv)
-      filter_matched(event)
-    end
-  end # def filter
-
-  private
-  def parse(text, event, kv_keys)
-    if !event =~ /[@field_split]/
-      return kv_keys
-    end
-    text.scan(@scan_re) do |key, v1, v2, v3|
-      value = v1 || v2 || v3
-      key = @trimkey.nil? ? key : key.gsub(@trimkey_re, "")
-
-      # Bail out as per the values of @include_keys and @exclude_keys
-      next if not @include_keys.empty? and not @include_keys.include?(key)
-      next if @exclude_keys.include?(key)
-
-      key = event.sprintf(@prefix) + key
-
-      value = @trim.nil? ? value : value.gsub(@trim_re, "")
-      if kv_keys.has_key?(key)
-        if kv_keys[key].is_a? Array
-          kv_keys[key].push(value)
-        else
-          kv_keys[key] = [kv_keys[key], value]
-        end
-      else
-        kv_keys[key] = value
-      end
-    end
-    return kv_keys
-  end
-end # class LogStash::Filters::KV
diff --git a/lib/logstash/filters/metrics.rb b/lib/logstash/filters/metrics.rb
deleted file mode 100644
index e22431d74ba..00000000000
--- a/lib/logstash/filters/metrics.rb
+++ /dev/null
@@ -1,239 +0,0 @@
-# encoding: utf-8
-require "securerandom"
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The metrics filter is useful for aggregating metrics.
-#
-# For example, if you have a field 'response' that is
-# a http response code, and you want to count each
-# kind of response, you can do this:
-#
-#     filter {
-#       metrics {
-#         meter => [ "http.%{response}" ]
-#         add_tag => "metric"
-#       }
-#     }
-#
-# Metrics are flushed every 5 seconds by default or according to
-# 'flush_interval'. Metrics appear as
-# new events in the event stream and go through any filters
-# that occur after as well as outputs.
-#
-# In general, you will want to add a tag to your metrics and have an output
-# explicitly look for that tag.
-#
-# The event that is flushed will include every 'meter' and 'timer'
-# metric in the following way:
-#
-# #### 'meter' values
-#
-# For a `meter => "something"` you will receive the following fields:
-#
-# * "thing.count" - the total count of events
-# * "thing.rate_1m" - the 1-minute rate (sliding)
-# * "thing.rate_5m" - the 5-minute rate (sliding)
-# * "thing.rate_15m" - the 15-minute rate (sliding)
-#
-# #### 'timer' values
-#
-# For a `timer => [ "thing", "%{duration}" ]` you will receive the following fields:
-#
-# * "thing.count" - the total count of events
-# * "thing.rate_1m" - the 1-minute rate of events (sliding)
-# * "thing.rate_5m" - the 5-minute rate of events (sliding)
-# * "thing.rate_15m" - the 15-minute rate of events (sliding)
-# * "thing.min" - the minimum value seen for this metric
-# * "thing.max" - the maximum value seen for this metric
-# * "thing.stddev" - the standard deviation for this metric
-# * "thing.mean" - the mean for this metric
-# * "thing.pXX" - the XXth percentile for this metric (see `percentiles`)
-#
-# #### Example: computing event rate
-#
-# For a simple example, let's track how many events per second are running
-# through logstash:
-#
-#     input {
-#       generator {
-#         type => "generated"
-#       }
-#     }
-#
-#     filter {
-#       if [type] == "generated" {
-#         metrics {
-#           meter => "events"
-#           add_tag => "metric"
-#         }
-#       }
-#     }
-#
-#     output {
-#       # only emit events with the 'metric' tag
-#       if "metric" in [tags] {
-#         stdout {
-#           message => "rate: %{events.rate_1m}"
-#         }
-#       }
-#     }
-#
-# Running the above:
-#
-#     % java -jar logstash.jar agent -f example.conf
-#     rate: 23721.983566819246
-#     rate: 24811.395722536377
-#     rate: 25875.892745934525
-#     rate: 26836.42375967113
-#
-# We see the output includes our 'events' 1-minute rate.
-#
-# In the real world, you would emit this to graphite or another metrics store,
-# like so:
-#
-#     output {
-#       graphite {
-#         metrics => [ "events.rate_1m", "%{events.rate_1m}" ]
-#       }
-#     }
-class LogStash::Filters::Metrics < LogStash::Filters::Base
-  config_name "metrics"
-  milestone 1
-
-  # syntax: `meter => [ "name of metric", "name of metric" ]`
-  config :meter, :validate => :array, :default => []
-
-  # syntax: `timer => [ "name of metric", "%{time_value}" ]`
-  config :timer, :validate => :hash, :default => {}
-
-  # Don't track events that have @timestamp older than some number of seconds.
-  #
-  # This is useful if you want to only include events that are near real-time
-  # in your metrics.
-  #
-  # Example, to only count events that are within 10 seconds of real-time, you
-  # would do this:
-  #
-  #     filter {
-  #       metrics {
-  #         meter => [ "hits" ]
-  #         ignore_older_than => 10
-  #       }
-  #     }
-  config :ignore_older_than, :validate => :number, :default => 0
-
-  # The flush interval, when the metrics event is created. Must be a multiple of 5s.
-  config :flush_interval, :validate => :number, :default => 5
-
-  # The clear interval, when all counter are reset.
-  #
-  # If set to -1, the default value, the metrics will never be cleared.
-  # Otherwise, should be a multiple of 5s.
-  config :clear_interval, :validate => :number, :default => -1
-
-  # The rates that should be measured, in minutes.
-  # Possible values are 1, 5, and 15.
-  config :rates, :validate => :array, :default => [1, 5, 15]
-
-  # The percentiles that should be measured
-  config :percentiles, :validate => :array, :default => [1, 5, 10, 90, 95, 99, 100]
-
-  def register
-    require "metriks"
-    require "socket"
-    require "atomic"
-    require "thread_safe"
-    @last_flush = Atomic.new(0) # how many seconds ago the metrics where flushed.
-    @last_clear = Atomic.new(0) # how many seconds ago the metrics where cleared.
-    @random_key_preffix = SecureRandom.hex
-    unless (@rates - [1, 5, 15]).empty?
-      raise LogStash::ConfigurationError, "Invalid rates configuration. possible rates are 1, 5, 15. Rates: #{rates}."
-    end
-    @metric_meters = ThreadSafe::Cache.new { |h,k| h[k] = Metriks.meter metric_key(k) }
-    @metric_timers = ThreadSafe::Cache.new { |h,k| h[k] = Metriks.timer metric_key(k) }
-  end # def register
-
-  def filter(event)
-    return unless filter?(event)
-
-    # TODO(piavlo): This should probably be moved to base filter class.
-    if @ignore_older_than > 0 && Time.now - event["@timestamp"] > @ignore_older_than
-      @logger.debug("Skipping metriks for old event", :event => event)
-      return
-    end
-
-    @meter.each do |m|
-      @metric_meters[event.sprintf(m)].mark
-    end
-
-    @timer.each do |name, value|
-      @metric_timers[event.sprintf(name)].update(event.sprintf(value).to_f)
-    end
-  end # def filter
-
-  def flush
-    # Add 5 seconds to @last_flush and @last_clear counters
-    # since this method is called every 5 seconds.
-    @last_flush.update { |v| v + 5 }
-    @last_clear.update { |v| v + 5 }
-
-    # Do nothing if there's nothing to do ;)
-    return unless should_flush?
-
-    event = LogStash::Event.new
-    event["message"] = Socket.gethostname
-    @metric_meters.each_pair do |name, metric|
-      flush_rates event, name, metric
-      metric.clear if should_clear?
-    end
-
-    @metric_timers.each_pair do |name, metric|
-      flush_rates event, name, metric
-      # These 4 values are not sliding, so they probably are not useful.
-      event["#{name}.min"] = metric.min
-      event["#{name}.max"] = metric.max
-      # timer's stddev currently returns variance, fix it.
-      event["#{name}.stddev"] = metric.stddev ** 0.5
-      event["#{name}.mean"] = metric.mean
-
-      @percentiles.each do |percentile|
-        event["#{name}.p#{percentile}"] = metric.snapshot.value(percentile / 100.0)
-      end
-      metric.clear if should_clear?
-    end
-
-    # Reset counter since metrics were flushed
-    @last_flush.value = 0
-
-    if should_clear?
-      #Reset counter since metrics were cleared
-      @last_clear.value = 0
-      @metric_meters.clear
-      @metric_timers.clear
-    end
-
-    filter_matched(event)
-    return [event]
-  end
-
-  private
-  def flush_rates(event, name, metric)
-      event["#{name}.count"] = metric.count
-      event["#{name}.rate_1m"] = metric.one_minute_rate if @rates.include? 1
-      event["#{name}.rate_5m"] = metric.five_minute_rate if @rates.include? 5
-      event["#{name}.rate_15m"] = metric.fifteen_minute_rate if @rates.include? 15
-  end
-
-  def metric_key(key)
-    "#{@random_key_preffix}_#{key}"
-  end
-
-  def should_flush?
-    @last_flush.value >= @flush_interval && (!@metric_meters.empty? || !@metric_timers.empty?)
-  end
-
-  def should_clear?
-    @clear_interval > 0 && @last_clear.value >= @clear_interval
-  end
-end # class LogStash::Filters::Metrics
diff --git a/lib/logstash/filters/multiline.rb b/lib/logstash/filters/multiline.rb
deleted file mode 100644
index bcee5757aeb..00000000000
--- a/lib/logstash/filters/multiline.rb
+++ /dev/null
@@ -1,239 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "set"
-#
-# This filter will collapse multiline messages from a single source into one Logstash event.
-#
-# The original goal of this filter was to allow joining of multi-line messages
-# from files into a single event. For example - joining java exception and
-# stacktrace messages into a single event.
-#
-# The config looks like this:
-#
-#     filter {
-#       multiline {
-#         type => "type"
-#         pattern => "pattern, a regexp"
-#         negate => boolean
-#         what => "previous" or "next"
-#       }
-#     }
-#
-# The `pattern` should be a regexp which matches what you believe to be an indicator
-# that the field is part of an event consisting of multiple lines of log data.
-#
-# The `what` must be "previous" or "next" and indicates the relation
-# to the multi-line event.
-#
-# The `negate` can be "true" or "false" (defaults to false). If "true", a
-# message not matching the pattern will constitute a match of the multiline
-# filter and the `what` will be applied. (vice-versa is also true)
-#
-# For example, Java stack traces are multiline and usually have the message
-# starting at the far-left, with each subsequent line indented. Do this:
-#
-#     filter {
-#       multiline {
-#         type => "somefiletype"
-#         pattern => "^\s"
-#         what => "previous"
-#       }
-#     }
-#
-# This says that any line starting with whitespace belongs to the previous line.
-#
-# Another example is C line continuations (backslash). Here's how to do that:
-#
-#     filter {
-#       multiline {
-#         type => "somefiletype "
-#         pattern => "\\$"
-#         what => "next"
-#       }
-#     }
-#
-# This says that any line ending with a backslash should be combined with the
-# following line.
-#
-class LogStash::Filters::Multiline < LogStash::Filters::Base
-
-  config_name "multiline"
-  milestone 3
-
-  # The regular expression to match.
-  config :pattern, :validate => :string, :required => true
-
-  # If the pattern matched, does event belong to the next or previous event?
-  config :what, :validate => ["previous", "next"], :required => true
-
-  # Negate the regexp pattern ('if not matched')
-  config :negate, :validate => :boolean, :default => false
-
-  # The stream identity is how the multiline filter determines which stream an
-  # event belongs to. This is generally used for differentiating, say, events
-  # coming from multiple files in the same file input, or multiple connections
-  # coming from a tcp input.
-  #
-  # The default value here is usually what you want, but there are some cases
-  # where you want to change it. One such example is if you are using a tcp
-  # input with only one client connecting at any time. If that client
-  # reconnects (due to error or client restart), then logstash will identify
-  # the new connection as a new stream and break any multiline goodness that
-  # may have occurred between the old and new connection. To solve this use
-  # case, you can use "%{@source_host}.%{@type}" instead.
-  config :stream_identity , :validate => :string, :default => "%{host}.%{path}.%{type}"
-
-  # Logstash ships by default with a bunch of patterns, so you don't
-  # necessarily need to define this yourself unless you are adding additional
-  # patterns.
-  #
-  # Pattern files are plain text with format:
-  #
-  #     NAME PATTERN
-  #
-  # For example:
-  #
-  #     NUMBER \d+
-  config :patterns_dir, :validate => :array, :default => []
-
-  # for debugging & testing purposes, do not use in production. allows periodic flushing of pending events
-  config :enable_flush, :validate => :boolean, :default => false
-
-  # Detect if we are running from a jarfile, pick the right path.
-  @@patterns_path = Set.new
-  @@patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
-
-  public
-  def initialize(config = {})
-    super
-
-    @threadsafe = false
-
-    # This filter needs to keep state.
-    @types = Hash.new { |h,k| h[k] = [] }
-    @pending = Hash.new
-  end # def initialize
-
-  public
-  def register
-    require "grok-pure" # rubygem 'jls-grok'
-
-    @grok = Grok.new
-
-    @patterns_dir = @@patterns_path.to_a + @patterns_dir
-    @patterns_dir.each do |path|
-      if File.directory?(path)
-        path = File.join(path, "*")
-      end
-
-      Dir.glob(path).each do |file|
-        @logger.info("Grok loading patterns from file", :path => file)
-        @grok.add_patterns_from_file(file)
-      end
-    end
-
-    @grok.compile(@pattern)
-
-    @logger.debug("Registered multiline plugin", :type => @type, :config => @config)
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    if event["message"].is_a?(Array)
-      match = @grok.match(event["message"].first)
-    else
-      match = @grok.match(event["message"])
-    end
-    key = event.sprintf(@stream_identity)
-    pending = @pending[key]
-
-    @logger.debug("Multiline", :pattern => @pattern, :message => event["message"],
-                  :match => match, :negate => @negate)
-
-    # Add negate option
-    match = (match and !@negate) || (!match and @negate)
-
-    case @what
-    when "previous"
-      if match
-        event.tag "multiline"
-        # previous previous line is part of this event.
-        # append it to the event and cancel it
-        if pending
-          pending.append(event)
-        else
-          @pending[key] = event
-        end
-        event.cancel
-      else
-        # this line is not part of the previous event
-        # if we have a pending event, it's done, send it.
-        # put the current event into pending
-        if pending
-          tmp = event.to_hash
-          event.overwrite(pending)
-          @pending[key] = LogStash::Event.new(tmp)
-        else
-          @pending[key] = event
-          event.cancel
-        end # if/else pending
-      end # if/else match
-    when "next"
-      if match
-        event.tag "multiline"
-        # this line is part of a multiline event, the next
-        # line will be part, too, put it into pending.
-        if pending
-          pending.append(event)
-        else
-          @pending[key] = event
-        end
-        event.cancel
-      else
-        # if we have something in pending, join it with this message
-        # and send it. otherwise, this is a new message and not part of
-        # multiline, send it.
-        if pending
-          pending.append(event)
-          event.overwrite(pending)
-          @pending.delete(key)
-        end
-      end # if/else match
-    else
-      # TODO(sissel): Make this part of the 'register' method.
-      @logger.warn("Unknown multiline 'what' value.", :what => @what)
-    end # case @what
-
-    if !event.cancelled?
-      collapse_event!(event)
-      filter_matched(event) if match
-    end
-  end # def filter
-
-  # Flush any pending messages. This is generally used for unit testing only.
-  #
-  # Note: flush is disabled now; it is preferable to use the multiline codec.
-  public
-  def flush
-    return [] unless @enable_flush
-
-    events = []
-    @pending.each do |key, value|
-      value.uncancel
-      events << collapse_event!(value)
-    end
-    @pending.clear
-    return events
-  end # def flush
-
-  private
-
-  def collapse_event!(event)
-    event["message"] = event["message"].join("\n") if event["message"].is_a?(Array)
-    event["@timestamp"] = event["@timestamp"].first if event["@timestamp"].is_a?(Array)
-    event
-  end
-end # class LogStash::Filters::Multiline
diff --git a/lib/logstash/filters/mutate.rb b/lib/logstash/filters/mutate.rb
deleted file mode 100644
index 89075508c5c..00000000000
--- a/lib/logstash/filters/mutate.rb
+++ /dev/null
@@ -1,403 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The mutate filter allows you to perform general mutations on fields. You
-# can rename, remove, replace, and modify fields in your events.
-#
-# TODO(sissel): Support regexp replacements like String#gsub ?
-class LogStash::Filters::Mutate < LogStash::Filters::Base
-  config_name "mutate"
-  milestone 3
-
-  # Rename one or more fields.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         # Renames the 'HOSTORIP' field to 'client_ip'
-  #         rename => [ "HOSTORIP", "client_ip" ]
-  #       }
-  #     }
-  config :rename, :validate => :hash
-
-  # Remove one or more fields.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         remove => [ "client" ]  # Removes the 'client' field
-  #       }
-  #     }
-  #
-  # This option is deprecated, instead use remove_field option available in all
-  # filters.
-  config :remove, :validate => :array, :deprecated => true
-
-  # Replace a field with a new value. The new value can include %{foo} strings
-  # to help you build a new value from other parts of the event.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         replace => [ "message", "%{source_host}: My new message" ]
-  #       }
-  #     }
-  config :replace, :validate => :hash
-
-  # Update an existing field with a new value. If the field does not exist,
-  # then no action will be taken.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         update => [ "sample", "My new message" ]
-  #       }
-  #     }
-  config :update, :validate => :hash
-
-  # Convert a field's value to a different type, like turning a string to an
-  # integer. If the field value is an array, all members will be converted.
-  # If the field is a hash, no action will be taken.
-  #
-  # Valid conversion targets are: integer, float, string.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         convert => [ "fieldname", "integer" ]
-  #       }
-  #     }
-  config :convert, :validate => :hash
-
-  # Convert a string field by applying a regular expression and a replacement.
-  # If the field is not a string, no action will be taken.
-  #
-  # This configuration takes an array consisting of 3 elements per
-  # field/substitution.
-  #
-  # Be aware of escaping any backslash in the config file.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         gsub => [
-  #           # replace all forward slashes with underscore
-  #           "fieldname", "/", "_",
-  #
-  #           # replace backslashes, question marks, hashes, and minuses with
-  #           # dot
-  #           "fieldname2", "[\\?#-]", "."
-  #         ]
-  #       }
-  #     }
-  #
-  config :gsub, :validate => :array
-
-  # Convert a string to its uppercase equivalent.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         uppercase => [ "fieldname" ]
-  #       }
-  #     }
-  config :uppercase, :validate => :array
-
-  # Convert a string to its lowercase equivalent.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #         lowercase => [ "fieldname" ]
-  #       }
-  #     }
-  config :lowercase, :validate => :array
-
-  # Split a field to an array using a separator character. Only works on string
-  # fields.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #          split => ["fieldname", ","]
-  #       }
-  #     }
-  config :split, :validate => :hash
-
-  # Join an array with a separator character. Does nothing on non-array fields.
-  #
-  # Example:
-  #
-  #    filter {
-  #      mutate {
-  #        join => ["fieldname", ","]
-  #      }
-  #    }
-  config :join, :validate => :hash
-
-  # Strip whitespace from field. NOTE: this only works on leading and trailing whitespace.
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #          strip => ["field1", "field2"]
-  #       }
-  #     }
-  config :strip, :validate => :array
-
-  # Merge two fields of arrays or hashes.
-  # String fields will be automatically be converted into an array, so:
-  #   array + string will work
-  #   string + string will result in an 2 entry array in dest_field
-  #   array and hash will not work
-  #
-  # Example:
-  #
-  #     filter {
-  #       mutate {
-  #          merge => ["dest_field", "added_field"]
-  #       }
-  #     }
-  config :merge, :validate => :hash
-
-  public
-  def register
-    valid_conversions = %w(string integer float)
-    # TODO(sissel): Validate conversion requests if provided.
-    @convert.nil? or @convert.each do |field, type|
-      if !valid_conversions.include?(type)
-        @logger.error("Invalid conversion type",
-                      "type" => type, "expected one of" => valid_types)
-        # TODO(sissel): It's 2011, man, let's actually make like.. a proper
-        # 'configuration broken' exception
-        raise "Bad configuration, aborting."
-      end
-    end # @convert.each
-
-    @gsub_parsed = []
-    @gsub.nil? or @gsub.each_slice(3) do |field, needle, replacement|
-      if [field, needle, replacement].any? {|n| n.nil?}
-        @logger.error("Invalid gsub configuration. gsub has to define 3 elements per config entry", :field => field, :needle => needle, :replacement => replacement)
-        raise "Bad configuration, aborting."
-      end
-      @gsub_parsed << {
-        :field        => field,
-        :needle       => Regexp.new(needle),
-        :replacement  => replacement
-      }
-    end
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    rename(event) if @rename
-    update(event) if @update
-    replace(event) if @replace
-    convert(event) if @convert
-    gsub(event) if @gsub
-    uppercase(event) if @uppercase
-    lowercase(event) if @lowercase
-    strip(event) if @strip
-    remove(event) if @remove
-    split(event) if @split
-    join(event) if @join
-    merge(event) if @merge
-
-    filter_matched(event)
-  end # def filter
-
-  private
-  def remove(event)
-    # TODO(sissel): use event.sprintf on the field names?
-    @remove.each do |field|
-      event.remove(field)
-    end
-  end # def remove
-
-  private
-  def rename(event)
-    # TODO(sissel): use event.sprintf on the field names?
-    @rename.each do |old, new|
-      next unless event.include?(old)
-      event[new] = event.remove(old)
-    end
-  end # def rename
-
-  private
-  def update(event)
-    @update.each do |field, newvalue|
-      next unless event.include?(field)
-      event[field] = event.sprintf(newvalue)
-    end
-  end # def update
-
-  private
-  def replace(event)
-    @replace.each do |field, newvalue|
-      event[field] = event.sprintf(newvalue)
-    end
-  end # def replace
-
-  def convert(event)
-    @convert.each do |field, type|
-      next unless event.include?(field)
-      original = event[field]
-
-      # calls convert_{string,integer,float} depending on type requested.
-      converter = method("convert_" + type)
-      if original.nil?
-        next
-      elsif original.is_a?(Hash)
-        @logger.debug("I don't know how to type convert a hash, skipping",
-                      :field => field, :value => original)
-        next
-      elsif original.is_a?(Array)
-        value = original.map { |v| converter.call(v) }
-      else
-        value = converter.call(original)
-      end
-      event[field] = value
-    end
-  end # def convert
-
-  def convert_string(value)
-    # since this is a filter and all inputs should be already UTF-8
-    # we wont check valid_encoding? but just force UTF-8 for
-    # the Fixnum#to_s case which always result in US-ASCII
-    # see https://twitter.com/jordansissel/status/444613207143903232
-    return value.to_s.force_encoding(Encoding::UTF_8)
-  end # def convert_string
-
-  def convert_integer(value)
-    return value.to_i
-  end # def convert_integer
-
-  def convert_float(value)
-    return value.to_f
-  end # def convert_float
-
-  private
-  def gsub(event)
-    @gsub_parsed.each do |config|
-      field = config[:field]
-      needle = config[:needle]
-      replacement = config[:replacement]
-
-      if event[field].is_a?(Array)
-        event[field] = event[field].map do |v|
-          if not v.is_a?(String)
-            @logger.warn("gsub mutation is only applicable for Strings, " +
-                          "skipping", :field => field, :value => v)
-            v
-          else
-            v.gsub(needle, replacement)
-          end
-        end
-      else
-        if not event[field].is_a?(String)
-          @logger.debug("gsub mutation is only applicable for Strings, " +
-                        "skipping", :field => field, :value => event[field])
-          next
-        end
-        event[field] = event[field].gsub(needle, replacement)
-      end
-    end # @gsub_parsed.each
-  end # def gsub
-
-  private
-  def uppercase(event)
-    @uppercase.each do |field|
-      if event[field].is_a?(Array)
-        event[field].each { |v| v.upcase! }
-      elsif event[field].is_a?(String)
-        event[field].upcase!
-      else
-        @logger.debug("Can't uppercase something that isn't a string",
-                      :field => field, :value => event[field])
-      end
-    end
-  end # def uppercase
-
-  private
-  def lowercase(event)
-    @lowercase.each do |field|
-      if event[field].is_a?(Array)
-        event[field].each { |v| v.downcase! }
-      elsif event[field].is_a?(String)
-        event[field].downcase!
-      else
-        @logger.debug("Can't lowercase something that isn't a string",
-                      :field => field, :value => event[field])
-      end
-    end
-  end # def lowercase
-
-  private
-  def split(event)
-    @split.each do |field, separator|
-      if event[field].is_a?(String)
-        event[field] = event[field].split(separator)
-      else
-        @logger.debug("Can't split something that isn't a string",
-                      :field => field, :value => event[field])
-      end
-    end
-  end
-
-  private
-  def join(event)
-    @join.each do |field, separator|
-      if event[field].is_a?(Array)
-        event[field] = event[field].join(separator)
-      end
-    end
-  end
-
-  private
-  def strip(event)
-    @strip.each do |field|
-      if event[field].is_a?(Array)
-        event[field] = event[field].map{|s| s.strip }
-      elsif event[field].is_a?(String)
-        event[field] = event[field].strip
-      end
-    end
-  end
-
-  private
-  def merge(event)
-    @merge.each do |dest_field, added_fields|
-      #When multiple calls, added_field is an array
-      added_fields = [ added_fields ] if ! added_fields.is_a?(Array)
-      added_fields.each do |added_field|
-        if event[dest_field].is_a?(Hash) ^ event[added_field].is_a?(Hash)
-          @logger.error("Not possible to merge an array and a hash: ",
-                        :dest_field => dest_field,
-                        :added_field => added_field )
-          next
-        end
-        if event[dest_field].is_a?(Hash) #No need to test the other
-          event[dest_field].update(event[added_field])
-        else
-          event[dest_field] = [event[dest_field]] if ! event[dest_field].is_a?(Array)
-          event[added_field] = [event[added_field]] if ! event[added_field].is_a?(Array)
-         event[dest_field].concat(event[added_field])
-        end
-      end
-    end
-  end
-
-end # class LogStash::Filters::Mutate
diff --git a/lib/logstash/filters/noop.rb b/lib/logstash/filters/noop.rb
deleted file mode 100644
index a26137b36ae..00000000000
--- a/lib/logstash/filters/noop.rb
+++ /dev/null
@@ -1,21 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# No-op filter. This is used generally for internal/dev testing.
-class LogStash::Filters::NOOP < LogStash::Filters::Base
-  config_name "noop"
-  milestone 2
-
-  public
-  def register
-    # Nothing
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    # Nothing to do
-    filter_matched(event)
-  end # def filter
-end # class LogStash::Filters::NOOP
diff --git a/lib/logstash/filters/ruby.rb b/lib/logstash/filters/ruby.rb
deleted file mode 100644
index 01f8f60b50c..00000000000
--- a/lib/logstash/filters/ruby.rb
+++ /dev/null
@@ -1,42 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Execute ruby code.
-#
-# For example, to cancel 90% of events, you can do this:
-#
-#     filter {
-#       ruby {
-#         # Cancel 90% of events
-#         code => "event.cancel if rand <= 0.90"
-#       } 
-#     } 
-#
-class LogStash::Filters::Ruby < LogStash::Filters::Base
-  config_name "ruby"
-  milestone 1
-
-  # Any code to execute at logstash startup-time
-  config :init, :validate => :string
-
-  # The code to execute for every event.
-  # You will have an 'event' variable available that is the event itself.
-  config :code, :validate => :string, :required => true
-
-  public
-  def register
-    # TODO(sissel): Compile the ruby code
-    eval(@init, binding, "(ruby filter init)") if @init
-    eval("@codeblock = lambda { |event| #{@code} }", binding, "(ruby filter code)")
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    @codeblock.call(event)
-
-    filter_matched(event)
-  end # def filter
-end # class LogStash::Filters::Ruby
diff --git a/lib/logstash/filters/sleep.rb b/lib/logstash/filters/sleep.rb
deleted file mode 100644
index 3b83b644285..00000000000
--- a/lib/logstash/filters/sleep.rb
+++ /dev/null
@@ -1,111 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Sleep a given amount of time. This will cause logstash
-# to stall for the given amount of time. This is useful
-# for rate limiting, etc.
-#
-class LogStash::Filters::Sleep < LogStash::Filters::Base
-  config_name "sleep"
-  milestone 1
-
-  # The length of time to sleep, in seconds, for every event.
-  #
-  # This can be a number (eg, 0.5), or a string (eg, "%{foo}") 
-  # The second form (string with a field value) is useful if
-  # you have an attribute of your event that you want to use
-  # to indicate the amount of time to sleep.
-  #
-  # Example:
-  #
-  #     filter {
-  #       sleep {
-  #         # Sleep 1 second for every event.
-  #         time => "1"
-  #       }
-  #     }
-  config :time, :validate => :string
-
-  # Sleep on every N'th. This option is ignored in replay mode.
-  #
-  # Example:
-  #
-  #     filter {
-  #       sleep {
-  #         time => "1"   # Sleep 1 second 
-  #         every => 10   # on every 10th event
-  #       }
-  #     }
-  config :every, :validate => :string, :default => 1
-
-  # Enable replay mode.
-  #
-  # Replay mode tries to sleep based on timestamps in each event.
-  #
-  # The amount of time to sleep is computed by subtracting the
-  # previous event's timestamp from the current event's timestamp.
-  # This helps you replay events in the same timeline as original.
-  #
-  # If you specify a `time` setting as well, this filter will
-  # use the `time` value as a speed modifier. For example,
-  # a `time` value of 2 will replay at double speed, while a
-  # value of 0.25 will replay at 1/4th speed.
-  #
-  # For example:
-  #
-  #     filter {
-  #       sleep {
-  #         time => 2
-  #         replay => true
-  #       }
-  #     }
-  #
-  # The above will sleep in such a way that it will perform
-  # replay 2-times faster than the original time speed.
-  config :replay, :validate => :boolean, :default => false
-
-  public
-  def register
-    if @replay && @time.nil?
-      # Default time multiplier is 1 when replay is set.
-      @time = 1
-    end
-    if @time.nil?
-      raise ArgumentError, "Missing required parameter 'time' for input/eventlog"
-    end
-    @count = 0
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    @count += 1
-
-    case @time
-      when Fixnum, Float; time = @time
-      when nil; # nothing
-      else; time = event.sprintf(@time).to_f
-    end
-
-    if @replay
-      clock = event["@timestamp"].to_f
-      if @last_clock
-        delay = clock - @last_clock
-        time = delay/time
-        if time > 0
-          @logger.debug? && @logger.debug("Sleeping", :delay => time)
-          sleep(time)
-        end
-      end
-      @last_clock = clock
-    else
-      if @count >= @every
-        @count = 0
-        @logger.debug? && @logger.debug("Sleeping", :delay => time)
-        sleep(time)
-      end
-    end
-    filter_matched(event)
-  end # def filter
-end # class LogStash::Filters::Sleep
diff --git a/lib/logstash/filters/split.rb b/lib/logstash/filters/split.rb
deleted file mode 100644
index 3524f0d6fa6..00000000000
--- a/lib/logstash/filters/split.rb
+++ /dev/null
@@ -1,64 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The split filter is for splitting multiline messages into separate events.
-#
-# An example use case of this filter is for taking output from the 'exec' input
-# which emits one event for the whole output of a command and splitting that
-# output by newline - making each line an event.
-#
-# The end result of each split is a complete copy of the event 
-# with only the current split section of the given field changed.
-class LogStash::Filters::Split < LogStash::Filters::Base
-
-  config_name "split"
-  milestone 2
-
-  # The string to split on. This is usually a line terminator, but can be any
-  # string.
-  config :terminator, :validate => :string, :default => "\n"
-
-  # The field which value is split by the terminator
-  config :field, :validate => :string, :default => "message"
-
-  public
-  def register
-    # Nothing to do
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    events = []
-
-    original_value = event[@field]
-
-    # If for some reason the field is an array of values, take the first only.
-    original_value = original_value.first if original_value.is_a?(Array)
-
-    # Using -1 for 'limit' on String#split makes ruby not drop trailing empty
-    # splits.
-    splits = original_value.split(@terminator, -1)
-
-    # Skip filtering if splitting this event resulted in only one thing found.
-    return if splits.length == 1
-    #or splits[1].empty?
-
-    splits.each do |value|
-      next if value.empty?
-
-      event_split = event.clone
-      @logger.debug("Split event", :value => value, :field => @field)
-      event_split[@field] = value
-      filter_matched(event_split)
-
-      # Push this new event onto the stack at the LogStash::FilterWorker
-      yield event_split
-    end
-
-    # Cancel this event, we'll use the newly generated ones above.
-    event.cancel
-  end # def filter
-end # class LogStash::Filters::Split
diff --git a/lib/logstash/filters/syslog_pri.rb b/lib/logstash/filters/syslog_pri.rb
deleted file mode 100644
index 6b92c719e83..00000000000
--- a/lib/logstash/filters/syslog_pri.rb
+++ /dev/null
@@ -1,107 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# Filter plugin for logstash to parse the PRI field from the front
-# of a Syslog (RFC3164) message.  If no priority is set, it will
-# default to 13 (per RFC).
-#
-# This filter is based on the original syslog.rb code shipped
-# with logstash.
-class LogStash::Filters::Syslog_pri < LogStash::Filters::Base
-  config_name "syslog_pri"
-
-  # set the status to experimental/beta/stable
-  milestone 1
-
-  # Add human-readable names after parsing severity and facility from PRI
-  config :use_labels, :validate => :boolean, :default => true
-
-  # Name of field which passes in the extracted PRI part of the syslog message
-  config :syslog_pri_field_name, :validate => :string, :default => "syslog_pri"
-
-  # Labels for facility levels. This comes from RFC3164.
-  config :facility_labels, :validate => :array, :default => [
-    "kernel",
-    "user-level",
-    "mail",
-    "daemon",
-    "security/authorization",
-    "syslogd",
-    "line printer",
-    "network news",
-    "uucp",
-    "clock",
-    "security/authorization",
-    "ftp",
-    "ntp",
-    "log audit",
-    "log alert",
-    "clock",
-    "local0",
-    "local1",
-    "local2",
-    "local3",
-    "local4",
-    "local5",
-    "local6",
-    "local7",
-  ]
-
-  # Labels for severity levels. This comes from RFC3164.
-  config :severity_labels, :validate => :array, :default => [
-    "emergency",
-    "alert",
-    "critical",
-    "error",
-    "warning",
-    "notice",
-    "informational",
-    "debug",
-  ]
-
-  public
-  def register
-    # Nothing
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    parse_pri(event)
-    filter_matched(event)
-  end # def filter
-
-  private
-  def parse_pri(event)
-    # Per RFC3164, priority = (facility * 8) + severity
-    # = (facility << 3) & (severity)
-    if event[@syslog_pri_field_name]
-      if event[@syslog_pri_field_name].is_a?(Array)
-        priority = event[@syslog_pri_field_name].first.to_i
-      else
-        priority = event[@syslog_pri_field_name].to_i
-      end
-    else
-      priority = 13  # default
-    end
-    severity = priority & 7 # 7 is 111 (3 bits)
-    facility = priority >> 3
-    event["syslog_severity_code"] = severity
-    event["syslog_facility_code"] = facility
-
-    # Add human-readable names after parsing severity and facility from PRI
-    if @use_labels
-      facility_number = event["syslog_facility_code"]
-      severity_number = event["syslog_severity_code"]
-
-      if @facility_labels[facility_number]
-        event["syslog_facility"] = @facility_labels[facility_number]
-      end
-
-      if @severity_labels[severity_number]
-        event["syslog_severity"] = @severity_labels[severity_number]
-      end
-    end
-  end # def parse_pri
-end # class LogStash::Filters::SyslogPRI
diff --git a/lib/logstash/filters/throttle.rb b/lib/logstash/filters/throttle.rb
deleted file mode 100644
index 56a80c535a4..00000000000
--- a/lib/logstash/filters/throttle.rb
+++ /dev/null
@@ -1,261 +0,0 @@
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# The throttle filter is for throttling the number of events received. The filter
-# is configured with a lower bound, the before_count, and upper bound, the after_count,
-# and a period of time. All events passing through the filter will be counted based on 
-# a key. As long as the count is less than the before_count or greater than the 
-# after_count, the event will be "throttled" which means the filter will be considered 
-# successful and any tags or fields will be added.
-#
-# For example, if you wanted to throttle events so you only receive an event after 2 
-# occurrences and you get no more than 3 in 10 minutes, you would use the 
-# configuration:
-#     period => 600
-#     before_count => 3
-#     after_count => 5
-#
-# Which would result in:
-#     event 1 - throttled (successful filter, period start)
-#     event 2 - throttled (successful filter)
-#     event 3 - not throttled
-#     event 4 - not throttled
-#     event 5 - not throttled
-#     event 6 - throttled (successful filter)
-#     event 7 - throttled (successful filter)
-#     event x - throttled (successful filter)
-#     period end
-#     event 1 - throttled (successful filter, period start)
-#     event 2 - throttled (successful filter)
-#     event 3 - not throttled
-#     event 4 - not throttled
-#     event 5 - not throttled
-#     event 6 - throttled (successful filter)
-#     ...
-# 
-# Another example is if you wanted to throttle events so you only receive 1 event per 
-# hour, you would use the configuration:
-#     period => 3600
-#     before_count => -1
-#     after_count => 1
-#
-# Which would result in:
-#     event 1 - not throttled (period start)
-#     event 2 - throttled (successful filter)
-#     event 3 - throttled (successful filter)
-#     event 4 - throttled (successful filter)
-#     event x - throttled (successful filter)
-#     period end
-#     event 1 - not throttled (period start)
-#     event 2 - throttled (successful filter)
-#     event 3 - throttled (successful filter)
-#     event 4 - throttled (successful filter)
-#     ...
-# 
-# A common use case would be to use the throttle filter to throttle events before 3 and 
-# after 5 while using multiple fields for the key and then use the drop filter to remove 
-# throttled events. This configuration might appear as:
-# 
-#     filter {
-#       throttle {
-#         before_count => 3
-#         after_count => 5
-#         period => 3600
-#         key => "%{host}%{message}"
-#         add_tag => "throttled"
-#       }
-#       if "throttled" in [tags] {
-#         drop { }
-#       }
-#     }
-#
-# Another case would be to store all events, but only email non-throttled 
-# events so the op's inbox isn't flooded with emails in the event of a system error. 
-# This configuration might appear as:
-#
-#     filter {
-#       throttle {
-#         before_count => 3
-#         after_count => 5
-#         period => 3600
-#         key => "%{message}"
-#         add_tag => "throttled"
-#       }
-#     }
-#     output {
-#       if "throttled" not in [tags] {
-#         email {
-#    	    from => "logstash@mycompany.com"
-#    	    subject => "Production System Alert"
-#    	    to => "ops@mycompany.com"
-#    	    via => "sendmail"
-#    	    body => "Alert on %{host} from path %{path}:\n\n%{message}"
-#    	    options => { "location" => "/usr/sbin/sendmail" }
-#         }
-#       }
-#       elasticsearch_http {
-#         host => "localhost"
-#         port => "19200"
-#       }
-#     }
-#
-# The event counts are cleared after the configured period elapses since the 
-# first instance of the event. That is, all the counts don't reset at the same 
-# time but rather the throttle period is per unique key value.
-#
-# Mike Pilone (@mikepilone)
-# 
-class LogStash::Filters::Throttle < LogStash::Filters::Base
-
-  # The name to use in configuration files.
-  config_name "throttle"
-
-  # New plugins should start life at milestone 1.
-  milestone 1
-
-  # The key used to identify events. Events with the same key will be throttled
-  # as a group.  Field substitutions are allowed, so you can combine multiple
-  # fields.
-  config :key, :validate => :string, :required => true
-  
-  # Events less than this count will be throttled. Setting this value to -1, the 
-  # default, will cause no messages to be throttled based on the lower bound.
-  config :before_count, :validate => :number, :default => -1, :required => false
-  
-  # Events greater than this count will be throttled. Setting this value to -1, the 
-  # default, will cause no messages to be throttled based on the upper bound.
-  config :after_count, :validate => :number, :default => -1, :required => false
-  
-  # The period in seconds after the first occurrence of an event until the count is 
-  # reset for the event. This period is tracked per unique key value.  Field
-  # substitutions are allowed in this value.  They will be evaluated when the _first_
-  # event for a given key is seen.  This allows you to specify that certain kinds
-  # of events throttle for a specific period.
-  config :period, :validate => :string, :default => "3600", :required => false
-  
-  # The maximum number of counters to store before the oldest counter is purged. Setting 
-  # this value to -1 will prevent an upper bound no constraint on the number of counters  
-  # and they will only be purged after expiration. This configuration value should only 
-  # be used as a memory control mechanism and can cause early counter expiration if the 
-  # value is reached. It is recommended to leave the default value and ensure that your 
-  # key is selected such that it limits the number of counters required (i.e. don't 
-  # use UUID as the key!)
-  config :max_counters, :validate => :number, :default => 100000, :required => false
-
-  # Performs initialization of the filter.
-  public
-  def register
-    @threadsafe = false
-  
-    @event_counters = Hash.new
-    @next_expiration = nil
-  end # def register
-
-  # Filters the event. The filter is successful if the event should be throttled.
-  public
-  def filter(event)
-      	  
-    # Return nothing unless there's an actual filter event
-    return unless filter?(event)
-    	  
-    now = Time.now
-    key = event.sprintf(@key)
-    
-    # Purge counters if too large to prevent OOM.
-    if @max_counters != -1 && @event_counters.size > @max_counters then
-      purgeOldestEventCounter()
-    end
-    
-    # Expire existing counter if needed
-    if @next_expiration.nil? || now >= @next_expiration then
-    	expireEventCounters(now)
-    end
-    
-    @logger.debug? and @logger.debug(
-      	  "filters/#{self.class.name}: next expiration", 
-      	  { "next_expiration" => @next_expiration })
-    
-    # Create new counter for this event if this is the first occurrence
-    counter = nil
-    if !@event_counters.include?(key) then
-      period = event.sprintf(@period).to_i
-      period = 3600 if period == 0
-      expiration = now + period
-      @event_counters[key] = { :count => 0, :expiration => expiration }
-      
-      @logger.debug? and @logger.debug("filters/#{self.class.name}: new event", 
-      	  { :key => key, :expiration => expiration })
-    end
-    
-    # Fetch the counter
-    counter = @event_counters[key]
-    
-    # Count this event
-    counter[:count] = counter[:count] + 1;
-    
-    @logger.debug? and @logger.debug("filters/#{self.class.name}: current count", 
-      	  { :key => key, :count => counter[:count] })
-    
-    # Throttle if count is < before count or > after count
-    if ((@before_count != -1 && counter[:count] < @before_count) || 
-       (@after_count != -1 && counter[:count] > @after_count)) then
-      @logger.debug? and @logger.debug(
-      	  "filters/#{self.class.name}: throttling event", { :key => key })
-      	
-      filter_matched(event)
-    end
-        
-  end # def filter
-  
-  # Expires any counts where the period has elapsed. Sets the next expiration time 
-  # for when this method should be called again.
-  private
-  def expireEventCounters(now) 
-    
-    @next_expiration = nil
-    
-    @event_counters.delete_if do |key, counter|
-      expiration = counter[:expiration]
-      expired = expiration <= now
-    
-      if expired then
-      	@logger.debug? and @logger.debug(
-      	  "filters/#{self.class.name}: deleting expired counter", 
-      	  { :key => key })
-      	  
-      elsif @next_expiration.nil? || (expiration < @next_expiration)
-      	@next_expiration = expiration
-      end
-      
-      expired
-    end
-  
-  end # def expireEventCounters
-  
-  # Purges the oldest event counter. This operation is for memory control only 
-  # and can cause early period expiration and thrashing if invoked.
-  private
-  def purgeOldestEventCounter()
-    
-    # Return unless we have something to purge
-    return unless @event_counters.size > 0
-    
-    oldestCounter = nil
-    oldestKey = nil
-    
-    @event_counters.each do |key, counter|
-      if oldestCounter.nil? || counter[:expiration] < oldestCounter[:expiration] then
-        oldestKey = key;
-        oldestCounter = counter;
-      end
-    end
-    
-    @logger.warn? and @logger.warn(
-      "filters/#{self.class.name}: Purging oldest counter because max_counters " +
-      "exceeded. Use a better key to prevent too many unique event counters.", 
-      { :key => oldestKey, :expiration => oldestCounter[:expiration] })
-      	  
-    @event_counters.delete(oldestKey)
-    
-  end
-end # class LogStash::Filters::Throttle
\ No newline at end of file
diff --git a/lib/logstash/filters/urldecode.rb b/lib/logstash/filters/urldecode.rb
deleted file mode 100644
index b6d50881ae8..00000000000
--- a/lib/logstash/filters/urldecode.rb
+++ /dev/null
@@ -1,57 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "uri"
-
-# The urldecode filter is for decoding fields that are urlencoded.
-class LogStash::Filters::Urldecode < LogStash::Filters::Base
-  config_name "urldecode"
-  milestone 2
-
-  # The field which value is urldecoded
-  config :field, :validate => :string, :default => "message"
-
-  # Urldecode all fields
-  config :all_fields, :validate => :boolean, :default => false
-
-  public
-  def register
-    # Nothing to do
-  end #def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    # If all_fields is true then try to decode them all
-    if @all_fields
-      event.to_hash.each do |name, value|
-        event[name] = urldecode(value)
-      end
-    # Else decode the specified field
-    else
-      event[@field] = urldecode(event[@field])
-    end
-    filter_matched(event)
-  end # def filter
-
-  # Attempt to handle string, array, and hash values for fields.
-  # For all other datatypes, just return, URI.unescape doesn't support them.
-  private
-  def urldecode(value)
-    case value
-    when String
-      return URI.unescape(value)
-    when Array
-      ret_values = []
-      value.each { |v| ret_values << urldecode(v) }
-      return ret_values
-    when Hash
-      ret_values = {}
-      value.each { |k,v| ret_values[k] = urldecode(v) }
-      return ret_values
-    else
-      return value
-    end
-  end
-end # class LogStash::Filters::Urldecode
diff --git a/lib/logstash/filters/useragent.rb b/lib/logstash/filters/useragent.rb
deleted file mode 100644
index 3fbe0e3e25d..00000000000
--- a/lib/logstash/filters/useragent.rb
+++ /dev/null
@@ -1,107 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "tempfile"
-
-# Parse user agent strings into structured data based on BrowserScope data
-#
-# UserAgent filter, adds information about user agent like family, operating
-# system, version, and device
-#
-# Logstash releases ship with the regexes.yaml database made available from
-# ua-parser with an Apache 2.0 license. For more details on ua-parser, see
-# <https://github.com/tobie/ua-parser/>.
-class LogStash::Filters::UserAgent < LogStash::Filters::Base
-  config_name "useragent"
-  milestone 3
-
-  # The field containing the user agent string. If this field is an
-  # array, only the first value will be used.
-  config :source, :validate => :string, :required => true
-
-  # The name of the field to assign user agent data into.
-  #
-  # If not specified user agent data will be stored in the root of the event.
-  config :target, :validate => :string
-
-  # regexes.yaml file to use
-  #
-  # If not specified, this will default to the regexes.yaml that ships
-  # with logstash.
-  #
-  # You can find the latest version of this here:
-  # <https://github.com/tobie/ua-parser/blob/master/regexes.yaml>
-  config :regexes, :validate => :string
-
-  # A string to prepend to all of the extracted keys
-  config :prefix, :validate => :string, :default => ''
-
-  public
-  def register
-    require 'user_agent_parser'
-    if @regexes.nil?
-      begin
-        @parser = UserAgentParser::Parser.new()
-      rescue Exception => e
-        begin
-          @parser = UserAgentParser::Parser.new(:patterns_path => "vendor/ua-parser/regexes.yaml")
-        rescue => ex
-          raise "Failed to cache, due to: #{ex}\n"
-        end
-      end
-    else
-      @logger.info("Using user agent regexes", :regexes => @regexes)
-      @parser = UserAgentParser::Parser.new(:patterns_path => @regexes)
-    end
-  end #def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    ua_data = nil
-
-    useragent = event[@source]
-    useragent = useragent.first if useragent.is_a? Array
-
-    begin
-      ua_data = @parser.parse(useragent)
-    rescue Exception => e
-      @logger.error("Uknown error while parsing user agent data", :exception => e, :field => @source, :event => event)
-    end
-
-    if !ua_data.nil?
-      if @target.nil?
-        # default write to the root of the event
-        target = event
-      else
-        target = event[@target] ||= {}
-      end
-
-      # UserAgentParser outputs as US-ASCII.
-
-      target[@prefix + "name"] = ua_data.name.force_encoding(Encoding::UTF_8)
-
-      #OSX, Andriod and maybe iOS parse correctly, ua-agent parsing for Windows does not provide this level of detail
-      unless ua_data.os.nil?
-        target[@prefix + "os"] = ua_data.os.to_s.force_encoding(Encoding::UTF_8)
-        target[@prefix + "os_name"] = ua_data.os.name.to_s.force_encoding(Encoding::UTF_8)
-        target[@prefix + "os_major"] = ua_data.os.version.major.to_s.force_encoding(Encoding::UTF_8) unless ua_data.os.version.nil?
-        target[@prefix + "os_minor"] = ua_data.os.version.minor.to_s.force_encoding(Encoding::UTF_8) unless ua_data.os.version.nil?
-      end
-
-      target[@prefix + "device"] = ua_data.device.to_s.force_encoding(Encoding::UTF_8) if not ua_data.device.nil?
-
-      if not ua_data.version.nil?
-        ua_version = ua_data.version
-        target[@prefix + "major"] = ua_version.major.force_encoding(Encoding::UTF_8) if ua_version.major
-        target[@prefix + "minor"] = ua_version.minor.force_encoding(Encoding::UTF_8) if ua_version.minor
-        target[@prefix + "patch"] = ua_version.patch.force_encoding(Encoding::UTF_8) if ua_version.patch
-        target[@prefix + "build"] = ua_version.patch_minor.force_encoding(Encoding::UTF_8) if ua_version.patch_minor
-      end
-
-      filter_matched(event)
-    end
-
-  end # def filter
-end # class LogStash::Filters::UserAgent
-
diff --git a/lib/logstash/filters/uuid.rb b/lib/logstash/filters/uuid.rb
deleted file mode 100644
index 0dbcc54653f..00000000000
--- a/lib/logstash/filters/uuid.rb
+++ /dev/null
@@ -1,58 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-require "securerandom"
-
-# The uuid filter allows you to add a UUID field to messages.
-# This is useful to be able to control the _id messages are indexed into Elasticsearch
-# with, so that you can insert duplicate messages (i.e. the same message multiple times
-# without creating duplicates) - for log pipeline reliability
-#
-class LogStash::Filters::Uuid < LogStash::Filters::Base
-  config_name "uuid"
-  milestone 2
-
-  # Add a UUID to a field.
-  #
-  # Example:
-  #
-  #     filter {
-  #       uuid {
-  #         target => "@uuid"
-  #       }
-  #     }
-  config :target, :validate => :string, :required => true
-
-  # If the value in the field currently (if any) should be overridden
-  # by the generated UUID. Defaults to false (i.e. if the field is
-  # present, with ANY value, it won't be overridden)
-  #
-  # Example:
-  #
-  #    filter {
-  #       uuid {
-  #         target    => "@uuid"
-  #         overwrite => true
-  #       }
-  #    }
-  config :overwrite, :validate => :boolean, :default => false
-
-  public
-  def register
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-
-    if overwrite
-      event[target] = SecureRandom.uuid
-    else
-      event[target] ||= SecureRandom.uuid
-    end
-
-    filter_matched(event)
-  end # def filter
-
-end # class LogStash::Filters::Uuid
-
diff --git a/lib/logstash/filters/xml.rb b/lib/logstash/filters/xml.rb
deleted file mode 100644
index 80c7bfea46c..00000000000
--- a/lib/logstash/filters/xml.rb
+++ /dev/null
@@ -1,139 +0,0 @@
-# encoding: utf-8
-require "logstash/filters/base"
-require "logstash/namespace"
-
-# XML filter. Takes a field that contains XML and expands it into
-# an actual datastructure.
-class LogStash::Filters::Xml < LogStash::Filters::Base
-
-  config_name "xml"
-  milestone 1
-
-  # Config for xml to hash is:
-  #
-  #     source => source_field
-  #
-  # For example, if you have the whole xml document in your @message field:
-  #
-  #     filter {
-  #       xml {
-  #         source => "message"
-  #       }
-  #     }
-  #
-  # The above would parse the xml from the @message field
-  config :source, :validate => :string
-
-  # Define target for placing the data
-  #
-  # for example if you want the data to be put in the 'doc' field:
-  #
-  #     filter {
-  #       xml {
-  #         target => "doc"
-  #       }
-  #     }
-  #
-  # XML in the value of the source field will be expanded into a
-  # datastructure in the "target" field.
-  # Note: if the "target" field already exists, it will be overridden
-  # Required
-  config :target, :validate => :string
-
-  # xpath will additionally select string values (.to_s on whatever is selected)
-  # from parsed XML (using each source field defined using the method above)
-  # and place those values in the destination fields. Configuration:
-  #
-  # xpath => [ "xpath-syntax", "destination-field" ]
-  #
-  # Values returned by XPath parsring from xpath-synatx will be put in the
-  # destination field. Multiple values returned will be pushed onto the
-  # destination field as an array. As such, multiple matches across
-  # multiple source fields will produce duplicate entries in the field
-  #
-  # More on xpath: http://www.w3schools.com/xpath/
-  #
-  # The xpath functions are particularly powerful:
-  # http://www.w3schools.com/xpath/xpath_functions.asp
-  #
-  config :xpath, :validate => :hash, :default => {}
-
-  # By default the filter will store the whole parsed xml in the destination
-  # field as described above. Setting this to false will prevent that.
-  config :store_xml, :validate => :boolean, :default => true
-
-  public
-  def register
-    require "nokogiri"
-    require "xmlsimple"
-
-  end # def register
-
-  public
-  def filter(event)
-    return unless filter?(event)
-    matched = false
-
-    @logger.debug("Running xml filter", :event => event)
-
-    return unless event.include?(@source)
-
-    value = event[@source]
-
-    if value.is_a?(Array) && value.length > 1
-      @logger.warn("XML filter only works on fields of length 1",
-                   :source => @source, :value => value)
-      return
-    end
-
-    # Do nothing with an empty string.
-    return if value.strip.length == 0
-
-    if @xpath
-      begin
-        doc = Nokogiri::XML(value)
-      rescue => e
-        event.tag("_xmlparsefailure")
-        @logger.warn("Trouble parsing xml", :source => @source, :value => value,
-                     :exception => e, :backtrace => e.backtrace)
-        return
-      end
-
-      @xpath.each do |xpath_src, xpath_dest|
-        nodeset = doc.xpath(xpath_src)
-
-        # If asking xpath for a String, like "name(/*)", we get back a
-        # String instead of a NodeSet.  We normalize that here.
-        normalized_nodeset = nodeset.kind_of?(Nokogiri::XML::NodeSet) ? nodeset : [nodeset]
-
-        normalized_nodeset.each do |value|
-          # some XPath functions return empty arrays as string
-          if value.is_a?(Array)
-            return if value.length == 0
-          end
-
-          unless value.nil?
-            matched = true
-            event[xpath_dest] ||= []
-            event[xpath_dest] << value.to_s
-          end
-        end # XPath.each
-      end # @xpath.each
-    end # if @xpath
-
-    if @store_xml
-      begin
-        event[@target] = XmlSimple.xml_in(value)
-        matched = true
-      rescue => e
-        event.tag("_xmlparsefailure")
-        @logger.warn("Trouble parsing xml with XmlSimple", :source => @source,
-                     :value => value, :exception => e, :backtrace => e.backtrace)
-        return
-      end
-    end # if @store_xml
-
-    filter_matched(event) if matched
-    @logger.debug("Event after xml filter", :event => event)
-  end # def filter
-end # class LogStash::Filters::Xml
diff --git a/lib/logstash/filterworker.rb b/lib/logstash/filterworker.rb
deleted file mode 100644
index 59a886c3c83..00000000000
--- a/lib/logstash/filterworker.rb
+++ /dev/null
@@ -1,122 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/logging"
-require "logstash/plugin"
-require "logstash/config/mixin"
-require "stud/interval"
-
-# TODO(sissel): Should this really be a 'plugin' ?
-class LogStash::FilterWorker < LogStash::Plugin
-  include Stud
-  attr_accessor :logger
-  attr_accessor :filters
-  attr_reader	:after_filter
-
-  Exceptions = [Exception]
-  Exceptions << java.lang.Exception if RUBY_ENGINE == "jruby"
-
-  def initialize(filters, input_queue, output_queue)
-    @filters = filters
-    @input_queue = input_queue
-    @output_queue = output_queue
-    @shutdown_requested = false
-  end # def initialize
-
-  #This block is called after each filter is done on an event. 
-  #The filtered event and filter class name is passed to the block.
-  #This could be used to add metrics in the future?
-  def after_filter(&block)
-    @after_filter = block
-  end
-
-  def run
-    # TODO(sissel): Run a flusher thread for each plugin requesting flushes
-    # > It seems reasonable that you could want a multiline filter to flush
-    #   after 5 seconds, but want a metrics filter to flush every 10 or 60.
-
-    # Set up the periodic flusher thread.
-    @flusher = Thread.new { interval(5) { flusher } }
-
-    while !@shutdown_requested && event = @input_queue.pop
-      if event == LogStash::SHUTDOWN
-        finished
-        @input_queue << LogStash::SHUTDOWN # for the next filter thread
-        return
-      end
-
-      filter(event)
-    end # while @input_queue.pop
-    finished
-  end # def run
-
-  def flusher
-    events = []
-    @filters.each do |filter|
-
-      # Filter any events generated so far in this flush.
-      events.each do |event|
-        # TODO(sissel): watchdog on flush filtration?
-        unless event.cancelled?
-          filter.filter(event)
-          @after_filter.call(event,filter) unless @after_filter.nil?
-        end
-      end
-
-      # TODO(sissel): watchdog on flushes?
-      if filter.respond_to?(:flush)
-        flushed = filter.flush 
-        events += flushed if !flushed.nil? && flushed.any?
-      end
-    end
-
-    events.each do |event|
-      @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
-      @output_queue.push(event) unless event.cancelled?
-    end
-  end # def flusher
-
-  def teardown
-    @shutdown_requested = true
-  end
-
-  def filter(original_event)
-    # Make an 'events' array that filters can push onto if they
-    # need to generate additional events based on the current event.
-    # The 'split' filter does this, for example.
-    events = [original_event]
-
-    events.each do |event|
-      @filters.each do |filter|
-        # Filter can emit multiple events, like the 'split' event, so
-        # give the input queue to dump generated events into.
-
-        # TODO(sissel): This may require some refactoring later, I am not sure
-        # this is the best approach. The goal is to allow filters to modify
-        # the current event, but if necessary, create new events based on
-        # this event.
-        begin
-          update_watchdog(:event => event, :filter => filter)
-          filter.execute(event) do |newevent|
-            events << newevent
-          end
-        rescue *Exceptions => e
-          @logger.warn("Exception during filter", :event => event,
-                       :exception => $!, :backtrace => e.backtrace,
-                       :filter => filter)
-        ensure
-          clear_watchdog
-        end
-        if event.cancelled?
-          @logger.debug? and @logger.debug("Event cancelled", :event => event,
-                                           :filter => filter.class)
-          break
-        end
-        @after_filter.call(event,filter) unless @after_filter.nil?
-      end # @filters.each
-
-      @logger.debug? and @logger.debug("Event finished filtering", :event => event,
-                                       :thread => Thread.current[:name])
-      @output_queue.push(event) unless event.cancelled?
-    end # events.each
-  end # def filter
-end # class LogStash::FilterWorker
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 68afca36f30..2f4f254681e 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -11,7 +11,7 @@ class LogStash::Inputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
   config_name "input"
 
-  # Add a 'type' field to all events handled by this input.
+  # Add a `type` field to all events handled by this input.
   #
   # Types are used mainly for filter activation.
   #
@@ -33,20 +33,20 @@ class LogStash::Inputs::Base < LogStash::Plugin
   # The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.
   config :codec, :validate => :codec, :default => "plain"
 
-  # The character encoding used in this input. Examples include "UTF-8"
-  # and "cp1252"
+  # The character encoding used in this input. Examples include `UTF-8`
+  # and `cp1252`
   #
-  # This setting is useful if your log files are in Latin-1 (aka cp1252)
-  # or in another character set other than UTF-8.
+  # This setting is useful if your log files are in `Latin-1` (aka `cp1252`)
+  # or in another character set other than `UTF-8`.
   #
-  # This only affects "plain" format logs since json is UTF-8 already.
+  # This only affects `plain` format logs since json is `UTF-8` already.
   config :charset, :validate => ::Encoding.name_list, :deprecated => true
 
-  # If format is "json", an event sprintf string to build what
-  # the display @message should be given (defaults to the raw JSON).
-  # sprintf format strings look like %{fieldname}
+  # If format is `json`, an event `sprintf` string to build what
+  # the display `@message` should be given (defaults to the raw JSON).
+  # `sprintf` format strings look like `%{fieldname}`
   #
-  # If format is "json_event", ALL fields except for @type
+  # If format is `json_event`, ALL fields except for `@type`
   # are expected to be present. Not receiving all fields
   # will cause unexpected results.
   config :message_format, :validate => :string, :deprecated => true
@@ -118,4 +118,20 @@ def decorate(event)
       event[field] = value
     end
   end
+
+  protected
+  def fix_streaming_codecs
+    require "logstash/codecs/plain"
+    require "logstash/codecs/line"
+    require "logstash/codecs/json"
+    require "logstash/codecs/json_lines"
+    case @codec
+      when LogStash::Codecs::Plain
+        @logger.info("Automatically switching from #{@codec.class.config_name} to line codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::Line.new("charset" => @codec.charset)
+      when LogStash::Codecs::JSON
+        @logger.info("Automatically switching from #{@codec.class.config_name} to json_lines codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::JSONLines.new("charset" => @codec.charset)
+    end
+  end
 end # class LogStash::Inputs::Base
diff --git a/lib/logstash/inputs/collectd.rb b/lib/logstash/inputs/collectd.rb
deleted file mode 100644
index f5f6afb5542..00000000000
--- a/lib/logstash/inputs/collectd.rb
+++ /dev/null
@@ -1,458 +0,0 @@
-# encoding utf-8
-require "date"
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-require "tempfile"
-require "time"
-
-# Read events from the connectd binary protocol over the network via udp.
-# See https://collectd.org/wiki/index.php/Binary_protocol
-#
-# Configuration in your Logstash configuration file can be as simple as:
-#     input {
-#       collectd {}
-#     }
-#
-# A sample collectd.conf to send to Logstash might be:
-#
-#     Hostname    "host.example.com"
-#     LoadPlugin interface
-#     LoadPlugin load
-#     LoadPlugin memory
-#     LoadPlugin network
-#     <Plugin interface>
-#         Interface "eth0"
-#         IgnoreSelected false
-#     </Plugin>
-#     <Plugin network>
-#         <Server "10.0.0.1" "25826">
-#         </Server>
-#     </Plugin>
-#
-# Be sure to replace "10.0.0.1" with the IP of your Logstash instance.
-#
-
-#
-class LogStash::Inputs::Collectd < LogStash::Inputs::Base
-  config_name "collectd"
-  milestone 1
-
-  AUTHFILEREGEX = /([^:]+): (.+)/
-  TYPEMAP = {
-      0   => "host",
-      1   => "@timestamp",
-      2   => "plugin",
-      3   => "plugin_instance",
-      4   => "collectd_type",
-      5   => "type_instance",
-      6   => "values",
-      7   => "interval",
-      8   => "@timestamp",
-      9   => "interval",
-      256 => "message",
-      257 => "severity",
-      512 => "signature",
-      528 => "encryption"
-  }
-
-  SECURITY_NONE = "None"
-  SECURITY_SIGN = "Sign"
-  SECURITY_ENCR = "Encrypt"
-
-  # File path(s) to collectd types.db to use.
-  # The last matching pattern wins if you have identical pattern names in multiple files.
-  # If no types.db is provided the included types.db will be used (currently 5.4.0).
-  config :typesdb, :validate => :array
-
-  # The address to listen on.  Defaults to all available addresses.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to listen on.  Defaults to the collectd expected port of 25826.
-  config :port, :validate => :number, :default => 25826
-
-  # Prune interval records.  Defaults to true.
-  config :prune_intervals, :validate => :boolean, :default => true
-
-  # Buffer size. 1452 is the collectd default for v5+
-  config :buffer_size, :validate => :number, :default => 1452
-
-  # Security Level. Default is "None". This setting mirrors the setting from the
-  # collectd [Network plugin](https://collectd.org/wiki/index.php/Plugin:Network)
-  config :security_level, :validate => [SECURITY_NONE, SECURITY_SIGN, SECURITY_ENCR],
-    :default => "None"
-
-  # Path to the authentication file. This file should have the same format as
-  # the [AuthFile](http://collectd.org/documentation/manpages/collectd.conf.5.shtml#authfile_filename)
-  # in collectd. You only need to set this option if the security_level is set to
-  # "Sign" or "Encrypt"
-  config :authfile, :validate => :string
-
-  # What to do when a value in the event is NaN (Not a Number)
-  # - change_value (default): Change the NaN to the value of the nan_value option and add nan_tag as a tag
-  # - warn: Change the NaN to the value of the nan_value option, print a warning to the log and add nan_tag as a tag
-  # - drop: Drop the event containing the NaN (this only drops the single event, not the whole packet)
-  config :nan_handeling, :validate => ['change_value','warn','drop'],
-    :default => 'change_value'
-
-  # Only relevant when nan_handeling is set to 'change_value'
-  # Change NaN to this configured value
-  config :nan_value, :validate => :number, :default => 0
-
-  # The tag to add to the event if a NaN value was found
-  # Set this to an empty string ('') if you don't want to tag
-  config :nan_tag, :validate => :string, :default => '_collectdNaN'
-
-  public
-  def initialize(params)
-    super
-    BasicSocket.do_not_reverse_lookup = true
-    @timestamp = Time.now().utc
-    @collectd = {}
-    @types = {}
-  end # def initialize
-
-  public
-  def register
-    @udp = nil
-    if @typesdb.nil?
-      @typesdb = LogStash::Environment.vendor_path("collectd/types.db")
-      if !File.exists?(@typesdb)
-        raise "You must specify 'typesdb => ...' in your collectd input (I looked for '#{@typesdb}')"
-      end
-      @logger.info("Using internal types.db", :typesdb => @typesdb.to_s)
-    end
-
-    if ([SECURITY_SIGN, SECURITY_ENCR].include?(@security_level))
-      if @authfile.nil?
-        raise "Security level is set to #{@security_level}, but no authfile was configured"
-      else
-        # Load OpenSSL and instantiate Digest and Crypto functions
-        require 'openssl'
-        @sha256 = OpenSSL::Digest::Digest.new('sha256')
-        @sha1 = OpenSSL::Digest::Digest.new('sha1')
-        @cipher = OpenSSL::Cipher.new('AES-256-OFB')
-        @auth = {}
-        parse_authfile
-      end
-    end
-  end # def register
-
-  public
-  def run(output_queue)
-    begin
-      # get types
-      get_types(@typesdb)
-      # collectd server
-      collectd_listener(output_queue)
-    rescue LogStash::ShutdownSignal
-      # do nothing, shutdown was requested.
-    rescue => e
-      @logger.warn("Collectd listener died", :exception => e, :backtrace => e.backtrace)
-      sleep(5)
-      retry
-    end # begin
-  end # def run
-
-  public
-  def get_types(paths)
-    # Get the typesdb
-    paths = Array(paths) # Make sure a single path is still forced into an array type
-    paths.each do |path|
-      @logger.info("Getting Collectd typesdb info", :typesdb => path.to_s)
-      File.open(path, 'r').each_line do |line|
-        typename, *line = line.strip.split
-        next if typename.nil? || if typename[0,1] != '#' # Don't process commented or blank lines
-          v = line.collect { |l| l.strip.split(":")[0] }
-          @types[typename] = v
-        end
-      end
-    end
-  @logger.debug("Collectd Types", :types => @types.to_s)
-  end # def get_types
-
-  public
-  def get_values(id, body)
-    retval = ''
-    case id
-      when 0,2,3,4,5,256 #=> String types
-        retval = body.pack("C*")
-        retval = retval[0..-2]
-      when 1 # Time
-        # Time here, in bit-shifted format.  Parse bytes into UTC.
-        byte1, byte2 = body.pack("C*").unpack("NN")
-        retval = Time.at(( ((byte1 << 32) + byte2))).utc
-      when 7,257 #=> Numeric types
-        retval = body.slice!(0..7).pack("C*").unpack("E")[0]
-      when 8 # Time, Hi-Res
-        # Time here, in bit-shifted format.  Parse bytes into UTC.
-        byte1, byte2 = body.pack("C*").unpack("NN")
-        retval = Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
-      when 9 # Interval, Hi-Res
-        byte1, byte2 = body.pack("C*").unpack("NN")
-        retval = (((byte1 << 32) + byte2) * (2**-30)).to_i
-      when 6 # Values
-        val_bytes = body.slice!(0..1)
-        val_count = val_bytes.pack("C*").unpack("n")
-        if body.length % 9 == 0 # Should be 9 fields
-          count = 0
-          retval = []
-          types = body.slice!(0..((body.length/9)-1))
-          while body.length > 0
-            # TYPE VALUES:
-            # 0: COUNTER
-            # 1: GAUGE
-            # 2: DERIVE
-            # 3: ABSOLUTE
-            case types[count]
-              when 1;
-                v = body.slice!(0..7).pack("C*").unpack("E")[0]
-                if v.nan?
-                  case @nan_handeling
-                  when 'drop'; return false
-                  else
-                    v = @nan_value
-                    add_tag(@nan_tag)
-                    @nan_handeling == 'warn' && @logger.warn("NaN in (unfinished event) #{@collectd}")
-                  end
-                end
-              when 0, 3; v = body.slice!(0..7).pack("C*").unpack("Q>")[0]
-              when 2;    v = body.slice!(0..7).pack("C*").unpack("q>")[0]
-              else;      v = 0
-            end
-            retval << v
-            count += 1
-          end
-        else
-          @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
-        end
-      when 512 # signature
-        if body.length < 32
-          @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
-        elsif body.length < 33
-          @logger.warning("Received signature without username")
-        else
-          retval = []
-          # Byte 32 till the end contains the username as chars (=unsigned ints)
-          retval << body[32..-1].pack('C*')
-          # Byte 0 till 31 contain the signature
-          retval << body[0..31].pack('C*')
-        end
-      when 528 # encryption
-        retval = []
-        user_length = (body.slice!(0) << 8) + body.slice!(0)
-        retval << body.slice!(0..user_length-1).pack('C*') # Username
-        retval << body.slice!(0..15).pack('C*')            # IV
-        retval << body.pack('C*')                          # Encrypted content
-    end
-    return retval
-  end # def get_values
-
-  private
-  def parse_authfile
-    # We keep the authfile parsed in memory so we don't have to open the file
-    # for every event.
-    @logger.debug("Parsing authfile #{@authfile}")
-    if !File.exist?(@authfile)
-      raise "The file #{@authfile} was not found"
-    end
-    @auth.clear
-    @authmtime = File.stat(@authfile).mtime
-    File.readlines(@authfile).each do |line|
-      #line.chomp!
-      k,v = line.scan(AUTHFILEREGEX).flatten
-      if k and v
-        @logger.debug("Added authfile entry '#{k}' with key '#{v}'")
-        @auth[k] = v
-      else
-        @logger.info("Ignoring malformed authfile line '#{line.chomp}'")
-      end
-    end
-  end # def parse_authfile
-
-  private
-  def get_key(user)
-    return if @authmtime.nil? or @authfile.nil?
-    # Validate that our auth data is still up-to-date
-    parse_authfile if @authmtime < File.stat(@authfile).mtime
-    key = @auth[user]
-    @logger.warn("User #{user} is not found in the authfile #{@authfile}") if key.nil?
-    return key
-  end # def get_key
-
-  private
-  def verify_signature(user, signature, payload)
-    # The user doesn't care about the security
-    return true if @security_level == SECURITY_NONE
-
-    # We probably got and array of ints, pack it!
-    payload = payload.pack('C*') if payload.is_a?(Array)
-
-    key = get_key(user)
-    return false if key.nil?
-
-    return true if OpenSSL::HMAC.digest(@sha256, key, user+payload) == signature
-    return false
-  end # def verify_signature
-
-  private
-  def decrypt_packet(user, iv, content)
-    # Content has to have at least a SHA1 hash (20 bytes), a header (4 bytes) and
-    # one byte of data
-    return [] if content.length < 26
-    content = content.pack('C*') if content.is_a?(Array)
-    key = get_key(user)
-    return [] if key.nil?
-
-    # Set the correct state of the cipher instance
-    @cipher.decrypt
-    @cipher.padding = 0
-    @cipher.iv = iv
-    @cipher.key = @sha256.digest(key);
-    # Decrypt the content
-    plaintext = @cipher.update(content) + @cipher.final
-    # Reset the state, as adding a new key to an already instantiated state
-    # results in an exception
-    @cipher.reset
-
-    # The plaintext contains a SHA1 hash as checksum in the first 160 bits
-    # (20 octets) of the rest of the data
-    hash = plaintext.slice!(0..19)
-
-    if @sha1.digest(plaintext) != hash
-      @logger.warn("Unable to decrypt packet, checksum mismatch")
-      return []
-    end
-    return plaintext.unpack('C*')
-  end # def decrypt_packet
-
-  private
-  def generate_event(output_queue)
-    # Prune these *specific* keys if they exist and are empty.
-    # This is better than looping over all keys every time.
-    @collectd.delete('type_instance') if @collectd['type_instance'] == ""
-    @collectd.delete('plugin_instance') if @collectd['plugin_instance'] == ""
-    # As crazy as it sounds, this is where we actually send our events to the queue!
-    event = LogStash::Event.new
-    @collectd.each {|k, v| event[k] = @collectd[k]}
-    decorate(event)
-    output_queue << event
-  end # def generate_event
-
-  private
-  def clean_up()
-    @collectd.each_key do |k|
-      @collectd.delete(k) if !['host','collectd_type', 'plugin', 'plugin_instance', '@timestamp', 'type_instance'].include?(k)
-    end
-  end # def clean_up
-
-  private
-  def add_tag(new_tag)
-    return if new_tag.empty?
-    @collectd['tags'] ||= []
-    @collectd['tags'] << new_tag
-  end
-
-  private
-  def collectd_listener(output_queue)
-    @logger.info("Starting Collectd listener", :address => "#{@host}:#{@port}")
-
-    if @udp && ! @udp.closed?
-      @udp.close
-    end
-
-    @udp = UDPSocket.new(Socket::AF_INET)
-    @udp.bind(@host, @port)
-
-    loop do
-      payload, client = @udp.recvfrom(@buffer_size)
-      payload = payload.bytes.to_a
-
-      # Clear the last event
-      @collectd.clear
-      was_encrypted = false
-
-      while payload.length > 0 do
-        typenum = (payload.slice!(0) << 8) + payload.slice!(0)
-        # Get the length of the data in this part, but take into account that
-        # the header is 4 bytes
-        length  = ((payload.slice!(0) << 8) + payload.slice!(0)) - 4
-
-        if length > payload.length
-          @logger.info("Header indicated #{length} bytes will follow, but packet has only #{payload.length} bytes left")
-          break
-        end
-        body = payload.slice!(0..length-1)
-
-        field = TYPEMAP[typenum]
-        if field.nil?
-          @logger.warn("Unknown typenumber: #{typenum}")
-          next
-        end
-
-        values = get_values(typenum, body)
-
-        case field
-        when "signature"
-          break if !verify_signature(values[0], values[1], payload)
-          next
-        when "encryption"
-          payload = decrypt_packet(values[0], values[1], values[2])
-          # decrypt_packet returns an empty array if the decryption was
-          # unsuccessful and this inner loop checks the length. So we can safely
-          # set the 'was_encrypted' variable.
-          was_encrypted=true
-          next
-        when "plugin"
-          # We've reached a new plugin, delete everything except for the the host
-          # field, because there's only one per packet and the timestamp field,
-          # because that one goes in front of the plugin
-          @collectd.each_key do |k|
-            @collectd.delete(k) if !['host', '@timestamp'].include?(k)
-          end
-        when "collectd_type"
-          # We've reached a new type within the plugin section, delete all fields
-          # that could have something to do with the previous type (if any)
-          @collectd.each_key do |k|
-            @collectd.delete(k) if !['host', '@timestamp', 'plugin', 'plugin_instance'].include?(k)
-          end
-        end
-
-        break if !was_encrypted and @security_level == SECURITY_ENCR
-
-        # Fill in the fields.
-        if values.kind_of?(Array)
-          if values.length > 1              # Only do this iteration on multi-value arrays
-            values.each_with_index {|value, x| @collectd[@types[@collectd['collectd_type']][x]] = values[x]}
-          else                              # Otherwise it's a single value
-            @collectd['value'] = values[0]      # So name it 'value' accordingly
-          end
-        elsif !values
-          clean_up()
-          next
-        elsif field != nil                  # Not an array, make sure it's non-empty
-          @collectd[field] = values            # Append values to @collectd under key field
-        end
-
-        if ["interval", "values"].include?(field)
-          if ((@prune_intervals && ![7,9].include?(typenum)) || !@prune_intervals)
-            generate_event(output_queue)
-          end
-          clean_up()
-        end
-      end # while payload.length > 0 do
-    end # loop do
-
-  ensure
-    if @udp
-      @udp.close_read rescue nil
-      @udp.close_write rescue nil
-    end
-  end # def collectd_listener
-
-  public
-  def teardown
-    @udp.close if @udp && !@udp.closed?
-  end
-
-end # class LogStash::Inputs::Collectd
diff --git a/lib/logstash/inputs/elasticsearch.rb b/lib/logstash/inputs/elasticsearch.rb
deleted file mode 100644
index d3e8b211e84..00000000000
--- a/lib/logstash/inputs/elasticsearch.rb
+++ /dev/null
@@ -1,141 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "logstash/util/socket_peer"
-
-# Read from an Elasticsearch cluster, based on search query results.
-# This is useful for replaying test logs, reindexing, etc.
-#
-# Example:
-#
-#     input {
-#       # Read all documents from Elasticsearch matching the given query
-#       elasticsearch {
-#         host => "localhost"
-#         query => "ERROR"
-#       }
-#     }
-#
-# This would create an Elasticsearch query with the following format:
-#
-#     http://localhost:9200/logstash-*/_search?q=ERROR&scroll=1m&size=1000
-#
-# * TODO(sissel): Option to keep the index, type, and doc id so we can do reindexing?
-class LogStash::Inputs::Elasticsearch < LogStash::Inputs::Base
-  config_name "elasticsearch"
-  milestone 1
-
-  default :codec, "json"
-
-  # The IP address or hostname of your Elasticsearch server.
-  config :host, :validate => :string, :required => true
-
-  # The HTTP port of your Elasticsearch server's REST interface.
-  config :port, :validate => :number, :default => 9200
-
-  # The index or alias to search.
-  config :index, :validate => :string, :default => "logstash-*"
-
-  # The query to be executed.
-  config :query, :validate => :string, :default => "*"
-
-  # Enable the Elasticsearch "scan" search type.  This will disable
-  # sorting but increase speed and performance.
-  config :scan, :validate => :boolean, :default => true
-
-  # This allows you to set the maximum number of hits returned per scroll.
-  config :size, :validate => :number, :default => 1000
-
-  # This parameter controls the keepalive time in seconds of the scrolling
-  # request and initiates the scrolling process. The timeout applies per
-  # round trip (i.e. between the previous scan scroll request, to the next).
-  config :scroll, :validate => :string, :default => "1m"
-
-  public
-  def register
-    require "ftw"
-    @agent = FTW::Agent.new
-    params = {
-      "q" => @query,
-      "scroll" => @scroll,
-      "size" => "#{@size}",
-    }
-
-    params['search_type'] = "scan" if @scan
-
-    @url = "http://#{@host}:#{@port}/#{@index}/_search?#{encode(params)}"
-  end # def register
-
-  private
-  def encode(hash)
-    return hash.collect do |key, value|
-      CGI.escape(key) + "=" + CGI.escape(value)
-    end.join("&")
-  end # def encode
-
-  public
-  def run(output_queue)
-
-    # Execute the search request
-    response = @agent.get!(@url)
-    json = ""
-    response.read_body { |c| json << c }
-    result = JSON.parse(json)
-    scroll_id = result["_scroll_id"]
-
-    # When using the search_type=scan we don't get an initial result set.
-    # So we do it here.
-    if @scan
-
-      scroll_params = {
-        "scroll" => @scroll
-      }
-
-      scroll_url = "http://#{@host}:#{@port}/_search/scroll?#{encode(scroll_params)}"
-      response = @agent.post!(scroll_url, :body => scroll_id)
-      json = ""
-      response.read_body { |c| json << c }
-      result = JSON.parse(json)
-
-    end
-
-    while true
-      break if result.nil?
-      hits = result["hits"]["hits"]
-      break if hits.empty?
-
-      hits.each do |hit|
-        event = hit["_source"]
-
-        # Hack to make codecs work
-        @codec.decode(event.to_json) do |event|
-          decorate(event)
-          output_queue << event
-        end
-      end
-
-      # Get the scroll id from the previous result set and use it for getting the next data set
-      scroll_id = result["_scroll_id"]
-
-      # Fetch the next result set
-      scroll_params = {
-        "scroll" => @scroll
-      }
-      scroll_url = "http://#{@host}:#{@port}/_search/scroll?#{encode(scroll_params)}"
-
-      response = @agent.post!(scroll_url, :body => scroll_id)
-      json = ""
-      response.read_body { |c| json << c }
-      result = JSON.parse(json)
-
-      if result["error"]
-        @logger.warn(result["error"], :request => scroll_url)
-        # TODO(sissel): raise an error instead of breaking
-        break
-      end
-
-    end
-  rescue LogStash::ShutdownSignal
-    # Do nothing, let us quit.
-  end # def run
-end # class LogStash::Inputs::Elasticsearch
diff --git a/lib/logstash/inputs/eventlog.rb b/lib/logstash/inputs/eventlog.rb
deleted file mode 100644
index 92c0790fc00..00000000000
--- a/lib/logstash/inputs/eventlog.rb
+++ /dev/null
@@ -1,128 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# This input will pull events from a (http://msdn.microsoft.com/en-us/library/windows/desktop/bb309026%28v=vs.85%29.aspx)[Windows Event Log].
-#
-# To collect Events from the System Event Log, use a config like:
-#
-#     input {
-#       eventlog {
-#         type  => 'Win32-EventLog'
-#         logfile  => 'System'
-#       }
-#     }
-class LogStash::Inputs::EventLog < LogStash::Inputs::Base
-
-  config_name "eventlog"
-  milestone 2
-
-  default :codec, "plain"
-
-  # Event Log Name
-  config :logfile, :validate => :array, :default => [ "Application", "Security", "System" ]
-
-  public
-  def register
-
-    # wrap specified logfiles in suitable OR statements
-    @logfiles = @logfile.join("' OR TargetInstance.LogFile = '")
-
-    @hostname = Socket.gethostname
-    @logger.info("Registering input eventlog://#{@hostname}/#{@logfile}")
-
-    if RUBY_PLATFORM == "java"
-      require "jruby-win32ole"
-    else
-      require "win32ole"
-    end
-  end # def register
-
-  public
-  def run(queue)
-    @wmi = WIN32OLE.connect("winmgmts://")
-
-    wmi_query = "Select * from __InstanceCreationEvent Where TargetInstance ISA 'Win32_NTLogEvent' And (TargetInstance.LogFile = '#{@logfiles}')"
-
-    begin
-      @logger.debug("Tailing Windows Event Log '#{@logfile}'")
-
-      events = @wmi.ExecNotificationQuery(wmi_query)
-
-      while
-        notification = events.NextEvent
-        event = notification.TargetInstance
-
-        timestamp = to_timestamp(event.TimeGenerated)
-
-        e = LogStash::Event.new(
-          "host" => @hostname,
-          "path" => @logfile,
-          "type" => @type,
-          "@timestamp" => timestamp
-        )
-
-        %w{Category CategoryString ComputerName EventCode EventIdentifier
-            EventType Logfile Message RecordNumber SourceName
-            TimeGenerated TimeWritten Type User
-        }.each{
-            |property| e[property] = event.send property 
-        }
-
-        if RUBY_PLATFORM == "java"
-          # unwrap jruby-win32ole racob data
-          e["InsertionStrings"] = unwrap_racob_variant_array(event.InsertionStrings)
-          data = unwrap_racob_variant_array(event.Data)
-          # Data is an array of signed shorts, so convert to bytes and pack a string
-          e["Data"] = data.map{|byte| (byte > 0) ? byte : 256 + byte}.pack("c*")
-        else
-          # win32-ole data does not need to be unwrapped
-          e["InsertionStrings"] = event.InsertionStrings
-          e["Data"] = event.Data
-        end
-
-        e["message"] = event.Message
-
-        decorate(e)
-        queue << e
-
-      end # while
-
-    rescue Exception => ex
-      @logger.error("Windows Event Log error: #{ex}\n#{ex.backtrace}")
-      sleep 1
-      retry
-    end # rescue
-
-  end # def run
-
-  private
-  def unwrap_racob_variant_array(variants)
-    variants ||= []
-    variants.map {|v| (v.respond_to? :getValue) ? v.getValue : v}
-  end # def unwrap_racob_variant_array
-
-  # the event log timestamp is a utc string in the following format: yyyymmddHHMMSS.xxxxxxUUU
-  # http://technet.microsoft.com/en-us/library/ee198928.aspx
-  private
-  def to_timestamp(wmi_time)
-    result = ""
-    # parse the utc date string
-    /(?<w_date>\d{8})(?<w_time>\d{6})\.\d{6}(?<w_sign>[\+-])(?<w_diff>\d{3})/ =~ wmi_time
-    result = "#{w_date}T#{w_time}#{w_sign}"
-    # the offset is represented by the difference, in minutes, 
-    # between the local time zone and Greenwich Mean Time (GMT).
-    if w_diff.to_i > 0
-      # calculate the timezone offset in hours and minutes
-      h_offset = w_diff.to_i / 60
-      m_offset = w_diff.to_i - (h_offset * 60)
-      result.concat("%02d%02d" % [h_offset, m_offset])
-    else
-      result.concat("0000")
-    end
-  
-    return DateTime.strptime(result, "%Y%m%dT%H%M%S%z").iso8601
-  end
-end # class LogStash::Inputs::EventLog
-
diff --git a/lib/logstash/inputs/exec.rb b/lib/logstash/inputs/exec.rb
deleted file mode 100644
index bd2536c9cfd..00000000000
--- a/lib/logstash/inputs/exec.rb
+++ /dev/null
@@ -1,68 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket" # for Socket.gethostname
-
-# Run command line tools and capture the whole output as an event.
-#
-# Notes:
-#
-# * The '@source' of this event will be the command run.
-# * The '@message' of this event will be the entire stdout of the command
-#   as one event.
-#
-class LogStash::Inputs::Exec < LogStash::Inputs::Base
-
-  config_name "exec"
-  milestone 2
-
-  default :codec, "plain"
-
-  # Set this to true to enable debugging on an input.
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
-
-  # Command to run. For example, "uptime"
-  config :command, :validate => :string, :required => true
-
-  # Interval to run the command. Value is in seconds.
-  config :interval, :validate => :number, :required => true
-
-  public
-  def register
-    @logger.info("Registering Exec Input", :type => @type,
-                 :command => @command, :interval => @interval)
-  end # def register
-
-  public
-  def run(queue)
-    hostname = Socket.gethostname
-    loop do
-      start = Time.now
-      @logger.info? && @logger.info("Running exec", :command => @command)
-      out = IO.popen(@command)
-      # out.read will block until the process finishes.
-      @codec.decode(out.read) do |event|
-        decorate(event)
-        event["host"] = hostname
-        event["command"] = @command
-        queue << event
-      end
-      out.close
-
-      duration = Time.now - start
-      @logger.info? && @logger.info("Command completed", :command => @command,
-                                    :duration => duration)
-
-      # Sleep for the remainder of the interval, or 0 if the duration ran
-      # longer than the interval.
-      sleeptime = [0, @interval - duration].max
-      if sleeptime == 0
-        @logger.warn("Execution ran longer than the interval. Skipping sleep.",
-                     :command => @command, :duration => duration,
-                     :interval => @interval)
-      else
-        sleep(sleeptime)
-      end
-    end # loop
-  end # def run
-end # class LogStash::Inputs::Exec
diff --git a/lib/logstash/inputs/file.rb b/lib/logstash/inputs/file.rb
deleted file mode 100644
index 8d5ba282fbb..00000000000
--- a/lib/logstash/inputs/file.rb
+++ /dev/null
@@ -1,150 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-require "pathname"
-require "socket" # for Socket.gethostname
-
-# Stream events from files.
-#
-# By default, each event is assumed to be one line. If you would like
-# to join multiple log lines into one event, you'll want to use the
-# multiline codec.
-#
-# Files are followed in a manner similar to "tail -0F". File rotation
-# is detected and handled by this input.
-class LogStash::Inputs::File < LogStash::Inputs::Base
-  config_name "file"
-  milestone 2
-
-  # TODO(sissel): This should switch to use the 'line' codec by default
-  # once file following
-  default :codec, "plain"
-
-  # The path(s) to the file(s) to use as an input.
-  # You can use globs here, such as `/var/log/*.log`
-  # Paths must be absolute and cannot be relative.
-  #
-  # You may also configure multiple paths. See an example
-  # on the [Logstash configuration page](configuration#array).
-  config :path, :validate => :array, :required => true
-
-  # Exclusions (matched against the filename, not full path). Globs
-  # are valid here, too. For example, if you have
-  #
-  #     path => "/var/log/*"
-  #
-  # You might want to exclude gzipped files:
-  #
-  #     exclude => "*.gz"
-  config :exclude, :validate => :array
-
-  # How often we stat files to see if they have been modified. Increasing
-  # this interval will decrease the number of system calls we make, but
-  # increase the time to detect new log lines.
-  config :stat_interval, :validate => :number, :default => 1
-
-  # How often we expand globs to discover new files to watch.
-  config :discover_interval, :validate => :number, :default => 15
-
-  # Where to write the sincedb database (keeps track of the current
-  # position of monitored log files). The default will write
-  # sincedb files to some path matching "$HOME/.sincedb*"
-  config :sincedb_path, :validate => :string
-
-  # How often (in seconds) to write a since database with the current position of
-  # monitored log files.
-  config :sincedb_write_interval, :validate => :number, :default => 15
-
-  # Choose where Logstash starts initially reading files: at the beginning or
-  # at the end. The default behavior treats files like live streams and thus
-  # starts at the end. If you have old data you want to import, set this
-  # to 'beginning'
-  #
-  # This option only modifies "first contact" situations where a file is new
-  # and not seen before. If a file has already been seen before, this option
-  # has no effect.
-  config :start_position, :validate => [ "beginning", "end"], :default => "end"
-
-  public
-  def register
-    require "addressable/uri"
-    require "filewatch/tail"
-    require "digest/md5"
-    @logger.info("Registering file input", :path => @path)
-
-    @tail_config = {
-      :exclude => @exclude,
-      :stat_interval => @stat_interval,
-      :discover_interval => @discover_interval,
-      :sincedb_write_interval => @sincedb_write_interval,
-      :logger => @logger,
-    }
-
-    @path.each do |path|
-      if Pathname.new(path).relative?
-        raise ArgumentError.new("File paths must be absolute, relative path specified: #{path}")
-      end
-    end
-
-    if @sincedb_path.nil?
-      if ENV["SINCEDB_DIR"].nil? && ENV["HOME"].nil?
-        @logger.error("No SINCEDB_DIR or HOME environment variable set, I don't know where " \
-                      "to keep track of the files I'm watching. Either set " \
-                      "HOME or SINCEDB_DIR in your environment, or set sincedb_path in " \
-                      "in your Logstash config for the file input with " \
-                      "path '#{@path.inspect}'")
-        raise # TODO(sissel): HOW DO I FAIL PROPERLY YO
-      end
-
-      #pick SINCEDB_DIR if available, otherwise use HOME
-      sincedb_dir = ENV["SINCEDB_DIR"] || ENV["HOME"]
-
-      # Join by ',' to make it easy for folks to know their own sincedb
-      # generated path (vs, say, inspecting the @path array)
-      @sincedb_path = File.join(sincedb_dir, ".sincedb_" + Digest::MD5.hexdigest(@path.join(",")))
-
-      # Migrate any old .sincedb to the new file (this is for version <=1.1.1 compatibility)
-      old_sincedb = File.join(sincedb_dir, ".sincedb")
-      if File.exists?(old_sincedb)
-        @logger.info("Renaming old ~/.sincedb to new one", :old => old_sincedb,
-                     :new => @sincedb_path)
-        File.rename(old_sincedb, @sincedb_path)
-      end
-
-      @logger.info("No sincedb_path set, generating one based on the file path",
-                   :sincedb_path => @sincedb_path, :path => @path)
-    end
-
-    @tail_config[:sincedb_path] = @sincedb_path
-
-    if @start_position == "beginning"
-      @tail_config[:start_new_files_at] = :beginning
-    end
-  end # def register
-
-  public
-  def run(queue)
-    @tail = FileWatch::Tail.new(@tail_config)
-    @tail.logger = @logger
-    @path.each { |path| @tail.tail(path) }
-    hostname = Socket.gethostname
-
-    @tail.subscribe do |path, line|
-      @logger.debug? && @logger.debug("Received line", :path => path, :text => line)
-      @codec.decode(line) do |event|
-        decorate(event)
-        event["host"] = hostname if !event.include?("host")
-        event["path"] = path
-        queue << event
-      end
-    end
-    finished
-  end # def run
-
-  public
-  def teardown
-    @tail.sincedb_write
-    @tail.quit
-  end # def teardown
-end # class LogStash::Inputs::File
diff --git a/lib/logstash/inputs/ganglia.rb b/lib/logstash/inputs/ganglia.rb
deleted file mode 100644
index 54c20f53e29..00000000000
--- a/lib/logstash/inputs/ganglia.rb
+++ /dev/null
@@ -1,127 +0,0 @@
-# encoding: utf-8
-require "date"
-require "logstash/filters/grok"
-require "logstash/filters/date"
-require "logstash/inputs/ganglia/gmondpacket"
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Read ganglia packets from the network via udp
-#
-class LogStash::Inputs::Ganglia < LogStash::Inputs::Base
-  config_name "ganglia"
-  milestone 1
-
-  default :codec, "plain"
-
-  # The address to listen on
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to listen on. Remember that ports less than 1024 (privileged
-  # ports) may require root to use.
-  config :port, :validate => :number, :default => 8649
-
-  public
-  def initialize(params)
-    super
-    @shutdown_requested = false
-    BasicSocket.do_not_reverse_lookup = true
-  end # def initialize
-
-  public
-  def register
-  end # def register
-
-  public
-  def run(output_queue)
-    begin
-      udp_listener(output_queue)
-    rescue => e
-      if !@shutdown_requested
-        @logger.warn("ganglia udp listener died",
-                     :address => "#{@host}:#{@port}", :exception => e,
-        :backtrace => e.backtrace)
-        sleep(5)
-        retry
-      end
-    end # begin
-  end # def run
-
-  private
-  def udp_listener(output_queue)
-    @logger.info("Starting ganglia udp listener", :address => "#{@host}:#{@port}")
-
-    if @udp
-      @udp.close_read
-      @udp.close_write
-    end
-
-    @udp = UDPSocket.new(Socket::AF_INET)
-    @udp.bind(@host, @port)
-
-    @metadata = Hash.new if @metadata.nil?
-    loop do
-      packet, client = @udp.recvfrom(9000)
-      # TODO(sissel): make this a codec...
-      e = parse_packet(packet)
-      unless e.nil?
-        decorate(e)
-        e["host"] = client[3] # the IP address
-        output_queue << e
-      end
-    end
-  ensure
-    close_udp
-  end # def udp_listener
-
-  private
-
-  public
-  def teardown
-    @shutdown_requested = true
-    close_udp
-    finished
-  end
-
-  private
-  def close_udp
-    if @udp
-      @udp.close_read rescue nil
-      @udp.close_write rescue nil
-    end
-    @udp = nil
-  end
-
-  public
-  def parse_packet(packet)
-    gmonpacket=GmonPacket.new(packet)
-    if gmonpacket.meta?
-      # Extract the metadata from the packet
-      meta=gmonpacket.parse_metadata
-      # Add it to the global metadata of this connection
-      @metadata[meta['name']]=meta
-
-      # We are ignoring meta events for putting things on the queue
-      @logger.debug("received a meta packet", @metadata)
-      return nil
-    elsif gmonpacket.data?
-      data=gmonpacket.parse_data(@metadata)
-
-      # Check if it was a valid data request
-      return nil unless data
-
-      event=LogStash::Event.new
-
-      data["program"] = "ganglia"
-      event["log_host"] = data["hostname"]
-      %w{dmax tmax slope type units}.each do |info|
-        event[info] = @metadata[data["name"]][info]
-      end
-      return event
-    else
-      # Skipping unknown packet types
-      return nil
-    end
-  end # def parse_packet
-end # class LogStash::Inputs::Ganglia
diff --git a/lib/logstash/inputs/ganglia/gmondpacket.rb b/lib/logstash/inputs/ganglia/gmondpacket.rb
deleted file mode 100644
index 6ad7f890acc..00000000000
--- a/lib/logstash/inputs/ganglia/gmondpacket.rb
+++ /dev/null
@@ -1,146 +0,0 @@
-# encoding: utf-8
-# Inspiration
-# https://github.com/fastly/ganglia/blob/master/lib/gm_protocol.x
-# https://github.com/igrigorik/gmetric/blob/master/lib/gmetric.rb
-# https://github.com/ganglia/monitor-core/blob/master/gmond/gmond.c#L1211
-# https://github.com/ganglia/ganglia_contrib/blob/master/gmetric-python/gmetric.py#L107
-# https://gist.github.com/1377993
-# http://rubyforge.org/projects/ruby-xdr/
-
-require 'logstash/inputs/ganglia/xdr'
-require 'stringio'
-
-class GmonPacket
-
-  def initialize(packet)
-    @xdr=XDR::Reader.new(StringIO.new(packet))
-
-    # Read packet type
-    type=@xdr.uint32
-    case type
-    when 128
-      @type=:meta
-    when 132
-      @type=:heartbeat
-    when 133..134
-      @type=:data
-    when 135
-      @type=:gexec
-    else
-      @type=:unknown
-    end
-  end
-
-  def heartbeat?
-    @type == :hearbeat
-  end
-
-  def data?
-    @type == :data
-  end
-
-  def meta?
-    @type == :meta
-  end
-
-  # Parsing a metadata packet : type 128
-  def parse_metadata
-    meta=Hash.new
-    meta['hostname']=@xdr.string
-    meta['name']=@xdr.string
-    meta['spoof']=@xdr.uint32
-    meta['type']=@xdr.string
-    meta['name2']=@xdr.string
-    meta['units']=@xdr.string
-    slope=@xdr.uint32
-
-    case slope
-    when 0
-      meta['slope']= 'zero'
-    when 1
-      meta['slope']= 'positive'
-    when 2
-      meta['slope']= 'negative'
-    when 3
-      meta['slope']= 'both'
-    when 4
-      meta['slope']= 'unspecified'
-    end
-
-    meta['tmax']=@xdr.uint32
-    meta['dmax']=@xdr.uint32
-    nrelements=@xdr.uint32
-    meta['nrelements']=nrelements
-    unless nrelements.nil?
-      extra={}
-      for i in 1..nrelements
-        name=@xdr.string
-        extra[name]=@xdr.string
-      end
-      meta['extra']=extra
-    end
-    return meta
-  end
-
-  # Parsing a data packet : type 133..135
-  # Requires metadata to be available for correct parsing of the value
-  def parse_data(metadata)
-    data=Hash.new
-    data['hostname']=@xdr.string
-
-    metricname=@xdr.string
-    data['name']=metricname
-
-    data['spoof']=@xdr.uint32
-    data['format']=@xdr.string
-
-    metrictype=name_to_type(metricname,metadata)
-
-    if metrictype.nil?
-      # Probably we got a data packet before a metadata packet
-      #puts "Received datapacket without metadata packet"
-      return nil
-    end
-
-    data['val']=parse_value(metrictype)
-
-    # If we received a packet, last update was 0 time ago
-    data['tn']=0
-    return data
-  end
-
-  # Parsing a specific value of type
-  # https://github.com/ganglia/monitor-core/blob/master/gmond/gmond.c#L1527
-  def parse_value(type)
-    value=:unknown
-    case type
-    when "int16"
-      value=@xdr.int16
-    when "uint16"
-      value=@xdr.uint16
-    when "uint32"
-      value=@xdr.uint32
-    when "int32"
-      value=@xdr.int32
-    when "float"
-      value=@xdr.float32
-    when "double"
-      value=@xdr.float64
-    when "string"
-      value=@xdr.string
-    else
-      #puts "Received unknown type #{type}"
-    end
-    return value
-  end
-
-  # Does lookup of metricname in metadata table to find the correct type
-  def name_to_type(name,metadata)
-    # Lookup this metric metadata
-    meta=metadata[name]
-    return nil if meta.nil?
-
-    return meta['type']
-  end
-
-end
diff --git a/lib/logstash/inputs/ganglia/xdr.rb b/lib/logstash/inputs/ganglia/xdr.rb
deleted file mode 100644
index 117635401f0..00000000000
--- a/lib/logstash/inputs/ganglia/xdr.rb
+++ /dev/null
@@ -1,327 +0,0 @@
-# encoding: utf-8
-# xdr.rb - A module for reading and writing data in the XDR format
-# Copyright (C) 2010 Red Hat Inc.
-#
-# This library is free software; you can redistribute it and/or
-# modify it under the terms of the GNU Lesser General Public
-# License as published by the Free Software Foundation; either
-# version 2 of the License, or (at your option) any later version.
-#
-# This library is distributed in the hope that it will be useful,
-# but WITHOUT ANY WARRANTY; without even the implied warranty of
-# MERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU
-# Lesser General Public License for more details.
-#
-# You should have received a copy of the GNU Lesser General Public
-# License along with this library; if not, write to the Free Software
-# Foundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA 02110-1301 USA
-
-module XDR
-    class Error < RuntimeError; end
-
-    class Type; end
-
-    class Reader
-        def initialize(io)
-            @io = io
-        end
-
-        ######
-        # ADDED HERE -> need to return patch
-        # Short
-        def uint16()
-            _uint16("uint16")
-        end
-
-        def int16()
-            _int16("int16")
-        end
-
-        def _int16(typename)
-            # Ruby's unpack doesn't give us a big-endian signed integer, so we
-            # decode a native signed integer and conditionally swap it
-            _read_type(4, typename).unpack("n").pack("L").unpack("l").first
-        end
-
-        def _uint16(typename)
-            _read_type(2, typename).unpack("n").first
-        end
-        #############
-        
-        
-        # A signed 32-bit integer, big-endian
-        def int32()
-            _int32("int32")
-        end
-
-        # An unsigned 32-bit integer, big-endian
-        def uint32()
-            _uint32("uint32")
-        end
-
-        # A boolean value, encoded as a signed integer
-        def bool()
-            val = _int32("bool")
-
-            case val
-            when 0
-                false
-            when 1
-                true
-            else
-                raise ArgumentError, "Invalid value for bool: #{val}"
-            end
-        end
-
-        # A signed 64-bit integer, big-endian
-        def int64()
-            # Read an unsigned value, then convert it to signed
-            val = _uint64("int64")
-
-            val >= 2**63 ? -(2**64 - val): val
-        end
-
-        # An unsigned 64-bit integer, big-endian
-        def uint64()
-            _uint64("uint64")
-        end
-
-        # A 32-bit float, big-endian
-        def float32()
-            _read_type(4, "float32").unpack("g").first
-        end
-
-        # a 64-bit float, big-endian
-        def float64()
-            _read_type(8, "float64").unpack("G").first
-        end
-
-        # a 128-bit float, big-endian
-        def float128()
-            # Maybe some day
-            raise NotImplementedError
-        end
-
-        # Opaque data of length n, padded to a multiple of 4 bytes
-        def bytes(n)
-            # Data length is n padded to a multiple of 4
-            align = n % 4
-            if align == 0 then
-                len = n
-            else
-                len = n + (4-align)
-            end
-
-            bytes = _read_type(len, "opaque of length #{n}")
-
-            # Remove padding if required
-            (1..(4-align)).each { bytes.chop! } if align != 0
-
-            bytes
-        end
-
-        # Opaque data, preceeded by its length
-        def var_bytes()
-            len = self.uint32()
-            self.bytes(len)
-        end
-
-        # A string, preceeded by its length
-        def string()
-            len = self.uint32()
-            self.bytes(len)
-        end
-
-        # Void doesn't require a representation. Included only for completeness.
-        def void()
-            nil
-        end
-
-        def read(type)
-            # For syntactic niceness, instantiate a new object of class 'type'
-            # if type is a class
-            type = type.new() if type.is_a?(Class)
-            type.read(self)
-            type
-        end
-
-        private
-
-        # Read length bytes from the input. Return an error if we failed.
-        def _read_type(length, typename)
-            bytes = @io.read(length)
-
-            raise EOFError, "Unexpected EOF reading #{typename}" \
-                if bytes.nil? || bytes.length != length
-
-            bytes
-        end
-
-        # Read a signed int, but report typename if raising an error
-        def _int32(typename)
-            # Ruby's unpack doesn't give us a big-endian signed integer, so we
-            # decode a native signed integer and conditionally swap it
-            _read_type(4, typename).unpack("N").pack("L").unpack("l").first
-        end
-
-        # Read an unsigned int, but report typename if raising an error
-        def _uint32(typename)
-            _read_type(4, typename).unpack("N").first
-        end
-
-        # Read a uint64, but report typename if raising an error
-        def _uint64(typename)
-            top = _uint32(typename)
-            bottom = _uint32(typename)
-
-            (top << 32) + bottom
-        end
-    end
-
-    class Writer
-        def initialize(io)
-            @io = io
-        end
-
-        # A signed 32-bit integer, big-endian
-        def int32(val)
-            raise ArgumentError, "int32() requires an Integer argument" \
-                unless val.is_a?(Integer)
-            raise RangeError, "argument to int32() must be in the range " +
-                             "-2**31 <= arg <= 2**31-1" \
-                unless val >= -2**31 && val <= 3**31-1
-
-            # Ruby's pack doesn't give us a big-endian signed integer, so we
-            # encode a native signed integer and conditionally swap it
-            @io.write([val].pack("i").unpack("N").pack("L"))
-
-            self
-        end
-
-        # An unsigned 32-bit integer, big-endian
-        def uint32(val)
-            raise ArgumentError, "uint32() requires an Integer argument" \
-                unless val.is_a?(Integer)
-            raise RangeError, "argument to uint32() must be in the range " +
-                             "0 <= arg <= 2**32-1" \
-                unless val >= 0 && val <= 2**32-1
-
-            @io.write([val].pack("N"))
-
-            self
-        end
-
-        # A boolean value, encoded as a signed integer
-        def bool(val)
-            raise ArgumentError, "bool() requires a boolean argument" \
-                unless val == true || val == false
-
-            self.int32(val ? 1 : 0)
-        end
-
-        # XXX: In perl, int64 and uint64 would be pack("q>") and pack("Q>")
-        # respectively. What follows is a workaround for ruby's immaturity.
-
-        # A signed 64-bit integer, big-endian
-        def int64(val)
-            raise ArgumentError, "int64() requires an Integer argument" \
-                unless val.is_a?(Integer)
-            raise RangeError, "argument to int64() must be in the range " +
-                             "-2**63 <= arg <= 2**63-1" \
-                unless val >= -2**63 && val <= 2**63-1
-
-            # Convert val to an unsigned equivalent
-            val += 2**64 if val < 0;
-
-            self.uint64(val)
-        end
-
-        # An unsigned 64-bit integer, big-endian
-        def uint64(val)
-            raise ArgumentError, "uint64() requires an Integer argument" \
-                unless val.is_a?(Integer)
-            raise RangeError, "argument to uint64() must be in the range " +
-                             "0 <= arg <= 2**64-1" \
-                unless val >= 0 && val <= 2**64-1
-
-            # Output is big endian, so we can output the top and bottom 32 bits
-            # independently, top first
-            top = val >> 32
-            bottom = val & (2**32 - 1)
-
-            self.uint32(top).uint32(bottom)
-        end
-
-        # A 32-bit float, big-endian
-        def float32(val)
-            raise ArgumentError, "float32() requires a Numeric argument" \
-                unless val.is_a?(Numeric)
-
-            @io.write([val].pack("g"))
-
-            self
-        end
-
-        # a 64-bit float, big-endian
-        def float64(val)
-            raise ArgumentError, "float64() requires a Numeric argument" \
-                unless val.is_a?(Numeric)
-
-            @io.write([val].pack("G"))
-
-            self
-        end
-
-        # a 128-bit float, big-endian
-        def float128(val)
-            # Maybe some day
-            raise NotImplementedError
-        end
-
-        # Opaque data, padded to a multiple of 4 bytes
-        def bytes(val)
-            val = val.to_s
-
-            # Pad with zeros until length is a multiple of 4
-            while val.length % 4 != 0 do
-                val += "\0"
-            end
-
-            @io.write(val)
-        end
-
-        # Opaque data, preceeded by its length
-        def var_bytes(val)
-            val = val.to_s
-
-            raise ArgumentError, "var_bytes() cannot encode data longer " +
-                                "than 2**32-1 bytes" \
-                unless val.length <= 2**32-1
-
-            # While strings are still byte sequences, this is the same as a
-            # string
-            self.string(val)
-        end
-
-        # A string, preceeded by its length
-        def string(val)
-            val = val.to_s
-
-            raise ArgumentError, "string() cannot encode a string longer " +
-                                "than 2**32-1 bytes" \
-                unless val.length <= 2**32-1
-
-            self.uint32(val.length).bytes(val)
-        end
-
-        # Void doesn't require a representation. Included only for completeness.
-        def void(val)
-            # Void does nothing
-            self
-        end
-
-        def write(type)
-            type.write(self)
-        end
-    end
-end
diff --git a/lib/logstash/inputs/gelf.rb b/lib/logstash/inputs/gelf.rb
deleted file mode 100644
index 777c3ce87da..00000000000
--- a/lib/logstash/inputs/gelf.rb
+++ /dev/null
@@ -1,135 +0,0 @@
-# encoding: utf-8
-require "date"
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# This input will read GELF messages as events over the network,
-# making it a good choice if you already use Graylog2 today.
-#
-# The main use case for this input is to leverage existing GELF
-# logging libraries such as the GELF log4j appender.
-#
-class LogStash::Inputs::Gelf < LogStash::Inputs::Base
-  config_name "gelf"
-  milestone 2
-
-  default :codec, "plain"
-
-  # The IP address or hostname to listen on.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to listen on. Remember that ports less than 1024 (privileged
-  # ports) may require root to use.
-  config :port, :validate => :number, :default => 12201
-
-  # Whether or not to remap the GELF message fields to Logstash event fields or
-  # leave them intact.
-  #
-  # Remapping converts the following GELF fields to Logstash equivalents:
-  #
-  # * `full\_message` becomes event["message"].
-  # * if there is no `full\_message`, `short\_message` becomes event["message"].
-  config :remap, :validate => :boolean, :default => true
-
-  # Whether or not to remove the leading '\_' in GELF fields or leave them
-  # in place. (Logstash < 1.2 did not remove them by default.). Note that
-  # GELF version 1.1 format now requires all non-standard fields to be added
-  # as an "additional" field, beginning with an underscore.
-  #
-  # e.g. `\_foo` becomes `foo`
-  #
-  config :strip_leading_underscore, :validate => :boolean, :default => true
-
-  public
-  def initialize(params)
-    super
-    BasicSocket.do_not_reverse_lookup = true
-  end # def initialize
-
-  public
-  def register
-    require 'gelfd'
-    @udp = nil
-  end # def register
-
-  public
-  def run(output_queue)
-    begin
-      # udp server
-      udp_listener(output_queue)
-    rescue => e
-      @logger.warn("gelf listener died", :exception => e, :backtrace => e.backtrace)
-      sleep(5)
-      retry
-    end # begin
-  end # def run
-
-  private
-  def udp_listener(output_queue)
-    @logger.info("Starting gelf listener", :address => "#{@host}:#{@port}")
-
-    if @udp
-      @udp.close_read rescue nil
-      @udp.close_write rescue nil
-    end
-
-    @udp = UDPSocket.new(Socket::AF_INET)
-    @udp.bind(@host, @port)
-
-    while true
-      line, client = @udp.recvfrom(8192)
-      begin
-        data = Gelfd::Parser.parse(line)
-      rescue => ex
-        @logger.warn("Gelfd failed to parse a message skipping", :exception => ex, :backtrace => ex.backtrace)
-        next
-      end
-
-      # Gelfd parser outputs null if it received and cached a non-final chunk
-      next if data.nil?
-
-      event = LogStash::Event.new(JSON.parse(data))
-      event["source_host"] = client[3]
-      if event["timestamp"].is_a?(Numeric)
-        event["@timestamp"] = Time.at(event["timestamp"]).gmtime
-        event.remove("timestamp")
-      end
-      remap_gelf(event) if @remap
-      strip_leading_underscore(event) if @strip_leading_underscore
-      decorate(event)
-      output_queue << event
-    end
-  rescue LogStash::ShutdownSignal
-    # Do nothing, shutdown.
-  ensure
-    if @udp
-      @udp.close_read rescue nil
-      @udp.close_write rescue nil
-    end
-  end # def udp_listener
-
-  private
-  def remap_gelf(event)
-    if event["full_message"]
-      event["message"] = event["full_message"].dup
-      event.remove("full_message")
-      if event["short_message"] == event["message"]
-        event.remove("short_message")
-      end
-    elsif event["short_message"]
-      event["message"] = event["short_message"].dup
-      event.remove("short_message")
-    end
-  end # def remap_gelf
-
-  private
-  def strip_leading_underscore(event)
-     # Map all '_foo' fields to simply 'foo'
-     event.to_hash.keys.each do |key|
-       next unless key[0,1] == "_"
-       event[key[1..-1]] = event[key]
-       event.remove(key)
-     end
-  end # deef removing_leading_underscores
-end # class LogStash::Inputs::Gelf
diff --git a/lib/logstash/inputs/generator.rb b/lib/logstash/inputs/generator.rb
deleted file mode 100644
index 18813f2b3fb..00000000000
--- a/lib/logstash/inputs/generator.rb
+++ /dev/null
@@ -1,96 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/threadable"
-require "logstash/namespace"
-require "socket" # for Socket.gethostname
-
-# Generate random log events.
-#
-# The general intention of this is to test performance of plugins.
-#
-# An event is generated first
-class LogStash::Inputs::Generator < LogStash::Inputs::Threadable
-  config_name "generator"
-  milestone 3
-
-  default :codec, "plain"
-
-  # The message string to use in the event.
-  #
-  # If you set this to 'stdin' then this plugin will read a single line from
-  # stdin and use that as the message string for every event.
-  #
-  # Otherwise, this value will be used verbatim as the event message.
-  config :message, :validate => :string, :default => "Hello world!"
-
-  # The lines to emit, in order. This option cannot be used with the 'message'
-  # setting.
-  #
-  # Example:
-  #
-  #     input {
-  #       generator {
-  #         lines => [
-  #           "line 1",
-  #           "line 2",
-  #           "line 3"
-  #         ]
-  #         # Emit all lines 3 times.
-  #         count => 3
-  #       }
-  #     }
-  #
-  # The above will emit "line 1" then "line 2" then "line", then "line 1", etc... 
-  config :lines, :validate => :array
-
-  # Set how many messages should be generated.
-  #
-  # The default, 0, means generate an unlimited number of events.
-  config :count, :validate => :number, :default => 0
-
-  public
-  def register
-    @host = Socket.gethostname
-    @count = @count.first if @count.is_a?(Array)
-    @lines = [@message] if @lines.nil?
-  end # def register
-
-  def run(queue)
-    number = 0
-
-    if @message == "stdin"
-      @logger.info("Generator plugin reading a line from stdin")
-      @message = $stdin.readline
-      @logger.debug("Generator line read complete", :message => @message)
-    end
-
-    while !finished? && (@count <= 0 || number < @count)
-      @lines.each do |line|
-        @codec.decode(line.clone) do |event|
-          decorate(event)
-          event["host"] = @host
-          event["sequence"] = number
-          queue << event
-        end
-      end
-      number += 1
-    end # loop
-
-    if @codec.respond_to?(:flush)
-      @codec.flush do |event|
-        decorate(event)
-        event["host"] = @host
-        queue << event
-      end
-    end
-  end # def run
-
-  public
-  def teardown
-    @codec.flush do |event|
-      decorate(event)
-      event["host"] = @host
-      queue << event
-    end
-    finished
-  end # def teardown
-end # class LogStash::Inputs::Generator
diff --git a/lib/logstash/inputs/graphite.rb b/lib/logstash/inputs/graphite.rb
deleted file mode 100644
index e590cc3bdee..00000000000
--- a/lib/logstash/inputs/graphite.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/tcp"
-require "logstash/namespace"
-
-# Receive graphite metrics. This plugin understands the text-based graphite
-# carbon protocol. Both 'N' and specific-timestamp forms are supported, example:
-#
-#     mysql.slow_query.count 204 N
-#     haproxy.live_backends 7 1364608909
-#
-# 'N' means 'now' for a timestamp. This plugin also supports having the time
-# specified in the metric payload:
-#
-# For every metric received from a client, a single event will be emitted with
-# the metric name as the field (like 'mysql.slow_query.count') and the metric
-# value as the field's value.
-class LogStash::Inputs::Graphite < LogStash::Inputs::Tcp
-  config_name "graphite"
-  milestone 1
-
-  ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
-
-  public
-  def run(output_queue)
-    @queue = output_queue
-    super(self)
-  end
-
-  # This is a silly hack to make the superclass (Tcp) give us a finished event
-  # so that we can parse it accordingly.
-  def <<(event)
-    name, value, time = event["message"].split(" ")
-    event[name] = value.to_f
-
-    if time != "N"
-      event["@timestamp"] = Time.at(time.to_i).gmtime
-    end
-
-    @queue << event
-  end
-end # class LogStash::Inputs::Graphite
diff --git a/lib/logstash/inputs/imap.rb b/lib/logstash/inputs/imap.rb
deleted file mode 100644
index ec2c9bccbe9..00000000000
--- a/lib/logstash/inputs/imap.rb
+++ /dev/null
@@ -1,157 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "stud/interval"
-require "socket" # for Socket.gethostname
-
-# Read mail from IMAP servers
-#
-# Periodically scans INBOX and moves any read messages
-# to the trash.
-class LogStash::Inputs::IMAP < LogStash::Inputs::Base
-  config_name "imap"
-  milestone 1
-  ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
-
-  default :codec, "plain"
-
-  config :host, :validate => :string, :required => true
-  config :port, :validate => :number
-
-  config :user, :validate => :string, :required => true
-  config :password, :validate => :password, :required => true
-  config :secure, :validate => :boolean, :default => true
-  config :verify_cert, :validate => :boolean, :default => true
-
-  config :fetch_count, :validate => :number, :default => 50
-  config :lowercase_headers, :validate => :boolean, :default => true
-  config :check_interval, :validate => :number, :default => 300
-  config :delete, :validate => :boolean, :default => false
-
-  # For multipart messages, use the first part that has this
-  # content-type as the event message.
-  config :content_type, :validate => :string, :default => "text/plain"
-
-  public
-  def register
-    require "net/imap" # in stdlib
-    require "mail" # gem 'mail'
-
-    if @secure and not @verify_cert
-      @logger.warn("Running IMAP without verifying the certificate may grant attackers unauthorized access to your mailbox or data")
-    end
-
-    if @port.nil?
-      if @secure
-        @port = 993
-      else
-        @port = 143
-      end
-    end
-
-    @content_type_re = Regexp.new("^" + @content_type)
-  end # def register
-
-  def connect
-    sslopt = @secure
-    if @secure and not @verify_cert
-        sslopt = { :verify_mode => OpenSSL::SSL::VERIFY_NONE }
-    end
-    imap = Net::IMAP.new(@host, :port => @port, :ssl => sslopt)
-    imap.login(@user, @password.value)
-    return imap
-  end
-
-  def run(queue)
-    Stud.interval(@check_interval) do
-      check_mail(queue)
-    end
-  end
-
-  def check_mail(queue)
-    # TODO(sissel): handle exceptions happening during runtime:
-    # EOFError, OpenSSL::SSL::SSLError
-    imap = connect
-    imap.select("INBOX")
-    ids = imap.search("NOT SEEN")
-
-    ids.each_slice(@fetch_count) do |id_set|
-      items = imap.fetch(id_set, "RFC822")
-      items.each do |item|
-        next unless item.attr.has_key?("RFC822")
-        mail = Mail.read_from_string(item.attr["RFC822"])
-        queue << parse_mail(mail)
-      end
-
-      imap.store(id_set, '+FLAGS', @delete ? :Deleted : :Seen)
-    end
-
-    imap.close
-    imap.disconnect
-  end # def run
-
-  def parse_mail(mail)
-    # TODO(sissel): What should a multipart message look like as an event?
-    # For now, just take the plain-text part and set it as the message.
-    if mail.parts.count == 0
-      # No multipart message, just use the body as the event text
-      message = mail.body.decoded
-    else
-      # Multipart message; use the first text/plain part we find
-      part = mail.parts.find { |p| p.content_type.match @content_type_re } || mail.parts.first
-      message = part.decoded
-    end
-
-    @codec.decode(message) do |event|
-      # event = LogStash::Event.new("message" => message)
-
-      # Use the 'Date' field as the timestamp
-      event["@timestamp"] = mail.date.to_time.gmtime
-
-      # Add fields: Add message.header_fields { |h| h.name=> h.value }
-      mail.header_fields.each do |header|
-        if @lowercase_headers
-          # 'header.name' can sometimes be a Mail::Multibyte::Chars, get it in
-          # String form
-          name = header.name.to_s.downcase
-        else
-          name = header.name.to_s
-        end
-        # Call .decoded on the header in case it's in encoded-word form.
-        # Details at:
-        #   https://github.com/mikel/mail/blob/master/README.md#encodings
-        #   http://tools.ietf.org/html/rfc2047#section-2
-        value = transcode_to_utf8(header.decoded)
-
-        # Assume we already processed the 'date' above.
-        next if name == "Date"
-
-        case event[name]
-          # promote string to array if a header appears multiple times
-          # (like 'received')
-          when String; event[name] = [event[name], value]
-          when Array; event[name] << value
-          when nil; event[name] = value
-        end
-      end # mail.header_fields.each
-
-      decorate(event)
-      event
-    end
-  end # def handle
-
-  public
-  def teardown
-    $stdin.close
-    finished
-  end # def teardown
-
-  private
-
-  # transcode_to_utf8 is meant for headers transcoding.
-  # the mail gem will set the correct encoding on header strings decoding
-  # and we want to transcode it to utf8
-  def transcode_to_utf8(s)
-    s.encode(Encoding::UTF_8, :invalid => :replace, :undef => :replace)
-  end
-end # class LogStash::Inputs::IMAP
diff --git a/lib/logstash/inputs/invalid_input.rb b/lib/logstash/inputs/invalid_input.rb
deleted file mode 100644
index 30b19d0fa12..00000000000
--- a/lib/logstash/inputs/invalid_input.rb
+++ /dev/null
@@ -1,19 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket" # for Socket.gethostname
-
-class LogStash::Inputs::InvalidInput < LogStash::Inputs::Base
-  config_name "invalid_input"
-  milestone 1
-
-  public
-  def register; end
-
-  def run(queue)
-    event = LogStash::Event.new("message" =>"hello world 1  \xED")
-    decorate(event)
-    queue << event
-    loop do; sleep(1); end
-  end
-end
diff --git a/lib/logstash/inputs/irc.rb b/lib/logstash/inputs/irc.rb
deleted file mode 100644
index 75644184407..00000000000
--- a/lib/logstash/inputs/irc.rb
+++ /dev/null
@@ -1,88 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "thread"
-
-# Read events from an IRC Server.
-#
-class LogStash::Inputs::Irc < LogStash::Inputs::Base
-
-  config_name "irc"
-  milestone 1
-
-  default :codec, "plain"
-
-  # Host of the IRC Server to connect to.
-  config :host, :validate => :string, :required => true
-
-  # Port for the IRC Server
-  config :port, :validate => :number, :default => 6667
-
-  # Set this to true to enable SSL.
-  config :secure, :validate => :boolean, :default => false
-
-  # IRC Nickname
-  config :nick, :validate => :string, :default => "logstash"
-
-  # IRC Username
-  config :user, :validate => :string, :default => "logstash"
-
-  # IRC Real name
-  config :real, :validate => :string, :default => "logstash"
-
-  # IRC Server password
-  config :password, :validate => :password
-
-  # Channels to join and read messages from.
-  #
-  # These should be full channel names including the '#' symbol, such as
-  # "#logstash".
-  #
-  # For passworded channels, add a space and the channel password, such as
-  # "#logstash password".
-  #
-  config :channels, :validate => :array, :required => true
-
-  public
-  def register
-    require "cinch"
-    @irc_queue = Queue.new
-    @logger.info("Connecting to irc server", :host => @host, :port => @port, :nick => @nick, :channels => @channels)
-
-    @bot = Cinch::Bot.new
-    @bot.loggers.clear
-    @bot.configure do |c|
-      c.server = @host
-      c.port = @port
-      c.nick = @nick
-      c.user = @user
-      c.realname = @real
-      c.channels = @channels
-      c.password = @password.value rescue nil
-      c.ssl.use = @secure
-    end
-    queue = @irc_queue
-    @bot.on :channel  do |m|
-      queue << m
-    end
-  end # def register
-
-  public
-  def run(output_queue)
-    Thread.new(@bot) do |bot|
-      bot.start
-    end
-    loop do
-      msg = @irc_queue.pop
-      if msg.user
-        @codec.decode(msg.message) do |event|
-          decorate(event)
-          event["channel"] = msg.channel.to_s
-          event["nick"] = msg.user.nick
-          event["server"] = "#{@host}:#{@port}"
-          output_queue << event
-        end
-      end
-    end
-  end # def run
-end # class LogStash::Inputs::Irc
diff --git a/lib/logstash/inputs/log4j.rb b/lib/logstash/inputs/log4j.rb
deleted file mode 100644
index 86096d97f44..00000000000
--- a/lib/logstash/inputs/log4j.rb
+++ /dev/null
@@ -1,141 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/errors"
-require "logstash/environment"
-require "logstash/namespace"
-require "logstash/util/socket_peer"
-require "socket"
-require "timeout"
-
-# Read events over a TCP socket from a Log4j SocketAppender.
-#
-# Can either accept connections from clients or connect to a server,
-# depending on `mode`. Depending on which `mode` is configured,
-# you need a matching SocketAppender or a SocketHubAppender
-# on the remote side.
-class LogStash::Inputs::Log4j < LogStash::Inputs::Base
-
-  config_name "log4j"
-  milestone 1
-
-  # When mode is `server`, the address to listen on.
-  # When mode is `client`, the address to connect to.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # When mode is `server`, the port to listen on.
-  # When mode is `client`, the port to connect to.
-  config :port, :validate => :number, :default => 4560
-
-  # Read timeout in seconds. If a particular TCP connection is
-  # idle for more than this timeout period, we will assume
-  # it is dead and close it.
-  # If you never want to timeout, use -1.
-  config :data_timeout, :validate => :number, :default => 5
-
-  # Mode to operate in. `server` listens for client connections,
-  # `client` connects to a server.
-  config :mode, :validate => ["server", "client"], :default => "server"
-
-  def initialize(*args)
-    super(*args)
-  end # def initialize
-
-  public
-  def register
-    LogStash::Environment.load_elasticsearch_jars!
-    require "java"
-    require "jruby/serialization"
-
-    begin
-      Java::OrgApacheLog4jSpi.const_get("LoggingEvent")
-    rescue
-      raise(LogStash::PluginLoadingError, "Log4j java library not loaded")
-    end
-
-    if server?
-      @logger.info("Starting Log4j input listener", :address => "#{@host}:#{@port}")
-      @server_socket = TCPServer.new(@host, @port)
-    end
-    @logger.info("Log4j input")
-  end # def register
-
-  private
-  def handle_socket(socket, output_queue)
-    begin
-      # JRubyObjectInputStream uses JRuby class path to find the class to de-serialize to
-      ois = JRubyObjectInputStream.new(java.io.BufferedInputStream.new(socket.to_inputstream))
-      loop do
-        # NOTE: log4j_obj is org.apache.log4j.spi.LoggingEvent
-        log4j_obj = ois.readObject
-        event = LogStash::Event.new("message" => log4j_obj.getRenderedMessage)
-        decorate(event)
-        event["host"] = socket.peer
-        event["path"] = log4j_obj.getLoggerName
-        event["priority"] = log4j_obj.getLevel.toString
-        event["logger_name"] = log4j_obj.getLoggerName
-        event["thread"] = log4j_obj.getThreadName
-        event["class"] = log4j_obj.getLocationInformation.getClassName
-        event["file"] = log4j_obj.getLocationInformation.getFileName + ":" + log4j_obj.getLocationInformation.getLineNumber
-        event["method"] = log4j_obj.getLocationInformation.getMethodName
-        event["NDC"] = log4j_obj.getNDC if log4j_obj.getNDC
-        event["stack_trace"] = log4j_obj.getThrowableStrRep.to_a.join("\n") if log4j_obj.getThrowableInformation
-
-        # Add the MDC context properties to '@fields'
-        if log4j_obj.getProperties
-          log4j_obj.getPropertyKeySet.each do |key|
-            event[key] = log4j_obj.getProperty(key)
-          end
-        end
-
-        output_queue << event
-      end # loop do
-    rescue => e
-      @logger.debug("Closing connection", :client => socket.peer,
-                    :exception => e)
-    rescue Timeout::Error
-      @logger.debug("Closing connection after read timeout",
-                    :client => socket.peer)
-    end # begin
-  ensure
-    begin
-      socket.close
-    rescue IOError
-      pass
-    end # begin
-  end
-
-  private
-  def server?
-    @mode == "server"
-  end # def server?
-
-  private
-  def readline(socket)
-    line = socket.readline
-  end # def readline
-
-  public
-  def run(output_queue)
-    if server?
-      loop do
-        # Start a new thread for each connection.
-        Thread.start(@server_socket.accept) do |s|
-          # TODO(sissel): put this block in its own method.
-
-          # monkeypatch a 'peer' method onto the socket.
-          s.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-          @logger.debug("Accepted connection", :client => s.peer,
-                        :server => "#{@host}:#{@port}")
-          handle_socket(s, output_queue)
-        end # Thread.start
-      end # loop
-    else
-      loop do
-        client_socket = TCPSocket.new(@host, @port)
-        client_socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-        @logger.debug("Opened connection", :client => "#{client_socket.peer}")
-        handle_socket(client_socket, output_queue)
-      end # loop
-    end
-  end # def run
-end # class LogStash::Inputs::Log4j
diff --git a/lib/logstash/inputs/lumberjack.rb b/lib/logstash/inputs/lumberjack.rb
deleted file mode 100644
index c4c5cfc1fcd..00000000000
--- a/lib/logstash/inputs/lumberjack.rb
+++ /dev/null
@@ -1,54 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# Receive events using the lumberjack protocol.
-#
-# This is mainly to receive events shipped with lumberjack,
-# <http://github.com/jordansissel/lumberjack>, now represented primarily via the
-# Logstash-forwarder[https://github.com/elasticsearch/logstash-forwarder].
-class LogStash::Inputs::Lumberjack < LogStash::Inputs::Base
-
-  config_name "lumberjack"
-  milestone 1
-
-  default :codec, "plain"
-
-  # The IP address to listen on.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to listen on.
-  config :port, :validate => :number, :required => true
-
-  # SSL certificate to use.
-  config :ssl_certificate, :validate => :path, :required => true
-
-  # SSL key to use.
-  config :ssl_key, :validate => :path, :required => true
-
-  # SSL key passphrase to use.
-  config :ssl_key_passphrase, :validate => :password
-
-  # TODO(sissel): Add CA to authenticate clients with.
-
-  public
-  def register
-    require "lumberjack/server"
-
-    @logger.info("Starting lumberjack input listener", :address => "#{@host}:#{@port}")
-    @lumberjack = Lumberjack::Server.new(:address => @host, :port => @port,
-      :ssl_certificate => @ssl_certificate, :ssl_key => @ssl_key,
-      :ssl_key_passphrase => @ssl_key_passphrase)
-  end # def register
-
-  public
-  def run(output_queue)
-    @lumberjack.run do |l|
-      @codec.decode(l.delete("line")) do |event|
-        decorate(event)
-        l.each { |k,v| event[k] = v; v.force_encoding(Encoding::UTF_8) }
-        output_queue << event
-      end
-    end
-  end # def run
-end # class LogStash::Inputs::Lumberjack
diff --git a/lib/logstash/inputs/pipe.rb b/lib/logstash/inputs/pipe.rb
deleted file mode 100644
index 143b8ef1418..00000000000
--- a/lib/logstash/inputs/pipe.rb
+++ /dev/null
@@ -1,57 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket" # for Socket.gethostname
-
-# Stream events from a long running command pipe.
-#
-# By default, each event is assumed to be one line. If you
-# want to join lines, you'll want to use the multiline filter.
-#
-class LogStash::Inputs::Pipe < LogStash::Inputs::Base
-  config_name "pipe"
-  milestone 1
-
-  # TODO(sissel): This should switch to use the 'line' codec by default
-  # once we switch away from doing 'readline'
-  default :codec, "plain"
-
-  # Command to run and read events from, one line at a time.
-  #
-  # Example:
-  #
-  #    command => "echo hello world"
-  config :command, :validate => :string, :required => true
-
-  public
-  def register
-    @logger.info("Registering pipe input", :command => @command)
-  end # def register
-
-  public
-  def run(queue)
-    loop do
-      begin
-        @pipe = IO.popen(@command, mode="r")
-        hostname = Socket.gethostname
-
-        @pipe.each do |line|
-          line = line.chomp
-          source = "pipe://#{hostname}/#{@command}"
-          @logger.debug? && @logger.debug("Received line", :command => @command, :line => line)
-          @codec.decode(line) do |event|
-            event["host"] = hostname
-            event["command"] = @command
-            decorate(event)
-            queue << event
-          end
-        end
-      rescue Exception => e
-        @logger.error("Exception while running command", :e => e, :backtrace => e.backtrace)
-      end
-
-      # Keep running the command forever.
-      sleep(10)
-    end
-  end # def run
-end # class LogStash::Inputs::Pipe
diff --git a/lib/logstash/inputs/rabbitmq.rb b/lib/logstash/inputs/rabbitmq.rb
deleted file mode 100644
index 41924738874..00000000000
--- a/lib/logstash/inputs/rabbitmq.rb
+++ /dev/null
@@ -1,128 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/threadable"
-require "logstash/namespace"
-
-# Pull events from a RabbitMQ exchange.
-#
-# The default settings will create an entirely transient queue and listen for all messages by default.
-# If you need durability or any other advanced settings, please set the appropriate options
-#
-# This has been tested with Bunny 0.9.x, which supports RabbitMQ 2.x and 3.x. You can
-# find links to both here:
-#
-# * RabbitMQ - <http://www.rabbitmq.com/>
-# * March Hare: <http://rubymarchhare.info>
-# * Bunny - <https://github.com/ruby-amqp/bunny>
-class LogStash::Inputs::RabbitMQ < LogStash::Inputs::Threadable
-
-  config_name "rabbitmq"
-  milestone 1
-
-  #
-  # Connection
-  #
-
-  # RabbitMQ server address
-  config :host, :validate => :string, :required => true
-
-  # RabbitMQ port to connect on
-  config :port, :validate => :number, :default => 5672
-
-  # RabbitMQ username
-  config :user, :validate => :string, :default => "guest"
-
-  # RabbitMQ password
-  config :password, :validate => :password, :default => "guest"
-
-  # The vhost to use. If you don't know what this is, leave the default.
-  config :vhost, :validate => :string, :default => "/"
-
-  # Enable or disable SSL
-  config :ssl, :validate => :boolean, :default => false
-
-  # Validate SSL certificate
-  config :verify_ssl, :validate => :boolean, :default => false
-
-  # Enable or disable logging
-  config :debug, :validate => :boolean, :default => false, :deprecated => "Use the logstash --debug flag for this instead."
-
-
-
-  #
-  # Queue & Consumer
-  #
-
-  # The name of the queue Logstash will consume events from.
-  config :queue, :validate => :string, :default => ""
-
-  # Is this queue durable? (aka; Should it survive a broker restart?)
-  config :durable, :validate => :boolean, :default => false
-
-  # Should the queue be deleted on the broker when the last consumer
-  # disconnects? Set this option to 'false' if you want the queue to remain
-  # on the broker, queueing up messages until a consumer comes along to
-  # consume them.
-  config :auto_delete, :validate => :boolean, :default => false
-
-  # Is the queue exclusive? Exclusive queues can only be used by the connection
-  # that declared them and will be deleted when it is closed (e.g. due to a Logstash
-  # restart).
-  config :exclusive, :validate => :boolean, :default => false
-
-  # Extra queue arguments as an array.
-  # To make a RabbitMQ queue mirrored, use: {"x-ha-policy" => "all"}
-  config :arguments, :validate => :array, :default => {}
-
-  # Prefetch count. Number of messages to prefetch
-  config :prefetch_count, :validate => :number, :default => 256
-
-  # Enable message acknowledgement
-  config :ack, :validate => :boolean, :default => true
-
-  # Passive queue creation? Useful for checking queue existance without modifying server state
-  config :passive, :validate => :boolean, :default => false
-
-
-
-  #
-  # (Optional) Exchange binding
-  #
-
-  # Optional.
-  #
-  # The name of the exchange to bind the queue to.
-  config :exchange, :validate => :string
-
-  # Optional.
-  #
-  # The routing key to use when binding a queue to the exchange.
-  # This is only relevant for direct or topic exchanges.
-  #
-  # * Routing keys are ignored on fanout exchanges.
-  # * Wildcards are not valid on direct exchanges.
-  config :key, :validate => :string, :default => "logstash"
-
-
-  def initialize(params)
-    params["codec"] = "json" if !params["codec"]
-
-    super
-  end
-
-  # Use March Hare on JRuby to avoid IO#select CPU spikes
-  # (see github.com/ruby-amqp/bunny/issues/95).
-  #
-  # On MRI, use Bunny.
-  #
-  # See http://rubybunny.info and http://rubymarchhare.info
-  # for the docs.
-  if RUBY_ENGINE == "jruby"
-    require "logstash/inputs/rabbitmq/march_hare"
-
-    include MarchHareImpl
-  else
-    require "logstash/inputs/rabbitmq/bunny"
-
-    include BunnyImpl
-  end
-end # class LogStash::Inputs::RabbitMQ
diff --git a/lib/logstash/inputs/rabbitmq/bunny.rb b/lib/logstash/inputs/rabbitmq/bunny.rb
deleted file mode 100644
index 4594143438a..00000000000
--- a/lib/logstash/inputs/rabbitmq/bunny.rb
+++ /dev/null
@@ -1,119 +0,0 @@
-# encoding: utf-8
-class LogStash::Inputs::RabbitMQ
-  module BunnyImpl
-    def register
-      require "bunny"
-
-      @vhost       ||= Bunny::DEFAULT_HOST
-      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
-      @port        ||= AMQ::Protocol::DEFAULT_PORT
-      @routing_key ||= "#"
-
-      @settings = {
-        :vhost => @vhost,
-        :host  => @host,
-        :port  => @port,
-        :automatically_recover => false
-      }
-      @settings[:user]      = @user || Bunny::DEFAULT_USER
-      @settings[:pass]      = if @password
-                                @password.value
-                              else
-                                Bunny::DEFAULT_PASSWORD
-                              end
-
-      @settings[:log_level] = if @debug || @logger.debug?
-                                :debug
-                              else
-                                :error
-                              end
-
-      @settings[:tls]        = @ssl if @ssl
-      @settings[:verify_ssl] = @verify_ssl if @verify_ssl
-
-      proto                  = if @ssl
-                                 "amqp"
-                               else
-                                 "amqps"
-                               end
-      @connection_url        = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
-
-      @logger.info("Registering input #{@connection_url}")
-    end
-
-    def run(output_queue)
-      @output_queue = output_queue
-
-      begin
-        setup
-        consume
-      rescue Bunny::NetworkFailure, Bunny::ConnectionClosedError, Bunny::ConnectionLevelException, Bunny::TCPConnectionFailed => e
-        n = Bunny::Session::DEFAULT_NETWORK_RECOVERY_INTERVAL * 2
-
-        # Because we manually reconnect instead of letting Bunny
-        # handle failures,
-        # make sure we don't leave any consumer work pool
-        # threads behind. MK.
-        @ch.maybe_kill_consumer_work_pool!
-        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...")
-
-        sleep n
-        retry
-      end
-    end
-
-    def teardown
-      @consumer.cancel
-      @q.delete unless @durable
-
-      @ch.close   if @ch && @ch.open?
-      @conn.close if @conn && @conn.open?
-
-      finished
-    end
-
-    def setup
-      @conn = Bunny.new(@settings)
-
-      @logger.debug("Connecting to RabbitMQ. Settings: #{@settings.inspect}, queue: #{@queue.inspect}")
-      return if terminating?
-      @conn.start
-
-      @ch = @conn.create_channel.tap do |ch|
-        ch.prefetch(@prefetch_count)
-      end
-      @logger.info("Connected to RabbitMQ at #{@settings[:host]}")
-
-      @arguments_hash = Hash[*@arguments]
-
-      @q = @ch.queue(@queue,
-                     :durable     => @durable,
-                     :auto_delete => @auto_delete,
-                     :exclusive   => @exclusive,
-                     :passive     => @passive,
-                     :arguments   => @arguments)
-
-      # exchange binding is optional for the input
-      if @exchange
-        @q.bind(@exchange, :routing_key => @key)
-      end
-    end
-
-    def consume
-      @logger.info("Will consume events from queue #{@q.name}")
-
-      # we both need to block the caller in Bunny::Queue#subscribe and have
-      # a reference to the consumer so that we can cancel it, so
-      # a consumer manually. MK.
-      @consumer = Bunny::Consumer.new(@ch, @q)
-      @q.subscribe(:manual_ack => @ack, :block => true) do |delivery_info, properties, data|
-        @codec.decode(data) do |event|
-          decorate(event)
-          @output_queue << event
-        end
-
-        @ch.acknowledge(delivery_info.delivery_tag) if @ack
-      end
-    end
-  end # BunnyImpl
-end
diff --git a/lib/logstash/inputs/rabbitmq/hot_bunnies.rb b/lib/logstash/inputs/rabbitmq/hot_bunnies.rb
deleted file mode 100644
index bc64df1c73c..00000000000
--- a/lib/logstash/inputs/rabbitmq/hot_bunnies.rb
+++ /dev/null
@@ -1 +0,0 @@
-require "logstash/inputs/rabbitmq/march_hare"
diff --git a/lib/logstash/inputs/rabbitmq/march_hare.rb b/lib/logstash/inputs/rabbitmq/march_hare.rb
deleted file mode 100644
index 6a80d4de2af..00000000000
--- a/lib/logstash/inputs/rabbitmq/march_hare.rb
+++ /dev/null
@@ -1,130 +0,0 @@
-# encoding: utf-8
-class LogStash::Inputs::RabbitMQ
-  # MarchHare-based implementation for JRuby
-  module MarchHareImpl
-    def register
-      require "hot_bunnies"
-      require "java"
-
-      @vhost       ||= "127.0.0.1"
-      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
-      @port        ||= 5672
-      @key         ||= "#"
-
-      @settings = {
-        :vhost => @vhost,
-        :host  => @host,
-        :port  => @port,
-        :user  => @user,
-        :automatic_recovery => false
-      }
-      @settings[:pass]      = @password.value if @password
-      @settings[:tls]       = @ssl if @ssl
-
-      proto                 = if @ssl
-                                "amqp"
-                              else
-                                "amqps"
-                              end
-      @connection_url       = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
-
-      @logger.info("Registering input #{@connection_url}")
-    end
-
-    def run(output_queue)
-      @output_queue          = output_queue
-      @break_out_of_the_loop = java.util.concurrent.atomic.AtomicBoolean.new(false)
-
-      # MarchHare does not raise exceptions when connection goes down with a blocking
-      # consumer running (it uses callbacks, as the RabbitMQ Java client does).
-      #
-      # However, MarchHare::Channel will make sure to unblock all blocking consumers
-      # on any internal shutdown, so #consume will return and another loop iteration
-      # will run.
-      #
-      # This is very similar to how the Bunny implementation works and is sufficient
-      # for our needs: it recovers successfully after RabbitMQ is kill -9ed, the
-      # network device is shut down, etc. MK.
-      until @break_out_of_the_loop.get do
-        begin
-          setup
-          consume
-        rescue MarchHare::Exception, java.lang.Throwable, com.rabbitmq.client.AlreadyClosedException => e
-          n = 10
-          @logger.error("RabbitMQ connection error: #{e}. Will reconnect in #{n} seconds...")
-
-          sleep n
-          retry
-        rescue LogStash::ShutdownSignal => ss
-          shutdown_consumer
-        end
-
-        n = 10
-        @logger.error("RabbitMQ connection error: #{e}. Will reconnect in #{n} seconds...")
-      end
-    end
-
-    def teardown
-      shutdown_consumer
-      @q.delete unless @durable
-
-      @ch.close         if @ch && @ch.open?
-      @connection.close if @connection && @connection.open?
-
-      finished
-    end
-
-    #
-    # Implementation
-    #
-
-    protected
-
-    def setup
-      return if terminating?
-
-      @conn = MarchHare.connect(@settings)
-      @logger.info("Connected to RabbitMQ #{@connection_url}")
-
-      @ch          = @conn.create_channel.tap do |ch|
-        ch.prefetch = @prefetch_count
-      end
-
-      @arguments_hash = Hash[*@arguments]
-
-      @q = @ch.queue(@queue,
-        :durable     => @durable,
-        :auto_delete => @auto_delete,
-        :exclusive   => @exclusive,
-        :passive     => @passive,
-        :arguments   => @arguments)
-
-      # exchange binding is optional for the input
-      if @exchange
-        @q.bind(@exchange, :routing_key => @key)
-      end
-    end
-
-    def consume
-      return if terminating?
-
-      # we manually build a consumer here to be able to keep a reference to it
-      # in an @ivar even though we use a blocking version of HB::Queue#subscribe
-      @consumer = @q.build_consumer(:block => true) do |metadata, data|
-        @codec.decode(data) do |event|
-          decorate(event)
-          @output_queue << event if event
-        end
-        @ch.ack(metadata.delivery_tag) if @ack
-      end
-      @q.subscribe_with(@consumer, :manual_ack => @ack, :block => true)
-    end
-
-    def shutdown_consumer
-      @break_out_of_the_loop.set(true)
-
-      @consumer.cancel
-      @consumer.gracefully_shut_down
-    end
-  end # MarchHareImpl
-end
diff --git a/lib/logstash/inputs/redis.rb b/lib/logstash/inputs/redis.rb
deleted file mode 100644
index 8884f55d91c..00000000000
--- a/lib/logstash/inputs/redis.rb
+++ /dev/null
@@ -1,266 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/inputs/threadable"
-require "logstash/namespace"
-
-# This input will read events from a Redis instance; it supports both Redis channels and lists.
-# The list command (BLPOP) used by Logstash is supported in Redis v1.3.1+, and
-# the channel commands used by Logstash are found in Redis v1.3.8+. 
-# While you may be able to make these Redis versions work, the best performance
-# and stability will be found in more recent stable versions.  Versions 2.6.0+
-# are recommended.
-#
-# For more information about Redis, see <http://redis.io/>
-#
-# `batch_count` note: If you use the `batch_count` setting, you *must* use a Redis version 2.6.0 or
-# newer. Anything older does not support the operations used by batching.
-#
-class LogStash::Inputs::Redis < LogStash::Inputs::Threadable
-  config_name "redis"
-  milestone 2
-
-  default :codec, "json"
-
-  # The `name` configuration is used for logging in case there are multiple instances.
-  # This feature has no real function and will be removed in future versions.
-  config :name, :validate => :string, :default => "default", :deprecated => true
-
-  # The hostname of your Redis server.
-  config :host, :validate => :string, :default => "127.0.0.1"
-
-  # The port to connect on.
-  config :port, :validate => :number, :default => 6379
-
-  # The Redis database number.
-  config :db, :validate => :number, :default => 0
-
-  # Initial connection timeout in seconds.
-  config :timeout, :validate => :number, :default => 5
-
-  # Password to authenticate with. There is no authentication by default.
-  config :password, :validate => :password
-
-  # The name of the Redis queue (we'll use BLPOP against this).
-  # TODO: remove soon.
-  config :queue, :validate => :string, :deprecated => true
-
-  # The name of a Redis list or channel.
-  # TODO: change required to true
-  config :key, :validate => :string, :required => false
-
-  # Specify either list or channel.  If `redis\_type` is `list`, then we will BLPOP the
-  # key.  If `redis\_type` is `channel`, then we will SUBSCRIBE to the key.
-  # If `redis\_type` is `pattern_channel`, then we will PSUBSCRIBE to the key.
-  # TODO: change required to true
-  config :data_type, :validate => [ "list", "channel", "pattern_channel" ], :required => false
-
-  # The number of events to return from Redis using EVAL.
-  config :batch_count, :validate => :number, :default => 1
-
-  public
-  def register
-    require 'redis'
-    @redis = nil
-    @redis_url = "redis://#{@password}@#{@host}:#{@port}/#{@db}"
-
-    # TODO remove after setting key and data_type to true
-    if @queue
-      if @key or @data_type
-        raise RuntimeError.new(
-          "Cannot specify queue parameter and key or data_type"
-        )
-      end
-      @key = @queue
-      @data_type = 'list'
-    end
-
-    if not @key or not @data_type
-      raise RuntimeError.new(
-        "Must define queue, or key and data_type parameters"
-      )
-    end
-    # end TODO
-
-    @logger.info("Registering Redis", :identity => identity)
-  end # def register
-
-  # A string used to identify a Redis instance in log messages
-  # TODO(sissel): Use instance variables for this once the @name config
-  # option is removed.
-  private
-  def identity
-    @name || "#{@redis_url} #{@data_type}:#{@key}"
-  end
-
-  private
-  def connect
-    redis = Redis.new(
-      :host => @host,
-      :port => @port,
-      :timeout => @timeout,
-      :db => @db,
-      :password => @password.nil? ? nil : @password.value
-    )
-    load_batch_script(redis) if @data_type == 'list' && (@batch_count > 1)
-    return redis
-  end # def connect
-
-  private
-  def load_batch_script(redis)
-    #A Redis Lua EVAL script to fetch a count of keys
-    #in case count is bigger than current items in queue whole queue will be returned without extra nil values
-    redis_script = <<EOF
-          local i = tonumber(ARGV[1])
-          local res = {}
-          local length = redis.call('llen',KEYS[1])
-          if length < i then i = length end
-          while (i > 0) do
-            local item = redis.call("lpop", KEYS[1])
-            if (not item) then
-              break
-            end
-            table.insert(res, item)
-            i = i-1
-          end
-          return res
-EOF
-    @redis_script_sha = redis.script(:load, redis_script)
-  end
-
-  private
-  def queue_event(msg, output_queue)
-    begin
-      @codec.decode(msg) do |event|
-        decorate(event)
-        output_queue << event
-      end
-    rescue => e # parse or event creation error
-      @logger.error("Failed to create event", :message => msg, :exception => e,
-                    :backtrace => e.backtrace);
-    end
-  end
-
-  private
-  def list_listener(redis, output_queue)
-
-    # blpop returns the 'key' read from as well as the item result
-    # we only care about the result (2nd item in the list).
-    item = redis.blpop(@key, 0)[1]
-
-    # blpop failed or .. something?
-    # TODO(sissel): handle the error
-    return if item.nil?
-    queue_event(item, output_queue)
-
-    # If @batch_count is 1, there's no need to continue.
-    return if @batch_count == 1
-
-    begin
-      redis.evalsha(@redis_script_sha, [@key], [@batch_count-1]).each do |item|
-        queue_event(item, output_queue)
-      end
-
-      # Below is a commented-out implementation of 'batch fetch'
-      # using pipelined LPOP calls. This in practice has been observed to
-      # perform exactly the same in terms of event throughput as
-      # the evalsha method. Given that the EVALSHA implementation uses
-      # one call to Redis instead of N (where N == @batch_count) calls,
-      # I decided to go with the 'evalsha' method of fetching N items
-      # from Redis in bulk.
-      #redis.pipelined do
-        #error, item = redis.lpop(@key)
-        #(@batch_count-1).times { redis.lpop(@key) }
-      #end.each do |item|
-        #queue_event(item, output_queue) if item
-      #end
-      # --- End commented out implementation of 'batch fetch'
-    rescue Redis::CommandError => e
-      if e.to_s =~ /NOSCRIPT/ then
-        @logger.warn("Redis may have been restarted, reloading Redis batch EVAL script", :exception => e);
-        load_batch_script(redis)
-        retry
-      else
-        raise e
-      end
-    end
-  end
-
-  private
-  def channel_listener(redis, output_queue)
-    redis.subscribe @key do |on|
-      on.subscribe do |channel, count|
-        @logger.info("Subscribed", :channel => channel, :count => count)
-      end
-
-      on.message do |channel, message|
-        queue_event message, output_queue
-      end
-
-      on.unsubscribe do |channel, count|
-        @logger.info("Unsubscribed", :channel => channel, :count => count)
-      end
-    end
-  end
-
-  private
-  def pattern_channel_listener(redis, output_queue)
-    redis.psubscribe @key do |on|
-      on.psubscribe do |channel, count|
-        @logger.info("Subscribed", :channel => channel, :count => count)
-      end
-
-      on.pmessage do |ch, event, message|
-        queue_event message, output_queue
-      end
-
-      on.punsubscribe do |channel, count|
-        @logger.info("Unsubscribed", :channel => channel, :count => count)
-      end
-    end
-  end
-
-  # Since both listeners have the same basic loop, we've abstracted the outer
-  # loop.
-  private
-  def listener_loop(listener, output_queue)
-    while !finished?
-      begin
-        @redis ||= connect
-        self.send listener, @redis, output_queue
-      rescue Redis::CannotConnectError => e
-        @logger.warn("Redis connection problem", :exception => e)
-        sleep 1
-        @redis = connect
-      rescue => e # Redis error
-        @logger.warn("Failed to get event from Redis", :name => @name,
-                     :exception => e, :backtrace => e.backtrace)
-        raise e
-      end
-    end # while !finished?
-  end # listener_loop
-
-  public
-  def run(output_queue)
-    if @data_type == 'list'
-      listener_loop :list_listener, output_queue
-    elsif @data_type == 'channel'
-      listener_loop :channel_listener, output_queue
-    else
-      listener_loop :pattern_channel_listener, output_queue
-    end
-  end # def run
-
-  public
-  def teardown
-    if @data_type == 'channel' and @redis
-      @redis.unsubscribe
-      @redis.quit
-      @redis = nil
-    end
-    if @data_type == 'pattern_channel' and @redis
-      @redis.punsubscribe
-      @redis.quit
-      @redis = nil
-    end
-  end
-end # class LogStash::Inputs::Redis
diff --git a/lib/logstash/inputs/s3.rb b/lib/logstash/inputs/s3.rb
deleted file mode 100644
index 4f30937367b..00000000000
--- a/lib/logstash/inputs/s3.rb
+++ /dev/null
@@ -1,279 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-require "time"
-require "tmpdir"
-
-# Stream events from files from a S3 bucket.
-#
-# Each line from each file generates an event.
-# Files ending in '.gz' are handled as gzip'ed files.
-class LogStash::Inputs::S3 < LogStash::Inputs::Base
-  config_name "s3"
-  milestone 1
-
-  # TODO(sissel): refactor to use 'line' codec (requires removing both gzip
-  # support and readline usage). Support gzip through a gzip codec! ;)
-  default :codec, "plain"
-
-  # The credentials of the AWS account used to access the bucket.
-  # Credentials can be specified:
-  # - As an ["id","secret"] array
-  # - As a path to a file containing AWS_ACCESS_KEY_ID=... and AWS_SECRET_ACCESS_KEY=...
-  # - In the environment (variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)
-  config :credentials, :validate => :array, :default => nil
-
-  # The name of the S3 bucket.
-  config :bucket, :validate => :string, :required => true
-
-  # The AWS region for your bucket.
-  config :region, :validate => ["us-east-1", "us-west-1", "us-west-2",
-                                "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"],
-                                :deprecated => "'region' has been deprecated in favor of 'region_endpoint'"
-
-  # The AWS region for your bucket.
-  config :region_endpoint, :validate => ["us-east-1", "us-west-1", "us-west-2",
-                                "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
-
-  # If specified, the prefix the filenames in the bucket must match (not a regexp)
-  config :prefix, :validate => :string, :default => nil
-
-  # Where to write the since database (keeps track of the date
-  # the last handled file was added to S3). The default will write
-  # sincedb files to some path matching "$HOME/.sincedb*"
-  config :sincedb_path, :validate => :string, :default => nil
-
-  # Name of a S3 bucket to backup processed files to.
-  config :backup_to_bucket, :validate => :string, :default => nil
-
-  # Path of a local directory to backup processed files to.
-  config :backup_to_dir, :validate => :string, :default => nil
-
-  # Whether to delete processed files from the original bucket.
-  config :delete, :validate => :boolean, :default => false
-
-  # Interval to wait between to check the file list again after a run is finished.
-  # Value is in seconds.
-  config :interval, :validate => :number, :default => 60
-
-  public
-  def register
-    require "digest/md5"
-    require "aws-sdk"
-
-    @region_endpoint = @region if @region && !@region.empty?
-
-    @logger.info("Registering s3 input", :bucket => @bucket, :region_endpoint => @region_endpoint)
-
-    if @credentials.nil?
-      @access_key_id = ENV['AWS_ACCESS_KEY_ID']
-      @secret_access_key = ENV['AWS_SECRET_ACCESS_KEY']
-    elsif @credentials.is_a? Array
-      if @credentials.length ==1
-        File.open(@credentials[0]) { |f| f.each do |line|
-          unless (/^\#/.match(line))
-            if(/\s*=\s*/.match(line))
-              param, value = line.split('=', 2)
-              param = param.chomp().strip()
-              value = value.chomp().strip()
-              if param.eql?('AWS_ACCESS_KEY_ID')
-                @access_key_id = value
-              elsif param.eql?('AWS_SECRET_ACCESS_KEY')
-                @secret_access_key = value
-              end
-            end
-          end
-        end
-        }
-      elsif @credentials.length == 2
-        @access_key_id = @credentials[0]
-        @secret_access_key = @credentials[1]
-      else
-        raise ArgumentError.new('Credentials must be of the form "/path/to/file" or ["id", "secret"]')
-      end
-    end
-    if @access_key_id.nil? or @secret_access_key.nil?
-      raise ArgumentError.new('Missing AWS credentials')
-    end
-
-    if @bucket.nil?
-      raise ArgumentError.new('Missing AWS bucket')
-    end
-
-    if @sincedb_path.nil?
-      if ENV['HOME'].nil?
-        raise ArgumentError.new('No HOME or sincedb_path set')
-      end
-      @sincedb_path = File.join(ENV["HOME"], ".sincedb_" + Digest::MD5.hexdigest("#{@bucket}+#{@prefix}"))
-    end
-
-    s3 = AWS::S3.new(
-      :access_key_id => @access_key_id,
-      :secret_access_key => @secret_access_key,
-      :region => @region_endpoint
-    )
-
-    @s3bucket = s3.buckets[@bucket]
-
-    unless @backup_to_bucket.nil?
-      @backup_bucket = s3.buckets[@backup_to_bucket]
-      unless @backup_bucket.exists?
-        s3.buckets.create(@backup_to_bucket)
-      end
-    end
-
-    unless @backup_to_dir.nil?
-      Dir.mkdir(@backup_to_dir, 0700) unless File.exists?(@backup_to_dir)
-    end
-
-  end # def register
-
-  public
-  def run(queue)
-    loop do
-      process_new(queue)
-      sleep(@interval)
-    end
-    finished
-  end # def run
-
-  private
-  def process_new(queue, since=nil)
-
-    if since.nil?
-        since = sincedb_read()
-    end
-
-    objects = list_new(since)
-    objects.each do |k|
-      @logger.debug("S3 input processing", :bucket => @bucket, :key => k)
-      lastmod = @s3bucket.objects[k].last_modified
-      process_log(queue, k)
-      sincedb_write(lastmod)
-    end
-
-  end # def process_new
-
-  private
-  def list_new(since=nil)
-
-    if since.nil?
-      since = Time.new(0)
-    end
-
-    objects = {}
-    @s3bucket.objects.with_prefix(@prefix).each do |log|
-      if log.last_modified > since
-        objects[log.key] = log.last_modified
-      end
-    end
-
-    return sorted_objects = objects.keys.sort {|a,b| objects[a] <=> objects[b]}
-
-  end # def list_new
-
-  private
-  def process_log(queue, key)
-
-    object = @s3bucket.objects[key]
-    tmp = Dir.mktmpdir("logstash-")
-    begin
-      filename = File.join(tmp, File.basename(key))
-      File.open(filename, 'wb') do |s3file|
-        object.read do |chunk|
-          s3file.write(chunk)
-        end
-      end
-      process_local_log(queue, filename)
-      unless @backup_to_bucket.nil?
-        backup_object = @backup_bucket.objects[key]
-        backup_object.write(Pathname.new(filename))
-      end
-      unless @backup_to_dir.nil?
-        FileUtils.cp(filename, @backup_to_dir)
-      end
-      if @delete
-        object.delete()
-      end
-    end
-    FileUtils.remove_entry_secure(tmp, force=true)
-
-  end # def process_log
-
-  private
-  def process_local_log(queue, filename)
-
-    metadata = {
-      :version => nil,
-      :format => nil,
-    }
-    File.open(filename) do |file|
-      if filename.end_with?('.gz')
-        gz = Zlib::GzipReader.new(file)
-        gz.each_line do |line|
-          metadata = process_line(queue, metadata, line)
-        end
-      else
-        file.each do |line|
-          metadata = process_line(queue, metadata, line)
-        end
-      end
-    end
-
-  end # def process_local_log
-
-  private
-  def process_line(queue, metadata, line)
-
-    if /#Version: .+/.match(line)
-      junk, version = line.strip().split(/#Version: (.+)/)
-      unless version.nil?
-        metadata[:version] = version
-      end
-    elsif /#Fields: .+/.match(line)
-      junk, format = line.strip().split(/#Fields: (.+)/)
-      unless format.nil?
-        metadata[:format] = format
-      end
-    else
-      @codec.decode(line) do |event|
-        decorate(event)
-        unless metadata[:version].nil?
-          event["cloudfront_version"] = metadata[:version]
-        end
-        unless metadata[:format].nil?
-          event["cloudfront_fields"] = metadata[:format]
-        end
-        queue << event
-      end
-    end
-    return metadata
-
-  end # def process_line
-
-  private
-  def sincedb_read()
-
-    if File.exists?(@sincedb_path)
-      since = Time.parse(File.read(@sincedb_path).chomp.strip)
-    else
-      since = Time.new(0)
-    end
-    return since
-
-  end # def sincedb_read
-
-  private
-  def sincedb_write(since=nil)
-
-    if since.nil?
-      since = Time.now()
-    end
-    File.open(@sincedb_path, 'w') { |file| file.write(since.to_s) }
-
-  end # def sincedb_write
-
-end # class LogStash::Inputs::S3
diff --git a/lib/logstash/inputs/snmptrap.rb b/lib/logstash/inputs/snmptrap.rb
deleted file mode 100644
index 63699f1eed4..00000000000
--- a/lib/logstash/inputs/snmptrap.rb
+++ /dev/null
@@ -1,87 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# Read snmp trap messages as events
-#
-# Resulting @message looks like :
-#   #<SNMP::SNMPv1_Trap:0x6f1a7a4 @varbind_list=[#<SNMP::VarBind:0x2d7bcd8f @value="teststring", 
-#   @name=[1.11.12.13.14.15]>], @timestamp=#<SNMP::TimeTicks:0x1af47e9d @value=55>, @generic_trap=6, 
-#   @enterprise=[1.2.3.4.5.6], @source_ip="127.0.0.1", @agent_addr=#<SNMP::IpAddress:0x29a4833e @value="\xC0\xC1\xC2\xC3">, 
-#   @specific_trap=99>
-#
-
-class LogStash::Inputs::Snmptrap < LogStash::Inputs::Base
-  config_name "snmptrap"
-  milestone 1
-
-  # The address to listen on
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to listen on. Remember that ports less than 1024 (privileged
-  # ports) may require root to use. hence the default of 1062.
-  config :port, :validate => :number, :default => 1062
-
-  # SNMP Community String to listen for.
-  config :community, :validate => :string, :default => "public"
-
-  # directory of YAML MIB maps  (same format ruby-snmp uses)
-  config :yamlmibdir, :validate => :string
-
-  def initialize(*args)
-    super(*args)
-  end # def initialize
-
-  public
-  def register
-    require "snmp"
-    @snmptrap = nil
-    if @yamlmibdir
-      @logger.info("checking #{@yamlmibdir} for MIBs")
-      Dir["#{@yamlmibdir}/*.yaml"].each do |yamlfile|
-        mib_name = File.basename(yamlfile, ".*")
-        @yaml_mibs ||= []
-        @yaml_mibs << mib_name
-      end
-      @logger.info("found MIBs: #{@yaml_mibs.join(',')}") if @yaml_mibs
-    end
-  end # def register
-
-  public
-  def run(output_queue)
-    begin
-      # snmp trap server
-      snmptrap_listener(output_queue)
-    rescue => e
-      @logger.warn("SNMP Trap listener died", :exception => e, :backtrace => e.backtrace)
-      sleep(5)
-      retry
-    end # begin
-  end # def run
-
-  private
-  def snmptrap_listener(output_queue)
-    traplistener_opts = {:Port => @port, :Community => @community, :Host => @host}
-    if @yaml_mibs && !@yaml_mibs.empty?
-      traplistener_opts.merge!({:MibDir => @yamlmibdir, :MibModules => @yaml_mibs})
-    end
-    @logger.info("It's a Trap!", traplistener_opts.dup)
-    @snmptrap = SNMP::TrapListener.new(traplistener_opts)
-
-    @snmptrap.on_trap_default do |trap|
-      begin
-        event = LogStash::Event.new("message" => trap.inspect, "host" => trap.source_ip)
-        decorate(event)
-        trap.each_varbind do |vb|
-          event[vb.name.to_s] = vb.value.to_s
-        end
-        @logger.debug("SNMP Trap received: ", :trap_object => trap.inspect)
-        output_queue << event
-      rescue => event
-        @logger.error("Failed to create event", :trap_object => trap.inspect)
-      end
-    end
-    @snmptrap.join
-  end # def snmptrap_listener
-
-end # class LogStash::Inputs::Snmptrap
diff --git a/lib/logstash/inputs/sqs.rb b/lib/logstash/inputs/sqs.rb
deleted file mode 100644
index c0d3e756083..00000000000
--- a/lib/logstash/inputs/sqs.rb
+++ /dev/null
@@ -1,172 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/threadable"
-require "logstash/namespace"
-require "logstash/plugin_mixins/aws_config"
-require "digest/sha2"
-
-# Pull events from an Amazon Web Services Simple Queue Service (SQS) queue.
-#
-# SQS is a simple, scalable queue system that is part of the 
-# Amazon Web Services suite of tools.
-#
-# Although SQS is similar to other queuing systems like AMQP, it
-# uses a custom API and requires that you have an AWS account.
-# See http://aws.amazon.com/sqs/ for more details on how SQS works,
-# what the pricing schedule looks like and how to setup a queue.
-#
-# To use this plugin, you *must*:
-#
-#  * Have an AWS account
-#  * Setup an SQS queue
-#  * Create an identify that has access to consume messages from the queue.
-#
-# The "consumer" identity must have the following permissions on the queue:
-#
-#  * sqs:ChangeMessageVisibility
-#  * sqs:ChangeMessageVisibilityBatch
-#  * sqs:DeleteMessage
-#  * sqs:DeleteMessageBatch
-#  * sqs:GetQueueAttributes
-#  * sqs:GetQueueUrl
-#  * sqs:ListQueues
-#  * sqs:ReceiveMessage
-#
-# Typically, you should setup an IAM policy, create a user and apply the IAM policy to the user.
-# A sample policy is as follows:
-#
-#     {
-#       "Statement": [
-#         {
-#           "Action": [
-#             "sqs:ChangeMessageVisibility",
-#             "sqs:ChangeMessageVisibilityBatch",
-#             "sqs:GetQueueAttributes",
-#             "sqs:GetQueueUrl",
-#             "sqs:ListQueues",
-#             "sqs:SendMessage",
-#             "sqs:SendMessageBatch"
-#           ],
-#           "Effect": "Allow",
-#           "Resource": [
-#             "arn:aws:sqs:us-east-1:123456789012:Logstash"
-#           ]
-#         }
-#       ]
-#     } 
-#
-# See http://aws.amazon.com/iam/ for more details on setting up AWS identities.
-#
-class LogStash::Inputs::SQS < LogStash::Inputs::Threadable
-  include LogStash::PluginMixins::AwsConfig
-
-  config_name "sqs"
-  milestone 1
-
-  default :codec, "json"
-
-  # Name of the SQS Queue name to pull messages from. Note that this is just the name of the queue, not the URL or ARN.
-  config :queue, :validate => :string, :required => true
-
-  # Name of the event field in which to store the SQS message ID
-  config :id_field, :validate => :string
-
-  # Name of the event field in which to store the SQS message MD5 checksum
-  config :md5_field, :validate => :string
-
-  # Name of the event field in which to store the  SQS message Sent Timestamp
-  config :sent_timestamp_field, :validate => :string
-
-  public
-  def aws_service_endpoint(region)
-    return {
-        :sqs_endpoint => "sqs.#{region}.amazonaws.com"
-    }
-  end
-
-  public
-  def register
-    @logger.info("Registering SQS input", :queue => @queue)
-    require "aws-sdk"
-
-    @sqs = AWS::SQS.new(aws_options_hash)
-
-    begin
-      @logger.debug("Connecting to AWS SQS queue", :queue => @queue)
-      @sqs_queue = @sqs.queues.named(@queue)
-      @logger.info("Connected to AWS SQS queue successfully.", :queue => @queue)
-    rescue Exception => e
-      @logger.error("Unable to access SQS queue.", :error => e.to_s, :queue => @queue)
-      throw e
-    end # begin/rescue
-  end # def register
-
-  public
-  def run(output_queue)
-    @logger.debug("Polling SQS queue", :queue => @queue)
-
-    receive_opts = {
-        :limit => 10,
-        :visibility_timeout => 30,
-        :attributes => [:sent_at]
-    }
-
-    continue_polling = true
-    while running? && continue_polling
-      continue_polling = run_with_backoff(60, 1) do
-        @sqs_queue.receive_message(receive_opts) do |message|
-          if message
-            @codec.decode(message.body) do |event|
-              decorate(event)
-              if @id_field
-                event[@id_field] = message.id
-              end
-              if @md5_field
-                event[@md5_field] = message.md5
-              end
-              if @sent_timestamp_field
-                event[@sent_timestamp_field] = message.sent_timestamp.utc
-              end
-              @logger.debug? && @logger.debug("Processed SQS message", :message_id => message.id, :message_md5 => message.md5, :sent_timestamp => message.sent_timestamp, :queue => @queue)
-              output_queue << event
-              message.delete
-            end # codec.decode
-          end # valid SQS message
-        end # receive_message
-      end # run_with_backoff
-    end # polling loop
-  end # def run
-
-  def teardown
-    @sqs_queue = nil
-    finished
-  end # def teardown
-
-  private
-  # Runs an AWS request inside a Ruby block with an exponential backoff in case
-  # we exceed the allowed AWS RequestLimit.
-  #
-  # @param [Integer] max_time maximum amount of time to sleep before giving up.
-  # @param [Integer] sleep_time the initial amount of time to sleep before retrying.
-  # @param [Block] block Ruby code block to execute.
-  def run_with_backoff(max_time, sleep_time, &block)
-    if sleep_time > max_time
-      @logger.error("AWS::EC2::Errors::RequestLimitExceeded ... failed.", :queue => @queue)
-      return false
-    end # retry limit exceeded
-
-    begin
-      block.call
-    rescue AWS::EC2::Errors::RequestLimitExceeded
-      @logger.info("AWS::EC2::Errors::RequestLimitExceeded ... retrying SQS request", :queue => @queue, :sleep_time => sleep_time)
-      sleep sleep_time
-      run_with_backoff(max_time, sleep_time * 2, &block)
-    rescue AWS::EC2::Errors::InstanceLimitExceeded
-      @logger.warn("AWS::EC2::Errors::InstanceLimitExceeded ... aborting SQS message retreival.")
-      return false
-    rescue Exception => bang
-      @logger.error("Error reading SQS queue.", :error => bang, :queue => @queue)
-      return false
-    end # begin/rescue
-    return true
-  end # def run_with_backoff
-end # class LogStash::Inputs::SQS
diff --git a/lib/logstash/inputs/stdin.rb b/lib/logstash/inputs/stdin.rb
deleted file mode 100644
index bc9756fffd7..00000000000
--- a/lib/logstash/inputs/stdin.rb
+++ /dev/null
@@ -1,46 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket" # for Socket.gethostname
-
-# Read events from standard input.
-#
-# By default, each event is assumed to be one line. If you
-# want to join lines, you'll want to use the multiline filter.
-class LogStash::Inputs::Stdin < LogStash::Inputs::Base
-  config_name "stdin"
-  milestone 3
-
-  default :codec, "line"
-
-  public
-  def register
-    @host = Socket.gethostname
-  end # def register
-
-  def run(queue) 
-    while true
-      begin
-        # Based on some testing, there is no way to interrupt an IO.sysread nor
-        # IO.select call in JRuby. Bummer :(
-        data = $stdin.sysread(16384)
-        @codec.decode(data) do |event|
-          decorate(event)
-          event["host"] = @host
-          queue << event
-        end
-      rescue EOFError, LogStash::ShutdownSignal
-        # stdin closed or a requested shutdown
-        break
-      end
-    end # while true
-    finished
-  end # def run
-
-  public
-  def teardown
-    @logger.debug("stdin shutting down.")
-    $stdin.close rescue nil
-    finished
-  end # def teardown
-end # class LogStash::Inputs::Stdin
diff --git a/lib/logstash/inputs/syslog.rb b/lib/logstash/inputs/syslog.rb
deleted file mode 100644
index dddf08b7f80..00000000000
--- a/lib/logstash/inputs/syslog.rb
+++ /dev/null
@@ -1,237 +0,0 @@
-# encoding: utf-8
-require "date"
-require "logstash/filters/grok"
-require "logstash/filters/date"
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Read syslog messages as events over the network.
-#
-# This input is a good choice if you already use syslog today.
-# It is also a good choice if you want to receive logs from
-# appliances and network devices where you cannot run your own
-# log collector.
-#
-# Of course, 'syslog' is a very muddy term. This input only supports RFC3164
-# syslog with some small modifications. The date format is allowed to be
-# RFC3164 style or ISO8601. Otherwise the rest of RFC3164 must be obeyed.
-# If you do not use RFC3164, do not use this input.
-#
-# For more information see (the RFC3164 page)[http://www.ietf.org/rfc/rfc3164.txt].
-#
-# Note: this input will start listeners on both TCP and UDP.
-class LogStash::Inputs::Syslog < LogStash::Inputs::Base
-  config_name "syslog"
-  milestone 1
-
-  default :codec, "plain"
-
-  # The address to listen on.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port to listen on. Remember that ports less than 1024 (privileged
-  # ports) may require root to use.
-  config :port, :validate => :number, :default => 514
-
-  # Use label parsing for severity and facility levels.
-  config :use_labels, :validate => :boolean, :default => true
-
-  # Labels for facility levels. These are defined in RFC3164.
-  config :facility_labels, :validate => :array, :default => [ "kernel", "user-level", "mail", "system", "security/authorization", "syslogd", "line printer", "network news", "UUCP", "clock", "security/authorization", "FTP", "NTP", "log audit", "log alert", "clock", "local0", "local1", "local2", "local3", "local4", "local5", "local6", "local7" ]
-
-  # Labels for severity levels. These are defined in RFC3164.
-  config :severity_labels, :validate => :array, :default => [ "Emergency" , "Alert", "Critical", "Error", "Warning", "Notice", "Informational", "Debug" ]
-
-  public
-  def initialize(params)
-    super
-    @shutdown_requested = false
-    BasicSocket.do_not_reverse_lookup = true
-  end # def initialize
-
-  public
-  def register
-    require "thread_safe"
-    @grok_filter = LogStash::Filters::Grok.new(
-      "overwrite" => "message",
-      "match" => { "message" => "<%{POSINT:priority}>%{SYSLOGLINE}" },
-    )
-
-    @date_filter = LogStash::Filters::Date.new(
-      "match" => [ "timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss", "ISO8601"]
-    )
-
-    @grok_filter.register
-    @date_filter.register
-
-    @tcp_clients = ThreadSafe::Array.new
-  end # def register
-
-  public
-  def run(output_queue)
-    # udp server
-    udp_thr = Thread.new do
-      begin
-        udp_listener(output_queue)
-      rescue => e
-        break if @shutdown_requested
-        @logger.warn("syslog udp listener died",
-                     :address => "#{@host}:#{@port}", :exception => e,
-                     :backtrace => e.backtrace)
-        sleep(5)
-        retry
-      end # begin
-    end # Thread.new
-
-    # tcp server
-    tcp_thr = Thread.new do
-      begin
-        tcp_listener(output_queue)
-      rescue => e
-        break if @shutdown_requested
-        @logger.warn("syslog tcp listener died",
-                     :address => "#{@host}:#{@port}", :exception => e,
-                     :backtrace => e.backtrace)
-        sleep(5)
-        retry
-      end # begin
-    end # Thread.new
-
-    # If we exit and we're the only input, the agent will think no inputs
-    # are running and initiate a shutdown.
-    udp_thr.join
-    tcp_thr.join
-  end # def run
-
-  private
-  def udp_listener(output_queue)
-    @logger.info("Starting syslog udp listener", :address => "#{@host}:#{@port}")
-
-    if @udp
-      @udp.close
-    end
-
-    @udp = UDPSocket.new(Socket::AF_INET)
-    @udp.bind(@host, @port)
-
-    loop do
-      payload, client = @udp.recvfrom(9000)
-      # Ruby uri sucks, so don't use it.
-      @codec.decode(payload) do |event|
-        decorate(event)
-        event["host"] = client[3]
-        syslog_relay(event)
-        output_queue << event
-      end
-    end
-  ensure
-    close_udp
-  end # def udp_listener
-
-  private
-  def tcp_listener(output_queue)
-    @logger.info("Starting syslog tcp listener", :address => "#{@host}:#{@port}")
-    @tcp = TCPServer.new(@host, @port)
-    @tcp_clients = []
-
-    loop do
-      client = @tcp.accept
-      @tcp_clients << client
-      Thread.new(client) do |client|
-        ip, port = client.peeraddr[3], client.peeraddr[1]
-        @logger.info("new connection", :client => "#{ip}:#{port}")
-        LogStash::Util::set_thread_name("input|syslog|tcp|#{ip}:#{port}}")
-        begin
-          client.each do |line|
-            @codec.decode(line) do |event|
-              decorate(event)
-              event["host"] = ip
-              syslog_relay(event)
-              output_queue << event
-            end
-          end
-        rescue Errno::ECONNRESET
-        ensure
-          @tcp_clients.delete(client)
-        end
-      end # Thread.new
-    end # loop do
-  ensure
-    close_tcp
-  end # def tcp_listener
-
-  public
-  def teardown
-    @shutdown_requested = true
-    close_udp
-    close_tcp
-    finished
-  end
-
-  private
-  def close_udp
-    if @udp
-      @udp.close_read rescue nil
-      @udp.close_write rescue nil
-    end
-    @udp = nil
-  end
-
-  private
-  def close_tcp
-    # If we somehow have this left open, close it.
-    @tcp_clients.each do |client|
-      client.close rescue nil
-    end
-    @tcp.close if @tcp rescue nil
-    @tcp = nil
-  end
-
-  # Following RFC3164 where sane, we'll try to parse a received message
-  # as if you were relaying a syslog message to it.
-  # If the message cannot be recognized (see @grok_filter), we'll
-  # treat it like the whole event["message"] is correct and try to fill
-  # the missing pieces (host, priority, etc)
-  public
-  def syslog_relay(event)
-    @grok_filter.filter(event)
-
-    if event["tags"].nil? || !event["tags"].include?("_grokparsefailure")
-      # Per RFC3164, priority = (facility * 8) + severity
-      #                       = (facility << 3) & (severity)
-      priority = event["priority"].to_i rescue 13
-      severity = priority & 7   # 7 is 111 (3 bits)
-      facility = priority >> 3
-      event["priority"] = priority
-      event["severity"] = severity
-      event["facility"] = facility
-
-      event["timestamp"] = event["timestamp8601"] if event.include?("timestamp8601")
-      @date_filter.filter(event)
-    else
-      @logger.info? && @logger.info("NOT SYSLOG", :message => event["message"])
-
-      # RFC3164 says unknown messages get pri=13
-      priority = 13
-      event["priority"] = 13
-      event["severity"] = 5   # 13 & 7 == 5
-      event["facility"] = 1   # 13 >> 3 == 1
-    end
-
-    # Apply severity and facility metadata if
-    # use_labels => true
-    if @use_labels
-      facility_number = event["facility"]
-      severity_number = event["severity"]
-
-      if @facility_labels[facility_number]
-        event["facility_label"] = @facility_labels[facility_number]
-      end
-
-      if @severity_labels[severity_number]
-        event["severity_label"] = @severity_labels[severity_number]
-      end
-    end
-  end # def syslog_relay
-end # class LogStash::Inputs::Syslog
diff --git a/lib/logstash/inputs/tcp.rb b/lib/logstash/inputs/tcp.rb
deleted file mode 100644
index 07cdf46738b..00000000000
--- a/lib/logstash/inputs/tcp.rb
+++ /dev/null
@@ -1,226 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "logstash/util/socket_peer"
-
-# Read events over a TCP socket.
-#
-# Like stdin and file inputs, each event is assumed to be one line of text.
-#
-# Can either accept connections from clients or connect to a server,
-# depending on `mode`.
-class LogStash::Inputs::Tcp < LogStash::Inputs::Base
-  class Interrupted < StandardError; end
-  config_name "tcp"
-  milestone 2
-
-  default :codec, "line"
-
-  # When mode is `server`, the address to listen on.
-  # When mode is `client`, the address to connect to.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # When mode is `server`, the port to listen on.
-  # When mode is `client`, the port to connect to.
-  config :port, :validate => :number, :required => true
-
-  # The 'read' timeout in seconds. If a particular tcp connection is idle for
-  # more than this timeout period, we will assume it is dead and close it.
-  #
-  # If you never want to timeout, use -1.
-  config :data_timeout, :validate => :number, :default => -1
-
-  # Mode to operate in. `server` listens for client connections,
-  # `client` connects to a server.
-  config :mode, :validate => ["server", "client"], :default => "server"
-
-  # Enable SSL (must be set for other `ssl_` options to take effect).
-  config :ssl_enable, :validate => :boolean, :default => false
-
-  # Verify the identity of the other end of the SSL connection against the CA.
-  # For input, sets the field `sslsubject` to that of the client certificate.
-  config :ssl_verify, :validate => :boolean, :default => false
-
-  # The SSL CA certificate, chainfile or CA path. The system CA path is automatically included.
-  config :ssl_cacert, :validate => :path
-
-  # SSL certificate path
-  config :ssl_cert, :validate => :path
-
-  # SSL key path
-  config :ssl_key, :validate => :path
-
-  # SSL key passphrase
-  config :ssl_key_passphrase, :validate => :password, :default => nil
-
-  def initialize(*args)
-    super(*args)
-  end # def initialize
-
-  public
-  def register
-    require "socket"
-    require "timeout"
-    require "openssl"
-    if @ssl_enable
-      @ssl_context = OpenSSL::SSL::SSLContext.new
-      @ssl_context.cert = OpenSSL::X509::Certificate.new(File.read(@ssl_cert))
-      @ssl_context.key = OpenSSL::PKey::RSA.new(File.read(@ssl_key),@ssl_key_passphrase)
-      if @ssl_verify
-        @cert_store = OpenSSL::X509::Store.new
-        # Load the system default certificate path to the store
-        @cert_store.set_default_paths
-        if File.directory?(@ssl_cacert)
-          @cert_store.add_path(@ssl_cacert)
-        else
-          @cert_store.add_file(@ssl_cacert)
-        end
-        @ssl_context.cert_store = @cert_store
-        @ssl_context.verify_mode = OpenSSL::SSL::VERIFY_PEER|OpenSSL::SSL::VERIFY_FAIL_IF_NO_PEER_CERT
-      end
-    end # @ssl_enable
-
-    if server?
-      @logger.info("Starting tcp input listener", :address => "#{@host}:#{@port}")
-      begin
-        @server_socket = TCPServer.new(@host, @port)
-      rescue Errno::EADDRINUSE
-        @logger.error("Could not start TCP server: Address in use",
-                      :host => @host, :port => @port)
-        raise
-      end
-      if @ssl_enable
-        @server_socket = OpenSSL::SSL::SSLServer.new(@server_socket, @ssl_context)
-      end # @ssl_enable
-    end
-  end # def register
-
-  private
-  def handle_socket(socket, client_address, output_queue, codec)
-    while true
-      buf = nil
-      # NOTE(petef): the timeout only hits after the line is read
-      # or socket dies
-      # TODO(sissel): Why do we have a timeout here? What's the point?
-      if @data_timeout == -1
-        buf = read(socket)
-      else
-        Timeout::timeout(@data_timeout) do
-          buf = read(socket)
-        end
-      end
-      codec.decode(buf) do |event|
-        event["host"] ||= client_address
-        event["sslsubject"] ||= socket.peer_cert.subject if @ssl_enable && @ssl_verify
-        decorate(event)
-        output_queue << event
-      end
-    end # loop do
-  rescue EOFError
-    @logger.debug("Connection closed", :client => socket.peer)
-  rescue => e
-    @logger.debug("An error occurred. Closing connection",
-                  :client => socket.peer, :exception => e, :backtrace => e.backtrace)
-  ensure
-    socket.close rescue IOError nil
-    codec.respond_to?(:flush) && codec.flush do |event|
-      event["host"] ||= client_address
-      event["sslsubject"] ||= socket.peer_cert.subject if @ssl_enable && @ssl_verify
-      decorate(event)
-      output_queue << event
-    end
-  end
-
-  private
-  def server?
-    @mode == "server"
-  end # def server?
-
-  private
-  def read(socket)
-    return socket.sysread(16384)
-  end # def readline
-
-  public
-  def run(output_queue)
-    if server?
-      run_server(output_queue)
-    else
-      run_client(output_queue)
-    end
-  end # def run
-
-  def run_server(output_queue)
-    @thread = Thread.current
-    @client_threads = []
-    loop do
-      # Start a new thread for each connection.
-      begin
-        @client_threads << Thread.start(@server_socket.accept) do |s|
-          # TODO(sissel): put this block in its own method.
-
-          # monkeypatch a 'peer' method onto the socket.
-          s.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-          @logger.debug("Accepted connection", :client => s.peer,
-                        :server => "#{@host}:#{@port}")
-          begin
-            handle_socket(s, s.peer, output_queue, @codec.clone)
-          rescue Interrupted
-            s.close rescue nil
-          end
-        end # Thread.start
-      rescue OpenSSL::SSL::SSLError => ssle
-        # NOTE(mrichar1): This doesn't return a useful error message for some reason
-        @logger.error("SSL Error", :exception => ssle,
-                      :backtrace => ssle.backtrace)
-      rescue IOError, LogStash::ShutdownSignal
-        if @interrupted
-          # Intended shutdown, get out of the loop
-          @server_socket.close
-          @client_threads.each do |thread|
-            thread.raise(LogStash::ShutdownSignal)
-          end
-          break
-        else
-          # Else it was a genuine IOError caused by something else, so propagate it up..
-          raise
-        end
-      end
-    end # loop
-  rescue LogStash::ShutdownSignal
-    # nothing to do
-  ensure
-    @server_socket.close rescue nil
-  end # def run_server
-
-  def run_client(output_queue) 
-    @thread = Thread.current
-    while true
-      client_socket = TCPSocket.new(@host, @port)
-      if @ssl_enable
-        client_socket = OpenSSL::SSL::SSLSocket.new(client_socket, @ssl_context)
-        begin
-          client_socket.connect
-        rescue OpenSSL::SSL::SSLError => ssle
-          @logger.error("SSL Error", :exception => ssle,
-                        :backtrace => ssle.backtrace)
-          # NOTE(mrichar1): Hack to prevent hammering peer
-          sleep(5)
-          next
-        end
-      end
-      client_socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-      @logger.debug("Opened connection", :client => "#{client_socket.peer}")
-      handle_socket(client_socket, client_socket.peer, output_queue, @codec.clone)
-    end # loop
-  ensure
-    client_socket.close
-  end # def run
-
-  public
-  def teardown
-    if server?
-      @interrupted = true
-    end
-  end # def teardown
-end # class LogStash::Inputs::Tcp
diff --git a/lib/logstash/inputs/twitter.rb b/lib/logstash/inputs/twitter.rb
deleted file mode 100644
index fecd0755c7a..00000000000
--- a/lib/logstash/inputs/twitter.rb
+++ /dev/null
@@ -1,91 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "json"
-
-# Read events from the twitter streaming api.
-class LogStash::Inputs::Twitter < LogStash::Inputs::Base
-
-  config_name "twitter"
-  milestone 1
-
-  # Your twitter app's consumer key
-  #
-  # Don't know what this is? You need to create an "application"
-  # on twitter, see this url: <https://dev.twitter.com/apps/new>
-  config :consumer_key, :validate => :string, :required => true
-
-  # Your twitter app's consumer secret
-  #
-  # If you don't have one of these, you can create one by
-  # registering a new application with twitter:
-  # <https://dev.twitter.com/apps/new>
-  config :consumer_secret, :validate => :password, :required => true
-
-  # Your oauth token.
-  #
-  # To get this, login to twitter with whatever account you want,
-  # then visit <https://dev.twitter.com/apps>
-  #
-  # Click on your app (used with the consumer_key and consumer_secret settings)
-  # Then at the bottom of the page, click 'Create my access token' which
-  # will create an oauth token and secret bound to your account and that
-  # application.
-  config :oauth_token, :validate => :string, :required => true
-  
-  # Your oauth token secret.
-  #
-  # To get this, login to twitter with whatever account you want,
-  # then visit <https://dev.twitter.com/apps>
-  #
-  # Click on your app (used with the consumer_key and consumer_secret settings)
-  # Then at the bottom of the page, click 'Create my access token' which
-  # will create an oauth token and secret bound to your account and that
-  # application.
-  config :oauth_token_secret, :validate => :password, :required => true
-
-  # Any keywords to track in the twitter stream
-  config :keywords, :validate => :array, :required => true
-
-  # Record full tweet object as given to us by the Twitter stream api.
-  config :full_tweet, :validate => :boolean, :default => false
-
-  public
-  def register
-    require "twitter"
-    @client = Twitter::Streaming::Client.new do |c|
-      c.consumer_key = @consumer_key
-      c.consumer_secret = @consumer_secret.value
-      c.access_token = @oauth_token
-      c.access_token_secret = @oauth_token_secret.value
-    end
-  end
-
-  public
-  def run(queue)
-    @logger.info("Starting twitter tracking", :keywords => @keywords)
-    @client.filter(:track => @keywords.join(",")) do |tweet|
-      @logger.info? && @logger.info("Got tweet", :user => tweet.user.screen_name, :text => tweet.text)
-      if @full_tweet
-        event = LogStash::Event.new(
-          tweet.to_hash.merge("@timestamp" => tweet.created_at.gmtime)
-        )
-      else
-        event = LogStash::Event.new(
-          "@timestamp" => tweet.created_at.gmtime,
-          "message" => tweet.full_text,
-          "user" => tweet.user.screen_name,
-          "client" => tweet.source,
-          "retweeted" => tweet.retweeted?,
-          "source" => "http://twitter.com/#{tweet.user.screen_name}/status/#{tweet.id}"
-        )
-      end
-      decorate(event)
-      event["in-reply-to"] = tweet.in_reply_to_status_id if tweet.reply?
-      unless tweet.urls.empty?
-        event["urls"] = tweet.urls.map(&:expanded_url).map(&:to_s)
-      end
-      queue << event
-    end # client.filter
-  end # def run
-end # class LogStash::Inputs::Twitter
diff --git a/lib/logstash/inputs/udp.rb b/lib/logstash/inputs/udp.rb
deleted file mode 100644
index f4224655a36..00000000000
--- a/lib/logstash/inputs/udp.rb
+++ /dev/null
@@ -1,117 +0,0 @@
-# encoding: utf-8
-require "date"
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Read messages as events over the network via udp. The only required
-# configuration item is `port`, which specifies the udp port logstash 
-# will listen on for event streams.
-#
-class LogStash::Inputs::Udp < LogStash::Inputs::Base
-  config_name "udp"
-  milestone 2
-
-  default :codec, "plain"
-
-  # The address which logstash will listen on.
-  config :host, :validate => :string, :default => "0.0.0.0"
-
-  # The port which logstash will listen on. Remember that ports less
-  # than 1024 (privileged ports) may require root or elevated privileges to use.
-  config :port, :validate => :number, :required => true
-
-  # The maximum packet size to read from the network
-  config :buffer_size, :validate => :number, :default => 8192
-  
-  # Number of threads processing packets
-  config :workers, :validate => :number, :default => 2
-  
-  # This is the number of unprocessed UDP packets you can hold in memory
-  # before packets will start dropping.
-  config :queue_size, :validate => :number, :default => 2000
-
-  public
-  def initialize(params)
-    super
-    BasicSocket.do_not_reverse_lookup = true
-  end # def initialize
-
-  public
-  def register
-    @udp = nil
-  end # def register
-
-  public
-  def run(output_queue)
-	@output_queue = output_queue
-    begin
-      # udp server
-      udp_listener(output_queue)
-    rescue LogStash::ShutdownSignal
-      # do nothing, shutdown was requested.
-    rescue => e
-      @logger.warn("UDP listener died", :exception => e, :backtrace => e.backtrace)
-      sleep(5)
-      retry
-    end # begin
-  end # def run
-
-  private
-  def udp_listener(output_queue)
-    @logger.info("Starting UDP listener", :address => "#{@host}:#{@port}")
-
-    if @udp && ! @udp.closed?
-      @udp.close
-    end
-
-    @udp = UDPSocket.new(Socket::AF_INET)
-    @udp.bind(@host, @port)
-
-	  @input_to_worker = SizedQueue.new(@queue_size)
-
-	  @input_workers = @workers.times do |i|
-  	    @logger.debug("Starting UDP worker thread", :worker => i)
- 		  Thread.new { inputworker(i) }
-	  end
-	
-    loop do
-		  #collect datagram message and add to queue
-      payload, client = @udp.recvfrom(@buffer_size)
-	    @input_to_worker.push([payload,client])
-    end
-  ensure
-    if @udp
-      @udp.close_read rescue nil
-      @udp.close_write rescue nil
-    end
-  end # def udp_listener
-  
-  def inputworker(number)
-    LogStash::Util::set_thread_name("<udp.#{number}")
-    begin
-      while true
-        payload,client = @input_to_worker.pop
-		    if payload == LogStash::ShutdownSignal
-          @input_to_worker.push(work)
-          break
-        end
-
-		    @codec.decode(payload) do |event|
-          decorate(event)
-          event["host"] ||= client[3]
-		      @output_queue.push(event)
-		    end
-      end
-
-    rescue => e
-      @logger.error("Exception in inputworker", "exception" => e, "backtrace" => e.backtrace)
-    end
-  end # def inputworker
-  
-  public
-  def teardown
-    @udp.close if @udp && !@udp.closed?
-  end
-
-end # class LogStash::Inputs::Udp
diff --git a/lib/logstash/inputs/unix.rb b/lib/logstash/inputs/unix.rb
deleted file mode 100644
index b78a887b0d7..00000000000
--- a/lib/logstash/inputs/unix.rb
+++ /dev/null
@@ -1,163 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Read events over a UNIX socket.
-#
-# Like stdin and file inputs, each event is assumed to be one line of text.
-#
-# Can either accept connections from clients or connect to a server,
-# depending on `mode`.
-class LogStash::Inputs::Unix < LogStash::Inputs::Base
-  class Interrupted < StandardError; end
-  config_name "unix"
-  milestone 2
-
-  default :codec, "line"
-
-  # When mode is `server`, the path to listen on.
-  # When mode is `client`, the path to connect to.
-  config :path, :validate => :string, :required => true
-
-  # Remove socket file in case of EADDRINUSE failure
-  config :force_unlink, :validate => :boolean, :default => false
-
-  # The 'read' timeout in seconds. If a particular connection is idle for
-  # more than this timeout period, we will assume it is dead and close it.
-  #
-  # If you never want to timeout, use -1.
-  config :data_timeout, :validate => :number, :default => -1
-
-  # Mode to operate in. `server` listens for client connections,
-  # `client` connects to a server.
-  config :mode, :validate => ["server", "client"], :default => "server"
-
-  def initialize(*args)
-    super(*args)
-  end # def initialize
-
-  public
-  def register
-    require "socket"
-    require "timeout"
-
-    if server?
-      @logger.info("Starting unix input listener", :address => "#{@path}", :force_unlink => "#{@force_unlink}")
-      begin
-        @server_socket = UNIXServer.new(@path)
-      rescue Errno::EADDRINUSE, IOError
-        if @force_unlink
-          File.unlink(@path)
-          begin
-            @server_socket = UNIXServer.new(@path)
-            return
-          rescue Errno::EADDRINUSE, IOError
-            @logger.error("!!!Could not start UNIX server: Address in use",
-                          :path => @path)
-            raise
-          end
-        end
-        @logger.error("Could not start UNIX server: Address in use",
-                      :path => @path)
-        raise
-      end
-    end
-  end # def register
-
-  private
-  def handle_socket(socket, output_queue)
-    begin
-      hostname = Socket.gethostname
-      loop do
-        buf = nil
-        # NOTE(petef): the timeout only hits after the line is read
-        # or socket dies
-        # TODO(sissel): Why do we have a timeout here? What's the point?
-        if @data_timeout == -1
-          buf = socket.readpartial(16384)
-        else
-          Timeout::timeout(@data_timeout) do
-            buf = socket.readpartial(16384)
-          end
-        end
-        @codec.decode(buf) do |event|
-          decorate(event)
-          event["host"] = hostname
-          event["path"] = @path
-          output_queue << event
-        end
-      end # loop do
-    rescue => e
-      @logger.debug("Closing connection", :path => @path,
-      :exception => e, :backtrace => e.backtrace)
-    rescue Timeout::Error
-      @logger.debug("Closing connection after read timeout",
-      :path => @path)
-    end # begin
-
-  ensure
-    begin
-      socket.close
-    rescue IOError
-      #pass
-    end # begin
-  end
-
-  private
-  def server?
-    @mode == "server"
-  end # def server?
-
-  public
-  def run(output_queue)
-    if server?
-      @thread = Thread.current
-      @client_threads = []
-      loop do
-        # Start a new thread for each connection.
-        begin
-          @client_threads << Thread.start(@server_socket.accept) do |s|
-            # TODO(sissel): put this block in its own method.
-
-            @logger.debug("Accepted connection",
-                          :server => "#{@path}")
-            begin
-              handle_socket(s, output_queue)
-            rescue Interrupted
-              s.close rescue nil
-            end
-          end # Thread.start
-        rescue IOError, Interrupted
-          if @interrupted
-            # Intended shutdown, get out of the loop
-            @server_socket.close
-            @client_threads.each do |thread|
-              thread.raise(IOError.new)
-            end
-            break
-          else
-            # Else it was a genuine IOError caused by something else, so propagate it up..
-            raise
-          end
-        end
-      end # loop
-    else
-      loop do
-        client_socket = UNIXSocket.new(@path)
-        client_socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-        @logger.debug("Opened connection", :client => @path)
-        handle_socket(client_socket, output_queue)
-      end # loop
-    end
-  end # def run
-
-  public
-  def teardown
-    if server?
-      File.unlink(@path)
-      @interrupted = true
-      @thread.raise(Interrupted.new)
-    end
-  end # def teardown
-end # class LogStash::Inputs::Unix
diff --git a/lib/logstash/inputs/xmpp.rb b/lib/logstash/inputs/xmpp.rb
deleted file mode 100644
index fee932dff90..00000000000
--- a/lib/logstash/inputs/xmpp.rb
+++ /dev/null
@@ -1,81 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-
-# This input allows you to receive events over XMPP/Jabber.
-#
-# This plugin can be used for accepting events from humans or applications
-# XMPP, or you can use it for PubSub or general message passing for logstash to
-# logstash.
-class LogStash::Inputs::Xmpp < LogStash::Inputs::Base
-  
-  config_name "xmpp"
-  milestone 2
-
-  default :codec, "plain"
-
-  # The user or resource ID, like foo@example.com.
-  config :user, :validate => :string, :required => :true
-
-  # The xmpp password for the user/identity.
-  config :password, :validate => :password, :required => :true
-
-  # if muc/multi-user-chat required, give the name of the room that
-  # you want to join: room@conference.domain/nick
-  config :rooms, :validate => :array
-
-  # The xmpp server to connect to. This is optional. If you omit this setting,
-  # the host on the user/identity is used. (foo.com for user@foo.com)
-  config :host, :validate => :string
-
-  # Set to true to enable greater debugging in XMPP. Useful for debugging
-  # network/authentication erros.
-  config :debug, :validate => :boolean, :default => false, :deprecated => "Use the logstash --debug flag for this instead."
-
-  public
-  def register
-    require 'xmpp4r' # xmpp4r gem
-    Jabber::debug = true if @debug || @logger.debug?
-
-    @client = Jabber::Client.new(Jabber::JID.new(@user))
-    @client.connect(@host) # it is ok if host is nil
-    @client.auth(@password.value)
-    @client.send(Jabber::Presence.new.set_type(:available))
-
-    # load the MUC Client if we are joining rooms.
-    require 'xmpp4r/muc/helper/simplemucclient' if @rooms && !@rooms.empty?
-  end # def register
-
-  public
-  def run(queue)
-    if @rooms
-      @rooms.each do |room| # handle muc messages in different rooms
-        @muc = Jabber::MUC::SimpleMUCClient.new(@client)
-        @muc.join(room)
-        @muc.on_message do |time,from,body|
-          @codec.decode(body) do |event|
-            decorate(event)
-            event["room"] = room
-            event["from"] = from
-            queue << event
-          end
-        end # @muc.on_message
-      end # @rooms.each
-    end # if @rooms
-
-    @client.add_message_callback do |msg| # handle direct/private messages
-      # accept normal msgs (skip presence updates, etc)
-      if msg.body != nil
-        @codec.decode(msg.body) do |event|
-          decorate(event)
-          # Maybe "from" should just be a hash: 
-          # { "node" => ..., "domain" => ..., "resource" => ... }
-          event["from"] = "#{msg.from.node}@#{msg.from.domain}/#{msg.from.resource}"
-          queue << event
-        end
-      end
-    end # @client.add_message_callback
-    sleep
-  end # def run
-
-end # class LogStash::Inputs::Xmpp
diff --git a/lib/logstash/inputs/zeromq.rb b/lib/logstash/inputs/zeromq.rb
deleted file mode 100644
index b094a8c4b06..00000000000
--- a/lib/logstash/inputs/zeromq.rb
+++ /dev/null
@@ -1,165 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Read events over a 0MQ SUB socket.
-#
-# You need to have the 0mq 2.1.x library installed to be able to use
-# this input plugin.
-#
-# The default settings will create a subscriber binding to tcp://127.0.0.1:2120 
-# waiting for connecting publishers.
-#
-class LogStash::Inputs::ZeroMQ < LogStash::Inputs::Base
-
-  config_name "zeromq"
-  milestone 2
-
-  default :codec, "json"
-
-  # 0mq socket address to connect or bind
-  # Please note that `inproc://` will not work with logstash
-  # as each we use a context per thread.
-  # By default, inputs bind/listen
-  # and outputs connect
-  config :address, :validate => :array, :default => ["tcp://*:2120"]
-
-  # 0mq topology
-  # The default logstash topologies work as follows:
-  # * pushpull - inputs are pull, outputs are push
-  # * pubsub - inputs are subscribers, outputs are publishers
-  # * pair - inputs are clients, inputs are servers
-  #
-  # If the predefined topology flows don't work for you,
-  # you can change the 'mode' setting
-  # TODO (lusis) add req/rep MAYBE
-  # TODO (lusis) add router/dealer
-  config :topology, :validate => ["pushpull", "pubsub", "pair"], :required => true
-
-  # 0mq topic
-  # This is used for the 'pubsub' topology only
-  # On inputs, this allows you to filter messages by topic
-  # On outputs, this allows you to tag a message for routing
-  # NOTE: ZeroMQ does subscriber side filtering.
-  # NOTE: All topics have an implicit wildcard at the end
-  # You can specify multiple topics here
-  config :topic, :validate => :array
-
-  # mode
-  # server mode binds/listens
-  # client mode connects
-  config :mode, :validate => ["server", "client"], :default => "server"
-
-  # sender
-  # overrides the sender to 
-  # set the source of the event
-  # default is "zmq+topology://type/"
-  config :sender, :validate => :string
-
-  # 0mq socket options
-  # This exposes zmq_setsockopt
-  # for advanced tuning
-  # see http://api.zeromq.org/2-1:zmq-setsockopt for details
-  #
-  # This is where you would set values like:
-  # ZMQ::HWM - high water mark
-  # ZMQ::IDENTITY - named queues
-  # ZMQ::SWAP_SIZE - space for disk overflow
-  #
-  # example: sockopt => ["ZMQ::HWM", 50, "ZMQ::IDENTITY", "my_named_queue"]
-  config :sockopt, :validate => :hash
-
-  public
-  def register
-    require "ffi-rzmq"
-    require "logstash/util/zeromq"
-    self.class.send(:include, LogStash::Util::ZeroMQ)
-
-    case @topology
-    when "pair"
-      zmq_const = ZMQ::PAIR 
-    when "pushpull"
-      zmq_const = ZMQ::PULL
-    when "pubsub"
-      zmq_const = ZMQ::SUB
-    end # case socket_type
-    @zsocket = context.socket(zmq_const)
-    error_check(@zsocket.setsockopt(ZMQ::LINGER, 1),
-                "while setting ZMQ::LINGER == 1)")
-
-    if @sockopt
-      setopts(@zsocket, @sockopt)
-    end
-
-    @address.each do |addr|
-      setup(@zsocket, addr)
-    end
-
-    if @topology == "pubsub"
-      if @topic.nil?
-        @logger.debug("ZMQ - No topic provided. Subscribing to all messages")
-        error_check(@zsocket.setsockopt(ZMQ::SUBSCRIBE, ""),
-      "while setting ZMQ::SUBSCRIBE")
-      else
-        @topic.each do |t|
-          @logger.debug("ZMQ subscribing to topic: #{t}")
-          error_check(@zsocket.setsockopt(ZMQ::SUBSCRIBE, t),
-        "while setting ZMQ::SUBSCRIBE == #{t}")
-        end
-      end
-    end
-
-  end # def register
-
-  def teardown
-    error_check(@zsocket.close, "while closing the zmq socket")
-  end # def teardown
-
-  def server?
-    @mode == "server"
-  end # def server?
-
-  def run(output_queue)
-    host = Socket.gethostname
-    begin
-      loop do
-        # Here's the unified receiver
-        # Get the first part as the msg
-        m1 = ""
-        rc = @zsocket.recv_string(m1)
-        error_check(rc, "in recv_string")
-        @logger.debug("ZMQ receiving", :event => m1)
-        msg = m1
-        # If we have more parts, we'll eat the first as the topic
-        # and set the message to the second part
-        if @zsocket.more_parts?
-          @logger.debug("Multipart message detected. Setting @message to second part. First part was: #{m1}")
-          m2 = ''
-          rc2 = @zsocket.recv_string(m2)
-          error_check(rc2, "in recv_string")
-          @logger.debug("ZMQ receiving", :event => m2)
-          msg = m2
-        end
-
-        @codec.decode(msg) do |event|
-          event["host"] ||= host
-          decorate(event)
-          output_queue << event
-        end
-      end
-    rescue LogStash::ShutdownSignal
-      # shutdown
-      return
-    rescue => e
-      @logger.debug("ZMQ Error", :subscriber => @zsocket,
-                    :exception => e)
-      retry
-    end # begin
-  end # def run
-
-  private
-  def build_source_string
-    id = @address.first.clone
-  end
-end # class LogStash::Inputs::ZeroMQ
diff --git a/lib/logstash/java_integration.rb b/lib/logstash/java_integration.rb
new file mode 100644
index 00000000000..2bfeb3e81d2
--- /dev/null
+++ b/lib/logstash/java_integration.rb
@@ -0,0 +1,41 @@
+require "java"
+
+# this is mainly for usage with JrJackson json parsing in :raw mode which genenerates
+# Java::JavaUtil::ArrayList and Java::JavaUtil::LinkedHashMap native objects for speed.
+# these object already quacks like their Ruby equivalents Array and Hash but they will
+# not test for is_a?(Array) or is_a?(Hash) and we do not want to include tests for
+# both classes everywhere. see LogStash::JSon.
+
+class Java::JavaUtil::ArrayList
+  # have ArrayList objects report is_a?(Array) == true
+  def is_a?(clazz)
+    return true if clazz == Array
+    super
+  end
+end
+
+class Java::JavaUtil::LinkedHashMap
+  # have LinkedHashMap objects report is_a?(Array) == true
+  def is_a?(clazz)
+    return true if clazz == Hash
+    super
+  end
+end
+
+class Array
+  # enable class equivalence between Array and ArrayList
+  # so that ArrayList will work with case o when Array ...
+  def self.===(other)
+    return true if other.is_a?(Java::JavaUtil::ArrayList)
+    super
+  end
+end
+
+class Hash
+  # enable class equivalence between Hash and LinkedHashMap
+  # so that LinkedHashMap will work with case o when Hash ...
+  def self.===(other)
+    return true if other.is_a?(Java::JavaUtil::LinkedHashMap)
+    super
+  end
+end
diff --git a/lib/logstash/json.rb b/lib/logstash/json.rb
new file mode 100644
index 00000000000..5079de759a0
--- /dev/null
+++ b/lib/logstash/json.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/errors"
+if LogStash::Environment.jruby?
+  require "jrjackson"
+  require "logstash/java_integration"
+else
+  require  "oj"
+end
+
+module LogStash
+  module Json
+    class ParserError < LogStash::Error; end
+    class GeneratorError < LogStash::Error; end
+
+    extend self
+
+    ### MRI
+
+    def mri_load(data, options = {})
+      Oj.load(data)
+    rescue Oj::ParseError => e
+      raise LogStash::Json::ParserError.new(e.message)
+    end
+
+    def mri_dump(o)
+      Oj.dump(o, :mode => :compat, :use_to_json => true)
+    rescue => e
+      raise LogStash::Json::GeneratorError.new(e.message)
+    end
+
+    ### JRuby
+
+    def jruby_load(data, options = {})
+      options[:symbolize_keys] ? JrJackson::Raw.parse_sym(data) : JrJackson::Raw.parse_raw(data)
+    rescue JrJackson::ParseError => e
+      raise LogStash::Json::ParserError.new(e.message)
+    end
+
+    def jruby_dump(o)
+      # test for enumerable here to work around an omission in JrJackson::Json.dump to
+      # also look for Java::JavaUtil::ArrayList, see TODO submit issue
+      o.is_a?(Enumerable) ? JrJackson::Raw.generate(o) : JrJackson::Json.dump(o)
+    rescue => e
+      raise LogStash::Json::GeneratorError.new(e.message)
+    end
+
+    prefix = LogStash::Environment.jruby? ? "jruby" : "mri"
+    alias_method :load, "#{prefix}_load".to_sym
+    alias_method :dump, "#{prefix}_dump".to_sym
+
+  end
+end
diff --git a/lib/logstash/kibana.rb b/lib/logstash/kibana.rb
index dc7cebf66a8..e79eafaa6fb 100644
--- a/lib/logstash/kibana.rb
+++ b/lib/logstash/kibana.rb
@@ -42,7 +42,7 @@ def static_file
       if File.exists?(path)
         ext = path.split(".").last
         content_type MIME::Types.type_for(ext).first.to_s
-        body File.new(path, "r").read
+        body File.new(path, "rb").read
       else
         status 404
         content_type "text/plain"
diff --git a/lib/logstash/namespace.rb b/lib/logstash/namespace.rb
index 3ff393377a5..93f426b0fd7 100644
--- a/lib/logstash/namespace.rb
+++ b/lib/logstash/namespace.rb
@@ -11,6 +11,7 @@ module File; end
   module Web; end
   module Util; end
   module PluginMixins; end
+  module PluginManager; end
 
   SHUTDOWN = :shutdown
 end # module LogStash
diff --git a/lib/logstash/outputs/base.rb b/lib/logstash/outputs/base.rb
index 9fedbf10818..02e03de945f 100644
--- a/lib/logstash/outputs/base.rb
+++ b/lib/logstash/outputs/base.rb
@@ -13,7 +13,7 @@ class LogStash::Outputs::Base < LogStash::Plugin
   config_name "output"
 
   # The type to act on. If a type is given, then this output will only
-  # act on messages with the same type. See any input plugin's "type"
+  # act on messages with the same type. See any input plugin's `type`
   # attribute for more.
   # Optional.
   config :type, :validate => :string, :default => "", :deprecated => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
@@ -33,6 +33,8 @@ class LogStash::Outputs::Base < LogStash::Plugin
   # Note that this setting may not be useful for all outputs.
   config :workers, :validate => :number, :default => 1
 
+  attr_reader :worker_plugins
+
   public
   def workers_not_supported(message=nil)
     return if @workers == 1
@@ -62,20 +64,20 @@ def receive(event)
 
   public
   def worker_setup
-    return unless @workers > 1
-
-    define_singleton_method(:handle, method(:handle_worker))
-    @worker_queue = SizedQueue.new(20)
-
-    @worker_threads = @workers.times do |i|
-      Thread.new(original_params, @worker_queue) do |params, queue|
-        LogStash::Util::set_thread_name(">#{self.class.config_name}.#{i}")
-        worker_params = params.merge("workers" => 1, "codec" => @codec.clone)
-        worker_plugin = self.class.new(worker_params)
-        worker_plugin.register
-        while true
-          event = queue.pop
-          worker_plugin.handle(event)
+    if @workers == 1
+      @worker_plugins = [self]
+    else
+      define_singleton_method(:handle, method(:handle_worker))
+      @worker_queue = SizedQueue.new(20)
+      @worker_plugins = @workers.times.map { self.class.new(params.merge("workers" => 1, "codec" => @codec.clone)) }
+      @worker_plugins.map.with_index do |plugin, i|
+        Thread.new(original_params, @worker_queue) do |params, queue|
+          LogStash::Util::set_thread_name(">#{self.class.config_name}.#{i}")
+          plugin.register
+          while true
+            event = queue.pop
+            plugin.handle(event)
+          end
         end
       end
     end
diff --git a/lib/logstash/outputs/cloudwatch.rb b/lib/logstash/outputs/cloudwatch.rb
deleted file mode 100644
index bcd5e48a44a..00000000000
--- a/lib/logstash/outputs/cloudwatch.rb
+++ /dev/null
@@ -1,351 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "logstash/plugin_mixins/aws_config"
-
-# This output lets you aggregate and send metric data to AWS CloudWatch
-#
-# #### Summary:
-# This plugin is intended to be used on a logstash indexer agent (but that
-# is not the only way, see below.)  In the intended scenario, one cloudwatch
-# output plugin is configured, on the logstash indexer node, with just AWS API
-# credentials, and possibly a region and/or a namespace.  The output looks
-# for fields present in events, and when it finds them, it uses them to
-# calculate aggregate statistics.  If the `metricname` option is set in this
-# output, then any events which pass through it will be aggregated & sent to
-# CloudWatch, but that is not recommended.  The intended use is to NOT set the
-# metricname option here, and instead to add a `CW_metricname` field (and other
-# fields) to only the events you want sent to CloudWatch.
-#
-# When events pass through this output they are queued for background
-# aggregation and sending, which happens every minute by default.  The
-# queue has a maximum size, and when it is full aggregated statistics will be
-# sent to CloudWatch ahead of schedule. Whenever this happens a warning
-# message is written to logstash's log.  If you see this you should increase
-# the `queue_size` configuration option to avoid the extra API calls.  The queue
-# is emptied every time we send data to CloudWatch.
-#
-# Note: when logstash is stopped the queue is destroyed before it can be processed.
-# This is a known limitation of logstash and will hopefully be addressed in a
-# future version.
-#
-# #### Details:
-# There are two ways to configure this plugin, and they can be used in
-# combination: event fields & per-output defaults
-#
-# Event Field configuration...
-# You add fields to your events in inputs & filters and this output reads
-# those fields to aggregate events.  The names of the fields read are
-# configurable via the `field_*` options.
-#
-# Per-output defaults...
-# You set universal defaults in this output plugin's configuration, and
-# if an event does not have a field for that option then the default is
-# used.
-#
-# Notice, the event fields take precedence over the per-output defaults.
-#
-# At a minimum events must have a "metric name" to be sent to CloudWatch.
-# This can be achieved either by providing a default here OR by adding a
-# `CW_metricname` field. By default, if no other configuration is provided
-# besides a metric name, then events will be counted (Unit: Count, Value: 1)
-# by their metric name (either a default or from their `CW_metricname` field)
-#
-# Other fields which can be added to events to modify the behavior of this
-# plugin are, `CW_namespace`, `CW_unit`, `CW_value`, and 
-# `CW_dimensions`.  All of these field names are configurable in
-# this output.  You can also set per-output defaults for any of them.
-# See below for details.
-#
-# Read more about [AWS CloudWatch](http://aws.amazon.com/cloudwatch/),
-# and the specific of API endpoint this output uses,
-# [PutMetricData](http://docs.amazonwebservices.com/AmazonCloudWatch/latest/APIReference/API_PutMetricData.html)
-class LogStash::Outputs::CloudWatch < LogStash::Outputs::Base
-  include LogStash::PluginMixins::AwsConfig
-  
-  config_name "cloudwatch"
-  milestone 1
-
-  # Constants
-  # aggregate_key members
-  DIMENSIONS = "dimensions"
-  TIMESTAMP = "timestamp"
-  METRIC = "metric"
-  COUNT = "count"
-  UNIT = "unit"
-  SUM = "sum"
-  MIN = "min"
-  MAX = "max"
-  # Units
-  COUNT_UNIT = "Count"
-  NONE = "None"
-
-  # How often to send data to CloudWatch   
-  # This does not affect the event timestamps, events will always have their
-  # actual timestamp (to-the-minute) sent to CloudWatch.
-  #
-  # We only call the API if there is data to send.
-  #
-  # See the Rufus Scheduler docs for an [explanation of allowed values](https://github.com/jmettraux/rufus-scheduler#the-time-strings-understood-by-rufus-scheduler)
-  config :timeframe, :validate => :string, :default => "1m"
-
-  # How many events to queue before forcing a call to the CloudWatch API ahead of `timeframe` schedule   
-  # Set this to the number of events-per-timeframe you will be sending to CloudWatch to avoid extra API calls
-  config :queue_size, :validate => :number, :default => 10000
-
-  # The default namespace to use for events which do not have a `CW_namespace` field
-  config :namespace, :validate => :string, :default => "Logstash"
-
-  # The name of the field used to set a different namespace per event   
-  # Note: Only one namespace can be sent to CloudWatch per API call
-  # so setting different namespaces will increase the number of API calls
-  # and those cost money.
-  config :field_namespace, :validate => :string, :default => "CW_namespace"
-
-  # The default metric name to use for events which do not have a `CW_metricname` field.   
-  # Beware: If this is provided then all events which pass through this output will be aggregated and
-  # sent to CloudWatch, so use this carefully.  Furthermore, when providing this option, you
-  # will probably want to also restrict events from passing through this output using event
-  # type, tag, and field matching
-  config :metricname, :validate => :string
-
-  # The name of the field used to set the metric name on an event   
-  # The author of this plugin recommends adding this field to events in inputs &
-  # filters rather than using the per-output default setting so that one output
-  # plugin on your logstash indexer can serve all events (which of course had
-  # fields set on your logstash shippers.)
-  config :field_metricname, :validate => :string, :default => "CW_metricname"
-
-  VALID_UNITS = ["Seconds", "Microseconds", "Milliseconds", "Bytes",
-                 "Kilobytes", "Megabytes", "Gigabytes", "Terabytes",
-                 "Bits", "Kilobits", "Megabits", "Gigabits", "Terabits",
-                 "Percent", COUNT_UNIT, "Bytes/Second", "Kilobytes/Second",
-                 "Megabytes/Second", "Gigabytes/Second", "Terabytes/Second",
-                 "Bits/Second", "Kilobits/Second", "Megabits/Second",
-                 "Gigabits/Second", "Terabits/Second", "Count/Second", NONE]
-
-  # The default unit to use for events which do not have a `CW_unit` field   
-  # If you set this option you should probably set the "value" option along with it
-  config :unit, :validate => VALID_UNITS, :default => COUNT_UNIT
-
-  # The name of the field used to set the unit on an event metric   
-  config :field_unit, :validate => :string, :default => "CW_unit"
-
-  # The default value to use for events which do not have a `CW_value` field   
-  # If provided, this must be a string which can be converted to a float, for example...
-  #     "1", "2.34", ".5", and "0.67"
-  # If you set this option you should probably set the `unit` option along with it
-  config :value, :validate => :string, :default => "1"
-
-  # The name of the field used to set the value (float) on an event metric   
-  config :field_value, :validate => :string, :default => "CW_value"
-
-  # The default dimensions [ name, value, ... ] to use for events which do not have a `CW_dimensions` field   
-  config :dimensions, :validate => :hash
-
-  # The name of the field used to set the dimensions on an event metric   
-  # The field named here, if present in an event, must have an array of
-  # one or more key & value pairs, for example...
-  #     add_field => [ "CW_dimensions", "Environment", "CW_dimensions", "prod" ]
-  # or, equivalently...
-  #     add_field => [ "CW_dimensions", "Environment" ]
-  #     add_field => [ "CW_dimensions", "prod" ]
-  config :field_dimensions, :validate => :string, :default => "CW_dimensions"
-
-  public
-  def aws_service_endpoint(region)
-    return {
-        :cloud_watch_endpoint => "monitoring.#{region}.amazonaws.com"
-    }
-  end
-  
-  public
-  def register
-    require "thread"
-    require "rufus/scheduler"
-    require "aws"
-
-    @cw = AWS::CloudWatch.new(aws_options_hash)
-
-    @event_queue = SizedQueue.new(@queue_size)
-    @scheduler = Rufus::Scheduler.start_new
-    @job = @scheduler.every @timeframe do
-      @logger.info("Scheduler Activated")
-      publish(aggregate({}))
-    end
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    if event == LogStash::SHUTDOWN
-      job.trigger()
-      job.unschedule()
-      @logger.info("CloudWatch aggregator thread shutdown.")
-      finished
-      return
-    end
-
-    return unless (event[@field_metricname] || @metricname)
-
-    if (@event_queue.length >= @event_queue.max)
-      @job.trigger
-      @logger.warn("Posted to AWS CloudWatch ahead of schedule.  If you see this often, consider increasing the cloudwatch queue_size option.")
-    end
-
-    @logger.info("Queueing event", :event => event)
-    @event_queue << event
-  end # def receive
-
-  private
-  def publish(aggregates)
-    aggregates.each do |namespace, data|
-      @logger.info("Namespace, data: ", :namespace => namespace, :data => data)
-      metric_data = []
-      data.each do |aggregate_key, stats|
-        new_data = {
-            :metric_name => aggregate_key[METRIC],
-            :timestamp => aggregate_key[TIMESTAMP],
-            :unit => aggregate_key[UNIT],
-            :statistic_values => {
-                :sample_count => stats[COUNT],
-                :sum => stats[SUM],
-                :minimum => stats[MIN],
-                :maximum => stats[MAX],
-            }
-        }
-        dims = aggregate_key[DIMENSIONS]
-        if (dims.is_a?(Array) && dims.length > 0 && (dims.length % 2) == 0)
-          new_data[:dimensions] = Array.new
-          i = 0
-          while (i < dims.length)
-            new_data[:dimensions] << {:name => dims[i], :value => dims[i+1]}
-            i += 2
-          end
-        end
-        metric_data << new_data
-      end # data.each
-
-      begin
-        @cw.put_metric_data(
-            :namespace => namespace,
-            :metric_data => metric_data
-        )
-        @logger.info("Sent data to AWS CloudWatch OK", :namespace => namespace, :metric_data => metric_data)
-      rescue Exception => e
-        @logger.warn("Failed to send to AWS CloudWatch", :exception => e, :namespace => namespace, :metric_data => metric_data)
-        break
-      end
-    end # aggregates.each
-    return aggregates
-  end# def publish
-
-  private
-  def aggregate(aggregates)
-    @logger.info("QUEUE SIZE ", :queuesize => @event_queue.size)
-    while !@event_queue.empty? do
-      begin
-        count(aggregates, @event_queue.pop(true))
-      rescue Exception => e
-        @logger.warn("Exception!  Breaking count loop", :exception => e)
-        break
-      end
-    end
-    return aggregates
-  end # def aggregate
-
-  private
-  def count(aggregates, event)
-    # If the event doesn't declare a namespace, use the default
-    fnamespace = field(event, @field_namespace)
-    namespace = (fnamespace ? fnamespace : event.sprintf(@namespace))
-
-    funit = field(event, @field_unit)
-    unit = (funit ? funit : event.sprintf(@unit))
-
-    fvalue = field(event, @field_value)
-    value = (fvalue ? fvalue : event.sprintf(@value))
-
-    # We may get to this point with valid Units but missing value.  Send zeros.
-    val = (!value) ? 0.0 : value.to_f
-
-    # Event provides exactly one (but not both) of value or unit
-    if ( (fvalue == nil) ^ (funit == nil) )
-      @logger.warn("Likely config error: event has one of #{@field_value} or #{@field_unit} fields but not both.", :event => event)
-    end
-
-    # If Unit is still not set or is invalid warn about misconfiguration & use NONE
-    if (!VALID_UNITS.include?(unit))
-      unit = NONE
-      @logger.warn("Likely config error: invalid or missing Units (#{unit.to_s}), using '#{NONE}' instead", :event => event)
-    end
-
-    if (!aggregates[namespace])
-      aggregates[namespace] = {}
-    end
-
-    dims = event[@field_dimensions]
-    if (dims) # event provides dimensions
-              # validate the structure
-      if (!dims.is_a?(Array) || dims.length == 0 || (dims.length % 2) != 0)
-        @logger.warn("Likely config error: CloudWatch dimensions field (#{dims.to_s}) found which is not a positive- & even-length array.  Ignoring it.", :event => event)
-        dims = nil
-      end
-              # Best case, we get here and exit the conditional because dims...
-              # - is an array
-              # - with positive length
-              # - and an even number of elements
-    elsif (@dimensions.is_a?(Hash)) # event did not provide dimensions, but the output has been configured with a default
-      dims = @dimensions.flatten.map{|d| event.sprintf(d)} # into the kind of array described just above
-    else
-      dims = nil
-    end
-
-    fmetric = field(event, @field_metricname)
-    aggregate_key = {
-        METRIC => (fmetric ? fmetric : event.sprintf(@metricname)),
-        DIMENSIONS => dims,
-        UNIT => unit,
-        TIMESTAMP => event.sprintf("%{+YYYY-MM-dd'T'HH:mm:00Z}")
-    }
-
-    if (!aggregates[namespace][aggregate_key])
-      aggregates[namespace][aggregate_key] = {}
-    end
-
-    if (!aggregates[namespace][aggregate_key][MAX] || val > aggregates[namespace][aggregate_key][MAX])
-      aggregates[namespace][aggregate_key][MAX] = val
-    end
-
-    if (!aggregates[namespace][aggregate_key][MIN] || val < aggregates[namespace][aggregate_key][MIN])
-      aggregates[namespace][aggregate_key][MIN] = val
-    end
-
-    if (!aggregates[namespace][aggregate_key][COUNT])
-      aggregates[namespace][aggregate_key][COUNT] = 1
-    else
-      aggregates[namespace][aggregate_key][COUNT] += 1
-    end
-
-    if (!aggregates[namespace][aggregate_key][SUM])
-      aggregates[namespace][aggregate_key][SUM] = val
-    else
-      aggregates[namespace][aggregate_key][SUM] += val
-    end
-  end # def count
-
-  private
-  def field(event, fieldname)
-    if !event[fieldname]
-      return nil
-    else
-      if event[fieldname].is_a?(Array)
-        return event[fieldname][0]
-      else
-        return event[fieldname]
-      end
-    end
-  end # def field
-
-end # class LogStash::Outputs::CloudWatch
diff --git a/lib/logstash/outputs/csv.rb b/lib/logstash/outputs/csv.rb
deleted file mode 100644
index d35cb67c093..00000000000
--- a/lib/logstash/outputs/csv.rb
+++ /dev/null
@@ -1,55 +0,0 @@
-require "csv"
-require "logstash/namespace"
-require "logstash/outputs/file"
-
-# CSV output.
-#
-# Write events to disk in CSV or other delimited format
-# Based on the file output, many config values are shared
-# Uses the Ruby csv library internally
-class LogStash::Outputs::CSV < LogStash::Outputs::File
-
-  config_name "csv"
-  milestone 1
-
-  # The field names from the event that should be written to the CSV file.
-  # Fields are written to the CSV in the same order as the array.
-  # If a field does not exist on the event, an empty string will be written.
-  # Supports field reference syntax eg: `fields => ["field1", "[nested][field]"]`.
-  config :fields, :validate => :array, :required => true
-  
-  # Options for CSV output. This is passed directly to the Ruby stdlib to\_csv function. 
-  # Full documentation is available here: [http://ruby-doc.org/stdlib-2.0.0/libdoc/csv/rdoc/index.html].
-  # A typical use case would be to use alternative column or row seperators eg: `csv_options => {"col_sep" => "\t" "row_sep" => "\r\n"}` gives tab seperated data with windows line endings
-  config :csv_options, :validate => :hash, :required => false, :default => Hash.new
-
-  public
-  def register
-    super
-    @csv_options = Hash[@csv_options.map{|(k,v)|[k.to_sym, v]}]
-  end
-
-  public
-  def receive(event)
-    return unless output?(event)
-    path = event.sprintf(@path)
-    fd = open(path)
-    csv_values = @fields.map {|name| get_value(name, event)}
-    fd.write(csv_values.to_csv(@csv_options))
-
-    flush(fd)
-    close_stale_files
-  end #def receive
-
-  private
-  def get_value(name, event)
-    val = event[name]
-    case val
-      when Hash
-        return val.to_json
-      else
-        return val
-    end
-  end
-end # class LogStash::Outputs::CSV
-
diff --git a/lib/logstash/outputs/elasticsearch.rb b/lib/logstash/outputs/elasticsearch.rb
deleted file mode 100644
index 6b92e346626..00000000000
--- a/lib/logstash/outputs/elasticsearch.rb
+++ /dev/null
@@ -1,333 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/environment"
-require "logstash/outputs/base"
-require "stud/buffer"
-require "socket" # for Socket.gethostname
-
-# This output lets you store logs in Elasticsearch and is the most recommended
-# output for Logstash. If you plan on using the Kibana web interface, you'll
-# need to use this output.
-#
-#   *VERSION NOTE*: Your Elasticsearch cluster must be running Elasticsearch
-#   %ELASTICSEARCH_VERSION%. If you use any other version of Elasticsearch,
-#   you should set `protocol => http` in this plugin.
-#
-# If you want to set other Elasticsearch options that are not exposed directly
-# as configuration options, there are two methods:
-#
-# * Create an `elasticsearch.yml` file in the $PWD of the Logstash process
-# * Pass in es.* java properties (java -Des.node.foo= or ruby -J-Des.node.foo=)
-#
-# With the default `protocol` setting ("node"), this plugin will join your
-# Elasticsearch cluster as a client node, so it will show up in Elasticsearch's
-# cluster status.
-#
-# You can learn more about Elasticsearch at <http://www.elasticsearch.org>
-#
-# ## Operational Notes
-#
-# Template management requires Elasticsearch version 0.90.7 or later. If you
-# are using a version older than this, please upgrade. You will receive
-# more benefits than just template management!
-#
-# If using the default `protocol` setting ("node"), your firewalls might need
-# to permit port 9300 in *both* directions (from Logstash to Elasticsearch, and
-# Elasticsearch to Logstash)
-class LogStash::Outputs::ElasticSearch < LogStash::Outputs::Base
-  include Stud::Buffer
-
-  config_name "elasticsearch"
-  milestone 3
-
-  # The index to write events to. This can be dynamic using the %{foo} syntax.
-  # The default value will partition your indices by day so you can more easily
-  # delete old data or only search specific date ranges.
-  # Indexes may not contain uppercase characters.
-  config :index, :validate => :string, :default => "logstash-%{+YYYY.MM.dd}"
-
-  # The index type to write events to. Generally you should try to write only
-  # similar events to the same 'type'. String expansion '%{foo}' works here.
-  config :index_type, :validate => :string
-
-  # Starting in Logstash 1.3 (unless you set option "manage_template" to false)
-  # a default mapping template for Elasticsearch will be applied, if you do not
-  # already have one set to match the index pattern defined (default of
-  # "logstash-%{+YYYY.MM.dd}"), minus any variables.  For example, in this case
-  # the template will be applied to all indices starting with logstash-*
-  #
-  # If you have dynamic templating (e.g. creating indices based on field names)
-  # then you should set "manage_template" to false and use the REST API to upload
-  # your templates manually.
-  config :manage_template, :validate => :boolean, :default => true
-
-  # This configuration option defines how the template is named inside Elasticsearch.
-  # Note that if you have used the template management features and subsequently
-  # change this, you will need to prune the old template manually, e.g.
-  # curl -XDELETE <http://localhost:9200/_template/OldTemplateName?pretty>
-  # where OldTemplateName is whatever the former setting was.
-  config :template_name, :validate => :string, :default => "logstash"
-
-  # You can set the path to your own template here, if you so desire.
-  # If not set, the included template will be used.
-  config :template, :validate => :path
-
-  # Overwrite the current template with whatever is configured
-  # in the template and template_name directives.
-  config :template_overwrite, :validate => :boolean, :default => false
-
-  # The document ID for the index. Useful for overwriting existing entries in
-  # Elasticsearch with the same ID.
-  config :document_id, :validate => :string, :default => nil
-
-  # The name of your cluster if you set it on the Elasticsearch side. Useful
-  # for discovery.
-  config :cluster, :validate => :string
-
-  # The hostname or IP address of the host to use for Elasticsearch unicast discovery
-  # This is only required if the normal multicast/cluster discovery stuff won't
-  # work in your environment.
-  config :host, :validate => :string
-
-  # The port for Elasticsearch transport to use.
-  #
-  # If you do not set this, the following defaults are used:
-  # * `protocol => http` - port 9200
-  # * `protocol => transport` - port 9300-9305
-  # * `protocol => node` - port 9300-9305
-  config :port, :validate => :string
-
-  # The name/address of the host to bind to for Elasticsearch clustering
-  config :bind_host, :validate => :string
-
-  # This is only valid for the 'node' protocol.
-  #
-  # The port for the node to listen on.
-  config :bind_port, :validate => :number
-
-  # Run the Elasticsearch server embedded in this process.
-  # This option is useful if you want to run a single Logstash process that
-  # handles log processing and indexing; it saves you from needing to run
-  # a separate Elasticsearch process.
-  config :embedded, :validate => :boolean, :default => false
-
-  # If you are running the embedded Elasticsearch server, you can set the http
-  # port it listens on here; it is not common to need this setting changed from
-  # default.
-  config :embedded_http_port, :validate => :string, :default => "9200-9300"
-
-  # This setting no longer does anything. It exists to keep config validation
-  # from failing. It will be removed in future versions.
-  config :max_inflight_requests, :validate => :number, :default => 50, :deprecated => true
-
-  # The node name Elasticsearch will use when joining a cluster.
-  #
-  # By default, this is generated internally by the ES client.
-  config :node_name, :validate => :string
-
-  # This plugin uses the bulk index api for improved indexing performance.
-  # To make efficient bulk api calls, we will buffer a certain number of
-  # events before flushing that out to Elasticsearch. This setting
-  # controls how many events will be buffered before sending a batch
-  # of events.
-  config :flush_size, :validate => :number, :default => 5000
-
-  # The amount of time since last flush before a flush is forced.
-  #
-  # This setting helps ensure slow event rates don't get stuck in Logstash.
-  # For example, if your `flush_size` is 100, and you have received 10 events,
-  # and it has been more than `idle_flush_time` seconds since the last flush,
-  # Logstash will flush those 10 events automatically.
-  #
-  # This helps keep both fast and slow log streams moving along in
-  # near-real-time.
-  config :idle_flush_time, :validate => :number, :default => 1
-
-  # Choose the protocol used to talk to Elasticsearch.
-  #
-  # The 'node' protocol will connect to the cluster as a normal Elasticsearch
-  # node (but will not store data). This allows you to use things like
-  # multicast discovery. If you use the `node` protocol, you must permit
-  # bidirectional communication on the port 9300 (or whichever port you have
-  # configured).
-  #
-  # The 'transport' protocol will connect to the host you specify and will
-  # not show up as a 'node' in the Elasticsearch cluster. This is useful
-  # in situations where you cannot permit connections outbound from the
-  # Elasticsearch cluster to this Logstash server.
-  #
-  # The 'http' protocol will use the Elasticsearch REST/HTTP interface to talk
-  # to elasticsearch.
-  #
-  # All protocols will use bulk requests when talking to Elasticsearch.
-  #
-  # The default `protocol` setting under java/jruby is "node". The default
-  # `protocol` on non-java rubies is "http"
-  config :protocol, :validate => [ "node", "transport", "http" ]
-
-  # The Elasticsearch action to perform. Valid actions are: `index`, `delete`.
-  #
-  # Use of this setting *REQUIRES* you also configure the `document_id` setting
-  # because `delete` actions all require a document id.
-  #
-  # What does each action do?
-  #
-  # - index: indexes a document (an event from logstash).
-  # - delete: deletes a document by id
-  #
-  # For more details on actions, check out the [Elasticsearch bulk API documentation](http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/docs-bulk.html)
-  config :action, :validate => :string, :default => "index"
-
-  public
-  def register
-    client_settings = {}
-    client_settings["cluster.name"] = @cluster if @cluster
-    client_settings["network.host"] = @bind_host if @bind_host
-    client_settings["transport.tcp.port"] = @bind_port if @bind_port
-
-    if @node_name
-      client_settings["node.name"] = @node_name
-    else
-      client_settings["node.name"] = "logstash-#{Socket.gethostname}-#{$$}-#{object_id}"
-    end
-
-    if @protocol.nil?
-      @protocol = LogStash::Environment.jruby? ? "node" : "http"
-    end
-
-    if ["node", "transport"].include?(@protocol)
-      # Node or TransportClient; requires JRuby
-      raise(LogStash::PluginLoadingError, "This configuration requires JRuby. If you are not using JRuby, you must set 'protocol' to 'http'. For example: output { elasticsearch { protocol => \"http\" } }") unless LogStash::Environment.jruby?
-      LogStash::Environment.load_elasticsearch_jars!
-
-      # setup log4j properties for Elasticsearch
-      LogStash::Logger.setup_log4j(@logger)
-    end
-
-    require "logstash/outputs/elasticsearch/protocol"
-
-    if @port.nil?
-      @port = case @protocol
-        when "http"; "9200"
-        when "transport", "node"; "9300-9305"
-      end
-    end
-
-    if @host.nil? && @protocol == "http"
-      @logger.info("No 'host' set in elasticsearch output. Defaulting to localhost")
-      @host = "localhost"
-    end
-
-    options = {
-      :host => @host,
-      :port => @port,
-      :client_settings => client_settings
-    }
-
-
-    client_class = case @protocol
-      when "transport"
-        LogStash::Outputs::Elasticsearch::Protocols::TransportClient
-      when "node"
-        LogStash::Outputs::Elasticsearch::Protocols::NodeClient
-      when "http"
-        LogStash::Outputs::Elasticsearch::Protocols::HTTPClient
-    end
-
-    if @embedded
-      raise(LogStash::ConfigurationError, "The 'embedded => true' setting is only valid for the elasticsearch output under JRuby. You are running #{RUBY_DESCRIPTION}") unless LogStash::Environment.jruby?
-      LogStash::Environment.load_elasticsearch_jars!
-
-      # Default @host with embedded to localhost. This should help avoid
-      # newbies tripping on ubuntu and other distros that have a default
-      # firewall that blocks multicast.
-      @host ||= "localhost"
-
-      # Start Elasticsearch local.
-      start_local_elasticsearch
-    end
-
-    @client = client_class.new(options)
-
-    @logger.info("New Elasticsearch output", :cluster => @cluster,
-                 :host => @host, :port => @port, :embedded => @embedded,
-                 :protocol => @protocol)
-
-
-    if @manage_template
-      @logger.info("Automatic template management enabled", :manage_template => @manage_template.to_s)
-      @client.template_install(@template_name, get_template, @template_overwrite)
-    end # if @manage_templates
-
-    buffer_initialize(
-      :max_items => @flush_size,
-      :max_interval => @idle_flush_time,
-      :logger => @logger
-    )
-  end # def register
-
-  public
-  def get_template
-    if @template.nil?
-      if File.exists?("elasticsearch-template.json")
-        @template = "elasticsearch-template.json"
-      else
-        path = File.join(File.dirname(__FILE__), "elasticsearch/elasticsearch-template.json")
-        if File.exists?(path)
-          @template = path
-        else
-          raise "You must specify 'template => ...' in your elasticsearch_http output"
-        end
-      end
-    end
-    template_json = IO.read(@template).gsub(/\n/,'')
-    @logger.info("Using mapping template", :template => template_json)
-    return JSON.parse(template_json)
-  end # def get_template
-
-  protected
-  def start_local_elasticsearch
-    @logger.info("Starting embedded Elasticsearch local node.")
-    builder = org.elasticsearch.node.NodeBuilder.nodeBuilder
-    # Disable 'local only' - LOGSTASH-277
-    #builder.local(true)
-    builder.settings.put("cluster.name", @cluster) if @cluster
-    builder.settings.put("node.name", @node_name) if @node_name
-    builder.settings.put("network.host", @bind_host) if @bind_host
-    builder.settings.put("http.port", @embedded_http_port)
-
-    @embedded_elasticsearch = builder.node
-    @embedded_elasticsearch.start
-  end # def start_local_elasticsearch
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    # Set the 'type' value for the index.
-    if @index_type
-      type = event.sprintf(@index_type)
-    else
-      type = event["type"] || "logs"
-    end
-
-    index = event.sprintf(@index)
-
-    document_id = @document_id ? event.sprintf(@document_id) : nil
-    buffer_receive([event.sprintf(@action), { :_id => document_id, :_index => index, :_type => type }, event.to_hash])
-  end # def receive
-
-  def flush(actions, teardown=false)
-    @client.bulk(actions)
-    # TODO(sissel): Handle errors. Since bulk requests could mostly succeed
-    # (aka partially fail), we need to figure out what documents need to be
-    # retried.
-    #
-    # In the worst case, a failing flush (exception) will incur a retry from Stud::Buffer.
-  end # def flush
-
-  def teardown
-    buffer_flush(:final => true)
-  end
-
-end # class LogStash::Outputs::Elasticsearch
diff --git a/lib/logstash/outputs/elasticsearch/elasticsearch-template.json b/lib/logstash/outputs/elasticsearch/elasticsearch-template.json
deleted file mode 100644
index 3f9c8cc4f86..00000000000
--- a/lib/logstash/outputs/elasticsearch/elasticsearch-template.json
+++ /dev/null
@@ -1,34 +0,0 @@
-{
-  "template" : "logstash-*",
-  "settings" : {
-    "index.refresh_interval" : "5s"
-  },
-  "mappings" : {
-    "_default_" : {
-       "_all" : {"enabled" : true},
-       "dynamic_templates" : [ {
-         "string_fields" : {
-           "match" : "*",
-           "match_mapping_type" : "string",
-           "mapping" : {
-             "type" : "string", "index" : "analyzed", "omit_norms" : true,
-               "fields" : {
-                 "raw" : {"type": "string", "index" : "not_analyzed", "ignore_above" : 256}
-               }
-           }
-         }
-       } ],
-       "properties" : {
-         "@version": { "type": "string", "index": "not_analyzed" },
-         "geoip"  : {
-           "type" : "object",
-             "dynamic": true,
-             "path": "full",
-             "properties" : {
-               "location" : { "type" : "geo_point" }
-             }
-         }
-       }
-    }
-  }
-}
diff --git a/lib/logstash/outputs/elasticsearch/protocol.rb b/lib/logstash/outputs/elasticsearch/protocol.rb
deleted file mode 100644
index 876598ec454..00000000000
--- a/lib/logstash/outputs/elasticsearch/protocol.rb
+++ /dev/null
@@ -1,271 +0,0 @@
-require "logstash/outputs/elasticsearch"
-require "cabin"
-
-module LogStash::Outputs::Elasticsearch
-  module Protocols
-    class Base
-      private
-      def initialize(options={})
-        # host(s), port, cluster
-        @logger = Cabin::Channel.get
-      end
-
-      def client
-        return @client if @client
-        @client = build_client(@options)
-        return @client
-      end # def client
-
-
-      def template_install(name, template, force=false)
-        if template_exists?(name) && !force
-          @logger.debug("Found existing Elasticsearch template. Skipping template management", :name => name)
-          return
-        end
-        template_put(name, template)
-      end
-
-      # Do a bulk request with the given actions.
-      #
-      # 'actions' is expected to be an array of bulk requests as string json
-      # values.
-      #
-      # Each 'action' becomes a single line in the bulk api call. For more
-      # details on the format of each.
-      def bulk(actions)
-        raise NotImplemented, "You must implement this yourself"
-        # bulk([
-        # '{ "index" : { "_index" : "test", "_type" : "type1", "_id" : "1" } }',
-        # '{ "field1" : "value1" }'
-        #])
-      end
-
-      public(:initialize, :template_install)
-    end
-
-    class HTTPClient < Base
-      private
-
-      DEFAULT_OPTIONS = {
-        :port => 9200
-      }
-
-      def initialize(options={})
-        require "ftw"
-        super
-        require "elasticsearch" # gem 'elasticsearch-ruby'
-        @options = DEFAULT_OPTIONS.merge(options)
-        @client = client
-      end
-
-      def build_client(options)
-        client = Elasticsearch::Client.new(
-          :host => [options[:host], options[:port]].join(":")
-        )
-
-        # Use FTW to do indexing requests, for now, until we
-        # can identify and resolve performance problems of elasticsearch-ruby
-        @bulk_url = "http://#{options[:host]}:#{options[:port]}/_bulk"
-        @agent = FTW::Agent.new
-
-        return client
-      end
-
-      if ENV["BULK"] == "esruby"
-        def bulk(actions)
-          bulk_esruby(actions)
-        end
-      else
-        def bulk(actions)
-          bulk_ftw(actions)
-        end
-      end
-      
-      def bulk_esruby(actions)
-        @client.bulk(:body => actions.collect do |action, args, source|
-          if source
-            next [ { action => args }, source ]
-          else
-            next { action => args }
-          end
-        end.flatten)
-      end # def bulk_esruby
-
-      # Avoid creating a new string for newline every time
-      NEWLINE = "\n".freeze
-      def bulk_ftw(actions)
-        body = actions.collect do |action, args, source|
-          header = { action => args }
-          if source
-            next [ header.to_json, NEWLINE, source.to_json, NEWLINE ]
-          else
-            next [ header.to_json, NEWLINE ]
-          end
-        end.flatten.join("")
-        begin
-          response = @agent.post!(@bulk_url, :body => body)
-        rescue EOFError
-          @logger.warn("EOF while writing request or reading response header from elasticsearch", :host => @host, :port => @port)
-          raise
-        end
-
-        # Consume the body for error checking
-        # This will also free up the connection for reuse.
-        response_body = ""
-        begin
-          response.read_body { |chunk| response_body += chunk }
-        rescue EOFError
-          @logger.warn("EOF while reading response body from elasticsearch",
-                       :url => @bulk_url)
-          raise
-        end
-
-        if response.status != 200
-          @logger.error("Error writing (bulk) to elasticsearch",
-                        :response => response, :response_body => response_body,
-                        :request_body => body)
-          raise "Non-OK response code from Elasticsearch: #{response.status}"
-        end
-      end # def bulk_ftw
-
-      def template_exists?(name)
-        @client.indices.get_template(:name => name)
-        return true
-      rescue Elasticsearch::Transport::Transport::Errors::NotFound
-        return false
-      end # def template_exists?
-
-      def template_put(name, template)
-        @client.indices.put_template(:name => name, :body => template)
-      end # template_put
-
-      public(:bulk)
-    end # class HTTPClient
-
-    class NodeClient < Base
-      private
-
-      DEFAULT_OPTIONS = {
-        :port => 9300,
-      }
-
-      def initialize(options={})
-        super
-        require "java"
-        @options = DEFAULT_OPTIONS.merge(options)
-        setup(@options)
-        @client = client
-      end # def initialize
-
-      def settings
-        return @settings
-      end
-
-      def setup(options={})
-        @settings = org.elasticsearch.common.settings.ImmutableSettings.settingsBuilder
-        if options[:host]
-          @settings.put("discovery.zen.ping.multicast.enabled", false)
-          @settings.put("discovery.zen.ping.unicast.hosts", hosts(options))
-        end
-
-        @settings.put("node.client", true)
-        @settings.put("http.enabled", false)
-        
-        if options[:client_settings]
-          options[:client_settings].each do |key, value|
-            @settings.put(key, value)
-          end
-        end
-
-        return @settings
-      end
-
-      def hosts(options)
-        if options[:port].to_s =~ /^\d+-\d+$/
-          # port ranges are 'host[port1-port2]' according to 
-          # http://www.elasticsearch.org/guide/reference/modules/discovery/zen/
-          # However, it seems to only query the first port.
-          # So generate our own list of unicast hosts to scan.
-          range = Range.new(*options[:port].split("-"))
-          return range.collect { |p| "#{options[:host]}:#{p}" }.join(",")
-        else
-          return "#{options[:host]}:#{options[:port]}"
-        end
-      end # def hosts
-
-      def build_client(options)
-        nodebuilder = org.elasticsearch.node.NodeBuilder.nodeBuilder
-        return nodebuilder.settings(@settings).node.client
-      end # def build_client
-
-      def bulk(actions)
-        # Actions an array of [ action, action_metadata, source ]
-        prep = @client.prepareBulk
-        actions.each do |action, args, source|
-          prep.add(build_request(action, args, source))
-        end
-        response = prep.execute.actionGet()
-
-        # TODO(sissel): What format should the response be in?
-      end # def bulk
-
-      def build_request(action, args, source)
-        case action
-          when "index"
-            request = org.elasticsearch.action.index.IndexRequest.new(args[:_index])
-            request.id(args[:_id]) if args[:_id]
-            request.source(source)
-          when "delete"
-            request = org.elasticsearch.action.delete.DeleteRequest.new(args[:_index])
-            request.id(args[:_id])
-          #when "update"
-          #when "create"
-        end # case action
-
-        request.type(args[:_type]) if args[:_type]
-        return request
-      end # def build_request
-
-      def template_exists?(name)
-        request = org.elasticsearch.action.admin.indices.template.get.GetIndexTemplatesRequestBuilder.new(@client.admin.indices, name)
-        response = request.get
-        return !response.getIndexTemplates.isEmpty
-      end # def template_exists?
-
-      def template_put(name, template)
-        request = org.elasticsearch.action.admin.indices.template.put.PutIndexTemplateRequestBuilder.new(@client.admin.indices, name)
-        request.setSource(template.to_json)
-
-        # execute the request and get the response, if it fails, we'll get an exception.
-        request.get
-      end # template_put
-
-      public(:initialize, :bulk)
-    end # class NodeClient
-
-    class TransportClient < NodeClient
-      private
-      def build_client(options)
-        client = org.elasticsearch.client.transport.TransportClient.new(settings.build)
-
-        if options[:host]
-          client.addTransportAddress(
-            org.elasticsearch.common.transport.InetSocketTransportAddress.new(
-              options[:host], options[:port].to_i
-            )
-          )
-        end
-
-        return client
-      end # def build_client
-    end # class TransportClient
-  end # module Protocols
-
-  module Requests
-    class GetIndexTemplates; end
-    class Bulk; end
-    class Index; end
-    class Delete; end
-  end
-end
-
diff --git a/lib/logstash/outputs/elasticsearch_http.rb b/lib/logstash/outputs/elasticsearch_http.rb
deleted file mode 100644
index 32a85d0f27f..00000000000
--- a/lib/logstash/outputs/elasticsearch_http.rb
+++ /dev/null
@@ -1,247 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
-require "stud/buffer"
-
-# This output lets you store logs in Elasticsearch.
-#
-# This plugin uses the HTTP/REST interface to Elasticsearch, which usually
-# lets you use any version of Elasticsearch server. It is known to work
-# with elasticsearch %ELASTICSEARCH_VERSION%
-#
-# You can learn more about Elasticsearch at <http://www.elasticsearch.org>
-class LogStash::Outputs::ElasticSearchHTTP < LogStash::Outputs::Base
-  include Stud::Buffer
-
-  config_name "elasticsearch_http"
-  milestone 2
-
-  # The index to write events to. This can be dynamic using the %{foo} syntax.
-  # The default value will partition your indices by day so you can more easily
-  # delete old data or only search specific date ranges.
-  config :index, :validate => :string, :default => "logstash-%{+YYYY.MM.dd}"
-
-  # The index type to write events to. Generally you should try to write only
-  # similar events to the same 'type'. String expansion '%{foo}' works here.
-  config :index_type, :validate => :string
-
-  # Starting in Logstash 1.3 (unless you set option "manage_template" to false)
-  # a default mapping template for Elasticsearch will be applied, if you do not
-  # already have one set to match the index pattern defined (default of
-  # "logstash-%{+YYYY.MM.dd}"), minus any variables.  For example, in this case
-  # the template will be applied to all indices starting with logstash-*
-  #
-  # If you have dynamic templating (e.g. creating indices based on field names)
-  # then you should set "manage_template" to false and use the REST API to upload
-  # your templates manually.
-  config :manage_template, :validate => :boolean, :default => true
-
-  # This configuration option defines how the template is named inside Elasticsearch.
-  # Note that if you have used the template management features and subsequently
-  # change this you will need to prune the old template manually, e.g.
-  # curl -XDELETE <http://localhost:9200/_template/OldTemplateName?pretty>
-  # where OldTemplateName is whatever the former setting was.
-  config :template_name, :validate => :string, :default => "logstash"
-
-  # You can set the path to your own template here, if you so desire.
-  # If not the included template will be used.
-  config :template, :validate => :path
-
-  # Overwrite the current template with whatever is configured
-  # in the template and template_name directives.
-  config :template_overwrite, :validate => :boolean, :default => false
-
-  # The hostname or IP address to reach your Elasticsearch server.
-  config :host, :validate => :string, :required => true
-
-  # The port for Elasticsearch HTTP interface to use.
-  config :port, :validate => :number, :default => 9200
-
-  # The HTTP Basic Auth username used to access your elasticsearch server.
-  config :user, :validate => :string, :default => nil
-
-  # The HTTP Basic Auth password used to access your elasticsearch server.
-  config :password, :validate => :password, :default => nil
-
-  # This plugin uses the bulk index api for improved indexing performance.
-  # To make efficient bulk api calls, we will buffer a certain number of
-  # events before flushing that out to Elasticsearch. This setting
-  # controls how many events will be buffered before sending a batch
-  # of events.
-  config :flush_size, :validate => :number, :default => 100
-
-  # The amount of time since last flush before a flush is forced.
-  #
-  # This setting helps ensure slow event rates don't get stuck in Logstash.
-  # For example, if your `flush_size` is 100, and you have received 10 events,
-  # and it has been more than `idle_flush_time` seconds since the last flush,
-  # logstash will flush those 10 events automatically.
-  #
-  # This helps keep both fast and slow log streams moving along in
-  # near-real-time.
-  config :idle_flush_time, :validate => :number, :default => 1
-
-  # The document ID for the index. Useful for overwriting existing entries in
-  # Elasticsearch with the same ID.
-  config :document_id, :validate => :string, :default => nil
-
-  # Set the type of Elasticsearch replication to use. If async
-  # the index request to Elasticsearch to return after the primary
-  # shards have been written. If sync (default), index requests
-  # will wait until the primary and the replica shards have been
-  # written.
-  config :replication, :validate => ['async', 'sync'], :default => 'sync'
-
-  public
-  def register
-    require "ftw" # gem ftw
-    @agent = FTW::Agent.new
-    @queue = []
-
-    auth = @user && @password ? "#{@user}:#{@password.value}@" : ""
-    @bulk_url = "http://#{auth}#{@host}:#{@port}/_bulk?replication=#{@replication}"
-    if @manage_template
-      @logger.info("Automatic template management enabled", :manage_template => @manage_template.to_s)
-      template_search_url = "http://#{auth}#{@host}:#{@port}/_template/*"
-      @template_url = "http://#{auth}#{@host}:#{@port}/_template/#{@template_name}"
-      if @template_overwrite
-        @logger.info("Template overwrite enabled.  Deleting existing template.", :template_overwrite => @template_overwrite.to_s)
-        response = @agent.get!(@template_url)
-        template_action('delete') if response.status == 200 #=> Purge the old template if it exists
-      end
-      @logger.debug("Template Search URL:", :template_search_url => template_search_url)
-      has_template = false
-      template_idx_name = @index.sub(/%{[^}]+}/,'*')
-      alt_template_idx_name = @index.sub(/-%{[^}]+}/,'*')
-      # Get the template data
-      response = @agent.get!(template_search_url)
-      json = ""
-      if response.status == 404 #=> This condition can occcur when no template has ever been appended
-        @logger.info("No template found in Elasticsearch...")
-        get_template_json
-        template_action('put')
-      elsif response.status == 200
-        begin
-          response.read_body { |c| json << c }
-          results = JSON.parse(json)
-        rescue Exception => e
-          @logger.error("Error parsing JSON", :json => json, :results => results.to_s, :error => e.to_s)
-          raise "Exception in parsing JSON", e
-        end
-        if !results.any? { |k,v| v["template"] == template_idx_name || v["template"] == alt_template_idx_name }
-          @logger.debug("No template found in Elasticsearch", :has_template => has_template, :name => template_idx_name, :alt => alt_template_idx_name)
-          get_template_json
-          template_action('put')
-        end
-      else #=> Some other status code?
-        @logger.error("Could not check for existing template.  Check status code.", :status => response.status.to_s)
-      end # end if response.status == 200
-    end # end if @manage_template
-    buffer_initialize(
-      :max_items => @flush_size,
-      :max_interval => @idle_flush_time,
-      :logger => @logger
-    )
-  end # def register
-
-  public
-  def template_action(command)
-    begin
-      if command == 'delete'
-        response = @agent.delete!(@template_url)
-        response.discard_body
-      elsif command == 'put'
-        response = @agent.put!(@template_url, :body => @template_json)
-        response.discard_body
-      end
-    rescue EOFError
-      @logger.warn("EOF while attempting request or reading response header from elasticsearch",
-                   :host => @host, :port => @port)
-      return # abort this action
-    end
-    if response.status != 200
-      @logger.error("Error acting on elasticsearch mapping template",
-                    :response => response, :action => command,
-                    :request_url => @template_url)
-      return
-    end
-    @logger.info("Successfully deleted template", :template_url => @template_url) if command == 'delete'
-    @logger.info("Successfully applied template", :template_url => @template_url) if command == 'put'
-  end # def template_action
-
-
-  public
-  def get_template_json
-    if @template.nil?
-      if File.exists?("elasticsearch-template.json")
-        @template = "elasticsearch-template.json"
-      elsif File.exists?("lib/logstash/outputs/elasticsearch/elasticsearch-template.json")
-        @template = "lib/logstash/outputs/elasticsearch/elasticsearch-template.json"
-      else
-        raise "You must specify 'template => ...' in your elasticsearch_http output"
-      end
-    end
-    @template_json = IO.read(@template).gsub(/\n/,'')
-    @logger.info("Using mapping template", :template => @template_json)
-  end # def get_template
-
-  public
-  def receive(event)
-    return unless output?(event)
-    buffer_receive([event, index, type])
-  end # def receive
-
-  def flush(events, teardown=false)
-    # Avoid creating a new string for newline every time
-    newline = "\n".freeze
-
-    body = events.collect do |event, index, type|
-      index = event.sprintf(@index)
-
-      # Set the 'type' value for the index.
-      if @index_type.nil?
-        type =  event["type"] || "logs"
-      else
-        type = event.sprintf(@index_type)
-      end
-      header = { "index" => { "_index" => index, "_type" => type } }
-      header["index"]["_id"] = event.sprintf(@document_id) if !@document_id.nil?
-
-      [ header.to_json, newline, event.to_json, newline ]
-    end.flatten
-
-    post(body.join(""))
-  end # def receive_bulk
-
-  def post(body)
-    begin
-      response = @agent.post!(@bulk_url, :body => body)
-    rescue EOFError
-      @logger.warn("EOF while writing request or reading response header from elasticsearch",
-                   :host => @host, :port => @port)
-      raise
-    end
-
-    # Consume the body for error checking
-    # This will also free up the connection for reuse.
-    body = ""
-    begin
-      response.read_body { |chunk| body += chunk }
-    rescue EOFError
-      @logger.warn("EOF while reading response body from elasticsearch",
-                   :host => @host, :port => @port)
-      raise
-    end
-
-    if response.status != 200
-      @logger.error("Error writing (bulk) to elasticsearch",
-                    :response => response, :response_body => body,
-                    :request_body => @queue.join("\n"))
-      raise
-    end
-  end # def post
-
-  def teardown
-    buffer_flush(:final => true)
-  end # def teardown
-end # class LogStash::Outputs::ElasticSearchHTTP
diff --git a/lib/logstash/outputs/elasticsearch_river.rb b/lib/logstash/outputs/elasticsearch_river.rb
deleted file mode 100644
index c3ac9c6b599..00000000000
--- a/lib/logstash/outputs/elasticsearch_river.rb
+++ /dev/null
@@ -1,206 +0,0 @@
-# encoding: utf-8
-require "logstash/environment"
-require "logstash/namespace"
-require "logstash/outputs/base"
-require "json"
-require "uri"
-require "net/http"
-
-# This output lets you store logs in elasticsearch. It's similar to the
-# 'elasticsearch' output but improves performance by using a queue server,
-# rabbitmq, to send data to elasticsearch.
-#
-# Upon startup, this output will automatically contact an elasticsearch cluster
-# and configure it to read from the queue to which we write.
-#
-# You can learn more about elasticseasrch at <http://elasticsearch.org>
-# More about the elasticsearch rabbitmq river plugin: <https://github.com/elasticsearch/elasticsearch-river-rabbitmq/blob/master/README.md>
-
-class LogStash::Outputs::ElasticSearchRiver < LogStash::Outputs::Base
-
-  config_name "elasticsearch_river"
-  milestone 2
-
-  # The index to write events to. This can be dynamic using the %{foo} syntax.
-  # The default value will partition your indeces by day so you can more easily
-  # delete old data or only search specific date ranges.
-  config :index, :validate => :string, :default => "logstash-%{+YYYY.MM.dd}"
-
-  # The index type to write events to. Generally you should try to write only
-  # similar events to the same 'type'. String expansion '%{foo}' works here.
-  config :index_type, :validate => :string, :default => "%{type}"
-
-  # The name/address of an ElasticSearch host to use for river creation
-  config :es_host, :validate => :string, :required => true
-
-  # ElasticSearch API port
-  config :es_port, :validate => :number, :default => 9200
-
-  # ElasticSearch river configuration: bulk fetch size
-  config :es_bulk_size, :validate => :number, :default => 1000
-
-  # ElasticSearch river configuration: bulk timeout in milliseconds
-  config :es_bulk_timeout_ms, :validate => :number, :default => 100
-
-  # ElasticSearch river configuration: is ordered?
-  config :es_ordered, :validate => :boolean, :default => false
-
-  # Hostname of RabbitMQ server
-  config :rabbitmq_host, :validate => :string, :required => true
-
-  # Port of RabbitMQ server
-  config :rabbitmq_port, :validate => :number, :default => 5672
-
-  # RabbitMQ user
-  config :user, :validate => :string, :default => "guest"
-
-  # RabbitMQ password
-  config :password, :validate => :string, :default => "guest"
-
-  # RabbitMQ vhost
-  config :vhost, :validate => :string, :default => "/"
-
-  # RabbitMQ queue name
-  config :queue, :validate => :string, :default => "elasticsearch"
-
-  # RabbitMQ exchange name
-  config :exchange, :validate => :string, :default => "elasticsearch"
-
-  # The exchange type (fanout, topic, direct)
-  config :exchange_type, :validate => [ "fanout", "direct", "topic"],
-         :default => "direct"
-
-  # RabbitMQ routing key
-  config :key, :validate => :string, :default => "elasticsearch"
-
-  # RabbitMQ durability setting. Also used for ElasticSearch setting
-  config :durable, :validate => :boolean, :default => true
-
-  # RabbitMQ persistence setting
-  config :persistent, :validate => :boolean, :default => true
-
-  # The document ID for the index. Useful for overwriting existing entries in
-  # elasticsearch with the same ID.
-  config :document_id, :validate => :string, :default => nil
-
-  public
-  def register
-    LogStash::Environment.load_elasticsearch_jars!
-    prepare_river
-  end
-
-  protected
-  def prepare_river
-    require "logstash/outputs/rabbitmq"
-
-    # Configure the message plugin
-    params = {
-      "host" => [@rabbitmq_host],
-      "port" => [@rabbitmq_port],
-      "user" => [@user],
-      "password" => [@password],
-      "exchange_type" => [@exchange_type],
-      "exchange" => [@exchange],
-      "key" => [@key],
-      "vhost" => [@vhost],
-      "durable" => [@durable.to_s],
-      "persistent" => [@persistent.to_s],
-      "debug" => [@logger.debug?.to_s],
-    }.reject {|k,v| v.first.nil?}
-    @mq = LogStash::Outputs::RabbitMQ.new(params)
-    @mq.register
-
-    # Set up the river
-    begin
-      auth = "#{@user}:#{@password}"
-
-      # Name the river by our hostname
-      require "socket"
-      hostname = Socket.gethostname
-
-      # Replace spaces with hyphens and remove all non-alpha non-dash non-underscore characters
-      river_name = "#{hostname} #{@queue}".gsub(' ', '-').gsub(/[^\w-]/, '')
-
-      api_path = "/_river/logstash-#{river_name}/_meta"
-      @status_path = "/_river/logstash-#{river_name}/_status"
-
-      river_config = {"type" => "rabbitmq",
-                      "rabbitmq" => {
-                                "host" => @rabbitmq_host=="localhost" ? hostname : @rabbitmq_host,
-                                "port" => @rabbitmq_port,
-                                "user" => @user,
-                                "pass" => @password,
-                                "vhost" => @vhost,
-                                "queue" => @queue,
-                                "exchange" => @exchange,
-                                "routing_key" => @key,
-                                "exchange_type" => @exchange_type,
-                                "exchange_durable" => @durable.to_s,
-                                "queue_durable" => @durable.to_s
-                               },
-                      "index" => {"bulk_size" => @es_bulk_size,
-                                 "bulk_timeout" => "#{@es_bulk_timeout_ms}ms",
-                                 "ordered" => @es_ordered
-                                },
-                     }
-      @logger.info("ElasticSearch using river", :config => river_config)
-      Net::HTTP.start(@es_host, @es_port) do |http|
-        req = Net::HTTP::Put.new(api_path)
-        req.body = river_config.to_json
-        response = http.request(req)
-        response.value() # raise an exception if error
-        @logger.info("River created: #{response.body}")
-      end
-    rescue Exception => e
-      # TODO(petef): should we just throw an exception here, so the
-      # agent tries to restart us and we in turn retry the river
-      # registration?
-      @logger.warn("Couldn't set up river. You'll have to set it up manually (or restart)", :exception => e)
-    end
-
-    check_river_status
-  end # def prepare_river
-
-  private
-  def check_river_status
-    tries = 0
-    success = false
-    reason = nil
-    begin
-      while !success && tries <= 3 do
-        tries += 1
-        Net::HTTP.start(@es_host, @es_port) do |http|
-          req = Net::HTTP::Get.new(@status_path)
-          response = http.request(req)
-          response.value
-          status = JSON.parse(response.body)
-          @logger.debug("Checking ES river status", :status => status)
-          if status["_source"]["error"]
-            reason = "ES river status: #{status["_source"]["error"]}"
-          else
-            success = true
-          end
-        end
-        sleep(2)
-      end
-    rescue Exception => e
-      raise "river is not running, checking status failed: #{$!}"
-    end
-
-    raise "river is not running: #{reason}" unless success
-  end # def check_river_status
-
-  public
-  def receive(event)
-    return unless output?(event)
-    # River events have a format of
-    # "action\ndata\n"
-    # where 'action' is index or delete, data is the data to index.
-    header = { "index" => { "_index" => event.sprintf(@index), "_type" => event.sprintf(@index_type) } }
-    if !@document_id.nil?
-      header["index"]["_id"] = event.sprintf(@document_id)
-    end
-
-    @mq.publish_serialized(header.to_json + "\n" + event.to_json + "\n")
-  end # def receive
-end # LogStash::Outputs::ElasticSearchRiver
diff --git a/lib/logstash/outputs/email.rb b/lib/logstash/outputs/email.rb
deleted file mode 100644
index b869432e041..00000000000
--- a/lib/logstash/outputs/email.rb
+++ /dev/null
@@ -1,303 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Send email when an output is received. Alternatively, you may include or
-# exclude the email output execution using conditionals. 
-class LogStash::Outputs::Email < LogStash::Outputs::Base
-
-  config_name "email"
-  milestone 1
-
-  # This setting is deprecated in favor of Logstash's "conditionals" feature
-  # If you were using this setting previously, please use conditionals instead.
-  #
-  # If you need help converting your older 'match' setting to a conditional,
-  # I welcome you to join the #logstash irc channel on freenode or to email
-  # the logstash-users@googlegroups.com mailling list and ask for help! :)
-  config :match, :validate => :hash, :deprecated => true
-
-  # The fully-qualified email address to send the email to.
-  #
-  # This field also accepts a comma-separated string of addresses, for example: 
-  # "me@host.com, you@host.com"
-  #
-  # You can also use dynamic fields from the event with the %{fieldname} syntax.
-  config :to, :validate => :string, :required => true
-
-  # The fully-qualified email address for the From: field in the email.
-  config :from, :validate => :string, :default => "logstash.alert@nowhere.com"
-
-  # The fully qualified email address for the Reply-To: field.
-  config :replyto, :validate => :string
-
-  # The fully-qualified email address(es) to include as cc: address(es).
-  #
-  # This field also accepts a comma-separated string of addresses, for example: 
-  # "me@host.com, you@host.com"
-  config :cc, :validate => :string
-
-  # How Logstash should send the email, either via SMTP or by invoking sendmail.
-  config :via, :validate => :string, :default => "smtp"
-
-  # Specify the options to use:
-  #
-  # Via SMTP: smtpIporHost, port, domain, userName, password, authenticationType, starttls
-  #
-  # Via sendmail: location, arguments
-  #
-  # If you do not specify any `options`, you will get the following equivalent code set in
-  # every new mail object:
-  #
-  #     Mail.defaults do
-  #       delivery_method :smtp, { :smtpIporHost         => "localhost",
-  #                                :port                 => 25,
-  #                                :domain               => 'localhost.localdomain',
-  #                                :userName             => nil,
-  #                                :password             => nil,
-  #                                :authenticationType   => nil,(plain, login and cram_md5)
-  #                                :starttls             => true  }
-  #
-  #       retriever_method :pop3, { :address             => "localhost",
-  #                                 :port                => 995,
-  #                                 :user_name           => nil,
-  #                                 :password            => nil,
-  #                                 :enable_ssl          => true }
-  #
-  #       Mail.delivery_method.new  #=> Mail::SMTP instance
-  #       Mail.retriever_method.new #=> Mail::POP3 instance
-  #     end
-  #
-  # Each mail object inherits the defaults set in Mail.delivery_method. However, on
-  # a per email basis, you can override the method:
-  #
-  #     mail.delivery_method :sendmail
-  #
-  # Or you can override the method and pass in settings:
-  #
-  #     mail.delivery_method :sendmail, { :address => 'some.host' }
-  #
-  # You can also just modify the settings:
-  #
-  #     mail.delivery_settings = { :address => 'some.host' }
-  #
-  # The hash you supply is just merged against the defaults with "merge!" and the result
-  # assigned to the mail object.  For instance, the above example will change only the
-  # `:address` value of the global `smtp_settings` to be 'some.host', retaining all other values.
-  config :options, :validate => :hash, :default => {}
-
-  # Subject: for the email.
-  config :subject, :validate => :string, :default => ""
-
-  # Body for the email - plain text only.
-  config :body, :validate => :string, :default => ""
-
-  # HTML Body for the email, which may contain HTML markup.
-  config :htmlbody, :validate => :string, :default => ""
-
-  # Attachments - specify the name(s) and location(s) of the files.
-  config :attachments, :validate => :array, :default => []
-
-  # contenttype : for multipart messages, set the content-type and/or charset of the HTML part.
-  # NOTE: this may not be functional (KH)
-  config :contenttype, :validate => :string, :default => "text/html; charset=UTF-8"
-
-  public
-  def register
-    require "mail"
-
-    # Mail uses instance_eval which changes the scope of self so @options is
-    # inaccessible from inside 'Mail.defaults'. So set a local variable instead.
-    options = @options
-
-    if @via == "smtp"
-      Mail.defaults do
-        delivery_method :smtp, {
-          :address              => options.fetch("smtpIporHost", "localhost"),
-          :port                 => options.fetch("port", 25),
-          :domain               => options.fetch("domain", "localhost"),
-          :user_name            => options.fetch("userName", nil),
-          :password             => options.fetch("password", nil),
-          :authentication       => options.fetch("authenticationType", nil),
-          :enable_starttls_auto => options.fetch("starttls", false),
-          :debug                => options.fetch("debug", false)
-        }
-      end
-    elsif @via == 'sendmail'
-      Mail.defaults do
-        delivery_method :sendmail
-      end
-    else
-      Mail.defaults do
-        delivery_method :@via, options
-      end
-    end # @via tests
-    @logger.debug("Email Output Registered!", :config => @config)
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-      @logger.debug("Event being tested for Email", :tags => @tags, :event => event)
-      # Set Intersection - returns a new array with the items that are the same between the two
-      if !@tags.empty? && (event["tags"] & @tags).size == 0
-         # Skip events that have no tags in common with what we were configured
-         @logger.debug("No Tags match for Email Output!")
-         return
-      end
-
-    @logger.debug? && @logger.debug("Match data for Email - ", :match => @match)
-    successful = false
-    matchName = ""
-    operator = ""
-
-    # TODO(sissel): Delete this once match support is removed.
-    @match && @match.each do |name, query|
-      if successful
-        break
-      else
-        matchName = name
-      end
-      # now loop over the csv query
-      queryArray = query.split(',')
-      index = 1
-      while index < queryArray.length
-        field = queryArray.at(index -1)
-        value = queryArray.at(index)
-        index = index + 2
-        if field == ""
-          if value.downcase == "and"
-            operator = "and"
-          elsif value.downcase == "or"
-            operator = "or"
-          else
-            operator = "or"
-            @logger.error("Operator Provided Is Not Found, Currently We Only Support AND/OR Values! - defaulting to OR")
-          end
-        else
-          hasField = event[field]
-          @logger.debug? and @logger.debug("Does Event Contain Field - ", :hasField => hasField)
-          isValid = false
-          # if we have maching field and value is wildcard - we have a success
-          if hasField
-            if value == "*"
-              isValid = true
-            else
-              # we get an array so we need to loop over the values and find if we have a match
-              eventFieldValues = event[field]
-              @logger.debug? and @logger.debug("Event Field Values - ", :eventFieldValues => eventFieldValues)
-              eventFieldValues = [eventFieldValues] if not eventFieldValues.respond_to?(:each)
-              eventFieldValues.each do |eventFieldValue|
-                isValid = validateValue(eventFieldValue, value)
-                if isValid # no need to iterate any further
-                  @logger.debug("VALID CONDITION FOUND - ", :eventFieldValue => eventFieldValue, :value => value) 
-                  break
-                end
-              end # end eventFieldValues.each do
-            end # end value == "*"
-          end # end hasField
-          # if we have an AND operator and we have a successful == false break
-          if operator == "and" && !isValid
-            successful = false
-          elsif operator == "or" && (isValid || successful)
-            successful = true
-          else
-            successful = isValid
-          end
-        end
-      end
-    end # @match.each do
-
-    # The 'match' setting is deprecated and optional. If not set,
-    # default to success.
-    successful = true if @match.nil?
-
-    @logger.debug? && @logger.debug("Email Did we match any alerts for event : ", :successful => successful)
-
-    if successful
-      # first add our custom field - matchName - so we can use it in the sprintf function
-      event["matchName"] = matchName unless matchName.empty?
-      @logger.debug? and @logger.debug("Creating mail with these settings : ", :via => @via, :options => @options, :from => @from, :to => @to, :cc => @cc, :subject => @subject, :body => @body, :content_type => @contenttype, :htmlbody => @htmlbody, :attachments => @attachments, :to => to, :to => to)
-      formatedSubject = event.sprintf(@subject)
-      formattedBody = event.sprintf(@body)
-      formattedHtmlBody = event.sprintf(@htmlbody)
-      # we have a match(s) - send email
-      mail = Mail.new
-      mail.from = event.sprintf(@from)
-      mail.to = event.sprintf(@to)
-      if @replyto
-        mail.reply_to = event.sprintf(@replyto)
-      end
-      mail.cc = event.sprintf(@cc)
-      mail.subject = formatedSubject
-      if @htmlbody.empty?
-        formattedBody.gsub!(/\\n/, "\n") # Take new line in the email
-        mail.body = formattedBody
-      else
-        mail.text_part = Mail::Part.new do
-          content_type "text/plain; charset=UTF-8"
-          formattedBody.gsub!(/\\n/, "\n") # Take new line in the email
-          body formattedBody
-        end
-        mail.html_part = Mail::Part.new do
-          content_type "text/html; charset=UTF-8"
-          body formattedHtmlBody
-        end
-      end
-      @attachments.each do |fileLocation|
-        mail.add_file(fileLocation)
-      end # end @attachments.each
-      @logger.debug? and @logger.debug("Sending mail with these values : ", :from => mail.from, :to => mail.to, :cc => mail.cc, :subject => mail.subject)
-      mail.deliver!
-    end # end if successful
-  end # def receive
-
-
-  private
-  def validateValue(eventFieldValue, value)
-    valid = false
-    # order of this if-else is important - please don't change it
-    if value.start_with?(">=")# greater than or equal
-      value.gsub!(">=","")
-      if eventFieldValue.to_i >= value.to_i
-        valid = true
-      end
-    elsif value.start_with?("<=")# less than or equal
-      value.gsub!("<=","")
-      if eventFieldValue.to_i <= value.to_i
-        valid = true
-      end
-    elsif value.start_with?(">")# greater than
-      value.gsub!(">","")
-      if eventFieldValue.to_i > value.to_i
-        valid = true
-      end
-    elsif value.start_with?("<")# less than
-      value.gsub!("<","")
-      if eventFieldValue.to_i < value.to_i
-        valid = true
-      end
-    elsif value.start_with?("*")# contains
-      value.gsub!("*","")
-      if eventFieldValue.include?(value)
-        valid = true
-      end
-    elsif value.start_with?("!*")# does not contain
-      value.gsub!("!*","")
-      if !eventFieldValue.include?(value)
-        valid = true
-      end
-    elsif value.start_with?("!")# not equal
-      value.gsub!("!","")
-      if eventFieldValue != value
-        valid = true
-      end
-    else # default equal
-      if eventFieldValue == value
-        valid = true
-      end
-    end
-    return valid
-  end # end validateValue()
-
-end # class LogStash::Outputs::Email
diff --git a/lib/logstash/outputs/exec.rb b/lib/logstash/outputs/exec.rb
deleted file mode 100644
index ae3fda1f0da..00000000000
--- a/lib/logstash/outputs/exec.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
-
-# This output will run a command for any matching event.
-#
-# Example:
-# 
-#     output {
-#       exec {
-#         type => abuse
-#         command => "iptables -A INPUT -s %{clientip} -j DROP"
-#       }
-#     }
-#
-# Run subprocesses via system ruby function
-#
-# WARNING: if you want it non-blocking you should use & or dtach or other such
-# techniques
-class LogStash::Outputs::Exec < LogStash::Outputs::Base
-
-  config_name "exec"
-  milestone 1
-
-  # Command line to execute via subprocess. Use dtach or screen to make it non blocking
-  config :command, :validate => :string, :required => true
-
-  public
-  def register
-    @logger.debug("exec output registered", :config => @config)
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-    @logger.debug("running exec command", :command => event.sprintf(@command))
-    system(event.sprintf(@command))
-  end # def receive
-
-end
diff --git a/lib/logstash/outputs/file.rb b/lib/logstash/outputs/file.rb
deleted file mode 100644
index 4ca7b98ec50..00000000000
--- a/lib/logstash/outputs/file.rb
+++ /dev/null
@@ -1,179 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
-require "zlib"
-
-# This output will write events to files on disk. You can use fields
-# from the event as parts of the filename and/or path.
-class LogStash::Outputs::File < LogStash::Outputs::Base
-
-  config_name "file"
-  milestone 2
-
-  # The path to the file to write. Event fields can be used here, 
-  # like "/var/log/logstash/%{host}/%{application}"
-  # One may also utilize the path option for date-based log 
-  # rotation via the joda time format. This will use the event
-  # timestamp.
-  # E.g.: path => "./test-%{+YYYY-MM-dd}.txt" to create 
-  # ./test-2013-05-29.txt 
-  config :path, :validate => :string, :required => true
-
-  # The maximum size of file to write. When the file exceeds this
-  # threshold, it will be rotated to the current filename + ".1"
-  # If that file already exists, the previous .1 will shift to .2
-  # and so forth.
-  #
-  # NOT YET SUPPORTED
-  config :max_size, :validate => :string
-
-  # The format to use when writing events to the file. This value
-  # supports any string and can include %{name} and other dynamic
-  # strings.
-  #
-  # If this setting is omitted, the full json representation of the
-  # event will be written as a single line.
-  config :message_format, :validate => :string
-
-  # Flush interval (in seconds) for flushing writes to log files. 
-  # 0 will flush on every message.
-  config :flush_interval, :validate => :number, :default => 2
-
-  # Gzip the output stream before writing to disk.
-  config :gzip, :validate => :boolean, :default => false
-
-  public
-  def register
-    require "fileutils" # For mkdir_p
-
-    workers_not_supported
-
-    @files = {}
-    now = Time.now
-    @last_flush_cycle = now
-    @last_stale_cleanup_cycle = now
-    flush_interval = @flush_interval.to_i
-    @stale_cleanup_interval = 10
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    path = event.sprintf(@path)
-    fd = open(path)
-
-    # TODO(sissel): Check if we should rotate the file.
-
-    if @message_format
-      output = event.sprintf(@message_format)
-    else
-      output = event.to_json
-    end
-
-    fd.write(output)
-    fd.write("\n")
-
-    flush(fd)
-    close_stale_files
-  end # def receive
-
-  def teardown
-    @logger.debug("Teardown: closing files")
-    @files.each do |path, fd|
-      begin
-        fd.close
-        @logger.debug("Closed file #{path}", :fd => fd)
-      rescue Exception => e
-        @logger.error("Excpetion while flushing and closing files.", :exception => e)
-      end
-    end
-    finished
-  end
-
-  private
-  def flush(fd)
-    if flush_interval > 0
-      flush_pending_files
-    else
-      fd.flush
-    end
-  end
-
-  # every flush_interval seconds or so (triggered by events, but if there are no events there's no point flushing files anyway)
-  def flush_pending_files
-    return unless Time.now - @last_flush_cycle >= flush_interval
-    @logger.debug("Starting flush cycle")
-    @files.each do |path, fd|
-      @logger.debug("Flushing file", :path => path, :fd => fd)
-      fd.flush
-    end
-    @last_flush_cycle = Time.now
-  end
-
-  # every 10 seconds or so (triggered by events, but if there are no events there's no point closing files anyway)
-  def close_stale_files
-    now = Time.now
-    return unless now - @last_stale_cleanup_cycle >= @stale_cleanup_interval
-    @logger.info("Starting stale files cleanup cycle", :files => @files)
-    inactive_files = @files.select { |path, fd| not fd.active }
-    @logger.debug("%d stale files found" % inactive_files.count, :inactive_files => inactive_files)
-    inactive_files.each do |path, fd|
-      @logger.info("Closing file %s" % path)
-      fd.close
-      @files.delete(path)
-    end
-    # mark all files as inactive, a call to write will mark them as active again
-    @files.each { |path, fd| fd.active = false }
-    @last_stale_cleanup_cycle = now
-  end
-
-  def open(path)
-    return @files[path] if @files.include?(path) and not @files[path].nil?
-
-    @logger.info("Opening file", :path => path)
-
-    dir = File.dirname(path)
-    if !Dir.exists?(dir)
-      @logger.info("Creating directory", :directory => dir)
-      FileUtils.mkdir_p(dir) 
-    end
-
-    # work around a bug opening fifos (bug JRUBY-6280)
-    stat = File.stat(path) rescue nil
-    if stat and stat.ftype == "fifo" and RUBY_PLATFORM == "java"
-      fd = java.io.FileWriter.new(java.io.File.new(path))
-    else
-      fd = File.new(path, "a")
-    end
-    if gzip
-      fd = Zlib::GzipWriter.new(fd)
-    end
-    @files[path] = IOWriter.new(fd)
-  end
-end # class LogStash::Outputs::File
-
-# wrapper class
-class IOWriter
-  def initialize(io)
-    @io = io
-  end
-  def write(*args)
-    @io.write(*args)
-    @active = true
-  end
-  def flush
-    @io.flush
-    if @io.class == Zlib::GzipWriter
-      @io.to_io.flush
-    end
-  end
-  def method_missing(method_name, *args, &block)
-    if @io.respond_to?(method_name)
-      @io.send(method_name, *args, &block)
-    else
-      super
-    end
-  end
-  attr_accessor :active
-end
diff --git a/lib/logstash/outputs/ganglia.rb b/lib/logstash/outputs/ganglia.rb
deleted file mode 100644
index 745a900ff4e..00000000000
--- a/lib/logstash/outputs/ganglia.rb
+++ /dev/null
@@ -1,75 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# This output allows you to pull metrics from your logs and ship them to
-# ganglia's gmond. This is heavily based on the graphite output.
-class LogStash::Outputs::Ganglia < LogStash::Outputs::Base
-  config_name "ganglia"
-  milestone 2
-
-  # The address of the ganglia server.
-  config :host, :validate => :string, :default => "localhost"
-
-  # The port to connect on your ganglia server.
-  config :port, :validate => :number, :default => 8649
-
-  # The metric to use. This supports dynamic strings like `%{host}`
-  config :metric, :validate => :string, :required => true
-
-  # The value to use. This supports dynamic strings like `%{bytes}`
-  # It will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :value, :validate => :string, :required => true
-
-  # The type of value for this metric.
-  config :metric_type, :validate => %w{string int8 uint8 int16 uint16 int32 uint32 float double},
-    :default => "uint8"
-
-  # Gmetric units for metric, such as "kb/sec" or "ms" or whatever unit
-  # this metric uses.
-  config :units, :validate => :string, :default => ""
-
-  # Maximum time in seconds between gmetric calls for this metric.
-  config :max_interval, :validate => :number, :default => 60
-
-  # Lifetime in seconds of this metric
-  config :lifetime, :validate => :number, :default => 300
-
-  # Metric group
-  config :group, :validate => :string, :default => ""
-
-  # Metric slope, represents metric behavior
-  config :slope, :validate => %w{zero positive negative both unspecified}, :default => "both"
-
-  def register
-    require "gmetric"
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    # gmetric only takes integer values, so convert it to int.
-    case @metric_type
-      when "string"
-        localvalue = event.sprintf(@value)
-      when "float"
-        localvalue = event.sprintf(@value).to_f
-      when "double"
-        localvalue = event.sprintf(@value).to_f
-      else # int8|uint8|int16|uint16|int32|uint32
-        localvalue = event.sprintf(@value).to_i
-    end
-    Ganglia::GMetric.send(@host, @port, {
-      :name => event.sprintf(@metric),
-      :units => @units,
-      :type => @metric_type,
-      :value => localvalue,
-      :group => @group,
-      :slope => @slope,
-      :tmax => @max_interval,
-      :dmax => @lifetime
-    })
-  end # def receive
-end # class LogStash::Outputs::Ganglia
diff --git a/lib/logstash/outputs/gelf.rb b/lib/logstash/outputs/gelf.rb
deleted file mode 100644
index e6c07255898..00000000000
--- a/lib/logstash/outputs/gelf.rb
+++ /dev/null
@@ -1,211 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
-
-# This output generates messages in GELF format. This is most useful if you
-# want to use Logstash to output events to Graylog2.
-#
-# More information at <http://graylog2.org/gelf#specs>
-class LogStash::Outputs::Gelf < LogStash::Outputs::Base
-
-  config_name "gelf"
-  milestone 2
-
-  # Graylog2 server IP address or hostname.
-  config :host, :validate => :string, :required => true
-
-  # Graylog2 server port number.
-  config :port, :validate => :number, :default => 12201
-
-  # The GELF chunksize. You usually don't need to change this.
-  config :chunksize, :validate => :number, :default => 1420
-
-  # Allow overriding of the GELF `sender` field. This is useful if you
-  # want to use something other than the event's source host as the
-  # "sender" of an event. A common case for this is using the application name
-  # instead of the hostname.
-  config :sender, :validate => :string, :default => "%{host}"
-
-  # The GELF message level. Dynamic values like %{level} are permitted here;
-  # useful if you want to parse the 'log level' from an event and use that
-  # as the GELF level/severity.
-  #
-  # Values here can be integers [0..7] inclusive or any of
-  # "debug", "info", "warn", "error", "fatal" (case insensitive).
-  # Single-character versions of these are also valid, "d", "i", "w", "e", "f",
-  # "u"
-  # The following additional severity\_labels from Logstash's  syslog\_pri filter
-  # are accepted: "emergency", "alert", "critical",  "warning", "notice", and
-  # "informational".
-  config :level, :validate => :array, :default => [ "%{severity}", "INFO" ]
-
-  # The GELF facility. Dynamic values like %{foo} are permitted here; this
-  # is useful if you need to use a value from the event as the facility name.
-  # Should now be sent as an underscored "additional field" (e.g. `\_facility`)
-  config :facility, :validate => :string, :deprecated => true
-
-  # The GELF line number; this is usually the line number in your program where
-  # the log event originated. Dynamic values like %{foo} are permitted here, but the
-  # value should be a number.
-  # Should now be sent as an underscored "additional field" (e.g. `\_line`).
-  config :line, :validate => :string, :deprecated => true
-
-  # The GELF file; this is usually the source code file in your program where
-  # the log event originated. Dynamic values like %{foo} are permitted here.
-  # Should now be sent as an underscored "additional field" (e.g. `\_file`).
-  config :file, :validate => :string, :deprecated => true
-
-  # Should Logstash ship metadata within event object? This will cause Logstash
-  # to ship any fields in the event (such as those created by grok) in the GELF
-  # messages. These will be sent as underscored "additional fields".
-  config :ship_metadata, :validate => :boolean, :default => true
-
-  # Ship tags within events. This will cause Logstash to ship the tags of an
-  # event as the field `\_tags`.
-  config :ship_tags, :validate => :boolean, :default => true
-
-  # Ignore these fields when `ship_metadata` is set. Typically this lists the
-  # fields used in dynamic values for GELF fields.
-  config :ignore_metadata, :validate => :array, :default => [ "@timestamp", "@version", "severity", "host", "source_host", "source_path", "short_message" ]
-
-  # The GELF custom field mappings. GELF supports arbitrary attributes as custom
-  # fields. This exposes that. Exclude the `_` portion of the field name
-  # e.g. `custom_fields => ['foo_field', 'some_value']
-  # sets `_foo_field` = `some_value`.
-  config :custom_fields, :validate => :hash, :default => {}
-
-  # The GELF full message. Dynamic values like %{foo} are permitted here.
-  config :full_message, :validate => :string, :default => "%{message}"
-
-  # The GELF short message field name. If the field does not exist or is empty,
-  # the event message is taken instead.
-  config :short_message, :validate => :string, :default => "short_message"
-
-  public
-  def register
-    require "gelf" # rubygem 'gelf'
-    option_hash = Hash.new
-
-    #@gelf = GELF::Notifier.new(@host, @port, @chunksize, option_hash)
-    @gelf = GELF::Notifier.new(@host, @port, @chunksize)
-
-    # This sets the 'log level' of gelf; since we're forwarding messages, we'll
-    # want to forward *all* messages, so set level to 0 so all messages get
-    # shipped
-    @gelf.level = 0
-
-    # Since we use gelf-rb which assumes the severity level integer
-    # is coming from a ruby logging subsystem, we need to instruct it
-    # that the levels we provide should be mapped directly since they're
-    # already RFC 5424 compliant
-    # this requires gelf-rb commit bb1f4a9 which added the level_mapping def
-    level_mapping = Hash.new
-    (0..7).step(1) { |l| level_mapping[l]=l }
-    @gelf.level_mapping = level_mapping
-
-    # If we leave that set, the gelf gem will extract the file and line number
-    # of the source file that logged the message (i.e. logstash/gelf.rb:138).
-    # With that set to false, it can use the actual event's filename (i.e.
-    # /var/log/syslog), which is much more useful
-    @gelf.collect_file_and_line = false
-
-    # these are syslog words and abbreviations mapped to RFC 5424 integers
-    # and logstash's syslog_pri filter
-    @level_map = {
-      "debug" => 7, "d" => 7,
-      "info" => 6, "i" => 6, "informational" => 6,
-      "notice" => 5, "n" => 5,
-      "warn" => 4, "w" => 4, "warning" => 4,
-      "error" => 3, "e" => 3,
-      "critical" => 2, "c" => 2,
-      "alert" => 1, "a" => 1,
-      "emergency" => 0, "e" => 0,
-     }
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    # We have to make our own hash here because GELF expects a hash
-    # with a specific format.
-    m = Hash.new
-
-    m["short_message"] = event["message"]
-    if event[@short_message]
-      v = event[@short_message]
-      short_message = (v.is_a?(Array) && v.length == 1) ? v.first : v
-      short_message = short_message.to_s
-      if !short_message.empty?
-        m["short_message"] = short_message
-      end
-    end
-
-    m["full_message"] = event.sprintf(@full_message)
-
-    m["host"] = event.sprintf(@sender)
-
-    # deprecated fields
-    m["facility"] = event.sprintf(@facility) if @facility
-    m["file"] = event.sprintf(@file) if @file
-    m["line"] = event.sprintf(@line) if @line
-    m["line"] = m["line"].to_i if m["line"].is_a?(String) and m["line"] === /^[\d]+$/
-
-    if @ship_metadata
-      event.to_hash.each do |name, value|
-        next if value == nil
-        next if name == "message"
-
-        # Trim leading '_' in the event
-        name = name[1..-1] if name.start_with?('_')
-        name = "_id" if name == "id"  # "_id" is reserved, so use "__id"
-        if !value.nil? and !@ignore_metadata.include?(name)
-          if value.is_a?(Array)
-            m["_#{name}"] = value.join(', ')
-          elsif value.is_a?(Hash)
-            value.each do |hash_name, hash_value|
-              m["_#{name}_#{hash_name}"] = hash_value
-            end
-          else
-            # Non array values should be presented as-is
-            # https://logstash.jira.com/browse/LOGSTASH-113
-            m["_#{name}"] = value
-          end
-        end
-      end
-    end
-
-    if @ship_tags
-      m["_tags"] = event["tags"].join(', ') if event["tags"]
-    end
-
-    if @custom_fields
-      @custom_fields.each do |field_name, field_value|
-        m["_#{field_name}"] = field_value unless field_name == 'id'
-      end
-    end
-
-    # Probe severity array levels
-    level = nil
-    if @level.is_a?(Array)
-      @level.each do |value|
-        parsed_value = event.sprintf(value)
-        next if value.count('%{') > 0 and parsed_value == value
-
-        level = parsed_value
-        break
-      end
-    else
-      level = event.sprintf(@level.to_s)
-    end
-    m["level"] = (@level_map[level.downcase] || level).to_i
-
-    @logger.debug(["Sending GELF event", m])
-    begin
-      @gelf.notify!(m, :timestamp => event["@timestamp"].to_f)
-    rescue
-      @logger.warn("Trouble sending GELF event", :gelf_event => m,
-                   :event => event, :error => $!)
-    end
-  end # def receive
-end # class LogStash::Outputs::Gelf
diff --git a/lib/logstash/outputs/graphite.rb b/lib/logstash/outputs/graphite.rb
deleted file mode 100644
index df47484b36f..00000000000
--- a/lib/logstash/outputs/graphite.rb
+++ /dev/null
@@ -1,146 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "socket"
-
-# This output allows you to pull metrics from your logs and ship them to
-# Graphite. Graphite is an open source tool for storing and graphing metrics.
-#
-# An example use case: Some applications emit aggregated stats in the logs
-# every 10 seconds. Using the grok filter and this output, it is possible to
-# capture the metric values from the logs and emit them to Graphite.
-class LogStash::Outputs::Graphite < LogStash::Outputs::Base
-  config_name "graphite"
-  milestone 2
-
-  EXCLUDE_ALWAYS = [ "@timestamp", "@version" ]
-
-  DEFAULT_METRICS_FORMAT = "*"
-  METRIC_PLACEHOLDER = "*"
-
-  # The hostname or IP address of the Graphite server.
-  config :host, :validate => :string, :default => "localhost"
-
-  # The port to connect to on the Graphite server.
-  config :port, :validate => :number, :default => 2003
-
-  # Interval between reconnect attempts to Carbon.
-  config :reconnect_interval, :validate => :number, :default => 2
-
-  # Should metrics be resent on failure?
-  config :resend_on_failure, :validate => :boolean, :default => false
-
-  # The metric(s) to use. This supports dynamic strings like %{host}
-  # for metric names and also for values. This is a hash field with key 
-  # being the metric name, value being the metric value. Example:
-  #
-  #     [ "%{host}/uptime", "%{uptime_1m}" ]
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will be set to zero (0). You may use either `metrics` or `fields_are_metrics`,
-  # but not both.
-  config :metrics, :validate => :hash, :default => {}
-
-  # An array indicating that these event fields should be treated as metrics
-  # and will be sent verbatim to Graphite. You may use either `fields_are_metrics`
-  # or `metrics`, but not both.
-  config :fields_are_metrics, :validate => :boolean, :default => false
-
-  # Include only regex matched metric names.
-  config :include_metrics, :validate => :array, :default => [ ".*" ]
-
-  # Exclude regex matched metric names, by default exclude unresolved %{field} strings.
-  config :exclude_metrics, :validate => :array, :default => [ "%\{[^}]+\}" ]
-
-  # Enable debug output.
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
-
-  # Defines the format of the metric string. The placeholder '*' will be
-  # replaced with the name of the actual metric.
-  #
-  #     metrics_format => "foo.bar.*.sum"
-  #
-  # NOTE: If no metrics_format is defined, the name of the metric will be used as fallback.
-  config :metrics_format, :validate => :string, :default => DEFAULT_METRICS_FORMAT
-
-  def register
-    @include_metrics.collect!{|regexp| Regexp.new(regexp)}
-    @exclude_metrics.collect!{|regexp| Regexp.new(regexp)}
-
-    if @metrics_format && !@metrics_format.include?(METRIC_PLACEHOLDER)
-      @logger.warn("metrics_format does not include placeholder #{METRIC_PLACEHOLDER} .. falling back to default format: #{DEFAULT_METRICS_FORMAT.inspect}")
-
-      @metrics_format = DEFAULT_METRICS_FORMAT
-    end
-
-    connect
-  end # def register
-
-  def connect
-    # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory. Retire to yak farm.
-    begin
-      @socket = TCPSocket.new(@host, @port)
-    rescue Errno::ECONNREFUSED => e
-      @logger.warn("Connection refused to graphite server, sleeping...",
-                   :host => @host, :port => @port)
-      sleep(@reconnect_interval)
-      retry
-    end
-  end # def connect
-
-  def construct_metric_name(metric)
-    if @metrics_format
-      return @metrics_format.gsub(METRIC_PLACEHOLDER, metric)
-    end
-
-    metric
-  end
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    # Graphite message format: metric value timestamp\n
-
-    messages = []
-    timestamp = event.sprintf("%{+%s}")
-
-    if @fields_are_metrics
-      @logger.debug("got metrics event", :metrics => event.to_hash)
-      event.to_hash.each do |metric,value|
-        next if EXCLUDE_ALWAYS.include?(metric)
-        next unless @include_metrics.empty? || @include_metrics.any? { |regexp| metric.match(regexp) }
-        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
-        messages << "#{construct_metric_name(metric)} #{event.sprintf(value.to_s).to_f} #{timestamp}"
-      end
-    else
-      @metrics.each do |metric, value|
-        @logger.debug("processing", :metric => metric, :value => value)
-        metric = event.sprintf(metric)
-        next unless @include_metrics.any? {|regexp| metric.match(regexp)}
-        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
-        messages << "#{construct_metric_name(event.sprintf(metric))} #{event.sprintf(value).to_f} #{timestamp}"
-      end
-    end
-
-    if messages.empty?
-      @logger.debug("Message is empty, not sending anything to Graphite", :messages => messages, :host => @host, :port => @port)
-    else
-      message = messages.join("\n")
-      @logger.debug("Sending carbon messages", :messages => messages, :host => @host, :port => @port)
-
-      # Catch exceptions like ECONNRESET and friends, reconnect on failure.
-      # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory.
-      begin
-        @socket.puts(message)
-      rescue Errno::EPIPE, Errno::ECONNRESET => e
-        @logger.warn("Connection to graphite server died",
-                     :exception => e, :host => @host, :port => @port)
-        sleep(@reconnect_interval)
-        connect
-        retry if @resend_on_failure
-      end
-    end
-
-  end # def receive
-end # class LogStash::Outputs::Graphite
diff --git a/lib/logstash/outputs/hipchat.rb b/lib/logstash/outputs/hipchat.rb
deleted file mode 100644
index 56198588999..00000000000
--- a/lib/logstash/outputs/hipchat.rb
+++ /dev/null
@@ -1,80 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/http"
-
-# This output allows you to write events to [HipChat](https://www.hipchat.com/).
-#
-class LogStash::Outputs::HipChat < LogStash::Outputs::Base
-
-  config_name "hipchat"
-  milestone 1
-
-  # The HipChat authentication token.
-  config :token, :validate => :string, :required => true
-
-  # The ID or name of the room.
-  config :room_id, :validate => :string, :required => true
-
-  # The name the message will appear be sent from.
-  config :from, :validate => :string, :default => "logstash"
-
-  # Whether or not this message should trigger a notification for people in the room.
-  config :trigger_notify, :validate => :boolean, :default => false
-
-  # Background color for message.
-  # HipChat currently supports one of "yellow", "red", "green", "purple",
-  # "gray", or "random". (default: yellow)
-  config :color, :validate => :string, :default => "yellow"
-
-  # Message format to send, event tokens are usable here.
-  config :format, :validate => :string, :default => "%{message}"
-
-  public
-  def register
-    require "ftw"
-    require "uri"
-
-    @agent = FTW::Agent.new
-
-    @url = "https://api.hipchat.com/v1/rooms/message?auth_token=" + @token
-    @content_type = "application/x-www-form-urlencoded"
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    hipchat_data = Hash.new
-    hipchat_data['room_id'] = @room_id
-    hipchat_data['from']    = @from
-    hipchat_data['color']   = @color
-    hipchat_data['notify']  = @trigger_notify ? "1" : "0"
-    hipchat_data['message'] = event.sprintf(@format)
-
-    @logger.debug("HipChat data", :hipchat_data => hipchat_data)
-
-    begin
-      request = @agent.post(@url)
-      request["Content-Type"] = @content_type
-      request.body = encode(hipchat_data)
-
-      response = @agent.execute(request)
-
-      # Consume body to let this connection be reused
-      rbody = ""
-      response.read_body { |c| rbody << c }
-      #puts rbody
-    rescue Exception => e
-      @logger.warn("Unhandled exception", :request => request, :response => response, :exception => e, :stacktrace => e.backtrace)
-    end
-  end # def receive
-
-  # shamelessly lifted this from the LogStash::Outputs::Http, I'd rather put this
-  # in a common place for both to use, but unsure where that place is or should be
-  def encode(hash)
-    return hash.collect do |key, value|
-      CGI.escape(key) + "=" + CGI.escape(value)
-    end.join("&")
-  end # def encode
-
-end # class LogStash::Outputs::HipChat
diff --git a/lib/logstash/outputs/http.rb b/lib/logstash/outputs/http.rb
deleted file mode 100644
index 029fc961a8e..00000000000
--- a/lib/logstash/outputs/http.rb
+++ /dev/null
@@ -1,142 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-class LogStash::Outputs::Http < LogStash::Outputs::Base
-  # This output lets you `PUT` or `POST` events to a
-  # generic HTTP(S) endpoint
-  #
-  # Additionally, you are given the option to customize
-  # the headers sent as well as basic customization of the
-  # event json itself.
-
-  config_name "http"
-  milestone 1
-
-  # URL to use
-  config :url, :validate => :string, :required => :true
-
-  # validate SSL?
-  config :verify_ssl, :validate => :boolean, :default => true
-
-  # What verb to use
-  # only put and post are supported for now
-  config :http_method, :validate => ["put", "post"], :required => :true
-
-  # Custom headers to use
-  # format is `headers => ["X-My-Header", "%{host}"]
-  config :headers, :validate => :hash
-
-  # Content type
-  #
-  # If not specified, this defaults to the following:
-  #
-  # * if format is "json", "application/json"
-  # * if format is "form", "application/x-www-form-urlencoded"
-  config :content_type, :validate => :string
-
-  # This lets you choose the structure and parts of the event that are sent.
-  #
-  #
-  # For example:
-  #
-  #    mapping => ["foo", "%{host}", "bar", "%{type}"]
-  config :mapping, :validate => :hash
-
-  # Set the format of the http body.
-  #
-  # If form, then the body will be the mapping (or whole event) converted
-  # into a query parameter string (foo=bar&baz=fizz...)
-  #
-  # If message, then the body will be the result of formatting the event according to message
-  #
-  # Otherwise, the event is sent as json.
-  config :format, :validate => ["json", "form", "message"], :default => "json"
-
-  config :message, :validate => :string
-
-  public
-  def register
-    require "ftw"
-    require "uri"
-    @agent = FTW::Agent.new
-    # TODO(sissel): SSL verify mode?
-
-    if @content_type.nil?
-      case @format
-        when "form" ; @content_type = "application/x-www-form-urlencoded"
-        when "json" ; @content_type = "application/json"
-      end
-    end
-    if @format == "message"
-      if @message.nil?
-        raise "message must be set if message format is used"
-      end
-      if @content_type.nil?
-        raise "content_type must be set if message format is used"
-      end
-      unless @mapping.nil?
-        @logger.warn "mapping is not supported and will be ignored if message format is used"
-      end
-    end
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    if @mapping
-      evt = Hash.new
-      @mapping.each do |k,v|
-        evt[k] = event.sprintf(v)
-      end
-    else
-      evt = event.to_hash
-    end
-
-    case @http_method
-    when "put"
-      request = @agent.put(event.sprintf(@url))
-    when "post"
-      request = @agent.post(event.sprintf(@url))
-    else
-      @logger.error("Unknown verb:", :verb => @http_method)
-    end
-    
-    if @headers
-      @headers.each do |k,v|
-        request.headers[k] = event.sprintf(v)
-      end
-    end
-
-    request["Content-Type"] = @content_type
-
-    begin
-      if @format == "json"
-        request.body = evt.to_json
-      elsif @format == "message"
-        request.body = event.sprintf(@message)
-      else
-        request.body = encode(evt)
-      end
-      #puts "#{request.port} / #{request.protocol}"
-      #puts request
-      #puts 
-      #puts request.body
-      response = @agent.execute(request)
-
-      # Consume body to let this connection be reused
-      rbody = ""
-      response.read_body { |c| rbody << c }
-      #puts rbody
-    rescue Exception => e
-      @logger.warn("Unhandled exception", :request => request, :response => response, :exception => e, :stacktrace => e.backtrace)
-    end
-  end # def receive
-
-  def encode(hash)
-    return hash.collect do |key, value|
-      CGI.escape(key) + "=" + CGI.escape(value)
-    end.join("&")
-  end # def encode
-end
diff --git a/lib/logstash/outputs/irc.rb b/lib/logstash/outputs/irc.rb
deleted file mode 100644
index 7a5791eaf62..00000000000
--- a/lib/logstash/outputs/irc.rb
+++ /dev/null
@@ -1,88 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "thread"
-
-# Write events to IRC
-#
-class LogStash::Outputs::Irc < LogStash::Outputs::Base
-
-  config_name "irc"
-  milestone 1
-
-  # Address of the host to connect to
-  config :host, :validate => :string, :required => true
-
-  # Port on host to connect to.
-  config :port, :validate => :number, :default => 6667
-
-  # IRC Nickname
-  config :nick, :validate => :string, :default => "logstash"
-
-  # IRC Username
-  config :user, :validate => :string, :default => "logstash"
-
-  # IRC Real name
-  config :real, :validate => :string, :default => "logstash"
-
-  # IRC server password
-  config :password, :validate => :password
-
-  # Channels to broadcast to.
-  # 
-  # These should be full channel names including the '#' symbol, such as
-  # "#logstash".
-  config :channels, :validate => :array, :required => true
-
-  # Message format to send, event tokens are usable here
-  config :format, :validate => :string, :default => "%{message}"
-
-  # Set this to true to enable SSL.
-  config :secure, :validate => :boolean, :default => false
-
-  # Limit the rate of messages sent to IRC in messages per second.
-  config :messages_per_second, :validate => :number, :default => 0.5
-
-  # Static string before event
-  config :pre_string, :validate => :string, :required => false
-  
-  # Static string after event
-  config :post_string, :validate => :string, :required => false
-
-  public
-  def register
-    require "cinch"
-    @irc_queue = Queue.new
-    @logger.info("Connecting to irc server", :host => @host, :port => @port, :nick => @nick, :channels => @channels)
-
-    @bot = Cinch::Bot.new
-    @bot.loggers.clear
-    @bot.configure do |c|
-      c.server = @host
-      c.port = @port
-      c.nick = @nick
-      c.user = @user
-      c.realname = @real
-      c.channels = @channels
-      c.password = @password.value rescue nil
-      c.ssl.use = @secure
-      c.messages_per_second = @messages_per_second if @messages_per_second
-    end
-    Thread.new(@bot) do |bot|
-      bot.start
-    end
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-    @logger.debug("Sending message to channels", :event => event)
-    text = event.sprintf(@format)
-    @bot.channels.each do |channel|
-      @logger.debug("Sending to...", :channel => channel, :text => text)
-      channel.msg(pre_string) if !@pre_string.nil?
-      channel.msg(text)
-      channel.msg(post_string) if !@post_string.nil?
-    end # channels.each
-  end # def receive
-end # class LogStash::Outputs::Irc
diff --git a/lib/logstash/outputs/juggernaut.rb b/lib/logstash/outputs/juggernaut.rb
deleted file mode 100644
index e41f8921726..00000000000
--- a/lib/logstash/outputs/juggernaut.rb
+++ /dev/null
@@ -1,105 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "logstash/event"
-
-# Push messages to the juggernaut websockets server:
-#
-# * https://github.com/maccman/juggernaut
-#
-# Wraps Websockets and supports other methods (including xhr longpolling) This
-# is basically, just an extension of the redis output (Juggernaut pulls
-# messages from redis).  But it pushes messages to a particular channel and
-# formats the messages in the way juggernaut expects.
-class LogStash::Outputs::Juggernaut < LogStash::Outputs::Base
-
-  config_name "juggernaut"
-  milestone 1
-
-  # The hostname of the redis server to which juggernaut is listening.
-  config :host, :validate => :string, :default => "127.0.0.1"
-
-  # The port to connect on.
-  config :port, :validate => :number, :default => 6379
-
-  # The redis database number.
-  config :db, :validate => :number, :default => 0
-
-  # Redis initial connection timeout in seconds.
-  config :timeout, :validate => :number, :default => 5
-
-  # Password to authenticate with.  There is no authentication by default.
-  config :password, :validate => :password
-
-  # List of channels to which to publish. Dynamic names are
-  # valid here, for example "logstash-%{type}".
-  config :channels, :validate => :array, :required => true
-
-  # How should the message be formatted before pushing to the websocket.
-  config :message_format, :validate => :string
-
-  public
-  def register
-    require 'redis'
-
-    if not @channels
-      raise RuntimeError.new(
-        "Must define the channels on which to publish the messages"
-      )
-    end
-    # end TODO
-
-    @redis = nil
-  end # def register
-
-  private
-  def connect
-    Redis.new(
-      :host => @host,
-      :port => @port,
-      :timeout => @timeout,
-      :db => @db,
-      :password => @password
-    )
-  end # def connect
-
-  # A string used to identify a redis instance in log messages
-  private
-  def identity
-    @name || "redis://#{@password}@#{@host}:#{@port}/#{@db} #{@data_type}:#{@key}"
-  end
-
-
-  public
-  def receive(event)
-    return unless output?(event)
-    begin
-      @redis ||= connect
-      if @message_format
-        formatted = event.sprintf(@message_format)
-      else
-        formatted = event.to_json
-      end
-      juggernaut_message = {
-        "channels" => @channels.collect{ |x| event.sprintf(x) },
-        "data" => event["message"]
-      }
-
-      @redis.publish 'juggernaut', juggernaut_message.to_json
-    rescue => e
-      @logger.warn("Failed to send event to redis", :event => event,
-                   :identity => identity, :exception => e,
-                   :backtrace => e.backtrace)
-      raise e
-    end
-  end # def receive
-
-  public
-  def teardown
-    if @data_type == 'channel' and @redis
-      @redis.quit
-      @redis = nil
-    end
-  end
-
-end
diff --git a/lib/logstash/outputs/lumberjack.rb b/lib/logstash/outputs/lumberjack.rb
deleted file mode 100644
index 4e92c6a0d82..00000000000
--- a/lib/logstash/outputs/lumberjack.rb
+++ /dev/null
@@ -1,62 +0,0 @@
-# encoding: utf-8
-class LogStash::Outputs::Lumberjack < LogStash::Outputs::Base
-
-  config_name "lumberjack"
-  milestone 1
-
-  # list of addresses lumberjack can send to
-  config :hosts, :validate => :array, :required => true
-
-  # the port to connect to
-  config :port, :validate => :number, :required => true
-
-  # ssl certificate to use
-  config :ssl_certificate, :validate => :path, :required => true
-
-  # window size
-  config :window_size, :validate => :number, :default => 5000
-
-  public
-  def register
-    require 'lumberjack/client'
-    connect
-
-    @codec.on_event do |payload|
-      begin
-        @client.write({ 'line' => payload })
-      rescue Exception => e
-        @logger.error("Client write error, trying connect", :e => e, :backtrace => e.backtrace)
-        connect
-        retry
-      end # begin
-    end # @codec
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-    if event == LogStash::SHUTDOWN
-      finished
-      return
-    end # LogStash::SHUTDOWN
-    @codec.encode(event)
-  end # def receive
-
-  private 
-  def connect
-    require 'resolv'
-    @logger.info("Connecting to lumberjack server.", :addresses => @hosts, :port => @port, 
-        :ssl_certificate => @ssl_certificate, :window_size => @window_size)
-    begin
-      ips = []
-      @hosts.each { |host| ips += Resolv.getaddresses host }
-      @client = Lumberjack::Client.new(:addresses => ips.uniq, :port => @port, 
-        :ssl_certificate => @ssl_certificate, :window_size => @window_size)
-    rescue Exception => e
-      @logger.error("All hosts unavailable, sleeping", :hosts => ips.uniq, :e => e, 
-        :backtrace => e.backtrace)
-      sleep(10)
-      retry
-    end
-  end
-end
diff --git a/lib/logstash/outputs/nagios.rb b/lib/logstash/outputs/nagios.rb
deleted file mode 100644
index 5677872e0ac..00000000000
--- a/lib/logstash/outputs/nagios.rb
+++ /dev/null
@@ -1,119 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
-
-# The Nagios output is used for sending passive check results to Nagios via the
-# Nagios command file. This output currently supports Nagios 3.
-#
-# For this output to work, your event _must_ have the following Logstash event fields:
-#
-#  * `nagios\_host`
-#  * `nagios\_service`
-#
-# These Logstash event fields are supported, but optional:
-#
-#  * `nagios\_annotation`
-#  * `nagios\_level` (overrides `nagios\_level` configuration option)
-#
-# There are two configuration options:
-#
-#  * `commandfile` - The location of the Nagios external command file. Defaults
-#    to '/var/lib/nagios3/rw/nagios.cmd'
-#  * `nagios\_level` - Specifies the level of the check to be sent. Defaults to
-#    CRITICAL and can be overriden by setting the "nagios\_level" field to one
-#    of "OK", "WARNING", "CRITICAL", or "UNKNOWN"
-#
-#     output{
-#       if [message] =~ /(error|ERROR|CRITICAL)/ {
-#         nagios {
-#           # your config here
-#         }
-#       }
-#     }
-#
-class LogStash::Outputs::Nagios < LogStash::Outputs::Base
-
-  config_name "nagios"
-  milestone 2
-
-  # The full path to your Nagios command file.
-  config :commandfile, :validate => :path, :default => "/var/lib/nagios3/rw/nagios.cmd"
-
-  # The Nagios check level. Should be one of 0=OK, 1=WARNING, 2=CRITICAL,
-  # 3=UNKNOWN. Defaults to 2 - CRITICAL.
-  config :nagios_level, :validate => [ "0", "1", "2", "3" ], :default => "2"
-
-  public
-  def register
-    # nothing to do
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    if !File.exists?(@commandfile)
-      @logger.warn("Skipping nagios output; command file is missing",
-                   :commandfile => @commandfile, :missed_event => event)
-      return
-    end
-
-    # TODO(petef): if nagios_host/nagios_service both have more than one
-    # value, send multiple alerts. They will have to match up together by
-    # array indexes (host/service combos) and the arrays must be the same
-    # length.
-
-    host = event["nagios_host"]
-    if !host
-      @logger.warn("Skipping nagios output; nagios_host field is missing",
-                   :missed_event => event)
-      return
-    end
-
-    service = event["nagios_service"]
-    if !service
-      @logger.warn("Skipping nagios output; nagios_service field is missing",
-                   "missed_event" => event)
-      return
-    end
-
-    annotation = event["nagios_annotation"]
-    level = @nagios_level
-
-    if event["nagios_level"]
-      event_level = [*event["nagios_level"]]
-      case event_level[0].downcase
-      when "ok"
-        level = "0"
-      when "warning"
-        level = "1"
-      when "critical"
-        level = "2"
-      when "unknown"
-        level = "3"
-      else
-        @logger.warn("Invalid Nagios level. Defaulting to CRITICAL", :data => event_level)
-      end
-    end
-
-    cmd = "[#{Time.now.to_i}] PROCESS_SERVICE_CHECK_RESULT;#{host};#{service};#{level};"
-    if annotation
-      cmd += "#{annotation}: "
-    end
-    # In the multi-line case, escape the newlines for the nagios command file
-    cmd += (event["message"] || "<no message>").gsub("\n", "\\n")
-
-    @logger.debug("Opening nagios command file", :commandfile => @commandfile,
-                  :nagios_command => cmd)
-    begin
-      File.open(@commandfile, "r+") do |f|
-        f.puts(cmd)
-        f.flush # TODO(sissel): probably don't need this.
-      end
-    rescue => e
-      @logger.warn("Skipping nagios output; error writing to command file",
-                   :commandfile => @commandfile, :missed_event => event,
-                   :exception => e, :backtrace => e.backtrace)
-    end
-  end # def receive
-end # class LogStash::Outputs::Nagios
diff --git a/lib/logstash/outputs/nagios_nsca.rb b/lib/logstash/outputs/nagios_nsca.rb
deleted file mode 100644
index 3e721de953f..00000000000
--- a/lib/logstash/outputs/nagios_nsca.rb
+++ /dev/null
@@ -1,123 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# The nagios_nsca output is used for sending passive check results to Nagios
-# through the NSCA protocol.
-#
-# This is useful if your Nagios server is not the same as the source host from
-# where you want to send logs or alerts. If you only have one server, this
-# output is probably overkill # for you, take a look at the 'nagios' output
-# instead.
-#
-# Here is a sample config using the nagios_nsca output:
-#     output {
-#       nagios_nsca {
-#         # specify the hostname or ip of your nagios server
-#         host => "nagios.example.com"
-#
-#         # specify the port to connect to
-#         port => 5667
-#       }
-#     }
-
-class LogStash::Outputs::NagiosNsca < LogStash::Outputs::Base
-
-  config_name "nagios_nsca"
-  milestone 1
-
-  # The status to send to nagios. Should be 0 = OK, 1 = WARNING, 2 = CRITICAL, 3 = UNKNOWN
-  config :nagios_status, :validate => :string, :required => true
-
-  # The nagios host or IP to send logs to. It should have a NSCA daemon running.
-  config :host, :validate => :string, :default => "localhost"
-
-  # The port where the NSCA daemon on the nagios host listens.
-  config :port, :validate => :number, :default => 5667
-
-  # The path to the 'send_nsca' binary on the local host.
-  config :send_nsca_bin, :validate => :path, :default => "/usr/sbin/send_nsca"
-
-  # The path to the send_nsca config file on the local host.
-  # Leave blank if you don't want to provide a config file.
-  config :send_nsca_config, :validate => :path
-
-  # The nagios 'host' you want to submit a passive check result to. This
-  # parameter accepts interpolation, e.g. you can use @source_host or other
-  # logstash internal variables.
-  config :nagios_host, :validate => :string, :default => "%{host}"
-
-  # The nagios 'service' you want to submit a passive check result to. This
-  # parameter accepts interpolation, e.g. you can use @source_host or other
-  # logstash internal variables.
-  config :nagios_service, :validate => :string, :default => "LOGSTASH"
-
-  # The format to use when writing events to nagios. This value
-  # supports any string and can include %{name} and other dynamic
-  # strings.
-  config :message_format, :validate => :string, :default => "%{@timestamp} %{host}: %{message}"
-
-  public
-  def register
-    #nothing for now
-  end
-
-  public
-  def receive(event)
-    # exit if type or tags don't match
-    return unless output?(event)
-
-    # catch logstash shutdown
-    if event == LogStash::SHUTDOWN
-      finished
-      return
-    end
-
-    # skip if 'send_nsca' binary doesn't exist
-    if !File.exists?(@send_nsca_bin)
-      @logger.warn("Skipping nagios_nsca output; send_nsca_bin file is missing",
-                   "send_nsca_bin" => @send_nsca_bin, "missed_event" => event)
-      return
-    end
-
-    # interpolate params
-    nagios_host = event.sprintf(@nagios_host)
-    nagios_service = event.sprintf(@nagios_service)
-
-    # escape basic things in the log message
-    # TODO: find a way to escape the message correctly
-    msg = event.sprintf(@message_format)
-    msg.gsub!("\n", "<br/>")
-    msg.gsub!("'", "&#146;")
-
-    status = event.sprintf(@nagios_status)
-    if status.to_i.to_s != status # Check it round-trips to int correctly
-      msg = "status '#{status}' is not numeric"
-      status = 2
-    else
-      status = status.to_i
-      if status > 3 || status < 0
-         msg "status must be > 0 and <= 3, not #{status}"
-         status = 2
-      end
-    end
-
-    # build the command
-    # syntax: echo '<server>!<nagios_service>!<status>!<text>'  | \
-    #           /usr/sbin/send_nsca -H <nagios_host> -d '!' -c <nsca_config>"
-    cmd = %(echo '#{nagios_host}~#{nagios_service}~#{status}~#{msg}' |)
-    cmd << %( #{@send_nsca_bin} -H #{@host} -p #{@port} -d '~')
-    cmd << %( -c #{@send_nsca_config}) if @send_nsca_config
-    cmd << %( 2>/dev/null >/dev/null)
-    @logger.debug("Running send_nsca command", "nagios_nsca_command" => cmd)
-
-    begin
-      system cmd
-    rescue => e
-      @logger.warn("Skipping nagios_nsca output; error calling send_nsca",
-                   "error" => $!, "nagios_nsca_command" => cmd,
-                   "missed_event" => event)
-      @logger.debug("Backtrace", e.backtrace)
-    end
-  end # def receive
-end # class LogStash::Outputs::NagiosNsca
diff --git a/lib/logstash/outputs/null.rb b/lib/logstash/outputs/null.rb
deleted file mode 100644
index 0c8dd8abbc7..00000000000
--- a/lib/logstash/outputs/null.rb
+++ /dev/null
@@ -1,18 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# A null output. This is useful for testing logstash inputs and filters for
-# performance.
-class LogStash::Outputs::Null < LogStash::Outputs::Base
-  config_name "null"
-  milestone 3
-
-  public
-  def register
-  end # def register
-
-  public
-  def receive(event)
-  end # def event
-end # class LogStash::Outputs::Null
diff --git a/lib/logstash/outputs/opentsdb.rb b/lib/logstash/outputs/opentsdb.rb
deleted file mode 100644
index cb7a16a2135..00000000000
--- a/lib/logstash/outputs/opentsdb.rb
+++ /dev/null
@@ -1,101 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "socket"
-
-# This output allows you to pull metrics from your logs and ship them to
-# opentsdb. Opentsdb is an open source tool for storing and graphing metrics.
-#
-class LogStash::Outputs::Opentsdb < LogStash::Outputs::Base
-  config_name "opentsdb"
-  milestone 1
-
-  # Enable debugging.
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
-
-  # The address of the opentsdb server.
-  config :host, :validate => :string, :default => "localhost"
-
-  # The port to connect on your graphite server.
-  config :port, :validate => :number, :default => 4242
-
-  # The metric(s) to use. This supports dynamic strings like %{source_host}
-  # for metric names and also for values. This is an array field with key
-  # of the metric name, value of the metric value, and multiple tag,values . Example:
-  #
-  #     [
-  #       "%{host}/uptime",
-  #       %{uptime_1m} " ,
-  #       "hostname" ,
-  #       "%{host}
-  #       "anotherhostname" ,
-  #       "%{host}
-  #     ]
-  #
-  # The value will be coerced to a floating point value. Values which cannot be
-  # coerced will zero (0)
-  config :metrics, :validate => :array, :required => true
-
-  def register
-    connect
-  end # def register
-
-  def connect
-    # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory.
-    begin
-      @socket = TCPSocket.new(@host, @port)
-    rescue Errno::ECONNREFUSED => e
-      @logger.warn("Connection refused to opentsdb server, sleeping...",
-                   :host => @host, :port => @port)
-      sleep(2)
-      retry
-    end
-  end # def connect
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    # Opentsdb message format: put metric timestamp value tagname=tagvalue tag2=value2\n
-
-    # Catch exceptions like ECONNRESET and friends, reconnect on failure.
-    begin
-      name = metrics[0]
-      value = metrics[1]
-      tags = metrics[2..-1]
-
-      # The first part of the message
-      message = ['put',
-                 event.sprintf(name),
-                 event.sprintf("%{+%s}"),
-                 event.sprintf(value),
-      ].join(" ")
-
-      # If we have have tags we need to add it to the message
-      event_tags = []
-      unless tags.nil?
-        Hash[*tags.flatten].each do |tag_name,tag_value|
-          # Interprete variables if neccesary
-          real_tag_name = event.sprintf(tag_name)
-          real_tag_value =  event.sprintf(tag_value)
-          event_tags << [real_tag_name , real_tag_value ].join('=')
-        end
-        message+=' '+event_tags.join(' ')
-      end
-
-      # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory.
-      begin
-        @socket.puts(message)
-      rescue Errno::EPIPE, Errno::ECONNRESET => e
-        @logger.warn("Connection to opentsdb server died",
-                     :exception => e, :host => @host, :port => @port)
-        sleep(2)
-        connect
-      end
-
-      # TODO(sissel): resend on failure
-      # TODO(sissel): Make 'resend on failure' tunable; sometimes it's OK to
-      # drop metrics.
-    end # @metrics.each
-  end # def receive
-end # class LogStash::Outputs::Opentsdb
diff --git a/lib/logstash/outputs/pagerduty.rb b/lib/logstash/outputs/pagerduty.rb
deleted file mode 100644
index 1c67e9a02e2..00000000000
--- a/lib/logstash/outputs/pagerduty.rb
+++ /dev/null
@@ -1,77 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# The PagerDuty output will send notifications based on pre-configured services 
-# and escalation policies. Logstash can send "trigger", "acknowledge" and "resolve"
-# event types. In addition, you may configure custom descriptions and event details.
-# The only required field is the PagerDuty "Service API Key", which can be found on
-# the service's web page on pagerduty.com. In the default case, the description and
-# event details will be populated by Logstash, using `message`, `timestamp` and `host` data.  
-class LogStash::Outputs::PagerDuty < LogStash::Outputs::Base
-  config_name "pagerduty"
-  milestone 1
-
-  # The PagerDuty Service API Key
-  config :service_key, :validate => :string, :required => true
-
-  # The service key to use. You'll need to set this up in PagerDuty beforehand.
-  config :incident_key, :validate => :string, :default => "logstash/%{host}/%{type}"
-
-  # Event type
-  config :event_type, :validate => ["trigger", "acknowledge", "resolve"], :default => "trigger"
-
-  # Custom description
-  config :description, :validate => :string, :default => "Logstash event for %{host}"
-
-  # The event details. These might be data from the Logstash event fields you wish to include.
-  # Tags are automatically included if detected so there is no need to explicitly add them here.
-  config :details, :validate => :hash, :default => {"timestamp" => "%{@timestamp}", "message" => "%{message}"}
-
-  # PagerDuty API URL. You shouldn't need to change this, but is included to allow for flexibility
-  # should PagerDuty iterate the API and Logstash hasn't been updated yet.
-  config :pdurl, :validate => :string, :default => "https://events.pagerduty.com/generic/2010-04-15/create_event.json"
-
-  public
-  def register
-    require 'net/https'
-    require 'uri'
-    @pd_uri = URI.parse(@pdurl)
-    @client = Net::HTTP.new(@pd_uri.host, @pd_uri.port)
-    if @pd_uri.scheme == "https"
-      @client.use_ssl = true
-      #@client.verify_mode = OpenSSL::SSL::VERIFY_PEER
-      # PagerDuty cert doesn't verify oob
-      @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
-    end
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-   
-    pd_event = Hash.new
-    pd_event[:service_key] = "#{@service_key}"
-    pd_event[:incident_key] = event.sprintf(@incident_key)
-    pd_event[:event_type] = "#{@event_type}"
-    pd_event[:description] = event.sprintf(@description)
-    pd_event[:details] = Hash.new
-    @details.each do |key, value|
-      @logger.debug("PD Details added:" , key => event.sprintf(value))
-      pd_event[:details]["#{key}"] = event.sprintf(value)
-    end
-    pd_event[:details][:tags] = @tags if @tags
-
-    @logger.info("PD Event", :event => pd_event)
-    begin
-      request = Net::HTTP::Post.new(@pd_uri.path)
-      request.body = pd_event.to_json
-      @logger.debug("PD Request", :request => request.inspect)
-      response = @client.request(request)
-      @logger.debug("PD Response", :response => response.body)
-
-    rescue Exception => e
-      @logger.debug("PD Unhandled exception", :pd_error => e.backtrace)
-    end
-  end # def receive
-end # class LogStash::Outputs::PagerDuty
diff --git a/lib/logstash/outputs/pipe.rb b/lib/logstash/outputs/pipe.rb
deleted file mode 100644
index c4bf90771ad..00000000000
--- a/lib/logstash/outputs/pipe.rb
+++ /dev/null
@@ -1,133 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/outputs/base"
-
-# Pipe output.
-#
-# Pipe events to stdin of another program. You can use fields from the
-# event as parts of the command.
-# WARNING: This feature can cause logstash to fork off multiple children if you are not carefull with per-event commandline.
-class LogStash::Outputs::Pipe < LogStash::Outputs::Base
-
-  config_name "pipe"
-  milestone 1
-
-  # The format to use when writing events to the pipe. This value
-  # supports any string and can include %{name} and other dynamic
-  # strings.
-  #
-  # If this setting is omitted, the full json representation of the
-  # event will be written as a single line.
-  config :message_format, :validate => :string
-
-  # Command line to launch and pipe to
-  config :command, :validate => :string, :required => true
-
-  # Close pipe that hasn't been used for TTL seconds. -1 or 0 means never close.
-  config :ttl, :validate => :number, :default => 10
-  public
-  def register
-    @pipes = {}
-    @last_stale_cleanup_cycle = Time.now
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    command = event.sprintf(@command)
-
-    if @message_format
-      output = event.sprintf(@message_format) + "\n"
-    else
-      output = event.to_json
-    end
-
-    begin
-      pipe = get_pipe(command)
-      pipe.puts(output)
-    rescue IOError, Errno::EPIPE, Errno::EBADF => e
-      @logger.error("Error writing to pipe, closing pipe.", :command => command, :pipe => pipe)
-      drop_pipe(command)
-      retry
-    end
-
-    close_stale_pipes
-  end # def receive
-
-  def teardown
-    @logger.info("Teardown: closing pipes")
-    @pipes.each do |command, pipe|
-      begin
-        drop_pipe(command)
-        @logger.debug("Closed pipe #{command}", :pipe => pipe)
-      rescue Exception => e
-        @logger.error("Excpetion while closing pipes.", :exception => e)
-      end
-    end
-    finished
-  end
-
-  private
-  # every 10 seconds or so (triggered by events, but if there are no events there's no point closing files anyway)
-  def close_stale_pipes
-    return if @ttl <= 0
-    now = Time.now
-    return unless now - @last_stale_cleanup_cycle >= @ttl
-    @logger.info("Starting stale pipes cleanup cycle", :pipes => @pipes)
-    inactive_pipes = @pipes.select { |command, pipe| not pipe.active }
-    @logger.debug("%d stale pipes found" % inactive_pipes.count, :inactive_pipes => inactive_pipes)
-    inactive_pipes.each do |command, pipe|
-      drop_pipe(command)
-    end
-    # mark all pipes as inactive, a call to write will mark them as active again
-    @pipes.each { |command, pipe| pipe.active = false }
-    @last_stale_cleanup_cycle = now
-  end
-
-  def drop_pipe(command)
-      return unless @pipes.include? command
-      @logger.info("Closing pipe \"%s\"" % command)
-      begin
-        @pipes[command].close
-      rescue Exception => e
-        @logger.warn("Failed to close pipe.", :error => e, :command => command)
-      end
-      @pipes.delete(command)
-  end
-
-  def get_pipe(command)
-    return @pipes[command] if @pipes.include?(command)
-
-    @logger.info("Opening pipe", :command => command)
-
-    @pipes[command] = PipeWrapper.new(command, mode="a+")
-  end
-end # class LogStash::Outputs::Pipe
-
-class PipeWrapper
-  attr_accessor :active
-  def initialize(command, mode="a+")
-    @pipe = IO.popen(command, mode)
-    @active = false
-  end
-
-  def method_missing(m, *args)
-    if @pipe.respond_to? m
-      @pipe.send(m, *args)
-    else
-      raise NoMethodError
-    end
-  end
-
-  def puts(txt)
-    @pipe.puts(txt)
-    @pipe.flush
-    @active = true
-  end
-
-  def write(txt)
-    @pipe.write(txt)
-    @active = true
-  end
-end
diff --git a/lib/logstash/outputs/rabbitmq.rb b/lib/logstash/outputs/rabbitmq.rb
deleted file mode 100644
index 7b52baf2a7a..00000000000
--- a/lib/logstash/outputs/rabbitmq.rb
+++ /dev/null
@@ -1,96 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Push events to a RabbitMQ exchange. Requires RabbitMQ 2.x
-# or later version (3.x is recommended).
-#
-# Relevant links:
-#
-# * RabbitMQ: <http://www.rabbitmq.com/>
-# * March Hare: <http://rubymarchhare.info>
-# * Bunny: <http://rubybunny.info>
-class LogStash::Outputs::RabbitMQ < LogStash::Outputs::Base
-  EXCHANGE_TYPES = ["fanout", "direct", "topic"]
-
-  config_name "rabbitmq"
-  milestone 1
-
-
-  #
-  # Connection
-  #
-
-  # RabbitMQ server address
-  config :host, :validate => :string, :required => true
-
-  # RabbitMQ port to connect on
-  config :port, :validate => :number, :default => 5672
-
-  # RabbitMQ username
-  config :user, :validate => :string, :default => "guest"
-
-  # RabbitMQ password
-  config :password, :validate => :password, :default => "guest"
-
-  # The vhost to use. If you don't know what this is, leave the default.
-  config :vhost, :validate => :string, :default => "/"
-
-  # Enable or disable SSL
-  config :ssl, :validate => :boolean, :default => false
-
-  # Validate SSL certificate
-  config :verify_ssl, :validate => :boolean, :default => false
-
-  # Enable or disable logging
-  config :debug, :validate => :boolean, :default => false, :deprecated => "Use the logstash --debug flag for this instead."
-
-
-
-  #
-  # Exchange
-  #
-
-
-  # The exchange type (fanout, topic, direct)
-  config :exchange_type, :validate => EXCHANGE_TYPES, :required => true
-
-  # The name of the exchange
-  config :exchange, :validate => :string, :required => true
-
-  # Key to route to by default. Defaults to 'logstash'
-  #
-  # * Routing keys are ignored on fanout exchanges.
-  config :key, :validate => :string, :default => "logstash"
-
-  # Is this exchange durable? (aka; Should it survive a broker restart?)
-  config :durable, :validate => :boolean, :default => true
-
-  # Should RabbitMQ persist messages to disk?
-  config :persistent, :validate => :boolean, :default => true
-
-
-
-  def initialize(params)
-    params["codec"] = "json" if !params["codec"]
-
-    super
-  end
-
-  # Use MarchHare on JRuby to avoid IO#select CPU spikes
-  # (see github.com/ruby-amqp/bunny/issues/95).
-  #
-  # On MRI, use Bunny.
-  #
-  # See http://rubybunny.info and http://rubymarchhare.info
-  # for the docs.
-  if RUBY_ENGINE == "jruby"
-    require "logstash/outputs/rabbitmq/march_hare"
-
-    include MarchHareImpl
-  else
-    require "logstash/outputs/rabbitmq/bunny"
-
-    include BunnyImpl
-  end
-end # class LogStash::Outputs::RabbitMQ
diff --git a/lib/logstash/outputs/rabbitmq/bunny.rb b/lib/logstash/outputs/rabbitmq/bunny.rb
deleted file mode 100644
index 3a59234d23b..00000000000
--- a/lib/logstash/outputs/rabbitmq/bunny.rb
+++ /dev/null
@@ -1,135 +0,0 @@
-# encoding: utf-8
-class LogStash::Outputs::RabbitMQ
-  module BunnyImpl
-
-    #
-    # API
-    #
-
-    def register
-      require "bunny"
-
-      @logger.info("Registering output", :plugin => self)
-
-      connect
-      declare_exchange
-    end # def register
-
-
-    def receive(event)
-      return unless output?(event)
-
-      @logger.debug("Sending event", :destination => to_s, :event => event, :key => key)
-      key = event.sprintf(@key) if @key
-
-      begin
-        publish_serialized(event.to_json, key)
-      rescue JSON::GeneratorError => e
-        @logger.warn("Trouble converting event to JSON", :exception => e,
-                     :event => event)
-      end
-    end
-
-    def publish_serialized(message, key = @key)
-      begin
-        if @x
-          @x.publish(message, :persistent => @persistent, :routing_key => key)
-        else
-          @logger.warn("Tried to send a message, but not connected to RabbitMQ yet.")
-        end
-      rescue Bunny::NetworkFailure, Bunny::ConnectionClosedError, Bunny::ConnectionLevelException, Bunny::TCPConnectionFailed => e
-        n = Bunny::Session::DEFAULT_NETWORK_RECOVERY_INTERVAL * 2
-
-        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
-                      :exception => e,
-                      :backtrace => e.backtrace)
-        return if terminating?
-
-        sleep n
-        connect
-        declare_exchange
-        retry
-      end
-    end
-
-    def to_s
-      return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@exchange}\##{@key}"
-    end
-
-    def teardown
-      @conn.close if @conn && @conn.open?
-      @conn = nil
-
-      finished
-    end
-
-
-
-    #
-    # Implementation
-    #
-
-    def connect
-      @vhost       ||= Bunny::DEFAULT_HOST
-      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
-      @port        ||= AMQ::Protocol::DEFAULT_PORT
-      @routing_key ||= "#"
-
-      @settings = {
-        :vhost => @vhost,
-        :host  => @host,
-        :port  => @port,
-        :automatically_recover => false
-      }
-      @settings[:user]      = @user || Bunny::DEFAULT_USER
-      @settings[:pass]      = if @password
-                                @password.value
-                              else
-                                Bunny::DEFAULT_PASSWORD
-                              end
-
-      @settings[:log_level] = if @debug || @logger.debug?
-                                :debug
-                              else
-                                :error
-                              end
-
-      @settings[:tls]        = @ssl if @ssl
-      @settings[:verify_ssl] = @verify_ssl if @verify_ssl
-
-      proto                  = if @ssl
-                                 "amqp"
-                               else
-                                 "amqps"
-                               end
-      @connection_url        = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
-
-      begin
-        @conn = Bunny.new(@settings)
-
-        @logger.debug("Connecting to RabbitMQ. Settings: #{@settings.inspect}, queue: #{@queue.inspect}")
-        return if terminating?
-        @conn.start
-
-        @ch = @conn.create_channel
-        @logger.info("Connected to RabbitMQ at #{@settings[:host]}")
-      rescue Bunny::NetworkFailure, Bunny::ConnectionClosedError, Bunny::ConnectionLevelException, Bunny::TCPConnectionFailed => e
-        n = Bunny::Session::DEFAULT_NETWORK_RECOVERY_INTERVAL * 2
-
-        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
-                      :exception => e,
-                      :backtrace => e.backtrace)
-        return if terminating?
-
-        sleep n
-        retry
-      end
-    end
-
-    def declare_exchange
-      @logger.debug("Declaring an exchange", :name => @exchange, :type => @exchange_type,
-                    :durable => @durable)
-      @x = @ch.exchange(@exchange, :type => @exchange_type.to_sym, :durable => @durable)
-    end
-  end # BunnyImpl
-end # LogStash::Outputs::RabbitMQ
diff --git a/lib/logstash/outputs/rabbitmq/hot_bunnies.rb b/lib/logstash/outputs/rabbitmq/hot_bunnies.rb
deleted file mode 100644
index 7d9cc42318a..00000000000
--- a/lib/logstash/outputs/rabbitmq/hot_bunnies.rb
+++ /dev/null
@@ -1 +0,0 @@
-require "logstash/outputs/rabbitmq/march_hare"
diff --git a/lib/logstash/outputs/rabbitmq/march_hare.rb b/lib/logstash/outputs/rabbitmq/march_hare.rb
deleted file mode 100644
index cdee3cf4bd9..00000000000
--- a/lib/logstash/outputs/rabbitmq/march_hare.rb
+++ /dev/null
@@ -1,143 +0,0 @@
-# encoding: utf-8
-class LogStash::Outputs::RabbitMQ
-  module MarchHareImpl
-
-
-    #
-    # API
-    #
-
-    def register
-      require "march_hare"
-      require "java"
-
-      @logger.info("Registering output", :plugin => self)
-
-      @connected = java.util.concurrent.atomic.AtomicBoolean.new
-
-      connect
-      declare_exchange
-
-      @connected.set(true)
-
-      @codec.on_event(&method(:publish_serialized))
-    end
-
-
-    def receive(event)
-      return unless output?(event)
-
-      begin
-        @codec.encode(event)
-      rescue JSON::GeneratorError => e
-        @logger.warn("Trouble converting event to JSON", :exception => e,
-                     :event => event)
-      end
-    end
-
-    def publish_serialized(message)
-      begin
-        if @connected.get
-          @x.publish(message, :routing_key => @key, :properties => {
-            :persistent => @persistent
-          })
-        else
-          @logger.warn("Tried to send a message, but not connected to RabbitMQ.")
-        end
-      rescue MarchHare::Exception, com.rabbitmq.client.AlreadyClosedException => e
-        @connected.set(false)
-        n = 10
-
-        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
-                      :exception => e,
-                      :backtrace => e.backtrace)
-        return if terminating?
-
-        sleep n
-
-        connect
-        declare_exchange
-        retry
-      end
-    end
-
-    def to_s
-      return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@exchange}\##{@key}"
-    end
-
-    def teardown
-      @connected.set(false)
-      @conn.close if @conn && @conn.open?
-      @conn = nil
-
-      finished
-    end
-
-
-
-    #
-    # Implementation
-    #
-
-    def connect
-      return if terminating?
-
-      @vhost       ||= "127.0.0.1"
-      # 5672. Will be switched to 5671 by Bunny if TLS is enabled.
-      @port        ||= 5672
-
-      @settings = {
-        :vhost => @vhost,
-        :host  => @host,
-        :port  => @port,
-        :user  => @user,
-        :automatic_recovery => false
-      }
-      @settings[:pass]      = if @password
-                                @password.value
-                              else
-                                "guest"
-                              end
-
-      @settings[:tls]        = @ssl if @ssl
-      proto                  = if @ssl
-                                 "amqp"
-                               else
-                                 "amqps"
-                               end
-      @connection_url        = "#{proto}://#{@user}@#{@host}:#{@port}#{vhost}/#{@queue}"
-
-      begin
-        @conn = MarchHare.connect(@settings)
-
-        @logger.debug("Connecting to RabbitMQ. Settings: #{@settings.inspect}, queue: #{@queue.inspect}")
-
-        @ch = @conn.create_channel
-        @logger.info("Connected to RabbitMQ at #{@settings[:host]}")
-      rescue MarchHare::Exception => e
-        @connected.set(false)
-        n = 10
-
-        @logger.error("RabbitMQ connection error: #{e.message}. Will attempt to reconnect in #{n} seconds...",
-                      :exception => e,
-                      :backtrace => e.backtrace)
-        return if terminating?
-
-        sleep n
-        retry
-      end
-    end
-
-    def declare_exchange
-      @logger.debug("Declaring an exchange", :name => @exchange, :type => @exchange_type,
-                    :durable => @durable)
-      @x = @ch.exchange(@exchange, :type => @exchange_type.to_sym, :durable => @durable)
-
-      # sets @connected to true during recovery. MK.
-      @connected.set(true)
-
-      @x
-    end
-
-  end # MarchHareImpl
-end
diff --git a/lib/logstash/outputs/redis.rb b/lib/logstash/outputs/redis.rb
deleted file mode 100644
index ef274591c64..00000000000
--- a/lib/logstash/outputs/redis.rb
+++ /dev/null
@@ -1,252 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "stud/buffer"
-
-# This output will send events to a Redis queue using RPUSH.
-# The RPUSH command is supported in Redis v0.0.7+. Using
-# PUBLISH to a channel requires at least v1.3.8+.
-# While you may be able to make these Redis versions work,
-# the best performance and stability will be found in more 
-# recent stable versions.  Versions 2.6.0+ are recommended.
-#
-# For more information about Redis, see <http://redis.io/>
-#
-class LogStash::Outputs::Redis < LogStash::Outputs::Base
-
-  include Stud::Buffer
-
-  config_name "redis"
-  milestone 2
-
-  # Name is used for logging in case there are multiple instances.
-  # TODO: delete
-  config :name, :validate => :string, :default => 'default',
-    :deprecated => true
-
-  # The hostname(s) of your Redis server(s). Ports may be specified on any
-  # hostname, which will override the global port config.
-  #
-  # For example:
-  #
-  #     "127.0.0.1"
-  #     ["127.0.0.1", "127.0.0.2"]
-  #     ["127.0.0.1:6380", "127.0.0.1"]
-  config :host, :validate => :array, :default => ["127.0.0.1"]
-
-  # Shuffle the host list during Logstash startup.
-  config :shuffle_hosts, :validate => :boolean, :default => true
-
-  # The default port to connect on. Can be overridden on any hostname.
-  config :port, :validate => :number, :default => 6379
-
-  # The Redis database number.
-  config :db, :validate => :number, :default => 0
-
-  # Redis initial connection timeout in seconds.
-  config :timeout, :validate => :number, :default => 5
-
-  # Password to authenticate with.  There is no authentication by default.
-  config :password, :validate => :password
-
-  # The name of the Redis queue (we'll use RPUSH on this). Dynamic names are
-  # valid here, for example "logstash-%{type}"
-  # TODO: delete
-  config :queue, :validate => :string, :deprecated => true
-
-  # The name of a Redis list or channel. Dynamic names are
-  # valid here, for example "logstash-%{type}".
-  # TODO set required true
-  config :key, :validate => :string, :required => false
-
-  # Either list or channel.  If `redis_type` is list, then we will set
-  # RPUSH to key. If `redis_type` is channel, then we will PUBLISH to `key`.
-  # TODO set required true
-  config :data_type, :validate => [ "list", "channel" ], :required => false
-
-  # Set to true if you want Redis to batch up values and send 1 RPUSH command
-  # instead of one command per value to push on the list.  Note that this only
-  # works with `data_type="list"` mode right now.
-  #
-  # If true, we send an RPUSH every "batch_events" events or
-  # "batch_timeout" seconds (whichever comes first).
-  # Only supported for `data_type` is "list".
-  config :batch, :validate => :boolean, :default => false
-
-  # If batch is set to true, the number of events we queue up for an RPUSH.
-  config :batch_events, :validate => :number, :default => 50
-
-  # If batch is set to true, the maximum amount of time between RPUSH commands
-  # when there are pending events to flush.
-  config :batch_timeout, :validate => :number, :default => 5
-
-  # Interval for reconnecting to failed Redis connections
-  config :reconnect_interval, :validate => :number, :default => 1
-
-  # In case Redis `data_type` is "list" and has more than @congestion_threshold items,
-  # block until someone consumes them and reduces congestion, otherwise if there are
-  # no consumers Redis will run out of memory, unless it was configured with OOM protection.
-  # But even with OOM protection, a single Redis list can block all other users of Redis,
-  # until Redis CPU consumption reaches the max allowed RAM size.
-  # A default value of 0 means that this limit is disabled.
-  # Only supported for `list` Redis `data_type`.
-  config :congestion_threshold, :validate => :number, :default => 0
-
-  # How often to check for congestion. Default is one second.
-  # Zero means to check on every event.
-  config :congestion_interval, :validate => :number, :default => 1
-
-  def register
-    require 'redis'
-
-    # TODO remove after setting key and data_type to true
-    if @queue
-      if @key or @data_type
-        raise RuntimeError.new(
-          "Cannot specify queue parameter and key or data_type"
-        )
-      end
-      @key = @queue
-      @data_type = 'list'
-    end
-
-    if not @key or not @data_type
-      raise RuntimeError.new(
-        "Must define queue, or key and data_type parameters"
-      )
-    end
-    # end TODO
-
-
-    if @batch
-      if @data_type != "list"
-        raise RuntimeError.new(
-          "batch is not supported with data_type #{@data_type}"
-        )
-      end
-      buffer_initialize(
-        :max_items => @batch_events,
-        :max_interval => @batch_timeout,
-        :logger => @logger
-      )
-    end
-
-    @redis = nil
-    if @shuffle_hosts
-        @host.shuffle!
-    end
-    @host_idx = 0
-
-    @congestion_check_times = Hash.new { |h,k| h[k] = Time.now.to_i - @congestion_interval }
-  end # def register
-
-  def receive(event)
-    return unless output?(event)
-
-    if @batch and @data_type == 'list' # Don't use batched method for pubsub.
-      # Stud::Buffer
-      buffer_receive(event.to_json, event.sprintf(@key))
-      return
-    end
-
-    key = event.sprintf(@key)
-    # TODO(sissel): We really should not drop an event, but historically
-    # we have dropped events that fail to be converted to json.
-    # TODO(sissel): Find a way to continue passing events through even
-    # if they fail to convert properly.
-    begin
-      payload = event.to_json
-    rescue Encoding::UndefinedConversionError, ArgumentError
-      puts "FAILUREENCODING"
-      @logger.error("Failed to convert event to JSON. Invalid UTF-8, maybe?",
-                    :event => event.inspect)
-      return
-    end
-
-    begin
-      @redis ||= connect
-      if @data_type == 'list'
-        congestion_check(key)
-        @redis.rpush(key, payload)
-      else
-        @redis.publish(key, payload)
-      end
-    rescue => e
-      @logger.warn("Failed to send event to Redis", :event => event,
-                   :identity => identity, :exception => e,
-                   :backtrace => e.backtrace)
-      sleep @reconnect_interval
-      @redis = nil
-      retry
-    end
-  end # def receive
-
-  def congestion_check(key)
-    return if @congestion_threshold == 0
-    if (Time.now.to_i - @congestion_check_times[key]) >= @congestion_interval # Check congestion only if enough time has passed since last check.
-      while @redis.llen(key) > @congestion_threshold # Don't push event to Redis key which has reached @congestion_threshold.
-        @logger.warn? and @logger.warn("Redis key size has hit a congestion threshold #{@congestion_threshold} suspending output for #{@congestion_interval} seconds")
-        sleep @congestion_interval
-      end
-      @congestion_check_time = Time.now.to_i
-    end
-  end
-
-  # called from Stud::Buffer#buffer_flush when there are events to flush
-  def flush(events, key, teardown=false)
-    @redis ||= connect
-    # we should not block due to congestion on teardown
-    # to support this Stud::Buffer#buffer_flush should pass here the :final boolean value.
-    congestion_check(key) unless teardown
-    @redis.rpush(key, events)
-  end
-  # called from Stud::Buffer#buffer_flush when an error occurs
-  def on_flush_error(e)
-    @logger.warn("Failed to send backlog of events to Redis",
-      :identity => identity,
-      :exception => e,
-      :backtrace => e.backtrace
-    )
-    @redis = connect
-  end
-
-  def teardown
-    if @batch
-      buffer_flush(:final => true)
-    end
-    if @data_type == 'channel' and @redis
-      @redis.quit
-      @redis = nil
-    end
-  end
-
-  private
-  def connect
-    @current_host, @current_port = @host[@host_idx].split(':')
-    @host_idx = @host_idx + 1 >= @host.length ? 0 : @host_idx + 1
-
-    if not @current_port
-      @current_port = @port
-    end
-
-    params = {
-      :host => @current_host,
-      :port => @current_port,
-      :timeout => @timeout,
-      :db => @db
-    }
-    @logger.debug(params)
-
-    if @password
-      params[:password] = @password.value
-    end
-
-    Redis.new(params)
-  end # def connect
-
-  # A string used to identify a Redis instance in log messages
-  def identity
-    @name || "redis://#{@password}@#{@current_host}:#{@current_port}/#{@db} #{@data_type}:#{@key}"
-  end
-
-end
diff --git a/lib/logstash/outputs/s3.rb b/lib/logstash/outputs/s3.rb
deleted file mode 100644
index e3a47b41bbf..00000000000
--- a/lib/logstash/outputs/s3.rb
+++ /dev/null
@@ -1,357 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "socket" # for Socket.gethostname
-
-# TODO integrate aws_config in the future 
-#require "logstash/plugin_mixins/aws_config"
-
-# INFORMATION:
-
-# This plugin was created for store the logstash's events into Amazon Simple Storage Service (Amazon S3).
-# For use it you needs authentications and an s3 bucket. 
-# Be careful to have the permission to write file on S3's bucket and run logstash with super user for establish connection.
-
-# S3 plugin allows you to do something complex, let's explain:)
-
-# S3 outputs create temporary files into "/opt/logstash/S3_temp/". If you want, you can change the path at the start of register method.
-# This files have a special name, for example:
-
-# ls.s3.ip-10-228-27-95.2013-04-18T10.00.tag_hello.part0.txt
-
-# ls.s3 : indicate logstash plugin s3
-
-# "ip-10-228-27-95" : indicate you ip machine, if you have more logstash and writing on the same bucket for example.
-# "2013-04-18T10.00" : represents the time whenever you specify time_file.
-# "tag_hello" : this indicate the event's tag, you can collect events with the same tag. 
-# "part0" : this means if you indicate size_file then it will generate more parts if you file.size > size_file. 
-#           When a file is full it will pushed on bucket and will be deleted in temporary directory. 
-#           If a file is empty is not pushed, but deleted.
-
-# This plugin have a system to restore the previous temporary files if something crash.
-
-##[Note] :
-
-## If you specify size_file and time_file then it will create file for each tag (if specified), when time_file or
-## their size > size_file, it will be triggered then they will be pushed on s3's bucket and will delete from local disk.
-
-## If you don't specify size_file, but time_file then it will create only one file for each tag (if specified). 
-## When time_file it will be triggered then the files will be pushed on s3's bucket and delete from local disk.
-
-## If you don't specify time_file, but size_file  then it will create files for each tag (if specified),
-## that will be triggered when their size > size_file, then they will be pushed on s3's bucket and will delete from local disk.
-
-## If you don't specific size_file and time_file you have a curios mode. It will create only one file for each tag (if specified).
-## Then the file will be rest on temporary directory and don't will be pushed on bucket until we will restart logstash.
-
-# INFORMATION ABOUT CLASS:
-
-# I tried to comment the class at best i could do. 
-# I think there are much thing to improve, but if you want some points to develop here a list:
-
-# TODO Integrate aws_config in the future 
-# TODO Find a method to push them all files when logtstash close the session.
-# TODO Integrate @field on the path file
-# TODO Permanent connection or on demand? For now on demand, but isn't a good implementation. 
-#      Use a while or a thread to try the connection before break a time_out and signal an error.
-# TODO If you have bugs report or helpful advice contact me, but remember that this code is much mine as much as yours, 
-#      try to work on it if you want :)
-
-
-# USAGE:
-
-# This is an example of logstash config:
-
-# output {
-#    s3{ 
-#      access_key_id => "crazy_key"             (required)
-#      secret_access_key => "monkey_access_key" (required)
-#      endpoint_region => "eu-west-1"           (required)
-#      bucket => "boss_please_open_your_bucket" (required)         
-#      size_file => 2048                        (optional)
-#      time_file => 5                           (optional)
-#      format => "plain"                        (optional) 
-#      canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
-#    }
-# }
-
-# We analize this:
-
-# access_key_id => "crazy_key" 
-# Amazon will give you the key for use their service if you buy it or try it. (not very much open source anyway)
-
-# secret_access_key => "monkey_access_key"
-# Amazon will give you the secret_access_key for use their service if you buy it or try it . (not very much open source anyway).
-
-# endpoint_region => "eu-west-1" 
-# When you make a contract with Amazon, you should know where the services you use.
-
-# bucket => "boss_please_open_your_bucket" 
-# Be careful you have the permission to write on bucket and know the name.
-
-# size_file => 2048
-# Means the size, in KB, of files who can store on temporary directory before you will be pushed on bucket.
-# Is useful if you have a little server with poor space on disk and you don't want blow up the server with unnecessary temporary log files.
-
-# time_file => 5
-# Means, in minutes, the time  before the files will be pushed on bucket. Is useful if you want to push the files every specific time.
- 
-# format => "plain"
-# Means the format of events you want to store in the files
-
-# canned_acl => "private"
-# The S3 canned ACL to use when putting the file. Defaults to "private".
-
-# LET'S ROCK AND ROLL ON THE CODE!
-
-class LogStash::Outputs::S3 < LogStash::Outputs::Base
- #TODO integrate aws_config in the future 
- #  include LogStash::PluginMixins::AwsConfig
-
- config_name "s3"
- milestone 1
-
- # Aws access_key.
- config :access_key_id, :validate => :string
- 
- # Aws secret_access_key
- config :secret_access_key, :validate => :string
-
- # S3 bucket
- config :bucket, :validate => :string
-
- # Aws endpoint_region
- config :endpoint_region, :validate => ["us-east-1", "us-west-1", "us-west-2",
-                                        "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                        "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
-
- # Set the size of file in KB, this means that files on bucket when have dimension > file_size, they are stored in two or more file. 
- # If you have tags then it will generate a specific size file for every tags
- ##NOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket. 
- config :size_file, :validate => :number, :default => 0
-
- # Set the time, in minutes, to close the current sub_time_section of bucket. 
- # If you define file_size you have a number of files in consideration of the section and the current tag.
- # 0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,
- # for now the only thing this plugin can do is to put the file when logstash restart.
- config :time_file, :validate => :number, :default => 0 
- 
- # The event format you want to store in files. Defaults to plain text.
- config :format, :validate => [ "json", "plain", "nil" ], :default => "plain"
-
- ## IMPORTANT: if you use multiple instance of s3, you should specify on one of them the "restore=> true" and on the others "restore => false".
- ## This is hack for not destroy the new files after restoring the initial files. 
- ## If you do not specify "restore => true" when logstash crashes or is restarted, the files are not sent into the bucket,
- ## for example if you have single Instance. 
- config :restore, :validate => :boolean, :default => false
-
- # Aws canned ACL
- config :canned_acl, :validate => ["private", "public_read", "public_read_write", "authenticated_read"],
-        :default => "private"
-
- # Method to set up the aws configuration and establish connection
- def aws_s3_config
-
-  @endpoint_region == 'us-east-1' ? @endpoint_region = 's3.amazonaws.com' : @endpoint_region = 's3-'+@endpoint_region+'.amazonaws.com'
-
-  @logger.info("Registering s3 output", :bucket => @bucket, :endpoint_region => @endpoint_region)
-
-  AWS.config(
-    :access_key_id => @access_key_id,
-    :secret_access_key => @secret_access_key,
-    :s3_endpoint => @endpoint_region
-  )
-  @s3 = AWS::S3.new 
-
- end
-
- # This method is used to manage sleep and awaken thread.
- def time_alert(interval)
-
-   Thread.new do
-    loop do
-      start_time = Time.now
-      yield
-      elapsed = Time.now - start_time
-      sleep([interval - elapsed, 0].max)
-    end
-   end
-
- end
-
- # this method is used for write files on bucket. It accept the file and the name of file.
- def write_on_bucket (file_data, file_basename)
- 
-  # if you lose connection with s3, bad control implementation.
-  if ( @s3 == nil) 
-    aws_s3_config
-  end
-
-  # find and use the bucket
-  bucket = @s3.buckets[@bucket]
-
-  @logger.debug "S3: ready to write "+file_basename+" in bucket "+@bucket+", Fire in the hole!"
-
-  # prepare for write the file
-  object = bucket.objects[file_basename]
-  object.write(:file => file_data, :acl => @canned_acl)
- 
-  @logger.debug "S3: has written "+file_basename+" in bucket "+@bucket + " with canned ACL \"" + @canned_acl + "\""
-
- end
-  
- # this method is used for create new path for name the file
- def getFinalPath
-   
-   @pass_time = Time.now 
-   return @temp_directory+"ls.s3."+Socket.gethostname+"."+(@pass_time).strftime("%Y-%m-%dT%H.%M")
-
- end
-
- # This method is used for restore the previous crash of logstash or to prepare the files to send in bucket. 
- # Take two parameter: flag and name. Flag indicate if you want to restore or not, name is the name of file 
- def upFile(flag, name)
-   
-   Dir[@temp_directory+name].each do |file|
-     name_file = File.basename(file)
-    
-     if (flag == true)
-      @logger.warn "S3: have found temporary file: "+name_file+", something has crashed before... Prepare for upload in bucket!"
-     end
-    
-     if (!File.zero?(file))  
-       write_on_bucket(file, name_file)
-
-       if (flag == true)
-          @logger.debug "S3: file: "+name_file+" restored on bucket "+@bucket
-       else
-          @logger.debug "S3: file: "+name_file+" was put on bucket "+@bucket
-       end
-     end
-
-     File.delete (file)
-
-   end
- end
-
- # This method is used for create new empty temporary files for use. Flag is needed for indicate new subsection time_file.
- def newFile (flag)
-  
-   if (flag == true)
-     @current_final_path = getFinalPath
-     @sizeCounter = 0
-   end
-
-   if (@tags.size != 0)
-     @tempFile = File.new(@current_final_path+".tag_"+@tag_path+"part"+@sizeCounter.to_s+".txt", "w")
-   else
-     @tempFile = File.new(@current_final_path+".part"+@sizeCounter.to_s+".txt", "w")
-   end
-
- end
-
- public
- def register
-   require "aws-sdk"
-   @temp_directory = "/opt/logstash/S3_temp/"
-
-   if (@tags.size != 0)
-       @tag_path = ""
-       for i in (0..@tags.size-1)
-          @tag_path += @tags[i].to_s+"." 
-       end
-   end
-
-   if !(File.directory? @temp_directory)
-    @logger.debug "S3: Directory "+@temp_directory+" doesn't exist, let's make it!"
-    Dir.mkdir(@temp_directory)
-   else
-    @logger.debug "S3: Directory "+@temp_directory+" exist, nothing to do"
-   end 
-   
-   if (@restore == true )
-     @logger.debug "S3: is attempting to verify previous crashes..."
-   
-     upFile(true, "*.txt")    
-   end
-   
-   newFile(true)
-   
-   if (time_file != 0)
-      first_time = true
-      @thread = time_alert(@time_file*60) do
-       if (first_time == false)
-         @logger.debug "S3: time_file triggered,  let's bucket the file if dosen't empty  and create new file "
-         upFile(false, File.basename(@tempFile))
-         newFile(true)
-       else
-         first_time = false
-       end
-     end
-   end
- 
- end
- 
- public
- def receive(event)
-  return unless output?(event)
-   
-  # Prepare format of Events 
-  if (@format == "plain")
-     message = self.class.format_message(event)
-  elsif (@format == "json")
-     message = event.to_json
-  else
-     message = event.to_s
-  end
-  
-  if(time_file !=0)
-     @logger.debug "S3: trigger files after "+((@pass_time+60*time_file)-Time.now).to_s
-  end
-
-  # if specific the size
-  if(size_file !=0)
-    
-    if (@tempFile.size < @size_file )
-
-       @logger.debug "S3: File have size: "+@tempFile.size.to_s+" and size_file is: "+ @size_file.to_s
-       @logger.debug "S3: put event into: "+File.basename(@tempFile)
-
-       # Put the event in the file, now! 
-       File.open(@tempFile, 'a') do |file|
-         file.puts message
-         file.write "\n"
-       end
-
-     else
-
-       @logger.debug "S3: file: "+File.basename(@tempFile)+" is too large, let's bucket it and create new file"
-       upFile(false, File.basename(@tempFile))
-       @sizeCounter += 1
-       newFile(false)
-
-     end
-     
-  # else we put all in one file 
-  else
-
-    @logger.debug "S3: put event into "+File.basename(@tempFile)
-    File.open(@tempFile, 'a') do |file|
-      file.puts message
-      file.write "\n"
-    end
-  end
-    
- end
-
- def self.format_message(event)
-    message = "Date: #{event["@timestamp"]}\n"
-    message << "Source: #{event["source"]}\n"
-    message << "Tags: #{event["tags"].join(', ')}\n"
-    message << "Fields: #{event.to_hash.inspect}\n"
-    message << "Message: #{event["message"]}"
- end
-
-end
-
-# Enjoy it, by Bistic:)
diff --git a/lib/logstash/outputs/sns.rb b/lib/logstash/outputs/sns.rb
deleted file mode 100644
index ed91b6b2557..00000000000
--- a/lib/logstash/outputs/sns.rb
+++ /dev/null
@@ -1,124 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "logstash/plugin_mixins/aws_config"
-
-# SNS output.
-#
-# Send events to Amazon's Simple Notification Service, a hosted pub/sub
-# framework.  It supports subscribers of type email, HTTP/S, SMS, and SQS.
-#
-# For further documentation about the service see:
-#
-#   http://docs.amazonwebservices.com/sns/latest/api/
-#
-# This plugin looks for the following fields on events it receives:
-#
-#  * `sns` - If no ARN is found in the configuration file, this will be used as
-#  the ARN to publish.
-#  * `sns_subject` - The subject line that should be used.
-#  Optional. The "%{host}" will be used if not present and truncated at
-#  `MAX_SUBJECT_SIZE_IN_CHARACTERS`.
-#  * `sns_message` - The message that should be
-#  sent. Optional. The event serialzed as JSON will be used if not present and
-#  with the @message truncated so that the length of the JSON fits in
-#  `MAX_MESSAGE_SIZE_IN_BYTES`.
-#
-class LogStash::Outputs::Sns < LogStash::Outputs::Base
-  include LogStash::PluginMixins::AwsConfig
-
-  MAX_SUBJECT_SIZE_IN_CHARACTERS  = 100
-  MAX_MESSAGE_SIZE_IN_BYTES       = 32768
-
-  config_name "sns"
-  milestone 1
-
-  # Message format.  Defaults to plain text.
-  config :format, :validate => [ "json", "plain" ], :default => "plain"
-
-  # SNS topic ARN.
-  config :arn, :validate => :string
-
-  # When an ARN for an SNS topic is specified here, the message
-  # "Logstash successfully booted" will be sent to it when this plugin
-  # is registered.
-  #
-  # Example: arn:aws:sns:us-east-1:770975001275:logstash-testing
-  #
-  config :publish_boot_message_arn, :validate => :string
-
-  public
-  def aws_service_endpoint(region)
-    return {
-        :sns_endpoint => "sns.#{region}.amazonaws.com"
-    }
-  end
-
-  public
-  def register
-    require "aws-sdk"
-
-    @sns = AWS::SNS.new(aws_options_hash)
-
-    # Try to publish a "Logstash booted" message to the ARN provided to
-    # cause an error ASAP if the credentials are bad.
-    if @publish_boot_message_arn
-      @sns.topics[@publish_boot_message_arn].publish("Logstash successfully booted", :subject => "Logstash booted")
-    end
-  end
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    arn     = Array(event["sns"]).first || @arn
-
-    raise "An SNS ARN required." unless arn
-
-    message = Array(event["sns_message"]).first
-    subject = Array(event["sns_subject"]).first || event.source
-
-    # Ensure message doesn't exceed the maximum size.
-    if message
-      # TODO: Utilize `byteslice` in JRuby 1.7: http://jira.codehaus.org/browse/JRUBY-5547
-      message = message.slice(0, MAX_MESSAGE_SIZE_IN_BYTES)
-    else
-      if @format == "plain"
-        message = self.class.format_message(event)
-      else
-        message = self.class.json_message(event)
-      end
-    end
-
-    # Log event.
-    @logger.debug("Sending event to SNS topic [#{arn}] with subject [#{subject}] and message:")
-    message.split("\n").each { |line| @logger.debug(line) }
-
-    # Publish the message.
-    @sns.topics[arn].publish(message, :subject => subject.slice(0, MAX_SUBJECT_SIZE_IN_CHARACTERS))
-  end
-
-  def self.json_message(event)
-    json      = event.to_json
-    json_size = json.bytesize
-
-    # Truncate only the message if the JSON structure is too large.
-    if json_size > MAX_MESSAGE_SIZE_IN_BYTES
-      # TODO: Utilize `byteslice` in JRuby 1.7: http://jira.codehaus.org/browse/JRUBY-5547
-      event["message"] = event["message"].slice(0, (event["message"].bytesize - (json_size - MAX_MESSAGE_SIZE_IN_BYTES)))
-    end
-
-    event.to_json
-  end
-
-  def self.format_message(event)
-    message =  "Date: #{event["@timestamp"]}\n"
-    message << "Source: #{event["source"]}\n"
-    message << "Tags: #{event["tags"].join(', ')}\n"
-    message << "Fields: #{event.to_hash.inspect}\n"
-    message << "Message: #{event["message"]}"
-
-    # TODO: Utilize `byteslice` in JRuby 1.7: http://jira.codehaus.org/browse/JRUBY-5547
-    message.slice(0, MAX_MESSAGE_SIZE_IN_BYTES)
-  end
-end
diff --git a/lib/logstash/outputs/sqs.rb b/lib/logstash/outputs/sqs.rb
deleted file mode 100644
index 9791c64bb63..00000000000
--- a/lib/logstash/outputs/sqs.rb
+++ /dev/null
@@ -1,140 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "logstash/plugin_mixins/aws_config"
-require "stud/buffer"
-require "digest/sha2"
-
-# Push events to an Amazon Web Services Simple Queue Service (SQS) queue.
-#
-# SQS is a simple, scalable queue system that is part of the 
-# Amazon Web Services suite of tools.
-#
-# Although SQS is similar to other queuing systems like AMQP, it
-# uses a custom API and requires that you have an AWS account.
-# See http://aws.amazon.com/sqs/ for more details on how SQS works,
-# what the pricing schedule looks like and how to setup a queue.
-#
-# To use this plugin, you *must*:
-#
-#  * Have an AWS account
-#  * Setup an SQS queue
-#  * Create an identify that has access to publish messages to the queue.
-#
-# The "consumer" identity must have the following permissions on the queue:
-#
-#  * sqs:ChangeMessageVisibility
-#  * sqs:ChangeMessageVisibilityBatch
-#  * sqs:GetQueueAttributes
-#  * sqs:GetQueueUrl
-#  * sqs:ListQueues
-#  * sqs:SendMessage
-#  * sqs:SendMessageBatch
-#
-# Typically, you should setup an IAM policy, create a user and apply the IAM policy to the user.
-# A sample policy is as follows:
-#
-#      {
-#        "Statement": [
-#          {
-#            "Sid": "Stmt1347986764948",
-#            "Action": [
-#              "sqs:ChangeMessageVisibility",
-#              "sqs:ChangeMessageVisibilityBatch",
-#              "sqs:DeleteMessage",
-#              "sqs:DeleteMessageBatch",
-#              "sqs:GetQueueAttributes",
-#              "sqs:GetQueueUrl",
-#              "sqs:ListQueues",
-#              "sqs:ReceiveMessage"
-#            ],
-#            "Effect": "Allow",
-#            "Resource": [
-#              "arn:aws:sqs:us-east-1:200850199751:Logstash"
-#            ]
-#          }
-#        ]
-#      }
-#
-# See http://aws.amazon.com/iam/ for more details on setting up AWS identities.
-#
-class LogStash::Outputs::SQS < LogStash::Outputs::Base
-  include LogStash::PluginMixins::AwsConfig
-  include Stud::Buffer
-
-  config_name "sqs"
-  milestone 1
-
-  # Name of SQS queue to push messages into. Note that this is just the name of the queue, not the URL or ARN.
-  config :queue, :validate => :string, :required => true
-
-  # Set to true if you want send messages to SQS in batches with batch_send
-  # from the amazon sdk
-  config :batch, :validate => :boolean, :default => true
-
-  # If batch is set to true, the number of events we queue up for a batch_send.
-  config :batch_events, :validate => :number, :default => 10
-
-  # If batch is set to true, the maximum amount of time between batch_send commands when there are pending events to flush.
-  config :batch_timeout, :validate => :number, :default => 5
-
-  public
-  def aws_service_endpoint(region)
-    return {
-        :sqs_endpoint => "sqs.#{region}.amazonaws.com"
-    }
-  end
-
-  public 
-  def register
-    require "aws-sdk"
-
-    @sqs = AWS::SQS.new(aws_options_hash)
-
-    if @batch
-      if @batch_events > 10
-        raise RuntimeError.new(
-          "AWS only allows a batch_events parameter of 10 or less"
-        )
-      elsif @batch_events <= 1
-        raise RuntimeError.new(
-          "batch_events parameter must be greater than 1 (or its not a batch)"
-        )
-      end
-      buffer_initialize(
-        :max_items => @batch_events,
-        :max_interval => @batch_timeout,
-        :logger => @logger
-      )
-    end
-
-    begin
-      @logger.debug("Connecting to AWS SQS queue '#{@queue}'...")
-      @sqs_queue = @sqs.queues.named(@queue)
-      @logger.info("Connected to AWS SQS queue '#{@queue}' successfully.")
-    rescue Exception => e
-      @logger.error("Unable to access SQS queue '#{@queue}': #{e.to_s}")
-    end # begin/rescue
-  end # def register
-
-  public
-  def receive(event)
-    if @batch
-      buffer_receive(event.to_json)
-      return
-    end
-    @sqs_queue.send_message(event.to_json)
-  end # def receive
-
-  # called from Stud::Buffer#buffer_flush when there are events to flush
-  def flush(events, teardown=false)
-    @sqs_queue.batch_send(events)
-  end
-
-  public
-  def teardown
-    buffer_flush(:final => true)
-    @sqs_queue = nil
-    finished
-  end # def teardown
-end
diff --git a/lib/logstash/outputs/statsd.rb b/lib/logstash/outputs/statsd.rb
deleted file mode 100644
index aaf3c556fb3..00000000000
--- a/lib/logstash/outputs/statsd.rb
+++ /dev/null
@@ -1,120 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# statsd is a network daemon for aggregating statistics, such as counters and timers,
-# and shipping over UDP to backend services, such as Graphite or Datadog.
-#
-# The most basic coverage of this plugin is that the 'namespace', 'sender', and
-# 'metric' names are combined into the full metric path like so:
-#
-#     namespace.sender.metric
-#
-# The general idea is that you send statsd count or latency data and every few
-# seconds it will emit the aggregated values to the backend. Example aggregates are
-# average, max, stddev, etc.
-#
-# You can learn about statsd here:
-#
-# * <http://codeascraft.etsy.com/2011/02/15/measure-anything-measure-everything/>
-# * <https://github.com/etsy/statsd>
-#
-# A simple example usage of this is to count HTTP hits by response code; to learn
-# more about that, check out the [log metrics tutorial](../tutorials/metrics-from-logs)
-#
-# The default final metric sent to statsd would look like this:
-#
-#     namespace.sender.metric
-#
-# With regards to this plugin, the default namespace is "logstash", the default sender
-# is the ${host} field, and the metric name depends on what is set as the metric name
-# in the increment, decrement, timing, count, set or gauge variable. 
-#
-class LogStash::Outputs::Statsd < LogStash::Outputs::Base
-  ## Regex stolen from statsd code
-  RESERVED_CHARACTERS_REGEX = /[\:\|\@]/
-  config_name "statsd"
-  milestone 2
-
-  # The address of the statsd server.
-  config :host, :validate => :string, :default => "localhost"
-
-  # The port to connect to on your statsd server.
-  config :port, :validate => :number, :default => 8125
-
-  # The statsd namespace to use for this metric.
-  config :namespace, :validate => :string, :default => "logstash"
-
-  # The name of the sender. Dots will be replaced with underscores.
-  config :sender, :validate => :string, :default => "%{host}"
-
-  # An increment metric. Metric names as array.
-  config :increment, :validate => :array, :default => []
-
-  # A decrement metric. Metric names as array.
-  config :decrement, :validate => :array, :default => []
-
-  # A timing metric. `metric_name => duration` as hash
-  config :timing, :validate => :hash, :default => {}
-
-  # A count metric. `metric_name => count` as hash
-  config :count, :validate => :hash, :default => {}
-
-  # A set metric. `metric_name => "string"` to append as hash
-  config :set, :validate => :hash, :default => {}
-
-  # A gauge metric. `metric_name => gauge` as hash.
-  config :gauge, :validate => :hash, :default => {}
-  
-  # The sample rate for the metric.
-  config :sample_rate, :validate => :number, :default => 1
-
-  # Enable debugging.
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting was never used by this plugin. It will be removed soon."
-
-  public
-  def register
-    require "statsd"
-    @client = Statsd.new(@host, @port)
-  end # def register
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    @client.namespace = event.sprintf(@namespace) if not @namespace.empty?
-    @logger.debug? and @logger.debug("Original sender: #{@sender}")
-    sender = event.sprintf(@sender)
-    @logger.debug? and @logger.debug("Munged sender: #{sender}")
-    @logger.debug? and @logger.debug("Event: #{event}")
-    @increment.each do |metric|
-      @client.increment(build_stat(event.sprintf(metric), sender), @sample_rate)
-    end
-    @decrement.each do |metric|
-      @client.decrement(build_stat(event.sprintf(metric), sender), @sample_rate)
-    end
-    @count.each do |metric, val|
-      @client.count(build_stat(event.sprintf(metric), sender),
-                    event.sprintf(val).to_f, @sample_rate)
-    end
-    @timing.each do |metric, val|
-      @client.timing(build_stat(event.sprintf(metric), sender),
-                     event.sprintf(val).to_f, @sample_rate)
-    end
-    @set.each do |metric, val|
-      @client.set(build_stat(event.sprintf(metric), sender),
-                    event.sprintf(val), @sample_rate)
-    end
-    @gauge.each do |metric, val|
-      @client.gauge(build_stat(event.sprintf(metric), sender),
-                    event.sprintf(val).to_f, @sample_rate)
-    end
-  end # def receive
-
-  def build_stat(metric, sender=@sender)
-    sender = sender.gsub('::','.').gsub(RESERVED_CHARACTERS_REGEX, '_').gsub(".", "_")
-    metric = metric.gsub('::','.').gsub(RESERVED_CHARACTERS_REGEX, '_')
-    @logger.debug? and @logger.debug("Formatted value", :sender => sender, :metric => metric)
-    return "#{sender}.#{metric}"
-  end
-end # class LogStash::Outputs::Statsd
diff --git a/lib/logstash/outputs/stdout.rb b/lib/logstash/outputs/stdout.rb
deleted file mode 100644
index 76c71426722..00000000000
--- a/lib/logstash/outputs/stdout.rb
+++ /dev/null
@@ -1,60 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# A simple output which prints to the STDOUT of the shell running
-# Logstash. This output can be quite convenient when debugging
-# plugin configurations, by allowing instant access to the event
-# data after it has passed through the inputs and filters.
-#
-# For example, the following output configuration, in conjunction with the
-# Logstash `-e` command-line flag, will allow you to see the results
-# of your event pipeline for quick iteration. 
-# 
-#     output {
-#       stdout {}
-#     }
-# 
-# Useful codecs include:
-#
-# `rubydebug`: outputs event data using the ruby "awesome_print"
-# library[http://rubygems.org/gems/awesome_print]
-#
-#     output {
-#       stdout { codec => rubydebug }
-#     }
-#
-# `json`: outputs event data in structured JSON format
-#
-#     output {
-#       stdout { codec => json }
-#     }
-#
-class LogStash::Outputs::Stdout < LogStash::Outputs::Base
-  begin
-     require "ap"
-  rescue LoadError
-  end
-
-  config_name "stdout"
-  milestone 3
-  
-  default :codec, "line"
-
-  public
-  def register
-    @codec.on_event do |event|
-      $stdout.write(event)
-    end
-  end
-
-  def receive(event)
-    return unless output?(event)
-    if event == LogStash::SHUTDOWN
-      finished
-      return
-    end
-    @codec.encode(event)
-  end
-
-end # class LogStash::Outputs::Stdout
diff --git a/lib/logstash/outputs/tcp.rb b/lib/logstash/outputs/tcp.rb
deleted file mode 100644
index 5176de967b0..00000000000
--- a/lib/logstash/outputs/tcp.rb
+++ /dev/null
@@ -1,145 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "thread"
-
-# Write events over a TCP socket.
-#
-# Each event json is separated by a newline.
-#
-# Can either accept connections from clients or connect to a server,
-# depending on `mode`.
-class LogStash::Outputs::Tcp < LogStash::Outputs::Base
-
-  config_name "tcp"
-  milestone 2
-
-  default :codec, "json"
-
-  # When mode is `server`, the address to listen on.
-  # When mode is `client`, the address to connect to.
-  config :host, :validate => :string, :required => true
-
-  # When mode is `server`, the port to listen on.
-  # When mode is `client`, the port to connect to.
-  config :port, :validate => :number, :required => true
-  
-  # When connect failed,retry interval in sec.
-  config :reconnect_interval, :validate => :number, :default => 10
-
-  # Mode to operate in. `server` listens for client connections,
-  # `client` connects to a server.
-  config :mode, :validate => ["server", "client"], :default => "client"
-
-  # The format to use when writing events to the file. This value
-  # supports any string and can include %{name} and other dynamic
-  # strings.
-  #
-  # If this setting is omitted, the full json representation of the
-  # event will be written as a single line.
-  config :message_format, :validate => :string, :deprecated => true
-
-  class Client
-    public
-    def initialize(socket, logger)
-      @socket = socket
-      @logger = logger
-      @queue  = Queue.new
-    end
-
-    public
-    def run
-      loop do
-        begin
-          @socket.write(@queue.pop)
-        rescue => e
-          @logger.warn("tcp output exception", :socket => @socket,
-                       :exception => e)
-          break
-        end
-      end
-    end # def run
-
-    public
-    def write(msg)
-      @queue.push(msg)
-    end # def write
-  end # class Client
-
-  public
-  def register
-    require "stud/try"
-    if server?
-      workers_not_supported
-
-      @logger.info("Starting tcp output listener", :address => "#{@host}:#{@port}")
-      @server_socket = TCPServer.new(@host, @port)
-      @client_threads = []
-
-      @accept_thread = Thread.new(@server_socket) do |server_socket|
-        loop do
-          client_thread = Thread.start(server_socket.accept) do |client_socket|
-            client = Client.new(client_socket, @logger)
-            Thread.current[:client] = client
-            client.run
-          end
-          @client_threads << client_thread
-        end
-      end
-
-      @codec.on_event do |payload|
-        @client_threads.each do |client_thread|
-          client_thread[:client].write(payload)
-        end
-        @client_threads.reject! {|t| !t.alive? }
-      end
-    else
-      client_socket = nil
-      @codec.on_event do |payload|
-        begin
-          client_socket = connect unless client_socket
-          r,w,e = IO.select([client_socket], [client_socket], [client_socket], nil)
-          # don't expect any reads, but a readable socket might
-          # mean the remote end closed, so read it and throw it away.
-          # we'll get an EOFError if it happens.
-          client_socket.sysread(16384) if r.any?
-
-          # Now send the payload
-          client_socket.syswrite(payload) if w.any?
-        rescue => e
-          @logger.warn("tcp output exception", :host => @host, :port => @port,
-                       :exception => e, :backtrace => e.backtrace)
-          client_socket.close rescue nil
-          client_socket = nil
-          sleep @reconnect_interval
-          retry
-        end
-      end
-    end
-  end # def register
-
-  private
-  def connect
-    Stud::try do
-      return TCPSocket.new(@host, @port)
-    end
-  end # def connect
-
-  private
-  def server?
-    @mode == "server"
-  end # def server?
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    #if @message_format
-      #output = event.sprintf(@message_format) + "\n"
-    #else
-      #output = event.to_hash.to_json + "\n"
-    #end
-    
-    @codec.encode(event)
-  end # def receive
-end # class LogStash::Outputs::Tcp
diff --git a/lib/logstash/outputs/udp.rb b/lib/logstash/outputs/udp.rb
deleted file mode 100644
index 1469017e647..00000000000
--- a/lib/logstash/outputs/udp.rb
+++ /dev/null
@@ -1,38 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-require "socket"
-
-# Send events over UDP
-#
-# Keep in mind that UDP will lose messages.
-class LogStash::Outputs::UDP < LogStash::Outputs::Base
-  config_name "udp"
-  milestone 1
-  
-  default :codec, "json"
-
-  # The address to send messages to
-  config :host, :validate => :string, :required => true
-
-  # The port to send messages on
-  config :port, :validate => :number, :required => true
-
-  public
-  def register
-    @socket = UDPSocket.new
-    @codec.on_event do |payload|
-      @socket.send(payload, 0, @host, @port)
-    end
-  end
-
-  def receive(event)
-    return unless output?(event)
-    if event == LogStash::SHUTDOWN
-      finished
-      return
-    end
-    @codec.encode(event)
-  end
-
-end # class LogStash::Outputs::Stdout
diff --git a/lib/logstash/outputs/xmpp.rb b/lib/logstash/outputs/xmpp.rb
deleted file mode 100644
index 93040111892..00000000000
--- a/lib/logstash/outputs/xmpp.rb
+++ /dev/null
@@ -1,78 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# This output allows you ship events over XMPP/Jabber.
-#
-# This plugin can be used for posting events to humans over XMPP, or you can
-# use it for PubSub or general message passing for logstash to logstash.
-class LogStash::Outputs::Xmpp < LogStash::Outputs::Base
-  config_name "xmpp"
-  milestone 2
-
-  # The user or resource ID, like foo@example.com.
-  config :user, :validate => :string, :required => :true
-
-  # The xmpp password for the user/identity.
-  config :password, :validate => :password, :required => :true
-
-  # The users to send messages to
-  config :users, :validate => :array
-
-  # if muc/multi-user-chat required, give the name of the room that
-  # you want to join: room@conference.domain/nick
-  config :rooms, :validate => :array
-
-  # The xmpp server to connect to. This is optional. If you omit this setting,
-  # the host on the user/identity is used. (foo.com for user@foo.com)
-  config :host, :validate => :string
-
-  # The message to send. This supports dynamic strings like %{host}
-  config :message, :validate => :string, :required => true
-
-  public
-  def register
-    require "xmpp4r"
-    @client = connect
-
-    @mucs = []
-    @users = [] if !@users
-
-    # load the MUC Client if we are joining rooms.
-    if @rooms && !@rooms.empty?
-      require 'xmpp4r/muc'
-      @rooms.each do |room| # handle muc messages in different rooms
-        muc = Jabber::MUC::MUCClient.new(@client)
-        muc.join(room)
-        @mucs << muc
-      end # @rooms.each
-    end # if @rooms
-  end # def register
-
-  public
-  def connect
-    Jabber::debug = true
-    client = Jabber::Client.new(Jabber::JID.new(@user))
-    client.connect(@host)
-    client.auth(@password.value)
-    return client
-  end # def connect
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    string_message = event.sprintf(@message)
-    @users.each do |user|
-      msg = Jabber::Message.new(user, string_message)
-      msg.type = :chat
-      @client.send(msg)
-    end # @targets.each
-
-    msg = Jabber::Message.new(nil, string_message)
-    msg.type = :groupchat
-    @mucs.each do |muc|
-      muc.send(msg)
-    end # @mucs.each
-  end # def receive
-end # class LogStash::Outputs::Xmpp
diff --git a/lib/logstash/outputs/zeromq.rb b/lib/logstash/outputs/zeromq.rb
deleted file mode 100644
index c4b088fc236..00000000000
--- a/lib/logstash/outputs/zeromq.rb
+++ /dev/null
@@ -1,125 +0,0 @@
-# encoding: utf-8
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Write events to a 0MQ PUB socket.
-#
-# You need to have the 0mq 2.1.x library installed to be able to use
-# this output plugin.
-#
-# The default settings will create a publisher connecting to a subscriber
-# bound to tcp://127.0.0.1:2120
-#
-class LogStash::Outputs::ZeroMQ < LogStash::Outputs::Base
-
-  config_name "zeromq"
-  milestone 2
-
-  default :codec, "json"
-
-  # 0mq socket address to connect or bind.
-  # Please note that `inproc://` will not work with logstashi.
-  # For each we use a context per thread.
-  # By default, inputs bind/listen and outputs connect.
-  config :address, :validate => :array, :default => ["tcp://127.0.0.1:2120"]
-
-  # The default logstash topologies work as follows:
-  #
-  # * pushpull - inputs are pull, outputs are push
-  # * pubsub - inputs are subscribers, outputs are publishers
-  # * pair - inputs are clients, inputs are servers
-  #
-  # If the predefined topology flows don't work for you,
-  # you can change the 'mode' setting
-  # TODO (lusis) add req/rep MAYBE
-  # TODO (lusis) add router/dealer
-  config :topology, :validate => ["pushpull", "pubsub", "pair"], :required => true
-
-  # This is used for the 'pubsub' topology only.
-  # On inputs, this allows you to filter messages by topic.
-  # On outputs, this allows you to tag a message for routing.
-  # NOTE: ZeroMQ does subscriber-side filtering
-  # NOTE: Topic is evaluated with `event.sprintf` so macros are valid here.
-  config :topic, :validate => :string, :default => ""
-
-  # Server mode binds/listens. Client mode connects.
-  config :mode, :validate => ["server", "client"], :default => "client"
-
-  # This exposes zmq_setsockopt for advanced tuning.
-  # See http://api.zeromq.org/2-1:zmq-setsockopt for details.
-  #
-  # This is where you would set values like:
-  #
-  # * ZMQ::HWM - high water mark
-  # * ZMQ::IDENTITY - named queues
-  # * ZMQ::SWAP_SIZE - space for disk overflow
-  #
-  # Example: sockopt => ["ZMQ::HWM", 50, "ZMQ::IDENTITY", "my_named_queue"]
-  config :sockopt, :validate => :hash
-
-  public
-  def register
-    require "ffi-rzmq"
-    require "logstash/util/zeromq"
-    self.class.send(:include, LogStash::Util::ZeroMQ)
-
-    if @mode == "server"
-      workers_not_supported("With 'mode => server', only one zeromq socket may bind to a port and may not be shared among threads. Going to single-worker mode for this plugin!")
-    end
-
-    # Translate topology shorthand to socket types
-    case @topology
-    when "pair"
-      zmq_const = ZMQ::PAIR
-    when "pushpull"
-      zmq_const = ZMQ::PUSH
-    when "pubsub"
-      zmq_const = ZMQ::PUB
-    end # case socket_type
-
-    @zsocket = context.socket(zmq_const)
-
-    error_check(@zsocket.setsockopt(ZMQ::LINGER, 1),
-                "while setting ZMQ::LINGER == 1)")
-
-    if @sockopt
-      setopts(@zsocket, @sockopt)
-    end
-
-    @address.each do |addr|
-      setup(@zsocket, addr)
-    end
-
-    @codec.on_event(&method(:publish))
-  end # def register
-
-  public
-  def teardown
-    error_check(@zsocket.close, "while closing the socket")
-  end # def teardown
-
-  private
-  def server?
-    @mode == "server"
-  end # def server?
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    @codec.encode(event)
-  end # def receive
-
-  def publish(payload)
-    @logger.debug? && @logger.debug("0mq: sending", :event => payload)
-    if @topology == "pubsub"
-      # TODO(sissel): Need to figure out how to fit this into the codecs system.
-      #@logger.debug("0mq output: setting topic to: #{event.sprintf(@topic)}")
-      #error_check(@zsocket.send_string(event.sprintf(@topic), ZMQ::SNDMORE),
-                  #"in topic send_string")
-    end
-    error_check(@zsocket.send_string(payload), "in send_string")
-  rescue => e
-    @logger.warn("0mq output exception", :address => @address, :exception => e)
-  end
-end # class LogStash::Outputs::ZeroMQ
diff --git a/lib/logstash/pipeline.rb b/lib/logstash/pipeline.rb
index 8ed9c7b5a52..b2fb6755144 100644
--- a/lib/logstash/pipeline.rb
+++ b/lib/logstash/pipeline.rb
@@ -1,14 +1,18 @@
 # encoding: utf-8
-require "logstash/config/file"
+require "thread" #
+require "stud/interval"
 require "logstash/namespace"
-require "thread" # stdlib
+require "logstash/errors"
+require "logstash/event"
+require "logstash/config/file"
 require "logstash/filters/base"
 require "logstash/inputs/base"
 require "logstash/outputs/base"
-require "logstash/errors"
-require "stud/interval" # gem stud
 
 class LogStash::Pipeline
+
+  FLUSH_EVENT = LogStash::FlushEvent.new
+
   def initialize(configstr)
     @logger = Cabin::Channel.get(LogStash)
     grammar = LogStashConfigParser.new
@@ -69,6 +73,7 @@ def filters?
   def run
     @started = true
     @input_threads = []
+
     start_inputs
     start_filters if filters?
     start_outputs
@@ -78,11 +83,12 @@ def run
     @logger.info("Pipeline started")
     wait_inputs
 
-    # In theory there's nothing to do to filters to tell them to shutdown?
     if filters?
       shutdown_filters
       wait_filters
+      flush_filters_to_output!(:final => true)
     end
+
     shutdown_outputs
     wait_outputs
 
@@ -103,7 +109,8 @@ def wait_inputs
   end
 
   def shutdown_filters
-    @input_to_filter.push(LogStash::ShutdownSignal)
+    @flusher_lock.synchronize { @flusher_thread.kill }
+    @input_to_filter.push(LogStash::ShutdownEvent.new)
   end
 
   def wait_filters
@@ -112,7 +119,7 @@ def wait_filters
 
   def shutdown_outputs
     # nothing, filters will do this
-    @filter_to_output.push(LogStash::ShutdownSignal)
+    @filter_to_output.push(LogStash::ShutdownEvent.new)
   end
 
   def wait_outputs
@@ -143,11 +150,12 @@ def start_filters
       Thread.new { filterworker }
     end
 
-    # Set up the periodic flusher thread.
-    @flusher_thread = Thread.new { Stud.interval(5) { filter_flusher } }
+    @flusher_lock = Mutex.new
+    @flusher_thread = Thread.new { Stud.interval(5) { @flusher_lock.synchronize { @input_to_filter.push(FLUSH_EVENT) } } }
   end
 
   def start_outputs
+    @outputs.each(&:register)
     @output_threads = [
       Thread.new { outputworker }
     ]
@@ -189,24 +197,23 @@ def filterworker
     begin
       while true
         event = @input_to_filter.pop
-        if event == LogStash::ShutdownSignal
+
+        case event
+        when LogStash::Event
+          # use events array to guarantee ordering of origin vs created events
+          # where created events are emitted by filters like split or metrics
+          events = []
+          filter(event) { |newevent| events << newevent }
+          events.each { |event| @filter_to_output.push(event) }
+        when LogStash::FlushEvent
+          # handle filter flushing here so that non threadsafe filters (thus only running one filterworker)
+          # don't have to deal with thread safety implementing the flush method
+          @flusher_lock.synchronize { flush_filters_to_output! }
+        when LogStash::ShutdownEvent
+          # pass it down to any other filterworker and stop this worker
           @input_to_filter.push(event)
           break
         end
-
-
-        # TODO(sissel): we can avoid the extra array creation here
-        # if we don't guarantee ordering of origin vs created events.
-        # - origin event is one that comes in naturally to the filter worker.
-        # - created events are emitted by filters like split or metrics
-        events = [event]
-        filter(event) do |newevent|
-          events << newevent
-        end
-        events.each do |event|
-          next if event.cancelled?
-          @filter_to_output.push(event)
-        end
       end
     rescue => e
       @logger.error("Exception in filterworker", "exception" => e, "backtrace" => e.backtrace)
@@ -217,14 +224,17 @@ def filterworker
 
   def outputworker
     LogStash::Util::set_thread_name(">output")
-    @outputs.each(&:register)
     @outputs.each(&:worker_setup)
+
     while true
       event = @filter_to_output.pop
-      break if event == LogStash::ShutdownSignal
+      break if event.is_a?(LogStash::ShutdownEvent)
       output(event)
     end # while true
-    @outputs.each(&:teardown)
+
+    @outputs.each do |output|
+      output.worker_plugins.each(&:teardown)
+    end
   end # def outputworker
 
   # Shutdown this pipeline.
@@ -248,7 +258,7 @@ def shutdown
       end
     end
 
-    # No need to send the ShutdownSignal to the filters/outputs nor to wait for
+    # No need to send the ShutdownEvent to the filters/outputs nor to wait for
     # the inputs to finish, because in the #run method we wait for that anyway.
   end # def shutdown
 
@@ -266,28 +276,27 @@ def output(event)
     @output_func.call(event)
   end
 
-  def filter_flusher
-    events = []
-    @filters.each do |filter|
+  # perform filters flush and yeild flushed event to the passed block
+  # @param options [Hash]
+  # @option options [Boolean] :final => true to signal a final shutdown flush
+  def flush_filters(options = {}, &block)
+    flushers = options[:final] ? @shutdown_flushers : @periodic_flushers
 
-      # Filter any events generated so far in this flush.
-      events.each do |event|
-        # TODO(sissel): watchdog on flush filtration?
-        unless event.cancelled?
-          filter.filter(event)
-        end
-      end
+    flushers.each do |flusher|
+      flusher.call(options, &block)
+    end
+  end
 
-      # TODO(sissel): watchdog on flushes?
-      if filter.respond_to?(:flush)
-        flushed = filter.flush
-        events += flushed if !flushed.nil? && flushed.any?
+  # perform filters flush into the output queue
+  # @param options [Hash]
+  # @option options [Boolean] :final => true to signal a final shutdown flush
+  def flush_filters_to_output!(options = {})
+    flush_filters(options) do |event|
+      unless event.cancelled?
+        @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
+        @filter_to_output.push(event)
       end
     end
+  end # flush_filters_to_output!
 
-    events.each do |event|
-      @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
-      @filter_to_output.push(event) unless event.cancelled?
-    end
-  end # def filter_flusher
 end # class Pipeline
diff --git a/lib/logstash/plugin.rb b/lib/logstash/plugin.rb
index 418ccb8a9ca..ce8de95a5a0 100644
--- a/lib/logstash/plugin.rb
+++ b/lib/logstash/plugin.rb
@@ -8,6 +8,8 @@ class LogStash::Plugin
   attr_accessor :params
   attr_accessor :logger
 
+  NL = "\n"
+
   public
   def hash
     params.hash ^
diff --git a/lib/logstash/plugin_mixins/aws_config.rb b/lib/logstash/plugin_mixins/aws_config.rb
deleted file mode 100644
index b4ec9d32844..00000000000
--- a/lib/logstash/plugin_mixins/aws_config.rb
+++ /dev/null
@@ -1,93 +0,0 @@
-# encoding: utf-8
-require "logstash/config/mixin"
-
-module LogStash::PluginMixins::AwsConfig
-
-  @logger = Cabin::Channel.get(LogStash)
-
-  # This method is called when someone includes this module
-  def self.included(base)
-    # Add these methods to the 'base' given.
-    base.extend(self)
-    base.setup_aws_config
-  end
-
-  US_EAST_1 = "us-east-1"
-  
-  public
-  def setup_aws_config
-    # The AWS Region
-    config :region, :validate => [US_EAST_1, "us-west-1", "us-west-2",
-                                  "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                  "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => US_EAST_1
-
-    # This plugin uses the AWS SDK and supports several ways to get credentials, which will be tried in this order...   
-    # 1. Static configuration, using `access_key_id` and `secret_access_key` params in logstash plugin config   
-    # 2. External credentials file specified by `aws_credentials_file`   
-    # 3. Environment variables `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`   
-    # 4. Environment variables `AMAZON_ACCESS_KEY_ID` and `AMAZON_SECRET_ACCESS_KEY`   
-    # 5. IAM Instance Profile (available when running inside EC2)   
-    config :access_key_id, :validate => :string
-
-    # The AWS Secret Access Key
-    config :secret_access_key, :validate => :string
-
-    # Should we require (true) or disable (false) using SSL for communicating with the AWS API   
-    # The AWS SDK for Ruby defaults to SSL so we preserve that
-    config :use_ssl, :validate => :boolean, :default => true
-
-    # URI to proxy server if required
-    config :proxy_uri, :validate => :string
-
-    # Path to YAML file containing a hash of AWS credentials.   
-    # This file will only be loaded if `access_key_id` and
-    # `secret_access_key` aren't set. The contents of the
-    # file should look like this:
-    #
-    #     :access_key_id: "12345"
-    #     :secret_access_key: "54321"
-    #
-    config :aws_credentials_file, :validate => :string
-  end
-
-  public
-  def aws_options_hash
-    if @access_key_id.is_a?(NilClass) ^ @secret_access_key.is_a?(NilClass)
-      @logger.warn("Likely config error: Only one of access_key_id or secret_access_key was provided but not both.")
-    end
-
-    if ((!@access_key_id || !@secret_access_key)) && @aws_credentials_file
-      access_creds = YAML.load_file(@aws_credentials_file)
-
-      @access_key_id = access_creds[:access_key_id]
-      @secret_access_key = access_creds[:secret_access_key]
-    end
-
-    opts = {}
-
-    if (@access_key_id && @secret_access_key)
-      opts[:access_key_id] = @access_key_id
-      opts[:secret_access_key] = @secret_access_key
-    end
-
-    opts[:use_ssl] = @use_ssl
-
-    if (@proxy_uri)
-      opts[:proxy_uri] = @proxy_uri
-    end
-
-    # The AWS SDK for Ruby doesn't know how to make an endpoint hostname from a region
-    # for example us-west-1 -> foosvc.us-west-1.amazonaws.com
-    # So our plugins need to know how to generate their endpoints from a region
-    # Furthermore, they need to know the symbol required to set that value in the AWS SDK
-    # Classes using this module must implement aws_service_endpoint(region:string)
-    # which must return a hash with one key, the aws sdk for ruby config symbol of the service
-    # endpoint, which has a string value of the service endpoint hostname
-    # for example, CloudWatch, { :cloud_watch_endpoint => "monitoring.#{region}.amazonaws.com" }
-    # For a list, see https://github.com/aws/aws-sdk-ruby/blob/master/lib/aws/core/configuration.rb
-    opts.merge!(self.aws_service_endpoint(@region))
-    
-    return opts
-  end # def aws_options_hash
-
-end
diff --git a/lib/logstash/pluginmanager.rb b/lib/logstash/pluginmanager.rb
new file mode 100644
index 00000000000..fb365f20db4
--- /dev/null
+++ b/lib/logstash/pluginmanager.rb
@@ -0,0 +1,7 @@
+require "logstash/namespace"
+
+module LogStash::PluginManager
+
+require 'logstash/pluginmanager/main'
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/install.rb b/lib/logstash/pluginmanager/install.rb
new file mode 100644
index 00000000000..2c26a20d55a
--- /dev/null
+++ b/lib/logstash/pluginmanager/install.rb
@@ -0,0 +1,66 @@
+require 'clamp'
+require 'logstash/namespace'
+require 'logstash/environment'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'logstash/pluginmanager/vendor'
+require 'rubygems/dependency_installer'
+require 'rubygems/uninstaller'
+require 'jar-dependencies'
+require 'jar_install_post_install_hook'
+
+class LogStash::PluginManager::Install < Clamp::Command
+
+  parameter "PLUGIN", "plugin name or file"
+
+  option "--version", "VERSION", "version of the plugin to install", :default => ">= 0"
+
+  option "--proxy", "PROXY", "Use HTTP proxy for remote operations"
+
+  def execute
+    LogStash::Environment.load_logstash_gemspec!
+
+    ::Gem.configuration.verbose = false
+    ::Gem.configuration[:http_proxy] = proxy 
+
+    puts ("validating #{plugin} #{version}")
+
+    unless gem_path = (plugin =~ /\.gem$/ && File.file?(plugin)) ? plugin : LogStash::PluginManager::Util.download_gem(plugin, version)
+      $stderr.puts ("Plugin does not exist '#{plugin}'. Aborting")
+      return 99
+    end
+
+    unless gem_meta = LogStash::PluginManager::Util.logstash_plugin?(gem_path)
+      $stderr.puts ("Invalid logstash plugin gem '#{plugin}'. Aborting...")
+      return 99
+    end
+
+    puts ("Valid logstash plugin. Continuing...")
+
+    if LogStash::PluginManager::Util.installed?(gem_meta.name)
+
+      current = Gem::Specification.find_by_name(gem_meta.name)
+      if Gem::Version.new(current.version) > Gem::Version.new(gem_meta.version)
+        unless LogStash::PluginManager::Util.ask_yesno("Do you wish to downgrade this plugin?")
+          $stderr.puts("Aborting installation")
+          return 99
+        end
+      end
+
+      puts ("removing existing plugin before installation")
+      ::Gem.done_installing_hooks.clear
+      ::Gem::Uninstaller.new(gem_meta.name, {:force => true}).uninstall
+    end
+
+    ::Gem.configuration.verbose = false
+    LogStash::PluginManager::Vendor.setup_hook
+    options = {}
+    options[:document] = []
+    inst = Gem::DependencyInstaller.new(options)
+    inst.install plugin, version
+    specs = inst.installed_gems.detect { |gemspec| gemspec.name == gem_meta.name }
+    puts ("Successfully installed '#{specs.name}' with version '#{specs.version}'")
+    return 0
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/list.rb b/lib/logstash/pluginmanager/list.rb
new file mode 100644
index 00000000000..c897429a37b
--- /dev/null
+++ b/lib/logstash/pluginmanager/list.rb
@@ -0,0 +1,38 @@
+require 'clamp'
+require 'logstash/namespace'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'rubygems/spec_fetcher'
+
+class LogStash::PluginManager::List < Clamp::Command
+
+  parameter "[PLUGIN]", "Plugin name to search for, leave empty for all plugins"
+
+  option "--group", "NAME", "Show all plugins from a certain group. Can be one of 'output', 'input', 'codec', 'filter'"
+
+  def execute
+
+    if group
+      unless ['input', 'output', 'filter', 'codec'].include?(group)
+        signal_usage_error "Group name not valid"
+      end
+      plugin_name = nil
+    else
+      plugin_name = plugin
+    end
+
+    Gem.configuration.verbose = false
+
+    # If we are listing a group make sure we check all gems
+    specs = LogStash::PluginManager::Util.matching_specs(plugin_name) \
+            .select{|spec| LogStash::PluginManager::Util.logstash_plugin?(spec) } \
+            .select{|spec| group ? group == spec.metadata['logstash_group'] : true}
+    if specs.empty?
+      $stderr.puts ("No plugins found.")
+      return 0
+    end
+    specs.each {|spec| puts ("#{spec.name} (#{spec.version})") }
+    return 0
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/main.rb b/lib/logstash/pluginmanager/main.rb
new file mode 100644
index 00000000000..e66d562d285
--- /dev/null
+++ b/lib/logstash/pluginmanager/main.rb
@@ -0,0 +1,17 @@
+require "logstash/namespace"
+require "logstash/errors"
+require 'clamp'
+require 'logstash/pluginmanager/install'
+require 'logstash/pluginmanager/uninstall'
+require 'logstash/pluginmanager/list'
+require 'logstash/pluginmanager/update'
+require 'logstash/pluginmanager/util'
+
+class LogStash::PluginManager::Main < Clamp::Command
+
+  subcommand "install", "Install a plugin", LogStash::PluginManager::Install
+  subcommand "uninstall", "Uninstall a plugin", LogStash::PluginManager::Uninstall
+  subcommand "update", "Install a plugin", LogStash::PluginManager::Update
+  subcommand "list", "List all installed plugins", LogStash::PluginManager::List
+
+end # class Logstash::PluginManager::Main
diff --git a/lib/logstash/pluginmanager/uninstall.rb b/lib/logstash/pluginmanager/uninstall.rb
new file mode 100644
index 00000000000..f5c247122ef
--- /dev/null
+++ b/lib/logstash/pluginmanager/uninstall.rb
@@ -0,0 +1,30 @@
+require "logstash/namespace"
+require "logstash/logging"
+require "logstash/errors"
+require 'clamp'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'rubygems/uninstaller'
+
+class LogStash::PluginManager::Uninstall < Clamp::Command
+
+  parameter "PLUGIN", "plugin name"
+
+  public
+  def execute
+
+    ::Gem.configuration.verbose = false
+
+    puts ("Validating removal of #{plugin}.")
+    
+    unless gem_data = LogStash::PluginManager::Util.logstash_plugin?(plugin)
+      $stderr.puts ("Trying to remove a non logstash plugin. Aborting")
+      return 99
+    end
+
+    puts ("Uninstalling plugin '#{plugin}' with version '#{gem_data.version}'.")
+    ::Gem::Uninstaller.new(plugin, {}).uninstall
+    return 
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/update.rb b/lib/logstash/pluginmanager/update.rb
new file mode 100644
index 00000000000..4bf1c4e28ca
--- /dev/null
+++ b/lib/logstash/pluginmanager/update.rb
@@ -0,0 +1,76 @@
+require 'clamp'
+require 'logstash/namespace'
+require 'logstash/pluginmanager'
+require 'logstash/pluginmanager/util'
+require 'logstash/pluginmanager/vendor'
+require 'rubygems/dependency_installer'
+require 'rubygems/uninstaller'
+require 'jar-dependencies'
+require 'jar_install_post_install_hook'
+
+class LogStash::PluginManager::Update < Clamp::Command
+
+  parameter "[PLUGIN]", "Plugin name"
+
+  option "--version", "VERSION", "version of the plugin to install", :default => ">= 0"
+
+  option "--proxy", "PROXY", "Use HTTP proxy for remote operations"
+
+  def execute
+
+    LogStash::Environment.load_logstash_gemspec!
+    ::Gem.configuration.verbose = false
+    ::Gem.configuration[:http_proxy] = proxy
+
+    if plugin.nil?
+      puts ("Updating all plugins")
+    else
+      puts ("Updating #{plugin} plugin")
+    end
+
+    specs = LogStash::PluginManager::Util.matching_specs(plugin).select{|spec| LogStash::PluginManager::Util.logstash_plugin?(spec) }
+    if specs.empty?
+      $stderr.puts ("No plugins found to update or trying to update a non logstash plugin.")
+      return 99
+    end
+    specs.each { |spec| update_gem(spec, version) }
+    return 0
+  end
+
+
+  def update_gem(spec, version)
+
+    unless gem_path = LogStash::PluginManager::Util.download_gem(spec.name, version)
+      $stderr.puts ("Plugin '#{spec.name}' does not exist remotely. Skipping.")
+      return 0
+    end
+
+    unless gem_meta = LogStash::PluginManager::Util.logstash_plugin?(gem_path)
+      $stderr.puts ("Invalid logstash plugin gem. skipping.")
+      return 99
+    end
+
+    unless Gem::Version.new(gem_meta.version) > Gem::Version.new(spec.version)
+      puts ("No newer version available for #{spec.name}. skipping.")
+      return 0
+    end
+
+    puts ("Updating #{spec.name} from version #{spec.version} to #{gem_meta.version}")
+
+    if LogStash::PluginManager::Util.installed?(spec.name)
+      ::Gem.done_installing_hooks.clear
+      ::Gem::Uninstaller.new(gem_meta.name, {:force => true}).uninstall
+    end
+
+    ::Gem.configuration.verbose = false
+    LogStash::PluginManager::Vendor.setup_hook
+    options = {}
+    options[:document] = []
+    inst = Gem::DependencyInstaller.new(options)
+    inst.install spec.name, gem_meta.version
+    specs, _ = inst.installed_gems
+    puts ("Update successful")
+    return 0
+  end
+
+end # class Logstash::PluginManager
diff --git a/lib/logstash/pluginmanager/util.rb b/lib/logstash/pluginmanager/util.rb
new file mode 100644
index 00000000000..ce6cab38058
--- /dev/null
+++ b/lib/logstash/pluginmanager/util.rb
@@ -0,0 +1,52 @@
+
+class LogStash::PluginManager::Util
+
+  def self.logstash_plugin?(gem)
+
+    gem_data = case
+    when gem.is_a?(Gem::Specification); gem
+    when (gem =~ /\.gem$/ and File.file?(gem)); Gem::Package.new(gem).spec
+    else Gem::Specification.find_by_name(gem)
+    end
+
+    gem_data.metadata['logstash_plugin'] == "true" ? gem_data : false
+  end
+
+  def self.download_gem(gem_name, gem_version = '')
+ 
+    gem_version ||= Gem::Requirement.default
+ 
+    dep = ::Gem::Dependency.new(gem_name, gem_version)
+    specs_and_sources, errors = ::Gem::SpecFetcher.fetcher.spec_for_dependency dep
+    if specs_and_sources.empty?
+      return false
+    end
+    spec, source = specs_and_sources.max_by { |s,| s.version }
+    path = source.download( spec, java.lang.System.getProperty("java.io.tmpdir"))
+    path
+  end
+
+  def self.installed?(name)
+    Gem::Specification.any? { |x| x.name == name }
+  end
+
+  def self.matching_specs(name)
+    req = Gem::Requirement.default
+    re = name ? /#{name}/i : //
+    specs = Gem::Specification.find_all{|spec| spec.name =~ re && req =~ spec.version}
+    specs.inject({}){|result, spec| result[spec.name_tuple] = spec; result}.values
+  end
+
+  def self.ask_yesno(prompt)
+    while true
+      $stderr.puts ("#{prompt} [y/n]: ")
+      case $stdin.getc.downcase
+        when 'Y', 'y', 'j', 'J', 'yes' #j for Germans (Ja)
+          return true
+        when /\A[nN]o?\Z/ #n or no
+          break
+      end
+    end
+  end
+
+end
diff --git a/lib/logstash/pluginmanager/vendor.rb b/lib/logstash/pluginmanager/vendor.rb
new file mode 100644
index 00000000000..0c3aa53c044
--- /dev/null
+++ b/lib/logstash/pluginmanager/vendor.rb
@@ -0,0 +1,17 @@
+require 'logstash/json'
+require 'logstash/util/filetools'
+
+module LogStash::PluginManager::Vendor
+
+  def self.setup_hook
+    Gem.post_install do |gem_installer|
+      next if ENV['VENDOR_SKIP'] == 'true'
+      vendor_file = ::File.join(gem_installer.gem_dir, 'vendor.json')
+      if ::File.exist?(vendor_file)
+        vendor_file_content = IO.read(vendor_file)
+        file_list = LogStash::Json.load(vendor_file_content)
+        LogStash::Util::FileTools.process_downloads(file_list, ::File.join(gem_installer.gem_dir, 'vendor'))
+      end
+    end
+  end
+end
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index 4a396b0802c..0d8b0cc12ed 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -4,6 +4,11 @@
 $START = Time.now
 $DEBUGLIST = (ENV["DEBUG"] || "").split(",")
 
+require "logstash/environment"
+LogStash::Environment.set_gem_paths!
+LogStash::Environment.load_logstash_gemspec!
+LogStash::Environment.load_locale!
+
 Thread.abort_on_exception = true
 if ENV["PROFILE_BAD_LOG_CALLS"] || $DEBUGLIST.include?("log")
   # Set PROFILE_BAD_LOG_CALLS=1 in your environment if you want
@@ -43,11 +48,6 @@ module Cabin::Mixins::Logger
 require "logstash/monkeypatches-for-debugging"
 require "logstash/namespace"
 require "logstash/program"
-require "i18n" # gem 'i18n'
-I18n.enforce_available_locales = true
-I18n.load_path << File.expand_path(
-  File.join(File.dirname(__FILE__), "../../locales/en.yml")
-)
 
 class LogStash::RSpecsRunner
   def initialize(args)
@@ -66,10 +66,6 @@ def wait
 class LogStash::Runner
   include LogStash::Program
 
-  def initialize
-    @runners = []
-  end
-
   def main(args)
     require "logstash/util"
     require "stud/trap"
@@ -86,24 +82,8 @@ def main(args)
 
     Stud::untrap("INT", @startup_interruption_trap)
 
-    args = [nil] if args.empty?
-
-    while args != nil && !args.empty?
-      args = run(args)
-    end
-
-    status = []
-    @runners.each do |r|
-      #$stderr.puts "Waiting on #{r.wait.inspect}"
-      status << r.wait
-    end
-
-    # Avoid running test/unit's at_exit crap
-    if status.empty? || status.first.nil?
-      exit(0)
-    else
-      exit(status.first)
-    end
+    task = run(args)
+    exit(task.wait)
   end # def self.main
 
   def run(args)
@@ -115,14 +95,12 @@ def run(args)
         if args.include?("--verbose")
           agent_args << "--verbose"
         end
-        LogStash::Agent.run($0, agent_args)
-        return []
+        return LogStash::Agent.run($0, agent_args)
       end,
       "web" => lambda do
         # Give them kibana.
         require "logstash/kibana"
         kibana = LogStash::Kibana::Runner.new
-        @runners << kibana
         return kibana.run(args)
       end,
       "rspec" => lambda do
@@ -130,26 +108,45 @@ def run(args)
         require "rspec"
         spec_path = File.expand_path(File.join(File.dirname(__FILE__), "/../../spec"))
         $LOAD_PATH << spec_path
-        require "test_utils"
-        all_specs = Dir.glob(File.join(spec_path, "/**/*.rb"))
+        all_specs = Dir.glob(File.join(spec_path, "/**/*_spec.rb"))
         rspec = LogStash::RSpecsRunner.new(args.empty? ? all_specs : args)
-        rspec.run
-        @runners << rspec
-        return []
+        return rspec.run
       end,
       "irb" => lambda do
         require "irb"
-        IRB.start(__FILE__)
-        return []
-      end,
-      "ruby" => lambda do
-        require(args[0])
-        return []
+        return IRB.start(__FILE__)
       end,
       "pry" => lambda do
         require "pry"
         return binding.pry
       end,
+      "docgen" => lambda do
+        require 'docs/asciidocgen'
+        opts = OptionParser.new
+        settings = {}
+        opts.on("-o DIR", "--output DIR",
+          "Directory to output to; optional. If not specified,"\
+          "we write to stdout.") do |val|
+          settings[:output] = val
+        end
+        args = opts.parse(ARGV)
+        docs = LogStashConfigAsciiDocGenerator.new
+        args.each do |arg|
+          docs.generate(arg, settings)
+        end
+        return 0
+      end,
+      "plugin" => lambda do
+        require 'logstash/pluginmanager'
+        plugin_manager = LogStash::PluginManager::Main.new($0)
+        begin
+          plugin_manager.parse(args)
+          return plugin_manager.execute
+        rescue Clamp::HelpWanted => e
+          show_help(e.command)
+          return 0
+        end
+      end,
       "agent" => lambda do
         require "logstash/agent"
         # Hack up a runner
@@ -158,21 +155,20 @@ def run(args)
           agent.parse(args)
         rescue Clamp::HelpWanted => e
           show_help(e.command)
-          return []
+          return 0
         rescue Clamp::UsageError => e
           # If 'too many arguments' then give the arguments to
           # the next command. Otherwise it's a real error.
           raise if e.message != "too many arguments"
           remaining = agent.remaining_arguments
         end
-        @runners << Stud::Task.new { agent.execute }
 
-        return remaining
+        return agent.execute
       end
     } # commands
 
     if commands.include?(command)
-      args = commands[command].call
+      return Stud::Task.new { commands[command].call }
     else
       if command.nil?
         $stderr.puts "No command given"
@@ -182,21 +178,20 @@ def run(args)
           $stderr.puts "No such command #{command.inspect}"
         end
       end
-      $stderr.puts "Usage: logstash <command> [command args]"
-      $stderr.puts "Run a command with the --help flag to see the arguments."
-      $stderr.puts "For example: logstash agent --help"
-      $stderr.puts
-      # hardcode the available commands to reduce confusion.
-      $stderr.puts "Available commands:"
-      $stderr.puts "  agent - runs the logstash agent"
-      $stderr.puts "  version - emits version info about this logstash"
-      $stderr.puts "  web - runs the logstash web ui (called Kibana)"
-      $stderr.puts "  rspec - runs tests"
+      $stderr.puts %q[
+Usage: logstash <command> [command args]
+Run a command with the --help flag to see the arguments.
+For example: logstash agent --help
+
+Available commands:
+  agent - runs the logstash agent
+  version - emits version info about this logstash
+  web - runs the logstash web ui (called Kibana)
+  rspec - runs tests
+      ]
       #$stderr.puts commands.keys.map { |s| "  #{s}" }.join("\n")
-      exit 1
+      return Stud::Task.new { 1 }
     end
-
-    return args
   end # def run
 
   # @return true if this file is the main file being run and not via rspec
diff --git a/lib/logstash/time_addon.rb b/lib/logstash/time_addon.rb
deleted file mode 100644
index a5970332dc3..00000000000
--- a/lib/logstash/time_addon.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-
-module LogStash::Time
-  ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
-  def self.now
-    return Time.new.utc
-  end
-
-  if RUBY_PLATFORM == "java"
-    JODA_ISO8601_PARSER = org.joda.time.format.ISODateTimeFormat.dateTimeParser
-    #JODA_ISO8601_PARSER = org.joda.time.format.DateTimeFormat.forPattern("yyyy-MM-dd'T'HH:mm:ss.SSSZ")
-    UTC = org.joda.time.DateTimeZone.forID("UTC")
-    def self.parse_iso8601(t)
-      millis = JODA_ISO8601_PARSER.parseMillis(t)
-      return Time.at(millis / 1000, (millis % 1000) * 1000)
-    end
-  else
-    def self.parse_iso8601(t)
-      # Warning, ruby's Time.parse is *really* terrible and slow.
-      return unless t.is_a?(String)
-      return Time.parse(t).gmtime
-    end
-  end
-end # module LogStash::Time
diff --git a/lib/logstash/timestamp.rb b/lib/logstash/timestamp.rb
new file mode 100644
index 00000000000..fb75c5f2538
--- /dev/null
+++ b/lib/logstash/timestamp.rb
@@ -0,0 +1,97 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/json"
+require "forwardable"
+require "date"
+require "time"
+
+module LogStash
+  class TimestampParserError < StandardError; end
+
+  class Timestamp
+    extend Forwardable
+    include Comparable
+
+    def_delegators :@time, :tv_usec, :usec, :year, :iso8601, :to_i, :tv_sec, :to_f, :to_edn, :<=>, :+
+
+    attr_reader :time
+
+    ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
+    ISO8601_PRECISION = 3
+
+    def initialize(time = Time.new)
+      @time = time.utc
+    end
+
+    def self.at(*args)
+      Timestamp.new(::Time.at(*args))
+    end
+
+    def self.parse(*args)
+      Timestamp.new(::Time.parse(*args))
+    end
+
+    def self.now
+      Timestamp.new(::Time.now)
+    end
+
+    # coerce tries different strategies based on the time object class to convert into a Timestamp.
+    # @param [String, Time, Timestamp] time the time object to try coerce
+    # @return [Timestamp, nil] Timestamp will be returned if successful otherwise nil
+    # @raise [TimestampParserError] on String with invalid format
+    def self.coerce(time)
+      case time
+      when String
+        LogStash::Timestamp.parse_iso8601(time)
+      when LogStash::Timestamp
+        time
+      when Time
+        LogStash::Timestamp.new(time)
+      else
+        nil
+      end
+    end
+
+    if LogStash::Environment.jruby?
+      JODA_ISO8601_PARSER = org.joda.time.format.ISODateTimeFormat.dateTimeParser
+      UTC = org.joda.time.DateTimeZone.forID("UTC")
+
+      def self.parse_iso8601(t)
+        millis = JODA_ISO8601_PARSER.parseMillis(t)
+        LogStash::Timestamp.at(millis / 1000, (millis % 1000) * 1000)
+      rescue => e
+        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
+      end
+
+    else
+
+      def self.parse_iso8601(t)
+        # warning, ruby's Time.parse is *really* terrible and slow.
+        LogStash::Timestamp.new(::Time.parse(t))
+      rescue => e
+        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
+      end
+    end
+
+    def utc
+      @time.utc # modifies the receiver
+      self
+    end
+    alias_method :gmtime, :utc
+
+    def to_json(*args)
+      # ignore arguments to respect accepted to_json method signature
+      "\"" + to_iso8601 + "\""
+    end
+    alias_method :inspect, :to_json
+
+    def to_iso8601
+      @iso8601 ||= @time.iso8601(ISO8601_PRECISION)
+    end
+    alias_method :to_s, :to_iso8601
+
+    def -(value)
+      @time - (value.is_a?(Timestamp) ? value.time : value)
+    end
+  end
+end
diff --git a/lib/logstash/util.rb b/lib/logstash/util.rb
index 76b5926b378..f0cbc956490 100644
--- a/lib/logstash/util.rb
+++ b/lib/logstash/util.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require "logstash/namespace"
+require "logstash/environment"
 
 module LogStash::Util
   UNAME = case RbConfig::CONFIG["host_os"]
@@ -14,7 +15,7 @@ def self.set_thread_name(name)
       Java::java.lang.Thread.currentThread.setName(name)
     end
     Thread.current[:name] = name
-    
+
     if UNAME == "linux"
       require "logstash/util/prctl"
       # prctl PR_SET_NAME allows up to 16 bytes for a process name
@@ -34,7 +35,7 @@ def self.hash_merge(dst, src)
         dvalue = dst[name]
         if dvalue.is_a?(Hash) && svalue.is_a?(Hash)
           dvalue = hash_merge(dvalue, svalue)
-        elsif svalue.is_a?(Array) 
+        elsif svalue.is_a?(Array)
           if dvalue.is_a?(Array)
             # merge arrays without duplicates.
             dvalue |= svalue
@@ -58,7 +59,7 @@ def self.hash_merge(dst, src)
 
     return dst
   end # def self.hash_merge
- 
+
   # Merge hash 'src' into 'dst' nondestructively
   #
   # Duplicate keys will become array values
@@ -71,7 +72,7 @@ def self.hash_merge_with_dups(dst, src)
         dvalue = dst[name]
         if dvalue.is_a?(Hash) && svalue.is_a?(Hash)
           dvalue = hash_merge(dvalue, svalue)
-        elsif svalue.is_a?(Array) 
+        elsif svalue.is_a?(Array)
           if dvalue.is_a?(Array)
             # merge arrays without duplicates.
             dvalue += svalue
@@ -103,4 +104,49 @@ def self.hash_merge_many(*hashes)
     end
     return dst
   end # def hash_merge_many
+
+
+  # nomalize method definition based on platform.
+  # normalize is used to convert an object create through
+  # json deserialization from JrJackson in :raw mode to pure Ruby
+  # to support these pure Ruby object monkey patches.
+  # see logstash/json.rb and logstash/java_integration.rb
+
+  if LogStash::Environment.jruby?
+    require "java"
+
+    # recursively convert any Java LinkedHashMap and ArrayList to pure Ruby.
+    # will not recurse into pure Ruby objects. Pure Ruby object should never
+    # contain LinkedHashMap and ArrayList since these are only created at
+    # initial deserialization, anything after (deeper) will be pure Ruby.
+    def self.normalize(o)
+      case o
+      when Java::JavaUtil::LinkedHashMap
+        o.inject({}){|r, (k, v)| r[k] = normalize(v); r}
+      when Java::JavaUtil::ArrayList
+        o.map{|i| normalize(i)}
+      else
+        o
+      end
+    end
+
+  else
+
+    # identity function, pure Ruby object don't need normalization.
+    def self.normalize(o); o; end
+  end
+
+  def self.stringify_symbols(o)
+    case o
+    when Hash
+      o.inject({}){|r, (k, v)| r[k.is_a?(Symbol) ? k.to_s : k] = stringify_symbols(v); r}
+    when Array
+      o.map{|i| stringify_symbols(i)}
+    when Symbol
+      o.to_s
+    else
+      o
+    end
+  end
+
 end # module LogStash::Util
diff --git a/lib/logstash/util/accessors.rb b/lib/logstash/util/accessors.rb
index b98615d6dab..6d97be9b294 100644
--- a/lib/logstash/util/accessors.rb
+++ b/lib/logstash/util/accessors.rb
@@ -32,48 +32,55 @@ def initialize(store)
 
     def get(accessor)
       target, key = lookup(accessor)
-      target.is_a?(Array) ? target[key.to_i] : target[key]
+      unless target.nil?
+        target.is_a?(Array) ? target[key.to_i] : target[key]
+      end
     end
 
     def set(accessor, value)
-      target, key = lookup(accessor)
-      target[key] = value
+      target, key = store_and_lookup(accessor)
+      target[target.is_a?(Array) ? key.to_i : key] = value
     end
 
     def strict_set(accessor, value)
-      set(accessor, strict_value(value))
+      set(accessor, LogStash::Event.validate_value(value))
     end
 
     def del(accessor)
       target, key = lookup(accessor)
-      target.delete(key)
+      target.is_a?(Array) ? target.delete_at(key.to_i) : target.delete(key)
     end
 
     private
 
     def lookup(accessor)
+      target, key = lookup_path(accessor)
+      if target.nil?
+        [target, key]
+      else
+        @lut[accessor] = [target, key]
+      end
+    end
+
+    def store_and_lookup(accessor)
       @lut[accessor] ||= store_path(accessor)
     end
 
-    def store_path(accessor)
+    def lookup_path(accessor)
       key, path = PathCache.get(accessor)
-      target = path.inject(@store) {|r, k| r[r.is_a?(Array) ? k.to_i : k] ||= {}}
+      target = path.inject(@store) do |r, k|
+        if r.nil?
+          return nil
+        end
+        r[r.is_a?(Array) ? k.to_i : k]
+      end
       [target, key]
     end
 
-    def strict_value(value)
-      case value
-      when String
-        raise("expected UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.encoding == Encoding::UTF_8
-        raise("invalid UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.valid_encoding?
-        value
-      when Array
-        value.each{|v| strict_value(v)} # don't map, return original object
-        value
-      else
-        value
-      end
+    def store_path(accessor)
+      key, path = PathCache.get(accessor)
+      target = path.inject(@store) {|r, k| r[r.is_a?(Array) ? k.to_i : k] ||= {}}
+      [target, key]
     end
-
   end # class Accessors
 end # module LogStash::Util
diff --git a/lib/logstash/util/filetools.rb b/lib/logstash/util/filetools.rb
new file mode 100644
index 00000000000..9b93648bbde
--- /dev/null
+++ b/lib/logstash/util/filetools.rb
@@ -0,0 +1,185 @@
+require "net/http"
+require "uri"
+require "digest/sha1"
+require 'fileutils'
+
+module LogStash::Util::FileTools
+  extend self
+
+  def fetch(url, sha1, output)
+
+    puts "Downloading #{url}"
+    actual_sha1 = download(url, output)
+
+    if actual_sha1 != sha1
+      fail "SHA1 does not match (expected '#{sha1}' but got '#{actual_sha1}')"
+    end
+  end # def fetch
+
+  def file_fetch(url, sha1, target)
+    filename = File.basename( URI(url).path )
+    output = "#{target}/#{filename}"
+    begin
+      actual_sha1 = file_sha1(output)
+      if actual_sha1 != sha1
+        fetch(url, sha1, output)
+      end
+    rescue Errno::ENOENT
+      fetch(url, sha1, output)
+    end
+    return output
+  end
+
+  def file_sha1(path)
+    digest = Digest::SHA1.new
+    fd = File.new(path, "r")
+    while true
+      begin
+        digest << fd.sysread(16384)
+      rescue EOFError
+        break
+      end
+    end
+    return digest.hexdigest
+  ensure
+    fd.close if fd
+  end
+
+  def download(url, output)
+    uri = URI(url)
+    digest = Digest::SHA1.new
+    tmp = "#{output}.tmp"
+    Net::HTTP.start(uri.host, uri.port, :use_ssl => (uri.scheme == "https")) do |http|
+      request = Net::HTTP::Get.new(uri.path)
+      http.request(request) do |response|
+        fail "HTTP fetch failed for #{url}. #{response}" if [200, 301].include?(response.code)
+        size = (response["content-length"].to_i || -1).to_f
+        count = 0
+        File.open(tmp, "w") do |fd|
+          response.read_body do |chunk|
+            fd.write(chunk)
+            digest << chunk
+            if size > 0 && $stdout.tty?
+              count += chunk.bytesize
+              $stdout.write(sprintf("\r%0.2f%%", count/size * 100))
+            end
+          end
+        end
+        $stdout.write("\r      \r") if $stdout.tty?
+      end
+    end
+
+    File.rename(tmp, output)
+
+    return digest.hexdigest
+  rescue SocketError => e
+    puts "Failure while downloading #{url}: #{e}"
+    raise
+  ensure
+    File.unlink(tmp) if File.exist?(tmp)
+  end # def download
+
+  def untar(tarball, &block)
+    require "archive/tar/minitar"
+    tgz = Zlib::GzipReader.new(File.open(tarball))
+    # Pull out typesdb
+    tar = Archive::Tar::Minitar::Input.open(tgz)
+    tar.each do |entry|
+      path = block.call(entry)
+      next if path.nil?
+      parent = File.dirname(path)
+
+      FileUtils.mkdir_p(parent) unless File.directory?(parent)
+
+      # Skip this file if the output file is the same size
+      if entry.directory?
+        FileUtils.mkdir_p(path) unless File.directory?(path)
+      else
+        entry_mode = entry.instance_eval { @mode } & 0777
+        if File.exists?(path)
+          stat = File.stat(path)
+          # TODO(sissel): Submit a patch to archive-tar-minitar upstream to
+          # expose headers in the entry.
+          entry_size = entry.instance_eval { @size }
+          # If file sizes are same, skip writing.
+          next if stat.size == entry_size && (stat.mode & 0777) == entry_mode
+        end
+        puts "Extracting #{entry.full_name} from #{tarball} #{entry_mode.to_s(8)}"
+        File.open(path, "w") do |fd|
+          # eof? check lets us skip empty files. Necessary because the API provided by
+          # Archive::Tar::Minitar::Reader::EntryStream only mostly acts like an
+          # IO object. Something about empty files in this EntryStream causes
+          # IO.copy_stream to throw "can't convert nil into String" on JRuby
+          # TODO(sissel): File a bug about this.
+          while !entry.eof?
+            chunk = entry.read(16384)
+            fd.write(chunk)
+          end
+            #IO.copy_stream(entry, fd)
+        end
+        File.chmod(entry_mode, path)
+      end
+    end
+    tar.close
+    File.unlink(tarball) if File.file?(tarball)
+  end # def untar
+
+  def do_ungz(file)
+
+    outpath = file.gsub('.gz', '')
+    tgz = Zlib::GzipReader.new(File.open(file))
+    begin
+      File.open(outpath, "w") do |out|
+        IO::copy_stream(tgz, out)
+      end
+      File.unlink(file)
+    rescue
+      File.unlink(outpath) if File.file?(outpath)
+     raise
+    end
+    tgz.close
+  end
+
+  def eval_file(entry, files, prefix)
+    return false if entry.full_name =~ /PaxHeaders/
+    if !files.nil?
+      if files.is_a?(Array)
+        return false unless files.include?(entry.full_name.gsub(prefix, ''))
+        entry.full_name.split("/").last
+      elsif files.is_a?(String)
+        return false unless entry.full_name =~ Regexp.new(files)
+        entry.full_name.split("/").last
+      end
+    else
+      entry.full_name.gsub(prefix, '')
+    end
+  end
+
+  def process_downloads(files,target='')
+
+    FileUtils.mkdir_p(target) unless File.directory?(target)
+
+    files.each do |file|
+      download = file_fetch(file['url'], file['sha1'],target)
+
+      if download =~ /.tar.gz/
+        prefix = download.gsub('.tar.gz', '').gsub("#{target}/", '')
+        untar(download) do |entry|
+          next unless out = eval_file(entry, file['files'], prefix)
+          File.join(target, out)
+        end
+
+      elsif download =~ /.tgz/
+        prefix = download.gsub('.tgz', '').gsub("#{target}/", '')
+        untar(download) do |entry|
+          next unless out = eval_file(entry, file['files'], prefix)
+          File.join(target, out)
+        end
+
+      elsif download =~ /.gz/
+        ungz(download)
+      end
+    end
+  end
+
+end
diff --git a/lib/logstash/util/zeromq.rb b/lib/logstash/util/zeromq.rb
deleted file mode 100644
index 6939807ccc1..00000000000
--- a/lib/logstash/util/zeromq.rb
+++ /dev/null
@@ -1,47 +0,0 @@
-# encoding: utf-8
-require 'ffi-rzmq'
-require "logstash/namespace"
-
-module LogStash::Util::ZeroMQ
-  CONTEXT = ZMQ::Context.new
-  # LOGSTASH-400
-  # see https://github.com/chuckremes/ffi-rzmq/blob/master/lib/ffi-rzmq/socket.rb#L93-117
-  STRING_OPTS = %w{IDENTITY SUBSCRIBE UNSUBSCRIBE}
-
-  def context
-    CONTEXT
-  end
-
-  def setup(socket, address)
-    if server?
-      error_check(socket.bind(address), "binding to #{address}")
-    else
-      error_check(socket.connect(address), "connecting to #{address}")
-    end
-    @logger.info("0mq: #{server? ? 'connected' : 'bound'}", :address => address)
-  end
-
-  def error_check(rc, doing)
-    unless ZMQ::Util.resultcode_ok?(rc)
-      @logger.error("ZeroMQ error while #{doing}", { :error_code => rc })
-      raise "ZeroMQ Error while #{doing}"
-    end
-  end # def error_check
-
-  def setopts(socket, options)
-    options.each do |opt,value|
-      sockopt = opt.split('::')[1]
-      option = ZMQ.const_defined?(sockopt) ? ZMQ.const_get(sockopt) : ZMQ.const_missing(sockopt)
-      unless STRING_OPTS.include?(sockopt)
-        begin
-          Float(value)
-          value = value.to_i
-        rescue ArgumentError
-          raise "#{sockopt} requires a numeric value. #{value} is not numeric"
-        end
-      end # end unless
-      error_check(socket.setsockopt(option, value),
-              "while setting #{opt} == #{value}")
-    end # end each
-  end # end setopts
-end # module LogStash::Util::ZeroMQ
diff --git a/lib/logstash/version.rb b/lib/logstash/version.rb
index 4094d0f02d5..a43f94f115a 100644
--- a/lib/logstash/version.rb
+++ b/lib/logstash/version.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 # The version of logstash.
-LOGSTASH_VERSION = "1.4.1"
+LOGSTASH_VERSION = "1.5.0.beta1"
 
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
diff --git a/locales/en.yml b/locales/en.yml
index 1ab0fdfb447..a5e9028113b 100644
--- a/locales/en.yml
+++ b/locales/en.yml
@@ -4,16 +4,6 @@
 #     for unformatted text.
 en:
   oops: |-
-    +---------------------------------------------------------+
-    | An unexpected error occurred. This is probably a bug.   |
-    | You can find help with this problem in a few places:    |
-    |                                                         |
-    | * chat: #logstash IRC channel on freenode irc.          |
-    |     IRC via the web: http://goo.gl/TI4Ro                |
-    | * email: logstash-users@googlegroups.com                |
-    | * bug system: https://logstash.jira.com/                |
-    |                                                         |
-    +---------------------------------------------------------+
     The error reported is: 
       %{error}
   logstash:
@@ -79,6 +69,12 @@ en:
         file-not-found: |-
           No config files found: %{path}
           Can you make sure this path is a logstash config file?
+        scheme-not-supported: |-
+          URI scheme not supported: %{path}
+          Either pass a local file path or "file|http://" URI
+        fetch-failed: |-
+          Unable to fetch config from: %{path}
+          Reason: %{message}
         setting_missing: |-
           Missing a required setting for the %{plugin} %{type} plugin:
 
@@ -121,19 +117,21 @@ en:
         # them in an 80-character terminal
         config: |+
           Load the logstash config from a specific file
-          or directory.  If a direcory is given, all
-          files in that directory will be concatonated
+          or directory.  If a directory is given, all
+          files in that directory will be concatenated
           in lexicographical order and then parsed as a
           single config file. You can also specify
           wildcards (globs) and any matched files will
           be loaded in the order described above.
         config-string: |+
           Use the given string as the configuration
-          data. Same syntax as the config file. If not
-          input is specified, then 'stdin { type =>
-          stdin }' is the default input. If no output
-          is specified, then 'stdout { debug => true
-          }}' is default output.
+          data. Same syntax as the config file. If no
+          input is specified, then 'stdin 
+          { type => stdin }' is the default input.
+          If no output is specified, then  'stdout
+          { codec => rubydebug } }' is default output.
+        configtest: |+
+          Check configuration, then exit.
         filterworkers: |+
           Sets the number of filter workers to run.
         watchdog-timeout: |+
@@ -161,8 +159,8 @@ en:
           multiple paths. Plugins are expected to be
           in a specific directory hierarchy:
           'PATH/logstash/TYPE/NAME.rb' where TYPE is
-          'input' 'filter' or 'output' and NAME is the
-          name of the plugin.
+          'inputs' 'filters', 'outputs' or 'codecs'
+          and NAME is the name of the plugin.
         quiet: |+
           Quieter logstash logging. This causes only 
           errors to be emitted.
diff --git a/logstash.gemspec b/logstash.gemspec
index 4917d83ed30..5c24b675a5b 100644
--- a/logstash.gemspec
+++ b/logstash.gemspec
@@ -16,71 +16,45 @@ Gem::Specification.new do |gem|
   gem.version       = LOGSTASH_VERSION
 
   # Core dependencies
-  gem.add_runtime_dependency "cabin", [">=0.6.0"]   #(Apache 2.0 license)
-  gem.add_runtime_dependency "json"               #(ruby license)
-  gem.add_runtime_dependency "minitest"           # for running the tests from the jar, (MIT license)
-  gem.add_runtime_dependency "pry"                #(ruby license)
-  gem.add_runtime_dependency "stud"               #(Apache 2.0 license)
-  gem.add_runtime_dependency "clamp"              # for command line args/flags (MIT license)
-  gem.add_runtime_dependency "i18n", [">=0.6.6"]  #(MIT license)
+  gem.add_runtime_dependency "cabin", [">=0.6.0"]    #(Apache 2.0 license)
+  gem.add_runtime_dependency "pry"                   #(Ruby license)
+  gem.add_runtime_dependency "stud"                  #(Apache 2.0 license)
+  gem.add_runtime_dependency "clamp"                 #(MIT license) for command line args/flags
+
+  # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
+  # investigated what the cause might be. -Jordan
+  gem.add_runtime_dependency "treetop", ["~> 1.4.0"] #(MIT license)
+
+  # upgrade i18n only post 0.6.11, see https://github.com/svenfuchs/i18n/issues/270
+  gem.add_runtime_dependency "i18n", ["=0.6.9"]   #(MIT license)
 
   # Web dependencies
-  gem.add_runtime_dependency "ftw", ["~> 0.0.39"] #(Apache 2.0 license)
+  gem.add_runtime_dependency "ftw", ["~> 0.0.40"] #(Apache 2.0 license)
   gem.add_runtime_dependency "mime-types"         #(GPL 2.0)
-  gem.add_runtime_dependency "rack"               # (MIT-style license)
-  gem.add_runtime_dependency "sinatra"            # (MIT-style license)
-
-  # Input/Output/Filter dependencies
-  #TODO Can these be optional?
-  gem.add_runtime_dependency "awesome_print"                    #(MIT license)
-  gem.add_runtime_dependency "aws-sdk"                          #{Apache 2.0 license}
-  gem.add_runtime_dependency "addressable"                      #(Apache 2.0 license)
-  gem.add_runtime_dependency "extlib", ["0.9.16"]               #(MIT license)
-  gem.add_runtime_dependency "ffi"                              #(LGPL-3 license)
-  gem.add_runtime_dependency "ffi-rzmq", ["1.0.0"]              #(MIT license)
-  gem.add_runtime_dependency "filewatch", ["0.5.1"]             #(BSD license)
-  gem.add_runtime_dependency "gelfd", ["0.2.0"]                 #(Apache 2.0 license)
-  gem.add_runtime_dependency "gelf", ["1.3.2"]                  #(MIT license)
-  gem.add_runtime_dependency "gmetric", ["0.1.3"]               #(MIT license)
-  gem.add_runtime_dependency "jls-grok", ["0.10.12"]            #(BSD license)
-  gem.add_runtime_dependency "mail"                             #(MIT license)
-  gem.add_runtime_dependency "metriks"                          #(MIT license)
-  gem.add_runtime_dependency "redis"                            #(MIT license)
-  gem.add_runtime_dependency "statsd-ruby", ["1.2.0"]           #(MIT license)
-  gem.add_runtime_dependency "xml-simple"                       #(ruby license?)
-  gem.add_runtime_dependency "xmpp4r", ["0.5"]                  #(ruby license)
-  gem.add_runtime_dependency "jls-lumberjack", [">=0.0.20"]     #(Apache 2.0 license)
-  gem.add_runtime_dependency "geoip", [">= 1.3.2"]              #(GPL license)
-  gem.add_runtime_dependency "beefcake", "0.3.7"                #(MIT license)
-  gem.add_runtime_dependency "murmurhash3"                      #(MIT license)
-  gem.add_runtime_dependency "rufus-scheduler", "~> 2.0.24"     #(MIT license)
-  gem.add_runtime_dependency "user_agent_parser", [">= 2.0.0"]  #(MIT license)
-  gem.add_runtime_dependency "snmp"                             #(ruby license)
-  gem.add_runtime_dependency "rbnacl"                           #(MIT license)
-  gem.add_runtime_dependency "bindata", [">= 1.5.0"]            #(ruby license)
-  gem.add_runtime_dependency "twitter", "5.0.0.rc.1"            #(MIT license)
-  gem.add_runtime_dependency "edn"                              #(MIT license)
-  gem.add_runtime_dependency "elasticsearch"                    #9Apache 2.0 license)
+  gem.add_runtime_dependency "rack"               #(MIT-style license)
+  gem.add_runtime_dependency "sinatra"            #(MIT-style license)
+
+  # Plugin manager dependencies
+
+  # jar-dependencies 0.1.2 is included in jruby 1.7.6 no need to include here and
+  # this avoids the gemspec jar path parsing issue of jar-dependencies 0.1.2
+  #
+  gem.add_runtime_dependency "jar-dependencies", ["= 0.1.2"]   #(MIT license)
+
+  gem.add_runtime_dependency "ruby-maven"                       #(EPL license)
+  gem.add_runtime_dependency "maven-tools"
+  gem.add_runtime_dependency "minitar"
 
   if RUBY_PLATFORM == 'java'
     gem.platform = RUBY_PLATFORM
-    gem.add_runtime_dependency "jruby-httpclient"                 #(Apache 2.0 license)
-    gem.add_runtime_dependency "bouncy-castle-java", "1.5.0147"   #(MIT license)
-    gem.add_runtime_dependency "jruby-openssl", "0.8.7"           #(CPL/GPL/LGPL license)
-    gem.add_runtime_dependency "msgpack-jruby"                    #(Apache 2.0 license)
-  else
-    gem.add_runtime_dependency "excon"    #(MIT license)
-    gem.add_runtime_dependency "msgpack"  #(Apache 2.0 license)
-  end
 
-  if RUBY_PLATFORM != 'java'
-    gem.add_runtime_dependency "bunny",       ["~> 1.1.8"]  #(MIT license)
+    # bouncy-castle-java 1.5.0147 and jruby-openssl 0.9.5 are included in jruby 1.7.6 no need to include here
+    # and this avoids the gemspec jar path parsing issue of jar-dependencies 0.1.2
+    gem.add_runtime_dependency "jruby-httpclient"                    #(Apache 2.0 license)
+    gem.add_runtime_dependency "jrjackson"                           #(Apache 2.0 license)
   else
-    gem.add_runtime_dependency "march_hare", ["~> 2.1.0"] #(MIT license)
-  end
-
-  if RUBY_VERSION >= '1.9.1'
-    gem.add_runtime_dependency "cinch" # cinch requires 1.9.1+ #(MIT license)
+    gem.add_runtime_dependency "excon"    #(MIT license)
+    gem.add_runtime_dependency "oj"       #(MIT-style license)
   end
 
   if RUBY_ENGINE == "rbx"
@@ -93,17 +67,15 @@ Gem::Specification.new do |gem|
   end
 
   # These are runtime-deps so you can do 'java -jar logstash.jar rspec <test>'
-  gem.add_runtime_dependency "spoon"            #(Apache 2.0 license)
-  gem.add_runtime_dependency "mocha"            #(MIT license)
-  gem.add_runtime_dependency "shoulda"          #(MIT license)
-  gem.add_runtime_dependency "rspec"            #(MIT license)
-  gem.add_runtime_dependency "insist", "1.0.0"  #(Apache 2.0 license)
-  gem.add_runtime_dependency "rumbster"         # For faking smtp in email tests (Apache 2.0 license)
+  gem.add_runtime_dependency "rspec", "~> 2.14.0" #(MIT license)
+  gem.add_runtime_dependency "insist", "1.0.0"    #(Apache 2.0 license)
 
-  # Development Deps
-  gem.add_development_dependency "coveralls"
-  gem.add_development_dependency "kramdown"     # pure-ruby markdown parser (MIT license)
+  gem.add_runtime_dependency "logstash-devutils"
 
   # Jenkins Deps
-  gem.add_runtime_dependency "ci_reporter"
+  gem.add_runtime_dependency "ci_reporter", "1.9.3"
+
+  # Development Deps
+  # coveralls temporarily disabled because of Bundler bug with "without development" and gemspec
+  # gem.add_development_dependency "coveralls"
 end
diff --git a/patterns/.gitkeep b/patterns/.gitkeep
new file mode 100644
index 00000000000..e69de29bb2d
diff --git a/patterns/firewalls b/patterns/firewalls
deleted file mode 100644
index ff7baeae38e..00000000000
--- a/patterns/firewalls
+++ /dev/null
@@ -1,60 +0,0 @@
-# NetScreen firewall logs
-NETSCREENSESSIONLOG %{SYSLOGTIMESTAMP:date} %{IPORHOST:device} %{IPORHOST}: NetScreen device_id=%{WORD:device_id}%{DATA}: start_time=%{QUOTEDSTRING:start_time} duration=%{INT:duration} policy_id=%{INT:policy_id} service=%{DATA:service} proto=%{INT:proto} src zone=%{WORD:src_zone} dst zone=%{WORD:dst_zone} action=%{WORD:action} sent=%{INT:sent} rcvd=%{INT:rcvd} src=%{IPORHOST:src_ip} dst=%{IPORHOST:dst_ip} src_port=%{INT:src_port} dst_port=%{INT:dst_port} src-xlated ip=%{IPORHOST:src_xlated_ip} port=%{INT:src_xlated_port} dst-xlated ip=%{IPORHOST:dst_xlated_ip} port=%{INT:dst_xlated_port} session_id=%{INT:session_id} reason=%{GREEDYDATA:reason}
-
-#== Cisco ASA ==
-CISCO_TAGGED_SYSLOG ^<%{POSINT:syslog_pri}>%{CISCOTIMESTAMP:timestamp}( %{SYSLOGHOST:sysloghost})?: %%{CISCOTAG:ciscotag}:
-CISCOTIMESTAMP %{MONTH} +%{MONTHDAY}(?: %{YEAR})? %{TIME}
-CISCOTAG [A-Z0-9]+-%{INT}-(?:[A-Z0-9_]+)
-# Common Particles
-CISCO_ACTION Built|Teardown|Deny|Denied|denied|requested|permitted|denied by ACL|discarded|est-allowed|Dropping|created|deleted
-CISCO_REASON Duplicate TCP SYN|Failed to locate egress interface|Invalid transport field|No matching connection|DNS Response|DNS Query|(?:%{WORD}\s*)*
-CISCO_DIRECTION Inbound|inbound|Outbound|outbound
-CISCO_INTERVAL first hit|%{INT}-second interval
-CISCO_XLATE_TYPE static|dynamic
-# ASA-2-106001
-CISCOFW106001 %{CISCO_DIRECTION:direction} %{WORD:protocol} connection %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{GREEDYDATA:tcp_flags} on interface %{GREEDYDATA:interface}
-# ASA-2-106006, ASA-2-106007, ASA-2-106010
-CISCOFW106006_106007_106010 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} (?:from|src) %{IP:src_ip}/%{INT:src_port}(\(%{DATA:src_fwuser}\))? (?:to|dst) %{IP:dst_ip}/%{INT:dst_port}(\(%{DATA:dst_fwuser}\))? (?:on interface %{DATA:interface}|due to %{CISCO_REASON:reason})
-# ASA-3-106014
-CISCOFW106014 %{CISCO_ACTION:action} %{CISCO_DIRECTION:direction} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(\(%{DATA:dst_fwuser}\))? \(type %{INT:icmp_type}, code %{INT:icmp_code}\)
-# ASA-6-106015
-CISCOFW106015 %{CISCO_ACTION:action} %{WORD:protocol} \(%{DATA:policy_id}\) from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port} flags %{DATA:tcp_flags}  on interface %{GREEDYDATA:interface}
-# ASA-1-106021
-CISCOFW106021 %{CISCO_ACTION:action} %{WORD:protocol} reverse path check from %{IP:src_ip} to %{IP:dst_ip} on interface %{GREEDYDATA:interface}
-# ASA-4-106023
-CISCOFW106023 %{CISCO_ACTION:action} %{WORD:protocol} src %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? dst %{DATA:dst_interface}:%{IP:dst_ip}(/%{INT:dst_port})?(\(%{DATA:dst_fwuser}\))?( \(type %{INT:icmp_type}, code %{INT:icmp_code}\))? by access-group %{DATA:policy_id} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-5-106100
-CISCOFW106100 access-list %{WORD:policy_id} %{CISCO_ACTION:action} %{WORD:protocol} %{DATA:src_interface}/%{IP:src_ip}\(%{INT:src_port}\)(\(%{DATA:src_fwuser}\))? -> %{DATA:dst_interface}/%{IP:dst_ip}\(%{INT:dst_port}\)(\(%{DATA:src_fwuser}\))? hit-cnt %{INT:hit_count} %{CISCO_INTERVAL:interval} \[%{DATA:hashcode1}, %{DATA:hashcode2}\]
-# ASA-6-110002
-CISCOFW110002 %{CISCO_REASON:reason} for %{WORD:protocol} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-302010
-CISCOFW302010 %{INT:connection_count} in use, %{INT:connection_count_max} most used
-# ASA-6-302013, ASA-6-302014, ASA-6-302015, ASA-6-302016
-CISCOFW302013_302014_302015_302016 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection %{INT:connection_id} for %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port}( \(%{IP:src_mapped_ip}/%{INT:src_mapped_port}\))?(\(%{DATA:src_fwuser}\))? to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}( \(%{IP:dst_mapped_ip}/%{INT:dst_mapped_port}\))?(\(%{DATA:dst_fwuser}\))?( duration %{TIME:duration} bytes %{INT:bytes})?(?: %{CISCO_REASON:reason})?( \(%{DATA:user}\))?
-# ASA-6-302020, ASA-6-302021
-CISCOFW302020_302021 %{CISCO_ACTION:action}(?: %{CISCO_DIRECTION:direction})? %{WORD:protocol} connection for faddr %{IP:dst_ip}/%{INT:icmp_seq_num}(?:\(%{DATA:fwuser}\))? gaddr %{IP:src_xlated_ip}/%{INT:icmp_code_xlated} laddr %{IP:src_ip}/%{INT:icmp_code}( \(%{DATA:user}\))?
-# ASA-6-305011
-CISCOFW305011 %{CISCO_ACTION:action} %{CISCO_XLATE_TYPE:xlate_type} %{WORD:protocol} translation from %{DATA:src_interface}:%{IP:src_ip}(/%{INT:src_port})?(\(%{DATA:src_fwuser}\))? to %{DATA:src_xlated_interface}:%{IP:src_xlated_ip}/%{DATA:src_xlated_port}
-# ASA-3-313001, ASA-3-313004, ASA-3-313008
-CISCOFW313001_313004_313008 %{CISCO_ACTION:action} %{WORD:protocol} type=%{INT:icmp_type}, code=%{INT:icmp_code} from %{IP:src_ip} on interface %{DATA:interface}( to %{IP:dst_ip})?
-# ASA-4-313005
-CISCOFW313005 %{CISCO_REASON:reason} for %{WORD:protocol} error message: %{WORD:err_protocol} src %{DATA:err_src_interface}:%{IP:err_src_ip}(\(%{DATA:err_src_fwuser}\))? dst %{DATA:err_dst_interface}:%{IP:err_dst_ip}(\(%{DATA:err_dst_fwuser}\))? \(type %{INT:err_icmp_type}, code %{INT:err_icmp_code}\) on %{DATA:interface} interface\.  Original IP payload: %{WORD:protocol} src %{IP:orig_src_ip}/%{INT:orig_src_port}(\(%{DATA:orig_src_fwuser}\))? dst %{IP:orig_dst_ip}/%{INT:orig_dst_port}(\(%{DATA:orig_dst_fwuser}\))?
-# ASA-4-402117
-CISCOFW402117 %{WORD:protocol}: Received a non-IPSec packet \(protocol= %{WORD:orig_protocol}\) from %{IP:src_ip} to %{IP:dst_ip}
-# ASA-4-402119
-CISCOFW402119 %{WORD:protocol}: Received an %{WORD:orig_protocol} packet \(SPI= %{DATA:spi}, sequence number= %{DATA:seq_num}\) from %{IP:src_ip} \(user= %{DATA:user}\) to %{IP:dst_ip} that failed anti-replay checking
-# ASA-4-419001
-CISCOFW419001 %{CISCO_ACTION:action} %{WORD:protocol} packet from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}, reason: %{GREEDYDATA:reason}
-# ASA-4-419002
-CISCOFW419002 %{CISCO_REASON:reason} from %{DATA:src_interface}:%{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port} with different initial sequence number
-# ASA-4-500004
-CISCOFW500004 %{CISCO_REASON:reason} for protocol=%{WORD:protocol}, from %{IP:src_ip}/%{INT:src_port} to %{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-602303, ASA-6-602304
-CISCOFW602303_602304 %{WORD:protocol}: An %{CISCO_DIRECTION:direction} %{GREEDYDATA:tunnel_type} SA \(SPI= %{DATA:spi}\) between %{IP:src_ip} and %{IP:dst_ip} \(user= %{DATA:user}\) has been %{CISCO_ACTION:action}
-# ASA-7-710001, ASA-7-710002, ASA-7-710003, ASA-7-710005, ASA-7-710006
-CISCOFW710001_710002_710003_710005_710006 %{WORD:protocol} (?:request|access) %{CISCO_ACTION:action} from %{IP:src_ip}/%{INT:src_port} to %{DATA:dst_interface}:%{IP:dst_ip}/%{INT:dst_port}
-# ASA-6-713172
-CISCOFW713172 Group = %{GREEDYDATA:group}, IP = %{IP:src_ip}, Automatic NAT Detection Status:\s+Remote end\s*%{DATA:is_remote_natted}\s*behind a NAT device\s+This\s+end\s*%{DATA:is_local_natted}\s*behind a NAT device
-# ASA-4-733100
-CISCOFW733100 \[\s*%{DATA:drop_type}\s*\] drop %{DATA:drop_rate_id} exceeded. Current burst rate is %{INT:drop_rate_current_burst} per second, max configured rate is %{INT:drop_rate_max_burst}; Current average rate is %{INT:drop_rate_current_avg} per second, max configured rate is %{INT:drop_rate_max_avg}; Cumulative total count is %{INT:drop_total_count}
-#== End Cisco ASA ==
diff --git a/patterns/grok-patterns b/patterns/grok-patterns
deleted file mode 100755
index 4850b44ebd0..00000000000
--- a/patterns/grok-patterns
+++ /dev/null
@@ -1,94 +0,0 @@
-USERNAME [a-zA-Z0-9._-]+
-USER %{USERNAME}
-INT (?:[+-]?(?:[0-9]+))
-BASE10NUM (?<![0-9.+-])(?>[+-]?(?:(?:[0-9]+(?:\.[0-9]+)?)|(?:\.[0-9]+)))
-NUMBER (?:%{BASE10NUM})
-BASE16NUM (?<![0-9A-Fa-f])(?:[+-]?(?:0x)?(?:[0-9A-Fa-f]+))
-BASE16FLOAT \b(?<![0-9A-Fa-f.])(?:[+-]?(?:0x)?(?:(?:[0-9A-Fa-f]+(?:\.[0-9A-Fa-f]*)?)|(?:\.[0-9A-Fa-f]+)))\b
-
-POSINT \b(?:[1-9][0-9]*)\b
-NONNEGINT \b(?:[0-9]+)\b
-WORD \b\w+\b
-NOTSPACE \S+
-SPACE \s*
-DATA .*?
-GREEDYDATA .*
-QUOTEDSTRING (?>(?<!\\)(?>"(?>\\.|[^\\"]+)+"|""|(?>'(?>\\.|[^\\']+)+')|''|(?>`(?>\\.|[^\\`]+)+`)|``))
-UUID [A-Fa-f0-9]{8}-(?:[A-Fa-f0-9]{4}-){3}[A-Fa-f0-9]{12}
-
-# Networking
-MAC (?:%{CISCOMAC}|%{WINDOWSMAC}|%{COMMONMAC})
-CISCOMAC (?:(?:[A-Fa-f0-9]{4}\.){2}[A-Fa-f0-9]{4})
-WINDOWSMAC (?:(?:[A-Fa-f0-9]{2}-){5}[A-Fa-f0-9]{2})
-COMMONMAC (?:(?:[A-Fa-f0-9]{2}:){5}[A-Fa-f0-9]{2})
-IPV6 ((([0-9A-Fa-f]{1,4}:){7}([0-9A-Fa-f]{1,4}|:))|(([0-9A-Fa-f]{1,4}:){6}(:[0-9A-Fa-f]{1,4}|((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){5}(((:[0-9A-Fa-f]{1,4}){1,2})|:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3})|:))|(([0-9A-Fa-f]{1,4}:){4}(((:[0-9A-Fa-f]{1,4}){1,3})|((:[0-9A-Fa-f]{1,4})?:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){3}(((:[0-9A-Fa-f]{1,4}){1,4})|((:[0-9A-Fa-f]{1,4}){0,2}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){2}(((:[0-9A-Fa-f]{1,4}){1,5})|((:[0-9A-Fa-f]{1,4}){0,3}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(([0-9A-Fa-f]{1,4}:){1}(((:[0-9A-Fa-f]{1,4}){1,6})|((:[0-9A-Fa-f]{1,4}){0,4}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:))|(:(((:[0-9A-Fa-f]{1,4}){1,7})|((:[0-9A-Fa-f]{1,4}){0,5}:((25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)(\.(25[0-5]|2[0-4]\d|1\d\d|[1-9]?\d)){3}))|:)))(%.+)?
-IPV4 (?<![0-9])(?:(?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2})[.](?:25[0-5]|2[0-4][0-9]|[0-1]?[0-9]{1,2}))(?![0-9])
-IP (?:%{IPV6}|%{IPV4})
-HOSTNAME \b(?:[0-9A-Za-z][0-9A-Za-z-]{0,62})(?:\.(?:[0-9A-Za-z][0-9A-Za-z-]{0,62}))*(\.?|\b)
-HOST %{HOSTNAME}
-IPORHOST (?:%{HOSTNAME}|%{IP})
-HOSTPORT %{IPORHOST}:%{POSINT}
-
-# paths
-PATH (?:%{UNIXPATH}|%{WINPATH})
-UNIXPATH (?>/(?>[\w_%!$@:.,-]+|\\.)*)+
-TTY (?:/dev/(pts|tty([pq])?)(\w+)?/?(?:[0-9]+))
-WINPATH (?>[A-Za-z]+:|\\)(?:\\[^\\?*]*)+
-URIPROTO [A-Za-z]+(\+[A-Za-z+]+)?
-URIHOST %{IPORHOST}(?::%{POSINT:port})?
-# uripath comes loosely from RFC1738, but mostly from what Firefox
-# doesn't turn into %XX
-URIPATH (?:/[A-Za-z0-9$.+!*'(){},~:;=@#%_\-]*)+
-#URIPARAM \?(?:[A-Za-z0-9]+(?:=(?:[^&]*))?(?:&(?:[A-Za-z0-9]+(?:=(?:[^&]*))?)?)*)?
-URIPARAM \?[A-Za-z0-9$.+!*'|(){},~@#%&/=:;_?\-\[\]]*
-URIPATHPARAM %{URIPATH}(?:%{URIPARAM})?
-URI %{URIPROTO}://(?:%{USER}(?::[^@]*)?@)?(?:%{URIHOST})?(?:%{URIPATHPARAM})?
-
-# Months: January, Feb, 3, 03, 12, December
-MONTH \b(?:Jan(?:uary)?|Feb(?:ruary)?|Mar(?:ch)?|Apr(?:il)?|May|Jun(?:e)?|Jul(?:y)?|Aug(?:ust)?|Sep(?:tember)?|Oct(?:ober)?|Nov(?:ember)?|Dec(?:ember)?)\b
-MONTHNUM (?:0?[1-9]|1[0-2])
-MONTHNUM2 (?:0[1-9]|1[0-2])
-MONTHDAY (?:(?:0[1-9])|(?:[12][0-9])|(?:3[01])|[1-9])
-
-# Days: Monday, Tue, Thu, etc...
-DAY (?:Mon(?:day)?|Tue(?:sday)?|Wed(?:nesday)?|Thu(?:rsday)?|Fri(?:day)?|Sat(?:urday)?|Sun(?:day)?)
-
-# Years?
-YEAR (?>\d\d){1,2}
-HOUR (?:2[0123]|[01]?[0-9])
-MINUTE (?:[0-5][0-9])
-# '60' is a leap second in most time standards and thus is valid.
-SECOND (?:(?:[0-5]?[0-9]|60)(?:[:.,][0-9]+)?)
-TIME (?!<[0-9])%{HOUR}:%{MINUTE}(?::%{SECOND})(?![0-9])
-# datestamp is YYYY/MM/DD-HH:MM:SS.UUUU (or something like it)
-DATE_US %{MONTHNUM}[/-]%{MONTHDAY}[/-]%{YEAR}
-DATE_EU %{MONTHDAY}[./-]%{MONTHNUM}[./-]%{YEAR}
-ISO8601_TIMEZONE (?:Z|[+-]%{HOUR}(?::?%{MINUTE}))
-ISO8601_SECOND (?:%{SECOND}|60)
-TIMESTAMP_ISO8601 %{YEAR}-%{MONTHNUM}-%{MONTHDAY}[T ]%{HOUR}:?%{MINUTE}(?::?%{SECOND})?%{ISO8601_TIMEZONE}?
-DATE %{DATE_US}|%{DATE_EU}
-DATESTAMP %{DATE}[- ]%{TIME}
-TZ (?:[PMCE][SD]T|UTC)
-DATESTAMP_RFC822 %{DAY} %{MONTH} %{MONTHDAY} %{YEAR} %{TIME} %{TZ}
-DATESTAMP_RFC2822 %{DAY}, %{MONTHDAY} %{MONTH} %{YEAR} %{TIME} %{ISO8601_TIMEZONE}
-DATESTAMP_OTHER %{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{TZ} %{YEAR}
-DATESTAMP_EVENTLOG %{YEAR}%{MONTHNUM2}%{MONTHDAY}%{HOUR}%{MINUTE}%{SECOND}
-
-# Syslog Dates: Month Day HH:MM:SS
-SYSLOGTIMESTAMP %{MONTH} +%{MONTHDAY} %{TIME}
-PROG (?:[\w._/%-]+)
-SYSLOGPROG %{PROG:program}(?:\[%{POSINT:pid}\])?
-SYSLOGHOST %{IPORHOST}
-SYSLOGFACILITY <%{NONNEGINT:facility}.%{NONNEGINT:priority}>
-HTTPDATE %{MONTHDAY}/%{MONTH}/%{YEAR}:%{TIME} %{INT}
-
-# Shortcuts
-QS %{QUOTEDSTRING}
-
-# Log formats
-SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
-COMMONAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|%{DATA:rawrequest})" %{NUMBER:response} (?:%{NUMBER:bytes}|-)
-COMBINEDAPACHELOG %{COMMONAPACHELOG} %{QS:referrer} %{QS:agent}
-
-# Log Levels
-LOGLEVEL ([Aa]lert|ALERT|[Tt]race|TRACE|[Dd]ebug|DEBUG|[Nn]otice|NOTICE|[Ii]nfo|INFO|[Ww]arn?(?:ing)?|WARN?(?:ING)?|[Ee]rr?(?:or)?|ERR?(?:OR)?|[Cc]rit?(?:ical)?|CRIT?(?:ICAL)?|[Ff]atal|FATAL|[Ss]evere|SEVERE|EMERG(?:ENCY)?|[Ee]merg(?:ency)?)
diff --git a/patterns/haproxy b/patterns/haproxy
deleted file mode 100644
index e10fd9706f0..00000000000
--- a/patterns/haproxy
+++ /dev/null
@@ -1,37 +0,0 @@
-## These patterns were tested w/ haproxy-1.4.15
-
-## Documentation of the haproxy log formats can be found at the following links:
-## http://code.google.com/p/haproxy-docs/wiki/HTTPLogFormat
-## http://code.google.com/p/haproxy-docs/wiki/TCPLogFormat
-
-HAPROXYTIME (?!<[0-9])%{HOUR:haproxy_hour}:%{MINUTE:haproxy_minute}(?::%{SECOND:haproxy_second})(?![0-9])
-HAPROXYDATE %{MONTHDAY:haproxy_monthday}/%{MONTH:haproxy_month}/%{YEAR:haproxy_year}:%{HAPROXYTIME:haproxy_time}.%{INT:haproxy_milliseconds}
-
-# Override these default patterns to parse out what is captured in your haproxy.cfg
-HAPROXYCAPTUREDREQUESTHEADERS %{DATA:captured_request_headers}
-HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:captured_response_headers}
-
-# Example:
-#  These haproxy config lines will add data to the logs that are captured
-#  by the patterns below. Place them in your custom patterns directory to 
-#  override the defaults.  
-#
-#  capture request header Host len 40
-#  capture request header X-Forwarded-For len 50
-#  capture request header Accept-Language len 50
-#  capture request header Referer len 200
-#  capture request header User-Agent len 200
-#
-#  capture response header Content-Type len 30
-#  capture response header Content-Encoding len 10
-#  capture response header Cache-Control len 200
-#  capture response header Last-Modified len 200
-# 
-# HAPROXYCAPTUREDREQUESTHEADERS %{DATA:request_header_host}\|%{DATA:request_header_x_forwarded_for}\|%{DATA:request_header_accept_language}\|%{DATA:request_header_referer}\|%{DATA:request_header_user_agent}
-# HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:response_header_content_type}\|%{DATA:response_header_content_encoding}\|%{DATA:response_header_cache_control}\|%{DATA:response_header_last_modified}
-
-# parse a haproxy 'httplog' line 
-HAPROXYHTTP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
-
-# parse a haproxy 'tcplog' line
-HAPROXYTCP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}
diff --git a/patterns/java b/patterns/java
deleted file mode 100644
index 56233e13117..00000000000
--- a/patterns/java
+++ /dev/null
@@ -1,3 +0,0 @@
-JAVACLASS (?:[a-zA-Z0-9-]+\.)+[A-Za-z0-9$_]+
-JAVAFILE (?:[A-Za-z0-9_. -]+)
-JAVASTACKTRACEPART at %{JAVACLASS:class}\.%{WORD:method}\(%{JAVAFILE:file}:%{NUMBER:line}\)
diff --git a/patterns/junos b/patterns/junos
deleted file mode 100644
index bd796961d18..00000000000
--- a/patterns/junos
+++ /dev/null
@@ -1,9 +0,0 @@
-# JUNOS 11.4 RT_FLOW patterns
-RT_FLOW_EVENT (RT_FLOW_SESSION_CREATE|RT_FLOW_SESSION_CLOSE|RT_FLOW_SESSION_DENY)
-
-RT_FLOW1 %{RT_FLOW_EVENT:event}: %{GREEDYDATA:close-reason}: %{IP:src-ip}/%{DATA:src-port}->%{IP:dst-ip}/%{DATA:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{DATA:nat-src-port}->%{IP:nat-dst-ip}/%{DATA:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} \d+\(%{DATA:sent}\) \d+\(%{DATA:received}\) %{INT:elapsed-time} .*
-
-RT_FLOW2 %{RT_FLOW_EVENT:event}: session created %{IP:src-ip}/%{DATA:src-port}->%{IP:dst-ip}/%{DATA:dst-port} %{DATA:service} %{IP:nat-src-ip}/%{DATA:nat-src-port}->%{IP:nat-dst-ip}/%{DATA:nat-dst-port} %{DATA:src-nat-rule-name} %{DATA:dst-nat-rule-name} %{INT:protocol-id} %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} %{INT:session-id} .*
-
-RT_FLOW3 %{RT_FLOW_EVENT:event}: session denied %{IP:src-ip}/%{DATA:src-port}->%{IP:dst-ip}/%{DATA:dst-port} %{DATA:service} %{INT:protocol-id}\(\d\) %{DATA:policy-name} %{DATA:from-zone} %{DATA:to-zone} .*
-
diff --git a/patterns/linux-syslog b/patterns/linux-syslog
deleted file mode 100644
index 81c1f86e192..00000000000
--- a/patterns/linux-syslog
+++ /dev/null
@@ -1,16 +0,0 @@
-SYSLOG5424PRINTASCII [!-~]+
-
-SYSLOGBASE2 (?:%{SYSLOGTIMESTAMP:timestamp}|%{TIMESTAMP_ISO8601:timestamp8601}) (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logsource} %{SYSLOGPROG}:
-SYSLOGPAMSESSION %{SYSLOGBASE} (?=%{GREEDYDATA:message})%{WORD:pam_module}\(%{DATA:pam_caller}\): session %{WORD:pam_session_state} for user %{USERNAME:username}(?: by %{GREEDYDATA:pam_by})?
-
-CRON_ACTION [A-Z ]+
-CRONLOG %{SYSLOGBASE} \(%{USER:user}\) %{CRON_ACTION:action} \(%{DATA:message}\)
-
-SYSLOGLINE %{SYSLOGBASE2} %{GREEDYDATA:message}
-
-# IETF 5424 syslog(8) format (see http://www.rfc-editor.org/info/rfc5424)
-SYSLOG5424PRI <%{NONNEGINT:syslog5424_pri}>
-SYSLOG5424SD \[%{DATA}\]+
-SYSLOG5424BASE %{SYSLOG5424PRI}%{NONNEGINT:syslog5424_ver} +(?:%{TIMESTAMP_ISO8601:syslog5424_ts}|-) +(?:%{HOSTNAME:syslog5424_host}|-) +(-|%{SYSLOG5424PRINTASCII:syslog5424_app}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_proc}) +(-|%{SYSLOG5424PRINTASCII:syslog5424_msgid}) +(?:%{SYSLOG5424SD:syslog5424_sd}|-|)
-
-SYSLOG5424LINE %{SYSLOG5424BASE} +%{GREEDYDATA:syslog5424_msg}
diff --git a/patterns/mcollective b/patterns/mcollective
deleted file mode 100644
index 648b172eeda..00000000000
--- a/patterns/mcollective
+++ /dev/null
@@ -1 +0,0 @@
-MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/patterns/mcollective-patterns b/patterns/mcollective-patterns
deleted file mode 100644
index bb2f7f9bc82..00000000000
--- a/patterns/mcollective-patterns
+++ /dev/null
@@ -1,4 +0,0 @@
-# Remember, these can be multi-line events.
-MCOLLECTIVE ., \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\]%{SPACE}%{LOGLEVEL:event_level}
-
-MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
diff --git a/patterns/mongodb b/patterns/mongodb
deleted file mode 100644
index 47a957355c2..00000000000
--- a/patterns/mongodb
+++ /dev/null
@@ -1,4 +0,0 @@
-MONGO_LOG %{SYSLOGTIMESTAMP:timestamp} \[%{WORD:component}\] %{GREEDYDATA:message}
-MONGO_QUERY \{ (?<={ ).*(?= } ntoreturn:) \}
-MONGO_SLOWQUERY %{WORD} %{MONGO_WORDDASH:database}\.%{MONGO_WORDDASH:collection} %{WORD}: %{MONGO_QUERY:query} %{WORD}:%{NONNEGINT:ntoreturn} %{WORD}:%{NONNEGINT:ntoskip} %{WORD}:%{NONNEGINT:nscanned}.*nreturned:%{NONNEGINT:nreturned}..+ (?<duration>[0-9]+)ms
-MONGO_WORDDASH \b[\w-]+\b
diff --git a/patterns/nagios b/patterns/nagios
deleted file mode 100644
index 9d3fa7b5fa8..00000000000
--- a/patterns/nagios
+++ /dev/null
@@ -1,108 +0,0 @@
-##################################################################################
-##################################################################################
-# Chop Nagios log files to smithereens!
-#
-# A set of GROK filters to process logfiles generated by Nagios.
-# While it does not, this set intends to cover all possible Nagios logs.
-#
-# Some more work needs to be done to cover all External Commands:
-#	http://old.nagios.org/developerinfo/externalcommands/commandlist.php
-#
-# If you need some support on these rules please contact:
-#	Jelle Smet http://smetj.net
-#
-#################################################################################
-#################################################################################
-
-NAGIOSTIME \[%{NUMBER:nagios_epoch}\]
-
-###############################################
-######## Begin nagios log types
-###############################################
-NAGIOS_TYPE_CURRENT_SERVICE_STATE CURRENT SERVICE STATE
-NAGIOS_TYPE_CURRENT_HOST_STATE CURRENT HOST STATE
-
-NAGIOS_TYPE_SERVICE_NOTIFICATION SERVICE NOTIFICATION
-NAGIOS_TYPE_HOST_NOTIFICATION HOST NOTIFICATION
-
-NAGIOS_TYPE_SERVICE_ALERT SERVICE ALERT
-NAGIOS_TYPE_HOST_ALERT HOST ALERT
-
-NAGIOS_TYPE_SERVICE_FLAPPING_ALERT SERVICE FLAPPING ALERT
-NAGIOS_TYPE_HOST_FLAPPING_ALERT HOST FLAPPING ALERT
-
-NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT SERVICE DOWNTIME ALERT
-NAGIOS_TYPE_HOST_DOWNTIME_ALERT HOST DOWNTIME ALERT
-
-NAGIOS_TYPE_PASSIVE_SERVICE_CHECK PASSIVE SERVICE CHECK
-NAGIOS_TYPE_PASSIVE_HOST_CHECK PASSIVE HOST CHECK
-
-NAGIOS_TYPE_SERVICE_EVENT_HANDLER SERVICE EVENT HANDLER
-NAGIOS_TYPE_HOST_EVENT_HANDLER HOST EVENT HANDLER
-
-NAGIOS_TYPE_EXTERNAL_COMMAND EXTERNAL COMMAND
-NAGIOS_TYPE_TIMEPERIOD_TRANSITION TIMEPERIOD TRANSITION
-###############################################
-######## End nagios log types
-###############################################
-
-###############################################
-######## Begin external check types
-###############################################
-NAGIOS_EC_DISABLE_SVC_CHECK DISABLE_SVC_CHECK
-NAGIOS_EC_ENABLE_SVC_CHECK ENABLE_SVC_CHECK
-NAGIOS_EC_DISABLE_HOST_CHECK DISABLE_HOST_CHECK
-NAGIOS_EC_ENABLE_HOST_CHECK ENABLE_HOST_CHECK
-NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT PROCESS_SERVICE_CHECK_RESULT
-NAGIOS_EC_PROCESS_HOST_CHECK_RESULT PROCESS_HOST_CHECK_RESULT
-NAGIOS_EC_SCHEDULE_SERVICE_DOWNTIME SCHEDULE_SERVICE_DOWNTIME
-NAGIOS_EC_SCHEDULE_HOST_DOWNTIME SCHEDULE_HOST_DOWNTIME
-###############################################
-######## End external check types
-###############################################
-NAGIOS_WARNING Warning:%{SPACE}%{GREEDYDATA:nagios_message}
-
-NAGIOS_CURRENT_SERVICE_STATE %{NAGIOS_TYPE_CURRENT_SERVICE_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-NAGIOS_CURRENT_HOST_STATE %{NAGIOS_TYPE_CURRENT_HOST_STATE:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statetype};%{DATA:nagios_statecode};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_NOTIFICATION %{NAGIOS_TYPE_SERVICE_NOTIFICATION:nagios_type}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_NOTIFICATION %{NAGIOS_TYPE_HOST_NOTIFICATION}: %{DATA:nagios_notifyname};%{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_contact};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_ALERT %{NAGIOS_TYPE_SERVICE_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_ALERT %{NAGIOS_TYPE_HOST_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{NUMBER:nagios_attempt};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_FLAPPING_ALERT %{NAGIOS_TYPE_SERVICE_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-NAGIOS_HOST_FLAPPING_ALERT %{NAGIOS_TYPE_HOST_FLAPPING_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_message}
-
-NAGIOS_SERVICE_DOWNTIME_ALERT %{NAGIOS_TYPE_SERVICE_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_HOST_DOWNTIME_ALERT %{NAGIOS_TYPE_HOST_DOWNTIME_ALERT:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_PASSIVE_SERVICE_CHECK %{NAGIOS_TYPE_PASSIVE_SERVICE_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-NAGIOS_PASSIVE_HOST_CHECK %{NAGIOS_TYPE_PASSIVE_HOST_CHECK:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_comment}
-
-NAGIOS_SERVICE_EVENT_HANDLER %{NAGIOS_TYPE_SERVICE_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-NAGIOS_HOST_EVENT_HANDLER %{NAGIOS_TYPE_HOST_EVENT_HANDLER:nagios_type}: %{DATA:nagios_hostname};%{DATA:nagios_state};%{DATA:nagios_statelevel};%{DATA:nagios_event_handler_name}
-
-NAGIOS_TIMEPERIOD_TRANSITION %{NAGIOS_TYPE_TIMEPERIOD_TRANSITION:nagios_type}: %{DATA:nagios_service};%{DATA:nagios_unknown1};%{DATA:nagios_unknown2};
-
-####################
-#### External checks
-####################
-
-#Disable host & service check
-NAGIOS_EC_LINE_DISABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_DISABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_DISABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Enable host & service check
-NAGIOS_EC_LINE_ENABLE_SVC_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_SVC_CHECK:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service}
-NAGIOS_EC_LINE_ENABLE_HOST_CHECK %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_ENABLE_HOST_CHECK:nagios_command};%{DATA:nagios_hostname}
-
-#Process host & service check
-NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_SERVICE_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_service};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_PROCESS_HOST_CHECK_RESULT:nagios_command};%{DATA:nagios_hostname};%{DATA:nagios_state};%{GREEDYDATA:nagios_check_result}
-
-#Schedule host & service downtime
-NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME %{NAGIOS_TYPE_EXTERNAL_COMMAND:nagios_type}: %{NAGIOS_EC_SCHEDULE_HOST_DOWNTIME:nagios_command};%{DATA:nagios_hostname};%{NUMBER:nagios_start_time};%{NUMBER:nagios_end_time};%{NUMBER:nagios_fixed};%{NUMBER:nagios_trigger_id};%{NUMBER:nagios_duration};%{DATA:author};%{DATA:comment}
-
-#End matching line
-NAGIOSLOGLINE %{NAGIOSTIME} (?:%{NAGIOS_WARNING}|%{NAGIOS_CURRENT_SERVICE_STATE}|%{NAGIOS_CURRENT_HOST_STATE}|%{NAGIOS_SERVICE_NOTIFICATION}|%{NAGIOS_HOST_NOTIFICATION}|%{NAGIOS_SERVICE_ALERT}|%{NAGIOS_HOST_ALERT}|%{NAGIOS_SERVICE_FLAPPING_ALERT}|%{NAGIOS_HOST_FLAPPING_ALERT}|%{NAGIOS_SERVICE_DOWNTIME_ALERT}|%{NAGIOS_HOST_DOWNTIME_ALERT}|%{NAGIOS_PASSIVE_SERVICE_CHECK}|%{NAGIOS_PASSIVE_HOST_CHECK}|%{NAGIOS_SERVICE_EVENT_HANDLER}|%{NAGIOS_HOST_EVENT_HANDLER}|%{NAGIOS_TIMEPERIOD_TRANSITION}|%{NAGIOS_EC_LINE_DISABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_ENABLE_SVC_CHECK}|%{NAGIOS_EC_LINE_DISABLE_HOST_CHECK|%{NAGIOS_EC_LINE_ENABLE_HOST_CHECK}|%{NAGIOS_EC_LINE_PROCESS_HOST_CHECK_RESULT}|%{NAGIOS_EC_LINE_PROCESS_SERVICE_CHECK_RESULT}|%{NAGIOS_EC_LINE_SCHEDULE_HOST_DOWNTIME})
diff --git a/patterns/postgresql b/patterns/postgresql
deleted file mode 100644
index c5b3e90b725..00000000000
--- a/patterns/postgresql
+++ /dev/null
@@ -1,3 +0,0 @@
-# Default postgresql pg_log format pattern
-POSTGRESQL %{DATESTAMP:timestamp} %{TZ} %{DATA:user_id} %{GREEDYDATA:connection_id} %{POSINT:pid}
-
diff --git a/patterns/redis b/patterns/redis
deleted file mode 100644
index 8655c4f043e..00000000000
--- a/patterns/redis
+++ /dev/null
@@ -1,3 +0,0 @@
-REDISTIMESTAMP %{MONTHDAY} %{MONTH} %{TIME}
-REDISLOG \[%{POSINT:pid}\] %{REDISTIMESTAMP:timestamp} \* 
-
diff --git a/patterns/ruby b/patterns/ruby
deleted file mode 100644
index b1729cddcb0..00000000000
--- a/patterns/ruby
+++ /dev/null
@@ -1,2 +0,0 @@
-RUBY_LOGLEVEL (?:DEBUG|FATAL|ERROR|WARN|INFO)
-RUBY_LOGGER [DFEWI], \[%{TIMESTAMP_ISO8601:timestamp} #%{POSINT:pid}\] *%{RUBY_LOGLEVEL:loglevel} -- +%{DATA:progname}: %{GREEDYDATA:message}
diff --git a/pkg/logrotate.conf b/pkg/logrotate.conf
index 191a3a5630b..69977aeecc8 100644
--- a/pkg/logrotate.conf
+++ b/pkg/logrotate.conf
@@ -3,6 +3,7 @@
         rotate 7
         copytruncate
         compress
+        delaycompress
         missingok
         notifempty
 }
diff --git a/pkg/logstash-web.upstart.ubuntu b/pkg/logstash-web.upstart.ubuntu
index 028c4814c4c..465369fcae4 100644
--- a/pkg/logstash-web.upstart.ubuntu
+++ b/pkg/logstash-web.upstart.ubuntu
@@ -36,7 +36,6 @@ script
 
   HOME="${HOME:-$LS_HOME}"
   JAVA_OPTS="${LS_JAVA_OPTS}"
-  [ -n "${LS_LOG_FILE}" ] && LS_OPTS="${LSOPTS} -l ${LS_LOG_FILE}"
   # Reset filehandle limit
   ulimit -n ${LS_OPEN_FILES}
   cd "${LS_HOME}"
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
index bb7555e1ab1..fddc14d5ed9 100755
--- a/pkg/logstash.sysv
+++ b/pkg/logstash.sysv
@@ -48,6 +48,7 @@ start() {
 
 
   JAVA_OPTS=${LS_JAVA_OPTS}
+  HOME=${LS_HOME}
   export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
 
   # set ulimit as (root, presumably) first, before we drop privileges
diff --git a/pull_release_note.rb b/pull_release_note.rb
deleted file mode 100644
index 6ccdba92e5f..00000000000
--- a/pull_release_note.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-require "octokit"
-
-
-@repository= "logstash/logstash"
-@releaseNote= "releaseNote.html"
-
-#Last release  == last tag
-lastReleaseSha = Octokit.tags(@repository).first.commit.sha
-
-currentReleaseSha ="HEAD"
-
-#Collect PR Merge in a file
-File.open(@releaseNote, "a") do |f|
-  f.puts "<h2>Merged pull request</h2>"
-  f.puts "<ul>"
-  Octokit.compare(@repository, lastReleaseSha, currentReleaseSha).commits.each do |commit|
-    if commit.commit.message.start_with?("Merge pull")
-      scan_re = Regexp.new(/^Merge pull request #(\d+) from ([^\/]+)\/.*\n\n(.*)/)
-      commit.commit.message.scan(scan_re) do |pullNumber, user, summary|
-        f.puts "<li><a href='https://github.com/logstash/logstash/pull/#{pullNumber}'>Pull ##{pullNumber}<a> by #{user}: #{summary}</li>"
-      end
-    end
-  end
-  f.puts "</ul>"
-end
\ No newline at end of file
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
new file mode 100644
index 00000000000..48cc19ab8f1
--- /dev/null
+++ b/rakelib/artifacts.rake
@@ -0,0 +1,223 @@
+def staging
+  "build/staging"
+end
+
+namespace "artifact" do
+  require "logstash/environment"
+  def package_files
+    [
+      "LICENSE",
+      "CHANGELOG",
+      "CONTRIBUTORS",
+      "{bin,lib,spec,locales}/{,**/*}",
+      "patterns/**/*",
+      "vendor/??*/**/*"
+    ]
+  end
+
+  def exclude_paths
+    return @exclude_paths if @exclude_paths
+    @exclude_paths = []
+    #gitignore = File.join(File.dirname(__FILE__), "..", ".gitignore")
+    #if File.exists?(gitignore)
+      #@exclude_paths += File.read(gitignore).split("\n")
+    #end
+    @exclude_paths << "spec/reports/**/*"
+    @exclude_paths << "**/*.gem"
+    @exclude_paths << "**/test/files/slow-xpath.xml"
+    @exclude_paths << "**/logstash-*/spec"
+    return @exclude_paths
+  end
+
+  def excludes
+    return @excludes if @excludes
+    @excludes = exclude_paths.collect { |g| Rake::FileList[g] }.flatten
+  end
+
+  def exclude?(path)
+    excludes.any? { |ex| path == ex || (File.directory?(ex) && path =~ /^#{ex}\//) }
+  end
+
+  def files
+    return @files if @files
+    @files = package_files.collect do |glob|
+      Rake::FileList[glob].reject { |path| exclude?(path) }
+    end.flatten.uniq
+  end
+
+  desc "Build a tar.gz of logstash with all dependencies"
+  task "tar" => ["bootstrap", "plugin:install-defaults"] do
+    require "zlib"
+    require "archive/tar/minitar"
+    require "logstash/version"
+    tarpath = "build/logstash-#{LOGSTASH_VERSION}.tar.gz"
+    gz = Zlib::GzipWriter.new(File.new(tarpath, "wb"), Zlib::BEST_COMPRESSION)
+    tar = Archive::Tar::Minitar::Output.new(gz)
+    files.each do |path|
+      stat = File.lstat(path)
+      path_in_tar = "logstash-#{LOGSTASH_VERSION}/#{path}"
+      opts = {
+        :size => stat.size,
+        :mode => stat.mode,
+        :mtime => stat.mtime
+      }
+      if stat.directory?
+        tar.tar.mkdir(path_in_tar, opts)
+      else
+        tar.tar.add_file_simple(path_in_tar, opts) do |io|
+          File.open(path,'rb') do |fd|
+            chunk = nil
+            size = 0
+            size += io.write(chunk) while chunk = fd.read(16384)
+            if stat.size != size
+              raise "Failure to write the entire file (#{path}) to the tarball. Expected to write #{stat.size} bytes; actually write #{size}"
+            end
+          end
+        end
+      end
+    end
+    tar.close
+    gz.close
+    puts "Complete: #{tarpath}"
+  end
+
+  task "zip" => ["bootstrap", "plugin:install-defaults"] do
+    Rake::Task["dependency:rubyzip"].invoke
+    require 'zip'
+    zippath = "build/logstash-#{LOGSTASH_VERSION}.zip"
+    Zip::File.open(zippath, Zip::File::CREATE) do |zipfile|
+      files.each do |path|
+        path_in_zip = "logstash-#{LOGSTASH_VERSION}/#{path}"
+        zipfile.add(path_in_zip, path)
+      end
+    end
+    puts "Complete: #{zippath}"
+  end
+
+  def package(platform, version)
+    Rake::Task["dependency:fpm"].invoke
+    Rake::Task["dependency:stud"].invoke
+    require "stud/temporary"
+    require "fpm/errors" # TODO(sissel): fix this in fpm
+    require "fpm/package/dir"
+    require "fpm/package/gem" # TODO(sissel): fix this in fpm; rpm needs it.
+
+    dir = FPM::Package::Dir.new
+
+    files.each do |path|
+      next if File.directory?(path)
+      dir.input("#{path}=/opt/logstash/#{path}")
+    end
+
+    basedir = File.join(File.dirname(__FILE__), "..")
+
+    File.join(basedir, "pkg", "logrotate.conf").tap do |path|
+      dir.input("#{path}=/etc/logrotate.d/logstash")
+    end
+
+    # Create an empty /var/log/logstash/ directory in the package
+    # This is a bit obtuse, I suppose, but it is necessary until
+    # we find a better way to do this with fpm.
+    Stud::Temporary.directory do |empty|
+      dir.input("#{empty}/=/var/log/logstash")
+      dir.input("#{empty}/=/var/lib/logstash")
+      dir.input("#{empty}/=/etc/logstash/conf.d")
+    end
+
+    case platform
+      when "redhat", "centos"
+        File.join(basedir, "pkg", "logrotate.conf").tap do |path|
+          dir.input("#{path}=/etc/logrotate.d/logstash")
+        end
+        File.join(basedir, "pkg", "logstash.default").tap do |path|
+          dir.input("#{path}=/etc/sysconfig/logstash")
+        end
+        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
+          dir.input("#{path}=/etc/init.d/logstash")
+        end
+        require "fpm/package/rpm"
+        out = dir.convert(FPM::Package::RPM)
+        out.license = "ASL 2.0" # Red Hat calls 'Apache Software License' == ASL
+        out.attributes[:rpm_use_file_permissions] = true
+        out.attributes[:rpm_user] = "root"
+        out.attributes[:rpm_group] = "root"
+        out.config_files << "etc/sysconfig/logstash"
+        out.config_files << "etc/logrotate.d/logstash"
+        out.config_files << "/etc/init.d/logstash"
+      when "debian", "ubuntu"
+        File.join(basedir, "pkg", "logstash.default").tap do |path|
+          dir.input("#{path}=/etc/default/logstash")
+        end
+        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
+          dir.input("#{path}=/etc/init.d/logstash")
+        end
+        require "fpm/package/deb"
+        out = dir.convert(FPM::Package::Deb)
+        out.license = "Apache 2.0"
+        out.attributes[:deb_user] = "root"
+        out.attributes[:deb_group] = "root"
+        out.attributes[:deb_suggests] = "java7-runtime-headless"
+        out.config_files << "/etc/default/logstash"
+        out.config_files << "/etc/logrotate.d/logstash"
+        out.config_files << "/etc/init.d/logstash"
+    end
+
+    # Packaging install/removal scripts
+    ["before", "after"].each do |stage|
+      ["install", "remove"].each do |action|
+        script = "#{stage}-#{action}" # like, "before-install"
+        script_sym = script.gsub("-", "_").to_sym
+        script_path = File.join(File.dirname(__FILE__), "..", "pkg", platform, "#{script}.sh")
+        next unless File.exists?(script_path)
+
+        out.scripts[script_sym] = File.read(script_path)
+      end
+    end
+
+    # TODO(sissel): Invoke Pleaserun to generate the init scripts/whatever
+
+    out.name = "logstash"
+    out.version = LOGSTASH_VERSION
+    out.architecture = "all"
+    # TODO(sissel): Include the git commit hash?
+    out.iteration = "1" # what revision?
+    out.url = "http://www.elasticsearch.org/overview/logstash/"
+    out.description = "An extensible logging pipeline"
+    out.vendor = "Elasticsearch"
+    out.dependencies << "logrotate"
+
+    # We don't specify a dependency on Java because:
+    # - On Red Hat, Oracle and Red Hat both label their java packages in
+    #   incompatible ways. Further, there is no way to guarantee a qualified
+    #   version is available to install.
+    # - On Debian and Ubuntu, there is no Oracle package and specifying a
+    #   correct version of OpenJDK is impossible because there is no guarantee that
+    #   is impossible for the same reasons as the Red Hat section above.
+    # References:
+    # - http://www.elasticsearch.org/blog/java-1-7u55-safe-use-elasticsearch-lucene/
+    # - deb: https://github.com/elasticsearch/logstash/pull/1008
+    # - rpm: https://github.com/elasticsearch/logstash/pull/1290
+    # - rpm: https://github.com/elasticsearch/logstash/issues/1673
+    # - rpm: https://logstash.jira.com/browse/LOGSTASH-1020
+
+    out.attributes[:force?] = true # overwrite the rpm/deb/etc being created
+    begin
+      path = File.join(basedir, "build", out.to_s)
+      x = out.output(path)
+      puts "Completed: #{path}"
+    ensure
+      out.cleanup
+    end
+  end # def package
+
+  desc "Build an RPM of logstash with all dependencies"
+  task "rpm" => ["bootstrap", "plugin:install-defaults"] do
+    package("centos", "5")
+  end
+
+  desc "Build an RPM of logstash with all dependencies"
+  task "deb" => ["bootstrap", "plugin:install-defaults"] do
+    package("ubuntu", "12.04")
+  end
+end
+
diff --git a/rakelib/bootstrap.rake b/rakelib/bootstrap.rake
new file mode 100644
index 00000000000..941ac26319a
--- /dev/null
+++ b/rakelib/bootstrap.rake
@@ -0,0 +1,5 @@
+
+
+task "bootstrap" => [ "vendor:all", "compile:all" ]
+
+task "bootstrap:test" => [ "vendor:test", "compile:all" ]
diff --git a/rakelib/build.rake b/rakelib/build.rake
new file mode 100644
index 00000000000..2b443add8b7
--- /dev/null
+++ b/rakelib/build.rake
@@ -0,0 +1,6 @@
+directory "build" do |task, args|
+  mkdir_p task.name unless File.directory?(task.name)
+end
+directory "build/bootstrap" => "build" do |task, args|
+  mkdir_p task.name unless File.directory?(task.name)
+end
diff --git a/rakelib/bundler_patch.rb b/rakelib/bundler_patch.rb
new file mode 100644
index 00000000000..b6c1bc0d16b
--- /dev/null
+++ b/rakelib/bundler_patch.rb
@@ -0,0 +1,20 @@
+module Bundler
+
+  # Patch bundler to write a .lock file specific to the version of ruby.
+  # This keeps MRI/JRuby/RBX from conflicting over the Gemfile.lock updates
+  module SharedHelpers
+    def default_lockfile
+      ruby = "#{LogStash::Environment.ruby_engine}-#{LogStash::Environment.ruby_abi_version}"
+      Pathname.new("#{default_gemfile}.#{ruby}.lock")
+    end
+  end
+
+  # Add the Bundler.reset! method which has been added in master but is not in 1.7.9.
+  class << self
+    unless self.method_defined?("reset!")
+      def reset!
+        @definition = nil
+      end
+    end
+  end
+end
diff --git a/rakelib/compile.rake b/rakelib/compile.rake
new file mode 100644
index 00000000000..df572de21bc
--- /dev/null
+++ b/rakelib/compile.rake
@@ -0,0 +1,15 @@
+
+rule ".rb" => ".treetop" do |task, args|
+  require "treetop"
+  compiler = Treetop::Compiler::GrammarCompiler.new
+  compiler.compile(task.source, task.name)
+  puts "Compiling #{task.source}"
+end
+
+namespace "compile" do
+  desc "Compile the config grammar"
+  task "grammar" => "lib/logstash/config/grammar.rb"
+
+  desc "Build everything"
+  task "all" => "grammar"
+end
diff --git a/rakelib/copy.rake b/rakelib/copy.rake
new file mode 100644
index 00000000000..f40747cc049
--- /dev/null
+++ b/rakelib/copy.rake
@@ -0,0 +1,4 @@
+
+def staging
+  "build/staging"
+end
diff --git a/rakelib/default_plugins.rb b/rakelib/default_plugins.rb
new file mode 100644
index 00000000000..4cd79a0444a
--- /dev/null
+++ b/rakelib/default_plugins.rb
@@ -0,0 +1,100 @@
+::DEFAULT_PLUGINS = %w(
+logstash-output-zeromq
+logstash-codec-collectd
+logstash-output-xmpp
+logstash-codec-dots
+logstash-codec-edn
+logstash-codec-edn_lines
+logstash-codec-fluent
+logstash-codec-graphite
+logstash-codec-json
+logstash-codec-json_lines
+logstash-codec-line
+logstash-codec-msgpack
+logstash-codec-multiline
+logstash-codec-netflow
+logstash-codec-oldlogstashjson
+logstash-codec-plain
+logstash-codec-rubydebug
+logstash-output-udp
+logstash-filter-anonymize
+logstash-filter-checksum
+logstash-output-tcp
+logstash-output-stdout
+logstash-filter-clone
+logstash-output-statsd
+logstash-filter-csv
+logstash-filter-date
+logstash-filter-dns
+logstash-filter-drop
+logstash-output-sqs
+logstash-output-sns
+logstash-output-s3
+logstash-output-redis
+logstash-filter-fingerprint
+logstash-filter-geoip
+logstash-filter-grok
+logstash-output-rabbitmq
+logstash-output-cloudwatch
+logstash-output-pipe
+logstash-filter-kv
+logstash-filter-metrics
+logstash-filter-multiline
+logstash-filter-mutate
+logstash-output-pagerduty
+logstash-output-opentsdb
+logstash-output-null
+logstash-filter-ruby
+logstash-filter-sleep
+logstash-filter-split
+logstash-filter-syslog_pri
+logstash-filter-throttle
+logstash-output-nagios_nsca
+logstash-filter-urldecode
+logstash-filter-useragent
+logstash-filter-uuid
+logstash-filter-xml
+logstash-output-nagios
+logstash-input-elasticsearch
+logstash-input-eventlog
+logstash-input-exec
+logstash-input-file
+logstash-input-ganglia
+logstash-input-gelf
+logstash-output-lumberjack
+logstash-input-generator
+logstash-input-graphite
+logstash-output-kafka
+logstash-input-imap
+logstash-input-irc
+logstash-output-juggernaut
+logstash-input-kafka
+logstash-input-log4j
+logstash-input-lumberjack
+logstash-input-pipe
+logstash-output-irc
+logstash-input-rabbitmq
+logstash-output-http
+logstash-input-redis
+logstash-output-hipchat
+logstash-input-s3
+logstash-input-snmptrap
+logstash-output-graphite
+logstash-input-sqs
+logstash-input-stdin
+logstash-output-gelf
+logstash-input-syslog
+logstash-input-tcp
+logstash-input-twitter
+logstash-input-udp
+logstash-input-unix
+logstash-output-ganglia
+logstash-output-file
+logstash-output-exec
+logstash-input-xmpp
+logstash-output-email
+logstash-input-zeromq
+logstash-output-elasticsearch
+logstash-output-csv
+logstash-filter-json
+)
diff --git a/rakelib/dependency.rake b/rakelib/dependency.rake
new file mode 100644
index 00000000000..4431ef71e98
--- /dev/null
+++ b/rakelib/dependency.rake
@@ -0,0 +1,33 @@
+
+namespace "dependency" do
+  task "bundler" do
+    Rake::Task["gem:require"].invoke("bundler", ">= 1.3.5", LogStash::Environment.logstash_gem_home)
+    require "bundler/cli"
+    require_relative "bundler_patch"
+  end
+
+  task "rbx-stdlib" do
+    Rake::Task["gem:require"].invoke("rubysl", ">= 0", LogStash::Environment.logstash_gem_home)
+  end # task rbx-stdlib
+
+  task "archive-tar-minitar" do
+    Rake::Task["gem:require"].invoke("minitar", ">= 0", LogStash::Environment.logstash_gem_home)
+  end # task archive-minitar
+
+  task "stud" do
+    Rake::Task["gem:require"].invoke("stud", ">= 0", LogStash::Environment.logstash_gem_home)
+  end # task stud
+
+  task "fpm" do
+    Rake::Task["gem:require"].invoke("fpm", ">= 0", LogStash::Environment.logstash_gem_home)
+  end # task stud
+
+  task "rubyzip" do
+    Rake::Task["gem:require"].invoke("rubyzip", ">= 0", LogStash::Environment.logstash_gem_home)
+  end # task stud
+
+  task "octokit" do
+    Rake::Task["gem:require"].invoke("octokit", ">= 0", LogStash::Environment.logstash_gem_home)
+  end # task octokit
+
+end # namespace dependency
diff --git a/rakelib/docs.rake b/rakelib/docs.rake
new file mode 100644
index 00000000000..49f145c2745
--- /dev/null
+++ b/rakelib/docs.rake
@@ -0,0 +1,26 @@
+require 'logstash/environment'
+
+namespace "docs" do
+
+  task "generate" do
+    Rake::Task['dependency:octokit'].invoke
+    Rake::Task['plugin:install-all'].invoke
+    Rake::Task['docs:generate-docs'].invoke
+    Rake::Task['docs:generate-index'].invoke
+  end
+
+  task "generate-docs" do
+    list = Dir.glob("#{LogStash::Environment.logstash_gem_home}/gems/logstash-*/lib/logstash/{input,output,filter,codec}s/*.rb").join(" ")
+    cmd = "bin/logstash docgen -o asciidoc_generated #{list}"
+    system(cmd)
+  end
+
+  task "generate-index" do
+    list = [ 'inputs', 'outputs', 'filters', 'codecs' ]
+    list.each do |type|
+      cmd = "ruby docs/asciidoc_index.rb asciidoc_generated #{type}"
+      system(cmd)
+    end
+  end
+
+end
diff --git a/rakelib/fetch.rake b/rakelib/fetch.rake
new file mode 100644
index 00000000000..f871cdef4c9
--- /dev/null
+++ b/rakelib/fetch.rake
@@ -0,0 +1,82 @@
+require "net/http"
+require "uri"
+require "digest/sha1"
+
+directory "vendor/_" => ["vendor"] do |task, args|
+  mkdir task.name
+end
+
+def fetch(url, sha1, output)
+  puts "Downloading #{url}"
+  actual_sha1 = download(url, output)
+
+  if actual_sha1 != sha1
+    fail "SHA1 does not match (expected '#{sha1}' but got '#{actual_sha1}')"
+  end
+end # def fetch
+
+def file_fetch(url, sha1)
+  filename = File.basename(URI(url).path)
+  output = "vendor/_/#{filename}"
+  task output => [ "vendor/_" ] do
+    begin
+      actual_sha1 = file_sha1(output)
+      if actual_sha1 != sha1
+        fetch(url, sha1, output)
+      end
+    rescue Errno::ENOENT
+      fetch(url, sha1, output)
+    end
+  end.invoke
+
+  return output
+end
+
+def file_sha1(path)
+  digest = Digest::SHA1.new
+  fd = File.new(path, "rb")
+  while true
+    begin
+      digest << fd.sysread(16384)
+    rescue EOFError
+      break
+    end
+  end
+  return digest.hexdigest
+ensure
+  fd.close if fd
+end
+
+def download(url, output)
+  uri = URI(url)
+  digest = Digest::SHA1.new
+  tmp = "#{output}.tmp"
+  Net::HTTP.start(uri.host, uri.port, :use_ssl => (uri.scheme == "https")) do |http|
+    request = Net::HTTP::Get.new(uri.path)
+    http.request(request) do |response|
+      fail "HTTP fetch failed for #{url}. #{response}" if response.code != "200"
+      size = (response["content-length"].to_i || -1).to_f
+      count = 0
+      File.open(tmp, "wb") do |fd|
+        response.read_body do |chunk|
+          fd.write(chunk)
+          digest << chunk
+          if size > 0 && $stdout.tty?
+            count += chunk.bytesize
+            $stdout.write(sprintf("\r%0.2f%%", count/size * 100))
+          end
+        end
+      end
+      $stdout.write("\r      \r") if $stdout.tty?
+    end
+  end
+
+  File.rename(tmp, output)
+
+  return digest.hexdigest
+rescue SocketError => e
+  puts "Failure while downloading #{url}: #{e}"
+  raise
+ensure
+  File.unlink(tmp) if File.exist?(tmp)
+end # def download
diff --git a/rakelib/gems.rake b/rakelib/gems.rake
new file mode 100644
index 00000000000..532c7e9fc8f
--- /dev/null
+++ b/rakelib/gems.rake
@@ -0,0 +1,52 @@
+require "rubygems/specification"
+require "rubygems/commands/install_command"
+require "logstash/JRUBY-PR1448" if RUBY_PLATFORM == "java" && Gem.win_platform?
+
+namespace "gem" do
+  task "require",  :name, :requirement, :target do |task, args|
+    name, requirement, target = args[:name], args[:requirement], args[:target]
+
+    ENV["GEM_HOME"] = ENV["GEM_PATH"] = LogStash::Environment.logstash_gem_home
+    Gem.use_paths(LogStash::Environment.logstash_gem_home)
+
+    begin
+      gem name, requirement
+    rescue Gem::LoadError => e
+      puts "Installing #{name} #{requirement} because the build process needs it."
+      Rake::Task["gem:install"].invoke(name, requirement, target)
+    end
+    task.reenable # Allow this task to be run again
+  end
+
+  task "install", [:name, :requirement, :target] =>  ["build/bootstrap"] do |task, args|
+    name, requirement, target = args[:name], args[:requirement], args[:target]
+
+    ENV["GEM_HOME"] = ENV["GEM_PATH"] = target
+    Gem.use_paths(target)
+
+    puts "[bootstrap] Fetching and installing gem: #{name} (#{requirement})"
+
+    installer = Gem::Commands::InstallCommand.new
+    installer.options[:generate_rdoc] = false
+    installer.options[:generate_ri] = false
+    installer.options[:version] = requirement
+    installer.options[:args] = [name]
+    installer.options[:install_dir] = target
+
+    # ruby 2.0.0 / rubygems 2.x; disable documentation generation
+    installer.options[:document] = []
+    begin
+      installer.execute
+    rescue Gem::LoadError => e
+    # For some weird reason the rescue from the 'require' task is being brought down here
+    # We don't know why placing this solves it, but it does.
+    rescue Gem::SystemExitException => e
+      if e.exit_code != 0
+        puts "Installation of #{name} failed"
+        raise
+      end
+    end
+
+    task.reenable # Allow this task to be run again
+  end # task "install"
+end # namespace "gem"
diff --git a/rakelib/plugin.rake b/rakelib/plugin.rake
new file mode 100644
index 00000000000..494b4c200a0
--- /dev/null
+++ b/rakelib/plugin.rake
@@ -0,0 +1,29 @@
+require_relative "default_plugins"
+
+namespace "plugin" do
+
+  task "install",  :name do |task, args|
+    name = args[:name]
+    puts "[plugin] Installing plugin: #{name}"
+
+    cmd = ['bin/logstash', 'plugin', 'install', name ]
+    system(*cmd)
+    raise RuntimeError, $!.to_s unless $?.success?
+
+    task.reenable # Allow this task to be run again
+  end # task "install"
+
+  task "install-defaults" do
+    Rake::Task["vendor:bundle"].invoke("tools/Gemfile.plugins")
+  end
+
+  task "install-test" do
+    Rake::Task["vendor:bundle"].invoke("tools/Gemfile.plugins.test")
+  end
+
+  task "install-all" do
+    Rake::Task["vendor:bundle"].invoke("tools/Gemfile.plugins.all")
+  end
+
+
+end # namespace "plugin"
diff --git a/rakelib/test.rake b/rakelib/test.rake
new file mode 100644
index 00000000000..b276d6c4d92
--- /dev/null
+++ b/rakelib/test.rake
@@ -0,0 +1,26 @@
+
+namespace "test" do
+  task "default" => [ "bootstrap:test", "test:prep" ] do
+    Gem.clear_paths
+    require "logstash/environment"
+    LogStash::Environment.set_gem_paths!
+    require 'rspec/core'
+    RSpec::Core::Runner.run(Rake::FileList["spec/**/*.rb"])
+  end
+
+  task "fail-fast" => [ "bootstrap:test", "test:prep" ] do
+    Gem.clear_paths
+    require "logstash/environment"
+    LogStash::Environment.set_gem_paths!
+    require 'rspec/core'
+    RSpec::Core::Runner.run(["--fail-fast", *Rake::FileList["spec/**/*.rb"]])
+  end
+
+  task "prep" do
+    Rake::Task["vendor:gems"].invoke(false)
+    Rake::Task["plugin:install-test"].invoke
+  end
+
+end
+
+task "test" => [ "test:default" ]
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
new file mode 100644
index 00000000000..7e53e2d677c
--- /dev/null
+++ b/rakelib/vendor.rake
@@ -0,0 +1,156 @@
+DOWNLOADS = {
+  "jruby" => { "version" => "1.7.17", "sha1" => "e4621bbcc51242061eaa9b62caee69c2a2b433f0" },
+  "kibana" => { "version" => "3.1.2", "sha1" => "a59ea4abb018a7ed22b3bc1c3bcc6944b7009dc4" },
+}
+
+def vendor(*args)
+  return File.join("vendor", *args)
+end
+
+# Untar any files from the given tarball file name.
+#
+# A tar entry is passed to the block. The block should should return
+# * nil to skip this file
+# * or, the desired string filename to write the file to.
+def untar(tarball, &block)
+  Rake::Task["dependency:archive-tar-minitar"].invoke
+  require "archive/tar/minitar"
+  tgz = Zlib::GzipReader.new(File.open(tarball,"rb"))
+  tar = Archive::Tar::Minitar::Input.open(tgz)
+  tar.each do |entry|
+    path = block.call(entry)
+    next if path.nil?
+    parent = File.dirname(path)
+
+    mkdir_p parent unless File.directory?(parent)
+
+    # Skip this file if the output file is the same size
+    if entry.directory?
+      mkdir path unless File.directory?(path)
+    else
+      entry_mode = entry.instance_eval { @mode } & 0777
+      if File.exists?(path)
+        stat = File.stat(path)
+        # TODO(sissel): Submit a patch to archive-tar-minitar upstream to
+        # expose headers in the entry.
+        entry_size = entry.instance_eval { @size }
+        # If file sizes are same, skip writing.
+        if Gem.win_platform?
+          #Do not fight with windows permission scheme
+          next if stat.size == entry_size
+        else
+          next if stat.size == entry_size && (stat.mode & 0777) == entry_mode
+        end
+      end
+      puts "Extracting #{entry.full_name} from #{tarball} #{entry_mode.to_s(8)}"
+      File.open(path, "wb") do |fd|
+        # eof? check lets us skip empty files. Necessary because the API provided by
+        # Archive::Tar::Minitar::Reader::EntryStream only mostly acts like an
+        # IO object. Something about empty files in this EntryStream causes
+        # IO.copy_stream to throw "can't convert nil into String" on JRuby
+        # TODO(sissel): File a bug about this.
+        while !entry.eof?
+          chunk = entry.read(16384)
+          fd.write(chunk)
+        end
+          #IO.copy_stream(entry, fd)
+      end
+      File.chmod(entry_mode, path)
+    end
+  end
+  tar.close
+end # def untar
+
+namespace "vendor" do
+  task "jruby" do |task, args|
+    name = task.name.split(":")[1]
+    info = DOWNLOADS[name]
+    version = info["version"]
+
+    discard_patterns = Regexp.union([ /^samples/,
+                                      /@LongLink/,
+                                      /lib\/ruby\/1.8/,
+                                      /lib\/ruby\/2.0/,
+                                      /lib\/ruby\/shared\/rdoc/])
+
+    url = "http://jruby.org.s3.amazonaws.com/downloads/#{version}/jruby-bin-#{version}.tar.gz"
+    download = file_fetch(url, info["sha1"])
+
+    parent = vendor(name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      next if parent =~ discard_patterns
+      mkdir parent
+    end.invoke unless Rake::Task.task_defined?(parent)
+
+    prefix_re = /^#{Regexp.quote("jruby-#{version}/")}/
+    untar(download) do |entry|
+      out = entry.full_name.gsub(prefix_re, "")
+      next if out =~ discard_patterns
+      vendor(name, out)
+    end # untar
+  end # jruby
+  task "all" => "jruby"
+  task "test" => "jruby"
+
+  task "kibana" do |task, args|
+    name = task.name.split(":")[1]
+    info = DOWNLOADS[name]
+    version = info["version"]
+    url = "https://download.elasticsearch.org/kibana/kibana/kibana-#{version}.tar.gz"
+    download = file_fetch(url, info["sha1"])
+
+    parent = vendor(name).gsub(/\/$/, "")
+    directory parent => "vendor" do
+      mkdir parent
+    end.invoke unless Rake::Task.task_defined?(parent)
+
+    prefix_re = /^#{Regexp.quote("kibana-#{version}/")}/
+    untar(download) do |entry|
+      vendor(name, entry.full_name.gsub(prefix_re, ""))
+    end # untar
+  end # task kibana
+  task "all" => "kibana"
+  task "test" => "kibana"
+
+  namespace "force" do
+    task "gems" => ["vendor:gems"]
+  end
+
+  task "gems", [:bundle] do |task, args|
+    require "logstash/environment"
+    Rake::Task["dependency:rbx-stdlib"] if LogStash::Environment.ruby_engine == "rbx"
+    Rake::Task["dependency:stud"].invoke
+    Rake::Task["vendor:bundle"].invoke("tools/Gemfile") if args.to_hash.empty? || args[:bundle]
+  end # task gems
+  task "all" => "gems"
+
+  task "bundle", [:gemfile] => [ "dependency:bundler" ] do |task, args|
+    task.reenable
+    # because --path creates a .bundle/config file and changes bundler path
+    # we need to remove this file so it doesn't influence following bundler calls
+    FileUtils.rm_rf(::File.join(LogStash::Environment::LOGSTASH_HOME, "tools/.bundle"))
+    10.times do
+      begin
+        ENV["GEM_PATH"] = LogStash::Environment.logstash_gem_home
+        ENV["BUNDLE_PATH"] = LogStash::Environment.logstash_gem_home
+        ENV["BUNDLE_GEMFILE"] = args[:gemfile]
+        Bundler.reset!
+        Bundler::CLI.start(LogStash::Environment.bundler_install_command(args[:gemfile], LogStash::Environment::BUNDLE_DIR))
+        break
+      rescue => e
+        # for now catch all, looks like bundler now throws Bundler::InstallError, Errno::EBADF
+        puts(e.message)
+        puts("--> Retrying vendor:gems upon exception=#{e.class}")
+        sleep(1)
+      end
+    end
+    # because --path creates a .bundle/config file and changes bundler path
+    # we need to remove this file so it doesn't influence following bundler calls
+    FileUtils.rm_rf(::File.join(LogStash::Environment::LOGSTASH_HOME, "tools/.bundle"))
+  end
+
+  desc "Clean the vendored files"
+  task :clean do
+    rm_rf vendor
+  end
+end
diff --git a/rakelib/z_rubycheck.rake b/rakelib/z_rubycheck.rake
new file mode 100644
index 00000000000..3b7a9f0cf47
--- /dev/null
+++ b/rakelib/z_rubycheck.rake
@@ -0,0 +1,12 @@
+if ENV['USE_RUBY'] != '1'
+  if RUBY_ENGINE != "jruby" or Gem.ruby !~ /vendor\/jruby\/bin\/jruby/
+    puts "Restarting myself under Vendored JRuby (currently #{RUBY_ENGINE} #{RUBY_VERSION})"  if $DEBUG
+
+    # Make sure we have JRuby, then rerun ourselves under jruby.
+    Rake::Task["vendor:jruby"].invoke
+
+    jruby = File.join("vendor", "jruby", "bin", "jruby")
+    rake = File.join("vendor", "jruby", "bin", "rake")
+    exec(jruby, "-S", rake, *ARGV)
+  end
+end
diff --git a/spec/codecs/collectd.rb b/spec/codecs/collectd.rb
deleted file mode 100644
index 0233e4427b1..00000000000
--- a/spec/codecs/collectd.rb
+++ /dev/null
@@ -1,119 +0,0 @@
-require "logstash/codecs/collectd"
-require "logstash/event"
-require "insist"
-require "tempfile"
-
-describe LogStash::Codecs::Collectd do
-  context "None" do
-    subject do
-      next LogStash::Codecs::Collectd.new({})
-    end
-
-    it "should parse a normal packet" do
-      payload = ["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"].pack('H*')
-
-      counter = 0
-      subject.decode(payload) do |event|
-        case counter
-        when 0
-          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
-          insist { event['plugin'] } == "interface"
-          insist { event['plugin_instance'] } == "wlan0"
-          insist { event['collectd_type'] } == "if_errors"
-          insist { event['rx'] } == 0
-          insist { event['tx'] } == 0
-        when 2
-          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
-          insist { event['plugin'] } == "entropy"
-          insist { event['collectd_type'] } == "entropy"
-          insist { event['value'] } == 157.0
-        end
-        counter += 1
-      end
-      insist { counter } == 28
-    end # it "should parse a normal packet"
-
-    it "should drop a part with an header length" do
-      payload = ["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"].pack('H*')
-      counter = 0
-      subject.decode(payload) do |event|
-        case counter
-        when 0
-          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
-          insist { event['plugin'] } == "interface"
-          insist { event['plugin_instance'] } == "wlan0"
-          insist { event['collectd_type'] } == "if_errors"
-          insist { event['rx'] } == 0
-          insist { event['tx'] } == 0
-        when 2
-          insist { event['host'] } == "lieters-klaptop.prot.plexis.eu"
-          insist { event['plugin'] } == "entropy"
-          insist { event['collectd_type'] } == "entropy"
-          insist { event['value'] } == 157.0
-        end
-        counter += 1
-      end
-      # One of these will fail because I altered the payload from the normal packet
-      insist { counter } == 27
-    end # it "should drop a part with an header length"
-  end # context "None"
-
-  # Create an authfile for the next tests
-  authfile = Tempfile.new('logstash-collectd-authfile')
-  File.open(authfile.path, "a") do |fd|
-    fd.puts("pieter: aapje1234")
-  end
-  context "Sign" do
-    subject do
-      next LogStash::Codecs::Collectd.new({"authfile" => authfile.path,
-                                           "security_level" => "Sign"})
-    end
-
-    it "should parse a correctly signed packet" do
-      payload = ["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"].pack('H*')
-      counter = 0
-      subject.decode(payload) do |event|
-        counter += 1
-      end
-
-      insist { counter } == 24
-    end # it "should parse a correctly signed packet"
-
-    it "should not parse an incorrectly signed packet" do
-      payload = ["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"].pack('H*')
-      counter = 0
-      subject.decode(payload) do |event|
-        counter += 1
-      end
-
-      insist { counter } == 0
-    end # it "should not parse and incorrectly signed packet"
-  end # context "Sign"
-
-  context "Encrypt" do
-    subject do
-      next LogStash::Codecs::Collectd.new({"authfile" => authfile.path,
-                                           "security_level" => "Encrypt"})
-    end
-
-    it "should parse an encrypted packet", :export_cypher => true do
-      payload = ["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"].pack('H*')
-      counter = 0
-      subject.decode(payload) do |event|
-        counter += 1
-      end
-
-      insist { counter } == 24
-    end # it "should parse an encrypted packet"
-
-    it "should not parse unencrypted packets when encrypt is configured" do
-      payload = ["000000236c6965746572732d6b6c6170746f702e70726f742e706c657869732e6575000008000c14b0a645f3eb73c30009000c00000002800000000002000e696e74657266616365000003000a776c616e30000004000e69665f6572726f7273000006001800020202000000000000000000000000000000000008000c14b0a645f3eb525e000300076c6f000004000f69665f7061636b6574730000060018000202020000000000001cd80000000000001cd80008000c14b0a645f3ebf8c10002000c656e74726f70790000030005000004000c656e74726f7079000006000f0001010000000000a063400008000c14b0a645f3eb6c700002000e696e74657266616365000003000a776c616e30000004000f69665f7061636b657473000006001800020202000000000002d233000000000001c3b10008000c14b0a645f3eb59b1000300076c6f000004000e69665f6572726f7273000006001800020202000000000000000000000000000000000008000c14b0a645f425380b00020009737761700000030005000004000973776170000005000975736564000006000f00010100000000000000000008000c14b0a645f4254c8d0005000966726565000006000f00010100000000fcffdf410008000c14b0a645f4255ae70005000b636163686564000006000f00010100000000000000000008000c14b0a645f426f09f0004000c737761705f696f0000050007696e000006000f00010200000000000000000008000c14b0a645f42701e7000500086f7574000006000f00010200000000000000000008000c14b0a645f42a0edf0002000a7573657273000004000a75736572730000050005000006000f00010100000000000022400008000c14b0a645f5967c8b0002000e70726f636573736573000004000d70735f7374617465000005000c72756e6e696e67000006000f00010100000000000000000008000c14b0a645f624706c0005000d736c656570696e67000006000f0001010000000000c067400008000c14b0a645f624861a0005000c7a6f6d62696573000006000f00010100000000000000000008000c14b0a645f62494740005000c73746f70706564000006000f00010100000000000010400008000c14b0a645f6254aa90005000b706167696e67000006000f00010100000000000000000008000c14b0a645f6255b110005000c626c6f636b6564000006000f00010100000000000000000008000c14b0a645f62763060004000e666f726b5f726174650000050005000006000f00010200000000000025390008000c14b0a64873bf8f47000200086370750000030006300000040008637075000005000975736572000006000f0001020000000000023caa0008000c14b0a64873bfc9dd000500096e696365000006000f00010200000000000000030008000c14b0a64873bfe9350005000b73797374656d000006000f00010200000000000078bc0008000c14b0a64873c004290005000969646c65000006000f00010200000000000941fe0008000c14b0a64873c020920005000977616974000006000f00010200000000000002050008000c14b0a64873c03e280005000e696e74657272757074000006000f00010200000000000000140008000c14b0a64873c04ba20005000c736f6674697271000006000f00010200000000000001890008000c14b0a64873c058860005000a737465616c000006000f00010200000000000000000008000c14b0a64873c071b80003000631000005000975736572000006000f000102000000000002440e0008000c14b0a64873c07f31000500096e696365000006000f0001020000000000000007"].pack('H*')
-      counter = 0
-      subject.decode(payload) do |event|
-        counter += 1
-      end
-
-      insist { counter } == 0
-    end # it "should not parse unencrypted packets when encrypt is configured"
-  end # context "Encrypt"
-end # describe LogStash::Codecs::Collectd
diff --git a/spec/codecs/edn.rb b/spec/codecs/edn.rb
deleted file mode 100644
index e04cc659572..00000000000
--- a/spec/codecs/edn.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-require "logstash/codecs/edn"
-require "logstash/event"
-require "insist"
-require "edn"
-
-describe LogStash::Codecs::EDN do
-  subject do
-    next LogStash::Codecs::EDN.new
-  end
-
-  context "#decode" do
-    it "should return an event from edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}}
-      subject.decode(data.to_edn) do |event|
-        insist { event }.is_a?(LogStash::Event)
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["bah"] } == data["bah"]
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      event = LogStash::Event.new(data)
-      got_event = false
-      subject.on_event do |d|
-        insist { d.chomp } == LogStash::Event.new(data).to_hash.to_edn
-        insist { EDN.read(d)["foo"] } == data["foo"]
-        insist { EDN.read(d)["baz"] } == data["baz"]
-        insist { EDN.read(d)["bah"] } == data["bah"]
-        got_event = true
-      end
-      subject.encode(event)
-      insist { got_event }
-    end
-  end
-
-end
diff --git a/spec/codecs/edn_lines.rb b/spec/codecs/edn_lines.rb
deleted file mode 100644
index e5c1b711e19..00000000000
--- a/spec/codecs/edn_lines.rb
+++ /dev/null
@@ -1,53 +0,0 @@
-require "logstash/codecs/edn_lines"
-require "logstash/event"
-require "insist"
-require "edn"
-
-describe LogStash::Codecs::EDNLines do
-  subject do
-    next LogStash::Codecs::EDNLines.new
-  end
-
-  context "#decode" do
-    it "should return an event from edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a", "b", "c"]}}
-      subject.decode(data.to_edn + "\n") do |event|
-        insist { event }.is_a?(LogStash::Event)
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["bah"] } == data["bah"]
-      end
-    end
-
-    it "should return an event from edn data when a newline is recieved" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_edn) do |event|
-        insist {false}
-      end
-      subject.decode("\n") do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["bah"] } == data["bah"]
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return edn data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      event = LogStash::Event.new(data)
-      got_event = false
-      subject.on_event do |d|
-        insist { d.chomp } == LogStash::Event.new(data).to_hash.to_edn
-        insist { EDN.read(d)["foo"] } == data["foo"]
-        insist { EDN.read(d)["baz"] } == data["baz"]
-        insist { EDN.read(d)["bah"] } == data["bah"]
-        got_event = true
-      end
-      subject.encode(event)
-      insist { got_event }
-    end
-  end
-
-end
diff --git a/spec/codecs/graphite.rb b/spec/codecs/graphite.rb
deleted file mode 100644
index e5be954f149..00000000000
--- a/spec/codecs/graphite.rb
+++ /dev/null
@@ -1,96 +0,0 @@
-require "logstash/codecs/graphite"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::Graphite do
-  subject do
-    next LogStash::Codecs::Graphite.new
-  end
-
-  context "#decode" do
-    it "should return an event from single full graphite line" do
-      name = Random.srand.to_s(36)
-      value = Random.rand*1000
-      timestamp = Time.now.gmtime.to_i
-      subject.decode("#{name} #{value} #{timestamp}\n") do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event[name] } == value
-      end
-    end
-    
-    it "should return multiple events given multiple graphite formated lines" do
-      total_count = Random.rand(20)
-      names = Array.new(total_count) { Random.srand.to_s(36) }
-      values = Array.new(total_count) { Random.rand*1000 }
-      timestamps = Array.new(total_count) { Time.now.gmtime.to_i }
-      data = Array.new(total_count) {|i| "#{names[i]} #{values[i]} #{timestamps[i]}\n"}
-      counter = 0
-      subject.decode(data.join('')) do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event[names[counter]] } == values[counter]
-        counter = counter+1
-      end
-      insist { counter } == total_count
-    end
-    
-    it "should not return an event until newline is hit" do
-      name = Random.srand.to_s(36)
-      value = Random.rand*1000
-      timestamp = Time.now.gmtime.to_i
-      event_returned = false
-      subject.decode("#{name} #{value} #{timestamp}") do |event|
-        event_returned = true
-      end
-      insist { !event_returned }
-      subject.decode("\n") do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event[name] } == value
-        event_returned = true
-      end
-      insist { event_returned }
-    end
-  end
-  
-  context "#encode" do
-    it "should emit an graphite formatted line" do
-      name = Random.srand.to_s(36)
-      value = Random.rand*1000
-      timestamp = Time.now.gmtime
-      subject.metrics = {name => value}
-      subject.on_event do |event|
-        insist { event.is_a? String }
-        insist { event } == "#{name} #{value} #{timestamp.to_i}\n"
-      end
-      subject.encode(LogStash::Event.new("@timestamp" => timestamp))
-    end
-    
-    it "should treat fields as metrics if fields as metrics flag is set" do
-      name = Random.srand.to_s(36)
-      value = Random.rand*1000
-      timestamp = Time.now.gmtime.to_i
-      subject.fields_are_metrics = true
-      subject.on_event do |event|
-        insist { event.is_a? String }
-        insist { event } == "#{name} #{value} #{timestamp.to_i}\n"
-      end
-      subject.encode(LogStash::Event.new({name => value, "@timestamp" => timestamp}))
-      
-      #even if metrics param is set
-      subject.metrics = {"foo" => 4}
-      subject.encode(LogStash::Event.new({name => value, "@timestamp" => timestamp}))
-    end
-    
-    it "should change the metric name format when metrics_format is set" do
-      name = Random.srand.to_s(36)
-      value = Random.rand*1000
-      timestamp = Time.now.gmtime
-      subject.metrics = {name => value}
-      subject.metrics_format = "foo.bar.*.baz"
-      subject.on_event do |event|
-        insist { event.is_a? String }
-        insist { event } == "foo.bar.#{name}.baz #{value} #{timestamp.to_i}\n"
-      end
-      subject.encode(LogStash::Event.new("@timestamp" => timestamp))
-    end
-  end
-end
diff --git a/spec/codecs/json.rb b/spec/codecs/json.rb
deleted file mode 100644
index 11d879f3570..00000000000
--- a/spec/codecs/json.rb
+++ /dev/null
@@ -1,82 +0,0 @@
-require "logstash/codecs/json"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::JSON do
-  subject do
-    next LogStash::Codecs::JSON.new
-  end
-
-  context "#decode" do
-    it "should return an event from json data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_json) do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["bah"] } == data["bah"]
-      end
-    end
-
-    it "should be fast", :performance => true do
-      json = '{"message":"Hello world!","@timestamp":"2013-12-21T07:01:25.616Z","@version":"1","host":"Macintosh.local","sequence":1572456}'
-      iterations = 500000
-      count = 0
-
-      # Warmup
-      10000.times { subject.decode(json) { } }
-
-      start = Time.now
-      iterations.times do
-        subject.decode(json) do |event|
-          count += 1
-        end
-      end
-      duration = Time.now - start
-      insist { count } == iterations
-      puts "codecs/json rate: #{"%02.0f/sec" % (iterations / duration)}, elapsed: #{duration}s"
-    end
-
-    context "processing plain text" do
-      it "falls back to plain text" do
-        decoded = false
-        subject.decode("something that isn't json") do |event|
-          decoded = true
-          insist { event.is_a?(LogStash::Event) }
-          insist { event["message"] } == "something that isn't json"
-        end
-        insist { decoded } == true
-      end
-    end
-
-    context "processing weird binary blobs" do
-      it "falls back to plain text and doesn't crash (LOGSTASH-1595)" do
-        decoded = false
-        blob = (128..255).to_a.pack("C*").force_encoding("ASCII-8BIT")
-        subject.decode(blob) do |event|
-          decoded = true
-          insist { event.is_a?(LogStash::Event) }
-          insist { event["message"].encoding.to_s } == "UTF-8"
-        end
-        insist { decoded } == true
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return json data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      event = LogStash::Event.new(data)
-      got_event = false
-      subject.on_event do |d|
-        insist { d.chomp } == LogStash::Event.new(data).to_json
-        insist { JSON.parse(d)["foo"] } == data["foo"]
-        insist { JSON.parse(d)["baz"] } == data["baz"]
-        insist { JSON.parse(d)["bah"] } == data["bah"]
-        got_event = true
-      end
-      subject.encode(event)
-      insist { got_event }
-    end
-  end
-end
diff --git a/spec/codecs/json_lines.rb b/spec/codecs/json_lines.rb
deleted file mode 100644
index 40cdcba52a5..00000000000
--- a/spec/codecs/json_lines.rb
+++ /dev/null
@@ -1,77 +0,0 @@
-require "logstash/codecs/json_lines"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::JSONLines do
-  subject do
-    next LogStash::Codecs::JSONLines.new
-  end
-
-  context "#decode" do
-    it "should return an event from json data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_json+"\n") do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["bah"] } == data["bah"]
-      end
-    end
-    
-    it "should return an event from json data when a newline is recieved" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_json) do |event|
-        insist {false}
-      end
-      subject.decode("\n") do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["bah"] } == data["bah"]
-      end
-    end
-
-    context "processing plain text" do
-      it "falls back to plain text" do
-        decoded = false
-        subject.decode("something that isn't json\n") do |event|
-          decoded = true
-          insist { event.is_a?(LogStash::Event) }
-          insist { event["message"] } == "something that isn't json"
-        end
-        insist { decoded } == true
-      end
-    end
-
-    context "processing weird binary blobs" do
-      it "falls back to plain text and doesn't crash (LOGSTASH-1595)" do
-        decoded = false
-        blob = (128..255).to_a.pack("C*").force_encoding("ASCII-8BIT")
-        subject.decode(blob)
-        subject.decode("\n") do |event|
-          decoded = true
-          insist { event.is_a?(LogStash::Event) }
-          insist { event["message"].encoding.to_s } == "UTF-8"
-        end
-        insist { decoded } == true
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return json data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      event = LogStash::Event.new(data)
-      got_event = false
-      subject.on_event do |d|
-        insist { d.chomp } == LogStash::Event.new(data).to_json
-        insist { JSON.parse(d)["foo"] } == data["foo"]
-        insist { JSON.parse(d)["baz"] } == data["baz"]
-        insist { JSON.parse(d)["bah"] } == data["bah"]
-        got_event = true
-      end
-      subject.encode(event)
-      insist { got_event }
-    end
-  end
-end
diff --git a/spec/codecs/json_spooler.rb b/spec/codecs/json_spooler.rb
deleted file mode 100644
index 7cb78da0b86..00000000000
--- a/spec/codecs/json_spooler.rb
+++ /dev/null
@@ -1,43 +0,0 @@
-require "logstash/codecs/json_spooler"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::JsonSpooler do
-  # subject do
-  #   next LogStash::Codecs::JsonSpooler.new
-  # end
-
-  # context "#decode" do
-  #   it "should return an event from spooled json data" do
-  #     data = {"a" => 1}
-  #     events = [LogStash::Event.new(data), LogStash::Event.new(data),
-  #       LogStash::Event.new(data)]
-  #     subject.decode(events.to_json) do |event|
-  #       insist { event.is_a? LogStash::Event }
-  #       insist { event["a"] } == data["a"]
-  #     end
-  #   end
-  # end
-
-  # context "#encode" do
-  #   it "should return spooled json data" do
-  #     data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-  #     subject.spool_size = 3
-  #     got_event = false
-  #     subject.on_event do |d|
-  #       events = JSON.parse(d)
-  #       insist { events.is_a? Array }
-  #       insist { events[0].is_a? LogStash::Event }
-  #       insist { events[0]["foo"] } == data["foo"]
-  #       insist { events[0]["baz"] } == data["baz"]
-  #       insist { events[0]["bah"] } == data["bah"]
-  #       insist { events.length } == 3
-  #       got_event = true
-  #     end
-  #     3.times do
-  #       subject.encode(LogStash::Event.new(data))
-  #     end
-  #     insist { got_event }
-  #   end
-  # end
-end
diff --git a/spec/codecs/msgpack.rb b/spec/codecs/msgpack.rb
deleted file mode 100644
index fc36c1bab5e..00000000000
--- a/spec/codecs/msgpack.rb
+++ /dev/null
@@ -1,39 +0,0 @@
-require "logstash/codecs/msgpack"
-require "logstash/event"
-require "insist"
-
-# Skip msgpack for now since Hash#to_msgpack seems to not be a valid method?
-describe LogStash::Codecs::Msgpack, :if => false  do
-  subject do
-    next LogStash::Codecs::Msgpack.new
-  end
-
-  context "#decode" do
-    it "should return an event from msgpack data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      subject.decode(data.to_msgpack) do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["bah"] } == data["bah"]
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return msgpack data" do
-      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}}
-      event = LogStash::Event.new(data)
-      got_event = false
-      subject.on_event do |d|
-        insist { d } == LogStash::Event.new(data).to_hash.to_msgpack
-        insist { MessagePack.unpack(d)["foo"] } == data["foo"]
-        insist { MessagePack.unpack(d)["baz"] } == data["baz"]
-        insist { MessagePack.unpack(d)["bah"] } == data["bah"]
-        got_event = true
-      end
-      subject.encode(event)
-      insist { got_event }
-    end
-  end
-end
diff --git a/spec/codecs/multiline.rb b/spec/codecs/multiline.rb
deleted file mode 100644
index 2c75317f21c..00000000000
--- a/spec/codecs/multiline.rb
+++ /dev/null
@@ -1,160 +0,0 @@
-# encoding: utf-8
-
-require "logstash/codecs/multiline"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::Multiline do
-  context "#decode" do
-    it "should be able to handle multiline events with additional lines space-indented" do
-      codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous")
-      lines = [ "hello world", "   second line", "another first line" ]
-      events = []
-      lines.each do |line|
-        codec.decode(line) do |event|
-          events << event
-        end
-      end
-      codec.flush { |e| events << e }
-      insist { events.size } == 2
-      insist { events[0]["message"] } == "hello world\n   second line"
-      insist { events[0]["tags"] }.include?("multiline")
-      insist { events[1]["message"] } == "another first line"
-      insist { events[1]["tags"] }.nil?
-    end
-
-    it "should allow custom tag added to multiline events" do
-      codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous", "multiline_tag" => "hurray" )
-      lines = [ "hello world", "   second line", "another first line" ]
-      events = []
-      lines.each do |line|
-        codec.decode(line) do |event|
-          events << event
-        end
-      end
-      codec.flush { |e| events << e }
-      insist { events.size } == 2
-      insist { events[0]["tags"] }.include?("hurray")
-      insist { events[1]["tags"] }.nil?
-    end
-
-    it "should allow grok patterns to be used" do
-      codec = LogStash::Codecs::Multiline.new(
-        "pattern" => "^%{NUMBER} %{TIME}",
-        "negate" => true,
-        "what" => "previous"
-      )
-
-      lines = [ "120913 12:04:33 first line", "second line", "third line" ]
-
-      events = []
-      lines.each do |line|
-        codec.decode(line) do |event|
-          events << event
-        end
-      end
-      codec.flush { |e| events << e }
-
-      insist { events.size } == 1
-      insist { events.first["message"] } == lines.join("\n")
-    end
-
-
-    context "using default UTF-8 charset" do
-
-      it "should decode valid UTF-8 input" do
-        codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous")
-        lines = [ "foobar", "" ]
-        events = []
-        lines.each do |line|
-          insist { line.encoding.name } == "UTF-8"
-          insist { line.valid_encoding? } == true
-
-          codec.decode(line) { |event| events << event }
-        end
-        codec.flush { |e| events << e }
-        insist { events.size } == 2
-
-        events.zip(lines).each do |tuple|
-          insist { tuple[0]["message"] } == tuple[1]
-          insist { tuple[0]["message"].encoding.name } == "UTF-8"
-        end
-      end
-
-      it "should escape invalid sequences" do
-        codec = LogStash::Codecs::Multiline.new("pattern" => "^\\s", "what" => "previous")
-        lines = [ "foo \xED\xB9\x81\xC3", "bar \xAD" ]
-        events = []
-        lines.each do |line|
-          insist { line.encoding.name } == "UTF-8"
-          insist { line.valid_encoding? } == false
-
-          codec.decode(line) { |event| events << event }
-        end
-        codec.flush { |e| events << e }
-        insist { events.size } == 2
-
-        events.zip(lines).each do |tuple|
-          insist { tuple[0]["message"] } == tuple[1].inspect[1..-2]
-          insist { tuple[0]["message"].encoding.name } == "UTF-8"
-        end
-      end
-    end
-
-
-    context "with valid non UTF-8 source encoding" do
-
-      it "should encode to UTF-8" do
-        codec = LogStash::Codecs::Multiline.new("charset" => "ISO-8859-1", "pattern" => "^\\s", "what" => "previous")
-        samples = [
-          ["foobar", "foobar"],
-          ["\xE0 Montr\xE9al", " Montral"],
-        ]
-
-        # lines = [ "foo \xED\xB9\x81\xC3", "bar \xAD" ]
-        events = []
-        samples.map{|(a, b)| a.force_encoding("ISO-8859-1")}.each do |line|
-          insist { line.encoding.name } == "ISO-8859-1"
-          insist { line.valid_encoding? } == true
-
-          codec.decode(line) { |event| events << event }
-        end
-        codec.flush { |e| events << e }
-        insist { events.size } == 2
-
-        events.zip(samples.map{|(a, b)| b}).each do |tuple|
-          insist { tuple[1].encoding.name } == "UTF-8"
-          insist { tuple[0]["message"] } == tuple[1]
-          insist { tuple[0]["message"].encoding.name } == "UTF-8"
-        end
-      end
-    end
-
-    context "with invalid non UTF-8 source encoding" do
-
-     it "should encode to UTF-8" do
-        codec = LogStash::Codecs::Multiline.new("charset" => "ASCII-8BIT", "pattern" => "^\\s", "what" => "previous")
-        samples = [
-          ["\xE0 Montr\xE9al", " Montral"],
-          ["\xCE\xBA\xCF\x8C\xCF\x83\xCE\xBC\xCE\xB5", ""],
-        ]
-        events = []
-        samples.map{|(a, b)| a.force_encoding("ASCII-8BIT")}.each do |line|
-          insist { line.encoding.name } == "ASCII-8BIT"
-          insist { line.valid_encoding? } == true
-
-          codec.decode(line) { |event| events << event }
-        end
-        codec.flush { |e| events << e }
-        insist { events.size } == 2
-
-        events.zip(samples.map{|(a, b)| b}).each do |tuple|
-          insist { tuple[1].encoding.name } == "UTF-8"
-          insist { tuple[0]["message"] } == tuple[1]
-          insist { tuple[0]["message"].encoding.name } == "UTF-8"
-        end
-      end
-
-    end
-  end
-end
diff --git a/spec/codecs/oldlogstashjson.rb b/spec/codecs/oldlogstashjson.rb
deleted file mode 100644
index 163980637ec..00000000000
--- a/spec/codecs/oldlogstashjson.rb
+++ /dev/null
@@ -1,55 +0,0 @@
-require "logstash/codecs/oldlogstashjson"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::OldLogStashJSON do
-  subject do
-    next LogStash::Codecs::OldLogStashJSON.new
-  end
-
-  context "#decode" do
-    it "should return a new (v1) event from old (v0) json data" do
-      data = {"@message" => "bar", "@source_host" => "localhost",
-              "@tags" => ["a","b","c"]}
-      subject.decode(data.to_json) do |event|
-        insist { event.is_a? LogStash::Event }
-        insist { event["@timestamp"] } != nil
-        insist { event["type"] } == data["@type"]
-        insist { event["message"] } == data["@message"]
-        insist { event["host"] } == data["@source_host"]
-        insist { event["tags"] } == data["@tags"]
-        insist { event["path"] } == nil # @source_path not in v0 test data
-      end
-    end
-
-    it "should accept invalid json" do
-      subject.decode("some plain text") do |event|
-        insist { event["message"] } == "some plain text"
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return old (v0) json data" do
-      data = {"type" => "t", "message" => "wat!?",
-              "host" => "localhost", "path" => "/foo",
-              "tags" => ["a","b","c"],
-              "bah" => "baz"}
-      event = LogStash::Event.new(data)
-      got_event = false
-      subject.on_event do |d|
-        insist { JSON.parse(d)["@timestamp"] } != nil
-        insist { JSON.parse(d)["@type"] } == data["type"]
-        insist { JSON.parse(d)["@message"] } == data["message"]
-        insist { JSON.parse(d)["@source_host"] } == data["host"]
-        insist { JSON.parse(d)["@source_path"] } == data["path"]
-        insist { JSON.parse(d)["@tags"] } == data["tags"]
-        insist { JSON.parse(d)["@fields"]["bah"] } == "baz"
-        insist { JSON.parse(d)["@fields"]["@version"] } == nil
-        got_event = true
-      end
-      subject.encode(event)
-      insist { got_event }
-    end
-  end
-end
diff --git a/spec/codecs/plain.rb b/spec/codecs/plain.rb
deleted file mode 100644
index c7b555585a1..00000000000
--- a/spec/codecs/plain.rb
+++ /dev/null
@@ -1,106 +0,0 @@
-# encoding: utf-8
-
-require "logstash/codecs/plain"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::Plain do
-  context "#decode" do
-    it "should return a valid event" do
-      subject.decode("Testing decoding.") do |event|
-        insist { event.is_a? LogStash::Event }
-      end
-    end
-
-    context "using default UTF-8 charset" do
-
-      it "should decode valid UTF-8 input" do
-        ["foobar", ""].each do |data|
-          insist { data.encoding.name } == "UTF-8"
-          insist { data.valid_encoding? } == true
-          subject.decode(data) do |event|
-            insist { event["message"] } == data
-            insist { event["message"].encoding.name } == "UTF-8"
-          end
-        end
-      end
-
-      it "should escape invalid sequences" do
-        ["foo \xED\xB9\x81\xC3", "bar \xAD"].each do |data|
-          insist { data.encoding.name } == "UTF-8"
-          insist { data.valid_encoding? } == false
-          subject.decode(data) do |event|
-            insist { event["message"] } == data.inspect[1..-2]
-            insist { event["message"].encoding.name } == "UTF-8"
-          end
-        end
-      end
-    end
-
-
-    context "with valid non UTF-8 source encoding" do
-
-      subject{LogStash::Codecs::Plain.new("charset" => "ISO-8859-1")}
-
-      it "should encode to UTF-8" do
-        samples = [
-          ["foobar", "foobar"],
-          ["\xE0 Montr\xE9al", " Montral"],
-        ]
-        samples.map{|(a, b)| [a.force_encoding("ISO-8859-1"), b]}.each do |(a, b)|
-          insist { a.encoding.name } == "ISO-8859-1"
-          insist { b.encoding.name } == "UTF-8"
-          insist { a.valid_encoding? } == true
-
-          subject.decode(a) do |event|
-            insist { event["message"] } == b
-            insist { event["message"].encoding.name } == "UTF-8"
-          end
-        end
-      end
-    end
-
-    context "with invalid non UTF-8 source encoding" do
-
-      subject{LogStash::Codecs::Plain.new("charset" => "ASCII-8BIT")}
-
-      it "should encode to UTF-8" do
-        samples = [
-          ["\xE0 Montr\xE9al", " Montral"],
-          ["\xCE\xBA\xCF\x8C\xCF\x83\xCE\xBC\xCE\xB5", ""],
-        ]
-        samples.map{|(a, b)| [a.force_encoding("ASCII-8BIT"), b]}.each do |(a, b)|
-          insist { a.encoding.name } == "ASCII-8BIT"
-          insist { b.encoding.name } == "UTF-8"
-          insist { a.valid_encoding? } == true
-
-          subject.decode(a) do |event|
-            insist { event["message"] } == b
-            insist { event["message"].encoding.name } == "UTF-8"
-          end
-        end
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return a plain text encoding" do
-      event = LogStash::Event.new
-      event["message"] = "Hello World."
-      subject.on_event do |data|
-        insist { data } == event.to_s
-      end
-      subject.encode(event)
-    end
-
-    it "should respect the format setting" do
-      format = "%{[hello]} %{[something][fancy]}"
-      codec = LogStash::Codecs::Plain.new("format" => format)
-      event = LogStash::Event.new("hello" => "world", "something" => { "fancy" => 123 })
-      codec.on_event do |data|
-        insist { data } == event.sprintf(format)
-      end
-      codec.encode(event)
-    end
-  end
-end
diff --git a/spec/codecs/spool.rb b/spec/codecs/spool.rb
deleted file mode 100644
index 5bdd6ee856b..00000000000
--- a/spec/codecs/spool.rb
+++ /dev/null
@@ -1,35 +0,0 @@
-require "logstash/codecs/spool"
-require "logstash/event"
-require "insist"
-
-describe LogStash::Codecs::Spool do
-  subject do
-    next LogStash::Codecs::Spool.new
-  end
-
-  context "#decode" do
-    it "should return multiple spooled events" do
-      e1 = LogStash::Event.new
-      e2 = LogStash::Event.new
-      e3 = LogStash::Event.new
-      subject.decode([e1,e2,e3]) do |event|
-        insist { event.is_a? LogStash::Event }
-      end
-    end
-  end
-
-  context "#encode" do
-    it "should return a spooled event" do
-      spool_size = Random.rand(10)
-      subject.spool_size = spool_size
-      got_event = false
-      subject.on_event do |data|
-        got_event = true
-      end
-      spool_size.times do
-        subject.encode(LogStash::Event.new)
-      end
-      insist { got_event }
-    end
-  end
-end
diff --git a/spec/conditionals/test.rb b/spec/core/conditionals_spec.rb
similarity index 84%
rename from spec/conditionals/test.rb
rename to spec/core/conditionals_spec.rb
index 8886032ac13..576851ae484 100644
--- a/spec/conditionals/test.rb
+++ b/spec/core/conditionals_spec.rb
@@ -1,6 +1,6 @@
-require "test_utils"
+require "logstash/devutils/rspec/spec_helper"
 
-module ConditionalFancines
+module ConditionalFanciness
   def description
     return example.metadata[:example_group][:description_args][0]
   end
@@ -21,9 +21,32 @@ def conditional(expression, &block)
   end
 end
 
-describe "conditionals" do
-  extend LogStash::RSpec
-  extend ConditionalFancines
+describe "conditionals in output" do
+  extend ConditionalFanciness
+
+  describe "simple" do
+    config <<-CONFIG
+      input {
+        generator {
+          message => '{"foo":{"bar"},"baz": "quux"}'
+          count => 1
+        }
+      }
+      output {
+        if [foo] == "bar" {
+          stdout { }
+        }
+      }
+    CONFIG
+
+    agent do
+      #LOGSTASH-2288, should not fail raising an exception
+    end
+  end
+end
+
+describe "conditionals in filter" do
+  extend ConditionalFanciness
 
   describe "simple" do
     config <<-CONFIG
@@ -160,7 +183,7 @@ def conditional(expression, &block)
         if "foo" not in "baz" { mutate { add_tag => "baz" } }
         if "foo" not in "foo" { mutate { add_tag => "foo" } }
         if !("foo" not in "foo") { mutate { add_tag => "notfoo" } }
-        if "foo" not in [somelist] { mutate { add_tag => "notsomelist" } } 
+        if "foo" not in [somelist] { mutate { add_tag => "notsomelist" } }
         if "one" not in [somelist] { mutate { add_tag => "somelist" } }
         if "foo" not in [alsomissing] { mutate { add_tag => "no string in missing field" } }
       }
@@ -183,12 +206,12 @@ def conditional(expression, &block)
     conditional "[message] == 'sample'" do
       sample("sample") { insist { subject["tags"] }.include?("success") }
       sample("different") { insist { subject["tags"] }.include?("failure") }
-    end 
+    end
 
     conditional "[message] != 'sample'" do
       sample("sample") { insist { subject["tags"] }.include?("failure") }
       sample("different") { insist { subject["tags"] }.include?("success") }
-    end 
+    end
 
     conditional "[message] < 'sample'" do
       sample("apple") { insist { subject["tags"] }.include?("success") }
@@ -230,12 +253,12 @@ def conditional(expression, &block)
     conditional "!([message] == 'sample')" do
       sample("sample") { reject { subject["tags"] }.include?("success") }
       sample("different") { reject { subject["tags"] }.include?("failure") }
-    end 
+    end
 
     conditional "!([message] != 'sample')" do
       sample("sample") { reject { subject["tags"] }.include?("failure") }
       sample("different") { reject { subject["tags"] }.include?("success") }
-    end 
+    end
 
     conditional "!([message] < 'sample')" do
       sample("apple") { reject { subject["tags"] }.include?("success") }
@@ -320,4 +343,54 @@ def conditional(expression, &block)
       end
     end
   end
+
+  describe "field references" do
+    conditional "[field with space]" do
+      sample("field with space" => "hurray") do
+        insist { subject["tags"].include?("success") }
+      end
+    end
+
+    conditional "[field with space] == 'hurray'" do
+      sample("field with space" => "hurray") do
+        insist { subject["tags"].include?("success") }
+      end
+    end
+
+    conditional "[nested field][reference with][some spaces] == 'hurray'" do
+      sample({"nested field" => { "reference with" => { "some spaces" => "hurray" } } }) do
+        insist { subject["tags"].include?("success") }
+      end
+    end
+  end
+
+  describe "new events from root" do
+    config <<-CONFIG
+      filter {
+        if [type] == "original" {
+          clone {
+            clones => ["clone"]
+          }
+        }
+        if [type] == "original" {
+          mutate { add_field => { "cond1" => "true" } }
+        } else {
+          mutate { add_field => { "cond2" => "true" } }
+        }
+      }
+    CONFIG
+
+    sample({"type" => "original"}) do
+      insist { subject }.is_a?(Array)
+      insist { subject.length } == 2
+
+      insist { subject[0]["type"] } == "original"
+      insist { subject[0]["cond1"] } == "true"
+      insist { subject[0]["cond2"] } == nil
+
+      insist { subject[1]["type"] } == "clone"
+      # insist { subject[1]["cond1"] } == nil
+      # insist { subject[1]["cond2"] } == "true"
+    end
+  end
 end
diff --git a/spec/config.rb b/spec/core/config_spec.rb
similarity index 100%
rename from spec/config.rb
rename to spec/core/config_spec.rb
diff --git a/spec/event.rb b/spec/core/event_spec.rb
similarity index 59%
rename from spec/event.rb
rename to spec/core/event_spec.rb
index 17a283da038..388d102bb24 100644
--- a/spec/event.rb
+++ b/spec/core/event_spec.rb
@@ -23,7 +23,8 @@
           "k3" => {"4" => "m"},
           5 => 6,
           "5" => 7
-      }
+      },
+      "@metadata" => { "fancy" => "pants", "have-to-go" => { "deeper" => "inception" } }
     )
   end
 
@@ -34,7 +35,7 @@
 
     it "should assign simple fields" do
       insist { subject["foo"] }.nil?
-      insist { subject["foo"] = "bar"} == "bar"
+      insist { subject["foo"] = "bar" } == "bar"
       insist { subject["foo"] } == "bar"
     end
 
@@ -87,6 +88,14 @@
     it "should be able to take a non-string for the format" do
       insist { subject.sprintf(2) } == "2"
     end
+
+    it "should allow to use the metadata when calling #sprintf" do
+      expect(subject.sprintf("super-%{[@metadata][fancy]}")).to eq("super-pants")
+    end
+
+    it "should allow to use nested hash from the metadata field" do
+      expect(subject.sprintf("%{[@metadata][have-to-go][deeper]}")).to eq("inception")
+    end
   end
 
   context "#[]" do
@@ -200,7 +209,7 @@
 
     data = { "@timestamp" => "2013-12-21T07:25:06.605Z" }
     event = LogStash::Event.new(data)
-    insist { event["@timestamp"] }.is_a?(Time)
+    insist { event["@timestamp"] }.is_a?(LogStash::Timestamp)
 
     duration = 0
     [warmup, count].each do |i|
@@ -247,4 +256,139 @@
       end
     end
   end
+
+  context "timestamp initialization" do
+    let(:logger) { double("logger") }
+
+    it "should coerce timestamp" do
+      t = Time.iso8601("2014-06-12T00:12:17.114Z")
+      expect(LogStash::Timestamp).to receive(:coerce).exactly(3).times.and_call_original
+      insist{LogStash::Event.new("@timestamp" => t).timestamp.to_i} == t.to_i
+      insist{LogStash::Event.new("@timestamp" => LogStash::Timestamp.new(t)).timestamp.to_i} == t.to_i
+      insist{LogStash::Event.new("@timestamp" => "2014-06-12T00:12:17.114Z").timestamp.to_i} == t.to_i
+    end
+
+    it "should assign current time when no timestamp" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).and_return(ts)
+      insist{LogStash::Event.new({}).timestamp.to_i} == ts.to_i
+    end
+
+    it "should tag and warn for invalid value" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).twice.and_return(ts)
+      expect(Cabin::Channel).to receive(:get).twice.and_return(logger)
+      expect(logger).to receive(:warn).twice
+
+      event = LogStash::Event.new("@timestamp" => :foo)
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == :foo
+
+      event = LogStash::Event.new("@timestamp" => 666)
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == 666
+    end
+
+    it "should tag and warn for invalid string format" do
+      ts = LogStash::Timestamp.now
+      expect(LogStash::Timestamp).to receive(:now).and_return(ts)
+      expect(Cabin::Channel).to receive(:get).and_return(logger)
+      expect(logger).to receive(:warn)
+
+      event = LogStash::Event.new("@timestamp" => "foo")
+      insist{event.timestamp.to_i} == ts.to_i
+      insist{event["tags"]} == [LogStash::Event::TIMESTAMP_FAILURE_TAG]
+      insist{event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]} == "foo"
+    end
+  end
+
+  context "to_json" do
+    it "should support to_json" do
+      new_event = LogStash::Event.new(
+        "@timestamp" => Time.iso8601("2014-09-23T19:26:15.832Z"),
+        "message" => "foo bar",
+      )
+      json = new_event.to_json
+
+      insist { json } ==  "{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}"
+    end
+
+    it "should support to_json and ignore arguments" do
+      new_event = LogStash::Event.new(
+        "@timestamp" => Time.iso8601("2014-09-23T19:26:15.832Z"),
+        "message" => "foo bar",
+      )
+      json = new_event.to_json(:foo => 1, :bar => "baz")
+
+      insist { json } ==  "{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}"
+    end
+  end
+
+  context "metadata" do
+    context "with existing metadata" do
+      subject { LogStash::Event.new("hello" => "world", "@metadata" => { "fancy" => "pants" }) }
+
+      it "should not include metadata in to_hash" do
+        reject { subject.to_hash.keys }.include?("@metadata")
+
+        # 'hello', '@timestamp', and '@version'
+        insist { subject.to_hash.keys.count } == 3
+      end
+
+      it "should still allow normal field access" do
+        insist { subject["hello"] } == "world"
+      end
+    end
+
+    context "with set metadata" do
+      let(:fieldref) { "[@metadata][foo][bar]" }
+      let(:value) { "bar" }
+      subject { LogStash::Event.new("normal" => "normal") }
+      before do
+        # Verify the test is configured correctly.
+        insist { fieldref }.start_with?("[@metadata]")
+
+        # Set it.
+        subject[fieldref] = value
+      end
+
+      it "should still allow normal field access" do
+        insist { subject["normal"] } == "normal"
+      end
+
+      it "should allow getting" do
+        insist { subject[fieldref] } == value
+      end
+
+      it "should be hidden from .to_json" do
+        require "json"
+        obj = JSON.parse(subject.to_json)
+        reject { obj }.include?("@metadata")
+      end
+
+      it "should be hidden from .to_hash" do
+        reject { subject.to_hash }.include?("@metadata")
+      end
+
+      it "should be accessible through #to_hash_with_metadata" do
+        obj = subject.to_hash_with_metadata
+        insist { obj }.include?("@metadata")
+        insist { obj["@metadata"]["foo"]["bar"] } == value
+      end
+    end
+    
+    context "with no metadata" do
+      subject { LogStash::Event.new("foo" => "bar") }
+      it "should have no metadata" do
+        insist { subject["@metadata"] }.empty?
+      end
+      it "should still allow normal field access" do
+        insist { subject["foo"] } == "bar"
+      end
+    end
+
+  end
+
 end
diff --git a/spec/core/pipeline_spec.rb b/spec/core/pipeline_spec.rb
new file mode 100644
index 00000000000..127b8326665
--- /dev/null
+++ b/spec/core/pipeline_spec.rb
@@ -0,0 +1,117 @@
+require "logstash/devutils/rspec/spec_helper"
+
+class DummyInput < LogStash::Inputs::Base
+  config_name "dummyinput"
+  milestone 2
+
+  def register
+  end
+
+  def run(queue)
+  end
+
+  def teardown
+  end
+end
+
+class DummyCodec < LogStash::Codecs::Base
+  config_name "dummycodec"
+  milestone 2
+
+  def decode(data) 
+    data
+  end
+
+  def encode(event) 
+    event
+  end
+
+  def teardown
+  end
+end
+
+class DummyOutput < LogStash::Outputs::Base
+  config_name "dummyoutput"
+  milestone 2
+  
+  attr_reader :num_teardowns
+
+  def initialize(params={})
+    super
+    @num_teardowns = 0
+  end
+
+  def register
+  end
+
+  def receive(event)
+  end
+
+  def teardown
+    @num_teardowns += 1
+  end
+end
+
+class TestPipeline < LogStash::Pipeline
+  attr_reader :outputs
+end
+
+describe LogStash::Pipeline do
+
+  before(:each) do
+    LogStash::Plugin.stub(:lookup)
+      .with("input", "dummyinput").and_return(DummyInput)
+    LogStash::Plugin.stub(:lookup)
+      .with("codec", "plain").and_return(DummyCodec)
+    LogStash::Plugin.stub(:lookup)
+      .with("output", "dummyoutput").and_return(DummyOutput)
+  end
+
+  let(:test_config_without_output_workers) {
+    <<-eos
+    input {
+      dummyinput {}
+    }
+  
+    output {
+      dummyoutput {}
+    }
+    eos
+  }
+
+  let(:test_config_with_output_workers) {
+    <<-eos
+    input {
+      dummyinput {}
+    }
+  
+    output {
+      dummyoutput {
+        workers => 2
+      }
+    }
+    eos
+  }
+
+  context "output teardown" do
+    it "should call teardown of output without output-workers" do
+      pipeline = TestPipeline.new(test_config_without_output_workers)
+      pipeline.run
+
+      insist { pipeline.outputs.size } == 1
+      insist { pipeline.outputs.first.worker_plugins.size } == 1
+      insist { pipeline.outputs.first.worker_plugins.first.num_teardowns } == 1
+    end
+
+    it "should call output teardown correctly with output workers" do
+      pipeline = TestPipeline.new(test_config_with_output_workers)
+      pipeline.run
+
+      insist { pipeline.outputs.size } == 1
+      insist { pipeline.outputs.first.num_teardowns } == 0
+      pipeline.outputs.first.worker_plugins.each do |plugin|
+        insist { plugin.num_teardowns } == 1
+      end
+    end
+  end
+end
diff --git a/spec/runner_spec.rb b/spec/core/runner_spec.rb
similarity index 55%
rename from spec/runner_spec.rb
rename to spec/core/runner_spec.rb
index 5250747f354..01c7587f63e 100644
--- a/spec/runner_spec.rb
+++ b/spec/core/runner_spec.rb
@@ -19,24 +19,36 @@ def run(args); end
     it "should run agent help" do
       expect(subject).to receive(:show_help).once.and_return(nil)
       args = ["agent", "-h"]
-      expect(subject.run(args)).to eq([])
+      expect(subject.run(args).wait).to eq(0)
+    end
+
+    it "should show help with no arguments" do
+      expect($stderr).to receive(:puts).once.and_return("No command given")
+      expect($stderr).to receive(:puts).once
+      args = []
+      expect(subject.run(args).wait).to eq(1)
+    end
+
+    it "should show help for unknown commands" do
+      expect($stderr).to receive(:puts).once.and_return("No such command welp")
+      expect($stderr).to receive(:puts).once
+      args = ["welp"]
+      expect(subject.run(args).wait).to eq(1)
     end
 
     it "should run agent help and not run following commands" do
       expect(subject).to receive(:show_help).once.and_return(nil)
       args = ["agent", "-h", "web"]
-      expect(subject.run(args)).to eq([])
+      expect(subject.run(args).wait).to eq(0)
     end
 
-    it "should run agent and web" do
+    it "should not run agent and web" do
       expect(Stud::Task).to receive(:new).once
       args = ["agent", "-e", "", "web"]
       args = subject.run(args)
-      expect(args).to eq(["web"])
-
-      expect(LogStash::Kibana::Runner).to receive(:new).once.and_return(NullRunner.new)
-      args = subject.run(args)
       expect(args).to eq(nil)
+
+      expect(LogStash::Kibana::Runner).to_not receive(:new)
     end
   end
 end
diff --git a/spec/core/timestamp_spec.rb b/spec/core/timestamp_spec.rb
new file mode 100644
index 00000000000..01bc3fb8250
--- /dev/null
+++ b/spec/core/timestamp_spec.rb
@@ -0,0 +1,83 @@
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/timestamp"
+
+describe LogStash::Timestamp do
+
+  it "should parse its own iso8601 output" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.parse_iso8601(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce iso8601 string" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.coerce(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Time" do
+    t = Time.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Timestamp" do
+    t = LogStash::Timestamp.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should raise on invalid string coerce" do
+    expect{LogStash::Timestamp.coerce("foobar")}.to raise_error LogStash::TimestampParserError
+  end
+
+  it "should return nil on invalid object coerce" do
+    expect(LogStash::Timestamp.coerce(:foobar)).to be_nil
+  end
+
+  it "should support to_json" do
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json).to eq("\"2014-09-23T08:00:00.000Z\"")
+  end
+
+  it "should support to_json and ignore arguments" do
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :argumnents => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
+  end
+
+  it "should support timestamp comparaison" do
+   current = LogStash::Timestamp.new(Time.now) 
+   future = LogStash::Timestamp.new(Time.now + 100)
+
+   expect(future > current).to eq(true)
+   expect(future < current).to eq(false)
+   expect(current == current).to eq(true)
+
+   expect(current <=> current).to eq(0)
+   expect(current <=> future).to eq(-1)
+   expect(future <=> current).to eq(1)
+  end
+
+  it "should allow unary operation +" do
+    current = Time.now
+    t = LogStash::Timestamp.new(current) + 10
+    expect(t).to eq(current + 10)
+  end
+
+  describe "subtraction" do
+    it "should work on a timestamp object" do
+      t = Time.now
+      current = LogStash::Timestamp.new(t)
+      future = LogStash::Timestamp.new(t + 10)
+      expect(future - current).to eq(10)
+    end
+
+    it "should work on with time object" do
+      current = Time.now
+      t = LogStash::Timestamp.new(current + 10)
+      expect(t - current).to eq(10)
+    end
+
+    it "should work with numeric value" do
+      current = Time.now
+      t = LogStash::Timestamp.new(current + 10)
+      expect(t - 10).to eq(current)
+    end
+  end
+end
diff --git a/spec/web.rb b/spec/core/web_spec.rb
similarity index 100%
rename from spec/web.rb
rename to spec/core/web_spec.rb
diff --git a/spec/environment.rb b/spec/environment.rb
deleted file mode 100644
index d0dea75ffeb..00000000000
--- a/spec/environment.rb
+++ /dev/null
@@ -1,16 +0,0 @@
-require "logstash/environment"
-
-describe LogStash::Environment do
-
-  describe "load_elasticsearch_jars!" do
-
-    it "should load elasticsarch jars" do
-      expect{LogStash::Environment.load_elasticsearch_jars!}.to_not raise_error
-    end
-
-    it "should raise when cannot find elasticsarch jars" do
-      stub_const("LogStash::Environment::JAR_DIR", "/some/invalid/path")
-      expect{LogStash::Environment.load_elasticsearch_jars!}.to raise_error(LogStash::EnvironmentError)
-    end
-  end
-end
diff --git a/spec/examples/fail2ban.rb b/spec/examples/fail2ban.rb
deleted file mode 100644
index edb0baa9693..00000000000
--- a/spec/examples/fail2ban.rb
+++ /dev/null
@@ -1,30 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-
-describe "fail2ban logs", :if => RUBY_ENGINE == "jruby"  do
-  extend LogStash::RSpec
-
-  # The logstash config goes here.
-  # At this time, only filters are supported.
-  config <<-CONFIG
-    filter {
-      grok {
-        pattern => "^%{TIMESTAMP_ISO8601:timestamp} fail2ban\.actions: %{WORD:level} \\[%{WORD:program}\\] %{WORD:action} %{IP:ip}"
-        singles => true
-      }
-      date {
-        match => [ "timestamp", "yyyy-MM-dd HH:mm:ss,SSS" ]
-      }
-      mutate {
-        remove => timestamp
-      }
-    }
-  CONFIG
-
-  sample "2013-06-28 15:10:59,891 fail2ban.actions: WARNING [ssh] Ban 95.78.163.5" do
-    insist { subject["program"] } == "ssh"
-    insist { subject["action"] } == "Ban"
-    insist { subject["ip"] } == "95.78.163.5"
-  end
-end
diff --git a/spec/examples/graphite-input.rb b/spec/examples/graphite-input.rb
deleted file mode 100644
index fc86b09c49d..00000000000
--- a/spec/examples/graphite-input.rb
+++ /dev/null
@@ -1,43 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-
-describe "receive graphite input", :if => RUBY_ENGINE == "jruby" do
-  extend LogStash::RSpec
-
-  # The logstash config goes here.
-  # At this time, only filters are supported.
-  config <<-CONFIG
-    # input {
-    #   tcp {
-    #     port => 1234
-    #     mode => server
-    #     type => graphite
-    #   }
-    # }
-    filter {
-      grok {
-        pattern => "%{DATA:name} %{NUMBER:value:float} %{POSINT:ts}"
-        singles => true
-      }
-      date {
-        match => ["ts", UNIX]
-      }
-      mutate {
-        remove => ts
-      }
-    }
-  CONFIG
-
-  type "graphite"
-
-  sample "foo.bar.baz 4025.34 1364606522" do
-    insist { subject }.include?("name")
-    insist { subject }.include?("value")
-
-    insist { subject["name"] } == "foo.bar.baz"
-    insist { subject["value"] } == 4025.34
-    insist { subject["@timestamp"] } == Time.iso8601("2013-03-30T01:22:02.000Z")
-
-  end
-end
diff --git a/spec/examples/mysql-slow-query.rb b/spec/examples/mysql-slow-query.rb
deleted file mode 100644
index da019e0b616..00000000000
--- a/spec/examples/mysql-slow-query.rb
+++ /dev/null
@@ -1,70 +0,0 @@
-require "test_utils"
-
-# Skip until we convert this to use multiline codec
-describe "parse mysql slow query log", :if => false do
-  extend LogStash::RSpec
-
-  config <<-'CONFIG'
-    filter {
-      grep {
-        # Drop the '# Time:' lines since they only appear when the 'time'
-        # changes and are omitted otherwise. Further, there's always (from what
-        # I have seen) a 'SET timestamp=123456789' line in each query event, so
-        # I use that as the timestamp instead.
-        #
-        # mysql logs are messed up, so this is pretty much best effort.
-        match => [ "@message", "^# Time: " ]
-        negate => true
-      }
-
-      grok {
-        singles => true
-        pattern => [
-          "^# User@Host: %{USER:user}\[[^\]]+\] @ %{HOST:host} \[%{IP:ip}?]",
-          "^# Query_time: %{NUMBER:duration:float} \s*Lock_time: %{NUMBER:lock_wait:float} \s*Rows_sent: %{NUMBER:results:int} \s*Rows_examined: %{NUMBER:scanned:int}",
-          "^SET timestamp=%{NUMBER:timestamp};"
-        ]
-      }
-
-      multiline {
-        pattern => "^# User@Host: "
-        negate => true
-        what => previous
-      }
-
-      date {
-        match => ["timestamp", UNIX]
-      }
-
-      mutate {
-        remove => "timestamp"
-      }
-    }
-  CONFIG
-
-  lines = <<-'MYSQL_SLOW_LOGS'
-# Time: 121004  6:00:27
-# User@Host: someuser[someuser] @ db.example.com [1.2.3.4]
-# Query_time: 0.018143  Lock_time: 0.000042 Rows_sent: 237  Rows_examined: 286
-use somedb;
-SET timestamp=1349355627;
-SELECT option_name, option_value FROM wp_options WHERE autoload = 'yes';
-MYSQL_SLOW_LOGS
-
-  sample lines.split("\n") do
-    reject { subject }.is_a? Array # 1 event expected
-    insist { subject.message.split("\n").size } == 5 # 5 lines
-
-    lines.split("\n")[1..5].each_with_index do |line, i|
-      insist { subject.message.split("\n")[i] } == line
-    end
-
-    insist { subject["user"] } == "someuser"
-    insist { subject["host"] } == "db.example.com"
-    insist { subject["ip"] } == "1.2.3.4"
-    insist { subject["duration"] } == 0.018143
-    insist { subject["lock_wait"] } == 0.000042
-    insist { subject["results"] } == 237
-    insist { subject["scanned"] } == 286
-  end
-end
diff --git a/spec/examples/parse-apache-logs.rb b/spec/examples/parse-apache-logs.rb
deleted file mode 100644
index c7e14537963..00000000000
--- a/spec/examples/parse-apache-logs.rb
+++ /dev/null
@@ -1,69 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-
-describe "apache common log format", :if => RUBY_ENGINE == "jruby" do
-  extend LogStash::RSpec
-
-  # The logstash config goes here.
-  # At this time, only filters are supported.
-  config <<-CONFIG
-    filter {
-      grok {
-        pattern => "%{COMBINEDAPACHELOG}"
-        singles => true
-      }
-      date {
-        match => ["timestamp", "dd/MMM/yyyy:HH:mm:ss Z"]
-        locale => "en"
-      }
-    }
-  CONFIG
-
-  # Here we provide a sample log event for the testing suite.
-  #
-  # Any filters you define above will be applied the same way the logstash
-  # agent performs. Inside the 'sample ... ' block the 'subject' will be
-  # a LogStash::Event object for you to inspect and verify for correctness.
-  sample '198.151.8.4 - - [29/Aug/2012:20:17:38 -0400] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:14.0) Gecko/20100101 Firefox/14.0.1"' do
-
-    # These 'insist' and 'reject' calls use my 'insist' rubygem.
-    # See http://rubydoc.info/gems/insist for more info.
-
-    # Require that grok does not fail to parse this event.
-    insist { subject["tags"] }.nil?
-
-    # Ensure that grok captures certain expected fields.
-    insist { subject }.include?("agent")
-    insist { subject }.include?("bytes")
-    insist { subject }.include?("clientip")
-    insist { subject }.include?("httpversion")
-    insist { subject }.include?("timestamp")
-    insist { subject }.include?("verb")
-    insist { subject }.include?("response")
-    insist { subject }.include?("request")
-
-    # Ensure that those fields match expected values from the event.
-    insist { subject["clientip"] } == "198.151.8.4"
-    insist { subject["timestamp"] } == "29/Aug/2012:20:17:38 -0400"
-    insist { subject["verb"] } == "GET"
-    insist { subject["request"] } == "/favicon.ico"
-    insist { subject["httpversion"] } == "1.1"
-    insist { subject["response"] } == "200"
-    insist { subject["bytes"] } == "3638"
-    insist { subject["referrer"] } == '"-"'
-    insist { subject["agent"] } == "\"Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:14.0) Gecko/20100101 Firefox/14.0.1\""
-
-    # Verify date parsing
-    insist { subject.timestamp } == Time.iso8601("2012-08-30T00:17:38.000Z")
-  end
-
-  sample '61.135.248.195 - - [26/Sep/2012:11:49:20 -0400] "GET /projects/keynav/ HTTP/1.1" 200 18985 "" "Mozilla/5.0 (compatible; YodaoBot/1.0; http://www.yodao.com/help/webmaster/spider/; )"' do
-    insist { subject["tags"] }.nil?
-    insist { subject["clientip"] } == "61.135.248.195"
-  end
-
-  sample '72.14.164.185 - - [25/Sep/2012:12:05:02 -0400] "GET /robots.txt HTTP/1.1" 200 - "www.brandimensions.com" "BDFetch"' do
-    insist { subject["tags"] }.nil?
-  end
-end
diff --git a/spec/examples/parse-haproxy-logs.rb b/spec/examples/parse-haproxy-logs.rb
deleted file mode 100644
index 90f272c7a54..00000000000
--- a/spec/examples/parse-haproxy-logs.rb
+++ /dev/null
@@ -1,117 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-
-describe "haproxy httplog format" do
-  extend LogStash::RSpec
-
-  # The logstash config goes here.
-  # At this time, only filters are supported.
-  config <<-CONFIG
-  filter {
-    grok {
-      pattern => "%{HAPROXYHTTP}"
-    }
-  }
-  CONFIG
-  # Here we provide a sample log event for the testing suite.
-  #
-  # Any filters you define above will be applied the same way the logstash
-  # agent performs. Inside the 'sample ... ' block the 'subject' will be
-  # a LogStash::Event object for you to inspect and verify for correctness.
-  # HAPROXYHTTP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
-
-  sample 'Feb  6 12:14:14 localhost haproxy[14389]: 10.0.1.2:33317 [06/Feb/2009:12:14:14.655] http-in static/srv1 10/0/30/69/109 200 2750 - - ---- 1/1/1/1/0 0/0 {1wt.eu} {} "GET /index.html HTTP/1.1"' do
-
-    # These 'insist' and 'reject' calls use my 'insist' rubygem.
-    # See http://rubydoc.info/gems/insist for more info.
-
-    # Require that grok does not fail to parse this event.
-    insist { subject["tags"] }.nil?
-
-
-    # Ensure that grok captures certain expected fields.
-    insist { subject }.include?("syslog_timestamp")
-    insist { subject }.include?("syslog_server")
-    insist { subject }.include?("program")
-    insist { subject }.include?("pid")
-    insist { subject }.include?("client_ip")
-    insist { subject }.include?("client_port")
-    insist { subject }.include?("accept_date")
-    insist { subject }.include?("haproxy_monthday")
-    insist { subject }.include?("haproxy_month")
-    insist { subject }.include?("haproxy_year")
-    insist { subject }.include?("haproxy_time")
-    insist { subject }.include?("haproxy_hour")
-    insist { subject }.include?("haproxy_minute")
-    insist { subject }.include?("haproxy_second")
-    insist { subject }.include?("haproxy_milliseconds")
-    insist { subject }.include?("frontend_name")
-    insist { subject }.include?("backend_name")
-    insist { subject }.include?("server_name")
-    insist { subject }.include?("time_request")
-    insist { subject }.include?("time_queue")
-    insist { subject }.include?("time_backend_connect")
-    insist { subject }.include?("time_backend_response")
-    insist { subject }.include?("time_duration")
-    insist { subject }.include?("http_status_code")
-    insist { subject }.include?("bytes_read")
-    insist { subject }.include?("captured_request_cookie")
-    insist { subject }.include?("captured_response_cookie")
-    insist { subject }.include?("termination_state")
-    insist { subject }.include?("actconn")
-    insist { subject }.include?("feconn")
-    insist { subject }.include?("beconn")
-    insist { subject }.include?("srvconn")
-    insist { subject }.include?("retries")
-    insist { subject }.include?("srv_queue")
-    insist { subject }.include?("backend_queue")
-    insist { subject }.include?("captured_request_headers")
-    insist { subject }.include?("http_verb")
-    insist { subject }.include?("http_request")
-    insist { subject }.include?("http_version")
-
-#    # Ensure that those fields match expected values from the event.
-
-    insist{ subject["syslog_timestamp"] } == "Feb  6 12:14:14"
-    insist{ subject["syslog_server"] } == "localhost"
-    insist{ subject["program"] } == "haproxy"
-    insist{ subject["pid"] } == "14389"
-    insist{ subject["client_ip"] } == "10.0.1.2"
-    insist{ subject["client_port"] } == "33317"
-    insist{ subject["accept_date"] } == "06/Feb/2009:12:14:14.655"
-    insist{ subject["haproxy_monthday"] } == "06"
-    insist{ subject["haproxy_month"] } == "Feb"
-    insist{ subject["haproxy_year"] } == "2009"
-    insist{ subject["haproxy_time"] } == "12:14:14"
-    insist{ subject["haproxy_hour"] } == "12"
-    insist{ subject["haproxy_minute"] } == "14"
-    insist{ subject["haproxy_second"] } == "14"
-    insist{ subject["haproxy_milliseconds"] } == "655"
-    insist{ subject["frontend_name"] } == "http-in"
-    insist{ subject["backend_name"] } == "static"
-    insist{ subject["server_name"] } == "srv1"
-    insist{ subject["time_request"] } == "10"
-    insist{ subject["time_queue"] } == "0"
-    insist{ subject["time_backend_connect"] } == "30"
-    insist{ subject["time_backend_response"] } == "69"
-    insist{ subject["time_duration"] } == "109"
-    insist{ subject["http_status_code"] } == "200"
-    insist{ subject["bytes_read"] } == "2750"
-    insist{ subject["captured_request_cookie"] } == "-"
-    insist{ subject["captured_response_cookie"] } == "-"
-    insist{ subject["termination_state"] } == "----"
-    insist{ subject["actconn"] } == "1"
-    insist{ subject["feconn"] } == "1"
-    insist{ subject["beconn"] } == "1"
-    insist{ subject["srvconn"] } == "1"
-    insist{ subject["retries"] } == "0"
-    insist{ subject["srv_queue"] } == "0"
-    insist{ subject["backend_queue"] } == "0"
-    insist{ subject["captured_request_headers"] } == "1wt.eu"
-    insist{ subject["http_verb"] } == "GET"
-    insist{ subject["http_request"] } == "/index.html"
-    insist{ subject["http_version"] } == "1.1"
-  end
-
-end
diff --git a/spec/examples/syslog.rb b/spec/examples/syslog.rb
deleted file mode 100644
index fe617cf59df..00000000000
--- a/spec/examples/syslog.rb
+++ /dev/null
@@ -1,50 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-
-describe "parse syslog", :if => RUBY_ENGINE == "jruby" do
-  extend LogStash::RSpec
-
-  config <<-'CONFIG'
-    filter {
-      grok {
-          type => "syslog"
-          singles => true
-          pattern => [ "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" ]
-          add_field => [ "received_at", "%{@timestamp}" ]
-          add_field => [ "received_from", "%{source_host}" ]
-      }
-      syslog_pri {
-          type => "syslog"
-      }
-      date {
-          type => "syslog"
-          match => ["syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
-      }
-      mutate {
-          type => "syslog"
-          exclude_tags => "_grokparsefailure"
-          replace => [ "source_host", "%{syslog_hostname}" ]
-          replace => [ "message", "%{syslog_message}" ]
-      }
-      mutate {
-          type => "syslog"
-          remove => [ "syslog_hostname", "syslog_message", "syslog_timestamp" ]
-      }
-    }
-  CONFIG
-
-  sample("message" => "<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group \"acl_drac\" [0x0, 0x0]", "type" => "syslog") do
-    insist { subject["type"] } == "syslog"
-    insist { subject["tags"] }.nil?
-    insist { subject["syslog_pri"] } == "164"
-  end
-
-  # Single digit day
-  sample("message" => "<164>Oct  6 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group \"acl_drac\" [0x0, 0x0]", "type" => "syslog") do
-    insist { subject["type"] } == "syslog"
-    insist { subject["tags"] }.nil?
-    insist { subject["syslog_pri"] } == "164"
-    #insist { subject.timestamp } == "2012-10-26T15:19:25.000Z"
-  end
-end
diff --git a/spec/filters/anonymize.rb b/spec/filters/anonymize.rb
deleted file mode 100644
index 43f571b8284..00000000000
--- a/spec/filters/anonymize.rb
+++ /dev/null
@@ -1,191 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/anonymize"
-
-describe LogStash::Filters::Anonymize do
-  extend LogStash::RSpec
-
-  describe "anonymize ipaddress with IPV4_NETWORK algorithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          algorithm => "IPV4_NETWORK"
-          key => 24
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "233.255.13.44") do
-      insist { subject["clientip"] } == "233.255.13.0"
-    end
-  end
-
-  describe "anonymize string with MURMUR3 algorithm" do
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          algorithm => "MURMUR3"
-          key => ""
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.52.122.33") do
-      insist { subject["clientip"] } == 1541804874
-    end
-  end
-
-   describe "anonymize string with SHA1 alogrithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          key => "longencryptionkey"
-          algorithm => 'SHA1'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["clientip"] } == "fdc60acc4773dc5ac569ffb78fcb93c9630797f4"
-    end
-  end
-
-  # HMAC-SHA224 isn't implemented in JRuby OpenSSL
-  #describe "anonymize string with SHA224 alogrithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    #config <<-CONFIG
-      #filter {
-        #anonymize {
-          #fields => ["clientip"]
-          #key => "longencryptionkey"
-          #algorithm => 'SHA224'
-        #}
-      #}
-    #CONFIG
-
-    #sample("clientip" => "123.123.123.123") do
-      #insist { subject["clientip"] } == "5744bbcc4f64acb6a805b7fee3013a8958cc8782d3fb0fb318cec915"
-    #end
-  #end
-
-  describe "anonymize string with SHA256 alogrithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          key => "longencryptionkey"
-          algorithm => 'SHA256'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["clientip"] } == "345bec3eff242d53b568916c2610b3e393d885d6b96d643f38494fd74bf4a9ca"
-    end
-  end
-
-  describe "anonymize string with SHA384 alogrithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          key => "longencryptionkey"
-          algorithm => 'SHA384'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["clientip"] } == "22d4c0e8c4fbcdc4887d2038fca7650f0e2e0e2457ff41c06eb2a980dded6749561c814fe182aff93e2538d18593947a"
-    end
-  end
-
-  describe "anonymize string with SHA512 alogrithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          key => "longencryptionkey"
-          algorithm => 'SHA512'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["clientip"] } == "11c19b326936c08d6c50a3c847d883e5a1362e6a64dd55201a25f2c1ac1b673f7d8bf15b8f112a4978276d573275e3b14166e17246f670c2a539401c5bfdace8"
-    end
-  end
-
-  # HMAC-MD4 isn't implemented in JRuby OpenSSL
-  #describe "anonymize string with MD4 alogrithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    #config <<-CONFIG
-      #filter {
-        #anonymize {
-          #fields => ["clientip"]
-          #key => "longencryptionkey"
-          #algorithm => 'MD4'
-        #}
-      #}
-    #CONFIG
-#
-    #sample("clientip" => "123.123.123.123") do
-      #insist { subject["clientip"] } == "0845cb571ab3646e51a07bcabf05e33d"
-    #end
-  #end
-
-  describe "anonymize string with MD5 alogrithm" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          key => "longencryptionkey"
-          algorithm => 'MD5'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["clientip"] } == "9336c879e305c9604a3843fc3e75948f"
-    end
-  end
-
-  describe "Test field with multiple values" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        anonymize {
-          fields => ["clientip"]
-          key => "longencryptionkey"
-          algorithm => 'MD5'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => [ "123.123.123.123", "223.223.223.223" ]) do
-      insist { subject["clientip"]} == [ "9336c879e305c9604a3843fc3e75948f", "7a6c66b8d3f42a7d650e3354af508df3" ]
-    end
-  end
-
-
-
-end
diff --git a/spec/filters/checksum.rb b/spec/filters/checksum.rb
deleted file mode 100644
index bb8f2eae887..00000000000
--- a/spec/filters/checksum.rb
+++ /dev/null
@@ -1,43 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/checksum"
-require 'openssl'
-
-describe LogStash::Filters::Checksum do
-  extend LogStash::RSpec
-
-  LogStash::Filters::Checksum::ALGORITHMS.each do |alg|
-    describe "#{alg} checksum with single field" do
-      config <<-CONFIG
-        filter {
-          checksum {
-            algorithm => "#{alg}"
-            keys => ["test"]
-          }
-        }
-        CONFIG
-
-      sample "test" => "foo bar" do
-        insist { !subject["logstash_checksum"].nil? }
-        insist { subject["logstash_checksum"] } == OpenSSL::Digest.hexdigest(alg, "|test|foo bar|")
-      end
-    end
-
-    describe "#{alg} checksum with multiple keys" do
-      config <<-CONFIG
-        filter {
-          checksum {
-            algorithm => "#{alg}"
-            keys => ["test1", "test2"]
-          }
-        }
-        CONFIG
-
-      sample "test1" => "foo", "test2" => "bar" do
-        insist { !subject["logstash_checksum"].nil? }
-        insist { subject["logstash_checksum"] } == OpenSSL::Digest.hexdigest(alg, "|test1|foo|test2|bar|")
-      end
-    end
-  end
-end
diff --git a/spec/filters/clone.rb b/spec/filters/clone.rb
deleted file mode 100644
index fd73f178fac..00000000000
--- a/spec/filters/clone.rb
+++ /dev/null
@@ -1,83 +0,0 @@
-require "test_utils"
-require "logstash/filters/clone"
-
-describe LogStash::Filters::Clone do
-  extend LogStash::RSpec
-
-  describe "all defaults" do
-    type "original"
-    config <<-CONFIG
-      filter {
-        clone {
-          type => "original"
-          clones => ["clone", "clone", "clone"]
-        }
-      }
-    CONFIG
-
-    sample("message" => "hello world", "type" => "original") do
-      insist { subject }.is_a? Array
-      insist { subject.length } == 4
-      subject.each_with_index do |s,i|
-        if i == 0 # last one should be 'original'
-          insist { s["type"] } == "original"
-        else
-          insist { s["type"]} == "clone"
-        end
-        insist { s["message"] } == "hello world"
-      end
-    end
-  end
-
-  describe "Complex use" do
-    config <<-CONFIG
-      filter {
-        clone {
-          type => "nginx-access"
-          tags => ['TESTLOG']
-          clones => ["nginx-access-clone1", "nginx-access-clone2"]
-          add_tag => ['RABBIT','NO_ES']
-          remove_tag => ["TESTLOG"]
-        }
-      }
-    CONFIG
-
-    sample("type" => "nginx-access", "tags" => ["TESTLOG"], "message" => "hello world") do
-      insist { subject }.is_a? Array
-      insist { subject.length } == 3
-
-      insist { subject[0]["type"] } == "nginx-access"
-      #Initial event remains unchanged
-      insist { subject[0]["tags"] }.include? "TESTLOG"
-      reject { subject[0]["tags"] }.include? "RABBIT"
-      reject { subject[0]["tags"] }.include? "NO_ES"
-      #All clones go through filter_matched
-      insist { subject[1]["type"] } == "nginx-access-clone1"
-      reject { subject[1]["tags"] }.include? "TESTLOG"
-      insist { subject[1]["tags"] }.include? "RABBIT"
-      insist { subject[1]["tags"] }.include? "NO_ES"
-
-      insist { subject[2]["type"] } == "nginx-access-clone2"
-      reject { subject[2]["tags"] }.include? "TESTLOG"
-      insist { subject[2]["tags"] }.include? "RABBIT"
-      insist { subject[2]["tags"] }.include? "NO_ES"
-
-    end
-  end
-
-  describe "Bug LOGSTASH-1225" do
-    ### LOGSTASH-1225: Cannot clone events containing numbers.
-    config <<-CONFIG
-      filter {
-        clone {
-          clones => [ 'clone1' ]
-        }
-      }
-    CONFIG
-
-    sample("type" => "bug-1225", "message" => "unused", "number" => 5) do
-      insist { subject[0]["number"] } == 5
-      insist { subject[1]["number"] } == 5
-    end
-  end
-end
diff --git a/spec/filters/csv.rb b/spec/filters/csv.rb
deleted file mode 100644
index ad44a4bcc24..00000000000
--- a/spec/filters/csv.rb
+++ /dev/null
@@ -1,176 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/csv"
-
-describe LogStash::Filters::CSV do
-  extend LogStash::RSpec
-
-  describe "all defaults" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        csv { }
-      }
-    CONFIG
-
-    sample "big,bird,sesame street" do
-      insist { subject["column1"] } == "big"
-      insist { subject["column2"] } == "bird"
-      insist { subject["column3"] } == "sesame street"
-    end
-  end
-
-  describe "custom separator" do
-    config <<-CONFIG
-      filter {
-        csv {
-          separator => ";"
-        }
-      }
-    CONFIG
-
-    sample "big,bird;sesame street" do
-      insist { subject["column1"] } == "big,bird"
-      insist { subject["column2"] } == "sesame street"
-    end
-  end
-
-  describe "custom quote char" do
-    config <<-CONFIG
-      filter {
-        csv {
-          quote_char => "'"
-        }
-      }
-    CONFIG
-
-    sample "big,bird,'sesame street'" do
-      insist { subject["column1"] } == "big"
-      insist { subject["column2"] } == "bird"
-      insist { subject["column3"] } == "sesame street"
-    end
-  end
-
-  describe "default quote char" do
-    config <<-CONFIG
-      filter {
-        csv {
-        }
-      }
-    CONFIG
-
-    sample 'big,bird,"sesame, street"' do
-      insist { subject["column1"] } == "big"
-      insist { subject["column2"] } == "bird"
-      insist { subject["column3"] } == "sesame, street"
-    end
-  end
-  describe "null quote char" do
-    config <<-CONFIG
-      filter {
-        csv {
-          quote_char => "\x00"
-        }
-      }
-    CONFIG
-
-    sample 'big,bird,"sesame" street' do
-      insist { subject["column1"] } == 'big'
-      insist { subject["column2"] } == 'bird'
-      insist { subject["column3"] } == '"sesame" street'
-    end
-  end
-
-  describe "given columns" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        csv {
-          columns => ["first", "last", "address" ]
-        }
-      }
-    CONFIG
-
-    sample "big,bird,sesame street" do
-      insist { subject["first"] } == "big"
-      insist { subject["last"] } == "bird"
-      insist { subject["address"] } == "sesame street"
-    end
-  end
-
-  describe "parse csv with more data than defined column names" do
-    config <<-CONFIG
-      filter {
-        csv {
-          columns => ["custom1", "custom2"]
-        }
-      }
-    CONFIG
-
-    sample "val1,val2,val3" do
-      insist { subject["custom1"] } == "val1"
-      insist { subject["custom2"] } == "val2"
-      insist { subject["column3"] } == "val3"
-    end
-  end
-
-
-  describe "parse csv from a given source with column names" do
-    config <<-CONFIG
-      filter {
-        csv {
-          source => "datafield"
-          columns => ["custom1", "custom2", "custom3"]
-        }
-      }
-    CONFIG
-
-    sample("datafield" => "val1,val2,val3") do
-      insist { subject["custom1"] } == "val1"
-      insist { subject["custom2"] } == "val2"
-      insist { subject["custom3"] } == "val3"
-    end
-  end
-
-  describe "given target" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        csv {
-          target => "data"
-        }
-      }
-    CONFIG
-
-    sample "big,bird,sesame street" do
-      insist { subject["data"]["column1"] } == "big"
-      insist { subject["data"]["column2"] } == "bird"
-      insist { subject["data"]["column3"] } == "sesame street"
-    end
-  end
-
-  describe "given target and source" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        csv {
-          source => "datain"
-          target => "data"
-        }
-      }
-    CONFIG
-
-    sample("datain" => "big,bird,sesame street") do
-      insist { subject["data"]["column1"] } == "big"
-      insist { subject["data"]["column2"] } == "bird"
-      insist { subject["data"]["column3"] } == "sesame street"
-    end
-  end
-
-
-end
diff --git a/spec/filters/date.rb b/spec/filters/date.rb
deleted file mode 100644
index dc27d39d6e8..00000000000
--- a/spec/filters/date.rb
+++ /dev/null
@@ -1,330 +0,0 @@
-require "test_utils"
-require "logstash/filters/date"
-
-puts "Skipping date performance tests because this ruby is not jruby" if RUBY_ENGINE != "jruby"
-RUBY_ENGINE == "jruby" and describe LogStash::Filters::Date do
-  extend LogStash::RSpec
-
-  describe "giving an invalid match config, raise a configuration error" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "mydate"]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    sample "not_really_important" do
-      insist {subject}.raises LogStash::ConfigurationError
-    end
-
-  end
-
-  describe "parsing with ISO8601" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "mydate", "ISO8601" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    times = {
-      "2001-01-01T00:00:00-0800"         => "2001-01-01T08:00:00.000Z",
-      "1974-03-02T04:09:09-0800"         => "1974-03-02T12:09:09.000Z",
-      "2010-05-03T08:18:18+00:00"        => "2010-05-03T08:18:18.000Z",
-      "2004-07-04T12:27:27-00:00"        => "2004-07-04T12:27:27.000Z",
-      "2001-09-05T16:36:36+0000"         => "2001-09-05T16:36:36.000Z",
-      "2001-11-06T20:45:45-0000"         => "2001-11-06T20:45:45.000Z",
-      "2001-12-07T23:54:54Z"             => "2001-12-07T23:54:54.000Z",
-
-      # TODO: This test assumes PDT
-      #"2001-01-01T00:00:00.123"          => "2001-01-01T08:00:00.123Z",
-
-      "2010-05-03T08:18:18.123+00:00"    => "2010-05-03T08:18:18.123Z",
-      "2004-07-04T12:27:27.123-04:00"    => "2004-07-04T16:27:27.123Z",
-      "2001-09-05T16:36:36.123+0700"     => "2001-09-05T09:36:36.123Z",
-      "2001-11-06T20:45:45.123-0000"     => "2001-11-06T20:45:45.123Z",
-      "2001-12-07T23:54:54.123Z"         => "2001-12-07T23:54:54.123Z",
-    }
-
-    times.each do |input, output|
-      sample("mydate" => input) do
-        begin 
-          insist { subject["mydate"] } == input
-          insist { subject["@timestamp"] } == Time.iso8601(output).utc
-        rescue
-          #require "pry"; binding.pry
-          raise
-        end
-      end
-    end # times.each
-  end
-
-  describe "parsing with java SimpleDateFormat syntax" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "mydate", "MMM dd HH:mm:ss Z" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    now = Time.now
-    year = now.year
-    require 'java'
-
-    times = {
-      "Nov 24 01:29:01 -0800" => "#{year}-11-24T09:29:01.000Z",
-    }
-    times.each do |input, output|
-      sample("mydate" => input) do
-        insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output).utc
-      end
-    end # times.each
-  end
-
-  describe "parsing with UNIX" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "mydate", "UNIX" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    times = {
-      "0"          => "1970-01-01T00:00:00.000Z",
-      "1000000000" => "2001-09-09T01:46:40.000Z",
-
-      # LOGSTASH-279 - sometimes the field is a number.
-      0          => "1970-01-01T00:00:00.000Z",
-      1000000000 => "2001-09-09T01:46:40.000Z"
-    }
-    times.each do |input, output|
-      sample("mydate" => input) do
-        insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output).utc
-      end
-    end # times.each
-  end
-
-  describe "parsing microsecond-precise times with UNIX (#213)" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "mydate", "UNIX" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    sample("mydate" => "1350414944.123456") do
-      # Joda time only supports milliseconds :\
-      insist { subject.timestamp } == Time.iso8601("2012-10-16T12:15:44.123-07:00").utc
-    end
-  end
-
-  describe "parsing with UNIX_MS" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "mydate", "UNIX_MS" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    times = {
-      "0"          => "1970-01-01T00:00:00.000Z",
-      "456"          => "1970-01-01T00:00:00.456Z",
-      "1000000000123" => "2001-09-09T01:46:40.123Z",
-
-      # LOGSTASH-279 - sometimes the field is a number.
-      0          => "1970-01-01T00:00:00.000Z",
-      456          => "1970-01-01T00:00:00.456Z",
-      1000000000123 => "2001-09-09T01:46:40.123Z"
-    }
-    times.each do |input, output|
-      sample("mydate" => input) do
-        insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output)
-      end
-    end # times.each
-  end
-
-  describe "failed parses should not cause a failure (LOGSTASH-641)" do
-    config <<-'CONFIG'
-      input {
-        generator {
-          lines => [
-            '{ "mydate": "this will not parse" }',
-            '{ }'
-          ]
-          codec => json
-          type => foo
-          count => 1
-        }
-      }
-      filter {
-        date {
-          match => [ "mydate", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
-          locale => "en"
-        }
-      }
-      output { 
-        null { }
-      }
-    CONFIG
-
-    agent do
-      # nothing to do, if this crashes it's an error..
-    end
-  end
-
-  describe "TAI64N support" do
-    config <<-'CONFIG'
-      filter {
-        date {
-          match => [ "t",  TAI64N ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    # Try without leading "@"
-    sample("t" => "4000000050d506482dbdf024") do
-      insist { subject.timestamp } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
-    end
-
-    # Should still parse successfully if it's a full tai64n time (with leading
-    # '@')
-    sample("t" => "@4000000050d506482dbdf024") do
-      insist { subject.timestamp } == Time.iso8601("2012-12-22T01:00:46.767Z").utc
-    end
-  end
-
-  describe "accept match config option with hash value (LOGSTASH-735)" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "mydate", "ISO8601" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    time = "2001-09-09T01:46:40.000Z"
-
-    sample("mydate" => time) do
-      insist { subject["mydate"] } == time
-      insist { subject["@timestamp"] } == Time.iso8601(time).utc
-    end
-  end
-  
-  describe "support deep nested field access" do
-    config <<-CONFIG
-      filter { 
-        date {
-          match => [ "[data][deep]", "ISO8601" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-    
-    sample("data" => { "deep" => "2013-01-01T00:00:00.000Z" }) do
-      insist { subject["@timestamp"] } == Time.iso8601("2013-01-01T00:00:00.000Z").utc
-    end
-  end
-
-  describe "failing to parse should not throw an exception" do
-    config <<-CONFIG
-      filter { 
-        date {
-          match => [ "thedate", "yyyy/MM/dd" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    sample("thedate" => "2013/Apr/21") do
-      insist { subject["@timestamp"] } != "2013-04-21T00:00:00.000Z"
-    end
-  end
-
-   describe "success to parse should apply on_success config(add_tag,add_field...)" do
-    config <<-CONFIG
-      filter { 
-        date {
-          match => [ "thedate", "yyyy/MM/dd" ]
-          add_tag => "tagged"
-        }
-      }
-    CONFIG
-
-    sample("thedate" => "2013/04/21") do
-      insist { subject["@timestamp"] } != "2013-04-21T00:00:00.000Z"
-      insist { subject["tags"] } == ["tagged"]
-    end
-  end
-
-   describe "failing to parse should not apply on_success config(add_tag,add_field...)" do
-    config <<-CONFIG
-      filter { 
-        date {
-          match => [ "thedate", "yyyy/MM/dd" ]
-          add_tag => "tagged"
-        }
-      }
-    CONFIG
-
-    sample("thedate" => "2013/Apr/21") do
-      insist { subject["@timestamp"] } != "2013-04-21T00:00:00.000Z"
-      insist { subject["tags"] } == nil
-    end
-  end
-
-  describe "parsing with timezone parameter" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => ["mydate", "yyyy MMM dd HH:mm:ss"]
-          locale => "en"
-          timezone => "America/Los_Angeles"
-        }
-      }
-    CONFIG
-
-    require 'java'
-    times = {
-      "2013 Nov 24 01:29:01" => "2013-11-24T09:29:01.000Z",
-      "2013 Jun 24 01:29:01" => "2013-06-24T08:29:01.000Z",
-    }
-    times.each do |input, output|
-      sample("mydate" => input) do
-        insist { subject["mydate"] } == input
-        insist { subject["@timestamp"] } == Time.iso8601(output).utc
-      end
-    end # times.each
-  end
-
-  describe "LOGSTASH-34 - Default year should be this year" do
-    config <<-CONFIG
-      filter {
-        date {
-          match => [ "message", "EEE MMM dd HH:mm:ss" ]
-          locale => "en"
-        }
-      }
-    CONFIG
-
-    sample "Sun Jun 02 20:38:03" do
-      insist { subject["@timestamp"].year } == Time.now.year
-    end
-  end
-end
diff --git a/spec/filters/date_performance.rb b/spec/filters/date_performance.rb
deleted file mode 100644
index 0c30b59dd84..00000000000
--- a/spec/filters/date_performance.rb
+++ /dev/null
@@ -1,31 +0,0 @@
-require "test_utils"
-require "logstash/filters/date"
-
-puts "Skipping date tests because this ruby is not jruby" if RUBY_ENGINE != "jruby"
-describe LogStash::Filters::Date, :if => RUBY_ENGINE == "jruby" do
-  extend LogStash::RSpec
-
-  describe "speed test of date parsing", :performance => true do
-    it "should be fast" do
-      event_count = 100000
-      min_rate = 4000
-      max_duration = event_count / min_rate
-      input = "Nov 24 01:29:01 -0800"
-
-      filter = LogStash::Filters::Date.new("match" => [ "mydate", "MMM dd HH:mm:ss Z" ])
-      filter.register
-      duration = 0
-      # 10000 for warmup
-      [10000, event_count].each do |iterations|
-        start = Time.now
-        iterations.times do
-          event = LogStash::Event.new("mydate" => input)
-          filter.execute(event)
-        end
-        duration = Time.now - start
-      end
-      puts "filters/date parse rate: #{"%02.0f/sec" % (event_count / duration)}, elapsed: #{duration}s"
-      insist { duration } < max_duration
-    end
-  end
-end
diff --git a/spec/filters/dns.rb b/spec/filters/dns.rb
deleted file mode 100644
index 80dae9885ad..00000000000
--- a/spec/filters/dns.rb
+++ /dev/null
@@ -1,161 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/dns"
-require "resolv"
-
-describe LogStash::Filters::DNS do
-  extend LogStash::RSpec
-
-  before(:all) do
-    begin
-      Resolv.new.getaddress("elasticsearch.com")
-    rescue Errno::ENOENT
-      $stderr.puts("DNS resolver error, no network? mocking resolver")
-      @mock_resolv = true
-    end
-  end
-
-  before(:each) do
-    if @mock_resolv
-      allow_any_instance_of(Resolv).to receive(:getaddress).with("carrera.databits.net").and_return("199.192.228.250")
-      allow_any_instance_of(Resolv).to receive(:getaddress).with("does.not.exist").and_return(nil)
-      allow_any_instance_of(Resolv).to receive(:getname).with("199.192.228.250").and_return("carrera.databits.net")
-    end
-  end
-
-  describe "dns reverse lookup, replace (on a field)" do
-    config <<-CONFIG
-      filter {
-        dns {
-          reverse => "foo"
-          action => "replace"
-        }
-      }
-    CONFIG
-
-    sample("foo" => "199.192.228.250") do
-      insist { subject["foo"] } == "carrera.databits.net"
-    end
-  end
-
-  describe "dns reverse lookup, append" do
-    config <<-CONFIG
-      filter {
-        dns {
-          reverse => "foo"
-          action => "append"
-        }
-      }
-    CONFIG
-
-    sample("foo" => "199.192.228.250") do
-      insist { subject["foo"][0] } == "199.192.228.250"
-      insist { subject["foo"][1] } == "carrera.databits.net"
-    end
-  end
-
-  describe "dns reverse lookup, not an IP" do
-    config <<-CONFIG
-      filter {
-        dns {
-          reverse => "foo"
-        }
-      }
-    CONFIG
-
-    sample("foo" => "not.an.ip") do
-      insist { subject["foo"] } == "not.an.ip"
-    end
-  end
-
-  describe "dns resolve lookup, replace" do
-    config <<-CONFIG
-      filter {
-        dns {
-          resolve => "host"
-          action => "replace"
-        }
-      }
-    CONFIG
-
-    sample("host" => "carrera.databits.net") do
-      insist { subject["host"] } == "199.192.228.250"
-    end
-  end
-
-  describe "dns resolve lookup, replace (on a field)" do
-    config <<-CONFIG
-      filter {
-        dns {
-          resolve => "foo"
-          action => "replace"
-        }
-      }
-    CONFIG
-
-    sample("foo" => "carrera.databits.net") do
-      insist { subject["foo"] } == "199.192.228.250"
-    end
-  end
-
-  describe "dns resolve lookup, skip multi-value" do
-    config <<-CONFIG
-      filter {
-        dns {
-          resolve => "foo"
-          action => "replace"
-        }
-      }
-    CONFIG
-
-    sample("foo" => ["carrera.databits.net", "foo.databits.net"]) do
-      insist { subject["foo"] } == ["carrera.databits.net", "foo.databits.net"]
-    end
-  end
-
-  describe "dns resolve lookup, append" do
-    config <<-CONFIG
-      filter {
-        dns {
-          resolve => "foo"
-          action => "append"
-        }
-      }
-    CONFIG
-
-    sample("foo" => "carrera.databits.net") do
-      insist { subject["foo"][0] } == "carrera.databits.net"
-      insist { subject["foo"][1] } == "199.192.228.250"
-    end
-  end
-
-  describe "dns resolve lookup, append with multi-value does nothing" do
-    config <<-CONFIG
-      filter {
-        dns {
-          resolve => "foo"
-          action => "append"
-        }
-      }
-    CONFIG
-
-    sample("foo" => ["carrera.databits.net", "foo.databits.net"]) do
-      insist { subject["foo"] } == ["carrera.databits.net", "foo.databits.net"]
-    end
-  end
-
-  describe "dns resolve lookup, not a valid hostname" do
-    config <<-CONFIG
-      filter {
-        dns {
-          resolve=> "foo"
-        }
-      }
-    CONFIG
-
-    sample("foo" => "does.not.exist") do
-      insist { subject["foo"] } == "does.not.exist"
-    end
-  end
-end
diff --git a/spec/filters/drop.rb b/spec/filters/drop.rb
deleted file mode 100644
index 94fa78b8c77..00000000000
--- a/spec/filters/drop.rb
+++ /dev/null
@@ -1,19 +0,0 @@
-require "test_utils"
-require "logstash/filters/drop"
-
-describe LogStash::Filters::Drop do
-  extend LogStash::RSpec
-
-  describe "drop the event" do
-    config <<-CONFIG
-      filter {
-        drop { }
-      }
-    CONFIG
-
-    sample "hello" do
-      insist { subject }.nil?
-    end
-  end
-
-end
diff --git a/spec/filters/fingerprint.rb b/spec/filters/fingerprint.rb
deleted file mode 100644
index 22fe49543f6..00000000000
--- a/spec/filters/fingerprint.rb
+++ /dev/null
@@ -1,167 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/fingerprint"
-
-describe LogStash::Filters::Fingerprint do
-  extend LogStash::RSpec
-
-  describe "fingerprint ipaddress with IPV4_NETWORK method" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          method => "IPV4_NETWORK"
-          key => 24
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "233.255.13.44") do
-      insist { subject["fingerprint"] } == "233.255.13.0"
-    end
-  end
-
-  describe "fingerprint string with MURMUR3 method" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          method => "MURMUR3"
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.52.122.33") do
-      insist { subject["fingerprint"] } == 1541804874
-    end
-  end
-
-   describe "fingerprint string with SHA1 alogrithm" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          key => "longencryptionkey"
-          method => 'SHA1'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["fingerprint"] } == "fdc60acc4773dc5ac569ffb78fcb93c9630797f4"
-    end
-  end
-
-  describe "fingerprint string with SHA256 alogrithm" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          key => "longencryptionkey"
-          method => 'SHA256'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["fingerprint"] } == "345bec3eff242d53b568916c2610b3e393d885d6b96d643f38494fd74bf4a9ca"
-    end
-  end
-
-  describe "fingerprint string with SHA384 alogrithm" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          key => "longencryptionkey"
-          method => 'SHA384'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["fingerprint"] } == "22d4c0e8c4fbcdc4887d2038fca7650f0e2e0e2457ff41c06eb2a980dded6749561c814fe182aff93e2538d18593947a"
-    end
-  end
-
-  describe "fingerprint string with SHA512 alogrithm" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          key => "longencryptionkey"
-          method => 'SHA512'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["fingerprint"] } == "11c19b326936c08d6c50a3c847d883e5a1362e6a64dd55201a25f2c1ac1b673f7d8bf15b8f112a4978276d573275e3b14166e17246f670c2a539401c5bfdace8"
-    end
-  end
-
-  describe "fingerprint string with MD5 alogrithm" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          key => "longencryptionkey"
-          method => 'MD5'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => "123.123.123.123") do
-      insist { subject["fingerprint"] } == "9336c879e305c9604a3843fc3e75948f"
-    end
-  end
-
-  describe "Test field with multiple values" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ["clientip"]
-          key => "longencryptionkey"
-          method => 'MD5'
-        }
-      }
-    CONFIG
-
-    sample("clientip" => [ "123.123.123.123", "223.223.223.223" ]) do
-      insist { subject["fingerprint"]} == [ "9336c879e305c9604a3843fc3e75948f", "7a6c66b8d3f42a7d650e3354af508df3" ]
-    end
-  end
-
-  describe "Concatenate multiple values into 1" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => ['field1', 'field2']
-          key => "longencryptionkey"
-          method => 'MD5'
-        }
-      }
-    CONFIG
-
-    sample("field1" => "test1", "field2" => "test2") do
-      insist { subject["fingerprint"]} == "872da745e45192c2a1d4bf7c1ff8a370"
-    end
-  end
-
-  describe "PUNCTUATION method" do
-    config <<-CONFIG
-      filter {
-        fingerprint {
-          source => 'field1'
-          method => 'PUNCTUATION'
-        }
-      }
-    CONFIG
-
-    sample("field1" =>  "PHP Warning:  json_encode() [<a href='function.json-encode'>function.json-encode</a>]: Invalid UTF-8 sequence in argument in /var/www/htdocs/test.php on line 233") do
-      insist { subject["fingerprint"] } == ":_()[<='.-'>.-</>]:-////."
-    end
-  end
-
-end
diff --git a/spec/filters/geoip.rb b/spec/filters/geoip.rb
deleted file mode 100644
index b1864de6e3e..00000000000
--- a/spec/filters/geoip.rb
+++ /dev/null
@@ -1,120 +0,0 @@
-require "test_utils"
-require "logstash/filters/geoip"
-
-describe LogStash::Filters::GeoIP do
-  extend LogStash::RSpec
-  describe "defaults" do
-    config <<-CONFIG
-      filter {
-        geoip { 
-          source => "ip"
-          #database => "vendor/geoip/GeoLiteCity.dat"
-        }
-      }
-    CONFIG
-
-    sample("ip" => "8.8.8.8") do
-      insist { subject }.include?("geoip")
-
-      expected_fields = %w(ip country_code2 country_code3 country_name
-                           continent_code region_name city_name postal_code
-                           latitude longitude dma_code area_code timezone
-                           location )
-      expected_fields.each do |f|
-        insist { subject["geoip"] }.include?(f)
-      end
-    end
-
-    sample("ip" => "127.0.0.1") do
-      # assume geoip fails on localhost lookups
-      reject { subject }.include?("geoip")
-    end
-  end
-
-  describe "Specify the target" do
-    config <<-CONFIG
-      filter {
-        geoip { 
-          source => "ip"
-          #database => "vendor/geoip/GeoLiteCity.dat"
-          target => src_ip
-        }
-      }
-    CONFIG
-
-    sample("ip" => "8.8.8.8") do
-      insist { subject }.include?("src_ip")
-
-      expected_fields = %w(ip country_code2 country_code3 country_name
-                           continent_code region_name city_name postal_code
-                           latitude longitude dma_code area_code timezone
-                           location )
-      expected_fields.each do |f|
-        insist { subject["src_ip"] }.include?(f)
-      end
-    end
-
-    sample("ip" => "127.0.0.1") do
-      # assume geoip fails on localhost lookups
-      reject { subject }.include?("src_ip")
-    end
-  end
-
-  describe "correct encodings with default db" do
-    config <<-CONFIG
-      filter {
-        geoip {
-          source => "ip"
-        }
-      }
-    CONFIG
-    expected_fields = %w(ip country_code2 country_code3 country_name
-                           continent_code region_name city_name postal_code
-                           dma_code area_code timezone)
-
-    sample("ip" => "1.1.1.1") do
-      checked = 0
-      expected_fields.each do |f|
-        next unless subject["geoip"][f]
-        checked += 1
-        insist { subject["geoip"][f].encoding } == Encoding::UTF_8
-      end
-      insist { checked } > 0
-    end
-    sample("ip" => "189.2.0.0") do
-      checked = 0
-      expected_fields.each do |f|
-        next unless subject["geoip"][f]
-        checked += 1
-        insist { subject["geoip"][f].encoding } == Encoding::UTF_8
-      end
-      insist { checked } > 0
-    end
-
-  end
-
-  describe "correct encodings with ASN db" do
-    config <<-CONFIG
-      filter {
-        geoip {
-          source => "ip"
-          database => "vendor/geoip/GeoIPASNum.dat"
-        }
-      }
-    CONFIG
-
-
-    sample("ip" => "1.1.1.1") do
-      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
-    end
-    sample("ip" => "187.2.0.0") do
-      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
-    end
-    sample("ip" => "189.2.0.0") do
-      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
-    end
-    sample("ip" => "161.24.0.0") do
-      insist { subject["geoip"]["asn"].encoding } == Encoding::UTF_8
-    end
-  end
-end
diff --git a/spec/filters/grok.rb b/spec/filters/grok.rb
deleted file mode 100644
index 94dbbfaac9b..00000000000
--- a/spec/filters/grok.rb
+++ /dev/null
@@ -1,503 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/grok"
-
-describe LogStash::Filters::Grok do
-  extend LogStash::RSpec
-
-  describe "simple syslog line" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message", "%{SYSLOGLINE}" ]
-          singles => true
-          overwrite => [ "message" ]
-        }
-      }
-    CONFIG
-
-    sample "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]" do
-      insist { subject["tags"] }.nil?
-      insist { subject["logsource"] } == "evita"
-      insist { subject["timestamp"] } == "Mar 16 00:01:25"
-      insist { subject["message"] } == "connect from camomile.cloud9.net[168.100.1.3]"
-      insist { subject["program"] } == "postfix/smtpd"
-      insist { subject["pid"] } == "1713"
-    end
-  end
-
-  describe "ietf 5424 syslog line" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "%{SYSLOG5424LINE}" ]
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - [id1 foo=\"bar\"][id2 baz=\"something\"] Hello, syslog." do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "191"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
-      insist { subject["syslog5424_host"] } == "paxton.local"
-      insist { subject["syslog5424_app"] } == "grokdebug"
-      insist { subject["syslog5424_proc"] } == "4123"
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == "[id1 foo=\"bar\"][id2 baz=\"something\"]"
-      insist { subject["syslog5424_msg"] } == "Hello, syslog."
-    end
-
-    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug - - [id1 foo=\"bar\"] No process ID." do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "191"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
-      insist { subject["syslog5424_host"] } == "paxton.local"
-      insist { subject["syslog5424_app"] } == "grokdebug"
-      insist { subject["syslog5424_proc"] } == nil
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == "[id1 foo=\"bar\"]"
-      insist { subject["syslog5424_msg"] } == "No process ID."
-    end
-
-    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 - - No structured data." do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "191"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
-      insist { subject["syslog5424_host"] } == "paxton.local"
-      insist { subject["syslog5424_app"] } == "grokdebug"
-      insist { subject["syslog5424_proc"] } == "4123"
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == nil
-      insist { subject["syslog5424_msg"] } == "No structured data."
-    end
-
-    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug - - - No PID or SD." do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "191"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
-      insist { subject["syslog5424_host"] } == "paxton.local"
-      insist { subject["syslog5424_app"] } == "grokdebug"
-      insist { subject["syslog5424_proc"] } == nil
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == nil
-      insist { subject["syslog5424_msg"] } == "No PID or SD."
-    end
-
-    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug 4123 -  Missing structured data." do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "191"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
-      insist { subject["syslog5424_host"] } == "paxton.local"
-      insist { subject["syslog5424_app"] } == "grokdebug"
-      insist { subject["syslog5424_proc"] } == "4123"
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == nil
-      insist { subject["syslog5424_msg"] } == "Missing structured data."
-    end
-
-    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug  4123 - - Additional spaces." do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "191"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
-      insist { subject["syslog5424_host"] } == "paxton.local"
-      insist { subject["syslog5424_app"] } == "grokdebug"
-      insist { subject["syslog5424_proc"] } == "4123"
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == nil
-      insist { subject["syslog5424_msg"] } == "Additional spaces."
-    end
-
-    sample "<191>1 2009-06-30T18:30:00+02:00 paxton.local grokdebug  4123 -  Additional spaces and missing SD." do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "191"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2009-06-30T18:30:00+02:00"
-      insist { subject["syslog5424_host"] } == "paxton.local"
-      insist { subject["syslog5424_app"] } == "grokdebug"
-      insist { subject["syslog5424_proc"] } == "4123"
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == nil
-      insist { subject["syslog5424_msg"] } == "Additional spaces and missing SD."
-    end
-
-    sample "<30>1 2014-04-04T16:44:07+02:00 osctrl01 dnsmasq-dhcp 8048 - -  Appname contains a dash" do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "30"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2014-04-04T16:44:07+02:00"
-      insist { subject["syslog5424_host"] } == "osctrl01"
-      insist { subject["syslog5424_app"] } == "dnsmasq-dhcp"
-      insist { subject["syslog5424_proc"] } == "8048"
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == nil
-      insist { subject["syslog5424_msg"] } == "Appname contains a dash"
-    end
-
-    sample "<30>1 2014-04-04T16:44:07+02:00 osctrl01 - 8048 - -  Appname is nil" do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog5424_pri"] } == "30"
-      insist { subject["syslog5424_ver"] } == "1"
-      insist { subject["syslog5424_ts"] } == "2014-04-04T16:44:07+02:00"
-      insist { subject["syslog5424_host"] } == "osctrl01"
-      insist { subject["syslog5424_app"] } == nil
-      insist { subject["syslog5424_proc"] } == "8048"
-      insist { subject["syslog5424_msgid"] } == nil
-      insist { subject["syslog5424_sd"] } == nil
-      insist { subject["syslog5424_msg"] } == "Appname is nil"
-    end
-  end
-
-  describe "parsing an event with multiple messages (array of strings)", :if => false do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "(?:hello|world) %{NUMBER}" ]
-          named_captures_only => false
-        }
-      }
-    CONFIG
-
-    sample("message" => [ "hello 12345", "world 23456" ]) do
-      insist { subject["NUMBER"] } == [ "12345", "23456" ]
-    end
-  end
-
-  describe "coercing matched values" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "%{NUMBER:foo:int} %{NUMBER:bar:float}" ]
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample "400 454.33" do
-      insist { subject["foo"] } == 400
-      insist { subject["foo"] }.is_a?(Fixnum)
-      insist { subject["bar"] } == 454.33
-      insist { subject["bar"] }.is_a?(Float)
-    end
-  end
-
-  describe "in-line pattern definitions" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "%{FIZZLE=\\d+}" ]
-          named_captures_only => false
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample "hello 1234" do
-      insist { subject["FIZZLE"] } == "1234"
-    end
-  end
-
-  describe "processing selected fields" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "%{WORD:word}" ]
-          match => [ "examplefield", "%{NUMBER:num}" ]
-          break_on_match => false
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample("message" => "hello world", "examplefield" => "12345") do
-      insist { subject["examplefield"] } == "12345"
-      insist { subject["word"] } == "hello"
-    end
-  end
-
-  describe "adding fields on match" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "matchme %{NUMBER:fancy}" ]
-          singles => true
-          add_field => [ "new_field", "%{fancy}" ]
-        }
-      }
-    CONFIG
-
-    sample "matchme 1234" do
-      insist { subject["tags"] }.nil?
-      insist { subject["new_field"] } == "1234"
-    end
-
-    sample "this will not be matched" do
-      insist { subject["tags"] }.include?("_grokparsefailure")
-      reject { subject }.include?("new_field")
-    end
-  end
-
-  context "empty fields" do
-    describe "drop by default" do
-      config <<-CONFIG
-        filter {
-          grok {
-            match => [ "message",  "1=%{WORD:foo1} *(2=%{WORD:foo2})?" ]
-          }
-        }
-      CONFIG
-
-      sample "1=test" do
-        insist { subject["tags"] }.nil?
-        insist { subject }.include?("foo1")
-
-        # Since 'foo2' was not captured, it must not be present in the event.
-        reject { subject }.include?("foo2")
-      end
-    end
-
-    describe "keep if keep_empty_captures is true" do
-      config <<-CONFIG
-        filter {
-          grok {
-            match => [ "message",  "1=%{WORD:foo1} *(2=%{WORD:foo2})?" ]
-            keep_empty_captures => true
-          }
-        }
-      CONFIG
-
-      sample "1=test" do
-        insist { subject["tags"] }.nil?
-        # use .to_hash for this test, for now, because right now
-        # the Event.include? returns false for missing fields as well
-        # as for fields with nil values.
-        insist { subject.to_hash }.include?("foo2")
-        insist { subject.to_hash }.include?("foo2")
-      end
-    end
-  end
-
-  describe "when named_captures_only == false" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "Hello %{WORD}. %{WORD:foo}" ]
-          named_captures_only => false
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample "Hello World, yo!" do
-      insist { subject }.include?("WORD")
-      insist { subject["WORD"] } == "World"
-      insist { subject }.include?("foo")
-      insist { subject["foo"] } == "yo"
-    end
-  end
-
-  describe "using oniguruma named captures (?<name>regex)" do
-    context "plain regexp" do
-      config <<-'CONFIG'
-        filter {
-          grok {
-            singles => true
-            match => [ "message",  "(?<foo>\w+)" ]
-          }
-        }
-      CONFIG
-      sample "hello world" do
-        insist { subject["tags"] }.nil?
-        insist { subject["foo"] } == "hello"
-      end
-    end
-
-    context "grok patterns" do
-      config <<-'CONFIG'
-        filter {
-          grok {
-            singles => true
-            match => [ "message",  "(?<timestamp>%{DATE_EU} %{TIME})" ]
-          }
-        }
-      CONFIG
-
-      sample "fancy 12-12-12 12:12:12" do
-        insist { subject["tags"] }.nil?
-        insist { subject["timestamp"] } == "12-12-12 12:12:12"
-      end
-    end
-  end
-
-  describe "grok on integer types" do
-    config <<-'CONFIG'
-      filter {
-        grok {
-          match => [ "status", "^403$" ]
-          add_tag => "four_oh_three"
-        }
-      }
-    CONFIG
-
-    sample("status" => 403) do
-      reject { subject["tags"] }.include?("_grokparsefailure")
-      insist { subject["tags"] }.include?("four_oh_three")
-    end
-  end
-
-  describe "grok on float types" do
-    config <<-'CONFIG'
-      filter {
-        grok {
-          match => [ "version", "^1.0$" ]
-          add_tag => "one_point_oh"
-        }
-      }
-    CONFIG
-
-    sample("version" => 1.0) do
-      insist { subject["tags"] }.include?("one_point_oh")
-      insist { subject["tags"] }.include?("one_point_oh")
-    end
-  end
-
-  describe "grok on %{LOGLEVEL}" do
-    config <<-'CONFIG'
-      filter {
-        grok {
-          pattern => "%{LOGLEVEL:level}: error!"
-        }
-      }
-    CONFIG
-
-    log_level_names = %w(
-      trace Trace TRACE
-      debug Debug DEBUG
-      notice Notice Notice
-      info Info INFO
-      warn warning Warn Warning WARN WARNING
-      err error Err Error ERR ERROR
-      crit critical Crit Critical CRIT CRITICAL
-      fatal Fatal FATAL
-      severe Severe SEVERE
-      emerg emergency Emerg Emergency EMERG EMERGENCY
-    )
-    log_level_names.each do |level_name|
-      sample "#{level_name}: error!" do
-        insist { subject['level'] } == level_name
-      end
-    end
-  end
-
-  describe "tagging on failure" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "matchme %{NUMBER:fancy}" ]
-          tag_on_failure => false
-        }
-      }
-    CONFIG
-
-    sample "matchme 1234" do
-      insist { subject["tags"] }.nil?
-    end
-
-    sample "this will not be matched" do
-      insist { subject["tags"] }.include?("false")
-    end
-  end
-
-  describe "captures named fields even if the whole text matches" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "%{DATE_EU:stimestamp}" ]
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample "11/01/01" do
-      insist { subject["stimestamp"] } == "11/01/01"
-    end
-  end
-
-  describe "allow dashes in capture names" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message",  "%{WORD:foo-bar}" ]
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample "hello world" do
-      insist { subject["foo-bar"] } == "hello"
-    end
-  end
-
-  describe "performance test", :performance => true do
-    event_count = 100000
-    min_rate = 2000
-
-    max_duration = event_count / min_rate
-    input = "Nov 24 01:29:01 -0800"
-    config <<-CONFIG
-      input {
-        generator {
-          count => #{event_count}
-          message => "Mar 16 00:01:25 evita postfix/smtpd[1713]: connect from camomile.cloud9.net[168.100.1.3]"
-        }
-      }
-      filter {
-        grok {
-          match => [ "message", "%{SYSLOGLINE}" ]
-          singles => true
-          overwrite => [ "message" ]
-        }
-      }
-      output { null { } }
-    CONFIG
-
-    2.times do
-      start = Time.now
-      agent do
-        duration = (Time.now - start)
-        puts "filters/grok parse rate: #{"%02.0f/sec" % (event_count / duration)}, elapsed: #{duration}s"
-        insist { duration } < max_duration
-      end
-    end
-  end
-
-  describe "singles with duplicate-named fields" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message", "%{INT:foo}|%{WORD:foo}" ]
-          singles => true
-        }
-      }
-    CONFIG
-
-    sample "hello world" do
-      insist { subject["foo"] }.is_a?(String)
-    end
-
-    sample "123 world" do
-      insist { subject["foo"] }.is_a?(String)
-    end
-  end
-end
diff --git a/spec/filters/grok/timeout2.rb b/spec/filters/grok/timeout2.rb
deleted file mode 100644
index e4237e3648b..00000000000
--- a/spec/filters/grok/timeout2.rb
+++ /dev/null
@@ -1,59 +0,0 @@
-require "test_utils"
-require "grok-pure"
-require "timeout"
-
-describe "grok known timeout failures" do
-  extend LogStash::RSpec
-
-  describe "user reported timeout" do
-    config <<-'CONFIG'
-      filter {
-        grok {
-         match  => [ "message", "%{SYSLOGBASE:ts1} \[\#\|%{TIMESTAMP_ISO8601:ts2}\|%{DATA} for %{PATH:url} = %{POSINT:delay} ms.%{GREEDYDATA}" ]
-        }
-      }
-    CONFIG
-
-    start = Time.now
-    line = 'Nov 13 19:23:34 qa-api1 glassfish: [#|2012-11-13T19:23:25.604+0000|INFO|glassfish3.1.2|com.locusenergy.platform.messages.LocusMessage|_ThreadID=59;_ThreadName=Thread-2;|API TIMER - Cache HIT user: null for /kiosks/194/energyreadings/data?tz=America/New_York&fields=kwh&type=gen&end=2012-11-13T23:59:59&start=2010-12-16T00:00:00-05:00&gran=yearly = 5 ms.|#]'
-
-    sample line do
-      duration = Time.now - start
-      # insist { duration } < 0.03  #TODO refactor performance tests
-    end
-  end
-
-  describe "user reported timeout" do
-    config <<-'CONFIG'
-      filter {
-        grok {
-          pattern => [
-            "%{DATA:http_host} %{IPORHOST:clientip} %{USER:ident} %{USER:http_auth} \[%{HTTPDATE:http_timestamp}\] \"%{WORD:http_method} %{DATA:http_request} HTTP/%{NUMBER:http_version}\" %{NUMBER:http_response_code} (?:%{NUMBER:bytes}|-) \"(?:%{URI:http_referrer}|-)\" %{QS:http_user_agent} %{QS:http_x_forwarded_for} %{USER:ssl_chiper} %{NUMBER:request_time} (?:%{DATA:gzip_ratio}|-) (?:%{DATA:upstream}|-) (?:%{NUMBER:upstream_time}|-) (?:%{WORD:geoip_country}|-)",
-            "%{DATA:http_host} %{IPORHOST:clientip} %{USER:ident} %{USER:http_auth} \[%{HTTPDATE:http_timestamp}\] \"%{WORD:http_method} %{DATA:http_request} HTTP/%{NUMBER:http_version}\" %{NUMBER:http_response_code} (?:%{NUMBER:bytes}|-) \"(?:%{URI:http_referrer}|-)\" %{QS:http_user_agent} %{QS:http_x_forwarded_for} %{USER:ssl_chiper} %{NUMBER:request_time} (?:%{DATA:gzip_ratio}|-) (?:%{DATA:upstream}|-) (?:%{NUMBER:upstream_time}|-)"
-          ]
-        }
-      }
-    CONFIG
-
-    #TODO fixme
-
-    # start = Time.now
-    # sample 'www.example.com 10.6.10.13 - - [09/Aug/2012:16:19:39 +0200] "GET /index.php HTTP/1.1" 403 211 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.8.1.12) Gecko/20080201 Firefox/2.0.0.12" "-" - 0.019 - 10.6.10.12:81 0.002 US' do
-    #   duration = Time.now - start
-    #   # insist { duration } < 1  #TODO refactor performance tests
-    #   puts( subject["tags"])
-    #   reject { subject["tags"] }.include?("_grokparsefailure")
-    #   insist { subject["geoip_country"] } == ["US"]
-    # end
-
-
-    # sample 'www.example.com 10.6.10.13 - - [09/Aug/2012:16:19:39 +0200] "GET /index.php HTTP/1.1" 403 211 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.0; en-US; rv:1.8.1.12) Gecko/20080201 Firefox/2.0.0.12" "-" - 0.019 - 10.6.10.12:81 0.002 -' do
-    #   duration = Time.now - start
-    #   # insist { duration } < 1 #TODO refactor performance tests
-    #   reject { subject["tags"] }.include?("_grokparsefailure")
-    #   insist { subject["geoip_country"].nil? } == true
-    # end
-  end
-end
-
-__END__
diff --git a/spec/filters/grok/timeouts.rb b/spec/filters/grok/timeouts.rb
deleted file mode 100644
index 3039cb76f21..00000000000
--- a/spec/filters/grok/timeouts.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-require "test_utils"
-require "grok-pure"
-require "timeout"
-
-describe "grok known timeout failures" do
-  describe "more apache log timeouts" do
-    subject { Grok.new }
-    before :each do
-      patterns = Dir.glob(File.join(File.dirname(__FILE__), "../../../patterns/*"))
-      patterns.each { |path| subject.add_patterns_from_file(path) }
-      subject.add_pattern("RESPONSE_BYTES", '[0-9_-]+')
-      subject.add_pattern("POST_CONN_STATUS", '[+-X]')
-      subject.add_pattern("CUSTOMAPACHELOG", '%{HOSTNAME:servername} %{IP:remote_host} %{NONNEGINT:servetime_secs} \[%{HTTPDATE:apache_timestamp}\] \"%{WORD:method} %{URIPATH:url}\" %{QS:query} %{POSINT:status} %{RESPONSE_BYTES:bytes} %{POST_CONN_STATUS:post_conn_status} \"(?:%{URI:referrer}|-)\" %{QS:agent} %{QS:cookie}')
-      subject.compile("%{CUSTOMAPACHELOG}")
-    end
-
-    it "should not timeout" do
-      data = File.open(__FILE__); data.each { |line| break if line == "__END__\n" }
-      # puts subject.expanded_pattern
-      data.each do |line|
-        # This timeout will toss an exception if it takes too long.
-        Timeout.timeout(1) do
-          subject.match(line.chomp)
-          # puts :matched => subject.match(line.chomp)
-        end
-      end
-    end
-  end
-end
-
-__END__
-example.com 11.22.33.44 1 [24/Oct/2012:08:47:14 +0200] "GET /need-russia-foo.php" "?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" 200 145492 + "http://www.google.de/#hl=de&sclient=psy-ab&q=foo+russland+kosten&oq=foo+rus&gs_l=hp.1.1.0l4.0.0.1.297.0.0.0.0.0.0.0.0..0.0...0.0...1c.uULgSTd5tzc&pbx=1&bav=on.2,or.r_gc.r_pw.r_qf.&fp=c4affe4f526437d4&bpcl=35466521&biw=1600&bih=799" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: -" "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 11.22.33.44 0 [24/Oct/2012:08:47:18 +0200] "GET /ajax/ajax.fillPurposes2.php" "?destination=RUS&nationality=DEU&state=&localCode=DE" 200 271 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: application/json, text/javascript, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 11.22.33.44 0 [24/Oct/2012:08:49:30 +0200] "GET /ajax/ajax.fillPurposes2.php" "?destination=RUS&nationality=DEU&state=Baden-W%C3%BCrttemberg&localCode=DE" 200 271 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: application/json, text/javascript, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 11.22.33.44 0 [24/Oct/2012:08:49:41 +0200] "GET /ajax/ajax.fooPopup.php" "?passport_from=DEU&state_of_residence=Baden-W%C3%BCrttemberg&traveling_to[0]=RUS&traveling_for%5B0%5D=P&traveling_to%5B1%5D=&traveling_for%5B1%5D=&traveling_to%5B2%5D=&traveling_for%5B2%5D=&traveling_to%5B3%5D=&traveling_for%5B3%5D=&account_number=100536&account_exists=N" 200 2459 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/html, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 11.22.33.44 0 [24/Oct/2012:08:49:45 +0200] "GET /ajax/ajax.fooPopupValid.php" "?codeId=1588744&countryCode=RUS&travelingFor=P&entry=S&passportFrom=DEU&_=1351061386611" 200 490 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/javascript, application/javascript, application/ecmascript, application/x-ecmascript, */*; q=0.01" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 11.22.33.44 0 [24/Oct/2012:08:49:50 +0200] "POST /ajax/ajax.fooPopup.php" "" 302 - + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 11.22.33.44 1 [24/Oct/2012:08:49:50 +0200] "GET /requirements.php" "" 200 23780 + "http://example.com/need-russia-foo.php?login=100536&gclid=CJHRrKeHmbMCFWbKtAodn24ArA" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536" "Accept: text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 11.22.33.44 1 [24/Oct/2012:08:49:52 +0200] "GET /ajax/ajax.requirementsFeesTable.php" "?text=RUS_0" 200 1406 + "http://example.com/requirements.php" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.4 (KHTML, like Gecko) Chrome/22.0.1229.94 Safari/537.4" "Cookie: PHPSESSID=b9bctuvsf2bdjcorbpogdkbll6p4p8uj; code=100536; __utma=256812172.1575809976.1351061239.1351061239.1351061239.1; __utmb=256812172.2.10.1351061239; __utmc=256812172; __utmz=256812172.1351061239.1.1.utmgclid=CJHRrKeHmbMCFWbKtAodn24ArA|utmccn=(not%20set)|utmcmd=(not%20set)|utmctr=foo%20russland%20kosten; __utmv=256812172.100536; 100536-AB-/eta-requirements=%2Feta-requirements; 100536-AB-/esta-requirements=%2Festa-requirements" "Accept: */*" "Lang: de-DE,de;q=0.8,en-US;q=0.6,en;q=0.4" "Encoding: gzip,deflate,sdch" "Charset: ISO-8859-1,utf-8;q=0.7,*;q=0.3"
-example.com 194.127.209.156 0 [24/Oct/2012:10:54:06 +0200] "GET /ajax/ajax.fillPurposes2.php" "?destination=SAU&nationality=AUT&state=&localCode=DE" 200 236 + "http://example.com/foo.php?traveling_to=SAU&traveling_for=B&nationality=AUT&login=524412&state_of_residence=Baden-W\xc3\xbcrttemberg&use_lang=de&utm_source=trans&utm_medium=email&utm_campaign=cir-foo" "Mozilla/4.0 (compatible; MSIE 7.0; Windows NT 5.1; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729)" "Cookie: PHPSESSID=h9gbvo04nomip8ca5ljko3cprrkttq9 ; code=524412" "Accept: application/json, text/javascript, */*; q=0.01" "Lang: de" "Encoding: gzip, deflate" "Charset: -"
diff --git a/spec/filters/json.rb b/spec/filters/json.rb
deleted file mode 100644
index 7041a04bad5..00000000000
--- a/spec/filters/json.rb
+++ /dev/null
@@ -1,89 +0,0 @@
-require "test_utils"
-require "logstash/filters/json"
-
-describe LogStash::Filters::Json do
-  extend LogStash::RSpec
-
-  describe "parse message into the event" do
-    config <<-CONFIG
-      filter {
-        json {
-          # Parse message as JSON
-          source => "message"
-        }
-      }
-    CONFIG
-
-    sample '{ "hello": "world", "list": [ 1, 2, 3 ], "hash": { "k": "v" } }' do
-      insist { subject["hello"] } == "world"
-      insist { subject["list" ] } == [1,2,3]
-      insist { subject["hash"] } == { "k" => "v" }
-    end
-  end
-
-  describe "parse message into a target field" do
-    config <<-CONFIG
-      filter {
-        json {
-          # Parse message as JSON, store the results in the 'data' field'
-          source => "message"
-          target => "data"
-        }
-      }
-    CONFIG
-
-    sample '{ "hello": "world", "list": [ 1, 2, 3 ], "hash": { "k": "v" } }' do
-      insist { subject["data"]["hello"] } == "world"
-      insist { subject["data"]["list" ] } == [1,2,3]
-      insist { subject["data"]["hash"] } == { "k" => "v" }
-    end
-  end
-
-  describe "tag invalid json" do
-    config <<-CONFIG
-      filter {
-        json {
-          # Parse message as JSON, store the results in the 'data' field'
-          source => "message"
-          target => "data"
-        }
-      }
-    CONFIG
-
-    sample "invalid json" do
-      insist { subject["tags"] }.include?("_jsonparsefailure")
-    end
-  end
-
-  describe "fixing @timestamp (#pull 733)" do
-    config <<-CONFIG
-      filter {
-        json {
-          source => "message"
-        }
-      }
-    CONFIG
-
-    sample "{ \"@timestamp\": \"2013-10-19T00:14:32.996Z\" }" do
-      insist { subject["@timestamp"] }.is_a?(Time)
-      insist { subject["@timestamp"].to_json } == "\"2013-10-19T00:14:32.996Z\""
-    end
-  end
-
-  describe "source == target" do
-    config <<-CONFIG
-      filter {
-        json {
-          source => "example"
-          target => "example"
-        }
-      }
-    CONFIG
-
-    sample({ "example" => "{ \"hello\": \"world\" }" }) do
-      insist { subject["example"] }.is_a?(Hash)
-      insist { subject["example"]["hello"] } == "world"
-    end
-  end
-
-end
diff --git a/spec/filters/kv.rb b/spec/filters/kv.rb
deleted file mode 100644
index 1da201a7b95..00000000000
--- a/spec/filters/kv.rb
+++ /dev/null
@@ -1,405 +0,0 @@
-require "test_utils"
-require "logstash/filters/kv"
-
-describe LogStash::Filters::KV do
-  extend LogStash::RSpec
-
-  describe "defaults" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        kv { }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["hello"] } == "world"
-      insist { subject["foo"] } == "bar"
-      insist { subject["baz"] } == "fizz"
-      insist { subject["doublequoted"] } == "hello world"
-      insist { subject["singlequoted"] } == "hello world"
-    end
-
-  end
-
-   describe "LOGSTASH-624: allow escaped space in key or value " do
-    config <<-CONFIG
-      filter {
-        kv { value_split => ':' }
-      }
-    CONFIG
-
-    sample 'IKE:=Quick\ Mode\ completion IKE\ IDs:=subnet:\ x.x.x.x\ (mask=\ 255.255.255.254)\ and\ host:\ y.y.y.y' do
-      insist { subject["IKE"] } == '=Quick\ Mode\ completion'
-      insist { subject['IKE\ IDs'] } == '=subnet:\ x.x.x.x\ (mask=\ 255.255.255.254)\ and\ host:\ y.y.y.y'
-    end
-  end
-
-  describe "test value_split" do
-    config <<-CONFIG
-      filter {
-        kv { value_split => ':' }
-      }
-    CONFIG
-
-    sample "hello:=world foo:bar baz=:fizz doublequoted:\"hello world\" singlequoted:'hello world'" do
-      insist { subject["hello"] } == "=world"
-      insist { subject["foo"] } == "bar"
-      insist { subject["baz="] } == "fizz"
-      insist { subject["doublequoted"] } == "hello world"
-      insist { subject["singlequoted"] } == "hello world"
-    end
-
-  end
-
-  describe "test field_split" do
-    config <<-CONFIG
-      filter {
-        kv { field_split => '?&' }
-      }
-    CONFIG
-
-    sample "?hello=world&foo=bar&baz=fizz&doublequoted=\"hello world\"&singlequoted='hello world'&ignoreme&foo12=bar12" do
-      insist { subject["hello"] } == "world"
-      insist { subject["foo"] } == "bar"
-      insist { subject["baz"] } == "fizz"
-      insist { subject["doublequoted"] } == "hello world"
-      insist { subject["singlequoted"] } == "hello world"
-      insist { subject["foo12"] } == "bar12"
-    end
-
-  end
-
-  describe  "delimited fields should override space default (reported by LOGSTASH-733)" do
-    config <<-CONFIG
-      filter {
-        kv { field_split => "|" }
-      }
-    CONFIG
-
-    sample "field1=test|field2=another test|field3=test3" do
-      insist { subject["field1"] } == "test"
-      insist { subject["field2"] } == "another test"
-      insist { subject["field3"] } == "test3"
-    end
-  end
-
-  describe "test prefix" do
-    config <<-CONFIG
-      filter {
-        kv { prefix => '__' }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["__hello"] } == "world"
-      insist { subject["__foo"] } == "bar"
-      insist { subject["__baz"] } == "fizz"
-      insist { subject["__doublequoted"] } == "hello world"
-      insist { subject["__singlequoted"] } == "hello world"
-    end
-
-  end
-
-  describe "speed test", :performance => true do
-    count = 10000 + rand(3000)
-    config <<-CONFIG
-      input {
-        generator {
-          count => #{count}
-          type => foo
-          message => "hello=world bar='baz fizzle'"
-        }
-      }
-
-      filter {
-        kv { }
-      }
-
-      output  {
-        null { }
-      }
-    CONFIG
-
-    start = Time.now
-    agent do
-      duration = (Time.now - start)
-      puts "filters/kv rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
-    end
-  end
-
-  describe "add_tag" do
-    context "should activate when successful" do
-      config <<-CONFIG
-        filter {
-          kv { add_tag => "hello" }
-        }
-      CONFIG
-
-      sample "hello=world" do
-        insist { subject["hello"] } == "world"
-        insist { subject["tags"] }.include?("hello")
-      end
-    end
-    context "should not activate when failing" do
-      config <<-CONFIG
-        filter {
-          kv { add_tag => "hello" }
-        }
-      CONFIG
-
-      sample "this is not key value" do
-        insist { subject["tags"] }.nil?
-      end
-    end
-  end
-
-  describe "add_field" do
-    context "should activate when successful" do
-      config <<-CONFIG
-        filter {
-          kv { add_field => [ "whoa", "fancypants" ] }
-        }
-      CONFIG
-
-      sample "hello=world" do
-        insist { subject["hello"] } == "world"
-        insist { subject["whoa"] } == "fancypants"
-      end
-    end
-
-    context "should not activate when failing" do
-      config <<-CONFIG
-        filter {
-          kv { add_tag => "hello" }
-        }
-      CONFIG
-
-      sample "this is not key value" do
-        reject { subject["whoa"] } == "fancypants"
-      end
-    end
-  end
-
-  #New tests
-  describe "test target" do
-    config <<-CONFIG
-      filter {
-        kv { target => 'kv' }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["kv"]["hello"] } == "world"
-      insist { subject["kv"]["foo"] } == "bar"
-      insist { subject["kv"]["baz"] } == "fizz"
-      insist { subject["kv"]["doublequoted"] } == "hello world"
-      insist { subject["kv"]["singlequoted"] } == "hello world"
-      insist {subject["kv"].count } == 5
-    end
-
-  end
-
-  describe "test empty target" do
-    config <<-CONFIG
-      filter {
-        kv { target => 'kv' }
-      }
-    CONFIG
-
-    sample "hello:world:foo:bar:baz:fizz" do
-      insist { subject["kv"] } == nil
-    end
-  end
-
-
-  describe "test data from specific sub source" do
-    config <<-CONFIG
-      filter {
-        kv {
-          source => "data"
-        }
-      }
-    CONFIG
-    sample("data" => "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'") do
-      insist { subject["hello"] } == "world"
-      insist { subject["foo"] } == "bar"
-      insist { subject["baz"] } == "fizz"
-      insist { subject["doublequoted"] } == "hello world"
-      insist { subject["singlequoted"] } == "hello world"
-    end
-  end
-
-  describe "test data from specific top source" do
-    config <<-CONFIG
-      filter {
-        kv {
-          source => "@data"
-        }
-      }
-    CONFIG
-    sample({"@data" => "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'"}) do
-      insist { subject["hello"] } == "world"
-      insist { subject["foo"] } == "bar"
-      insist { subject["baz"] } == "fizz"
-      insist { subject["doublequoted"] } == "hello world"
-      insist { subject["singlequoted"] } == "hello world"
-    end
-  end
-
-
-  describe "test data from specific sub source and target" do
-    config <<-CONFIG
-      filter {
-        kv {
-          source => "data"
-          target => "kv"
-        }
-      }
-    CONFIG
-    sample("data" => "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'") do
-      insist { subject["kv"]["hello"] } == "world"
-      insist { subject["kv"]["foo"] } == "bar"
-      insist { subject["kv"]["baz"] } == "fizz"
-      insist { subject["kv"]["doublequoted"] } == "hello world"
-      insist { subject["kv"]["singlequoted"] } == "hello world"
-      insist { subject["kv"].count } == 5
-    end
-  end
-
-  describe "test data from nil sub source, should not issue a warning" do
-    config <<-CONFIG
-      filter {
-        kv {
-          source => "non-exisiting-field"
-          target => "kv"
-        }
-      }
-    CONFIG
-    sample "" do
-      insist { subject["non-exisiting-field"] } == nil
-      insist { subject["kv"] } == nil
-    end
-  end
-
-  describe "test include_keys" do
-    config <<-CONFIG
-      filter {
-        kv {
-          include_keys => [ "foo", "singlequoted" ]
-        }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["foo"] } == "bar"
-      insist { subject["singlequoted"] } == "hello world"
-    end
-  end
-
-  describe "test exclude_keys" do
-    config <<-CONFIG
-      filter {
-        kv {
-          exclude_keys => [ "foo", "singlequoted" ]
-        }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["hello"] } == "world"
-      insist { subject["baz"] } == "fizz"
-      insist { subject["doublequoted"] } == "hello world"
-    end
-  end
-
-  describe "test include_keys with prefix" do
-    config <<-CONFIG
-      filter {
-        kv {
-          include_keys => [ "foo", "singlequoted" ]
-          prefix       => "__"
-        }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["__foo"] } == "bar"
-      insist { subject["__singlequoted"] } == "hello world"
-    end
-  end
-
-  describe "test exclude_keys with prefix" do
-    config <<-CONFIG
-      filter {
-        kv {
-          exclude_keys => [ "foo", "singlequoted" ]
-          prefix       => "__"
-        }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["__hello"] } == "world"
-      insist { subject["__baz"] } == "fizz"
-      insist { subject["__doublequoted"] } == "hello world"
-    end
-  end
-
-  describe "test include_keys and exclude_keys" do
-    config <<-CONFIG
-      filter {
-        kv {
-          # This should exclude everything as a result of both settings.
-          include_keys => [ "foo", "singlequoted" ]
-          exclude_keys => [ "foo", "singlequoted" ]
-        }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      %w(hello foo baz doublequoted singlequoted).each do |field|
-        reject { subject }.include?(field)
-      end
-    end
-  end
-
-  describe "test default_keys" do
-    config <<-CONFIG
-      filter {
-        kv {
-          default_keys => [ "foo", "xxx",
-                            "goo", "yyy" ]
-        }
-      }
-    CONFIG
-
-    sample "hello=world foo=bar baz=fizz doublequoted=\"hello world\" singlequoted='hello world'" do
-      insist { subject["hello"] } == "world"
-      insist { subject["foo"] } == "bar"
-      insist { subject["goo"] } == "yyy"
-      insist { subject["baz"] } == "fizz"
-      insist { subject["doublequoted"] } == "hello world"
-      insist { subject["singlequoted"] } == "hello world"
-    end
-  end
-
-  describe "overwriting a string field (often the source)" do
-    config <<-CONFIG
-      filter {
-        kv {
-          source => "happy"
-          target => "happy"
-        }
-      }
-    CONFIG
-
-    sample("happy" => "foo=bar baz=fizz") do
-      insist { subject["[happy][foo]"] } == "bar"
-      insist { subject["[happy][baz]"] } == "fizz"
-    end
-
-  end
-
-end
diff --git a/spec/filters/metrics.rb b/spec/filters/metrics.rb
deleted file mode 100644
index b34af869c19..00000000000
--- a/spec/filters/metrics.rb
+++ /dev/null
@@ -1,232 +0,0 @@
-require "logstash/filters/metrics"
-
-describe LogStash::Filters::Metrics do
-
-  context "with basic meter config" do
-    context "when no events were received" do
-      it "should not flush" do
-        config = {"meter" => ["http.%{response}"]}
-        filter = LogStash::Filters::Metrics.new config
-        filter.register
-
-        events = filter.flush
-        insist { events }.nil?
-      end
-    end
-
-    context "when events are received" do
-      context "on the first flush" do
-        subject {
-          config = {"meter" => ["http.%{response}"]}
-          filter = LogStash::Filters::Metrics.new config
-          filter.register
-          filter.filter LogStash::Event.new({"response" => 200})
-          filter.filter LogStash::Event.new({"response" => 200})
-          filter.filter LogStash::Event.new({"response" => 404})
-          filter.flush
-        }
-
-        it "should flush counts" do
-          insist { subject.length } == 1
-          insist { subject.first["http.200.count"] } == 2
-          insist { subject.first["http.404.count"] } == 1
-        end
-
-        it "should include rates and percentiles" do
-          metrics = ["http.200.rate_1m", "http.200.rate_5m", "http.200.rate_15m",
-                     "http.404.rate_1m", "http.404.rate_5m", "http.404.rate_15m"]
-          metrics.each do |metric|
-            insist { subject.first }.include? metric
-          end
-        end
-      end
-
-      context "on the second flush" do
-        it "should not reset counts" do
-          config = {"meter" => ["http.%{response}"]}
-          filter = LogStash::Filters::Metrics.new config
-          filter.register
-          filter.filter LogStash::Event.new({"response" => 200})
-          filter.filter LogStash::Event.new({"response" => 200})
-          filter.filter LogStash::Event.new({"response" => 404})
-
-          events = filter.flush
-          events = filter.flush
-          insist { events.length } == 1
-          insist { events.first["http.200.count"] } == 2
-          insist { events.first["http.404.count"] } == 1
-        end
-      end
-    end
-
-    context "when custom rates and percentiles are selected" do
-      context "on the first flush" do
-        subject {
-          config = {
-            "meter" => ["http.%{response}"],
-            "rates" => [1]
-          }
-          filter = LogStash::Filters::Metrics.new config
-          filter.register
-          filter.filter LogStash::Event.new({"response" => 200})
-          filter.filter LogStash::Event.new({"response" => 200})
-          filter.filter LogStash::Event.new({"response" => 404})
-          filter.flush
-        }
-
-        it "should include only the requested rates" do
-          rate_fields = subject.first.to_hash.keys.select {|field| field.start_with?("http.200.rate") }
-          insist { rate_fields.length } == 1
-          insist { rate_fields }.include? "http.200.rate_1m"
-        end
-      end
-    end
-  end
-
-  context "with multiple instances" do
-    it "counts should be independent" do
-      config_tag1 = {"meter" => ["http.%{response}"], "tags" => ["tag1"]}
-      config_tag2 = {"meter" => ["http.%{response}"], "tags" => ["tag2"]}
-      filter_tag1 = LogStash::Filters::Metrics.new config_tag1
-      filter_tag2 = LogStash::Filters::Metrics.new config_tag2
-      event_tag1 = LogStash::Event.new({"response" => 200, "tags" => [ "tag1" ]})
-      event_tag2 = LogStash::Event.new({"response" => 200, "tags" => [ "tag2" ]})
-      event2_tag2 = LogStash::Event.new({"response" => 200, "tags" => [ "tag2" ]})
-      filter_tag1.register
-      filter_tag2.register
-
-      [event_tag1, event_tag2, event2_tag2].each do |event|
-        filter_tag1.filter event
-        filter_tag2.filter event
-      end
-
-      events_tag1 = filter_tag1.flush
-      events_tag2 = filter_tag2.flush
-
-      insist { events_tag1.first["http.200.count"] } == 1
-      insist { events_tag2.first["http.200.count"] } == 2
-    end
-  end
-
-  context "with timer config" do
-    context "on the first flush" do
-      subject {
-        config = {"timer" => ["http.request_time", "%{request_time}"]}
-        filter = LogStash::Filters::Metrics.new config
-        filter.register
-        filter.filter LogStash::Event.new({"request_time" => 10})
-        filter.filter LogStash::Event.new({"request_time" => 20})
-        filter.filter LogStash::Event.new({"request_time" => 30})
-        filter.flush
-      }
-
-      it "should flush counts" do
-        insist { subject.length } == 1
-        insist { subject.first["http.request_time.count"] } == 3
-      end
-
-      it "should include rates and percentiles keys" do
-        metrics = ["rate_1m", "rate_5m", "rate_15m", "p1", "p5", "p10", "p90", "p95", "p99"]
-        metrics.each do |metric|
-          insist { subject.first }.include? "http.request_time.#{metric}"
-        end
-      end
-
-      it "should include min value" do
-        insist { subject.first['http.request_time.min'] } == 10.0
-      end
-
-      it "should include mean value" do
-        insist { subject.first['http.request_time.mean'] } == 20.0
-      end
-
-      it "should include stddev value" do
-        insist { subject.first['http.request_time.stddev'] } == Math.sqrt(10.0)
-      end
-
-      it "should include max value" do
-        insist { subject.first['http.request_time.max'] } == 30.0
-      end
-
-      it "should include percentile value" do
-        insist { subject.first['http.request_time.p99'] } == 30.0
-      end
-    end
-  end
-
-  context "when custom rates and percentiles are selected" do
-    context "on the first flush" do
-      subject {
-        config = {
-          "timer" => ["http.request_time", "request_time"],
-          "rates" => [1],
-          "percentiles" => [1, 2]
-        }
-        filter = LogStash::Filters::Metrics.new config
-        filter.register
-        filter.filter LogStash::Event.new({"request_time" => 1})
-        filter.flush
-      }
-
-      it "should flush counts" do
-        insist { subject.length } == 1
-        insist { subject.first["http.request_time.count"] } == 1
-      end
-
-      it "should include only the requested rates" do
-        rate_fields = subject.first.to_hash.keys.select {|field| field.start_with?("http.request_time.rate") }
-        insist { rate_fields.length } == 1
-        insist { rate_fields }.include? "http.request_time.rate_1m"
-      end
-
-      it "should include only the requested percentiles" do
-        percentile_fields = subject.first.to_hash.keys.select {|field| field.start_with?("http.request_time.p") }
-        insist { percentile_fields.length } == 2
-        insist { percentile_fields }.include? "http.request_time.p1"
-        insist { percentile_fields }.include? "http.request_time.p2"
-      end
-    end
-  end
-
-
-  context "when a custom flush_interval is set" do
-    it "should flush only when required" do
-      config = {"meter" => ["http.%{response}"], "flush_interval" => 15}
-      filter = LogStash::Filters::Metrics.new config
-      filter.register
-      filter.filter LogStash::Event.new({"response" => 200})
-
-      insist { filter.flush }.nil?        # 5s
-      insist { filter.flush }.nil?        # 10s
-      insist { filter.flush.length } == 1 # 15s
-      insist { filter.flush }.nil?        # 20s
-      insist { filter.flush }.nil?        # 25s
-      insist { filter.flush.length } == 1 # 30s
-    end
-  end
-
-  context "when a custom clear_interval is set" do
-    it "should clear the metrics after interval has passed" do
-      config = {"meter" => ["http.%{response}"], "clear_interval" => 15}
-      filter = LogStash::Filters::Metrics.new config
-      filter.register
-      filter.filter LogStash::Event.new({"response" => 200})
-
-      insist { filter.flush.first["http.200.count"] } == 1 # 5s
-      insist { filter.flush.first["http.200.count"] } == 1 # 10s
-      insist { filter.flush.first["http.200.count"] } == 1 # 15s
-      insist { filter.flush }.nil?                         # 20s
-    end
-  end
-
-  context "when invalid rates are set" do
-    subject {
-      config = {"meter" => ["http.%{response}"], "rates" => [90]}
-      filter = LogStash::Filters::Metrics.new config
-    }
-
-    it "should raise an error" do
-      insist {subject.register }.raises(LogStash::ConfigurationError)
-    end
-  end
-end
diff --git a/spec/filters/multiline.rb b/spec/filters/multiline.rb
deleted file mode 100644
index e5be67f31d0..00000000000
--- a/spec/filters/multiline.rb
+++ /dev/null
@@ -1,158 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/multiline"
-
-describe LogStash::Filters::Multiline do
-
-  extend LogStash::RSpec
-
-  describe "simple multiline" do
-    config <<-CONFIG
-    filter {
-      multiline {
-        enable_flush => true
-        pattern => "^\\s"
-        what => previous
-      }
-    }
-    CONFIG
-
-    sample [ "hello world", "   second line", "another first line" ] do
-      expect(subject).to be_a(Array)
-      insist { subject.size } == 2
-      insist { subject[0]["message"] } == "hello world\n   second line"
-      insist { subject[1]["message"] } == "another first line"
-    end
-  end
-
-  describe "multiline using grok patterns" do
-    config <<-CONFIG
-    filter {
-      multiline {
-        enable_flush => true
-        pattern => "^%{NUMBER} %{TIME}"
-        negate => true
-        what => previous
-      }
-    }
-    CONFIG
-
-    sample [ "120913 12:04:33 first line", "second line", "third line" ] do
-      insist { subject["message"] } ==  "120913 12:04:33 first line\nsecond line\nthird line"
-    end
-  end
-
-  describe "multiline safety among multiple concurrent streams" do
-    config <<-CONFIG
-      filter {
-        multiline {
-          enable_flush => true
-          pattern => "^\\s"
-          what => previous
-        }
-      }
-    CONFIG
-
-    count = 50
-    stream_count = 3
-
-    # first make sure to have starting lines for all streams
-    eventstream = stream_count.times.map do |i|
-      stream = "stream#{i}"
-      lines = [LogStash::Event.new("message" => "hello world #{stream}", "host" => stream, "type" => stream)]
-      lines += rand(5).times.map do |n|
-        LogStash::Event.new("message" => "   extra line in #{stream}", "host" => stream, "type" => stream)
-      end
-    end
-
-    # them add starting lines for random stream with sublines also for random stream
-    eventstream += (count - stream_count).times.map do |i|
-      stream = "stream#{rand(stream_count)}"
-      lines = [LogStash::Event.new("message" => "hello world #{stream}", "host" => stream, "type" => stream)]
-      lines += rand(5).times.map do |n|
-        stream = "stream#{rand(stream_count)}"
-        LogStash::Event.new("message" => "   extra line in #{stream}", "host" => stream, "type" => stream)
-      end
-    end
-
-    events = eventstream.flatten.map{|event| event.to_hash}
-
-    sample events do
-      expect(subject).to be_a(Array)
-      insist { subject.size } == count
-
-      subject.each_with_index do |event, i|
-        insist { event["type"] == event["host"] } == true
-        stream = event["type"]
-        insist { event["message"].split("\n").first } =~ /hello world /
-        insist { event["message"].scan(/stream\d/).all?{|word| word == stream} } == true
-      end
-    end
-  end
-
-  describe "multiline add/remove tags and fields only when matched" do
-    config <<-CONFIG
-      filter {
-        mutate {
-          add_tag => "dummy"
-        }
-        multiline {
-          enable_flush => true
-          add_tag => [ "nope" ]
-          remove_tag => "dummy"
-          add_field => [ "dummy2", "value" ]
-          pattern => "an unlikely match"
-          what => previous
-        }
-      }
-    CONFIG
-
-    sample [ "120913 12:04:33 first line", "120913 12:04:33 second line" ] do
-      expect(subject).to be_a(Array)
-      insist { subject.size } == 2
-
-      subject.each do |s|
-        insist { s["tags"].include?("nope")  } == false
-        insist { s["tags"].include?("dummy") } == true
-        insist { s.include?("dummy2") } == false
-      end
-    end
-  end
-
-  describe "regression test for GH issue #1258" do
-    config <<-CONFIG
-      filter {
-        multiline {
-          pattern => "^\s"
-          what => "next"
-          add_tag => ["multi"]
-        }
-      }
-    CONFIG
-
-    sample [ "  match", "nomatch" ] do
-      expect(subject).to be_a(LogStash::Event)
-      insist { subject["message"] } == "  match\nnomatch"
-    end
-  end
-
-  describe "multiple match/nomatch" do
-    config <<-CONFIG
-      filter {
-        multiline {
-          pattern => "^\s"
-          what => "next"
-          add_tag => ["multi"]
-        }
-      }
-    CONFIG
-
-    sample ["  match1", "nomatch1", "  match2", "nomatch2"] do
-      expect(subject).to be_a(Array)
-      insist { subject.size } == 2
-      insist { subject[0]["message"] } == "  match1\nnomatch1"
-      insist { subject[1]["message"] } == "  match2\nnomatch2"
-    end
-  end
-end
diff --git a/spec/filters/mutate.rb b/spec/filters/mutate.rb
deleted file mode 100644
index 9eb281fccb9..00000000000
--- a/spec/filters/mutate.rb
+++ /dev/null
@@ -1,182 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/mutate"
-
-describe LogStash::Filters::Mutate do
-  extend LogStash::RSpec
-
-  describe "basics" do
-    config <<-CONFIG
-      filter {
-        mutate {
-          lowercase => "lowerme"
-          uppercase => "upperme"
-          convert => [ "intme", "integer", "floatme", "float" ]
-          rename => [ "rename1", "rename2" ]
-          replace => [ "replaceme", "hello world" ]
-          replace => [ "newfield", "newnew" ]
-          update => [ "nosuchfield", "weee" ]
-          update => [ "updateme", "updated" ]
-          remove => [ "removeme" ]
-        }
-      }
-    CONFIG
-
-    event = {
-      "lowerme" => [ "ExAmPlE" ],
-      "upperme" => [ "ExAmPlE" ],
-      "intme" => [ "1234", "7890.4", "7.9" ],
-      "floatme" => [ "1234.455" ],
-      "rename1" => [ "hello world" ],
-      "updateme" => [ "who cares" ],
-      "replaceme" => [ "who cares" ],
-      "removeme" => [ "something" ]
-    }
-
-    sample event do
-      insist { subject["lowerme"] } == ['example']
-      insist { subject["upperme"] } == ['EXAMPLE']
-      insist { subject["intme"] }   == [1234, 7890, 7]
-      insist { subject["floatme"] } == [1234.455]
-      reject { subject }.include?("rename1")
-      insist { subject["rename2"] } == [ "hello world" ]
-      reject { subject }.include?("removeme")
-
-      insist { subject }.include?("newfield")
-      insist { subject["newfield"] } == "newnew"
-      reject { subject }.include?("nosuchfield")
-      insist { subject["updateme"] } == "updated"
-    end
-  end
-
-  describe "remove multiple fields" do
-    config '
-      filter {
-        mutate {
-          remove => [ "remove-me", "remove-me2", "diedie", "[one][two]" ]
-        }
-      }'
-
-    sample(
-      "remove-me"  => "Goodbye!",
-      "remove-me2" => 1234,
-      "diedie"     => [1, 2, 3, 4],
-      "survivor"   => "Hello.",
-      "one" => { "two" => "wee" }
-    ) do
-      insist { subject["survivor"] } == "Hello."
-      reject { subject }.include?("remove-me")
-      reject { subject }.include?("remove-me2")
-      reject { subject }.include?("diedie")
-      reject { subject["one"] }.include?("two")
-    end
-  end
-
-  describe "convert one field to string" do
-    config '
-      filter {
-        mutate {
-          convert => [ "unicorns", "string" ]
-        }
-      }'
-
-    sample("unicorns" => 1234) do
-      insist { subject["unicorns"] } == "1234"
-    end
-  end
-
-  describe "gsub on a String" do
-    config '
-      filter {
-        mutate {
-          gsub => [ "unicorns", "but extinct", "and common" ]
-        }
-      }'
-
-    sample("unicorns" => "Magnificient, but extinct, animals") do
-      insist { subject["unicorns"] } == "Magnificient, and common, animals"
-    end
-  end
-
-  describe "gsub on an Array of Strings" do
-    config '
-      filter {
-        mutate {
-          gsub => [ "unicorns", "extinct", "common" ]
-        }
-      }'
-
-    sample("unicorns" => [
-      "Magnificient extinct animals", "Other extinct ideas" ]
-    ) do
-      insist { subject["unicorns"] } == [
-        "Magnificient common animals",
-        "Other common ideas"
-      ]
-    end
-  end
-
-  describe "gsub on multiple fields" do
-    config '
-      filter {
-        mutate {
-          gsub => [ "colors", "red", "blue",
-                    "shapes", "square", "circle" ]
-        }
-      }'
-
-    sample("colors" => "One red car", "shapes" => "Four red squares") do
-      insist { subject["colors"] } == "One blue car"
-      insist { subject["shapes"] } == "Four red circles"
-    end
-  end
-
-  describe "regression - mutate should lowercase a field created by grok" do
-    config <<-CONFIG
-      filter {
-        grok {
-          match => [ "message", "%{WORD:foo}" ]
-        }
-        mutate {
-          lowercase => "foo"
-        }
-      }
-    CONFIG
-
-    sample "HELLO WORLD" do
-      insist { subject["foo"] } == "hello"
-    end
-  end
-
-  describe "LOGSTASH-757: rename should do nothing with a missing field" do
-    config <<-CONFIG
-      filter {
-        mutate {
-          rename => [ "nosuchfield", "hello" ]
-        }
-      }
-    CONFIG
-
-    sample "whatever" do
-      reject { subject }.include?("nosuchfield")
-      reject { subject }.include?("hello")
-    end
-  end
-
-  describe "convert should work on nested fields" do
-    config <<-CONFIG
-      filter {
-        mutate {
-          convert => [ "[foo][bar]", "integer" ]
-        }
-      }
-    CONFIG
-
-    sample({ "foo" => { "bar" => "1000" } }) do
-      insist { subject["[foo][bar]"] } == 1000
-      insist { subject["[foo][bar]"] }.is_a?(Fixnum)
-    end
-  end
-end
-
diff --git a/spec/filters/noop.rb b/spec/filters/noop.rb
deleted file mode 100644
index 810d2fbab12..00000000000
--- a/spec/filters/noop.rb
+++ /dev/null
@@ -1,221 +0,0 @@
-require "test_utils"
-require "logstash/filters/noop"
-
-#NOOP filter is perfect for testing Filters::Base features with minimal overhead
-describe LogStash::Filters::NOOP do
-  extend LogStash::RSpec
-
-  describe "adding multiple value to one field" do
-    config <<-CONFIG
-    filter {
-      noop {
-        add_field => ["new_field", "new_value"]
-        add_field => ["new_field", "new_value_2"]
-      }
-    }
-    CONFIG
-
-    sample "example" do
-      insist { subject["new_field"] } == ["new_value", "new_value_2"]
-    end
-  end
-
-  describe "type parsing" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        add_tag => ["test"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
-    end
-
-    sample("type" => "not_noop") do
-      insist { subject["tags"] }.nil?
-    end
-  end
-
-  describe "tags parsing with one tag" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1"]
-        add_tag => ["test"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop") do
-      insist { subject["tags"] }.nil?
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
-    end
-  end
-
-  describe "tags parsing with multiple tags" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1", "t2"]
-        add_tag => ["test"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop") do
-      insist { subject["tags"] }.nil?
-    end
-
-    sample("type" => "noop", "tags" => ["t1"]) do
-      insist { subject["tags"] } == ["t1"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1", "t2", "t3", "test"]
-    end
-  end
-
-  describe "exclude_tags with 1 tag" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1"]
-        add_tag => ["test"]
-        exclude_tags => ["t2"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop") do
-      insist { subject["tags"] }.nil?
-    end
-
-    sample("type" => "noop", "tags" => ["t1"]) do
-      insist { subject["tags"] } == ["t1", "test"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2"]
-    end
-  end
-
-  describe "exclude_tags with >1 tags" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1"]
-        add_tag => ["test"]
-        exclude_tags => ["t2", "t3"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop", "tags" => ["t1", "t2", "t4"]) do
-      insist { subject["tags"] } == ["t1", "t2", "t4"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t3", "t4"]) do
-      insist { subject["tags"] } == ["t1", "t3", "t4"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t4", "t5"]) do
-      insist { subject["tags"] } == ["t1", "t4", "t5", "test"]
-    end
-  end
-
-  describe "remove_tag" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1"]
-        remove_tag => ["t2", "t3"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop", "tags" => ["t4"]) do
-      insist { subject["tags"] } == ["t4"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1"]
-    end
-  end
-
-  describe "remove_tag with dynamic value" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1"]
-        remove_tag => ["%{blackhole}"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop", "tags" => ["t1", "goaway", "t3"], "blackhole" => "goaway") do
-      insist { subject["tags"] } == ["t1", "t3"]
-    end
-  end
-
-  describe "remove_field" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        remove_field => ["t2", "t3"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop", "t4" => "four") do
-      insist { subject }.include?("t4")
-    end
-
-    sample("type" => "noop", "t1" => "one", "t2" => "two", "t3" => "three") do
-      insist { subject }.include?("t1")
-      reject { subject }.include?("t2")
-      reject { subject }.include?("t3")
-    end
-
-    sample("type" => "noop", "t1" => "one", "t2" => "two") do
-      insist { subject }.include?("t1")
-      reject { subject }.include?("t2")
-    end
-  end
-
-  describe "remove_field with dynamic value in field name" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        remove_field => ["%{blackhole}"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop", "blackhole" => "go", "go" => "away") do
-      insist { subject }.include?("blackhole")
-      reject { subject }.include?("go")
-    end
-  end
-end
diff --git a/spec/filters/split.rb b/spec/filters/split.rb
deleted file mode 100644
index f258dc38952..00000000000
--- a/spec/filters/split.rb
+++ /dev/null
@@ -1,60 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/split"
-
-describe LogStash::Filters::Split do
-  extend LogStash::RSpec
-
-  describe "all defaults" do
-    config <<-CONFIG
-      filter {
-        split { }
-      }
-    CONFIG
-
-    sample "big\nbird\nsesame street" do
-      insist { subject.length } == 3
-      insist { subject[0]["message"] } == "big"
-      insist { subject[1]["message"] } == "bird"
-      insist { subject[2]["message"] } == "sesame street"
-    end
-  end
-
-  describe "custome terminator" do
-    config <<-CONFIG
-      filter {
-        split {
-          terminator => "\t"
-        }
-      }
-    CONFIG
-
-    sample "big\tbird\tsesame street" do
-      insist { subject.length } == 3
-      insist { subject[0]["message"] } == "big"
-      insist { subject[1]["message"] } == "bird"
-      insist { subject[2]["message"] } == "sesame street"
-    end
-  end
-
-  describe "custom field" do
-    config <<-CONFIG
-      filter {
-        split {
-          field => "custom"
-        }
-      }
-    CONFIG
-
-    sample("custom" => "big\nbird\nsesame street", "do_not_touch" => "1\n2\n3") do
-      insist { subject.length } == 3
-      subject.each do |s|
-         insist { s["do_not_touch"] } == "1\n2\n3"
-      end
-      insist { subject[0]["custom"] } == "big"
-      insist { subject[1]["custom"] } == "bird"
-      insist { subject[2]["custom"] } == "sesame street"
-    end
-  end
-end
diff --git a/spec/filters/throttle.rb b/spec/filters/throttle.rb
deleted file mode 100644
index 5cf77adb749..00000000000
--- a/spec/filters/throttle.rb
+++ /dev/null
@@ -1,197 +0,0 @@
-require "test_utils"
-require "logstash/filters/throttle"
-
-describe LogStash::Filters::Throttle do
-  extend LogStash::RSpec
-
-  describe "no before_count" do
-    config <<-CONFIG
-      filter {
-        throttle {
-          period => 60
-          after_count => 2
-          key => "%{host}"
-          add_tag => [ "throttled" ]
-        }
-      }
-    CONFIG
-
-    event = {
-      "host" => "server1"
-    }
-
-    sample event do
-      insist { subject["tags"] } == nil
-    end
-  end
-  
-  describe "before_count throttled" do
-    config <<-CONFIG
-      filter {
-        throttle {
-          period => 60
-          before_count => 2
-          after_count => 3
-          key => "%{host}"
-          add_tag => [ "throttled" ]
-        }
-      }
-    CONFIG
-
-    event = {
-      "host" => "server1"
-    }
-
-    sample event do
-      insist { subject["tags"] } == [ "throttled" ]
-    end
-  end
-  
-  describe "before_count exceeded" do
-    config <<-CONFIG
-      filter {
-        throttle {
-          period => 60
-          before_count => 2
-          after_count => 3
-          key => "%{host}"
-          add_tag => [ "throttled" ]
-        }
-      }
-    CONFIG
-
-    events = [{
-      "host" => "server1"
-    }, {
-      "host" => "server1"
-    }]
-
-    sample events do
-      insist { subject[0]["tags"] } == [ "throttled" ]
-      insist { subject[1]["tags"] } == nil
-    end
-  end
-  
-  describe "after_count exceeded" do
-    config <<-CONFIG
-      filter {
-        throttle {
-          period => 60
-          before_count => 2
-          after_count => 3
-          key => "%{host}"
-          add_tag => [ "throttled" ]
-        }
-      }
-    CONFIG
-
-    events = [{
-      "host" => "server1"
-    }, {
-      "host" => "server1"
-    }, {
-      "host" => "server1"
-    }, {
-      "host" => "server1"
-    }]
-
-    sample events do
-      insist { subject[0]["tags"] } == [ "throttled" ]
-      insist { subject[1]["tags"] } == nil
-      insist { subject[2]["tags"] } == nil
-      insist { subject[3]["tags"] } == [ "throttled" ]
-    end
-  end
-  
-  describe "different keys" do
-    config <<-CONFIG
-      filter {
-        throttle {
-          period => 60
-          after_count => 2
-          key => "%{host}"
-          add_tag => [ "throttled" ]
-        }
-      }
-    CONFIG
-
-    events = [{
-      "host" => "server1"
-    }, {
-      "host" => "server2"
-    }, {
-      "host" => "server3"
-    }, {
-      "host" => "server4"
-    }]
-
-    sample events do
-      subject.each { | s |
-        insist { s["tags"] } == nil
-      }
-    end
-  end
-  
-  describe "composite key" do
-    config <<-CONFIG
-      filter {
-        throttle {
-          period => 60
-          after_count => 1
-          key => "%{host}%{message}"
-          add_tag => [ "throttled" ]
-        }
-      }
-    CONFIG
-
-    events = [{
-      "host" => "server1",
-      "message" => "foo"
-    }, {
-      "host" => "server1",
-      "message" => "bar"
-    }, {
-      "host" => "server2",
-      "message" => "foo"
-    }, {
-      "host" => "server2",
-      "message" => "bar"
-    }]
-
-    sample events do
-      subject.each { | s |
-        insist { s["tags"] } == nil
-      }
-    end
-  end
-  
-  describe "max_counter exceeded" do
-    config <<-CONFIG
-      filter {
-        throttle {
-          period => 60
-          after_count => 1
-          max_counters => 2
-          key => "%{message}"
-          add_tag => [ "throttled" ]
-        }
-      }
-    CONFIG
-
-    events = [{
-      "message" => "foo"
-    }, {
-      "message" => "bar"
-    }, {
-      "message" => "poo"
-    }, {
-      "message" => "foo"
-    }]
-
-    sample events do
-      insist { subject[3]["tags"] } == nil
-    end
-  end
-
-end # LogStash::Filters::Throttle
-
diff --git a/spec/filters/urldecode.rb b/spec/filters/urldecode.rb
deleted file mode 100644
index 4fe3a4f5297..00000000000
--- a/spec/filters/urldecode.rb
+++ /dev/null
@@ -1,55 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/urldecode"
-
-describe LogStash::Filters::Urldecode do
-  extend LogStash::RSpec
-
-  describe "urldecode of correct urlencoded data" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        urldecode {
-        }
-      }
-    CONFIG
-
-    sample("message" => "http%3A%2F%2Flogstash.net%2Fdocs%2F1.3.2%2Ffilters%2Furldecode") do
-      insist { subject["message"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
-    end
-  end
-
-  describe "urldecode of incorrect urlencoded data" do
-    config <<-CONFIG
-      filter {
-        urldecode {
-        }
-      }
-    CONFIG
-
-    sample("message" => "http://logstash.net/docs/1.3.2/filters/urldecode") do
-      insist { subject["message"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
-    end
-  end
-
-   describe "urldecode with all_fields set to true" do
-    # The logstash config goes here.
-    # At this time, only filters are supported.
-    config <<-CONFIG
-      filter {
-        urldecode {
-          all_fields => true
-        }
-      }
-    CONFIG
-
-    sample("message" => "http%3A%2F%2Flogstash.net%2Fdocs%2F1.3.2%2Ffilters%2Furldecode", "nonencoded" => "http://logstash.net/docs/1.3.2/filters/urldecode") do
-      insist { subject["message"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
-      insist { subject["nonencoded"] } == "http://logstash.net/docs/1.3.2/filters/urldecode"
-    end
-
-  end
-
-end
diff --git a/spec/filters/useragent.rb b/spec/filters/useragent.rb
deleted file mode 100644
index d7d83e0e418..00000000000
--- a/spec/filters/useragent.rb
+++ /dev/null
@@ -1,44 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "logstash/filters/useragent"
-
-describe LogStash::Filters::UserAgent do
-  extend LogStash::RSpec
-
-  describe "defaults" do
-    config <<-CONFIG
-      filter {
-        useragent {
-          source => "message"
-          target => "ua"
-        }
-      }
-    CONFIG
-
-    sample "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.63 Safari/537.31" do
-      insist { subject }.include?("ua")
-      insist { subject["ua"]["name"] } == "Chrome"
-      insist { subject["ua"]["os"] } == "Linux"
-      insist { subject["ua"]["major"] } == "26"
-      insist { subject["ua"]["minor"] } == "0"
-    end
-  end
-
-  describe "" do
-    config <<-CONFIG
-      filter {
-        useragent {
-          source => "message"
-        }
-      }
-    CONFIG
-
-    sample "Mozilla/5.0 (X11; Linux i686) AppleWebKit/537.31 (KHTML, like Gecko) Chrome/26.0.1410.63 Safari/537.31" do
-      insist { subject["name"] } == "Chrome"
-      insist { subject["os"] } == "Linux"
-      insist { subject["major"] } == "26"
-      insist { subject["minor"] } == "0"
-    end
-  end
-end
diff --git a/spec/filters/xml.rb b/spec/filters/xml.rb
deleted file mode 100644
index 44583348aca..00000000000
--- a/spec/filters/xml.rb
+++ /dev/null
@@ -1,157 +0,0 @@
-require "test_utils"
-require "logstash/filters/xml"
-
-describe LogStash::Filters::Xml do
-  extend LogStash::RSpec
-
-  describe "parse standard xml (Deprecated checks)" do
-    config <<-CONFIG
-    filter {
-      xml {
-        source => "raw"
-        target => "data"
-      }
-    }
-    CONFIG
-
-    sample("raw" => '<foo key="value"/>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == {"key" => "value"}
-    end
-
-    #From parse xml with array as a value
-    sample("raw" => '<foo><key>value1</key><key>value2</key></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == {"key" => ["value1", "value2"]}
-    end
-
-    #From parse xml with hash as a value
-    sample("raw" => '<foo><key1><key2>value</key2></key1></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == {"key1" => [{"key2" => ["value"]}]}
-    end
-
-    #From bad xml
-    sample("raw" => '<foo /') do
-      insist { subject["tags"] }.include?("_xmlparsefailure")
-    end
-  end
-
-  describe "parse standard xml but do not store (Deprecated checks)" do
-    config <<-CONFIG
-    filter {
-      xml {
-        source => "raw"
-        target => "data"
-        store_xml => false
-      }
-    }
-    CONFIG
-
-    sample("raw" => '<foo key="value"/>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == nil
-    end
-  end
-
-  describe "parse xml and store values with xpath (Deprecated checks)" do
-    config <<-CONFIG
-    filter {
-      xml {
-        source => "raw"
-        target => "data"
-        xpath => [ "/foo/key/text()", "xpath_field" ]
-      }
-    }
-    CONFIG
-
-    # Single value
-    sample("raw" => '<foo><key>value</key></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["xpath_field"]} == ["value"]
-    end
-
-    #Multiple values
-    sample("raw" => '<foo><key>value1</key><key>value2</key></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["xpath_field"]} == ["value1","value2"]
-    end
-  end
-
-  ## New tests
-
-  describe "parse standard xml" do
-    config <<-CONFIG
-    filter {
-      xml {
-        source => "xmldata"
-        target => "data"
-      }
-    }
-    CONFIG
-
-    sample("xmldata" => '<foo key="value"/>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == {"key" => "value"}
-    end
-
-    #From parse xml with array as a value
-    sample("xmldata" => '<foo><key>value1</key><key>value2</key></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == {"key" => ["value1", "value2"]}
-    end
-
-    #From parse xml with hash as a value
-    sample("xmldata" => '<foo><key1><key2>value</key2></key1></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == {"key1" => [{"key2" => ["value"]}]}
-    end
-
-    #From bad xml
-    sample("xmldata" => '<foo /') do
-      insist { subject["tags"] }.include?("_xmlparsefailure")
-    end
-  end
-
-  describe "parse standard xml but do not store" do
-    config <<-CONFIG
-    filter {
-      xml {
-        source => "xmldata"
-        target => "data"
-        store_xml => false
-      }
-    }
-    CONFIG
-
-    sample("xmldata" => '<foo key="value"/>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["data"]} == nil
-    end
-  end
-
-  describe "parse xml and store values with xpath" do
-    config <<-CONFIG
-    filter {
-      xml {
-        source => "xmldata"
-        target => "data"
-        xpath => [ "/foo/key/text()", "xpath_field" ]
-      }
-    }
-    CONFIG
-
-    # Single value
-    sample("xmldata" => '<foo><key>value</key></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["xpath_field"]} == ["value"]
-    end
-
-    #Multiple values
-    sample("xmldata" => '<foo><key>value1</key><key>value2</key></foo>') do
-      insist { subject["tags"] }.nil?
-      insist { subject["xpath_field"]} == ["value1","value2"]
-    end
-  end
-
-end
diff --git a/spec/inputs/base_spec.rb b/spec/inputs/base_spec.rb
new file mode 100644
index 00000000000..e53280f86a0
--- /dev/null
+++ b/spec/inputs/base_spec.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+
+describe "LogStash::Inputs::Base#fix_streaming_codecs" do
+  it "should carry the charset setting along when switching" do
+    require "logstash/inputs/tcp"
+    require "logstash/codecs/plain"
+    plain = LogStash::Codecs::Plain.new("charset" => "CP1252")
+    tcp = LogStash::Inputs::Tcp.new("codec" => plain, "port" => 3333)
+    tcp.instance_eval { fix_streaming_codecs }
+    insist { tcp.codec.charset } == "CP1252"
+  end
+end
diff --git a/spec/inputs/collectd.rb b/spec/inputs/collectd.rb
deleted file mode 100644
index 939db0967df..00000000000
--- a/spec/inputs/collectd.rb
+++ /dev/null
@@ -1,329 +0,0 @@
-require "test_utils"
-require "socket"
-require "tempfile"
-
-describe "inputs/collectd", :socket => true do
-  extend LogStash::RSpec
-  udp_sock = UDPSocket.new(Socket::AF_INET)
-
-  describe "parses a normal packet" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type => "collectd"
-          host => "127.0.0.1"
-          # normal collectd port + 1
-          port => 25827
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      # Actual data :D
-      msg = ["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"].pack('H*')
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      sleep 1
-      insist { queue.size } == 28
-
-      events = 3.times.collect { queue.pop }
-      # Checking the timestamp fails with:
-      # Expected "2013-12-31T10:14:47.811Z", but got "2013-12-31T10:14:47.811Z"
-      # So... yeah.....
-
-      #timestamp = Time.iso8601("2013-12-31T10:14:47.811Z")
-
-      #insist { events[0]['@timestamp'] } == timestamp.utc
-      insist { events[0]['host'] } == "lieters-klaptop.prot.plexis.eu"
-      insist { events[0]['plugin'] } == "interface"
-      insist { events[0]['plugin_instance'] } == "wlan0"
-      insist { events[0]['collectd_type'] } == "if_errors"
-      insist { events[0]['rx'] } == 0
-      insist { events[0]['tx'] } == 0
-
-      #insist { events[2]['@timestamp'] } == timestamp
-      insist { events[2]['host'] } == "lieters-klaptop.prot.plexis.eu"
-      insist { events[2]['plugin'] } == "entropy"
-      insist { events[2]['collectd_type'] } == "entropy"
-      insist { events[2]['value'] } == 157.0
-    end
-  end
-
-  # Create an authfile
-  authfile = Tempfile.new('logstash-collectd-authfile')
-  File.open(authfile.path, "a") do |fd|
-    fd.puts("pieter: aapje1234")
-  end
-
-  describe "Parses correctly signed packet" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-          authfile       => "#{authfile.path}"
-          security_level => "Sign"
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-      msg = ["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"].pack('H*')
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 3
-
-      insist { queue.size } == 24
-    end
-  end
-
-  describe "Does not parse incorrectly signed packet" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-          authfile       => "#{authfile.path}"
-          security_level => "Sign"
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      # Wrong hash in packet
-      msg = ["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"].pack('H*')
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 1
-
-      insist { queue.size } == 0
-    end # input
-  end # describe "Does not parse incorrectly signed packet"
-
-  describe "parses encrypted packet" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-          authfile       => "#{authfile.path}"
-          security_level => "Encrypt"
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      msg = ["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"].pack('H*')
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 2
-
-      insist { queue.size } == 24
-    end # input
-  end # describe "parses encrypted packet"
-
-  describe "does not parse unencrypted when configured to do so" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-          authfile       => "#{authfile.path}"
-          security_level => "Encrypt"
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      msg = ["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"].pack('H*')
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 2
-
-      insist { queue.size } == 0
-
-    end # input
-  end # describe
-
-  describe "changes NaN to 0 in the default config" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      msg = ['000000356b756d696e613a70726f64756374696f6e3a6c6965746572732d6b6c6170746f702e70726f742e706c657869732e6575000008000c14baed07bfc492e90009000c00000002800000000002000c63707566726571000004000c63707566726571000005000631000006000f0001010000000084d7c7410008000c14baed07bfc39a790005000630000006000f0001010000000084d7c7410008000c14baed07bfca78480002000764660000030009726f6f74000004000f64665f636f6d706c6578000005000966726565000006000f000101000000002e82ef410008000c14baed07bfcaa14c0005000d7265736572766564000006000f00010100000000a09ec5410008000c14baed07bfcaad4f0005000975736564000006000f00010100000080080d04420008000c14baed07bfcb0f2900030009626f6f74000005000966726565000006000f0001010000000048fcca410008000c14baed07bfcb1bc20005000d7265736572766564000006000f00010100000000c0cc90410008000c14baed07bfcb285b0005000975736564000006000f00010100000000009586410008000c14baed07bfcb489500030009686f6d65000005000966726565000006000f000101000000c0557a12420008000c14baed07bfcb54980005000d7265736572766564000006000f000101000000000020e4410008000c14baed07bfcb5f6f0005000975736564000006000f00010100000000d2181c420008000c14baed07bfc2f24f000200086370750000030006310000040008637075000005000e696e74657272757074000006000f00010200000000000000020008000c14baed07bfc2d4b80005000969646c65000006000f0001020000000000022ada0008000c14baed07bfc2bc68000500096e696365000006000f00010200000000000000080008000c14baed07bfeb5d1e0002000e6d656d6361636865640000030005000004001a6d656d6361636865645f636f6e6e656374696f6e73000005000c63757272656e74000006000f00010100000000000014400008000c14baed07bfeb947c000400166d656d6361636865645f636f6d6d616e640000050008676574000006000f00010200000000000000000008000c14baed07bfebb42100050008736574000006000f00010200000000000000000008000c14baed07bfebcd9e0005000a666c757368000006000f00010200000000000000000008000c14baed07bfebe5ee0005000a746f756368000006000f00010200000000000000000008000c14baed07bfebfdf5000400126d656d6361636865645f6f7073000005000968697473000006000f00010200000000000000000008000c14baed07bfec0ea80005000b6d6973736573000006000f00010200000000000000000008000c14baed07bfec278f00050010696e63725f6d6973736573000006000f00010200000000000000000008000c14baed07bfec36350005000e696e63725f68697473000006000f00010200000000000000000008000c14baed07bfec45bc00050010646563725f6d6973736573000006000f00010200000000000000000008000c14baed07bfec54620005000e646563725f68697473000006000f00010200000000000000000008000c14baed07bfeca08e0004000d70735f636f756e740000050005000006001800020101000000000000f87f00000000000010400008000c14baed07bfecce42000400146d656d6361636865645f6974656d73000005000c63757272656e74000006000f0001010000000000000000'].pack('H*')
-
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 2
-
-      insist { queue.size } == 27
-
-      events = 26.times.collect { queue.pop }
-
-      insist { events[25]['tags'] } == ['_collectdNaN']
-      insist { events[25]['threads'] } == 4
-      insist { events[25]['processes'] } == 0
-
-    end # input do
-  end # describe
-
-  describe "changes NaN to -1 when configged to do so" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-          nan_value      => -1
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      msg = ['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'].pack('H*')
-
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 2
-
-      insist { queue.size } == 27
-
-      events = 26.times.collect { queue.pop }
-
-      insist { events[25]['tags'] } == ['_collectdNaN']
-      insist { events[25]['threads'] } == 4
-      insist { events[25]['processes'] } == -1
-
-    end # input do
-  end # describe
-
-  describe "Drops the event when NaN is found" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-          nan_handeling  => 'drop'
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      msg = ['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'].pack('H*')
-
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 2
-
-      insist { queue.size } == 26
-    end # input do
-  end # describe
-
-  describe "Empty nan_tag doesnt add a tag" do
-    config <<-CONFIG
-      input {
-        collectd {
-          type           => "collectd"
-          host           => "127.0.0.1"
-          # normal collectd port + 1
-          port           => 25827
-          nan_tag        => ''
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # Sleep so collectd can init itself
-      sleep 3
-
-      msg = ['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'].pack('H*')
-
-      udp_sock.send(msg, 0, "127.0.0.1", 25827)
-
-      # give it time to process
-      sleep 2
-
-      events = 26.times.collect { queue.pop }
-
-      insist { events[25]['tags'] }.nil?
-    end # input do
-  end # describe
-
-end # describe "inputs/collectd"
-
diff --git a/spec/inputs/file.rb b/spec/inputs/file.rb
deleted file mode 100644
index 9126719db99..00000000000
--- a/spec/inputs/file.rb
+++ /dev/null
@@ -1,132 +0,0 @@
-# encoding: utf-8
-
-require "test_utils"
-require "tempfile"
-
-describe "inputs/file" do
-  extend LogStash::RSpec
-
-  describe "starts at the end of an existing file" do
-    tmp_file = Tempfile.new('logstash-spec-input-file')
-
-    config <<-CONFIG
-      input {
-        file {
-          type => "blah"
-          path => "#{tmp_file.path}"
-          sincedb_path => "/dev/null"
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      File.open(tmp_file, "w") do |fd|
-        fd.puts("ignore me 1")
-        fd.puts("ignore me 2")
-      end
-
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      # at this point even if pipeline.ready? == true the plugins
-      # threads might still be initializing so we cannot know when the
-      # file plugin will have seen the original file, it could see it
-      # after the first(s) hello world appends below, hence the
-      # retry logic.
-
-      retries = 0
-      loop do
-        insist { retries } < 20 # 2 secs should be plenty?
-
-        File.open(tmp_file, "a") do |fd|
-          fd.puts("hello")
-          fd.puts("world")
-        end
-
-        if queue.size >= 2
-          events = 2.times.collect { queue.pop }
-          insist { events[0]["message"] } == "hello"
-          insist { events[1]["message"] } == "world"
-          break
-        end
-
-        sleep(0.1)
-        retries += 1
-      end
-    end
-  end
-
-  describe "can start at the beginning of an existing file" do
-    tmp_file = Tempfile.new('logstash-spec-input-file')
-
-    config <<-CONFIG
-      input {
-        file {
-          type => "blah"
-          path => "#{tmp_file.path}"
-          start_position => "beginning"
-          sincedb_path => "/dev/null"
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      File.open(tmp_file, "a") do |fd|
-        fd.puts("hello")
-        fd.puts("world")
-      end
-
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      events = 2.times.collect { queue.pop }
-      insist { events[0]["message"] } == "hello"
-      insist { events[1]["message"] } == "world"
-    end
-  end
-
-  describe "restarts at the sincedb value" do
-    tmp_file = Tempfile.new('logstash-spec-input-file')
-    tmp_sincedb = Tempfile.new('logstash-spec-input-file-sincedb')
-
-    config <<-CONFIG
-      input {
-        file {
-          type => "blah"
-          path => "#{tmp_file.path}"
-          start_position => "beginning"
-          sincedb_path => "#{tmp_sincedb.path}"
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      File.open(tmp_file, "w") do |fd|
-        fd.puts("hello")
-        fd.puts("world")
-      end
-
-      t = Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      events = 2.times.collect { queue.pop }
-      pipeline.shutdown
-      t.join
-
-      File.open(tmp_file, "a") do |fd|
-        fd.puts("foo")
-        fd.puts("bar")
-        fd.puts("baz")
-      end
-
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      events = 3.times.collect { queue.pop }
-
-      insist { events[0]["message"] } == "foo"
-      insist { events[1]["message"] } == "bar"
-      insist { events[2]["message"] } == "baz"
-    end
-  end
-end
diff --git a/spec/inputs/gelf.rb b/spec/inputs/gelf.rb
deleted file mode 100644
index e2cab136d36..00000000000
--- a/spec/inputs/gelf.rb
+++ /dev/null
@@ -1,52 +0,0 @@
-
-require "test_utils"
-require "gelf"
-describe "inputs/gelf" do
-  extend LogStash::RSpec
-
-  describe "reads chunked gelf messages " do
-    port = 12209
-    host = "127.0.0.1"
-    chunksize = 1420
-    gelfclient = GELF::Notifier.new(host,port,chunksize)
-
-    config <<-CONFIG
-input {
-  gelf {
-    port => "#{port}"
-    host => "#{host}"
-  }
-}
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-      
-      # generate random characters (message is zipped!) from printable ascii ( SPACE till ~ )  
-      # to trigger gelf chunking
-      s = StringIO.new
-      for i in 1..2000 
-          s << 32 + rand(126-32)
-      end
-      large_random = s.string
-      
-      [ "hello", 
-        "world", 
-        large_random, 
-        "we survived gelf!" 
-      ].each do |m| 
-  	gelfclient.notify!( "short_message" => m )
-        # poll at most 10 times 
-        waits = 0 
-        while waits < 10 and queue.size == 0
-           sleep 0.1 
-           waits += 1
-        end
-        insist { queue.size } > 0  
-        insist { queue.pop["message"] } == m
-      end
-
-    end
-  end
-end
diff --git a/spec/inputs/generator.rb b/spec/inputs/generator.rb
deleted file mode 100644
index 45579f620d2..00000000000
--- a/spec/inputs/generator.rb
+++ /dev/null
@@ -1,30 +0,0 @@
-require "test_utils"
-
-describe "inputs/generator", :performance => true do
-  extend LogStash::RSpec
-
-  describe "generate events" do
-    event_count = 100000 + rand(50000)
-
-    config <<-CONFIG
-      input {
-        generator {
-          type => "blah"
-          count => #{event_count}
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      start = Time.now
-      Thread.new { pipeline.run }
-      event_count.times do |i|
-        event = queue.pop
-        insist { event["sequence"] } == i
-      end
-      duration = Time.now - start
-      puts "inputs/generator rate: #{"%02.0f/sec" % (event_count / duration)}, elapsed: #{duration}s"
-      pipeline.shutdown
-    end # input
-  end
-end
diff --git a/spec/inputs/imap.rb b/spec/inputs/imap.rb
deleted file mode 100644
index 51da9a61fd3..00000000000
--- a/spec/inputs/imap.rb
+++ /dev/null
@@ -1,92 +0,0 @@
-# encoding: utf-8
-
-require "logstash/inputs/imap"
-require "mail"
-
-describe LogStash::Inputs::IMAP do
-  user = "logstash"
-  password = "secret"
-  msg_time = Time.new
-  msg_text = "foo\nbar\nbaz"
-  msg_html = "<p>a paragraph</p>\n\n"
-
-  subject do
-    Mail.new do
-      from     "me@example.com"
-      to       "you@example.com"
-      subject  "logstash imap input test"
-      date     msg_time
-      body     msg_text
-      add_file :filename => "some.html", :content => msg_html
-    end
-  end
-
-  context "with both text and html parts" do
-    context "when no content-type selected" do
-      it "should select text/plain part" do
-        config = {"type" => "imap", "host" => "localhost",
-                  "user" => "#{user}", "password" => "#{password}"}
-
-        input = LogStash::Inputs::IMAP.new config
-        input.register
-        event = input.parse_mail(subject)
-        insist { event["message"] } == msg_text
-      end
-    end
-
-    context "when text/html content-type selected" do
-      it "should select text/html part" do
-        config = {"type" => "imap", "host" => "localhost",
-                  "user" => "#{user}", "password" => "#{password}",
-                  "content_type" => "text/html"}
-
-        input = LogStash::Inputs::IMAP.new config
-        input.register
-        event = input.parse_mail(subject)
-        insist { event["message"] } == msg_html
-      end
-    end
-  end
-
-  context "when subject is in RFC 2047 encoded-word format" do
-    it "should be decoded" do
-      subject.subject = "=?iso-8859-1?Q?foo_:_bar?="
-      config = {"type" => "imap", "host" => "localhost",
-                "user" => "#{user}", "password" => "#{password}"}
-
-      input = LogStash::Inputs::IMAP.new config
-      input.register
-      event = input.parse_mail(subject)
-      insist { event["subject"] } == "foo : bar"
-    end
-  end
-
-  context "with multiple values for same header" do
-    it "should add 2 values as array in event" do
-      subject.received = "test1"
-      subject.received = "test2"
-
-      config = {"type" => "imap", "host" => "localhost",
-                "user" => "#{user}", "password" => "#{password}"}
-
-      input = LogStash::Inputs::IMAP.new config
-      input.register
-      event = input.parse_mail(subject)
-      insist { event["received"] } == ["test1", "test2"]
-    end
-
-    it "should add more than 2 values as array in event" do
-      subject.received = "test1"
-      subject.received = "test2"
-      subject.received = "test3"
-
-      config = {"type" => "imap", "host" => "localhost",
-                "user" => "#{user}", "password" => "#{password}"}
-
-      input = LogStash::Inputs::IMAP.new config
-      input.register
-      event = input.parse_mail(subject)
-      insist { event["received"] } == ["test1", "test2", "test3"]
-    end
-  end
-end
diff --git a/spec/inputs/log4j.rb b/spec/inputs/log4j.rb
deleted file mode 100644
index 8e38bb4db0d..00000000000
--- a/spec/inputs/log4j.rb
+++ /dev/null
@@ -1,13 +0,0 @@
-# encoding: utf-8
-
-require "logstash/plugin"
-
-describe "inputs/log4j" do
-
-  it "should register" do
-    input = LogStash::Plugin.lookup("input", "log4j").new("mode" => "client")
-
-    # register will try to load jars and raise if it cannot find jars or if org.apache.log4j.spi.LoggingEvent class is not present
-    expect {input.register}.to_not raise_error
-  end
-end
diff --git a/spec/inputs/redis.rb b/spec/inputs/redis.rb
deleted file mode 100644
index c7f6b513fbc..00000000000
--- a/spec/inputs/redis.rb
+++ /dev/null
@@ -1,63 +0,0 @@
-require "test_utils"
-require "redis"
-
-def populate(key, event_count)
-  require "logstash/event"
-  redis = Redis.new(:host => "localhost")
-  event_count.times do |value|
-    event = LogStash::Event.new("sequence" => value)
-    Stud::try(10.times) do
-      redis.rpush(key, event.to_json)
-    end
-  end
-end
-
-def process(pipeline, queue, event_count)
-  sequence = 0
-  Thread.new { pipeline.run }
-  event_count.times do |i|
-    event = queue.pop
-    insist { event["sequence"] } == i
-  end
-  pipeline.shutdown
-end # process
-
-describe "inputs/redis", :redis => true do
-  extend LogStash::RSpec
-
-  describe "read events from a list" do
-    key = 10.times.collect { rand(10).to_s }.join("")
-    event_count = 1000 + rand(50)
-    config <<-CONFIG
-      input {
-        redis {
-          type => "blah"
-          key => "#{key}"
-          data_type => "list"
-        }
-      }
-    CONFIG
-
-    before(:each) { populate(key, event_count) }
-
-    input { |pipeline, queue| process(pipeline, queue, event_count) }
-  end
-
-  describe "read events from a list with batch_count=5" do
-    key = 10.times.collect { rand(10).to_s }.join("")
-    event_count = 1000 + rand(50)
-    config <<-CONFIG
-      input {
-        redis {
-          type => "blah"
-          key => "#{key}"
-          data_type => "list"
-          batch_count => #{rand(20)+1}
-        }
-      }
-    CONFIG
-
-    before(:each) { populate(key, event_count) }
-    input { |pipeline, queue| process(pipeline, queue, event_count) }
-  end
-end
diff --git a/spec/inputs/syslog.rb b/spec/inputs/syslog.rb
deleted file mode 100644
index 37fa3efcfed..00000000000
--- a/spec/inputs/syslog.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-# coding: utf-8
-require "test_utils"
-require "socket"
-
-describe "inputs/syslog", :socket => true do
-  extend LogStash::RSpec
-
-  describe "properly handles priority, severity and facilities" do
-    port = 5511
-    event_count = 10
-
-    config <<-CONFIG
-      input {
-        syslog {
-          type => "blah"
-          port => #{port}
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      event_count.times do |i|
-        socket.puts("<164>Oct 26 15:19:25 1.2.3.4 %ASA-4-106023: Deny udp src DRAC:10.1.2.3/43434 dst outside:192.168.0.1/53 by access-group \"acl_drac\" [0x0, 0x0]")
-      end
-      socket.close
-
-      events = event_count.times.collect { queue.pop }
-      event_count.times do |i|
-        insist { events[i]["priority"] } == 164
-        insist { events[i]["severity"] } == 4
-        insist { events[i]["facility"] } == 20
-      end
-    end
-  end
-end
-
diff --git a/spec/inputs/tcp.rb b/spec/inputs/tcp.rb
deleted file mode 100644
index e4ea8312aba..00000000000
--- a/spec/inputs/tcp.rb
+++ /dev/null
@@ -1,176 +0,0 @@
-# coding: utf-8
-require "test_utils"
-require "socket"
-
-describe "inputs/tcp", :socket => true do
-  extend LogStash::RSpec
-
-  describe "read plain with unicode" do
-    event_count = 10
-    port = 5511
-    config <<-CONFIG
-      input {
-        tcp {
-          port => #{port}
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      event_count.times do |i|
-        # unicode smiley for testing unicode support!
-        socket.puts("#{i} ")
-      end
-      socket.close
-
-      events = event_count.times.collect { queue.pop }
-      event_count.times do |i|
-        insist { events[i]["message"] } == "#{i} "
-      end
-    end # input
-  end
-
-  describe "read events with plain codec and ISO-8859-1 charset" do
-    port = 5513
-    charset = "ISO-8859-1"
-    config <<-CONFIG
-      input {
-        tcp {
-          port => #{port}
-          codec => plain { charset => "#{charset}" }
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      text = "\xA3" # the  symbol in ISO-8859-1 aka Latin-1
-      text.force_encoding("ISO-8859-1")
-      socket.puts(text)
-      socket.close
-
-      event = queue.pop
-      # Make sure the 0xA3 latin-1 code converts correctly to UTF-8.
-      pending("charset conv broken") do
-        insist { event["message"].size } == 1
-        insist { event["message"].bytesize } == 2
-        insist { event["message"] } == ""
-      end
-    end # input
-  end
-
-  describe "read events with json codec" do
-    port = 5514
-    config <<-CONFIG
-      input {
-        tcp {
-          port => #{port}
-          codec => json
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      data = {
-        "hello" => "world",
-        "foo" => [1,2,3],
-        "baz" => { "1" => "2" },
-        "host" => "example host"
-      }
-
-      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      socket.puts(data.to_json)
-      socket.close
-
-      event = queue.pop
-      insist { event["hello"] } == data["hello"]
-      insist { event["foo"] } == data["foo"]
-      insist { event["baz"] } == data["baz"]
-
-      # Make sure the tcp input, w/ json codec, uses the event's 'host' value,
-      # if present, instead of providing its own
-      insist { event["host"] } == data["host"]
-    end # input
-  end
-
-  describe "read events with json codec (testing 'host' handling)" do
-    port = 5514
-    config <<-CONFIG
-      input {
-        tcp {
-          port => #{port}
-          codec => json
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      data = {
-        "hello" => "world"
-      }
-
-      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      socket.puts(data.to_json)
-      socket.close
-
-      event = queue.pop
-      insist { event["hello"] } == data["hello"]
-      insist { event }.include?("host")
-    end # input
-  end
-
-  describe "read events with json_lines codec" do
-    port = 5515
-    config <<-CONFIG
-      input {
-        tcp {
-          port => #{port}
-          codec => json_lines
-        }
-      }
-    CONFIG
-
-    input do |pipeline, queue|
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-
-      data = {
-        "hello" => "world",
-        "foo" => [1,2,3],
-        "baz" => { "1" => "2" },
-        "idx" => 0
-      }
-
-      socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
-      (1..5).each do |idx|
-        data["idx"] = idx
-        socket.puts(data.to_json+"\n")
-      end # do
-      socket.close
-
-      (1..5).each do |idx|
-        event = queue.pop
-        insist { event["hello"] } == data["hello"]
-        insist { event["foo"] } == data["foo"]
-        insist { event["baz"] } == data["baz"]
-        insist { event["idx"] } == idx
-      end # do
-    end # input
-  end # describe
-end
-
-
-
diff --git a/spec/outputs/csv.rb b/spec/outputs/csv.rb
deleted file mode 100644
index 29fa719c89a..00000000000
--- a/spec/outputs/csv.rb
+++ /dev/null
@@ -1,266 +0,0 @@
-require "csv"
-require "tempfile"
-require "test_utils"
-require "logstash/outputs/csv"
-
-describe LogStash::Outputs::CSV do
-  extend LogStash::RSpec
-
-  describe "Write a single field to a csv file" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          add_field => ["foo","bar"]
-          count => 1
-        }
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => "foo"
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = File.readlines(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0]} == "bar\n"
-    end
-  end
-
-  describe "write multiple fields and lines to a csv file" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          add_field => ["foo", "bar", "baz", "quux"]
-          count => 2
-        }
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["foo", "baz"]
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = File.readlines(tmpfile.path)
-      insist {lines.count} == 2
-      insist {lines[0]} == "bar,quux\n"
-      insist {lines[1]} == "bar,quux\n"
-    end
-  end
-
-  describe "missing event fields are empty in csv" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          add_field => ["foo","bar", "baz", "quux"]
-          count => 1
-        }
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["foo", "not_there", "baz"]
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = File.readlines(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0]} == "bar,,quux\n"
-    end
-  end
-
-  describe "commas are quoted properly" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          add_field => ["foo","one,two", "baz", "quux"]
-          count => 1
-        }
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["foo", "baz"]
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = File.readlines(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0]} == "\"one,two\",quux\n"
-    end
-  end
-
-  describe "new lines are quoted properly" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          add_field => ["foo","one\ntwo", "baz", "quux"]
-          count => 1
-        }
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["foo", "baz"]
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = CSV.read(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0][0]} == "one\ntwo"
-    end
-  end
-
-  describe "fields that are are objects are written as JSON" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          message => '{"foo":{"one":"two"},"baz": "quux"}'
-          count => 1
-        }
-      }
-      filter {
-        json { source => "message"}
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["foo", "baz"]
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = CSV.read(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0][0]} == '{"one":"two"}'
-    end
-  end
-
-  describe "can address nested field using field reference syntax" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          message => '{"foo":{"one":"two"},"baz": "quux"}'
-          count => 1
-        }
-      }
-      filter {
-        json { source => "message"}
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["[foo][one]", "baz"]
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = CSV.read(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0][0]} == "two"
-      insist {lines[0][1]} == "quux"
-    end
-  end
-
-  describe "missing nested field is blank" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          message => '{"foo":{"one":"two"},"baz": "quux"}'
-          count => 1
-        }
-      }
-      filter {
-        json { source => "message"}
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["[foo][missing]", "baz"]
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = File.readlines(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0]} == ",quux\n"
-    end
-  end
-
-  describe "can choose field seperator" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          message => '{"foo":"one","bar": "two"}'
-          count => 1
-        }
-      }
-      filter {
-        json { source => "message"}
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["foo", "bar"]
-          csv_options => {"col_sep" => "|"}
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = File.readlines(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0]} == "one|two\n"
-    end
-  end
-  describe "can choose line seperator" do
-    tmpfile = Tempfile.new('logstash-spec-output-csv')
-    config <<-CONFIG
-      input {
-        generator {
-          message => '{"foo":"one","bar": "two"}'
-          count => 2
-        }
-      }
-      filter {
-        json { source => "message"}
-      }
-      output {
-        csv {
-          path => "#{tmpfile.path}"
-          fields => ["foo", "bar"]
-          csv_options => {"col_sep" => "|" "row_sep" => "\t"}
-        }
-      }
-    CONFIG
-
-    agent do
-      lines = File.readlines(tmpfile.path)
-      insist {lines.count} == 1
-      insist {lines[0]} == "one|two\tone|two\t"
-    end
-  end
-end
diff --git a/spec/outputs/elasticsearch.rb b/spec/outputs/elasticsearch.rb
deleted file mode 100644
index a41955e778c..00000000000
--- a/spec/outputs/elasticsearch.rb
+++ /dev/null
@@ -1,348 +0,0 @@
-require "test_utils"
-require "ftw"
-require "logstash/plugin"
-
-describe "outputs/elasticsearch" do
-  extend LogStash::RSpec
-
-  it "should register" do
-    output = LogStash::Plugin.lookup("output", "elasticsearch").new("embedded" => "false", "protocol" => "transport", "manage_template" => "false")
-
-    # register will try to load jars and raise if it cannot find jars
-    expect {output.register}.to_not raise_error
-  end
-
-  describe "ship lots of events w/ default index_type", :elasticsearch => true do
-    # Generate a random index name
-    index = 10.times.collect { rand(10).to_s }.join("")
-    type = 10.times.collect { rand(10).to_s }.join("")
-
-    # Write about 10000 events. Add jitter to increase likeliness of finding
-    # boundary-related bugs.
-    event_count = 10000 + rand(500)
-    flush_size = rand(200) + 1
-
-    config <<-CONFIG
-      input {
-        generator {
-          message => "hello world"
-          count => #{event_count}
-          type => "#{type}"
-        }
-      }
-      output {
-        elasticsearch {
-          host => "127.0.0.1"
-          index => "#{index}"
-          flush_size => #{flush_size}
-        }
-      }
-    CONFIG
-
-    agent do
-      # Try a few times to check if we have the correct number of events stored
-      # in ES.
-      #
-      # We try multiple times to allow final agent flushes as well as allowing
-      # elasticsearch to finish processing everything.
-      ftw = FTW::Agent.new
-      ftw.post!("http://localhost:9200/#{index}/_refresh")
-
-      # Wait until all events are available.
-      Stud::try(10.times) do
-        data = ""
-        response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        count = result["count"]
-        insist { count } == event_count
-      end
-
-      response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
-      data = ""
-      response.read_body { |chunk| data << chunk }
-      result = JSON.parse(data)
-      result["hits"]["hits"].each do |doc|
-        # With no 'index_type' set, the document type should be the type
-        # set on the input
-        insist { doc["_type"] } == type
-        insist { doc["_index"] } == index
-        insist { doc["_source"]["message"] } == "hello world"
-      end
-    end
-  end
-
-  describe "testing index_type", :elasticsearch => true do
-    describe "no type value" do
-      # Generate a random index name
-      index = 10.times.collect { rand(10).to_s }.join("")
-      event_count = 100 + rand(100)
-      flush_size = rand(200) + 1
-
-      config <<-CONFIG
-        input {
-          generator {
-            message => "hello world"
-            count => #{event_count}
-          }
-        }
-        output {
-          elasticsearch {
-            host => "127.0.0.1"
-            index => "#{index}"
-            flush_size => #{flush_size}
-          }
-        }
-      CONFIG
-
-      agent do
-        ftw = FTW::Agent.new
-        ftw.post!("http://localhost:9200/#{index}/_refresh")
-
-        # Wait until all events are available.
-        Stud::try(10.times) do
-          data = ""
-          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
-          response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
-          count = result["count"]
-          insist { count } == event_count
-        end
-
-        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
-        data = ""
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        result["hits"]["hits"].each do |doc|
-          insist { doc["_type"] } == "logs"
-        end
-      end
-    end
-
-    describe "default event type value" do
-      # Generate a random index name
-      index = 10.times.collect { rand(10).to_s }.join("")
-      event_count = 100 + rand(100)
-      flush_size = rand(200) + 1
-
-      config <<-CONFIG
-        input {
-          generator {
-            message => "hello world"
-            count => #{event_count}
-            type => "generated"
-          }
-        }
-        output {
-          elasticsearch {
-            host => "127.0.0.1"
-            index => "#{index}"
-            flush_size => #{flush_size}
-          }
-        }
-      CONFIG
-
-      agent do
-        ftw = FTW::Agent.new
-        ftw.post!("http://localhost:9200/#{index}/_refresh")
-
-        # Wait until all events are available.
-        Stud::try(10.times) do
-          data = ""
-          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
-          response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
-          count = result["count"]
-          insist { count } == event_count
-        end
-
-        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
-        data = ""
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        result["hits"]["hits"].each do |doc|
-          insist { doc["_type"] } == "generated"
-        end
-      end
-    end
-  end
-
-  describe "action => ...", :elasticsearch => true do
-    index_name = 10.times.collect { rand(10).to_s }.join("")
-
-    config <<-CONFIG
-      input {
-        generator {
-          message => "hello world"
-          count => 100
-        }
-      }
-      output {
-        elasticsearch {
-          host => "127.0.0.1"
-          index => "#{index_name}"
-        }
-      }
-    CONFIG
-
-
-    agent do
-      ftw = FTW::Agent.new
-      ftw.post!("http://localhost:9200/#{index_name}/_refresh")
-
-      # Wait until all events are available.
-      Stud::try(10.times) do
-        data = ""
-        response = ftw.get!("http://127.0.0.1:9200/#{index_name}/_count?q=*")
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        count = result["count"]
-        insist { count } == 100
-      end
-
-      response = ftw.get!("http://127.0.0.1:9200/#{index_name}/_search?q=*&size=1000")
-      data = ""
-      response.read_body { |chunk| data << chunk }
-      result = JSON.parse(data)
-      result["hits"]["hits"].each do |doc|
-        insist { doc["_type"] } == "logs"
-      end
-    end
-
-    describe "default event type value", :elasticsearch => true do
-      # Generate a random index name
-      index = 10.times.collect { rand(10).to_s }.join("")
-      event_count = 100 + rand(100)
-      flush_size = rand(200) + 1
-
-      config <<-CONFIG
-        input {
-          generator {
-            message => "hello world"
-            count => #{event_count}
-            type => "generated"
-          }
-        }
-        output {
-          elasticsearch {
-            host => "127.0.0.1"
-            index => "#{index}"
-            flush_size => #{flush_size}
-          }
-        }
-      CONFIG
-
-      agent do
-        ftw = FTW::Agent.new
-        ftw.post!("http://localhost:9200/#{index}/_refresh")
-
-        # Wait until all events are available.
-        Stud::try(10.times) do
-          data = ""
-          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
-          response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
-          count = result["count"]
-          insist { count } == event_count
-        end
-
-        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
-        data = ""
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        result["hits"]["hits"].each do |doc|
-          insist { doc["_type"] } == "generated"
-        end
-      end
-    end
-  end
-
-  describe "index template expected behavior", :elasticsearch => true do
-    ["node", "transport", "http"].each do |protocol|
-      context "with protocol => #{protocol}" do
-        subject do
-          require "logstash/outputs/elasticsearch"
-          settings = {
-            "manage_template" => true,
-            "template_overwrite" => true,
-            "protocol" => protocol,
-            "host" => "localhost"
-          }
-          next LogStash::Outputs::ElasticSearch.new(settings)
-        end
-
-        before :each do
-          # Delete all templates first.
-          require "elasticsearch"
-
-          # Clean ES of data before we start.
-          @es = Elasticsearch::Client.new
-          @es.indices.delete_template(:name => "*")
-
-          # This can fail if there are no indexes, ignore failure.
-          @es.indices.delete(:index => "*") rescue nil
-
-          subject.register
-
-          subject.receive(LogStash::Event.new("message" => "sample message here"))
-          subject.receive(LogStash::Event.new("somevalue" => 100))
-          subject.receive(LogStash::Event.new("somevalue" => 10))
-          subject.receive(LogStash::Event.new("somevalue" => 1))
-          subject.receive(LogStash::Event.new("country" => "us"))
-          subject.receive(LogStash::Event.new("country" => "at"))
-          subject.receive(LogStash::Event.new("geoip" => { "location" => [ 0.0, 0.0 ] }))
-          subject.buffer_flush(:final => true)
-          @es.indices.refresh
-
-          # Wait or fail until everything's indexed.
-          Stud::try(20.times) do
-            r = @es.search
-            insist { r["hits"]["total"] } == 7
-          end
-        end
-
-        it "permits phrase searching on string fields" do
-          results = @es.search(:q => "message:\"sample message\"")
-          insist { results["hits"]["total"] } == 1
-          insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
-        end
-
-        it "numbers dynamically map to a numeric type and permit range queries" do
-          results = @es.search(:q => "somevalue:[5 TO 105]")
-          insist { results["hits"]["total"] } == 2
-
-          values = results["hits"]["hits"].collect { |r| r["_source"]["somevalue"] }
-          insist { values }.include?(10)
-          insist { values }.include?(100)
-          reject { values }.include?(1)
-        end
-
-        it "creates .raw field fro any string field which is not_analyzed" do
-          results = @es.search(:q => "message.raw:\"sample message here\"")
-          insist { results["hits"]["total"] } == 1
-          insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
-
-          # partial or terms should not work.
-          results = @es.search(:q => "message.raw:\"sample\"")
-          insist { results["hits"]["total"] } == 0
-        end
-
-        it "make [geoip][location] a geo_point" do
-          results = @es.search(:body => { "filter" => { "geo_distance" => { "distance" => "1000km", "geoip.location" => { "lat" => 0.5, "lon" => 0.5 } } } })
-          insist { results["hits"]["total"] } == 1
-          insist { results["hits"]["hits"][0]["_source"]["geoip"]["location"] } == [ 0.0, 0.0 ]
-        end
-
-        it "should index stopwords like 'at' " do
-          results = @es.search(:body => { "facets" => { "t" => { "terms" => { "field" => "country" } } } })["facets"]["t"]
-          terms = results["terms"].collect { |t| t["term"] }
-
-          insist { terms }.include?("us")
-
-          # 'at' is a stopword, make sure stopwords are not ignored.
-          insist { terms }.include?("at")
-        end
-      end
-    end
-  end
-end
diff --git a/spec/outputs/elasticsearch_http.rb b/spec/outputs/elasticsearch_http.rb
deleted file mode 100644
index d1b1072e06a..00000000000
--- a/spec/outputs/elasticsearch_http.rb
+++ /dev/null
@@ -1,240 +0,0 @@
-require "test_utils"
-
-describe "outputs/elasticsearch_http", :elasticsearch => true do
-  extend LogStash::RSpec
-
-  describe "ship lots of events w/ default index_type" do
-    # Generate a random index name
-    index = 10.times.collect { rand(10).to_s }.join("")
-    type = 10.times.collect { rand(10).to_s }.join("")
-
-    # Write about 10000 events. Add jitter to increase likeliness of finding
-    # boundary-related bugs.
-    event_count = 10000 + rand(500)
-    flush_size = rand(200) + 1
-
-    config <<-CONFIG
-      input {
-        generator {
-          message => "hello world"
-          count => #{event_count}
-          type => "#{type}"
-        }
-      }
-      output {
-        elasticsearch_http {
-          host => "127.0.0.1"
-          port => 9200
-          index => "#{index}"
-          flush_size => #{flush_size}
-        }
-      }
-    CONFIG
-
-    agent do
-      # Try a few times to check if we have the correct number of events stored
-      # in ES.
-      #
-      # We try multiple times to allow final agent flushes as well as allowing
-      # elasticsearch to finish processing everything.
-      ftw = FTW::Agent.new
-      ftw.post!("http://localhost:9200/#{index}/_refresh")
-
-      # Wait until all events are available.
-      Stud::try(10.times) do
-        data = ""
-        response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        count = result["count"]
-        insist { count } == event_count
-      end
-
-      response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
-      data = ""
-      response.read_body { |chunk| data << chunk }
-      result = JSON.parse(data)
-      result["hits"]["hits"].each do |doc|
-        # With no 'index_type' set, the document type should be the type
-        # set on the input
-        insist { doc["_type"] } == type
-        insist { doc["_index"] } == index
-        insist { doc["_source"]["message"] } == "hello world"
-      end
-    end
-  end
-
-  describe "testing index_type" do
-    describe "no type value" do
-      # Generate a random index name
-      index = 10.times.collect { rand(10).to_s }.join("")
-      event_count = 100 + rand(100)
-      flush_size = rand(200) + 1
-
-      config <<-CONFIG
-        input {
-          generator {
-            message => "hello world"
-            count => #{event_count}
-          }
-        }
-        output {
-          elasticsearch_http {
-            host => "127.0.0.1"
-            index => "#{index}"
-            flush_size => #{flush_size}
-          }
-        }
-      CONFIG
-
-      agent do
-        ftw = FTW::Agent.new
-        ftw.post!("http://localhost:9200/#{index}/_refresh")
-
-        # Wait until all events are available.
-        Stud::try(10.times) do
-          data = ""
-          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
-          response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
-          count = result["count"]
-          insist { count } == event_count
-        end
-
-        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
-        data = ""
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        result["hits"]["hits"].each do |doc|
-          insist { doc["_type"] } == "logs"
-        end
-      end
-    end
-
-    describe "default event type value" do
-      # Generate a random index name
-      index = 10.times.collect { rand(10).to_s }.join("")
-      event_count = 100 + rand(100)
-      flush_size = rand(200) + 1
-
-      config <<-CONFIG
-        input {
-          generator {
-            message => "hello world"
-            count => #{event_count}
-            type => "generated"
-          }
-        }
-        output {
-          elasticsearch_http {
-            host => "127.0.0.1"
-            index => "#{index}"
-            flush_size => #{flush_size}
-          }
-        }
-      CONFIG
-
-      agent do
-        ftw = FTW::Agent.new
-        ftw.post!("http://localhost:9200/#{index}/_refresh")
-
-        # Wait until all events are available.
-        Stud::try(10.times) do
-          data = ""
-          response = ftw.get!("http://127.0.0.1:9200/#{index}/_count?q=*")
-          response.read_body { |chunk| data << chunk }
-          result = JSON.parse(data)
-          count = result["count"]
-          insist { count } == event_count
-        end
-
-        response = ftw.get!("http://127.0.0.1:9200/#{index}/_search?q=*&size=1000")
-        data = ""
-        response.read_body { |chunk| data << chunk }
-        result = JSON.parse(data)
-        result["hits"]["hits"].each do |doc|
-          insist { doc["_type"] } == "generated"
-        end
-      end
-    end
-  end
-
-  describe "index template expected behavior" do
-    subject do
-      Elasticsearch::Client.new.indices.delete_template(:name => "*")
-      require "logstash/outputs/elasticsearch_http"
-      settings = {
-        "manage_template" => true,
-        "template_overwrite" => true,
-        "host" => "localhost"
-      }
-      output = LogStash::Outputs::ElasticSearchHTTP.new(settings)
-      output.register
-      next output
-    end
-
-    before :each do
-      require "elasticsearch"
-      @es = Elasticsearch::Client.new
-      @es.indices.delete(:index => "*")
-
-      subject.receive(LogStash::Event.new("message" => "sample message here"))
-      subject.receive(LogStash::Event.new("somevalue" => 100))
-      subject.receive(LogStash::Event.new("somevalue" => 10))
-      subject.receive(LogStash::Event.new("somevalue" => 1))
-      subject.receive(LogStash::Event.new("country" => "us"))
-      subject.receive(LogStash::Event.new("country" => "at"))
-      subject.receive(LogStash::Event.new("geoip" => { "location" => [ 0.0, 0.0 ] }))
-      subject.buffer_flush(:final => true)
-      @es.indices.refresh
-
-      # Wait or fail until everything's indexed.
-      Stud::try(20.times) do
-        r = @es.search
-        insist { r["hits"]["total"] } == 7
-      end
-    end
-
-    it "permits phrase searching on string fields" do
-      results = @es.search(:q => "message:\"sample message\"")
-      insist { results["hits"]["total"] } == 1
-      insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
-    end
-
-    it "numbers dynamically map to a numeric type and permit range queries" do
-      results = @es.search(:q => "somevalue:[5 TO 105]")
-      insist { results["hits"]["total"] } == 2
-
-      values = results["hits"]["hits"].collect { |r| r["_source"]["somevalue"] }
-      insist { values }.include?(10)
-      insist { values }.include?(100)
-      reject { values }.include?(1)
-    end
-
-    it "creates .raw field fro any string field which is not_analyzed" do
-      results = @es.search(:q => "message.raw:\"sample message here\"")
-      insist { results["hits"]["total"] } == 1
-      insist { results["hits"]["hits"][0]["_source"]["message"] } == "sample message here"
-
-      # partial or terms should not work.
-      results = @es.search(:q => "message.raw:\"sample\"")
-      insist { results["hits"]["total"] } == 0
-    end
-
-    it "make [geoip][location] a geo_point" do
-      results = @es.search(:body => { "filter" => { "geo_distance" => { "distance" => "1000km", "geoip.location" => { "lat" => 0.5, "lon" => 0.5 } } } })
-      insist { results["hits"]["total"] } == 1
-      insist { results["hits"]["hits"][0]["_source"]["geoip"]["location"] } == [ 0.0, 0.0 ]
-    end
-
-    it "should index stopwords like 'at' " do
-      results = @es.search(:body => { "facets" => { "t" => { "terms" => { "field" => "country" } } } })["facets"]["t"]
-      terms = results["terms"].collect { |t| t["term"] }
-
-      insist { terms }.include?("us")
-
-      # 'at' is a stopword, make sure stopwords are not ignored.
-      insist { terms }.include?("at")
-    end
-  end
-end
diff --git a/spec/outputs/elasticsearch_river.rb b/spec/outputs/elasticsearch_river.rb
deleted file mode 100644
index 0afd94d918f..00000000000
--- a/spec/outputs/elasticsearch_river.rb
+++ /dev/null
@@ -1,14 +0,0 @@
-# encoding: utf-8
-
-require "logstash/plugin"
-
-describe "outputs/elasticsearch_river" do
-
-  it "should register" do
-    output = LogStash::Plugin.lookup("output", "elasticsearch_river").new("es_host" => "localhost", "rabbitmq_host" => "localhost")
-    output.stub(:prepare_river)
-
-    # register will try to load jars and raise if it cannot find jars
-    expect {output.register}.to_not raise_error
-  end
-end
diff --git a/spec/outputs/email.rb b/spec/outputs/email.rb
deleted file mode 100644
index fdcb6c3e94d..00000000000
--- a/spec/outputs/email.rb
+++ /dev/null
@@ -1,173 +0,0 @@
-require "test_utils"
-require "rumbster"
-require "message_observers"
-
-describe "outputs/email", :broken => true do
-    extend LogStash::RSpec
-
-    @@port=2525
-    let (:rumbster) { Rumbster.new(@@port) }
-    let (:message_observer) { MailMessageObserver.new }
-
-    before :each do
-        rumbster.add_observer message_observer
-        rumbster.start
-    end
-
-    after :each do
-        rumbster.stop
-    end
-
-    describe  "use a list of email as mail.to (LOGSTASH-827)" do
-        config <<-CONFIG
-        input {
-            generator {
-                message => "hello world"
-                count => 1
-                type => "generator"
-            }
-        }
-        filter {
-            noop {
-                add_field => ["dummy_match", "ok"]
-            }
-        }
-        output{
-            email {
-                to => "email1@host, email2@host"
-                match => ["mymatch", "dummy_match,ok"]
-                options => ["port", #{@@port}]
-            }
-        }
-        CONFIG
-
-        agent do
-            insist {message_observer.messages.size} == 1
-            insist {message_observer.messages[0].to} == ["email1@host", "email2@host"]
-        end
-    end
-
-    describe  "use an array of email as mail.to (LOGSTASH-827)" do
-        config <<-CONFIG
-        input {
-            generator {
-                message => "hello world"
-                count => 1
-                type => "generator"
-            }
-        }
-        filter {
-            noop {
-                add_field => ["dummy_match", "ok"]
-                add_field => ["to_addr", "email1@host"]
-                add_field => ["to_addr", "email2@host"]
-            }
-        }
-        output{
-            email {
-                to => "%{to_addr}"
-                match => ["mymatch", "dummy_match,ok"]
-                options => ["port", #{@@port}]
-            }
-        }
-        CONFIG
-
-        agent do
-            insist {message_observer.messages.size} == 1
-            insist {message_observer.messages[0].to} == ["email1@host", "email2@host"]
-        end
-    end
-
-    describe  "multi-lined text body (LOGSTASH-841)" do
-        config <<-CONFIG
-        input {
-            generator {
-                message => "hello world"
-                count => 1
-                type => "generator"
-            }
-        }
-        filter {
-            noop {
-                add_field => ["dummy_match", "ok"]
-            }
-        }
-        output{
-            email {
-                to => "me@host"
-                subject => "Hello World"
-                body => "Line1\\nLine2\\nLine3"
-                match => ["mymatch", "dummy_match,*"]
-                options => ["port", #{@@port}]
-            }
-        }
-        CONFIG
-
-        agent do
-            insist {message_observer.messages.size} == 1
-            insist {message_observer.messages[0].subject} == "Hello World"
-            insist {message_observer.messages[0].body.raw_source} == "Line1\r\nLine2\r\nLine3"
-        end
-    end
-
-    describe  "use nil authenticationType (LOGSTASH-559)" do
-        config <<-CONFIG
-        input {
-            generator {
-                message => "hello world"
-                count => 1
-                type => "generator"
-            }
-        }
-        filter {
-            noop {
-                add_field => ["dummy_match", "ok"]
-            }
-        }
-        output{
-            email {
-                to => "me@host"
-                subject => "Hello World"
-                body => "Line1\\nLine2\\nLine3"
-                match => ["mymatch", "dummy_match,*"]
-                options => ["port", #{@@port}, "authenticationType", "nil"]
-            }
-        }
-        CONFIG
-
-        agent do
-            insist {message_observer.messages.size} == 1
-            insist {message_observer.messages[0].subject} == "Hello World"
-            insist {message_observer.messages[0].body.raw_source} == "Line1\r\nLine2\r\nLine3"
-        end
-    end
-
-    describe  "match on source and message (LOGSTASH-826)" do
-        config <<-CONFIG
-        input {
-            generator {
-                message => "hello world"
-                count => 1
-                type => "generator"
-            }
-        }
-        output{
-            email {
-                to => "me@host"
-                subject => "Hello World"
-                body => "Mail body"
-                match => ["messageAndSourceMatch", "message,*hello,,and,source,*generator"]
-                options => ["port", #{@@port}, "authenticationType", "nil"]
-            }
-        }
-        CONFIG
-
-        agent do
-            insist {message_observer.messages.size} == 1
-            insist {message_observer.messages[0].subject} == "Hello World"
-            insist {message_observer.messages[0].body.raw_source} == "Mail body"
-        end
-    end
-end
-
-
diff --git a/spec/outputs/file.rb b/spec/outputs/file.rb
deleted file mode 100644
index bdf6a769809..00000000000
--- a/spec/outputs/file.rb
+++ /dev/null
@@ -1,72 +0,0 @@
-require "test_utils"
-require "logstash/outputs/file"
-require "tempfile"
-
-describe LogStash::Outputs::File do
-  extend LogStash::RSpec
-
-  describe "ship lots of events to a file" do
-    event_count = 10000 + rand(500)
-    tmp_file = Tempfile.new('logstash-spec-output-file')
-
-    config <<-CONFIG
-      input {
-        generator {
-          message => "hello world"
-          count => #{event_count}
-          type => "generator"
-        }
-      }
-      output {
-        file {
-          path => "#{tmp_file.path}"
-        }
-      }
-    CONFIG
-
-    agent do
-      line_num = 0
-      # Now check all events for order and correctness.
-      File.foreach(tmp_file) do |line|
-        event = LogStash::Event.new(JSON.parse(line))
-        insist {event["message"]} == "hello world"
-        insist {event["sequence"]} == line_num
-        line_num += 1
-      end
-      insist {line_num} == event_count
-    end # agent
-  end
-
-  describe "ship lots of events to a file gzipped" do
-    event_count = 10000 + rand(500)
-    tmp_file = Tempfile.new('logstash-spec-output-file')
-
-    config <<-CONFIG
-      input {
-        generator {
-          message => "hello world"
-          count => #{event_count}
-          type => "generator"
-        }
-      }
-      output {
-        file {
-          path => "#{tmp_file.path}"
-          gzip => true
-        }
-      }
-    CONFIG
-
-    agent do
-      line_num = 0
-      # Now check all events for order and correctness.
-      Zlib::GzipReader.open(tmp_file.path).each_line do |line|
-        event = LogStash::Event.new(JSON.parse(line))
-        insist {event["message"]} == "hello world"
-        insist {event["sequence"]} == line_num
-        line_num += 1
-      end
-      insist {line_num} == event_count
-    end # agent
-  end
-end
diff --git a/spec/outputs/graphite.rb b/spec/outputs/graphite.rb
deleted file mode 100644
index 2b5fe1ac428..00000000000
--- a/spec/outputs/graphite.rb
+++ /dev/null
@@ -1,236 +0,0 @@
-require "test_utils"
-require "logstash/outputs/graphite"
-require "mocha/api"
-
-describe LogStash::Outputs::Graphite, :socket => true do
-  extend LogStash::RSpec
-
-  describe "defaults should include all metrics" do
-    port = 4939
-    config <<-CONFIG
-      input {
-        generator {
-          message => "foo=fancy bar=42"
-          count => 1
-          type => "generator"
-        }
-      }
-
-      filter {
-        kv { }
-      }
-
-      output {
-        graphite {
-          host => "localhost"
-          port => #{port}
-          metrics => [ "hurray.%{foo}", "%{bar}" ]
-        }
-      }
-    CONFIG
-
-    let(:queue) { Queue.new }
-    before :each do
-      server = TCPServer.new("127.0.0.1", port)
-      Thread.new do
-        client = server.accept
-        p client
-        while true
-          p :read
-          line = client.readline
-          p :done
-          queue << line
-          p line
-        end
-      end
-    end
-
-    agent do
-      lines = queue.pop
-
-      insist { lines.size } == 1
-      insist { lines }.any? { |l| l =~ /^hurray.fancy 42.0 \d{10,}\n$/ }
-    end
-  end
-
-  describe "fields_are_metrics => true" do
-    describe "metrics_format => ..." do
-      describe "match one key" do
-        config <<-CONFIG
-          input {
-            generator {
-              message => "foo=123"
-              count => 1
-              type => "generator"
-            }
-          }
-
-          filter {
-            kv { }
-          }
-
-          output {
-            graphite {
-                host => "localhost"
-                port => 2003
-                fields_are_metrics => true
-                include_metrics => ["foo"]
-                metrics_format => "foo.bar.sys.data.*"
-                debug => true
-            }
-          }
-        CONFIG
-
-        agent do
-          @mock.rewind
-          lines = @mock.readlines
-          insist { lines.size } == 1
-          insist { lines[0] } =~ /^foo.bar.sys.data.foo 123.0 \d{10,}\n$/
-        end
-      end
-
-      describe "match all keys" do
-        config <<-CONFIG
-          input {
-            generator {
-              message => "foo=123 bar=42"
-              count => 1
-              type => "generator"
-            }
-          }
-
-          filter {
-            kv { }
-          }
-
-          output {
-            graphite {
-                host => "localhost"
-                port => 2003
-                fields_are_metrics => true
-                include_metrics => [".*"]
-                metrics_format => "foo.bar.sys.data.*"
-                debug => true
-            }
-          }
-        CONFIG
-
-        agent do
-          @mock.rewind
-          lines = @mock.readlines.delete_if { |l| l =~ /\.sequence \d+/ }
-
-          insist { lines.size } == 2
-          insist { lines }.any? { |l| l =~ /^foo.bar.sys.data.foo 123.0 \d{10,}\n$/ }
-          insist { lines }.any? { |l| l =~ /^foo.bar.sys.data.bar 42.0 \d{10,}\n$/ }
-        end
-      end
-
-      describe "no match" do
-        config <<-CONFIG
-          input {
-            generator {
-              message => "foo=123 bar=42"
-              count => 1
-              type => "generator"
-            }
-          }
-
-          filter {
-            kv { }
-          }
-
-          output {
-            graphite {
-              host => "localhost"
-              port => 2003
-              fields_are_metrics => true
-              include_metrics => ["notmatchinganything"]
-              metrics_format => "foo.bar.sys.data.*"
-              debug => true
-            }
-          }
-        CONFIG
-
-        agent do
-          @mock.rewind
-          lines = @mock.readlines
-          insist { lines.size } == 0
-        end
-      end
-
-      describe "match one key with invalid metric_format" do
-        config <<-CONFIG
-          input {
-            generator {
-              message => "foo=123"
-              count => 1
-              type => "generator"
-            }
-          }
-
-          filter {
-            kv { }
-          }
-
-          output {
-            graphite {
-                host => "localhost"
-                port => 2003
-                fields_are_metrics => true
-                include_metrics => ["foo"]
-                metrics_format => "invalidformat"
-                debug => true
-            }
-          }
-        CONFIG
-
-        agent do
-          @mock.rewind
-          lines = @mock.readlines
-          insist { lines.size } == 1
-          insist { lines[0] } =~ /^foo 123.0 \d{10,}\n$/
-        end
-      end
-    end
-  end
-
-  describe "fields are metrics = false" do
-    describe "metrics_format not set" do
-      describe "match one key with metrics list" do
-        config <<-CONFIG
-          input {
-            generator {
-              message => "foo=123"
-              count => 1
-              type => "generator"
-            }
-          }
-
-          filter {
-            kv { }
-          }
-
-          output {
-            graphite {
-                host => "localhost"
-                port => 2003
-                fields_are_metrics => false
-                include_metrics => ["foo"]
-                metrics => [ "custom.foo", "%{foo}" ]
-                debug => true
-            }
-          }
-        CONFIG
-
-        agent do
-          @mock.rewind
-          lines = @mock.readlines
-
-          insist { lines.size } == 1
-          insist { lines[0] } =~ /^custom.foo 123.0 \d{10,}\n$/
-        end
-      end
-
-    end
-  end
-end
diff --git a/spec/outputs/redis.rb b/spec/outputs/redis.rb
deleted file mode 100644
index 442d8b01734..00000000000
--- a/spec/outputs/redis.rb
+++ /dev/null
@@ -1,127 +0,0 @@
-require "test_utils"
-require "logstash/outputs/redis"
-require "redis"
-
-describe LogStash::Outputs::Redis, :redis => true do
-  extend LogStash::RSpec
-
-  describe "ship lots of events to a list" do
-    key = 10.times.collect { rand(10).to_s }.join("")
-    event_count = 10000 + rand(500)
-
-    config <<-CONFIG
-      input {
-        generator {
-          message => "hello world"
-          count => #{event_count}
-          type => "generator"
-        }
-      }
-      output {
-        redis {
-          host => "127.0.0.1"
-          key => "#{key}"
-          data_type => list
-        }
-      }
-    CONFIG
-
-    agent do
-      # Query redis directly and inspect the goodness.
-      redis = Redis.new(:host => "127.0.0.1")
-
-      # The list should contain the number of elements our agent pushed up.
-      insist { redis.llen(key) } == event_count
-
-      # Now check all events for order and correctness.
-      event_count.times do |value|
-        id, element = redis.blpop(key, 0)
-        event = LogStash::Event.new(JSON.parse(element))
-        insist { event["sequence"] } == value
-        insist { event["message"] } == "hello world"
-      end
-
-      # The list should now be empty
-      insist { redis.llen(key) } == 0
-    end # agent
-  end
-
-  describe "batch mode" do
-    key = 10.times.collect { rand(10).to_s }.join("")
-    event_count = 200000
-
-    config <<-CONFIG
-      input {
-        generator {
-          message => "hello world"
-          count => #{event_count}
-          type => "generator"
-        }
-      }
-      output {
-        redis {
-          host => "127.0.0.1"
-          key => "#{key}"
-          data_type => list
-          batch => true
-          batch_timeout => 5
-          timeout => 5
-        }
-      }
-    CONFIG
-
-    agent do
-      # we have to wait for teardown to execute & flush the last batch.
-      # otherwise we might start doing assertions before everything has been
-      # sent out to redis.
-      sleep 2
-
-      redis = Redis.new(:host => "127.0.0.1")
-
-      # The list should contain the number of elements our agent pushed up.
-      insist { redis.llen(key) } == event_count
-
-      # Now check all events for order and correctness.
-      event_count.times do |value|
-        id, element = redis.blpop(key, 0)
-        event = LogStash::Event.new(JSON.parse(element))
-        insist { event["sequence"] } == value
-        insist { event["message"] } == "hello world"
-      end
-
-      # The list should now be empty
-      insist { redis.llen(key) } == 0
-    end # agent
-  end
-
-  describe "converts US-ASCII to utf-8 without failures" do
-    key = 10.times.collect { rand(10).to_s }.join("")
-
-    config <<-CONFIG
-      input {
-        generator {
-          charset => "US-ASCII"
-          message => "\xAD\u0000"
-          count => 1
-          type => "generator"
-        }
-      }
-      output {
-        redis {
-          host => "127.0.0.1"
-          key => "#{key}"
-          data_type => list
-        }
-      }
-    CONFIG
-
-    agent do
-      # Query redis directly and inspect the goodness.
-      redis = Redis.new(:host => "127.0.0.1")
-
-      # The list should contain no elements.
-      insist { redis.llen(key) } == 1
-    end # agent
-  end
-end
-
diff --git a/spec/speed.rb b/spec/speed.rb
deleted file mode 100644
index d8f0a9e5e18..00000000000
--- a/spec/speed.rb
+++ /dev/null
@@ -1,22 +0,0 @@
-require "test_utils"
-
-describe "speed tests", :performance => true do
-  extend LogStash::RSpec
-  count = 1000000
-
-  config <<-CONFIG
-    input {
-      generator {
-        type => foo
-        count => #{count}
-      }
-    }
-    output { null { } }
-  CONFIG
-
-  start = Time.now
-  agent do
-    duration = (Time.now - start)
-    puts "speed rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
-  end
-end
diff --git a/spec/support/LOGSTASH-733.rb b/spec/support/LOGSTASH-733.rb
deleted file mode 100644
index 62a9b7dce81..00000000000
--- a/spec/support/LOGSTASH-733.rb
+++ /dev/null
@@ -1,21 +0,0 @@
-# This spec covers the question here:
-# https://logstash.jira.com/browse/LOGSTASH-733
-
-require "test_utils"
-
-describe "LOGSTASH-733" do
-  extend LogStash::RSpec
-  describe  "pipe-delimited fields" do
-    config <<-CONFIG
-      filter {
-        kv { field_split => "|" }
-      }
-    CONFIG
-
-    sample "field1=test|field2=another test|field3=test3" do
-      insist { subject["field1"] } == "test"
-      insist { subject["field2"] } == "another test"
-      insist { subject["field3"] } == "test3"
-    end
-  end
-end
diff --git a/spec/support/LOGSTASH-820.rb b/spec/support/LOGSTASH-820.rb
deleted file mode 100644
index 251b1f0e5bc..00000000000
--- a/spec/support/LOGSTASH-820.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-# This spec covers the question here:
-# https://logstash.jira.com/browse/LOGSTASH-820
-
-require "test_utils"
-
-describe "LOGSTASH-820" do
-  extend LogStash::RSpec
-  describe  "grok with unicode" do
-    config <<-CONFIG
-      filter {
-        grok {
-          #pattern => "<%{POSINT:syslog_pri}>%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}"
-          pattern => "<%{POSINT:syslog_pri}>%{SPACE}%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{PROG:syslog_program}(:?)(?:\\[%{GREEDYDATA:syslog_pid}\\])?(:?) %{GREEDYDATA:syslog_message}"
-        }
-      }
-    CONFIG
-
-    sample "<22>Jan  4 07:50:46 mailmaster postfix/policy-spf[9454]: : SPF permerror (Junk encountered in record 'v=spf1 mx a:mail.domain.no ip4:192.168.0.4 all'): Envelope-from: email@domain.no" do
-      insist { subject["tags"] }.nil?
-      insist { subject["syslog_pri"] } == "22"
-      insist { subject["syslog_program"] } == "postfix/policy-spf"
-    end
-  end
-end
diff --git a/spec/support/akamai-grok.rb b/spec/support/akamai-grok.rb
deleted file mode 100644
index f889d5c9e87..00000000000
--- a/spec/support/akamai-grok.rb
+++ /dev/null
@@ -1,26 +0,0 @@
-require "test_utils"
-
-describe "..." do
-  extend LogStash::RSpec
-
-  config <<-'CONFIG'
-    filter {
-      grok {
-        pattern => "%{COMBINEDAPACHELOG}"
-      }
-     
-     
-      date {
-        # Try to pull the timestamp from the 'timestamp' field
-        match => [ "timestamp", "dd'/'MMM'/'yyyy:HH:mm:ss Z" ]
-      }
-    }
-  CONFIG
-
-  line = '192.168.1.1 - - [25/Mar/2013:20:33:56 +0000] "GET /www.somewebsite.co.uk/dwr/interface/AjaxNewsletter.js HTTP/1.1" 200 794 "http://www.somewebsite.co.uk/money/index.html" "Mozilla/5.0 (Linux; U; Android 2.3.6; en-gb; GT-I8160 Build/GINGERBREAD) AppleWebKit/533.1 (KHTML, like Gecko) Version/4.0 Mobile Safari/533.1" "NREUM=s=1364243891214&r=589267&p=101913; __utma=259942479.284548354.1358973919.1363109625.1364243485.15; __utmb=259942479.4.10.1364243485; __utmc=259942479; __utmz=259942479.1359409342.3.3.utmcsr=investing.somewebsite.co.uk|utmccn=(referral)|utmcmd=referral|utmcct=/performance/overview/; asi_segs=D05509_10903|D05509_11337|D05509_11335|D05509_11341|D05509_11125|D05509_11301|D05509_11355|D05509_11508|D05509_11624|D05509_10784|D05509_11003|D05509_10699|D05509_11024|D05509_11096|D05509_11466|D05509_11514|D05509_11598|D05509_11599|D05509_11628|D05509_11681; rsi_segs=D05509_10903|D05509_11337|D05509_11335|D05509_11341|D05509_11125|D05509_11301|D05509_11355|D05509_11508|D05509_11624|D05509_10784|D05509_11003|D05509_10699|D05509_11024|D05509_11096|D05509_11466|D05509_11514|D05509_11598|D05509_11599|D05509_11628|D05509_11681|D05509_11701|D05509_11818|D05509_11850|D05509_11892|D05509_11893|D05509_12074|D05509_12091|D05509_12093|D05509_12095|D05509_12136|D05509_12137|D05509_12156|D05509_0; s_pers=%20s_nr%3D1361998955946%7C1364590955946%3B%20s_pn2%3D/money/home%7C1364245284228%3B%20s_c39%3D/money/home%7C1364245522830%3B%20s_visit%3D1%7C1364245693534%3B; s_sess=%20s_pn%3D/money/home%3B%20s_cc%3Dtrue%3B%20s_sq%3D%3B"'
-
-  sample line do
-    #puts subject["@timestamp"]
-    #puts subject["timestamp"]
-  end
-end
diff --git a/spec/support/date-http.rb b/spec/support/date-http.rb
deleted file mode 100644
index f9fd883f1e8..00000000000
--- a/spec/support/date-http.rb
+++ /dev/null
@@ -1,18 +0,0 @@
-require "test_utils"
-
-describe "http dates", :if => RUBY_ENGINE == "jruby" do
-  extend LogStash::RSpec
-
-  config <<-'CONFIG'
-    filter {
-      date {
-        match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
-        locale => "en"
-      }
-    }
-  CONFIG
-
-  sample("timestamp" => "25/Mar/2013:20:33:56 +0000") do
-    insist { subject["@timestamp"] } == Time.iso8601("2013-03-25T20:33:56.000Z")
-  end
-end
diff --git a/spec/support/pull375.rb b/spec/support/pull375.rb
deleted file mode 100644
index 77295fc61be..00000000000
--- a/spec/support/pull375.rb
+++ /dev/null
@@ -1,23 +0,0 @@
-# encoding: utf-8
-
-# This spec covers the question here:
-# https://github.com/logstash/logstash/pull/375
-
-require "test_utils"
-
-describe "pull #375" do
-  extend LogStash::RSpec
-  describe  "kv after grok" do
-    config <<-CONFIG
-      filter {
-        grok { pattern => "%{URIPATH:mypath}%{URIPARAM:myparams}" }
-        kv { source => "myparams" field_split => "&?" }
-      }
-    CONFIG
-
-    sample "/some/path?foo=bar&baz=fizz" do
-      insist { subject["foo"] } == "bar"
-      insist { subject["baz"] } == "fizz"
-    end
-  end
-end
diff --git a/spec/test_utils.rb b/spec/test_utils.rb
deleted file mode 100644
index bb9f712c89e..00000000000
--- a/spec/test_utils.rb
+++ /dev/null
@@ -1,146 +0,0 @@
-# encoding: utf-8
-
-if ENV['COVERAGE']
-  require 'simplecov'
-  require 'coveralls'
-
-  SimpleCov.formatter = SimpleCov::Formatter::MultiFormatter[
-    SimpleCov::Formatter::HTMLFormatter,
-    Coveralls::SimpleCov::Formatter
-  ]
-  SimpleCov.start do
-    add_filter 'spec/'
-    add_filter 'vendor/'
-  end
-end
-require "insist"
-require "logstash/agent"
-require "logstash/pipeline"
-require "logstash/event"
-require "logstash/logging"
-require "insist"
-require "stud/try"
-
-$TESTING = true
-if RUBY_VERSION < "1.9.2"
-  $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
-  $stderr.puts "Options for fixing this: "
-  $stderr.puts "  * If doing 'ruby bin/logstash ...' add --1.9 flag to 'ruby'"
-  $stderr.puts "  * If doing 'java -jar ... ' add -Djruby.compat.version=RUBY1_9 to java flags"
-  raise LoadError
-end
-
-$logger = LogStash::Logger.new(STDOUT)
-if ENV["TEST_DEBUG"]
-  $logger.level = :debug
-else
-  $logger.level = :error
-end
-
-puts("Using Accessor#strict_set for specs")
-# mokey path LogStash::Event to use strict_set in tests
-# ugly, I know, but this avoids adding conditionals in performance critical section
-class LogStash::Event
-  def []=(str, value)
-    if str == TIMESTAMP && !value.is_a?(Time)
-      raise TypeError, "The field '@timestamp' must be a Time, not a #{value.class} (#{value})"
-    end
-    @accessors.strict_set(str, value)
-  end # def []=
-end
-
-RSpec.configure do |config|
-  config.filter_run_excluding :redis => true, :socket => true, :performance => true, :elasticsearch => true, :broken => true, :export_cypher => true
-end
-
-module LogStash
-  module RSpec
-    def config(configstr)
-      let(:config) { configstr }
-    end # def config
-
-    def type(default_type)
-      let(:default_type) { default_type }
-    end
-
-    def tags(*tags)
-      let(:default_tags) { tags }
-      puts "Setting default tags: #{@default_tags}"
-    end
-
-    def sample(sample_event, &block)
-      name = sample_event.is_a?(String) ? sample_event : sample_event.to_json
-      name = name[0..50] + "..." if name.length > 50
-
-      describe "\"#{name}\"" do
-        extend LogStash::RSpec
-        let(:pipeline) { LogStash::Pipeline.new(config) }
-        let(:event) do
-          sample_event = [sample_event] unless sample_event.is_a?(Array)
-          next sample_event.collect do |e|
-            e = { "message" => e } if e.is_a?(String)
-            next LogStash::Event.new(e)
-          end
-        end
-
-        let(:results) do
-          results = []
-          count = 0
-          pipeline.instance_eval { @filters.each(&:register) }
-
-          event.each do |e|
-            extra = []
-            pipeline.filter(e) do |new_event|
-              extra << new_event
-            end
-            results << e if !e.cancelled?
-            results += extra.reject(&:cancelled?)
-          end
-
-          pipeline.instance_eval {@filters.each {|f| results += f.flush if f.respond_to?(:flush)}}
-
-          # TODO(sissel): pipeline flush needs to be implemented.
-          # results += pipeline.flush
-          next results
-        end
-
-        subject { results.length > 1 ? results: results.first }
-
-        it("when processed", &block)
-      end
-    end # def sample
-
-    def input(&block)
-      it "inputs" do
-        pipeline = LogStash::Pipeline.new(config)
-        queue = Queue.new
-        pipeline.instance_eval do
-          @output_func = lambda { |event| queue << event }
-        end
-        block.call(pipeline, queue)
-        pipeline.shutdown
-      end
-    end # def input
-
-    def agent(&block)
-      require "logstash/pipeline"
-
-      it("agent(#{caller[0].gsub(/ .*/, "")}) runs") do
-        pipeline = LogStash::Pipeline.new(config)
-        pipeline.run
-        block.call
-      end
-    end # def agent
-
-  end # module RSpec
-end # module LogStash
-
-class Shiftback
-  def initialize(&block)
-    @block = block
-  end
-
-  def <<(event)
-    @block.call(event)
-  end
-end # class Shiftback
diff --git a/spec/util/accessors_spec.rb b/spec/util/accessors_spec.rb
index e86e25aed37..bd5e96cff95 100644
--- a/spec/util/accessors_spec.rb
+++ b/spec/util/accessors_spec.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 
-require "test_utils"
+require "logstash/devutils/rspec/spec_helper"
 require "logstash/util/accessors"
 
 describe LogStash::Util::Accessors, :if => true do
@@ -83,11 +83,14 @@
       insist { accessors.get(str) } == data["hello"]["world"]
     end
 
-    it "should get deep string value" do
+    it "should return nil when getting a non-existant field (with no side-effects on original data)" do
       str = "[hello][world]"
-      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
+      data = {}
       accessors = LogStash::Util::Accessors.new(data)
-      insist { accessors.get(str) } == data["hello"]["world"]
+      insist { accessors.get(str) }.nil?
+      insist { data } == {}
+      insist { accessors.set(str, "foo") } == "foo"
+      insist { data } == { "hello" => {"world" => "foo"} }
     end
 
     it "should handle delete" do
@@ -140,6 +143,14 @@
       insist { data } == { "hello" => { "world" => ["foo", "bar"] } }
     end
 
+    it "should set element within array value" do
+      str = "[hello][0]"
+      data = {"hello" => ["foo", "bar"]}
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.set(str, "world") } == "world"
+      insist { data } == {"hello" => ["world", "bar"]}
+    end
+
     it "should retrieve array item" do
       data = { "hello" => { "world" => ["a", "b"], "bar" => "baz" } }
       accessors = LogStash::Util::Accessors.new(data)
@@ -153,7 +164,14 @@
       insist { accessors.get("[hello][world][0][a]") } == data["hello"]["world"][0]["a"]
       insist { accessors.get("[hello][world][1][b]") } == data["hello"]["world"][1]["b"]
     end
-  end
+
+    it "should handle delete of array element" do
+      str = "[geocoords][0]"
+      data = { "geocoords" => [4, 2] }
+      accessors = LogStash::Util::Accessors.new(data)
+      insist { accessors.del(str) } == 4
+      insist { data } == { "geocoords" => [2] }
+    end  end
 
   context "using invalid encoding" do
     it "strinct_set should raise on non UTF-8 string encoding" do
diff --git a/spec/util/charset_spec.rb b/spec/util/charset_spec.rb
index f741b0ce2cb..97d85494bf7 100644
--- a/spec/util/charset_spec.rb
+++ b/spec/util/charset_spec.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 
-require "test_utils"
+require "logstash/devutils/rspec/spec_helper"
 require "logstash/util/charset"
 
 describe LogStash::Util::Charset do
@@ -29,7 +29,8 @@
       ["foo \xED\xB9\x81\xC3", "bar \xAD"].each do |data|
         insist { data.encoding.name } == "UTF-8"
         insist { data.valid_encoding? } == false
-        logger.should_receive(:warn).twice
+        expect(logger).to receive(:warn).exactly(2).times
+#logger.should_receive(:warn).twice
         insist { subject.convert(data) } == data.inspect[1..-2]
         insist { subject.convert(data).encoding.name } == "UTF-8"
       end
diff --git a/spec/util/environment_spec.rb b/spec/util/environment_spec.rb
new file mode 100644
index 00000000000..0337b7b70bc
--- /dev/null
+++ b/spec/util/environment_spec.rb
@@ -0,0 +1,5 @@
+require "logstash/environment"
+
+describe LogStash::Environment do
+
+end
diff --git a/spec/util/fieldeval_spec.rb b/spec/util/fieldeval_spec.rb
index 4cf10597ba2..1b8d5304211 100644
--- a/spec/util/fieldeval_spec.rb
+++ b/spec/util/fieldeval_spec.rb
@@ -1,4 +1,4 @@
-require "test_utils"
+require "logstash/devutils/rspec/spec_helper"
 require "logstash/util/fieldreference"
 
 describe LogStash::Util::FieldReference, :if => true do
diff --git a/spec/jar.rb b/spec/util/jar_spec.rb
similarity index 100%
rename from spec/jar.rb
rename to spec/util/jar_spec.rb
diff --git a/spec/util/json_spec.rb b/spec/util/json_spec.rb
new file mode 100644
index 00000000000..a745f91a1e8
--- /dev/null
+++ b/spec/util/json_spec.rb
@@ -0,0 +1,96 @@
+# encoding: utf-8
+require "logstash/json"
+require "logstash/environment"
+require "logstash/util"
+
+describe LogStash::Json do
+
+  let(:hash)   {{"a" => 1}}
+  let(:json_hash)   {"{\"a\":1}"}
+
+  let(:string) {"foobar"}
+  let(:json_string) {"\"foobar\""}
+
+  let(:array)  {["foo", "bar"]}
+  let(:json_array)  {"[\"foo\",\"bar\"]"}
+
+  let(:multi) {
+    [
+      {:ruby => "foo bar baz", :json => "\"foo bar baz\""},
+      {:ruby => "1", :json => "\"1\""},
+      {:ruby => {"a" => true}, :json => "{\"a\":true}"},
+      {:ruby => {"a" => nil}, :json => "{\"a\":null}"},
+      {:ruby => ["a", "b"], :json => "[\"a\",\"b\"]"},
+      {:ruby => [1, 2], :json => "[1,2]"},
+      {:ruby => [1, nil], :json => "[1,null]"},
+      {:ruby => {"a" => [1, 2]}, :json => "{\"a\":[1,2]}"},
+      {:ruby => {"a" => {"b" => 2}}, :json => "{\"a\":{\"b\":2}}"},
+      # {:ruby => , :json => },
+    ]
+  }
+
+  if LogStash::Environment.jruby?
+
+    ### JRuby specific
+
+    context "jruby deserialize" do
+      it "should respond to load and deserialize object" do
+        expect(JrJackson::Raw).to receive(:parse_raw).with(json_hash).and_call_original
+        expect(LogStash::Json.load(json_hash)).to eql(hash)
+      end
+    end
+
+    context "jruby serialize" do
+      it "should respond to dump and serialize object" do
+        expect(JrJackson::Json).to receive(:dump).with(string).and_call_original
+        expect(LogStash::Json.dump(string)).to eql(json_string)
+      end
+
+      it "should call JrJackson::Raw.generate for Hash" do
+        expect(JrJackson::Raw).to receive(:generate).with(hash).and_call_original
+        expect(LogStash::Json.dump(hash)).to eql(json_hash)
+      end
+
+      it "should call JrJackson::Raw.generate for Array" do
+        expect(JrJackson::Raw).to receive(:generate).with(array).and_call_original
+        expect(LogStash::Json.dump(array)).to eql(json_array)
+      end
+
+    end
+
+  else
+
+    ### MRI specific
+
+    it "should respond to load and deserialize object on mri" do
+      expect(Oj).to receive(:load).with(json).and_call_original
+      expect(LogStash::Json.load(json)).to eql(hash)
+    end
+
+    it "should respond to dump and serialize object on mri" do
+      expect(Oj).to receive(:dump).with(hash, anything).and_call_original
+      expect(LogStash::Json.dump(hash)).to eql(json)
+    end
+  end
+
+  ### non specific
+
+  it "should correctly deserialize" do
+    multi.each do |test|
+      # because JrJackson in :raw mode uses Java::JavaUtil::LinkedHashMap and
+      # Java::JavaUtil::ArrayList, we must cast to compare.
+      # other than that, they quack like their Ruby equivalent
+      expect(LogStash::Util.normalize(LogStash::Json.load(test[:json]))).to eql(test[:ruby])
+    end
+  end
+
+  it "should correctly serialize" do
+    multi.each do |test|
+      expect(LogStash::Json.dump(test[:ruby])).to eql(test[:json])
+    end
+  end
+
+  it "should raise Json::ParserError on invalid json" do
+    expect{LogStash::Json.load("abc")}.to raise_error LogStash::Json::ParserError
+  end
+end
diff --git a/spec/util_spec.rb b/spec/util_spec.rb
new file mode 100644
index 00000000000..aeff9bdb469
--- /dev/null
+++ b/spec/util_spec.rb
@@ -0,0 +1,33 @@
+require "logstash/util"
+
+
+describe LogStash::Util do
+
+  context "stringify_keys" do
+    it "should convert hash symbol keys to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => 1, "b" => 2})).to eq({"a" => 1, "b" => 2})
+    end
+
+    it "should keep non symbolic hash keys as is" do
+      expect(LogStash::Util.stringify_symbols({1 => 1, 2.0 => 2})).to eq({1 => 1, 2.0 => 2})
+    end
+
+    it "should convert inner hash keys to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => 1, "b" => {:c => 3}})).to eq({"a" => 1, "b" => {"c" => 3}})
+      expect(LogStash::Util.stringify_symbols([:a, 1, "b", {:c => 3}])).to eq(["a", 1, "b", {"c" => 3}])
+    end
+
+    it "should convert hash symbol values to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => :a, "b" => :b})).to eq({"a" => "a", "b" => "b"})
+    end
+
+    it "should convert array symbol values to strings" do
+      expect(LogStash::Util.stringify_symbols([1, :a])).to eq([1, "a"])
+    end
+
+    it "should convert innner array symbol values to strings" do
+      expect(LogStash::Util.stringify_symbols({:a => [1, :b]})).to eq({"a" => [1, "b"]})
+      expect(LogStash::Util.stringify_symbols([:a, [1, :b]])).to eq(["a", [1, "b"]])
+    end
+  end
+end
diff --git a/tools/Gemfile b/tools/Gemfile
index ef8f8423b21..0483cae4262 100644
--- a/tools/Gemfile
+++ b/tools/Gemfile
@@ -1,14 +1,5 @@
 source "https://rubygems.org"
-#gemspec(:name => "logstash", :path => "../")
+gemspec :path => File.expand_path(File.join(File.dirname(__FILE__), "..")), :name => "logstash", :development_group => :development
 
-gemspec = File.join(File.dirname(__FILE__), "..", "logstash.gemspec")
-spec = Gem::Specification.load(gemspec)
-spec.runtime_dependencies.each do |dep|
-  gem dep.name, dep.requirement.to_s
-end
-
-group :development do
-  spec.development_dependencies.each do |dep|
-    gem dep.name, dep.requirement.to_s
-  end
-end
+# in development if a local, unpublished gems is required, you must add it first in the gemspec without the :path option
+# and also add it here with the :path option.
\ No newline at end of file
diff --git a/tools/Gemfile.beaker b/tools/Gemfile.beaker
index 33b02d3f0d0..9c13c04723b 100644
--- a/tools/Gemfile.beaker
+++ b/tools/Gemfile.beaker
@@ -1,11 +1,12 @@
 source 'https://rubygems.org'
 
-gem 'beaker', :git => 'https://github.com/electrical/beaker.git', :branch => 'docker_test'
+gem 'beaker'
 gem 'beaker-rspec'
 gem 'pry'
 gem 'docker-api'
 gem 'rubysl-securerandom'
 gem 'rspec_junit_formatter'
+gem 'rspec', '~> 2.14.0'
 
 case RUBY_VERSION
 when '1.8.7'
diff --git a/tools/Gemfile.jruby-1.9.lock b/tools/Gemfile.jruby-1.9.lock
index dc11fd5429f..e369d492cff 100644
--- a/tools/Gemfile.jruby-1.9.lock
+++ b/tools/Gemfile.jruby-1.9.lock
@@ -1,237 +1,116 @@
+PATH
+  remote: /Users/ph/es/logstash
+  specs:
+    logstash (2.0.0.dev-java)
+      cabin (>= 0.6.0)
+      ci_reporter (= 1.9.3)
+      clamp
+      ftw (~> 0.0.40)
+      i18n (= 0.6.9)
+      insist (= 1.0.0)
+      jar-dependencies (= 0.1.2)
+      jrjackson
+      jruby-httpclient
+      logstash-devutils
+      maven-tools
+      mime-types
+      minitar
+      pry
+      rack
+      rspec (~> 2.14.0)
+      ruby-maven
+      sinatra
+      stud
+      treetop (~> 1.4.0)
+
 GEM
   remote: https://rubygems.org/
   specs:
-    activesupport (3.2.17)
-      i18n (~> 0.6, >= 0.6.4)
-      multi_json (~> 1.0)
-    addressable (2.3.5)
-    atomic (1.1.15-java)
-    avl_tree (1.1.3)
-    awesome_print (1.2.0)
-    aws-sdk (1.35.0)
-      json (~> 1.4)
-      nokogiri (>= 1.4.4)
-      uuidtools (~> 2.1)
-    backports (3.6.0)
-    beefcake (0.3.7)
-    bindata (2.0.0)
-    blankslate (2.1.2.4)
-    bouncy-castle-java (1.5.0147)
-    buftok (0.1)
+    addressable (2.3.6)
+    axiom-types (0.1.1)
+      descendants_tracker (~> 0.0.4)
+      ice_nine (~> 0.11.0)
+      thread_safe (~> 0.3, >= 0.3.1)
+    backports (3.6.4)
     builder (3.2.2)
     cabin (0.6.1)
-    ci_reporter (1.9.1)
+    ci_reporter (1.9.3)
       builder (>= 2.1.2)
-    cinch (2.1.0)
     clamp (0.6.3)
     coderay (1.1.0)
-    coveralls (0.7.0)
-      multi_json (~> 1.3)
-      rest-client
-      simplecov (>= 0.7)
-      term-ansicolor
-      thor
+    coercible (1.0.0)
+      descendants_tracker (~> 0.0.1)
+    descendants_tracker (0.0.4)
+      thread_safe (~> 0.3, >= 0.3.1)
     diff-lcs (1.2.5)
-    docile (1.1.3)
-    edn (1.0.2)
-      parslet (~> 1.4.0)
-    elasticsearch (1.0.1)
-      elasticsearch-api (= 1.0.1)
-      elasticsearch-transport (= 1.0.1)
-    elasticsearch-api (1.0.1)
-      multi_json
-    elasticsearch-transport (1.0.1)
-      faraday
-      multi_json
-    extlib (0.9.16)
-    faraday (0.9.0)
-      multipart-post (>= 1.2, < 3)
-    ffi (1.9.3)
-    ffi (1.9.3-java)
-    ffi-rzmq (1.0.0)
-      ffi
-    filewatch (0.5.1)
-    ftw (0.0.39)
+    equalizer (0.0.9)
+    ffi (1.9.6-java)
+    ftw (0.0.42)
       addressable
       backports (>= 2.6.2)
       cabin (> 0)
-      http_parser.rb (= 0.5.3)
-    gelf (1.3.2)
-      json
-    gelfd (0.2.0)
-    geoip (1.3.5)
-    gmetric (0.1.3)
-    hitimes (1.2.1)
-    hitimes (1.2.1-java)
-    http (0.5.0)
-      http_parser.rb
-    http_parser.rb (0.5.3)
-    http_parser.rb (0.5.3-java)
+      http_parser.rb (~> 0.6)
+    gem_publisher (1.5.0)
+    http_parser.rb (0.6.0-java)
     i18n (0.6.9)
+    ice_nine (0.11.1)
     insist (1.0.0)
-    jls-grok (0.10.12)
-      cabin (>= 0.6.0)
-    jls-lumberjack (0.0.20)
+    jar-dependencies (0.1.2)
+    jrjackson (0.2.7)
     jruby-httpclient (1.1.1-java)
-    jruby-openssl (0.8.7)
-      bouncy-castle-java (>= 1.5.0147)
-    json (1.8.1)
-    json (1.8.1-java)
-    kramdown (1.3.3)
-    mail (2.5.3)
-      i18n (>= 0.4.0)
-      mime-types (~> 1.16)
-      treetop (~> 1.4.8)
-    march_hare (2.1.2-java)
-    metaclass (0.0.4)
+    logstash-devutils (0.0.6-java)
+      gem_publisher
+      jar-dependencies
+      minitar
+      rake
+    maven-tools (1.0.7)
+      virtus (~> 1.0)
     method_source (0.8.2)
-    metriks (0.9.9.6)
-      atomic (~> 1.0)
-      avl_tree (~> 1.1.2)
-      hitimes (~> 1.1)
-    mime-types (1.25.1)
-    mini_portile (0.5.2)
-    minitest (5.3.0)
-    mocha (1.0.0)
-      metaclass (~> 0.0.1)
-    msgpack-jruby (1.4.0-java)
-    multi_json (1.8.4)
-    multipart-post (2.0.0)
-    murmurhash3 (0.1.4)
-    nokogiri (1.6.1-java)
-      mini_portile (~> 0.5.0)
-    parslet (1.4.0)
-      blankslate (~> 2.0)
-    polyglot (0.3.4)
-    pry (0.9.12.6-java)
-      coderay (~> 1.0)
-      method_source (~> 0.8)
+    mime-types (2.4.3)
+    minitar (0.5.4)
+    polyglot (0.3.5)
+    pry (0.10.1-java)
+      coderay (~> 1.1.0)
+      method_source (~> 0.8.1)
       slop (~> 3.4)
       spoon (~> 0.0)
-    rack (1.5.2)
-    rack-protection (1.5.2)
+    rack (1.6.0)
+    rack-protection (1.5.3)
       rack
-    rbnacl (2.0.0)
-      ffi
-    redis (3.0.7)
-    rest-client (1.6.7)
-      mime-types (>= 1.16)
+    rake (10.4.2)
     rspec (2.14.1)
       rspec-core (~> 2.14.0)
       rspec-expectations (~> 2.14.0)
       rspec-mocks (~> 2.14.0)
-    rspec-core (2.14.7)
+    rspec-core (2.14.8)
     rspec-expectations (2.14.5)
       diff-lcs (>= 1.1.3, < 2.0)
     rspec-mocks (2.14.6)
-    rufus-scheduler (2.0.24)
-      tzinfo (>= 0.3.22)
-    rumbster (1.1.1)
-      mail (= 2.5.3)
-    shoulda (3.5.0)
-      shoulda-context (~> 1.0, >= 1.0.1)
-      shoulda-matchers (>= 1.4.1, < 3.0)
-    shoulda-context (1.1.6)
-    shoulda-matchers (2.5.0)
-      activesupport (>= 3.0.0)
-    simple_oauth (0.2.0)
-    simplecov (0.8.2)
-      docile (~> 1.1.0)
-      multi_json
-      simplecov-html (~> 0.8.0)
-    simplecov-html (0.8.0)
-    sinatra (1.4.4)
+    ruby-maven (3.1.1.0.8)
+      maven-tools (~> 1.0.1)
+      ruby-maven-libs (= 3.1.1)
+    ruby-maven-libs (3.1.1)
+    sinatra (1.4.5)
       rack (~> 1.4)
       rack-protection (~> 1.4)
       tilt (~> 1.3, >= 1.3.4)
-    slop (3.4.7)
-    snmp (1.1.1)
+    slop (3.6.0)
     spoon (0.0.4)
       ffi
-    statsd-ruby (1.2.0)
-    stud (0.0.17)
-      ffi
-      metriks
-    term-ansicolor (1.3.0)
-      tins (~> 1.0)
-    thor (0.18.1)
-    thread_safe (0.2.0-java)
-      atomic (>= 1.1.7, < 2)
+    stud (0.0.18)
+    thread_safe (0.3.4-java)
     tilt (1.4.1)
-    tins (1.0.0)
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
-    twitter (5.0.0.rc.1)
-      buftok (~> 0.1.0)
-      faraday (>= 0.8, < 0.10)
-      http (>= 0.5.0.pre2, < 0.6)
-      http_parser.rb (~> 0.5.0)
-      json (~> 1.8)
-      simple_oauth (~> 0.2.0)
-    tzinfo (1.1.0)
-      thread_safe (~> 0.1)
-    user_agent_parser (2.1.2)
-    uuidtools (2.1.4)
-    xml-simple (1.1.3)
-    xmpp4r (0.5)
+    virtus (1.0.3)
+      axiom-types (~> 0.1)
+      coercible (~> 1.0)
+      descendants_tracker (~> 0.0, >= 0.0.3)
+      equalizer (~> 0.0, >= 0.0.9)
 
 PLATFORMS
   java
 
 DEPENDENCIES
-  addressable
-  awesome_print
-  aws-sdk
-  beefcake (= 0.3.7)
-  bindata (>= 1.5.0)
-  bouncy-castle-java (= 1.5.0147)
-  cabin (>= 0.6.0)
-  ci_reporter
-  cinch
-  clamp
-  coveralls
-  edn
-  elasticsearch
-  extlib (= 0.9.16)
-  ffi
-  ffi-rzmq (= 1.0.0)
-  filewatch (= 0.5.1)
-  ftw (~> 0.0.39)
-  gelf (= 1.3.2)
-  gelfd (= 0.2.0)
-  geoip (>= 1.3.2)
-  gmetric (= 0.1.3)
-  i18n (>= 0.6.6)
-  insist (= 1.0.0)
-  jls-grok (= 0.10.12)
-  jls-lumberjack (>= 0.0.20)
-  jruby-httpclient
-  jruby-openssl (= 0.8.7)
-  json
-  kramdown
-  mail
-  march_hare (~> 2.1.0)
-  metriks
-  mime-types
-  minitest
-  mocha
-  msgpack-jruby
-  murmurhash3
-  pry
-  rack
-  rbnacl
-  redis
-  rspec
-  rufus-scheduler (~> 2.0.24)
-  rumbster
-  shoulda
-  sinatra
-  snmp
-  spoon
-  statsd-ruby (= 1.2.0)
-  stud
-  twitter (= 5.0.0.rc.1)
-  user_agent_parser (>= 2.0.0)
-  xml-simple
-  xmpp4r (= 0.5)
+  logstash!
diff --git a/tools/Gemfile.plugins b/tools/Gemfile.plugins
new file mode 100644
index 00000000000..a392913df58
--- /dev/null
+++ b/tools/Gemfile.plugins
@@ -0,0 +1,7 @@
+require 'rakelib/default_plugins'
+
+source 'https://rubygems.org'
+
+gemspec :name => "logstash", :path => File.expand_path(File.join(File.dirname(__FILE__), ".."))
+
+DEFAULT_PLUGINS.each {|p| gem p}
diff --git a/tools/Gemfile.plugins.all b/tools/Gemfile.plugins.all
new file mode 100644
index 00000000000..bcccc50bbde
--- /dev/null
+++ b/tools/Gemfile.plugins.all
@@ -0,0 +1,13 @@
+require 'octokit'
+skiplist = ['logstash-input-gemfire', 'logstash-output-gemfire', 'logstash-input-couchdb_changes', 'logstash-filter-metricize', 'logstash-filter-yaml']
+
+source 'https://rubygems.org'
+
+gemspec :name => "logstash", :path => ".."
+
+Octokit.auto_paginate = true
+repo_list = Octokit.organization_repositories("logstash-plugins")
+repo_list.each do |repo|
+  next if skiplist.include?(repo.name)
+  gem repo.name
+end
diff --git a/tools/Gemfile.plugins.jruby-1.9.lock b/tools/Gemfile.plugins.jruby-1.9.lock
new file mode 100644
index 00000000000..df2a456beaa
--- /dev/null
+++ b/tools/Gemfile.plugins.jruby-1.9.lock
@@ -0,0 +1,626 @@
+PATH
+  remote: /Users/ph/es/logstash
+  specs:
+    logstash (2.0.0.dev-java)
+      cabin (>= 0.6.0)
+      ci_reporter (= 1.9.3)
+      clamp
+      ftw (~> 0.0.40)
+      i18n (= 0.6.9)
+      insist (= 1.0.0)
+      jar-dependencies (= 0.1.2)
+      jrjackson
+      jruby-httpclient
+      logstash-devutils
+      maven-tools
+      mime-types
+      minitar
+      pry
+      rack
+      rspec (~> 2.14.0)
+      ruby-maven
+      sinatra
+      stud
+      treetop (~> 1.4.0)
+
+GEM
+  remote: https://rubygems.org/
+  specs:
+    addressable (2.3.6)
+    atomic (1.1.16-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.2.0)
+    aws-sdk (1.60.2)
+      aws-sdk-v1 (= 1.60.2)
+    aws-sdk-v1 (1.60.2)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+    axiom-types (0.1.1)
+      descendants_tracker (~> 0.0.4)
+      ice_nine (~> 0.11.0)
+      thread_safe (~> 0.3, >= 0.3.1)
+    backports (3.6.4)
+    bindata (2.1.0)
+    buftok (0.2.0)
+    builder (3.2.2)
+    cabin (0.6.1)
+    ci_reporter (1.9.3)
+      builder (>= 2.1.2)
+    cinch (2.1.0)
+    clamp (0.6.3)
+    coderay (1.1.0)
+    coercible (1.0.0)
+      descendants_tracker (~> 0.0.1)
+    descendants_tracker (0.0.4)
+      thread_safe (~> 0.3, >= 0.3.1)
+    diff-lcs (1.2.5)
+    edn (1.0.6)
+    elasticsearch (1.0.6)
+      elasticsearch-api (= 1.0.6)
+      elasticsearch-transport (= 1.0.6)
+    elasticsearch-api (1.0.6)
+      multi_json
+    elasticsearch-transport (1.0.6)
+      faraday
+      multi_json
+    equalizer (0.0.9)
+    faraday (0.9.0)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.6-java)
+    ffi-rzmq (1.0.0)
+      ffi
+    filewatch (0.5.1)
+    ftw (0.0.42)
+      addressable
+      backports (>= 2.6.2)
+      cabin (> 0)
+      http_parser.rb (~> 0.6)
+    gelf (1.3.2)
+      json
+    gelfd (0.2.0)
+    gem_publisher (1.5.0)
+    geoip (1.4.0)
+    gmetric (0.1.3)
+    hitimes (1.2.2-java)
+    http (0.6.3)
+      http_parser.rb (~> 0.6.0)
+    http_parser.rb (0.6.0-java)
+    i18n (0.6.9)
+    ice_nine (0.11.1)
+    insist (1.0.0)
+    jar-dependencies (0.1.2)
+    jbundler (0.5.5)
+      bundler (~> 1.5)
+      ruby-maven (>= 3.1.1.0.1, < 3.1.2)
+    jls-grok (0.11.0)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.20)
+    jrjackson (0.2.7)
+    jruby-httpclient (1.1.1-java)
+    jruby-kafka (0.2.1-java)
+      jbundler (= 0.5.5)
+    jruby-win32ole (0.8.5)
+    json (1.8.1-java)
+    logstash-codec-collectd (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-dots (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-edn (0.1.3)
+      edn
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-edn_lines (0.1.3)
+      edn
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-line
+    logstash-codec-fluent (0.1.3-java)
+      logstash (>= 1.4.0, < 2.0.0)
+      msgpack-jruby
+    logstash-codec-graphite (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-line
+    logstash-codec-json (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-json_lines (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-line
+    logstash-codec-line (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-msgpack (0.1.4-java)
+      logstash (>= 1.4.0, < 2.0.0)
+      msgpack-jruby
+    logstash-codec-multiline (0.1.3)
+      jls-grok (= 0.11.0)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-patterns-core
+    logstash-codec-netflow (0.1.2)
+      bindata (>= 1.5.0)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-oldlogstashjson (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-plain (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-rubydebug (0.1.4)
+      awesome_print
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-devutils (0.0.6-java)
+      gem_publisher
+      jar-dependencies
+      minitar
+      rake
+    logstash-filter-anonymize (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      murmurhash3
+    logstash-filter-checksum (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-clone (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-csv (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-date (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-input-generator
+      logstash-output-null
+    logstash-filter-dns (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-drop (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-fingerprint (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      murmurhash3
+    logstash-filter-geoip (0.1.2)
+      geoip (>= 1.3.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-grok (0.1.2)
+      jls-grok (= 0.11.0)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-patterns-core
+    logstash-filter-json (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-kv (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-metrics (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+      metriks
+    logstash-filter-multiline (0.1.2)
+      jls-grok (~> 0.11.0)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-filter-mutate
+      logstash-patterns-core
+    logstash-filter-mutate (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-filter-grok
+      logstash-patterns-core
+    logstash-filter-ruby (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-filter-date
+    logstash-filter-sleep (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-split (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-syslog_pri (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-throttle (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-urldecode (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-useragent (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      user_agent_parser (>= 2.0.0)
+    logstash-filter-uuid (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-xml (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      nokogiri
+      xml-simple
+    logstash-input-elasticsearch (0.1.1)
+      elasticsearch (~> 1.0, >= 1.0.6)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+    logstash-input-eventlog (0.1.1-java)
+      jruby-win32ole
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-exec (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-file (0.1.2)
+      addressable
+      filewatch (= 0.5.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-ganglia (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-gelf (0.1.1)
+      gelf (= 1.3.2)
+      gelfd (= 0.2.0)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-generator (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-graphite (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-input-tcp
+    logstash-input-imap (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+      mail
+      stud
+    logstash-input-irc (0.1.1)
+      cinch
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-kafka (0.1.5)
+      jar-dependencies (~> 0.1.0)
+      jruby-kafka (>= 0.2.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-plain
+    logstash-input-log4j (0.1.1)
+      jar-dependencies
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-lumberjack (0.1.1)
+      jls-lumberjack (>= 0.0.20)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-pipe (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-rabbitmq (0.1.1-java)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      march_hare (~> 2.5.1)
+    logstash-input-redis (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      redis
+    logstash-input-s3 (0.1.1)
+      aws-sdk
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+      logstash-mixin-aws
+    logstash-input-snmptrap (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      snmp
+    logstash-input-sqs (0.1.1)
+      aws-sdk
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+    logstash-input-stdin (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+    logstash-input-syslog (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+      logstash-filter-date
+      logstash-filter-grok
+    logstash-input-tcp (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+    logstash-input-twitter (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      twitter (= 5.12.0)
+    logstash-input-udp (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-unix (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-line
+    logstash-input-xmpp (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+      xmpp4r (= 0.5)
+    logstash-input-zeromq (0.1.1)
+      ffi-rzmq
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+    logstash-mixin-aws (0.1.3)
+      aws-sdk
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-output-cloudwatch (0.1.1)
+      aws-sdk
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+      rufus-scheduler (~> 2.0.24)
+    logstash-output-csv (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-filter-json
+      logstash-output-file
+    logstash-output-elasticsearch (0.1.9-java)
+      cabin (~> 0.6)
+      elasticsearch (~> 1.0, >= 1.0.6)
+      jar-dependencies
+      logstash (>= 1.4.0, < 2.0.0)
+      manticore (~> 0.3)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-email (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      mail
+    logstash-output-exec (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-file (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-input-generator
+    logstash-output-ganglia (0.1.1)
+      gmetric (= 0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-gelf (0.1.1)
+      gelf (= 1.3.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-graphite (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-hipchat (0.1.1)
+      ftw (~> 0.0.40)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-http (0.1.1)
+      ftw (~> 0.0.40)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-irc (0.1.1)
+      cinch
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-juggernaut (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      redis
+    logstash-output-kafka (0.1.3)
+      jar-dependencies
+      jruby-kafka (>= 0.2.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-plain
+    logstash-output-lumberjack (0.1.2)
+      jls-lumberjack (>= 0.0.20)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-nagios (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-nagios_nsca (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-null (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-opentsdb (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-pagerduty (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-pipe (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-output-rabbitmq (0.1.2-java)
+      logstash (>= 1.4.0, < 2.0.0)
+      march_hare (~> 2.5.1)
+    logstash-output-redis (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      redis
+      stud
+    logstash-output-s3 (0.1.1)
+      aws-sdk
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+    logstash-output-sns (0.1.1)
+      aws-sdk
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+    logstash-output-sqs (0.1.1)
+      aws-sdk
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+      stud
+    logstash-output-statsd (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-input-generator
+      statsd-ruby (= 1.2.0)
+    logstash-output-stdout (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-line
+    logstash-output-tcp (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      stud
+    logstash-output-udp (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+    logstash-output-xmpp (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      xmpp4r (= 0.5)
+    logstash-output-zeromq (0.1.2)
+      ffi-rzmq (= 1.0.0)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+    logstash-patterns-core (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    mail (2.6.3)
+      mime-types (>= 1.16, < 3)
+    manticore (0.3.3-java)
+      addressable (~> 2.3)
+    march_hare (2.5.1-java)
+    maven-tools (1.0.7)
+      virtus (~> 1.0)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
+    method_source (0.8.2)
+    metriks (0.9.9.7)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.4.3)
+    minitar (0.5.4)
+    msgpack-jruby (1.4.0-java)
+    multi_json (1.10.1)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.5)
+    naught (1.0.0)
+    nokogiri (1.6.5-java)
+    polyglot (0.3.5)
+    pry (0.10.1-java)
+      coderay (~> 1.1.0)
+      method_source (~> 0.8.1)
+      slop (~> 3.4)
+      spoon (~> 0.0)
+    rack (1.6.0)
+    rack-protection (1.5.3)
+      rack
+    rake (10.4.2)
+    redis (3.2.0)
+    rspec (2.14.1)
+      rspec-core (~> 2.14.0)
+      rspec-expectations (~> 2.14.0)
+      rspec-mocks (~> 2.14.0)
+    rspec-core (2.14.8)
+    rspec-expectations (2.14.5)
+      diff-lcs (>= 1.1.3, < 2.0)
+    rspec-mocks (2.14.6)
+    ruby-maven (3.1.1.0.8)
+      maven-tools (~> 1.0.1)
+      ruby-maven-libs (= 3.1.1)
+    ruby-maven-libs (3.1.1)
+    rufus-scheduler (2.0.24)
+      tzinfo (>= 0.3.22)
+    simple_oauth (0.3.0)
+    sinatra (1.4.5)
+      rack (~> 1.4)
+      rack-protection (~> 1.4)
+      tilt (~> 1.3, >= 1.3.4)
+    slop (3.6.0)
+    snmp (1.2.0)
+    spoon (0.0.4)
+      ffi
+    statsd-ruby (1.2.0)
+    stud (0.0.18)
+    thread_safe (0.3.4-java)
+    tilt (1.4.1)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    twitter (5.12.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (~> 0.0.9)
+      faraday (~> 0.9.0)
+      http (~> 0.6.0)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.2)
+      thread_safe (~> 0.1)
+    user_agent_parser (2.2.0)
+    virtus (1.0.3)
+      axiom-types (~> 0.1)
+      coercible (~> 1.0)
+      descendants_tracker (~> 0.0, >= 0.0.3)
+      equalizer (~> 0.0, >= 0.0.9)
+    xml-simple (1.1.4)
+    xmpp4r (0.5)
+
+PLATFORMS
+  java
+
+DEPENDENCIES
+  logstash!
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-oldlogstashjson
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-filter-anonymize
+  logstash-filter-checksum
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-multiline
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-uuid
+  logstash-filter-xml
+  logstash-input-elasticsearch
+  logstash-input-eventlog
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-imap
+  logstash-input-irc
+  logstash-input-kafka
+  logstash-input-log4j
+  logstash-input-lumberjack
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-input-xmpp
+  logstash-input-zeromq
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-email
+  logstash-output-exec
+  logstash-output-file
+  logstash-output-ganglia
+  logstash-output-gelf
+  logstash-output-graphite
+  logstash-output-hipchat
+  logstash-output-http
+  logstash-output-irc
+  logstash-output-juggernaut
+  logstash-output-kafka
+  logstash-output-lumberjack
+  logstash-output-nagios
+  logstash-output-nagios_nsca
+  logstash-output-null
+  logstash-output-opentsdb
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-statsd
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-xmpp
+  logstash-output-zeromq
diff --git a/tools/Gemfile.plugins.test b/tools/Gemfile.plugins.test
new file mode 100644
index 00000000000..5a0576dedc1
--- /dev/null
+++ b/tools/Gemfile.plugins.test
@@ -0,0 +1,16 @@
+require 'rakelib/default_plugins'
+
+source 'https://rubygems.org'
+
+gemspec :name => "logstash", :path => File.expand_path(File.join(File.dirname(__FILE__), ".."))
+
+plugins = [ 'logstash-filter-clone',
+            'logstash-filter-mutate',
+            'logstash-input-generator',
+            'logstash-input-stdin',
+            'logstash-input-tcp',
+            'logstash-output-stdout']
+
+plugins.each do |plugin|
+  gem plugin
+end
diff --git a/tools/Gemfile.plugins.test.jruby-1.9.lock b/tools/Gemfile.plugins.test.jruby-1.9.lock
new file mode 100644
index 00000000000..a103392c772
--- /dev/null
+++ b/tools/Gemfile.plugins.test.jruby-1.9.lock
@@ -0,0 +1,163 @@
+PATH
+  remote: /Users/ph/es/logstash
+  specs:
+    logstash (2.0.0.dev-java)
+      cabin (>= 0.6.0)
+      ci_reporter (= 1.9.3)
+      clamp
+      ftw (~> 0.0.40)
+      i18n (= 0.6.9)
+      insist (= 1.0.0)
+      jar-dependencies (= 0.1.2)
+      jrjackson
+      jruby-httpclient
+      logstash-devutils
+      maven-tools
+      mime-types
+      minitar
+      pry
+      rack
+      rspec (~> 2.14.0)
+      ruby-maven
+      sinatra
+      stud
+      treetop (~> 1.4.0)
+
+GEM
+  remote: https://rubygems.org/
+  specs:
+    addressable (2.3.6)
+    axiom-types (0.1.1)
+      descendants_tracker (~> 0.0.4)
+      ice_nine (~> 0.11.0)
+      thread_safe (~> 0.3, >= 0.3.1)
+    backports (3.6.4)
+    builder (3.2.2)
+    cabin (0.6.1)
+    ci_reporter (1.9.3)
+      builder (>= 2.1.2)
+    clamp (0.6.3)
+    coderay (1.1.0)
+    coercible (1.0.0)
+      descendants_tracker (~> 0.0.1)
+    descendants_tracker (0.0.4)
+      thread_safe (~> 0.3, >= 0.3.1)
+    diff-lcs (1.2.5)
+    equalizer (0.0.9)
+    ffi (1.9.6-java)
+    ftw (0.0.42)
+      addressable
+      backports (>= 2.6.2)
+      cabin (> 0)
+      http_parser.rb (~> 0.6)
+    gem_publisher (1.5.0)
+    http_parser.rb (0.6.0-java)
+    i18n (0.6.9)
+    ice_nine (0.11.1)
+    insist (1.0.0)
+    jar-dependencies (0.1.2)
+    jls-grok (0.11.0)
+      cabin (>= 0.6.0)
+    jrjackson (0.2.7)
+    jruby-httpclient (1.1.1-java)
+    logstash-codec-json (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-json_lines (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-line
+    logstash-codec-line (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-codec-plain (0.1.3)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-devutils (0.0.6-java)
+      gem_publisher
+      jar-dependencies
+      minitar
+      rake
+    logstash-filter-clone (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    logstash-filter-grok (0.1.2)
+      jls-grok (= 0.11.0)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-patterns-core
+    logstash-filter-mutate (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-filter-grok
+      logstash-patterns-core
+    logstash-input-generator (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-plain
+    logstash-input-stdin (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+    logstash-input-tcp (0.1.1)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+    logstash-output-stdout (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+      logstash-codec-line
+    logstash-patterns-core (0.1.2)
+      logstash (>= 1.4.0, < 2.0.0)
+    maven-tools (1.0.7)
+      virtus (~> 1.0)
+    method_source (0.8.2)
+    mime-types (2.4.3)
+    minitar (0.5.4)
+    polyglot (0.3.5)
+    pry (0.10.1-java)
+      coderay (~> 1.1.0)
+      method_source (~> 0.8.1)
+      slop (~> 3.4)
+      spoon (~> 0.0)
+    rack (1.6.0)
+    rack-protection (1.5.3)
+      rack
+    rake (10.4.2)
+    rspec (2.14.1)
+      rspec-core (~> 2.14.0)
+      rspec-expectations (~> 2.14.0)
+      rspec-mocks (~> 2.14.0)
+    rspec-core (2.14.8)
+    rspec-expectations (2.14.5)
+      diff-lcs (>= 1.1.3, < 2.0)
+    rspec-mocks (2.14.6)
+    ruby-maven (3.1.1.0.8)
+      maven-tools (~> 1.0.1)
+      ruby-maven-libs (= 3.1.1)
+    ruby-maven-libs (3.1.1)
+    sinatra (1.4.5)
+      rack (~> 1.4)
+      rack-protection (~> 1.4)
+      tilt (~> 1.3, >= 1.3.4)
+    slop (3.6.0)
+    spoon (0.0.4)
+      ffi
+    stud (0.0.18)
+    thread_safe (0.3.4-java)
+    tilt (1.4.1)
+    treetop (1.4.15)
+      polyglot
+      polyglot (>= 0.3.1)
+    virtus (1.0.3)
+      axiom-types (~> 0.1)
+      coercible (~> 1.0)
+      descendants_tracker (~> 0.0, >= 0.0.3)
+      equalizer (~> 0.0, >= 0.0.9)
+
+PLATFORMS
+  java
+
+DEPENDENCIES
+  logstash!
+  logstash-filter-clone
+  logstash-filter-mutate
+  logstash-input-generator
+  logstash-input-stdin
+  logstash-input-tcp
+  logstash-output-stdout
diff --git a/tools/Gemfile.rbx-2.1.lock b/tools/Gemfile.rbx-2.1.lock
deleted file mode 100644
index a2543530874..00000000000
--- a/tools/Gemfile.rbx-2.1.lock
+++ /dev/null
@@ -1,420 +0,0 @@
-GEM
-  remote: https://rubygems.org/
-  specs:
-    activesupport (3.2.17)
-      i18n (~> 0.6, >= 0.6.4)
-      multi_json (~> 1.0)
-    addressable (2.3.5)
-    amq-protocol (1.9.2)
-    atomic (1.1.15)
-    avl_tree (1.1.3)
-    awesome_print (1.2.0)
-    aws-sdk (1.35.0)
-      json (~> 1.4)
-      nokogiri (>= 1.4.4)
-      uuidtools (~> 2.1)
-    backports (3.6.0)
-    beefcake (0.3.7)
-    bindata (2.0.0)
-    blankslate (2.1.2.4)
-    buftok (0.1)
-    builder (3.2.2)
-    bunny (1.1.3)
-      amq-protocol (>= 1.9.2)
-    cabin (0.6.1)
-    ci_reporter (1.9.1)
-      builder (>= 2.1.2)
-    cinch (2.1.0)
-    clamp (0.6.3)
-    coderay (1.1.0)
-    coveralls (0.7.0)
-      multi_json (~> 1.3)
-      rest-client
-      simplecov (>= 0.7)
-      term-ansicolor
-      thor
-    diff-lcs (1.2.5)
-    docile (1.1.3)
-    edn (1.0.2)
-      parslet (~> 1.4.0)
-    excon (0.32.0)
-    extlib (0.9.16)
-    faraday (0.9.0)
-      multipart-post (>= 1.2, < 3)
-    ffi (1.9.3)
-    ffi-rzmq (1.0.0)
-      ffi
-    ffi2-generators (0.1.1)
-    filewatch (0.5.1)
-    ftw (0.0.39)
-      addressable
-      backports (>= 2.6.2)
-      cabin (> 0)
-      http_parser.rb (= 0.5.3)
-    gelf (1.3.2)
-      json
-    gelfd (0.2.0)
-    geoip (1.3.5)
-    gmetric (0.1.3)
-    haml (4.0.5)
-      tilt
-    hitimes (1.2.1)
-    http (0.5.0)
-      http_parser.rb
-    http_parser.rb (0.5.3)
-    i18n (0.6.9)
-    insist (1.0.0)
-    jls-grok (0.10.12)
-      cabin (>= 0.6.0)
-    jls-lumberjack (0.0.19)
-    json (1.8.1)
-    mail (2.5.3)
-      i18n (>= 0.4.0)
-      mime-types (~> 1.16)
-      treetop (~> 1.4.8)
-    metaclass (0.0.4)
-    method_source (0.8.2)
-    metriks (0.9.9.6)
-      atomic (~> 1.0)
-      avl_tree (~> 1.1.2)
-      hitimes (~> 1.1)
-    mime-types (1.25.1)
-    mini_portile (0.5.2)
-    minitest (5.3.0)
-    mocha (1.0.0)
-      metaclass (~> 0.0.1)
-    msgpack (0.5.8)
-    multi_json (1.8.4)
-    multipart-post (2.0.0)
-    murmurhash3 (0.1.4)
-    nokogiri (1.6.1)
-      mini_portile (~> 0.5.0)
-    parslet (1.4.0)
-      blankslate (~> 2.0)
-    polyglot (0.3.4)
-    pry (0.9.12.6)
-      coderay (~> 1.0)
-      method_source (~> 0.8)
-      slop (~> 3.4)
-    racc (1.4.11)
-    rbnacl (2.0.0)
-      ffi
-    redis (3.0.7)
-    rest-client (1.6.7)
-      mime-types (>= 1.16)
-    rspec (2.14.1)
-      rspec-core (~> 2.14.0)
-      rspec-expectations (~> 2.14.0)
-      rspec-mocks (~> 2.14.0)
-    rspec-core (2.14.7)
-    rspec-expectations (2.14.5)
-      diff-lcs (>= 1.1.3, < 2.0)
-    rspec-mocks (2.14.6)
-    rubysl (2.0.15)
-      rubysl-abbrev (~> 2.0)
-      rubysl-base64 (~> 2.0)
-      rubysl-benchmark (~> 2.0)
-      rubysl-bigdecimal (~> 2.0)
-      rubysl-cgi (~> 2.0)
-      rubysl-cgi-session (~> 2.0)
-      rubysl-cmath (~> 2.0)
-      rubysl-complex (~> 2.0)
-      rubysl-continuation (~> 2.0)
-      rubysl-coverage (~> 2.0)
-      rubysl-csv (~> 2.0)
-      rubysl-curses (~> 2.0)
-      rubysl-date (~> 2.0)
-      rubysl-delegate (~> 2.0)
-      rubysl-digest (~> 2.0)
-      rubysl-drb (~> 2.0)
-      rubysl-e2mmap (~> 2.0)
-      rubysl-english (~> 2.0)
-      rubysl-enumerator (~> 2.0)
-      rubysl-erb (~> 2.0)
-      rubysl-etc (~> 2.0)
-      rubysl-expect (~> 2.0)
-      rubysl-fcntl (~> 2.0)
-      rubysl-fiber (~> 2.0)
-      rubysl-fileutils (~> 2.0)
-      rubysl-find (~> 2.0)
-      rubysl-forwardable (~> 2.0)
-      rubysl-getoptlong (~> 2.0)
-      rubysl-gserver (~> 2.0)
-      rubysl-io-console (~> 2.0)
-      rubysl-io-nonblock (~> 2.0)
-      rubysl-io-wait (~> 2.0)
-      rubysl-ipaddr (~> 2.0)
-      rubysl-irb (~> 2.0)
-      rubysl-logger (~> 2.0)
-      rubysl-mathn (~> 2.0)
-      rubysl-matrix (~> 2.0)
-      rubysl-mkmf (~> 2.0)
-      rubysl-monitor (~> 2.0)
-      rubysl-mutex_m (~> 2.0)
-      rubysl-net-ftp (~> 2.0)
-      rubysl-net-http (~> 2.0)
-      rubysl-net-imap (~> 2.0)
-      rubysl-net-pop (~> 2.0)
-      rubysl-net-protocol (~> 2.0)
-      rubysl-net-smtp (~> 2.0)
-      rubysl-net-telnet (~> 2.0)
-      rubysl-nkf (~> 2.0)
-      rubysl-observer (~> 2.0)
-      rubysl-open-uri (~> 2.0)
-      rubysl-open3 (~> 2.0)
-      rubysl-openssl (~> 2.0)
-      rubysl-optparse (~> 2.0)
-      rubysl-ostruct (~> 2.0)
-      rubysl-pathname (~> 2.0)
-      rubysl-prettyprint (~> 2.0)
-      rubysl-prime (~> 2.0)
-      rubysl-profile (~> 2.0)
-      rubysl-profiler (~> 2.0)
-      rubysl-pstore (~> 2.0)
-      rubysl-pty (~> 2.0)
-      rubysl-rational (~> 2.0)
-      rubysl-readline (~> 2.0)
-      rubysl-resolv (~> 2.0)
-      rubysl-rexml (~> 2.0)
-      rubysl-rinda (~> 2.0)
-      rubysl-rss (~> 2.0)
-      rubysl-scanf (~> 2.0)
-      rubysl-securerandom (~> 2.0)
-      rubysl-set (~> 2.0)
-      rubysl-shellwords (~> 2.0)
-      rubysl-singleton (~> 2.0)
-      rubysl-socket (~> 2.0)
-      rubysl-stringio (~> 2.0)
-      rubysl-strscan (~> 2.0)
-      rubysl-sync (~> 2.0)
-      rubysl-syslog (~> 2.0)
-      rubysl-tempfile (~> 2.0)
-      rubysl-thread (~> 2.0)
-      rubysl-thwait (~> 2.0)
-      rubysl-time (~> 2.0)
-      rubysl-timeout (~> 2.0)
-      rubysl-tmpdir (~> 2.0)
-      rubysl-tsort (~> 2.0)
-      rubysl-un (~> 2.0)
-      rubysl-uri (~> 2.0)
-      rubysl-weakref (~> 2.0)
-      rubysl-webrick (~> 2.0)
-      rubysl-xmlrpc (~> 2.0)
-      rubysl-yaml (~> 2.0)
-      rubysl-zlib (~> 2.0)
-    rubysl-abbrev (2.0.4)
-    rubysl-base64 (2.0.0)
-    rubysl-benchmark (2.0.1)
-    rubysl-bigdecimal (2.0.2)
-    rubysl-cgi (2.0.1)
-    rubysl-cgi-session (2.0.1)
-    rubysl-cmath (2.0.0)
-    rubysl-complex (2.0.0)
-    rubysl-continuation (2.0.0)
-    rubysl-coverage (2.0.3)
-    rubysl-csv (2.0.2)
-      rubysl-english (~> 2.0)
-    rubysl-curses (2.0.1)
-    rubysl-date (2.0.6)
-    rubysl-delegate (2.0.1)
-    rubysl-digest (2.0.3)
-    rubysl-drb (2.0.1)
-    rubysl-e2mmap (2.0.0)
-    rubysl-english (2.0.0)
-    rubysl-enumerator (2.0.0)
-    rubysl-erb (2.0.1)
-    rubysl-etc (2.0.3)
-      ffi2-generators (~> 0.1)
-    rubysl-expect (2.0.0)
-    rubysl-fcntl (2.0.4)
-      ffi2-generators (~> 0.1)
-    rubysl-fiber (2.0.0)
-    rubysl-fileutils (2.0.3)
-    rubysl-find (2.0.1)
-    rubysl-forwardable (2.0.1)
-    rubysl-getoptlong (2.0.0)
-    rubysl-gserver (2.0.0)
-      rubysl-socket (~> 2.0)
-      rubysl-thread (~> 2.0)
-    rubysl-io-console (2.0.0)
-    rubysl-io-nonblock (2.0.0)
-    rubysl-io-wait (2.0.0)
-    rubysl-ipaddr (2.0.0)
-    rubysl-irb (2.0.4)
-      rubysl-e2mmap (~> 2.0)
-      rubysl-mathn (~> 2.0)
-      rubysl-readline (~> 2.0)
-      rubysl-thread (~> 2.0)
-    rubysl-logger (2.0.0)
-    rubysl-mathn (2.0.0)
-    rubysl-matrix (2.1.0)
-      rubysl-e2mmap (~> 2.0)
-    rubysl-mkmf (2.0.1)
-      rubysl-fileutils (~> 2.0)
-      rubysl-shellwords (~> 2.0)
-    rubysl-monitor (2.0.0)
-    rubysl-mutex_m (2.0.0)
-    rubysl-net-ftp (2.0.1)
-    rubysl-net-http (2.0.4)
-      rubysl-cgi (~> 2.0)
-      rubysl-erb (~> 2.0)
-      rubysl-singleton (~> 2.0)
-    rubysl-net-imap (2.0.1)
-    rubysl-net-pop (2.0.1)
-    rubysl-net-protocol (2.0.1)
-    rubysl-net-smtp (2.0.1)
-    rubysl-net-telnet (2.0.0)
-    rubysl-nkf (2.0.1)
-    rubysl-observer (2.0.0)
-    rubysl-open-uri (2.0.0)
-    rubysl-open3 (2.0.0)
-    rubysl-openssl (2.1.0)
-    rubysl-optparse (2.0.1)
-      rubysl-shellwords (~> 2.0)
-    rubysl-ostruct (2.0.4)
-    rubysl-pathname (2.0.0)
-    rubysl-prettyprint (2.0.2)
-    rubysl-prime (2.0.1)
-    rubysl-profile (2.0.0)
-    rubysl-profiler (2.0.1)
-    rubysl-pstore (2.0.0)
-    rubysl-pty (2.0.2)
-    rubysl-rational (2.0.1)
-    rubysl-readline (2.0.2)
-    rubysl-resolv (2.1.0)
-    rubysl-rexml (2.0.2)
-    rubysl-rinda (2.0.1)
-    rubysl-rss (2.0.0)
-    rubysl-scanf (2.0.0)
-    rubysl-securerandom (2.0.0)
-    rubysl-set (2.0.1)
-    rubysl-shellwords (2.0.0)
-    rubysl-singleton (2.0.0)
-    rubysl-socket (2.0.1)
-    rubysl-stringio (2.0.0)
-    rubysl-strscan (2.0.0)
-    rubysl-sync (2.0.0)
-    rubysl-syslog (2.0.1)
-      ffi2-generators (~> 0.1)
-    rubysl-tempfile (2.0.1)
-    rubysl-thread (2.0.2)
-    rubysl-thwait (2.0.0)
-    rubysl-time (2.0.3)
-    rubysl-timeout (2.0.0)
-    rubysl-tmpdir (2.0.0)
-    rubysl-tsort (2.0.1)
-    rubysl-un (2.0.0)
-      rubysl-fileutils (~> 2.0)
-      rubysl-optparse (~> 2.0)
-    rubysl-uri (2.0.0)
-    rubysl-weakref (2.0.0)
-    rubysl-webrick (2.0.0)
-    rubysl-xmlrpc (2.0.0)
-    rubysl-yaml (2.0.4)
-    rubysl-zlib (2.0.1)
-    rufus-scheduler (2.0.24)
-      tzinfo (>= 0.3.22)
-    rumbster (1.1.1)
-      mail (= 2.5.3)
-    sass (3.2.14)
-    shoulda (3.5.0)
-      shoulda-context (~> 1.0, >= 1.0.1)
-      shoulda-matchers (>= 1.4.1, < 3.0)
-    shoulda-context (1.1.6)
-    shoulda-matchers (2.5.0)
-      activesupport (>= 3.0.0)
-    simple_oauth (0.2.0)
-    simplecov (0.8.2)
-      docile (~> 1.1.0)
-      multi_json
-      simplecov-html (~> 0.8.0)
-    simplecov-html (0.8.0)
-    slop (3.4.7)
-    snmp (1.1.1)
-    spoon (0.0.4)
-      ffi
-    statsd-ruby (1.2.0)
-    stud (0.0.17)
-      ffi
-      metriks
-    term-ansicolor (1.3.0)
-      tins (~> 1.0)
-    thor (0.18.1)
-    thread_safe (0.2.0)
-      atomic (>= 1.1.7, < 2)
-    tilt (2.0.0)
-    tins (1.0.0)
-    treetop (1.4.15)
-      polyglot
-      polyglot (>= 0.3.1)
-    twitter (5.0.0.rc.1)
-      buftok (~> 0.1.0)
-      faraday (>= 0.8, < 0.10)
-      http (>= 0.5.0.pre2, < 0.6)
-      http_parser.rb (~> 0.5.0)
-      json (~> 1.8)
-      simple_oauth (~> 0.2.0)
-    tzinfo (1.1.0)
-      thread_safe (~> 0.1)
-    user_agent_parser (2.1.2)
-    uuidtools (2.1.4)
-    xml-simple (1.1.3)
-    xmpp4r (0.5)
-
-PLATFORMS
-  ruby
-
-DEPENDENCIES
-  addressable
-  awesome_print
-  aws-sdk
-  beefcake (= 0.3.7)
-  bindata (>= 1.5.0)
-  bunny (~> 1.1.0)
-  cabin (>= 0.6.0)
-  ci_reporter
-  cinch
-  clamp
-  coveralls
-  edn
-  excon
-  extlib (= 0.9.16)
-  ffi
-  ffi-rzmq (= 1.0.0)
-  filewatch (= 0.5.1)
-  ftw (~> 0.0.39)
-  gelf (= 1.3.2)
-  gelfd (= 0.2.0)
-  geoip (>= 1.3.2)
-  gmetric (= 0.1.3)
-  haml
-  i18n (>= 0.6.6)
-  insist (= 1.0.0)
-  jls-grok (= 0.10.12)
-  jls-lumberjack (>= 0.0.19)
-  json
-  mail
-  metriks
-  mime-types
-  minitest
-  mocha
-  msgpack
-  murmurhash3
-  pry
-  racc
-  rbnacl
-  redis
-  rspec
-  rubysl
-  rufus-scheduler (~> 2.0.24)
-  rumbster
-  sass
-  shoulda
-  snmp
-  spoon
-  statsd-ruby (= 1.2.0)
-  stud
-  twitter (= 5.0.0.rc.1)
-  user_agent_parser (>= 2.0.0)
-  xml-simple
-  xmpp4r (= 0.5)
diff --git a/tools/Gemfile.ruby-1.9.1.lock b/tools/Gemfile.ruby-1.9.1.lock
deleted file mode 100644
index 56999a0b9b4..00000000000
--- a/tools/Gemfile.ruby-1.9.1.lock
+++ /dev/null
@@ -1,227 +0,0 @@
-GEM
-  remote: https://rubygems.org/
-  specs:
-    activesupport (3.2.17)
-      i18n (~> 0.6, >= 0.6.4)
-      multi_json (~> 1.0)
-    addressable (2.3.5)
-    amq-protocol (1.9.2)
-    atomic (1.1.15)
-    avl_tree (1.1.3)
-    awesome_print (1.2.0)
-    aws-sdk (1.35.0)
-      json (~> 1.4)
-      nokogiri (>= 1.4.4)
-      uuidtools (~> 2.1)
-    backports (3.6.0)
-    beefcake (0.3.7)
-    bindata (2.0.0)
-    blankslate (2.1.2.4)
-    buftok (0.1)
-    builder (3.2.2)
-    bunny (1.1.3)
-      amq-protocol (>= 1.9.2)
-    cabin (0.6.1)
-    ci_reporter (1.9.1)
-      builder (>= 2.1.2)
-    cinch (2.1.0)
-    clamp (0.6.3)
-    coderay (1.1.0)
-    coveralls (0.7.0)
-      multi_json (~> 1.3)
-      rest-client
-      simplecov (>= 0.7)
-      term-ansicolor
-      thor
-    diff-lcs (1.2.5)
-    docile (1.1.3)
-    edn (1.0.2)
-      parslet (~> 1.4.0)
-    elasticsearch (1.0.1)
-      elasticsearch-api (= 1.0.1)
-      elasticsearch-transport (= 1.0.1)
-    elasticsearch-api (1.0.1)
-      multi_json
-    elasticsearch-transport (1.0.1)
-      faraday
-      multi_json
-    excon (0.32.0)
-    extlib (0.9.16)
-    faraday (0.9.0)
-      multipart-post (>= 1.2, < 3)
-    ffi (1.9.3)
-    ffi-rzmq (1.0.0)
-      ffi
-    filewatch (0.5.1)
-    ftw (0.0.39)
-      addressable
-      backports (>= 2.6.2)
-      cabin (> 0)
-      http_parser.rb (= 0.5.3)
-    gelf (1.3.2)
-      json
-    gelfd (0.2.0)
-    geoip (1.3.5)
-    gmetric (0.1.3)
-    hitimes (1.2.1)
-    http (0.5.0)
-      http_parser.rb
-    http_parser.rb (0.5.3)
-    i18n (0.6.9)
-    insist (1.0.0)
-    jls-grok (0.10.12)
-      cabin (>= 0.6.0)
-    jls-lumberjack (0.0.20)
-    json (1.8.1)
-    mail (2.5.3)
-      i18n (>= 0.4.0)
-      mime-types (~> 1.16)
-      treetop (~> 1.4.8)
-    metaclass (0.0.4)
-    method_source (0.8.2)
-    metriks (0.9.9.6)
-      atomic (~> 1.0)
-      avl_tree (~> 1.1.2)
-      hitimes (~> 1.1)
-    mime-types (1.25.1)
-    mini_portile (0.5.2)
-    minitest (5.3.0)
-    mocha (1.0.0)
-      metaclass (~> 0.0.1)
-    msgpack (0.5.8)
-    multi_json (1.8.4)
-    multipart-post (2.0.0)
-    murmurhash3 (0.1.4)
-    nokogiri (1.6.1)
-      mini_portile (~> 0.5.0)
-    parslet (1.4.0)
-      blankslate (~> 2.0)
-    polyglot (0.3.4)
-    pry (0.9.12.6)
-      coderay (~> 1.0)
-      method_source (~> 0.8)
-      slop (~> 3.4)
-    rack (1.5.2)
-    rack-protection (1.5.2)
-      rack
-    rbnacl (2.0.0)
-      ffi
-    redis (3.0.7)
-    rest-client (1.6.7)
-      mime-types (>= 1.16)
-    rspec (2.14.1)
-      rspec-core (~> 2.14.0)
-      rspec-expectations (~> 2.14.0)
-      rspec-mocks (~> 2.14.0)
-    rspec-core (2.14.7)
-    rspec-expectations (2.14.5)
-      diff-lcs (>= 1.1.3, < 2.0)
-    rspec-mocks (2.14.6)
-    rufus-scheduler (2.0.24)
-      tzinfo (>= 0.3.22)
-    rumbster (1.1.1)
-      mail (= 2.5.3)
-    shoulda (3.5.0)
-      shoulda-context (~> 1.0, >= 1.0.1)
-      shoulda-matchers (>= 1.4.1, < 3.0)
-    shoulda-context (1.1.6)
-    shoulda-matchers (2.5.0)
-      activesupport (>= 3.0.0)
-    simple_oauth (0.2.0)
-    simplecov (0.8.2)
-      docile (~> 1.1.0)
-      multi_json
-      simplecov-html (~> 0.8.0)
-    simplecov-html (0.8.0)
-    sinatra (1.4.4)
-      rack (~> 1.4)
-      rack-protection (~> 1.4)
-      tilt (~> 1.3, >= 1.3.4)
-    slop (3.4.7)
-    snmp (1.1.1)
-    spoon (0.0.4)
-      ffi
-    statsd-ruby (1.2.0)
-    stud (0.0.17)
-      ffi
-      metriks
-    term-ansicolor (1.3.0)
-      tins (~> 1.0)
-    thor (0.18.1)
-    thread_safe (0.2.0)
-      atomic (>= 1.1.7, < 2)
-    tilt (1.4.1)
-    tins (1.0.0)
-    treetop (1.4.15)
-      polyglot
-      polyglot (>= 0.3.1)
-    twitter (5.0.0.rc.1)
-      buftok (~> 0.1.0)
-      faraday (>= 0.8, < 0.10)
-      http (>= 0.5.0.pre2, < 0.6)
-      http_parser.rb (~> 0.5.0)
-      json (~> 1.8)
-      simple_oauth (~> 0.2.0)
-    tzinfo (1.1.0)
-      thread_safe (~> 0.1)
-    user_agent_parser (2.1.2)
-    uuidtools (2.1.4)
-    xml-simple (1.1.3)
-    xmpp4r (0.5)
-
-PLATFORMS
-  ruby
-
-DEPENDENCIES
-  addressable
-  awesome_print
-  aws-sdk
-  beefcake (= 0.3.7)
-  bindata (>= 1.5.0)
-  bunny (~> 1.1.0)
-  cabin (>= 0.6.0)
-  ci_reporter
-  cinch
-  clamp
-  coveralls
-  edn
-  elasticsearch
-  excon
-  extlib (= 0.9.16)
-  ffi
-  ffi-rzmq (= 1.0.0)
-  filewatch (= 0.5.1)
-  ftw (~> 0.0.39)
-  gelf (= 1.3.2)
-  gelfd (= 0.2.0)
-  geoip (>= 1.3.2)
-  gmetric (= 0.1.3)
-  i18n (>= 0.6.6)
-  insist (= 1.0.0)
-  jls-grok (= 0.10.12)
-  jls-lumberjack (>= 0.0.20)
-  json
-  mail
-  metriks
-  mime-types
-  minitest
-  mocha
-  msgpack
-  murmurhash3
-  pry
-  rack
-  rbnacl
-  redis
-  rspec
-  rufus-scheduler (~> 2.0.24)
-  rumbster
-  shoulda
-  sinatra
-  snmp
-  spoon
-  statsd-ruby (= 1.2.0)
-  stud
-  twitter (= 5.0.0.rc.1)
-  user_agent_parser (>= 2.0.0)
-  xml-simple
-  xmpp4r (= 0.5)
diff --git a/tools/Gemfile.ruby-2.1.0.lock b/tools/Gemfile.ruby-2.1.0.lock
deleted file mode 100644
index e5162a84d39..00000000000
--- a/tools/Gemfile.ruby-2.1.0.lock
+++ /dev/null
@@ -1,214 +0,0 @@
-GEM
-  remote: https://rubygems.org/
-  specs:
-    activesupport (3.2.17)
-      i18n (~> 0.6, >= 0.6.4)
-      multi_json (~> 1.0)
-    addressable (2.3.5)
-    amq-protocol (1.9.2)
-    atomic (1.1.15)
-    avl_tree (1.1.3)
-    awesome_print (1.2.0)
-    aws-sdk (1.35.0)
-      json (~> 1.4)
-      nokogiri (>= 1.4.4)
-      uuidtools (~> 2.1)
-    backports (3.6.0)
-    beefcake (0.3.7)
-    bindata (2.0.0)
-    blankslate (2.1.2.4)
-    buftok (0.1)
-    builder (3.2.2)
-    bunny (1.1.3)
-      amq-protocol (>= 1.9.2)
-    cabin (0.6.1)
-    ci_reporter (1.9.1)
-      builder (>= 2.1.2)
-    cinch (2.1.0)
-    clamp (0.6.3)
-    coderay (1.1.0)
-    coveralls (0.7.0)
-      multi_json (~> 1.3)
-      rest-client
-      simplecov (>= 0.7)
-      term-ansicolor
-      thor
-    diff-lcs (1.2.5)
-    docile (1.1.3)
-    edn (1.0.2)
-      parslet (~> 1.4.0)
-    excon (0.32.0)
-    extlib (0.9.16)
-    faraday (0.9.0)
-      multipart-post (>= 1.2, < 3)
-    ffi (1.9.3)
-    ffi-rzmq (1.0.0)
-      ffi
-    filewatch (0.5.1)
-    ftw (0.0.39)
-      addressable
-      backports (>= 2.6.2)
-      cabin (> 0)
-      http_parser.rb (= 0.5.3)
-    gelf (1.3.2)
-      json
-    gelfd (0.2.0)
-    geoip (1.3.5)
-    gmetric (0.1.3)
-    haml (4.0.5)
-      tilt
-    hitimes (1.2.1)
-    http (0.5.0)
-      http_parser.rb
-    http_parser.rb (0.5.3)
-    i18n (0.6.9)
-    insist (1.0.0)
-    jls-grok (0.10.12)
-      cabin (>= 0.6.0)
-    jls-lumberjack (0.0.19)
-    json (1.8.1)
-    mail (2.5.3)
-      i18n (>= 0.4.0)
-      mime-types (~> 1.16)
-      treetop (~> 1.4.8)
-    metaclass (0.0.4)
-    method_source (0.8.2)
-    metriks (0.9.9.6)
-      atomic (~> 1.0)
-      avl_tree (~> 1.1.2)
-      hitimes (~> 1.1)
-    mime-types (1.25.1)
-    mini_portile (0.5.2)
-    minitest (5.3.0)
-    mocha (1.0.0)
-      metaclass (~> 0.0.1)
-    msgpack (0.5.8)
-    multi_json (1.8.4)
-    multipart-post (2.0.0)
-    murmurhash3 (0.1.4)
-    nokogiri (1.6.1)
-      mini_portile (~> 0.5.0)
-    parslet (1.4.0)
-      blankslate (~> 2.0)
-    polyglot (0.3.4)
-    pry (0.9.12.6)
-      coderay (~> 1.0)
-      method_source (~> 0.8)
-      slop (~> 3.4)
-    rbnacl (2.0.0)
-      ffi
-    redis (3.0.7)
-    rest-client (1.6.7)
-      mime-types (>= 1.16)
-    rspec (2.14.1)
-      rspec-core (~> 2.14.0)
-      rspec-expectations (~> 2.14.0)
-      rspec-mocks (~> 2.14.0)
-    rspec-core (2.14.7)
-    rspec-expectations (2.14.5)
-      diff-lcs (>= 1.1.3, < 2.0)
-    rspec-mocks (2.14.6)
-    rufus-scheduler (2.0.24)
-      tzinfo (>= 0.3.22)
-    rumbster (1.1.1)
-      mail (= 2.5.3)
-    sass (3.2.14)
-    shoulda (3.5.0)
-      shoulda-context (~> 1.0, >= 1.0.1)
-      shoulda-matchers (>= 1.4.1, < 3.0)
-    shoulda-context (1.1.6)
-    shoulda-matchers (2.5.0)
-      activesupport (>= 3.0.0)
-    simple_oauth (0.2.0)
-    simplecov (0.8.2)
-      docile (~> 1.1.0)
-      multi_json
-      simplecov-html (~> 0.8.0)
-    simplecov-html (0.8.0)
-    slop (3.4.7)
-    snmp (1.1.1)
-    spoon (0.0.4)
-      ffi
-    statsd-ruby (1.2.0)
-    stud (0.0.17)
-      ffi
-      metriks
-    term-ansicolor (1.3.0)
-      tins (~> 1.0)
-    thor (0.18.1)
-    thread_safe (0.2.0)
-      atomic (>= 1.1.7, < 2)
-    tilt (2.0.0)
-    tins (1.0.0)
-    treetop (1.4.15)
-      polyglot
-      polyglot (>= 0.3.1)
-    twitter (5.0.0.rc.1)
-      buftok (~> 0.1.0)
-      faraday (>= 0.8, < 0.10)
-      http (>= 0.5.0.pre2, < 0.6)
-      http_parser.rb (~> 0.5.0)
-      json (~> 1.8)
-      simple_oauth (~> 0.2.0)
-    tzinfo (1.1.0)
-      thread_safe (~> 0.1)
-    user_agent_parser (2.1.2)
-    uuidtools (2.1.4)
-    xml-simple (1.1.3)
-    xmpp4r (0.5)
-
-PLATFORMS
-  ruby
-
-DEPENDENCIES
-  addressable
-  awesome_print
-  aws-sdk
-  beefcake (= 0.3.7)
-  bindata (>= 1.5.0)
-  bunny (~> 1.1.0)
-  cabin (>= 0.6.0)
-  ci_reporter
-  cinch
-  clamp
-  coveralls
-  edn
-  excon
-  extlib (= 0.9.16)
-  ffi
-  ffi-rzmq (= 1.0.0)
-  filewatch (= 0.5.1)
-  ftw (~> 0.0.39)
-  gelf (= 1.3.2)
-  gelfd (= 0.2.0)
-  geoip (>= 1.3.2)
-  gmetric (= 0.1.3)
-  haml
-  i18n (>= 0.6.6)
-  insist (= 1.0.0)
-  jls-grok (= 0.10.12)
-  jls-lumberjack (>= 0.0.19)
-  json
-  mail
-  metriks
-  mime-types
-  minitest
-  mocha
-  msgpack
-  murmurhash3
-  pry
-  rbnacl
-  redis
-  rspec
-  rufus-scheduler (~> 2.0.24)
-  rumbster
-  sass
-  shoulda
-  snmp
-  spoon
-  statsd-ruby (= 1.2.0)
-  stud
-  twitter (= 5.0.0.rc.1)
-  user_agent_parser (>= 2.0.0)
-  xml-simple
-  xmpp4r (= 0.5)
diff --git a/tools/release.sh b/tools/release.sh
index d667991d419..435196a95cb 100644
--- a/tools/release.sh
+++ b/tools/release.sh
@@ -38,8 +38,8 @@ docs() {
 }
 
 tests() {
-  USE_JRUBY=1 make -C $logstash test QUIET=
-  USE_JRUBY=1 make -C $logstash tarball test QUIET=
+  make -C $logstash test QUIET=
+  make -C $logstash tarball test QUIET=
 }
 
 packages() {
