diff --git a/.github/ISSUE_TEMPLATE.md b/.github/ISSUE_TEMPLATE.md
new file mode 100644
index 00000000000..9d85c7cfddb
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE.md
@@ -0,0 +1,11 @@
+Please post all product and debugging questions on our [forum](https://discuss.elastic.co/c/logstash). Your questions will reach our wider community members there, and if we confirm that there is a bug, then we can open a new issue here.
+
+Logstash Plugins are located in a different organization: https://github.com/logstash-plugins. For bugs on specific Logstash plugins, for example, if Redis Output has a defect, please open it in the respective Redis Output repository. 
+
+For all general issues, please provide the following details for fast resolution:
+
+- Version:
+- Operating System:
+- Config File (if you have sensitive info, please remove it):
+- Sample Data:
+- Steps to Reproduce:
diff --git a/.github/PULL_REQUEST_TEMPLATE.md.unused b/.github/PULL_REQUEST_TEMPLATE.md.unused
new file mode 100644
index 00000000000..a1538275aec
--- /dev/null
+++ b/.github/PULL_REQUEST_TEMPLATE.md.unused
@@ -0,0 +1 @@
+Thanks for contributing to Logstash! If you haven't already signed our CLA, here's a handy link: https://www.elastic.co/contributor-agreement/
diff --git a/.gitignore b/.gitignore
index ced4dece32f..d6cbc6a259f 100644
--- a/.gitignore
+++ b/.gitignore
@@ -26,3 +26,10 @@ spec/reports
 rspec.xml
 .install-done
 .vendor
+integration_run
+.mvn/
+qa/.vm_ssh_config
+qa/.vagrant
+qa/.rspec
+qa/acceptance/.vagrant
+qa/Gemfile.lock
diff --git a/.tailor b/.tailor
deleted file mode 100644
index 5e883dba31d..00000000000
--- a/.tailor
+++ /dev/null
@@ -1,8 +0,0 @@
-Tailor.config do |config|
-  config.file_set '*.rb' do |style|
-    style.indentation_spaces 2, :level => :off
-    style.max_line_length 80, :level => :off
-    style.allow_trailing_line_spaces true, :level => :off
-    style.spaces_after_comma false, :level => :off
-  end
-end
diff --git a/.travis.yml b/.travis.yml
new file mode 100644
index 00000000000..2d9b5783022
--- /dev/null
+++ b/.travis.yml
@@ -0,0 +1,15 @@
+sudo: false
+language: ruby
+cache:
+  directories:
+    - vendor/bundle
+    - ~/.gradle/
+rvm:
+  - jruby-1.7.25
+jdk:
+  - oraclejdk8
+before_script:
+  - rake test:install-core
+  - echo "--order rand" > .rspec
+  - echo "--format documentation" >> .rspec
+script: rake test:core
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index d325328823b..e9eab8029ac 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -10,7 +10,7 @@ That said, some basic guidelines, which you are free to ignore :)
 
 ## Want to learn?
 
-Want to lurk about and see what others are doing with Logstash? 
+Want to lurk about and see what others are doing with Logstash?
 
 * The irc channel (#logstash on irc.freenode.org) is a good place for this
 * The [forum](https://discuss.elastic.co/c/logstash) is also
@@ -18,7 +18,7 @@ Want to lurk about and see what others are doing with Logstash?
 
 ## Got Questions?
 
-Have a problem you want Logstash to solve for you? 
+Have a problem you want Logstash to solve for you?
 
 * You can ask a question in the [forum](https://discuss.elastic.co/c/logstash)
 * Alternately, you are welcome to join the IRC channel #logstash on
@@ -36,12 +36,32 @@ If you think you found a bug, it probably is a bug.
 * If it is specific to a plugin, please file it in the respective repository under [logstash-plugins](https://github.com/logstash-plugins)
 * or ask the [forum](https://discuss.elastic.co/c/logstash).
 
+## Issue Prioritization
+The Logstash team takes time to digest, consider solutions, and weigh applicability of issues to both the broad
+Logstash user base and our own goals for the project. Through this process, we assign issues a priority using GitHub
+labels. Below is a description of priority labels.
+
+* P1: A high priority issue that affects almost all Logstash users. Bugs that would cause data loss, security
+issues and test failures. Workarounds for P1s generally don’t exist without a code change. A P1 issue is usually
+stop the world kinda scenario, so we need to make sure P1s are properly triaged and being worked upon.
+* P2: A broadly applicable, high visibility issue that enhances Logstash usability for a majority of users.
+* P3: Nice-to-have bug fixes or functionality.  Workarounds for P3s generally exist.
+* P4: Anything not in above, catch-all label.
+
 # Contributing Documentation and Code Changes
 
-If you have a bugfix or new feature that you would like to contribute to
-logstash, and you think it will take more than a few minutes to produce the fix
-(ie; write code), it is worth discussing the change with the Logstash users and developers first! You can reach us via [GitHub](https://github.com/elastic/logstash/issues), the [forum](https://discuss.elastic.co/c/logstash), or via IRC (#logstash on freenode irc)
-Please note that Pull Requests without tests will not be merged. If you would like to contribute but do not have experience with writing tests, please ping us on IRC/forum or create a PR and ask our help.
+If you have a bugfix or new feature that you would like to contribute to Logstash, and you think it will take
+more than a few minutes to produce the fix (ie; write code), it is worth discussing the change with the Logstash
+users and developers first! You can reach us via [GitHub](https://github.com/elastic/logstash/issues), the [forum](https://discuss.elastic.co/c/logstash), or via IRC (#logstash on freenode irc)
+Please note that Pull Requests without tests will not be merged. If you would like to contribute but do not have
+experience with writing tests, please ping us on IRC/forum or create a PR and ask our help.
+
+If you would like to contribute to Logstash, but don't know where to start, you can use the GitHub labels "adoptme"
+and "low hanging fruit". Issues marked with these labels are relatively easy, and provides a good starting
+point to contribute to Logstash.
+
+See: https://github.com/elastic/logstash/labels/adoptme
+https://github.com/elastic/logstash/labels/low%20hanging%20fruit
 
 ## Contributing to plugins
 
@@ -61,5 +81,3 @@ Check our [documentation](https://www.elastic.co/guide/en/logstash/current/contr
    request](https://help.github.com/articles/using-pull-requests). In the pull
    request, describe what your changes do and mention any bugs/issues related
    to the pull request.
-
-
diff --git a/Gemfile b/Gemfile
index cef2642798b..f4aab97ef6c 100644
--- a/Gemfile
+++ b/Gemfile
@@ -2,7 +2,9 @@
 # If you modify this file manually all comments and formatting will be lost.
 
 source "https://rubygems.org"
-gem "logstash-core", "3.0.0.dev", :path => "."
+gem "logstash-core", "5.0.0.dev", :path => "./logstash-core"
+gem "logstash-core-event-java", "5.0.0.dev", :path => "./logstash-core-event-java"
+gem "logstash-core-plugin-api", "2.0.0", :path => "./logstash-core-plugin-api"
 gem "file-dependencies", "0.1.6"
 gem "ci_reporter_rspec", "1.0.0", :group => :development
 gem "simplecov", :group => :development
@@ -12,11 +14,13 @@ gem "coveralls", :group => :development
 # 1.6 is the last supported version on jruby.
 gem "tins", "1.6", :group => :development
 gem "rspec", "~> 3.1.0", :group => :development
-gem "logstash-devutils", "~> 0.0.15", :group => :development
+gem "logstash-devutils", :group => :development
 gem "benchmark-ips", :group => :development
 gem "octokit", "3.8.0", :group => :build
-gem "stud", "~> 0.0.21", :group => :build
+gem "stud", "~> 0.0.22", :group => :build
 gem "fpm", "~> 1.3.3", :group => :build
 gem "rubyzip", "~> 1.1.7", :group => :build
 gem "gems", "~> 0.8.3", :group => :build
+gem "rack-test", :require => "rack/test", :group => :development
 gem "flores", "~> 0.0.6", :group => :development
+gem "pleaserun"
diff --git a/Gemfile.jruby-1.9.lock b/Gemfile.jruby-1.9.lock
index a2accef2d25..709340a1965 100644
--- a/Gemfile.jruby-1.9.lock
+++ b/Gemfile.jruby-1.9.lock
@@ -1,57 +1,210 @@
+GIT
+  remote: git://github.com/elastic/logstash-devutils.git
+  revision: e9af3a24824a41d0f19b025ca359e0735e820251
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-devutils (0.0.19-java)
+      fivemat
+      gem_publisher
+      insist (= 1.0.0)
+      kramdown
+      minitar
+      rake
+      rspec (~> 3.1.0)
+      rspec-wait
+      stud (>= 0.0.20)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-codec-json.git
+  revision: 141d488f3a73706d5ae08dd41468fed285fbbdf9
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-codec-json (2.1.4)
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-codec-json_lines.git
+  revision: f1a3fda4d1affaa6da4b037eec78ee5f10287eb8
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-codec-json_lines (2.1.3)
+      logstash-codec-line (>= 2.1.0)
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-codec-line.git
+  revision: f912b6ca5137bef914ea0e605cb34520b0f172d4
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-codec-line (2.1.2)
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-codec-plain.git
+  revision: 5c5424905d5e06d7f7adc888411057c8a5681194
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-codec-plain (2.0.4)
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-filter-clone.git
+  revision: 4da247817809ff68a47557f022c8049536651564
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-filter-clone (2.0.6)
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-filter-grok.git
+  revision: 284dc6614b0cac6770cda2c18a52f8405146f1eb
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-filter-grok (2.0.5)
+      jls-grok (~> 0.11.1)
+      logstash-core-plugin-api (~> 2.0)
+      logstash-patterns-core
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-filter-multiline.git
+  revision: fd1da31b7d1e0b44319c0f2865c767b36440583d
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-filter-multiline (2.0.5)
+      jls-grok (~> 0.11.0)
+      logstash-core-plugin-api (~> 2.0)
+      logstash-patterns-core
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-filter-mutate.git
+  revision: f9624cc0a05354c308b2d22a5895c737eceaa08b
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-filter-mutate (2.0.6)
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-input-generator.git
+  revision: 2557a4e7da667d6e801cac47e27ff297b6ecc79a
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-input-generator (2.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-input-stdin.git
+  revision: cca193c505f931500bd0f6be45afada2af0578ed
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-input-stdin (2.0.4)
+      concurrent-ruby
+      logstash-codec-line
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-input-tcp.git
+  revision: b6f2705d7fc226c08dd0905561ef9373d2801a58
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-input-tcp (3.0.5)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-output-stdout.git
+  revision: c150cb4f560b047372e15887e8b6bf421af22fc9
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-output-stdout (2.0.6)
+      logstash-codec-line
+      logstash-core-plugin-api (~> 2.0)
+
+GIT
+  remote: git://github.com/logstash-plugins/logstash-patterns-core.git
+  revision: e039ba2d3db6a01b6501fe0cab8a82453c6dc45f
+  branch: feature/plugin-api-2_0
+  specs:
+    logstash-patterns-core (2.0.5)
+      logstash-core-plugin-api (~> 2.0)
+
 PATH
-  remote: .
+  remote: ./logstash-core
   specs:
-    logstash-core (3.0.0.dev-java)
-      cabin (~> 0.7.0)
+    logstash-core (5.0.0.dev-java)
+      cabin (~> 0.8.0)
+      chronic_duration (= 0.10.6)
       clamp (~> 0.6.5)
-      concurrent-ruby (~> 0.9.1)
+      concurrent-ruby (= 1.0.0)
       filesize (= 0.0.4)
       gems (~> 0.8.3)
       i18n (= 0.6.9)
-      jrjackson (~> 0.3.5)
-      jruby-openssl (>= 0.9.11)
+      jrjackson (~> 0.3.7)
+      jruby-monitoring (~> 0.3.1)
+      jruby-openssl (= 0.9.16)
+      logstash-core-event-java (~> 5.0.0.dev)
       minitar (~> 0.5.4)
       pry (~> 0.10.1)
+      puma (~> 2.16, >= 2.16.0)
+      rubyzip (~> 1.1.7)
+      sinatra (~> 1.4, >= 1.4.6)
       stud (~> 0.0.19)
       thread_safe (~> 0.3.5)
       treetop (< 1.5.0)
 
+PATH
+  remote: ./logstash-core-event-java
+  specs:
+    logstash-core-event-java (5.0.0.dev-java)
+      jar-dependencies
+      ruby-maven (~> 3.3.9)
+
+PATH
+  remote: ./logstash-core-plugin-api
+  specs:
+    logstash-core-plugin-api (2.0.0-java)
+      logstash-core (= 5.0.0.dev)
+
 GEM
   remote: https://rubygems.org/
   specs:
     addressable (2.3.8)
     arr-pm (0.0.10)
       cabin (> 0)
-    backports (3.6.6)
-    benchmark-ips (2.3.0)
+    backports (3.6.8)
+    benchmark-ips (2.6.1)
     builder (3.2.2)
-    cabin (0.7.1)
-    childprocess (0.5.6)
+    cabin (0.8.1)
+    childprocess (0.5.9)
       ffi (~> 1.0, >= 1.0.11)
+    chronic_duration (0.10.6)
+      numerizer (~> 0.1.1)
     ci_reporter (2.0.0)
       builder (>= 2.1.2)
     ci_reporter_rspec (1.0.0)
       ci_reporter (~> 2.0)
       rspec (>= 2.14, < 4)
     clamp (0.6.5)
-    coderay (1.1.0)
-    concurrent-ruby (0.9.1-java)
-    coveralls (0.8.3)
+    coderay (1.1.1)
+    concurrent-ruby (1.0.0-java)
+    coveralls (0.8.13)
       json (~> 1.8)
-      rest-client (>= 1.6.8, < 2)
-      simplecov (~> 0.10.0)
+      simplecov (~> 0.11.0)
       term-ansicolor (~> 1.3)
       thor (~> 0.19.1)
+      tins (~> 1.6.0)
     diff-lcs (1.2.5)
     docile (1.1.5)
-    domain_name (0.5.25)
-      unf (>= 0.0.5, < 1.0.0)
     faraday (0.9.2)
       multipart-post (>= 1.2, < 3)
     ffi (1.9.10-java)
     file-dependencies (0.1.6)
       minitar
     filesize (0.0.4)
+    fivemat (1.3.2)
     flores (0.0.6)
     fpm (1.3.3)
       arr-pm (~> 0.0.9)
@@ -63,41 +216,35 @@ GEM
       json (>= 1.7.7)
     gem_publisher (1.5.0)
     gems (0.8.3)
-    http-cookie (1.0.2)
-      domain_name (~> 0.5)
     i18n (0.6.9)
     insist (1.0.0)
-    jrjackson (0.3.5)
-    jruby-openssl (0.9.12-java)
+    jar-dependencies (0.3.2)
+    jls-grok (0.11.2)
+      cabin (>= 0.6.0)
+    jrjackson (0.3.9-java)
+    jruby-monitoring (0.3.1)
+    jruby-openssl (0.9.16-java)
     json (1.8.3-java)
-    kramdown (1.9.0)
-    logstash-devutils (0.0.18-java)
-      gem_publisher
-      insist (= 1.0.0)
-      kramdown
-      minitar
-      rake
-      rspec (~> 3.1.0)
-      rspec-wait
-      stud (>= 0.0.20)
+    kramdown (1.10.0)
     method_source (0.8.2)
-    mime-types (2.6.2)
     minitar (0.5.4)
     multipart-post (2.0.0)
-    netrc (0.10.3)
+    numerizer (0.1.1)
     octokit (3.8.0)
       sawyer (~> 0.6.0, >= 0.5.3)
     polyglot (0.3.5)
-    pry (0.10.2-java)
+    pry (0.10.3-java)
       coderay (~> 1.1.0)
       method_source (~> 0.8.1)
       slop (~> 3.4)
       spoon (~> 0.0)
-    rake (10.4.2)
-    rest-client (1.8.0)
-      http-cookie (>= 1.0.2, < 2.0)
-      mime-types (>= 1.16, < 3.0)
-      netrc (~> 0.7)
+    puma (2.16.0-java)
+    rack (1.6.4)
+    rack-protection (1.5.3)
+      rack
+    rack-test (0.6.3)
+      rack (>= 1.0)
+    rake (11.1.2)
     rspec (3.1.0)
       rspec-core (~> 3.1.0)
       rspec-expectations (~> 3.1.0)
@@ -110,17 +257,24 @@ GEM
     rspec-mocks (3.1.3)
       rspec-support (~> 3.1.0)
     rspec-support (3.1.2)
-    rspec-wait (0.0.7)
-      rspec (>= 2.11, < 3.4)
+    rspec-wait (0.0.8)
+      rspec (>= 2.11, < 3.5)
+    ruby-maven (3.3.10)
+      ruby-maven-libs (~> 3.3.1)
+    ruby-maven-libs (3.3.3)
     rubyzip (1.1.7)
     sawyer (0.6.0)
       addressable (~> 2.3.5)
       faraday (~> 0.8, < 0.10)
-    simplecov (0.10.0)
+    simplecov (0.11.2)
       docile (~> 1.1.0)
       json (~> 1.8)
       simplecov-html (~> 0.10.0)
     simplecov-html (0.10.0)
+    sinatra (1.4.7)
+      rack (~> 1.5)
+      rack-protection (~> 1.4)
+      tilt (>= 1.3, < 3)
     slop (3.6.0)
     spoon (0.0.4)
       ffi
@@ -129,11 +283,11 @@ GEM
       tins (~> 1.0)
     thor (0.19.1)
     thread_safe (0.3.5-java)
+    tilt (2.0.2)
     tins (1.6.0)
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
-    unf (0.1.4-java)
 
 PLATFORMS
   java
@@ -146,10 +300,27 @@ DEPENDENCIES
   flores (~> 0.0.6)
   fpm (~> 1.3.3)
   gems (~> 0.8.3)
-  logstash-core (= 3.0.0.dev)!
-  logstash-devutils (~> 0.0.15)
+  logstash-codec-json!
+  logstash-codec-json_lines!
+  logstash-codec-line!
+  logstash-codec-plain!
+  logstash-core (= 5.0.0.dev)!
+  logstash-core-event-java (= 5.0.0.dev)!
+  logstash-core-plugin-api (= 2.0.0)!
+  logstash-devutils!
+  logstash-filter-clone!
+  logstash-filter-grok!
+  logstash-filter-multiline!
+  logstash-filter-mutate!
+  logstash-input-generator!
+  logstash-input-stdin!
+  logstash-input-tcp!
+  logstash-output-stdout!
+  logstash-patterns-core!
   octokit (= 3.8.0)
+  rack-test
   rspec (~> 3.1.0)
   rubyzip (~> 1.1.7)
   simplecov
-  stud (~> 0.0.21)
+  stud (~> 0.0.22)
+  tins (= 1.6)
diff --git a/LICENSE b/LICENSE
index 8026afdc77f..43976b73b2b 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,4 +1,4 @@
-Copyright (c) 2012–2015 Elasticsearch <http://www.elastic.co>
+Copyright (c) 2012–2016 Elasticsearch <http://www.elastic.co>
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
diff --git a/README.md b/README.md
index 7c908bfa0bf..e8aeedf903d 100644
--- a/README.md
+++ b/README.md
@@ -2,15 +2,14 @@
 
 ### Build status
 
-| Branch | master   | 2.0 | 1.5
+| Test | master | 5.0 | 2.3 |
 |---|---|---|---|
-| core |[![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/)   | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.0/job/logstash_regression_20/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.0/job/logstash_regression_20/)  | [![Build Status](http://build-eu-00.elastic.co/view/LS%201.5/job/logstash_regression_15/badge/icon)](http://build-eu-00.elastic.co/view/LS%201.5/job/logstash_regression_15/)   |
-| integration | [![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.0/job/Logstash_Default_Plugins_20/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.0/job/Logstash_Default_Plugins_20/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%201.5/job/Logstash_15_Default_Plugins/badge/icon)](http://build-eu-00.elastic.co/view/LS%201.5/job/Logstash_15_Default_Plugins/) |
+| core | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=master)](https://travis-ci.org/elastic/logstash) | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=2.3)](https://travis-ci.org/elastic/logstash) | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=2.3)](https://travis-ci.org/elastic/logstash) |
 
 Logstash is a tool for managing events and logs. You can use it to collect
 logs, parse them, and store them for later use (like, for searching).  If you
-store them in [Elasticsearch](http://www.elastic.co/guide/en/elasticsearch/reference/current/index.html),
-you can view and analyze them with [Kibana](http://www.elastic.co/guide/en/kibana/current/index.html).
+store them in [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html),
+you can view and analyze them with [Kibana](https://www.elastic.co/guide/en/kibana/current/index.html).
 
 It is fully free and fully open source. The license is Apache 2.0, meaning you
 are pretty much free to use it however you want in whatever way.
@@ -25,7 +24,7 @@ repositories under the [logstash-plugins](https://github.com/logstash-plugins) g
 gets published to RubyGems.org. Logstash has added plugin infrastructure to easily maintain the lifecyle of the plugin.
 For more details and rationale behind these changes, see our [blogpost](https://www.elastic.co/blog/plugin-ecosystem-changes/).
 
-[Elasticsearch logstash-contrib repo](https://github.com/elasticsearch/logstash-contrib) is deprecated. We
+[Elasticsearch logstash-contrib repo](https://github.com/elastic/logstash-contrib) is deprecated. We
 have moved all of the plugins that existed there into their own repositories. We are migrating all of the pull requests
 and issues from logstash-contrib to the new repositories.
 
@@ -43,7 +42,7 @@ Logstash core will continue to exist under this repository and all related issue
 
 - [#logstash on freenode IRC](https://webchat.freenode.net/?channels=logstash)
 - [logstash-users on Google Groups](https://groups.google.com/d/forum/logstash-users)
-- [Logstash Documentation](http://www.elastic.co/guide/en/logstash/current/index.html)
+- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)
 - [Logstash Product Information](https://www.elastic.co/products/logstash)
 - [Elastic Support](https://www.elastic.co/subscriptions)
 
@@ -68,6 +67,20 @@ To verify your environment, run `bin/logstash version` which should look like th
     $ bin/logstash version
     logstash 2.0.0.dev
 
+If you are seeing errors that look like
+
+    $ rake bootstrap
+    Installing minitar >= 0 because the build process needs it.
+    [bootstrap] Fetching and installing gem: minitar (>= 0)
+    rake aborted!
+    LoadError: no such file to load -- archive/tar/minitar
+    /Users/<user>/projects/logstash/rakelib/vendor.rake:17:in `untar'
+    /Users/<user>/projects/logstash/rakelib/vendor.rake:86:in `(root)'
+    Tasks: TOP => bootstrap => vendor:all => vendor:jruby
+    (See full trace by running task with --trace)
+
+then you may need to update your version of rubygems. Run `gem -v` to see the version of rubygems installed. Version 2.5.2 or higher should work. To update rubygems run `gem update --system` (you may need to run with `sudo` if you're using your system Ruby environment).
+
 ## Testing
 
 For testing you can use the *test* `rake` tasks and the `bin/rspec` command, see instructions below. Note that the `bin/logstash rspec` command has been replaced by `bin/rspec`.
@@ -87,11 +100,6 @@ For testing you can use the *test* `rake` tasks and the `bin/rspec` command, see
     bin/rspec
     bin/rspec spec/foo/bar_spec.rb
 
----
-Note that if a plugin is installed using the plugin manager `bin/plugin install ...` do not forget to also install the plugins development dependencies using the following command after the plugin installation:
-
-    bin/plugin install --development
-
 ### Plugins tests
 
 To run the tests of all currently installed plugins:
@@ -104,9 +112,9 @@ You can install the default set of plugins included in the logstash package or a
     rake test:install-all
 
 ---
-Note that if a plugin is installed using the plugin manager `bin/plugin install ...` do not forget to also install the plugins development dependencies using the following command after the plugin installation:
+Note that if a plugin is installed using the plugin manager `bin/logstash-plugin install ...` do not forget to also install the plugins development dependencies using the following command after the plugin installation:
 
-    bin/plugin install --development
+    bin/logstash-plugin install --development
 
 ## Developing plugins
 
@@ -119,7 +127,7 @@ The documentation for developing plugins can be found in the plugins README, see
 
 ## Drip Launcher
 
-[Drip](https://github.com/ninjudd/drip) is a tool which help solve the slow JVM startup problem. The drip script is intended to be a drop-in replacement for the java command. We recommend using drip during development, in particular for running tests. Using drip, the first invokation of a command will not be faster but the subsequent commands will be swift.
+[Drip](https://github.com/ninjudd/drip) is a tool that solves the slow JVM startup problem. The drip script is intended to be a drop-in replacement for the java command. We recommend using drip during development, in particular for running tests. Using drip, the first invocation of a command will not be faster but the subsequent commands will be swift.
 
 To tell logstash to use drip, either set the `USE_DRIP=1` environment variable or set `` JAVACMD=`which drip` ``.
 
@@ -130,7 +138,7 @@ Examples:
 
 **Caveats**
 
-Drip does not work with STDIN. You cannot use drip for running configs which uses the stdin plugin.
+Drip does not work with STDIN. You cannot use drip for running configs which use the stdin plugin.
 
 
 ## Building
@@ -163,4 +171,4 @@ see that here.
 It is more important to me that you are able to contribute.
 
 For more information about contributing, see the
-[CONTRIBUTING](CONTRIBUTING.md) file.
+[CONTRIBUTING](.github/CONTRIBUTING.md) file.
diff --git a/ROADMAP.md b/ROADMAP.md
new file mode 100644
index 00000000000..9583b301af1
--- /dev/null
+++ b/ROADMAP.md
@@ -0,0 +1,15 @@
+## Logstash Roadmap
+
+[Logstash](https://www.elastic.co/products/logstash "Logstash") is an open-source data collection engine, developed directly on Github and distributed under the Apache License 2.0. The roadmap is defined by the core themes of performance, resiliency, and manageability, along with the overall plugin ecosystem. User requirements are the main driving force behind our development efforts. If a user has a bad time, it’s a bug!  All submitted [issues](https://github.com/elastic/logstash/issues/new "New Issue"), suggestions, or ideas are greatly encouraged and appreciated.
+
+### Logstash Core
+
+With an open development model, the core roadmap can generally be distilled in Github. Long term roadmap features can be viewed with the "[roadmap](https://github.com/elastic/logstash/labels/roadmap "Roadmap Features")" label. The features and bug fixes targeted for an upcoming release can be viewed with the specific release label, e.g. "[v5.0.0](https://github.com/elastic/logstash/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3Av5.0.0 "Logstash 5.0 Release")". Please note that feature timing and priorities are susceptible to change and therefore not guaranteed.
+
+### Logstash Plugins
+
+The [Logstash Plugins](https://github.com/logstash-plugins "Logstash Plugins") ecosystem enables the innovation of additional integrations and processing capabilities for the core engine. Plugins in this organization are either maintained by Elastic or the community. Community maintained plugins have a special banner in the documentation page, e.g. [Salesforce input](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-salesforce.html "Salesforce Input Plugin"). Many awesome humans have taken the roadmap into their own hands by becoming community maintainers. If you’re actively working with specific community plugins and would like to get more involved, feel free to reach out in this [forum thread](https://discuss.elastic.co/t/logstash-plugins-community-maintainers/35953 "Community Maintainers").
+
+## Latest Changes
+
+For a list of latest changes across Logstash and its plugins, see the [release notes](https://www.elastic.co/guide/en/logstash/current/releasenotes.html "Current Release Notes").
diff --git a/Rakefile b/Rakefile
index cf702f08752..12b4a262110 100644
--- a/Rakefile
+++ b/Rakefile
@@ -1,6 +1,7 @@
 # encoding: utf-8
 
 $: << File.join(File.dirname(__FILE__), "lib")
+$: << File.join(File.dirname(__FILE__), "logstash-core/lib")
 
 task "default" => "help"
 
diff --git a/STYLE.md b/STYLE.md
index 894a31e88f3..5bc54a31766 100644
--- a/STYLE.md
+++ b/STYLE.md
@@ -27,6 +27,7 @@ Do this:
 * parentheses on function definitions/calls
 * explicit is better than implicit
   * implicit returns are forbidden except in the case of a single expression 
+* Avoid use of 'and' and 'or' in ruby code 
 
 The point is consistency and documentation. If you see inconsistencies, let me
 know, and I'll fix them :)
diff --git a/acceptance_spec/acceptance/install_spec.rb b/acceptance_spec/acceptance/install_spec.rb
deleted file mode 100644
index 45efc1bf6fb..00000000000
--- a/acceptance_spec/acceptance/install_spec.rb
+++ /dev/null
@@ -1,95 +0,0 @@
-require_relative '../spec_helper_acceptance'
-
-branch = ENV['LS_BRANCH'] || 'master'
-build_url = 'https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash'
-
-describe "Logstash class:" do
-
-  case fact('osfamily')
-  when 'RedHat'
-    core_package_name    = 'logstash'
-    service_name         = 'logstash'
-    core_url             = "#{build_url}/#{branch}/nightly/JDK7/logstash-latest-SNAPSHOT.rpm"
-    pid_file             = '/var/run/logstash.pid'
-  when 'Debian'
-    core_package_name    = 'logstash'
-    service_name         = 'logstash'
-    core_url             = "#{build_url}/#{branch}/nightly/JDK7/logstash-latest-SNAPSHOT.deb"
-    pid_file             = '/var/run/logstash.pid'
-  end
-
-  context "Install Nightly core package" do
-
-    it 'should run successfully' do
-      pp = "class { 'logstash': package_url => '#{core_url}', java_install => true }
-            logstash::configfile { 'basic_config': content => 'input { tcp { port => 2000 } } output { stdout { } } ' }
-           "
-
-      # Run it twice and test for idempotency
-      apply_manifest(pp, :catch_failures => true)
-      sleep 20
-      expect(apply_manifest(pp, :catch_failures => true).exit_code).to be_zero
-
-    end
-
-    describe package(core_package_name) do
-      it { should be_installed }
-    end
-
-    describe service(service_name) do
-      it { should be_enabled }
-      it { should be_running }
-    end
-
-    describe file(pid_file) do
-      it { should be_file }
-      its(:content) { should match /[0-9]+/ }
-    end
-
-    describe port(2000) do
-      it {
-        sleep 30
-        should be_listening
-      }
-    end
-
-  end
-
-  context "ensure we are still running" do
-
-    describe service(service_name) do
-      it {
-        sleep 30
-        should be_running
-      }
-    end
-
-    describe port(2000) do
-      it { should be_listening }
-    end
-
-  end
-
-  describe "module removal" do
-
-    it 'should run successfully' do
-      pp = "class { 'logstash': ensure => 'absent' }"
-
-      # Run it twice and test for idempotency
-      apply_manifest(pp, :catch_failures => true)
-
-    end
-
-    describe service(service_name) do
-      it { should_not be_enabled }
-      it { should_not be_running }
-    end
-
-    describe package(core_package_name) do
-      it { should_not be_installed }
-    end
-
-  end
-
-end
-
diff --git a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml b/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
deleted file mode 100644
index 4f33d28f788..00000000000
--- a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  centos-6-x64:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: el-6-x86_64
-    image: electrical/centos:6.4
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'yum install -y wget ntpdate rubygems ruby-augeas ruby-devel augeas-devel logrotate'
-      - 'touch /etc/sysconfig/network'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml b/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
deleted file mode 100644
index 7b2df7423e5..00000000000
--- a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  debian-6:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: debian-6-amd64
-    image: electrical/debian:6.0.8
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
-      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml b/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
deleted file mode 100644
index 704b6c7d424..00000000000
--- a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  debian-7:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: debian-7-amd64
-    image: electrical/debian:7.3
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
-      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
deleted file mode 100644
index 4d6879e74b0..00000000000
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-HOSTS:
-  ubuntu-12-04:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: ubuntu-12.04-amd64
-    image: electrical/ubuntu:12.04
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl logrotate'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
deleted file mode 100644
index 0f6e1772f29..00000000000
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-HOSTS:
-  ubuntu-14-04:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: ubuntu-14.04-amd64
-    image: electrical/ubuntu:14.04
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq ruby ruby1.9.1-dev libaugeas-dev libaugeas-ruby lsb-release wget net-tools curl logrotate'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/spec_helper_acceptance.rb b/acceptance_spec/spec_helper_acceptance.rb
deleted file mode 100644
index 752743b4aa5..00000000000
--- a/acceptance_spec/spec_helper_acceptance.rb
+++ /dev/null
@@ -1,70 +0,0 @@
-require 'beaker-rspec'
-require 'pry'
-require 'securerandom'
-
-files_dir = ENV['files_dir'] || '/home/jenkins/puppet'
-
-proxy_host = ENV['BEAKER_PACKAGE_PROXY'] || ''
-
-if !proxy_host.empty?
-  gem_proxy = "http_proxy=#{proxy_host}" unless proxy_host.empty?
-
-  hosts.each do |host|
-    on host, "echo 'export http_proxy='#{proxy_host}'' >> /root/.bashrc"
-    on host, "echo 'export https_proxy='#{proxy_host}'' >> /root/.bashrc"
-    on host, "echo 'export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,#{host.name}\"' >> /root/.bashrc"
-  end
-else
-  gem_proxy = ''
-end
-
-hosts.each do |host|
-  # Install Puppet
-  if host.is_pe?
-    install_pe
-  else
-    puppetversion = ENV['VM_PUPPET_VERSION']
-    on host, "#{gem_proxy} gem install puppet --no-ri --no-rdoc --version '~> #{puppetversion}'"
-    on host, "mkdir -p #{host['distmoduledir']}"
-
-    if fact('osfamily') == 'Suse'
-      install_package host, 'rubygems ruby-devel augeas-devel libxml2-devel'
-      on host, "#{gem_proxy} gem install ruby-augeas --no-ri --no-rdoc"
-    end
-
-  end
-
-  # on debian/ubuntu nodes ensure we get the latest info
-  # Can happen we have stalled data in the images
-  if fact('osfamily') == 'Debian'
-    on host, "apt-get update"
-  end
-
-end
-
-RSpec.configure do |c|
-  # Project root
-  proj_root = File.expand_path(File.join(File.dirname(__FILE__), '..'))
-
-  # Readable test descriptions
-  c.formatter = :documentation
-
-  # Configure all nodes in nodeset
-  c.before :suite do
-    # Install module and dependencies
-
-    hosts.each do |host|
-
-      on host, puppet('module','install','elasticsearch-logstash'), { :acceptable_exit_codes => [0,1] }
-
-      if fact('osfamily') == 'Debian'
-        scp_to(host, "#{files_dir}/puppetlabs-apt-1.4.2.tar.gz", '/tmp/puppetlabs-apt-1.4.2.tar.gz')
-        on host, puppet('module','install','/tmp/puppetlabs-apt-1.4.2.tar.gz'), { :acceptable_exit_codes => [0,1] }
-      end
-      if fact('osfamily') == 'Suse'
-        on host, puppet('module','install','darin-zypprepo'), { :acceptable_exit_codes => [0,1] }
-      end
-
-    end
-  end
-end
diff --git a/bin/logstash b/bin/logstash
index 02e4446009c..7ddc8bb568f 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -9,7 +9,7 @@
 # See 'bin/logstash help' for a list of commands.
 #
 # Supported environment variables:
-#   LS_HEAP_SIZE="xxx" size for the -Xmx${LS_HEAP_SIZE} maximum Java heap size option, default is "500m"
+#   LS_JVM_OPTS="xxx" path to file with JVM options
 #   LS_JAVA_OPTS="xxx" to append extra options to the defaults JAVA_OPTS provided by logstash
 #   JAVA_OPTS="xxx" to *completely override* the defauls set of JAVA_OPTS provided by logstash
 #
@@ -19,14 +19,31 @@
 #   DEBUG=1 to output debugging information
 
 unset CDPATH
-. "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+# This unwieldy bit of scripting is to try to catch instances where Logstash
+# was launched from a symlink, rather than a full path to the Logstash binary
+if [ -L $0 ]; then
+  # Launched from a symlink
+  # --Test for the readlink binary
+  RL=$(which readlink)
+  if [ $? -eq 0 ]; then
+    # readlink exists
+    SOURCEPATH=$($RL $0)
+  else
+    # readlink not found, attempt to parse the output of stat
+    SOURCEPATH=$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\‘//' -e 's/\’//')
+    if [ $? -ne 0 ]; then
+      # Failed to execute or parse stat
+      echo "Failed to find source library at path $(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+      echo "You may need to launch Logstash with a full path instead of a symlink."
+      exit 1
+    fi
+  fi
+else
+  # Not a symlink
+  SOURCEPATH=$0
+fi
+
+. "$(cd `dirname $SOURCEPATH`/..; pwd)/bin/logstash.lib.sh"
 setup
 
-case $1 in
-  -*)
-    ruby_exec "${LOGSTASH_HOME}/lib/bootstrap/environment.rb" "logstash/runner.rb" "agent" "$@"
-    ;;
-  *)
-    ruby_exec "${LOGSTASH_HOME}/lib/bootstrap/environment.rb" "logstash/runner.rb" "$@"
-    ;;
-esac
+ruby_exec "${LOGSTASH_HOME}/lib/bootstrap/environment.rb" "logstash/runner.rb" "$@"
diff --git a/bin/plugin b/bin/logstash-plugin
similarity index 75%
rename from bin/plugin
rename to bin/logstash-plugin
index 39b19b8df30..b357c749a6f 100755
--- a/bin/plugin
+++ b/bin/logstash-plugin
@@ -4,7 +4,7 @@ unset CDPATH
 . "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
 setup
 
-# bin/plugin is a short lived ruby script thus we can use aggressive "faster starting JRuby options"
+# bin/logstash-plugin is a short lived ruby script thus we can use aggressive "faster starting JRuby options"
 # see https://github.com/jruby/jruby/wiki/Improving-startup-time
 export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify -X-C -Xcompile.invokedynamic=false"
 
diff --git a/bin/plugin.bat b/bin/logstash-plugin.bat
similarity index 100%
rename from bin/plugin.bat
rename to bin/logstash-plugin.bat
diff --git a/bin/logstash.bat b/bin/logstash.bat
index 4d8a8db646e..b773bc6c52d 100644
--- a/bin/logstash.bat
+++ b/bin/logstash.bat
@@ -11,9 +11,9 @@ set first_arg=%1
 setlocal EnableDelayedExpansion
 if "!first_arg:~0,1!" equ "-" (
   if "%VENDORED_JRUBY%" == "" (
-    %RUBYCMD% "%LS_HOME%\lib\bootstrap\environment.rb" "logstash\runner.rb" agent %*
+    %RUBYCMD% "%LS_HOME%\lib\bootstrap\environment.rb" "logstash\runner.rb" %*
   ) else (
-    %JRUBY_BIN% %jruby_opts% "%LS_HOME%\lib\bootstrap\environment.rb" "logstash\runner.rb" agent %*
+    %JRUBY_BIN% %jruby_opts% "%LS_HOME%\lib\bootstrap\environment.rb" "logstash\runner.rb" %*
   )
 ) else (
   if "%VENDORED_JRUBY%" == "" (
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index a5d6ccee3ec..4e06913f35f 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -1,9 +1,38 @@
 unset CDPATH
-LOGSTASH_HOME=$(cd `dirname $0`/..; pwd)
+# This unwieldy bit of scripting is to try to catch instances where Logstash
+# was launched from a symlink, rather than a full path to the Logstash binary
+if [ -L $0 ]; then
+  # Launched from a symlink
+  # --Test for the readlink binary
+  RL=$(which readlink)
+  if [ $? -eq 0 ]; then
+    # readlink exists
+    SOURCEPATH=$($RL $0)
+  else
+    # readlink not found, attempt to parse the output of stat
+    SOURCEPATH=$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\‘//' -e 's/\’//')
+    if [ $? -ne 0 ]; then
+      # Failed to execute or parse stat
+      echo "Failed to set LOGSTASH_HOME from $(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+      echo "You may need to launch Logstash with a full path instead of a symlink."
+      exit 1
+    fi
+  fi
+else
+  # Not a symlink
+  SOURCEPATH=$0
+fi
+
+LOGSTASH_HOME=$(cd `dirname $SOURCEPATH`/..; pwd)
 export LOGSTASH_HOME
+SINCEDB_DIR=${LOGSTASH_HOME}
+export SINCEDB_DIR
 
-# Defaults you can override with environment variables
-LS_HEAP_SIZE="${LS_HEAP_SIZE:=1g}"
+parse_jvm_options() {
+  if [ -f "$1" ]; then
+    echo "$(grep "^-" "$1" | tr '\n' ' ')"
+  fi
+}
 
 setup_java() {
   if [ -z "$JAVACMD" ] ; then
@@ -26,37 +55,34 @@ setup_java() {
 
   if [ "$JAVA_OPTS" ] ; then
     echo "WARNING: Default JAVA_OPTS will be overridden by the JAVA_OPTS defined in the environment. Environment JAVA_OPTS are $JAVA_OPTS"  1>&2
-  else
-    # There are no JAVA_OPTS set from the client, we set a predefined
-    # set of options that think are good in general
-    JAVA_OPTS="-XX:+UseParNewGC"
-    JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
-    JAVA_OPTS="$JAVA_OPTS -Djava.awt.headless=true"
+  fi
 
-    JAVA_OPTS="$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75"
-    JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly"
+  # Set a default GC log file for use by jvm.options _before_ it's called.
+  if [ -z "$LS_GC_LOG_FILE" ] ; then
+    LS_GC_LOG_FILE="./logstash-gc.log"
   fi
 
+  # Set the initial JVM options from the jvm.options file.  Look in
+  # /etc/logstash first, and break if that file is found readable there.
+  if [ -z "$LS_JVM_OPTS" ]; then
+      for jvm_options in /etc/logstash/jvm.options \
+                        "$LOGSTASH_HOME"/config/jvm.options;
+                         do
+          if [ -r "$jvm_options" ]; then
+              LS_JVM_OPTS=$jvm_options
+              break
+          fi
+      done
+  fi
+  # use the defaults, first, then override with anything provided
+  LS_JAVA_OPTS="$(parse_jvm_options "$LS_JVM_OPTS") $LS_JAVA_OPTS"
+
   if [ "$LS_JAVA_OPTS" ] ; then
     # The client set the variable LS_JAVA_OPTS, choosing his own
     # set of java opts.
     JAVA_OPTS="$JAVA_OPTS $LS_JAVA_OPTS"
   fi
 
-  if [ "$LS_HEAP_SIZE" ] ; then
-    JAVA_OPTS="$JAVA_OPTS -Xmx${LS_HEAP_SIZE}"
-  fi
-
-  if [ "$LS_USE_GC_LOGGING" ] ; then
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCDetails"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCTimeStamps"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintClassHistogram"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintTenuringDistribution"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCApplicationStoppedTime"
-    JAVA_OPTS="$JAVA_OPTS -Xloggc:./logstash-gc.log"
-    echo "Writing garbage collection logs to ./logstash-gc.log"
-  fi
-
   export JAVACMD
   export JAVA_OPTS
 }
@@ -108,7 +134,9 @@ setup_ruby() {
 jruby_opts() {
   printf "%s" "--1.9"
   for i in $JAVA_OPTS ; do
-    printf "%s" " -J$i"
+    if [ -z "$i" ]; then
+      printf "%s" " -J$i"
+    fi
   done
 }
 
diff --git a/bin/setup.bat b/bin/setup.bat
index 557df31e1a9..f2ac30383f8 100644
--- a/bin/setup.bat
+++ b/bin/setup.bat
@@ -15,40 +15,58 @@ goto finally
 
 :setup_jruby
 REM setup_java()
+if not defined JAVA_HOME IF EXIST %ProgramData%\Oracle\java\javapath\java.exe (
+    for /f "tokens=2 delims=[]" %%a in ('dir %ProgramData%\Oracle\java\javapath\java.exe') do @set JAVA_EXE=%%a
+)
+if defined JAVA_EXE set JAVA_HOME=%JAVA_EXE:\bin\java.exe=%
+if defined JAVA_EXE echo Using JAVA_HOME=%JAVA_HOME% retrieved from %ProgramData%\Oracle\java\javapath\java.exe
+
 if not defined JAVA_HOME goto missing_java_home
 REM ***** JAVA options *****
 
 if "%LS_HEAP_SIZE%" == "" (
-set LS_HEAP_SIZE=1g
+    set LS_HEAP_SIZE=1g
+)
+
+IF NOT "%JAVA_OPTS%" == "" (
+    ECHO JAVA_OPTS was set to [%JAVA_OPTS%]. Logstash will trust these options, and not set any defaults that it might usually set
+    goto opts_defined
 )
 
-set JAVA_OPTS=%JAVA_OPTS% -Xmx%LS_HEAP_SIZE%
-
-REM Enable aggressive optimizations in the JVM
-REM    - Disabled by default as it might cause the JVM to crash
-REM set JAVA_OPTS=%JAVA_OPTS% -XX:+AggressiveOpts
-
-set JAVA_OPTS=%JAVA_OPTS% -XX:+UseParNewGC
-set JAVA_OPTS=%JAVA_OPTS% -XX:+UseConcMarkSweepGC
-set JAVA_OPTS=%JAVA_OPTS% -XX:+CMSParallelRemarkEnabled
-set JAVA_OPTS=%JAVA_OPTS% -XX:SurvivorRatio=8
-set JAVA_OPTS=%JAVA_OPTS% -XX:MaxTenuringThreshold=1
-set JAVA_OPTS=%JAVA_OPTS% -XX:CMSInitiatingOccupancyFraction=75
-set JAVA_OPTS=%JAVA_OPTS% -XX:+UseCMSInitiatingOccupancyOnly
-
-REM GC logging options -- uncomment to enable
-REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCDetails
-REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCTimeStamps
-REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintClassHistogram
-REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintTenuringDistribution
-REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCApplicationStoppedTime
-REM JAVA_OPTS=%JAVA_OPTS% -Xloggc:/var/log/logstash/gc.log
-
-REM Causes the JVM to dump its heap on OutOfMemory.
-set JAVA_OPTS=%JAVA_OPTS% -XX:+HeapDumpOnOutOfMemoryError
-REM The path to the heap dump location, note directory must exists and have enough
-REM space for a full heap dump.
-REM JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="$LS_HOME/logs/heapdump.hprof"
+    SET JAVA_OPTS=%JAVA_OPTS% -Xmx%LS_HEAP_SIZE%
+
+    REM Enable aggressive optimizations in the JVM
+    REM    - Disabled by default as it might cause the JVM to crash
+    REM set JAVA_OPTS=%JAVA_OPTS% -XX:+AggressiveOpts
+
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:+UseParNewGC
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:+UseConcMarkSweepGC
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:+CMSParallelRemarkEnabled
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:SurvivorRatio=8
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:MaxTenuringThreshold=1
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:CMSInitiatingOccupancyFraction=75
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:+UseCMSInitiatingOccupancyOnly
+
+    REM GC logging options -- uncomment to enable
+    REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCDetails
+    REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCTimeStamps
+    REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintClassHistogram
+    REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintTenuringDistribution
+    REM JAVA_OPTS=%JAVA_OPTS% -XX:+PrintGCApplicationStoppedTime
+    REM JAVA_OPTS=%JAVA_OPTS% -Xloggc:/var/log/logstash/gc.log
+
+    REM Causes the JVM to dump its heap on OutOfMemory.
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:+HeapDumpOnOutOfMemoryError
+    REM The path to the heap dump location, note directory must exists and have enough
+    REM space for a full heap dump.
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="$LS_HOME/heapdump.hprof"
+:opts_defined
+
+
+IF NOT "%LS_JAVA_OPTS%" == "" (
+    ECHO LS_JAVA_OPTS was set to [%LS_JAVA_OPTS%]. This will be appended to the JAVA_OPTS [%JAVA_OPTS%]
+    SET JAVA_OPTS=%JAVA_OPTS% %LS_JAVA_OPTS%
+)
 
 REM setup_vendored_jruby()
 set JRUBY_BIN="%LS_HOME%\vendor\jruby\bin\jruby"
diff --git a/bin/system-install b/bin/system-install
new file mode 100755
index 00000000000..5790a5a41e8
--- /dev/null
+++ b/bin/system-install
@@ -0,0 +1,41 @@
+#!/bin/bash
+
+unset CDPATH
+. "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+setup
+
+if [ -z "$1" ]; then
+  [ -r ${LOGSTASH_HOME}/config/startup.options ] && . ${LOGSTASH_HOME}/config/startup.options
+  [ -r /etc/logstash/startup.options ] && . /etc/logstash/startup.options
+else
+  if [ -r $1 ]; then
+    echo "Using provided startup.options file: ${1}"
+    . $1
+  else
+    echo "$1 is not a file path"
+  fi
+fi
+
+# bin/logstash-plugin is a short lived ruby script thus we can use aggressive "faster starting JRuby options"
+# see https://github.com/jruby/jruby/wiki/Improving-startup-time
+export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify -X-C -Xcompile.invokedynamic=false"
+
+tempfile=$(mktemp)
+if [ "x${PRESTART}" == "x" ]; then
+  opts=("--log" "$tempfile" "--overwrite" "--install" "--name" "${SERVICE_NAME}" "--user" "${LS_USER}" "--group" "${LS_GROUP}" "--description" "${SERVICE_DESCRIPTION}" "--nice" "${LS_NICE}" "--limit-open-files" "${LS_OPEN_FILES}")
+else
+  opts=("--log" "$tempfile" "--overwrite" "--install" "--name" "${SERVICE_NAME}" "--user" "${LS_USER}" "--group" "${LS_GROUP}" "--description" "${SERVICE_DESCRIPTION}" "--nice" "${LS_NICE}" "--limit-open-files" "${LS_OPEN_FILES}" "--prestart" "${PRESTART}")
+fi
+
+program="$(cd `dirname $0`/..; pwd)/bin/logstash"
+
+$(ruby_exec "${LOGSTASH_HOME}/lib/systeminstall/pleasewrap.rb" "${opts[@]}" ${program} ${LS_OPTS})
+exit_code=$?
+
+if [ $exit_code -ne 0 ]; then
+  cat $tempfile
+  echo "Unable to install system startup script for Logstash."
+else
+  echo "Successfully created system startup script for Logstash"
+fi
+rm $tempfile
diff --git a/bot/check_pull_changelog.rb b/bot/check_pull_changelog.rb
deleted file mode 100644
index 7e8ac7e1f21..00000000000
--- a/bot/check_pull_changelog.rb
+++ /dev/null
@@ -1,89 +0,0 @@
-require "octokit"
-##
-# This script will validate that any pull request submitted against a github 
-# repository will contains changes to CHANGELOG file.
-#
-# If not the case, an helpful text will be commented on the pull request
-# If ok, a thanksful message will be commented also containing a @mention to 
-# acts as a trigger for review notification by a human.
-## 
-
-
-@bot="" # Put here your bot github username
-@password="" # Put here your bot github password
-
-@repository="logstash/logstash"
-@mention="@jordansissel"
-
-@missing_changelog_message = <<MISSING_CHANGELOG
-Hello, I'm #{@bot}, I'm here to help you accomplish your pull request submission quest
-
-You still need to accomplish these tasks:
-
-* Please add a changelog information
-
-Also note that your pull request name will appears in the details section 
-of the release notes, so please make it clear
-MISSING_CHANGELOG
-
-@ok_changelog_message = <<OK_CHANGELOG
-You successfully completed the pre-requisite quest (aka updating CHANGELOG)
-
-Also note that your pull request name will appears in the details section 
-of the release notes, so please make it clear, if not already done.
-
-#{@mention} Dear master, would you please have a look to this humble request
-OK_CHANGELOG
-
-#Connect to Github
-@client=Octokit::Client.new(:login => @bot, :password => @password)
-
-
-#For each open pull
-Octokit.pull_requests(@repository).each do |pull|
-  #Get botComment
-  botComment = nil
-  @client.issue_comments(@repository, pull.number, {
-    :sort => "created",
-    :direction => "desc"
-  }).each do |comment|
-    if comment.user.login == @bot
-      botComment = comment
-      break
-    end
-  end
-
-  if !botComment.nil? and botComment.body.start_with?("[BOT-OK]")
-    #Pull already validated by bot, nothing to do
-    puts "Pull request #{pull.number}, already ok for bot"
-  else
-    #Firt encounter, or previous [BOT-WARN] status
-    #Check for changelog
-    warnOnMissingChangeLog = true
-    @client.pull_request_files(@repository, pull.number).each do |changedFile|
-      if changedFile.filename  == "CHANGELOG"
-        if changedFile.additions.to_i > 0
-          #Changelog looks good
-          warnOnMissingChangeLog = false
-        else
-          #No additions, means crazy deletion
-          warnOnMissingChangeLog = true
-        end
-      end
-    end
-    if warnOnMissingChangeLog
-      if botComment.nil?
-        puts "Pull request #{pull.number}, adding bot warning"
-        @client.add_comment(@repository, pull.number, "[BOT-WARN] #{@missing_changelog_message}")
-      else
-        puts "Pull request #{pull.number}, already warned, no changes yet"
-      end
-    else
-      if !botComment.nil?
-        @client.delete_comment(@repository,botComment.id)
-      end
-      puts "Pull request #{pull.number}, adding bot ok"
-      @client.add_comment(@repository, pull.number, "[BOT-OK] #{@ok_changelog_message}")
-    end
-  end
-end
diff --git a/ci/ci_acceptance.sh b/ci/ci_acceptance.sh
new file mode 100755
index 00000000000..5665fe5cc4a
--- /dev/null
+++ b/ci/ci_acceptance.sh
@@ -0,0 +1,49 @@
+#!/usr/bin/env bash
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+SELECTED_TEST_SUITE=$1
+
+if [[ $SELECTED_TEST_SUITE == $"redhat" ]]; then
+  echo "Generating the RPM, make sure you start with a clean environment before generating other packages."
+  rake artifact:rpm
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["redhat"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:redhat
+  bundle exec rake qa:vm:halt["redhat"]
+elif [[ $SELECTED_TEST_SUITE == $"debian" ]]; then
+  echo "Generating the DEB, make sure you start with a clean environment before generating other packages."
+  rake artifact:deb
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["debian"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:debian
+  bundle exec rake qa:vm:halt["debian"]
+elif [[ $SELECTED_TEST_SUITE == $"all" ]]; then
+  echo "Building Logstash artifacts"
+  rake artifact:all
+
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:all
+  bundle exec rake qa:vm:halt
+  cd ..
+fi
diff --git a/ci/ci_integration.sh b/ci/ci_integration.sh
new file mode 100755
index 00000000000..5aaeca0006f
--- /dev/null
+++ b/ci/ci_integration.sh
@@ -0,0 +1,10 @@
+#!/bin/sh
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+rake test:install-default
+rake test:integration
diff --git a/ci/ci_setup.sh b/ci/ci_setup.sh
index fea695cb2c5..887225c96cb 100755
--- a/ci/ci_setup.sh
+++ b/ci/ci_setup.sh
@@ -1,4 +1,5 @@
 #!/usr/bin/env bash
+set -e
 
 ##
 # Note this setup needs a system ruby to be available, this can not
@@ -13,6 +14,11 @@ rm -rf vendor       # make sure there are no vendorized dependencies
 rm -rf .bundle
 rm -rf spec/reports # no stale spec reports from previous executions
 
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
 # Setup the environment
 rake bootstrap # Bootstrap your logstash instance
 
diff --git a/ci/ci_test.sh b/ci/ci_test.sh
index c0eadda6424..a7f62d151bb 100755
--- a/ci/ci_test.sh
+++ b/ci/ci_test.sh
@@ -1,10 +1,16 @@
 #!/usr/bin/env bash
+set -e
 
 ##
 # Keep in mind to run ci/ci_setup.sh if you need to setup/clean up your environment before
 # running the test suites here.
 ##
 
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
 SELECTED_TEST_SUITE=$1
 
 if [[ $SELECTED_TEST_SUITE == $"core-fail-fast" ]]; then
diff --git a/config/jvm.options b/config/jvm.options
new file mode 100644
index 00000000000..68abc1ad17e
--- /dev/null
+++ b/config/jvm.options
@@ -0,0 +1,74 @@
+## JVM configuration
+
+# Xms represents the initial size of total heap space
+# Xmx represents the maximum size of total heap space
+
+-Xms256m
+-Xmx1g
+
+################################################################
+## Expert settings
+################################################################
+##
+## All settings below this section are considered
+## expert settings. Don't tamper with them unless
+## you understand what you are doing
+##
+################################################################
+
+## GC configuration
+-XX:+UseParNewGC
+-XX:+UseConcMarkSweepGC
+-XX:CMSInitiatingOccupancyFraction=75
+-XX:+UseCMSInitiatingOccupancyOnly
+
+## optimizations
+
+# disable calls to System#gc
+-XX:+DisableExplicitGC
+
+## Locale
+# Set the locale language
+#-Duser.language=en
+
+# Set the locale country
+#-Duser.country=US
+
+# Set the locale variant, if any
+#-Duser.variant=
+
+## basic
+
+# set the I/O temp directory
+#-Djava.io.tmpdir=$HOME
+
+# set to headless, just in case
+-Djava.awt.headless=true
+
+# ensure UTF-8 encoding by default (e.g. filenames)
+-Dfile.encoding=UTF-8
+
+# use our provided JNA always versus the system one
+#-Djna.nosys=true
+
+## heap dumps
+
+# generate a heap dump when an allocation from the Java heap fails
+# heap dumps are created in the working directory of the JVM
+-XX:+HeapDumpOnOutOfMemoryError
+
+# specify an alternative path for heap dumps
+# ensure the directory exists and has sufficient space
+#-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof
+
+## GC logging
+#-XX:+PrintGCDetails
+#-XX:+PrintGCTimeStamps
+#-XX:+PrintGCDateStamps
+#-XX:+PrintClassHistogram
+#-XX:+PrintTenuringDistribution
+#-XX:+PrintGCApplicationStoppedTime
+
+# log GC status to a file with time stamps
+# ensure the directory exists
+#-Xloggc:${LS_GC_LOG_FILE}
diff --git a/config/logstash.yml b/config/logstash.yml
new file mode 100644
index 00000000000..99cad344dfa
--- /dev/null
+++ b/config/logstash.yml
@@ -0,0 +1,106 @@
+# Settings file in YAML
+#
+# Settings can be specified either in hierarchical form, e.g.:
+#
+#   pipeline:
+#     batch:
+#       size: 125
+#       delay: 5
+#
+# Or as flat keys:
+#
+#   pipeline.batch.size: 125
+#   pipeline.batch.delay: 5
+#
+# ------------  Node identity ------------
+#
+# Use a descriptive name for the node:
+#
+# node.name: test
+#
+# If omitted the node name will default to the machine's host name
+#
+# ------------ Pipeline Settings --------------
+#
+# Set the number of workers that will, in parallel, execute the filters+outputs
+# stage of the pipeline.
+#
+# This defaults to the number of the host's CPU cores.
+#
+# pipeline.workers: 2
+#
+# How many workers should be used per output plugin instance
+#
+# pipeline.output.workers: 1
+#
+# How many events to retrieve from inputs before sending to filters+workers
+#
+# pipeline.batch.size: 125
+#
+# How long to wait before dispatching an undersized batch to filters+workers
+# Value is in seconds.
+#
+# pipeline.batch.delay: 5
+#
+# Force Logstash to exit during shutdown even if there are still inflight
+# events in memory. By default, logstash will refuse to quit until all
+# received events have been pushed to the outputs.
+#
+# WARNING: enabling this can lead to data loss during shutdown
+#
+# pipeline.unsafe_shutdown: false
+#
+# ------------ Pipeline Configuration Settings --------------
+#
+# Where to fetch the pipeline configuration for the main pipeline
+#
+# path.config:
+#
+# Pipeline configuration string for the main pipeline
+#
+# config.string:
+#
+# At startup, test if the configuration is valid and exit (dry run)
+#
+# config.test_and_exit: false
+#
+# Periodically check if the configuration has changed and reload the pipeline
+# This can also be triggered manually through the SIGHUP signal
+#
+# config.reload.automatic: false
+#
+# How often to check if the pipeline configuration has changed (in seconds)
+#
+# config.reload.interval: 3
+#
+# Show fully compiled configuration as debug log message
+# NOTE: --log.level must be 'debug'
+#
+# config.debug: false
+#
+# ------------ Metrics Settings --------------
+#
+# Bind address for the metrics REST endpoint
+#
+# http.host: "127.0.0.1"
+#
+# Bind port for the metrics REST endpoint
+#
+# http.port: 9600
+#
+# ------------ Debugging Settings --------------
+#
+# Options for log.level:
+#   * warn => warn (default)
+#   * quiet => error
+#   * verbose => info
+#   * debug => debug
+#
+# log.level: warn
+# log.format: plain (or 'json')
+# path.log:
+#
+# ------------ Other Settings --------------
+#
+# Where to find custom plugins
+# path.plugins: []
diff --git a/config/startup.options b/config/startup.options
new file mode 100644
index 00000000000..9d35f798dcd
--- /dev/null
+++ b/config/startup.options
@@ -0,0 +1,52 @@
+################################################################################
+# These settings are ONLY used by $LS_HOME/bin/system-install to create a custom
+# startup script for Logstash.  It should automagically use the init system
+# (systemd, upstart, sysv, etc.) that your Linux distribution uses.
+#
+# After changing anything here, you need to re-run $LS_HOME/bin/system-install
+# as root to push the changes to the init script.
+################################################################################
+
+# Override Java location
+JAVACMD=/usr/bin/java
+
+# Set a home directory
+LS_HOME=/usr/share/logstash
+
+# logstash settings directory, the path which contains logstash.yml
+LS_SETTINGS_DIR="${LS_HOME}/config"
+
+# Arguments to pass to logstash
+LS_OPTS="--path.settings ${LS_SETTINGS_DIR}"
+
+# Arguments to pass to java
+LS_JAVA_OPTS=""
+
+# pidfiles aren't used the same way for upstart and systemd; this is for sysv users.
+LS_PIDFILE=/var/run/logstash.pid
+
+# user and group id to be invoked as
+LS_USER=logstash
+LS_GROUP=logstash
+
+# Enable GC logging by uncommenting the appropriate lines in the GC logging
+# section in jvm.options
+LS_GC_LOG_FILE=/var/log/logstash/gc.log
+
+# Open file limit
+LS_OPEN_FILES=16384
+
+# Nice level
+LS_NICE=19
+
+# Change these to have the init script named and described differently
+# This is useful when running multiple instances of Logstash on the same
+# physical box or vm
+SERVICE_NAME="logstash"
+SERVICE_DESCRIPTION="logstash"
+
+# If you need to run a command or script before launching Logstash, put it
+# between the lines beginning with `read` and `EOM`, and uncomment those lines.
+###
+## read -r -d '' PRESTART << EOM
+## EOM
diff --git a/docs/asciidoc/static/advanced-pipeline.asciidoc b/docs/asciidoc/static/advanced-pipeline.asciidoc
deleted file mode 100644
index 867f0a80263..00000000000
--- a/docs/asciidoc/static/advanced-pipeline.asciidoc
+++ /dev/null
@@ -1,500 +0,0 @@
-[[advanced-pipeline]]
-=== Setting Up an Advanced Logstash Pipeline
-
-A Logstash pipeline in most use cases has one or more input, filter, and output plugins. The scenarios in this section 
-build Logstash configuration files to specify these plugins and discuss what each plugin is doing.
-
-The Logstash configuration file defines your _Logstash pipeline_. When you start a Logstash instance, use the 
-`-f <path/to/file>` option to specify the configuration file that defines that instance’s pipeline.
-
-A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input 
-plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write 
-the data to a destination.
-
-image::static/images/basic_logstash_pipeline.png[]
-
-The following text represents the skeleton of a configuration pipeline:
-
-[source,shell]
-# The # character at the beginning of a line indicates a comment. Use
-# comments to describe your configuration.
-input {
-}
-# The filter part of this file is commented out to indicate that it is
-# optional.
-# filter {
-# 
-# }
-output {
-}
-
-This skeleton is non-functional, because the input and output sections don’t have any valid options defined. The 
-examples in this tutorial build configuration files to address specific use cases.
-
-Paste the skeleton into a file named `first-pipeline.conf` in your home Logstash directory.
-
-[[parsing-into-es]]
-==== Parsing Apache Logs into Elasticsearch
-
-This example creates a Logstash pipeline that takes Apache web logs as input, parses those logs to create specific, 
-named fields from the logs, and writes the parsed data to an Elasticsearch cluster.
-
-You can download the sample data set used in this example 
-https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here]. Unpack this file.
-
-[float]
-[[configuring-file-input]]
-===== Configuring Logstash for File Input
-
-To start your Logstash pipeline, configure the Logstash instance to read from a file using the 
-{logstash}plugins-inputs-file.html[file] input plugin.
-
-Edit the `first-pipeline.conf` file to add the following text:
-
-[source,json]
-input {
-    file {
-        path => "/path/to/logstash-tutorial.log"
-        start_position => beginning <1>
-    }
-}
-
-<1> The default behavior of the file input plugin is to monitor a file for new information, in a manner similar to the 
-UNIX `tail -f` command. To change this default behavior and process the entire file, we need to specify the position 
-where Logstash starts processing the file.
-
-Replace `/path/to/` with the actual path to the location of `logstash-tutorial.log` in your file system.
-
-[float]
-[[configuring-grok-filter]]
-===== Parsing Web Logs with the Grok Filter Plugin
-
-The {logstash}plugins-filters-grok.html[`grok`] filter plugin is one of several plugins that are available by default in 
-Logstash. For details on how to manage Logstash plugins, see the <<working-with-plugins,reference documentation>> for 
-the plugin manager.
-
-Because the `grok` filter plugin looks for patterns in the incoming log data, configuration requires you to make 
-decisions about how to identify the patterns that are of interest to your use case. A representative line from the web 
-server log sample looks like this:
-
-[source,shell]
-83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png 
-HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel 
-Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-
-The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. In this tutorial, use 
-the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
-
-[horizontal]
-*Information*:: *Field Name*
-IP Address:: `clientip`
-User ID:: `ident`
-User Authentication:: `auth`
-timestamp:: `timestamp`
-HTTP Verb:: `verb`
-Request body:: `request`
-HTTP Version:: `httpversion`
-HTTP Status Code:: `response`
-Bytes served:: `bytes`
-Referrer URL:: `referrer`
-User agent:: `agent`
-
-Edit the `first-pipeline.conf` file to add the following text:
-
-[source,json]
-filter {
-    grok {
-        match => { "message" => "%{COMBINEDAPACHELOG}"}
-    }
-}
-
-After processing, the sample line has the following JSON representation:
-
-[source,json]
-{
-"clientip" : "83.149.9.216",
-"ident" : ,
-"auth" : , 
-"timestamp" : "04/Jan/2015:05:13:42 +0000",
-"verb" : "GET",
-"request" : "/presentations/logstash-monitorama-2013/images/kibana-search.png",
-"httpversion" : "HTTP/1.1",
-"response" : "200",
-"bytes" : "203023",
-"referrer" : "http://semicomplete.com/presentations/logstash-monitorama-2013/",
-"agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-}
-
-[float]
-[[indexing-parsed-data-into-elasticsearch]]
-===== Indexing Parsed Data into Elasticsearch
-
-Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an 
-Elasticsearch cluster. Edit the `first-pipeline.conf` file to add the following text after the `input` section:
-
-[source,json]
-output {
-    elasticsearch {
-    }
-}
-
-With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes Logstash
-and Elasticsearch to be running on the same instance. You can specify a remote Elasticsearch instance using `hosts`
-configuration like `hosts => "es-machine:9092"`. 
-
-[float]
-[[configuring-geoip-plugin]]
-===== Enhancing Your Data with the Geoip Filter Plugin
-
-In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing 
-data. As an example, the {logstash}plugins-filters-geoip.html[`geoip`] plugin looks up IP addresses, derives geographic 
-location information from the addresses, and adds that location information to the logs.
-
-Configure your Logstash instance to use the `geoip` filter plugin by adding the following lines to the `filter` section 
-of the `first-pipeline.conf` file:
-
-[source,json]
-geoip {
-    source => "clientip"
-}
-
-The `geoip` plugin configuration requires data that is already defined as separate fields. Make sure that the `geoip` 
-section is after the `grok` section of the configuration file.
-
-Specify the name of the field that contains the IP address to look up. In this tutorial, the field name is `clientip`.
-
-[float]
-[[testing-initial-pipeline]]
-===== Testing Your Initial Pipeline
-
-At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
-like this:
-
-[source,json]
-input {
-    file {
-        path => "/Users/palecur/logstash-1.5.2/logstash-tutorial-dataset"
-        start_position => beginning
-    }
-}
-filter {
-    grok {
-        match => { "message" => "%{COMBINEDAPACHELOG}"}
-    }
-    geoip {
-        source => "clientip"
-    }
-}
-output {
-    elasticsearch {}
-    stdout {}
-}
-
-To verify your configuration, run the following command:
-
-[source,shell]
-bin/logstash -f first-pipeline.conf --configtest
-
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
-
-[source,shell]
-bin/logstash -f first-pipeline.conf
-
-Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin:
-
-[source,shell]
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=response=200'
-
-Replace $DATE with the current date, in YYYY.MM.DD format.
-
-Since our sample has just one 200 HTTP response, we get one hit back:
-
-[source,json]
-{"took":2,
-"timed_out":false,
-"_shards":{"total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.5351382,
-  "hits":[{"_index":"logstash-2015.07.30",
-    "_type":"logs",
-    "_id":"AU7gqOky1um3U6ZomFaF",
-    "_score":1.5351382,
-    "_source":{"message":"83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
-      "@version":"1",
-      "@timestamp":"2015-07-30T20:30:41.265Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"83.149.9.216",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:13:45 +0000",
-      "verb":"GET",
-      "request":"/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"52878",
-      "referrer":"\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
-      "agent":"\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\""
-      }
-    }]
-  }
-}
-
-Try another search for the geographic information derived from the IP address:
-
-[source,shell]
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=geoip.city_name=Buffalo'
-
-Replace $DATE with the current date, in YYYY.MM.DD format.
-
-Only one of the log entries comes from Buffalo, so the query produces a single response:
-
-[source,json]
-{"took":3,
-"timed_out":false,
-"_shards":{
-  "total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.03399,
-  "hits":[{"_index":"logstash-2015.07.31",
-    "_type":"logs",
-    "_id":"AU7mK3CVSiMeBsJ0b_EP",
-    "_score":1.03399,
-    "_source":{
-      "message":"108.174.55.234 - - [04/Jan/2015:05:27:45 +0000] \"GET /?flav=rss20 HTTP/1.1\" 200 29941 \"-\" \"-\"",
-      "@version":"1",
-      "@timestamp":"2015-07-31T22:11:22.347Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"108.174.55.234",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:27:45 +0000",
-      "verb":"GET",
-      "request":"/?flav=rss20",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"29941",
-      "referrer":"\"-\"",
-      "agent":"\"-\"",
-      "geoip":{
-        "ip":"108.174.55.234",
-        "country_code2":"US",
-        "country_code3":"USA",
-        "country_name":"United States",
-        "continent_code":"NA",
-        "region_name":"NY",
-        "city_name":"Buffalo",
-        "postal_code":"14221",
-        "latitude":42.9864,
-        "longitude":-78.7279,
-        "dma_code":514,
-        "area_code":716,
-        "timezone":"America/New_York",
-        "real_region_name":"New York",
-        "location":[-78.7279,42.9864]
-      }
-    }
-  }]
- }
-}
-
-[[multiple-input-output-plugins]]
-==== Multiple Input and Output Plugins
-
-The information you need to manage often comes from several disparate sources, and use cases can require multiple 
-destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these 
-requirements.
-
-This example creates a Logstash pipeline that takes input from a Twitter feed and the Logstash Forwarder client, then 
-sends the information to an Elasticsearch cluster as well as writing the information directly to a file.
-
-[float]
-[[twitter-configuration]]
-===== Reading from a Twitter feed
-
-To add a Twitter feed, you need several pieces of information:
-
-* A _consumer_ key, which uniquely identifies your Twitter app, which is Logstash in this case.
-* A _consumer secret_, which serves as the password for your Twitter app.
-* One or more _keywords_ to search in the incoming feed.
-* An _oauth token_, which identifies the Twitter account using this app.
-* An _oauth token secret_, which serves as the password of the Twitter account.
-
-Visit https://dev.twitter.com/apps to set up a Twitter account and generate your consumer key and secret, as well as 
-your OAuth token and secret.
-
-Use this information to add the following lines to the `input` section of the `first-pipeline.conf` file:
-
-[source,json]
-twitter {
-    consumer_key =>
-    consumer_secret =>
-    keywords =>
-    oauth_token =>
-    oauth_token_secret => 
-}
-
-[float]
-[[configuring-lsf]]
-===== The Logstash Forwarder
-
-The https://github.com/elastic/logstash-forwarder[Logstash Forwarder] is a lightweight, resource-friendly tool that 
-collects logs from files on the server and forwards these logs to your Logstash instance for processing. The 
-Logstash Forwarder uses a secure protocol called _lumberjack_ to communicate with your Logstash instance. The 
-lumberjack protocol is designed for reliability and low latency. The Logstash Forwarder uses the computing resources of 
-the machine hosting the source data, and the Lumberjack input plugin minimizes the resource demands on the Logstash 
-instance.
-
-NOTE: In a typical use case, the Logstash Forwarder client runs on a separate machine from the machine running your 
-Logstash instance. For the purposes of this tutorial, both Logstash and the Logstash Forwarder will be running on the
-same machine.
-
-Default Logstash configuration includes the {logstash}plugins-inputs-lumberjack.html[Lumberjack input plugin], which is 
-designed to be resource-friendly. To install the Logstash Forwarder on your data source machine, install the 
-appropriate package from the main Logstash https://www.elastic.co/downloads/logstash[product page].
-
-Create a configuration file for the Logstash Forwarder similar to the following example:
-
-[source,json]
---------------------------------------------------------------------------------
-{
-    "network": {
-        "servers": [ "localhost:5043" ],
-        "ssl ca": "/path/to/localhost.crt", <1>
-        "timeout": 15
-    },
-    "files": [
-        {
-            "paths": [
-                "/path/to/sample-log" <2>
-            ],
-            "fields": { "type": "apache" }
-        }
-    ]
-}
---------------------------------------------------------------------------------
-
-<1> Path to the SSL certificate for the Logstash instance.
-<2> Path to the file or files that the Logstash Forwarder processes.
-
-Save this configuration file as `logstash-forwarder.conf`. 
-
-Configure your Logstash instance to use the Lumberjack input plugin by adding the following lines to the `input` section 
-of the `first-pipeline.conf` file:
-
-[source,json]
-lumberjack {
-    port => "5043"
-    ssl_certificate => "/path/to/ssl-cert" <1>
-    ssl_key => "/path/to/ssl-key" <2>
-}
-
-<1> Path to the SSL certificate that the Logstash instance uses to authenticate itself to Logstash Forwarder.
-<2> Path to the key for the SSL certificate.
-
-[float]
-[[logstash-file-output]]
-===== Writing Logstash Data to a File
-
-You can configure your Logstash pipeline to write data directly to a file with the 
-{logstash}plugins-outputs-file.html[`file`] output plugin.
-
-Configure your Logstash instance to use the `file` output plugin by adding the following lines to the `output` section 
-of the `first-pipeline.conf` file:
-
-[source,json]
-file {
-    path => /path/to/target/file
-}
-
-[float]
-[[multiple-es-nodes]]
-===== Writing to multiple Elasticsearch nodes
-
-Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as 
-providing redundant points of entry into the cluster when a particular node is unavailable.
-
-To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the output section of the `first-pipeline.conf` file to read:
-
-[source,json]
---------------------------------------------------------------------------------
-output {
-    elasticsearch {
-        hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
-    }
-}
---------------------------------------------------------------------------------
-
-Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the `hosts` 
-parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
-default port for Elasticsearch is `9200` and can be omitted in the configuration above.
-
-[float]
-[[testing-second-pipeline]]
-===== Testing the Pipeline
-
-At this point, your `first-pipeline.conf` file looks like this:
-
-[source,json]
---------------------------------------------------------------------------------
-input {
-    twitter {
-        consumer_key =>
-        consumer_secret =>
-        keywords =>
-        oauth_token =>
-        oauth_token_secret =>
-    }
-    lumberjack {
-        port => "5043"
-        ssl_certificate => "/path/to/ssl-cert"
-        ssl_key => "/path/to/ssl-key"
-    }
-}
-output {
-    elasticsearch {
-        hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
-    }
-    file {
-        path => /path/to/target/file
-    }
-}
---------------------------------------------------------------------------------
-
-Logstash is consuming data from the Twitter feed you configured, receiving data from the Logstash Forwarder, and 
-indexing this information to three nodes in an Elasticsearch cluster as well as writing to a file.
-
-At the data source machine, run the Logstash Forwarder with the following command:
-
-[source,shell]
-logstash-forwarder -config logstash-forwarder.conf
-
-Logstash Forwarder will attempt to connect on port 5403. Until Logstash starts with an active Lumberjack plugin, there 
-won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
-
-To verify your configuration, run the following command:
-
-[source,shell]
-bin/logstash -f first-pipeline.conf --configtest
-
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
-
-[source,shell]
-bin/logstash -f first-pipeline.conf
-
-Use the `grep` utility to search in the target file to verify that information is present:
-
-[source,shell]
-grep Mozilla /path/to/target/file
-
-Run an Elasticsearch query to find the same information in the Elasticsearch cluster:
-
-[source,shell]
-curl -XGET 'localhost:9200/logstash-2015.07.30/_search?q=agent=Mozilla'
diff --git a/docs/asciidoc/static/breaking-changes.asciidoc b/docs/asciidoc/static/breaking-changes.asciidoc
deleted file mode 100644
index 1232e49276c..00000000000
--- a/docs/asciidoc/static/breaking-changes.asciidoc
+++ /dev/null
@@ -1,61 +0,0 @@
-[[breaking-changes]]
-== Breaking changes
-
-Version 2.0 of Logstash has some changes that are incompatible with previous versions of Logstash. This section discusses 
-what you need to be aware of when migrating to this version.
-
-[float]
-=== Elasticsearch Output Default
-
-Starting with the 2.0 release of Logstash, the default Logstash output for Elasticsearch is HTTP. To use the `node` or
-`transport` protocols, download the https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-elasticsearch_java.html[Elasticsearch Java plugin]. The 
-Logstash HTTP output to Elasticsearch now supports sniffing.
-
-NOTE: The `elasticsearch_java` plugin has two versions specific to the version of the underlying Elasticsearch cluster. 
-Be sure to specify the correct value for the `--version` option during installation:
-* For Elasticsearch versions before 2.0, use the command 
-`bin/plugin install --version 1.5.x logstash-output-elasticsearch_java`
-* For Elasticsearch versions 2.0 and after, use the command 
-`bin/plugin install --version 2.0.0 logstash-output-elasticsearch_java`
-
-[float]
-==== Configuration Changes
-
-The Elasticsearch output plugin configuration has the following changes:
-
-* The `host` configuration option is now `hosts`, allowing you to specify multiple hosts and associated ports in the 
-`myhost:9200` format
-* New options: `bind_host`, `bind_port`, `cluster`, `embedded`, `embedded_http_port`, `port`, `sniffing_delay`
-* The `max_inflight_requests` option, which was deprecated in the 1.5 release, is now removed
-* Since the `hosts` option allows specification of ports for the hosts, the redundant `port` option is now removed
-* The `node_name` and `protocol` options have been moved to the `elasticsearch_java` plugin
-
-The following deprecated configuration settings are removed in this release:
-
-* input plugin configuration settings: `debug`, `format`, `charset`, `message_format`
-* output plugin configuration settings: `type`, `tags`, `exclude_tags`.
-* filter plugin configuration settings: `type`, `tags`, `exclude_tags`.
-
-Configuration files with these settings present are invalid and prevent Logstash from starting.
-
-[float]
-=== Kafka Output Configuration Changes
-
-The 2.0 release of Logstash includes a new version of the Kafka output plugin with significant configuration changes.
-Please compare the documentation pages for the 
-https://www.elastic.co/guide/en/logstash/1.5/plugins-outputs-kafka.html[Logstash 1.5] and
-https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-kafka.html[Logstash 2.0] versions of the Kafka output plugin 
-and update your configuration files accordingly.
-
-[float]
-=== Metrics Filter Changes
-Prior implementations of the metrics filter plugin used dotted field names. Elasticsearch does not allow field names to have dots, beginning with version 2.0, so a change was made to use sub-fields instead of dots in this plugin. Please note that these changes make version 3.0.0 of the metrics filter plugin incompatible with previous releases.
-
-
-[float]
-=== Filter Worker Default Change
-
-Starting with the 2.0 release of Logstash, the default value of the `filter_workers` configuration option for filter 
-plugins is half of the available CPU cores, instead of 1. This change increases parallelism in filter execution for 
-resource-intensive filtering operations. You can continue to use the `-w` flag to manually set the value for this option, 
-as in previous releases.
diff --git a/docs/asciidoc/static/command-line-flags.asciidoc b/docs/asciidoc/static/command-line-flags.asciidoc
deleted file mode 100644
index 839b162901b..00000000000
--- a/docs/asciidoc/static/command-line-flags.asciidoc
+++ /dev/null
@@ -1,49 +0,0 @@
-[[command-line-flags]]
-=== Command-line flags
-
-Logstash has the following flags. You can use the `--help` flag to display this information.
-
-[source,shell]
-----------------------------------
--f, --config CONFIGFILE
- Load the Logstash config from a specific file, directory, or a wildcard. If
- given a directory or wildcard, config files will be read from the directory in
- alphabetical order.
-
--e CONFIGSTRING
- Use the given string as the configuration data. Same syntax as the config file.
- If not input is specified, 'stdin { type => stdin }' is default. If no output
- is specified, 'stdout { codec => rubydebug }}' is default.
-
--w, --filterworkers COUNT
- Sets the number of filter workers to run (default: half the number of cores)
-
--l, --log FILE
- Log to a given path. Default is to log to stdout
-
---verbose
- Increase verbosity to the first level (info), less verbose.
-
---debug
- Increase verbosity to the last level (trace), more verbose.
-
--V, --version
-  Display the version of Logstash.
-
--p, --pluginpath
-  A path of where to find plugins. This flag can be given multiple times to include
-  multiple paths. Plugins are expected to be in a specific directory hierarchy:
-  'PATH/logstash/TYPE/NAME.rb' where TYPE is 'inputs' 'filters', 'outputs' or 'codecs'
-  and NAME is the name of the plugin.
-
--t, --configtest
-  Checks configuration and then exit. Note that grok patterns are not checked for 
-  correctness with this flag
-
--h, --help
-  Print help  
-
--v
- *DEPRECATED: see --verbose/debug* Increase verbosity. There are multiple levels
- of verbosity available with '-vv' currently being the highest
-----------------------------------
diff --git a/docs/asciidoc/static/getting-started-with-logstash.asciidoc b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
deleted file mode 100644
index 354200b548d..00000000000
--- a/docs/asciidoc/static/getting-started-with-logstash.asciidoc
+++ /dev/null
@@ -1,57 +0,0 @@
-[[getting-started-with-logstash]]
-== Getting Started with Logstash
-
-This section guides you through the process of installing Logstash and verifying that everything is running properly. 
-Later sections deal with increasingly complex configurations to address selected use cases.
-
-[float]
-[[installing-logstash]]
-=== Install Logstash
-
-NOTE: Logstash requires Java 7 or later. Use the 
-http://www.oracle.com/technetwork/java/javase/downloads/index.html[official Oracle distribution] or an open-source 
-distribution such as http://openjdk.java.net/[OpenJDK].
-
-To check your Java version, run the following command:
-
-[source,shell]
-java -version
-
-On systems with Java installed, this command produces output similar to the following:
-
-[source,shell]
-java version "1.7.0_45"
-Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
-Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
-
-[float]
-[[installing-binary]]
-==== Installing from a downloaded binary
-
-Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment. 
-Unpack the file. On supported Linux operating systems, you can <<package-repositories,use a package manager>> to 
-install Logstash.
-
-[[first-event]]
-=== Stashing Your First Event: Basic Logstash Example
-
-To test your Logstash installation, run the most basic Logstash pipeline:
-
-[source,shell]
-cd logstash-{logstash_version}
-bin/logstash -e 'input { stdin { } } output { stdout {} }'
-
-The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the 
-command line lets you quickly test configurations without having to edit a file between iterations.
-This pipeline takes input from the standard input, `stdin`, and moves that input to the standard output, `stdout`, in a 
-structured format. Type hello world at the command prompt to see Logstash respond:
-
-[source,shell]
-hello world
-2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
-
-Logstash adds timestamp and IP address information to the message. Exit Logstash by issuing a *CTRL-D* command in the 
-shell where Logstash is running.
-
-The <<advanced-pipeline,Advanced Tutorial>> expands the capabilities of your Logstash instance to cover broader 
-use cases.
diff --git a/docs/asciidoc/static/howtos-and-tutorials.asciidoc b/docs/asciidoc/static/howtos-and-tutorials.asciidoc
deleted file mode 100644
index fe98a8e0cb9..00000000000
--- a/docs/asciidoc/static/howtos-and-tutorials.asciidoc
+++ /dev/null
@@ -1,16 +0,0 @@
-[[howtos-and-tutorials]]
-== Logstash HOWTOs and Tutorials
-Pretty self-explanatory, really
-
-=== Downloads and Releases
-* http://elasticsearch.org/#[Getting Started with Logstash]
-* http://elasticsearch.org/#[Configuration file overview]
-* http://elasticsearch.org/#[Command-line flags]
-* http://elasticsearch.org/#[The life of an event in Logstash]
-* http://elasticsearch.org/#[Using conditional logic]
-* http://elasticsearch.org/#[Glossary]
-* http://elasticsearch.org/#[referring to fields `[like][this]`]
-* http://elasticsearch.org/#[using the `%{fieldname}` syntax]
-* http://elasticsearch.org/#[Metrics from Logs]
-* http://elasticsearch.org/#[Using RabbitMQ]
-* http://elasticsearch.org/#[Contributing to Logstash]
diff --git a/docs/asciidoc/static/images/basic_logstash_pipeline.png b/docs/asciidoc/static/images/basic_logstash_pipeline.png
deleted file mode 100644
index d1b31401a49..00000000000
Binary files a/docs/asciidoc/static/images/basic_logstash_pipeline.png and /dev/null differ
diff --git a/docs/asciidoc/static/images/deploy_3.png b/docs/asciidoc/static/images/deploy_3.png
deleted file mode 100644
index cda4337fa9d..00000000000
Binary files a/docs/asciidoc/static/images/deploy_3.png and /dev/null differ
diff --git a/docs/asciidoc/static/life-of-an-event.asciidoc b/docs/asciidoc/static/life-of-an-event.asciidoc
deleted file mode 100644
index 569bd545f7c..00000000000
--- a/docs/asciidoc/static/life-of-an-event.asciidoc
+++ /dev/null
@@ -1,176 +0,0 @@
-[[pipeline]]
-=== Logstash Processing Pipeline
-
-The Logstash event processing pipeline has three stages: inputs -> filters ->
-outputs. Inputs generate events, filters modify them, and outputs ship them
-elsewhere. Inputs and outputs support codecs that enable you to encode or decode
-the data as it enters or exits the pipeline without having to use a separate
-filter.
-
-[float]
-==== Inputs
-You use inputs to get data into Logstash. Some of the more commonly-used inputs
-are:
-
-* *file*: reads from a file on the filesystem, much like the UNIX command
-`tail -0F`
-* *syslog*: listens on the well-known port 514 for syslog messages and parses
-according to the RFC3164 format
-* *redis*: reads from a redis server, using both redis channels and redis lists.
-Redis is often used as a "broker" in a centralized Logstash installation, which
-queues Logstash events from remote Logstash "shippers".
-* *lumberjack*: processes events sent in the lumberjack protocol. Now called
-https://github.com/elastic/logstash-forwarder[logstash-forwarder].
-
-For more information about the available inputs, see
-<<input-plugins,Input Plugins>>.
-
-[float]
-==== Filters
-Filters are intermediary processing devices in the Logstash pipeline. You can
-combine filters with conditionals to perform an action on an event if it meets
-certain criteria. Some useful filters include:
-
-* *grok*: parse and structure arbitrary text. Grok is currently the best way in
-Logstash to parse unstructured log data into something structured and queryable.
-With 120 patterns built-in to Logstash, it's more than likely you'll find one
-that meets your needs!
-* *mutate*: perform general transformations on event fields. You can rename,
-remove, replace, and modify fields in your events.
-* *drop*: drop an event completely, for example, 'debug' events.
-* *clone*: make a copy of an event, possibly adding or removing fields.
-* *geoip*: add information about geographical location of IP addresses (also
-displays amazing charts in Kibana!)
-
-For more information about the available filters, see
-<<filter-plugins,Filter Plugins>>.
-
-[float]
-==== Outputs
-Outputs are the final phase of the Logstash pipeline. An event can pass through
-multiple outputs, but once all output processing is complete, the event has
-finished its execution. Some commonly used outputs include:
-
-* *elasticsearch*: send event data to Elasticsearch. If you're planning to save
-your data in an efficient, convenient, and easily queryable format...
-Elasticsearch is the way to go. Period. Yes, we're biased :)
-* *file*: write event data to a file on disk.
-* *graphite*: send event data to graphite, a popular open source tool for
-storing and graphing metrics. http://graphite.wikidot.com/
-* *statsd*: send event data to statsd, a service that "listens for statistics,
-like counters and timers, sent over UDP and sends aggregates to one or more
-pluggable backend services". If you're already using statsd, this could be
-useful for you!
-
-For more information about the available outputs, see
-<<output-plugins,Output Plugins>>.
-
-[float]
-==== Codecs
-Codecs are basically stream filters that can operate as part of an input or
-output. Codecs enable you to easily separate the transport of your messages from
-the serialization process. Popular codecs include `json`, `msgpack`, and `plain`
-(text).
-
-* *json*: encode or decode data in the JSON format.
-* *multiline*: merge multiple-line text events such as java exception and
-stacktrace messages into a single event.
-
-For more information about the available codecs, see
-<<codec-plugins,Codec Plugins>>.
-
-[float]
-=== Fault Tolerance
-
-Events are passed from stage to stage using internal queues implemented with a
-Ruby `SizedQueue`. A `SizedQueue` has a maximum number of items it can contain.
-When the queue is at maximum capacity, all writes to the queue are blocked.
-
-Logstash sets the size of each queue to 20. This means a maximum of 20 events
-can be pending for the next stage, which helps prevent data loss and keeps
-Logstash from acting as a data storage system. These internal queues are not
-intended for storing messages long-term.
-
-The small queue sizes mean that Logstash simply blocks and stalls safely when
-there's a heavy load or temporary pipeline problems. The alternatives would be
-to either have an unlimited queue or drop messages when there's a problem. An
-unlimited queue can grow unbounded and eventually exceed memory, causing a crash
-that loses all of the queued messages. In most cases, dropping messages outright
-is equally undesirable.
-
-An output can fail or have problems due to downstream issues, such as a full
-disk, permissions problems, temporary network failures, or service outages. Most
-outputs keep retrying to ship events affected by the failure.
-
-If an output is failing, the output thread waits until the output is able to
-successfully send the message. The output stops reading from the output queue,
-which means the queue can fill up with events.
-
-When the output queue is full, filters are blocked because they cannot write new
-events to the output queue. While they are blocked from writing to the output
-queue, filters stop reading from the filter queue. Eventually, this can cause
-the filter queue (input -> filter) to fill up.
-
-A full filter queue blocks inputs from writing to the filters. This causes all
-inputs to stop processing data from wherever they're getting new events.
-
-In ideal circumstances, this behaves similarly to when the tcp window closes to
-0. No new data is sent because the receiver hasn't finished processing the
-current queue of data, but as soon as the downstream (output) problem is
-resolved, messages start flowing again.
-
-[float]
-=== Thread Model
-
-The thread model in Logstash is currently:
-
-[source,js]
-----------------------------------
-input threads | filter worker threads | output worker
-----------------------------------
-
-Filters are optional, so if you have no filters defined it is simply:
-
-[source,js]
-----------------------------------
-input threads | output worker
-----------------------------------
-
-Each input runs in a thread by itself. This prevents busier inputs from being
-blocked by slower ones. It also allows for easier containment of scope because
-each input has a thread.
-
-The filter thread model is a 'worker' model where each worker receives an event
-and applies all filters, in order, before sending it on to the output queue.
-This allows scalability across CPUs because many filters are CPU intensive
-(permitting that we have thread safety).
-
-The default number of filter workers is 1, but you can increase this number by
-specifying the '-w' flag when you run the Logstash agent.
-
-The output worker model is currently a single thread. Outputs receive events in
-the order the outputs are defined in the config file.
-
-Outputs might decide to temporarily buffer events before publishing them. One
-example of this is the `elasticsearch` output, which buffers events and flushes
-them all at once using a separate thread. This mechanism (buffering many events
-and writing in a separate thread) can improve performance because it prevents
-the Logstash pipeline from being stalled waiting for a response from
-elasticsearch.
-
-[float]
-=== Resource Usage
-
-Logstash typically has at least 3 threads (2 if you have no filters). One input
-thread, one filter worker thread, and one output thread. If you see Logstash
-using multiple CPUs, this is likely why. If you want to know more about what
-each thread is doing, you should read this article:
-http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance].
-Threads in Java have names and you can use `jstack` and `top` to figure out who
-is using what resources.
-
-On Linux platforms, Logstash labels all the threads it can with something
-descriptive. For example, inputs show up as `<inputname`, filter workers show up
-as `|worker`, and outputs show up as `>outputworker`.  Where possible, other
-threads are also labeled to help you identify their purpose should you wonder
-why they are consuming resources!
diff --git a/docs/asciidoc/static/logstash-docs-home.asciidoc b/docs/asciidoc/static/logstash-docs-home.asciidoc
deleted file mode 100644
index 19bd3281184..00000000000
--- a/docs/asciidoc/static/logstash-docs-home.asciidoc
+++ /dev/null
@@ -1,30 +0,0 @@
-[[logstash-docs-home]]
-== Logstash Documentation
-Pretty self-explanatory, really
-
-=== Downloads and Releases
-* http://www.elasticsearch.org/overview/logstash/download/[Download Logstash 1.4.2]
-* http://www.elasticsearch.org/blog/apt-and-yum-repositories/[package repositories]
-* http://www.elasticsearch.org/blog/logstash-1-4-2/[release notes]
-* https://github.com/elasticsearch/logstash/blob/master/CHANGELOG[view changelog]
-* https://github.com/elasticsearch/puppet-logstash[Puppet Module]
-
-=== Plugins
-* http://elasticsearch.org/#[contrib plugins]
-* http://elasticsearch.org/#[writing your own plugins]
-* http://elasticsearch.org/#[Inputs] / http://elasticsearch.org/#[Filters] / http://elasticsearch.org/#[Outputs]
-* http://elasticsearch.org/#[Codecs]
-* http://elasticsearch.org/#[(more)]
-
-=== HOWTOs, References, Information
-* http://elasticsearch.org/#[Getting Started with Logstash]
-* http://elasticsearch.org/#[Configuration file overview]
-* http://elasticsearch.org/#[Command-line flags]
-* http://elasticsearch.org/#[The life of an event in Logstash]
-* http://elasticsearch.org/#[Using conditional logic]
-* http://elasticsearch.org/#[Glossary]
-* http://elasticsearch.org/#[(more)]
-
-=== About / Videos / Blogs
-* http://elasticsearch.org/#[Videos]
-* http://elasticsearch.org/#[Blogs]
diff --git a/docs/asciidoc/static/plugin-manager.asciidoc b/docs/asciidoc/static/plugin-manager.asciidoc
deleted file mode 100644
index f15ea99dbd0..00000000000
--- a/docs/asciidoc/static/plugin-manager.asciidoc
+++ /dev/null
@@ -1,103 +0,0 @@
-[[working-with-plugins]]
-== Working with plugins
-
-Logstash has a rich collection of input, filter, codec and output plugins. Plugins are available as self-contained packages called gems and hosted on RubyGems.org. The plugin manager accesed via `bin/plugin` script is used to manage the lifecycle of plugins in your Logstash deployment. You can install, uninstall and upgrade plugins using these Command Line Interface (CLI) described below.
-
-NOTE: Some sections here are for advanced users
-
-[float]
-[[listing-plugins]]
-=== Listing plugins
-
-Logstash release packages bundle common plugins so you can use them out of the box. To list the plugins currently available in your deployment:
-
-[source,shell]
-----------------------------------
-bin/plugin list <1>
-bin/plugin list --verbose <2>
-bin/plugin list *namefragment* <3>
-bin/plugin list --group output <4>
-----------------------------------
-<1> Will list all installed plugins
-
-<2> Will list installed plugins with version information
-
-<3> Will list all installed plugins containing a namefragment
-
-<4> Will list all installed plugins for a particular group (input, filter, codec, output)
-
-[float]
-[[installing-plugins]]
-=== Adding plugins to your deployment
-
-The most common situation when dealing with plugin installation is when you have access to internet. Using this method, you will be able to retrieve plugins hosted on the public repository (RubyGems.org) and install on top of your Logstash installation.
-
-[source,shell]
-----------------------------------
-bin/plugin install logstash-output-kafka
-----------------------------------
-
-Once the plugin is successfully installed, you can start using it in your configuration file.
-
-[[installing-local-plugins]]
-[float]
-==== Advanced: Adding a locally built plugin
-
-In some cases, you want to install plugins which have not yet been released and not hosted on RubyGems.org. Logstash provides you the option to install a locally built plugin which is packaged as a ruby gem. Using a file location:
-
-[source,shell]
-----------------------------------
-bin/plugin install /path/to/logstash-output-kafka-1.0.0.gem
-----------------------------------
-
-[[installing-local-plugins-path]]
-[float]
-==== Advanced: Using `--pluginpath`
-
-Using the `--pluginpath` flag, you can load a plugin source code located on your file system. Typically this is used by developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
-
-[source,shell]
-----------------------------------
-bin/logstash --pluginpath /opt/shared/lib/logstash/input/my-custom-plugin-code.rb
-----------------------------------
-
-[[updating-plugins]]
-[float]
-=== Updating plugins
-
-Plugins have their own release cycle and are often released independent of Logstash’s core release cycle. Using the update sub-command you can get the latest or update to a particular version of the plugin.
-
-[source,shell]
-----------------------------------
-bin/plugin update <1>
-bin/plugin update logstash-output-kafka <2>
-----------------------------------
-<1> will update all installed plugins
-
-<2> will update only this plugin
-
-[[removing-plugins]]
-[float]
-=== Removing plugins
-
-If you need to remove plugins from your Logstash installation:
-
-[source,shell]
-----------------------------------
-bin/plugin uninstall logstash-output-kafka
-----------------------------------
-
-[[proxy-plugins]]
-[float]
-=== Proxy Support
-
-The previous sections relied on Logstash being able to communicate with RubyGems.org. In certain environments, Forwarding Proxy is used to handle HTTP requests. Logstash Plugins can be installed and updated through a Proxy by setting the `HTTP_PROXY` environment variable:
-
-[source,shell]
-----------------------------------
-export HTTP_PROXY=http://127.0.0.1:3128
-
-bin/plugin install logstash-output-kafka
-----------------------------------
-
-Once set, plugin commands install, update can be used through this proxy.
diff --git a/docs/asciidoc/static/repositories.asciidoc b/docs/asciidoc/static/repositories.asciidoc
deleted file mode 100644
index 7a394885dac..00000000000
--- a/docs/asciidoc/static/repositories.asciidoc
+++ /dev/null
@@ -1,86 +0,0 @@
-[[package-repositories]]
-== Package Repositories
-
-We also have repositories available for APT and YUM based distributions. Note
-that we only provide binary packages, but no source packages, as the packages
-are created as part of the Logstash build.
-
-We have split the Logstash package repositories by version into separate urls 
-to avoid accidental upgrades across major or minor versions. For all 1.5.x 
-releases use 1.5 as version number, for 1.4.x use 1.4, etc.
-
-We use the PGP key
-http://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
-Elastic's Signing Key, with fingerprint
-
-    4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
-
-to sign all our packages. It is available from http://pgp.mit.edu.
-
-[float]
-=== APT
-
-Download and install the Public Signing Key:
-
-[source,sh]
---------------------------------------------------
-wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
---------------------------------------------------
-
-Add the repository definition to your `/etc/apt/sources.list` file:
-
-["source","sh",subs="attributes,callouts"]
---------------------------------------------------
-echo "deb http://packages.elastic.co/logstash/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list
---------------------------------------------------
-
-[WARNING]
-==================================================
-Use the `echo` method described above to add the Logstash repository.  Do not
-use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not
-provide a source package. If you have added the `deb-src` entry, you will see an
-error like the following:
-
-    Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)
-
-Just delete the `deb-src` entry from the `/etc/apt/sources.list` file and the
-installation should work as expected.
-==================================================
-
-Run `sudo apt-get update` and the repository is ready for use. You can install
-it with:
-
-[source,sh]
---------------------------------------------------
-sudo apt-get update && sudo apt-get install logstash
---------------------------------------------------
-
-[float]
-=== YUM
-
-Download and install the public signing key:
-
-[source,sh]
---------------------------------------------------
-rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
---------------------------------------------------
-
-Add the following in your `/etc/yum.repos.d/` directory
-in a file with a `.repo` suffix, for example `logstash.repo`
-
-["source","sh",subs="attributes,callouts"]
---------------------------------------------------
-[logstash-{branch}]
-name=Logstash repository for {branch}.x packages
-baseurl=http://packages.elastic.co/logstash/{branch}/centos
-gpgcheck=1
-gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
-enabled=1
---------------------------------------------------
-
-And your repository is ready for use. You can install it with:
-
-[source,sh]
---------------------------------------------------
-yum install logstash
---------------------------------------------------
diff --git a/docs/asciidocgen.rb b/docs/asciidocgen.rb
index c95a4f2b79d..5392e6a6440 100644
--- a/docs/asciidocgen.rb
+++ b/docs/asciidocgen.rb
@@ -6,6 +6,7 @@
 $: << File.join(File.dirname(__FILE__), "..", "lib")
 $: << File.join(File.dirname(__FILE__), "..", "rakelib")
 
+require_relative "../lib/bootstrap/environment" #needed for LogStash::Environment constants LOGSTASH_HOME
 require "logstash/config/mixin"
 require "logstash/inputs/base"
 require "logstash/codecs/base"
@@ -161,7 +162,7 @@ def generate(file, settings)
     load file
 
     # Get the correct base path
-    base = File.join(::LogStash::Environment::LOGSTASH_HOME,'lib/logstash', file.split("/")[-2])
+    base = File.join(::LogStash::Environment::LOGSTASH_HOME,'logstash-core/lib/logstash', file.split("/")[-2])
 
     # parse base first
     parse(File.new(File.join(base, "base.rb"), "r").read)
diff --git a/docs/configuration.md b/docs/configuration.md
deleted file mode 100644
index 7cd59fb6090..00000000000
--- a/docs/configuration.md
+++ /dev/null
@@ -1,322 +0,0 @@
----
-title: Configuration Language - Logstash
-layout: content_right
----
-# Logstash Config Language
-
-The Logstash config language aims to be simple.
-
-There are 3 main sections: inputs, filters, outputs. Each section has
-configurations for each plugin available in that section.
-
-Example:
-
-    # This is a comment. You should use comments to describe
-    # parts of your configuration.
-    input {
-      ...
-    }
-
-    filter {
-      ...
-    }
-
-    output {
-      ...
-    }
-
-## Filters and Ordering
-
-For a given event, are applied in the order of appearance in the
-configuration file.
-
-## Comments
-
-Comments are the same as in ruby, perl, and python. Starts with a '#' character.
-Example:
-
-    # this is a comment
-
-    input { # comments can appear at the end of a line, too
-      # ...
-    }
-
-## Plugins
-
-The input, filter and output sections all let you configure plugins. Plugin
-configuration consists of the plugin name followed by a block of settings for
-that plugin. For example, how about two file inputs:
-
-    input {
-      file {
-        path => "/var/log/messages"
-        type => "syslog"
-      }
-
-      file {
-        path => "/var/log/apache/access.log"
-        type => "apache"
-      }
-    }
-
-The above configures two file separate inputs. Both set two
-configuration settings each: 'path' and 'type'. Each plugin has different
-settings for configuring it; seek the documentation for your plugin to
-learn what settings are available and what they mean. For example, the
-[file input][fileinput] documentation will explain the meanings of the
-path and type settings.
-
-[fileinput]: inputs/file
-
-## Value Types
-
-The documentation for a plugin may enforce a configuration field having a
-certain type.  Examples include boolean, string, array, number, hash,
-etc.
-
-### <a name="boolean"></a>Boolean
-
-A boolean must be either `true` or `false`. Note the lack of quotes around
-`true` and `false`.
-
-Examples:
-
-    debug => true
-
-### <a name="string"></a>String
-
-A string must be a single value.
-
-Example:
-
-    name => "Hello world"
-
-Single, unquoted words are valid as strings, too, but you should use quotes.
-
-### <a name="number"></a>Number
-
-Numbers must be valid numerics (floating point or integer are OK).
-
-Example:
-
-    port => 33
-
-### <a name="array"></a>Array
-
-An array can be a single string value or multiple. If you specify the same
-field multiple times, it appends to the array.
-
-Examples:
-
-    path => [ "/var/log/messages", "/var/log/*.log" ]
-    path => "/data/mysql/mysql.log"
-
-The above makes 'path' a 3-element array including all 3 strings.
-
-### <a name="hash"></a>Hash
-
-A hash is basically the same syntax as Ruby hashes.
-The key and value are simply pairs, such as:
-
-    match => {
-      "field1" => "value1"
-      "field2" => "value2"
-      ...
-    }
-
-## <a name="eventdependent"></a>Event Dependent Configuration
-
-The logstash agent is a processing pipeline with 3 stages: inputs -> filters ->
-outputs. Inputs generate events, filters modify them, outputs ship them
-elsewhere.
-
-All events have properties. For example, an apache access log would have things
-like status code (200, 404), request path ("/", "index.html"), HTTP verb
-(GET, POST), client IP address, etc. Logstash calls these properties "fields."
-
-Some of the configuration options in Logstash require the existence of fields in
-order to function.  Because inputs generate events, there are no fields to
-evaluate within the input block--they do not exist yet!  
-
-Because of their dependency on events and fields, the following configuration
-options will only work within filter and output blocks.
-
-**IMPORTANT: Field references, sprintf format and conditionals, described below,
-will not work in an input block.
-
-### <a name="fieldreferences"></a>Field References
-
-In many cases, it is useful to be able to refer to a field by name. To do this,
-you can use the Logstash field reference syntax.
-
-By way of example, let us suppose we have this event:
-
-    {
-      "agent": "Mozilla/5.0 (compatible; MSIE 9.0)",
-      "ip": "192.168.24.44",
-      "request": "/index.html"
-      "response": {
-        "status": 200,
-        "bytes": 52353
-      },
-      "ua": {
-        "os": "Windows 7"
-      }
-    }
-
-- the syntax to access fields is `[fieldname]`.
-- if you are only referring to a **top-level field**, you can omit the `[]` and
-simply say `fieldname`.
-- in the case of **nested fields**, like the "os" field above, you need
-the full path to that field: `[ua][os]`.
-
-### <a name="sprintf"></a>sprintf format
-
-This syntax is also used in what Logstash calls 'sprintf format'. This format
-allows you to refer to field values from within other strings. For example, the
-statsd output has an 'increment' setting, to allow you to keep a count of
-apache logs by status code:
-
-    output {
-      statsd {
-        increment => "apache.%{[response][status]}"
-      }
-    }
-
-You can also do time formatting in this sprintf format. Instead of specifying a
-field name, use the `+FORMAT` syntax where `FORMAT` is a
-[time format](http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html).
-
-For example, if you want to use the file output to write to logs based on the
-hour and the 'type' field:
-
-    output {
-      file {
-        path => "/var/log/%{type}.%{+yyyy.MM.dd.HH}"
-      }
-    }
-
-### <a name="conditionals"></a>Conditionals
-
-Sometimes you only want a filter or output to process an event under
-certain conditions. For that, you'll want to use a conditional!
-
-Conditionals in Logstash look and act the same way they do in programming
-languages. You have `if`, `else if` and `else` statements. Conditionals may be
-nested if you need that.
-
-The syntax is follows:
-
-    if EXPRESSION {
-      ...
-    } else if EXPRESSION {
-      ...
-    } else {
-      ...
-    }
-
-What's an expression? Comparison tests, boolean logic, etc!
-
-The following comparison operators  are supported:
-
-* equality, etc: ==,  !=,  <,  >,  <=,  >=
-* regexp: =~, !~
-* inclusion: in, not in
-
-The following boolean operators are supported:
-
-* and, or, nand, xor
-
-The following unary operators are supported:
-
-* !
-
-Expressions may contain expressions. Expressions may be negated with `!`.
-Expressions may be grouped with parentheses `(...)`. Expressions can be long
-and complex.
-
-For example, if we want to remove the field `secret` if the field
-`action` has a value of `login`:
-
-    filter {
-      if [action] == "login" {
-        mutate { remove => "secret" }
-      }
-    }
-
-The above uses the field reference syntax to get the value of the
-`action` field. It is compared against the text `login` and, if equal,
-allows the mutate filter to delete the field named `secret`.
-
-How about a more complex example?
-
-* alert nagios of any apache events with status 5xx
-* record any 4xx status to elasticsearch
-* record all status code hits via statsd
-
-How about telling nagios of any http event that has a status code of 5xx?
-
-    output {
-      if [type] == "apache" {
-        if [status] =~ /^5\d\d/ {
-          nagios { ...  }
-        } else if [status] =~ /^4\d\d/ {
-          elasticsearch { ... }
-        }
-
-        statsd { increment => "apache.%{status}" }
-      }
-    }
-
-You can also do multiple expressions in a single condition:
-
-    output {
-      # Send production errors to pagerduty
-      if [loglevel] == "ERROR" and [deployment] == "production" {
-        pagerduty {
-          ...
-        }
-      }
-    }
-    
-You can test whether a field was present, regardless of its value:
-
-    if [exception_message] {
-      # If the event has an exception_message field, set the level
-      mutate { add_field => { "level" => "ERROR" } }
-    }
-
-Here are some examples for testing with the in conditional:
-
-    filter {
-      if [foo] in [foobar] {
-        mutate { add_tag => "field in field" }
-      }
-      if [foo] in "foo" {
-        mutate { add_tag => "field in string" }
-      }
-      if "hello" in [greeting] {
-        mutate { add_tag => "string in field" }
-      }
-      if [foo] in ["hello", "world", "foo"] {
-        mutate { add_tag => "field in list" }
-      }
-      if [missing] in [alsomissing] {
-        mutate { add_tag => "shouldnotexist" }
-      }
-      if !("foo" in ["hello", "world"]) {
-        mutate { add_tag => "shouldexist" }
-      }
-    }
-
-Or, to test if grok was successful:
-
-    output {
-      if "_grokparsefailure" not in [tags] {
-        elasticsearch { ... }
-      }
-    }
-
-## Further Reading
-
-For more information, see [the plugin docs index](index)
diff --git a/docs/contrib-plugins.md b/docs/contrib-plugins.md
deleted file mode 100644
index d4adbcee8d2..00000000000
--- a/docs/contrib-plugins.md
+++ /dev/null
@@ -1,59 +0,0 @@
----
-title: Logstash Contrib plugins
-layout: content_right
----
-
-# contrib plugins
-
-As logstash has grown, we've accumulated a massive repository of plugins. Well
-over 100 plugins, it became difficult for the project maintainers to adequately
-support everything effectively.
-
-In order to improve the quality of popular plugins, we've moved the
-less-commonly-used plugins to a separate repository we're calling "contrib".
-Concentrating common plugin usage into core solves a few problems, most notably
-user complaints about the size of logstash releases, support/maintenance costs,
-etc.
-
-It is our intent that this separation will improve life for users. If it
-doesn't, please file a bug so we can work to address it!
-
-If a plugin is available in the 'contrib' package, the documentation for that
-plugin will note this boldly at the top of that plugin's documentation.
-
-Contrib plugins reside in a [separate github project](https://github.com/elasticsearch/logstash-contrib).
-
-# Packaging
-
-At present, the contrib modules are available as a tarball.
-
-# Automated Installation
-
-The `bin/plugin` script will handle the installation for you:
-
-    cd /path/to/logstash
-    bin/plugin install contrib
-
-# Manual Installation
-
-The contrib plugins can be extracted on top of an existing Logstash installation. 
-
-For example, if I've extracted `logstash-%VERSION%.tar.gz` into `/path`, e.g.
- 
-    cd /path
-    tar zxf ~/logstash-%VERSION%.tar.gz
-
-It will have a `/path/logstash-%VERSION%` directory, e.g.
-
-    $ ls
-    logstash-%VERSION%
-
-The method to install the contrib tarball is identical.
-
-    cd /path
-    wget http://download.elasticsearch.org/logstash/logstash/logstash-contrib-%VERSION%.tar.gz
-    tar zxf ~/logstash-contrib-%VERSION%.tar.gz
-
-This will install the contrib plugins in the same directory as the core
-install. These plugins will be available to logstash the next time it starts.
-
diff --git a/docs/docgen.rb b/docs/docgen.rb
deleted file mode 100644
index f64a7f8f853..00000000000
--- a/docs/docgen.rb
+++ /dev/null
@@ -1,250 +0,0 @@
-require "rubygems"
-require "erb"
-require "optparse"
-require "kramdown" # markdown parser
-
-$: << Dir.pwd
-$: << File.join(File.dirname(__FILE__), "..", "lib")
-
-require "logstash/config/mixin"
-require "logstash/inputs/base"
-require "logstash/codecs/base"
-require "logstash/filters/base"
-require "logstash/outputs/base"
-require "logstash/version"
-
-class LogStashConfigDocGenerator
-  COMMENT_RE = /^ *#(?: (.*)| *$)/
-
-  def initialize
-    @rules = {
-      COMMENT_RE => lambda { |m| add_comment(m[1]) },
-      /^ *class.*< *LogStash::(Outputs|Filters|Inputs|Codecs)::(Base|Threadable)/ => \
-        lambda { |m| set_class_description },
-      /^ *config +[^=].*/ => lambda { |m| add_config(m[0]) },
-      /^ *milestone .*/ => lambda { |m| set_milestone(m[0]) },
-      /^ *config_name .*/ => lambda { |m| set_config_name(m[0]) },
-      /^ *flag[( ].*/ => lambda { |m| add_flag(m[0]) },
-      /^ *(class|def|module) / => lambda { |m| clear_comments },
-    }
-
-    if File.exists?("build/contrib_plugins")
-      @contrib_list = File.read("build/contrib_plugins").split("\n")
-    else
-      @contrib_list = []
-    end
-  end
-
-  def parse(string)
-    clear_comments
-    buffer = ""
-    string.split(/\r\n|\n/).each do |line|
-      # Join long lines
-      if line =~ COMMENT_RE
-        # nothing
-      else
-        # Join extended lines
-        if line =~ /(, *$)|(\\$)|(\[ *$)/
-          buffer += line.gsub(/\\$/, "")
-          next
-        end
-      end
-
-      line = buffer + line
-      buffer = ""
-
-      @rules.each do |re, action|
-        m = re.match(line)
-        if m
-          action.call(m)
-        end
-      end # RULES.each
-    end # string.split("\n").each
-  end # def parse
-
-  def set_class_description
-    @class_description = @comments.join("\n")
-    clear_comments
-  end # def set_class_description
-
-  def add_comment(comment)
-    return if comment == "encoding: utf-8"
-    @comments << comment
-  end # def add_comment
-
-  def add_config(code)
-    # I just care about the 'config :name' part
-    code = code.sub(/,.*/, "")
-
-    # call the code, which calls 'config' in this class.
-    # This will let us align comments with config options.
-    name, opts = eval(code)
-
-    # TODO(sissel): This hack is only required until regexp configs
-    # are gone from logstash.
-    name = name.to_s unless name.is_a?(Regexp)
-
-    description = Kramdown::Document.new(@comments.join("\n")).to_html
-    @attributes[name][:description] = description
-    clear_comments
-  end # def add_config
-
-  def add_flag(code)
-    # call the code, which calls 'config' in this class.
-    # This will let us align comments with config options.
-    #p :code => code
-    fixed_code = code.gsub(/ do .*/, "")
-    #p :fixedcode => fixed_code
-    name, description = eval(fixed_code)
-    @flags[name] = description
-    clear_comments
-  end # def add_flag
-
-  def set_config_name(code)
-    name = eval(code)
-    @name = name
-  end # def set_config_name
-
-  def set_milestone(code)
-    @milestone = eval(code)
-  end
-
-  # pretend to be the config DSL and just get the name
-  def config(name, opts={})
-    return name, opts
-  end # def config
-
-  # Pretend to support the flag DSL
-  def flag(*args, &block)
-    name = args.first
-    description = args.last
-    return name, description
-  end # def config
-
-  # pretend to be the config dsl's 'config_name' method
-  def config_name(name)
-    return name
-  end # def config_name
-
-  # pretend to be the config dsl's 'milestone' method
-  def milestone(m)
-    return m
-  end # def milestone
-
-  def clear_comments
-    @comments.clear
-  end # def clear_comments
-
-  def generate(file, settings)
-    @class_description = ""
-    @milestone = ""
-    @comments = []
-    @attributes = Hash.new { |h,k| h[k] = {} }
-    @flags = {}
-
-    # local scoping for the monkeypatch belowg
-    attributes = @attributes
-    # Monkeypatch the 'config' method to capture
-    # Note, this monkeypatch requires us do the config processing
-    # one at a time.
-    #LogStash::Config::Mixin::DSL.instance_eval do
-      #define_method(:config) do |name, opts={}|
-        #p name => opts
-        #attributes[name].merge!(opts)
-      #end
-    #end
-
-    # Loading the file will trigger the config dsl which should
-    # collect all the config settings.
-    load file
-
-    # parse base first
-    parse(File.new(File.join(File.dirname(file), "base.rb"), "r").read)
-
-    # Now parse the real library
-    code = File.new(file).read
-
-    # inputs either inherit from Base or Threadable.
-    if code =~ /\< LogStash::Inputs::Threadable/
-      parse(File.new(File.join(File.dirname(file), "threadable.rb"), "r").read)
-    end
-
-    if code =~ /include LogStash::PluginMixins/
-      mixin = code.gsub(/.*include LogStash::PluginMixins::(\w+)\s.*/m, '\1')
-      mixin.gsub!(/(.)([A-Z])/, '\1_\2')
-      mixin.downcase!
-      parse(File.new(File.join(File.dirname(file), "..", "plugin_mixins", "#{mixin}.rb")).read)
-    end
-
-    parse(code)
-
-    puts "Generating docs for #{file}"
-
-    if @name.nil?
-      $stderr.puts "Missing 'config_name' setting in #{file}?"
-      return nil
-    end
-
-    klass = LogStash::Config::Registry.registry[@name]
-    if klass.ancestors.include?(LogStash::Inputs::Base)
-      section = "input"
-    elsif klass.ancestors.include?(LogStash::Filters::Base)
-      section = "filter"
-    elsif klass.ancestors.include?(LogStash::Outputs::Base)
-      section = "output"
-    elsif klass.ancestors.include?(LogStash::Codecs::Base)
-      section = "codec"
-    end
-
-    template_file = File.join(File.dirname(__FILE__), "plugin-doc.html.erb")
-    template = ERB.new(File.new(template_file).read, nil, "-")
-
-    is_contrib_plugin = @contrib_list.include?(file)
-
-    # descriptions are assumed to be markdown
-    description = Kramdown::Document.new(@class_description).to_html
-
-    klass.get_config.each do |name, settings|
-      @attributes[name].merge!(settings)
-    end
-    sorted_attributes = @attributes.sort { |a,b| a.first.to_s <=> b.first.to_s }
-    klassname = LogStash::Config::Registry.registry[@name].to_s
-    name = @name
-
-    synopsis_file = File.join(File.dirname(__FILE__), "plugin-synopsis.html.erb")
-    synopsis = ERB.new(File.new(synopsis_file).read, nil, "-").result(binding)
-
-    if settings[:output]
-      dir = File.join(settings[:output], section + "s")
-      path = File.join(dir, "#{name}.html")
-      Dir.mkdir(settings[:output]) if !File.directory?(settings[:output])
-      Dir.mkdir(dir) if !File.directory?(dir)
-      File.open(path, "w") do |out|
-        html = template.result(binding)
-        html.gsub!("%VERSION%", LOGSTASH_VERSION)
-        html.gsub!("%PLUGIN%", @name)
-        out.puts(html)
-      end
-    else
-      puts template.result(binding)
-    end
-  end # def generate
-
-end # class LogStashConfigDocGenerator
-
-if __FILE__ == $0
-  opts = OptionParser.new
-  settings = {}
-  opts.on("-o DIR", "--output DIR",
-          "Directory to output to; optional. If not specified,"\
-          "we write to stdout.") do |val|
-    settings[:output] = val
-  end
-
-  args = opts.parse(ARGV)
-
-  args.each do |arg|
-    gen = LogStashConfigDocGenerator.new
-    gen.generate(arg, settings)
-  end
-end
diff --git a/docs/extending/example-add-a-new-filter.md b/docs/extending/example-add-a-new-filter.md
deleted file mode 100644
index 6b613226735..00000000000
--- a/docs/extending/example-add-a-new-filter.md
+++ /dev/null
@@ -1,108 +0,0 @@
----
-title: How to extend - logstash
-layout: content_right
----
-# Add a new filter
-
-This document shows you how to add a new filter to logstash.
-
-For a general overview of how to add a new plugin, see [the extending
-logstash](.) overview.
-
-## Write code.
-
-Let's write a 'hello world' filter. This filter will replace the 'message' in
-the event with "Hello world!"
-
-First, logstash expects plugins in a certain directory structure: `logstash/TYPE/PLUGIN_NAME.rb`
-
-Since we're creating a filter, let's mkdir this:
-
-    mkdir -p logstash/filters/
-    cd logstash/filters
-
-Now add the code:
-
-    # Call this file 'foo.rb' (in logstash/filters, as above)
-    require "logstash/filters/base"
-    require "logstash/namespace"
-
-    class LogStash::Filters::Foo < LogStash::Filters::Base
-
-      # Setting the config_name here is required. This is how you
-      # configure this filter from your logstash config.
-      #
-      # filter {
-      #   foo { ... }
-      # }
-      config_name "foo"
-
-      # New plugins should start life at milestone 1.
-      milestone 1
-
-      # Replace the message with this value.
-      config :message, :validate => :string
-
-      public
-      def register
-        # nothing to do
-      end # def register
-
-      public
-      def filter(event)
-        # return nothing unless there's an actual filter event
-        return unless filter?(event)
-        if @message
-          # Replace the event message with our message as configured in the
-          # config file.
-          event["message"] = @message
-        end
-        # filter_matched should go in the last line of our successful code 
-        filter_matched(event)
-      end # def filter
-    end # class LogStash::Filters::Foo
-
-## Add it to your configuration
-
-For this simple example, let's just use stdin input and stdout output.
-The config file looks like this:
-
-    input { 
-      stdin { type => "foo" } 
-    }
-    filter {
-      if [type] == "foo" {
-        foo {
-          message => "Hello world!"
-        }
-      }
-    }
-    output {
-      stdout { }
-    }
-
-Call this file 'example.conf'
-
-## Tell logstash about it.
-
-Depending on how you installed logstash, you have a few ways of including this
-plugin.
-
-You can use the agent flag --pluginpath flag to specify where the root of your
-plugin tree is. In our case, it's the current directory.
-
-    % bin/logstash --pluginpath your/plugin/root -f example.conf
-
-## Example running
-
-In the example below, I typed in "the quick brown fox" after running the java
-command.
-
-    % bin/logstash -f example.conf
-    the quick brown fox   
-    2011-05-12T01:05:09.495000Z stdin://snack.home/: Hello world!
-
-The output is the standard logstash stdout output, but in this case our "the
-quick brown fox" message was replaced with "Hello world!"
-
-All done! :)
diff --git a/docs/extending/index.md b/docs/extending/index.md
deleted file mode 100644
index 4a4ab66d877..00000000000
--- a/docs/extending/index.md
+++ /dev/null
@@ -1,91 +0,0 @@
----
-title: How to extend - logstash
-layout: content_right
----
-# Extending logstash
-
-You can add your own input, output, or filter plugins to logstash.
-
-If you're looking to extend logstash today, please look at the existing plugins.
-
-## Good examples of plugins
-
-* [inputs/tcp](https://github.com/logstash/logstash/blob/master/lib/logstash/inputs/tcp.rb)
-* [filters/multiline](https://github.com/logstash/logstash/blob/master/lib/logstash/filters/multiline.rb)
-* [outputs/mongodb](https://github.com/logstash/logstash/blob/master/lib/logstash/outputs/mongodb.rb)
-
-## Common concepts
-
-* The `config_name` sets the name used in the config file.
-* The `milestone` sets the milestone number of the plugin. See <../plugin-milestones> for more info.
-* The `config` lines define config options.
-* The `register` method is called per plugin instantiation. Do any of your initialization here.
-
-### Required modules
-
-All plugins should require the Logstash module.
-
-    require 'logstash/namespace'
-
-### Plugin name
-
-Every plugin must have a name set with the `config_name` method. If this
-is not specified plugins will fail to load with an error.
-
-### Milestones
-
-Every plugin needs a milestone set using `milestone`. See
-<../plugin-milestones> for more info.
-
-### Config lines
-
-The `config` lines define configuration options and are constructed like
-so:
-
-    config :host, :validate => :string, :default => "0.0.0.0"
-
-The name of the option is specified, here `:host` and then the
-attributes of the option. They can include `:validate`, `:default`,
-`:required` (a Boolean `true` or `false`), `:deprecated` (also a
-Boolean), and `:obsolete` (a String value).  
- 
-## Inputs
-
-All inputs require the LogStash::Inputs::Base class:
-
-    require 'logstash/inputs/base'
- 
-Inputs have two methods: `register` and `run`.
-
-* Each input runs as its own thread.
-* The `run` method is expected to run-forever.
-
-## Filters
-
-All filters require the LogStash::Filters::Base class:
-
-    require 'logstash/filters/base'
- 
-Filters have two methods: `register` and `filter`.
-
-* The `filter` method gets an event. 
-* Call `event.cancel` to drop the event.
-* To modify an event, simply make changes to the event you are given.
-* The return value is ignored.
-
-## Outputs
-
-All outputs require the LogStash::Outputs::Base class:
-
-    require 'logstash/outputs/base'
- 
-Outputs have two methods: `register` and `receive`.
-
-* The `register` method is called per plugin instantiation. Do any of your initialization here.
-* The `receive` method is called when an event gets pushed to your output
-
-## Example: a new filter
-
-Learn by example how to [add a new filter to logstash](example-add-a-new-filter)
-
-
diff --git a/docs/flags.md b/docs/flags.md
deleted file mode 100644
index e7777f372fe..00000000000
--- a/docs/flags.md
+++ /dev/null
@@ -1,45 +0,0 @@
----
-title: Command-line flags - logstash
-layout: content_right
----
-# Command-line flags
-
-## Agent
-
-The logstash agent has the following flags (also try using the '--help' flag)
-
-<dl>
-<dt> -f, --config CONFIGFILE </dt>
-<dd> Load the logstash config from a specific file, directory, or a
-wildcard. If given a directory or wildcard, config files will be read
-from the directory in alphabetical order. </dd>
-<dt> -e CONFIGSTRING </dt>
-<dd> Use the given string as the configuration data. Same syntax as the
-config file. If not input is specified, 'stdin { type => stdin }' is
-default. If no output is specified, 'stdout { debug => true }}' is
-default. </dd>
-<dt> -w, --filterworkers COUNT </dt>
-<dd> Run COUNT filter workers (default: 1) </dd>
-<dt> -l, --log FILE </dt>
-<dd> Log to a given path. Default is to log to stdout </dd>
-<dt> --verbose </dt>
-<dd> Increase verbosity to the first level, less verbose.</dd>
-<dt> --debug </dt>
-<dd> Increase verbosity to the last level, more verbose.</dd>
-<dt> -v  </dt>
-<dd> *DEPRECATED: see --verbose/debug* Increase verbosity. There are multiple levels of verbosity available with
-'-vv' currently being the highest </dd>
-<dt> --pluginpath PLUGIN_PATH </dt>
-<dd> A colon-delimted path to find other logstash plugins in </dd>
-</dl>
-
-
-## Web
-
-<dl>
-<dt> -a, --address ADDRESS </dt>
-<dd>Address on which to start webserver. Default is 0.0.0.0.</dd>
-<dt> -p, --port PORT</dt>
-<dd>Port on which to start webserver. Default is 9292.</dd>
-</dl>
-
diff --git a/docs/generate_index.rb b/docs/generate_index.rb
deleted file mode 100644
index 6e7bed8e45b..00000000000
--- a/docs/generate_index.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-#!/usr/bin/env ruby
-
-require "erb"
-
-if ARGV.size != 1
-  $stderr.puts "No path given to search for plugin docs"
-  $stderr.puts "Usage: #{$0} plugin_doc_dir"
-  exit 1
-end
-
-def plugins(glob)
-  files = Dir.glob(glob)
-  names = files.collect { |f| File.basename(f).gsub(".html", "") }
-  return names.sort
-end # def plugins
-
-basedir = ARGV[0]
-docs = {
-  "inputs" => plugins(File.join(basedir, "inputs/*.html")),
-  "codecs" => plugins(File.join(basedir, "codecs/*.html")),
-  "filters" => plugins(File.join(basedir, "filters/*.html")),
-  "outputs" => plugins(File.join(basedir, "outputs/*.html")),
-}
-
-template_path = File.join(File.dirname(__FILE__), "index.html.erb")
-template = File.new(template_path).read
-erb = ERB.new(template, nil, "-")
-puts erb.result(binding)
diff --git a/docs/learn.md b/docs/learn.md
deleted file mode 100644
index 2599edc4494..00000000000
--- a/docs/learn.md
+++ /dev/null
@@ -1,46 +0,0 @@
----
-title: Learn - logstash
-layout: content_right
----
-# What is Logstash?
-
-Logstash is a tool for managing your logs.
-
-It helps you take logs and other event data from your systems and move it into
-a central place. Logstash is open source and completely free. You can find
-support on the discussion forum and on IRC.
-
-For an overview of Logstash and why you would use it, you should watch the
-presentation I gave at CarolinaCon 2011: 
-[video here](http://carolinacon.blip.tv/file/5105901/). This presentation covers
-Logstash, how you can use it, some alternatives, logging best practices,
-parsing tools, etc. Video also below:
-
-<!--
-<embed src="http://blip.tv/play/gvE9grjcdQI" type="application/x-shockwave-flash" width="480" height="296" allowscriptaccess="always" allowfullscreen="true"></embed>
-
-The slides are available online here: [slides](http://goo.gl/68c62). The slides
-include speaker notes (click 'actions' then 'speaker notes').
--->
-<iframe width="480" height="296" src="http://www.youtube.com/embed/RuUFnog29M4" frameborder="0" allowfullscreen="allowfullscreen"></iframe>
-
-The slides are available online here: [slides](http://semicomplete.com/presentations/logstash-puppetconf-2012/).
-
-## Getting Help
-
-There's [documentation](.) here on this site. If that isn't sufficient, you can
-use the discussion [forum](https://discuss.elastic.co/c/logstash). Further, there is also
-an IRC channel - #logstash on irc.freenode.org.
-
-If you find a bug or have a feature request, file them
-on [github](https://github.com/elasticsearch/logstas/issues). (Honestly though, if you prefer email or irc
-for such things, that works for me, too.)
-
-## Download It
-
-[Download logstash-%VERSION%](https://download.elastic.co/logstash/logstash/logstash-%VERSION%.tar.gz)
-
-## What's next?
-
-Try this [guide](tutorials/getting-started-with-logstash) for a simple
-real-world example getting started using Logstash.
diff --git a/docs/life-of-an-event.md b/docs/life-of-an-event.md
deleted file mode 100644
index f7dd640995b..00000000000
--- a/docs/life-of-an-event.md
+++ /dev/null
@@ -1,109 +0,0 @@
----
-title: the life of an event - logstash
-layout: content_right
----
-# the life of an event
-
-The logstash agent is an event pipeline.
-
-## The Pipeline
-
-The logstash agent is a processing pipeline with 3 stages: inputs -> filters ->
-outputs. Inputs generate events, filters modify them, outputs ship them
-elsewhere.
-
-Internal to logstash, events are passed from each phase using internal queues.
-It is implemented with a 'SizedQueue' in Ruby. SizedQueue allows a bounded
-maximum of items in the queue such that any writes to the queue will block if
-the queue is full at maximum capacity.
-
-Logstash sets each queue size to 20. This means only 20 events can be pending
-into the next phase - this helps reduce any data loss and in general avoids
-logstash trying to act as a data storage system. These internal queues are not
-for storing messages long-term.
-
-## Fault Tolerance
-
-Starting at outputs, here's what happens when things break.
-
-An output can fail or have problems because of some downstream cause, such as
-full disk, permissions problems, temporary network failures, or service
-outages. Most outputs should keep retrying to ship any events that were
-involved in the failure.
-
-If an output is failing, the output thread will wait until this output is
-healthy again and able to successfully send the message. Therefore, the output
-queue will stop being read from by this output and will eventually fill up with
-events and block new events from being written to this queue.
-
-A full output queue means filters will block trying to write to the output
-queue. Because filters will be stuck, blocked writing to the output queue, they
-will stop reading from the filter queue which will eventually cause the filter
-queue (input -> filter) to fill up.
-
-A full filter queue will cause inputs to block when writing to the filters.
-This will cause each input to block, causing each input to stop processing new
-data from wherever that input is getting new events.
-
-In ideal circumstances, this will behave similarly to when the tcp window
-closes to 0, no new data is sent because the receiver hasn't finished
-processing the current queue of data, but as soon as the downstream (output)
-problem is resolved, messages will begin flowing again..
-
-## Thread Model
-
-The thread model in logstash is currently:
-
-    input threads | filter worker threads | output worker
-
-Filters are optional, so you will have this model if you have no filters
-defined:
-
-    input threads | output worker
-
-Each input runs in a thread by itself. This allows busier inputs to not be
-blocked by slower ones, etc. It also allows for easier containment of scope
-because each input has a thread.
-
-The filter thread model is a 'worker' model where each worker receives an event
-and applies all filters, in order, before emitting that to the output queue.
-This allows scalability across CPUs because many filters are CPU intensive
-(permitting that we have thread safety). 
-
-The default number of filter workers is 1, but you can increase this number
-with the '-w' flag on the agent.
-
-The output worker model is currently a single thread. Outputs will receive
-events in the order they are defined in the config file. 
-
-Outputs may decide to buffer events temporarily before publishing them,
-possibly in a separate thread. One example of this is the elasticsearch output
-which will buffer events and flush them all at once, in a separate thread. This
-mechanism (buffering many events + writing in a separate thread) can improve
-performance so the logstash pipeline isn't stalled waiting for a response from
-elasticsearch.
-
-## Consequences and Expectations
-
-Small queue sizes mean that logstash simply blocks and stalls safely during
-times of load or other temporary pipeline problems. There are two alternatives
-to this - unlimited queue length and dropping messages. Unlimited queues grow
-grow unbounded and eventually exceed memory causing a crash which loses all of
-those messages. Dropping messages is also an undesirable behavior in most cases.
-
-At a minimum, logstash will have probably 3 threads (2 if you have no filters).
-One input, one filter worker, and one output thread each.
-
-If you see logstash using multiple CPUs, this is likely why. If you want to
-know more about what each thread is doing, you should read this:
-<http://www.semicomplete.com/blog/geekery/debugging-java-performance.html>.
-
-Threads in java have names, and you can use jstack and top to figure out who is
-using what resources. The URL above will help you learn how to do this.
-
-On Linux platforms, logstash will label all the threads it can with something
-descriptive. Inputs will show up as "<inputname" and filter workers as
-"|worker" and outputs as ">outputworker" (or something similar).  Other threads
-may be labeled as well, and are intended to help you identify their purpose
-should you wonder why they are consuming resources!
-
diff --git a/docs/logging-tool-comparisons.md b/docs/logging-tool-comparisons.md
deleted file mode 100644
index a39fea0546e..00000000000
--- a/docs/logging-tool-comparisons.md
+++ /dev/null
@@ -1,60 +0,0 @@
----
-title: Logging tools comparisons - logstash
-layout: content_right
----
-# Logging tools comparison
-
-The information below is provided as "best effort" and is not strictly intended
-as a complete source of truth. If the information below is unclear or incorrect, please
-email the logstash-users list (or send a pull request with the fix) :)
-
-Where feasible, this document will also provide information on how you can use
-logstash with these other projects.
-
-# logstash
-
-Primary goal: Make log/event data and analytics accessible.
-
-Overview: Where your logs come from, how you store them, or what you do with
-them is up to you. Logstash exists to help make such actions easier and faster.
-
-It provides you a simple event pipeline for taking events and logs from any
-input, manipulating them with filters, and sending them to any output. Inputs
-can be files, network, message brokers, etc. Filters are date and string
-parsers, grep-like, etc. Outputs are data stores (elasticsearch, mongodb, etc),
-message systems (rabbitmq, stomp, etc), network (tcp, syslog), etc.
-
-It also provides a web interface for doing search and analytics on your
-logs.
-
-# graylog2
-
-[http://graylog2.org/](http://graylog2.org)
-
-_Overview to be written_
-
-You can use graylog2 with logstash by using the 'gelf' output to send logstash
-events to a graylog2 server. This gives you logstash's excellent input and
-filter features while still being able to use the graylog2 web interface.
-
-# whoops
-
-[whoops site](http://www.whoopsapp.com/)
-
-_Overview to be written_
-
-A logstash output to whoops is coming soon - <https://logstash.jira.com/browse/LOGSTASH-133>
-
-# flume
-
-[flume site](https://github.com/cloudera/flume/wiki)
-
-Flume is primarily a transport system aimed at reliably copying logs from
-application servers to HDFS.
-
-You can use it with logstash by having a syslog sink configured to shoot logs
-at a logstash syslog input.
-
-# scribe
-
-_Overview to be written_
diff --git a/docs/plugin-doc.asciidoc.erb b/docs/plugin-doc.asciidoc.erb
index e00319d537a..55fd81cff48 100644
--- a/docs/plugin-doc.asciidoc.erb
+++ b/docs/plugin-doc.asciidoc.erb
@@ -3,7 +3,7 @@
 === <%= name %>
 
 <% unless default_plugin %>
-NOTE: This is a community-maintained plugin! It does not ship with Logstash by default, but it is easy to install by running `bin/plugin install logstash-<%= section %>-<%= plugin_name %>`.
+NOTE: This is a community-maintained plugin! It does not ship with Logstash by default, but it is easy to install by running `bin/logstash-plugin install logstash-<%= section %>-<%= plugin_name %>`.
 <% end %>
 
 <%= description %>
diff --git a/docs/plugin-milestones.md b/docs/plugin-milestones.md
deleted file mode 100644
index 5d72e9ac472..00000000000
--- a/docs/plugin-milestones.md
+++ /dev/null
@@ -1,41 +0,0 @@
----
-title: Plugin Milestones - logstash
-layout: content_right
----
-# Plugin Milestones
-
-Plugins (inputs/outputs/filters/codecs) have a milestone label in logstash.
-This is to provide an indicator to the end-user as to the kinds of changes
-a given plugin could have between logstash releases.
-
-The desire here is to allow plugin developers to quickly iterate on possible
-new plugins while conveying to the end-user a set of expectations about that
-plugin.
-
-## Milestone 1
-
-Plugins at this milestone need your feedback to improve! Plugins at this
-milestone may change between releases as the community figures out the best way
-for the plugin to behave and be configured.
-
-## Milestone 2
-
-Plugins at this milestone are more likely to have backwards-compatibility to
-previous releases than do Milestone 1 plugins. This milestone also indicates
-a greater level of in-the-wild usage by the community than the previous
-milestone.
-
-## Milestone 3
-
-Plugins at this milestone have strong promises towards backwards-compatibility.
-This is enforced with automated tests to ensure behavior and configuration are
-consistent across releases.
-
-## Milestone 0
-
-This milestone appears at the bottom of the page because it is very
-infrequently used.
-
-This milestone marker is used to generally indicate that a plugin has no
-active code maintainer nor does it have support from the community in terms
-of getting help.
diff --git a/docs/release-notes.md b/docs/release-notes.md
deleted file mode 100644
index 254fee6b6c7..00000000000
--- a/docs/release-notes.md
+++ /dev/null
@@ -1,64 +0,0 @@
----
-title: release notes for %VERSION%
-layout: content_right
----
-
-# %VERSION% - Release Notes
-
-This document is targeted at existing users of Logstash who are upgrading from
-an older version to version %VERSION%. This document is intended to supplement
-a the [changelog
-file](https://github.com/elasticsearch/logstash/blob/v%VERSION%/CHANGELOG) by
-providing more details on certain changes.
-
-### tarball 
-
-With Logstash 1.4.0, we stopped shipping the jar file and started shipping a
-tarball instead.
-
-Past releases have been a single jar file which included all Ruby and Java
-library dependencies to eliminate deployment pains. We still ship all
-the dependencies for you! The jar file served us well, but over time we found
-Java’s default heap size, garbage collector, and other settings weren’t well
-suited to Logstash.
-
-In order to provide better Java defaults, we’ve changed to releasing a tarball
-(.tar.gz) that includes all the same dependencies. What does this mean to you?
-Instead of running `java -jar logstash.jar ...` you run `bin/logstash ...` (for
-Windows users, `bin/logstash.bat`)
-
-One pleasant side effect of using a tarball is that the Logstash code itself is
-much more accessible and able to satisfy any curiosity you may have.
-
-The new way to do things is:
-
-* Download logstash tarball
-* Unpack it (`tar -zxf logstash-%VERSION%.tar.gz`)
-* `cd logstash-%VERSION%`
-% Run it: `bin/logstash ...`
-
-The old way to run logstash of `java -jar logstash.jar` is now replaced with
-`bin/logstash`. The command line arguments are exactly the same after that.
-For example:
-
-    # Old way:
-    `% java -jar logstash-1.3.3-flatjar.jar agent -f logstash.conf`
-
-    # New way:
-    `% bin/logstash agent -f logstash.conf`
-
-### plugins
-
-Logstash has grown brilliantly over the past few years with great contributions
-from the community. Now having 165 plugins, it became hard for us (the Logstash
-engineering team) to reliably support all the wonderful technologies in each
-contributed plugin. We combed through all the plugins and picked the ones we
-felt strongly we could support, and those now ship by default with Logstash.
-
-All the other plugins are now available in a contrib package. All plugins
-continue to be open source and free, of course! Installing plugins is very easy:
-
-....
-    % cd /path/to/logstash-%VERSION%/
-    % bin/plugin install [PLUGIN_NAME]
-....
diff --git a/docs/repositories.md b/docs/repositories.md
deleted file mode 100644
index 21075147db7..00000000000
--- a/docs/repositories.md
+++ /dev/null
@@ -1,35 +0,0 @@
----
-title: repositories - logstash
-layout: content_right
----
-# Logstash repositories
-
-We also have Logstash available as APT and YUM repositories.
-
-Our public signing key can be found on the [Elasticsearch packages apt GPG signing key page](https://packages.elasticsearch.org/GPG-KEY-elasticsearch)
-
-## Apt based distributions
-
-Add the key:
-
-    wget -O - https://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
-
-Add the repo to /etc/apt/sources.list
-
-    deb http://packages.elasticsearch.org/logstash/1.4/debian stable main
-
-
-## YUM based distributions
-
-Add the key:
-
-    rpm --import https://packages.elasticsearch.org/GPG-KEY-elasticsearch
-
-Add the repo to /etc/yum.repos.d/ directory
-
-    [logstash-1.4]
-    name=logstash repository for 1.4.x packages
-    baseurl=https://packages.elasticsearch.org/logstash/1.4/centos
-    gpgcheck=1
-    gpgkey=https://packages.elasticsearch.org/GPG-KEY-elasticsearch
-    enabled=1
diff --git a/docs/static/advanced-pipeline.asciidoc b/docs/static/advanced-pipeline.asciidoc
new file mode 100644
index 00000000000..f38bc3aa1c4
--- /dev/null
+++ b/docs/static/advanced-pipeline.asciidoc
@@ -0,0 +1,584 @@
+[[advanced-pipeline]]
+=== Parsing Logs with Logstash
+
+In <<first-event>>, you created a basic Logstash pipeline to test your Logstash setup. In the real world, a Logstash
+pipeline is a bit more complex: it typically has one or more input, filter, and output plugins.  
+
+In this section, you create a Logstash pipeline that takes Apache web logs as input, parses those
+logs to create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Rather than
+defining the pipeline configuration at the command line, you'll define the pipeline in a config file. 
+
+The following text represents the skeleton of a configuration pipeline:
+
+[source,shell]
+--------------------------------------------------------------------------------
+# The # character at the beginning of a line indicates a comment. Use
+# comments to describe your configuration.
+input {
+}
+# The filter part of this file is commented out to indicate that it is
+# optional.
+# filter {
+#
+# }
+output {
+}
+--------------------------------------------------------------------------------
+
+This skeleton is non-functional, because the input and output sections don’t have any valid options defined. 
+
+To get started, copy and paste the skeleton configuration pipeline into a file named `first-pipeline.conf` in your home
+Logstash directory. Then go https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here] to
+download the sample data set used in this example. Unpack the file.
+
+[float]
+[[configuring-file-input]]
+==== Configuring Logstash for File Input
+
+NOTE: This example uses the file input plugin for convenience. To tail files in the real world, you'll use
+Filebeat to ship log events to Logstash. You learn how to <<configuring-lsf,configure the Filebeat input plugin>> later
+when you build a more sophisticated pipeline.
+
+To begin your Logstash pipeline, configure the Logstash instance to read from a file by using the
+{logstash}plugins-inputs-file.html[`file`] input plugin.
+
+Edit the `first-pipeline.conf` file and replace the entire `input` section with the following text:
+
+[source,json]
+--------------------------------------------------------------------------------
+input {
+    file {
+        path => "/path/to/file/*.log"
+        start_position => beginning <1>
+        ignore_older => 0 <2>
+    }
+}
+--------------------------------------------------------------------------------
+
+<1> The default behavior of the file input plugin is to monitor a file for new information, in a manner similar to the
+UNIX `tail -f` command. To change this default behavior and process the entire file, we need to specify the position
+where Logstash starts processing the file.
+<2> The default behavior of the file input plugin is to ignore files whose last modification is greater than 86400s. To change this default behavior and process the tutorial file (which is probably much older than a day), we need to configure Logstash so that it does not ignore old files.
+
+Replace `/path/to/file` with the absolute path to the location of `logstash-tutorial.log` in your file system.
+
+[float]
+[[configuring-grok-filter]]
+==== Parsing Web Logs with the Grok Filter Plugin
+
+The {logstash}plugins-filters-grok.html[`grok`] filter plugin is one of several plugins that are available by default in
+Logstash. For details on how to manage Logstash plugins, see the <<working-with-plugins,reference documentation>> for
+the plugin manager.
+
+The `grok` filter plugin enables you to parse the unstructured log data into something structured and queryable.
+
+Because the `grok` filter plugin looks for patterns in the incoming log data, configuring the plugin requires you to
+make decisions about how to identify the patterns that are of interest to your use case. A representative line from the
+web server log sample looks like this:
+
+[source,shell]
+--------------------------------------------------------------------------------
+83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png
+HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel
+Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+--------------------------------------------------------------------------------
+
+The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. To parse the data, you can use the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
+
+[horizontal]
+*Information*:: *Field Name*
+IP Address:: `clientip`
+User ID:: `ident`
+User Authentication:: `auth`
+timestamp:: `timestamp`
+HTTP Verb:: `verb`
+Request body:: `request`
+HTTP Version:: `httpversion`
+HTTP Status Code:: `response`
+Bytes served:: `bytes`
+Referrer URL:: `referrer`
+User agent:: `agent`
+
+Edit the `first-pipeline.conf` file and replace the entire `filter` section with the following text:
+
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+}
+--------------------------------------------------------------------------------
+
+After processing the log file with the grok pattern, the sample line will have the following JSON representation:
+
+[source,json]
+--------------------------------------------------------------------------------
+{
+"clientip" : "83.149.9.216",
+"ident" : ,
+"auth" : ,
+"timestamp" : "04/Jan/2015:05:13:42 +0000",
+"verb" : "GET",
+"request" : "/presentations/logstash-monitorama-2013/images/kibana-search.png",
+"httpversion" : "HTTP/1.1",
+"response" : "200",
+"bytes" : "203023",
+"referrer" : "http://semicomplete.com/presentations/logstash-monitorama-2013/",
+"agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+}
+--------------------------------------------------------------------------------
+
+[float]
+[[configuring-geoip-plugin]]
+==== Enhancing Your Data with the Geoip Filter Plugin
+
+In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing
+data. As an example, the {logstash}plugins-filters-geoip.html[`geoip`] plugin looks up IP addresses, derives geographic
+location information from the addresses, and adds that location information to the logs.
+
+Configure your Logstash instance to use the `geoip` filter plugin by adding the following lines to the `filter` section
+of the `first-pipeline.conf` file:
+
+[source,json]
+--------------------------------------------------------------------------------
+    geoip {
+        source => "clientip"
+    }
+--------------------------------------------------------------------------------
+
+The `geoip` plugin configuration requires you to specify the name of the source field that contains the IP address to look up. In this example, the `clientip` field contains the IP address.
+
+Since filters are evaluated in sequence, make sure that the `geoip` section is after the `grok` section of 
+the configuration file and that both the `grok` and `geoip` sections are nested within the `filter` section 
+like this:
+
+[source,json]
+--------------------------------------------------------------------------------
+ filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+    geoip {
+        source => "clientip"
+    }
+--------------------------------------------------------------------------------
+
+
+[float]
+[[indexing-parsed-data-into-elasticsearch]]
+==== Indexing Your Data into Elasticsearch
+
+Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
+Elasticsearch cluster. Edit the `first-pipeline.conf` file and replace the entire `output` section with the following
+text:
+
+[source,json]
+--------------------------------------------------------------------------------
+output {
+    elasticsearch {
+        hosts => [ "localhost:9200" ]
+    }
+}
+--------------------------------------------------------------------------------
+
+With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes that
+Logstash and Elasticsearch are running on the same instance. You can specify a remote Elasticsearch instance by using
+the `hosts` configuration to specify something like `hosts => "es-machine:9092"`.
+
+[float]
+[[testing-initial-pipeline]]
+===== Testing Your Initial Pipeline
+
+At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
+something like this:
+
+[source,json]
+--------------------------------------------------------------------------------
+input {
+    file {
+        path => "/Users/myusername/tutorialdata/*.log"
+        start_position => beginning
+        ignore_older => 0 
+    }
+}
+filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+    geoip {
+        source => "clientip"
+    }
+}
+output {
+    elasticsearch {
+        hosts => "localhost:9200"
+    }
+}
+--------------------------------------------------------------------------------
+
+To verify your configuration, run the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f first-pipeline.conf --config.test_and_exit
+--------------------------------------------------------------------------------
+
+The `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file
+passes the configuration test, start Logstash with the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f first-pipeline.conf
+--------------------------------------------------------------------------------
+
+Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=response=200'
+--------------------------------------------------------------------------------
+
+Replace $DATE with the current date, in YYYY.MM.DD format.
+
+We get multiple hits back. For example:
+
+[source,json]
+--------------------------------------------------------------------------------
+{
+  "took" : 4,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 98,
+    "max_score" : 4.833623,
+    "hits" : [ {
+      "_index" : "logstash-2016.05.27",
+      "_type" : "logs",
+      "_id" : "AVT0nBiGe_tzyi1erg7-",
+      "_score" : 4.833623,
+      "_source" : {
+        "request" : "/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
+        "agent" : "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+        "geoip" : {
+          "timezone" : "Europe/Moscow",
+          "ip" : "83.149.9.216",
+          "latitude" : 55.7522,
+          "continent_code" : "EU",
+          "city_name" : "Moscow",
+          "country_code2" : "RU",
+          "country_name" : "Russia",
+          "dma_code" : null,
+          "country_code3" : "RU",
+          "region_name" : "Moscow",
+          "location" : [ 37.6156, 55.7522 ],
+          "postal_code" : "101194",
+          "longitude" : 37.6156,
+          "region_code" : "MOW"
+        },
+        "auth" : "-",
+        "ident" : "-",
+        "verb" : "GET",
+        "message" : "83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+        "referrer" : "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
+        "@timestamp" : "2016-05-27T23:45:50.828Z",
+        "response" : "200",
+        "bytes" : "52878",
+        "clientip" : "83.149.9.216",
+        "@version" : "1",
+        "host" : "myexamplehost",
+        "httpversion" : "1.1",
+        "timestamp" : "04/Jan/2015:05:13:45 +0000"
+      }
+    }, 
+    ...
+--------------------------------------------------------------------------------
+
+Try another search for the geographic information derived from the IP address:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=geoip.city_name=Buffalo'
+--------------------------------------------------------------------------------
+
+Replace $DATE with the current date, in YYYY.MM.DD format.
+
+A few log entries come from Buffalo, so the query produces the following response:
+
+[source,json]
+--------------------------------------------------------------------------------
+{
+  "took" : 2,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 3,
+    "max_score" : 1.0520113,
+    "hits" : [ {
+      "_index" : "logstash-2016.05.27",
+      "_type" : "logs",
+      "_id" : "AVT0nBiHe_tzyi1erg9T",
+      "_score" : 1.0520113,
+      "_source" : {
+        "request" : "/blog/geekery/solving-good-or-bad-problems.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29",
+        "agent" : "\"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\"",
+        "geoip" : {
+          "timezone" : "America/New_York",
+          "ip" : "198.46.149.143",
+          "latitude" : 42.9864,
+          "continent_code" : "NA",
+          "city_name" : "Buffalo",
+          "country_code2" : "US",
+          "country_name" : "United States",
+          "dma_code" : 514,
+          "country_code3" : "US",
+          "region_name" : "New York",
+          "location" : [ -78.7279, 42.9864 ],
+          "postal_code" : "14221",
+          "longitude" : -78.7279,
+          "region_code" : "NY"
+        },
+        "auth" : "-",
+        "ident" : "-",
+        "verb" : "GET",
+        "message" : "198.46.149.143 - - [04/Jan/2015:05:29:13 +0000] \"GET /blog/geekery/solving-good-or-bad-problems.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1\" 200 10756 \"-\" \"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\"",
+        "referrer" : "\"-\"",
+        "@timestamp" : "2016-05-27T23:45:50.836Z",
+        "response" : "200",
+        "bytes" : "10756",
+        "clientip" : "198.46.149.143",
+        "@version" : "1",
+        "host" : "myexamplehost",
+        "httpversion" : "1.1",
+        "timestamp" : "04/Jan/2015:05:29:13 +0000"
+      }
+    }, 
+    ...
+--------------------------------------------------------------------------------
+
+[[multiple-input-output-plugins]]
+=== Stitching Together Multiple Input and Output Plugins
+
+The information you need to manage often comes from several disparate sources, and use cases can require multiple
+destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these
+requirements.
+
+In this section, you create a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
+sends the information to an Elasticsearch cluster as well as writing the information directly to a file.
+
+[float]
+[[twitter-configuration]]
+==== Reading from a Twitter Feed
+
+To add a Twitter feed, you use the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin. To
+configure the plugin, you need several pieces of information:
+
+* A _consumer_ key, which uniquely identifies your Twitter app.
+* A _consumer secret_, which serves as the password for your Twitter app.
+* One or more _keywords_ to search in the incoming feed. The example shows using "cloud" as a keyword, but you can use whatever you want.
+* An _oauth token_, which identifies the Twitter account using this app.
+* An _oauth token secret_, which serves as the password of the Twitter account.
+
+Visit https://dev.twitter.com/apps[https://dev.twitter.com/apps] to set up a Twitter account and generate your consumer
+key and secret, as well as your access token and secret. See the docs for the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin if you're not sure how to generate these keys. 
+
+Like you did earlier when you worked on <<advanced-pipeline>>, create a config file (called `second-pipeline.conf`) that
+contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make
+sure you pass in the correct config file name when you run Logstash. 
+
+Add the following lines to the `input` section of the `second-pipeline.conf` file, substituting your values for the 
+placeholder values shown here:
+
+[source,json]
+--------------------------------------------------------------------------------
+    twitter {
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
+    }
+--------------------------------------------------------------------------------
+
+[float]
+[[configuring-lsf]]
+==== The Filebeat Client
+
+The https://github.com/elastic/beats/tree/master/filebeat[Filebeat] client is a lightweight, resource-friendly tool that
+collects logs from files on the server and forwards these logs to your Logstash instance for processing. Filebeat is 
+designed for reliability and low latency. Filebeat uses the computing resources of the machine hosting the source data,
+and the {logstash}plugins-inputs-beats.html[`Beats input`] plugin minimizes the
+resource demands on the Logstash instance.
+
+NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your
+Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
+same machine.
+
+The default Logstash configuration includes the {logstash}plugins-inputs-beats.html[`Beats input`] plugin. To install
+Filebeat on your data source machine, download the appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page].
+
+After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
+directory, and replace the contents with the following lines. Make sure `paths` points to your syslog: 
+
+[source,shell]
+--------------------------------------------------------------------------------
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/*.log <1>
+  fields:
+    type: syslog <2>
+output.logstash:
+  hosts: ["localhost:5043"]
+--------------------------------------------------------------------------------
+
+<1> Absolute path to the file or files that Filebeat processes.
+<2> Adds a field called `type` with the value `syslog` to the event.
+
+Save your changes. 
+
+To keep the example configuration simple, you won't specify TLS/SSL settings as you would in a real world
+scenario.
+
+Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the `input` section
+of the `second-pipeline.conf` file:
+
+[source,json]
+--------------------------------------------------------------------------------
+beats {
+    port => "5043"
+}
+--------------------------------------------------------------------------------
+
+[float]
+[[logstash-file-output]]
+==== Writing Logstash Data to a File
+
+You can configure your Logstash pipeline to write data directly to a file with the
+{logstash}plugins-outputs-file.html[`file`] output plugin.
+
+Configure your Logstash instance to use the `file` output plugin by adding the following lines to the `output` section
+of the `second-pipeline.conf` file:
+
+[source,json]
+--------------------------------------------------------------------------------
+file {
+    path => /path/to/target/file
+}
+--------------------------------------------------------------------------------
+
+[float]
+[[multiple-es-nodes]]
+==== Writing to multiple Elasticsearch nodes
+
+Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as
+providing redundant points of entry into the cluster when a particular node is unavailable.
+
+To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the `output` section of the `second-pipeline.conf` file to read:
+
+[source,json]
+--------------------------------------------------------------------------------
+output {
+    elasticsearch {
+        hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
+    }
+}
+--------------------------------------------------------------------------------
+
+Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the `hosts`
+parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
+default port for Elasticsearch is `9200` and can be omitted in the configuration above.
+
+[float]
+[[testing-second-pipeline]]
+===== Testing the Pipeline
+
+At this point, your `second-pipeline.conf` file looks like this: 
+
+[source,json]
+--------------------------------------------------------------------------------
+input {
+    twitter {
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
+    }
+    beats {
+        port => "5043"
+    }
+}
+output {
+    elasticsearch {
+        hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
+    }
+    file {
+        path => "/path/to/target/file"
+    }
+}
+--------------------------------------------------------------------------------
+
+Logstash is consuming data from the Twitter feed you configured, receiving data from Filebeat, and
+indexing this information to three nodes in an Elasticsearch cluster as well as writing to a file.
+
+At the data source machine, run Filebeat with the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+sudo ./filebeat -e -c filebeat.yml -d "publish"
+--------------------------------------------------------------------------------
+
+Filebeat will attempt to connect on port 5403. Until Logstash starts with an active Beats plugin, there
+won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
+
+To verify your configuration, run the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f second-pipeline.conf --config.test_and_exit
+--------------------------------------------------------------------------------
+
+The `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file
+passes the configuration test, start Logstash with the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f second-pipeline.conf
+--------------------------------------------------------------------------------
+
+Use the `grep` utility to search in the target file to verify that information is present:
+
+[source,shell]
+--------------------------------------------------------------------------------
+grep syslog /path/to/target/file
+--------------------------------------------------------------------------------
+
+Run an Elasticsearch query to find the same information in the Elasticsearch cluster:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=fields.type:syslog'
+--------------------------------------------------------------------------------
+
+Replace $DATE with the current date, in YYYY.MM.DD format.
+
+To see data from the Twitter feed, try this query:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'http://localhost:9200/logstash-$DATE/_search?pretty&q=client:iphone'
+--------------------------------------------------------------------------------
+
+Again, remember to replace $DATE with the current date, in YYYY.MM.DD format. 
+
+
+
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
new file mode 100644
index 00000000000..fc722534fdd
--- /dev/null
+++ b/docs/static/breaking-changes.asciidoc
@@ -0,0 +1,43 @@
+[[breaking-changes]]
+== Breaking changes
+
+**Breaking changes in 5.0**
+
+Application Settings: Introduced a new way to configure application settings for Logstash through a settings.yml file. This file 
+is typically located in `LS_HOME/config`, or `/etc/logstash` when installed via packages. Logstash will not be able 
+to start without this file, so please make sure to pass in `--path.settings` if you are starting Logstash manually 
+after installing it via a package (RPM, DEB).
+
+Release Packages: When Logstash is installed via DEB, RPM packages, it uses `/usr/share/logstash` and `/var/lib/logstash` to install binaries and config files 
+respectively. Previously it used to install in `/opt` directory. This change was done to make the user experience 
+consistent with other Elastic products. Full directory layout is described https://www.elastic.co/guide/en/logstash/5.0/dir-layout.html[here].
+
+Command Line Interface: Most of the long form https://www.elastic.co/guide/en/logstash/5.0/command-line-flags.html[options] have been renamed 
+to adhere to the yml dot notation to be used in the settings file. Short form options have not been changed.
+
+Plugin Developers: The Event class has a https://github.com/elastic/logstash/issues/5141[new API] to access its data. You will no longer be able to directly use 
+the Event class through the ruby hash paradigm. All the plugins packaged with Logstash has been updated 
+to use the new API and their versions bumped to the next major.
+
+The command `bin/plugin` has been renamed to `bin/logstash-plugin`. `bin/plugin <plugin>`` which is the current 
+way of install packs/plugins is problematic because it pollutes the global namespace if it is put in the path. 
+This command can now install both plugins and "Packs" - a single zip that contains 0 or 1 plugin for each system 
+in the Elastic stack.
+
+**Environment Variables Support in Configuration**
+You can set environment variable references into Logstash plugins configuration using `${var}` or `$var` syntax.
+Previously if you had used `${var}` or `$var` as a value in configuration, 5.0 will try to resolve it assuming 
+it is an environment variable
+
+[float]
+== Kafka Input and Output Plugins
+
+Kafka version 0.9 brings in new security features (SSL, client based auth, access control), 
+improved consumer and producer API, and much more. For bringing in these features, Logstash 
+had to use the new 0.9 version of the consumer which is not compatible with previous versions of the broker.
+To use these new features, users have to first upgrade Kafka Brokers from 0.8.x to 0.9 and then use the 
+input plugins. Please note that several configurations have changed in the Logstash configuration.
+
+Configuration Changes:
+* <<plugins-inputs-kafka,Kafka Input Plugin>>
+* <<plugins-outputs-kafka,Kafka Output Plugin>>
\ No newline at end of file
diff --git a/docs/asciidoc/static/codec.asciidoc b/docs/static/codec.asciidoc
similarity index 100%
rename from docs/asciidoc/static/codec.asciidoc
rename to docs/static/codec.asciidoc
diff --git a/docs/static/command-line-flags.asciidoc b/docs/static/command-line-flags.asciidoc
new file mode 100644
index 00000000000..b765788069f
--- /dev/null
+++ b/docs/static/command-line-flags.asciidoc
@@ -0,0 +1,91 @@
+[[command-line-flags]]
+=== Command-Line Flags
+
+Logstash has the following flags. You can use the `--help` flag to display this information.
+
+You can also control Logstash execution by specifying options in the Logstash settings file. For more info, see <<logstash-settings-file>>.  
+
+coming[5.0.0-alpha3, Command-line flags have dots instead of dashes in their names]
+
+*`-f, --path.config CONFIGFILE`*::
+ Load the Logstash config from a specific file or directory, or a wildcard. If
+ given a directory or wildcard, config files will be read from the directory in
+ alphabetical order.
+
+*`-e, --config.string CONFIGSTRING`*::
+ Use the given string as the configuration data. Same syntax as the config file.
+ If no input is specified, `stdin { type => stdin }` is default. If no output
+ is specified, `stdout { codec => rubydebug }}` is default.
+
+*`-w, --pipeline.workers COUNT`*::
+ Sets the number of pipeline workers (threads) to run for filter processing (default: number of cores).
+ If you find that events are backing up, or that the CPU is not saturated, consider increasing
+ this number to better utilize machine processing power.
+ 
+*`-b, --pipeline.batch.size SIZE`*::
+ This parameter defines the maximum number of events an individual worker thread will collect
+ before attempting to execute its filters and outputs. Default is 125 events.
+ Larger batch sizes are generally more efficient, but come at the cost of increased memory
+ overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
+ variable to effectively use the option.
+
+*`-u, --pipeline.batch.delay DELAY_IN_MS`*::
+ When creating pipeline event batches, how long to wait while polling for the next event.
+ Default is 5ms.
+
+*`-l, --path.log FILE`*::
+ Log to a given path. Default is to log to stdout
+
+*`--log.level`*::
+ Set the log level to "quiet", "verbose", "warn" (default), or "debug".
+
+*`--log.format FORMAT`*::
+ Set to "json" to log in JSON format, or "plain" (default) to use `Object#.inspect`.
+ 
+*`--path.settings SETTINGS_DIR`*::
+ Directory containing the `logstash.yml` <<logstash-settings-file,settings file>>.
+ 
+*`--node.name NAME`*::
+ Set a descriptive name for the node. If no value is specified, defaults to the current hostname. 
+
+*`--config.debug`*::
+ Print the compiled config ruby code out as a debug log (you must also have `--log.level=debug` enabled).
+ WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result
+ in plaintext passwords appearing in your logs!
+
+*`-V, --version`*::
+  Display the version of Logstash.
+
+*`-p, --path.plugins`*::
+  A path of where to find plugins. This flag can be given multiple times to include
+  multiple paths. Plugins are expected to be in a specific directory hierarchy:
+  `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`,
+  and `NAME` is the name of the plugin.
+
+*`-t, --config.test_and_exit`*::
+  Check configuration and then exit. Note that grok patterns are not checked for
+  correctness with this flag.
+  Logstash can read multiple config files from a directory. If you combine this
+  flag with `--log.level=debug`, Logstash will log the combined config file, annotating
+  each config block with the source file it came from.
+  
+*`-r, --config.reload.automatic`*::
+  Monitor configuration changes and reload the configuration whenever it is changed.
+
+*`--config.reload.interval RELOAD_INTERVAL`*::
+  Specifies how often Logstash checks the config files for changes. The default is every 3 seconds.
+
+*`--http.host HTTP_HOST`*::
+  Web API binding host (default: "127.0.0.1")
+
+*`--http.port HTTP_PORT`*::
+  Web API http port (default: 9600)
+
+*`--pipeline.unsafe_shutdown`*::
+  Force Logstash to exit during shutdown even if there are still inflight events
+  in memory. By default, Logstash will refuse to quit until all received events
+  have been pushed to the outputs.
+
+*`-h, --help`*::
+  Print help
+
diff --git a/docs/asciidoc/static/configuration.asciidoc b/docs/static/configuration.asciidoc
similarity index 90%
rename from docs/asciidoc/static/configuration.asciidoc
rename to docs/static/configuration.asciidoc
index adcd454e700..d7834785798 100644
--- a/docs/asciidoc/static/configuration.asciidoc
+++ b/docs/static/configuration.asciidoc
@@ -323,10 +323,10 @@ output {
 }
 ----------------------------------
 
-You can also format times using this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[time format].
+Similarly, you can convert the timestamp in the `@timestamp` field into a string. Instead of specifying a field name inside the curly braces, use the `+FORMAT` syntax where `FORMAT` is a http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[time format].
 
 For example, if you want to use the file output to write to logs based on the
-hour and the 'type' field:
+event's date and hour and the `type` field:
 
 [source,js]
 ----------------------------------
@@ -366,7 +366,7 @@ What's an expression? Comparison tests, boolean logic, and so on!
 You can use the following comparison operators:
 
 * equality: `==`,  `!=`,  `<`,  `>`,  `<=`, `>=`
-* regexp: `=~`, `!~`
+* regexp: `=~`, `!~` (checks a pattern on the right against a string value on the left)
 * inclusion: `in`, `not in`
 
 The supported boolean operators are:
@@ -387,7 +387,7 @@ For example, the following conditional uses the mutate filter to remove the fiel
 ----------------------------------
 filter {
   if [action] == "login" {
-    mutate { remove => "secret" }
+    mutate { remove_field => "secret" }
   }
 }
 ----------------------------------
@@ -406,7 +406,7 @@ output {
 }
 ----------------------------------
 
-The `in` conditional enables you to compare against the value of a field:
+You can use the `in` operator to test whether a field contains a specific string, key, or (for lists) element:
 
 [source,js]
 ----------------------------------
@@ -433,7 +433,7 @@ filter {
 ----------------------------------
 
 You use the `not in` conditional the same way. For example,
-you could use `not in` to only route events to elasticsearch
+you could use `not in` to only route events to Elasticsearch
 when `grok` is successful:
 
 [source,js]
@@ -445,13 +445,20 @@ output {
 }
 ----------------------------------
 
+You can check for the existence of a specific field, but there's currently no way to differentiate between a field that
+doesn't exist versus a field that's simply false. The expression `if [foo]` returns `false` when:
+
+* `[foo]` doesn't exist in the event,
+* `[foo]` exists in the event, but is false, or
+* `[foo]` exists in the event, but is null
+
 For more complex examples, see <<using-conditionals, Using Conditionals>>.
 
 [float]
 [[metadata]]
 ==== The @metadata field
 
-In Logstash 1.5 there is a new, special field, called `@metadata`.  The contents
+In Logstash 1.5 and later, there is a special field called `@metadata`.  The contents
 of `@metadata` will not be part of any of your events at output time, which
 makes it great to use for conditionals, or extending and building event fields
 with field reference and sprintf formatting.
@@ -599,6 +606,160 @@ output {
 }
 ----------------------------------
 
+[[environment-variables]]
+=== Using Environment Variables in Configuration
+
+==== Overview
+
+* You can set environment variable references into Logstash plugins configuration using `${var}`.
+* Each reference will be replaced by environment variable value at Logstash startup.
+* The replacement is case-sensitive.
+* References to undefined variables raise a Logstash configuration error.
+* A default value can be given by using the form `${var:default value}`.
+* You can add environment variable references in any plugin option type : string, number, boolean, array or hash.
+* Environment variables are immutable. If you update the environment variable, you'll have to restart Logstash to pick the updated value.
+
+==== Examples
+
+[cols="a,a,a"]
+|==================================
+|Logstash config source	|Environment 	|Logstash config result
+|
+[source,ruby]
+----
+input {
+  tcp {
+    port => "${TCP_PORT}"
+  }
+}
+----
+
+|
+[source,shell]
+----
+export TCP_PORT=12345
+----
+|
+[source,ruby]
+----
+input {
+  tcp {
+    port => 12345
+  }
+}
+----
+|
+[source,ruby]
+----
+input {
+  tcp {
+    port => "${TCP_PORT}"
+  }
+}
+----
+
+|
+No TCP_PORT defined
+|
+Raise a logstash configuration error
+|
+[source,ruby]
+----
+input {
+  tcp {
+    port => "${TCP_PORT:54321}"
+  }
+}
+----
+
+|
+No TCP_PORT defined
+|
+[source,ruby]
+----
+input {
+  tcp {
+    port => 54321
+  }
+}
+----
+|
+[source,ruby]
+----
+input {
+  tcp {
+    port => "${TCP_PORT:54321}"
+  }
+}
+----
+
+|
+[source,shell]
+----
+export TCP_PORT=12345
+----
+|
+[source,ruby]
+----
+input {
+  tcp {
+    port => 12345
+  }
+}
+----
+|
+[source,ruby]
+----
+filter {
+  mutate {
+    add_tag => [ "tag1", "${ENV_TAG}" ]
+  }
+}
+----
+
+|
+[source,shell]
+----
+export ENV_TAG="tag2"
+----
+|
+[source,ruby]
+----
+filter {
+  mutate {
+    add_tag => [ "tag1", "tag2" ]
+  }
+}
+----
+|
+[source,ruby]
+----
+filter {
+  mutate {
+    add_field => {
+      "my_path" => "${HOME}/file.log"
+    }
+  }
+}
+----
+|
+[source,shell]
+----
+export HOME="/path"
+----
+|
+[source,ruby]
+----
+filter {
+  mutate {
+    add_field => {
+      "my_path" => "/path/file.log"
+    }
+  }
+}
+----
+|==================================
+
 [[config-examples]]
 === Logstash Configuration Examples
 The following examples illustrate how you can configure Logstash to filter events, process Apache logs and syslog messages, and use conditionals to control what events are processed by a filter or output.
@@ -773,7 +934,7 @@ This example labels all events using the `type` field, but doesn't actually pars
 Similarly, you can use conditionals to direct events to particular outputs. For example, you could:
 
 * alert nagios of any apache events with status 5xx
-* record any 4xx status to elasticsearch
+* record any 4xx status to Elasticsearch
 * record all status code hits via statsd
 
 To tell nagios about any http event that has a 5xx status code, you
@@ -822,7 +983,6 @@ filter {
       add_field => [ "received_at", "%{@timestamp}" ]
       add_field => [ "received_from", "%{host}" ]
     }
-    syslog_pri { }
     date {
       match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
     }
diff --git a/docs/static/contributing-patch.asciidoc b/docs/static/contributing-patch.asciidoc
new file mode 100644
index 00000000000..470841574f2
--- /dev/null
+++ b/docs/static/contributing-patch.asciidoc
@@ -0,0 +1,397 @@
+[[contributing-patch-plugin]]
+=== Contributing a Patch to a Logstash Plugin
+
+This section discusses the information you need to know to successfully contribute a patch to a Logstash plugin.
+
+Each plugin defines its own configuration options. These control the behaviour of the plugin to some degree. Configuration 
+option definitions commonly include:
+
+* Data validation
+* The default value
+* Any required flags
+
+Plugins are subclasses of a Logstash base class. A plugin's base class defines common configuration and methods.
+
+==== Input Plugins
+
+Input plugins ingest data from an external source. Input plugins are always associated with a codec. An input plugin 
+always has an associated codec plugin. Input and codec plugins operate in conjuction to create a Logstash event and add 
+that event to the processing queue. An input codec is a subclass of the `LogStash::Inputs::Base` class.
+
+.Input API
+[horizontal]
+`#register() -> nil`:: Required. This API sets up resources for the plugin, typically the connection to the 
+external source.
+`#run(queue) -> nil`:: Required. This API fetches or listens for source data, typically looping until stopped. Must handle 
+errors inside the loop. Pushes any created events to the queue object specified in the method argument. Some inputs may 
+receive batched data to minimize the external call overhead.
+`#stop() -> nil`:: Optional. Stops external connections and cleans up.
+
+==== Codec Plugins
+
+Codec plugins decode input data that has a specific structure, such as JSON input data. A codec plugin is a subclass of 
+`LogStash::Codecs::Base`.
+
+.Codec API
+[horizontal]
+`#register() -> nil`:: Identical to the API of the same name for input plugins.
+`#decode(data){|event| block} -> nil`:: Must be implemented. Used to create an Event from the raw data given in the method 
+argument. Must handle errors. The caller must provide a Ruby block. The block is called with the created Event.
+`#encode(event) -> nil`:: Required.  Used to create a structured data object from the given Event. May handle 
+errors. This method calls a block that was previously stored as @on_event with two arguments: the original event and the 
+data object.
+
+==== Filter Plugins
+
+A mechanism to change, mutate or merge one or more Events. A filter plugin is a subclass of the `LogStash::Filters::Base` 
+class.
+
+.Filter API
+[horizontal]
+`#register() -> nil`:: Identical to the API of the same name for input plugins.
+`#filter(event) -> nil`:: Required. May handle errors. Used to apply a mutation function to the given event.
+
+==== Output Plugins
+
+A mechanism to send an event to an external destination. This process may require serialization. An output plugin is a 
+subclass of the `LogStash::Outputs::Base` class.
+
+.Output API
+[horizontal]
+`#register() -> nil`:: Identical to the API of the same name for input plugins.
+`#receive(event) -> nil`:: Required. Must handle errors. Used to prepare the given event for transmission to 
+the external destination. Some outputs may buffer the prepared events to batch transmit to the destination.
+
+[[patch-process]]
+==== Process
+
+A bug or feature is identified. An issue is created in the plugin repository. A patch is created and a pull request (PR) 
+is submitted. After review and possible rework the PR is merged and the plugin is published.
+
+The <<community-maintainer,Community Maintainer Guide>> explains, in more detail, the process of getting a patch accepted, 
+merged and published.  The Community Maintainer Guide also details the roles that contributors and maintainers are 
+expected to perform.
+
+==== Testing Methodologies
+
+===== Test Driven Development
+
+Test Driven Development, colloquially known as TDD, describes a methodology for using tests to guide evolution of source
+code. For our purposes, we are only going to use a part of it, that is, before writing the fix - we create tests that 
+illustrate the bug by failing. We stop when we have written enough code to make the tests pass and submit the fix and 
+tests as a patch. It is not necessary to write the tests before the fix, but it is very easy to write a passing test 
+afterwards that may not actually verify that the fault is really fixed especially if the fault can be triggered via 
+multiple execution paths or varying input data.
+
+===== The RSpec Framework
+
+Logstash uses Rspec, a Ruby testing framework, to define and run the test suite. What follows is a summary of various 
+sources.
+
+. Rspec Example
+[source,ruby]
+ 1 # encoding: utf-8
+ 2 require "logstash/devutils/rspec/spec_helper"
+ 3 require "logstash/plugin"
+ 4
+ 5 describe "outputs/riemann" do
+ 6   describe "#register" do
+ 7     let(:output) do
+ 8       LogStash::Plugin.lookup("output", "riemann").new(configuration)
+ 9     end
+10
+11     context "when no protocol is specified" do
+12       let(:configuration) { Hash.new }
+13
+14       it "the method completes without error" do
+15         expect {output.register}.not_to raise_error
+16       end
+17     end
+18
+19     context "when a bad protocol is specified" do
+20       let(:configuration) { {"protocol" => "fake"} }
+21
+22       it "the method fails with error" do
+23         expect {output.register}.to raise_error
+24       end
+25     end
+26
+27     context "when the tcp protocol is specified" do
+28       let(:configuration) { {"protocol" => "tcp"} }
+29
+30       it "the method completes without error" do
+31         expect {output.register}.not_to raise_error
+32       end
+33     end
+34   end
+35
+36   describe "#receive" do
+37     let(:output) do
+38       LogStash::Plugin.lookup("output", "riemann").new(configuration)
+39     end
+40
+41     context "when operating normally" do
+42       let(:configuration) { Hash.new }
+43       let(:event) do
+44         data = {"message"=>"hello", "@version"=>"1",
+45                 "@timestamp"=>"2015-06-03T23:34:54.076Z",
+46                 "host"=>"vagrant-ubuntu-trusty-64"}
+47         LogStash::Event.new(data)
+48       end
+49
+50       before(:example) do
+51         output.register
+52       end
+53
+54       it "should accept the event" do
+55         expect { output.receive event }.not_to raise_error
+56       end
+57     end
+58   end
+59 end
+
+.Describe blocks (lines 5, 6 and 36 in Example 1)
+[source,ruby]
+describe(string){block} -> nil
+describe(Class){block} -> nil
+
+With RSpec, we are always describing the plugin method behavior. The describe block is added in logical sections and can
+accept either an existing class name or a string. The string used in line 5 is the plugin name. Line 6 is the register 
+method, line 36 is the receive method. It is a RSpec convention to prefix instance methods with one hash and class 
+methods with one dot.
+
+.Context blocks (lines 11, 19, 27 and 41)
+[source,ruby]
+context(string){block} -> nil
+
+In RSpec, context blocks define sections that group tests by a variation.  The string should start with the word `when` 
+and then detail the variation. See line 11.  The tests in the content block should should only be for that variation.
+
+.Let blocks (lines 7, 12, 20, 28, 37, 42 and 43)
+[source,ruby]
+let(symbol){block} -> nil
+
+In RSpec, `let` blocks define resources for use in the test blocks. These resources are reinitialized for every test 
+block. They are available as method calls inside the test block. Define `let` blocks in `describe` and `context` blocks, 
+which scope the `let` block and any other nested blocks.
+You can use other `let` methods defined later within the `let` block body. See lines 7-9, which define the output resource 
+and use the configuration method, defined with different variations in lines 12, 20 and 28.
+
+.Before blocks (line 50)
+[source,ruby]
+before(symbol){block} -> nil - symbol is one of :suite, :context, :example, but :all and :each are synonyms for :suite and :example respectively.
+
+In RSpec, `before` blocks are used to further set up any resources that would have been initialized in a `let` block.
+You cannot define `let` blocks inside `before` blocks.
+
+You can also define `after` blocks, which are typically used to clean up any setup activity performed by a `before` block.
+
+.It blocks (lines 14, 22, 30 and 54)
+[source,ruby]
+it(string){block} -> nil
+
+In RSpec, `it` blocks set the expectations that verify the behavior of the tested code. The string should not start with 
+'it' or 'should', but needs to express the outcome of the expectation.  When put together the texts from the enclosing 
+describe, `context` and `it` blocks should form a fairly readable sentence, as in lines 5, 6, 11 and 14:
+
+[source,ruby]
+outputs/riemann 
+#register when no protocol is specified the method completes without error
+
+Readable code like this make the goals of tests easy to understand.
+
+.Expect method (lines 15, 23, 31, 55)
+[source,ruby]
+expect(object){block} -> nil
+
+In RSpec, the expect method verifies a statement that compares an actual result to an expected result. The `expect` method 
+is usually paired with a call to the `to` or `not_to` methods. Use the block form when expecting errors or observing for 
+changes. The `to` or `not_to` methods require a `matcher` object that encapsulates the expected value. The argument form 
+of the `expect` method encapsulates the actual value. When put together the whole line tests the actual against the 
+expected value.
+
+.Matcher methods (lines 15, 23, 31, 55)
+[source,ruby]
+raise_error(error class|nil) -> matcher instance
+be(object) -> matcher instance
+eq(object) -> matcher instance
+eql(object) -> matcher instance
+  for more see http://www.relishapp.com/rspec/rspec-expectations/docs/built-in-matchers
+
+In RSpec, a matcher is an object generated by the equivalent method call (be, eq) that will be used to evaluate the 
+expected against the actual values.
+
+==== Putting it all together
+
+This example fixes an https://github.com/logstash-plugins/logstash-output-zeromq/issues/9[issue] in the ZeroMQ output 
+plugin. The issue does not require knowledge of ZeroMQ.
+
+The activities in this example have the following prerequisites:
+
+--
+* A minimal knowledge of Git and Github. See the https://help.github.com/categories/bootcamp/[Github boot camp].
+* A text editor.
+* A JRuby https://www.ruby-lang.org/en/documentation/installation/#managers[runtime] 
+https://howistart.org/posts/ruby/1[environment]. The `chruby` tool manages Ruby versions.
+* JRuby 1.7.22 or later.
+* The `bundler` and `rake` gems installed. 
+* ZeroMQ http://zeromq.org/intro:get-the-software[installed].
+--
+
+. In Github, fork the ZeroMQ https://github.com/logstash-plugins/logstash-output-zeromq[output plugin repository].
+
+. On your local machine, https://help.github.com/articles/fork-a-repo/[clone] the fork to a known folder such as
+`logstash/`.
+
+. Open the following files in a text editor:
+  * `logstash-output-zeromq/lib/logstash/outputs/zeromq.rb`
+  * `logstash-output-zeromq/lib/logstash/util/zeromq.rb`
+  * `logstash-output-zeromq/spec/outputs/zeromq_spec.rb`
+
+. According to the issue, log output in server mode must indicate `bound`. Furthermore, the test file contains no tests.
++
+NOTE: Line 21 of `util/zeromq.rb` reads `@logger.info("0mq: #{server? ? 'connected' : 'bound'}", :address => address)`
+
+. In the text editor, set the file encoding and require `zeromq.rb` for the file `zeromq_spec.rb` by adding the following 
+lines:
++
+[source,ruby]
+# encoding: utf-8
+require "logstash/outputs/zeromq"
+require "logstash/devutils/rspec/spec_helper"
+
+. The desired error message should read:
++
+[source,ruby]
+LogStash::Outputs::ZeroMQ when in server mode a 'bound' info line is logged 
++
+To properly generate this message, add a `describe` block with the fully qualified class name as the argument, a context 
+block, and an `it` block.
++
+[source,ruby]
+describe LogStash::Outputs::ZeroMQ do
+  context "when in server mode" do
+    it "a 'bound' info line is logged" do
+    end
+  end
+end
+
+. To add the missing test, use an instance of the ZeroMQ output and a substitute logger. This examle uses an RSpec feature 
+called _test doubles_ as the substitute logger.
++
+Add the following lines to `zeromq_spec.rb`, after `describe LogStash::Outputs::ZeroMQ do` and before `context "when in 
+server mode" do`:
+[source,ruby]
+  let(:output) { described_class.new("mode" => "server", "topology" => "pushpull" }
+  let(:tracer) { double("logger") }
+
+. Add the body to the `it` block. Add the following five lines after the line `context "when in server mode" do`: 
+[source,ruby]
+      allow(tracer).to receive(:debug)<1>
+      output.logger = logger<2>
+      expect(tracer).to receive(:info).with("0mq: bound", {:address=>"tcp://127.0.0.1:2120"})<3>
+      output.register<4>
+      output.do_close<5>
+
+<1> Allow the double to receive `debug` method calls.
+<2> Make the output use the test double.
+<3> Set an expectation on the test to receive an `info` method call.
+<4> Call `register` on the output.
+<5> Call `do_close` on the output so the test does not hang.
+
+At the end of the modifications, the relevant code section reads:
+
+[source,ruby]
+--------
+# encoding: utf-8
+require "logstash/outputs/zeromq"
+require "logstash/devutils/rspec/spec_helper"
+
+describe LogStash::Outputs::ZeroMQ do
+  let(:output) { described_class.new("mode" => "server", "topology" => "pushpull") }
+  let(:tracer) { double("logger") }
+
+  context "when in server mode" do
+    it "a ‘bound’ info line is logged" do
+      allow(tracer).to receive(:debug)
+      output.logger = tracer
+      expect(tracer).to receive(:info).with("0mq: bound", {:address=>"tcp://127.0.0.1:2120"})
+      output.register
+      output.do_close
+    end
+  end
+end
+--------
+
+To run this test:
+
+. Open a terminal window
+. Mavigate to the cloned plugin folder
+. The first time you run the test, run the command `bundle install`
+. Run the command `bundle exec rspec`
+
+Assuming all prerequisites were installed correctly, the test fails with output similar to:
+
+[source,shell]
+--------
+Using Accessor#strict_set for specs
+Run options: exclude {:redis=>true, :socket=>true, :performance=>true, :couchdb=>true, :elasticsearch=>true, 
+:elasticsearch_secure=>true, :export_cypher=>true, :integration=>true, :windows=>true}
+
+LogStash::Outputs::ZeroMQ
+  when in server mode
+    a ‘bound’ info line is logged (FAILED - 1)
+
+Failures:
+
+  1) LogStash::Outputs::ZeroMQ when in server mode a ‘bound’ info line is logged
+     Failure/Error: output.register
+       Double "logger" received :info with unexpected arguments
+         expected: ("0mq: bound", {:address=>"tcp://127.0.0.1:2120"})
+              got: ("0mq: connected", {:address=>"tcp://127.0.0.1:2120"})
+     # ./lib/logstash/util/zeromq.rb:21:in `setup'
+     # ./lib/logstash/outputs/zeromq.rb:92:in `register'
+     # ./lib/logstash/outputs/zeromq.rb:91:in `register'
+     # ./spec/outputs/zeromq_spec.rb:13:in `(root)'
+     # /Users/guy/.gem/jruby/1.9.3/gems/rspec-wait-0.0.7/lib/rspec/wait.rb:46:in `(root)'
+
+Finished in 0.133 seconds (files took 1.28 seconds to load)
+1 example, 1 failure
+
+Failed examples:
+
+rspec ./spec/outputs/zeromq_spec.rb:10 # LogStash::Outputs::ZeroMQ when in server mode a ‘bound’ info line is logged
+
+Randomized with seed 2568
+--------
+
+To correct the error, open the `util/zeromq.rb` file in your text editor and swap the positions of the words `connected` 
+and `bound` on line 21. Line 21 now reads:
+
+[source,ruby]
+@logger.info("0mq: #{server? ? 'bound' : 'connected'}", :address => address)
+
+Run the test again with the `bundle exec rspec` command.
+
+The test passes with output similar to:
+
+[source,shell]
+--------
+Using Accessor#strict_set for specs
+Run options: exclude {:redis=>true, :socket=>true, :performance=>true, :couchdb=>true, :elasticsearch=>true, :elasticsearch_secure=>true, :export_cypher=>true, :integration=>true, :windows=>true}
+
+LogStash::Outputs::ZeroMQ
+  when in server mode
+    a ‘bound’ info line is logged
+
+Finished in 0.114 seconds (files took 1.22 seconds to load)
+1 example, 0 failures
+
+Randomized with seed 45887
+--------
+
+https://help.github.com/articles/fork-a-repo/#next-steps[Commit] the changes to git and Github.
+
+Your pull request is visible from the https://github.com/logstash-plugins/logstash-output-zeromq/pulls[Pull Requests] 
+section of the original Github repository. The plugin maintainers review your work, suggest changes if necessary, and
+merge and publish a new version of the plugin.
diff --git a/docs/asciidoc/static/contributing-to-logstash.asciidoc b/docs/static/contributing-to-logstash.asciidoc
similarity index 87%
rename from docs/asciidoc/static/contributing-to-logstash.asciidoc
rename to docs/static/contributing-to-logstash.asciidoc
index ab9dc568ff7..05bdeade5e7 100644
--- a/docs/asciidoc/static/contributing-to-logstash.asciidoc
+++ b/docs/static/contributing-to-logstash.asciidoc
@@ -14,10 +14,14 @@ Since plugins can now be developed and deployed independently of the Logstash
 core, there are documents which guide you through the process of coding and
 deploying your own plugins:
 
+* <<plugin-generator,Generating a New Plugin>>
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html[How to write a Logstash input plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_codec_plugin.html[How to write a Logstash codec plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_filter_plugin.html[How to write a Logstash filter plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_output_plugin.html[How to write a Logstash output plugin]
+* <<contributing-patch-plugin,Contributing a Patch to a Logstash Plugin>>
+* <<community-maintainer,Community Maintainer's Guide>>
+* <<submitting-plugin,Submitting a Plugin>>
 
 [float]
 ==== Plugin API Changes added[2.0]
@@ -27,10 +31,10 @@ for plugin shutdown: `stop`, `stop?`, and `close`.
 
 * Call the `stop` method from outside the plugin thread. This method signals the plugin to stop.
 * The `stop?` method returns `true` when the `stop` method has already been called for that plugin.
-* The `close` method performs final bookkeeping and cleanup after the plugin's `run` method and the plugin's thread both 
+* The `close` method performs final bookkeeping and cleanup after the plugin's `run` method and the plugin's thread both
 exit. The `close` method is a a new name for the method known as `teardown` in previous versions of Logstash.
 
-The `shutdown`, `finished`, `finished?`, `running?`, and `terminating?` methods are redundant and no longer present in the 
+The `shutdown`, `finished`, `finished?`, `running?`, and `terminating?` methods are redundant and no longer present in the
 Plugin Base class.
 
 Sample code for the new plugin shutdown APIs is https://github.com/logstash-plugins/logstash-input-example/blob/master/lib/logstash/inputs/example.rb[available].
diff --git a/docs/asciidoc/static/deploying.asciidoc b/docs/static/deploying.asciidoc
similarity index 62%
rename from docs/asciidoc/static/deploying.asciidoc
rename to docs/static/deploying.asciidoc
index a81239ddcfe..e4d7cce4f79 100644
--- a/docs/asciidoc/static/deploying.asciidoc
+++ b/docs/static/deploying.asciidoc
@@ -1,20 +1,20 @@
 [[deploying-and-scaling]]
 === Deploying and Scaling Logstash
 
-As your use case for Logstash evolves, the preferred architecture at a given scale will change. This section discusses 
-a range of Logstash architectures in increasing order of complexity, starting from a minimal installation and adding 
-elements to the system. The example deployments in this section write to an Elasticsearch cluster, but Logstash can 
+As your use case for Logstash evolves, the preferred architecture at a given scale will change. This section discusses
+a range of Logstash architectures in increasing order of complexity, starting from a minimal installation and adding
+elements to the system. The example deployments in this section write to an Elasticsearch cluster, but Logstash can
 write to a large variety of {logstash}output-plugins.html[endpoints].
 
 [float]
 [[deploying-minimal-install]]
 ==== The Minimal Installation
 
-The minimal Logstash installation has one Logstash instance and one Elasticsearch instance. These instances are 
-directly connected. Logstash uses an {logstash}input-plugins.html[_input plugin_] to ingest data and an 
-Elasticsearch {logstash}output-plugins.html[_output plugin_] to index the data in Elasticsearch, following the Logstash 
-{logstash}pipeline.html[_processing pipeline_]. A Logstash instance has a fixed pipeline constructed at startup, 
-based on the instance’s configuration file. You must specify an input plugin. Output defaults to `stdout`, and the 
+The minimal Logstash installation has one Logstash instance and one Elasticsearch instance. These instances are
+directly connected. Logstash uses an {logstash}input-plugins.html[_input plugin_] to ingest data and an
+Elasticsearch {logstash}output-plugins.html[_output plugin_] to index the data in Elasticsearch, following the Logstash
+{logstash}pipeline.html[_processing pipeline_]. A Logstash instance has a fixed pipeline constructed at startup,
+based on the instance’s configuration file. You must specify an input plugin. Output defaults to `stdout`, and the
 filtering section of the pipeline, which is discussed in the next section, is optional.
 
 image::static/images/deploy_1.png[]
@@ -23,34 +23,33 @@ image::static/images/deploy_1.png[]
 [[deploying-filter-threads]]
 ==== Using Filters
 
-Log data is typically unstructured, often contains extraneous information that isn’t relevant to your use case, and 
-sometimes is missing relevant information that can be derived from the log contents. You can use a 
-{logstash}filter-plugins.html[filter plugin] to parse the log into fields, remove unnecessary information, and derive 
-additional information from the existing fields. For example, filters can derive geolocation information from an IP 
-address and add that information to the logs, or parse and structure arbitrary text with the 
+Log data is typically unstructured, often contains extraneous information that isn’t relevant to your use case, and
+sometimes is missing relevant information that can be derived from the log contents. You can use a
+{logstash}filter-plugins.html[filter plugin] to parse the log into fields, remove unnecessary information, and derive
+additional information from the existing fields. For example, filters can derive geolocation information from an IP
+address and add that information to the logs, or parse and structure arbitrary text with the
 {logstash}plugins-filters-grok.html[grok] filter.
 
-Adding a filter plugin can significantly affect performance, depending on the amount of computation the filter plugin 
-performs, as well as on the volume of the logs being processed. The `grok` filter’s regular expression computation is 
-particularly resource-intensive. One way to address this increased demand for computing resources is to use 
-parallel processing on multicore machines. Use the `-w` switch to set the number of execution threads for Logstash 
+Adding a filter plugin can significantly affect performance, depending on the amount of computation the filter plugin
+performs, as well as on the volume of the logs being processed. The `grok` filter’s regular expression computation is
+particularly resource-intensive. One way to address this increased demand for computing resources is to use
+parallel processing on multicore machines. Use the `-w` switch to set the number of execution threads for Logstash
 filtering tasks. For example the `bin/logstash -w 8` command uses eight different threads for filter processing.
 
 image::static/images/deploy_2.png[]
 
 [float]
-[[deploying-logstash-forwarder]]
-==== Using Logstash Forwarder
+[[deploying-filebeat]]
+==== Using Filebeat
 
-The https://github.com/elastic/logstash-forwarder[Logstash Forwarder] is a lightweight, resource-friendly tool written 
-in Go that collects logs from files on the server and forwards these logs to other machines for processing. The 
-Logstash Forwarder uses a secure protocol called Lumberjack to communicate with a centralized Logstash instance. 
-Configure the Logstash instances that receive Lumberjack data to use the 
-{logstash}plugins-inputs-lumberjack.html[Lumberjack input plugin].
+https://www.elastic.co/guide/en/beats/filebeat/current/index.html[Filebeat] is a lightweight, resource-friendly tool
+written in Go that collects logs from files on the server and forwards these logs to other machines for processing.
+Filebeat uses the https://www.elastic.co/guide/en/beats/libbeat/current/index.html[Beats] protocol to communicate with a
+centralized Logstash instance. Configure the Logstash instances that receive Beats data to use the
+{logstash}plugins-inputs-beats.html[Beats input plugin].
 
-The Logstash Forwarder uses the computing resources of the machine hosting the source data, and the Lumberjack input 
-plugin minimizes the resource demands on the Logstash instance, making this architecture attractive for use cases with 
-resource constraints.
+Filebeat uses the computing resources of the machine hosting the source data, and the Beats input plugin minimizes the
+resource demands on the Logstash instance, making this architecture attractive for use cases with resource constraints.
 
 image::static/images/deploy_3.png[]
 
@@ -58,33 +57,33 @@ image::static/images/deploy_3.png[]
 [[deploying-larger-cluster]]
 ==== Scaling to a Larger Elasticsearch Cluster
 
-Typically, Logstash does not communicate with a single Elasticsearch node, but with a cluster that comprises several 
+Typically, Logstash does not communicate with a single Elasticsearch node, but with a cluster that comprises several
 nodes. By default, Logstash uses the HTTP protocol to move data into the cluster.
 
-You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the 
-indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR 
-files and has no performance disadvantages compared to the transport or node protocols. You can secure communications 
-that use the HTTP REST APIs with the {shield}[Shield] plugin, which supports SSL and HTTP basic authentication.
+You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the
+indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR
+files and has no performance disadvantages compared to the transport or node protocols. You can secure communications
+that use the HTTP REST APIs by using {shield}[{security}], which supports SSL and HTTP basic authentication.
 
-When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically 
-load-balance indexing requests across a 
+When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically
+load-balance indexing requests across a
 specified set of hosts in the Elasticsearch cluster. Specifying multiple Elasticsearch nodes also provides high availability for the Elasticsearch cluster by routing traffic to active Elasticsearch nodes.
 
-You can also use the Elasticsearch Java APIs to serialize the data into a binary representation, using 
-the transport protocol. The transport protocol can sniff the endpoint of the request and select an 
-arbitrary client or data node in the Elasticsearch cluster. 
+You can also use the Elasticsearch Java APIs to serialize the data into a binary representation, using
+the transport protocol. The transport protocol can sniff the endpoint of the request and select an
+arbitrary client or data node in the Elasticsearch cluster.
 
-Using the HTTP or transport protocols keep your Logstash instances separate from the Elasticsearch cluster. The node 
-protocol, by contrast, has the machine running the Logstash instance join the Elasticsearch cluster, running an 
-Elasticsearch instance. The data that needs indexing propagates from this node to the rest of the cluster. Since the 
-machine is part of the cluster, the cluster topology is available, making the node protocol a good fit for use cases 
+Using the HTTP or transport protocols keep your Logstash instances separate from the Elasticsearch cluster. The node
+protocol, by contrast, has the machine running the Logstash instance join the Elasticsearch cluster, running an
+Elasticsearch instance. The data that needs indexing propagates from this node to the rest of the cluster. Since the
+machine is part of the cluster, the cluster topology is available, making the node protocol a good fit for use cases
 that use a relatively small number of persistent connections.
 
-You can also use a third-party hardware or software load balancer to handle connections between Logstash and 
+You can also use a third-party hardware or software load balancer to handle connections between Logstash and
 external applications.
 
 NOTE: Make sure that your Logstash configuration does not connect directly to Elasticsearch dedicated
-{ref}modules-node.html[master nodes], which perform dedicated cluster management. Connect Logstash to client or data 
+{ref}modules-node.html[master nodes], which perform dedicated cluster management. Connect Logstash to client or data
 nodes to protect the stability of your Elasticsearch cluster.
 
 image::static/images/deploy_4.png[]
@@ -93,19 +92,19 @@ image::static/images/deploy_4.png[]
 [[deploying-message-queueing]]
 ==== Managing Throughput Spikes with Message Queueing
 
-When the data coming into a Logstash pipeline exceeds the Elasticsearch cluster's ability to ingest the data, you can 
-use a message queue as a buffer. By default, Logstash throttles incoming events when 
-indexer consumption rates fall below incoming data rates. Since this throttling can lead to events being buffered at 
+When the data coming into a Logstash pipeline exceeds the Elasticsearch cluster's ability to ingest the data, you can
+use a message queue as a buffer. By default, Logstash throttles incoming events when
+indexer consumption rates fall below incoming data rates. Since this throttling can lead to events being buffered at
 the data source, preventing backpressure with message queues becomes an important part of managing your deployment.
 
-Adding a message queue to your Logstash deployment also provides a level of protection from data loss. When a Logstash 
-instance that has consumed data from the message queue fails, the data can be replayed from the message queue to an 
+Adding a message queue to your Logstash deployment also provides a level of protection from data loss. When a Logstash
+instance that has consumed data from the message queue fails, the data can be replayed from the message queue to an
 active Logstash instance.
 
-Several third-party message queues exist, such as Redis, Kafka, or RabbitMQ. Logstash provides input and output plugins 
-to integrate with several of these third-party message queues. When your Logstash deployment has a message queue 
-configured, Logstash functionally exists in two phases: shipping instances, which handles data ingestion and storage in 
-the message queue, and indexing instances, which retrieve the data from the message queue, apply any configured 
+Several third-party message queues exist, such as Redis, Kafka, or RabbitMQ. Logstash provides input and output plugins
+to integrate with several of these third-party message queues. When your Logstash deployment has a message queue
+configured, Logstash functionally exists in two phases: shipping instances, which handles data ingestion and storage in
+the message queue, and indexing instances, which retrieve the data from the message queue, apply any configured
 filtering, and write the filtered data to an Elasticsearch index.
 
 image::static/images/deploy_5.png[]
@@ -114,20 +113,20 @@ image::static/images/deploy_5.png[]
 [[deploying-logstash-ha]]
 ==== Multiple Connections for Logstash High Availability
 
-To make your Logstash deployment more resilient to individual instance failures, you can set up a load balancer between 
-your data source machines and the Logstash cluster. The load balancer handles the individual connections to the 
+To make your Logstash deployment more resilient to individual instance failures, you can set up a load balancer between
+your data source machines and the Logstash cluster. The load balancer handles the individual connections to the
 Logstash instances to ensure continuity of data ingestion and processing even when an individual instance is unavailable.
 
 image::static/images/deploy_6.png[]
 
-The architecture in the previous diagram is unable to process input from a specific type, such as an RSS feed or a 
-file, if the Logstash instance dedicated to that input type becomes unavailable. For more robust input processing, 
+The architecture in the previous diagram is unable to process input from a specific type, such as an RSS feed or a
+file, if the Logstash instance dedicated to that input type becomes unavailable. For more robust input processing,
 configure each Logstash instance for multiple inputs, as in the following diagram:
 
 image::static/images/deploy_7.png[]
 
-This architecture parallelizes the Logstash workload based on the inputs you configure. With more inputs, you can add 
-more Logstash instances to scale horizontally. Separate parallel pipelines also increases the reliability of your stack 
+This architecture parallelizes the Logstash workload based on the inputs you configure. With more inputs, you can add
+more Logstash instances to scale horizontally. Separate parallel pipelines also increases the reliability of your stack
 by eliminating single points of failure.
 
 [float]
@@ -141,7 +140,7 @@ A mature Logstash deployment typically has the following pipeline:
 * The _filter_ tier applies parsing and other processing to the data consumed from the message queue.
 * The _indexing_ tier moves the processed data into Elasticsearch.
 
-Any of these layers can be scaled by adding computing resources. Examine the performance of these components regularly 
-as your use case evolves and add resources as needed. When Logstash routinely throttles incoming events, consider 
-adding storage for your message queue. Alternately, increase the Elasticsearch cluster's rate of data consumption by 
+Any of these layers can be scaled by adding computing resources. Examine the performance of these components regularly
+as your use case evolves and add resources as needed. When Logstash routinely throttles incoming events, consider
+adding storage for your message queue. Alternately, increase the Elasticsearch cluster's rate of data consumption by
 adding more Logstash indexing instances.
diff --git a/docs/asciidoc/static/filter.asciidoc b/docs/static/filter.asciidoc
similarity index 100%
rename from docs/asciidoc/static/filter.asciidoc
rename to docs/static/filter.asciidoc
diff --git a/docs/static/getting-started-with-logstash.asciidoc b/docs/static/getting-started-with-logstash.asciidoc
new file mode 100644
index 00000000000..95479e260f1
--- /dev/null
+++ b/docs/static/getting-started-with-logstash.asciidoc
@@ -0,0 +1,167 @@
+[[getting-started-with-logstash]]
+== Getting Started with Logstash
+
+This section guides you through the process of installing Logstash and verifying that everything is running properly.
+After learning how to stash your first event, you go on to create a more advanced pipeline that takes Apache web logs as
+input, parses the logs, and writes the parsed data to an Elasticsearch cluster. Then you learn how to stitch together multiple input and output plugins to unify data from a variety of disparate sources.
+
+This section includes the following topics:
+
+* <<installing-logstash>>
+* <<first-event>>
+* <<advanced-pipeline>>
+* <<multiple-input-output-plugins>>
+
+[[installing-logstash]]
+=== Installing Logstash
+
+NOTE: Logstash requires Java 8 or later. Use the
+http://www.oracle.com/technetwork/java/javase/downloads/index.html[official Oracle distribution] or an open-source
+distribution such as http://openjdk.java.net/[OpenJDK].
+
+To check your Java version, run the following command:
+
+[source,shell]
+java -version
+
+On systems with Java installed, this command produces output similar to the following:
+
+[source,shell]
+java version "1.8.0_65"
+Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
+Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)
+
+[float]
+[[installing-binary]]
+=== Installing from a Downloaded Binary
+
+Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
+Unpack the file. Do not install Logstash into a directory path that contains colon (:) characters. 
+
+On supported Linux operating systems, you can use a package manager to install Logstash.
+
+[float]
+[[package-repositories]]
+=== Installing from Package Repositories
+
+We also have repositories available for APT and YUM based distributions. Note
+that we only provide binary packages, but no source packages, as the packages
+are created as part of the Logstash build.
+
+We have split the Logstash package repositories by version into separate urls
+to avoid accidental upgrades across major or minor versions. For all 2.3.x
+releases use 2.3 as version number, for 2.2.x use 2.2, etc.
+
+We use the PGP key
+https://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
+Elastic's Signing Key, with fingerprint
+
+    4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
+
+to sign all our packages. It is available from https://pgp.mit.edu.
+
+[float]
+==== APT
+
+Download and install the Public Signing Key:
+
+[source,sh]
+--------------------------------------------------
+wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
+--------------------------------------------------
+
+Add the repository definition to your `/etc/apt/sources.list` file:
+
+["source","sh",subs="attributes,callouts"]
+--------------------------------------------------
+echo "deb https://packages.elastic.co/logstash/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list
+--------------------------------------------------
+
+[WARNING]
+==================================================
+Use the `echo` method described above to add the Logstash repository.  Do not
+use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not
+provide a source package. If you have added the `deb-src` entry, you will see an
+error like the following:
+
+    Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)
+
+Just delete the `deb-src` entry from the `/etc/apt/sources.list` file and the
+installation should work as expected.
+==================================================
+
+Run `sudo apt-get update` and the repository is ready for use. You can install
+it with:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get update && sudo apt-get install logstash
+--------------------------------------------------
+
+[float]
+==== YUM
+
+Download and install the public signing key:
+
+[source,sh]
+--------------------------------------------------
+rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
+--------------------------------------------------
+
+Add the following in your `/etc/yum.repos.d/` directory
+in a file with a `.repo` suffix, for example `logstash.repo`
+
+["source","sh",subs="attributes,callouts"]
+--------------------------------------------------
+[logstash-{branch}]
+name=Logstash repository for {branch}.x packages
+baseurl=https://packages.elastic.co/logstash/{branch}/centos
+gpgcheck=1
+gpgkey=https://packages.elastic.co/GPG-KEY-elasticsearch
+enabled=1
+--------------------------------------------------
+
+And your repository is ready for use. You can install it with:
+
+[source,sh]
+--------------------------------------------------
+yum install logstash
+--------------------------------------------------
+
+[[first-event]]
+=== Stashing Your First Event
+
+First, let's test your Logstash installation by running the most basic _Logstash pipeline_.
+
+A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
+plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
+the data to a destination.
+
+//TODO: REPLACE WITH NEW IMAGE
+
+image::static/images/basic_logstash_pipeline.png[]
+
+To test your Logstash installation, run the most basic Logstash pipeline:
+
+["source","sh",subs="attributes"]
+--------------------------------------------------
+cd logstash-{logstash_version}
+bin/logstash -e 'input { stdin { } } output { stdout {} }'
+--------------------------------------------------
+
+The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the
+command line lets you quickly test configurations without having to edit a file between iterations.
+The pipeline in the example takes input from the standard input, `stdin`, and moves that input to the standard output,
+`stdout`, in a structured format.
+
+After starting Logstash, wait until you see "Pipeline main started" and then enter `hello world` at the command prompt:
+
+[source,shell]
+hello world
+2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
+
+Logstash adds timestamp and IP address information to the message. Exit Logstash by issuing a *CTRL-D* command in the
+shell where Logstash is running.
+
+Congratulations! You've created and run a basic Logstash pipeline. Next, you learn how to create a more realistic pipeline.
+
diff --git a/docs/static/glossary.asciidoc b/docs/static/glossary.asciidoc
new file mode 100644
index 00000000000..f65088dcaf8
--- /dev/null
+++ b/docs/static/glossary.asciidoc
@@ -0,0 +1,79 @@
+[[glossary]]
+== Glossary of Terms
+
+[[glossary-metadata]]@metadata ::
+  A special field for storing content that you don't want to include in output <<glossary-event,events>>. For example, the `@metadata`
+  field is useful for creating transient fields for use in <<glossary-conditional,conditional>> statements.
+    
+[[glossary-codec-plugin]]codec plugin::
+  A Logstash <<glossary-plugin,plugin>> that changes the data representation of an <<glossary-event,event>>. Codecs are essentially stream filters that can operate as part of an input or output. Codecs enable you to separate the transport of messages from the serialization process. Popular codecs include json, msgpack, and plain (text).
+  
+[[glossary-conditional]]conditional::
+  A control flow that executes certain actions based on whether a statement (also called a condition) is true or false. Logstash supports `if`, `else if`, and `else` statements. You can use conditional statements to apply filters and send events to a specific output based on conditions that you specify. 
+    
+[[glossary-event]]event::
+	A single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the Logstash <<glossary-pipeline,pipeline>>.
+    
+[[glossary-field]]field::
+  An <<glossary-event,event>> property. For example, each event in an apache access log has properties, such as a status
+  code (200, 404), request path ("/", "index.html"), HTTP verb (GET, POST), client IP address, and so on. Logstash uses
+  the term "fields" to refer to these properties.
+  
+[[glossary-field-reference]]field reference::
+  A reference to an event <<glossary-field,field>>. This reference may appear in an output block or filter block in the
+  Logstash config file. Field references are typically wrapped in square (`[]`) brackets, for example `[fieldname]`. If
+  you are referring to a top-level field, you can omit the `[]` and simply use the field name. To refer to a nested
+  field, you specify the full path to that field: `[top-level field][nested field]`.
+
+[[glossary-filter-plugin]]filter plugin::
+  A Logstash <<glossary-plugin,plugin>> that performs intermediary processing on an <<glossary-event,event>>. Typically, filters act upon
+  event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to
+  configuration rules. Filters are often applied conditionally depending on the characteristics of the event. Popular
+  filter plugins include grok, mutate, drop, clone, and geoip. Filter stages are optional.
+  
+[[glossary-gem]]gem::
+  A self-contained package of code that's hosted on https://rubygems.org[RubyGems.org]. Logstash <<glossary-plugin,plugins>> are packaged as
+  Ruby Gems. You can use the Logstash <<glossary-plugin-manager,plugin manager>> to manage Logstash gems.
+  
+[[glossary-hot-thread]]hot thread::
+  A Java thread that has high CPU usage and executes for a longer than normal period of time.
+  
+[[glossary-input-plugin]]input plugin::
+  A Logstash <<glossary-plugin,plugin>> that reads <<glossary-event,event>> data from a specific source. Input plugins are the first stage in the Logstash event processing <<glossary-pipeline,pipeline>>. Popular input plugins include file, syslog, redis, and beats.
+  
+[[glossary-indexer]]indexer::
+	A Logstash instance that is tasked with interfacing with an Elasticsearch cluster in order to index <<glossary-event,event>> data.
+    
+[[glossary-message-broker]]message broker::
+  Also referred to as a _message buffer_ or _message queue_, a message broker is external software (such as Redis, Kafka, or RabbitMQ) that stores messages from the Logstash shipper instance as an intermediate store, waiting to be processed by the Logstash indexer instance.
+ 
+[[glossary-output-plugin]]output plugin::
+  A Logstash <<glossary-plugin,plugin>> that writes <<glossary-event,event>> data to a specific destination. Outputs are the final stage in
+  the event <<glossary-pipeline,pipeline>>. Popular output plugins include elasticsearch, file, graphite, and
+  statsd.  
+  
+[[glossary-pipeline]]pipeline::
+  A term used to describe the flow of <<glossary-event,events>> through the Logstash workflow. A pipeline typically consists of a series of
+  input, filter, and output stages. <<glossary-input-plugin,Input>> stages get data from a source and generate events,
+  <<glossary-filter-plugin,filter>> stages, which are optional, modify the event data, and
+  <<glossary-output-plugin,output>> stages write the data to a destination. Inputs and outputs support <<glossary-codec-plugin,codecs>> that enable you to encode or decode the data as it enters or exits the pipeline without having to use
+  a separate filter. 
+  
+[[glossary-plugin]]plugin::
+  A self-contained software package that implements one of the stages in the Logstash event processing
+  <<glossary-pipeline,pipeline>>. The list of available plugins includes <<glossary-input-plugin,input plugins>>,
+  <<glossary-output-plugin,output plugins>>, <<glossary-codec-plugin,codec plugins>>, and
+  <<glossary-filter-plugin,filter plugins>>. The plugins are implemented as Ruby <<glossary-gem,gems>> and hosted on
+  https://rubygems.org[RubyGems.org]. You define the stages of an event processing <<glossary-pipeline,pipeline>> by configuring plugins. 
+ 
+[[glossary-plugin-manager]]plugin manager::
+  Accessed via the `bin/logstash-plugin` script, the plugin manager enables you to manage the lifecycle of
+  <<glossary-plugin,plugins>> in your Logstash deployment. You can install, uninstall, and upgrade plugins by using the
+  plugin manager Command Line Interface (CLI).
+
+[[shipper]]shipper::
+	An instance of Logstash that send events to another instance of Logstash, or some other application.
+    
+[[worker]]worker::
+	The filter thread model used by Logstash, where each worker receives an <<glossary-event,event>> and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive.
+
diff --git a/docs/static/images/basic_logstash_pipeline.png b/docs/static/images/basic_logstash_pipeline.png
new file mode 100644
index 00000000000..61341fc68ac
Binary files /dev/null and b/docs/static/images/basic_logstash_pipeline.png differ
diff --git a/docs/asciidoc/static/images/deploy_1.png b/docs/static/images/deploy_1.png
similarity index 100%
rename from docs/asciidoc/static/images/deploy_1.png
rename to docs/static/images/deploy_1.png
diff --git a/docs/asciidoc/static/images/deploy_2.png b/docs/static/images/deploy_2.png
similarity index 100%
rename from docs/asciidoc/static/images/deploy_2.png
rename to docs/static/images/deploy_2.png
diff --git a/docs/static/images/deploy_3.png b/docs/static/images/deploy_3.png
new file mode 100644
index 00000000000..96bc119c3e0
Binary files /dev/null and b/docs/static/images/deploy_3.png differ
diff --git a/docs/asciidoc/static/images/deploy_4.png b/docs/static/images/deploy_4.png
similarity index 100%
rename from docs/asciidoc/static/images/deploy_4.png
rename to docs/static/images/deploy_4.png
diff --git a/docs/asciidoc/static/images/deploy_5.png b/docs/static/images/deploy_5.png
similarity index 100%
rename from docs/asciidoc/static/images/deploy_5.png
rename to docs/static/images/deploy_5.png
diff --git a/docs/asciidoc/static/images/deploy_6.png b/docs/static/images/deploy_6.png
similarity index 100%
rename from docs/asciidoc/static/images/deploy_6.png
rename to docs/static/images/deploy_6.png
diff --git a/docs/asciidoc/static/images/deploy_7.png b/docs/static/images/deploy_7.png
similarity index 100%
rename from docs/asciidoc/static/images/deploy_7.png
rename to docs/static/images/deploy_7.png
diff --git a/docs/asciidoc/static/images/logstash.png b/docs/static/images/logstash.png
similarity index 100%
rename from docs/asciidoc/static/images/logstash.png
rename to docs/static/images/logstash.png
diff --git a/docs/static/images/pipeline_correct_load.png b/docs/static/images/pipeline_correct_load.png
new file mode 100644
index 00000000000..02e7f484ed7
Binary files /dev/null and b/docs/static/images/pipeline_correct_load.png differ
diff --git a/docs/static/images/pipeline_overload.png b/docs/static/images/pipeline_overload.png
new file mode 100644
index 00000000000..4b8dd0dc667
Binary files /dev/null and b/docs/static/images/pipeline_overload.png differ
diff --git a/docs/asciidoc/static/include/pluginbody.asciidoc b/docs/static/include/pluginbody.asciidoc
similarity index 96%
rename from docs/asciidoc/static/include/pluginbody.asciidoc
rename to docs/static/include/pluginbody.asciidoc
index 2a073e03290..bb44db58c3b 100644
--- a/docs/asciidoc/static/include/pluginbody.asciidoc
+++ b/docs/static/include/pluginbody.asciidoc
@@ -1,4 +1,4 @@
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/asciidoc/static/include/pluginbody.asciidoc ?>]
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/include/pluginbody.asciidoc ?>]
 
 === How to write a Logstash {plugintype} plugin
 
@@ -262,7 +262,7 @@ endif::filter_method[]
 // Output (conditionally recognized by the presence of the receive_method
 // attribute)
 // /////////////////////////////////////////////////////////////////////////////
-ifdef::receive_method[]
+ifdef::multi_receive_method[]
 [source,ruby]
 [subs="attributes"]
 ----------------------------------
@@ -275,17 +275,43 @@ require "logstash/namespace"
 class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
   config_name "example"
 
+  # If declared logstash will only allow a single instance of this plugin
+  # to exist, regardless of how many CPU cores logstash detects. This is best
+  # used in cases like the File output, where separate threads writing to a single
+  # File would only cause problems.
+  #
+  # respond_to? check needed for backwards compatibility with < 2.2 Logstashes
+  declare_workers_not_supported! if self.respond_to?(:declare_workers_not_supported!)
+
+  # If declared threadsafe logstash will only ever create one
+  # instance of this plugin per pipeline.
+  # That instance will be shared across all workers
+  # It is up to the plugin author to correctly write concurrent code!
+  #
+  # respond_to? check needed for backwards compatibility with < 2.2 Logstashes
+  declare_threadsafe! if self.respond_to?(:declare_threadsafe!)
+
   public
   def register
+    # Does the same thing as declare_workers_not_supported!
+    # But works in < 2.2 logstashes
+    # workers_not_supported
   end # def register
 
   public
+  # Takes an array of events
+  def multi_receive(events)
+  end # def multi_receive
+
+  public
+  # Needed for logstash < 2.2 compatibility
+  # Takes events one at a time
   def receive(event)
   end # def receive
 
 end # class LogStash::{pluginclass}::{pluginnamecap}
 ----------------------------------
-endif::receive_method[]
+endif::multi_receive_method[]
 
 ==== Coding {plugintype} plugins
 
@@ -681,7 +707,7 @@ endif::blockinput[]
 endif::run_method[]
 
 // /////////////////////////////////////////////////////////////////////////////
-// If receive_method is defined (should only be for output plugin page)
+// If multi_receive_method is defined (should only be for output plugin page)
 // /////////////////////////////////////////////////////////////////////////////
 ifdef::receive_method[]
 
@@ -819,7 +845,7 @@ Gem::Specification.new do |s|
   s.version = '0.1.0'
   s.licenses = ['Apache License (2.0)']
   s.summary = "This {plugintype} does x, y, z in Logstash"
-  s.description = "This gem is a logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/plugin install gemname. This gem is not a stand-alone program"
+  s.description = "This gem is a logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program"
   s.authors = ["Elastic"]
   s.email = 'info@elastic.co'
   s.homepage = "http://www.elastic.co/guide/en/logstash/current/index.html"
@@ -857,7 +883,7 @@ time.
 
 **Version messaging from Logstash**
 
-If you start Logstash with the `--verbose` flag, you will see messages like
+If you start Logstash with the `--log.level verbose` flag, you will see messages like
 these to indicate the relative maturity indicated by the plugin version number:
 
 ** **0.1.x**
@@ -1025,7 +1051,7 @@ environment, and `0.1.0` with the correct version number from the gemspec file.
 [source,sh]
 [subs="attributes"]
 ----------------------------------
-bin/plugin install /my/logstash/plugins/logstash-{plugintype}-{pluginname}/logstash-{plugintype}-{pluginname}-0.1.0.gem
+bin/logstash-plugin install /my/logstash/plugins/logstash-{plugintype}-{pluginname}/logstash-{plugintype}-{pluginname}-0.1.0.gem
 ----------------------------------
 +
 * After running this, you should see feedback from Logstash that it was
@@ -1046,7 +1072,7 @@ currently available:
 
 [source,sh]
 ----------------------------------
-bin/plugin list
+bin/logstash-plugin list
 ----------------------------------
 Depending on what you have installed, you might see a short or long list of
 plugins: inputs, codecs, filters and outputs.
@@ -1249,7 +1275,7 @@ by running:
 [source,sh]
 [subs="attributes"]
 ----------------------------------
-bin/plugin install logstash-{plugintype}-mypluginname
+bin/logstash-plugin install logstash-{plugintype}-mypluginname
 ----------------------------------
 
 ==== Contributing your source code to https://github.com/logstash-plugins[logstash-plugins]
diff --git a/docs/asciidoc/static/input.asciidoc b/docs/static/input.asciidoc
similarity index 100%
rename from docs/asciidoc/static/input.asciidoc
rename to docs/static/input.asciidoc
diff --git a/docs/asciidoc/static/introduction.asciidoc b/docs/static/introduction.asciidoc
similarity index 72%
rename from docs/asciidoc/static/introduction.asciidoc
rename to docs/static/introduction.asciidoc
index ec9af52cd15..597e5b41c58 100644
--- a/docs/asciidoc/static/introduction.asciidoc
+++ b/docs/static/introduction.asciidoc
@@ -1,15 +1,3 @@
-[[introduction]]
-== Logstash Introduction
-
-Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically 
-unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all 
-your data for diverse advanced downstream analytics and visualization use cases.
-
-While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any 
-type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many 
-native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater 
-volume and variety of data.
-
 [float]
 [[power-of-logstash]]
 == The Power of Logstash
@@ -39,14 +27,13 @@ Collect more, so you can know more. Logstash welcomes data of all shapes and siz
 Where it all started.
 
 * Handle all types of logging data
-** Easily ingest a multitude of web logs like <<parsing-into-es,Apache>>, and application 
+** Easily ingest a multitude of web logs like <<advanced-pipeline,Apache>>, and application
 logs like <<plugins-inputs-log4j,log4j>> for Java
-** Capture many other log formats like <<plugins-inputs-syslog,syslog>>, 
+** Capture many other log formats like <<plugins-inputs-syslog,syslog>>,
 <<plugins-inputs-eventlog,Windows event logs>>, networking and firewall logs, and more
-* Enjoy complementary secure log forwarding capabilities with https://github.com/elastic/logstash-forwarder[Logstash 
-Forwarder]
-* Collect metrics from <<plugins-inputs-ganglia,Ganglia>>, <<plugins-codecs-collectd,collectd>>, 
-<<plugins-codecs-netflow,NetFlow>>, <<plugins-inputs-jmx,JMX>>, and many other infrastructure 
+* Enjoy complementary secure log forwarding capabilities with https://github.com/elastic/beats/tree/master/filebeat[Filebeat]
+* Collect metrics from <<plugins-inputs-ganglia,Ganglia>>, <<plugins-codecs-collectd,collectd>>,
+<<plugins-codecs-netflow,NetFlow>>, <<plugins-inputs-jmx,JMX>>, and many other infrastructure
 and application platforms over <<plugins-inputs-tcp,TCP>> and <<plugins-inputs-udp,UDP>>
 
 [float]
@@ -54,12 +41,12 @@ and application platforms over <<plugins-inputs-tcp,TCP>> and <<plugins-inputs-u
 
 Unlock the World Wide Web.
 
-* Transform <<plugins-inputs-http,HTTP requests>> into events 
+* Transform <<plugins-inputs-http,HTTP requests>> into events
 (https://www.elastic.co/blog/introducing-logstash-input-http-plugin[blog])
 ** Consume from web service firehoses like <<plugins-inputs-twitter,Twitter>> for social sentiment analysis
 ** Webhook support for GitHub, HipChat, JIRA, and countless other applications
 ** Enables many https://www.elastic.co/guide/en/watcher/current/logstash-integration.html[Watcher] alerting use cases
-* Create events by polling <<plugins-inputs-http_poller,HTTP endpoints>> on demand 
+* Create events by polling <<plugins-inputs-http_poller,HTTP endpoints>> on demand
 (https://www.elastic.co/blog/introducing-logstash-http-poller[blog])
 ** Universally capture health, performance, metrics, and other types of data from web application interfaces
 ** Perfect for scenarios where the control of polling is preferred over receiving
@@ -69,10 +56,10 @@ Unlock the World Wide Web.
 
 Discover more value from the data you already own.
 
-* Better understand your data from any relational database or NoSQL store with a 
+* Better understand your data from any relational database or NoSQL store with a
 <<plugins-inputs-jdbc,JDBC>> interface (https://www.elastic.co/blog/logstash-jdbc-input-plugin[blog])
-* Unify diverse data streams from messaging queues like Apache <<plugins-outputs-kafka,Kafka>> 
-(https://www.elastic.co/blog/logstash-kafka-intro[blog]), <<plugins-outputs-rabbitmq,RabbitMQ>>, 
+* Unify diverse data streams from messaging queues like Apache <<plugins-outputs-kafka,Kafka>>
+(https://www.elastic.co/blog/logstash-kafka-intro[blog]), <<plugins-outputs-rabbitmq,RabbitMQ>>,
 <<plugins-outputs-sqs,Amazon SQS>>, and <<plugins-outputs-zeromq,ZeroMQ>>
 
 [float]
@@ -80,41 +67,41 @@ Discover more value from the data you already own.
 
 Explore an expansive breadth of other data.
 
-* In this age of technological advancement, the massive IoT world unleashes endless use cases through capturing and 
+* In this age of technological advancement, the massive IoT world unleashes endless use cases through capturing and
 harnessing data from connected sensors.
-* Logstash is the common event collection backbone for ingestion of data shipped from mobile devices to intelligent 
+* Logstash is the common event collection backbone for ingestion of data shipped from mobile devices to intelligent
 homes, connected vehicles, healthcare sensors, and many other industry specific applications.
-* https://www.elastic.co/elasticon/2015/sf/if-it-moves-measure-it-logging-iot-with-elk[Watch] as Logstash, in 
-conjunction with the broader ELK stack, centralizes and enriches sensor data to gain deeper knowledge regarding a 
+* https://www.elastic.co/elasticon/2015/sf/if-it-moves-measure-it-logging-iot-with-elk[Watch] as Logstash, in
+conjunction with the broader ELK stack, centralizes and enriches sensor data to gain deeper knowledge regarding a
 residential home.
 
 [float]
 == Easily Enrich Everything
 
-The better the data, the better the knowledge. Clean and transform your data during ingestion to gain near real-time 
-insights immediately at index or output time. Logstash comes out-of-box with many aggregations and mutations along 
+The better the data, the better the knowledge. Clean and transform your data during ingestion to gain near real-time
+insights immediately at index or output time. Logstash comes out-of-box with many aggregations and mutations along
 with pattern matching, geo mapping, and dynamic lookup capabilities.
 
-* <<plugins-filters-grok,Grok>> is the bread and butter of Logstash filters and is used ubiquitously to derive 
-structure out of unstructured data. Enjoy a wealth of integrated patterns aimed to help quickly resolve web, systems, 
+* <<plugins-filters-grok,Grok>> is the bread and butter of Logstash filters and is used ubiquitously to derive
+structure out of unstructured data. Enjoy a wealth of integrated patterns aimed to help quickly resolve web, systems,
 networking, and other types of event formats.
-* Expand your horizons by deciphering <<plugins-filters-geoip,geo coordinates>> from IP addresses, normalizing 
-<<plugins-filters-date,date>> complexity, simplifying <<plugins-filters-kv,key-value pairs>> and 
-<<plugins-filters-csv,CSV>> data, <<plugins-filters-anonymize,anonymizing>> sensitive information, and further 
-enriching your data with <<plugins-filters-translate,local lookups>> or Elasticsearch 
+* Expand your horizons by deciphering <<plugins-filters-geoip,geo coordinates>> from IP addresses, normalizing
+<<plugins-filters-date,date>> complexity, simplifying <<plugins-filters-kv,key-value pairs>> and
+<<plugins-filters-csv,CSV>> data, <<plugins-filters-anonymize,anonymizing>> sensitive information, and further
+enriching your data with <<plugins-filters-translate,local lookups>> or Elasticsearch
 <<plugins-filters-elasticsearch,queries>>.
-* Codecs are often used to ease the processing of common event structures like <<plugins-codecs-json,JSON>> 
+* Codecs are often used to ease the processing of common event structures like <<plugins-codecs-json,JSON>>
 and <<plugins-codecs-multiline,multiline>> events.
 
 [float]
 == Choose Your Stash
 
-Route your data where it matters most. Unlock various downstream analytical and operational use cases by storing, 
+Route your data where it matters most. Unlock various downstream analytical and operational use cases by storing,
 analyzing, and taking action on your data.
 
 [cols="a,a"]
 |=======================================================================
-| 
+|
 
 *Analysis*
 
@@ -129,9 +116,9 @@ analyzing, and taking action on your data.
 * <<plugins-outputs-s3,S3>>
 * <<plugins-outputs-google_cloud_storage,Google Cloud Storage>>
 
-| 
+|
 
-*Monitoring*          
+*Monitoring*
 
 * <<plugins-outputs-nagios,Nagios>>
 * <<plugins-outputs-ganglia,Ganglia>>
@@ -140,7 +127,7 @@ analyzing, and taking action on your data.
 * <<plugins-outputs-datadog,Datadog>>
 * <<plugins-outputs-cloudwatch,CloudWatch>>
 
-| 
+|
 
 *Alerting*
 
diff --git a/docs/static/life-of-an-event.asciidoc b/docs/static/life-of-an-event.asciidoc
new file mode 100644
index 00000000000..d1431d116b6
--- /dev/null
+++ b/docs/static/life-of-an-event.asciidoc
@@ -0,0 +1,159 @@
+[[pipeline]]
+== How Logstash Works
+
+The Logstash event processing pipeline has three stages: inputs -> filters ->
+outputs. Inputs generate events, filters modify them, and outputs ship them
+elsewhere. Inputs and outputs support codecs that enable you to encode or decode
+the data as it enters or exits the pipeline without having to use a separate
+filter.
+
+[float]
+=== Inputs
+You use inputs to get data into Logstash. Some of the more commonly-used inputs
+are:
+
+* *file*: reads from a file on the filesystem, much like the UNIX command
+`tail -0F`
+* *syslog*: listens on the well-known port 514 for syslog messages and parses
+according to the RFC3164 format
+* *redis*: reads from a redis server, using both redis channels and redis lists.
+Redis is often used as a "broker" in a centralized Logstash installation, which
+queues Logstash events from remote Logstash "shippers".
+* *beats*: processes events sent by https://www.elastic.co/downloads/beats/filebeat[Filebeat].
+
+For more information about the available inputs, see
+<<input-plugins,Input Plugins>>.
+
+[float]
+=== Filters
+Filters are intermediary processing devices in the Logstash pipeline. You can
+combine filters with conditionals to perform an action on an event if it meets
+certain criteria. Some useful filters include:
+
+* *grok*: parse and structure arbitrary text. Grok is currently the best way in
+Logstash to parse unstructured log data into something structured and queryable.
+With 120 patterns built-in to Logstash, it's more than likely you'll find one
+that meets your needs!
+* *mutate*: perform general transformations on event fields. You can rename,
+remove, replace, and modify fields in your events.
+* *drop*: drop an event completely, for example, 'debug' events.
+* *clone*: make a copy of an event, possibly adding or removing fields.
+* *geoip*: add information about geographical location of IP addresses (also
+displays amazing charts in Kibana!)
+
+For more information about the available filters, see
+<<filter-plugins,Filter Plugins>>.
+
+[float]
+=== Outputs
+Outputs are the final phase of the Logstash pipeline. An event can pass through
+multiple outputs, but once all output processing is complete, the event has
+finished its execution. Some commonly used outputs include:
+
+* *elasticsearch*: send event data to Elasticsearch. If you're planning to save
+your data in an efficient, convenient, and easily queryable format...
+Elasticsearch is the way to go. Period. Yes, we're biased :)
+* *file*: write event data to a file on disk.
+* *graphite*: send event data to graphite, a popular open source tool for
+storing and graphing metrics. http://graphite.readthedocs.io/en/latest/
+* *statsd*: send event data to statsd, a service that "listens for statistics,
+like counters and timers, sent over UDP and sends aggregates to one or more
+pluggable backend services". If you're already using statsd, this could be
+useful for you!
+
+For more information about the available outputs, see
+<<output-plugins,Output Plugins>>.
+
+[float]
+=== Codecs
+Codecs are basically stream filters that can operate as part of an input or
+output. Codecs enable you to easily separate the transport of your messages from
+the serialization process. Popular codecs include `json`, `msgpack`, and `plain`
+(text).
+
+* *json*: encode or decode data in the JSON format.
+* *multiline*: merge multiple-line text events such as java exception and
+stacktrace messages into a single event.
+
+For more information about the available codecs, see
+<<codec-plugins,Codec Plugins>>.
+
+[[fault-tolerance]]
+=== Fault Tolerance
+
+Logstash keeps all events in main memory during processing. Logstash responds to a SIGTERM by attempting to halt inputs and waiting for pending events to finish processing before shutting down. When the pipeline cannot be flushed due to a stuck output or filter, Logstash waits indefinitely. For example, when a pipeline sends output to a database that is unreachable by the Logstash instance, the instance waits indefinitely after receiving a SIGTERM.
+
+To enable Logstash to detect these situations and terminate with a stalled pipeline, use the `--pipeline.unsafe_shutdown` flag.
+
+WARNING: Unsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason result in data loss. Shut down Logstash safely whenever possible.
+
+[[execution-model]]
+==== Execution Model
+
+The Logstash pipeline coordinates the execution of inputs, filters, and outputs. The following schematic sketches the data flow of a pipeline:
+
+[source,js]
+---------------------------------------------------
+input threads | pipeline worker threads
+---------------------------------------------------
+
+Pipelines in the current release of Logstash process filtering and output in the same thread. Prior to the 2.2 release of Logstash, filtering and output were distinct stages handled by distinct threads.
+This change to the Logstash architecture improves performance and enables future persistence capabilities. The new pipeline substantially improves thread liveness, decreases resource usage, and increases throughput. The current Logstash pipeline is a micro-batching pipeline, which is inherently more efficient than a one-at-a-time approach. These efficiencies come in many places, two of the more prominent ones being a reduction in contention and a consequent improvement in thread liveness. These efficiencies are especially noticeable on many-core machines.
+
+Each `input {}` statement in the Logstash configuration file runs in its own thread. Inputs write events to a common Java https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/SynchronousQueue.html[SynchronousQueue]. This queue holds no events, instead transferring each pushed event to a free worker, blocking if all workers are busy. Each pipeline worker thread takes a batch of events off this queue, creating a buffer per worker, runs the batch of  events through the configured filters, then runs the filtered events through any outputs. The size of the batch and number of pipeline worker threads are configurable. The following pseudocode illustrates the process flow:
+
+[source,ruby]
+synchronous_queue = SynchronousQueue.new
+inputs.each do |input|
+  Thread.new do
+    input.run(synchronous_queue)
+  end
+end
+num_pipeline_workers.times do
+  Thread.new do
+    while true
+      batch = take_batch(synchronous_queue, batch_size, batch_delay)
+      output_batch(filter_batch(batch))
+    end
+  end
+end
+wait_for_threads_to_terminate()
+
+There are three configurable options in the pipeline, `--pipeline.workers`, `--pipeline.batch.size`, and `--pipeline.batch.delay`.
+The `--pipeline.workers` or `-w` parameter determines how many threads to run for filter and output processing. If you find that events are backing up, or that the CPU is not saturated, consider increasing the value of this parameter to make better use of available processing power. Good results can even be found increasing this number past the number of available processors as these threads may spend significant time in an I/O wait state when writing to external systems. Legal values for this parameter are positive integers.
+
+The `--pipeline.batch.size` or `-b` parameter defines the maximum number of events an individual worker thread collects before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Some hardware configurations require you to increase JVM heap size by setting the `LS_HEAP_SIZE` variable to avoid performance degradation with this option. Values of this parameter in excess of the optimum range cause performance degradation due to frequent garbage collection or JVM crashes related to out-of-memory exceptions. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, issues https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[bulk requests] for each batch received. Tuning the `-b` parameter adjusts the size of bulk requests sent to Elasticsearch.
+
+The `--pipeline.batch.delay` option rarely needs to be tuned. This option adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that Logstash waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, Logstash begins to execute filters and outputs.The maximum time that Logstash waits between receiving an event and processing that event in a filter is the product of the `pipeline_batch_delay` and  `pipeline_batch_size` settings.
+
+[float]
+==== Notes on Pipeline Configuration and Performance
+
+The total number of inflight events is determined by the product of the  `pipeline_workers` and `pipeline_batch_size` parameters. This product is referred to as the _inflight count_.  Keep the value of the inflight count in mind as you adjust the `pipeline_workers` and `pipeline_batch_size` parameters. Pipelines that intermittently receive large events at irregular intervals require sufficient memory to handle these spikes. Configure the `LS_HEAP_SIZE` option accordingly.
+
+The Logstash defaults are chosen to provide fast, safe performance for most users. To increase performance, increase the number of pipeline workers or the batch size, taking into account the following suggestions:
+
+Measure each change to make sure it increases, rather than decreases, performance.
+Ensure that you leave enough memory available to cope with a sudden increase in event size. For example, an application that generates exceptions that are represented as large blobs of text.
+The number of workers may be set higher than the number of CPU cores since outputs often spend idle time in I/O wait conditions.
+
+Threads in Java have names and you can use the `jstack`, `top`, and the VisualVM graphical tools to figure out which resources a given thread uses.
+
+On Linux platforms, Logstash labels all the threads it can with something descriptive. For example, inputs show up as `[base]<inputname`, filter/output workers show up as `[base]>workerN`, where N is an integer.  Where possible, other threads are also labeled to help you identify their purpose.
+
+[float]
+==== Profiling the Heap
+
+When tuning Logstash you may have to adjust the heap size. You can use the https://visualvm.java.net/[VisualVM] tool to profile the heap. The *Monitor* pane in particular is useful for checking whether your heap allocation is sufficient for the current workload. The screenshots below show sample *Monitor* panes. The first pane examines a Logstash instance configured with too many inflight events. The second pane examines a Logstash instance configured with an appropriate amount of inflight events. Note that the specific batch sizes used here are most likely not applicable to your specific workload, as the memory demands of Logstash vary in large part based on the type of messages you are sending.
+
+image::static/images/pipeline_overload.png[]
+
+image::static/images/pipeline_correct_load.png[]
+
+In the first example we see that the CPU isn’t being used very efficiently. In fact, the JVM is often times having to stop the VM for “full GCs”. Full garbage collections are a common symptom of excessive memory pressure. This is visible in the spiky pattern on the CPU chart. In the more efficiently configured example, the GC graph pattern is more smooth, and the CPU is used in a more uniform manner. You can also see that there is ample headroom between the allocated heap size, and the maximum allowed, giving the JVM GC a lot of room to work with.
+
+Examining the in-depth GC statistics with a tool similar to the excellent https://visualvm.java.net/plugins.html[VisualGC] plugin shows that the over-allocated VM spends very little time in the efficient Eden GC, compared to the time spent in the more resource-intensive Old Gen “Full” GCs.
+
+NOTE: As long as the GC pattern is acceptable, heap sizes that occasionally increase to the maximum are acceptable. Such heap size spikes happen in response to a burst of large events passing through the pipeline. In general practice, maintain a gap between the used amount of heap memory and the maximum.
+This document is not a comprehensive guide to JVM GC tuning. Read the official http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html[Oracle guide] for more information on the topic. We also recommend reading http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance].
+
diff --git a/docs/asciidoc/static/logstash-glossary.asciidoc b/docs/static/logstash-glossary.asciidoc
similarity index 96%
rename from docs/asciidoc/static/logstash-glossary.asciidoc
rename to docs/static/logstash-glossary.asciidoc
index 9715ffa6c76..82f04943471 100644
--- a/docs/asciidoc/static/logstash-glossary.asciidoc
+++ b/docs/static/logstash-glossary.asciidoc
@@ -1,4 +1,4 @@
-== Glossary 
+== Glossary
 Logstash Glossary
 
 apache ::
@@ -9,7 +9,7 @@ agent ::
 
 
 broker ::
-	An intermediary used in a multi-tiered Logstash deployment which allows a queueing mechanism to be used. Examples of brokers are Redis, RabbitMQ, and Apache Kafka. This pattern is a common method of building fault-tolerance into a Logstash architecture. 
+	An intermediary used in a multi-tiered Logstash deployment which allows a queueing mechanism to be used. Examples of brokers are Redis, RabbitMQ, and Apache Kafka. This pattern is a common method of building fault-tolerance into a Logstash architecture.
 
 buffer::
 	Within Logstash, a temporary storage area where events can queue up, waiting to be processed. The default queue size is 20 events, but it is not recommended to increase this, as Logstash is not designed to operate as a queueing mechanism.
@@ -27,7 +27,7 @@ conditional::
 	In a computer programming context, a control flow which executes certain actions based on true/false values of a statement (called the condition). Often expressed in the form of "if ... then ... (elseif ...) else". Logstash has built-in conditionals to allow users control of the plugin pipeline.
 
 elasticsearch::
-	An open-source, Lucene-based, RESTful search and analytics engine written in Java, with supported clients in various languages such as Perl, Python, Ruby, Java, etc. 
+	An open-source, Lucene-based, RESTful search and analytics engine written in Java, with supported clients in various languages such as Perl, Python, Ruby, Java, etc.
 
 event::
 	In Logstash parlance, a single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the Logstash pipeline.
@@ -39,7 +39,7 @@ file::
 	A resource storing binary data (which might be text, image, application, etc.) on a physical storage media. In the Logstash context, a common input source which monitors a growing collection of text-based log lines.
 
 filter:
-	An intermediary processing mechanism in the Lostash pipeline. Typically, filters act upon event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to configuration rules. The second phase of the typical Logstash pipeline (inputs->filters->outputs). 
+	An intermediary processing mechanism in the Lostash pipeline. Typically, filters act upon event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to configuration rules. The second phase of the typical Logstash pipeline (inputs->filters->outputs).
 
 fluentd::
 	Like Logstash, another open-source tool for collecting logs and events, with plugins to extend functionality.
@@ -60,7 +60,7 @@ indexer::
 	Refers to a Logstash instance which is tasked with interfacing with an Elasticsearch cluster in order to index event data.
 
 input::
-	The means for ingesting data into Logstash. Inputs allow users to pull data from files, network sockets, other applications, etc. The initial phase of the typical Logstash pipeline (inputs->filters->outputs). 
+	The means for ingesting data into Logstash. Inputs allow users to pull data from files, network sockets, other applications, etc. The initial phase of the typical Logstash pipeline (inputs->filters->outputs).
 
 jar / jarfile::
 	A packaging method for Java libraries. Since Logstash runs on the JRuby runtime environment, it is possible to use these Java libraries to provide extra functionality to Logstash.
@@ -69,7 +69,7 @@ java::
 	An object-oriented programming language popular for its flexibility, extendability and portability.
 
 jRuby:
-	JRuby is a 100% Java implementation of the Ruby programming language, which allows Ruby to run in the JVM. Logstash typically runs in JRuby, which provides it with a fast, extensible runtime environment. 
+	JRuby is a 100% Java implementation of the Ruby programming language, which allows Ruby to run in the JVM. Logstash typically runs in JRuby, which provides it with a fast, extensible runtime environment.
 
 kibana::
 	A visual tool for viewing time-based data which has been stored in Elasticsearch. Kibana features a powerful set of functionality based on panels which query Elasticsearch in different ways.
@@ -87,7 +87,7 @@ lumberjack::
 	A protocol for shipping logs from one location to another, in a secure and optimized manner. Also the (deprecated) name of a software application, now known as Logstash Forwarder (LSF).
 
 output::
-	The means for passing event data out of Logstash into other applications, network endpoints, files, etc. The last phase of the typical Logstash pipeline (inputs->filters->outputs). 
+	The means for passing event data out of Logstash into other applications, network endpoints, files, etc. The last phase of the typical Logstash pipeline (inputs->filters->outputs).
 
 pipeline::
 	A term used to describe the flow of events through the Logstash workflow. The pipeline typically consists of a series of inputs, filters, and outputs.
@@ -129,4 +129,4 @@ type::
 	In Elasticsearch type, a type can be compared to a table in a relational database. Each type has a list of fields that can be specified for documents of that type. The mapping defines how each field in the document is analyzed. To index documents, it is required to specify both an index and a type.
 
 worker::
-	The filter thread model used by Logstash, where each worker receives an event and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety). 
+	The filter thread model used by Logstash, where each worker receives an event and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety).
diff --git a/docs/static/maintainer-guide.asciidoc b/docs/static/maintainer-guide.asciidoc
new file mode 100644
index 00000000000..7d3e0c9a029
--- /dev/null
+++ b/docs/static/maintainer-guide.asciidoc
@@ -0,0 +1,222 @@
+[[community-maintainer]]
+=== Logstash Plugins Community Maintainer Guide
+
+This document, to be read by new Maintainers, should explain their responsibilities.  It was inspired by the
+http://rfc.zeromq.org/spec:22[C4] document from the ZeroMQ project.  This document is subject to change and suggestions
+through Pull Requests and issues are strongly encouraged.
+
+[float]
+=== Contribution Guidelines
+
+For general guidance around contributing to Logstash Plugins, see the
+https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html[_Contributing to Logstash_] section.
+
+[float]
+=== Document Goals
+
+To help make the Logstash plugins community participation easy with positive feedback.
+
+To increase diversity.
+
+To reduce code review, merge and release dependencies on the core team by providing support and tools to the Community and
+Maintainers.
+
+To support the natural life cycle of a plugin.
+
+To codify the roles and responsibilities of: Maintainers and Contributors with specific focus on patch testing, code
+review, merging and release.
+
+[float]
+=== Development Workflow
+
+All Issues and Pull Requests must be tracked using the Github issue tracker.
+
+The plugin uses the http://www.apache.org/licenses/LICENSE-2.0[Apache 2.0 license]. Maintainers should check whether a
+patch introduces code which has an incompatible license. Patch ownership and copyright is defined in the Elastic
+https://www.elastic.co/contributor-agreement[Contributor License Agreement] (CLA).
+
+[float]
+==== Terminology
+
+A "Contributor" is a role a person assumes when providing a patch. Contributors will not have commit access to the
+repository. They need to sign the Elastic https://www.elastic.co/contributor-agreement[Contributor License Agreement]
+before a patch can be reviewed. Contributors can add themselves to the plugin Contributor list.
+
+A "Maintainer" is a role a person assumes when maintaining a plugin and keeping it healthy, including triaging issues, and
+reviewing and merging patches.
+
+[float]
+==== Patch Requirements
+
+A patch is a minimal and accurate answer to exactly one identified and agreed upon problem. It must conform to the
+https://github.com/elastic/logstash/blob/master/STYLE.md[code style guidelines] and must include RSpec tests that verify
+the fitness of the solution.
+
+A patch will be automatically tested by a CI system that will report on the Pull Request status.
+
+A patch CLA will be automatically verified and reported on the Pull Request status.
+
+A patch commit message has a single short (less than 50 character) first line summarizing the change, a blank second line,
+and any additional lines as necessary for change explanation and rationale.
+
+A patch is mergeable when it satisfies the above requirements and has been reviewed positively by at least one other
+person.
+
+[float]
+==== Development Process
+
+A user will log an issue on the issue tracker describing the problem they face or observe with as much detail as possible.
+
+To work on an issue, a Contributor forks the plugin repository and then works on their forked repository and submits a
+patch by creating a pull request back to the plugin.
+
+Maintainers must not merge patches where the author has not signed the CLA.
+
+Before a patch can be accepted it should be reviewed. Maintainers should merge accepted patches without delay.
+
+Maintainers should not merge their own patches except in exceptional cases, such as non-responsiveness from other
+Maintainers or core team for an extended period (more than 2 weeks).
+
+Reviewer’s comments should not be based on personal preferences.
+
+The Maintainers should label Issues and Pull Requests.
+
+Maintainers should involve the core team if help is needed to reach consensus.
+
+Review non-source changes such as documentation in the same way as source code changes.
+
+[float]
+==== Branch Management
+
+The plugin has a master branch that always holds the latest in-progress version and should always build.  Topic branches
+should kept to the minimum.
+
+[float]
+==== Changelog Management
+
+Every plugin should have a changelog (CHANGELOG.md).  If not, please create one.  When changes are made to a plugin, make sure to include a changelog entry under the respective version to document the change.  The changelog should be easily understood from a user point of view.  As we iterate and release plugins rapidly, users use the changelog as a mechanism for deciding whether to update.
+
+Changes that are not user facing should be tagged as `internal:`.  For example:
+
+[source,markdown]
+ - internal: Refactored specs for better testing
+ - config: Default timeout configuration changed from 10s to 5s
+
+[float]
+===== Detailed format of CHANGELOG.md
+
+Sharing a similar format of CHANGELOG.md in plugins ease readability for users.
+Please see following annotated example and see a concrete example in https://raw.githubusercontent.com/logstash-plugins/logstash-filter-date/master/CHANGELOG.md[logstash-filter-date].
+
+[source,markdown]
+----
+## 1.0.x                              // <1> <2>
+ - change description                 // <3>
+ - tag: change description            // <3> <4>
+ - tag1,tag2: change description      // <3> <5>
+ - tag: Multi-line description        // <3> <6>
+   must be indented and can use
+   additional markdown syntax
+                                      // <7>
+## 1.0.0                              // <8>
+[...]
+
+----
+<1> Latest version is the first line of CHANGELOG.md
+<2> Each version identifier should be a level-2 header using `##`
+<3> One change description is described as a list item using a dash `-` aligned under the version identifier
+<4> One change can be tagged by a word and suffixed by `:`. +
+    Common tags are `bugfix`, `feature`, `doc`, `test` or `internal`.
+<5> One change can have multiple tags separated by a comma and suffixed by `:`
+<6> A multi-line change description must be properly indented
+<7> Please take care to *separate versions with an empty line*
+<8> Previous version identifier
+
+[float]
+==== Continuous Integration
+
+Plugins are setup with automated continuous integration (CI) environments and there should be a corresponding badge on each Github page.  If it’s missing, please contact the Logstash core team.
+
+Every Pull Request opened automatically triggers a CI run.  To conduct a manual run, comment “Jenkins, please test this.” on the Pull Request.
+
+[float]
+=== Versioning Plugins
+
+Logstash core and its plugins have separate product development lifecycles. Hence the versioning and release strategy for
+the core and plugins do not have to be aligned. In fact, this was one of our goals during the great separation of plugins
+work in Logstash 1.5.
+
+At times, there will be changes in core API in Logstash, which will require mass update of plugins to reflect the changes
+in core. However, this does not happen frequently.
+
+For plugins, we would like to adhere to a versioning and release strategy that can better inform our users, about any
+breaking changes to the Logstash configuration formats and functionality.
+
+Plugin releases follows a three-placed numbering scheme X.Y.Z. where X denotes a major release version which may break
+compatibility with existing configuration or functionality. Y denotes releases which includes features which are backward
+compatible. Z denotes releases which includes bug fixes and patches.
+
+[float]
+==== Changing the version
+
+Version can be changed in the Gemspec, which needs to be associated with a changelog entry. Following this, we can publish
+the gem to RubyGem.org manually. At this point only the core developers can publish a gem.
+
+[float]
+==== Labeling
+
+Labeling is a critical aspect of maintaining plugins. All issues in GitHub should be labeled correctly so it can:
+
+* Provide good feedback to users/developers
+* Help prioritize changes
+* Be used in release notes
+
+Most labels are self explanatory, but here’s a quick recap of few important labels:
+
+* `bug`: Labels an issue as an unintentional defect
+* `needs details`: If a the issue reporter has incomplete details, please ask them for more info and label as needs
+details.
+* `missing cla`: Contributor License Agreement is missing and patch cannot be accepted without it
+* `adopt me`: Ask for help from the community to take over this issue
+* `enhancement`: New feature, not a bug fix
+* `needs tests`: Patch has no tests, and cannot be accepted without unit/integration tests
+* `docs`: Documentation related issue/PR
+
+[float]
+=== Logging
+
+Although it’s important not to bog down performance with excessive logging, debug level logs can be immensely helpful when
+diagnosing and troubleshooting issues with Logstash.  Please remember to liberally add debug logs wherever it makes sense
+as users will be forever gracious.
+
+[source,shell]
+@logger.debug("Logstash loves debug logs!", :actions => actions)
+
+[float]
+=== Contributor License Agreement (CLA) Guidance
+
+[qanda]
+Why is a https://www.elastic.co/contributor-agreement[CLA] required?::
+     We ask this of all Contributors in order to assure our users of the origin and continuing existence of the code. We
+     are not asking Contributors to assign copyright to us, but to give us the right to distribute a Contributor’s code
+     without restriction.
+
+Please make sure the CLA is signed by every Contributor prior to reviewing PRs and commits.::
+     Contributors only need to sign the CLA once and should sign with the same email as used in Github. If a Contributor
+     signs the CLA after a PR is submitted, they can refresh the automated CLA checker by pushing another
+     comment on the PR after 5 minutes of signing.
+
+[float]
+=== Need Help?
+
+Ping @logstash-core on Github to get the attention of the Logstash core team.
+
+[float]
+=== Community Administration
+
+The core team is there to support the plugin Maintainers and overall ecosystem.
+
+Maintainers should propose Contributors to become a Maintainer.
+
+Contributors and Maintainers should follow the Elastic Community https://www.elastic.co/community/codeofconduct[Code of
+Conduct].  The core team should block or ban "bad actors".
diff --git a/docs/asciidoc/static/managing-multiline-events.asciidoc b/docs/static/managing-multiline-events.asciidoc
similarity index 65%
rename from docs/asciidoc/static/managing-multiline-events.asciidoc
rename to docs/static/managing-multiline-events.asciidoc
index 1185348bc7d..e9ee8bc12fa 100644
--- a/docs/asciidoc/static/managing-multiline-events.asciidoc
+++ b/docs/static/managing-multiline-events.asciidoc
@@ -1,43 +1,32 @@
 [[multiline]]
 === Managing Multiline Events
 
-Several use cases generate events that span multiple lines of text. In order to correctly handle these multline events, 
+Several use cases generate events that span multiple lines of text. In order to correctly handle these multiline events,
 Logstash needs to know how to tell which lines are part of a single event.
 
-Multiline event processing is complex and relies on proper event ordering. The best way to guarantee ordered log 
-processing is to implement the processing as early in the pipeline as possible. The preferred tool in the Logstash 
-pipeline is the {logstash}plugins-codecs-multiline.html[multiline codec], which merges lines from a single input using 
+Multiline event processing is complex and relies on proper event ordering. The best way to guarantee ordered log
+processing is to implement the processing as early in the pipeline as possible. The preferred tool in the Logstash
+pipeline is the {logstash}plugins-codecs-multiline.html[multiline codec], which merges lines from a single input using
 a simple set of rules.
 
-For more complex needs, the {logstash}plugins-filters-multiline.html[multiline filter] performs a similar task at the 
-filter stage of processing, where the Logstash instance aggregates multiple inputs.
 
 The most important aspects of configuring either multiline plugin are the following:
 
-* The `pattern` option specifies a regular expression. Lines that match the specified regular expression are considered 
-either continuations of a previous line or the start of a new multiline event. You can use 
+* The `pattern` option specifies a regular expression. Lines that match the specified regular expression are considered
+either continuations of a previous line or the start of a new multiline event. You can use
 {logstash}plugins-filters-grok.html[grok] regular expression templates with this configuration option.
-* The `what` option takes two values: `previous` or `next`. The `previous` value specifies that lines that match the 
-value in the `pattern` option are part of the previous line. The `next` value specifies that lines that match the value 
-in the `pattern` option are part of the following line.* The `negate` option applies the multiline codec to lines that 
+* The `what` option takes two values: `previous` or `next`. The `previous` value specifies that lines that match the
+value in the `pattern` option are part of the previous line. The `next` value specifies that lines that match the value
+in the `pattern` option are part of the following line.* The `negate` option applies the multiline codec to lines that
 _do not_ match the regular expression specified in the `pattern` option.
 
-See the full documentation for the {logstash}plugins-codecs-multiline.html[multiline codec] or the 
+See the full documentation for the {logstash}plugins-codecs-multiline.html[multiline codec] or the
 {logstash}plugins-filters-multiline.html[multiline filter] plugin for more information on configuration options.
 
-==== Multiline Special Cases
-
-* The current release of the multiline codec plugin treats all input from the 
-{logstash}plugins-inputs-lumberjack[lumberjack] input plugin as a single stream. When your use case involves the 
-Logstash Forwarder processing multiple files concurrently, proper event ordering can be challenging to maintain, and 
-any resulting errors can be difficult to diagnose. Carefully monitor the output of Logstash configurations that involve 
-multiline processing of multiple files handled by the Logstash Forwarder.
-
-* The multiline codec plugin does not support file input from files that contain events from multiple sources.
-
-* The multiline filter plugin is not thread-safe. Avoid using multiple filter workers with the multiline filter.
-
-NOTE: You can track the progress of upgrades to the functionality of the multiline codec at 
+NOTE: For more complex needs, the {logstash}plugins-filters-multiline.html[multiline filter] performs a similar task at
+the filter stage of processing, where the Logstash instance aggregates multiple inputs.
+The multiline filter plugin is not thread-safe. Avoid using multiple filter workers with the multiline filter. You can
+track the progress of upgrades to the functionality of the multiline codec at
 https://github.com/logstash-plugins/logstash-codec-multiline/issues/10[this Github issue].
 
 ==== Examples of Multiline Plugin Configuration
@@ -50,7 +39,7 @@ The examples in this section cover the following use cases:
 
 ===== Java Stack Traces
 
-Java stack traces consist of multiple lines, with each line after the initial line beginning with whitespace, as in 
+Java stack traces consist of multiple lines, with each line after the initial line beginning with whitespace, as in
 this example:
 
 [source,java]
@@ -75,7 +64,7 @@ This configuration merges any line that begins with whitespace up to the previou
 
 ===== Line Continuations
 
-Several programming languages use the `\` character at the end of a line to denote that the line continues, as in this 
+Several programming languages use the `\` character at the end of a line to denote that the line continues, as in this
 example:
 
 [source,c]
@@ -98,11 +87,11 @@ This configuration merges any line that ends with the `\` character with the fol
 
 ===== Timestamps
 
-Activity logs from services such as Elasticsearch typically begin with a timestamp, followed by information on the 
+Activity logs from services such as Elasticsearch typically begin with a timestamp, followed by information on the
 specific activity, as in this example:
 
 [source,shell]
-[2015-08-24 11:49:14,389][INFO ][env                      ] [Letha] using [1] data paths, mounts [[/ 
+[2015-08-24 11:49:14,389][INFO ][env                      ] [Letha] using [1] data paths, mounts [[/
 (/dev/disk1)]], net usable_space [34.5gb], net total_space [118.9gb], types [hfs]
 
 To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:
@@ -119,5 +108,5 @@ input {
   }
 }
 
-This configuration uses the `negate` option to specify that any line that does not begin with a timestamp belongs to 
+This configuration uses the `negate` option to specify that any line that does not begin with a timestamp belongs to
 the previous line.
diff --git a/docs/static/monitoring-apis.asciidoc b/docs/static/monitoring-apis.asciidoc
new file mode 100644
index 00000000000..7e5717e04ac
--- /dev/null
+++ b/docs/static/monitoring-apis.asciidoc
@@ -0,0 +1,289 @@
+[[monitoring]]
+== Monitoring APIs
+
+experimental[]
+
+Logstash provides the following monitoring APIs to retrieve runtime metrics
+about Logstash:
+
+* <<plugins-api>>
+* <<stats-info-api>>
+* <<hot-threads-api>>
+
+
+You can use the root resource to retrieve general information about the Logstash instance, including
+the host name and version information.
+
+[source,js]
+--------------------------------------------------
+GET /
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+   "hostname": "skywalker",
+    "version" : {
+        "number" : "2.1.0",       
+    }
+  }
+--------------------------------------------------
+
+NOTE: By default, the monitoring API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
+instance, you need to launch Logstash with the `--http.port` flag specified to bind to a different port. See 
+<<command-line-flags>> for more information.
+
+[float]
+[[monitoring-common-options]]
+=== Common Options
+
+The following options can be applied to all of the Logstash monitoring APIs.
+
+[float]
+==== Pretty Results
+
+When appending `?pretty=true` to any request made, the JSON returned
+will be pretty formatted (use it for debugging only!). Another option is
+to set `?format=yaml` which will cause the result to be returned in the
+(sometimes) more readable yaml format.
+
+[float]
+==== Human-Readable Output
+
+NOTE: For Logstash {logstash_version}, the `human` option is supported for the <<hot-threads-api>>
+only. When you specify `human=true`, the results are returned in plain text instead of
+JSON format. The default is false.
+
+Statistics are returned in a format suitable for humans
+(eg `"exists_time": "1h"` or `"size": "1kb"`) and for computers
+(eg `"exists_time_in_millis": 3600000` or `"size_in_bytes": 1024`).
+The human-readable values can be turned off by adding `?human=false`
+to the query string. This makes sense when the stats results are
+being consumed by a monitoring tool, rather than intended for human
+consumption.  The default for the `human` flag is
+`false`.
+
+[[plugins-api]]
+=== Plugins API
+
+experimental[]
+
+The plugins API gets information about all Logstash plugins that are currently installed.
+This API basically returns the output of running the `bin/logstash-plugin list --verbose` command.
+
+[source,js]
+--------------------------------------------------
+GET /_plugins
+--------------------------------------------------
+
+The output is a JSON document.
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+"total": 102
+"plugins" : [
+  {
+      "name": "logstash-output-pagerduty"
+      "version": "2.0.2"
+  },
+  {
+      "name": "logstash-output-elasticsearch"
+      "version": "2.1.2"
+  }
+....
+] 
+--------------------------------------------------
+
+[[stats-info-api]]
+=== Node Stats API
+
+coming[5.0.0-beta3,Replaces the Stats Info API]
+
+experimental[]
+
+The node stats API retrieves runtime stats about Logstash. 
+
+// COMMENTED OUT until Logstash supports multiple pipelines: To retrieve all stats for the Logstash instance, use the `_node/stats` endpoint:
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/<types>
+--------------------------------------------------
+
+////
+COMMENTED OUT until Logstash supports multiple pipelines: To retrieve all stats per pipeline, use the `_pipelines/stats` endpoint:
+
+[source,js]
+--------------------------------------------------
+GET /_pipelines/stats/<types>
+--------------------------------------------------
+////
+
+Where `<types>` is optional and specifies the types of stats you want to return.
+
+By default, all stats are returned. You can limit this by combining any of the following types: 
+
+[horizontal]
+`events`::
+Gets event information since startup. 
+`jvm`::
+Gets JVM stats, including stats about threads. coming[5.0.0-alpha3,Adds thread count]
+`process`::
+Gets process stats, including stats about file descriptors, memory consumption, and CPU usage. coming[5.0.0-alpha3]   
+
+For example, the following request returns a JSON document that shows the number of events
+that were input, filtered, and output by Logstash since startup:
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/events
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "events" : {
+    "in" : 59685,
+    "filtered" : 59685,
+    "out" : 59685
+  }
+--------------------------------------------------
+
+The following request returns a JSON document containing JVM stats:
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/jvm
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "jvm" : {
+    "threads" : {
+      "count" : 32,
+      "peak_count" : 34
+    }
+  }
+--------------------------------------------------
+
+The following request returns a JSON document containing process stats: 
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/process
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "process" : {
+    "peak_open_file_descriptors" : 64,
+    "max_file_descriptors" : 10240,
+    "open_file_descriptors" : 64,
+    "mem" : {
+      "total_virtual_in_bytes" : 5278068736
+    },
+    "cpu" : {
+      "total_in_millis" : 103290097000,
+      "percent" : 0
+    }
+  }
+--------------------------------------------------
+
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
+
+[[hot-threads-api]]
+=== Hot Threads API
+
+experimental[]
+
+The hot threads API gets the current hot threads for Logstash. A hot thread is a
+Java thread that has high CPU usage and executes for a longer than normal period
+of time.
+
+[source,js]
+--------------------------------------------------
+GET /_node/hot_threads
+--------------------------------------------------
+
+The output is a JSON document that contains a breakdown of the top hot threads for
+Logstash. The parameters allowed are:
+
+[horizontal]
+`threads`:: 	        The number of hot threads to return. The default is 3. 
+`human`:: 	            If true, returns plain text instead of JSON format. The default is false. 
+`ignore_idle_threads`:: If true, does not return idle threads. The default is true.
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "hostname" : "Example-MBP-2",
+  "time" : "2016-03-08T17:58:18-08:00",
+  "busiest_threads" : 3,
+  "threads" : [ {
+    "name" : "LogStash::Runner",
+    "percent_of_cpu_time" : 16.93,
+    "state" : "timed_waiting",
+    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\tjava.lang.Thread.join(Thread.java:1253)\n\t\torg.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)\n\t\torg.jruby.RubyThread.join(RubyThread.java:697)\n\t\torg.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)\n\t\torg.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)\n\t\torg.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)\n"
+  }, {
+    "name" : "Api Webserver",
+    "percent_of_cpu_time" : 0.39,
+    "state" : "timed_waiting",
+    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\tjava.lang.Thread.join(Thread.java:1253)\n\t\torg.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)\n\t\torg.jruby.RubyThread.join(RubyThread.java:697)\n\t\torg.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)\n\t\torg.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)\n\t\torg.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)\n"
+  }, {
+    "name" : "Ruby-0-Thread-13",
+    "percent_of_cpu_time" : 0.15,
+    "state" : "timed_waiting",
+    "path" : "/Users/suyog/ws/elastic/logstash/build/logstash-3.0.0.dev/vendor/local_gems/f5685da5/logstash-core-3.0.0.dev-java/lib/logstash/pipeline.rb:496",
+    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\torg.jruby.RubyThread.sleep(RubyThread.java:1002)\n\t\torg.jruby.RubyKernel.sleep(RubyKernel.java:803)\n\t\torg.jruby.RubyKernel$INVOKER$s$0$1$sleep.call(RubyKernel$INVOKER$s$0$1$sleep.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:667)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:206)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:168)\n\t\torg.jruby.ast.FCallOneArgNode.interpret(FCallOneArgNode.java:36)\n\t\torg.jruby.ast.NewlineNode.interpret(NewlineNode.java:105)\n\t\torg.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n"
+  } ]
+--------------------------------------------------
+
+You can use the `?human` parameter to return the document in a human-readable format.
+
+[source,js]
+--------------------------------------------------
+GET /_node/hot_threads?human=true
+--------------------------------------------------
+
+Example of a human-readable response: 
+
+[source,js]
+--------------------------------------------------
+::: {Ringo Kid}{Gv3UrzR3SqmPQIgfG4qJMA}{127.0.0.1}{127.0.0.1:9300}
+   Hot threads at 2016-01-13T16:55:49.988Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:
+
+    0.0% (216micros out of 500ms) cpu usage by thread 'elasticsearch[Ringo Kid][transport_client_timer][T#1]{Hashed wheel timer #1}'
+     10/10 snapshots sharing following 5 elements
+       java.lang.Thread.sleep(Native Method)
+       org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
+       org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
+       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
+       java.lang.Thread.run(Thread.java:745)
+
+    0.0% (216micros out of 500ms) cpu usage by thread 'elasticsearch[Ringo Kid][transport_client_timer][T#1]{Hashed wheel timer #1}'
+     10/10 snapshots sharing following 5 elements
+       java.lang.Thread.sleep(Native Method)
+       org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
+       org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
+       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
+       java.lang.Thread.run(Thread.java:745)
+--------------------------------------------------
+
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
diff --git a/docs/static/offline-plugins.asciidoc b/docs/static/offline-plugins.asciidoc
new file mode 100644
index 00000000000..e099faec8a8
--- /dev/null
+++ b/docs/static/offline-plugins.asciidoc
@@ -0,0 +1,67 @@
+[[offline-plugins]]
+=== Offline Plugin Management
+
+The Logstash <<working-with-plugins,plugin manager>> was introduced in the 1.5 release. This section discusses setting up
+local repositories of plugins for use on systems without access to the Internet.
+
+The procedures in this section require a staging machine running Logstash that has access to a public or private Rubygems
+server. This staging machine downloads and packages the files used for offline installation.
+
+See the <<private-rubygem,Private Gem Repositories>> section for information on setting up your own private
+Rubygems server.
+
+Users who can work with a larger Logstash artifact size can use the *Logstash (All Plugins)* download link from the
+https://www.elastic.co/downloads/logstash[Logstash product page] to download Logstash bundled with the latest version of
+all available plugins. You can distribute this bundle to all nodes without further plugin staging.
+
+[float]
+=== Building the Offline Package
+
+Working with offline plugins requires you to create an _offline package_, which is a compressed file that contains all of
+the plugins your offline Logstash installation requires, along with the dependencies for those plugins.
+
+. Create the offline package with the `bin/logstash-plugin pack` subcommand.
++
+When you run the `bin/logstash-plugin pack` subcommand, Logstash creates a compressed bundle that contains all of the currently
+installed plugins and the dependencies for those plugins. By default, the compressed bundle is a GZipped TAR file when you
+run the `bin/logstash-plugin pack` subcommand on a UNIX machine. By default, when you run the `bin/logstash-plugin pack` subcommand on a
+Windows machine, the compressed bundle is a ZIP file. See <<managing-packs,Managing Plugin Packs>> for details on changing
+these default behaviors.
++
+NOTE: Downloading all dependencies for the specified plugins may take some time, depending on the plugins listed.
+
+. Move the compressed bundle to the offline machines that are the source for offline plugin installation, then use the
+`bin/logstash-plugin unpack` subcommand to make the packaged plugins available.
+
+[float]
+=== Install or Update a local plugin
+
+To install or update a local plugin, use the `--local` option with the install and update commands, as in the following
+examples:
+
+.Installing a local plugin
+============
+`bin/logstash-plugin install --local logstash-input-jdbc`
+============
+
+.Updating a local plugin
+============
+`bin/logstash-plugin update --local logstash-input-jdbc`
+============
+
+.Updating all local plugins in one command
+============
+`bin/logstash-plugin update --local`
+============
+
+[float]
+[[managing-packs]]
+=== Managing Plugin Packs
+
+The `pack` and `unpack` subcommands for `bin/logstash-plugin` take the following options:
+
+[horizontal]
+`--tgz`:: Generate the offline package as a GZipped TAR file. The default behavior on UNIX systems.
+`--zip`:: Generate the offline package as a ZIP file. The default behavior on Windows systems.
+`[packname] --override`:: Generates a new offline package that overwrites an existing offline with the specified name.
+`[packname] --[no-]clean`: Deletes offline packages matching the specified name.
diff --git a/docs/asciidoc/static/output.asciidoc b/docs/static/output.asciidoc
similarity index 88%
rename from docs/asciidoc/static/output.asciidoc
rename to docs/static/output.asciidoc
index ac6ccdee269..0f9b1b1c0a1 100644
--- a/docs/asciidoc/static/output.asciidoc
+++ b/docs/static/output.asciidoc
@@ -1,5 +1,5 @@
 :register_method:	true
-:receive_method:	true
+:multi_receive_method:	true
 :plugintype:    	output
 :pluginclass:   	Outputs
 :pluginname:    	example
@@ -9,6 +9,6 @@
 
 :getstarted: Let's step through creating an {plugintype} plugin using the https://github.com/logstash-plugins/logstash-output-example/[example {plugintype} plugin].
 
-:methodheader: Logstash outputs must implement the `register` and `receive` methods.
+:methodheader: Logstash outputs must implement the `register` and `multi_receive` methods.
 
 include::include/pluginbody.asciidoc[]
diff --git a/docs/static/plugin-generator.asciidoc b/docs/static/plugin-generator.asciidoc
new file mode 100644
index 00000000000..0563b466b88
--- /dev/null
+++ b/docs/static/plugin-generator.asciidoc
@@ -0,0 +1,19 @@
+[[plugin-generator]]
+== Generating Plugins
+
+You can now create your own Logstash plugin in seconds! The generate subcommand of `bin/logstash-plugin` creates the foundation 
+for a new Logstash plugin with templatized files. It creates the right directory structure, gemspec files and dependencies so you 
+can start adding custom code process data with Logstash.
+
+**Example Usage**
+
+[source,sh]
+--------------------------------------------
+bin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins
+-------------------------------------------
+
+* `--type`: Type of plugin - input, filter, output and codec
+* `--name`: Name for the new plugin
+* `--path`: Directory path where the new plugin structure will be created. If not specified, it will be '
+created in the current directory.
+
diff --git a/docs/static/plugin-manager.asciidoc b/docs/static/plugin-manager.asciidoc
new file mode 100644
index 00000000000..294aedcb334
--- /dev/null
+++ b/docs/static/plugin-manager.asciidoc
@@ -0,0 +1,118 @@
+[[working-with-plugins]]
+== Working with plugins
+
+Logstash has a rich collection of input, filter, codec and output plugins. Plugins are available as self-contained
+packages called gems and hosted on RubyGems.org. The plugin manager accesed via `bin/logstash-plugin` script is used to manage the
+lifecycle of plugins in your Logstash deployment. You can install, uninstall and upgrade plugins using these Command Line
+Interface (CLI) described below.
+
+[float]
+[[listing-plugins]]
+=== Listing plugins
+
+Logstash release packages bundle common plugins so you can use them out of the box. To list the plugins currently
+available in your deployment:
+
+[source,shell]
+----------------------------------
+bin/logstash-plugin list <1>
+bin/logstash-plugin list --verbose <2>
+bin/logstash-plugin list *namefragment* <3>
+bin/logstash-plugin list --group output <4>
+----------------------------------
+<1> Will list all installed plugins
+
+<2> Will list installed plugins with version information
+
+<3> Will list all installed plugins containing a namefragment
+
+<4> Will list all installed plugins for a particular group (input, filter, codec, output)
+
+[float]
+[[installing-plugins]]
+=== Adding plugins to your deployment
+
+The most common situation when dealing with plugin installation is when you have access to internet. Using this method,
+you will be able to retrieve plugins hosted on the public repository (RubyGems.org) and install on top of your Logstash
+installation.
+
+[source,shell]
+----------------------------------
+bin/logstash-plugin install logstash-output-kafka
+----------------------------------
+
+Once the plugin is successfully installed, you can start using it in your configuration file.
+
+[[installing-local-plugins]]
+[float]
+==== Advanced: Adding a locally built plugin
+
+In some cases, you want to install plugins which have not yet been released and not hosted on RubyGems.org. Logstash
+provides you the option to install a locally built plugin which is packaged as a ruby gem. Using a file location:
+
+[source,shell]
+----------------------------------
+bin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem
+----------------------------------
+
+[[installing-local-plugins-path]]
+[float]
+==== Advanced: Using `--path.plugins`
+
+Using the Logstash `--path.plugins` flag, you can load a plugin source code located on your file system. Typically this is used by
+developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
+
+[source,shell]
+----------------------------------
+bin/logstash --path.plugins /opt/shared/lib/logstash/input/my-custom-plugin-code.rb
+----------------------------------
+
+[[updating-plugins]]
+[float]
+=== Updating plugins
+
+Plugins have their own release cycle and are often released independent of Logstash’s core release cycle. Using the update
+subcommand you can get the latest or update to a particular version of the plugin.
+
+[source,shell]
+----------------------------------
+bin/logstash-plugin update <1>
+bin/logstash-plugin update logstash-output-kafka <2>
+----------------------------------
+<1> will update all installed plugins
+
+<2> will update only this plugin
+
+[[removing-plugins]]
+[float]
+=== Removing plugins
+
+If you need to remove plugins from your Logstash installation:
+
+[source,shell]
+----------------------------------
+bin/logstash-plugin uninstall logstash-output-kafka
+----------------------------------
+
+[[proxy-plugins]]
+[float]
+=== Proxy Support
+
+The previous sections relied on Logstash being able to communicate with RubyGems.org. In certain environments, Forwarding
+Proxy is used to handle HTTP requests. Logstash Plugins can be installed and updated through a Proxy by setting the
+`HTTP_PROXY` environment variable:
+
+[source,shell]
+----------------------------------
+export HTTP_PROXY=http://127.0.0.1:3128
+
+bin/logstash-plugin install logstash-output-kafka
+----------------------------------
+
+Once set, plugin commands install, update can be used through this proxy.
+
+include::plugin-generator.asciidoc[]
+
+include::offline-plugins.asciidoc[]
+
+include::private-gem-repo.asciidoc[]
diff --git a/docs/static/private-gem-repo.asciidoc b/docs/static/private-gem-repo.asciidoc
new file mode 100644
index 00000000000..dd96f63a60d
--- /dev/null
+++ b/docs/static/private-gem-repo.asciidoc
@@ -0,0 +1,53 @@
+[[private-rubygem]]
+=== Private Gem Repositories
+
+The Logstash plugin manager connects to a Ruby gems repository to install and update Logstash plugins. By default, this
+repository is http://rubygems.org.
+
+Some use cases are unable to use the default repository, as in the following examples:
+
+* A firewall blocks access to the default repository.
+* You are developing your own plugins locally.
+* Airgap requirements on the local system.
+
+When you use a custom gem repository, be sure to make plugin dependencies available.
+
+Several open source projects enable you to run your own plugin server, among them:
+
+* https://github.com/geminabox/geminabox[Geminabox]
+* https://github.com/PierreRambaud/gemirro[Gemirro]
+* https://gemfury.com/[Gemfury]
+* http://www.jfrog.com/open-source/[Artifactory]
+
+==== Editing the Gemfile
+
+The gemfile is a configuration file that specifies information required for plugin management. Each gem file has a
+`source` line that specifies a location for plugin content.
+
+By default, the gemfile's `source` line reads:
+
+[source,shell]
+----------
+# This is a Logstash generated Gemfile.
+# If you modify this file manually all comments and formatting will be lost.
+
+source "https://rubygems.org"
+----------
+
+To change the source, edit the `source` line to contain your preferred source, as in the following example:
+
+[source,shell]
+----------
+# This is a Logstash generated Gemfile.
+# If you modify this file manually all comments and formatting will be lost.
+
+source "https://my.private.repository"
+----------
+
+After saving the new version of the gemfile, use <<working-with-plugins,plugin management commands>> normally.
+
+The following links contain further material on setting up some commonly used repositories:
+
+* https://github.com/geminabox/geminabox/blob/master/README.markdown[Geminabox]
+* https://www.jfrog.com/confluence/display/RTF/RubyGems+Repositories[Artifactory]
+* Running a http://guides.rubygems.org/run-your-own-gem-server/[rubygems mirror]
diff --git a/docs/static/releasenotes.asciidoc b/docs/static/releasenotes.asciidoc
new file mode 100644
index 00000000000..899b9cc9a75
--- /dev/null
+++ b/docs/static/releasenotes.asciidoc
@@ -0,0 +1,61 @@
+[[releasenotes]]
+== Logstash 2.1 Release Notes
+
+[float]
+== General
+
+* {lsissue}2376[Issue 2376]: Added ability to install and upgrade Logstash plugins without requiring internet
+connectivity.
+* {lsissue}3576[Issue 3576]: Support alternate or private Ruby gems server to install and update plugins.
+* {lsissue}3451[Issue 3451]: Added ability to reliably shutdown Logstash when there is a stall in event processing. This
+option can be enabled by passing `--allow-unsafe-shutdown` flag while starting Logstash. Please be aware that any in-
+flight events will be lost when shutdown happens.
+* {lsissue}4222[Issue 4222]: Fixed a memory leak which could be triggered when events having a date were serialized to
+string.
+* Added JDBC input to default package.
+* {lsissue}3243[Issue 3243]: Adding `--debug` to `--configtest` now shows the configuration in blocks annotated by source
+config file. Very useful when using multiple config files in a directory.
+* {lsissue}4130[Issue 4130]: Reset default worker threads to 1 when using non thread-safe filters like multiline.
+* Fixed file permissions for the `logrotate` configuration file.
+* {lsissue}3861[Issue 3861]: Changed the default heap size from 500MB to 1GB.
+* {lsissue}3645[Issue 3645]: Fixed config check option when starting Logstash through init scripts.
+
+[float]
+== Input Plugins
+
+[float]
+=== Twitter
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/21[Issue 21]: Added an option to fetch data from the
+sample Twitter streaming endpoint.
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/22[Issue 22]: Added hashtags, symbols and
+user_mentions as data for the non extended tweet event.
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/20[Issue 20]: Added an option to filter per location
+and language.
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/11[Issue 11]: Added an option to stream data from a
+list of users.
+
+[float]
+=== Beats
+* https://github.com/logstash-plugins/logstash-input-beats/issues/10[Issue 10]: Properly handle multiline events from
+multiple sources, originating from Filebeat.
+
+[float]
+=== File
+* https://github.com/logstash-plugins/logstash-input-file/issues/44[Issue 44]: Properly handle multiline events from
+multiple sources.
+
+[float]
+=== Eventlog
+* https://github.com/logstash-plugins/logstash-input-eventlog/issues/11[Issue 11]: Change the underlying library to
+capture Event Logs from Windows more reliably.
+
+[float]
+== Output
+
+[float]
+=== Elasticsearch
+* Improved the default template to use doc_values wherever possible.
+* Improved the default template to disable fielddata on analyzed string fields.
+* https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/260[Issue 260]: Added New setting: timeout.
+This lets you control the behavior of a slow/stuck request to Elasticsearch that could be, for example, caused by network,
+firewall, or load balancer issues.
diff --git a/docs/static/reloading-config.asciidoc b/docs/static/reloading-config.asciidoc
new file mode 100644
index 00000000000..22cc383d7dd
--- /dev/null
+++ b/docs/static/reloading-config.asciidoc
@@ -0,0 +1,43 @@
+[[reloading-config]]
+=== Reloading the Config File
+
+Starting with Logstash 2.3, you can set Logstash to detect and reload configuration
+changes automatically.
+
+To enable automatic config reloading, start Logstash with the `--config.reload.automatic` (or `-r`)
+command-line option specified. For example:
+
+[source,shell]
+----------------------------------
+bin/logstash –f apache.config --config.reload.automatic
+----------------------------------
+
+NOTE: The `--config.reload.automatic` option is not available when you specify the `-e` flag to pass
+in  configuration settings from the command-line.
+
+By default, Logstash checks for configuration changes every 3 seconds. To change this interval,
+use the `----config.reload.interval <seconds>` option,  where `seconds` specifies how often Logstash
+checks the config files for changes. 
+
+If Logstash is already running without auto-reload enabled, you can force Logstash to
+reload the config file and restart the pipeline by sending a SIGHUP (signal hangup) to the
+process running Logstash. For example:
+
+[source,shell]
+----------------------------------
+kill -1 14175
+----------------------------------
+
+Where 14175 is the ID of the process running Logstash.
+
+==== How Automatic Config Reloading Works
+
+When Logstash detects a change in a config file, it stops the current pipeline by stopping
+all inputs, and it attempts to create a new pipeline that uses the updated configuration.
+After validating the syntax of the new configuration, Logstash verifies that all inputs
+and outputs can be initialized (for example, that all required ports are open). If the checks
+are successful, Logstash swaps the existing pipeline with the new pipeline. If the checks
+fail, the old pipeline continues to function, and the errors are propagated to the console.
+
+During automatic config reloading, the JVM is not restarted. The creating and swapping of
+pipelines all happens within the same process. 
diff --git a/docs/asciidoc/static/roadmap/index.asciidoc b/docs/static/roadmap/index.asciidoc
similarity index 89%
rename from docs/asciidoc/static/roadmap/index.asciidoc
rename to docs/static/roadmap/index.asciidoc
index b6c33abcc92..b52e271675a 100644
--- a/docs/asciidoc/static/roadmap/index.asciidoc
+++ b/docs/static/roadmap/index.asciidoc
@@ -72,8 +72,7 @@ https://github.com/elastic/logstash/labels/resiliency[resiliency] tag.
 
 *Known unknowns.* If we don’t know it’s happening, it’s hard for us to fix it!
 Please report your issues in GitHub, under the
-https://github.com/elastic/logstash/issues[Logstash],
-https://github.com/elastic/logstash-forwarder/issues[Logstash Forwarder], or
+https://github.com/elastic/logstash/issues[Logstash] or
 individual https://github.com/logstash-plugins/[Logstash plugin] repositories.
 
 == Manageability
@@ -125,12 +124,6 @@ distributing the load between instances based on the latest cluster state. This
 is a complex use case that will require input from the community on current
 approaches to implementing HA and load balancing of Logstash instances.
 
-== Logstash Forwarder
-[float]
-=== status: ongoing
-
-https://github.com/elastic/logstash-forwarder/[Logstash Forwarder] is a lightweight shipper for tailing files and forwarding this data to Logstash for further processing. It is often used in lieu of running Logstash on the servers, because it is lightweight, written in Go, and consumes less resources. It was created before we had the Beats framework for shipping data from servers and is currently maintained separately. We plan to move the Logstash Forwarder functionality to https://github.com/elastic/filebeat/[Filebeat] in the Beats framework, which is also written in Go. The first version of Filebeat will leverage the libbeat infrastructure and preserve existing features. Over time, we plan to enhance Filebeat with capabilities such as multiline and filtering support. Since Filebeat will serve as the direct replacement for Logstash Forwarder, we are not planning additional releases beyond http://www.elasticsearch.org/blog/logstash-forwarder-0-4-0-released/[Logstash Forwarder 0.4.0].
-
 == Performance
 [float]
 === status: ongoing; v1.5, v2.x
@@ -187,4 +180,4 @@ In Logstash 1.5, we made it easier than ever to add and maintain plugins by
 putting each plugin into its own repository (see "Plugin Framework" section).
 We also greatly improved the S3, Twitter, RabbitMQ plugins. To follow requests
 for new Logstash plugins or contribute to the discussion, look for issues that
-have the {LABELS}new-plugin[new-plugin] tag in Github.
\ No newline at end of file
+have the {LABELS}new-plugin[new-plugin] tag in Github.
diff --git a/docs/static/setting-up-logstash.asciidoc b/docs/static/setting-up-logstash.asciidoc
new file mode 100644
index 00000000000..9790f8674e0
--- /dev/null
+++ b/docs/static/setting-up-logstash.asciidoc
@@ -0,0 +1,186 @@
+[[setup-logstash]]
+== Setting Up and Running Logstash
+
+Before reading this section, see <<installing-logstash>> for basic installation instructions to get you started. 
+
+This section includes additional information on how to set up and run Logstash, including:
+
+* <<dir-layout>>
+* <<config-setting-files>>
+* <<running-logstash>>
+* <<command-line-flags>>
+* <<logstash-settings-file>>
+
+[[dir-layout]]
+=== Logstash Directory Layout
+
+This section describes the default directory structure that is created when you unpack the Logstash installation packages.
+
+coming[5.0.0-alpha3, Includes breaking changes to the Logstash directory structure]
+
+[[zip-targz-layout]]
+==== Directory Layout of `.zip` and `.tar.gz` Archives
+
+The `.zip` and `.tar.gz` packages are entirely self-contained. All files and
+directories are, by default, contained within the home directory -- the directory
+created when unpacking the archive.
+
+This is very convenient because you don't have to create any directories to start using Logstash, and uninstalling
+Lostash is as easy as removing the home directory.  However, it is advisable to change the default locations of the
+config and the logs directories so that you do not delete important data later on.
+
+[cols="<h,<,<m,<m",options="header",]
+|=======================================================================
+| Type | Description | Default Location | Setting
+| home
+  | Home directory of the Logstash installation.
+  | `{extract.path}`- Directory created by unpacking the archive
+ d| 
+
+| bin
+  | Binary scripts, including `logstash` to start Logstash
+    and `logstash-plugin` to install plugins
+  | `{extract.path}/bin`
+ d|
+
+| settings
+  | Configuration files, including `logstash.yml` and `jvm.options`
+  | `{extract.path}/config`
+  | `path.settings`
+
+| logs
+  | Log files
+  | `{extract.path}/logs`
+  | `path.logs`
+
+| plugins
+  | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.
+  | `{extract.path}/plugins`
+  | `path.plugins`
+
+|=======================================================================
+
+[[deb-layout]]
+==== Directory Layout of Debian and RPM Packages
+
+The Debian package and the RPM package each place config files, logs, and the settings files in the appropriate
+locations for the system:
+
+[cols="<h,<,<m,<m",options="header",]
+|=======================================================================
+| Type | Description | Default Location | Setting
+| home
+  | Home directory of the Logstash installation.
+  | `/usr/share/logstash`
+ d| 
+
+| bin
+  | Binary scripts including `logstash` to start Logstash
+    and `logstash-plugin` to install plugins
+  | `/usr/share/logstash/bin`
+ d|
+
+| settings
+  | Configuration files, including `logstash.yml`, `jvm.options`, and `startup.options`
+  | `/etc/logstash`
+  | `path.settings`
+
+| conf
+  | Logstash pipeline configuration files
+  | `/etc/logstash/conf.d`
+  | `path.config`
+
+| logs
+  | Log files
+  | `/var/log/logstash`
+  | `path.log`
+
+| plugins
+  | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.
+  | `/usr/share/logstash/plugins`
+  | `path.plugins`
+
+|=======================================================================
+
+[[config-setting-files]]
+=== Logstash Configuration Files
+
+Logstash has two types of configuration files: _pipeline configuration files_, which define the Logstash processing
+pipeline, and _settings files_, which specify options that control Logstash startup and execution. 
+
+==== Pipeline Configuration Files
+
+You create pipeline configuration files when you define the stages of your Logstash processing pipeline. On deb and
+rpm, you place the pipeline configuration files in the `/etc/logstash/conf.d` directory. Logstash tries to load all
+files in the `/etc/logstash/conf.d directory`, so don't store any non-config files or backup files in this directory. 
+
+See <<configuration>> for more info.
+
+==== Settings Files
+
+coming[5.0.0-alpha3]
+
+The settings files are already defined in the Logstash installation. Logstash includes the following settings files:
+
+*`logstash.yml`*:: 
+  Contains Logstash configuration flags. You can set flags in this file instead of passing the flags at the command
+  line. Any flags that you set at the command line override the corresponding settings in the `logstash.yml` file. See <<logstash-settings-file>> for more info.
+*`jvm.options`*:: 
+  Contains JVM configuration flags. Specify each flag on a separate line. You can also use this file to set the locale
+  for Logstash.
+*`startup.options` (Linux)*::
+  Contains options used by the `system-install` script in `/usr/share/logstash/bin` to build the appropriate startup
+  script for your system. When you install the Logstash package, the `system-install` script executes at the end of the
+  installation process and uses the settings specified in `startup.options` to set options such as the user, group,
+  service name, and service description. By default, Logstash services are installed under the user `logstash`. The `startup.options` file makes it easier for you to install multiple instances of the Logstash service. You can copy
+  the file and change the values for specific settings. Note that the `startup.options` file is not read at startup. If
+  you want to change the Logstash startup script (for example, to change the Logstash user or read from a different
+  configuration path), you must re-run the `system-install` script (as root) to pass in the new settings.
+
+[[running-logstash]]
+=== Running Logstash as a Service on Debian or RPM
+
+coming[5.0.0-alpha3]
+
+Logstash is not started automatically after installation. How to start and stop Logstash depends on whether your system
+uses systemd, upstart, or SysV. 
+
+[[running-logstash-systemd]]
+==== Running Logstash by Using Systemd
+
+Distributions like Debian Jessie, Ubuntu 15.10+, and many of the SUSE derivatives use systemd and the
+`systemctl` command to start and stop services. Logstash places the systemd unit files in `/etc/systemd/system` for both deb and rpm. After installing the package, you can start up Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo systemctl start logstash.service
+-------------------------------------------
+
+[[running-logstash-upstart]]
+==== Running Logstash by Using Upstart
+
+For systems that use upstart, you can start Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo initctl start logstash
+-------------------------------------------
+
+The auto-generated configuration file for upstart systems is `/etc/init/logstash.conf`.
+
+[[running-logstash-sysv]]
+==== Running Logstash by Using SysV
+
+For systems that use SysV, you can start Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo /etc/init.d/logstash start
+-------------------------------------------
+
+The auto-generated configuration file for SysV systems is `/etc/init.d/logstash`. 
+
+
+
+
+
diff --git a/docs/static/settings-file.asciidoc b/docs/static/settings-file.asciidoc
new file mode 100644
index 00000000000..748b0e79c4d
--- /dev/null
+++ b/docs/static/settings-file.asciidoc
@@ -0,0 +1,31 @@
+[[logstash-settings-file]]
+=== Settings File
+
+coming[5.0.0-alpha3]
+
+You can set options in the Logstash settings file, `logstash.yml`, to control Logstash execution. Each setting in the
+`logstash.yml` file corresponds to a <<command-line-flags,command-line flag>>. 
+
+Any flags that you set at the command line override the corresponding settings in the `logstash.yml` file. 
+
+The `logstash.yml` file, which is written in http://http://yaml.org/[YAML], is located in `LOGSTASH_HOME/config`. You can
+specify settings in hierarchical form or use flat keys. For example, to use hierarchical form to set the pipeline batch
+size and batch delay, you specify:
+
+[source,yaml]
+-------------------------------------------------------------------------------------
+pipeline:
+  batch:
+    size: 125
+    delay: 5
+-------------------------------------------------------------------------------------
+
+To express the same values as flat keys, you specify:
+
+[source,yaml]
+-------------------------------------------------------------------------------------
+pipeline.batch.size: 125
+pipeline.batch.delay: 5
+-------------------------------------------------------------------------------------
+
+See <<command-line-flags>> for a description of the available options.
diff --git a/docs/static/stalled-shutdown.asciidoc b/docs/static/stalled-shutdown.asciidoc
new file mode 100644
index 00000000000..14fde1ee3de
--- /dev/null
+++ b/docs/static/stalled-shutdown.asciidoc
@@ -0,0 +1,63 @@
+[[stalled-shutdown]]
+=== Stalled Shutdown Detection
+
+When you attempt to shut down a running Logstash instance, Logstash performs several steps before it can safely shut down. It must:
+
+* Stop all input, filter and output plugins
+* Process all in-flight events
+* Terminate the Logstash process
+
+The following conditions affect the shutdown process:
+
+* An input plugin receiving data at a slow pace.
+* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy
+query.
+* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+
+These situations make the duration and success of the shutdown process unpredictable.
+
+Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
+This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
+worker threads.
+
+To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--pipeline.unsafe_shutdown` flag when
+you start Logstash.
+
+[[shutdown-stall-example]]
+==== Stall Detection Example
+
+In this example, slow filter execution prevents the pipeline from clean shutdown. By starting Logstash with the
+`--pipeline.unsafe_shutdown` flag, quitting with *Ctrl+C* results in an eventual shutdown that loses 20 events.
+
+========
+[source,shell]
+% bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } \
+                     output { stdout { codec => dots } }' -w 1 --pipeline.unsafe_shutdown
+Default settings used: Filter workers: 1
+Logstash startup completed
+^CSIGINT received. Shutting down the pipeline. {:level=>:warn}
+Received shutdown signal, but pipeline is still waiting for in-flight events
+to be processed. Sending another ^C will force quit Logstash, but this may cause
+data loss. {:level=>:warn}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+The shutdown process appears to be stalled due to busy or blocked plugins. Check
+    the logs for more information.
+{:level=>:error}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+Forcefully quitting logstash.. {:level=>:fatal}
+========
+
+When `--pipeline.unsafe_shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
\ No newline at end of file
diff --git a/docs/static/submitting-a-plugin.asciidoc b/docs/static/submitting-a-plugin.asciidoc
new file mode 100644
index 00000000000..d85db91a8ff
--- /dev/null
+++ b/docs/static/submitting-a-plugin.asciidoc
@@ -0,0 +1,107 @@
+[[submitting-plugin]]
+=== Submitting your plugin to RubyGems.org and the logstash-plugins repository
+
+Logstash uses http://rubygems.org[RubyGems.org] as its repository for all plugin
+artifacts. Once you have developed your new plugin, you can make it available to
+Logstash users by simply publishing it to RubyGems.org.
+
+==== Licensing
+Logstash and all its plugins are licensed under
+https://github.com/elasticsearch/logstash/blob/master/LICENSE[Apache License, version 2 ("ALv2")].
+If you make your plugin publicly available via http://rubygems.org[RubyGems.org],
+please make sure to have this line in your gemspec:
+
+* `s.licenses = ['Apache License (2.0)']`
+
+==== Publishing to http://rubygems.org[RubyGems.org]
+
+To begin, you’ll need an account on RubyGems.org
+
+* https://rubygems.org/sign_up[Sign-up for a RubyGems account].
+
+After creating an account,
+http://guides.rubygems.org/rubygems-org-api/#api-authorization[obtain] an API
+key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials`
+to store your API key. These credentials will be used to publish the gem.
+Replace `username` and `password` with the credentials you created at
+RubyGems.org:
+
+[source,sh]
+----------------------------------
+curl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials
+chmod 0600 ~/.gem/credentials
+----------------------------------
+
+Before proceeding, make sure you have the right version in your gemspec file
+and commit your changes.
+
+* `s.version = '0.1.0'`
+
+To publish version 0.1.0 of your new logstash gem:
+
+[source,sh]
+----------------------------------
+bundle install
+bundle exec rake vendor
+bundle exec rspec
+bundle exec rake publish_gem
+----------------------------------
+
+[NOTE]
+========
+Executing `rake publish_gem`:
+
+. Reads the version from the gemspec file (`s.version = '0.1.0'`)
+. Checks in your local repository if a tag exists for that version. If the tag
+already exists, it aborts the process. Otherwise, it creates a new version tag
+in your local repository.
+. Builds the gem
+. Publishes the gem to RubyGems.org
+========
+
+That's it! Your plugin is published! Logstash users can now install your plugin
+by running:
+
+[source,sh]
+[subs="attributes"]
+----------------------------------
+bin/plugin install logstash-{plugintype}-mypluginname
+----------------------------------
+
+==== Contributing your source code to https://github.com/logstash-plugins[logstash-plugins]
+
+It is not required to contribute your source code to
+https://github.com/logstash-plugins[logstash-plugins] github organization, but
+we always welcome new plugins!
+
+==== Benefits
+
+Some of the many benefits of having your plugin in the logstash-plugins
+repository are:
+
+* **Discovery** Your plugin will appear in the http://www.elasticsearch.org/guide/en/logstash/current/index.html[Logstash Reference],
+where Logstash users look first for plugins and documentation.
+* **Documentation** Your plugin documentation will automatically be added to the
+ http://www.elasticsearch.org/guide/en/logstash/current/index.html[Logstash Reference].
+* **Testing** With our testing infrastructure, your plugin will be continuously
+tested against current and future releases of Logstash.  As a result, users will
+have the assurance that if incompatibilities arise, they will be quickly
+discovered and corrected.
+
+==== Acceptance Guidelines
+
+* **Code Review** Your plugin must be reviewed by members of the community for
+coherence, quality, readability, stability and security.
+* **Tests** Your plugin must contain tests to be accepted.  These tests are also
+subject to code review for scope and completeness.  It's ok if you don't know
+how to write tests -- we will guide you. We are working on publishing a guide to
+creating tests for Logstash which will make it easier.  In the meantime, you can
+refer to http://betterspecs.org/ for examples.
+
+To begin migrating your plugin to logstash-plugins, simply create a new
+https://github.com/elasticsearch/logstash/issues[issue] in
+the Logstash repository. When the acceptance guidelines are completed, we will
+facilitate the move to the logstash-plugins organization using the recommended
+https://help.github.com/articles/transferring-a-repository/#transferring-from-a-user-to-an-organization[github process].
+
+pass::[<?edit_url?>]
diff --git a/docs/asciidoc/static/upgrading.asciidoc b/docs/static/upgrading.asciidoc
similarity index 54%
rename from docs/asciidoc/static/upgrading.asciidoc
rename to docs/static/upgrading.asciidoc
index 3043c67d5b0..f466673f39b 100644
--- a/docs/asciidoc/static/upgrading.asciidoc
+++ b/docs/static/upgrading.asciidoc
@@ -16,8 +16,8 @@ This procedure uses <<package-repositories,package managers>> to upgrade Logstas
 1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
 2. Using the directions in the _Package Repositories_ section, update your repository links to point to the 2.0 repositories
 instead of the previous version.
-3. Run the `apt-get update logstash` or `yum update logstash` command as appropriate for your operating system.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
+3. Run the `apt-get upgrade logstash` or `yum update logstash` command as appropriate for your operating system.
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
 some Logstash plugins have changed in the 2.0 release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
@@ -28,7 +28,7 @@ This procedure downloads the relevant Logstash binaries directly from Elastic.
 1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
 2. Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
 3. Unpack the installation file into your Logstash directory.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
 some Logstash plugins have changed in the 2.0 release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
@@ -69,7 +69,7 @@ These plugin updates are available for Logstash 2.0. To upgrade to the latest ve
 plugins, the command is:
 
 [source,shell]
-bin/plugin update <plugin_name>
+bin/logstash-plugin update <plugin_name>
 
 **Multiline Filter:** If you are using the Multiline Filter in your configuration and upgrade to Logstash 2.0,
 you will get an error. Make sure to explicitly set the number of filter workers (`-w`) to `1`. You can set the number
@@ -78,3 +78,33 @@ of workers by passing a command line flag such as:
 [source,shell]
 bin/logstash `-w 1`
 
+[[upgrading-logstash-2.2]]
+=== Upgrading Logstash to 2.2
+
+Logstash 2.2 re-architected the pipeline stages to provide more performance and help future enhancements in resiliency.
+The new pipeline introduced micro-batches, processing groups of events at a time. The default batch size is
+125 per worker. Also, the filter and output stages are executed in the same thread, but still, as different stages.
+The CLI flag `--pipeline-workers` or `-w` control the number of execution threads, which is set by default to number of cores.
+
+**Considerations for Elasticsearch Output**
+The default batch size of the pipeline is 125 events per worker. This will by default also be the bulk size
+used for the elasticsearch output. The Elasticsearch output's `flush_size` now acts only as a maximum bulk
+size (still defaulting to 500). For example, if your pipeline batch size is 3000 events, Elasticsearch
+Output will send 500 events at a time, in 6 separate bulk requests. In other words, for Elasticsearch output,
+bulk request size is chunked based on `flush_size` and `--pipeline.batch.size`. If `flush_size` is set greater
+than `--pipeline.batch.size`, it is ignored and `--pipeline.batch.size` will be used.
+
+The default number of output workers in Logstash 2.2 is now equal to the number of pipeline workers (`-w`)
+unless overridden in the Logstash config file. This can be problematic for some users as the
+extra workers may consume extra resources like file handles, especially in the case of the Elasticsearch
+output. Users with more than one Elasticsearch host may want to override the `workers` setting
+for the Elasticsearch output in their Logstash config to constrain that number to a low value, between 1 to 4.
+
+**Performance Tuning in 2.2**
+Since both filters and output workers are on the same thread, this could lead to threads being idle in I/O wait state.
+Thus, in 2.2, you can safely set `-w` to a number which is a multiple of the number of cores on your machine.
+A common way to tune performance is keep increasing the `-w` beyond the # of cores until performance no longer
+improves. A note of caution - make sure you also keep heapsize in mind, because the number of in-flight events
+are `#workers * batch_size * average_event size`. More in-flight events could add to memory pressure, eventually
+leading to Out of Memory errors. You can change the heapsize in Logstash by setting `LS_HEAP_SIZE`
+
diff --git a/docs/tutorials/10-minute-walkthrough/apache-elasticsearch.conf b/docs/tutorials/10-minute-walkthrough/apache-elasticsearch.conf
deleted file mode 100644
index 854fb1d9d27..00000000000
--- a/docs/tutorials/10-minute-walkthrough/apache-elasticsearch.conf
+++ /dev/null
@@ -1,35 +0,0 @@
-input {
-  tcp { 
-    type => "apache"
-    port => 3333
-  } 
-}
-
-filter {
-  if [type] == "apache" {
-    grok {
-      # See the following URL for a complete list of named patterns
-      # logstash/grok ships with by default:
-      # https://github.com/logstash/logstash/tree/master/patterns
-      #
-      # The grok filter will use the below pattern and on successful match use
-      # any captured values as new fields in the event.
-      match => { "message" => "%{COMBINEDAPACHELOG}" }
-    }
-
-    date {
-      # Try to pull the timestamp from the 'timestamp' field (parsed above with
-      # grok). The apache time format looks like: "18/Aug/2011:05:44:34 -0700"
-      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
-    }
-  }
-}
-
-output {
-  elasticsearch {
-    # Setting 'embedded' will run  a real elasticsearch server inside logstash.
-    # This option below saves you from having to run a separate process just
-    # for ElasticSearch, so you can get started quicker!
-    embedded => true
-  }
-}
diff --git a/docs/tutorials/10-minute-walkthrough/apache-parse.conf b/docs/tutorials/10-minute-walkthrough/apache-parse.conf
deleted file mode 100644
index dc0653cfa76..00000000000
--- a/docs/tutorials/10-minute-walkthrough/apache-parse.conf
+++ /dev/null
@@ -1,33 +0,0 @@
-input {
-  tcp { 
-    type => "apache"
-    port => 3333
-  } 
-}
-
-filter {
-  if [type] == "apache" {
-    grok {
-      # See the following URL for a complete list of named patterns
-      # logstash/grok ships with by default:
-      # https://github.com/logstash/logstash/tree/master/patterns
-      #
-      # The grok filter will use the below pattern and on successful match use
-      # any captured values as new fields in the event.
-      match => { "message" => "%{COMBINEDAPACHELOG}" }
-    }
-
-    date {
-      # Try to pull the timestamp from the 'timestamp' field (parsed above with
-      # grok). The apache time format looks like: "18/Aug/2011:05:44:34 -0700"
-      match => [ "timestamp", "dd/MMM/yyyy:HH:mm:ss Z" ]
-    }
-  }
-}
-
-output {
-  # Use stdout in debug mode again to see what logstash makes of the event.
-  stdout {
-    codec => rubydebug
-  }
-}
diff --git a/docs/tutorials/10-minute-walkthrough/apache_log.1 b/docs/tutorials/10-minute-walkthrough/apache_log.1
deleted file mode 100644
index f7911a7eb0a..00000000000
--- a/docs/tutorials/10-minute-walkthrough/apache_log.1
+++ /dev/null
@@ -1 +0,0 @@
-129.92.249.70 - - [18/Aug/2011:06:00:14 -0700] "GET /style2.css HTTP/1.1" 200 1820 "http://www.semicomplete.com/blog/geekery/bypassing-captive-portals.html" "Mozilla/5.0 (iPad; U; CPU OS 4_3_5 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8L1 Safari/6533.18.5"
diff --git a/docs/tutorials/10-minute-walkthrough/apache_log.2.bz2 b/docs/tutorials/10-minute-walkthrough/apache_log.2.bz2
deleted file mode 100644
index 841e7b6b1f0..00000000000
Binary files a/docs/tutorials/10-minute-walkthrough/apache_log.2.bz2 and /dev/null differ
diff --git a/docs/tutorials/10-minute-walkthrough/hello-search.conf b/docs/tutorials/10-minute-walkthrough/hello-search.conf
deleted file mode 100644
index c99f014658a..00000000000
--- a/docs/tutorials/10-minute-walkthrough/hello-search.conf
+++ /dev/null
@@ -1,25 +0,0 @@
-input {
-  stdin { 
-    # A type is a label applied to an event. It is used later with filters
-    # to restrict what filters are run against each event.
-    type => "human"
-  } 
-}
-
-output {
-  # Print each event to stdout.
-  stdout {
-    # Enabling 'rubydebug' codec on the stdout output will make logstash
-    # pretty-print the entire event as something similar to a JSON representation.
-    codec => rubydebug
-  }
-  
-  # You can have multiple outputs. All events generally to all outputs.
-  # Output events to elasticsearch
-  elasticsearch {
-    # Setting 'embedded' will run  a real elasticsearch server inside logstash.
-    # This option below saves you from having to run a separate process just
-    # for ElasticSearch, so you can get started quicker!
-    embedded => true
-  }
-}
diff --git a/docs/tutorials/10-minute-walkthrough/hello.conf b/docs/tutorials/10-minute-walkthrough/hello.conf
deleted file mode 100644
index 3d80679931d..00000000000
--- a/docs/tutorials/10-minute-walkthrough/hello.conf
+++ /dev/null
@@ -1,16 +0,0 @@
-input {
-  stdin { 
-    # A type is a label applied to an event. It is used later with filters
-    # to restrict what filters are run against each event.
-    type => "human"
-  } 
-}
-
-output {
-  # Print each event to stdout.
-  stdout {
-    # Enabling 'rubydebug' codec on the stdout output will make logstash
-    # pretty-print the entire event as something similar to a JSON representation.
-    codec => rubydebug
-  }
-}
diff --git a/docs/tutorials/10-minute-walkthrough/index.md b/docs/tutorials/10-minute-walkthrough/index.md
deleted file mode 100644
index a8602e68d60..00000000000
--- a/docs/tutorials/10-minute-walkthrough/index.md
+++ /dev/null
@@ -1,101 +0,0 @@
----
-title: Logstash 10-Minute Tutorial
-layout: content_right
----
-# Logstash 10-minute Tutorial
-
-## Step 1 - Download
-
-### Download logstash:
-
-* [logstash-%VERSION%.tar.gz](https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz)
-
-    curl -O https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz
-
-### Unpack it
-
-    tar -xzf logstash-%VERSION%.tar.gz
-    cd logstash-%VERSION%
-
-### Requirements:
-
-* Java
-
-### The Secret:
-
-Logstash is written in JRuby, but I release standalone jar files for easy
-deployment, so you don't need to download JRuby or most any other dependencies.
-
-I bake as much as possible into the single release file.
-
-## Step 2 - A hello world.
-
-### Download this config file:
-
-* [hello.conf](hello.conf)
-
-### Run it:
-
-    bin/logstash agent -f hello.conf
-
-Type stuff on standard input. Press enter. Watch what event Logstash sees.
-Press ^C to kill it.
-
-## Step 3 - Add ElasticSearch
-
-### Download this config file:
-
-* [hello-search.conf](hello-search.conf)
-
-### Run it:
-
-    bin/logstash agent -f hello-search.conf
-
-Same config as step 2, but now we are also writing events to ElasticSearch. Do
-a search for `*` (all):
-
-    curl 'http://localhost:9200/_search?pretty=1&q=*'
-
-### Download
-
-* [apache-parse.conf](apache-parse.conf)
-* [apache_log.1](apache_log.1) (a single apache log line)
-
-### Run it
-
-    bin/logstash agent -f apache-parse.conf
-
-Logstash will now be listening on TCP port 3333. Send an Apache log message at it:
-
-    nc localhost 3333 < apache_log.1
-
-The expected output can be viewed here: [step-5-output.txt](step-5-output.txt)
-
-## Step 6 - real world example + search
-
-Same as the previous step, but we'll output to ElasticSearch now.
-
-### Download
-
-* [apache-elasticsearch.conf](apache-elasticsearch.conf)
-* [apache_log.2.bz2](apache_log.2.bz2) (2 days of apache logs)
-
-### Run it
-
-    bin/logstash agent -f apache-elasticsearch.conf
-
-Logstash should be all set for you now. Start feeding it logs:
-
-    bzip2 -d apache_log.2.bz2
-
-    nc localhost 3333 < apache_log.2
-
-## Want more?
-
-For further learning, try these:
-
-* [Watch a presentation on logstash](http://www.youtube.com/embed/RuUFnog29M4)
-* [Getting started 'standalone' guide](http://logstash.net/docs/%VERSION%/tutorials/getting-started-simple)
-* [Getting started 'centralized' guide](http://logstash.net/docs/%VERSION%/tutorials/getting-started-centralized) -
-  learn how to build out your logstash infrastructure and centralize your logs.
-* [Dive into the docs](http://logstash.net/docs/%VERSION%/)
diff --git a/docs/tutorials/10-minute-walkthrough/step-5-output.txt b/docs/tutorials/10-minute-walkthrough/step-5-output.txt
deleted file mode 100644
index ec87c0cebda..00000000000
--- a/docs/tutorials/10-minute-walkthrough/step-5-output.txt
+++ /dev/null
@@ -1,17 +0,0 @@
-{
-  "type"        => "apache",
-  "clientip"          => "129.92.249.70",
-  "ident"             => "-",
-  "auth"              => "-",
-  "timestamp"         => "18/Aug/2011:06:00:14 -0700",
-  "verb"              => "GET",
-  "request"           => "/style2.css",
-  "httpversion"       => "1.1",
-  "response"          => "200",
-  "bytes"             => "1820",
-  "referrer"          => "http://www.semicomplete.com/blog/geekery/bypassing-captive-portals.html",
-  "agent"             => "\"Mozilla/5.0 (iPad; U; CPU OS 4_3_5 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8L1 Safari/6533.18.5\"",
-  "@timestamp"   => "2011-08-18T13:00:14.000Z",
-  "host" => "127.0.0.1",
-  "message"     => "129.92.249.70 - - [18/Aug/2011:06:00:14 -0700] \"GET /style2.css HTTP/1.1\" 200 1820 \"http://www.semicomplete.com/blog/geekery/bypassing-captive-portals.html\" \"Mozilla/5.0 (iPad; U; CPU OS 4_3_5 like Mac OS X; en-us) AppleWebKit/533.17.9 (KHTML, like Gecko) Version/5.0.2 Mobile/8L1 Safari/6533.18.5\"\n"
-}
diff --git a/docs/tutorials/getting-started-with-logstash.asciidoc b/docs/tutorials/getting-started-with-logstash.asciidoc
deleted file mode 100644
index e38ebb62508..00000000000
--- a/docs/tutorials/getting-started-with-logstash.asciidoc
+++ /dev/null
@@ -1,436 +0,0 @@
-= Getting Started with Logstash
-
-== Introduction
-Logstash is a tool for receiving, processing and outputting logs. All kinds of logs. System logs, webserver logs, error logs, application logs and just about anything you can throw at it. Sounds great, eh?
-
-Using Elasticsearch as a backend datastore, and kibana as a frontend reporting tool, Logstash acts as the workhorse, creating a powerful pipeline for storing, querying and analyzing your logs. With an arsenal of built-in inputs, filters, codecs and outputs, you can harness some powerful functionality with a small amount of effort. So, let's get started!
-
-=== Prerequisite: Java
-The only prerequisite required by Logstash is a Java runtime. You can check that you have it installed by running the  command `java -version` in your shell. Here's something similar to what you might see:
-----
-> java -version
-java version "1.7.0_45"
-Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
-Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
-----
-It is recommended to run a recent version of Java in order to ensure the greatest success in running Logstash.
-
-It's fine to run an open-source version such as OpenJDK: +
-http://openjdk.java.net/
-
-Or you can use the official Oracle version: +
-http://www.oracle.com/technetwork/java/index.html
-
-Once you have verified the existence of Java on your system, we can move on!
-
-== Up and Running!
-
-=== Logstash in two commands
-First, we're going to download the 'logstash' binary and run it with a very simple configuration.
-----
-curl -O https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz
-----
-Now you should have the file named 'logstash-%VERSION%.tar.gz' on your local filesystem. Let's unpack it:
-----
-tar zxvf logstash-%VERSION%.tar.gz
-cd logstash-%VERSION%
-----
-Here, we are telling the *tar* command that we are sending it a gzipped file (*z* flag), that we would like to extract the file (*x* flag), that we would like to do so verbosely (*v* flag), and that we will provide a filename for *tar* (*f* flag).
-
-Now let's run it:
-----
-bin/logstash -e 'input { stdin { } } output { stdout {} }'
-----
-
-Now type something into your command prompt, and you will see it output by Logstash:
-----
-hello world
-2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
-----
-
-OK, that's interesting... We ran Logstash with an input called "stdin", and an output named "stdout", and Logstash basically echoed back whatever we typed in some sort of structured format. Note that specifying the *-e* command line flag allows Logstash to accept a configuration directly from the command line. This is especially useful for quickly testing configurations without having to edit a file between iterations.
-
-Let's try a slightly fancier example. First, you should exit Logstash by issuing a 'CTRL-D' command (or 'CTRL-C Enter') in the shell in which it is running. Now run Logstash again with the following command:
-----
-bin/logstash -e 'input { stdin { } } output { stdout { codec => rubydebug } }'
-----
-
-And then try another test input, typing the text "goodnight moon":
-----
-goodnight moon
-{
-  "message" => "goodnight moon",
-  "@timestamp" => "2013-11-20T23:48:05.335Z",
-  "@version" => "1",
-  "host" => "my-laptop"
-}
-----
-
-So, by re-configuring the "stdout" output (adding a "codec"), we can change the output of Logstash. By adding inputs, outputs and filters to your configuration, it's possible to massage the log data in many ways, in order to maximize flexibility of the stored data when you are querying it.
-
-== Storing logs with Elasticsearch
-Now, you're probably saying, "that's all fine and dandy, but typing all my logs into Logstash isn't really an option, and merely seeing them spit to STDOUT isn't very useful." Good point. First, let's set up Elasticsearch to store the messages we send into Logstash. If you don't have Elasticsearch already installed, you can http://www.elasticsearch.org/download/[download the RPM or DEB package], or install manually by downloading the current release tarball, by issuing the following four commands:
-----
-curl -O https://download.elasticsearch.org/elasticsearch/elasticsearch/elasticsearch-%ELASTICSEARCH_VERSION%.tar.gz
-tar zxvf elasticsearch-%ELASTICSEARCH_VERSION%.tar.gz
-cd elasticsearch-%ELASTICSEARCH_VERSION%/
-./bin/elasticsearch
-----
-
-NOTE: This tutorial specifies running Logstash %VERSION% with Elasticsearch %ELASTICSEARCH_VERSION%. Each release of Logstash has a *recommended* version of Elasticsearch to pair with. Make sure the versions match based on the http://www.elasticsearch.org/overview/logstash[Logstash version] you're running!
-
-More detailed information on installing and configuring Elasticsearch can be found on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/index.html[The Elasticsearch reference pages]. However, for the purposes of Getting Started with Logstash, the default installation and configuration of Elasticsearch should be sufficient.
-
-Now that we have Elasticsearch running on port 9200 (we do, right?), Logstash can be simply configured to use Elasticsearch as its backend. The defaults for both Logstash and Elasticsearch are fairly sane and well thought out, so we can omit the optional configurations within the elasticsearch output:
-----
-bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } }'
-----
-
-Type something, and Logstash will process it as before (this time you won't see any output, since we don't have the stdout output configured)
-----
-you know, for logs
-----
-
-You can confirm that ES actually received the data by making a curl request and inspecting the return:
-----
-curl 'http://localhost:9200/_search?pretty'
-----
-
-which should return something like this:
-----
-{
-  "took" : 2,
-  "timed_out" : false,
-  "_shards" : {
-    "total" : 5,
-    "successful" : 5,
-    "failed" : 0
-  },
-  "hits" : {
-    "total" : 1,
-    "max_score" : 1.0,
-    "hits" : [ {
-      "_index" : "logstash-2013.11.21",
-      "_type" : "logs",
-      "_id" : "2ijaoKqARqGvbMgP3BspJA",
-      "_score" : 1.0, "_source" : {"message":"you know, for logs","@timestamp":"2013-11-21T18:45:09.862Z","@version":"1","host":"my-laptop"}
-    } ]
-  }
-}
-----
-
-Congratulations! You've successfully stashed logs in Elasticsearch via Logstash.
-
-=== Elasticsearch Plugins (an aside)
-Another very useful tool for querying your Logstash data (and Elasticsearch in general) is the Elasticsearch-kopf plugin. Here is more information on http://www.elasticsearch.org/guide/en/elasticsearch/reference/current/modules-plugins.html[Elasticsearch plugins]. To install elasticsearch-kopf, simply issue the following command in your Elasticsearch directory (the same one in which you ran Elasticsearch earlier):
-----
-bin/plugin -install lmenezes/elasticsearch-kopf
-----
-Now you can browse to http://localhost:9200/_plugin/kopf[http://localhost:9200/_plugin/kopf] to browse your Elasticsearch data, settings and mappings!
-
-=== Multiple Outputs
-As a quick exercise in configuring multiple Logstash outputs, let's invoke Logstash again, using both the 'stdout' as well as the 'elasticsearch' output:
-----
-bin/logstash -e 'input { stdin { } } output { elasticsearch { host => localhost } stdout { } }'
-----
-Typing a phrase will now echo back to your terminal, as well as save in Elasticsearch! (Feel free to verify this using curl, kibana or elasticsearch-kopf).
-
-=== Default - Daily Indices
-You might notice that Logstash was smart enough to create a new index in Elasticsearch... The default index name is in the form of 'logstash-YYYY.MM.DD', which essentially creates one index per day. At midnight (GMT?), Logstash will automagically rotate the index to a fresh new one, with the new current day's timestamp. This allows you to keep windows of data, based on how far retroactively you'd like to query your log data. Of course, you can always archive (or re-index) your data to an alternate location, where you are able to query further into the past. If you'd like to simply delete old indices after a certain time period, you can use the https://github.com/elasticsearch/curator[Elasticsearch Curator tool].
-
-== Moving On
-Now you're ready for more advanced configurations. At this point, it makes sense for a quick discussion of some of the core features of Logstash, and how they interact with the Logstash engine.
-
-=== The Life of an Event
-
-Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configuration. By creating a pipeline of event processing, Logstash is able to extract the relevant data from your logs and make it available to elasticsearch, in order to efficiently query your data. To get you thinking about the various options available in Logstash, let's discuss some of the more common configurations currently in use. For more details, read about http://logstash.net/docs/latest/life-of-an-event[the Logstash event pipeline].
-
-==== Inputs
-Inputs are the mechanism for passing log data to Logstash. Some of the more useful, commonly-used ones are:
-
-* *file*: reads from a file on the filesystem, much like the UNIX command "tail -0F"
-* *syslog*: listens on the well-known port 514 for syslog messages and parses according to RFC3164 format
-* *redis*: reads from a redis server, using both redis channels and also redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
-* *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
-
-==== Filters
-Filters are used as intermediary processing devices in the Logstash chain. They are often combined with conditionals in order to perform a certain action on an event, if it matches particular criteria. Some useful filters:
-
-* *grok*: parses arbitrary text and structure it. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns shipped built-in to Logstash, it's more than likely you'll find one that meets your needs!
-* *mutate*: The mutate filter allows you to do general mutations to fields. You can rename, remove, replace, and modify fields in your events.
-* *drop*: drop an event completely, for example, 'debug' events.
-* *clone*: make a copy of an event, possibly adding or removing fields.
-* *geoip*: adds information about geographical location of IP addresses (and displays amazing charts in kibana)
-
-==== Outputs
-Outputs are the final phase of the Logstash pipeline. An event may pass through multiple outputs during processing, but once all outputs are complete, the event has finished its execution. Some commonly used outputs include:
-
-* *elasticsearch*: If you're planning to save your data in an efficient, convenient and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
-* *file*: writes event data to a file on disk.
-* *graphite*: sends event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
-* *statsd*: a service which "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
-
-==== Codecs
-Codecs are basically stream filters which can operate as part of an input, or an output. Codecs allow you to easily separate the transport of your messages from the serialization process. Popular codecs include 'json', 'msgpack' and 'plain' (text).
-
-* *json*: encode / decode data in JSON format
-* *multiline*: Takes multiple-line text events and merge them into a single event, e.g. java exception and stacktrace messages
-
-For the complete list of (current) configurations, visit the Logstash "plugin configuration" section of the http://www.elasticsearch.org/overview/logstash[Logstash documentation page].
-
-
-== More fun with Logstash
-=== Persistent Configuration files
-
-Specifying configurations on the command line using '-e' is only so helpful, and more advanced setups will require more lengthy, long-lived configurations. First, let's create a simple configuration file, and invoke Logstash using it. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
-
-----
-input { stdin { } }
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----
-
-Then, run this command:
-
-----
-bin/logstash -f logstash-simple.conf
-----
-
-Et voilà! Logstash will read in the configuration file you just created and run as in the example we saw earlier. Note that we used the '-f' to read in the file, rather than the '-e' to read the configuration from the command line. This is a very simple case, of course, so let's move on to some more complex examples.
-
-=== Testing Your Configuration Files
-
-After creating a new or complex configuration file, it can be helpful to quickly test that the file is formatted correctly. We can verify our configuration file is formatted correctly by using the *--configtest* flag.
-
-----
-bin/logstash -f logstash-simple.conf --configtest
-----
-
-=== Filters
-Filters are an in-line processing mechanism which provide the flexibility to slice and dice your data to fit your needs. Let's see one in action, namely the *grok filter*.
-
-----
-input { stdin { } }
-
-filter {
-  grok {
-    match => { "message" => "%{COMBINEDAPACHELOG}" }
-  }
-  date {
-    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
-  }
-}
-
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----
-Run Logstash with this configuration:
-
-----
-bin/logstash -f logstash-filter.conf
-----
-
-Now paste this line into the terminal (so it will be processed by the stdin input):
-----
-127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
-----
-You should see something returned to STDOUT which looks like this:
-----
-{
-        "message" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
-     "@timestamp" => "2013-12-11T08:01:45.000Z",
-       "@version" => "1",
-           "host" => "cadenza",
-       "clientip" => "127.0.0.1",
-          "ident" => "-",
-           "auth" => "-",
-      "timestamp" => "11/Dec/2013:00:01:45 -0800",
-           "verb" => "GET",
-        "request" => "/xampp/status.php",
-    "httpversion" => "1.1",
-       "response" => "200",
-          "bytes" => "3891",
-       "referrer" => "\"http://cadenza/xampp/navi.php\"",
-          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
-}
-----
-As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This will be extremely useful later when we start querying and analyzing our log data... for example, we'll be able to run reports on HTTP response codes, IP addresses, referrers, etc. very easily. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you're attempting to parse a fairly common log format, someone has already done the work for you. For more details, see the list of https://github.com/logstash/logstash/blob/master/patterns/grok-patterns[logstash grok patterns] on github.
-
-The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs, for example... the ability to tell Logstash "use this value as the timestamp for this event". For non-english installation you may have to precise the locale in date filter (locale => en).
-
-== Useful Examples
-
-=== Apache logs (from files)
-Now, let's configure something actually *useful*... apache2 access log files! We are going to read the input from a file on the localhost, and use a *conditional* to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you'll need to change the log's file path to suit your needs):
-
-----
-input {
-  file {
-    path => "/tmp/access_log"
-    start_position => "beginning"
-  }
-}
-
-filter {
-  if [path] =~ "access" {
-    mutate { replace => { "type" => "apache_access" } }
-    grok {
-      match => { "message" => "%{COMBINEDAPACHELOG}" }
-    }
-  }
-  date {
-    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
-  }
-}
-
-output {
-  elasticsearch {
-    host => localhost
-  }
-  stdout { codec => rubydebug }
-}
-
-----
-Then, create the file you configured above (in this example, "/tmp/access_log") with the following log lines as contents (or use some from your own webserver):
-
-----
-71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
-134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
-98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
-----
-
-Now run it with the -f flag as in the last example:
-----
-bin/logstash -f logstash-apache.conf
-----
-You should be able to see your apache log data in Elasticsearch now! You'll notice that Logstash opened the file you configured, and read through it, processing any events it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events and stored in Elasticsearch. As an added bonus, they will be stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
-
-In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching '*log'), by changing one line in the above configuration, like this:
-
-----
-input {
-  file {
-    path => "/tmp/*_log"
-...
-----
-Now, rerun Logstash, and you will see both the error and access logs processed via Logstash. However, if you inspect your data (using elasticsearch-kopf, perhaps), you will see that the access_log was broken up into discrete fields, but not the error_log. That's because we used a "grok" filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
-
-Also, you might have noticed that Logstash did not reprocess the events which were already seen in the access_log file. Logstash is able to save its position in files, only processing new lines as they are added to the file. Neat!
-
-=== Conditionals
-Now we can build on the previous example, where we introduced the concept of a *conditional*. A conditional should be familiar to most Logstash users, in the general sense. You may use 'if', 'else if' and 'else' statements, as in many other programming languages. Let's label each event according to which file it appeared in (access_log, error_log and other random files which end with "log").
-
-----
-input {
-  file {
-    path => "/tmp/*_log"
-  }
-}
-
-filter {
-  if [path] =~ "access" {
-    mutate { replace => { type => "apache_access" } }
-    grok {
-      match => { "message" => "%{COMBINEDAPACHELOG}" }
-    }
-    date {
-      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
-    }
-  } else if [path] =~ "error" {
-    mutate { replace => { type => "apache_error" } }
-  } else {
-    mutate { replace => { type => "random_logs" } }
-  }
-}
-
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----
-
-You'll notice we've labeled all events using the "type" field, but we didn't actually parse the "error" or "random" files... There are so many types of error logs that it's better left as an exercise for you, depending on the logs you're seeing.
-
-=== Syslog
-OK, now we can move on to another incredibly useful example: *syslog*. Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164 :). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line, so you can get a feel for what happens.
-
-First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
-
-----
-input {
-  tcp {
-    port => 5000
-    type => syslog
-  }
-  udp {
-    port => 5000
-    type => syslog
-  }
-}
-
-filter {
-  if [type] == "syslog" {
-    grok {
-      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
-      add_field => [ "received_at", "%{@timestamp}" ]
-      add_field => [ "received_from", "%{host}" ]
-    }
-    syslog_pri { }
-    date {
-      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
-    }
-  }
-}
-
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----
-Run it as normal:
-----
-bin/logstash -f logstash-syslog.conf
-----
-Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. In this simplified case, we're simply going to telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). First, open another shell window to interact with the Logstash syslog input and type the following command:
-
-----
-telnet localhost 5000
-----
-
-You can copy and paste the following lines as samples (feel free to try some of your own, but keep in mind they might not parse if the grok filter is not correct for your data):
-
-----
-Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
-Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
-Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)
-Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
-----
-
-Now you should see the output of Logstash in your original shell as it processes and parses messages!
-
-----
-{
-                 "message" => "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
-              "@timestamp" => "2013-12-23T22:30:01.000Z",
-                "@version" => "1",
-                    "type" => "syslog",
-                    "host" => "0:0:0:0:0:0:0:1:52617",
-        "syslog_timestamp" => "Dec 23 14:30:01",
-         "syslog_hostname" => "louis",
-          "syslog_program" => "CRON",
-              "syslog_pid" => "619",
-          "syslog_message" => "(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
-             "received_at" => "2013-12-23 22:49:22 UTC",
-           "received_from" => "0:0:0:0:0:0:0:1:52617",
-    "syslog_severity_code" => 5,
-    "syslog_facility_code" => 1,
-         "syslog_facility" => "user-level",
-         "syslog_severity" => "notice"
-}
-----
-
-Congratulations! You're well on your way to being a real Logstash power user. You should be comfortable configuring, running and sending events to Logstash, but there's much more to explore.
diff --git a/docs/tutorials/just-enough-rabbitmq-for-logstash.md b/docs/tutorials/just-enough-rabbitmq-for-logstash.md
deleted file mode 100644
index 060fa6f0ac2..00000000000
--- a/docs/tutorials/just-enough-rabbitmq-for-logstash.md
+++ /dev/null
@@ -1,201 +0,0 @@
----
-title: Just Enough RabbitMQ - logstash
-layout: content_right
----
-
-While configuring your RabbitMQ broker is out of scope for logstash, it's important
-to understand how logstash uses RabbitMQ. To do that, we need to understand a
-little about AMQP.
-
-You should also consider reading
-[this](http://www.rabbitmq.com/tutorials/amqp-concepts.html) at the RabbitMQ
-website.
-
-# Exchanges, queues and bindings; OH MY!
-
-You can get a long way by understanding a few key terms.
-
-## Exchanges
-
-Exchanges are for message **producers**. In Logstash, we map these to
-**outputs**.  Logstash puts messages on exchanges.  There are many types of
-exchanges and they are discussed below.
-
-## Queues
-
-Queues are for message **consumers**. In Logstash, we map these to inputs.
-Logstash reads messages from queues.  Optionally, queues can consume only a
-subset of messages. This is done with "routing keys".
-
-## Bindings
-
-Just having a producer and a consumer is not enough. We must `bind` a queue to
-an exchange.  When we bind a queue to an exchange, we can optionally provide a
-routing key.  Routing keys are discussed below.
-
-## Broker
-
-A broker is simply the AMQP server software. There are several brokers, but this
-tutorial will cover the most common (and arguably popular), [RabbitMQ](http://www.rabbitmq.com).
-
-# Routing Keys
-
-Simply put, routing keys are somewhat like tags for messages. In practice, they
-are hierarchical in nature with the each level separated by a dot:
-
-- `messages.servers.production`
-- `sports.atlanta.baseball`
-- `company.myorg.mydepartment`
-
-Routing keys are really handy with a tool like logstash where you
-can programatically define the routing key for a given event using the metadata that logstash provides:
-
-- `logs.servers.production.host1`
-- `logs.servers.development.host1.syslog`
-- `logs.servers.application_foo.critical`
-
-From a consumer/queue perspective, routing keys also support two types wildcards - `#` and `*`.
-
-- `*` (asterisk) matches any single word.
-- `#` (hash) matches any number of words and behaves like a traditional wildcard.
-
-Using the above examples, if you wanted to bind to an exchange and see messages
-for just production, you would use the routing key `logs.servers.production.*`.
-If you wanted to see messages for host1, regardless of environment you could
-use `logs.servers.%.host1.#`.
-
-Wildcards can be a bit confusing but a good general rule to follow is to use
-`*` in places where you need wildcards for a known element.  Use `#` when you
-need to match any remaining placeholders. Note that wildcards in routing keys
-only make sense on the consumer/queue binding, not in the publishing/exchange
-side.
-
-We'll get into some of that neat stuff below. For now, it's enough to
-understand the general idea behind routing keys.
-
-# Exchange types
-
-There are three primary types of exchanges that you'll see.
-
-## Direct
-
-A direct exchange is one that is probably most familiar to people. Message
-comes in and, assuming there is a queue bound, the message is picked up.  You
-can have multiple queues bound to the same direct exchange. The best way to
-understand this pattern is pool of workers (queues) that read from a direct
-exchange to get units of work. Only one consumer will see a given message in a
-direct exchange.
-
-You can set routing keys on messages published to a direct exchange. This
-allows you do have workers that do different tasks read from the same global
-pool of messages yet consume only the ones they know how to handle.
-
-The RabbitMQ concepts guide (linked below) does a good job of describing this
-visually
-[here](http://www.rabbitmq.com/img/tutorials/intro/exchange-direct.png)
-
-## Fanout
-
-Fanouts are another type of exchange. Unlike direct exchanges, every queue
-bound to a fanout exchange will see the same messages.  This is best described
-as a PUB/SUB pattern. This is helpful when you need broadcast messages to
-multiple interested parties.
-
-Fanout exchanges do NOT support routing keys. All bound queues see all
-messages.
-
-## Topic
-
-Topic exchanges are special type of fanout exchange. Fanout exchanges don't
-support routing keys. Topic exchanges do support them.  Just like a fanout
-exchange, all bound queues see all messages with the additional filter of the
-routing key.
-
-# RabbitMQ in logstash
-
-As stated earlier, in Logstash, Outputs publish to Exchanges. Inputs read from
-Queues that are bound to Exchanges.  Logstash uses the `bunny` RabbitMQ library for
-interaction with a broker. Logstash endeavors to expose as much of the
-configuration for both exchanges and queues.  There are many different tunables
-that you might be concerned with setting - including things like message
-durability or persistence of declared queues/exchanges.  See the relevant input
-and output documentation for RabbitMQ for a full list of tunables.
-
-# Sample configurations, tips, tricks and gotchas
-
-There are several examples in the logstash source directory of RabbitMQ usage,
-however a few general rules might help eliminate any issues.
-
-## Check your bindings
-
-If logstash is publishing the messages and logstash is consuming the messages,
-the `exchange` value for the input should match the `name` in the output.
-
-sender agent
-
-    input { stdin { type = "test" } }
-    output {
-      rabbitmq {
-        exchange => "test_exchange"
-        host => "my_rabbitmq_server"
-        exchange_type => "fanout"
-      }
-    }
-
-receiver agent
-
-    input {
-      rabbitmq {
-        queue => "test_queue"
-        host => "my_rabbitmq_server"
-        exchange => "test_exchange" # This matches the exchange declared above
-      }
-    }
-    output { stdout { debug => true }}
-
-## Message persistence
-
-By default, logstash will attempt to ensure that you don't lose any messages.
-This is reflected in the RabbitMQ default settings as well.  However there are
-cases where you might not want this. A good example is where RabbitMQ is not your
-primary method of shipping.
-
-In the following example, we use RabbitMQ as a sniffing interface. Our primary
-destination is the embedded ElasticSearch instance. We have a secondary RabbitMQ
-output that we use for duplicating messages. However we disable persistence and
-durability on this interface so that messages don't pile up waiting for
-delivery. We only use RabbitMQ when we want to watch messages in realtime.
-Additionally, we're going to leverage routing keys so that we can optionally
-filter incoming messages to subsets of hosts. The exercise of getting messages
-to this logstash agent are left up to the user.
-
-    input { 
-      # some input definition here
-    }
-
-    output {
-      elasticsearch { embedded => true }
-      rabbitmq {
-        exchange => "logtail"
-        host => "my_rabbitmq_server"
-        exchange_type => "topic" # We use topic here to enable pub/sub with routing keys
-        key => "logs.%{host}"
-        durable => false # If rabbitmq restarts, the exchange disappears.
-        auto_delete => true # If logstash disconnects, the exchange goes away
-        persistent => false # Messages are not persisted to disk
-      }
-    }
-
-Now if you want to stream logs in realtime, you can use the programming
-language of your choice to bind a queue to the `logtail` exchange.  If you do
-not specify a routing key, you will see every message that comes in to
-logstash. However, you can specify a routing key like `logs.apache1` and see
-only messages from host `apache1`.
-
-Note that any logstash variable is valid in the key definition. This allows you
-to create really complex routing key hierarchies for advanced filtering.
-
-Note that RabbitMQ has specific rules about durability and persistence matching
-on both the queue and exchange. You should read the RabbitMQ documentation to
-make sure you don't crash your RabbitMQ server with messages awaiting someone
-to pick them up.
diff --git a/docs/tutorials/media/frontend-response-codes.png b/docs/tutorials/media/frontend-response-codes.png
deleted file mode 100644
index e5b0ed47ee9..00000000000
Binary files a/docs/tutorials/media/frontend-response-codes.png and /dev/null differ
diff --git a/docs/tutorials/metrics-from-logs.md b/docs/tutorials/metrics-from-logs.md
deleted file mode 100644
index a044fef0fb9..00000000000
--- a/docs/tutorials/metrics-from-logs.md
+++ /dev/null
@@ -1,84 +0,0 @@
----
-title: Metrics from Logs - logstash
-layout: content_right
----
-# Pull metrics from logs
-
-Logs are more than just text. How many customers signed up today? How many HTTP
-errors happened this week? When was your last puppet run?
-
-Apache logs give you the http response code and bytes sent - that's useful in a
-graph. Metrics occur in logs so frequently there are piles of tools available to
-help process them.
-
-Logstash can help (and even replace some tools you might already be using).
-
-## Example: Replacing Etsy's Logster
-
-[Etsy](https://github.com/etsy) has some excellent open source tools. One of
-them, [logster](https://github.com/etsy/logster), is meant to help you pull
-metrics from logs and ship them to [graphite](http://graphite.wikidot.com/) so
-you can make pretty graphs of those metrics.
-
-One sample logster parser is one that pulls http response codes out of your
-apache logs: [SampleLogster.py](https://github.com/etsy/logster/blob/master/logster/parsers/SampleLogster.py)
-
-The above code is roughly 50 lines of python and only solves one specific
-problem in only apache logs: count http response codes by major number (1xx,
-2xx, 3xx, etc). To be completely fair, you could shrink the code required for
-a Logster parser, but size is not strictly the point, here.
-
-## Keep it simple
-
-Logstash can do more than the above, simpler, and without much coding skill:
-
-    input {
-      file { 
-        path => "/var/log/apache/access.log" 
-        type => "apache-access"
-      }
-    }
-
-    filter {
-      grok { 
-        type => "apache-access"
-        pattern => "%{COMBINEDAPACHELOG}" 
-      }
-    }
-
-    output {
-      statsd { 
-        # Count one hit every event by response
-        increment => "apache.response.%{response}" 
-      }
-    }
-
-The above uses grok to parse fields out of apache logs and using the statsd
-output to increment counters based on the response code. Of course, now that we
-are parsing apache logs fully, we can trivially add additional metrics:
-
-    output {
-      statsd {
-        # Count one hit every event by response
-        increment => "apache.response.%{response}"
-
-        # Use the 'bytes' field from the apache log as the count value.
-        count => [ "apache.bytes", "%{bytes}" ]
-      }
-    }
-
-Now adding additional metrics is just one more line in your logstash config
-file. BTW, the 'statsd' output writes to another Etsy tool,
-[statsd](https://github.com/etsy/statsd), which helps build counters/latency
-data and ship it to graphite for graphing.
-
-Using the logstash config above and a bunch of apache access requests, you might end up
-with a graph that looks like this:
-
-![apache response codes graphed with graphite, fed data with logstash](media/frontend-response-codes.png)
-
-The point made above is not "logstash is better than Logster" - the point is
-that logstash is a general-purpose log management and pipelining tool and that
-while you can centralize logs with logstash, you can read, modify, and write
-them to and from just about anywhere.
-
diff --git a/docs/tutorials/zeromq.md b/docs/tutorials/zeromq.md
deleted file mode 100644
index 796ec0ea3ae..00000000000
--- a/docs/tutorials/zeromq.md
+++ /dev/null
@@ -1,118 +0,0 @@
----
-title: ZeroMQ - logstash
-layout: content_right
----
-
-*ZeroMQ support in Logstash is currently in an experimental phase. As such, parts of this document are subject to change.*
-
-# ZeroMQ
-Simply put ZeroMQ (0mq) is a socket on steroids. This makes it a perfect compliment to Logstash - a pipe on steroids.
-
-ZeroMQ allows you to easily create sockets of various types for moving data around. These sockets are refered to in ZeroMQ by the behavior of each side of the socket pair:
-
-* PUSH/PULL
-* REQ/REP
-* PUB/SUB
-* ROUTER/DEALER
-
-There is also a `PAIR` socket type as well.
-
-Additionally, the socket type is independent of the connection method. A PUB/SUB socket pair could have the SUB side of the socket be a listener and the PUB side a connecting client. This makes it very easy to fit ZeroMQ into various firewalled architectures.
-
-Note that this is not a full-fledged tutorial on ZeroMQ. It is a tutorial on how Logstash uses ZeroMQ.
-
-# ZeroMQ and logstash
-In the spirit of ZeroMQ, Logstash takes these socket type pairs and uses them to create topologies with some very simply rules that make usage very easy to understand:
-
-* The receiving end of a socket pair is always a logstash input
-* The sending end of a socket pair is always a logstash output
-* By default, inputs `bind`/listen and outputs `connect`
-* Logstash refers to the socket pairs as topologies and mirrors the naming scheme from ZeroMQ
-* By default, ZeroMQ inputs listen on all interfaces on port 2120, ZeroMQ outputs connect to `localhost` on port 2120
-
-The currently understood Logstash topologies for ZeroMQ inputs and outputs are:
-
-* `pushpull`
-* `pubsub`
-* `pair`
-
-We have found from various discussions that these three topologies will cover most of user's needs. We hope to expose the full span of ZeroMQ socket types as time goes on.
-
-By keeping the options simple, this allows you to get started VERY easily with what are normally complex message flows. No more confusion over `exchanges` and `queues` and `brokers`. If you need to add fanout capability to your flow, you can simply use the following configs:
-
-* _node agent lives at 192.168.1.2_
-* _indexer agent lives at 192.168.1.1_
-
-    # Node agent config
-    input { stdin { type => "test-stdin-input" } }
-    output { zeromq { topology => "pubsub" address => "tcp://192.168.1.1.:2120" } }
-
-    # Indexer agent config
-    input { zeromq { topology => "pubsub" } }
-    output { stdout { debug => true }}
-
-If for some reason you need connections to initiate from the indexer because of firewall rules:
-
-    # Node agent config - now listening on all interfaces port 2120
-    input { stdin { type => "test-stdin-input" } }
-    output { zeromq { topology => "pubsub" address => "tcp://*.:2120" mode => "server" } }
-
-    # Indexer agent config
-    input { zeromq { topology => "pubsub" address => "tcp://192.168.1.2" mode => "client" } }
-    output { stdout { debug => true }}
-
-As stated above, by default `inputs` always start as listeners and `outputs` always start as initiators. Please don't confuse what happens once the socket is connect with the direction of the connection. ZeroMQ separates connection from topology. In the second case of the above configs, once the two sockets are connected, regardless of who initiated the connection, the message flow itself is absolute. The indexer is reading events from the node.
-
-# Which topology to use
-The choice of topology can be broken down very easily based on need
-
-## one to one
-Use `pair` topology. On the output side, specify the ipaddress and port of the input side.
-
-## broadcast
-Use `pubsub`
-If you need to broadcast ALL messages to multiple hosts that each need to see all events, use `pubsub`. Note that all events are broadcast to all subscribers. When using `pubsub` you might also want to investigate the `topic` configuration option which allows subscribers to see only a subset of messages.
-
-## Filter workers
-Use `pushpull`
-In `pushpull`, ZeroMQ automatically load balances to all connected peers. This means that no peer sees the same message as any other peer.
-
-# What's with the address format?
-ZeroMQ supports multiple types of transports:
-
-* inproc:// (unsupported by logstash due to threading)
-* tcp:// (exactly what it sounds like)
-* ipc:// (probably useless in logstash)
-* pgm:// and epgm:// (a multicast format - only usable with PUB and SUB socket types)
-
-For pretty much all cases, you'll be using `tcp://` transports with Logstash.
-
-## Topic - applies to `pubsub`
-This opt mimics the routing keys functionality in AMQP. Imagine you have a network of receivers but only a subset of the messages need to be seen by a subset of the hosts. You can use this option as a routing key to facilite that:
-
-    # This output is a PUB
-    output {
-    zeromq { topology => "pubsub" topic => "logs.production.%{host}" }
-    }
-
-    # This input is a SUB
-    # I only care about db1 logs
-    input { zeromq { type => "db1logs" address => "tcp://<ipaddress>:2120" topic => "logs.production.db1"}}
-
-One thing important to note about 0mq PUBSUB and topics is that all filtering is done on the subscriber side. The subscriber will get ALL messages but discard any that don't match the topic.
-
-Also important to note is that 0mq doesn't do topic in the same sense as an AMQP broker might. When a SUB socket gets a message, it compares the first bytes of the message against the topic. However, this isn't always flexible depending on the format of your message. The common practice then, is to send a 0mq multipart message and make the first part the topic. The next parts become the actual message body.
-
-This is approach is how logstash handles this. When using PUBSUB, Logstash will send a multipart message where the first part is the name of the topic and the second part is the event. This is important to know if you are sending to a SUB input from sources other than Logstash.
-
-# sockopts
-Sockopts is not you choosing between blue or black socks. ZeroMQ supports setting various flags or options on sockets. In the interest of minimizing configuration syntax, these are _hidden_ behind a logstash configuration element called `sockopts`. You probably won't need to tune these for most cases. If you do need to tune them, you'll probably set the following:
-
-## ZMQ::HWM - sets the high water mark
-The high water mark is the maximum number of messages a given socket pair can have in its internal queue. Use this to throttle essentially.
-
-## ZMQ::SWAP_SIZE
-TODO
-
-## ZMQ::IDENTITY
-TODO
diff --git a/lib/bootstrap/bundler.rb b/lib/bootstrap/bundler.rb
index 23944d347fe..1a7bb057219 100644
--- a/lib/bootstrap/bundler.rb
+++ b/lib/bootstrap/bundler.rb
@@ -27,6 +27,16 @@ def set_key(key, value, hash, file)
           value
         end
       end
+
+      # This patch makes rubygems fetch directly from the remote servers
+      # the dependencies he need and might not have downloaded in a local
+      # repository. This basically enabled the offline feature to work as
+      # we remove the gems from the vendor directory before packacing.
+      ::Bundler::Source::Rubygems.module_exec do
+        def cached_gem(spec)
+          cached_built_in_gem(spec)
+        end
+      end
     end
 
     def setup!(options = {})
@@ -56,11 +66,19 @@ def setup!(options = {})
 
     # execute bundle install and capture any $stdout output. any raised exception in the process will be trapped
     # and returned. logs errors to $stdout.
-    # @param options [Hash] invoke options with default values, :max_tries => 10, :clean => false, :install => false, :update => false
-    # @param   options[:update] must be either false or a String or an Array of String
+    # @param [Hash] options invoke options with default values, :max_tries => 10, :clean => false, :install => false, :update => false
+    # @option options [Boolean] :max_tries The number of times bundler is going to try the installation before failing (default: 10)
+    # @option options [Boolean] :clean It cleans the unused gems (default: false)
+    # @option options [Boolean] :install Run the installation of a set of gems defined in a Gemfile (default: false)
+    # @option options [Boolean, String, Array] :update Update the current environment, must be either false or a String or an Array of String (default: false)
+    # @option options [Boolean] :local Do not attempt to fetch gems remotely and use the gem cache instead (default: false)
+    # @option options [Boolean] :package Locks and then caches all dependencies to be reused later on (default: false)
+    # @option options [Boolean] :all It packages dependencies defined with :git or :path (default: false)
+    # @option options [Array] :without  Exclude gems that are part of the specified named group (default: [:development])
     # @return [String, Exception] the installation captured output and any raised exception or nil if none
     def invoke!(options = {})
-      options = {:max_tries => 10, :clean => false, :install => false, :update => false, :without => [:development]}.merge(options)
+      options = {:max_tries => 10, :clean => false, :install => false, :update => false, :local => false,
+                 :all => false, :package => false, :without => [:development]}.merge(options)
       options[:without] = Array(options[:without])
       options[:update] = Array(options[:update]) if options[:update]
 
@@ -80,20 +98,28 @@ def invoke!(options = {})
       LogStash::Bundler.patch!
 
       # force Rubygems sources to our Gemfile sources
-      ::Gem.sources = options[:rubygems_source] if options[:rubygems_source]
+      ::Gem.sources = ::Gem::SourceList.from(options[:rubygems_source]) if options[:rubygems_source]
 
       ::Bundler.settings[:path] = LogStash::Environment::BUNDLE_DIR
       ::Bundler.settings[:gemfile] = LogStash::Environment::GEMFILE_PATH
       ::Bundler.settings[:without] = options[:without].join(":")
 
-      try = 0
+      if !debug?
+        # Will deal with transient network errors
+        execute_bundler_with_retry(options)
+      else
+        options[:verbose] = true
+        execute_bundler(options)
+      end
+    end
 
+    def execute_bundler_with_retry(options)
+      try = 0
       # capture_stdout also traps any raised exception and pass them back as the function return [output, exception]
       output, exception = capture_stdout do
         loop do
           begin
-            ::Bundler.reset!
-            ::Bundler::CLI.start(bundler_arguments(options))
+            execute_bundler(options)
             break
           rescue ::Bundler::VersionConflict => e
             $stderr.puts("Plugin version conflict, aborting")
@@ -115,12 +141,20 @@ def invoke!(options = {})
           end
         end
       end
-
       raise exception if exception
 
       return output
     end
 
+    def execute_bundler(options)
+      ::Bundler.reset!
+      ::Bundler::CLI.start(bundler_arguments(options))
+    end
+
+    def debug?
+      ENV["DEBUG"]
+    end
+
     # build Bundler::CLI.start arguments array from the given options hash
     # @param option [Hash] the invoke! options hash
     # @return [Array<String>] Bundler::CLI.start string arguments array
@@ -130,13 +164,23 @@ def bundler_arguments(options = {})
       if options[:install]
         arguments << "install"
         arguments << "--clean" if options[:clean]
+        if options[:local]
+          arguments << "--local"
+          arguments << "--no-prune" # From bundler docs: Don't remove stale gems from the cache.
+        end
       elsif options[:update]
         arguments << "update"
         arguments << options[:update]
+        arguments << "--local" if options[:local]
       elsif options[:clean]
         arguments << "clean"
+      elsif options[:package]
+        arguments << "package"
+        arguments << "--all" if options[:all]
       end
 
+      arguments << "--verbose" if options[:verbose]
+
       arguments.flatten
     end
 
diff --git a/lib/bootstrap/environment.rb b/lib/bootstrap/environment.rb
index 9f3e59f5b08..ab73f5e37e4 100644
--- a/lib/bootstrap/environment.rb
+++ b/lib/bootstrap/environment.rb
@@ -16,6 +16,7 @@ module Environment
     BUNDLE_DIR = ::File.join(LOGSTASH_HOME, "vendor", "bundle")
     GEMFILE_PATH = ::File.join(LOGSTASH_HOME, "Gemfile")
     LOCAL_GEM_PATH = ::File.join(LOGSTASH_HOME, 'vendor', 'local_gems')
+    CACHE_PATH = File.join(LOGSTASH_HOME, "vendor", "cache")
 
     # @return [String] the ruby version string bundler uses to craft its gem path
     def gem_ruby_version
@@ -32,19 +33,36 @@ def ruby_engine
       RUBY_ENGINE
     end
 
+    def windows?
+      ::Gem.win_platform?
+    end
+
+    def jruby?
+      @jruby ||= !!(RUBY_PLATFORM == "java")
+    end
+
     def logstash_gem_home
       ::File.join(BUNDLE_DIR, ruby_engine, gem_ruby_version)
     end
+
+    def vendor_path(path)
+      return ::File.join(LOGSTASH_HOME, "vendor", path)
+    end
+
+    def pattern_path(path)
+      return ::File.join(LOGSTASH_HOME, "patterns", path)
+    end
+
   end
 end
 
-
-# when launched as a script, not require'd, (currently from bin/logstash and bin/plugin) the first
+# when launched as a script, not require'd, (currently from bin/logstash and bin/logstash-plugin) the first
 # argument is the path of a Ruby file to require and a LogStash::Runner class is expected to be
 # defined and exposing the LogStash::Runner#main instance method which will be called with the current ARGV
 # currently lib/logstash/runner.rb and lib/pluginmanager/main.rb are called using this.
 if $0 == __FILE__
   LogStash::Bundler.setup!({:without => [:build, :development]})
   require ARGV.shift
-  LogStash::Runner.new.main(ARGV)
+  exit_status = LogStash::Runner.run("bin/logstash", ARGV)
+  exit(exit_status || 0)
 end
diff --git a/lib/bootstrap/rspec.rb b/lib/bootstrap/rspec.rb
index f32057c7f9c..4c95f3bfc76 100755
--- a/lib/bootstrap/rspec.rb
+++ b/lib/bootstrap/rspec.rb
@@ -7,6 +7,7 @@
 
 require "rspec/core"
 require "rspec"
+require 'ci/reporter/rake/rspec_loader'
 
 status = RSpec::Core::Runner.run(ARGV.empty? ? ["spec"] : ARGV).to_i
 exit status if status != 0
diff --git a/lib/bootstrap/util/compress.rb b/lib/bootstrap/util/compress.rb
new file mode 100644
index 00000000000..79bd38461b4
--- /dev/null
+++ b/lib/bootstrap/util/compress.rb
@@ -0,0 +1,122 @@
+# encoding: utf-8
+require "zip"
+require "rubygems/package"
+require "fileutils"
+require "zlib"
+require "stud/temporary"
+
+module LogStash
+
+  class CompressError < StandardError; end
+
+  module Util
+    module Zip
+
+      extend self
+
+      # Extract a zip file into a destination directory.
+      # @param source [String] The location of the file to extract
+      # @param target [String] Where you do want the file to be extracted
+      # @raise [IOError] If the target directory already exist
+      def extract(source, target)
+        raise CompressError.new("Directory #{target} exist") if ::File.exist?(target)
+        ::Zip::File.open(source) do |zip_file|
+          zip_file.each do |file|
+            path = ::File.join(target, file.name)
+            FileUtils.mkdir_p(::File.dirname(path))
+            zip_file.extract(file, path)
+          end
+        end
+      end
+
+      # Compress a directory into a zip file
+      # @param dir [String] The directory to be compressed
+      # @param target [String] Destination to save the generated file
+      # @raise [IOError] If the target file already exist
+      def compress(dir, target)
+        raise CompressError.new("File #{target} exist") if ::File.exist?(target)
+        ::Zip::File.open(target, ::Zip::File::CREATE) do |zipfile|
+          Dir.glob("#{dir}/**/*").each do |file|
+            path_in_zip = file.gsub("#{dir}/","")
+            zipfile.add(path_in_zip, file)
+          end
+        end
+      end
+    end
+
+    module Tar
+
+      extend self
+
+      # Extract a tar.gz file into a destination directory.
+      # @param source [String] The location of the file to extract
+      # @param target [String] Where you do want the file to be extracted
+      # @raise [IOError] If the target directory already exist
+      def extract(file, target)
+        raise CompressError.new("Directory #{target} exist") if ::File.exist?(target)
+
+        FileUtils.mkdir(target)
+        Zlib::GzipReader.open(file) do |gzip_file|
+          ::Gem::Package::TarReader.new(gzip_file) do |tar_file|
+            tar_file.each do |entry|
+              target_path = ::File.join(target, entry.full_name)
+
+              if entry.directory?
+                FileUtils.mkdir_p(target_path)
+              else # is a file to be extracted
+                ::File.open(target_path, "wb") { |f| f.write(entry.read) }
+              end
+            end
+          end
+        end
+      end
+
+      # Compress a directory into a tar.gz file
+      # @param dir [String] The directory to be compressed
+      # @param target [String] Destination to save the generated file
+      # @raise [IOError] If the target file already exist
+      def compress(dir, target)
+        raise CompressError.new("File #{target} exist") if ::File.exist?(target)
+
+        Stud::Temporary.file do |tar_file|
+          ::Gem::Package::TarWriter.new(tar_file) do |tar|
+            Dir.glob("#{dir}/**/*").each do |file|
+              name = file.gsub("#{dir}/","")
+              stats = ::File.stat(file)
+              mode  = stats.mode
+
+              if ::File.directory?(file)
+                tar.mkdir(name, mode)
+              else # is a file to be added
+                tar.add_file(name,mode) do |out|
+                  File.open(file, "rb") do |fd|
+                    chunk = nil
+                    size = 0
+                    size += out.write(chunk) while chunk = fd.read(16384)
+                    if stats.size != size
+                      raise "Failure to write the entire file (#{path}) to the tarball. Expected to write #{stats.size} bytes; actually write #{size}"
+                    end
+                  end
+                end
+              end
+            end
+          end
+
+          tar_file.rewind
+          gzip(target, tar_file)
+        end
+      end
+
+      # Compress a file using gzip
+      # @param path [String] The location to be compressed
+      # @param target_file [String] Destination of the generated file
+      def gzip(path, target_file)
+        ::File.open(path, "wb") do |file|
+          gzip_file = ::Zlib::GzipWriter.new(file)
+          gzip_file.write(target_file.read)
+          gzip_file.close
+        end
+      end
+    end
+  end
+end
diff --git a/lib/logstash-event.rb b/lib/logstash-event.rb
deleted file mode 100644
index 0f44322944b..00000000000
--- a/lib/logstash-event.rb
+++ /dev/null
@@ -1,2 +0,0 @@
-# encoding: utf-8
-require "logstash/event"
diff --git a/lib/logstash/agent.rb b/lib/logstash/agent.rb
deleted file mode 100644
index bb6734f8ad1..00000000000
--- a/lib/logstash/agent.rb
+++ /dev/null
@@ -1,339 +0,0 @@
-# encoding: utf-8
-require "clamp" # gem 'clamp'
-require "logstash/environment"
-require "logstash/errors"
-require "logstash/config/cpu_core_strategy"
-require "uri"
-require "net/http"
-LogStash::Environment.load_locale!
-
-class LogStash::Agent < Clamp::Command
-  DEFAULT_INPUT = "input { stdin { type => stdin } }"
-  DEFAULT_OUTPUT = "output { stdout { codec => rubydebug } }"
-
-  option ["-f", "--config"], "CONFIG_PATH",
-    I18n.t("logstash.agent.flag.config"),
-    :attribute_name => :config_path
-
-  option "-e", "CONFIG_STRING",
-    I18n.t("logstash.agent.flag.config-string",
-           :default_input => DEFAULT_INPUT, :default_output => DEFAULT_OUTPUT),
-    :default => "", :attribute_name => :config_string
-
-  option ["-w", "--filterworkers"], "COUNT",
-    I18n.t("logstash.agent.flag.filterworkers"),
-    :attribute_name => :filter_workers,
-    :default => LogStash::Config::CpuCoreStrategy.fifty_percent, &:to_i
-
-  option ["-l", "--log"], "FILE",
-    I18n.t("logstash.agent.flag.log"),
-    :attribute_name => :log_file
-
-  # Old support for the '-v' flag'
-  option "-v", :flag,
-    I18n.t("logstash.agent.flag.verbosity"),
-    :attribute_name => :verbosity, :multivalued => true
-
-  option "--quiet", :flag, I18n.t("logstash.agent.flag.quiet")
-  option "--verbose", :flag, I18n.t("logstash.agent.flag.verbose")
-  option "--debug", :flag, I18n.t("logstash.agent.flag.debug")
-
-  option ["-V", "--version"], :flag,
-    I18n.t("logstash.agent.flag.version")
-
- option ["-p", "--pluginpath"] , "PATH",
-   I18n.t("logstash.agent.flag.pluginpath"),
-   :multivalued => true,
-   :attribute_name => :plugin_paths
-
-  option ["-t", "--configtest"], :flag,
-    I18n.t("logstash.agent.flag.configtest"),
-    :attribute_name => :config_test
-
-  # Emit a warning message.
-  def warn(message)
-    # For now, all warnings are fatal.
-    raise LogStash::ConfigurationError, message
-  end # def warn
-
-  # Emit a failure message and abort.
-  def fail(message)
-    raise LogStash::ConfigurationError, message
-  end # def fail
-
-  def report(message)
-    # Print to stdout just in case we're logging to a file
-    puts message
-    @logger.log(message) if log_file
-  end
-
-  # Run the agent. This method is invoked after clamp parses the
-  # flags given to this program.
-  def execute
-    require "logstash/pipeline"
-    require "cabin" # gem 'cabin'
-    require "logstash/plugin"
-    @logger = Cabin::Channel.get(LogStash)
-
-    if version?
-      show_version
-      return 0
-    end
-
-    # temporarily send logs to stdout as well if a --log is specified
-    # and stdout appears to be a tty
-    show_startup_errors = log_file && STDOUT.tty?
-
-    if show_startup_errors
-      stdout_logs = @logger.subscribe(STDOUT)
-    end
-    configure
-
-    # You must specify a config_string or config_path
-    if @config_string.nil? && @config_path.nil?
-      fail(help + "\n" + I18n.t("logstash.agent.missing-configuration"))
-    end
-
-    @config_string = @config_string.to_s
-
-    if @config_path
-      # Append the config string.
-      # This allows users to provide both -f and -e flags. The combination
-      # is rare, but useful for debugging.
-      @config_string = @config_string + load_config(@config_path)
-    else
-      # include a default stdin input if no inputs given
-      if @config_string !~ /input *{/
-        @config_string += DEFAULT_INPUT
-      end
-      # include a default stdout output if no outputs given
-      if @config_string !~ /output *{/
-        @config_string += DEFAULT_OUTPUT
-      end
-    end
-
-    begin
-      pipeline = LogStash::Pipeline.new(@config_string)
-    rescue LoadError => e
-      fail("Configuration problem.")
-    end
-
-    # Make SIGINT shutdown the pipeline.
-    sigint_id = Stud::trap("INT") do
-
-      if @interrupted_once
-        @logger.fatal(I18n.t("logstash.agent.forced_sigint"))
-        exit
-      else
-        @logger.warn(I18n.t("logstash.agent.sigint"))
-        Thread.new(@logger) {|logger| sleep 5; logger.warn(I18n.t("logstash.agent.slow_shutdown")) }
-        @interrupted_once = true
-        shutdown(pipeline)
-      end
-    end
-
-    # Make SIGTERM shutdown the pipeline.
-    sigterm_id = Stud::trap("TERM") do
-      @logger.warn(I18n.t("logstash.agent.sigterm"))
-      shutdown(pipeline)
-    end
-
-    Stud::trap("HUP") do
-      @logger.info(I18n.t("logstash.agent.sighup"))
-      configure_logging(log_file)
-    end
-
-    pipeline.configure("filter-workers", filter_workers)
-
-    # Stop now if we are only asking for a config test.
-    if config_test?
-      report "Configuration OK"
-      return
-    end
-
-    @logger.unsubscribe(stdout_logs) if show_startup_errors
-
-    # TODO(sissel): Get pipeline completion status.
-    pipeline.run
-    return 0
-  rescue LogStash::ConfigurationError => e
-    @logger.unsubscribe(stdout_logs) if show_startup_errors
-    report I18n.t("logstash.agent.error", :error => e)
-    if !config_test?
-      report I18n.t("logstash.agent.configtest-flag-information")
-    end
-    return 1
-  rescue => e
-    @logger.unsubscribe(stdout_logs) if show_startup_errors
-    report I18n.t("oops", :error => e)
-    report e.backtrace if @logger.debug? || $DEBUGLIST.include?("stacktrace")
-    return 1
-  ensure
-    @log_fd.close if @log_fd
-    Stud::untrap("INT", sigint_id) unless sigint_id.nil?
-    Stud::untrap("TERM", sigterm_id) unless sigterm_id.nil?
-  end # def execute
-
-  def shutdown(pipeline)
-    pipeline.shutdown do
-      InflightEventsReporter.logger = @logger
-      InflightEventsReporter.start(pipeline.input_to_filter, pipeline.filter_to_output, pipeline.outputs)
-    end
-  end
-
-  def show_version
-    show_version_logstash
-
-    if [:info, :debug].include?(verbosity?) || debug? || verbose?
-      show_version_ruby
-      show_version_java if LogStash::Environment.jruby?
-      show_gems if [:debug].include?(verbosity?) || debug?
-    end
-  end # def show_version
-
-  def show_version_logstash
-    require "logstash/version"
-    puts "logstash #{LOGSTASH_VERSION}"
-  end # def show_version_logstash
-
-  def show_version_ruby
-    puts RUBY_DESCRIPTION
-  end # def show_version_ruby
-
-  def show_version_java
-    properties = java.lang.System.getProperties
-    puts "java #{properties["java.version"]} (#{properties["java.vendor"]})"
-    puts "jvm #{properties["java.vm.name"]} / #{properties["java.vm.version"]}"
-  end # def show_version_java
-
-  def show_gems
-    require "rubygems"
-    Gem::Specification.each do |spec|
-      puts "gem #{spec.name} #{spec.version}"
-    end
-  end # def show_gems
-
-  # Do any start-time configuration.
-  #
-  # Log file stuff, plugin path checking, etc.
-  def configure
-    configure_logging(log_file)
-    configure_plugin_paths(plugin_paths)
-  end # def configure
-
-  # Point logging at a specific path.
-  def configure_logging(path)
-    # Set with the -v (or -vv...) flag
-    if quiet?
-      @logger.level = :error
-    elsif verbose?
-      @logger.level = :info
-    elsif debug?
-      @logger.level = :debug
-    else
-      # Old support for the -v and -vv stuff.
-      if verbosity? && verbosity?.any?
-        # this is an array with length of how many times the flag is given
-        if verbosity?.length == 1
-          @logger.warn("The -v flag is deprecated and will be removed in a future release. You should use --verbose instead.")
-          @logger.level = :info
-        else
-          @logger.warn("The -vv flag is deprecated and will be removed in a future release. You should use --debug instead.")
-          @logger.level = :debug
-        end
-      else
-        @logger.level = :warn
-      end
-    end
-
-    if log_file
-      # TODO(sissel): Implement file output/rotation in Cabin.
-      # TODO(sissel): Catch exceptions, report sane errors.
-      begin
-        @log_fd.close if @log_fd
-        @log_fd = File.new(path, "a")
-      rescue => e
-        fail(I18n.t("logstash.agent.configuration.log_file_failed",
-                    :path => path, :error => e))
-      end
-
-      puts "Sending logstash logs to #{path}."
-      @logger.unsubscribe(@logger_subscription) if @logger_subscription
-      @logger_subscription = @logger.subscribe(@log_fd)
-    else
-      @logger.subscribe(STDOUT)
-    end
-
-    # TODO(sissel): redirect stdout/stderr to the log as well
-    # http://jira.codehaus.org/browse/JRUBY-7003
-  end # def configure_logging
-
-  # add the given paths for ungemified/bare plugins lookups
-  # @param paths [String, Array<String>] plugins path string or list of path strings to add
-  def configure_plugin_paths(paths)
-    Array(paths).each do |path|
-      fail(I18n.t("logstash.agent.configuration.plugin_path_missing", :path => path)) unless File.directory?(path)
-      LogStash::Environment.add_plugin_path(path)
-    end
-  end
-
-  def load_config(path)
-    begin
-      uri = URI.parse(path)
-
-      case uri.scheme
-      when nil then
-        local_config(path)
-      when /http/ then
-        fetch_config(uri)
-      when "file" then
-        local_config(uri.path)
-      else
-        fail(I18n.t("logstash.agent.configuration.scheme-not-supported", :path => path))
-      end
-    rescue URI::InvalidURIError
-      # fallback for windows.
-      # if the parsing of the file failed we assume we can reach it locally.
-      # some relative path on windows arent parsed correctly (.\logstash.conf)
-      local_config(path)
-    end
-  end
-
-  def local_config(path)
-    path = File.expand_path(path)
-    path = File.join(path, "*") if File.directory?(path)
-
-    if Dir.glob(path).length == 0
-      fail(I18n.t("logstash.agent.configuration.file-not-found", :path => path))
-    end
-
-    config = ""
-    encoding_issue_files = []
-    Dir.glob(path).sort.each do |file|
-      next unless File.file?(file)
-      if file.match(/~$/)
-        @logger.debug("NOT reading config file because it is a temp file", :file => file)
-        next
-      end
-      @logger.debug("Reading config file", :file => file)
-      cfg = File.read(file)
-      if !cfg.ascii_only? && !cfg.valid_encoding?
-        encoding_issue_files << file
-      end
-      config << cfg + "\n"
-    end
-    if (encoding_issue_files.any?)
-      fail("The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}")
-    end
-    return config
-  end # def load_config
-
-  def fetch_config(uri)
-    begin
-      Net::HTTP.get(uri) + "\n"
-    rescue Exception => e
-      fail(I18n.t("logstash.agent.configuration.fetch-failed", :path => uri.to_s, :message => e.message))
-    end
-  end
-
-end # class LogStash::Agent
diff --git a/lib/logstash/patches/bundler.rb b/lib/logstash/patches/bundler.rb
deleted file mode 100644
index 25d93a09148..00000000000
--- a/lib/logstash/patches/bundler.rb
+++ /dev/null
@@ -1,36 +0,0 @@
-# encoding: utf-8
-# Bundler monkey patches
-module ::Bundler
-  # Patch bundler to write a .lock file specific to the version of ruby.
-  # This keeps MRI/JRuby/RBX from conflicting over the Gemfile.lock updates
-  module SharedHelpers
-    def default_lockfile
-      ruby = "#{LogStash::Environment.ruby_engine}-#{LogStash::Environment.ruby_abi_version}"
-      Pathname.new("#{default_gemfile}.#{ruby}.lock")
-    end
-  end
-
-  # Patch to prevent Bundler to save a .bundle/config file in the root 
-  # of the application
-  class Settings
-    def set_key(key, value, hash, file)
-      key = key_for(key)
-
-      unless hash[key] == value
-        hash[key] = value
-        hash.delete(key) if value.nil?
-      end
-
-      value
-    end
-  end
-
-  # Add the Bundler.reset! method which has been added in master but is not in 1.7.9.
-  class << self
-    unless self.method_defined?("reset!")
-      def reset!
-        @definition = nil
-      end
-    end
-  end
-end
diff --git a/lib/logstash/pipeline.rb b/lib/logstash/pipeline.rb
deleted file mode 100644
index b3081073704..00000000000
--- a/lib/logstash/pipeline.rb
+++ /dev/null
@@ -1,312 +0,0 @@
-# encoding: utf-8
-require "thread"
-require "stud/interval"
-require "concurrent"
-require "logstash/namespace"
-require "logstash/errors"
-require "logstash/event"
-require "logstash/config/file"
-require "logstash/filters/base"
-require "logstash/inputs/base"
-require "logstash/outputs/base"
-require "logstash/util/reporter"
-require "logstash/config/cpu_core_strategy"
-require "logstash/util/defaults_printer"
-
-class LogStash::Pipeline
-  attr_reader :inputs, :filters, :outputs, :input_to_filter, :filter_to_output
-
-  def initialize(configstr)
-    @logger = Cabin::Channel.get(LogStash)
-
-    @inputs = nil
-    @filters = nil
-    @outputs = nil
-
-    grammar = LogStashConfigParser.new
-    @config = grammar.parse(configstr)
-    if @config.nil?
-      raise LogStash::ConfigurationError, grammar.failure_reason
-    end
-    # This will compile the config to ruby and evaluate the resulting code.
-    # The code will initialize all the plugins and define the
-    # filter and output methods.
-    code = @config.compile
-    # The config code is hard to represent as a log message...
-    # So just print it.
-    @logger.debug? && @logger.debug("Compiled pipeline code:\n#{code}")
-    begin
-      eval(code)
-    rescue => e
-      raise
-    end
-
-    @input_to_filter = SizedQueue.new(20)
-    # if no filters, pipe inputs directly to outputs
-    @filter_to_output = filters? ? SizedQueue.new(20) : @input_to_filter
-
-    @settings = {
-      "filter-workers" => LogStash::Config::CpuCoreStrategy.fifty_percent
-    }
-
-    # @ready requires thread safety since it is typically polled from outside the pipeline thread
-    @ready = Concurrent::AtomicBoolean.new(false)
-    @input_threads = []
-  end # def initialize
-
-  def ready?
-    @ready.value
-  end
-
-  def configure(setting, value)
-    if setting == "filter-workers" && value > 1
-      # Abort if we have any filters that aren't threadsafe
-      plugins = @filters.select { |f| !f.threadsafe? }.collect { |f| f.class.config_name }
-      if !plugins.size.zero?
-        raise LogStash::ConfigurationError, "Cannot use more than 1 filter worker because the following plugins don't work with more than one worker: #{plugins.join(", ")}"
-      end
-    end
-    @settings[setting] = value
-  end
-
-  def filters?
-    return @filters.any?
-  end
-
-  def run
-    @logger.terminal(LogStash::Util::DefaultsPrinter.print(@settings))
-
-    begin
-      start_inputs
-      start_filters if filters?
-      start_outputs
-    ensure
-      # it is important to garantee @ready to be true after the startup sequence has been completed
-      # to potentially unblock the shutdown method which may be waiting on @ready to proceed
-      @ready.make_true
-    end
-
-    @logger.info("Pipeline started")
-    @logger.terminal("Logstash startup completed")
-
-    wait_inputs
-
-    if filters?
-      shutdown_filters
-      wait_filters
-      flush_filters_to_output!(:final => true)
-    end
-
-    shutdown_outputs
-    wait_outputs
-
-    @logger.info("Pipeline shutdown complete.")
-    @logger.terminal("Logstash shutdown completed")
-
-    # exit code
-    return 0
-  end # def run
-
-  def wait_inputs
-    @input_threads.each(&:join)
-  end
-
-  def shutdown_filters
-    @flusher_thread.kill
-    @input_to_filter.push(LogStash::SHUTDOWN)
-  end
-
-  def wait_filters
-    @filter_threads.each(&:join) if @filter_threads
-  end
-
-  def shutdown_outputs
-    # nothing, filters will do this
-    @filter_to_output.push(LogStash::SHUTDOWN)
-  end
-
-  def wait_outputs
-    # Wait for the outputs to stop
-    @output_threads.each(&:join)
-  end
-
-  def start_inputs
-    moreinputs = []
-    @inputs.each do |input|
-      if input.threadable && input.threads > 1
-        (input.threads - 1).times do |i|
-          moreinputs << input.clone
-        end
-      end
-    end
-    @inputs += moreinputs
-
-    @inputs.each do |input|
-      input.register
-      start_input(input)
-    end
-  end
-
-  def start_filters
-    @filters.each(&:register)
-    to_start = @settings["filter-workers"]
-    @filter_threads = to_start.times.collect do
-      Thread.new { filterworker }
-    end
-    actually_started = @filter_threads.select(&:alive?).size
-    msg = "Worker threads expected: #{to_start}, worker threads started: #{actually_started}"
-    if actually_started < to_start
-      @logger.warn(msg)
-    else
-      @logger.info(msg)
-    end
-    @flusher_thread = Thread.new { Stud.interval(5) { @input_to_filter.push(LogStash::FLUSH) } }
-  end
-
-  def start_outputs
-    @outputs.each(&:register)
-    @output_threads = [
-      Thread.new { outputworker }
-    ]
-  end
-
-  def start_input(plugin)
-    @input_threads << Thread.new { inputworker(plugin) }
-  end
-
-  def inputworker(plugin)
-    LogStash::Util::set_thread_name("<#{plugin.class.config_name}")
-    begin
-      plugin.run(@input_to_filter)
-    rescue => e
-      # if plugin is stopping, ignore uncatched exceptions and exit worker
-      if plugin.stop?
-        @logger.debug("Input plugin raised exception during shutdown, ignoring it.",
-                      :plugin => plugin.class.config_name, :exception => e,
-                      :backtrace => e.backtrace)
-        return
-      end
-
-      # otherwise, report error and restart
-      if @logger.debug?
-        @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
-                             :plugin => plugin.inspect, :error => e.to_s,
-                             :exception => e.class,
-                             :stacktrace => e.backtrace.join("\n")))
-      else
-        @logger.error(I18n.t("logstash.pipeline.worker-error",
-                             :plugin => plugin.inspect, :error => e))
-      end
-
-      # Assuming the failure that caused this exception is transient,
-      # let's sleep for a bit and execute #run again
-      sleep(1)
-      retry
-    ensure
-      plugin.do_close
-    end
-  end # def inputworker
-
-  def filterworker
-    LogStash::Util.set_thread_name("|worker")
-    begin
-      while true
-        event = @input_to_filter.pop
-
-        case event
-        when LogStash::Event
-          # filter_func returns all filtered events, including cancelled ones
-          filter_func(event).each { |e| @filter_to_output.push(e) unless e.cancelled? }
-        when LogStash::FlushEvent
-          # handle filter flushing here so that non threadsafe filters (thus only running one filterworker)
-          # don't have to deal with thread safety implementing the flush method
-          flush_filters_to_output!
-        when LogStash::ShutdownEvent
-          # pass it down to any other filterworker and stop this worker
-          @input_to_filter.push(event)
-          break
-        end
-      end
-    rescue Exception => e
-      # Plugins authors should manage their own exceptions in the plugin code
-      # but if an exception is raised up to the worker thread they are considered
-      # fatal and logstash will not recover from this situation.
-      #
-      # Users need to check their configuration or see if there is a bug in the
-      # plugin.
-      @logger.error("Exception in filterworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
-                    "exception" => e, "backtrace" => e.backtrace)
-      raise
-    ensure
-      @filters.each(&:do_close)
-    end
-  end # def filterworker
-
-  def outputworker
-    LogStash::Util.set_thread_name(">output")
-    @outputs.each(&:worker_setup)
-
-    while true
-      event = @filter_to_output.pop
-      break if event == LogStash::SHUTDOWN
-      output_func(event)
-    end
-  ensure
-    @outputs.each do |output|
-      output.worker_plugins.each(&:do_close)
-    end
-  end # def outputworker
-
-  # initiate the pipeline shutdown sequence
-  # this method is intended to be called from outside the pipeline thread
-  # @param before_stop [Proc] code block called before performing stop operation on input plugins
-  def shutdown(&before_stop)
-    # shutdown can only start once the pipeline has completed its startup.
-    # avoid potential race conditoon between the startup sequence and this
-    # shutdown method which can be called from another thread at any time
-    sleep(0.1) while !ready?
-
-    # TODO: should we also check against calling shutdown multiple times concurently?
-
-    before_stop.call if block_given?
-
-    @inputs.each(&:do_stop)
-  end # def shutdown
-
-  def plugin(plugin_type, name, *args)
-    args << {} if args.empty?
-    klass = LogStash::Plugin.lookup(plugin_type, name)
-    return klass.new(*args)
-  end
-
-  # for backward compatibility in devutils for the rspec helpers, this method is not used
-  # in the pipeline anymore.
-  def filter(event, &block)
-    # filter_func returns all filtered events, including cancelled ones
-    filter_func(event).each { |e| block.call(e) }
-  end
-
-  # perform filters flush and yeild flushed event to the passed block
-  # @param options [Hash]
-  # @option options [Boolean] :final => true to signal a final shutdown flush
-  def flush_filters(options = {}, &block)
-    flushers = options[:final] ? @shutdown_flushers : @periodic_flushers
-
-    flushers.each do |flusher|
-      flusher.call(options, &block)
-    end
-  end
-
-  # perform filters flush into the output queue
-  # @param options [Hash]
-  # @option options [Boolean] :final => true to signal a final shutdown flush
-  def flush_filters_to_output!(options = {})
-    flush_filters(options) do |event|
-      unless event.cancelled?
-        @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
-        @filter_to_output.push(event)
-      end
-    end
-  end # flush_filters_to_output!
-
-end # class Pipeline
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
deleted file mode 100644
index 4831d533025..00000000000
--- a/lib/logstash/runner.rb
+++ /dev/null
@@ -1,124 +0,0 @@
-# encoding: utf-8
-Thread.abort_on_exception = true
-Encoding.default_external = Encoding::UTF_8
-$DEBUGLIST = (ENV["DEBUG"] || "").split(",")
-
-require "logstash/environment"
-
-LogStash::Environment.load_locale!
-
-require "logstash/namespace"
-require "logstash/program"
-
-class LogStash::Runner
-  include LogStash::Program
-
-  def main(args)
-    require "logstash/util"
-    require "logstash/util/java_version"
-    require "stud/trap"
-    require "stud/task"
-    @startup_interruption_trap = Stud::trap("INT") { puts "Interrupted"; exit 0 }
-
-    LogStash::Util::set_thread_name(self.class.name)
-
-    if RUBY_VERSION < "1.9.2"
-      $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
-      return 1
-    end
-
-    # Print a warning to STDERR for bad java versions
-    LogStash::Util::JavaVersion.warn_on_bad_java_version
-
-    Stud::untrap("INT", @startup_interruption_trap)
-
-    task = run(args)
-    exit(task.wait)
-  end # def self.main
-
-  def run(args)
-    command = args.shift
-    commands = {
-      "version" => lambda do
-        require "logstash/agent"
-        agent_args = ["--version"]
-        if args.include?("--verbose")
-          agent_args << "--verbose"
-        end
-        return LogStash::Agent.run($0, agent_args)
-      end,
-      "irb" => lambda do
-        require "irb"
-        return IRB.start(__FILE__)
-      end,
-      "pry" => lambda do
-        require "pry"
-        return binding.pry
-      end,
-      "docgen" => lambda do
-        require 'docs/asciidocgen'
-        opts = OptionParser.new
-        settings = {}
-        opts.on("-o DIR", "--output DIR",
-          "Directory to output to; optional. If not specified,"\
-          "we write to stdout.") do |val|
-          settings[:output] = val
-        end
-        args = opts.parse(ARGV)
-        docs = LogStashConfigAsciiDocGenerator.new
-        args.each do |arg|
-          docs.generate(arg, settings)
-        end
-        return 0
-      end,
-      "agent" => lambda do
-        require "logstash/agent"
-        # Hack up a runner
-        agent = LogStash::Agent.new("/bin/logstash agent", $0)
-        begin
-          agent.parse(args)
-        rescue Clamp::HelpWanted => e
-          show_help(e.command)
-          return 0
-        rescue Clamp::UsageError => e
-          # If 'too many arguments' then give the arguments to
-          # the next command. Otherwise it's a real error.
-          raise if e.message != "too many arguments"
-          remaining = agent.remaining_arguments
-        end
-
-        return agent.execute
-      end
-    } # commands
-
-    if commands.include?(command)
-      return Stud::Task.new { commands[command].call }
-    else
-      if command.nil?
-        $stderr.puts "No command given"
-      else
-        if !%w(--help -h help).include?(command)
-          # Emit 'no such command' if it's not someone asking for help.
-          $stderr.puts "No such command #{command.inspect}"
-        end
-      end
-      $stderr.puts %q[
-Usage: logstash <command> [command args]
-Run a command with the --help flag to see the arguments.
-For example: logstash agent --help
-
-Available commands:
-  agent - runs the logstash agent
-  version - emits version info about this logstash
-]
-      #$stderr.puts commands.keys.map { |s| "  #{s}" }.join("\n")
-      return Stud::Task.new { 1 }
-    end
-  end # def run
-
-  private
-
-  def show_help(command)
-    puts command.help
-  end
-end # class LogStash::Runner
diff --git a/lib/logstash/sized_queue.rb b/lib/logstash/sized_queue.rb
deleted file mode 100644
index d96e27aba02..00000000000
--- a/lib/logstash/sized_queue.rb
+++ /dev/null
@@ -1,8 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/logging"
-
-require "thread" # for SizedQueue
-class LogStash::SizedQueue < SizedQueue
-  # TODO(sissel): Soon will implement push/pop stats, etc
-end
diff --git a/lib/logstash/util/defaults_printer.rb b/lib/logstash/util/defaults_printer.rb
deleted file mode 100644
index 13764e2414a..00000000000
--- a/lib/logstash/util/defaults_printer.rb
+++ /dev/null
@@ -1,31 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/util"
-require "logstash/util/worker_threads_default_printer"
-
-
-# This class exists to format the settings for defaults used
-module LogStash module Util class DefaultsPrinter
-  def self.print(settings)
-    new(settings).print
-  end
-
-  def initialize(settings)
-    @settings = settings
-    @printers = [workers]
-  end
-
-  def print
-    collector = []
-    @printers.each do |printer|
-      printer.visit(collector)
-    end
-    "Default settings used: " + collector.join(', ')
-  end
-
-  private
-
-  def workers
-    WorkerThreadsDefaultPrinter.new(@settings)
-  end
-end end end
diff --git a/lib/logstash/util/reporter.rb b/lib/logstash/util/reporter.rb
deleted file mode 100644
index 4d983a25e3e..00000000000
--- a/lib/logstash/util/reporter.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-# encoding: utf-8
-class InflightEventsReporter
-  def self.logger=(logger)
-    @logger = logger
-  end
-
-  def self.start(input_to_filter, filter_to_output, outputs)
-    Thread.new do
-      loop do
-        sleep 5
-        report(input_to_filter, filter_to_output, outputs)
-      end
-    end
-  end
-
-  def self.report(input_to_filter, filter_to_output, outputs)
-    report = {
-      "input_to_filter" => input_to_filter.size,
-      "filter_to_output" => filter_to_output.size,
-      "outputs" => []
-    }
-    outputs.each do |output|
-      next unless output.worker_queue && output.worker_queue.size > 0
-      report["outputs"] << [output.inspect, output.worker_queue.size]
-    end
-    @logger.warn ["INFLIGHT_EVENTS_REPORT", Time.now.iso8601, report]
-  end
-end
diff --git a/lib/logstash/util/worker_threads_default_printer.rb b/lib/logstash/util/worker_threads_default_printer.rb
deleted file mode 100644
index c8b086635cb..00000000000
--- a/lib/logstash/util/worker_threads_default_printer.rb
+++ /dev/null
@@ -1,17 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/util"
-
-# This class exists to format the settings for default worker threads
-module LogStash module Util class WorkerThreadsDefaultPrinter
-
-  def initialize(settings)
-    @setting = settings.fetch('filter-workers', 1)
-  end
-
-  def visit(collector)
-    collector.push "Filter workers: #{@setting}"
-  end
-
-end end end
-
diff --git a/lib/pluginmanager/gemfile.rb b/lib/pluginmanager/gemfile.rb
index b1648187764..68bf88334f1 100644
--- a/lib/pluginmanager/gemfile.rb
+++ b/lib/pluginmanager/gemfile.rb
@@ -41,13 +41,20 @@ def add(name, *requirements)
       @gemset.add_gem(Gem.parse(name, *requirements))
     end
 
-    # update existing or add new
+    # update existing or add new and merge passed options with current gem options if it exists
     # @param name [String] gem name
     # @param *requirements params following name use the same notation as the Gemfile gem DSL statement
     def update(name, *requirements)
       @gemset.update_gem(Gem.parse(name, *requirements))
     end
 
+    # overwrite existing or add new
+    # @param name [String] gem name
+    # @param *requirements params following name use the same notation as the Gemfile gem DSL statement
+    def overwrite(name, *requirements)
+      @gemset.overwrite_gem(Gem.parse(name, *requirements))
+    end
+
     # @return [Gem] removed gem or nil if not found
     def remove(name)
       @gemset.remove_gem(name)
@@ -99,6 +106,19 @@ def add_gem(_gem)
 
     # update existing or add new
     def update_gem(_gem)
+      if old = find_gem(_gem.name)
+        # always overwrite requirements if specified
+        old.requirements = _gem.requirements unless no_constrains?(_gem.requirements)
+        # but merge options
+        old.options = old.options.merge(_gem.options)
+      else
+        @gems << _gem
+        @gems_by_name[_gem.name.downcase] = _gem
+      end
+    end
+
+    # update existing or add new
+    def overwrite_gem(_gem)
       if old = find_gem(_gem.name)
         @gems[@gems.index(old)] = _gem
       else
@@ -119,8 +139,19 @@ def remove_gem(name)
     def copy
       Marshal.load(Marshal.dump(self))
     end
+
     private
 
+    def no_constrains?(requirements)
+      return true if requirements.nil? || requirements.empty?
+
+      # check for the dummy ">= 0" version constrain or any variations thereof
+      # which is in fact a "no constrain" constrain which we should discard
+      return true if requirements.size == 1 && requirements.first.to_s.gsub(/\s+/, "") == ">=0"
+
+      false
+    end
+
     def sources_to_s
       return "" if @sources.empty?
       @sources.map{|source| "source #{source.inspect}"}.join("\n")
diff --git a/lib/pluginmanager/generate.rb b/lib/pluginmanager/generate.rb
new file mode 100644
index 00000000000..6717682e021
--- /dev/null
+++ b/lib/pluginmanager/generate.rb
@@ -0,0 +1,92 @@
+# encoding: utf-8
+require "pluginmanager/command"
+require "pluginmanager/templates/render_context"
+require "erb"
+require "ostruct"
+require "fileutils"
+require "pathname"
+
+class LogStash::PluginManager::Generate < LogStash::PluginManager::Command
+
+  TYPES = [ "input", "filter", "output", "codec" ]
+
+  option "--type", "TYPE", "Type of the plugin {input, filter, codec, output}s", :required => true
+  option "--name", "PLUGIN", "Name of the new plugin", :required => true
+  option "--path", "PATH", "Location where the plugin skeleton will be created", :default => Dir.pwd
+
+  def execute
+    validate_params
+    source = File.join(File.dirname(__FILE__), "templates", "#{type}-plugin")
+    @target_path = File.join(path, full_plugin_name)
+    FileUtils.mkdir(@target_path)
+    puts " Creating #{@target_path}"
+
+    begin
+      create_scaffold(source, @target_path)
+    rescue Errno::EACCES => exception
+      report_exception("Permission denied when executing the plugin manager", exception)
+    rescue => exception
+      report_exception("Plugin creation Aborted", exception)
+    end
+  end
+
+  private
+
+  def validate_params
+    raise(ArgumentError, "should be one of: input, filter, codec or output") unless TYPES.include?(type)
+  end
+
+  def create_scaffold(source, target)
+    transform_r(source, target)
+  end
+
+  def transform_r(source, target)
+    Dir.entries(source).each do |entry|
+      next if [ ".", ".." ].include?(entry)
+      source_entry = File.join(source, entry)
+      target_entry = File.join(target, entry)
+
+      if File.directory?(source_entry)
+        FileUtils.mkdir(target_entry) unless File.exists?(target_entry)
+        transform_r(source_entry, target_entry)
+      else
+        # copy the new file, in case of being an .erb file should render first
+        if source_entry.end_with?("erb")
+          target_entry = target_entry.gsub(/.erb$/,"").gsub("example", name)
+          File.open(target_entry, "w") { |f| f.write(render(source_entry)) }
+        else
+          FileUtils.cp(source_entry, target_entry)
+        end
+        puts "\t create #{File.join(full_plugin_name, Pathname.new(target_entry).relative_path_from(Pathname.new(@target_path)))}"
+      end
+    end
+  end
+
+  def render(source)
+    template = File.read(source)
+    renderer = ERB.new(template)
+    context  = LogStash::PluginManager::RenderContext.new(options)
+    renderer.result(context.get_binding)
+  end
+
+  def options
+    git_data = get_git_info
+    @options ||= {
+      :plugin_name => name,
+      :author => git_data.author,
+      :email  => git_data.email,
+      :min_version => "2.0",
+    }
+  end
+
+  def get_git_info
+    git = OpenStruct.new
+    git.author = %x{ git config --get user.name  }.strip rescue "your_username"
+    git.email  = %x{ git config --get user.email }.strip rescue "your_username@example.com"
+    git
+  end
+
+  def full_plugin_name
+    @full_plugin_name ||= "logstash-#{type}-#{name.downcase}"
+  end
+end
diff --git a/lib/pluginmanager/install.rb b/lib/pluginmanager/install.rb
index bbc486ab236..0e6383d725b 100644
--- a/lib/pluginmanager/install.rb
+++ b/lib/pluginmanager/install.rb
@@ -9,7 +9,9 @@ class LogStash::PluginManager::Install < LogStash::PluginManager::Command
   parameter "[PLUGIN] ...", "plugin name(s) or file", :attribute_name => :plugins_arg
   option "--version", "VERSION", "version of the plugin to install"
   option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
+  option "--preserve", :flag, "preserve current gem options", :default => false
   option "--development", :flag, "install all development dependencies of currently installed plugins", :default => false
+  option "--local", :flag, "force local-only plugin installation. see bin/logstash-plugin package|unpack", :default => false
 
   # the install logic below support installing multiple plugins with each a version specification
   # but the argument parsing does not support it for now so currently if specifying --version only
@@ -23,7 +25,7 @@ def execute
       gems = plugins_development_gems
     else
       gems = plugins_gems
-      verify_remote!(gems) if verify?
+      verify_remote!(gems) if !local? && verify?
     end
 
     install_gems_list!(gems)
@@ -45,12 +47,20 @@ def validate_cli_options!
   # Check if the specified gems contains
   # the logstash `metadata`
   def verify_remote!(gems)
+    options = { :rubygems_source => gemfile.gemset.sources }
     gems.each do |plugin, version|
       puts("Validating #{[plugin, version].compact.join("-")}")
-      signal_error("Installation aborted, verification failed for #{plugin} #{version}") unless LogStash::PluginManager.logstash_plugin?(plugin, version)
+      next if validate_plugin(plugin, version, options)
+      signal_error("Installation aborted, verification failed for #{plugin} #{version}")
     end
   end
 
+  def validate_plugin(plugin, version, options)
+    LogStash::PluginManager.logstash_plugin?(plugin, version, options)
+  rescue SocketError
+    false
+  end
+
   def plugins_development_gems
     # Get currently defined gems and their dev dependencies
     specs = []
@@ -81,7 +91,15 @@ def install_gems_list!(install_list)
 
     # Add plugins/gems to the current gemfile
     puts("Installing" + (install_list.empty? ? "..." : " " + install_list.collect(&:first).join(", ")))
-    install_list.each { |plugin, version, options| gemfile.update(plugin, version, options) }
+    install_list.each do |plugin, version, options|
+      if preserve?
+        plugin_gem = gemfile.find(plugin)
+        puts("Preserving Gemfile gem options for plugin #{plugin}") if plugin_gem && !plugin_gem.options.empty?
+        gemfile.update(plugin, version, options)
+      else
+        gemfile.overwrite(plugin, version, options)
+      end
+    end
 
     # Sync gemfiles changes to disk to make them available to the `bundler install`'s API
     gemfile.save
@@ -89,6 +107,7 @@ def install_gems_list!(install_list)
     bundler_options = {:install => true}
     bundler_options[:without] = [] if development?
     bundler_options[:rubygems_source] = gemfile.gemset.sources
+    bundler_options[:local] = true if local?
 
     output = LogStash::Bundler.invoke!(bundler_options)
 
@@ -102,7 +121,7 @@ def install_gems_list!(install_list)
 
   # Extract the specified local gems in a predefined local path
   # Update the gemfile to use a relative path to this plugin and run
-  # Bundler, this will mark the gem not updatable by `bin/plugin update`
+  # Bundler, this will mark the gem not updatable by `bin/logstash-plugin update`
   # This is the most reliable way to make it work in bundler without
   # hacking with `how bundler works`
   #
diff --git a/lib/pluginmanager/list.rb b/lib/pluginmanager/list.rb
index b4b96cafbc3..65f28c31cc0 100644
--- a/lib/pluginmanager/list.rb
+++ b/lib/pluginmanager/list.rb
@@ -6,7 +6,7 @@ class LogStash::PluginManager::List < LogStash::PluginManager::Command
 
   parameter "[PLUGIN]", "Part of plugin name to search for, leave empty for all plugins"
 
-  option "--installed", :flag, "List only explicitly installed plugins using bin/plugin install ...", :default => false
+  option "--installed", :flag, "List only explicitly installed plugins using bin/logstash-plugin install ...", :default => false
   option "--verbose", :flag, "Also show plugin version number", :default => false
   option "--group", "NAME", "Filter plugins per group: input, output, filter or codec" do |arg|
     raise(ArgumentError, "should be one of: input, output, filter or codec") unless ['input', 'output', 'filter', 'codec'].include?(arg)
diff --git a/lib/pluginmanager/main.rb b/lib/pluginmanager/main.rb
index a2004e4d6be..0ac63b29329 100644
--- a/lib/pluginmanager/main.rb
+++ b/lib/pluginmanager/main.rb
@@ -18,6 +18,9 @@ module PluginManager
 require "pluginmanager/uninstall"
 require "pluginmanager/list"
 require "pluginmanager/update"
+require "pluginmanager/pack"
+require "pluginmanager/unpack"
+require "pluginmanager/generate"
 
 module LogStash
   module PluginManager
@@ -27,14 +30,17 @@ class Main < Clamp::Command
       subcommand "install", "Install a plugin", LogStash::PluginManager::Install
       subcommand "uninstall", "Uninstall a plugin", LogStash::PluginManager::Uninstall
       subcommand "update", "Update a plugin", LogStash::PluginManager::Update
+      subcommand "pack", "Package currently installed plugins", LogStash::PluginManager::Pack
+      subcommand "unpack", "Unpack packaged plugins", LogStash::PluginManager::Unpack
       subcommand "list", "List all installed plugins", LogStash::PluginManager::List
+      subcommand "generate", "Create the foundation for a new plugin", LogStash::PluginManager::Generate
     end
   end
 end
 
 if $0 == __FILE__
   begin
-    LogStash::PluginManager::Main.run("bin/plugin", ARGV)
+    LogStash::PluginManager::Main.run("bin/logstash-plugin", ARGV)
   rescue LogStash::PluginManager::Error => e
     $stderr.puts(e.message)
     exit(1)
diff --git a/lib/pluginmanager/pack.rb b/lib/pluginmanager/pack.rb
new file mode 100644
index 00000000000..18b46e18511
--- /dev/null
+++ b/lib/pluginmanager/pack.rb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+require_relative "pack_command"
+
+class LogStash::PluginManager::Pack < LogStash::PluginManager::PackCommand
+  option "--tgz", :flag, "compress package as a tar.gz file", :default => !LogStash::Environment.windows?
+  option "--zip", :flag, "compress package as a zip file", :default => LogStash::Environment.windows?
+  option "--[no-]clean", :flag, "clean up the generated dump of plugins", :default => true
+  option "--overwrite", :flag, "Overwrite a previously generated package file", :default => false
+
+  def execute
+    puts("Packaging plugins for offline usage")
+
+    validate_target_file
+    LogStash::Bundler.invoke!({:package => true, :all => true})
+    archive_manager.compress(LogStash::Environment::CACHE_PATH, target_file)
+    FileUtils.rm_rf(LogStash::Environment::CACHE_PATH) if clean?
+
+    puts("Generated at #{target_file}")
+  end
+
+  private
+
+  def delete_target_file?
+    return true if overwrite?
+    puts("File #{target_file} exist, do you want to overwrite it? (Y/N)")
+    ( "y" == STDIN.gets.strip.downcase ? true : false)
+  end
+
+  def validate_target_file
+    if File.exist?(target_file)
+      if  delete_target_file?
+        File.delete(target_file)
+      else
+        signal_error("Package creation cancelled, a previously generated package exist at location: #{target_file}, move this file to safe place and run the command again")
+      end
+    end
+  end
+
+  def target_file
+    target_file = File.join(LogStash::Environment::LOGSTASH_HOME, "plugins_package")
+    "#{target_file}#{file_extension}"
+  end
+end
diff --git a/lib/pluginmanager/pack_command.rb b/lib/pluginmanager/pack_command.rb
new file mode 100644
index 00000000000..2409b212f97
--- /dev/null
+++ b/lib/pluginmanager/pack_command.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "bootstrap/util/compress"
+require "fileutils"
+
+class LogStash::PluginManager::PackCommand < LogStash::PluginManager::Command
+  def archive_manager
+    zip? ? LogStash::Util::Zip : LogStash::Util::Tar
+  end
+
+  def file_extension
+    zip? ? ".zip" : ".tar.gz"
+  end
+end
diff --git a/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..654a05b6614
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-codec-<%= plugin_name %>
+Example codec plugin. This should help bootstrap your effort to write your own codec plugin!
diff --git a/lib/pluginmanager/templates/codec-plugin/Gemfile b/lib/pluginmanager/templates/codec-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/codec-plugin/LICENSE b/lib/pluginmanager/templates/codec-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/codec-plugin/README.md b/lib/pluginmanager/templates/codec-plugin/README.md
new file mode 100644
index 00000000000..a75e88df936
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-codec-awesome", :path => "/your/local/logstash-codec-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'codec {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-codec-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-codec-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/codec-plugin/Rakefile b/lib/pluginmanager/templates/codec-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
new file mode 100644
index 00000000000..b1a618562d1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
@@ -0,0 +1,44 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/namespace"
+
+# This <%= @plugin_name %> codec will append a string to the message field
+# of an event, either in the decoding or encoding methods
+#
+# This is only intended to be used as an example.
+#
+# input {
+#   stdin { codec => <%= @plugin_name %> }
+# }
+#
+# or
+#
+# output {
+#   stdout { codec => <%= @plugin_name %> }
+# }
+#
+class LogStash::Codecs::<%= classify(plugin_name) %> < LogStash::Codecs::Base
+
+  # The codec name
+  config_name "<%= plugin_name %>"
+
+  # Append a string to the message
+  config :append, :validate => :string, :default => ', Hello World!'
+
+  def register
+    @lines = LogStash::Codecs::Line.new
+    @lines.charset = "UTF-8"
+  end # def register
+
+  def decode(data)
+    @lines.decode(data) do |line|
+      replace = { "message" => line.get("message").to_s + @append }
+      yield LogStash::Event.new(replace)
+    end
+  end # def decode
+
+  def encode(event)
+    @on_event.call(event, event.get("message").to_s + @append + NL)
+  end # def encode
+
+end # class LogStash::Codecs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
new file mode 100644
index 00000000000..91e1b0600f1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-codec-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "codec" }
+
+  # Gem dependencies
+  s.add_runtime_dependency 'logstash-core-plugin-api', "~> <%= min_version %>"
+  s.add_runtime_dependency 'logstash-codec-line'
+  s.add_development_dependency 'logstash-devutils'
+end
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
new file mode 100644
index 00000000000..48cca741ab2
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
@@ -0,0 +1,3 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require "logstash/codecs/<%= plugin_name %>"
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
new file mode 100644
index 00000000000..dc64aba12c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
diff --git a/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md b/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..6b18c6221de
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-filter-<%= plugin_name %>
+Example filter plugin. This should help bootstrap your effort to write your own filter plugin!
diff --git a/lib/pluginmanager/templates/filter-plugin/Gemfile b/lib/pluginmanager/templates/filter-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/filter-plugin/LICENSE b/lib/pluginmanager/templates/filter-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/filter-plugin/README.md b/lib/pluginmanager/templates/filter-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/filter-plugin/Rakefile b/lib/pluginmanager/templates/filter-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb b/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb
new file mode 100644
index 00000000000..ca5d9f7ca3b
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# This <%= @plugin_name %> filter will replace the contents of the default 
+# message field with whatever you specify in the configuration.
+#
+# It is only intended to be used as an <%= @plugin_name %>.
+class LogStash::Filters::<%= classify(plugin_name) %> < LogStash::Filters::Base
+
+  # Setting the config_name here is required. This is how you
+  # configure this filter from your Logstash config.
+  #
+  # filter {
+  #   <%= @plugin_name %> {
+  #     message => "My message..."
+  #   }
+  # }
+  #
+  config_name "<%= plugin_name %>"
+  
+  # Replace the message with this value.
+  config :message, :validate => :string, :default => "Hello World!"
+  
+
+  public
+  def register
+    # Add instance variables 
+  end # def register
+
+  public
+  def filter(event)
+
+    if @message
+      # Replace the event message with our message as configured in the
+      # config file.
+      event.set("message", @message)
+    end
+
+    # filter_matched should go in the last line of our successful code
+    filter_matched(event)
+  end # def filter
+end # class LogStash::Filters::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb b/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb
new file mode 100644
index 00000000000..5f910dc40fb
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb
@@ -0,0 +1,23 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-filter-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_development_dependency 'logstash-devutils'
+end
diff --git a/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb b/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb
new file mode 100644
index 00000000000..5dceafe7fc4
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require "logstash/filters/<%= plugin_name %>"
+
+describe LogStash::Filters::<%= classify(plugin_name) %> do
+  describe "Set to Hello World" do
+    let(:config) do <<-CONFIG
+      filter {
+        <%= plugin_name %> {
+          message => "Hello World"
+        }
+      }
+    CONFIG
+    end
+
+    sample("message" => "some text") do
+      expect(subject).to include("message")
+      expect(subject.get('message')).to eq('Hello World')
+    end
+  end
+end
diff --git a/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb b/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb
new file mode 100644
index 00000000000..dc64aba12c1
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
diff --git a/lib/pluginmanager/templates/input-plugin/CHANGELOG.md b/lib/pluginmanager/templates/input-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..eca3db404e8
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-input-<%= plugin_name %>
+Example input plugin. This should help bootstrap your effort to write your own input plugin!
diff --git a/lib/pluginmanager/templates/input-plugin/Gemfile b/lib/pluginmanager/templates/input-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/input-plugin/LICENSE b/lib/pluginmanager/templates/input-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/input-plugin/README.md b/lib/pluginmanager/templates/input-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/input-plugin/Rakefile b/lib/pluginmanager/templates/input-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb b/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb
new file mode 100644
index 00000000000..176467ccb5c
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "stud/interval"
+require "socket" # for Socket.gethostname
+
+# Generate a repeating message.
+#
+# This plugin is intented only as an example.
+
+class LogStash::Inputs::<%= classify(plugin_name) %> < LogStash::Inputs::Base
+  config_name "<%= @plugin_name %>"
+
+  # If undefined, Logstash will complain, even if codec is unused.
+  default :codec, "plain"
+
+  # The message string to use in the event.
+  config :message, :validate => :string, :default => "Hello World!"
+
+  # Set how frequently messages should be sent.
+  #
+  # The default, `1`, means send a message every second.
+  config :interval, :validate => :number, :default => 1
+
+  public
+  def register
+    @host = Socket.gethostname
+  end # def register
+
+  def run(queue)
+    # we can abort the loop if stop? becomes true
+    while !stop?
+      event = LogStash::Event.new("message" => @message, "host" => @host)
+      decorate(event)
+      queue << event
+      # because the sleep interval can be big, when shutdown happens
+      # we want to be able to abort the sleep
+      # Stud.stoppable_sleep will frequently evaluate the given block
+      # and abort the sleep(@interval) if the return value is true
+      Stud.stoppable_sleep(@interval) { stop? }
+    end # loop
+  end # def run
+
+  def stop
+    # nothing to do in this case so it is not necessary to define stop
+    # examples of common "stop" tasks:
+    #  * close sockets (unblocking blocking reads/accepts)
+    #  * cleanup temporary files
+    #  * terminate spawned threads
+  end
+end # class LogStash::Inputs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb b/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb
new file mode 100644
index 00000000000..9f8543887b2
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb
@@ -0,0 +1,25 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-input-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = '{TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "input" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_runtime_dependency 'logstash-codec-plain'
+  s.add_runtime_dependency 'stud', '>= 0.0.22'
+  s.add_development_dependency 'logstash-devutils', '>= 0.0.16'
+end
diff --git a/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb b/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb
new file mode 100644
index 00000000000..7b8bfde8ea3
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb
@@ -0,0 +1,11 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/inputs/<%= plugin_name %>"
+
+describe LogStash::Inputs::<%= classify(plugin_name) %> do
+
+  it_behaves_like "an interruptible input plugin" do
+    let(:config) { { "interval" => 100 } }
+  end
+
+end
diff --git a/lib/pluginmanager/templates/output-plugin/CHANGELOG.md b/lib/pluginmanager/templates/output-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..2593de38fc7
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-output-<%= plugin_name %>
+Example output plugin. This should help bootstrap your effort to write your own output plugin!
diff --git a/lib/pluginmanager/templates/output-plugin/Gemfile b/lib/pluginmanager/templates/output-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/output-plugin/LICENSE b/lib/pluginmanager/templates/output-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/output-plugin/README.md b/lib/pluginmanager/templates/output-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/output-plugin/Rakefile b/lib/pluginmanager/templates/output-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb b/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb
new file mode 100644
index 00000000000..eadd499bf98
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# An <%= plugin_name %> output that does nothing.
+class LogStash::Outputs::<%= classify(plugin_name) %> < LogStash::Outputs::Base
+  config_name "<%= plugin_name %>"
+
+  public
+  def register
+  end # def register
+
+  public
+  def receive(event)
+    return "Event received"
+  end # def event
+end # class LogStash::Outputs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb b/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb
new file mode 100644
index 00000000000..db396e1ff1c
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-output-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "output" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_runtime_dependency "logstash-codec-plain"
+  s.add_development_dependency "logstash-devutils"
+end
diff --git a/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb b/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb
new file mode 100644
index 00000000000..220d967bd63
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/outputs/<%= plugin_name %>"
+require "logstash/codecs/plain"
+require "logstash/event"
+
+describe LogStash::Outputs::<%= classify(plugin_name) %> do
+  let(:sample_event) { LogStash::Event.new }
+  let(:output) { LogStash::Outputs::<%= classify(plugin_name) %>.new }
+
+  before do
+    output.register
+  end
+
+  describe "receive message" do
+    subject { output.receive(sample_event) }
+
+    it "returns a string" do
+      expect(subject).to eq("Event received")
+    end
+  end
+end
diff --git a/lib/pluginmanager/templates/render_context.rb b/lib/pluginmanager/templates/render_context.rb
new file mode 100644
index 00000000000..583c6a9ee07
--- /dev/null
+++ b/lib/pluginmanager/templates/render_context.rb
@@ -0,0 +1,20 @@
+require "erb"
+
+module LogStash::PluginManager
+  class RenderContext
+    def initialize(options = {})
+      options.each do |name, value|
+        define_singleton_method(name) { value }
+      end
+    end
+
+    def get_binding
+      binding()
+    end
+
+    def classify(klass_name)
+      klass_name.split(/-|_/).map { |e| e.capitalize }.join("")
+    end
+
+  end
+end
diff --git a/lib/pluginmanager/unpack.rb b/lib/pluginmanager/unpack.rb
new file mode 100644
index 00000000000..7937e7d2e24
--- /dev/null
+++ b/lib/pluginmanager/unpack.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "pack_command"
+
+class LogStash::PluginManager::Unpack < LogStash::PluginManager::PackCommand
+  option "--tgz", :flag, "unpack a packaged tar.gz file", :default => !LogStash::Environment.windows?
+  option "--zip", :flag, "unpack a packaged  zip file", :default => LogStash::Environment.windows?
+
+  parameter "file", "the package file name", :attribute_name => :package_file, :required => true
+
+  def execute
+    puts("Unpacking #{package_file}")
+
+    FileUtils.rm_rf(LogStash::Environment::CACHE_PATH)
+    validate_cache_location
+    archive_manager.extract(package_file, LogStash::Environment::CACHE_PATH)
+    puts("Unpacked at #{LogStash::Environment::CACHE_PATH}")
+    puts("The unpacked plugins can now be installed in local-only mode using bin/logstash-plugin install --local [plugin name]")
+  end
+
+  private
+
+  def validate_cache_location
+    cache_location = LogStash::Environment::CACHE_PATH
+    if File.exist?(cache_location)
+      puts("Directory #{cache_location} is going to be overwritten, do you want to continue? (Y/N)")
+      override = ( "y" == STDIN.gets.strip.downcase ? true : false)
+      if override
+        FileUtils.rm_rf(cache_location)
+      else
+        puts("Unpack cancelled: file #{cache_location} already exists, please delete or move it")
+        exit
+      end
+    end
+  end
+end
diff --git a/lib/pluginmanager/update.rb b/lib/pluginmanager/update.rb
index 3fc8b6b12a7..fd840e91183 100644
--- a/lib/pluginmanager/update.rb
+++ b/lib/pluginmanager/update.rb
@@ -8,6 +8,8 @@ class LogStash::PluginManager::Update < LogStash::PluginManager::Command
   REJECTED_OPTIONS = [:path, :git, :github]
 
   parameter "[PLUGIN] ...", "Plugin name(s) to upgrade to latest version", :attribute_name => :plugins_arg
+  option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
+  option "--local", :flag, "force local-only plugin update. see bin/logstash-plugin package|unpack", :default => false
 
   def execute
     local_gems = gemfile.locally_installed_gems
@@ -21,7 +23,6 @@ def execute
 
       warn_local_gems(plugins_with_path)
     end
-
     update_gems!
   end
 
@@ -41,10 +42,12 @@ def update_gems!
     # remove any version constrain from the Gemfile so the plugin(s) can be updated to latest version
     # calling update without requiremend will remove any previous requirements
     plugins = plugins_to_update(previous_gem_specs_map)
+    # Skipping the major version validation when using a local cache as we can have situations
+    # without internet connection.
     filtered_plugins = plugins.map { |plugin| gemfile.find(plugin) }
       .compact
       .reject { |plugin| REJECTED_OPTIONS.any? { |key| plugin.options.has_key?(key) } }
-      .select { |plugin| validates_version(plugin.name) }
+      .select { |plugin| local? || (verify? ? validates_version(plugin.name) : true) }
       .each   { |plugin| gemfile.update(plugin.name) }
 
     # force a disk sync before running bundler
@@ -54,9 +57,10 @@ def update_gems!
 
     # any errors will be logged to $stderr by invoke!
     # Bundler cannot update and clean gems in one operation so we have to call the CLI twice.
-    output = LogStash::Bundler.invoke!(:update => plugins)
+    options = {:update => plugins, :rubygems_source => gemfile.gemset.sources}
+    options[:local] = true if local?
+    output = LogStash::Bundler.invoke!(options)
     output = LogStash::Bundler.invoke!(:clean => true)
-
     display_updated_plugins(previous_gem_specs_map)
   rescue => exception
     gemfile.restore!
diff --git a/lib/pluginmanager/util.rb b/lib/pluginmanager/util.rb
index 78bb7d38926..149ff6256d4 100644
--- a/lib/pluginmanager/util.rb
+++ b/lib/pluginmanager/util.rb
@@ -2,12 +2,18 @@
 require "rubygems/package"
 
 module LogStash::PluginManager
+
+  class ValidationError < StandardError; end
+
   # check for valid logstash plugin gem name & version or .gem file, logs errors to $stdout
   # uses Rubygems API and will remotely validated agains the current Gem.sources
   # @param plugin [String] plugin name or .gem file path
   # @param version [String] gem version requirement string
+  # @param [Hash] options the options used to setup external components
+  # @option options [Array<String>] :rubygems_source Gem sources to lookup for the verification
   # @return [Boolean] true if valid logstash plugin gem name & version or a .gem file
-  def self.logstash_plugin?(plugin, version = nil)
+  def self.logstash_plugin?(plugin, version = nil, options={})
+
     if plugin_file?(plugin)
       begin
         return logstash_plugin_gem_spec?(plugin_file_spec(plugin))
@@ -18,6 +24,7 @@ def self.logstash_plugin?(plugin, version = nil)
       end
     else
       dep = Gem::Dependency.new(plugin, version || Gem::Requirement.default)
+      Gem.sources = Gem::SourceList.from(options[:rubygems_source]) if options[:rubygems_source]
       specs, errors = Gem::SpecFetcher.fetcher.spec_for_dependency(dep)
 
       # dump errors
@@ -46,6 +53,7 @@ def self.fetch_latest_version_info(plugin, options={})
     require "gems"
     exclude_prereleases =  options.fetch(:pre, false)
     versions = Gems.versions(plugin)
+    raise ValidationError.new("Something went wrong with the validation. You can skip the validation with the --no-verify option") if !versions.is_a?(Array) || versions.empty?
     versions = versions.select { |version| !version["prerelease"] } if !exclude_prereleases
     versions.first
   end
diff --git a/lib/systeminstall/pleasewrap.rb b/lib/systeminstall/pleasewrap.rb
new file mode 100755
index 00000000000..f7ee00bd447
--- /dev/null
+++ b/lib/systeminstall/pleasewrap.rb
@@ -0,0 +1,12 @@
+# encoding: utf-8
+$LOAD_PATH.unshift(File.expand_path(File.join(__FILE__, "..", "..")))
+
+require "bootstrap/environment"
+
+ENV["GEM_HOME"] = ENV["GEM_PATH"] = LogStash::Environment.logstash_gem_home
+Gem.use_paths(LogStash::Environment.logstash_gem_home)
+
+#libdir = File.expand_path("../lib", File.dirname(__FILE__))
+#$LOAD_PATH << libdir if File.exist?(File.join(libdir, "pleaserun", "cli.rb"))
+require "pleaserun/cli"
+exit(PleaseRun::CLI.run || 0)
diff --git a/logstash-core-event-java/.gitignore b/logstash-core-event-java/.gitignore
new file mode 100644
index 00000000000..a453cb95034
--- /dev/null
+++ b/logstash-core-event-java/.gitignore
@@ -0,0 +1,9 @@
+*.class
+
+# build dirs
+build
+.gradle
+
+# Intellij
+.idea
+*.iml
diff --git a/logstash-core-event-java/README.md b/logstash-core-event-java/README.md
new file mode 100644
index 00000000000..7b12d19f135
--- /dev/null
+++ b/logstash-core-event-java/README.md
@@ -0,0 +1,63 @@
+# logstash-core-event-java
+
+## dev install
+
+1- build code with
+
+```
+$ cd logstash-core-event-java
+$ gradle build
+```
+
+A bunch of warning are expected, it should end with:
+
+```
+BUILD SUCCESSFUL
+```
+
+2- update root logstash `Gemfile` to use this gem with:
+
+```
+# gem "logstash-core-event", "x.y.z", :path => "./logstash-core-event"
+gem "logstash-core-event-java", "x.y.z", :path => "./logstash-core-event-java"
+```
+
+3- update `logstash-core/logstash-core.gemspec` with:
+
+```
+# gem.add_runtime_dependency "logstash-core-event", "x.y.z"
+gem.add_runtime_dependency "logstash-core-event-java", "x.y.z"
+```
+
+4- and install:
+
+```
+$ bin/bundle
+```
+
+- install core plugins for tests
+
+```
+$ rake test:install-core
+```
+
+## specs
+
+```
+$ bin/rspec spec
+$ bin/rspec logstash-core/spec
+$ bin/rspec logstash-core-event/spec
+$ bin/rspec logstash-core-event-java/spec
+```
+
+or
+
+```
+$ rake test:core
+```
+
+also
+
+```
+$ rake test:plugins
+```
\ No newline at end of file
diff --git a/logstash-core-event-java/build.gradle b/logstash-core-event-java/build.gradle
new file mode 100644
index 00000000000..b2a4a55ec43
--- /dev/null
+++ b/logstash-core-event-java/build.gradle
@@ -0,0 +1,107 @@
+buildscript {
+    repositories {
+        mavenLocal()
+        mavenCentral()
+        jcenter()
+    }
+    dependencies {
+        classpath 'net.saliman:gradle-cobertura-plugin:2.2.8'
+    }
+}
+
+repositories {
+    mavenLocal()
+    mavenCentral()
+    jcenter()
+}
+
+gradle.projectsEvaluated {
+    tasks.withType(JavaCompile) {
+        options.compilerArgs << "-Xlint:deprecation"
+//        options.compilerArgs << "-Xlint:unchecked" << "-Xlint:deprecation"
+    }
+}
+
+apply plugin: 'java'
+apply plugin: 'idea'
+
+group = 'org.logstash'
+
+project.sourceCompatibility = 1.7
+
+task sourcesJar(type: Jar, dependsOn: classes) {
+    from sourceSets.main.allSource
+    classifier 'sources'
+    extension 'jar'
+}
+
+task javadocJar(type: Jar, dependsOn: javadoc) {
+    from javadoc.destinationDir
+    classifier 'javadoc'
+    extension 'jar'
+}
+
+task copyGemjar(type: Copy, dependsOn: sourcesJar) {
+    from project.jar
+    into project.file('lib/logstash-core-event-java/')
+}
+
+task cleanGemjar {
+    delete fileTree(project.file('lib/logstash-core-event-java/')) {
+        include '*.jar'
+    }
+}
+
+clean.dependsOn(cleanGemjar)
+jar.finalizedBy(copyGemjar)
+
+configurations.create('sources')
+configurations.create('javadoc')
+configurations.archives {
+    extendsFrom configurations.sources
+    extendsFrom configurations.javadoc
+}
+
+artifacts {
+    sources(sourcesJar) {
+        // Weird Gradle quirk where type will be used for the extension, but only for sources
+        type 'jar'
+    }
+
+    javadoc(javadocJar) {
+        type 'javadoc'
+    }
+}
+
+configurations {
+    provided
+}
+
+project.sourceSets {
+    main.compileClasspath += project.configurations.provided
+    main.runtimeClasspath += project.configurations.provided
+    test.compileClasspath += project.configurations.provided
+    test.runtimeClasspath += project.configurations.provided
+}
+project.javadoc.classpath += project.configurations.provided
+
+idea {
+    module {
+        scopes.PROVIDED.plus += [project.configurations.provided]
+    }
+}
+
+dependencies {
+    compile 'com.fasterxml.jackson.core:jackson-core:2.7.1'
+    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.1-1'
+    provided 'org.jruby:jruby-core:1.7.22'
+    testCompile 'junit:junit:4.12'
+    testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
+}
+
+// See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
+task wrapper(type: Wrapper) {
+    description = 'Install Gradle wrapper'
+    gradleVersion = '2.8'
+}
+
diff --git a/logstash-core-event-java/gradle.properties b/logstash-core-event-java/gradle.properties
new file mode 100644
index 00000000000..b5cdaba6a69
--- /dev/null
+++ b/logstash-core-event-java/gradle.properties
@@ -0,0 +1 @@
+VERSION=0.0.1-SNAPSHOT
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar
new file mode 100644
index 00000000000..13372aef5e2
Binary files /dev/null and b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar differ
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
new file mode 100644
index 00000000000..25611753f15
--- /dev/null
+++ b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
@@ -0,0 +1,6 @@
+#Fri Jan 22 14:29:02 EST 2016
+distributionBase=GRADLE_USER_HOME
+distributionPath=wrapper/dists
+zipStoreBase=GRADLE_USER_HOME
+zipStorePath=wrapper/dists
+distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-bin.zip
diff --git a/logstash-core-event-java/gradlew b/logstash-core-event-java/gradlew
new file mode 100755
index 00000000000..9d82f789151
--- /dev/null
+++ b/logstash-core-event-java/gradlew
@@ -0,0 +1,160 @@
+#!/usr/bin/env bash
+
+##############################################################################
+##
+##  Gradle start up script for UN*X
+##
+##############################################################################
+
+# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
+DEFAULT_JVM_OPTS=""
+
+APP_NAME="Gradle"
+APP_BASE_NAME=`basename "$0"`
+
+# Use the maximum available, or set MAX_FD != -1 to use that value.
+MAX_FD="maximum"
+
+warn ( ) {
+    echo "$*"
+}
+
+die ( ) {
+    echo
+    echo "$*"
+    echo
+    exit 1
+}
+
+# OS specific support (must be 'true' or 'false').
+cygwin=false
+msys=false
+darwin=false
+case "`uname`" in
+  CYGWIN* )
+    cygwin=true
+    ;;
+  Darwin* )
+    darwin=true
+    ;;
+  MINGW* )
+    msys=true
+    ;;
+esac
+
+# Attempt to set APP_HOME
+# Resolve links: $0 may be a link
+PRG="$0"
+# Need this for relative symlinks.
+while [ -h "$PRG" ] ; do
+    ls=`ls -ld "$PRG"`
+    link=`expr "$ls" : '.*-> \(.*\)$'`
+    if expr "$link" : '/.*' > /dev/null; then
+        PRG="$link"
+    else
+        PRG=`dirname "$PRG"`"/$link"
+    fi
+done
+SAVED="`pwd`"
+cd "`dirname \"$PRG\"`/" >/dev/null
+APP_HOME="`pwd -P`"
+cd "$SAVED" >/dev/null
+
+CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar
+
+# Determine the Java command to use to start the JVM.
+if [ -n "$JAVA_HOME" ] ; then
+    if [ -x "$JAVA_HOME/jre/sh/java" ] ; then
+        # IBM's JDK on AIX uses strange locations for the executables
+        JAVACMD="$JAVA_HOME/jre/sh/java"
+    else
+        JAVACMD="$JAVA_HOME/bin/java"
+    fi
+    if [ ! -x "$JAVACMD" ] ; then
+        die "ERROR: JAVA_HOME is set to an invalid directory: $JAVA_HOME
+
+Please set the JAVA_HOME variable in your environment to match the
+location of your Java installation."
+    fi
+else
+    JAVACMD="java"
+    which java >/dev/null 2>&1 || die "ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
+
+Please set the JAVA_HOME variable in your environment to match the
+location of your Java installation."
+fi
+
+# Increase the maximum file descriptors if we can.
+if [ "$cygwin" = "false" -a "$darwin" = "false" ] ; then
+    MAX_FD_LIMIT=`ulimit -H -n`
+    if [ $? -eq 0 ] ; then
+        if [ "$MAX_FD" = "maximum" -o "$MAX_FD" = "max" ] ; then
+            MAX_FD="$MAX_FD_LIMIT"
+        fi
+        ulimit -n $MAX_FD
+        if [ $? -ne 0 ] ; then
+            warn "Could not set maximum file descriptor limit: $MAX_FD"
+        fi
+    else
+        warn "Could not query maximum file descriptor limit: $MAX_FD_LIMIT"
+    fi
+fi
+
+# For Darwin, add options to specify how the application appears in the dock
+if $darwin; then
+    GRADLE_OPTS="$GRADLE_OPTS \"-Xdock:name=$APP_NAME\" \"-Xdock:icon=$APP_HOME/media/gradle.icns\""
+fi
+
+# For Cygwin, switch paths to Windows format before running java
+if $cygwin ; then
+    APP_HOME=`cygpath --path --mixed "$APP_HOME"`
+    CLASSPATH=`cygpath --path --mixed "$CLASSPATH"`
+    JAVACMD=`cygpath --unix "$JAVACMD"`
+
+    # We build the pattern for arguments to be converted via cygpath
+    ROOTDIRSRAW=`find -L / -maxdepth 1 -mindepth 1 -type d 2>/dev/null`
+    SEP=""
+    for dir in $ROOTDIRSRAW ; do
+        ROOTDIRS="$ROOTDIRS$SEP$dir"
+        SEP="|"
+    done
+    OURCYGPATTERN="(^($ROOTDIRS))"
+    # Add a user-defined pattern to the cygpath arguments
+    if [ "$GRADLE_CYGPATTERN" != "" ] ; then
+        OURCYGPATTERN="$OURCYGPATTERN|($GRADLE_CYGPATTERN)"
+    fi
+    # Now convert the arguments - kludge to limit ourselves to /bin/sh
+    i=0
+    for arg in "$@" ; do
+        CHECK=`echo "$arg"|egrep -c "$OURCYGPATTERN" -`
+        CHECK2=`echo "$arg"|egrep -c "^-"`                                 ### Determine if an option
+
+        if [ $CHECK -ne 0 ] && [ $CHECK2 -eq 0 ] ; then                    ### Added a condition
+            eval `echo args$i`=`cygpath --path --ignore --mixed "$arg"`
+        else
+            eval `echo args$i`="\"$arg\""
+        fi
+        i=$((i+1))
+    done
+    case $i in
+        (0) set -- ;;
+        (1) set -- "$args0" ;;
+        (2) set -- "$args0" "$args1" ;;
+        (3) set -- "$args0" "$args1" "$args2" ;;
+        (4) set -- "$args0" "$args1" "$args2" "$args3" ;;
+        (5) set -- "$args0" "$args1" "$args2" "$args3" "$args4" ;;
+        (6) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" ;;
+        (7) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" ;;
+        (8) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" ;;
+        (9) set -- "$args0" "$args1" "$args2" "$args3" "$args4" "$args5" "$args6" "$args7" "$args8" ;;
+    esac
+fi
+
+# Split up the JVM_OPTS And GRADLE_OPTS values into an array, following the shell quoting and substitution rules
+function splitJvmOpts() {
+    JVM_OPTS=("$@")
+}
+eval splitJvmOpts $DEFAULT_JVM_OPTS $JAVA_OPTS $GRADLE_OPTS
+JVM_OPTS[${#JVM_OPTS[*]}]="-Dorg.gradle.appname=$APP_BASE_NAME"
+
+exec "$JAVACMD" "${JVM_OPTS[@]}" -classpath "$CLASSPATH" org.gradle.wrapper.GradleWrapperMain "$@"
diff --git a/logstash-core-event-java/gradlew.bat b/logstash-core-event-java/gradlew.bat
new file mode 100644
index 00000000000..aec99730b4e
--- /dev/null
+++ b/logstash-core-event-java/gradlew.bat
@@ -0,0 +1,90 @@
+@if "%DEBUG%" == "" @echo off
+@rem ##########################################################################
+@rem
+@rem  Gradle startup script for Windows
+@rem
+@rem ##########################################################################
+
+@rem Set local scope for the variables with windows NT shell
+if "%OS%"=="Windows_NT" setlocal
+
+@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
+set DEFAULT_JVM_OPTS=
+
+set DIRNAME=%~dp0
+if "%DIRNAME%" == "" set DIRNAME=.
+set APP_BASE_NAME=%~n0
+set APP_HOME=%DIRNAME%
+
+@rem Find java.exe
+if defined JAVA_HOME goto findJavaFromJavaHome
+
+set JAVA_EXE=java.exe
+%JAVA_EXE% -version >NUL 2>&1
+if "%ERRORLEVEL%" == "0" goto init
+
+echo.
+echo ERROR: JAVA_HOME is not set and no 'java' command could be found in your PATH.
+echo.
+echo Please set the JAVA_HOME variable in your environment to match the
+echo location of your Java installation.
+
+goto fail
+
+:findJavaFromJavaHome
+set JAVA_HOME=%JAVA_HOME:"=%
+set JAVA_EXE=%JAVA_HOME%/bin/java.exe
+
+if exist "%JAVA_EXE%" goto init
+
+echo.
+echo ERROR: JAVA_HOME is set to an invalid directory: %JAVA_HOME%
+echo.
+echo Please set the JAVA_HOME variable in your environment to match the
+echo location of your Java installation.
+
+goto fail
+
+:init
+@rem Get command-line arguments, handling Windowz variants
+
+if not "%OS%" == "Windows_NT" goto win9xME_args
+if "%@eval[2+2]" == "4" goto 4NT_args
+
+:win9xME_args
+@rem Slurp the command line arguments.
+set CMD_LINE_ARGS=
+set _SKIP=2
+
+:win9xME_args_slurp
+if "x%~1" == "x" goto execute
+
+set CMD_LINE_ARGS=%*
+goto execute
+
+:4NT_args
+@rem Get arguments from the 4NT Shell from JP Software
+set CMD_LINE_ARGS=%$
+
+:execute
+@rem Setup the command line
+
+set CLASSPATH=%APP_HOME%\gradle\wrapper\gradle-wrapper.jar
+
+@rem Execute Gradle
+"%JAVA_EXE%" %DEFAULT_JVM_OPTS% %JAVA_OPTS% %GRADLE_OPTS% "-Dorg.gradle.appname=%APP_BASE_NAME%" -classpath "%CLASSPATH%" org.gradle.wrapper.GradleWrapperMain %CMD_LINE_ARGS%
+
+:end
+@rem End local scope for the variables with windows NT shell
+if "%ERRORLEVEL%"=="0" goto mainEnd
+
+:fail
+rem Set variable GRADLE_EXIT_CONSOLE if you need the _script_ return code instead of
+rem the _cmd.exe /c_ return code!
+if  not "" == "%GRADLE_EXIT_CONSOLE%" exit 1
+exit /b 1
+
+:mainEnd
+if "%OS%"=="Windows_NT" endlocal
+
+:omega
diff --git a/logstash-core-event-java/lib/logstash-core-event-java.rb b/logstash-core-event-java/lib/logstash-core-event-java.rb
new file mode 100644
index 00000000000..29b487aa192
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash-core-event-java.rb
@@ -0,0 +1 @@
+require "logstash-core-event-java/logstash-core-event-java"
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb b/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb
new file mode 100644
index 00000000000..cf86fec4d16
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+
+require "java"
+
+module LogStash
+end
+
+require "logstash-core-event-java_jars"
+
+# local dev setup
+classes_dir = File.expand_path("../../../build/classes/main", __FILE__)
+
+if File.directory?(classes_dir)
+  # if in local dev setup, add target to classpath
+  $CLASSPATH << classes_dir unless $CLASSPATH.include?(classes_dir)
+else
+  # otherwise use included jar
+  begin
+    require "logstash-core-event-java/logstash-core-event-java.jar"
+  rescue Exception => e
+    raise("Error loading logstash-core-event-java/logstash-core-event-java.jar file, cause: #{e.message}")
+  end
+end
+
+require "jruby_event_ext"
+require "jruby_timestamp_ext"
+require "logstash/event"
+require "logstash/timestamp"
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash-core-event-java/version.rb b/logstash-core-event-java/lib/logstash-core-event-java/version.rb
new file mode 100644
index 00000000000..cfc6a162494
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash-core-event-java/version.rb
@@ -0,0 +1,8 @@
+# encoding: utf-8
+
+# The version of logstash core event java gem.
+#
+# Note to authors: this should not include dashes because 'gem' barfs if
+# you include a dash in the version string.
+
+LOGSTASH_CORE_EVENT_JAVA_VERSION = "5.0.0.dev"
diff --git a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
new file mode 100644
index 00000000000..143d7a3e068
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
@@ -0,0 +1,6 @@
+# this is a generated file, to avoid over-writing it just delete this comment
+require 'jar_dependencies'
+
+require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.1' )
+require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
+require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.1-1' )
diff --git a/logstash-core-event-java/lib/logstash-core-event.rb b/logstash-core-event-java/lib/logstash-core-event.rb
new file mode 100644
index 00000000000..29b487aa192
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash-core-event.rb
@@ -0,0 +1 @@
+require "logstash-core-event-java/logstash-core-event-java"
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash/event.rb b/logstash-core-event-java/lib/logstash/event.rb
new file mode 100644
index 00000000000..8f6a1908901
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash/event.rb
@@ -0,0 +1,26 @@
+# encoding: utf-8
+
+require "logstash/namespace"
+require "logstash/json"
+require "logstash/string_interpolation"
+require "cabin"
+
+# transcient pipeline events for normal in-flow signaling as opposed to
+# flow altering exceptions. for now having base classes is adequate and
+# in the future it might be necessary to refactor using like a BaseEvent
+# class to have a common interface for all pileline events to support
+# eventual queueing persistence for example, TBD.
+class LogStash::ShutdownEvent; end
+class LogStash::FlushEvent; end
+
+module LogStash
+  FLUSH = LogStash::FlushEvent.new
+
+  # LogStash::SHUTDOWN is used by plugins
+  SHUTDOWN = LogStash::ShutdownEvent.new
+end
+
+# for backward compatibility, require "logstash/event" is used a lots of places so let's bootstrap the
+# Java code loading from here.
+# TODO: (colin) I think we should mass replace require "logstash/event" with require "logstash-core-event"
+require "logstash-core-event"
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash/string_interpolation.rb b/logstash-core-event-java/lib/logstash/string_interpolation.rb
new file mode 100644
index 00000000000..7baf091f304
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash/string_interpolation.rb
@@ -0,0 +1,18 @@
+# encoding: utf-8
+
+module LogStash
+  module StringInterpolation
+    extend self
+
+    # clear the global compiled templates cache
+    def clear_cache
+      Java::ComLogstash::StringInterpolation.get_instance.clear_cache;
+    end
+
+    # @return [Fixnum] the compiled templates cache size
+    def cache_size
+      Java::ComLogstash::StringInterpolation.get_instance.cache_size;
+    end
+  end
+end
+
diff --git a/logstash-core-event-java/lib/logstash/timestamp.rb b/logstash-core-event-java/lib/logstash/timestamp.rb
new file mode 100644
index 00000000000..0a4661a2d19
--- /dev/null
+++ b/logstash-core-event-java/lib/logstash/timestamp.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+
+require "logstash/namespace"
+require "logstash-core-event"
+
+module LogStash
+  class TimestampParserError < StandardError; end
+
+  class Timestamp
+    include Comparable
+
+    # TODO (colin) implement in Java
+    def <=>(other)
+      self.time <=> other.time
+    end
+
+    # TODO (colin) implement in Java
+    def +(other)
+      self.time + other
+    end
+
+    # TODO (colin) implement in Java
+    def -(value)
+      self.time - (value.is_a?(Timestamp) ? value.time : value)
+    end
+
+  end
+end
diff --git a/logstash-core-event-java/logstash-core-event-java.gemspec b/logstash-core-event-java/logstash-core-event-java.gemspec
new file mode 100644
index 00000000000..ef8d2a2bad5
--- /dev/null
+++ b/logstash-core-event-java/logstash-core-event-java.gemspec
@@ -0,0 +1,31 @@
+# -*- encoding: utf-8 -*-
+lib = File.expand_path('../lib', __FILE__)
+$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
+require 'logstash-core-event-java/version'
+
+Gem::Specification.new do |gem|
+  gem.authors       = ["Elastic"]
+  gem.email         = ["info@elastic.co"]
+  gem.description   = %q{The core event component of logstash, the scalable log and event management tool}
+  gem.summary       = %q{logstash-core-event-java - The core event component of logstash}
+  gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
+  gem.license       = "Apache License (2.0)"
+
+  gem.files         = Dir.glob(["logstash-core-event-java.gemspec", "lib/**/*.jar", "lib/**/*.rb", "spec/**/*.rb"])
+  gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
+  gem.name          = "logstash-core-event-java"
+  gem.require_paths = ["lib"]
+  gem.version       = LOGSTASH_CORE_EVENT_JAVA_VERSION
+
+  gem.platform = "java"
+
+  gem.add_runtime_dependency "jar-dependencies"
+
+  # as of Feb 3rd 2016, the ruby-maven gem is resolved to version 3.3.3 and that version
+  # has an rdoc problem that causes a bundler exception. 3.3.9 is the current latest version
+  # which does not have this problem.
+  gem.add_runtime_dependency "ruby-maven", "~> 3.3.9"
+
+  gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.1"
+  gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.1-1"
+end
diff --git a/logstash-core-event-java/settings.gradle b/logstash-core-event-java/settings.gradle
new file mode 100644
index 00000000000..3885bfa1686
--- /dev/null
+++ b/logstash-core-event-java/settings.gradle
@@ -0,0 +1,2 @@
+rootProject.name = 'logstash-core-event-java'
+
diff --git a/logstash-core-event-java/spec/event_spec.rb b/logstash-core-event-java/spec/event_spec.rb
new file mode 100644
index 00000000000..a70c2307aef
--- /dev/null
+++ b/logstash-core-event-java/spec/event_spec.rb
@@ -0,0 +1,310 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/util"
+require "logstash/event"
+require "json"
+require "java"
+
+TIMESTAMP = "@timestamp"
+
+describe LogStash::Event do
+  context "to_json" do
+    it "should serialize simple values" do
+      e = LogStash::Event.new({"foo" => "bar", "bar" => 1, "baz" => 1.0, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":\"bar\",\"bar\":1,\"baz\":1.0,\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
+    end
+
+    it "should serialize deep hash values" do
+      e = LogStash::Event.new({"foo" => {"bar" => 1, "baz" => 1.0, "biz" => "boz"}, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":{\"bar\":1,\"baz\":1.0,\"biz\":\"boz\"},\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
+    end
+
+    it "should serialize deep array values" do
+      e = LogStash::Event.new({"foo" => ["bar", 1, 1.0], TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":[\"bar\",1,1.0],\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
+    end
+
+    it "should serialize deep hash from field reference assignments" do
+      e = LogStash::Event.new({TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      e.set("foo", "bar")
+      e.set("bar", 1)
+      e.set("baz", 1.0)
+      e.set("[fancy][pants][socks]", "shoes")
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\",\"foo\":\"bar\",\"bar\":1,\"baz\":1.0,\"fancy\":{\"pants\":{\"socks\":\"shoes\"}}}"))
+    end
+  end
+
+  context "[]" do
+    it "should get simple values" do
+      e = LogStash::Event.new({"foo" => "bar", "bar" => 1, "baz" => 1.0, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(e.get("foo")).to eq("bar")
+      expect(e.get("[foo]")).to eq("bar")
+      expect(e.get("bar")).to eq(1)
+      expect(e.get("[bar]")).to eq(1)
+      expect(e.get("baz")).to eq(1.0)
+      expect(e.get("[baz]")).to eq(1.0)
+      expect(e.get(TIMESTAMP).to_s).to eq("2015-05-28T23:02:05.350Z")
+      expect(e.get("[#{TIMESTAMP}]").to_s).to eq("2015-05-28T23:02:05.350Z")
+    end
+
+    it "should get deep hash values" do
+      e = LogStash::Event.new({"foo" => {"bar" => 1, "baz" => 1.0}})
+      expect(e.get("[foo][bar]")).to eq(1)
+      expect(e.get("[foo][baz]")).to eq(1.0)
+    end
+
+    it "should get deep array values" do
+      e = LogStash::Event.new({"foo" => ["bar", 1, 1.0]})
+      expect(e.get("[foo][0]")).to eq("bar")
+      expect(e.get("[foo][1]")).to eq(1)
+      expect(e.get("[foo][2]")).to eq(1.0)
+      expect(e.get("[foo][3]")).to be_nil
+    end
+  end
+
+  context "[]=" do
+    it "should set simple values" do
+      e = LogStash::Event.new()
+      expect(e.set("foo", "bar")).to eq("bar")
+      expect(e.get("foo")).to eq("bar")
+
+      e = LogStash::Event.new({"foo" => "test"})
+      expect(e.set("foo", "bar")).to eq("bar")
+      expect(e.get("foo")).to eq("bar")
+    end
+
+    it "should set deep hash values" do
+      e = LogStash::Event.new()
+      expect(e.set("[foo][bar]", "baz")).to eq("baz")
+      expect(e.get("[foo][bar]")).to eq("baz")
+      expect(e.get("[foo][baz]")).to be_nil
+    end
+
+    it "should set deep array values" do
+      e = LogStash::Event.new()
+      expect(e.set("[foo][0]", "bar")).to eq("bar")
+      expect(e.get("[foo][0]")).to eq("bar")
+      expect(e.set("[foo][1]", 1)).to eq(1)
+      expect(e.get("[foo][1]")).to eq(1)
+      expect(e.set("[foo][2]", 1.0)).to eq(1.0)
+      expect(e.get("[foo][2]")).to eq(1.0)
+      expect(e.get("[foo][3]")).to be_nil
+    end
+
+    it "should add key when setting nil value" do
+      e = LogStash::Event.new()
+      e.set("[foo]", nil)
+      expect(e.to_hash).to include("foo" => nil)
+    end
+
+    # BigDecinal is now natively converted by JRuby, see https://github.com/elastic/logstash/pull/4838
+    it "should set BigDecimal" do
+      e = LogStash::Event.new()
+      e.set("[foo]", BigDecimal.new(1))
+      expect(e.get("foo")).to be_kind_of(BigDecimal)
+      expect(e.get("foo")).to eq(BigDecimal.new(1))
+    end
+
+    it "should set RubyBignum" do
+      e = LogStash::Event.new()
+      e.set("[foo]", -9223372036854776000)
+      expect(e.get("foo")).to be_kind_of(Bignum)
+      expect(e.get("foo")).to eq(-9223372036854776000)
+    end
+  end
+
+  context "timestamp" do
+    it "getters should present a Ruby LogStash::Timestamp" do
+      e = LogStash::Event.new()
+      expect(e.timestamp.class).to eq(LogStash::Timestamp)
+      expect(e.get(TIMESTAMP).class).to eq(LogStash::Timestamp)
+    end
+
+    it "to_hash should inject a Ruby LogStash::Timestamp" do
+      e = LogStash::Event.new()
+
+      expect(e.to_java).to be_kind_of(Java::ComLogstash::Event)
+      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::ComLogstash::Timestamp)
+
+      expect(e.to_hash[TIMESTAMP]).to be_kind_of(LogStash::Timestamp)
+      # now make sure the original map was not touched
+      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::ComLogstash::Timestamp)
+    end
+
+    it "should set timestamp" do
+      e = LogStash::Event.new
+      now = Time.now
+      e.set("@timestamp", LogStash::Timestamp.at(now.to_i))
+      expect(e.timestamp.to_i).to eq(now.to_i)
+      expect(e.get("@timestamp").to_i).to eq(now.to_i)
+    end
+  end
+
+  context "append" do
+    it "should append" do
+      event = LogStash::Event.new("message" => "hello world")
+      event.append(LogStash::Event.new("message" => "another thing"))
+      expect(event.get("message")).to eq(["hello world", "another thing"])
+    end
+  end
+
+  context "tags" do
+    it "should tag" do
+      event = LogStash::Event.new("message" => "hello world")
+      expect(event.get("tags")).to be_nil
+      event.tag("foo")
+      expect(event.get("tags")).to eq(["foo"])
+    end
+  end
+
+
+  # noop logger used to test the injectable logger in Event
+  # this implementation is not complete because only the warn
+  # method is used in Event.
+  module DummyLogger
+    def self.warn(message)
+      # do nothing
+    end
+  end
+
+  context "logger" do
+
+    let(:logger) { double("Logger") }
+    after(:each) {  LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER }
+
+    # the following 2 specs are using both a real module (DummyLogger)
+    # and a mock. both tests are needed to make sure the implementation
+    # supports both types of objects.
+
+    it "should set logger using a module" do
+      LogStash::Event.logger = DummyLogger
+      expect(DummyLogger).to receive(:warn).once
+      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
+    end
+
+    it "should set logger using a mock" do
+      LogStash::Event.logger = logger
+      expect(logger).to receive(:warn).once
+      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
+    end
+
+    it "should unset logger" do
+      # first set
+      LogStash::Event.logger = logger
+      expect(logger).to receive(:warn).once
+      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
+
+      # then unset
+      LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER
+      expect(logger).to receive(:warn).never
+      # this will produce a log line in stdout by the Java Event
+      LogStash::Event.new(TIMESTAMP => "ignore this log")
+    end
+
+
+    it "should warn on parsing error" do
+      LogStash::Event.logger = logger
+      expect(logger).to receive(:warn).once.with(/^Error parsing/)
+      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
+    end
+
+    it "should warn on invalid timestamp object" do
+      LogStash::Event.logger = logger
+      expect(logger).to receive(:warn).once.with(/^Unrecognized/)
+      LogStash::Event.new(TIMESTAMP => Array.new)
+    end
+  end
+
+  context "to_hash" do
+    let (:source_hash) {  {"a" => 1, "b" => [1, 2, 3, {"h" => 1, "i" => "baz"}], "c" => {"d" => "foo", "e" => "bar", "f" => [4, 5, "six"]}} }
+    let (:source_hash_with_matada) {  source_hash.merge({"@metadata" => {"a" => 1, "b" => 2}}) }
+    subject { LogStash::Event.new(source_hash_with_matada) }
+
+    it "should include @timestamp and @version" do
+      h = subject.to_hash
+      expect(h).to include("@timestamp")
+      expect(h).to include("@version")
+      expect(h).not_to include("@metadata")
+    end
+
+    it "should include @timestamp and @version and @metadata" do
+      h = subject.to_hash_with_metadata
+      expect(h).to include("@timestamp")
+      expect(h).to include("@version")
+      expect(h).to include("@metadata")
+    end
+
+    it "should produce valid deep Ruby hash without metadata" do
+      h = subject.to_hash
+      h.delete("@timestamp")
+      h.delete("@version")
+      expect(h).to eq(source_hash)
+    end
+
+    it "should produce valid deep Ruby hash with metadata" do
+      h = subject.to_hash_with_metadata
+      h.delete("@timestamp")
+      h.delete("@version")
+      expect(h).to eq(source_hash_with_matada)
+    end
+  end
+
+  context "from_json" do
+    let (:source_json) { "{\"foo\":1, \"bar\":\"baz\"}" }
+    let (:blank_strings) {["", "  ",  "   "]}
+    let (:bare_strings) {["aa", "  aa", "aa  "]}
+
+    it "should produce a new event from json" do
+      expect(LogStash::Event.from_json(source_json).size).to eq(1)
+
+      event = LogStash::Event.from_json(source_json)[0]
+      expect(event.get("[foo]")).to eq(1)
+      expect(event.get("[bar]")).to eq("baz")
+    end
+
+    it "should ignore blank strings" do
+      blank_strings.each do |s|
+        expect(LogStash::Event.from_json(s).size).to eq(0)
+      end
+    end
+
+    it "should raise TypeError on nil string" do
+      expect{LogStash::Event.from_json(nil)}.to raise_error TypeError
+    end
+
+    it "should consistently handle nil" do
+      blank_strings.each do |s|
+        expect{LogStash::Event.from_json(nil)}.to raise_error
+        expect{LogStash::Event.new(LogStash::Json.load(nil))}.to raise_error
+      end
+    end
+
+    it "should consistently handle bare string" do
+      bare_strings.each do |s|
+        expect{LogStash::Event.from_json(s)}.to raise_error LogStash::Json::ParserError
+        expect{LogStash::Event.new(LogStash::Json.load(s))}.to raise_error LogStash::Json::ParserError
+       end
+    end
+  end
+
+  context "initialize" do
+
+    it "should accept Ruby Hash" do
+      e = LogStash::Event.new({"foo" => 1, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(e.get("foo")).to eq(1)
+      expect(e.timestamp.to_iso8601).to eq("2015-05-28T23:02:05.350Z")
+    end
+
+    it "should accept Java Map" do
+      h = Java::JavaUtil::HashMap.new
+      h.put("foo", 2);
+      h.put(TIMESTAMP, "2016-05-28T23:02:05.350Z");
+      e = LogStash::Event.new(h)
+
+      expect(e.get("foo")).to eq(2)
+      expect(e.timestamp.to_iso8601).to eq("2016-05-28T23:02:05.350Z")
+    end
+
+  end
+end
diff --git a/logstash-core-event-java/spec/timestamp_spec.rb b/logstash-core-event-java/spec/timestamp_spec.rb
new file mode 100644
index 00000000000..1c092696389
--- /dev/null
+++ b/logstash-core-event-java/spec/timestamp_spec.rb
@@ -0,0 +1,29 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/timestamp"
+
+describe LogStash::Timestamp do
+  context "constructors" do
+    it "should work" do
+      t = LogStash::Timestamp.new
+      expect(t.time.to_i).to be_within(1).of Time.now.to_i
+
+      t = LogStash::Timestamp.now
+      expect(t.time.to_i).to be_within(1).of Time.now.to_i
+
+      now = Time.now.utc
+      t = LogStash::Timestamp.new(now)
+      expect(t.time).to eq(now)
+
+      t = LogStash::Timestamp.at(now.to_i)
+      expect(t.time.to_i).to eq(now.to_i)
+    end
+
+    it "should raise exception on invalid format" do
+      expect{LogStash::Timestamp.new("foobar")}.to raise_error
+    end
+
+  end
+
+end
diff --git a/logstash-core-event-java/src/main/java/JrubyEventExtService.java b/logstash-core-event-java/src/main/java/JrubyEventExtService.java
new file mode 100644
index 00000000000..306a45f3971
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/JrubyEventExtService.java
@@ -0,0 +1,14 @@
+import com.logstash.ext.JrubyEventExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.runtime.load.BasicLibraryService;
+
+import java.io.IOException;
+
+public class JrubyEventExtService implements BasicLibraryService {
+    public boolean basicLoad(final Ruby runtime)
+        throws IOException
+    {
+        new JrubyEventExtLibrary().load(runtime, false);
+        return true;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java b/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java
new file mode 100644
index 00000000000..32d8eb2bf98
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java
@@ -0,0 +1,15 @@
+import com.logstash.ext.JrubyEventExtLibrary;
+import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.runtime.load.BasicLibraryService;
+
+import java.io.IOException;
+
+public class JrubyTimestampExtService implements BasicLibraryService {
+    public boolean basicLoad(final Ruby runtime)
+            throws IOException
+    {
+        new JrubyTimestampExtLibrary().load(runtime, false);
+        return true;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Accessors.java b/logstash-core-event-java/src/main/java/com/logstash/Accessors.java
new file mode 100644
index 00000000000..9a9506f934a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Accessors.java
@@ -0,0 +1,177 @@
+package com.logstash;
+
+import java.util.HashMap;
+import java.util.Map;
+import java.util.List;
+
+public class Accessors {
+
+    private Map<String, Object> data;
+    protected Map<String, Object> lut;
+
+    public Accessors(Map<String, Object> data) {
+        this.data = data;
+        this.lut = new HashMap<>(); // reference -> target LUT
+    }
+
+    public Object get(String reference) {
+        FieldReference field = PathCache.getInstance().cache(reference);
+        Object target = findTarget(field);
+        return (target == null) ? null : fetch(target, field.getKey());
+    }
+
+    public Object set(String reference, Object value) {
+        FieldReference field = PathCache.getInstance().cache(reference);
+        Object target = findCreateTarget(field);
+        return store(target, field.getKey(), value);
+    }
+
+    public Object del(String reference) {
+        FieldReference field = PathCache.getInstance().cache(reference);
+        Object target = findTarget(field);
+        if (target != null) {
+            if (target instanceof Map) {
+                return ((Map<String, Object>) target).remove(field.getKey());
+            } else if (target instanceof List) {
+                int i = Integer.parseInt(field.getKey());
+                if (i < 0 || i >= ((List) target).size()) {
+                    return null;
+                }
+                return ((List<Object>) target).remove(i);
+            } else {
+                throw newCollectionException(target);
+            }
+        }
+        return null;
+    }
+
+    public boolean includes(String reference) {
+        FieldReference field = PathCache.getInstance().cache(reference);
+        Object target = findTarget(field);
+        if (target instanceof Map && foundInMap((Map<String, Object>) target, field.getKey())) {
+            return true;
+        } else if (target instanceof List && foundInList((List<Object>) target, Integer.parseInt(field.getKey()))) {
+            return true;
+        } else {
+            return false;
+        }
+    }
+
+    private Object findTarget(FieldReference field) {
+        Object target;
+
+        if ((target = this.lut.get(field.getReference())) != null) {
+            return target;
+        }
+
+        target = this.data;
+        for (String key : field.getPath()) {
+            target = fetch(target, key);
+            if (! isCollection(target)) {
+                return null;
+            }
+        }
+
+        this.lut.put(field.getReference(), target);
+
+        return target;
+    }
+
+    private Object findCreateTarget(FieldReference field) {
+        Object target;
+
+        // flush the @lut to prevent stale cached fieldref which may point to an old target
+        // which was overwritten with a new value. for example, if "[a][b]" is cached and we
+        // set a new value for "[a]" then reading again "[a][b]" would point in a stale target.
+        // flushing the complete @lut is suboptimal, but a hierarchical lut would be required
+        // to be able to invalidate fieldrefs from a common root.
+        // see https://github.com/elastic/logstash/pull/5132
+        this.lut.clear();
+
+        target = this.data;
+        for (String key : field.getPath()) {
+            Object result = fetch(target, key);
+            if (result == null) {
+                result = new HashMap<String, Object>();
+                if (target instanceof Map) {
+                    ((Map<String, Object>)target).put(key, result);
+                } else if (target instanceof List) {
+                    int i = Integer.parseInt(key);
+                    // TODO: what about index out of bound?
+                    ((List<Object>)target).set(i, result);
+                } else if (target != null) {
+                    throw newCollectionException(target);
+                }
+            }
+            target = result;
+        }
+
+        this.lut.put(field.getReference(), target);
+
+        return target;
+    }
+
+    private boolean foundInList(List<Object> target, int index) {
+        if (index < 0 || index >= target.size()) {
+            return false;
+        }
+        return target.get(index) != null;
+    }
+
+    private boolean foundInMap(Map<String, Object> target, String key) {
+        return target.containsKey(key);
+    }
+
+    private Object fetch(Object target, String key) {
+        if (target instanceof Map) {
+            Object result = ((Map<String, Object>) target).get(key);
+            return result;
+        } else if (target instanceof List) {
+            int i = Integer.parseInt(key);
+            if (i < 0 || i >= ((List) target).size()) {
+                return null;
+            }
+            Object result = ((List<Object>) target).get(i);
+            return result;
+        } else if (target == null) {
+            return null;
+        } else {
+            throw newCollectionException(target);
+        }
+    }
+
+    private Object store(Object target, String key, Object value) {
+        if (target instanceof Map) {
+            ((Map<String, Object>) target).put(key, value);
+        } else if (target instanceof List) {
+            int i = Integer.parseInt(key);
+            int size = ((List<Object>) target).size();
+            if (i >= size) {
+                // grow array by adding trailing null items
+                // this strategy reflects legacy Ruby impl behaviour and is backed by specs
+                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbritary big numbers
+                // TODO: (colin) should be guard against this?
+                for (int j = size; j < i; j++) {
+                    ((List<Object>) target).add(null);
+                }
+                ((List<Object>) target).add(value);
+            } else {
+                ((List<Object>) target).set(i, value);
+            }
+        } else {
+            throw newCollectionException(target);
+        }
+        return value;
+    }
+
+    private boolean isCollection(Object target) {
+        if (target == null) {
+            return false;
+        }
+        return (target instanceof Map || target instanceof List);
+    }
+
+    private ClassCastException newCollectionException(Object target) {
+        return new ClassCastException("expecting List or Map, found "  + target.getClass());
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Cloner.java b/logstash-core-event-java/src/main/java/com/logstash/Cloner.java
new file mode 100644
index 00000000000..4823f10726a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Cloner.java
@@ -0,0 +1,56 @@
+package com.logstash;
+
+import java.util.*;
+
+public final class Cloner {
+
+    private Cloner(){}
+
+    public static <T> T deep(final T input) {
+        if (input instanceof Map<?, ?>) {
+            return (T) deepMap((Map<?, ?>) input);
+        } else if (input instanceof List<?>) {
+            return (T) deepList((List<?>) input);
+        } else if (input instanceof Collection<?>) {
+            throw new ClassCastException("unexpected Collection type " + input.getClass());
+        }
+
+        return input;
+    }
+
+    private static <E> List<E> deepList(final List<E> list) {
+        List<E> clone;
+        if (list instanceof LinkedList<?>) {
+            clone = new LinkedList<E>();
+        } else if (list instanceof ArrayList<?>) {
+            clone = new ArrayList<E>();
+        } else {
+            throw new ClassCastException("unexpected List type " + list.getClass());
+        }
+
+        for (E item : list) {
+            clone.add(deep(item));
+        }
+
+        return clone;
+    }
+
+    private static <K, V> Map<K, V> deepMap(final Map<K, V> map) {
+        Map<K, V> clone;
+        if (map instanceof LinkedHashMap<?, ?>) {
+            clone = new LinkedHashMap<K, V>();
+        } else if (map instanceof TreeMap<?, ?>) {
+            clone = new TreeMap<K, V>();
+        } else if (map instanceof HashMap<?, ?>) {
+            clone = new HashMap<K, V>();
+        } else {
+            throw new ClassCastException("unexpected Map type " + map.getClass());
+        }
+
+        for (Map.Entry<K, V> entry : map.entrySet()) {
+            clone.put(entry.getKey(), deep(entry.getValue()));
+        }
+
+        return clone;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/DateNode.java b/logstash-core-event-java/src/main/java/com/logstash/DateNode.java
new file mode 100644
index 00000000000..560d9f53d3c
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/DateNode.java
@@ -0,0 +1,24 @@
+package com.logstash;
+
+import org.joda.time.DateTimeZone;
+import org.joda.time.format.DateTimeFormat;
+import org.joda.time.format.DateTimeFormatter;
+
+import java.io.IOError;
+import java.io.IOException;
+
+/**
+ * Created by ph on 15-05-22.
+ */
+public class DateNode implements TemplateNode {
+    private DateTimeFormatter formatter;
+
+    public DateNode(String format) {
+        this.formatter = DateTimeFormat.forPattern(format).withZone(DateTimeZone.UTC);
+    }
+
+    @Override
+    public String evaluate(Event event) throws IOException {
+        return event.getTimestamp().getTime().toString(this.formatter);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/EpochNode.java b/logstash-core-event-java/src/main/java/com/logstash/EpochNode.java
new file mode 100644
index 00000000000..4451ffa73c4
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/EpochNode.java
@@ -0,0 +1,15 @@
+package com.logstash;
+
+import java.io.IOException;
+
+/**
+ * Created by ph on 15-05-22.
+ */
+public class EpochNode implements TemplateNode {
+    public EpochNode(){ }
+
+    @Override
+    public String evaluate(Event event) throws IOException {
+        return String.valueOf(event.getTimestamp().getTime().getMillis() / 1000);
+    }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Event.java b/logstash-core-event-java/src/main/java/com/logstash/Event.java
new file mode 100644
index 00000000000..d8979b41003
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Event.java
@@ -0,0 +1,296 @@
+package com.logstash;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.joda.time.DateTime;
+import org.jruby.RubySymbol;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.util.*;
+
+
+public class Event implements Cloneable, Serializable {
+
+    private boolean cancelled;
+    private Map<String, Object> data;
+    private Map<String, Object> metadata;
+    private Timestamp timestamp;
+    private Accessors accessors;
+    private Accessors metadata_accessors;
+
+    public static final String METADATA = "@metadata";
+    public static final String METADATA_BRACKETS = "[" + METADATA + "]";
+    public static final String TIMESTAMP = "@timestamp";
+    public static final String TIMESTAMP_FAILURE_TAG = "_timestampparsefailure";
+    public static final String TIMESTAMP_FAILURE_FIELD = "_@timestamp";
+    public static final String VERSION = "@version";
+    public static final String VERSION_ONE = "1";
+
+    private static final Logger DEFAULT_LOGGER = new StdioLogger();
+    private static final ObjectMapper mapper = new ObjectMapper();
+
+    // logger is static since once set there is no point in changing it at runtime
+    // for other reasons than in tests/specs.
+    private transient static Logger logger = DEFAULT_LOGGER;
+
+    public Event()
+    {
+        this.metadata = new HashMap<String, Object>();
+        this.data = new HashMap<String, Object>();
+        this.data.put(VERSION, VERSION_ONE);
+        this.cancelled = false;
+        this.timestamp = new Timestamp();
+        this.data.put(TIMESTAMP, this.timestamp);
+        this.accessors = new Accessors(this.data);
+        this.metadata_accessors = new Accessors(this.metadata);
+    }
+
+    public Event(Map data)
+    {
+        this.data = data;
+        if (!this.data.containsKey(VERSION)) {
+            this.data.put(VERSION, VERSION_ONE);
+        }
+
+        if (this.data.containsKey(METADATA)) {
+            this.metadata = (HashMap<String, Object>) this.data.remove(METADATA);
+        } else {
+            this.metadata = new HashMap<String, Object>();
+        }
+        this.metadata_accessors = new Accessors(this.metadata);
+
+        this.cancelled = false;
+        this.timestamp = initTimestamp(data.get(TIMESTAMP));
+        this.data.put(TIMESTAMP, this.timestamp);
+        this.accessors = new Accessors(this.data);
+    }
+
+    public Map<String, Object> getData() {
+        return this.data;
+    }
+
+    public Map<String, Object> getMetadata() {
+        return this.metadata;
+    }
+
+    public void setData(Map<String, Object> data) {
+        this.data = data;
+    }
+
+    public Accessors getAccessors() {
+        return this.accessors;
+    }
+
+    public Accessors getMetadataAccessors() {
+        return this.metadata_accessors;
+    }
+
+    public void setAccessors(Accessors accessors) {
+        this.accessors = accessors;
+    }
+
+    public void setMetadataAccessors(Accessors accessors) {
+        this.metadata_accessors = accessors;
+    }
+
+    public void cancel() {
+        this.cancelled = true;
+    }
+
+    public void uncancel() {
+        this.cancelled = false;
+    }
+
+    public boolean isCancelled() {
+        return this.cancelled;
+    }
+
+    public Timestamp getTimestamp() throws IOException {
+        if (this.data.containsKey(TIMESTAMP)) {
+            return this.timestamp;
+        } else {
+            throw new IOException("fails");
+        }
+    }
+
+    public void setTimestamp(Timestamp t) {
+        this.timestamp = t;
+        this.data.put(TIMESTAMP, this.timestamp);
+    }
+
+    public Object getField(String reference) {
+        if (reference.equals(METADATA)) {
+            return this.metadata;
+        } else if (reference.startsWith(METADATA_BRACKETS)) {
+            return this.metadata_accessors.get(reference.substring(METADATA_BRACKETS.length()));
+        } else {
+            return this.accessors.get(reference);
+        }
+    }
+
+    public void setField(String reference, Object value) {
+        if (reference.equals(TIMESTAMP)) {
+            // TODO(talevy): check type of timestamp
+            this.accessors.set(reference, value);
+        } else if (reference.equals(METADATA_BRACKETS) || reference.equals(METADATA)) {
+            this.metadata = (HashMap<String, Object>) value;
+            this.metadata_accessors = new Accessors(this.metadata);
+        } else if (reference.startsWith(METADATA_BRACKETS)) {
+            this.metadata_accessors.set(reference.substring(METADATA_BRACKETS.length()), value);
+        } else {
+            this.accessors.set(reference, value);
+        }
+    }
+
+    public boolean includes(String reference) {
+        if (reference.equals(METADATA_BRACKETS) || reference.equals(METADATA)) {
+            return true;
+        } else if (reference.startsWith(METADATA_BRACKETS)) {
+            return this.metadata_accessors.includes(reference.substring(METADATA_BRACKETS.length()));
+        } else {
+            return this.accessors.includes(reference);
+        }
+    }
+
+    public String toJson()
+            throws IOException
+    {
+        return mapper.writeValueAsString((Map<String, Object>)this.data);
+    }
+
+    public static Event[] fromJson(String json)
+            throws IOException
+    {
+        // empty/blank json string does not generate an event
+        if (json == null || json.trim().isEmpty()) {
+            return new Event[]{ };
+        }
+
+        Event[] result;
+        Object o = mapper.readValue(json, Object.class);
+        // we currently only support Map or Array json objects
+        if (o instanceof Map) {
+            result = new Event[]{ new Event((Map)o) };
+        } else if (o instanceof List) {
+            result = new Event[((List) o).size()];
+            int i = 0;
+            for (Object e : (List)o) {
+                if (!(e instanceof Map)) {
+                    throw new IOException("incompatible inner json array object type=" + e.getClass().getName() + " , only hash map is suppoted");
+                }
+                result[i++] = new Event((Map)e);
+            }
+        } else {
+            throw new IOException("incompatible json object type=" + o.getClass().getName() + " , only hash map or arrays are suppoted");
+        }
+
+        return result;
+    }
+
+    public Map toMap() {
+        return Cloner.deep(this.data);
+    }
+
+    public Event overwrite(Event e) {
+        this.data = e.getData();
+        this.accessors = e.getAccessors();
+        this.cancelled = e.isCancelled();
+        try {
+            this.timestamp = e.getTimestamp();
+        } catch (IOException exception) {
+            this.timestamp = new Timestamp();
+        }
+
+        return this;
+    }
+
+
+    public Event append(Event e) {
+        Util.mapMerge(this.data, e.data);
+
+        return this;
+    }
+
+    public Object remove(String path) {
+        return this.accessors.del(path);
+    }
+
+    public String sprintf(String s) throws IOException {
+        return StringInterpolation.getInstance().evaluate(this, s);
+    }
+
+    public Event clone()
+            throws CloneNotSupportedException
+    {
+//        Event clone = (Event)super.clone();
+//        clone.setAccessors(new Accessors(clone.getData()));
+
+        Event clone = new Event(Cloner.deep(getData()));
+        return clone;
+    }
+
+    public String toString() {
+        // TODO: (colin) clean this IOException handling, not sure why we bubble IOException here
+        try {
+            return (getTimestamp().toIso8601() + " " + this.sprintf("%{host} %{message}"));
+        } catch (IOException e) {
+            String host = (String)this.data.get("host");
+            host = (host != null ? host : "%{host}");
+
+            String message = (String)this.data.get("message");
+            message = (message != null ? message : "%{message}");
+
+            return (host + " " + message);
+        }
+    }
+
+    private Timestamp initTimestamp(Object o) {
+        try {
+            if (o == null) {
+                // most frequent
+                return new Timestamp();
+            } else if (o instanceof String) {
+                // second most frequent
+                return new Timestamp((String) o);
+            } else if (o instanceof JrubyTimestampExtLibrary.RubyTimestamp) {
+                return new Timestamp(((JrubyTimestampExtLibrary.RubyTimestamp) o).getTimestamp());
+            } else if (o instanceof Timestamp) {
+                return new Timestamp((Timestamp) o);
+            } else if (o instanceof DateTime) {
+                return new Timestamp((DateTime) o);
+            } else if (o instanceof Date) {
+                return new Timestamp((Date) o);
+            } else if (o instanceof RubySymbol) {
+                return new Timestamp(((RubySymbol) o).asJavaString());
+            } else {
+                Event.logger.warn("Unrecognized " + TIMESTAMP + " value type=" + o.getClass().toString());
+            }
+        } catch (IllegalArgumentException e) {
+            Event.logger.warn("Error parsing " + TIMESTAMP + " string value=" + o.toString());
+        }
+
+        tag(TIMESTAMP_FAILURE_TAG);
+        this.data.put(TIMESTAMP_FAILURE_FIELD, o);
+
+        return Timestamp.now();
+    }
+
+    public void tag(String tag) {
+        List<Object> tags = (List<Object>) this.data.get("tags");
+        if (tags == null) {
+            tags = new ArrayList<>();
+            this.data.put("tags", tags);
+        }
+
+        if (!tags.contains(tag)) {
+            tags.add(tag);
+        }
+    }
+
+    // Event.logger is static since once set there is no point in changing it at runtime
+    // for other reasons than in tests/specs.
+    public static void setLogger(Logger logger) {
+        Event.logger = logger;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/FieldReference.java b/logstash-core-event-java/src/main/java/com/logstash/FieldReference.java
new file mode 100644
index 00000000000..e0d7e3ee969
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/FieldReference.java
@@ -0,0 +1,40 @@
+package com.logstash;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+// TODO: implement thread-safe path cache singleton to avoid parsing
+
+public class FieldReference {
+
+    private List<String> path;
+    private String key;
+    private String reference;
+    private static List<String> EMPTY_STRINGS = Arrays.asList("");
+
+    public FieldReference(List<String> path, String key, String reference) {
+        this.path = path;
+        this.key = key;
+        this.reference = reference;
+    }
+
+    public List<String> getPath() {
+        return path;
+    }
+
+    public String getKey() {
+        return key;
+    }
+
+    public String getReference() {
+        return reference;
+    }
+
+    public static FieldReference parse(String reference) {
+        List<String> path = new ArrayList(Arrays.asList(reference.split("[\\[\\]]")));
+        path.removeAll(EMPTY_STRINGS);
+        String key = path.remove(path.size() - 1);
+        return new FieldReference(path, key, reference);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Javafier.java b/logstash-core-event-java/src/main/java/com/logstash/Javafier.java
new file mode 100644
index 00000000000..e1e03156be7
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Javafier.java
@@ -0,0 +1,161 @@
+package com.logstash;
+
+import org.jruby.RubyArray;
+import org.jruby.RubyHash;
+import org.jruby.RubyString;
+import org.jruby.RubyObject;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyArray;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.RubyNil;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyTime;
+import org.jruby.RubySymbol;
+import org.jruby.RubyBignum;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.jruby.runtime.builtin.IRubyObject;
+import java.math.BigDecimal;
+import org.joda.time.DateTime;
+
+import java.math.BigInteger;
+import java.util.*;
+
+public class Javafier {
+
+    private Javafier(){}
+
+    public static List<Object> deep(RubyArray a) {
+        final ArrayList<Object> result = new ArrayList();
+
+        // TODO: (colin) investagate why .toJavaArrayUnsafe() which should be faster by avoiding copying produces nil values spec errors in arrays
+        for (IRubyObject o : a.toJavaArray()) {
+            result.add(deep(o));
+        }
+        return result;
+    }
+
+    public static HashMap<String, Object> deep(RubyHash h) {
+        final HashMap result = new HashMap();
+
+        h.visitAll(new RubyHash.Visitor() {
+            @Override
+            public void visit(IRubyObject key, IRubyObject value) {
+                result.put(deep(key).toString(), deep(value));
+            }
+        });
+        return result;
+    }
+
+    public static String deep(RubyString s) {
+        return s.asJavaString();
+    }
+
+    public static long deep(RubyInteger i) {
+        return i.getLongValue();
+    }
+
+    public static long deep(RubyFixnum n) {
+        return n.getLongValue();
+    }
+
+    public static double deep(RubyFloat f) {
+        return f.getDoubleValue();
+    }
+
+    public static BigDecimal deep(RubyBigDecimal bd) {
+        return bd.getBigDecimalValue();
+    }
+
+    public static BigInteger deep(RubyBignum bn) {
+        return bn.getBigIntegerValue();
+    }
+
+    public static Timestamp deep(JrubyTimestampExtLibrary.RubyTimestamp t) {
+        return t.getTimestamp();
+    }
+
+    public static boolean deep(RubyBoolean b) {
+        return b.isTrue();
+    }
+
+    public static Object deep(RubyNil n) {
+        return null;
+    }
+
+    public static DateTime deep(RubyTime t) {
+        return t.getDateTime();
+    }
+
+    public static String deep(RubySymbol s) {
+        return s.asJavaString();
+    }
+
+    public static Object deep(RubyBoolean.True b) {
+        return true;
+    }
+
+    public static Object deep(RubyBoolean.False b) {
+        return false;
+    }
+
+    public static Object deep(IRubyObject o) {
+        // TODO: (colin) this enum strategy is cleaner but I am hoping that is not slower than using a instanceof cascade
+
+        RUBYCLASS clazz;
+        try {
+            clazz = RUBYCLASS.valueOf(o.getClass().getSimpleName());
+        } catch (IllegalArgumentException e) {
+            throw new IllegalArgumentException("Missing Ruby class handling for full class name=" + o.getClass().getName() + ", simple name=" + o.getClass().getSimpleName());
+        }
+
+        switch(clazz) {
+            case RubyArray: return deep((RubyArray)o);
+            case RubyHash: return deep((RubyHash)o);
+            case RubyString: return deep((RubyString)o);
+            case RubyInteger: return deep((RubyInteger)o);
+            case RubyFloat: return deep((RubyFloat)o);
+            case RubyBigDecimal: return deep((RubyBigDecimal)o);
+            case RubyTimestamp: return deep((JrubyTimestampExtLibrary.RubyTimestamp)o);
+            case RubyBoolean: return deep((RubyBoolean)o);
+            case RubyFixnum: return deep((RubyFixnum)o);
+            case RubyBignum: return deep((RubyBignum)o);
+            case RubyTime: return deep((RubyTime)o);
+            case RubySymbol: return deep((RubySymbol)o);
+            case RubyNil: return deep((RubyNil)o);
+            case True: return deep((RubyBoolean.True)o);
+            case False: return deep((RubyBoolean.False)o);
+        }
+
+        if (o.isNil()) {
+            return null;
+        }
+
+        // TODO: (colin) temporary trace to spot any unhandled types
+        System.out.println("***** WARN: UNHANDLED IRubyObject full class name=" + o.getMetaClass().getRealClass().getName() + ", simple name=" + o.getClass().getSimpleName() + " java class=" + o.getJavaClass().toString() + " toString=" + o.toString());
+
+        return o.toJava(o.getJavaClass());
+    }
+
+    enum RUBYCLASS {
+        RubyString,
+        RubyInteger,
+        RubyFloat,
+        RubyBigDecimal,
+        RubyTimestamp,
+        RubyArray,
+        RubyHash,
+        RubyBoolean,
+        RubyFixnum,
+        RubyBignum,
+        RubyObject,
+        RubyNil,
+        RubyTime,
+        RubySymbol,
+        True,
+        False;
+    }
+}
+
diff --git a/logstash-core-event-java/src/main/java/com/logstash/KeyNode.java b/logstash-core-event-java/src/main/java/com/logstash/KeyNode.java
new file mode 100644
index 00000000000..cfc46861f69
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/KeyNode.java
@@ -0,0 +1,63 @@
+package com.logstash;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.io.IOException;
+import java.util.List;
+import java.util.Map;
+
+/**
+ * Created by ph on 15-05-22.
+ */
+public class KeyNode implements TemplateNode {
+    private String key;
+
+    public KeyNode(String key) {
+        this.key = key;
+    }
+
+    /**
+     This will be more complicated with hash and array.
+     leverage jackson lib to do the actual.
+     */
+    @Override
+    public String evaluate(Event event) throws IOException {
+        Object value = event.getField(this.key);
+
+        if (value != null) {
+            if (value instanceof List) {
+                return join((List)value, ",");
+            } else if (value instanceof Map) {
+                ObjectMapper mapper = new ObjectMapper();
+                return mapper.writeValueAsString((Map<String, Object>)value);
+            } else {
+                return event.getField(this.key).toString();
+            }
+
+        } else {
+            return "%{" + this.key + "}";
+        }
+    }
+
+    // TODO: (colin) this should be moved somewhere else to make it reusable
+    //   this is a quick fix to compile on JDK7 a not use String.join that is
+    //   only available in JDK8
+    public static String join(List<?> list, String delim) {
+        int len = list.size();
+
+        if (len == 0) return "";
+
+        StringBuilder result = new StringBuilder(toString(list.get(0), delim));
+        for (int i = 1; i < len; i++) {
+            result.append(delim);
+            result.append(toString(list.get(i), delim));
+        }
+        return result.toString();
+    }
+
+    private static String toString(Object value, String delim) {
+        if (value == null) return "";
+        if (value instanceof List) return join((List)value, delim);
+        return value.toString();
+    }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Logger.java b/logstash-core-event-java/src/main/java/com/logstash/Logger.java
new file mode 100644
index 00000000000..fc425542715
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Logger.java
@@ -0,0 +1,13 @@
+package com.logstash;
+
+// minimalist Logger interface to wire a logger callback in the Event class
+// for now only warn is defined because this is the only method that's required
+// in the Event class.
+// TODO: (colin) generalize this
+
+public interface Logger {
+
+    // TODO: (colin) complete interface beyond warn when needed
+
+    void warn(String message);
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/PathCache.java b/logstash-core-event-java/src/main/java/com/logstash/PathCache.java
new file mode 100644
index 00000000000..b7beff95b89
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/PathCache.java
@@ -0,0 +1,47 @@
+package com.logstash;
+
+import java.util.concurrent.ConcurrentHashMap;
+
+public class PathCache {
+
+    private static PathCache instance = null;
+    private static ConcurrentHashMap<String, FieldReference> cache = new ConcurrentHashMap<>();
+
+    private FieldReference timestamp;
+
+    // TODO: dry with Event
+    public static final String TIMESTAMP = "@timestamp";
+    public static final String BRACKETS_TIMESTAMP = "[" + TIMESTAMP + "]";
+
+    protected PathCache() {
+        // inject @timestamp
+        this.timestamp = cache(TIMESTAMP);
+        cache(BRACKETS_TIMESTAMP, this.timestamp);
+    }
+
+    public static PathCache getInstance() {
+        if (instance == null) {
+            instance = new PathCache();
+        }
+        return instance;
+    }
+
+    public boolean isTimestamp(String reference) {
+        return (cache(reference) == this.timestamp);
+    }
+
+    public FieldReference cache(String reference) {
+        // atomicity between the get and put is not important
+        FieldReference result = cache.get(reference);
+        if (result == null) {
+            result = FieldReference.parse(reference);
+            cache.put(reference, result);
+        }
+        return result;
+    }
+
+    public FieldReference cache(String reference, FieldReference field) {
+        cache.put(reference, field);
+        return field;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java b/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java
new file mode 100644
index 00000000000..0bafab8c9da
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java
@@ -0,0 +1,65 @@
+package com.logstash;
+
+import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.RubyArray;
+import org.jruby.RubyHash;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.jruby.javasupport.JavaUtil;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.math.BigDecimal;
+import java.util.*;
+
+public final class Rubyfier {
+
+    private Rubyfier(){}
+
+    public static IRubyObject deep(Ruby runtime, final Object input) {
+        if (input instanceof IRubyObject) return (IRubyObject)input;
+        if (input instanceof Map) return deepMap(runtime, (Map) input);
+        if (input instanceof List) return deepList(runtime, (List) input);
+        if (input instanceof Timestamp) return JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(runtime, (Timestamp)input);
+        if (input instanceof Collection) throw new ClassCastException("unexpected Collection type " + input.getClass());
+
+        // BigDecimal is not currenly handled by JRuby and this is the type Jackson uses for floats
+        if (input instanceof BigDecimal) return new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), (BigDecimal)input);
+
+        return JavaUtil.convertJavaToUsableRubyObject(runtime, input);
+    }
+
+    public static Object deepOnly(Ruby runtime, final Object input) {
+        if (input instanceof Map) return deepMap(runtime, (Map) input);
+        if (input instanceof List) return deepList(runtime, (List) input);
+        if (input instanceof Timestamp) return JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(runtime, (Timestamp)input);
+        if (input instanceof Collection) throw new ClassCastException("unexpected Collection type " + input.getClass());
+
+        // BigDecimal is not currenly handled by JRuby and this is the type Jackson uses for floats
+        if (input instanceof BigDecimal) return new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), (BigDecimal)input);
+
+        return input;
+    }
+
+    private static RubyArray deepList(Ruby runtime, final List list) {
+        final int length = list.size();
+        final RubyArray array = runtime.newArray(length);
+
+        for (Object item : list) {
+            // use deepOnly because RubyArray.add already calls JavaUtil.convertJavaToUsableRubyObject on item
+            array.add(deepOnly(runtime, item));
+        }
+
+        return array;
+    }
+
+    private static RubyHash deepMap(Ruby runtime, final Map<?, ?> map) {
+        RubyHash hash = RubyHash.newHash(runtime);
+
+        for (Map.Entry<?, ?> entry : map.entrySet()) {
+            // use deepOnly on value because RubyHash.put already calls JavaUtil.convertJavaToUsableRubyObject on items
+            hash.put(entry.getKey(), deepOnly(runtime, entry.getValue()));
+        }
+
+        return hash;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StaticNode.java b/logstash-core-event-java/src/main/java/com/logstash/StaticNode.java
new file mode 100644
index 00000000000..73b5c160440
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/StaticNode.java
@@ -0,0 +1,19 @@
+package com.logstash;
+
+import java.io.IOException;
+
+/**
+ * Created by ph on 15-05-22.
+ */
+public class StaticNode implements TemplateNode {
+    private String content;
+
+    public StaticNode(String content) {
+        this.content = content;
+    }
+
+    @Override
+    public String evaluate(Event event) throws IOException {
+        return this.content;
+    }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java b/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java
new file mode 100644
index 00000000000..c12bb3e0573
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java
@@ -0,0 +1,10 @@
+package com.logstash;
+
+public class StdioLogger implements Logger {
+
+    // TODO: (colin) complete implementation beyond warn when needed
+
+    public void warn(String message) {
+        System.out.println(message);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java b/logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java
new file mode 100644
index 00000000000..5830cc89672
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java
@@ -0,0 +1,101 @@
+package com.logstash;
+
+
+import java.io.IOException;
+import java.util.Map;
+import java.util.Objects;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
+public class StringInterpolation {
+    static Pattern TEMPLATE_TAG = Pattern.compile("%\\{([^}]+)\\}");
+    static Map cache;
+
+    protected static class HoldCurrent {
+        private static final StringInterpolation INSTANCE = new StringInterpolation();
+    }
+
+    private StringInterpolation() {
+        // TODO:
+        // This may need some tweaking for the concurrency level to get better memory usage.
+        // The current implementation doesn't allow the keys to expire, I think under normal usage
+        // the keys will converge to a fixed number.
+        //
+        // If this code make logstash goes OOM, we have the following options:
+        //  - If the key doesn't contains a `%` do not cache it, this will reduce the key size at a performance cost.
+        //  - Use some kind LRU cache
+        //  - Create a new data structure that use weakref or use Google Guava for the cache https://code.google.com/p/guava-libraries/
+        this.cache = new ConcurrentHashMap<>();
+    }
+
+    public void clearCache() {
+        this.cache.clear();
+    }
+
+    public int cacheSize() {
+        return this.cache.size();
+    }
+
+    public String evaluate(Event event, String template) throws IOException {
+        TemplateNode compiledTemplate = (TemplateNode) this.cache.get(template);
+
+        if (compiledTemplate == null) {
+            compiledTemplate = this.compile(template);
+            this.cache.put(template, compiledTemplate);
+        }
+
+        return compiledTemplate.evaluate(event);
+    }
+
+    public TemplateNode compile(String template) {
+        Template compiledTemplate = new Template();
+
+        if (template.indexOf('%') == -1) {
+            // Move the nodes to a custom instance
+            // so we can remove the iterator and do one `.evaluate`
+            compiledTemplate.add(new StaticNode(template));
+        } else {
+            Matcher matcher = TEMPLATE_TAG.matcher(template);
+            String tag;
+            int pos = 0;
+
+            while (matcher.find()) {
+                if (matcher.start() > 0) {
+                    compiledTemplate.add(new StaticNode(template.substring(pos, matcher.start())));
+                }
+
+                tag = matcher.group(1);
+                compiledTemplate.add(identifyTag(tag));
+                pos = matcher.end();
+            }
+
+            if(pos <= template.length() - 1) {
+                compiledTemplate.add(new StaticNode(template.substring(pos)));
+            }
+        }
+
+        // if we only have one node return the node directly
+        // and remove the need to loop.
+        if(compiledTemplate.size() == 1) {
+            return compiledTemplate.get(0);
+        } else {
+            return compiledTemplate;
+        }
+    }
+
+    public TemplateNode identifyTag(String tag) {
+        if(tag.equals("+%s")) {
+            return new EpochNode();
+        } else if(tag.charAt(0) == '+') {
+                return new DateNode(tag.substring(1));
+
+        } else {
+            return new KeyNode(tag);
+        }
+    }
+
+    static StringInterpolation getInstance() {
+        return HoldCurrent.INSTANCE;
+    }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Template.java b/logstash-core-event-java/src/main/java/com/logstash/Template.java
new file mode 100644
index 00000000000..a17e69b3946
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Template.java
@@ -0,0 +1,32 @@
+package com.logstash;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+
+public class Template implements TemplateNode {
+    public List nodes = new ArrayList<>();
+    public Template() {}
+
+    public void add(TemplateNode node) {
+        nodes.add(node);
+    }
+
+    public int size() {
+        return nodes.size();
+    }
+
+    public TemplateNode get(int index) {
+        return (TemplateNode) nodes.get(index);
+    }
+
+    @Override
+    public String evaluate(Event event) throws IOException {
+        StringBuffer results = new StringBuffer();
+
+        for (int i = 0; i < nodes.size(); i++) {
+            results.append(((TemplateNode) nodes.get(i)).evaluate(event));
+        }
+        return results.toString();
+    }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java b/logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java
new file mode 100644
index 00000000000..942bbc1ee03
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java
@@ -0,0 +1,10 @@
+package com.logstash;
+
+import java.io.IOException;
+
+/**
+ * Created by ph on 15-05-22.
+ */
+public interface TemplateNode {
+    String evaluate(Event event) throws IOException;
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java b/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
new file mode 100644
index 00000000000..434dc93a13c
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
@@ -0,0 +1,84 @@
+package com.logstash;
+
+import com.fasterxml.jackson.databind.annotation.JsonSerialize;
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.joda.time.LocalDateTime;
+import org.joda.time.Duration;
+import org.joda.time.format.DateTimeFormatter;
+import org.joda.time.format.ISODateTimeFormat;
+
+import java.util.Date;
+
+@JsonSerialize(using = TimestampSerializer.class)
+public class Timestamp implements Cloneable {
+
+    // all methods setting the time object must set it in the UTC timezone
+    private DateTime time;
+
+    // TODO: is this DateTimeFormatter thread safe?
+    private static DateTimeFormatter iso8601Formatter = ISODateTimeFormat.dateTime();
+
+    private static final LocalDateTime JAN_1_1970 = new LocalDateTime(1970, 1, 1, 0, 0);
+
+    public Timestamp() {
+        this.time = new DateTime(DateTimeZone.UTC);
+    }
+
+    public Timestamp(String iso8601) {
+        this.time = ISODateTimeFormat.dateTimeParser().parseDateTime(iso8601).toDateTime(DateTimeZone.UTC);
+    }
+
+    public Timestamp(Timestamp t) {
+        this.time = t.getTime();
+    }
+
+    public Timestamp(long epoch_milliseconds) {
+        this.time = new DateTime(epoch_milliseconds, DateTimeZone.UTC);
+    }
+
+    public Timestamp(Long epoch_milliseconds) {
+        this.time = new DateTime(epoch_milliseconds, DateTimeZone.UTC);
+    }
+
+    public Timestamp(Date date) {
+        this.time = new DateTime(date, DateTimeZone.UTC);
+    }
+
+    public Timestamp(DateTime date) {
+        this.time = date.toDateTime(DateTimeZone.UTC);
+    }
+
+    public DateTime getTime() {
+        return time;
+    }
+
+    public void setTime(DateTime time) {
+        this.time = time.toDateTime(DateTimeZone.UTC);
+    }
+
+    public static Timestamp now() {
+        return new Timestamp();
+    }
+
+    public String toIso8601() {
+        return this.iso8601Formatter.print(this.time);
+    }
+
+    public String toString() {
+        return toIso8601();
+    }
+
+    public long usec() {
+        // JodaTime only supports milliseconds precision we can only return usec at millisec precision.
+        // note that getMillis() return millis since epoch
+        return (new Duration(JAN_1_1970.toDateTime(DateTimeZone.UTC), this.time).getMillis() % 1000) * 1000;
+    }
+
+    @Override
+    public Timestamp clone() throws CloneNotSupportedException {
+        Timestamp clone = (Timestamp)super.clone();
+        clone.setTime(this.getTime());
+        return clone;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java b/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
new file mode 100644
index 00000000000..c90afdd9227
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
@@ -0,0 +1,17 @@
+package com.logstash;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+
+import java.io.IOException;
+
+public class TimestampSerializer extends JsonSerializer<Timestamp> {
+
+    @Override
+    public void serialize(Timestamp value, JsonGenerator jgen, SerializerProvider provider)
+            throws IOException
+    {
+        jgen.writeString(value.toIso8601());
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Util.java b/logstash-core-event-java/src/main/java/com/logstash/Util.java
new file mode 100644
index 00000000000..907fd5489b1
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/Util.java
@@ -0,0 +1,49 @@
+package com.logstash;
+
+import java.util.ArrayList;
+import java.util.LinkedHashSet;
+import java.util.List;
+import java.util.Map;
+
+public class Util {
+    private Util() {}
+
+    public static void mapMerge(Map<String, Object> target, Map<String, Object> add) {
+        for (Map.Entry<String, Object> e : add.entrySet()) {
+            if (target.containsKey(e.getKey())) {
+                if (target.get(e.getKey()) instanceof Map && e.getValue() instanceof Map) {
+                    mapMerge((Map<String, Object>) target.get(e.getKey()), (Map<String, Object>) e.getValue());
+                } else if (e.getValue() instanceof List) {
+                    if (target.get(e.getKey()) instanceof List) {
+                        // needs optimizing
+                        List targetList = (List) target.get(e.getKey());
+                        targetList.addAll((List) e.getValue());
+                        target.put(e.getKey(), new ArrayList<Object>(new LinkedHashSet<Object>(targetList)));
+                    } else {
+                        Object targetValue = target.get(e.getKey());
+                        List targetValueList = new ArrayList();
+                        targetValueList.add(targetValue);
+                        for (Object o : (List) e.getValue()) {
+                            if (!targetValue.equals(o)) {
+                                targetValueList.add(o);
+                            }
+                        }
+                        target.put(e.getKey(), targetValueList);
+                    }
+                } else if (target.get(e.getKey()) instanceof List) {
+                    List t = ((List) target.get(e.getKey()));
+                    if (!t.contains(e.getValue())) {
+                        t.add(e.getValue());
+                    }
+                } else if (!target.get(e.getKey()).equals(e.getValue())) {
+                    List targetValue = new ArrayList();
+                    targetValue.add(target.get(e.getKey()));
+                    ((List) targetValue).add(e.getValue());
+                    target.put(e.getKey(), targetValue);
+                }
+            } else {
+                target.put(e.getKey(), e.getValue());
+            }
+        }
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java b/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
new file mode 100644
index 00000000000..d468def96dd
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
@@ -0,0 +1,341 @@
+package com.logstash.ext;
+
+import com.logstash.Logger;
+import com.logstash.Event;
+import com.logstash.PathCache;
+import com.logstash.Javafier;
+import com.logstash.Timestamp;
+import com.logstash.Rubyfier;
+import com.logstash.Javafier;
+import org.jruby.Ruby;
+import org.jruby.RubyObject;
+import org.jruby.RubyClass;
+import org.jruby.RubyModule;
+import org.jruby.RubyString;
+import org.jruby.RubyHash;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyArray;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.exceptions.RaiseException;
+import org.jruby.java.proxies.MapJavaProxy;
+import org.jruby.javasupport.JavaUtil;
+import org.jruby.runtime.Arity;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import java.io.IOException;
+import java.util.Map;
+import java.util.HashMap;
+import java.util.List;
+
+
+public class JrubyEventExtLibrary implements Library {
+
+    private static RubyClass PARSER_ERROR = null;
+    private static RubyClass GENERATOR_ERROR = null;
+    private static RubyClass LOGSTASH_ERROR = null;
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("Event", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyEvent(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.setConstant("METADATA", runtime.newString(Event.METADATA));
+        clazz.setConstant("METADATA_BRACKETS", runtime.newString(Event.METADATA_BRACKETS));
+        clazz.setConstant("TIMESTAMP", runtime.newString(Event.TIMESTAMP));
+        clazz.setConstant("TIMESTAMP_FAILURE_TAG", runtime.newString(Event.TIMESTAMP_FAILURE_TAG));
+        clazz.setConstant("TIMESTAMP_FAILURE_FIELD", runtime.newString(Event.TIMESTAMP_FAILURE_FIELD));
+        clazz.setConstant("DEFAULT_LOGGER", runtime.getModule("Cabin").getClass("Channel").callMethod("get", runtime.getModule("LogStash")));
+        clazz.setConstant("VERSION", runtime.newString(Event.VERSION));
+        clazz.setConstant("VERSION_ONE", runtime.newString(Event.VERSION_ONE));
+        clazz.defineAnnotatedMethods(RubyEvent.class);
+        clazz.defineAnnotatedConstants(RubyEvent.class);
+
+        PARSER_ERROR = runtime.getModule("LogStash").defineOrGetModuleUnder("Json").getClass("ParserError");
+        if (PARSER_ERROR == null) {
+            throw new RaiseException(runtime, runtime.getClass("StandardError"), "Could not find LogStash::Json::ParserError class", true);
+        }
+        GENERATOR_ERROR = runtime.getModule("LogStash").defineOrGetModuleUnder("Json").getClass("GeneratorError");
+        if (GENERATOR_ERROR == null) {
+            throw new RaiseException(runtime, runtime.getClass("StandardError"), "Could not find LogStash::Json::GeneratorError class", true);
+        }
+        LOGSTASH_ERROR = runtime.getModule("LogStash").getClass("Error");
+        if (LOGSTASH_ERROR == null) {
+            throw new RaiseException(runtime, runtime.getClass("StandardError"), "Could not find LogStash::Error class", true);
+        }
+    }
+
+    public static class ProxyLogger implements Logger {
+        private RubyObject logger;
+
+        public ProxyLogger(RubyObject logger) {
+             this.logger = logger;
+        }
+
+        // TODO: (colin) complete implementation beyond warn when needed
+
+        public void warn(String message) {
+            logger.callMethod("warn", RubyString.newString(logger.getRuntime(), message));
+        }
+    }
+
+    @JRubyClass(name = "Event", parent = "Object")
+    public static class RubyEvent extends RubyObject {
+        private Event event;
+        private static RubyObject logger;
+
+        public RubyEvent(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+        }
+
+        public RubyEvent(Ruby runtime) {
+            this(runtime, runtime.getModule("LogStash").getClass("Event"));
+        }
+
+        public RubyEvent(Ruby runtime, Event event) {
+            this(runtime);
+            this.event = event;
+        }
+
+        public static RubyEvent newRubyEvent(Ruby runtime, Event event) {
+            return new RubyEvent(runtime, event);
+        }
+
+        public Event getEvent() {
+            return event;
+        }
+
+        public void setEvent(Event event) {
+            this.event = event;
+        }
+
+        // def initialize(data = {})
+        @JRubyMethod(name = "initialize", optional = 1)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
+        {
+            args = Arity.scanArgs(context.runtime, args, 0, 1);
+            IRubyObject data = args[0];
+
+            if (data == null || data.isNil()) {
+                this.event = new Event();
+            } else if (data instanceof RubyHash) {
+                this.event = new Event(Javafier.deep((RubyHash) data));
+            } else if (data instanceof MapJavaProxy) {
+                this.event = new Event((Map)((MapJavaProxy)data).getObject());
+            } else {
+                throw context.runtime.newTypeError("wrong argument type " + data.getMetaClass() + " (expected Hash)");
+            }
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "get", required = 1)
+        public IRubyObject ruby_get_field(ThreadContext context, RubyString reference)
+        {
+            Object value = this.event.getField(reference.asJavaString());
+            return Rubyfier.deep(context.runtime, value);
+        }
+
+        @JRubyMethod(name = "set", required = 2)
+        public IRubyObject ruby_set_field(ThreadContext context, RubyString reference, IRubyObject value)
+        {
+            String r = reference.asJavaString();
+
+            if (PathCache.getInstance().isTimestamp(r)) {
+                if (!(value instanceof JrubyTimestampExtLibrary.RubyTimestamp)) {
+                    throw context.runtime.newTypeError("wrong argument type " + value.getMetaClass() + " (expected LogStash::Timestamp)");
+                }
+                this.event.setTimestamp(((JrubyTimestampExtLibrary.RubyTimestamp)value).getTimestamp());
+            } else {
+                this.event.setField(r, Javafier.deep(value));
+            }
+            return value;
+        }
+
+        @JRubyMethod(name = "cancel")
+        public IRubyObject ruby_cancel(ThreadContext context)
+        {
+            this.event.cancel();
+            return RubyBoolean.createTrueClass(context.runtime);
+        }
+
+        @JRubyMethod(name = "uncancel")
+        public IRubyObject ruby_uncancel(ThreadContext context)
+        {
+            this.event.uncancel();
+            return RubyBoolean.createFalseClass(context.runtime);
+        }
+
+        @JRubyMethod(name = "cancelled?")
+        public IRubyObject ruby_cancelled(ThreadContext context)
+        {
+            return RubyBoolean.newBoolean(context.runtime, this.event.isCancelled());
+        }
+
+        @JRubyMethod(name = "include?", required = 1)
+        public IRubyObject ruby_includes(ThreadContext context, RubyString reference)
+        {
+            return RubyBoolean.newBoolean(context.runtime, this.event.includes(reference.asJavaString()));
+        }
+
+        @JRubyMethod(name = "remove", required = 1)
+        public IRubyObject ruby_remove(ThreadContext context, RubyString reference)
+        {
+            return Rubyfier.deep(context.runtime, this.event.remove(reference.asJavaString()));
+        }
+
+        @JRubyMethod(name = "clone")
+        public IRubyObject ruby_clone(ThreadContext context)
+        {
+            try {
+                return RubyEvent.newRubyEvent(context.runtime, this.event.clone());
+            } catch (CloneNotSupportedException e) {
+                throw context.runtime.newRuntimeError(e.getMessage());
+            }
+        }
+
+        @JRubyMethod(name = "overwrite", required = 1)
+        public IRubyObject ruby_overwrite(ThreadContext context, IRubyObject value)
+        {
+            if (!(value instanceof RubyEvent)) {
+                throw context.runtime.newTypeError("wrong argument type " + value.getMetaClass() + " (expected LogStash::Event)");
+            }
+
+            return RubyEvent.newRubyEvent(context.runtime, this.event.overwrite(((RubyEvent) value).event));
+        }
+
+        @JRubyMethod(name = "append", required = 1)
+        public IRubyObject ruby_append(ThreadContext context, IRubyObject value)
+        {
+            if (!(value instanceof RubyEvent)) {
+                throw context.runtime.newTypeError("wrong argument type " + value.getMetaClass() + " (expected LogStash::Event)");
+            }
+
+            this.event.append(((RubyEvent) value).getEvent());
+
+            return this;
+        }
+
+        @JRubyMethod(name = "sprintf", required = 1)
+        public IRubyObject ruby_sprintf(ThreadContext context, IRubyObject format) throws IOException {
+            try {
+                return RubyString.newString(context.runtime, event.sprintf(format.toString()));
+            } catch (IOException e) {
+                throw new RaiseException(getRuntime(), LOGSTASH_ERROR, "timestamp field is missing", true);
+            }
+        }
+
+        @JRubyMethod(name = "to_s")
+        public IRubyObject ruby_to_s(ThreadContext context)
+        {
+            return RubyString.newString(context.runtime, event.toString());
+        }
+
+        @JRubyMethod(name = "to_hash")
+        public IRubyObject ruby_to_hash(ThreadContext context) throws IOException
+        {
+            return Rubyfier.deep(context.runtime, this.event.getData());
+        }
+
+        @JRubyMethod(name = "to_hash_with_metadata")
+        public IRubyObject ruby_to_hash_with_metadata(ThreadContext context) throws IOException
+        {
+            Map data = this.event.toMap();
+            Map metadata = this.event.getMetadata();
+
+            if (!metadata.isEmpty()) {
+                data.put(Event.METADATA, metadata);
+            }
+            return Rubyfier.deep(context.runtime, data);
+        }
+
+        @JRubyMethod(name = "to_java")
+        public IRubyObject ruby_to_java(ThreadContext context)
+        {
+            return JavaUtil.convertJavaToUsableRubyObject(context.runtime, this.event);
+        }
+
+        @JRubyMethod(name = "to_json", rest = true)
+        public IRubyObject ruby_to_json(ThreadContext context, IRubyObject[] args)
+        {
+            try {
+                return RubyString.newString(context.runtime, event.toJson());
+            } catch (Exception e) {
+                throw new RaiseException(context.runtime, GENERATOR_ERROR, e.getMessage(), true);
+            }
+        }
+
+        // @param value [String] the json string. A json object/map will convert to an array containing a single Event.
+        // and a json array will convert each element into individual Event
+        // @return Array<Event> array of events
+        @JRubyMethod(name = "from_json", required = 1, meta = true)
+        public static IRubyObject ruby_from_json(ThreadContext context, IRubyObject recv, RubyString value)
+        {
+            Event[] events;
+            try {
+                events = Event.fromJson(value.asJavaString());
+            } catch (Exception e) {
+                throw new RaiseException(context.runtime, PARSER_ERROR, e.getMessage(), true);
+            }
+
+            RubyArray result = RubyArray.newArray(context.runtime, events.length);
+
+            if (events.length == 1) {
+                // micro optimization for the 1 event more common use-case.
+                result.set(0, RubyEvent.newRubyEvent(context.runtime, events[0]));
+            } else {
+                for (int i = 0; i < events.length; i++) {
+                    result.set(i, RubyEvent.newRubyEvent(context.runtime, events[i]));
+                }
+            }
+            return result;
+        }
+
+        @JRubyMethod(name = "validate_value", required = 1, meta = true)
+        public static IRubyObject ruby_validate_value(ThreadContext context, IRubyObject recv, IRubyObject value)
+        {
+            // TODO: add UTF-8 validation
+            return value;
+        }
+
+        @JRubyMethod(name = "tag", required = 1)
+        public IRubyObject ruby_tag(ThreadContext context, RubyString value)
+        {
+            this.event.tag(((RubyString) value).asJavaString());
+            return context.runtime.getNil();
+        }
+
+        @JRubyMethod(name = "timestamp")
+        public IRubyObject ruby_timestamp(ThreadContext context) throws IOException {
+            return new JrubyTimestampExtLibrary.RubyTimestamp(context.getRuntime(), this.event.getTimestamp());
+        }
+
+        @JRubyMethod(name = "timestamp=", required = 1)
+        public IRubyObject ruby_set_timestamp(ThreadContext context, IRubyObject value)
+        {
+            if (!(value instanceof JrubyTimestampExtLibrary.RubyTimestamp)) {
+                throw context.runtime.newTypeError("wrong argument type " + value.getMetaClass() + " (expected LogStash::Timestamp)");
+            }
+            this.event.setTimestamp(((JrubyTimestampExtLibrary.RubyTimestamp)value).getTimestamp());
+            return value;
+        }
+
+        // set a new logger for all Event instances
+        // there is no point in changing it at runtime for other reasons than in tests/specs.
+        @JRubyMethod(name = "logger=", required = 1, meta = true)
+        public static IRubyObject ruby_set_logger(ThreadContext context, IRubyObject recv, IRubyObject value)
+        {
+            Event.setLogger(new ProxyLogger((RubyObject)value));
+            return value;
+        }
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java b/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java
new file mode 100644
index 00000000000..9748a815ccb
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java
@@ -0,0 +1,246 @@
+package com.logstash.ext;
+
+import com.logstash.*;
+import org.jruby.*;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.exceptions.RaiseException;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.jruby.javasupport.JavaUtil;
+import org.jruby.runtime.Arity;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+
+import java.io.IOException;
+
+public class JrubyTimestampExtLibrary implements Library {
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+        RubyClass clazz = runtime.defineClassUnder("Timestamp", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyTimestamp(runtime, rubyClass);
+            }
+        }, module);
+        clazz.defineAnnotatedMethods(RubyTimestamp.class);
+    }
+
+    @JRubyClass(name = "Timestamp", parent = "Object")
+    public static class RubyTimestamp extends RubyObject {
+
+        private Timestamp timestamp;
+
+        public RubyTimestamp(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+        }
+
+        public RubyTimestamp(Ruby runtime, RubyClass klass, Timestamp timestamp) {
+            this(runtime, klass);
+            this.timestamp = timestamp;
+        }
+
+        public RubyTimestamp(Ruby runtime, Timestamp timestamp) {
+            this(runtime, runtime.getModule("LogStash").getClass("Timestamp"), timestamp);
+        }
+
+        public RubyTimestamp(Ruby runtime) {
+            this(runtime, new Timestamp());
+        }
+
+        public static RubyTimestamp newRubyTimestamp(Ruby runtime) {
+            return new RubyTimestamp(runtime);
+        }
+
+        public static RubyTimestamp newRubyTimestamp(Ruby runtime, long epoch) {
+            // Ruby epoch is in seconds, Java in milliseconds
+            return new RubyTimestamp(runtime, new Timestamp(epoch * 1000));
+        }
+
+        public static RubyTimestamp newRubyTimestamp(Ruby runtime, Timestamp timestamp) {
+            return new RubyTimestamp(runtime, timestamp);
+        }
+
+        public Timestamp getTimestamp() {
+            return timestamp;
+        }
+
+        public void setTimestamp(Timestamp timestamp) {
+            this.timestamp = timestamp;
+        }
+
+        // def initialize(time = Time.new)
+        @JRubyMethod(name = "initialize", optional = 1)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
+        {
+            args = Arity.scanArgs(context.runtime, args, 0, 1);
+            IRubyObject time = args[0];
+
+            if (time.isNil()) {
+                this.timestamp = new Timestamp();
+            } else if (time instanceof RubyTime) {
+                this.timestamp = new Timestamp(((RubyTime)time).getDateTime());
+            } else if (time instanceof RubyString) {
+                try {
+                    this.timestamp = new Timestamp(((RubyString) time).toString());
+                } catch (IllegalArgumentException e) {
+                    throw new RaiseException(
+                            getRuntime(),
+                            getRuntime().getModule("LogStash").getClass("TimestampParserError"),
+                            "invalid timestamp string format " + time,
+                            true
+                    );
+
+                }
+            } else {
+                throw context.runtime.newTypeError("wrong argument type " + time.getMetaClass() + " (expected Time)");
+            }
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "time")
+        public IRubyObject ruby_time(ThreadContext context)
+        {
+            return RubyTime.newTime(context.runtime, this.timestamp.getTime());
+        }
+
+        @JRubyMethod(name = "to_i")
+        public IRubyObject ruby_to_i(ThreadContext context)
+        {
+            return RubyFixnum.newFixnum(context.runtime, this.timestamp.getTime().getMillis() / 1000);
+        }
+
+        @JRubyMethod(name = "to_f")
+        public IRubyObject ruby_to_f(ThreadContext context)
+        {
+            return RubyFloat.newFloat(context.runtime, this.timestamp.getTime().getMillis() / 1000.0d);
+        }
+
+        @JRubyMethod(name = "to_s")
+        public IRubyObject ruby_to_s(ThreadContext context)
+        {
+            return ruby_to_iso8601(context);
+        }
+
+        @JRubyMethod(name = "to_iso8601")
+        public IRubyObject ruby_to_iso8601(ThreadContext context)
+        {
+            return RubyString.newString(context.runtime, this.timestamp.toIso8601());
+        }
+
+        @JRubyMethod(name = "to_java")
+        public IRubyObject ruby_to_java(ThreadContext context)
+        {
+            return JavaUtil.convertJavaToUsableRubyObject(context.runtime, this.timestamp);
+        }
+
+        @JRubyMethod(name = "to_json", rest = true)
+        public IRubyObject ruby_to_json(ThreadContext context, IRubyObject[] args)
+        {
+            return RubyString.newString(context.runtime,  "\"" + this.timestamp.toIso8601() + "\"");
+        }
+
+        public static Timestamp newTimestamp(IRubyObject time)
+        {
+            if (time.isNil()) {
+                return new Timestamp();
+            } else if (time instanceof RubyTime) {
+                return new Timestamp(((RubyTime)time).getDateTime());
+            } else if (time instanceof RubyString) {
+                return new Timestamp(((RubyString) time).toString());
+            } else if (time instanceof RubyTimestamp) {
+                return new Timestamp(((RubyTimestamp) time).timestamp);
+            } else {
+               return null;
+            }
+        }
+
+
+        @JRubyMethod(name = "coerce", required = 1, meta = true)
+        public static IRubyObject ruby_coerce(ThreadContext context, IRubyObject recv, IRubyObject time)
+        {
+            try {
+                Timestamp ts = newTimestamp(time);
+                return (ts == null) ? context.runtime.getNil() : RubyTimestamp.newRubyTimestamp(context.runtime, ts);
+             } catch (IllegalArgumentException e) {
+                throw new RaiseException(
+                        context.runtime,
+                        context.runtime.getModule("LogStash").getClass("TimestampParserError"),
+                        "invalid timestamp format " + e.getMessage(),
+                        true
+                );
+
+            }
+         }
+
+        @JRubyMethod(name = "parse_iso8601", required = 1, meta = true)
+        public static IRubyObject ruby_parse_iso8601(ThreadContext context, IRubyObject recv, IRubyObject time)
+        {
+            if (time instanceof RubyString) {
+                try {
+                    return RubyTimestamp.newRubyTimestamp(context.runtime, newTimestamp(time));
+                } catch (IllegalArgumentException e) {
+                    throw new RaiseException(
+                            context.runtime,
+                            context.runtime.getModule("LogStash").getClass("TimestampParserError"),
+                            "invalid timestamp format " + e.getMessage(),
+                            true
+                    );
+
+                }
+            } else {
+                throw context.runtime.newTypeError("wrong argument type " + time.getMetaClass() + " (expected String)");
+            }
+        }
+
+        @JRubyMethod(name = "at", required = 1, optional = 1, meta = true)
+        public static IRubyObject ruby_at(ThreadContext context, IRubyObject recv, IRubyObject[] args)
+        {
+            RubyTime t;
+            if (args.length == 1) {
+                IRubyObject epoch = args[0];
+
+                if (epoch instanceof RubyBigDecimal) {
+                    // bug in JRuby prevents correcly parsing a BigDecimal fractional part, see https://github.com/elastic/logstash/issues/4565
+                    double usec = ((RubyBigDecimal)epoch).frac().convertToFloat().getDoubleValue() * 1000000;
+                    t = (RubyTime)RubyTime.at(context, context.runtime.getTime(), ((RubyBigDecimal)epoch).to_int(), new RubyFloat(context.runtime, usec));
+                } else {
+                    t = (RubyTime)RubyTime.at(context, context.runtime.getTime(), epoch);
+                }
+            } else {
+                t = (RubyTime)RubyTime.at(context, context.runtime.getTime(), args[0], args[1]);
+            }
+            return RubyTimestamp.newRubyTimestamp(context.runtime, new Timestamp(t.getDateTime()));
+        }
+
+        @JRubyMethod(name = "now", meta = true)
+        public static IRubyObject ruby_now(ThreadContext context, IRubyObject recv)
+        {
+            return RubyTimestamp.newRubyTimestamp(context.runtime);
+        }
+
+        @JRubyMethod(name = "utc")
+        public IRubyObject ruby_utc(ThreadContext context)
+        {
+            return this;
+        }
+
+        @JRubyMethod(name = "gmtime")
+        public IRubyObject ruby_gmtime(ThreadContext context)
+        {
+            return this;
+        }
+
+        @JRubyMethod(name = {"usec", "tv_usec"})
+        public IRubyObject ruby_usec(ThreadContext context)
+        {
+            return RubyFixnum.newFixnum(context.runtime, this.timestamp.usec());
+        }
+
+        @JRubyMethod(name = "year")
+        public IRubyObject ruby_year(ThreadContext context)
+        {
+            return RubyFixnum.newFixnum(context.runtime, this.timestamp.getTime().getYear());
+        }
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java b/logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java
new file mode 100644
index 00000000000..4e7192c70b4
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java
@@ -0,0 +1,209 @@
+package com.logstash;
+
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+public class AccessorsTest {
+
+    public class TestableAccessors extends Accessors {
+
+        public TestableAccessors(Map data) {
+            super(data);
+        }
+
+        public Map<String, Object> getLut() {
+            return lut;
+        }
+
+        public Object lutGet(String reference) {
+            return this.lut.get(reference);
+        }
+    }
+
+    @Test
+    public void testBareGet() throws Exception {
+        Map data = new HashMap();
+        data.put("foo", "bar");
+        String reference = "foo";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), "bar");
+        assertEquals(accessors.lutGet(reference), data);
+    }
+
+    @Test
+    public void testAbsentBareGet() throws Exception {
+        Map data = new HashMap();
+        data.put("foo", "bar");
+        String reference = "baz";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), null);
+        assertEquals(accessors.lutGet(reference), data);
+    }
+
+    @Test
+    public void testBareBracketsGet() throws Exception {
+        Map data = new HashMap();
+        data.put("foo", "bar");
+        String reference = "[foo]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), "bar");
+        assertEquals(accessors.lutGet(reference), data);
+    }
+
+    @Test
+    public void testDeepMapGet() throws Exception {
+        Map data = new HashMap();
+        Map inner = new HashMap();
+        data.put("foo", inner);
+        inner.put("bar", "baz");
+
+        String reference = "[foo][bar]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), "baz");
+        assertEquals(accessors.lutGet(reference), inner);
+    }
+
+    @Test
+    public void testAbsentDeepMapGet() throws Exception {
+        Map data = new HashMap();
+        Map inner = new HashMap();
+        data.put("foo", inner);
+        inner.put("bar", "baz");
+
+        String reference = "[foo][foo]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), null);
+        assertEquals(accessors.lutGet(reference), inner);
+    }
+
+    @Test
+    public void testDeepListGet() throws Exception {
+        Map data = new HashMap();
+        List inner = new ArrayList();
+        data.put("foo", inner);
+        inner.add("bar");
+
+        String reference = "[foo][0]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), "bar");
+        assertEquals(accessors.lutGet(reference), inner);
+    }
+
+    @Test
+    public void testAbsentDeepListGet() throws Exception {
+        Map data = new HashMap();
+        List inner = new ArrayList();
+        data.put("foo", inner);
+        inner.add("bar");
+
+        String reference = "[foo][1]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), null);
+        assertEquals(accessors.lutGet(reference), inner);
+    }
+
+    @Test
+    public void testBarePut() throws Exception {
+        Map data = new HashMap();
+        String reference = "foo";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.set(reference, "bar"), "bar");
+        assertEquals(accessors.lutGet(reference), data);
+        assertEquals(accessors.get(reference), "bar");
+    }
+
+    @Test
+    public void testBareBracketsPut() throws Exception {
+        Map data = new HashMap();
+        String reference = "[foo]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.set(reference, "bar"), "bar");
+        assertEquals(accessors.lutGet(reference), data);
+        assertEquals(accessors.get(reference), "bar");
+    }
+
+    @Test
+    public void testDeepMapSet() throws Exception {
+        Map data = new HashMap();
+
+        String reference = "[foo][bar]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.set(reference, "baz"), "baz");
+        assertEquals(accessors.lutGet(reference), data.get("foo"));
+        assertEquals(accessors.get(reference), "baz");
+    }
+
+    @Test
+    public void testDel() throws Exception {
+        Map data = new HashMap();
+        List inner = new ArrayList();
+        data.put("foo", inner);
+        inner.add("bar");
+        data.put("bar", "baz");
+        TestableAccessors accessors = new TestableAccessors(data);
+
+        assertEquals(accessors.del("[foo][0]"), "bar");
+        assertEquals(accessors.del("[foo][0]"), null);
+        assertEquals(accessors.get("[foo]"), new ArrayList<>());
+        assertEquals(accessors.del("[bar]"), "baz");
+        assertEquals(accessors.get("[bar]"), null);
+    }
+
+    @Test
+    public void testNilInclude() throws Exception {
+        Map data = new HashMap();
+        data.put("nilfield", null);
+        TestableAccessors accessors = new TestableAccessors(data);
+
+        assertEquals(accessors.includes("nilfield"), true);
+    }
+
+    @Test
+    public void testInvalidPath() throws Exception {
+        Map data = new HashMap();
+        Accessors accessors = new Accessors(data);
+
+        assertEquals(accessors.set("[foo]", 1), 1);
+        assertEquals(accessors.get("[foo][bar]"), null);
+    }
+
+    @Test
+    public void testStaleTargetCache() throws Exception {
+        Map data = new HashMap();
+
+        Accessors accessors = new Accessors(data);
+
+        assertEquals(accessors.get("[foo][bar]"), null);
+        assertEquals(accessors.set("[foo][bar]", "baz"), "baz");
+        assertEquals(accessors.get("[foo][bar]"), "baz");
+
+        assertEquals(accessors.set("[foo]", "boom"), "boom");
+        assertEquals(accessors.get("[foo][bar]"), null);
+        assertEquals(accessors.get("[foo]"), "boom");
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/EventTest.java b/logstash-core-event-java/src/test/java/com/logstash/EventTest.java
new file mode 100644
index 00000000000..61bb4bb7a58
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/EventTest.java
@@ -0,0 +1,199 @@
+package com.logstash;
+
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.*;
+
+import static org.junit.Assert.*;
+import static net.javacrumbs.jsonunit.JsonAssert.assertJsonEquals;
+
+public class EventTest {
+    @Test
+    public void testBareToJson() throws Exception {
+        Event e = new Event();
+        assertJsonEquals("{\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"@version\":\"1\"}", e.toJson());
+    }
+
+    @Test
+    public void testSimpleStringFieldToJson() throws Exception {
+        Map data = new HashMap();
+        data.put("foo", "bar");
+        Event e = new Event(data);
+        assertJsonEquals("{\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"foo\":\"bar\",\"@version\":\"1\"}", e.toJson());
+    }
+
+    @Test
+    public void testSimpleIntegerFieldToJson() throws Exception {
+        Map data = new HashMap();
+        data.put("foo", 1);
+        Event e = new Event(data);
+        assertJsonEquals("{\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"foo\":1,\"@version\":\"1\"}", e.toJson());
+    }
+
+    @Test
+    public void testSimpleDecimalFieldToJson() throws Exception {
+        Map data = new HashMap();
+        data.put("foo", 1.0);
+        Event e = new Event(data);
+        assertJsonEquals("{\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"foo\":1.0,\"@version\":\"1\"}", e.toJson());
+    }
+
+    @Test
+    public void testSimpleMultipleFieldToJson() throws Exception {
+        Map data = new HashMap();
+        data.put("foo", 1.0);
+        data.put("bar", "bar");
+        data.put("baz", 1);
+        Event e = new Event(data);
+        assertJsonEquals("{\"bar\":\"bar\",\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"foo\":1.0,\"@version\":\"1\",\"baz\":1}", e.toJson());
+    }
+
+    @Test
+    public void testDeepMapFieldToJson() throws Exception {
+        Event e = new Event();
+        e.setField("[foo][bar][baz]", 1);
+        assertJsonEquals("{\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"foo\":{\"bar\":{\"baz\":1}},\"@version\":\"1\"}", e.toJson());
+
+        e = new Event();
+        e.setField("[foo][0][baz]", 1);
+        assertJsonEquals("{\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"foo\":{\"0\":{\"baz\":1}},\"@version\":\"1\"}", e.toJson());
+    }
+
+    @Test
+    public void testGetFieldList() throws Exception {
+        Map data = new HashMap();
+        List l = new ArrayList();
+        data.put("foo", l);
+        l.add(1);
+        Event e = new Event(data);
+        assertEquals(1, e.getField("[foo][0]"));
+    }
+
+    @Test
+    public void testDeepGetField() throws Exception {
+        Map data = new HashMap();
+        List l = new ArrayList();
+        data.put("foo", l);
+        Map m = new HashMap();
+        m.put("bar", "baz");
+        l.add(m);
+        Event e = new Event(data);
+        assertEquals("baz", e.getField("[foo][0][bar]"));
+    }
+
+
+    @Test
+    public void testClone() throws Exception {
+        Map data = new HashMap();
+        List l = new ArrayList();
+        data.put("array", l);
+
+        Map m = new HashMap();
+        m.put("foo", "bar");
+        l.add(m);
+
+        data.put("foo", 1.0);
+        data.put("bar", "bar");
+        data.put("baz", 1);
+
+        Event e = new Event(data);
+
+        Event f = e.clone();
+
+        assertJsonEquals("{\"bar\":\"bar\",\"@timestamp\":\"" + e.getTimestamp().toIso8601() + "\",\"array\":[{\"foo\":\"bar\"}],\"foo\":1.0,\"@version\":\"1\",\"baz\":1}", f.toJson());
+        assertJsonEquals(f.toJson(), e.toJson());
+    }
+
+    @Test
+    public void testToMap() throws Exception {
+        Event e = new Event();
+        Map original = e.getData();
+        Map clone = e.toMap();
+        assertFalse(original == clone);
+        assertEquals(original, clone);
+    }
+
+    @Test
+    public void testAppend() throws Exception {
+        Map data1 = new HashMap();
+        data1.put("field1", Arrays.asList("original1", "original2"));
+
+        Map data2 = new HashMap();
+        data2.put("field1", "original1");
+
+        Event e = new Event(data1);
+        Event e2 = new Event(data2);
+        e.append(e2);
+
+        assertEquals(Arrays.asList("original1", "original2"), e.getField("field1"));
+    }
+
+    @Test
+    public void testFromJsonWithNull() throws Exception {
+        Event[] events = Event.fromJson(null);
+        assertEquals(0, events.length);
+    }
+
+    @Test
+    public void testFromJsonWithEmptyString() throws Exception {
+        Event[] events = Event.fromJson("");
+        assertEquals(0, events.length);
+    }
+
+    @Test
+    public void testFromJsonWithBlankString() throws Exception {
+        Event[] events = Event.fromJson("   ");
+        assertEquals(0, events.length);
+    }
+
+    @Test
+    public void testFromJsonWithValidJsonMap() throws Exception {
+        Event e = Event.fromJson("{\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"foo\":\"bar\"}")[0];
+
+        assertEquals("bar", e.getField("[foo]"));
+        assertEquals("2015-05-28T23:02:05.350Z", e.getTimestamp().toIso8601());
+    }
+
+    @Test
+    public void testFromJsonWithValidJsonArrayOfMap() throws Exception {
+        Event[] l = Event.fromJson("[{\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"foo\":\"bar\"}]");
+
+        assertEquals(1, l.length);
+        assertEquals("bar", l[0].getField("[foo]"));
+        assertEquals("2015-05-28T23:02:05.350Z", l[0].getTimestamp().toIso8601());
+
+        l = Event.fromJson("[{}]");
+
+        assertEquals(1, l.length);
+        assertEquals(null, l[0].getField("[foo]"));
+
+        l = Event.fromJson("[{\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"foo\":\"bar\"}, {\"@timestamp\":\"2016-05-28T23:02:05.350Z\",\"foo\":\"baz\"}]");
+
+        assertEquals(2, l.length);
+        assertEquals("bar", l[0].getField("[foo]"));
+        assertEquals("2015-05-28T23:02:05.350Z", l[0].getTimestamp().toIso8601());
+        assertEquals("baz", l[1].getField("[foo]"));
+        assertEquals("2016-05-28T23:02:05.350Z", l[1].getTimestamp().toIso8601());
+    }
+
+    @Test(expected=IOException.class)
+    public void testFromJsonWithInvalidJsonString() throws Exception {
+        Event.fromJson("gabeutch");
+    }
+
+    @Test(expected=IOException.class)
+    public void testFromJsonWithInvalidJsonArray1() throws Exception {
+        Event.fromJson("[1,2]");
+    }
+
+    @Test(expected=IOException.class)
+    public void testFromJsonWithInvalidJsonArray2() throws Exception {
+        Event.fromJson("[\"gabeutch\"]");
+    }
+
+    @Test(expected=IOException.class)
+    public void testFromJsonWithPartialInvalidJsonArray() throws Exception {
+        Event.fromJson("[{\"foo\":\"bar\"}, 1]");
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java b/logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java
new file mode 100644
index 00000000000..73f04e3a7c4
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java
@@ -0,0 +1,40 @@
+package com.logstash;
+
+import org.junit.Test;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+
+import static org.junit.Assert.*;
+
+public class FieldReferenceTest {
+
+    @Test
+    public void testParseSingleBareField() throws Exception {
+        FieldReference f = FieldReference.parse("foo");
+        assertTrue(f.getPath().isEmpty());
+        assertEquals(f.getKey(), "foo");
+    }
+
+    @Test
+    public void testParseSingleFieldPath() throws Exception {
+        FieldReference f = FieldReference.parse("[foo]");
+        assertTrue(f.getPath().isEmpty());
+        assertEquals(f.getKey(), "foo");
+    }
+
+    @Test
+    public void testParse2FieldsPath() throws Exception {
+        FieldReference f = FieldReference.parse("[foo][bar]");
+        assertArrayEquals(f.getPath().toArray(), new String[]{"foo"});
+        assertEquals(f.getKey(), "bar");
+    }
+
+    @Test
+    public void testParse3FieldsPath() throws Exception {
+        FieldReference f = FieldReference.parse("[foo][bar]]baz]");
+        assertArrayEquals(f.getPath().toArray(), new String[]{"foo", "bar"});
+        assertEquals(f.getKey(), "baz");
+    }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/test/java/com/logstash/JavafierTest.java b/logstash-core-event-java/src/test/java/com/logstash/JavafierTest.java
new file mode 100644
index 00000000000..40aaa7b6c53
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/JavafierTest.java
@@ -0,0 +1,18 @@
+package com.logstash;
+
+import org.jruby.Ruby;
+import org.jruby.RubyBignum;
+import java.math.BigInteger;
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+public class JavafierTest {
+    @Test
+    public void testRubyBignum() {
+        RubyBignum v = RubyBignum.newBignum(Ruby.getGlobalRuntime(), "-9223372036854776000");
+
+        Object result = Javafier.deep(v);
+        assertEquals(BigInteger.class, result.getClass());
+        assertEquals( "-9223372036854776000", result.toString());
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java b/logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java
new file mode 100644
index 00000000000..23cb27ac997
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java
@@ -0,0 +1,47 @@
+package com.logstash;
+
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+
+import static org.junit.Assert.assertEquals;
+
+public class KeyNodeTest {
+
+    @Test
+    public void testNoElementJoin() throws IOException {
+        assertEquals("", KeyNode.join(new ArrayList(), ","));
+    }
+
+    @Test
+    public void testOneElementJoin() throws IOException {
+        assertEquals("foo", KeyNode.join(Arrays.asList("foo"), ","));
+    }
+
+    @Test
+    public void testOneNullElementJoin() throws IOException {
+        assertEquals("", KeyNode.join(Arrays.asList(new Object[] { null }), ","));
+    }
+
+    @Test
+    public void testTwoElementJoin() throws IOException {
+        assertEquals("foo,bar", KeyNode.join(Arrays.asList("foo", "bar"), ","));
+    }
+
+    @Test
+    public void testTwoElementWithLeadingNullJoin() throws IOException {
+        assertEquals(",foo", KeyNode.join(Arrays.asList(null, "foo"), ","));
+    }
+
+    @Test
+    public void testTwoElementWithTailingNullJoin() throws IOException {
+        assertEquals("foo,", KeyNode.join(Arrays.asList("foo", null), ","));
+    }
+
+    @Test
+    public void testListInListJoin() throws IOException {
+        assertEquals("foo,bar,", KeyNode.join(Arrays.asList("foo", Arrays.asList("bar", null)), ","));
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java b/logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java
new file mode 100644
index 00000000000..5773ce65ae5
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java
@@ -0,0 +1,230 @@
+package com.logstash;
+
+import org.jruby.*;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.jruby.javasupport.JavaUtil;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.junit.Test;
+
+import java.lang.reflect.Method;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static org.junit.Assert.*;
+
+public class RubyfierTest {
+
+    @Test
+    public void testDeepWithString() {
+        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), "foo");
+        assertEquals(RubyString.class, result.getClass());
+        assertEquals("foo", result.toString());
+    }
+
+    @Test
+    public void testDeepMapWithString()
+            throws Exception
+    {
+        Map data = new HashMap();
+        data.put("foo", "bar");
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // Hack to be able to retrieve the original, unconverted Ruby object from Map
+        // it seems the only method providing this is internalGet but it is declared protected.
+        // I know this is bad practice but I think this is practically acceptable.
+        Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
+        internalGet.setAccessible(true);
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+
+        assertEquals(RubyString.class, result.getClass());
+        assertEquals("bar", result.toString());
+    }
+
+    @Test
+    public void testDeepListWithString()
+            throws Exception
+    {
+        List data = new ArrayList();
+        data.add("foo");
+
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // toJavaArray does not convert inner elemenst to Java types \o/
+        assertEquals(RubyString.class, rubyArray.toJavaArray()[0].getClass());
+        assertEquals("foo", rubyArray.toJavaArray()[0].toString());
+    }
+
+    @Test
+    public void testDeepWithInteger() {
+        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1);
+        assertEquals(RubyFixnum.class, result.getClass());
+        assertEquals(1L, ((RubyFixnum)result).getLongValue());
+    }
+
+    @Test
+    public void testDeepMapWithInteger()
+            throws Exception
+    {
+        Map data = new HashMap();
+        data.put("foo", 1);
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // Hack to be able to retrieve the original, unconverted Ruby object from Map
+        // it seems the only method providing this is internalGet but it is declared protected.
+        // I know this is bad practice but I think this is practically acceptable.
+        Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
+        internalGet.setAccessible(true);
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+
+        assertEquals(RubyFixnum.class, result.getClass());
+        assertEquals(1L, ((RubyFixnum)result).getLongValue());
+    }
+
+    @Test
+    public void testDeepListWithInteger()
+            throws Exception
+    {
+        List data = new ArrayList();
+        data.add(1);
+
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // toJavaArray does not convert inner elemenst to Java types \o/
+        assertEquals(RubyFixnum.class, rubyArray.toJavaArray()[0].getClass());
+        assertEquals(1L, ((RubyFixnum)rubyArray.toJavaArray()[0]).getLongValue());
+    }
+
+    @Test
+    public void testDeepWithFloat() {
+        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1.0F);
+        assertEquals(RubyFloat.class, result.getClass());
+        assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepMapWithFloat()
+            throws Exception
+    {
+        Map data = new HashMap();
+        data.put("foo", 1.0F);
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // Hack to be able to retrieve the original, unconverted Ruby object from Map
+        // it seems the only method providing this is internalGet but it is declared protected.
+        // I know this is bad practice but I think this is practically acceptable.
+        Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
+        internalGet.setAccessible(true);
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+
+        assertEquals(RubyFloat.class, result.getClass());
+        assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepListWithFloat()
+            throws Exception
+    {
+        List data = new ArrayList();
+        data.add(1.0F);
+
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // toJavaArray does not convert inner elemenst to Java types \o/
+        assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
+        assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepWithDouble() {
+        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1.0D);
+        assertEquals(RubyFloat.class, result.getClass());
+        assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepMapWithDouble()
+            throws Exception
+    {
+        Map data = new HashMap();
+        data.put("foo", 1.0D);
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // Hack to be able to retrieve the original, unconverted Ruby object from Map
+        // it seems the only method providing this is internalGet but it is declared protected.
+        // I know this is bad practice but I think this is practically acceptable.
+        Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
+        internalGet.setAccessible(true);
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+
+        assertEquals(RubyFloat.class, result.getClass());
+        assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepListWithDouble()
+            throws Exception
+    {
+        List data = new ArrayList();
+        data.add(1.0D);
+
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // toJavaArray does not convert inner elemenst to Java types \o/
+        assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
+        assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepWithBigDecimal() {
+        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), new BigDecimal(1));
+        assertEquals(RubyBigDecimal.class, result.getClass());
+        assertEquals(1.0D, ((RubyBigDecimal)result).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepMapWithBigDecimal()
+            throws Exception
+    {
+        Map data = new HashMap();
+        data.put("foo", new BigDecimal(1));
+
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // Hack to be able to retrieve the original, unconverted Ruby object from Map
+        // it seems the only method providing this is internalGet but it is declared protected.
+        // I know this is bad practice but I think this is practically acceptable.
+        Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
+        internalGet.setAccessible(true);
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+
+        assertEquals(RubyBigDecimal.class, result.getClass());
+        assertEquals(1.0D, ((RubyBigDecimal)result).getDoubleValue(), 0);
+    }
+
+    @Test
+    public void testDeepListWithBigDecimal()
+            throws Exception
+    {
+        List data = new ArrayList();
+        data.add(new BigDecimal(1));
+
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+
+        // toJavaArray does not convert inner elemenst to Java types \o/
+        assertEquals(RubyBigDecimal.class, rubyArray.toJavaArray()[0].getClass());
+        assertEquals(1.0D, ((RubyBigDecimal)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
+    }
+
+
+    @Test
+    public void testDeepWithBigInteger() {
+        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), new BigInteger("1"));
+        assertEquals(RubyBignum.class, result.getClass());
+        assertEquals(1L, ((RubyBignum)result).getLongValue());
+    }
+
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java b/logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java
new file mode 100644
index 00000000000..52d4563db4b
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java
@@ -0,0 +1,143 @@
+package com.logstash;
+
+
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.junit.Test;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.Map;
+
+import static org.junit.Assert.*;
+
+
+public class StringInterpolationTest {
+    @Test
+    public void testCompletelyStaticTemplate() throws IOException {
+        Event event = getTestEvent();
+        String path = "/full/path/awesome";
+        StringInterpolation si = StringInterpolation.getInstance();
+
+        assertEquals(path, si.evaluate(event, path));
+    }
+
+    @Test
+    public void testOneLevelField() throws IOException {
+        Event event = getTestEvent();
+        String path = "/full/%{bar}/awesome";
+        StringInterpolation si = StringInterpolation.getInstance();
+
+        assertEquals("/full/foo/awesome", si.evaluate(event, path));
+    }
+
+    @Test
+    public void testMultipleLevelField() throws IOException {
+        Event event = getTestEvent();
+        String path = "/full/%{bar}/%{awesome}";
+        StringInterpolation si = StringInterpolation.getInstance();
+
+        assertEquals("/full/foo/logstash", si.evaluate(event, path));
+    }
+
+    @Test
+    public void testMissingKey() throws IOException {
+        Event event = getTestEvent();
+        String path = "/full/%{do-not-exist}";
+        StringInterpolation si = StringInterpolation.getInstance();
+
+        assertEquals("/full/%{do-not-exist}", si.evaluate(event, path));
+    }
+
+    @Test
+    public void testDateFormater() throws IOException {
+        Event event = getTestEvent();
+        String path = "/full/%{+YYYY}";
+        StringInterpolation si = StringInterpolation.getInstance();
+
+        assertEquals("/full/2015", si.evaluate(event, path));
+    }
+
+    @Test
+    public void TestMixDateAndFields() throws IOException {
+        Event event = getTestEvent();
+        String path = "/full/%{+YYYY}/weeee/%{bar}";
+        StringInterpolation si = StringInterpolation.getInstance();
+
+        assertEquals("/full/2015/weeee/foo", si.evaluate(event, path));
+    }
+
+    @Test
+    public void testUnclosedTag() throws IOException {
+        Event event = getTestEvent();
+        String path = "/full/%{+YYY/web";
+        StringInterpolation si = StringInterpolation.getInstance();
+
+        assertEquals("/full/%{+YYY/web", si.evaluate(event, path));
+    }
+
+    @Test
+    public void TestStringIsOneDateTag() throws IOException {
+        Event event = getTestEvent();
+        String path = "%{+YYYY}";
+        StringInterpolation si = StringInterpolation.getInstance();
+        assertEquals("2015", si.evaluate(event, path));
+    }
+
+    @Test
+    public void TestFieldRef() throws IOException {
+        Event event = getTestEvent();
+        String path = "%{[j][k1]}";
+        StringInterpolation si = StringInterpolation.getInstance();
+        assertEquals("v", si.evaluate(event, path));
+    }
+
+    @Test
+    public void TestEpoch() throws IOException {
+        Event event = getTestEvent();
+        String path = "%{+%s}";
+        StringInterpolation si = StringInterpolation.getInstance();
+        assertEquals("1443657600", si.evaluate(event, path));
+    }
+
+    @Test
+    public void TestValueIsArray() throws IOException {
+        ArrayList l = new ArrayList();
+        l.add("Hello");
+        l.add("world");
+
+        Event event = getTestEvent();
+        event.setField("message", l);
+
+        String path = "%{message}";
+        StringInterpolation si = StringInterpolation.getInstance();
+        assertEquals("Hello,world", si.evaluate(event, path));
+    }
+
+    @Test
+    public void TestValueIsHash() throws IOException {
+        Event event = getTestEvent();
+
+        String path = "%{j}";
+        StringInterpolation si = StringInterpolation.getInstance();
+        assertEquals("{\"k1\":\"v\"}", si.evaluate(event, path));
+    }
+
+    public Event getTestEvent() {
+        Map data = new HashMap();
+        Map inner = new HashMap();
+
+        inner.put("k1", "v");
+
+        data.put("bar", "foo");
+        data.put("awesome", "logstash");
+        data.put("j", inner);
+        data.put("@timestamp", new DateTime(2015, 10, 1, 0, 0, 0, DateTimeZone.UTC));
+
+
+        Event event = new Event(data);
+
+        return event;
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java b/logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java
new file mode 100644
index 00000000000..539fbe227cb
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java
@@ -0,0 +1,46 @@
+package com.logstash;
+
+import org.joda.time.DateTime;
+import org.joda.time.DateTimeZone;
+import org.junit.Test;
+import static org.junit.Assert.*;
+
+public class TimestampTest {
+
+
+    @Test
+    public void testCircularIso8601() throws Exception {
+        Timestamp t1 = new Timestamp();
+        Timestamp t2 = new Timestamp(t1.toIso8601());
+        assertEquals(t1.getTime(), t2.getTime());
+    }
+
+    @Test
+    public void testToIso8601() throws Exception {
+        Timestamp t = new Timestamp("2014-09-23T00:00:00-0800");
+        assertEquals("2014-09-23T08:00:00.000Z", t.toIso8601());
+    }
+
+    // Timestamp should always be in a UTC representation
+    @Test
+    public void testUTC() throws Exception {
+        Timestamp t;
+
+        t = new Timestamp();
+        assertEquals(DateTimeZone.UTC, t.getTime().getZone());
+
+        t = new Timestamp("2014-09-23T00:00:00-0800");
+        assertEquals(DateTimeZone.UTC, t.getTime().getZone());
+
+        t = new Timestamp("2014-09-23T08:00:00.000Z");
+        assertEquals(DateTimeZone.UTC, t.getTime().getZone());
+
+        t = new Timestamp(new Timestamp());
+        assertEquals(DateTimeZone.UTC, t.getTime().getZone());
+
+        long ms = DateTime.now(DateTimeZone.forID("EST")).getMillis();
+        t = new Timestamp(ms);
+        assertEquals(DateTimeZone.UTC, t.getTime().getZone());
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core-event/lib/logstash-core-event.rb b/logstash-core-event/lib/logstash-core-event.rb
new file mode 100644
index 00000000000..b2979326dac
--- /dev/null
+++ b/logstash-core-event/lib/logstash-core-event.rb
@@ -0,0 +1 @@
+require "logstash-core-event/logstash-core-event"
\ No newline at end of file
diff --git a/logstash-core-event/lib/logstash-core-event/logstash-core-event.rb b/logstash-core-event/lib/logstash-core-event/logstash-core-event.rb
new file mode 100644
index 00000000000..b0f773e203c
--- /dev/null
+++ b/logstash-core-event/lib/logstash-core-event/logstash-core-event.rb
@@ -0,0 +1,5 @@
+# encoding: utf-8
+module LogStash
+end
+
+require "logstash/event"
\ No newline at end of file
diff --git a/logstash-core-event/lib/logstash-core-event/version.rb b/logstash-core-event/lib/logstash-core-event/version.rb
new file mode 100644
index 00000000000..e1e0774da55
--- /dev/null
+++ b/logstash-core-event/lib/logstash-core-event/version.rb
@@ -0,0 +1,8 @@
+# encoding: utf-8
+
+# The version of logstash core event gem.
+#
+# Note to authors: this should not include dashes because 'gem' barfs if
+# you include a dash in the version string.
+
+LOGSTASH_CORE_EVENT_VERSION = "5.0.0.dev"
diff --git a/lib/logstash/event.rb b/logstash-core-event/lib/logstash/event.rb
similarity index 81%
rename from lib/logstash/event.rb
rename to logstash-core-event/lib/logstash/event.rb
index c00d5531305..7a9b7d133c9 100644
--- a/lib/logstash/event.rb
+++ b/logstash-core-event/lib/logstash/event.rb
@@ -54,6 +54,7 @@ class DeprecatedMethod < StandardError; end
   VERSION_ONE = "1"
   TIMESTAMP_FAILURE_TAG = "_timestampparsefailure"
   TIMESTAMP_FAILURE_FIELD = "_@timestamp"
+  TAGS = "tags".freeze
 
   METADATA = "@metadata".freeze
   METADATA_BRACKETS = "[#{METADATA}]".freeze
@@ -63,9 +64,9 @@ class DeprecatedMethod < StandardError; end
   MIN_FLOAT_BEFORE_SCI_NOT = 0.0001
   MAX_FLOAT_BEFORE_SCI_NOT = 1000000000000000.0
 
-  LOGGER = Cabin::Channel.get(LogStash)
+  DEFAULT_LOGGER = Cabin::Channel.get(LogStash)
+  @@logger = DEFAULT_LOGGER
 
-  public
   def initialize(data = {})
     @cancelled = false
     @data = data
@@ -76,53 +77,44 @@ def initialize(data = {})
 
     @metadata = @data.delete(METADATA) || {}
     @metadata_accessors = LogStash::Util::Accessors.new(@metadata)
-  end # def initialize
+  end
 
-  public
   def cancel
     @cancelled = true
-  end # def cancel
+  end
 
-  public
   def uncancel
     @cancelled = false
-  end # def uncancel
+  end
 
-  public
   def cancelled?
-    return @cancelled
-  end # def cancelled?
+    @cancelled
+  end
 
   # Create a deep-ish copy of this event.
-  public
   def clone
     copy = {}
     @data.each do |k,v|
       # TODO(sissel): Recurse if this is a hash/array?
       copy[k] = begin v.clone rescue v end
     end
-    return self.class.new(copy)
-  end # def clone
 
-  public
-  def to_s
-    self.sprintf("#{timestamp.to_iso8601} %{host} %{message}")
-  end # def to_s
+    self.class.new(copy)
+  end
 
-  public
-  def timestamp; return @data[TIMESTAMP]; end # def timestamp
-  def timestamp=(val); return @data[TIMESTAMP] = val; end # def timestamp=
+  def to_s
+    "#{timestamp.to_iso8601} #{self.sprintf("%{host} %{message}")}"
+  end
 
-  def unix_timestamp
-    raise DeprecatedMethod
-  end # def unix_timestamp
+  def timestamp
+    @data[TIMESTAMP]
+  end
 
-  def ruby_timestamp
-    raise DeprecatedMethod
-  end # def unix_timestamp
+  def timestamp=(val)
+    @data[TIMESTAMP] = val
+  end
 
-  public
-  def [](fieldref)
+  def get(fieldref)
     if fieldref.start_with?(METADATA_BRACKETS)
       @metadata_accessors.get(fieldref[METADATA_BRACKETS.length .. -1])
     elsif fieldref == METADATA
@@ -130,10 +122,9 @@ def [](fieldref)
     else
       @accessors.get(fieldref)
     end
-  end # def []
+  end
 
-  public
-  def []=(fieldref, value)
+  def set(fieldref, value)
     if fieldref == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
       raise TypeError, "The field '@timestamp' must be a (LogStash::Timestamp, not a #{value.class} (#{value})"
     end
@@ -145,25 +136,17 @@ def []=(fieldref, value)
     else
       @accessors.set(fieldref, value)
     end
-  end # def []=
-
-  public
-  def fields
-    raise DeprecatedMethod
   end
 
-  public
   def to_json(*args)
     # ignore arguments to respect accepted to_json method signature
     LogStash::Json.dump(@data)
-  end # def to_json
+  end
 
-  public
   def to_hash
     @data
-  end # def to_hash
+  end
 
-  public
   def overwrite(event)
     # pickup new event @data and also pickup @accessors
     # otherwise it will be pointing on previous data
@@ -176,7 +159,6 @@ def overwrite(event)
     end
   end
 
-  public
   def include?(fieldref)
     if fieldref.start_with?(METADATA_BRACKETS)
       @metadata_accessors.include?(fieldref[METADATA_BRACKETS.length .. -1])
@@ -185,24 +167,21 @@ def include?(fieldref)
     else
       @accessors.include?(fieldref)
     end
-  end # def include?
+  end
 
   # Append an event to this one.
-  public
   def append(event)
     # non-destructively merge that event with ourselves.
 
     # no need to reset @accessors here because merging will not disrupt any existing field paths
     # and if new ones are created they will be picked up.
     LogStash::Util.hash_merge(@data, event.to_hash)
-  end # append
+  end
 
-  # Remove a field or field reference. Returns the value of that field when
-  # deleted
-  public
+  # Remove a field or field reference. Returns the value of that field when deleted
   def remove(fieldref)
     @accessors.del(fieldref)
-  end # def remove
+  end
 
   # sprintf. This could use a better method name.
   # The idea is to take an event and convert it to a string based on
@@ -217,47 +196,29 @@ def remove(fieldref)
   #
   # If a %{name} value is an array, then we will join by ','
   # If a %{name} value does not exist, then no substitution occurs.
-  public
   def sprintf(format)
     LogStash::StringInterpolation.evaluate(self, format)
   end
 
   def tag(value)
     # Generalize this method for more usability
-    self["tags"] ||= []
-    self["tags"] << value unless self["tags"].include?(value)
+    tags = @accessors.get(TAGS) || []
+    tags << value unless tags.include?(value)
+    @accessors.set(TAGS, tags)
   end
 
-  private
-
-  def init_timestamp(o)
-    begin
-      timestamp = LogStash::Timestamp.coerce(o)
-      return timestamp if timestamp
-
-      LOGGER.warn("Unrecognized #{TIMESTAMP} value, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD}field", :value => o.inspect)
-    rescue LogStash::TimestampParserError => e
-      LOGGER.warn("Error parsing #{TIMESTAMP} string, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD} field", :value => o.inspect, :exception => e.message)
-    end
-
-    @data["tags"] ||= []
-    @data["tags"] << TIMESTAMP_FAILURE_TAG unless @data["tags"].include?(TIMESTAMP_FAILURE_TAG)
-    @data[TIMESTAMP_FAILURE_FIELD] = o
-
-    LogStash::Timestamp.now
-  end
-
-  public
   def to_hash_with_metadata
     @metadata.empty? ? to_hash : to_hash.merge(METADATA => @metadata)
   end
 
-  public
   def to_json_with_metadata(*args)
     # ignore arguments to respect accepted to_json method signature
     LogStash::Json.dump(to_hash_with_metadata)
-  end # def to_json
+  end
 
+  # this is used by logstash-devutils spec_helper.rb to monkey patch the Event field setter []=
+  # and add systematic encoding validation on every field set in specs.
+  # TODO: (colin) this should be moved, probably in logstash-devutils ?
   def self.validate_value(value)
     case value
     when String
@@ -272,4 +233,47 @@ def self.validate_value(value)
     end
   end
 
-end # class LogStash::Event
+  # depracated public methods
+  # TODO: (colin) since these depracated mothods are still exposed in 2.x we should remove them in 3.0
+
+  def unix_timestamp
+    raise DeprecatedMethod
+  end
+
+  def ruby_timestamp
+    raise DeprecatedMethod
+  end
+
+  def fields
+    raise DeprecatedMethod
+  end
+
+  # set a new logger for all Event instances
+  # there is no point in changing it at runtime for other reasons than in tests/specs.
+  # @param logger [Cabin::Channel] logger instance that will be used by all Event instances
+  def self.logger=(logger)
+    @@logger = logger
+  end
+
+  private
+
+  def logger
+    @@logger
+  end
+
+  def init_timestamp(o)
+    begin
+      timestamp = LogStash::Timestamp.coerce(o)
+      return timestamp if timestamp
+
+      logger.warn("Unrecognized #{TIMESTAMP} value, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD}field", :value => o.inspect)
+    rescue LogStash::TimestampParserError => e
+      logger.warn("Error parsing #{TIMESTAMP} string, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD} field", :value => o.inspect, :exception => e.message)
+    end
+
+    tag(TIMESTAMP_FAILURE_TAG)
+    @accessors.set(TIMESTAMP_FAILURE_FIELD, o)
+
+    LogStash::Timestamp.now
+  end
+end
diff --git a/lib/logstash/string_interpolation.rb b/logstash-core-event/lib/logstash/string_interpolation.rb
similarity index 88%
rename from lib/logstash/string_interpolation.rb
rename to logstash-core-event/lib/logstash/string_interpolation.rb
index fc357f67515..aaa54981165 100644
--- a/lib/logstash/string_interpolation.rb
+++ b/logstash-core-event/lib/logstash/string_interpolation.rb
@@ -4,7 +4,7 @@
 
 module LogStash
   module StringInterpolation
-    extend self 
+    extend self
 
     # Floats outside of these upper and lower bounds are forcibly converted
     # to scientific notation by Float#to_s
@@ -27,6 +27,16 @@ def evaluate(event, template)
       compiled.evaluate(event)
     end
 
+    # clear the global compiled templates cache
+    def clear_cache
+      CACHE.clear
+    end
+
+    # @return [Fixnum] the compiled templates cache size
+    def cache_size
+      CACHE.size
+    end
+
     private
     def not_cachable?(template)
       template.index("%").nil?
@@ -105,7 +115,7 @@ def initialize(key)
     end
 
     def evaluate(event)
-      value = event[@key]
+      value = event.get(@key)
 
       case value
       when nil
@@ -115,7 +125,9 @@ def evaluate(event)
       when Hash
         LogStash::Json.dump(value)
       else
-        value
+        # Make sure we dont work on the refence of the value
+        # The Java Event implementation was always returning a string.
+        "#{value}"
       end
     end
   end
diff --git a/lib/logstash/timestamp.rb b/logstash-core-event/lib/logstash/timestamp.rb
similarity index 88%
rename from lib/logstash/timestamp.rb
rename to logstash-core-event/lib/logstash/timestamp.rb
index fb75c5f2538..ab6b6edb3bc 100644
--- a/lib/logstash/timestamp.rb
+++ b/logstash-core-event/lib/logstash/timestamp.rb
@@ -24,7 +24,13 @@ def initialize(time = Time.new)
     end
 
     def self.at(*args)
-      Timestamp.new(::Time.at(*args))
+      epoch = args.first
+      if epoch.is_a?(BigDecimal)
+        # bug in JRuby prevents correcly parsing a BigDecimal fractional part, see https://github.com/elastic/logstash/issues/4565
+        Timestamp.new(::Time.at(epoch.to_i, epoch.frac.to_f * 1000000))
+      else
+        Timestamp.new(::Time.at(*args))
+      end
     end
 
     def self.parse(*args)
diff --git a/lib/logstash/util/accessors.rb b/logstash-core-event/lib/logstash/util/accessors.rb
similarity index 90%
rename from lib/logstash/util/accessors.rb
rename to logstash-core-event/lib/logstash/util/accessors.rb
index 01c16910855..23248f2c3ea 100644
--- a/lib/logstash/util/accessors.rb
+++ b/logstash-core-event/lib/logstash/util/accessors.rb
@@ -95,7 +95,14 @@ def lookup(field_reference)
     # @param field_reference [String] the field referece
     # @return [[Object, String]] the  [target, key] tuple associated with this field reference
     def lookup_or_create(field_reference)
-      @lut[field_reference] ||= find_or_create_target(field_reference)
+      # flush the @lut to prevent stale cached fieldref which may point to an old target
+      # which was overwritten with a new value. for example, if "[a][b]" is cached and we
+      # set a new value for "[a]" then reading again "[a][b]" would point in a stale target.
+      # flushing the complete @lut is suboptimal, but a hierarchical lut would be required
+      # to be able to invalidate fieldrefs from a common root.
+      # see https://github.com/elastic/logstash/pull/5132
+      @lut.clear
+      @lut[field_reference] = find_or_create_target(field_reference)
     end
 
     # find the target container object in store for this field reference
diff --git a/logstash-core-event/logstash-core-event.gemspec b/logstash-core-event/logstash-core-event.gemspec
new file mode 100644
index 00000000000..9e0a757a870
--- /dev/null
+++ b/logstash-core-event/logstash-core-event.gemspec
@@ -0,0 +1,23 @@
+# -*- encoding: utf-8 -*-
+lib = File.expand_path('../lib', __FILE__)
+$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
+require 'logstash-core-event/version'
+
+Gem::Specification.new do |gem|
+  gem.authors       = ["Elastic"]
+  gem.email         = ["info@elastic.co"]
+  gem.description   = %q{The core event component of logstash, the scalable log and event management tool}
+  gem.summary       = %q{logstash-core-event - The core event component of logstash}
+  gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
+  gem.license       = "Apache License (2.0)"
+
+  gem.files         = Dir.glob(["logstash-core-event.gemspec", "lib/**/*.rb", "spec/**/*.rb"])
+  gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
+  gem.name          = "logstash-core-event"
+  gem.require_paths = ["lib"]
+  gem.version       = LOGSTASH_CORE_EVENT_VERSION
+
+  if RUBY_PLATFORM == 'java'
+    gem.platform = RUBY_PLATFORM
+  end
+end
diff --git a/spec/core/event_spec.rb b/logstash-core-event/spec/logstash/event_spec.rb
similarity index 58%
rename from spec/core/event_spec.rb
rename to logstash-core-event/spec/logstash/event_spec.rb
index 52b22c3115c..8c6d60db291 100644
--- a/spec/core/event_spec.rb
+++ b/logstash-core-event/spec/logstash/event_spec.rb
@@ -1,51 +1,95 @@
 # encoding: utf-8
 require "spec_helper"
+require "logstash/util/decorators"
+require "json"
 
 describe LogStash::Event do
 
   shared_examples "all event tests" do
     context "[]=" do
       it "should raise an exception if you attempt to set @timestamp to a value type other than a Time object" do
-        expect{subject["@timestamp"] = "crash!"}.to raise_error(TypeError)
+        expect{subject.set("@timestamp", "crash!")}.to raise_error(TypeError)
       end
 
       it "should assign simple fields" do
-        expect(subject["foo"]).to be_nil
-        expect(subject["foo"] = "bar").to eq("bar")
-        expect(subject["foo"]).to eq("bar")
+        expect(subject.get("foo")).to be_nil
+        expect(subject.set("foo", "bar")).to eq("bar")
+        expect(subject.get("foo")).to eq("bar")
       end
 
       it "should overwrite simple fields" do
-        expect(subject["foo"]).to be_nil
-        expect(subject["foo"] = "bar").to eq("bar")
-        expect(subject["foo"]).to eq("bar")
+        expect(subject.get("foo")).to be_nil
+        expect(subject.set("foo", "bar")).to eq("bar")
+        expect(subject.get("foo")).to eq("bar")
 
-        expect(subject["foo"] = "baz").to eq("baz")
-        expect(subject["foo"]).to eq("baz")
+        expect(subject.set("foo", "baz")).to eq("baz")
+        expect(subject.get("foo")).to eq("baz")
       end
 
       it "should assign deep fields" do
-        expect(subject["[foo][bar]"]).to be_nil
-        expect(subject["[foo][bar]"] = "baz").to eq("baz")
-        expect(subject["[foo][bar]"]).to eq("baz")
+        expect(subject.get("[foo][bar]")).to be_nil
+        expect(subject.set("[foo][bar]", "baz")).to eq("baz")
+        expect(subject.get("[foo][bar]")).to eq("baz")
       end
 
       it "should overwrite deep fields" do
-        expect(subject["[foo][bar]"]).to be_nil
-        expect(subject["[foo][bar]"] = "baz").to eq("baz")
-        expect(subject["[foo][bar]"]).to eq("baz")
+        expect(subject.get("[foo][bar]")).to be_nil
+        expect(subject.set("[foo][bar]", "baz")).to eq("baz")
+        expect(subject.get("[foo][bar]")).to eq("baz")
 
-        expect(subject["[foo][bar]"] = "zab").to eq("zab")
-        expect(subject["[foo][bar]"]).to eq("zab")
+        expect(subject.set("[foo][bar]", "zab")).to eq("zab")
+        expect(subject.get("[foo][bar]")).to eq("zab")
       end
 
       it "allow to set the @metadata key to a hash" do
-        subject["@metadata"] = { "action" => "index" }
-        expect(subject["[@metadata][action]"]).to eq("index")
+        subject.set("@metadata", { "action" => "index" })
+        expect(subject.get("[@metadata][action]")).to eq("index")
+      end
+
+      it "should add key when setting nil value" do
+        subject.set("[baz]", nil)
+        expect(subject.to_hash).to include("baz" => nil)
+      end
+
+      it "should set nil element within existing array value" do
+        subject.set("[foo]", ["bar", "baz"])
+
+        expect(subject.set("[foo][0]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil, "baz"])
+      end
+
+      it "should set nil in first element within empty array" do
+        subject.set("[foo]", [])
+
+        expect(subject.set("[foo][0]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil])
+      end
+
+      it "should set nil in second element within empty array" do
+        subject.set("[foo]", [])
+
+        expect(subject.set("[foo][1]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil, nil])
       end
     end
 
     context "#sprintf" do
+      it "should not return a String reference" do
+        data = "NOT-A-REFERENCE"
+        event = LogStash::Event.new({ "reference" => data })
+        LogStash::Util::Decorators.add_fields({"reference_test" => "%{reference}"}, event, "dummy-plugin")
+        data.downcase!
+        expect(event.get("reference_test")).not_to eq(data)
+      end
+
+      it "should not return a Fixnum reference" do
+        data = 1
+        event = LogStash::Event.new({ "reference" => data })
+        LogStash::Util::Decorators.add_fields({"reference_test" => "%{reference}"}, event, "dummy-plugin")
+        data += 41
+        expect(event.get("reference_test")).to eq("1")
+      end
+
       it "should report a unix timestamp for %{+%s}" do
         expect(subject.sprintf("%{+%s}")).to eq("1356998400")
       end
@@ -80,7 +124,7 @@
 
       it "should report fields with %{field} syntax" do
         expect(subject.sprintf("%{type}")).to eq("sprintf")
-        expect(subject.sprintf("%{message}")).to eq(subject["message"])
+        expect(subject.sprintf("%{message}")).to eq(subject.get("message"))
       end
 
       it "should print deep fields" do
@@ -108,6 +152,39 @@
         expect(subject.sprintf("%{type}%{message}|")).to eq("sprintfhello world|")
       end
 
+      it "should render nil array values as leading empty string" do
+        expect(subject.set("foo", [nil, "baz"])).to eq([nil, "baz"])
+
+        expect(subject.get("[foo][0]")).to be_nil
+        expect(subject.get("[foo][1]")).to eq("baz")
+
+        expect(subject.sprintf("%{[foo]}")).to eq(",baz")
+      end
+
+      it "should render nil array values as middle empty string" do
+        expect(subject.set("foo", ["bar", nil, "baz"])).to eq(["bar", nil, "baz"])
+
+        expect(subject.get("[foo][0]")).to eq("bar")
+        expect(subject.get("[foo][1]")).to be_nil
+        expect(subject.get("[foo][2]")).to eq("baz")
+
+        expect(subject.sprintf("%{[foo]}")).to eq("bar,,baz")
+      end
+
+     it "should render nil array values as trailing empty string" do
+        expect(subject.set("foo", ["bar", nil])).to eq(["bar", nil])
+
+        expect(subject.get("[foo][0]")).to eq("bar")
+        expect(subject.get("[foo][1]")).to be_nil
+
+        expect(subject.sprintf("%{[foo]}")).to eq("bar,")
+     end
+
+      it "should render deep arrays with nil value" do
+        subject.set("[foo]", [[12, nil], 56])
+        expect(subject.sprintf("%{[foo]}")).to eq("12,,56")
+      end
+
       context "#encoding" do
         it "should return known patterns as UTF-8" do
           expect(subject.sprintf("%{message}").encoding).to eq(Encoding::UTF_8)
@@ -121,18 +198,18 @@
 
     context "#[]" do
       it "should fetch data" do
-        expect(subject["type"]).to eq("sprintf")
+        expect(subject.get("type")).to eq("sprintf")
       end
       it "should fetch fields" do
-        expect(subject["a"]).to eq("b")
-        expect(subject['c']['d']).to eq("f")
+        expect(subject.get("a")).to eq("b")
+        expect(subject.get('c')['d']).to eq("f")
       end
       it "should fetch deep fields" do
-        expect(subject["[j][k1]"]).to eq("v")
-        expect(subject["[c][d]"]).to eq("f")
-        expect(subject['[f][g][h]']).to eq("i")
-        expect(subject['[j][k3][4]']).to eq("m")
-        expect(subject['[j][5]']).to eq(7)
+        expect(subject.get("[j][k1]")).to eq("v")
+        expect(subject.get("[c][d]")).to eq("f")
+        expect(subject.get('[f][g][h]')).to eq("i")
+        expect(subject.get('[j][k3][4]')).to eq("m")
+        expect(subject.get('[j][5]')).to eq(7)
 
       end
 
@@ -140,7 +217,7 @@
         count = 1000000
         2.times do
           start = Time.now
-          count.times { subject["[j][k1]"] }
+          count.times { subject.get("[j][k1]") }
           duration = Time.now - start
           puts "event #[] rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
         end
@@ -186,11 +263,11 @@
         )
         subject.overwrite(new_event)
 
-        expect(subject["message"]).to eq("foo bar")
-        expect(subject["type"]).to eq("new")
+        expect(subject.get("message")).to eq("foo bar")
+        expect(subject.get("type")).to eq("new")
 
         ["tags", "source", "a", "c", "f", "j"].each do |field|
-          expect(subject[field]).to be_nil
+          expect(subject.get(field)).to be_nil
         end
       end
     end
@@ -198,7 +275,7 @@
     context "#append" do
       it "should append strings to an array" do
         subject.append(LogStash::Event.new("message" => "another thing"))
-        expect(subject["message"]).to eq([ "hello world", "another thing" ])
+        expect(subject.get("message")).to eq([ "hello world", "another thing" ])
       end
 
       it "should concatenate tags" do
@@ -206,56 +283,57 @@
         # added to_a for when array is a Java Collection when produced from json input
         # TODO: we have to find a better way to handle this in tests. maybe override
         # rspec eq or == to do an explicit to_a when comparing arrays?
-        expect(subject["tags"].to_a).to eq([ "tag1", "tag2" ])
+        expect(subject.get("tags").to_a).to eq([ "tag1", "tag2" ])
       end
 
       context "when event field is nil" do
         it "should add single value as string" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq("append1")
+          expect(subject.get("field1")).to eq("append1")
         end
         it "should add multi values as array" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","append2" ]}))
-          expect(subject[ "field1" ]).to eq([ "append1","append2" ])
+          expect(subject.get("field1")).to eq([ "append1","append2" ])
         end
       end
 
       context "when event field is a string" do
-        before { subject[ "field1" ] = "original1" }
+        before { subject.set("field1", "original1") }
 
         it "should append string to values, if different from current" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
         it "should not change value, if appended value is equal current" do
           subject.append(LogStash::Event.new({"field1" => "original1"}))
-          expect(subject[ "field1" ]).to eq("original1")
+          expect(subject.get("field1")).to eq("original1")
         end
         it "should concatenate values in an array" do
           subject.append(LogStash::Event.new({"field1" => [ "append1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
         it "should join array, removing duplicates" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
       end
       context "when event field is an array" do
-        before { subject[ "field1" ] = [ "original1", "original2" ] }
+        before { subject.set("field1", [ "original1", "original2" ] )}
 
         it "should append string values to array, if not present in array" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2", "append1" ])
         end
         it "should not append string values, if the array already contains it" do
           subject.append(LogStash::Event.new({"field1" => "original1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2" ])
         end
         it "should join array, removing duplicates" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2", "append1" ])
         end
       end
+
     end
 
     it "timestamp parsing speed", :performance => true do
@@ -264,7 +342,7 @@
 
       data = { "@timestamp" => "2013-12-21T07:25:06.605Z" }
       event = LogStash::Event.new(data)
-      expect(event["@timestamp"]).to be_a(LogStash::Timestamp)
+      expect(event.get("@timestamp")).to be_a(LogStash::Timestamp)
 
       duration = 0
       [warmup, count].each do |i|
@@ -313,47 +391,56 @@
     end
 
     context "timestamp initialization" do
-      let(:logger) { double("logger") }
+      let(:logger_mock) { double("logger") }
+
+      after(:each) do
+        LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER
+      end
 
       it "should coerce timestamp" do
         t = Time.iso8601("2014-06-12T00:12:17.114Z")
-        expect(LogStash::Timestamp).to receive(:coerce).exactly(3).times.and_call_original
         expect(LogStash::Event.new("@timestamp" => t).timestamp.to_i).to eq(t.to_i)
         expect(LogStash::Event.new("@timestamp" => LogStash::Timestamp.new(t)).timestamp.to_i).to eq(t.to_i)
         expect(LogStash::Event.new("@timestamp" => "2014-06-12T00:12:17.114Z").timestamp.to_i).to eq(t.to_i)
       end
 
       it "should assign current time when no timestamp" do
-        ts = LogStash::Timestamp.now
-        expect(LogStash::Timestamp).to receive(:now).and_return(ts)
-        expect(LogStash::Event.new({}).timestamp.to_i).to eq(ts.to_i)
+        expect(LogStash::Event.new({}).timestamp.to_i).to be_within(1).of (Time.now.to_i)
       end
 
-      it "should tag and warn for invalid value" do
-        ts = LogStash::Timestamp.now
-        expect(LogStash::Timestamp).to receive(:now).twice.and_return(ts)
-        expect(LogStash::Event::LOGGER).to receive(:warn).twice
-
-        event = LogStash::Event.new("@timestamp" => :foo)
-        expect(event.timestamp.to_i).to eq(ts.to_i)
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq(:foo)
+      it "should tag for invalid value" do
+        event = LogStash::Event.new("@timestamp" => "foo")
+        expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
 
         event = LogStash::Event.new("@timestamp" => 666)
-        expect(event.timestamp.to_i).to eq(ts.to_i)
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq(666)
+        expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq(666)
       end
 
-      it "should tag and warn for invalid string format" do
-        ts = LogStash::Timestamp.now
-        expect(LogStash::Timestamp).to receive(:now).and_return(ts)
-        expect(LogStash::Event::LOGGER).to receive(:warn)
+      it "should warn for invalid value" do
+        LogStash::Event.logger = logger_mock
+
+        expect(logger_mock).to receive(:warn).twice
+
+        LogStash::Event.new("@timestamp" => :foo)
+        LogStash::Event.new("@timestamp" => 666)
+      end
 
+      it "should tag for invalid string format" do
         event = LogStash::Event.new("@timestamp" => "foo")
-        expect(event.timestamp.to_i).to eq(ts.to_i)
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq("foo")
+        expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
+      end
+
+      it "should warn for invalid string format" do
+        LogStash::Event.logger = logger_mock
+
+        expect(logger_mock).to receive(:warn)
+        LogStash::Event.new("@timestamp" => "foo")
       end
     end
 
@@ -365,7 +452,7 @@
         )
         json = new_event.to_json
 
-        expect(json).to eq( "{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}")
+        expect(JSON.parse(json)).to eq( JSON.parse("{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}"))
       end
 
       it "should support to_json and ignore arguments" do
@@ -375,7 +462,7 @@
         )
         json = new_event.to_json(:foo => 1, :bar => "baz")
 
-        expect(json).to eq( "{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}")
+        expect(JSON.parse(json)).to eq( JSON.parse("{\"@timestamp\":\"2014-09-23T19:26:15.832Z\",\"message\":\"foo bar\",\"@version\":\"1\"}"))
       end
     end
 
@@ -391,7 +478,7 @@
         end
 
         it "should still allow normal field access" do
-          expect(subject["hello"]).to eq("world")
+          expect(subject.get("hello")).to eq("world")
         end
       end
 
@@ -404,15 +491,15 @@
           expect(fieldref).to start_with("[@metadata]")
 
           # Set it.
-          subject[fieldref] = value
+          subject.set(fieldref, value)
         end
 
         it "should still allow normal field access" do
-          expect(subject["normal"]).to eq("normal")
+          expect(subject.get("normal")).to eq("normal")
         end
 
         it "should allow getting" do
-          expect(subject[fieldref]).to eq(value)
+          expect(subject.get(fieldref)).to eq(value)
         end
 
         it "should be hidden from .to_json" do
@@ -435,10 +522,10 @@
       context "with no metadata" do
         subject { LogStash::Event.new("foo" => "bar") }
         it "should have no metadata" do
-          expect(subject["@metadata"]).to be_empty
+          expect(subject.get("@metadata")).to be_empty
         end
         it "should still allow normal field access" do
-          expect(subject["foo"]).to eq("bar")
+          expect(subject.get("foo")).to eq("bar")
         end
 
         it "should not include the @metadata key" do
@@ -496,4 +583,52 @@
       subject{LogStash::Event.new(LogStash::Json.load(LogStash::Json.dump(event_hash)))}
     end
   end
+
+
+  describe "#to_s" do
+    let(:timestamp) { LogStash::Timestamp.new }
+    let(:event1) { LogStash::Event.new({ "@timestamp" => timestamp, "host" => "foo", "message" => "bar"}) }
+    let(:event2) { LogStash::Event.new({ "host" => "bar", "message" => "foo"}) }
+
+    it "should cache only one template" do
+      LogStash::StringInterpolation.clear_cache
+      expect {
+        event1.to_s
+        event2.to_s
+      }.to change { LogStash::StringInterpolation.cache_size }.by(1)
+    end
+
+    it "return the string containing the timestamp, the host and the message" do
+      expect(event1.to_s).to eq("#{timestamp.to_iso8601} #{event1.get("host")} #{event1.get("message")}")
+    end
+  end
+
+  describe "Event accessors" do
+    let(:event) { LogStash::Event.new({ "message" => "foo" }) }
+
+    it "should invalidate target caching" do
+      expect(event.get("[a][0]")).to be_nil
+
+      expect(event.set("[a][0]", 42)).to eq(42)
+      expect(event.get("[a][0]")).to eq(42)
+      expect(event.get("[a]")).to eq({"0" => 42})
+
+      expect(event.set("[a]", [42, 24])).to eq([42, 24])
+      expect(event.get("[a]")).to eq([42, 24])
+
+      expect(event.get("[a][0]")).to eq(42)
+
+      expect(event.set("[a]", [24, 42])).to eq([24, 42])
+      expect(event.get("[a][0]")).to eq(24)
+
+      expect(event.set("[a][0]", {"a "=> 99, "b" => 98})).to eq({"a "=> 99, "b" => 98})
+      expect(event.get("[a][0]")).to eq({"a "=> 99, "b" => 98})
+
+      expect(event.get("[a]")).to eq([{"a "=> 99, "b" => 98}, 42])
+      expect(event.get("[a][0]")).to eq({"a "=> 99, "b" => 98})
+      expect(event.get("[a][1]")).to eq(42)
+      expect(event.get("[a][0][b]")).to eq(98)
+    end
+  end
 end
+
diff --git a/logstash-core-event/spec/logstash/timestamp_spec.rb b/logstash-core-event/spec/logstash/timestamp_spec.rb
new file mode 100644
index 00000000000..196b895c39e
--- /dev/null
+++ b/logstash-core-event/spec/logstash/timestamp_spec.rb
@@ -0,0 +1,170 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/timestamp"
+require "bigdecimal"
+
+describe LogStash::Timestamp do
+
+  it "should parse its own iso8601 output" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.parse_iso8601(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce iso8601 string" do
+    t = Time.now
+    ts = LogStash::Timestamp.new(t)
+    expect(LogStash::Timestamp.coerce(ts.to_iso8601).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Time" do
+    t = Time.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should coerce Timestamp" do
+    t = LogStash::Timestamp.now
+    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
+  end
+
+  it "should raise on invalid string coerce" do
+    expect{LogStash::Timestamp.coerce("foobar")}.to raise_error LogStash::TimestampParserError
+  end
+
+  it "should return nil on invalid object coerce" do
+    expect(LogStash::Timestamp.coerce(:foobar)).to be_nil
+  end
+
+  it "should support to_json" do
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json).to eq("\"2014-09-23T08:00:00.000Z\"")
+  end
+
+  it "should support to_json and ignore arguments" do
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :argumnents => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
+  end
+
+  it "should support timestamp comparaison" do
+   current = LogStash::Timestamp.new(Time.now) 
+   future = LogStash::Timestamp.new(Time.now + 100)
+
+   expect(future > current).to eq(true)
+   expect(future < current).to eq(false)
+   expect(current == current).to eq(true)
+
+   expect(current <=> current).to eq(0)
+   expect(current <=> future).to eq(-1)
+   expect(future <=> current).to eq(1)
+  end
+
+  it "should allow unary operation +" do
+    current = Time.now
+    t = LogStash::Timestamp.new(current) + 10
+    expect(t).to eq(current + 10)
+  end
+
+  describe "subtraction" do
+    it "should work on a timestamp object" do
+      t = Time.now
+      current = LogStash::Timestamp.new(t)
+      future = LogStash::Timestamp.new(t + 10)
+      expect(future - current).to eq(10)
+    end
+
+    it "should work on with time object" do
+      current = Time.now
+      t = LogStash::Timestamp.new(current + 10)
+      expect(t - current).to eq(10)
+    end
+
+    it "should work with numeric value" do
+      current = Time.now
+      t = LogStash::Timestamp.new(current + 10)
+      expect(t - 10).to eq(current)
+    end
+  end
+
+  context "identity methods" do
+    subject { LogStash::Timestamp.new }
+
+    it "should support utc" do
+      expect(subject.utc).to eq(subject)
+    end
+
+    it "should support gmtime" do
+      expect(subject.gmtime).to eq(subject)
+    end
+  end
+
+  context "numeric casting methods" do
+    let (:now) {Time.now}
+    subject { LogStash::Timestamp.new(now) }
+
+    it "should support to_i" do
+      expect(subject.to_i).to eq(now.to_i)
+    end
+
+    it "should support to_f" do
+      expect(subject.to_f).to eq(now.to_f)
+    end
+  end
+
+  context "at" do
+    context "with integer epoch" do
+      it "should convert to correct date" do
+        expect(LogStash::Timestamp.at(946702800).to_iso8601).to eq("2000-01-01T05:00:00.000Z")
+      end
+
+      it "should return zero usec" do
+        expect(LogStash::Timestamp.at(946702800).usec).to eq(0)
+      end
+
+      it "should return prior to epoch date on negative input" do
+        expect(LogStash::Timestamp.at(-1).to_iso8601).to eq("1969-12-31T23:59:59.000Z")
+      end
+    end
+
+    context "with float epoch" do
+      it "should convert to correct date" do
+        expect(LogStash::Timestamp.at(946702800.123456.to_f).to_iso8601).to eq("2000-01-01T05:00:00.123Z")
+      end
+
+      it "should return usec with a minimum of millisec precision" do
+        expect(LogStash::Timestamp.at(946702800.123456.to_f).usec).to be_within(1000).of(123456)
+      end
+    end
+
+    context "with BigDecimal epoch" do
+      it "should convert to correct date" do
+        expect(LogStash::Timestamp.at(BigDecimal.new("946702800.123456")).to_iso8601).to eq("2000-01-01T05:00:00.123Z")
+      end
+
+      it "should return usec with a minimum of millisec precision" do
+        # since Java Timestamp relies on JodaTime which supports only milliseconds precision
+        # the usec method will only be precise up to milliseconds.
+        expect(LogStash::Timestamp.at(BigDecimal.new("946702800.123456")).usec).to be_within(1000).of(123456)
+      end
+    end
+
+    context "with illegal parameters" do
+      it "should raise exception on nil input" do
+        expect{LogStash::Timestamp.at(nil)}.to raise_error
+      end
+
+      it "should raise exception on invalid input type" do
+        expect{LogStash::Timestamp.at(:foo)}.to raise_error
+      end
+    end
+  end
+
+  context "usec" do
+    it "should support millisecond precision" do
+      expect(LogStash::Timestamp.at(946702800.123).usec).to eq(123000)
+    end
+
+    it "should try to preserve and report microseconds precision if possible" do
+      # since Java Timestamp relies on JodaTime which supports only milliseconds precision
+      # the usec method will only be precise up to milliseconds.
+      expect(LogStash::Timestamp.at(946702800.123456).usec).to be_within(1000).of(123456)
+    end
+  end
+end
diff --git a/spec/util/accessors_spec.rb b/logstash-core-event/spec/logstash/util/accessors_spec.rb
similarity index 93%
rename from spec/util/accessors_spec.rb
rename to logstash-core-event/spec/logstash/util/accessors_spec.rb
index af719a32999..e3c1a73e60e 100644
--- a/spec/util/accessors_spec.rb
+++ b/logstash-core-event/spec/logstash/util/accessors_spec.rb
@@ -1,8 +1,17 @@
 # encoding: utf-8
 require "spec_helper"
-require "logstash/util/accessors"
 
-describe LogStash::Util::Accessors, :if => true do
+# this is to skip specs when running agains an alternate logstash-core-event implementation
+# that does not define the Accessors class. For example, in logstash-core-event-java
+# the Accessors class does not exists in the Ruby namespace.
+class_exists = begin
+  require "logstash/util/accessors"
+  true
+rescue LoadError
+  false
+end
+
+describe "LogStash::Util::Accessors", :if => class_exists do
 
   context "using simple field" do
 
diff --git a/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
new file mode 100644
index 00000000000..ced5939392b
--- /dev/null
+++ b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+LOGSTASH_CORE_PLUGIN_API = "2.0.0"
diff --git a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
new file mode 100644
index 00000000000..8764ad5b24c
--- /dev/null
+++ b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
@@ -0,0 +1,29 @@
+# -*- encoding: utf-8 -*-
+lib = File.expand_path('../lib', __FILE__)
+$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
+require "logstash-core-plugin-api/version"
+
+Gem::Specification.new do |gem|
+  gem.authors       = ["Elastic"]
+  gem.email         = ["info@elastic.co"]
+  gem.description   = %q{Logstash plugin API}
+  gem.summary       = %q{Define the plugin API that the plugin need to follow.}
+  gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
+  gem.license       = "Apache License (2.0)"
+
+  gem.files         = Dir.glob(["logstash-core-event.gemspec", "lib/**/*.rb", "spec/**/*.rb"])
+  gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
+  gem.name          = "logstash-core-plugin-api"
+  gem.require_paths = ["lib"]
+  gem.version       = LOGSTASH_CORE_PLUGIN_API
+
+  gem.add_runtime_dependency "logstash-core", "5.0.0.dev"
+
+  # Make sure we dont build this gem from a non jruby
+  # environment.
+  if RUBY_PLATFORM == "java"
+    gem.platform = "java"
+  else
+    raise "The logstash-core-api need to be build on jruby"
+  end
+end
diff --git a/logstash-core/lib/logstash-core.rb b/logstash-core/lib/logstash-core.rb
new file mode 100644
index 00000000000..c2e4557afa8
--- /dev/null
+++ b/logstash-core/lib/logstash-core.rb
@@ -0,0 +1 @@
+require "logstash-core/logstash-core"
diff --git a/lib/logstash-core.rb b/logstash-core/lib/logstash-core/logstash-core.rb
similarity index 100%
rename from lib/logstash-core.rb
rename to logstash-core/lib/logstash-core/logstash-core.rb
diff --git a/lib/logstash/version.rb b/logstash-core/lib/logstash-core/version.rb
similarity index 64%
rename from lib/logstash/version.rb
rename to logstash-core/lib/logstash-core/version.rb
index 17a5cd8c15b..737441c5abc 100644
--- a/lib/logstash/version.rb
+++ b/logstash-core/lib/logstash-core/version.rb
@@ -1,6 +1,8 @@
 # encoding: utf-8
-# The version of logstash.
-LOGSTASH_VERSION = "3.0.0.dev"
 
+# The version of logstash core gem.
+#
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
+
+LOGSTASH_CORE_VERSION = "5.0.0.dev"
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
new file mode 100644
index 00000000000..8bb3649e9ca
--- /dev/null
+++ b/logstash-core/lib/logstash/agent.rb
@@ -0,0 +1,283 @@
+# encoding: utf-8
+require "logstash/environment"
+require "logstash/errors"
+require "logstash/config/cpu_core_strategy"
+require "logstash/instrument/collector"
+require "logstash/instrument/metric"
+require "logstash/instrument/periodic_pollers"
+require "logstash/instrument/collector"
+require "logstash/instrument/metric"
+require "logstash/pipeline"
+require "logstash/webserver"
+require "stud/trap"
+require "logstash/config/loader"
+require "uri"
+require "socket"
+require "securerandom"
+
+LogStash::Environment.load_locale!
+
+class LogStash::Agent
+  STARTED_AT = Time.now.freeze
+
+  attr_reader :metric, :node_name, :pipelines, :settings
+  attr_accessor :logger
+
+  # initialize method for LogStash::Agent
+  # @param params [Hash] potential parameters are:
+  #   :node_name [String] - identifier for the agent
+  #   :auto_reload [Boolean] - enable reloading of pipelines
+  #   :reload_interval [Integer] - reload pipelines every X seconds
+  #   :logger [Cabin::Channel] - logger instance
+  def initialize(settings = LogStash::SETTINGS)
+    @settings = settings
+    @logger = Cabin::Channel.get(LogStash)
+    @auto_reload = setting("config.reload.automatic")
+
+    @pipelines = {}
+    @node_name = setting("node.name")
+    @http_host = setting("http.host")
+    @http_port = setting("http.port")
+    @http_environment = setting("http.environment")
+
+    @config_loader = LogStash::Config::Loader.new(@logger)
+    @reload_interval = setting("config.reload.interval")
+    @upgrade_mutex = Mutex.new
+
+    @collect_metric = setting("metric.collect")
+    @metric = create_metric_collector
+    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(create_metric_collector)
+  end
+
+  def execute
+    @thread = Thread.current # this var is implicilty used by Stud.stop?
+    @logger.info("starting agent")
+
+    start_background_services
+    start_pipelines
+    start_webserver
+
+    return 1 if clean_state?
+
+    Stud.stoppable_sleep(@reload_interval) # sleep before looping
+
+    if @auto_reload
+      Stud.interval(@reload_interval) { reload_state! }
+    else
+      while !Stud.stop?
+        if clean_state? || running_pipelines?
+          sleep 0.5
+        else
+          break
+        end
+      end
+    end
+  end
+
+  # register_pipeline - adds a pipeline to the agent's state
+  # @param pipeline_id [String] pipeline string identifier
+  # @param settings [Hash] settings that will be passed when creating the pipeline.
+  #   keys should be symbols such as :pipeline_workers and :pipeline_batch_delay
+  def register_pipeline(pipeline_id, settings = @settings)
+    pipeline_settings = settings.clone
+    pipeline_settings.set("pipeline.id", pipeline_id)
+
+    pipeline = create_pipeline(pipeline_settings)
+    return unless pipeline.is_a?(LogStash::Pipeline)
+    if @auto_reload && pipeline.non_reloadable_plugins.any?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_register"),
+                    :pipeline_id => pipeline_id,
+                    :plugins => pipeline.non_reloadable_plugins.map(&:class))
+      return
+    end
+    @pipelines[pipeline_id] = pipeline
+  end
+
+  def reload_state!
+    @upgrade_mutex.synchronize do
+      @pipelines.each do |pipeline_id, _|
+        begin
+          reload_pipeline!(pipeline_id)
+        rescue => e
+          @logger.error(I18n.t("oops"), :message => e.message, :class => e.class.name, :backtrace => e.backtrace)
+        end
+      end
+    end
+  end
+
+  # Calculate the Logstash uptime in milliseconds
+  #
+  # @return [Fixnum] Uptime in milliseconds
+  def uptime
+    ((Time.now.to_f - STARTED_AT.to_f) * 1000.0).to_i
+  end
+
+  def shutdown
+    stop_background_services
+    stop_webserver
+    shutdown_pipelines
+  end
+
+  def node_uuid
+    @node_uuid ||= SecureRandom.uuid
+  end
+
+  def running_pipelines?
+    @upgrade_mutex.synchronize do
+      @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }.any?
+    end
+  end
+
+  private
+  def start_webserver
+    options = {:http_host => @http_host, :http_port => @http_port, :http_environment => @http_environment }
+    @webserver = LogStash::WebServer.new(@logger, options)
+    Thread.new(@webserver) do |webserver|
+      LogStash::Util.set_thread_name("Api Webserver")
+      webserver.run
+    end
+  end
+
+  def stop_webserver
+    @webserver.stop if @webserver
+  end
+
+  def start_background_services
+    if collect_metrics?
+      @logger.debug("Agent: Starting metric periodic pollers")
+      @periodic_pollers.start
+    end
+  end
+
+  def stop_background_services
+    if collect_metrics?
+      @logger.debug("Agent: Stopping metric periodic pollers")
+      @periodic_pollers.stop
+    end
+  end
+
+  def create_metric_collector
+    if collect_metrics?
+      @logger.debug("Agent: Configuring metric collection")
+      LogStash::Instrument::Collector.instance.agent = self
+      LogStash::Instrument::Metric.new
+    else
+      LogStash::Instrument::NullMetric.new
+    end
+  end
+
+  def collect_metrics?
+    @collect_metric
+  end
+
+  def create_pipeline(settings, config=nil)
+
+    if config.nil?
+      begin
+        config = fetch_config(settings)
+      rescue => e
+        @logger.error("failed to fetch pipeline configuration", :message => e.message)
+        return
+      end
+    end
+
+    begin
+      LogStash::Pipeline.new(config, settings)
+    rescue => e
+      @logger.error("fetched an invalid config", :config => config, :reason => e.message)
+      return
+    end
+  end
+
+  def fetch_config(settings)
+    @config_loader.format_config(settings.get("path.config"), settings.get("config.string"))
+  end
+
+  # since this method modifies the @pipelines hash it is
+  # wrapped in @upgrade_mutex in the parent call `reload_state!`
+  def reload_pipeline!(id)
+    old_pipeline = @pipelines[id]
+    new_config = fetch_config(old_pipeline.settings)
+    if old_pipeline.config_str == new_config
+      @logger.debug("no configuration change for pipeline",
+                    :pipeline => id, :config => new_config)
+      return
+    end
+
+    new_pipeline = create_pipeline(old_pipeline.settings, new_config)
+
+    return if new_pipeline.nil?
+
+    if new_pipeline.non_reloadable_plugins.any?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"),
+                    :pipeline_id => id,
+                    :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
+      return
+    else
+      @logger.warn("fetched new config for pipeline. upgrading..",
+                   :pipeline => id, :config => new_pipeline.config_str)
+      upgrade_pipeline(id, new_pipeline)
+    end
+  end
+
+  def start_pipeline(id)
+    pipeline = @pipelines[id]
+    return unless pipeline.is_a?(LogStash::Pipeline)
+    return if pipeline.ready?
+    @logger.info("starting pipeline", :id => id)
+
+    # Reset the current collected stats,
+    # starting a pipeline with a new configuration should be the same as restarting
+    # logstash.
+    reset_collector
+
+    Thread.new do
+      LogStash::Util.set_thread_name("pipeline.#{id}")
+      begin
+        pipeline.run
+      rescue => e
+        @logger.error("Pipeline aborted due to error", :exception => e, :backtrace => e.backtrace)
+      end
+    end
+    sleep 0.01 until pipeline.ready?
+  end
+
+  def stop_pipeline(id)
+    pipeline = @pipelines[id]
+    return unless pipeline
+    @logger.warn("stopping pipeline", :id => id)
+    pipeline.shutdown { LogStash::ShutdownWatcher.start(pipeline) }
+    @pipelines[id].thread.join
+  end
+
+  def start_pipelines
+    @pipelines.each { |id, _| start_pipeline(id) }
+  end
+
+  def shutdown_pipelines
+    @pipelines.each { |id, _| stop_pipeline(id) }
+  end
+
+  def running_pipeline?(pipeline_id)
+    thread = @pipelines[pipeline_id].thread
+    thread.is_a?(Thread) && thread.alive?
+  end
+
+  def upgrade_pipeline(pipeline_id, new_pipeline)
+    stop_pipeline(pipeline_id)
+    @pipelines[pipeline_id] = new_pipeline
+    start_pipeline(pipeline_id)
+  end
+
+  def clean_state?
+    @pipelines.empty?
+  end
+
+  def reset_collector
+    LogStash::Instrument::Collector.instance.clear
+  end
+
+  def setting(key)
+    @settings.get(key)
+  end
+end # class LogStash::Agent
diff --git a/logstash-core/lib/logstash/api/app_helpers.rb b/logstash-core/lib/logstash/api/app_helpers.rb
new file mode 100644
index 00000000000..cd872edc51d
--- /dev/null
+++ b/logstash-core/lib/logstash/api/app_helpers.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "logstash/json"
+
+module LogStash::Api::AppHelpers
+
+  def respond_with(data, options={})
+    as     = options.fetch(:as, :json)
+    pretty = params.has_key?("pretty")
+    if as == :json
+      content_type "application/json"
+      LogStash::Json.dump(data, {:pretty => pretty})
+    else
+      content_type "text/plain"
+      data.to_s
+    end
+  end
+
+  def as_boolean(string)
+    return true   if string == true   || string =~ (/(true|t|yes|y|1)$/i)
+    return false  if string == false  || string.blank? || string =~ (/(false|f|no|n|0)$/i)
+    raise ArgumentError.new("invalid value for Boolean: \"#{string}\"")
+  end
+end
diff --git a/logstash-core/lib/logstash/api/command_factory.rb b/logstash-core/lib/logstash/api/command_factory.rb
new file mode 100644
index 00000000000..df26ccad546
--- /dev/null
+++ b/logstash-core/lib/logstash/api/command_factory.rb
@@ -0,0 +1,34 @@
+# encoding: utf-8
+require "logstash/api/service"
+require "logstash/api/commands/system/basicinfo_command"
+require "logstash/api/commands/system/plugins_command"
+require "logstash/api/commands/stats"
+
+
+module LogStash
+  module Api
+    class CommandFactory
+      attr_reader :factory, :service
+
+      def initialize(service)
+        @service = service
+        @factory = {
+          :system_basic_info => ::LogStash::Api::Commands::System::BasicInfo,
+          :plugins_command => ::LogStash::Api::Commands::System::Plugins,
+          :stats => ::LogStash::Api::Commands::Stats
+        }
+      end
+
+      def build(*klass_path)
+        # Get a nested path with args like (:parent, :child)
+        klass = klass_path.reduce(factory) {|acc,v| acc[v]}
+
+        if klass
+          klass.new(service)
+        else
+          raise ArgumentError, "Class path '#{klass_path}' does not map to command!"
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/base.rb b/logstash-core/lib/logstash/api/commands/base.rb
new file mode 100644
index 00000000000..1ca1d879832
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/base.rb
@@ -0,0 +1,25 @@
+module LogStash
+  module Api
+    module Commands
+      class Base
+        attr_reader :service
+        
+        def initialize(service = LogStash::Api::Service.instance)
+          @service = service
+        end
+
+        def hostname
+          service.agent.node_name
+        end
+
+        def uptime
+          service.agent.uptime
+        end
+
+        def started_at
+          (LogStash::Agent::STARTED_AT.to_f * 1000.0).to_i
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/stats.rb b/logstash-core/lib/logstash/api/commands/stats.rb
new file mode 100644
index 00000000000..6d4ef2430fc
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/stats.rb
@@ -0,0 +1,105 @@
+require "logstash/api/commands/base"
+require 'logstash/util/thread_dump'
+
+module LogStash
+  module Api
+    module Commands
+      class Stats < Commands::Base
+
+        def jvm
+          {:threads => service.get_shallow(:jvm, :threads)}
+        end
+
+        def process
+          service.get_shallow(:jvm, :process)
+        end
+
+        def events
+          service.get_shallow(:stats, :events)
+        end
+
+        def memory
+          memory = LogStash::Json.load(service.get(:jvm_memory_stats))
+          {
+            :heap_used_in_bytes => memory["heap"]["used_in_bytes"],
+            :heap_used_percent => memory["heap"]["used_percent"],
+            :heap_committed_in_bytes => memory["heap"]["committed_in_bytes"],
+            :heap_max_in_bytes => memory["heap"]["max_in_bytes"],
+            :heap_used_in_bytes => memory["heap"]["used_in_bytes"],
+            :non_heap_used_in_bytes => memory["non_heap"]["used_in_bytes"],
+            :non_heap_committed_in_bytes => memory["non_heap"]["committed_in_bytes"],
+            :pools => memory["pools"].inject({}) do |acc, (type, hash)|
+              hash.delete("committed_in_bytes")
+              acc[type] = hash
+              acc
+            end
+          }
+        end
+
+        def hot_threads(options={})
+          HotThreadsReport.new(self, options)
+        end
+
+        class HotThreadsReport
+          HOT_THREADS_STACK_TRACES_SIZE_DEFAULT = 10.freeze
+          
+          def initialize(cmd, options)
+            @cmd = cmd
+            filter = { :stacktrace_size => options.fetch(:stacktrace_size, HOT_THREADS_STACK_TRACES_SIZE_DEFAULT) }
+            jr_dump = JRMonitor.threads.generate(filter)
+            @thread_dump = ::LogStash::Util::ThreadDump.new(options.merge(:dump => jr_dump))
+          end
+          
+          def to_s
+            hash = to_hash
+            report =  "#{I18n.t("logstash.web_api.hot_threads.title", :hostname => hash[:hostname], :time => hash[:time], :top_count => @thread_dump.top_count )} \n"
+            report << '=' * 80
+            report << "\n"
+            hash[:threads].each do |thread|
+              thread_report = ""
+              thread_report = "#{I18n.t("logstash.web_api.
+                                hot_threads.thread_title", :percent_of_cpu_time => thread[:percent_of_cpu_time], :thread_state => thread[:state], :thread_name => thread[:name])} \n"
+              thread_report = "#{thread[:percent_of_cpu_time]} % of of cpu usage by #{thread[:state]} thread named '#{thread[:name]}'\n"
+              thread_report << "#{thread[:path]}\n" if thread[:path]
+              thread[:traces].each do |trace|
+                thread_report << "\t#{trace}\n"
+              end
+              report << thread_report
+              report << '-' * 80
+              report << "\n"
+            end
+            report
+          end
+
+          def to_hash
+            hash = { :hostname => @cmd.hostname, :time => Time.now.iso8601, :busiest_threads => @thread_dump.top_count, :threads => [] }
+            @thread_dump.each do |thread_name, _hash|
+              thread_name, thread_path = _hash["thread.name"].split(": ")
+              thread = { :name => thread_name,
+                         :percent_of_cpu_time => cpu_time_as_percent(_hash),
+                         :state => _hash["thread.state"]
+                       }
+              thread[:path] = thread_path if thread_path
+              traces = []
+              _hash["thread.stacktrace"].each do |trace|
+                traces << trace
+              end
+              thread[:traces] = traces unless traces.empty?
+              hash[:threads] << thread
+            end
+            hash
+          end
+
+          def cpu_time_as_percent(hash)
+            (((cpu_time(hash) / @cmd.uptime * 1.0)*10000).to_i)/100.0
+          end
+
+          def cpu_time(hash)
+            hash["cpu.time"] / 1000000.0
+          end
+
+        end       
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb b/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb
new file mode 100644
index 00000000000..7a32073990e
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require 'logstash/api/commands/base'
+require "logstash/util/duration_formatter"
+
+module LogStash
+  module Api
+    module Commands
+      module System
+        class BasicInfo < Commands::Base
+
+          def run
+            {
+              "hostname" => hostname,
+              "version" => {
+                "number" => LOGSTASH_VERSION
+              }
+            }
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/system/plugins_command.rb b/logstash-core/lib/logstash/api/commands/system/plugins_command.rb
new file mode 100644
index 00000000000..378f65e8598
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/system/plugins_command.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require "logstash/api/commands/base"
+
+module LogStash
+  module Api
+    module Commands
+      module System
+        class Plugins < Commands::Base
+          def run
+            { :total => plugins.count, :plugins => plugins }
+          end
+
+          private
+
+          def plugins
+            @plugins ||= find_plugins_gem_specs.map do |spec|
+              { :name => spec.name, :version => spec.version.to_s }
+            end.sort_by do |spec|
+              spec[:name]
+            end
+          end
+
+          def find_plugins_gem_specs
+            @specs ||= ::Gem::Specification.find_all.select{|spec| logstash_plugin_gem_spec?(spec)}
+          end
+
+          def logstash_plugin_gem_spec?(spec)
+            spec.metadata && spec.metadata["logstash_plugin"] == "true"
+          end
+
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/base.rb b/logstash-core/lib/logstash/api/modules/base.rb
new file mode 100644
index 00000000000..7a750d02f32
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/base.rb
@@ -0,0 +1,38 @@
+require "logstash/api/app_helpers"
+require "logstash/api/command_factory"
+
+module LogStash
+  module Api
+    module Modules
+      class Base < ::Sinatra::Base
+        helpers AppHelpers
+
+        # These options never change
+        # Sinatra isn't good at letting you change internal settings at runtime
+        # which is a requirement. We always propagate errors up and catch them
+        # in a custom rack handler in the RackApp class
+        set :environment, :production
+        set :raise_errors, true
+        set :show_exceptions, false
+
+        attr_reader :factory
+
+        include LogStash::Util::Loggable
+
+        helpers AppHelpers
+
+        def initialize(app=nil)
+          super(app)
+          @factory = ::LogStash::Api::CommandFactory.new(LogStash::Api::Service.instance)
+        end
+
+        not_found do
+          status 404
+          as   = params.has_key?("human") ? :string : :json
+          text = as == :string ? "" : {}
+          respond_with(text, :as => as)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/node.rb b/logstash-core/lib/logstash/api/modules/node.rb
new file mode 100644
index 00000000000..38ae44f7b7c
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/node.rb
@@ -0,0 +1,26 @@
+# encoding: utf-8
+require "logstash/api/modules/base"
+
+module LogStash
+  module Api
+    module Modules
+      class Node < ::LogStash::Api::Modules::Base
+        # return hot threads information
+        get "/hot_threads" do
+          ignore_idle_threads = params["ignore_idle_threads"] || true
+
+          options = {
+            :ignore_idle_threads => as_boolean(ignore_idle_threads),
+            :human => params.has_key?("human")
+          }
+          options[:threads] = params["threads"].to_i if params.has_key?("threads")
+
+          stats = factory.build(:stats)
+          as    = options[:human] ? :string : :json
+          respond_with(stats.hot_threads(options), {:as => as})
+        end
+
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/node_stats.rb b/logstash-core/lib/logstash/api/modules/node_stats.rb
new file mode 100644
index 00000000000..a744eb8ee33
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/node_stats.rb
@@ -0,0 +1,59 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class NodeStats < ::LogStash::Api::Modules::Base
+        
+        before do
+          @stats = factory.build(:stats)
+        end
+
+        # Global _stats resource where all information is
+        # retrieved and show
+        get "/" do          
+          payload = {
+            :events => events_payload,
+            :jvm => jvm_payload,
+            :process => process_payload
+          }
+
+          respond_with payload
+        end
+
+        # Show all events stats information
+        # (for ingested, emitted, dropped)
+        # - #events since startup
+        # - #data (bytes) since startup
+        # - events/s
+        # - bytes/s
+        # - dropped events/s
+        # - events in the pipeline
+        get "/events" do
+          respond_with({ :events => events_payload })
+        end
+
+        get "/jvm" do
+          respond_with :jvm => jvm_payload
+        end
+
+        get "/process" do
+          respond_with :process => process_payload
+        end
+
+        private
+
+        def events_payload
+          @stats.events
+        end
+
+        def jvm_payload
+          @stats.jvm
+        end
+
+        def process_payload
+          @stats.process
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/plugins.rb b/logstash-core/lib/logstash/api/modules/plugins.rb
new file mode 100644
index 00000000000..7edd3da154a
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/plugins.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Plugins < ::LogStash::Api::Modules::Base
+
+        get "/" do
+          command = factory.build(:plugins_command)
+          respond_with(command.run())
+        end
+
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/root.rb b/logstash-core/lib/logstash/api/modules/root.rb
new file mode 100644
index 00000000000..66dc4d8730b
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/root.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Root < ::LogStash::Api::Modules::Base
+        
+        get "/" do
+          command = factory.build(:system_basic_info)
+          respond_with command.run
+        end
+        
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/stats.rb b/logstash-core/lib/logstash/api/modules/stats.rb
new file mode 100644
index 00000000000..fde364f2432
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/stats.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Stats < ::LogStash::Api::Modules::Base
+
+        def stats_command
+          factory.build(:stats)
+        end
+
+        # Global _stats resource where all information is 
+        # retrieved and show
+        get "/" do
+          payload = {
+            :events => stats_command.events,
+            :jvm => { :memory => stats_command.memory }
+          }
+          respond_with payload
+        end
+
+
+        # return hot threads information
+        get "/jvm" do
+          jvm_payload = {
+            :timestamp => stats_command.started_at,
+            :uptime_in_millis => stats_command.uptime,
+            :mem => stats_command.memory
+          }
+          respond_with({:jvm => jvm_payload})
+        end
+
+        # Show all events stats information
+        # (for ingested, emitted, dropped)
+        # - #events since startup
+        # - #data (bytes) since startup
+        # - events/s
+        # - bytes/s
+        # - dropped events/s
+        # - events in the pipeline
+        get "/events" do
+          respond_with({ :events => stats_command.events })
+        end
+
+        # return hot threads information
+        get "/jvm/hot_threads" do
+          top_threads_count = params["threads"] || 3
+          ignore_idle_threads = params["ignore_idle_threads"] || true
+          options = {
+            :threads => top_threads_count.to_i,
+            :ignore_idle_threads => as_boolean(ignore_idle_threads)
+          }
+
+          respond_with(stats_command.hot_threads(options))
+        end
+
+        # return hot threads information
+        get "/jvm/memory" do
+          respond_with({ :memory => stats_command.memory })
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/rack_app.rb b/logstash-core/lib/logstash/api/rack_app.rb
new file mode 100644
index 00000000000..861e26d0697
--- /dev/null
+++ b/logstash-core/lib/logstash/api/rack_app.rb
@@ -0,0 +1,109 @@
+require "sinatra"
+require "rack"
+require "logstash/api/modules/base"
+require "logstash/api/modules/node"
+require "logstash/api/modules/node_stats"
+require "logstash/api/modules/plugins"
+require "logstash/api/modules/root"
+require "logstash/api/modules/stats"
+
+module LogStash
+  module Api
+    module RackApp
+      # Cabin is not compatible with CommonLogger, and this gives us more control anyway
+      METADATA_FIELDS = [:request_method, :path_info, :query_string, :http_version, :http_accept].freeze
+      def self.log_metadata(status, env)
+        METADATA_FIELDS.reduce({:status => status}) do |acc, field|
+          acc[field] = env[field.to_s.upcase]
+          acc
+        end        
+      end
+      
+      class ApiLogger
+        LOG_MESSAGE = "API HTTP Request".freeze
+        
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          res = @app.call(env)
+          status, headers, body = res
+          
+          if fatal_error?(status)
+            @logger.warn? && @logger.warn(LOG_MESSAGE, RackApp.log_metadata(status, env))                      
+          else          
+            @logger.info? && @logger.info(LOG_MESSAGE, RackApp.log_metadata(status, env))                      
+          end
+
+          res          
+        end
+
+        def fatal_error?(status)
+          status >= 500 && status < 600
+        end
+      end
+      
+      class ApiErrorHandler
+        LOG_MESSAGE = "Internal API server error".freeze
+        
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          @app.call(env)
+        rescue => e
+          body = RackApp.log_metadata(500, env).
+                   merge({
+                           :error => "Unexpected Internal Error",
+                           :class => e.class.name,
+                           :message => e.message,
+                           :backtrace => e.backtrace
+                         })
+
+          @logger.error(LOG_MESSAGE, body)
+          
+          [500,
+           {'Content-Type' => 'application/json'},
+           [LogStash::Json.dump(body)]
+          ]
+        end
+      end
+      
+      def self.app(logger, environment)
+        namespaces = rack_namespaces 
+        Rack::Builder.new do
+          # Custom logger object. Rack CommonLogger does not work with cabin
+          use ApiLogger, logger
+          
+          # In test env we want errors to propogate up the chain
+          # so we get easy to understand test failures.
+          # In production / dev we don't want a bad API endpoint
+          # to crash the process
+          if environment != "test"
+            use ApiErrorHandler, logger
+          end
+          
+          run LogStash::Api::Modules::Root
+          namespaces.each_pair do |namespace, app|
+            map(namespace) do
+              run app
+            end
+          end
+        end
+      end
+
+      def self.rack_namespaces
+        {
+          "/_node" => LogStash::Api::Modules::Node,
+          "/_stats" => LogStash::Api::Modules::Stats,
+          "/_node/stats" => LogStash::Api::Modules::NodeStats,
+          "/_plugins" => LogStash::Api::Modules::Plugins
+        }
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/service.rb b/logstash-core/lib/logstash/api/service.rb
new file mode 100644
index 00000000000..3eaeb2535ef
--- /dev/null
+++ b/logstash-core/lib/logstash/api/service.rb
@@ -0,0 +1,73 @@
+# encoding: utf-8
+require "logstash/instrument/collector"
+require "logstash/util/loggable"
+
+module LogStash
+  module Api
+    class Service
+
+      include Singleton
+      include LogStash::Util::Loggable
+
+      def initialize
+        @snapshot_rotation_mutex = Mutex.new
+        @snapshot = nil
+        logger.debug("[api-service] start") if logger.debug?
+        LogStash::Instrument::Collector.instance.add_observer(self)
+      end
+
+      def stop
+        logger.debug("[api-service] stop") if logger.debug?
+        LogStash::Instrument::Collector.instance.delete_observer(self)
+      end
+
+      def agent
+        LogStash::Instrument::Collector.instance.agent
+      end
+
+      def started?
+        !@snapshot.nil? && has_counters?        
+      end
+
+      def update(snapshot)
+        logger.debug("[api-service] snapshot received", :snapshot_time => snapshot.created_at) if logger.debug?
+
+        @snapshot_rotation_mutex.synchronize do
+          @snapshot = snapshot
+        end
+      end
+
+      def snapshot
+        @snapshot_rotation_mutex.synchronize { @snapshot }
+      end
+
+      def get_shallow(*path)
+        snapshot.metric_store.get_shallow(*path)
+      end
+
+      def get(key)
+        metric_store = @snapshot_rotation_mutex.synchronize { @snapshot.metric_store }
+        if key == :jvm_memory_stats
+          data = metric_store.get_shallow(:jvm, :memory)
+        else
+          data = metric_store.get_with_path("stats/events")
+        end
+        LogStash::Json.dump(data)
+      end
+
+      private
+
+      def has_counters?
+        (["LogStash::Instrument::MetricType::Counter", "LogStash::Instrument::MetricType::Gauge"] - metric_types).empty?
+      end
+
+      def metric_types
+        types = []
+        @snapshot_rotation_mutex.synchronize do
+          types = @snapshot.metric_store.all.map { |t| t.class.to_s }
+        end
+        return types
+      end
+    end
+  end
+end
diff --git a/lib/logstash/certs/cacert.pem b/logstash-core/lib/logstash/certs/cacert.pem
similarity index 100%
rename from lib/logstash/certs/cacert.pem
rename to logstash-core/lib/logstash/certs/cacert.pem
diff --git a/lib/logstash/codecs/base.rb b/logstash-core/lib/logstash/codecs/base.rb
similarity index 93%
rename from lib/logstash/codecs/base.rb
rename to logstash-core/lib/logstash/codecs/base.rb
index 25fad9da702..4fba2b21742 100644
--- a/lib/logstash/codecs/base.rb
+++ b/logstash-core/lib/logstash/codecs/base.rb
@@ -7,11 +7,16 @@
 # This is the base class for logstash codecs.
 module LogStash::Codecs; class Base < LogStash::Plugin
   include LogStash::Config::Mixin
+
   config_name "codec"
 
+  def self.plugin_type
+    "codec"
+  end
+
   def initialize(params={})
     super
-    config_init(params)
+    config_init(@params)
     register if respond_to?(:register)
   end
 
@@ -27,7 +32,7 @@ def encode(event)
     raise "#{self.class}#encode must be overidden"
   end # def encode
 
-  public 
+  public
   def close; end;
 
   # @param block [Proc(event, data)] the callback proc passing the original event and the encoded event
diff --git a/lib/logstash/config/config_ast.rb b/logstash-core/lib/logstash/config/config_ast.rb
similarity index 89%
rename from lib/logstash/config/config_ast.rb
rename to logstash-core/lib/logstash/config/config_ast.rb
index ace7322fedb..b235dded4ed 100644
--- a/lib/logstash/config/config_ast.rb
+++ b/logstash-core/lib/logstash/config/config_ast.rb
@@ -94,6 +94,7 @@ def compile
         @outputs = []
         @periodic_flushers = []
         @shutdown_flushers = []
+        @generated_objects = {}
       CODE
 
       sections = recursive_select(LogStash::Config::AST::PluginSection)
@@ -107,7 +108,11 @@ def compile
       ["filter", "output"].each do |type|
         # defines @filter_func and @output_func
 
-        definitions << "def #{type}_func(event)"
+        # This need to be defined as a singleton method
+        # so each instance of the pipeline has his own implementation
+        # of the output/filter function
+        definitions << "define_singleton_method :#{type}_func do |event|"
+        definitions << "  targeted_outputs = []" if type == "output"
         definitions << "  events = [event]" if type == "filter"
         definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", :event => event.to_hash)"
 
@@ -116,6 +121,7 @@ def compile
         end
 
         definitions << "  events" if type == "filter"
+        definitions << "  targeted_outputs" if type == "output"
         definitions << "end"
       end
 
@@ -132,7 +138,10 @@ class Whitespace < Node; end
   class PluginSection < Node
     # Global plugin numbering for the janky instance variable naming we use
     # like @filter_<name>_1
-    @@i = 0
+    def initialize(*args)
+      super(*args)
+      @i = 0
+    end
 
     # Generate ruby code to initialize all the plugins.
     def compile_initializer
@@ -142,31 +151,31 @@ def compile_initializer
 
 
         code << <<-CODE
-          #{name} = #{plugin.compile_initializer}
-          @#{plugin.plugin_type}s << #{name}
+          @generated_objects[:#{name}] = #{plugin.compile_initializer}
+          @#{plugin.plugin_type}s << @generated_objects[:#{name}]
         CODE
 
         # The flush method for this filter.
         if plugin.plugin_type == "filter"
 
           code << <<-CODE
-            #{name}_flush = lambda do |options, &block|
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name})
+            @generated_objects[:#{name}_flush] = lambda do |options, &block|
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}])
 
-              events = #{name}.flush(options)
+              events = @generated_objects[:#{name}].flush(options)
 
               return if events.nil? || events.empty?
 
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name}, :events => events)
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}], :events => events)
 
               #{plugin.compile_starting_here.gsub(/^/, "  ")}
 
               events.each{|e| block.call(e)}
             end
 
-            if #{name}.respond_to?(:flush)
-              @periodic_flushers << #{name}_flush if #{name}.periodic_flush
-              @shutdown_flushers << #{name}_flush
+            if @generated_objects[:#{name}].respond_to?(:flush)
+              @periodic_flushers << @generated_objects[:#{name}_flush] if @generated_objects[:#{name}].periodic_flush
+              @shutdown_flushers << @generated_objects[:#{name}_flush]
             end
           CODE
 
@@ -187,9 +196,10 @@ def generate_variables
 
       plugins.each do |plugin|
         # Unique number for every plugin.
-        @@i += 1
+        @i += 1
         # store things as ivars, like @filter_grok_3
-        var = "@#{plugin.plugin_type}_#{plugin.plugin_name}_#{@@i}"
+        var = :"#{plugin.plugin_type}_#{plugin.plugin_name}_#{@i}"
+        # puts("var=#{var.inspect}")
         @variables[plugin] = var
       end
       return @variables
@@ -231,13 +241,13 @@ def compile_initializer
     def compile
       case plugin_type
       when "input"
-        return "start_input(#{variable_name})"
+        return "start_input(@generated_objects[:#{variable_name}])"
       when "filter"
         return <<-CODE
-          events = #{variable_name}.multi_filter(events)
+          events = @generated_objects[:#{variable_name}].multi_filter(events)
         CODE
       when "output"
-        return "#{variable_name}.handle(event)\n"
+        return "targeted_outputs << @generated_objects[:#{variable_name}]\n"
       when "codec"
         settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
         attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
@@ -339,7 +349,7 @@ def validate!
 
       if duplicate_values.size > 0
         raise ConfigurationError.new(
-          I18n.t("logstash.agent.configuration.invalid_plugin_settings_duplicate_keys",
+          I18n.t("logstash.runner.configuration.invalid_plugin_settings_duplicate_keys",
                  :keys => duplicate_values.join(', '),
                  :line => input.line_of(interval.first),
                  :column => input.column_of(interval.first),
@@ -386,7 +396,7 @@ def compile
       if type == "filter"
         i = LogStash::Config::AST.defered_conditionals_index += 1
         source = <<-CODE
-          def cond_func_#{i}(input_events)
+          @generated_objects[:cond_func_#{i}] = lambda do |input_events|
             result = []
             input_events.each do |event|
               events = [event]
@@ -400,9 +410,9 @@ def cond_func_#{i}(input_events)
         LogStash::Config::AST.defered_conditionals << source
 
         <<-CODE
-          events = cond_func_#{i}(events)
+          events = @generated_objects[:cond_func_#{i}].call(events)
         CODE
-      else
+      else # Output
         <<-CODE
           #{super}
           end
@@ -507,7 +517,7 @@ def compile
   end
   class Selector < RValue
     def compile
-      return "event[#{text_value.inspect}]"
+      return "event.get(#{text_value.inspect})"
     end
   end
   class SelectorElement < Node; end
diff --git a/lib/logstash/config/cpu_core_strategy.rb b/logstash-core/lib/logstash/config/cpu_core_strategy.rb
similarity index 100%
rename from lib/logstash/config/cpu_core_strategy.rb
rename to logstash-core/lib/logstash/config/cpu_core_strategy.rb
diff --git a/lib/logstash/config/defaults.rb b/logstash-core/lib/logstash/config/defaults.rb
similarity index 60%
rename from lib/logstash/config/defaults.rb
rename to logstash-core/lib/logstash/config/defaults.rb
index ac3466f771d..c0c18fd7c04 100644
--- a/lib/logstash/config/defaults.rb
+++ b/logstash-core/lib/logstash/config/defaults.rb
@@ -6,6 +6,14 @@ module LogStash module Config module Defaults
 
   extend self
 
+  def input
+    "input { stdin { type => stdin } }"
+  end
+
+  def output
+    "output { stdout { codec => rubydebug } }"
+  end
+
   def cpu_cores
     Concurrent.processor_count
   end
diff --git a/lib/logstash/config/file.rb b/logstash-core/lib/logstash/config/file.rb
similarity index 100%
rename from lib/logstash/config/file.rb
rename to logstash-core/lib/logstash/config/file.rb
diff --git a/lib/logstash/config/grammar.rb b/logstash-core/lib/logstash/config/grammar.rb
similarity index 100%
rename from lib/logstash/config/grammar.rb
rename to logstash-core/lib/logstash/config/grammar.rb
diff --git a/lib/logstash/config/grammar.treetop b/logstash-core/lib/logstash/config/grammar.treetop
similarity index 100%
rename from lib/logstash/config/grammar.treetop
rename to logstash-core/lib/logstash/config/grammar.treetop
diff --git a/logstash-core/lib/logstash/config/loader.rb b/logstash-core/lib/logstash/config/loader.rb
new file mode 100644
index 00000000000..b260830d741
--- /dev/null
+++ b/logstash-core/lib/logstash/config/loader.rb
@@ -0,0 +1,95 @@
+require "logstash/config/defaults"
+
+module LogStash; module Config; class Loader
+  def initialize(logger)
+    @logger = logger
+    @config_debug = LogStash::SETTINGS.get_value("config.debug")
+  end
+
+  def format_config(config_path, config_string)
+    config_string = config_string.to_s
+    if config_path
+      # Append the config string.
+      # This allows users to provide both -f and -e flags. The combination
+      # is rare, but useful for debugging.
+      config_string = config_string + load_config(config_path)
+    else
+      # include a default stdin input if no inputs given
+      if config_string !~ /input *{/
+        config_string += LogStash::Config::Defaults.input
+      end
+      # include a default stdout output if no outputs given
+      if config_string !~ /output *{/
+        config_string += LogStash::Config::Defaults.output
+      end
+    end
+    config_string
+  end
+
+  def load_config(path)
+    begin
+      uri = URI.parse(path)
+
+      case uri.scheme
+      when nil then
+        local_config(path)
+      when /http/ then
+        fetch_config(uri)
+      when "file" then
+        local_config(uri.path)
+      else
+        fail(I18n.t("logstash.runner.configuration.scheme-not-supported", :path => path))
+      end
+    rescue URI::InvalidURIError
+      # fallback for windows.
+      # if the parsing of the file failed we assume we can reach it locally.
+      # some relative path on windows arent parsed correctly (.\logstash.conf)
+      local_config(path)
+    end
+  end
+
+  def local_config(path)
+    path = ::File.expand_path(path)
+    path = ::File.join(path, "*") if ::File.directory?(path)
+
+    if Dir.glob(path).length == 0
+      fail(I18n.t("logstash.runner.configuration.file-not-found", :path => path))
+    end
+
+    config = ""
+    encoding_issue_files = []
+    Dir.glob(path).sort.each do |file|
+      next unless ::File.file?(file)
+      if file.match(/~$/)
+        @logger.debug("NOT reading config file because it is a temp file", :config_file => file)
+        next
+      end
+      @logger.debug("Reading config file", :config_file => file)
+      cfg = ::File.read(file)
+      if !cfg.ascii_only? && !cfg.valid_encoding?
+        encoding_issue_files << file
+      end
+      config << cfg + "\n"
+      if @config_debug
+        @logger.debug? && @logger.debug("\nThe following is the content of a file", :config_file => file.to_s)
+        @logger.debug? && @logger.debug("\n" + cfg + "\n\n")
+      end
+    end
+    if encoding_issue_files.any?
+      fail("The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}")
+    end
+    if @config_debug
+      @logger.debug? && @logger.debug("\nThe following is the merged configuration")
+      @logger.debug? && @logger.debug("\n" + config + "\n\n")
+    end
+    return config
+  end # def load_config
+
+  def fetch_config(uri)
+    begin
+      Net::HTTP.get(uri) + "\n"
+    rescue Exception => e
+      fail(I18n.t("logstash.runner.configuration.fetch-failed", :path => uri.to_s, :message => e.message))
+    end
+  end
+end end end
diff --git a/lib/logstash/config/mixin.rb b/logstash-core/lib/logstash/config/mixin.rb
similarity index 86%
rename from lib/logstash/config/mixin.rb
rename to logstash-core/lib/logstash/config/mixin.rb
index cbfdcf62331..d5a1637f354 100644
--- a/lib/logstash/config/mixin.rb
+++ b/logstash-core/lib/logstash/config/mixin.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/namespace"
 require "logstash/config/registry"
+require "logstash/plugins/registry"
 require "logstash/logging"
 require "logstash/util/password"
 require "logstash/version"
@@ -38,6 +39,8 @@ module LogStash::Config::Mixin
   PLUGIN_VERSION_1_0_0 = LogStash::Util::PluginVersion.new(1, 0, 0)
   PLUGIN_VERSION_0_9_0 = LogStash::Util::PluginVersion.new(0, 9, 0)
 
+  ENV_PLACEHOLDER_REGEX = /\$\{(?<name>\w+)(\:(?<default>[^}]*))?\}/
+
   # This method is called when someone does 'include LogStash::Config'
   def self.included(base)
     # Add the DSL methods to the 'base' given.
@@ -51,7 +54,7 @@ def config_init(params)
     # Keep a copy of the original config params so that we can later
     # differentiate between explicit configuration and implicit (default)
     # configuration.
-    @original_params = params.clone
+    original_params = params.clone
     
     # store the plugin type, turns LogStash::Inputs::Base into 'input'
     @plugin_type = self.class.ancestors.find { |a| a.name =~ /::Base$/ }.config_name
@@ -74,7 +77,7 @@ def config_init(params)
         extra = opts[:obsolete].is_a?(String) ? opts[:obsolete] : ""
         extra.gsub!("%PLUGIN%", self.class.config_name)
         raise LogStash::ConfigurationError,
-          I18n.t("logstash.agent.configuration.obsolete", :name => name,
+          I18n.t("logstash.runner.configuration.obsolete", :name => name,
                  :plugin => self.class.config_name, :extra => extra)
       end
     end
@@ -99,9 +102,27 @@ def config_init(params)
       end
     end
 
+    # Resolve environment variables references
+    params.each do |name, value|
+      if (value.is_a?(Hash))
+        value.each do |valueHashKey, valueHashValue|
+          value[valueHashKey.to_s] = replace_env_placeholders(valueHashValue)
+        end
+      else
+        if (value.is_a?(Array))
+          value.each_index do |valueArrayIndex|
+            value[valueArrayIndex] = replace_env_placeholders(value[valueArrayIndex])
+          end
+        else
+          params[name.to_s] = replace_env_placeholders(value)
+        end
+      end
+    end
+
+
     if !self.class.validate(params)
       raise LogStash::ConfigurationError,
-        I18n.t("logstash.agent.configuration.invalid_plugin_settings")
+        I18n.t("logstash.runner.configuration.invalid_plugin_settings")
     end
 
     # We remove any config options marked as obsolete,
@@ -123,9 +144,35 @@ def config_init(params)
       instance_variable_set("@#{key}", value)
     end
 
+    # now that we know the parameters are valid, we can obfuscate the original copy
+    # of the parameters before storing them as an instance variable
+    self.class.secure_params!(original_params)
+    @original_params = original_params
+
     @config = params
   end # def config_init
 
+  # Replace all environment variable references in 'value' param by environment variable value and return updated value
+  # Process following patterns : $VAR, ${VAR}, ${VAR:defaultValue}
+  def replace_env_placeholders(value)
+    return value unless value.is_a?(String)
+
+    value.gsub(ENV_PLACEHOLDER_REGEX) do |placeholder|
+      # Note: Ruby docs claim[1] Regexp.last_match is thread-local and scoped to
+      # the call, so this should be thread-safe.
+      #
+      # [1] http://ruby-doc.org/core-2.1.1/Regexp.html#method-c-last_match
+      name = Regexp.last_match(:name)
+      default = Regexp.last_match(:default)
+
+      replacement = ENV.fetch(name, default)
+      if replacement.nil?
+        raise LogStash::ConfigurationError, "Cannot evaluate `#{placeholder}`. Environment variable `#{name}` is not set and there is no default value given."
+      end
+      replacement
+    end
+  end # def replace_env_placeholders
+
   module DSL
     attr_accessor :flags
 
@@ -134,8 +181,12 @@ module DSL
     def config_name(name = nil)
       @config_name = name if !name.nil?
       LogStash::Config::Registry.registry[@config_name] = self
+      if self.respond_to?("plugin_type")
+        declare_plugin(self.plugin_type, @config_name)
+      end
       return @config_name
     end
+    alias_method :config_plugin, :config_name
 
     # Deprecated: Declare the version of the plugin
     # inside the gemspec.
@@ -147,7 +198,7 @@ def plugin_status(status = nil)
     # inside the gemspec.
     def milestone(m = nil)
       @logger = Cabin::Channel.get(LogStash)
-      @logger.warn(I18n.t('logstash.plugin.deprecated_milestone', :plugin => config_name))
+      @logger.debug(I18n.t('logstash.plugin.deprecated_milestone', :plugin => config_name))
     end
 
     # Define a new configuration setting
@@ -287,7 +338,7 @@ def validate_check_required_parameter_names(params)
         elsif config_key.is_a?(String)
           next if params.keys.member?(config_key)
         end
-        @logger.error(I18n.t("logstash.agent.configuration.setting_missing",
+        @logger.error(I18n.t("logstash.runner.configuration.setting_missing",
                              :setting => config_key, :plugin => @plugin_name,
                              :type => @plugin_type))
         is_valid = false
@@ -314,7 +365,7 @@ def validate_check_parameter_values(params)
             # Used for converting values in the config to proper objects.
             params[key] = result if !result.nil?
           else
-            @logger.error(I18n.t("logstash.agent.configuration.setting_invalid",
+            @logger.error(I18n.t("logstash.runner.configuration.setting_invalid",
                                  :plugin => @plugin_name, :type => @plugin_type,
                                  :setting => key, :value => value.inspect,
                                  :value_type => config_val,
@@ -495,6 +546,14 @@ def validate_value(value, validator)
       return true, result
     end # def validate_value
 
+    def secure_params!(params)
+      params.each do |key, value|
+        if @config[key][:validate] == :password && !value.is_a?(::LogStash::Util::Password)
+          params[key] = ::LogStash::Util::Password.new(value)
+        end
+      end
+    end
+
     def hash_or_array(value)
       if !value.is_a?(Hash)
         value = [*value] # coerce scalar to array if necessary
diff --git a/lib/logstash/config/registry.rb b/logstash-core/lib/logstash/config/registry.rb
similarity index 100%
rename from lib/logstash/config/registry.rb
rename to logstash-core/lib/logstash/config/registry.rb
diff --git a/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
similarity index 60%
rename from lib/logstash/environment.rb
rename to logstash-core/lib/logstash/environment.rb
index 8f710eed088..9f1777498fd 100644
--- a/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -1,18 +1,40 @@
 # encoding: utf-8
 require "logstash/errors"
-require "logstash/version"
+require "logstash/config/cpu_core_strategy"
+require "logstash/settings"
 
 module LogStash
+
+  [
+            Setting::String.new("node.name", Socket.gethostname),
+            Setting::String.new("path.config", nil, false),
+            Setting::String.new("config.string", nil, false),
+           Setting::Boolean.new("config.test_and_exit", false),
+           Setting::Boolean.new("config.reload.automatic", false),
+           Setting::Numeric.new("config.reload.interval", 3),
+           Setting::Boolean.new("metric.collect", true) {|v| v == true }, # metric collection cannot be disabled
+            Setting::String.new("pipeline.id", "main"),
+           Setting::Numeric.new("pipeline.workers", LogStash::Config::CpuCoreStrategy.maximum),
+           Setting::Numeric.new("pipeline.output.workers", 1),
+           Setting::Numeric.new("pipeline.batch.size", 125),
+           Setting::Numeric.new("pipeline.batch.delay", 5), # in milliseconds
+           Setting::Boolean.new("pipeline.unsafe_shutdown", false),
+                    Setting.new("path.plugins", Array, []),
+            Setting::String.new("interactive", nil, false),
+           Setting::Boolean.new("config.debug", false),
+            Setting::String.new("log.level", "warn", true, ["quiet", "verbose", "warn", "debug"]),
+           Setting::Boolean.new("version", false),
+           Setting::Boolean.new("help", false),
+            Setting::String.new("path.log", nil, false),
+            Setting::String.new("log.format", "plain", true, ["json", "plain"]),
+            Setting::String.new("http.host", "127.0.0.1"),
+              Setting::Port.new("http.port", 9600),
+            Setting::String.new("http.environment", "production"),
+  ].each {|setting| SETTINGS.register(setting) }
+
   module Environment
     extend self
 
-    # rehydrate the bootstrap environment if the startup was not done by executing bootstrap.rb
-    # and we are in the context of the logstash package
-    if !LogStash::Environment.const_defined?("LOGSTASH_HOME") &&  !ENV["LOGSTASH_HOME"].to_s.empty?
-      $LOAD_PATH << ::File.join(ENV["LOGSTASH_HOME"], "lib")
-      require "bootstrap/environment"
-    end
-
     LOGSTASH_CORE = ::File.expand_path(::File.join(::File.dirname(__FILE__), "..", ".."))
     LOGSTASH_ENV = (ENV["LS_ENV"] || 'production').to_s.freeze
 
@@ -81,14 +103,6 @@ def windows?
       ::Gem.win_platform?
     end
 
-    def vendor_path(path)
-      return ::File.join(LOGSTASH_HOME, "vendor", path)
-    end
-
-    def pattern_path(path)
-      return ::File.join(LOGSTASH_HOME, "patterns", path)
-    end
-
     def locales_path(path)
       return ::File.join(LOGSTASH_CORE, "locales", path)
     end
diff --git a/lib/logstash/errors.rb b/logstash-core/lib/logstash/errors.rb
similarity index 100%
rename from lib/logstash/errors.rb
rename to logstash-core/lib/logstash/errors.rb
diff --git a/logstash-core/lib/logstash/filter_delegator.rb b/logstash-core/lib/logstash/filter_delegator.rb
new file mode 100644
index 00000000000..132d03f933e
--- /dev/null
+++ b/logstash-core/lib/logstash/filter_delegator.rb
@@ -0,0 +1,65 @@
+# encoding: utf-8
+#
+module LogStash
+  class FilterDelegator
+    extend Forwardable
+    DELEGATED_METHODS = [
+      :register,
+      :close,
+      :threadsafe?,
+      :do_close,
+      :do_stop,
+      :periodic_flush
+    ]
+    def_delegators :@filter, *DELEGATED_METHODS
+
+    def initialize(logger, klass, metric, *args)
+      options = args.reduce({}, :merge)
+
+      @logger = logger
+      @klass = klass
+      @filter = klass.new(options)
+
+      # Scope the metrics to the plugin
+      namespaced_metric = metric.namespace(@filter.plugin_unique_name.to_sym)
+      @filter.metric = metric
+
+      @metric_events = namespaced_metric.namespace(:events)
+
+      # Not all the filters will do bufferings
+      define_flush_method if @filter.respond_to?(:flush)
+    end
+
+    def config_name
+      @klass.config_name
+    end
+
+    def multi_filter(events)
+      @metric_events.increment(:in, events.size)
+
+      new_events = @filter.multi_filter(events)
+
+      # There is no garantee in the context of filter
+      # that EVENTS_INT == EVENTS_OUT, see the aggregates and
+      # the split filter
+      c = new_events.count { |event| !event.cancelled? }
+      @metric_events.increment(:out, c) if c > 0
+
+      return new_events
+    end
+
+    private
+    def define_flush_method
+      define_singleton_method(:flush) do |options = {}|
+        # we also need to trace the number of events
+        # coming from a specific filters.
+        new_events = @filter.flush(options)
+
+        # Filter plugins that does buffering or spooling of events like the
+        # `Logstash-filter-aggregates` can return `NIL` and will flush on the next flush ticks.
+        @metric_events.increment(:out, new_events.size) if new_events && new_events.size > 0
+        new_events
+      end
+    end
+  end
+end
diff --git a/lib/logstash/filters/base.rb b/logstash-core/lib/logstash/filters/base.rb
similarity index 92%
rename from lib/logstash/filters/base.rb
rename to logstash-core/lib/logstash/filters/base.rb
index 4ce752a0e33..d97c2342d63 100644
--- a/lib/logstash/filters/base.rb
+++ b/logstash-core/lib/logstash/filters/base.rb
@@ -117,10 +117,14 @@ class LogStash::Filters::Base < LogStash::Plugin
   # Optional.
   config :periodic_flush, :validate => :boolean, :default => false
 
+  def self.plugin_type
+    "filter"
+  end
+
   public
   def initialize(params)
     super
-    config_init(params)
+    config_init(@params)
     @threadsafe = true
   end # def initialize
 
@@ -143,6 +147,7 @@ def filter(event)
   # @return [Array<LogStash::Event] filtered events and any new events generated by the filter
   public
   def multi_filter(events)
+    LogStash::Util.set_thread_plugin(self)
     result = []
     events.each do |event|
       unless event.cancelled?
@@ -178,12 +183,16 @@ def filter_matched(event)
 
     LogStash::Util::Decorators.add_tags(@add_tag,event,"filters/#{self.class.name}")
 
+    # note below that the tags array field needs to be updated then reassigned to the event.
+    # this is important because a construct like event["tags"].delete(tag) will not work
+    # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
     @remove_tag.each do |tag|
-      break if event["tags"].nil?
+      tags = event.get("tags")
+      break if tags.nil? || tags.empty?
       tag = event.sprintf(tag)
-      @logger.debug? and @logger.debug("filters/#{self.class.name}: removing tag",
-                                       :tag => tag)
-      event["tags"].delete(tag)
+      @logger.debug? and @logger.debug("filters/#{self.class.name}: removing tag", :tag => tag)
+      tags.delete(tag)
+      event.set("tags", tags)
     end
   end # def filter_matched
 
diff --git a/lib/logstash/inputs/base.rb b/logstash-core/lib/logstash/inputs/base.rb
similarity index 95%
rename from lib/logstash/inputs/base.rb
rename to logstash-core/lib/logstash/inputs/base.rb
index f72dfd743ac..6249c1b4e1c 100644
--- a/lib/logstash/inputs/base.rb
+++ b/logstash-core/lib/logstash/inputs/base.rb
@@ -10,6 +10,7 @@
 # This is the base class for Logstash inputs.
 class LogStash::Inputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
+
   config_name "input"
 
   # Add a `type` field to all events handled by this input.
@@ -48,12 +49,16 @@ class LogStash::Inputs::Base < LogStash::Plugin
   attr_accessor :params
   attr_accessor :threadable
 
+  def self.plugin_type
+    "input"
+  end
+
   public
   def initialize(params={})
     super
     @threadable = false
     @stop_called = Concurrent::AtomicBoolean.new(false)
-    config_init(params)
+    config_init(@params)
     @tags ||= []
   end # def initialize
 
@@ -78,7 +83,7 @@ def stop
 
   public
   def do_stop
-    @logger.debug("stopping", :plugin => self)
+    @logger.debug("stopping", :plugin => self.class.name)
     @stop_called.make_true
     stop
   end
@@ -92,7 +97,7 @@ def stop?
   protected
   def decorate(event)
     # Only set 'type' if not already set. This is backwards-compatible behavior
-    event["type"] = @type if @type && !event.include?("type")
+    event.set("type", @type) if @type && !event.include?("type")
 
     LogStash::Util::Decorators.add_fields(@add_field,event,"inputs/#{self.class.name}")
     LogStash::Util::Decorators.add_tags(@tags,event,"inputs/#{self.class.name}")
diff --git a/logstash-core/lib/logstash/inputs/metrics.rb b/logstash-core/lib/logstash/inputs/metrics.rb
new file mode 100644
index 00000000000..8a8ce92dcf0
--- /dev/null
+++ b/logstash-core/lib/logstash/inputs/metrics.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+require "logstash/event"
+require "logstash/inputs/base"
+require "logstash/instrument/collector"
+
+module LogStash module Inputs
+  # The Metrics inputs is responable of registring itself to the collector.
+  # The collector class will periodically emits new snapshot of the system,
+  # The metrics need to take that information and transform it into
+  # a `Logstash::Event`, which can be consumed by the shipper and send to
+  # Elasticsearch
+  class Metrics < LogStash::Inputs::Base
+    config_name "metrics"
+    milestone 3
+
+    def register
+    end
+
+    def run(queue)
+      @logger.debug("Metric: input started")
+      @queue = queue
+
+      # we register to the collector after receiving the pipeline queue
+      LogStash::Instrument::Collector.instance.add_observer(self)
+
+      # Keep this plugin thread alive,
+      # until we shutdown the metric pipeline
+      sleep(1) while !stop?
+    end
+
+    def stop
+      @logger.debug("Metrics input: stopped")
+      LogStash::Instrument::Collector.instance.delete_observer(self)
+    end
+
+    def update(snapshot)
+      @logger.debug("Metrics input: received a new snapshot", :created_at => snapshot.created_at, :snapshot => snapshot, :event => snapshot.metric_store.to_event) if @logger.debug?
+
+      # The back pressure is handled in the collector's
+      # scheduled task (running into his own thread) if something append to one of the listener it will
+      # will timeout. In a sane pipeline, with a low traffic of events it shouldn't be a problems.
+      snapshot.metric_store.each do |metric|
+        @queue << LogStash::Event.new({ "@timestamp" => snapshot.created_at }.merge(metric.to_hash))
+      end
+    end
+  end
+end;end
diff --git a/lib/logstash/inputs/threadable.rb b/logstash-core/lib/logstash/inputs/threadable.rb
similarity index 100%
rename from lib/logstash/inputs/threadable.rb
rename to logstash-core/lib/logstash/inputs/threadable.rb
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
new file mode 100644
index 00000000000..302fe4c3ac7
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/collector.rb
@@ -0,0 +1,109 @@
+# encoding: utf-8
+require "logstash/instrument/snapshot"
+require "logstash/instrument/metric_store"
+require "logstash/util/loggable"
+require "concurrent/timer_task"
+require "observer"
+require "singleton"
+require "thread"
+
+module LogStash module Instrument
+  # The Collector singleton is the single point of reference for all
+  # the metrics collection inside logstash, the metrics library will make
+  # direct calls to this class.
+  #
+  # This class is an observable responsable of periodically emitting view of the system
+  # to other components like the internal metrics pipelines.
+  class Collector
+    include LogStash::Util::Loggable
+    include Observable
+    include Singleton
+
+    SNAPSHOT_ROTATION_TIME_SECS = 1 # seconds
+    SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS = 10 * 60 # seconds
+
+    attr_accessor :agent
+
+    def initialize
+      @metric_store = MetricStore.new
+      @agent = nil
+      start_periodic_snapshotting
+    end
+
+    # The metric library will call this unique interface
+    # its the job of the collector to update the store with new metric
+    # of update the metric
+    #
+    # If there is a problem with the key or the type of metric we will record an error
+    # but we wont stop processing events, theses errors are not considered fatal.
+    #
+    def push(namespaces_path, key, type, *metric_type_params)
+      begin
+        metric = @metric_store.fetch_or_store(namespaces_path, key) do
+          LogStash::Instrument::MetricType.create(type, namespaces_path, key)
+        end
+
+        metric.execute(*metric_type_params)
+
+        changed # we had changes coming in so we can notify the observers
+      rescue MetricStore::NamespacesExpectedError => e
+        logger.error("Collector: Cannot record metric", :exception => e)
+      rescue NameError => e
+        logger.error("Collector: Cannot create concrete class for this metric type",
+                     :type => type,
+                     :namespaces_path => namespaces_path,
+                     :key => key,
+                     :metrics_params => metric_type_params,
+                     :exception => e,
+                     :stacktrace => e.backtrace)
+      end
+    end
+
+    def clear
+      @metric_store = MetricStore.new
+    end
+
+    # Monitor the `Concurrent::TimerTask` this update is triggered on every successful or not
+    # run of the task, TimerTask implement Observable and the collector acts as
+    # the observer and will keep track if something went wrong in the execution.
+    #
+    # @param [Time] Time of execution
+    # @param [result] Result of the execution
+    # @param [Exception] Exception
+    def update(time_of_execution, result, exception)
+      return true if exception.nil?
+      logger.error("Collector: Something went wrong went sending data to the observers",
+                   :execution_time => time_of_execution,
+                   :result => result,
+                   :exception => exception.class.name)
+    end
+
+    # Snapshot the current Metric Store and return it immediately,
+    # This is useful if you want to get access to the current metric store without
+    # waiting for a periodic call.
+    #
+    # @return [LogStash::Instrument::MetricStore]
+    def snapshot_metric
+      Snapshot.new(@metric_store)
+    end
+
+    # Configure and start the periodic task for snapshotting the `MetricStore`
+    def start_periodic_snapshotting
+      @snapshot_task = Concurrent::TimerTask.new { publish_snapshot }
+      @snapshot_task.execution_interval = SNAPSHOT_ROTATION_TIME_SECS
+      @snapshot_task.timeout_interval = SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS
+      @snapshot_task.add_observer(self)
+      @snapshot_task.execute
+    end
+
+    # Create a snapshot of the MetricStore and send it to to the registered observers
+    # The observer will receive the following signature in the update methode.
+    #
+    # `#update(created_at, metric_store)`
+    def publish_snapshot
+      created_at = Time.now
+      logger.debug("Collector: Sending snapshot to observers", :created_at => created_at) if logger.debug?
+      notify_observers(snapshot_metric)
+    end
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/metric.rb b/logstash-core/lib/logstash/instrument/metric.rb
new file mode 100644
index 00000000000..601c7b0ed4b
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric.rb
@@ -0,0 +1,102 @@
+# encoding: utf-8
+require "logstash/instrument/collector"
+require "concurrent"
+
+module LogStash module Instrument
+  class MetricException < Exception; end
+  class MetricNoKeyProvided < MetricException; end
+  class MetricNoBlockProvided < MetricException; end
+  class MetricNoNamespaceProvided < MetricException; end
+
+  # This class provide the interface between the code, the collector and the format
+  # of the recorded metric.
+  class Metric
+    attr_reader :collector
+
+    def initialize(collector = LogStash::Instrument::Collector.instance)
+      @collector = collector
+    end
+
+    def increment(namespace, key, value = 1)
+      validate_key!(key)
+      collector.push(namespace, key, :counter, :increment, value)
+    end
+
+    def decrement(namespace, key, value = 1)
+      validate_key!(key)
+      collector.push(namespace, key, :counter, :decrement, value)
+    end
+
+    def gauge(namespace, key, value)
+      validate_key!(key)
+      collector.push(namespace, key, :gauge, :set, value)
+    end
+
+    def time(namespace, key)
+      validate_key!(key)
+
+      if block_given?
+        timer = TimedExecution.new(self, namespace, key)
+        content = yield
+        timer.stop
+        return content
+      else
+        TimedExecution.new(self, namespace, key)
+      end
+    end
+
+    def report_time(namespace, key, duration)
+      collector.push(namespace, key, :mean, :increment, duration)
+    end
+
+    # This method return a metric instance tied to a specific namespace
+    # so instead of specifying the namespace on every call.
+    #
+    # Example:
+    #   metric.increment(:namespace, :mykey, 200)
+    #   metric.increment(:namespace, :mykey_2, 200)
+    #
+    #   namespaced_metric = metric.namespace(:namespace)
+    #   namespaced_metric.increment(:mykey, 200)
+    #   namespaced_metric.increment(:mykey_2, 200)
+    # ```
+    #
+    # @param name [Array<String>] Name of the namespace
+    # @param name [String] Name of the namespace
+    def namespace(name)
+      raise MetricNoNamespaceProvided if name.nil? || name.empty?
+
+      NamespacedMetric.new(self, name)
+    end
+
+    private
+    def validate_key!(key)
+      raise MetricNoKeyProvided if key.nil? || key.empty?
+    end
+
+    # Allow to calculate the execution of a block of code.
+    # This class support 2 differents syntax a block or the return of
+    # the object itself, but in the later case the metric wont be recorded
+    # Until we call `#stop`.
+    #
+    # @see LogStash::Instrument::Metric#time
+    class TimedExecution
+      MILLISECONDS = 1_000_000.0.freeze
+
+      def initialize(metric, namespace, key)
+        @metric = metric
+        @namespace = namespace
+        @key = key
+        start
+      end
+
+      def start
+        @start_time = Time.now
+      end
+
+      def stop
+        @metric.report_time(@namespace, @key, (MILLISECONDS * (Time.now - @start_time)).to_i)
+      end
+    end
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_store.rb b/logstash-core/lib/logstash/instrument/metric_store.rb
new file mode 100644
index 00000000000..653f6774440
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric_store.rb
@@ -0,0 +1,244 @@
+# encoding: utf-8
+require "concurrent"
+require "logstash/event"
+require "logstash/instrument/metric_type"
+require "thread"
+
+module LogStash module Instrument
+  # The Metric store the data structure that make sure the data is
+  # saved in a retrievable way, this is a wrapper around multiples ConcurrentHashMap
+  # acting as a tree like structure.
+  class MetricStore
+    class NamespacesExpectedError < StandardError; end
+    class MetricNotFound < StandardError; end
+
+    KEY_PATH_SEPARATOR = "/".freeze
+
+    # Lets me a bit flexible on the coma usage in the path
+    # definition
+    FILTER_KEYS_SEPARATOR = /\s?*,\s*/.freeze
+
+    def initialize
+      # We keep the structured cache to allow
+      # the api to search the content of the differents nodes
+      @store = Concurrent::Map.new
+
+      # This hash has only one dimension
+      # and allow fast retrieval of the metrics
+      @fast_lookup = Concurrent::Map.new
+
+      # This Mutex block the critical section for the
+      # structured hash, it block the zone when we first insert a metric
+      # in the structured hash or when we query it for search or to make
+      # the result available in the API.
+      @structured_lookup_mutex = Mutex.new
+    end
+
+    # This method use the namespace and key to search the corresponding value of
+    # the hash, if it doesn't exist it will create the appropriate namespaces
+    # path in the hash and return `new_value`
+    #
+    # @param [Array] The path where the values should be located
+    # @param [Symbol] The metric key
+    # @return [Object] Return the new_value of the retrieve object in the tree
+    def fetch_or_store(namespaces, key, default_value = nil)
+      provided_value =  block_given? ? yield(key) : default_value
+
+      # We first check in the `@fast_lookup` store to see if we have already see that metrics before,
+      # This give us a `o(1)` access, which is faster than searching through the structured
+      # data store (Which is a `o(n)` operation where `n` is the number of element in the namespace and
+      # the value of the key). If the metric is already present in the `@fast_lookup`, the call to
+      # `#put_if_absent` will return the value. This value is send back directly to the caller.
+      #
+      # BUT. If the value is not present in the `@fast_lookup` the value will be inserted and
+      # `#puf_if_absent` will return nil. With this returned value of nil we assume that we don't
+      # have it in the `@metric_store` for structured search so we add it there too.
+      if found_value = @fast_lookup.put_if_absent([namespaces, key], provided_value)
+        return found_value
+      else
+        @structured_lookup_mutex.synchronize do
+          # If we cannot find the value this mean we need to save it in the store.
+          fetch_or_store_namespaces(namespaces).fetch_or_store(key, provided_value)
+        end
+        return provided_value
+      end
+    end
+
+    # This method allow to retrieve values for a specific path,
+    # This method support the following queries
+    #
+    # stats/pipelines/pipeline_X
+    # stats/pipelines/pipeline_X,pipeline_2
+    # stats/os,jvm
+    #
+    # If you use the `,` on a key the metric store will return the both values at that level
+    #
+    # The returned hash will keep the same structure as it had in the `Concurrent::Map`
+    # but will be a normal ruby hash. This will allow the api to easily seriliaze the content
+    # of the map
+    #
+    # @param [Array] The path where values should be located
+    # @return [Hash]
+    def get_with_path(path)
+      key_paths = path.gsub(/^#{KEY_PATH_SEPARATOR}+/, "").split(KEY_PATH_SEPARATOR)
+      get(*key_paths)
+    end
+
+    # Similar to `get_with_path` but use symbols instead of string
+    #
+    # @param [Array<Symbol>]
+    # @return [Hash]
+    def get(*key_paths)
+      # Normalize the symbols access
+      key_paths.map(&:to_sym)
+      new_hash = Hash.new
+
+      @structured_lookup_mutex.synchronize do
+        get_recursively(key_paths, @store, new_hash)
+      end
+
+      new_hash
+    end
+
+    # Retrieve values like `get`, but don't return them fully nested.
+    # This means that if you call `get_shallow(:foo, :bar)` the result will not
+    # be nested inside of `{:foo {:bar => values}`.
+    #
+    # @param [Array<Symbol>]
+    # @return [Hash]
+    def get_shallow(*key_paths)
+      key_paths.reduce(get(*key_paths)) {|acc, p| acc[p]}
+    end
+
+    # Return all the individuals Metric,
+    # This call mimic a Enum's each if a block is provided
+    #
+    # @param path [String] The search path for metrics
+    # @param [Array] The metric for the specific path
+    def each(path = nil, &block)
+      metrics = if path.nil?
+        get_all
+      else
+        transform_to_array(get_with_path(path))
+      end
+
+      block_given? ? metrics.each(&block) : metrics
+    end
+    alias_method :all, :each
+
+    private
+    def get_all
+      @fast_lookup.values
+    end
+
+    # This method take an array of keys and recursively search the metric store structure
+    # and return a filtered hash of the structure. This method also take into consideration
+    # getting two different branchs.
+    #
+    #
+    # If one part of the `key_paths` contains a filter key with the following format.
+    # "pipeline01, pipeline_02", It know that need to fetch the branch `pipeline01` and `pipeline02`
+    #
+    # Look at the rspec test for more usage.
+    #
+    # @param key_paths [Array<Symbol>] The list of keys part to filter
+    # @param map [Concurrent::Map] The the part of map to search in
+    # @param new_hash [Hash] The hash to populate with the results.
+    # @return Hash
+    def get_recursively(key_paths, map, new_hash)
+      key_candidates = extract_filter_keys(key_paths.shift)
+
+      key_candidates.each do |key_candidate|
+        raise MetricNotFound, "For path: #{key_candidate}" if map[key_candidate].nil?
+
+        if key_paths.empty? # End of the user requested path
+          if map[key_candidate].is_a?(Concurrent::Map)
+            new_hash[key_candidate] = transform_to_hash(map[key_candidate])
+          else
+            new_hash[key_candidate] = map[key_candidate]
+          end
+        else
+          if map[key_candidate].is_a?(Concurrent::Map)
+            new_hash[key_candidate] = get_recursively(key_paths, map[key_candidate], {})
+          else
+            new_hash[key_candidate] = map[key_candidate]
+          end
+        end
+      end
+      return new_hash
+    end
+
+    def extract_filter_keys(key)
+      key.to_s.strip.split(FILTER_KEYS_SEPARATOR).map(&:to_sym)
+    end
+
+    # Take a hash and recursively flatten it into an array.
+    # This is useful if you are only interested in the leaf of the tree.
+    # Mostly used with `each` to get all the metrics from a specific namespaces
+    #
+    # This could be moved to `LogStash::Util` once this api stabilize
+    #
+    # @return [Array] One dimension array
+     def transform_to_array(map)
+      map.values.collect do |value|
+        value.is_a?(Hash) ? transform_to_array(value) : value
+      end.flatten
+    end
+
+    # Transform the Concurrent::Map hash into a ruby hash format,
+    # This is used to be serialize at the web api layer.
+    #
+    # This could be moved to `LogStash::Util` once this api stabilize
+    #
+    # @return [Hash]
+    def transform_to_hash(map, new_hash = Hash.new)
+      map.each_pair do |key, value|
+        if value.is_a?(Concurrent::Map)
+          new_hash[key] = {}
+          transform_to_hash(value, new_hash[key])
+        else
+          new_hash[key] = value
+        end
+      end
+
+      return new_hash
+    end
+
+    # This method iterate through the namespace path and try to find the corresponding
+    # value for the path, if any part of the path is not found it will
+    # create it.
+    #
+    # @param [Array] The path where values should be located
+    # @raise [ConcurrentMapExpected] Raise if the retrieved object isn't a `Concurrent::Map`
+    # @return [Concurrent::Map] Map where the metrics should be saved
+    def fetch_or_store_namespaces(namespaces_path)
+      path_map = fetch_or_store_namespace_recursively(@store, namespaces_path)
+
+      # This mean one of the namespace and key are colliding
+      # and we have to deal it upstream.
+      unless path_map.is_a?(Concurrent::Map)
+        raise NamespacesExpectedError, "Expecting a `Namespaces` but found class:  #{path_map.class.name} for namespaces_path: #{namespaces_path}"
+      end
+
+      return path_map
+    end
+
+    # Recursively fetch or create the namespace paths through the `MetricStove`
+    # This algorithm use an index to known which keys to search in the map.
+    # This doesn't cloning the array if we want to give a better feedback to the user
+    #
+    # @param [Concurrent::Map] Map to search for the key
+    # @param [Array] List of path to create
+    # @param [Fixnum] Which part from the list to create
+    #
+    def fetch_or_store_namespace_recursively(map, namespaces_path, idx = 0)
+      current = namespaces_path[idx]
+
+      # we are at the end of the namespace path, break out of the recursion
+      return map if current.nil?
+
+      new_map = map.fetch_or_store(current) { Concurrent::Map.new }
+      return fetch_or_store_namespace_recursively(new_map, namespaces_path, idx + 1)
+    end
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type.rb b/logstash-core/lib/logstash/instrument/metric_type.rb
new file mode 100644
index 00000000000..127d43ce3b0
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric_type.rb
@@ -0,0 +1,24 @@
+# encoding: utf-8
+require "logstash/instrument/metric_type/counter"
+require "logstash/instrument/metric_type/mean"
+require "logstash/instrument/metric_type/gauge"
+
+module LogStash module Instrument
+  module MetricType
+    METRIC_TYPE_LIST = {
+      :counter => LogStash::Instrument::MetricType::Counter,
+      :mean => LogStash::Instrument::MetricType::Mean,
+      :gauge => LogStash::Instrument::MetricType::Gauge
+    }.freeze
+
+    # Use the string to generate a concrete class for this metrics
+    #
+    # @param [String] The name of the class
+    # @param [Array] Namespaces list
+    # @param [String] The metric key
+    # @raise [NameError] If the class is not found
+    def self.create(type, namespaces, key)
+      METRIC_TYPE_LIST[type].new(namespaces, key)
+    end
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/base.rb b/logstash-core/lib/logstash/instrument/metric_type/base.rb
new file mode 100644
index 00000000000..5711c3f83b6
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric_type/base.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require "logstash/event"
+require "logstash/util"
+
+module LogStash module Instrument module MetricType
+  class Base
+    attr_reader :namespaces, :key
+
+    def initialize(namespaces, key)
+      @namespaces = namespaces
+      @key = key
+    end
+
+    def inspect
+      "#{self.class.name} - namespaces: #{namespaces} key: #{key} value: #{value}"
+    end
+
+    def to_hash
+      {
+        "namespaces" => namespaces,
+        "key" => key,
+        "type" => type,
+        "value" => value
+      }
+    end
+
+    def to_json_data
+      value
+    end
+
+    def type
+      @type ||= LogStash::Util.class_name(self).downcase
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/counter.rb b/logstash-core/lib/logstash/instrument/metric_type/counter.rb
new file mode 100644
index 00000000000..e99bca57939
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric_type/counter.rb
@@ -0,0 +1,29 @@
+# encoding: utf-8
+require "logstash/instrument/metric_type/base"
+require "concurrent"
+
+module LogStash module Instrument module MetricType
+  class Counter < Base
+    def initialize(namespaces, key, value = 0)
+      super(namespaces, key)
+
+      @counter = Concurrent::AtomicFixnum.new(value)
+    end
+
+    def increment(value = 1)
+      @counter.increment(value)
+    end
+
+    def decrement(value = 1)
+      @counter.decrement(value)
+    end
+
+    def execute(action, value = 1)
+      @counter.send(action, value)
+    end
+
+    def value
+      @counter.value
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/gauge.rb b/logstash-core/lib/logstash/instrument/metric_type/gauge.rb
new file mode 100644
index 00000000000..7981bc877a5
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric_type/gauge.rb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require "logstash/instrument/metric_type/base"
+require "concurrent/atomic_reference/mutex_atomic"
+require "logstash/json"
+
+module LogStash module Instrument module MetricType
+  class Gauge < Base
+    def initialize(namespaces, key)
+      super(namespaces, key)
+
+      @gauge = Concurrent::MutexAtomicReference.new()
+    end
+
+    def execute(action, value = nil)
+      @gauge.set(value)
+    end
+
+    def value
+      @gauge.get
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/mean.rb b/logstash-core/lib/logstash/instrument/metric_type/mean.rb
new file mode 100644
index 00000000000..f2cf7c5bc46
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric_type/mean.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require "logstash/instrument/metric_type/base"
+require "concurrent"
+
+module LogStash module Instrument module MetricType
+  class Mean < Base
+    def initialize(namespaces, key)
+      super(namespaces, key)
+
+      @counter = Concurrent::AtomicFixnum.new
+      @sum = Concurrent::AtomicFixnum.new
+    end
+
+    def increment(value = 1)
+      @counter.increment
+      @sum.increment(value)
+    end
+
+    def decrement(value = 1)
+      @counter.decrement
+      @sum.decrement(value)
+    end
+
+    def mean
+      if @counter > 0
+        @sum.value / @counter.value
+      else
+        0
+      end
+    end
+    alias_method :value, :mean
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/namespaced_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
new file mode 100644
index 00000000000..6b0ad020e60
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
@@ -0,0 +1,54 @@
+# encoding: utf-8
+require "logstash/instrument/metric"
+
+module LogStash module Instrument
+  # This class acts a a proxy between the metric library and the user calls.
+  #
+  # This is the class that plugins authors will use to interact with the `MetricStore`
+  # It has the same public interface as `Metric` class but doesnt require to send
+  # the namespace on every call.
+  #
+  # @see Logstash::Instrument::Metric
+  class NamespacedMetric
+    attr_reader :namespace_name
+    # Create metric with a specific namespace
+    #
+    # @param metric [LogStash::Instrument::Metric] The metric instance to proxy
+    # @param namespace [Array] The namespace to use
+    def initialize(metric, namespace_name)
+      @metric = metric
+      @namespace_name = Array(namespace_name)
+    end
+
+    def increment(key, value = 1)
+      @metric.increment(namespace_name, key, value)
+    end
+
+    def decrement(namespace, key, value = 1)
+      @metric.decrement(namespace_name, key, value)
+    end
+
+    def gauge(key, value)
+      @metric.gauge(namespace_name, key, value)
+    end
+
+    def report_time(key, duration)
+      @metric.report_time(namespace_name, key, duration)
+    end
+
+    def time(key, &block)
+      @metric.time(namespace_name, key, &block)
+    end
+
+    def collector
+      @metric.collector
+    end
+
+    def namespace(name)
+      NamespacedMetric.new(metric, namespace_name.concat(Array(name)))
+    end
+
+    private
+    attr_reader :metric
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/null_metric.rb b/logstash-core/lib/logstash/instrument/null_metric.rb
new file mode 100644
index 00000000000..b8054b766dc
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/null_metric.rb
@@ -0,0 +1,46 @@
+# encoding: utf-8
+require "logstash/instrument/metric"
+
+module LogStash module Instrument
+ # This class is used in the context when we disable the metric collection
+ # for specific plugin to replace the `NamespacedMetric` class with this one
+ # which doesn't produce any metric to the collector.
+ class NullMetric
+   attr_reader :namespace_name, :collector
+
+   def increment(key, value = 1)
+   end
+
+   def decrement(namespace, key, value = 1)
+   end
+
+   def gauge(key, value)
+   end
+
+   def report_time(key, duration)
+   end
+
+   # We have to manually redefine this method since it can return an
+   # object this object also has to be implemented as a NullObject
+   def time(key)
+     if block_given?
+       yield
+     else
+       NullTimedExecution
+     end
+   end
+
+   def namespace(key)
+     self.class.new
+   end
+
+   private
+   # Null implementation of the internal timer class
+   #
+   # @see LogStash::Instrument::TimedExecution`
+   class NullTimedExecution
+     def self.stop
+     end
+   end
+ end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/base.rb b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
new file mode 100644
index 00000000000..313f52b2504
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
@@ -0,0 +1,59 @@
+# encoding: utf-8
+require "logstash/util/loggable"
+require "logstash/util"
+require "concurrent"
+
+module LogStash module Instrument module PeriodicPoller
+  class Base
+    include LogStash::Util::Loggable
+
+    DEFAULT_OPTIONS = {
+      :polling_interval => 1,
+      :polling_timeout => 60
+    }
+
+    public
+    def initialize(metric, options = {})
+      @metric = metric
+      @options = DEFAULT_OPTIONS.merge(options)
+      configure_task
+    end
+
+    def update(time, result, exception)
+      return unless exception
+
+      logger.error("PeriodicPoller: exception",
+                   :poller => self,
+                   :result => result,
+                   :exception => exception,
+                   :executed_at => time)
+    end
+
+    def collect
+      raise NotImplementedError, "#{self.class.name} need to implement `#collect`"
+    end
+
+    def start
+      logger.debug("PeriodicPoller: Starting",
+                   :polling_interval => @options[:polling_interval],
+                   :polling_timeout => @options[:polling_timeout]) if logger.debug?
+      
+      collect # Collect data right away if possible
+      @task.execute
+    end
+
+    def stop
+      logger.debug("PeriodicPoller: Stopping")
+      @task.shutdown
+    end
+
+    protected
+    def configure_task
+      @task = Concurrent::TimerTask.new { collect }
+      @task.execution_interval = @options[:polling_interval]
+      @task.timeout_interval = @options[:polling_timeout]
+      @task.add_observer(self)
+    end
+  end
+end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
new file mode 100644
index 00000000000..6cd6495d49f
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
@@ -0,0 +1,137 @@
+
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/base"
+require 'jrmonitor'
+
+java_import 'java.lang.management.ManagementFactory'
+java_import 'java.lang.management.OperatingSystemMXBean'
+java_import 'com.sun.management.UnixOperatingSystemMXBean'
+java_import 'javax.management.MBeanServer'
+java_import 'javax.management.ObjectName'
+java_import 'javax.management.AttributeList'
+java_import 'javax.naming.directory.Attribute'
+
+module LogStash module Instrument module PeriodicPoller
+  class JVM < Base
+
+    attr_reader :metric
+
+    def initialize(metric, options = {})
+      super(metric, options)
+      @metric = metric
+    end
+
+    def collect
+      raw = JRMonitor.memory.generate      
+      collect_heap_metrics(raw)
+      collect_non_heap_metrics(raw)
+      collect_pools_metrics(raw)
+      collect_threads_metrics
+      collect_process_metrics
+    end
+
+    private
+
+    def collect_threads_metrics      
+      threads = JRMonitor.threads.generate
+      
+      current = threads.count
+      if @peak_threads.nil? || @peak_threads < current
+        @peak_threads = current
+      end      
+      
+      metric.gauge([:jvm, :threads], :count, threads.count)     
+      metric.gauge([:jvm, :threads], :peak_count, @peak_threads)
+    end
+
+    def collect_process_metrics
+      process_metrics = JRMonitor.process.generate
+      
+      path = [:jvm, :process]
+
+
+      open_fds = process_metrics["open_file_descriptors"]
+      if @peak_open_fds.nil? || open_fds > @peak_open_fds
+        @peak_open_fds = open_fds
+      end
+      metric.gauge(path, :open_file_descriptors, open_fds)
+      metric.gauge(path, :peak_open_file_descriptors, @peak_open_fds)
+      metric.gauge(path, :max_file_descriptors, process_metrics["max_file_descriptors"])
+
+      cpu_path = path + [:cpu]
+      cpu_metrics = process_metrics["cpu"]
+      metric.gauge(cpu_path, :percent, cpu_metrics["process_percent"])
+      metric.gauge(cpu_path, :total_in_millis, cpu_metrics["total_in_millis"])
+
+      metric.gauge(path + [:mem], :total_virtual_in_bytes, process_metrics["mem"]["total_virtual_in_bytes"])
+    end
+
+    def collect_heap_metrics(data)
+      heap = aggregate_information_for(data["heap"].values)
+      heap[:used_percent] = (heap[:used_in_bytes] / heap[:max_in_bytes].to_f)*100.0
+
+      heap.each_pair do |key, value|
+        metric.gauge([:jvm, :memory, :heap], key, value.to_i)
+      end
+    end
+
+    def collect_non_heap_metrics(data)
+      non_heap = aggregate_information_for(data["non_heap"].values)
+      non_heap.each_pair do |key, value|
+        metric.gauge([:jvm, :memory, :non_heap],key, value.to_i)
+      end
+    end
+
+    def collect_pools_metrics(data)
+      metrics = build_pools_metrics(data)
+      metrics.each_pair do |key, hash|
+        hash.each_pair do |p,v|
+          metric.gauge([:jvm, :memory, :pools, key.to_sym], p, v)
+        end
+      end
+    end
+
+    def build_pools_metrics(data)
+      heap = data["heap"]
+      old  = {}
+      old = old.merge!(heap["CMS Old Gen"]) if heap.has_key?("CMS Old Gen")
+      old = old.merge!(heap["PS Old Gen"])  if heap.has_key?("PS Old Gen")
+      young = {}
+      young = young.merge!(heap["Par Eden Space"]) if heap.has_key?("Par Eden Space")
+      young = young.merge!(heap["PS Eden Space"])  if heap.has_key?("PS Eden Space")
+      survivor = {}
+      survivor = survivor.merge!(heap["Par Survivor Space"]) if heap.has_key?("Par Survivor Space")
+      survivor = survivor.merge!(heap["PS Survivor Space"])  if heap.has_key?("PS Survivor Space")
+      {
+        "young"    => aggregate_information_for(young),
+        "old"      => aggregate_information_for(old),
+        "survivor" => aggregate_information_for(survivor)
+      }
+    end
+
+    def aggregate_information_for(collection)
+      collection.reduce(default_information_accumulator) do |m,e|
+        e = { e[0] => e[1] } if e.is_a?(Array)
+        e.each_pair do |k,v|
+          m[:used_in_bytes] += v       if k.include?("used")
+          m[:committed_in_bytes] += v  if k.include?("committed")
+          m[:max_in_bytes] += v        if k.include?("max")
+          m[:peak_max_in_bytes] += v   if k.include?("peak.max")
+          m[:peak_used_in_bytes] += v  if k.include?("peak.used")
+        end
+        m
+      end
+    end
+
+    def default_information_accumulator
+      {
+        :used_in_bytes => 0,
+        :committed_in_bytes => 0,
+        :max_in_bytes => 0,
+        :peak_used_in_bytes => 0,
+        :peak_max_in_bytes  => 0
+      }
+    end
+
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/os.rb b/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
new file mode 100644
index 00000000000..8ad09dfc7d7
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/base"
+
+module LogStash module Instrument module PeriodicPoller
+  class Os < Base
+    def initialize(metric, options = {})
+      super(metric, options)
+    end
+
+    def collect
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/periodic_poller_observer.rb b/logstash-core/lib/logstash/instrument/periodic_poller/periodic_poller_observer.rb
new file mode 100644
index 00000000000..382b350968d
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/periodic_poller_observer.rb
@@ -0,0 +1,19 @@
+# encoding: utf-8
+module LogStash module Instrument module PeriodicPoller
+  class PeriodicPollerObserver
+    include LogStash::Util::Loggable
+    
+    def initialize(poller)
+      @poller = poller
+    end
+
+    def update(time, result, exception)
+      if exception
+        logger.error("PeriodicPoller exception", :poller => @poller,
+                     :result => result,
+                     :exception => exception,
+                     :executed_at => time)
+      end
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_pollers.rb b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
new file mode 100644
index 00000000000..09c4feebd57
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
@@ -0,0 +1,26 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/os"
+require "logstash/instrument/periodic_poller/jvm"
+
+module LogStash module Instrument
+  # Each PeriodPoller manager his own thread to do the poller
+  # of the stats, this class encapsulate the starting and stopping of the poller
+  # if the unique timer uses too much resource we can refactor this behavior here.
+  class PeriodicPollers
+    attr_reader :metric
+
+    def initialize(metric)
+      @metric = metric
+      @periodic_pollers = [PeriodicPoller::Os.new(metric),
+                          PeriodicPoller::JVM.new(metric)]
+    end
+
+    def start
+      @periodic_pollers.map(&:start)
+    end
+
+    def stop
+      @periodic_pollers.map(&:stop)
+    end
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/snapshot.rb b/logstash-core/lib/logstash/instrument/snapshot.rb
new file mode 100644
index 00000000000..f46068439ad
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/snapshot.rb
@@ -0,0 +1,16 @@
+# encoding: utf-8
+require "logstash/util/loggable"
+require "logstash/event"
+
+module LogStash module Instrument
+  class Snapshot
+    include LogStash::Util::Loggable
+
+    attr_reader :metric_store, :created_at
+
+    def initialize(metric_store, created_at = Time.now)
+      @metric_store = metric_store
+      @created_at = created_at
+    end
+  end
+end; end
diff --git a/lib/logstash/java_integration.rb b/logstash-core/lib/logstash/java_integration.rb
similarity index 100%
rename from lib/logstash/java_integration.rb
rename to logstash-core/lib/logstash/java_integration.rb
diff --git a/lib/logstash/json.rb b/logstash-core/lib/logstash/json.rb
similarity index 95%
rename from lib/logstash/json.rb
rename to logstash-core/lib/logstash/json.rb
index adbabff18c5..7380b630463 100644
--- a/lib/logstash/json.rb
+++ b/logstash-core/lib/logstash/json.rb
@@ -41,13 +41,12 @@ def jruby_load(data, options = {})
       raise LogStash::Json::ParserError.new(e.message)
     end
 
-    def jruby_dump(o)
+    def jruby_dump(o, options={})
       # TODO [guyboertje] remove these comments in 5.0
       # test for enumerable here to work around an omission in JrJackson::Json.dump to
       # also look for Java::JavaUtil::ArrayList, see TODO submit issue
       # o.is_a?(Enumerable) ? JrJackson::Raw.generate(o) : JrJackson::Json.dump(o)
-
-      JrJackson::Base.generate(o, {})
+      JrJackson::Base.generate(o, options)
 
     rescue => e
       raise LogStash::Json::GeneratorError.new(e.message)
diff --git a/lib/logstash/logging.rb b/logstash-core/lib/logstash/logging.rb
similarity index 100%
rename from lib/logstash/logging.rb
rename to logstash-core/lib/logstash/logging.rb
diff --git a/logstash-core/lib/logstash/logging/json.rb b/logstash-core/lib/logstash/logging/json.rb
new file mode 100644
index 00000000000..1637fa11ce4
--- /dev/null
+++ b/logstash-core/lib/logstash/logging/json.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/logging"
+require "logstash/json"
+
+module LogStash; class Logging; class JSON
+  def initialize(io)
+    raise ArgumentError, "Expected IO, got #{io.class.name}" unless io.is_a?(IO)
+
+    @io = io
+    @lock = Mutex.new
+  end
+
+  def <<(obj)
+    serialized = LogStash::Json.dump(obj)
+    @lock.synchronize do
+      @io.puts(serialized)
+      @io.flush
+    end
+  end
+end; end; end
diff --git a/lib/logstash/namespace.rb b/logstash-core/lib/logstash/namespace.rb
similarity index 93%
rename from lib/logstash/namespace.rb
rename to logstash-core/lib/logstash/namespace.rb
index 44701c38450..355f0ac25fa 100644
--- a/lib/logstash/namespace.rb
+++ b/logstash-core/lib/logstash/namespace.rb
@@ -10,4 +10,5 @@ module Web; end
   module Util; end
   module PluginMixins; end
   module PluginManager; end
+  module Api; end
 end # module LogStash
diff --git a/logstash-core/lib/logstash/output_delegator.rb b/logstash-core/lib/logstash/output_delegator.rb
new file mode 100644
index 00000000000..a8d05c18e36
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator.rb
@@ -0,0 +1,185 @@
+# encoding: utf-8
+require "concurrent/atomic/atomic_fixnum"
+java_import "java.util.concurrent.CopyOnWriteArrayList"
+
+# This class goes hand in hand with the pipeline to provide a pool of
+# free workers to be used by pipeline worker threads. The pool is
+# internally represented with a SizedQueue set the the size of the number
+# of 'workers' the output plugin is configured with.
+#
+# This plugin also records some basic statistics
+module LogStash class OutputDelegator
+  attr_reader :workers, :config, :threadsafe
+
+  # The *args this takes are the same format that a Outputs::Base takes. A list of hashes with parameters in them
+  # Internally these just get merged together into a single hash
+  def initialize(logger, klass, default_worker_count, metric, *plugin_args)
+    @logger = logger
+    @threadsafe = klass.threadsafe?
+    @config = plugin_args.reduce({}, :merge)
+    @klass = klass
+    @workers = java.util.concurrent.CopyOnWriteArrayList.new
+    @default_worker_count = default_worker_count
+    @registered = false
+
+    # Create an instance of the input so we can fetch the identifier
+    output = @klass.new(@config)
+
+    # Scope the metrics to the plugin
+    namespaced_metric = metric.namespace(output.plugin_unique_name.to_sym)
+    @metric_events = namespaced_metric.namespace(:events)
+
+    @events_received = Concurrent::AtomicFixnum.new(0)
+  end
+
+  def threadsafe?
+    !!@threadsafe
+  end
+
+  def warn_on_worker_override!
+    # The user has configured extra workers, but this plugin doesn't support it :(
+    if worker_limits_overriden?
+      message = @klass.workers_not_supported_message
+      warning_meta = {:plugin => @klass.config_name, :worker_count => @config["workers"]}
+      if message
+        warning_meta[:message] = message
+        @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported-with-message", warning_meta))
+      else
+        @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported", warning_meta))
+      end
+    end
+  end
+
+  def worker_limits_overriden?
+    @config["workers"] && @config["workers"] > 1 && @klass.workers_not_supported?
+  end
+
+  def target_worker_count
+    # Remove in 5.0 after all plugins upgraded to use class level declarations
+    raise ArgumentError, "Attempted to detect target worker count before instantiating a worker to test for legacy workers_not_supported!" if @workers.size == 0
+
+    if @threadsafe || @klass.workers_not_supported?
+      1
+    else
+      @config["workers"] || @default_worker_count
+    end
+  end
+
+  def config_name
+    @klass.config_name
+  end
+
+  def register
+    raise ArgumentError, "Attempted to register #{self} twice!" if @registered
+    @registered = true
+    # We define this as an array regardless of threadsafety
+    # to make reporting simpler, even though a threadsafe plugin will just have
+    # a single instance
+    #
+    # Older plugins invoke the instance method Outputs::Base#workers_not_supported
+    # To detect these we need an instance to be created first :()
+    # TODO: In the next major version after 2.x remove support for this
+    @workers << @klass.new(@config)
+    @workers.first.register # Needed in case register calls `workers_not_supported`
+
+    @logger.debug("Will start workers for output", :worker_count => target_worker_count, :class => @klass.name)
+
+    # Threadsafe versions don't need additional workers
+    setup_additional_workers!(target_worker_count) unless @threadsafe
+    # We skip the first worker because that's pre-registered to deal with legacy workers_not_supported
+    @workers.subList(1,@workers.size).each(&:register)
+    setup_multi_receive!
+  end
+
+  def setup_additional_workers!(target_worker_count)
+    warn_on_worker_override!
+
+    (target_worker_count - 1).times do
+      inst = @klass.new(@config)
+      inst.metric = @metric
+      @workers << inst
+    end
+
+    # This queue is used to manage sharing across threads
+    @worker_queue = SizedQueue.new(target_worker_count)
+    @workers.each {|w| @worker_queue << w }
+  end
+
+  def setup_multi_receive!
+    # One might wonder why we don't use something like
+    # define_singleton_method(:multi_receive, method(:threadsafe_multi_receive)
+    # and the answer is this is buggy on Jruby 1.7.x . It works 98% of the time!
+    # The other 2% you get weird errors about rebinding to the same object
+    # Until we switch to Jruby 9.x keep the define_singleton_method parts
+    # the way they are, with a block
+    # See https://github.com/jruby/jruby/issues/3582
+    if threadsafe?
+      @threadsafe_worker = @workers.first
+      define_singleton_method(:multi_receive) do |events|
+        threadsafe_multi_receive(events)
+      end
+    else
+      define_singleton_method(:multi_receive) do |events|
+        worker_multi_receive(events)
+      end
+    end
+  end
+
+  def threadsafe_multi_receive(events)
+    @events_received.increment(events.length)
+    @metric_events.increment(:in, events.length)
+
+    @threadsafe_worker.multi_receive(events)
+    @metric_events.increment(:out, events.length)
+  end
+
+  def worker_multi_receive(events)
+    @events_received.increment(events.length)
+    @metric_events.increment(:in, events.length)
+
+    worker = @worker_queue.pop
+    begin
+      worker.multi_receive(events)
+      @metric_events.increment(:out, events.length)
+    ensure
+      @worker_queue.push(worker)
+    end
+  end
+
+  def do_close
+    @logger.debug("closing output delegator", :klass => @klass.name)
+
+    if @threadsafe
+      @workers.each(&:do_close)
+    else
+      worker_count.times do
+        worker = @worker_queue.pop
+        worker.do_close
+      end
+    end
+  end
+
+  def events_received
+    @events_received.value
+  end
+
+  # There's no concept of 'busy' workers for a threadsafe plugin!
+  def busy_workers
+    if @threadsafe
+      0
+    else
+      # The pipeline reporter can run before the outputs are registered trying to pull a value here
+      # In that case @worker_queue is empty, we just return 0
+      return 0 unless @worker_queue
+      @workers.size - @worker_queue.size
+    end
+  end
+
+  def worker_count
+    @workers.size
+  end
+
+  private
+  # Needed for testing, so private
+  attr_reader :threadsafe_worker, :worker_queue
+end end
diff --git a/lib/logstash/outputs/base.rb b/logstash-core/lib/logstash/outputs/base.rb
similarity index 58%
rename from lib/logstash/outputs/base.rb
rename to logstash-core/lib/logstash/outputs/base.rb
index d3c49899860..3df4e3b4a81 100644
--- a/lib/logstash/outputs/base.rb
+++ b/logstash-core/lib/logstash/outputs/base.rb
@@ -4,6 +4,8 @@
 require "logstash/plugin"
 require "logstash/namespace"
 require "logstash/config/mixin"
+require "logstash/util/wrapped_synchronous_queue"
+require "concurrent/atomic/atomic_fixnum"
 
 class LogStash::Outputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
@@ -23,23 +25,50 @@ class LogStash::Outputs::Base < LogStash::Plugin
   # Note that this setting may not be useful for all outputs.
   config :workers, :validate => :number, :default => 1
 
-  attr_reader :worker_plugins, :worker_queue
+  attr_reader :worker_plugins, :available_workers, :workers, :worker_plugins, :workers_not_supported
+
+  def self.declare_threadsafe!
+    declare_workers_not_supported!
+    @threadsafe = true
+  end
+
+  def self.threadsafe?
+    @threadsafe == true
+  end
+
+  def self.declare_workers_not_supported!(message=nil)
+    @workers_not_supported_message = message
+    @workers_not_supported = true
+  end
+
+  def self.workers_not_supported_message
+    @workers_not_supported_message
+  end
+
+  def self.workers_not_supported?
+    !!@workers_not_supported
+  end
 
   public
+  # TODO: Remove this in the next major version after Logstash 2.x
+  # Post 2.x it should raise an error and tell people to use the class level
+  # declaration
   def workers_not_supported(message=nil)
-    return if @workers == 1
-    if message
-      @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported-with-message", :plugin => self.class.config_name, :worker_count => @workers, :message => message))
-    else
-      @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported", :plugin => self.class.config_name, :worker_count => @workers))
-    end
-    @workers = 1
+    self.class.declare_workers_not_supported!(message)
+  end
+
+  def self.plugin_type
+    "output"
   end
 
   public
   def initialize(params={})
     super
-    config_init(params)
+    config_init(@params)
+
+    # If we're running with a single thread we must enforce single-threaded concurrency by default
+    # Maybe in a future version we'll assume output plugins are threadsafe
+    @single_worker_mutex = Mutex.new
   end
 
   public
@@ -53,33 +82,9 @@ def receive(event)
   end # def receive
 
   public
-  def worker_setup
-    if @workers == 1
-      @worker_plugins = [self]
-    else
-      define_singleton_method(:handle, method(:handle_worker))
-      @worker_queue = SizedQueue.new(20)
-      @worker_plugins = @workers.times.map { self.class.new(@original_params.merge("workers" => 1)) }
-      @worker_plugins.map.with_index do |plugin, i|
-        Thread.new(original_params, @worker_queue) do |params, queue|
-          LogStash::Util::set_thread_name(">#{self.class.config_name}.#{i}")
-          plugin.register
-          while true
-            event = queue.pop
-            plugin.handle(event)
-          end
-        end
-      end
-    end
-  end
-
-  public
-  def handle(event)
-    receive(event)
-  end # def handle
-
-  def handle_worker(event)
-    @worker_queue.push(event)
+  # To be overriden in implementations
+  def multi_receive(events)
+    events.each {|event| receive(event) }
   end
 
   private
diff --git a/lib/logstash/patches.rb b/logstash-core/lib/logstash/patches.rb
similarity index 100%
rename from lib/logstash/patches.rb
rename to logstash-core/lib/logstash/patches.rb
diff --git a/lib/logstash/patches/bugfix_jruby_2558.rb b/logstash-core/lib/logstash/patches/bugfix_jruby_2558.rb
similarity index 100%
rename from lib/logstash/patches/bugfix_jruby_2558.rb
rename to logstash-core/lib/logstash/patches/bugfix_jruby_2558.rb
diff --git a/lib/logstash/patches/cabin.rb b/logstash-core/lib/logstash/patches/cabin.rb
similarity index 100%
rename from lib/logstash/patches/cabin.rb
rename to logstash-core/lib/logstash/patches/cabin.rb
diff --git a/logstash-core/lib/logstash/patches/clamp.rb b/logstash-core/lib/logstash/patches/clamp.rb
new file mode 100644
index 00000000000..0934157a9e3
--- /dev/null
+++ b/logstash-core/lib/logstash/patches/clamp.rb
@@ -0,0 +1,69 @@
+require 'clamp'
+require 'logstash/environment'
+
+module Clamp
+  module Attribute
+    class Instance
+      def default_from_environment
+        # we don't want uncontrolled var injection from the environment
+        # since we're establishing that settings can be pulled from only three places:
+        # 1. default settings
+        # 2. yaml file
+        # 3. cli arguments
+      end
+    end
+  end
+
+  module Option
+
+    module StrictDeclaration
+
+      include Clamp::Attribute::Declaration
+
+      # Instead of letting Clamp set up accessors for the options
+      # weŕe going to tightly controlling them through
+      # LogStash::SETTINGS
+      def define_simple_writer_for(option, &block)
+        LogStash::SETTINGS.get(option.attribute_name)
+        define_method(option.write_method) do |value|
+          value = instance_exec(value, &block) if block
+          LogStash::SETTINGS.set_value(option.attribute_name, value)
+        end
+      end
+
+      def define_reader_for(option)
+        define_method(option.read_method) do
+          LogStash::SETTINGS.get_value(option.attribute_name)
+        end
+      end
+
+    end
+
+    class Definition
+      # Allow boolean flags to optionally receive a true/false argument
+      # to explicitly set them, i.e.
+      # --long.flag.name       => sets flag to true
+      # --long.flag.name true  => sets flag to true
+      # --long.flag.name false => sets flag to false
+      # --long.flag.name=true  => sets flag to true
+      # --long.flag.name=false => sets flag to false
+      def extract_value(switch, arguments)
+        if flag? && (arguments.first.nil? || arguments.first.match("^-"))
+          flag_value(switch)
+        else
+          arguments.shift
+        end
+      end
+    end
+  end
+
+  # Create a subclass of Clamp::Command that enforces the use of
+  # LogStash::SETTINGS for setting validation
+  class StrictCommand < Command
+    class << self
+      include ::Clamp::Option::StrictDeclaration
+    end
+  end
+end
+
+
diff --git a/lib/logstash/patches/profile_require_calls.rb b/logstash-core/lib/logstash/patches/profile_require_calls.rb
similarity index 100%
rename from lib/logstash/patches/profile_require_calls.rb
rename to logstash-core/lib/logstash/patches/profile_require_calls.rb
diff --git a/lib/logstash/patches/rubygems.rb b/logstash-core/lib/logstash/patches/rubygems.rb
similarity index 100%
rename from lib/logstash/patches/rubygems.rb
rename to logstash-core/lib/logstash/patches/rubygems.rb
diff --git a/lib/logstash/patches/stronger_openssl_defaults.rb b/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
similarity index 100%
rename from lib/logstash/patches/stronger_openssl_defaults.rb
rename to logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
new file mode 100644
index 00000000000..a8c1421473f
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -0,0 +1,536 @@
+# encoding: utf-8
+require "thread"
+require "stud/interval"
+require "concurrent"
+require "logstash/namespace"
+require "logstash/errors"
+require "logstash/event"
+require "logstash/config/file"
+require "logstash/filters/base"
+require "logstash/inputs/base"
+require "logstash/outputs/base"
+require "logstash/shutdown_watcher"
+require "logstash/util/wrapped_synchronous_queue"
+require "logstash/pipeline_reporter"
+require "logstash/instrument/metric"
+require "logstash/instrument/namespaced_metric"
+require "logstash/instrument/null_metric"
+require "logstash/instrument/collector"
+require "logstash/output_delegator"
+require "logstash/filter_delegator"
+
+module LogStash; class Pipeline
+  attr_reader :inputs,
+    :filters,
+    :outputs,
+    :worker_threads,
+    :events_consumed,
+    :events_filtered,
+    :reporter,
+    :pipeline_id,
+    :logger,
+    :started_at,
+    :thread,
+    :config_str,
+    :settings
+  attr_accessor :metric
+
+  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
+
+  RELOAD_INCOMPATIBLE_PLUGINS = [
+    "LogStash::Inputs::Stdin"
+  ]
+
+  def initialize(config_str, settings = LogStash::SETTINGS)
+    @config_str = config_str
+    @logger = Cabin::Channel.get(LogStash)
+    @settings = settings
+    @pipeline_id = @settings.get_value("pipeline.id") || self.object_id
+    @reporter = LogStash::PipelineReporter.new(@logger, self)
+
+    @inputs = nil
+    @filters = nil
+    @outputs = nil
+
+    @worker_threads = []
+
+    # This needs to be configured before we evaluate the code to make
+    # sure the metric instance is correctly send to the plugins to make the namespace scoping work
+    @metric = settings.get_value("metric.collect") ? Instrument::Metric.new : Instrument::NullMetric.new 
+
+    grammar = LogStashConfigParser.new
+    @config = grammar.parse(config_str)
+    if @config.nil?
+      raise LogStash::ConfigurationError, grammar.failure_reason
+    end
+    # This will compile the config to ruby and evaluate the resulting code.
+    # The code will initialize all the plugins and define the
+    # filter and output methods.
+    code = @config.compile
+    @code = code
+
+    # The config code is hard to represent as a log message...
+    # So just print it.
+
+    if @settings.get_value("config.debug") && logger.debug?
+      logger.debug("Compiled pipeline code", :code => code)
+    end
+
+    begin
+      eval(code)
+    rescue => e
+      raise
+    end
+
+    @input_queue = LogStash::Util::WrappedSynchronousQueue.new
+    @events_filtered = Concurrent::AtomicFixnum.new(0)
+    @events_consumed = Concurrent::AtomicFixnum.new(0)
+
+    # We generally only want one thread at a time able to access pop/take/poll operations
+    # from this queue. We also depend on this to be able to block consumers while we snapshot
+    # in-flight buffers
+    @input_queue_pop_mutex = Mutex.new
+    @input_threads = []
+    # @ready requires thread safety since it is typically polled from outside the pipeline thread
+    @ready = Concurrent::AtomicBoolean.new(false)
+    @running = Concurrent::AtomicBoolean.new(false)
+    @flushing = Concurrent::AtomicReference.new(false)
+  end # def initialize
+
+  def ready?
+    @ready.value
+  end
+
+  def safe_pipeline_worker_count
+    default = @settings.get_default("pipeline.workers")
+    pipeline_workers = @settings.get("pipeline.workers") #override from args "-w 8" or config
+    safe_filters, unsafe_filters = @filters.partition(&:threadsafe?)
+    plugins = unsafe_filters.collect { |f| f.config_name }
+
+    return pipeline_workers if unsafe_filters.empty?
+
+    if @settings.set?("pipeline.workers")
+      if pipeline_workers > 1
+        @logger.warn("Warning: Manual override - there are filters that might not work with multiple worker threads",
+                     :worker_threads => pipeline_workers, :filters => plugins)
+      end
+    else
+      # user did not specify a worker thread count
+      # warn if the default is multiple
+      if default > 1
+        @logger.warn("Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads",
+                     :count_was => default, :filters => plugins)
+        return 1 # can't allow the default value to propagate if there are unsafe filters
+      end
+    end
+    pipeline_workers
+  end
+
+  def filters?
+    return @filters.any?
+  end
+
+  def run
+    @started_at = Time.now
+
+    @thread = Thread.current
+    LogStash::Util.set_thread_name("[#{pipeline_id}]-pipeline-manager")
+
+    start_workers
+
+    @logger.log("Pipeline #{@pipeline_id} started")
+
+    # Block until all inputs have stopped
+    # Generally this happens if SIGINT is sent and `shutdown` is called from an external thread
+
+    transition_to_running
+    start_flusher # Launches a non-blocking thread for flush events
+    wait_inputs
+    transition_to_stopped
+
+    @logger.info("Input plugins stopped! Will shutdown filter/output workers.")
+
+    shutdown_flusher
+    shutdown_workers
+
+    @logger.log("Pipeline #{@pipeline_id} has been shutdown")
+
+    # exit code
+    return 0
+  end # def run
+
+  def transition_to_running
+    @running.make_true
+  end
+
+  def transition_to_stopped
+    @running.make_false
+  end
+
+  def running?
+    @running.true?
+  end
+
+  def stopped?
+    @running.false?
+  end
+
+  def start_workers
+    @inflight_batches = {}
+
+    @worker_threads.clear # In case we're restarting the pipeline
+    begin
+      start_inputs
+      @outputs.each {|o| o.register }
+      @filters.each {|f| f.register }
+
+      pipeline_workers = safe_pipeline_worker_count
+      batch_size = @settings.get("pipeline.batch.size")
+      batch_delay = @settings.get("pipeline.batch.delay")
+      max_inflight = batch_size * pipeline_workers
+      @logger.info("Starting pipeline",
+                   "id" => self.pipeline_id,
+                   "pipeline.workers" => pipeline_workers,
+                   "pipeline.batch.size" => batch_size,
+                   "pipeline.batch.delay" => batch_delay,
+                   "pipeline.max_inflight" => max_inflight)
+      if max_inflight > MAX_INFLIGHT_WARN_THRESHOLD
+        @logger.warn "CAUTION: Recommended inflight events max exceeded! Logstash will run with up to #{max_inflight} events in memory in your current configuration. If your message sizes are large this may cause instability with the default heap size. Please consider setting a non-standard heap size, changing the batch size (currently #{batch_size}), or changing the number of pipeline workers (currently #{pipeline_workers})"
+      end
+
+      pipeline_workers.times do |t|
+        @worker_threads << Thread.new do
+          LogStash::Util.set_thread_name("[#{pipeline_id}]>worker#{t}")
+          worker_loop(batch_size, batch_delay)
+        end
+      end
+    ensure
+      # it is important to garantee @ready to be true after the startup sequence has been completed
+      # to potentially unblock the shutdown method which may be waiting on @ready to proceed
+      @ready.make_true
+    end
+  end
+
+  # Main body of what a worker thread does
+  # Repeatedly takes batches off the queue, filters, then outputs them
+  def worker_loop(batch_size, batch_delay)
+    running = true
+
+    namespace_events = metric.namespace([:stats, :events])
+    namespace_pipeline = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
+
+    while running
+      # To understand the purpose behind this synchronize please read the body of take_batch
+      input_batch, signal = @input_queue_pop_mutex.synchronize { take_batch(batch_size, batch_delay) }
+      running = false if signal == LogStash::SHUTDOWN
+
+      @events_consumed.increment(input_batch.size)
+      namespace_events.increment(:in, input_batch.size)
+      namespace_pipeline.increment(:in, input_batch.size)
+
+      filtered_batch = filter_batch(input_batch)
+
+      if signal # Flush on SHUTDOWN or FLUSH
+        flush_options = (signal == LogStash::SHUTDOWN) ? {:final => true} : {}
+        flush_filters_to_batch(filtered_batch, flush_options)
+      end
+
+      @events_filtered.increment(filtered_batch.size)
+
+      namespace_events.increment(:filtered, filtered_batch.size)
+      namespace_pipeline.increment(:filtered, filtered_batch.size)
+
+      output_batch(filtered_batch)
+
+      namespace_events.increment(:out, filtered_batch.size)
+      namespace_pipeline.increment(:out, filtered_batch.size)
+
+      inflight_batches_synchronize { set_current_thread_inflight_batch(nil) }
+    end
+  end
+
+  def take_batch(batch_size, batch_delay)
+    batch = []
+    # Since this is externally synchronized in `worker_look` wec can guarantee that the visibility of an insight batch
+    # guaranteed to be a full batch not a partial batch
+    set_current_thread_inflight_batch(batch)
+
+    signal = false
+    batch_size.times do |t|
+      event = (t == 0) ? @input_queue.take : @input_queue.poll(batch_delay)
+
+      if event.nil?
+        next
+      elsif event == LogStash::SHUTDOWN || event == LogStash::FLUSH
+        # We MUST break here. If a batch consumes two SHUTDOWN events
+        # then another worker may have its SHUTDOWN 'stolen', thus blocking
+        # the pipeline. We should stop doing work after flush as well.
+        signal = event
+        break
+      else
+        batch << event
+      end
+    end
+
+    [batch, signal]
+  end
+
+  def filter_batch(batch)
+    batch.reduce([]) do |acc,e|
+      if e.is_a?(LogStash::Event)
+        filtered = filter_func(e)
+        filtered.each {|fe| acc << fe unless fe.cancelled?}
+      end
+      acc
+    end
+  rescue Exception => e
+    # Plugins authors should manage their own exceptions in the plugin code
+    # but if an exception is raised up to the worker thread they are considered
+    # fatal and logstash will not recover from this situation.
+    #
+    # Users need to check their configuration or see if there is a bug in the
+    # plugin.
+    @logger.error("Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
+                  "exception" => e, "backtrace" => e.backtrace)
+    raise
+  end
+
+  # Take an array of events and send them to the correct output
+  def output_batch(batch)
+    # Build a mapping of { output_plugin => [events...]}
+    outputs_events = batch.reduce(Hash.new { |h, k| h[k] = [] }) do |acc, event|
+      # We ask the AST to tell us which outputs to send each event to
+      # Then, we stick it in the correct bin
+
+      # output_func should never return anything other than an Array but we have lots of legacy specs
+      # that monkeypatch it and return nil. We can deprecate  "|| []" after fixing these specs
+      outputs_for_event = output_func(event) || []
+
+      outputs_for_event.each { |output| acc[output] << event }
+      acc
+    end
+
+    # Now that we have our output to event mapping we can just invoke each output
+    # once with its list of events
+    outputs_events.each { |output, events| output.multi_receive(events) }
+  end
+
+  def set_current_thread_inflight_batch(batch)
+    @inflight_batches[Thread.current] = batch
+  end
+
+  def inflight_batches_synchronize
+    @input_queue_pop_mutex.synchronize do
+      yield(@inflight_batches)
+    end
+  end
+
+  def wait_inputs
+    @input_threads.each(&:join)
+  end
+
+  def start_inputs
+    moreinputs = []
+    @inputs.each do |input|
+      if input.threadable && input.threads > 1
+        (input.threads - 1).times do |i|
+          moreinputs << input.clone
+        end
+      end
+    end
+    @inputs += moreinputs
+
+    @inputs.each do |input|
+      input.register
+      start_input(input)
+    end
+  end
+
+  def start_input(plugin)
+    @input_threads << Thread.new { inputworker(plugin) }
+  end
+
+  def inputworker(plugin)
+    LogStash::Util::set_thread_name("[#{pipeline_id}]<#{plugin.class.config_name}")
+    begin
+      plugin.run(@input_queue)
+    rescue => e
+      if plugin.stop?
+        @logger.debug("Input plugin raised exception during shutdown, ignoring it.",
+                      :plugin => plugin.class.config_name, :exception => e,
+                      :backtrace => e.backtrace)
+        return
+      end
+
+      # otherwise, report error and restart
+      if @logger.debug?
+        @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
+                             :plugin => plugin.inspect, :error => e.to_s,
+                             :exception => e.class,
+                             :stacktrace => e.backtrace.join("\n")))
+      else
+        @logger.error(I18n.t("logstash.pipeline.worker-error",
+                             :plugin => plugin.inspect, :error => e))
+      end
+
+      # Assuming the failure that caused this exception is transient,
+      # let's sleep for a bit and execute #run again
+      sleep(1)
+      retry
+    ensure
+      plugin.do_close
+    end
+  end # def inputworker
+
+  # initiate the pipeline shutdown sequence
+  # this method is intended to be called from outside the pipeline thread
+  # @param before_stop [Proc] code block called before performing stop operation on input plugins
+  def shutdown(&before_stop)
+    # shutdown can only start once the pipeline has completed its startup.
+    # avoid potential race conditoon between the startup sequence and this
+    # shutdown method which can be called from another thread at any time
+    sleep(0.1) while !ready?
+
+    # TODO: should we also check against calling shutdown multiple times concurently?
+
+    before_stop.call if block_given?
+
+    @logger.info "Closing inputs"
+    @inputs.each(&:do_stop)
+    @logger.info "Closed inputs"
+  end # def shutdown
+
+  # After `shutdown` is called from an external thread this is called from the main thread to
+  # tell the worker threads to stop and then block until they've fully stopped
+  # This also stops all filter and output plugins
+  def shutdown_workers
+    # Each worker thread will receive this exactly once!
+    @worker_threads.each do |t|
+      @logger.debug("Pushing shutdown", :thread => t.inspect)
+      @input_queue.push(LogStash::SHUTDOWN)
+    end
+
+    @worker_threads.each do |t|
+      @logger.debug("Shutdown waiting for worker thread #{t}")
+      t.join
+    end
+
+    @filters.each(&:do_close)
+    @outputs.each(&:do_close)
+  end
+
+  def plugin(plugin_type, name, *args)
+    args << {} if args.empty?
+
+    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
+
+    klass = LogStash::Plugin.lookup(plugin_type, name)
+
+    if plugin_type == "output"
+      LogStash::OutputDelegator.new(@logger, klass, @settings.get("pipeline.output.workers"), pipeline_scoped_metric.namespace(:outputs), *args)
+    elsif plugin_type == "filter"
+      LogStash::FilterDelegator.new(@logger, klass, pipeline_scoped_metric.namespace(:filters), *args)
+    else
+      klass.new(*args)
+    end
+  end
+
+  # for backward compatibility in devutils for the rspec helpers, this method is not used
+  # in the pipeline anymore.
+  def filter(event, &block)
+    # filter_func returns all filtered events, including cancelled ones
+    filter_func(event).each { |e| block.call(e) }
+  end
+
+
+  # perform filters flush and yeild flushed event to the passed block
+  # @param options [Hash]
+  # @option options [Boolean] :final => true to signal a final shutdown flush
+  def flush_filters(options = {}, &block)
+    flushers = options[:final] ? @shutdown_flushers : @periodic_flushers
+
+    flushers.each do |flusher|
+      flusher.call(options, &block)
+    end
+  end
+
+  def start_flusher
+    # Invariant to help detect improper initialization
+    raise "Attempted to start flusher on a stopped pipeline!" if stopped?
+
+    @flusher_thread = Thread.new do
+      while Stud.stoppable_sleep(5, 0.1) { stopped? }
+        flush
+        break if stopped?
+      end
+    end
+  end
+
+  def shutdown_flusher
+    @flusher_thread.join
+  end
+
+  def flush
+    if @flushing.compare_and_set(false, true)
+      @logger.debug? && @logger.debug("Pushing flush onto pipeline")
+      @input_queue.push(LogStash::FLUSH)
+    end
+  end
+
+
+  # Calculate the uptime in milliseconds
+  #
+  # @return [Fixnum] Uptime in milliseconds, 0 if the pipeline is not started
+  def uptime
+    return 0 if started_at.nil?
+    ((Time.now.to_f - started_at.to_f) * 1000.0).to_i
+  end
+
+  # perform filters flush into the output queue
+  # @param options [Hash]
+  # @option options [Boolean] :final => true to signal a final shutdown flush
+  def flush_filters_to_batch(batch, options = {})
+    flush_filters(options) do |event|
+      unless event.cancelled?
+        @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
+        batch << event
+      end
+    end
+
+    @flushing.set(false)
+  end # flush_filters_to_output!
+
+  def plugin_threads_info
+    input_threads = @input_threads.select {|t| t.alive? }
+    worker_threads = @worker_threads.select {|t| t.alive? }
+    (input_threads + worker_threads).map {|t| LogStash::Util.thread_info(t) }
+  end
+
+  def stalling_threads_info
+    plugin_threads_info
+      .reject {|t| t["blocked_on"] } # known benign blocking statuses
+      .each {|t| t.delete("backtrace") }
+      .each {|t| t.delete("blocked_on") }
+      .each {|t| t.delete("status") }
+  end
+
+  def non_reloadable_plugins
+    (inputs + filters + outputs).select do |plugin|
+      RELOAD_INCOMPATIBLE_PLUGINS.include?(plugin.class.name)
+    end
+  end
+
+  # Sometimes we log stuff that will dump the pipeline which may contain
+  # sensitive information (like the raw syntax tree which can contain passwords)
+  # We want to hide most of what's in here
+  def inspect
+    {
+      :pipeline_id => @pipeline_id,
+      :settings => @settings.inspect,
+      :ready => @ready,
+      :running => @running,
+      :flushing => @flushing
+    }
+  end
+
+end end
diff --git a/logstash-core/lib/logstash/pipeline_reporter.rb b/logstash-core/lib/logstash/pipeline_reporter.rb
new file mode 100644
index 00000000000..c7ae6ca847c
--- /dev/null
+++ b/logstash-core/lib/logstash/pipeline_reporter.rb
@@ -0,0 +1,114 @@
+# encoding: utf-8
+require 'ostruct'
+
+module LogStash; class PipelineReporter
+  attr_reader :logger, :pipeline
+
+  # This is an immutable copy of the pipeline state,
+  # It is a proxy to a hash to allow us to add methods dynamically to the hash
+  class Snapshot
+    def initialize(data)
+      @data = data
+    end
+
+    def to_hash
+      @data
+    end
+
+    def to_simple_hash
+      {"inflight_count" => inflight_count, "stalling_thread_info" => format_threads_by_plugin}
+    end
+
+    def to_str
+      to_simple_hash.to_s
+    end
+    alias_method :to_s, :to_str
+
+    def method_missing(meth)
+      @data[meth]
+    end
+
+    def format_threads_by_plugin
+      stalled_plugins = {}
+      stalling_threads_info.each do |thr|
+        key = (thr.delete("plugin") || "other")
+        stalled_plugins[key] ||= []
+        stalled_plugins[key] << thr
+      end
+      stalled_plugins
+    end
+  end
+
+  def initialize(logger,pipeline)
+    @logger = logger
+    @pipeline = pipeline
+  end
+
+  # The main way of accessing data from the reporter,,
+  # this provides a (more or less) consistent snapshot of what's going on in the
+  # pipeline with some extra decoration
+  def snapshot
+    Snapshot.new(self.to_hash)
+  end
+
+  def to_hash
+    pipeline.inflight_batches_synchronize do |batch_map|
+      worker_states_snap = worker_states(batch_map) # We only want to run this once
+      inflight_count = worker_states_snap.map {|s| s[:inflight_count] }.reduce(0, :+)
+
+      {
+        :events_filtered => events_filtered,
+        :events_consumed => events_consumed,
+        :worker_count => pipeline.worker_threads.size,
+        :inflight_count => inflight_count,
+        :worker_states => worker_states_snap,
+        :output_info => output_info,
+        :thread_info => pipeline.plugin_threads_info,
+        :stalling_threads_info => pipeline.stalling_threads_info
+      }
+    end
+  end
+
+  private
+
+  def events_filtered
+    pipeline.events_filtered.value
+  end
+
+  def events_consumed
+    pipeline.events_consumed.value
+  end
+
+  def plugin_threads
+    pipeline.plugin_threads
+  end
+
+  # Not threadsafe! must be called within an `inflight_batches_synchronize` block
+  def worker_states(batch_map)
+      pipeline.worker_threads.map.with_index do |thread,idx|
+        status = thread.status || "dead"
+        inflight_count = batch_map[thread] ? batch_map[thread].size : 0
+        {
+          :status => status,
+          :alive => thread.alive?,
+          :index => idx,
+          :inflight_count => inflight_count
+        }
+    end
+  end
+
+  def output_info
+    pipeline.outputs.map do |output_delegator|
+      is_multi_worker = output_delegator.worker_count > 1
+
+      {
+        :type => output_delegator.config_name,
+        :config => output_delegator.config,
+        :is_multi_worker => is_multi_worker,
+        :events_received => output_delegator.events_received,
+        :workers => output_delegator.workers,
+        :busy_workers => output_delegator.busy_workers
+      }
+    end
+  end
+end end
\ No newline at end of file
diff --git a/lib/logstash/plugin.rb b/logstash-core/lib/logstash/plugin.rb
similarity index 58%
rename from lib/logstash/plugin.rb
rename to logstash-core/lib/logstash/plugin.rb
index e4ed6171ecc..0c6ce147fa8 100644
--- a/lib/logstash/plugin.rb
+++ b/logstash-core/lib/logstash/plugin.rb
@@ -2,8 +2,11 @@
 require "logstash/namespace"
 require "logstash/logging"
 require "logstash/config/mixin"
+require "logstash/instrument/null_metric"
 require "cabin"
 require "concurrent"
+require "securerandom"
+require "logstash/plugins/registry"
 
 class LogStash::Plugin
   attr_accessor :params
@@ -11,34 +14,69 @@ class LogStash::Plugin
 
   NL = "\n"
 
-  public
+  include LogStash::Config::Mixin
+
+  # Disable or enable metric logging for this specific plugin instance
+  # by default we record all the metrics we can, but you can disable metrics collection
+  # for a specific plugin.
+  config :enable_metric, :validate => :boolean, :default => true
+
+  # Add a unique `ID` to the plugin instance, this `ID` is used for tracking
+  # information for a specific configuration of the plugin.
+  #
+  # ```
+  # output {
+  #  stdout {
+  #    id => "ABC"
+  #  }
+  # }
+  # ```
+  #
+  # If you don't explicitely set this variable Logstash will generate a unique name.
+  config :id, :validate => :string
+
   def hash
     params.hash ^
     self.class.name.hash
   end
 
-  public
+
   def eql?(other)
     self.class.name == other.class.name && @params == other.params
   end
 
-  public
   def initialize(params=nil)
-    @params = params
+    @params = LogStash::Util.deep_clone(params)
     @logger = Cabin::Channel.get(LogStash)
   end
 
+  # Return a uniq ID for this plugin configuration, by default
+  # we will generate a UUID
+  #
+  # If the user defines a `id => 'ABC'` in the configuration we will return
+  #
+  # @return [String] A plugin ID
+  def id
+    (@params["id"].nil? || @params["id"].empty?) ? SecureRandom.uuid : @params["id"]
+  end
+
+  # Return a unique_name, This is composed by the name of
+  # the plugin and the generated ID (of the configured one)
+  #
+  # @return [String] a unique name
+  def plugin_unique_name
+    "#{config_name}_#{id}"
+  end
+
   # close is called during shutdown, after the plugin worker
   # main task terminates
-  public
   def do_close
-    @logger.debug("closing", :plugin => self)
+    @logger.debug("closing", :plugin => self.class.name)
     close
   end
 
   # Subclasses should implement this close method if you need to perform any
   # special tasks during shutdown (like flushing, etc.)
-  public
   def close
     # ..
   end
@@ -47,7 +85,6 @@ def to_s
     return "#{self.class.name}: #{@params}"
   end
 
-  public
   def inspect
     if !@params.nil?
       description = @params
@@ -59,29 +96,44 @@ def inspect
     end
   end
 
+  def debug_info
+    [self.class.to_s, original_params]
+  end
+
+  def metric=(new_metric)
+    @metric = new_metric
+  end
+
+  def metric
+    @metric_plugin ||= enable_metric ? @metric : LogStash::Instrument::NullMetric.new
+  end
+
+  # return the configured name of this plugin
+  # @return [String] The name of the plugin defined by `config_name`
+  def config_name
+    self.class.config_name
+  end
+
+
   # Look up a plugin by type and name.
-  public
   def self.lookup(type, name)
     path = "logstash/#{type}s/#{name}"
-
-    # first check if plugin already exists in namespace and continue to next step if not
-    begin
-      return namespace_lookup(type, name)
-    rescue NameError
-      logger.debug("Plugin not defined in namespace, checking for plugin file", :type => type, :name => name, :path => path)
+    LogStash::Registry.instance.lookup(type ,name) do |plugin_klass, plugin_name|
+      is_a_plugin?(plugin_klass, plugin_name)
     end
-
-    # try to load the plugin file. ex.: lookup("filter", "grok") will require logstash/filters/grok
-    require(path)
-
-    # check again if plugin is now defined in namespace after the require
-    namespace_lookup(type, name)
   rescue LoadError, NameError => e
+    logger.debug("Problems loading the plugin with", :type => type, :name => name, :path => path)
     raise(LogStash::PluginLoadingError, I18n.t("logstash.pipeline.plugin-loading-error", :type => type, :name => name, :path => path, :error => e.to_s))
   end
 
-  private
+  public
+  def self.declare_plugin(type, name)
+    path = "logstash/#{type}s/#{name}"
+    registry = LogStash::Registry.instance
+    registry.register(path, self)
+  end
 
+  private
   # lookup a plugin by type and name in the existing LogStash module namespace
   # ex.: namespace_lookup("filter", "grok") looks for LogStash::Filters::Grok
   # @param type [String] plugin type, "input", "ouput", "filter"
@@ -112,4 +164,5 @@ def self.is_a_plugin?(klass, name)
   def self.logger
     @logger ||= Cabin::Channel.get(LogStash)
   end
+
 end # class LogStash::Plugin
diff --git a/logstash-core/lib/logstash/plugins/registry.rb b/logstash-core/lib/logstash/plugins/registry.rb
new file mode 100644
index 00000000000..b69ba9e08af
--- /dev/null
+++ b/logstash-core/lib/logstash/plugins/registry.rb
@@ -0,0 +1,83 @@
+# encoding: utf-8
+require 'singleton'
+require "rubygems/package"
+
+module LogStash
+  class Registry
+
+    ##
+    # Placeholder class for registered plugins
+    ##
+    class Plugin
+      attr_reader :type, :name
+
+      def initialize(type, name)
+        @type  = type
+        @name  = name
+      end
+
+      def path
+        "logstash/#{type}s/#{name}"
+      end
+
+      def cannonic_gem_name
+        "logstash-#{type}-#{name}"
+      end
+
+      def installed?
+        find_plugin_spec(cannonic_gem_name).any?
+      end
+
+      private
+
+      def find_plugin_spec(name)
+        specs = ::Gem::Specification.find_all_by_name(name)
+        specs.select{|spec| logstash_plugin_spec?(spec)}
+      end
+
+      def logstash_plugin_spec?(spec)
+        spec.metadata && spec.metadata["logstash_plugin"] == "true"
+      end
+
+    end
+
+    include Singleton
+
+    def initialize
+      @registry = {}
+      @logger = Cabin::Channel.get(LogStash)
+    end
+
+    def lookup(type, plugin_name, &block)
+
+      plugin = Plugin.new(type, plugin_name)
+
+      if plugin.installed?
+        return @registry[plugin.path] if registered?(plugin.path)
+        require plugin.path
+        klass = @registry[plugin.path]
+        if block_given? # if provided pass a block to do validation
+          raise LoadError unless block.call(klass, plugin_name)
+        end
+        return klass
+      else
+        # The plugin was defined directly in the code, so there is no need to use the
+        # require way of loading classes
+        return @registry[plugin.path] if registered?(plugin.path)
+        raise LoadError
+      end
+    rescue => e
+      @logger.debug("Problems loading a plugin with", :type => type, :name => plugin, :path => plugin.path, :error => e) if @logger.debug?
+      raise LoadError, "Problems loading the requested plugin named #{plugin_name} of type #{type}."
+    end
+
+    def register(path, klass)
+      @registry[path] = klass
+    end
+
+    def registered?(path)
+      @registry.has_key?(path)
+    end
+
+  end
+end
diff --git a/lib/logstash/program.rb b/logstash-core/lib/logstash/program.rb
similarity index 100%
rename from lib/logstash/program.rb
rename to logstash-core/lib/logstash/program.rb
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
new file mode 100644
index 00000000000..f0a630a324a
--- /dev/null
+++ b/logstash-core/lib/logstash/runner.rb
@@ -0,0 +1,418 @@
+# encoding: utf-8
+Thread.abort_on_exception = true
+Encoding.default_external = Encoding::UTF_8
+$DEBUGLIST = (ENV["DEBUG"] || "").split(",")
+
+require "clamp"
+require "cabin"
+require "net/http"
+require "logstash/environment"
+
+LogStash::Environment.load_locale!
+
+require "logstash/namespace"
+require "logstash/agent"
+require "logstash/config/defaults"
+require "logstash/shutdown_watcher"
+require "logstash/patches/clamp"
+require "logstash/settings"
+
+class LogStash::Runner < Clamp::StrictCommand
+  # The `path.settings` need to be defined in the runner instead of the `logstash-core/lib/logstash/environment.r`
+  # because the `Environment::LOGSTASH_HOME` doesn't exist in the context of the `logstash-core` gem.
+  # 
+  # See issues https://github.com/elastic/logstash/issues/5361
+  LogStash::SETTINGS.register(LogStash::Setting::String.new("path.settings", ::File.join(LogStash::Environment::LOGSTASH_HOME, "config")))
+
+  # Node Settings
+  option ["-n", "--node.name"], "NAME",
+    I18n.t("logstash.runner.flag.node_name"),
+    :attribute_name => "node.name",
+    :default => LogStash::SETTINGS.get_default("node.name")
+
+  # Config Settings
+  option ["-f", "--path.config"], "CONFIG_PATH",
+    I18n.t("logstash.runner.flag.config"),
+    :attribute_name => "path.config"
+
+  option ["-e", "--config.string"], "CONFIG_STRING",
+    I18n.t("logstash.runner.flag.config-string",
+      :default_input => LogStash::Config::Defaults.input,
+      :default_output => LogStash::Config::Defaults.output),
+    :default => LogStash::SETTINGS.get_default("config.string"),
+    :attribute_name => "config.string"
+
+  # Pipeline settings
+  option ["-w", "--pipeline.workers"], "COUNT",
+    I18n.t("logstash.runner.flag.pipeline-workers"),
+    :attribute_name => "pipeline.workers",
+    :default => LogStash::SETTINGS.get_default("pipeline.workers"), &:to_i
+
+  option ["-b", "--pipeline.batch.size"], "SIZE",
+    I18n.t("logstash.runner.flag.pipeline-batch-size"),
+    :attribute_name => "pipeline.batch.size",
+    :default => LogStash::SETTINGS.get_default("pipeline.batch.size"), &:to_i
+
+  option ["-u", "--pipeline.batch.delay"], "DELAY_IN_MS",
+    I18n.t("logstash.runner.flag.pipeline-batch-delay"),
+    :attribute_name => "pipeline.batch.delay",
+    :default => LogStash::SETTINGS.get_default("pipeline.batch.delay"), &:to_i
+
+  option ["--pipeline.unsafe_shutdown"], :flag,
+    I18n.t("logstash.runner.flag.unsafe_shutdown"),
+    :attribute_name => "pipeline.unsafe_shutdown",
+    :default => LogStash::SETTINGS.get_default("pipeline.unsafe_shutdown")
+
+  # Plugins Settings
+  option ["-p", "--path.plugins"] , "PATH",
+    I18n.t("logstash.runner.flag.pluginpath"),
+    :multivalued => true, :attribute_name => "path.plugins",
+    :default => LogStash::SETTINGS.get_default("path.plugins")
+
+  # Logging Settings
+  option ["-l", "--path.log"], "FILE",
+    I18n.t("logstash.runner.flag.log"),
+    :attribute_name => "path.log"
+
+  option "--log.level", "LEVEL", I18n.t("logstash.runner.flag.log_level"),
+    :default => LogStash::SETTINGS.get_default("log.level")
+
+  option "--config.debug", :flag,
+    I18n.t("logstash.runner.flag.config_debug"),
+    :default => LogStash::SETTINGS.get_default("config.debug"),
+    :attribute_name => "config.debug"
+
+  # Other settings
+  option ["-i", "--interactive"], "SHELL",
+    I18n.t("logstash.runner.flag.rubyshell"),
+    :attribute_name => "interactive"
+
+  option ["-V", "--version"], :flag,
+    I18n.t("logstash.runner.flag.version")
+
+  option ["-t", "--config.test_and_exit"], :flag,
+    I18n.t("logstash.runner.flag.configtest"),
+    :attribute_name => "config.test_and_exit",
+    :default => LogStash::SETTINGS.get_default("config.test_and_exit")
+
+  option ["-r", "--config.reload.automatic"], :flag,
+    I18n.t("logstash.runner.flag.auto_reload"),
+    :attribute_name => "config.reload.automatic",
+    :default => LogStash::SETTINGS.get_default("config.reload.automatic")
+
+  option ["--config.reload.interval"], "RELOAD_INTERVAL",
+    I18n.t("logstash.runner.flag.reload_interval"),
+    :attribute_name => "config.reload.interval",
+    :default => LogStash::SETTINGS.get_default("config.reload.interval"), &:to_i
+
+  option ["--http.host"], "HTTP_HOST",
+    I18n.t("logstash.runner.flag.http_host"),
+    :attribute_name => "http.host",
+    :default => LogStash::SETTINGS.get_default("http.host")
+
+  option ["--http.port"], "HTTP_PORT",
+    I18n.t("logstash.runner.flag.http_port"),
+    :attribute_name => "http.port",
+    :default => LogStash::SETTINGS.get_default("http.port"), &:to_i
+
+  option ["--log.format"], "FORMAT",
+    I18n.t("logstash.runner.flag.log_format"),
+    :attribute_name => "log.format",
+    :default => LogStash::SETTINGS.get_default("log.format")
+
+  option ["--path.settings"], "SETTINGS_DIR",
+    I18n.t("logstash.runner.flag.path_settings"),
+    :attribute_name => "path.settings",
+    :default => LogStash::SETTINGS.get_default("path.settings")
+
+  attr_reader :agent
+
+  def initialize(*args)
+    @logger = Cabin::Channel.get(LogStash)
+    @settings = LogStash::SETTINGS
+    super(*args)
+  end
+
+  def run(args)
+    settings_path = fetch_settings_path(args)
+
+    @settings.set("path.settings", settings_path) if settings_path
+
+    begin
+      LogStash::SETTINGS.from_yaml(LogStash::SETTINGS.get("path.settings"))
+    rescue => e
+      @logger.subscribe(STDOUT)
+      @logger.warn("Logstash has a new settings file which defines start up time settings. This file is typically located in $LS_HOME/config or /etc/logstash. If you installed Logstash through a package and are starting it manually please specify the location to this settings file by passing in \"--path.settings=/path/..\" in the command line options")
+      @logger.fatal("Failed to load settings file from \"path.settings\". Aborting...", "path.settings" => LogStash::SETTINGS.get("path.settings"), "exception" => e.class, "message" => e.message)
+      exit(-1)
+    end
+
+    super(*[args])
+  end
+
+  def execute
+    require "logstash/util"
+    require "logstash/util/java_version"
+    require "stud/task"
+    require "cabin" # gem 'cabin'
+    require "logstash/logging/json"
+
+    # Configure Logstash logging facility, this need to be done before everything else to
+    # make sure the logger has the correct settings and the log level is correctly defined.
+    configure_logging(setting("path.log"), setting("log.level"))
+
+    LogStash::Util::set_thread_name(self.class.name)
+
+    if RUBY_VERSION < "1.9.2"
+      $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
+      return 1
+    end
+
+    # Exit on bad java versions
+    java_version = LogStash::Util::JavaVersion.version
+    if LogStash::Util::JavaVersion.bad_java_version?(java_version)
+      $stderr.puts "Java version 1.8.0 or later is required. (You are running: #{java_version})"
+      return 1
+    end
+
+    LogStash::ShutdownWatcher.unsafe_shutdown = setting("pipeline.unsafe_shutdown")
+    LogStash::ShutdownWatcher.logger = @logger
+
+    configure_plugin_paths(setting("path.plugins"))
+
+    if version?
+      show_version
+      return 0
+    end
+
+    return start_shell(setting("interactive"), binding) if setting("interactive")
+
+    @settings.format_settings.each {|line| @logger.info(line) }
+
+    if setting("config.string").nil? && setting("path.config").nil?
+      fail(I18n.t("logstash.runner.missing-configuration"))
+    end
+
+    if setting("config.reload.automatic") && setting("path.config").nil?
+      # there's nothing to reload
+      signal_usage_error(I18n.t("logstash.runner.reload-without-config-path"))
+    end
+
+    if setting("config.test_and_exit")
+      config_loader = LogStash::Config::Loader.new(@logger)
+      config_str = config_loader.format_config(setting("path.config"), setting("config.string"))
+      begin
+        LogStash::Pipeline.new(config_str)
+        @logger.terminal "Configuration OK"
+        return 0
+      rescue => e
+        @logger.fatal I18n.t("logstash.runner.invalid-configuration", :error => e.message)
+        return 1
+      end
+    end
+
+    @agent = create_agent(@settings)
+
+    @agent.register_pipeline("main", @settings)
+
+    # enable sigint/sigterm before starting the agent
+    # to properly handle a stalled agent
+    sigint_id = trap_sigint()
+    sigterm_id = trap_sigterm()
+
+    @agent_task = Stud::Task.new { @agent.execute }
+
+    # no point in enabling config reloading before the agent starts
+    sighup_id = trap_sighup()
+
+    agent_return = @agent_task.wait
+
+    @agent.shutdown
+
+    agent_return
+
+  rescue Clamp::UsageError => e
+    $stderr.puts "ERROR: #{e.message}"
+    show_short_help
+    return 1
+  rescue => e
+    @logger.fatal(I18n.t("oops"), :error => e, :backtrace => e.backtrace)
+    return 1
+  ensure
+    Stud::untrap("INT", sigint_id) unless sigint_id.nil?
+    Stud::untrap("TERM", sigterm_id) unless sigterm_id.nil?
+    Stud::untrap("HUP", sighup_id) unless sighup_id.nil?
+    @log_fd.close if @log_fd
+  end # def self.main
+
+  def show_version
+    show_version_logstash
+
+    if @logger.debug? || @logger.info?
+      show_version_ruby
+      show_version_java if LogStash::Environment.jruby?
+      show_gems if @logger.debug?
+    end
+  end # def show_version
+
+  def show_version_logstash
+    require "logstash/version"
+    puts "logstash #{LOGSTASH_VERSION}"
+  end # def show_version_logstash
+
+  def show_version_ruby
+    puts RUBY_DESCRIPTION
+  end # def show_version_ruby
+
+  def show_version_java
+    properties = java.lang.System.getProperties
+    puts "java #{properties["java.version"]} (#{properties["java.vendor"]})"
+    puts "jvm #{properties["java.vm.name"]} / #{properties["java.vm.version"]}"
+  end # def show_version_java
+
+  def show_gems
+    require "rubygems"
+    Gem::Specification.each do |spec|
+      puts "gem #{spec.name} #{spec.version}"
+    end
+  end # def show_gems
+
+  # add the given paths for ungemified/bare plugins lookups
+  # @param paths [String, Array<String>] plugins path string or list of path strings to add
+  def configure_plugin_paths(paths)
+    Array(paths).each do |path|
+      fail(I18n.t("logstash.runner.configuration.plugin_path_missing", :path => path)) unless File.directory?(path)
+      LogStash::Environment.add_plugin_path(path)
+    end
+  end
+
+  def create_agent(*args)
+    LogStash::Agent.new(*args)
+  end
+
+  # Point logging at a specific path.
+  def configure_logging(path, level)
+    @logger = Cabin::Channel.get(LogStash)
+    # Set with the -v (or -vv...) flag
+    case level
+    when "quiet"
+      @logger.level = :error
+    when "verbose"
+      @logger.level = :info
+    when "debug"
+      @logger.level = :debug
+    else
+      @logger.level = :warn
+    end
+
+    if path
+      # TODO(sissel): Implement file output/rotation in Cabin.
+      # TODO(sissel): Catch exceptions, report sane errors.
+      begin
+        @log_fd.close if @log_fd
+        @log_fd = File.new(path, "a")
+      rescue => e
+        fail(I18n.t("logstash.runner.configuration.log_file_failed",
+                    :path => path, :error => e))
+      end
+
+      if setting("log.format") == "json"
+        @logger.subscribe(LogStash::Logging::JSON.new(STDOUT), :level => :fatal)
+        @logger.subscribe(LogStash::Logging::JSON.new(@log_fd))
+      else
+        @logger.subscribe(STDOUT, :level => :fatal)
+        @logger.subscribe(@log_fd)
+      end
+      @logger.terminal "Sending logstash logs to #{path}."
+    else
+      if setting("log.format") == "json"
+        @logger.subscribe(LogStash::Logging::JSON.new(STDOUT))
+      else
+        @logger.subscribe(STDOUT)
+      end
+    end
+
+    if setting("config.debug") && @logger.level != :debug
+      @logger.warn("--config.debug was specified, but log.level was not set to \'debug\'! No config info will be logged.")
+    end
+
+    # TODO(sissel): redirect stdout/stderr to the log as well
+    # http://jira.codehaus.org/browse/JRUBY-7003
+  end # def configure_logging
+
+  # Emit a failure message and abort.
+  def fail(message)
+    signal_usage_error(message)
+  end # def fail
+
+  def show_short_help
+    puts I18n.t("logstash.runner.short-help")
+  end
+
+  def start_shell(shell, start_binding)
+    case shell
+    when "pry"
+      require 'pry'
+      start_binding.pry
+    when "irb"
+      require 'irb'
+      ARGV.clear
+      # TODO: set binding to this instance of Runner
+      # currently bugged as per https://github.com/jruby/jruby/issues/384
+      IRB.start(__FILE__)
+    else
+      fail(I18n.t("logstash.runner.invalid-shell"))
+    end
+  end
+
+  def trap_sighup
+    Stud::trap("HUP") do
+      @logger.warn(I18n.t("logstash.agent.sighup"))
+      @agent.reload_state!
+    end
+  end
+
+  def trap_sigterm
+    Stud::trap("TERM") do
+      @logger.warn(I18n.t("logstash.agent.sigterm"))
+      @agent_task.stop!
+    end
+  end
+
+  def trap_sigint
+    Stud::trap("INT") do
+      if @interrupted_once
+        @logger.fatal(I18n.t("logstash.agent.forced_sigint"))
+        exit
+      else
+        @logger.warn(I18n.t("logstash.agent.sigint"))
+        Thread.new(@logger) {|logger| sleep 5; logger.warn(I18n.t("logstash.agent.slow_shutdown")) }
+        @interrupted_once = true
+        @agent_task.stop!
+      end
+    end
+  end
+
+  def setting(key)
+    @settings.get_value(key)
+  end
+
+  # where can I find the logstash.yml file?
+  # 1. look for a "--path.settings path"
+  # 2. look for a "--path.settings=path"
+  # 3. check if the LS_SETTINGS_DIR environment variable is set
+  # 4. return nil if not found
+  def fetch_settings_path(cli_args)
+    if i=cli_args.find_index("--path.settings")
+      cli_args[i+1]
+    elsif settings_arg = cli_args.find {|v| v.match(/--path.settings=/) }
+      match = settings_arg.match(/--path.settings=(.*)/)
+      match[1]
+    elsif ENV['LS_SETTINGS_DIR']
+      ENV['LS_SETTINGS_DIR']
+    else
+      nil
+    end
+  end
+
+end
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
new file mode 100644
index 00000000000..7cc0e043b38
--- /dev/null
+++ b/logstash-core/lib/logstash/settings.rb
@@ -0,0 +1,267 @@
+# encoding: utf-8
+
+module LogStash
+  class Settings
+
+    def initialize
+      @settings = {}
+    end
+
+    def register(setting)
+      if @settings.key?(setting.name)
+        raise ArgumentError.new("Setting \"#{setting.name}\" has already been registered as #{setting.inspect}")
+      else
+        @settings[setting.name] = setting
+      end
+    end
+
+    def get_setting(setting_name)
+      setting = @settings[setting_name]
+      raise ArgumentError.new("Setting \"#{setting_name}\" hasn't been registered") if setting.nil?
+      setting
+    end
+
+    def get_subset(setting_regexp)
+      regexp = setting_regexp.is_a?(Regexp) ? setting_regexp : Regexp.new(setting_regexp)
+      settings = self.class.new
+      @settings.each do |setting_name, setting|
+        next unless setting_name.match(regexp)
+        settings.register(setting.clone)
+      end
+      settings
+    end
+
+    def set?(setting_name)
+      get_setting(setting_name).set?
+    end
+
+    def clone
+      get_subset(".*")
+    end
+
+    def get_default(setting_name)
+      get_setting(setting_name).default
+    end
+
+    def get_value(setting_name)
+      get_setting(setting_name).value
+    end
+    alias_method :get, :get_value
+
+    def set_value(setting_name, value)
+      get_setting(setting_name).set(value)
+    end
+    alias_method :set, :set_value
+
+    def to_hash
+      hash = {}
+      @settings.each do |name, setting|
+        hash[name] = setting.value
+      end
+      hash
+    end
+
+    def merge(hash)
+      hash.each {|key, value| set_value(key, value) }
+      self
+    end
+
+    def format_settings
+      output = []
+      output << "-------- Logstash Settings (* means modified) ---------"
+      @settings.each do |setting_name, setting|
+        value = setting.value
+        default_value = setting.default
+        if default_value == value # print setting and its default value
+          output << "#{setting_name}: #{value.inspect}" unless value.nil?
+        elsif default_value.nil? # print setting and warn it has been set
+          output << "*#{setting_name}: #{value.inspect}"
+        elsif value.nil? # default setting not set by user
+          output << "#{setting_name}: #{default_value.inspect}"
+        else # print setting, warn it has been set, and show default value
+          output << "*#{setting_name}: #{value.inspect} (default: #{default_value.inspect})"
+        end
+      end
+      output << "--------------- Logstash Settings -------------------"
+      output
+    end
+
+    def reset
+      @settings.values.each(&:reset)
+    end
+
+    def from_yaml(yaml_path)
+      settings = read_yaml(::File.join(yaml_path, "logstash.yml"))
+      self.merge(flatten_hash(settings))
+    end
+
+    private
+    def read_yaml(path)
+      YAML.safe_load(IO.read(path)) || {}
+    end
+
+    def flatten_hash(h,f="",g={})
+      return g.update({ f => h }) unless h.is_a? Hash
+      if f.empty?
+        h.each { |k,r| flatten_hash(r,k,g) }
+      else
+        h.each { |k,r| flatten_hash(r,"#{f}.#{k}",g) }
+      end
+      g
+    end
+  end
+
+  class Setting
+    attr_reader :name, :default
+
+    def initialize(name, klass, default=nil, strict=true, &validator_proc)
+      @name = name
+      unless klass.is_a?(Class)
+        raise ArgumentError.new("Setting \"#{@name}\" must be initialized with a class (received #{klass})")
+      end
+      @klass = klass
+      @validator_proc = validator_proc
+      @value = nil
+      @value_is_set = false
+
+      validate(default) if strict
+      @default = default
+    end
+
+    def value
+      @value_is_set ? @value : default
+    end
+
+    def set?
+      @value_is_set
+    end
+
+    def set(value)
+      validate(value)
+      @value = value
+      @value_is_set = true
+      @value
+    end
+
+    def reset
+      @value = nil
+      @value_is_set = false
+    end
+
+    def to_hash
+      {
+        "name" => @name,
+        "klass" => @klass,
+        "value" => @value,
+        "value_is_set" => @value_is_set,
+        "default" => @default,
+        # Proc#== will only return true if it's the same obj
+        # so no there's no point in comparing it
+        # also thereś no use case atm to return the proc
+        # so let's not expose it
+        #"validator_proc" => @validator_proc
+      }
+    end
+
+    def ==(other)
+      self.to_hash == other.to_hash
+    end
+
+    private
+    def validate(value)
+      if !value.is_a?(@klass)
+        raise ArgumentError.new("Setting \"#{@name}\" must be a #{@klass}. Received: #{value} (#{value.class})")
+      elsif @validator_proc && !@validator_proc.call(value)
+        raise ArgumentError.new("Failed to validate setting \"#{@name}\" with value: #{value}")
+      end
+    end
+
+    ### Specific settings #####
+
+    class Boolean < Setting
+      def initialize(name, default, strict=true, &validator_proc)
+        @name = name
+        @klass = Object
+        @value = nil
+        @value_is_set = false
+        @validator_proc = validator_proc
+        coerced_default = coerce(default)
+        validate(coerced_default)
+        @default = coerced_default
+      end
+
+      def coerce(value)
+        case value
+        when TrueClass, "true"
+          true
+        when FalseClass, "false"
+          false
+        else
+          raise ArgumentError.new("could not coerce #{value} into a boolean")
+        end
+      end
+
+      def set(value)
+        coerced_value = coerce(value)
+        validate(coerced_value)
+        @value = coerce(coerced_value)
+        @value_is_set = true
+        @value
+      end
+    end
+
+    class String < Setting
+      def initialize(name, default=nil, strict=true)
+        super(name, ::String, default, strict)
+      end
+    end
+
+    class Numeric < Setting
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Numeric, default, strict)
+      end
+    end
+
+    class Port < Setting
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Numeric, default, strict) {|value| value >= 1 && value <= 65535 }
+      end
+    end
+
+    class Validator < Setting
+      def initialize(name, default=nil, strict=true, validator_class=nil)
+        @validator_class = validator_class
+        super(name, ::Object, default, strict)
+      end
+
+      def validate(value)
+        @validator_class.validate(value)
+      end
+    end
+
+    class String < Setting
+      def initialize(name, default=nil, strict=true, possible_strings=[])
+        super(name, ::String, default, strict)
+      end
+
+      def validate(value)
+        super(value) && possible_strings.include?(value)
+      end
+    end
+
+    class ExistingFilePath < Setting
+      def initialize(name, default=nil, strict=true)
+        super(name, ::String, default, strict) do |file_path|
+          if !::File.exists?(file_path)
+            raise ::ArgumentError.new("File \"#{file_path}\" must exist but was not found.")
+          else
+            true
+          end
+        end
+      end
+    end
+
+  end
+
+  SETTINGS = Settings.new
+end
diff --git a/logstash-core/lib/logstash/shutdown_watcher.rb b/logstash-core/lib/logstash/shutdown_watcher.rb
new file mode 100644
index 00000000000..fa0d1f01fd4
--- /dev/null
+++ b/logstash-core/lib/logstash/shutdown_watcher.rb
@@ -0,0 +1,101 @@
+# encoding: utf-8
+
+module LogStash
+  class ShutdownWatcher
+
+    CHECK_EVERY = 1 # second
+    REPORT_EVERY = 5 # checks
+    ABORT_AFTER = 3 # stalled reports
+
+    attr_reader :cycle_period, :report_every, :abort_threshold
+
+    def initialize(pipeline, cycle_period=CHECK_EVERY, report_every=REPORT_EVERY, abort_threshold=ABORT_AFTER)
+      @pipeline = pipeline
+      @cycle_period = cycle_period
+      @report_every = report_every
+      @abort_threshold = abort_threshold
+      @reports = []
+    end
+
+    def self.unsafe_shutdown=(boolean)
+      @unsafe_shutdown = boolean
+    end
+
+    def self.unsafe_shutdown?
+      @unsafe_shutdown
+    end
+
+    def self.logger=(logger)
+      @logger = logger
+    end
+
+    def self.logger
+      @logger ||= Cabin::Channel.get(LogStash)
+    end
+
+    def self.start(pipeline, cycle_period=CHECK_EVERY, report_every=REPORT_EVERY, abort_threshold=ABORT_AFTER)
+      controller = self.new(pipeline, cycle_period, report_every, abort_threshold)
+      Thread.new(controller) { |controller| controller.start }
+    end
+
+    def logger
+      self.class.logger
+    end
+
+    def start
+      sleep(@cycle_period)
+      cycle_number = 0
+      stalled_count = 0
+      Stud.interval(@cycle_period) do
+        break unless @pipeline.thread.alive?
+        @reports << pipeline_report_snapshot
+        @reports.delete_at(0) if @reports.size > @report_every # expire old report
+        if cycle_number == (@report_every - 1) # it's report time!
+          logger.warn(@reports.last)
+
+          if shutdown_stalled?
+            logger.error("The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.") if stalled_count == 0
+            stalled_count += 1
+
+            if self.class.unsafe_shutdown? && @abort_threshold == stalled_count
+              logger.fatal("Forcefully quitting logstash..")
+              force_exit()
+              break
+            end
+          else
+            stalled_count = 0
+          end
+        end
+        cycle_number = (cycle_number + 1) % @report_every
+      end
+    end
+
+    def pipeline_report_snapshot
+      @pipeline.reporter.snapshot
+    end
+
+    # A pipeline shutdown is stalled if
+    # * at least REPORT_EVERY reports have been created
+    # * the inflight event count is in monotonically increasing
+    # * there are worker threads running which aren't blocked on SizedQueue pop/push
+    # * the stalled thread list is constant in the previous REPORT_EVERY reports
+    def shutdown_stalled?
+      return false unless @reports.size == @report_every #
+      # is stalled if inflight count is either constant or increasing
+      stalled_event_count = @reports.each_cons(2).all? do |prev_report, next_report|
+        prev_report.inflight_count <= next_report.inflight_count
+      end
+      if stalled_event_count
+        @reports.each_cons(2).all? do |prev_report, next_report|
+          prev_report.stalling_threads == next_report.stalling_threads
+        end
+      else
+        false
+      end
+    end
+
+    def force_exit
+      exit(-1)
+    end
+  end
+end
diff --git a/lib/logstash/util.rb b/logstash-core/lib/logstash/util.rb
similarity index 71%
rename from lib/logstash/util.rb
rename to logstash-core/lib/logstash/util.rb
index 2034803f43c..88f8b999200 100644
--- a/lib/logstash/util.rb
+++ b/logstash-core/lib/logstash/util.rb
@@ -24,6 +24,41 @@ def self.set_thread_name(name)
     end
   end # def set_thread_name
 
+  def self.set_thread_plugin(plugin)
+    Thread.current[:plugin] = plugin
+  end
+
+  def self.get_thread_id(thread)
+    if RUBY_ENGINE == "jruby"
+      JRuby.reference(thread).native_thread.id
+    else
+      raise Exception.new("Native thread IDs aren't supported outside of JRuby")
+    end
+  end
+
+  def self.thread_info(thread)
+    backtrace = thread.backtrace.map do |line|
+      line.gsub(LogStash::Environment::LOGSTASH_HOME, "[...]")
+    end
+
+    blocked_on = case backtrace.first
+                 when /in `push'/ then "blocked_on_push"
+                 when /(?:pipeline|base).*pop/ then "waiting_for_events"
+                 else nil
+                 end
+
+    {
+      "thread_id" => get_thread_id(thread),
+      "name" => thread[:name],
+      "plugin" => (thread[:plugin] ? thread[:plugin].debug_info : nil),
+      "backtrace" => backtrace,
+      "blocked_on" => blocked_on,
+      "status" => thread.status,
+      "current_call" => backtrace.first
+    }
+  end
+
+
   # Merge hash 'src' into 'dst' nondestructively
   #
   # Duplicate keys will become array values
@@ -148,4 +183,30 @@ def self.stringify_symbols(o)
       o
     end
   end
+
+  # Take a instance reference and return the name of the class
+  # stripping all the modules.
+  #
+  # @param [Object] The object to return the class)
+  # @return [String] The name of the class
+  def self.class_name(instance)
+    instance.class.name.split("::").last
+  end
+
+  def self.deep_clone(o)
+    case o
+    when Hash
+      o.inject({}) {|h, (k,v)| h[k] = deep_clone(v); h }
+    when Array
+      o.map {|v| deep_clone(v) }
+    when Fixnum, Symbol, IO, TrueClass, FalseClass, NilClass
+      o
+    when LogStash::Codecs::Base
+      o.clone
+    when String
+      o.clone #need to keep internal state e.g. frozen
+    else
+      Marshal.load(Marshal.dump(o))
+    end
+  end
 end # module LogStash::Util
diff --git a/lib/logstash/util/buftok.rb b/logstash-core/lib/logstash/util/buftok.rb
similarity index 100%
rename from lib/logstash/util/buftok.rb
rename to logstash-core/lib/logstash/util/buftok.rb
diff --git a/lib/logstash/util/charset.rb b/logstash-core/lib/logstash/util/charset.rb
similarity index 100%
rename from lib/logstash/util/charset.rb
rename to logstash-core/lib/logstash/util/charset.rb
diff --git a/lib/logstash/util/decorators.rb b/logstash-core/lib/logstash/util/decorators.rb
similarity index 53%
rename from lib/logstash/util/decorators.rb
rename to logstash-core/lib/logstash/util/decorators.rb
index 0ea2c021aca..c7807e883b1 100644
--- a/lib/logstash/util/decorators.rb
+++ b/logstash-core/lib/logstash/util/decorators.rb
@@ -7,7 +7,7 @@ module LogStash::Util
   # Decorators provides common manipulation on the event data.
   module Decorators
     extend self
-    
+
     @logger = Cabin::Channel.get(LogStash)
 
     # fields is a hash of field => value
@@ -19,13 +19,16 @@ def add_fields(fields,event, pluginname)
         value.each do |v|
           v = event.sprintf(v)
           if event.include?(field)
-            event[field] = Array(event[field])
-            event[field] << v
+            # note below that the array field needs to be updated then reassigned to the event.
+            # this is important because a construct like event[field] << v will not work
+            # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
+            a = Array(event.get(field))
+            a << v
+            event.set(field, a)
           else
-            event[field] = v
+            event.set(field, v)
           end
-          @logger.debug? and @logger.debug("#{pluginname}: adding value to field",
-                                         :field => field, :value => value)
+          @logger.debug? and @logger.debug("#{pluginname}: adding value to field", :field => field, :value => value)
         end
       end
     end
@@ -34,9 +37,13 @@ def add_fields(fields,event, pluginname)
     def add_tags(tags, event, pluginname)
       tags.each do |tag|
         tag = event.sprintf(tag)
-        @logger.debug? and @logger.debug("#{pluginname}: adding tag",
-                                       :tag => tag)
-        (event["tags"] ||= []) << tag
+        @logger.debug? and @logger.debug("#{pluginname}: adding tag", :tag => tag)
+        # note below that the tags array field needs to be updated then reassigned to the event.
+        # this is important because a construct like event["tags"] << tag will not work
+        # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
+        tags = event.get("tags") || []
+        tags << tag
+        event.set("tags", tags)
       end
     end
 
diff --git a/logstash-core/lib/logstash/util/duration_formatter.rb b/logstash-core/lib/logstash/util/duration_formatter.rb
new file mode 100644
index 00000000000..42cf6ff66f1
--- /dev/null
+++ b/logstash-core/lib/logstash/util/duration_formatter.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require "chronic_duration"
+module LogStash::Util::DurationFormatter
+  CHRONIC_OPTIONS = { :format => :short }
+
+  # Take a duration in milliseconds and transform it into
+  # a format that a human can understand. This is currently used by
+  # the API.
+  #
+  # @param [Fixnum] Duration in milliseconds
+  # @return [String] Duration in human format
+  def self.human_format(duration)
+    ChronicDuration.output(duration / 1000, CHRONIC_OPTIONS)
+  end
+end
diff --git a/lib/logstash/util/filetools.rb b/logstash-core/lib/logstash/util/filetools.rb
similarity index 100%
rename from lib/logstash/util/filetools.rb
rename to logstash-core/lib/logstash/util/filetools.rb
diff --git a/lib/logstash/util/java_version.rb b/logstash-core/lib/logstash/util/java_version.rb
similarity index 76%
rename from lib/logstash/util/java_version.rb
rename to logstash-core/lib/logstash/util/java_version.rb
index 48477580fe4..cebc7927c92 100644
--- a/lib/logstash/util/java_version.rb
+++ b/logstash-core/lib/logstash/util/java_version.rb
@@ -5,16 +5,7 @@ module LogStash::Util::JavaVersion
   def self.logger
     @logger ||= Cabin::Channel.get(LogStash)
   end
-
-  # Print a warning if we're on a bad version of java
-  def self.warn_on_bad_java_version
-    if self.bad_java_version?(self.version)
-      msg = "!!! Please upgrade your java version, the current version '#{self.version}' may cause problems. We recommend a minimum version of 1.7.0_51"
-      STDERR.puts(msg)
-      logger.warn(msg)
-    end
-  end
-
+  
   # Return the current java version string. Returns nil if this is a non-java platform (e.g. MRI).
   def self.version
     return nil unless LogStash::Environment.jruby?
@@ -55,9 +46,7 @@ def self.bad_java_version?(version_string)
     parsed = parse_java_version(version_string)
     return false unless parsed
 
-    if parsed[:major] == 1 && parsed[:minor] == 7 && parsed[:patch] == 0 && parsed[:update] < 51
-      true
-    elsif parsed[:major] == 1 && parsed[:minor] < 7
+    if parsed[:major] == 1 && parsed[:minor] < 8
       true
     else
       false
diff --git a/logstash-core/lib/logstash/util/loggable.rb b/logstash-core/lib/logstash/util/loggable.rb
new file mode 100644
index 00000000000..0add9f3e2b0
--- /dev/null
+++ b/logstash-core/lib/logstash/util/loggable.rb
@@ -0,0 +1,29 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "cabin"
+
+module LogStash module Util
+  module Loggable
+    class << self
+      def logger=(new_logger)
+        @logger = new_logger
+      end
+
+      def logger
+        @logger ||= Cabin::Channel.get(LogStash)
+      end
+    end
+
+    def self.included(base)
+      class << base
+        def logger
+          Loggable.logger
+        end
+      end
+    end
+
+    def logger
+      Loggable.logger
+    end
+  end
+end; end
diff --git a/lib/logstash/util/password.rb b/logstash-core/lib/logstash/util/password.rb
similarity index 100%
rename from lib/logstash/util/password.rb
rename to logstash-core/lib/logstash/util/password.rb
diff --git a/lib/logstash/util/plugin_version.rb b/logstash-core/lib/logstash/util/plugin_version.rb
similarity index 100%
rename from lib/logstash/util/plugin_version.rb
rename to logstash-core/lib/logstash/util/plugin_version.rb
diff --git a/lib/logstash/util/prctl.rb b/logstash-core/lib/logstash/util/prctl.rb
similarity index 100%
rename from lib/logstash/util/prctl.rb
rename to logstash-core/lib/logstash/util/prctl.rb
diff --git a/lib/logstash/util/retryable.rb b/logstash-core/lib/logstash/util/retryable.rb
similarity index 100%
rename from lib/logstash/util/retryable.rb
rename to logstash-core/lib/logstash/util/retryable.rb
diff --git a/lib/logstash/util/socket_peer.rb b/logstash-core/lib/logstash/util/socket_peer.rb
similarity index 100%
rename from lib/logstash/util/socket_peer.rb
rename to logstash-core/lib/logstash/util/socket_peer.rb
diff --git a/logstash-core/lib/logstash/util/thread_dump.rb b/logstash-core/lib/logstash/util/thread_dump.rb
new file mode 100644
index 00000000000..11d1a8da066
--- /dev/null
+++ b/logstash-core/lib/logstash/util/thread_dump.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+module LogStash
+  module Util
+    class ThreadDump
+      SKIPPED_THREADS             = [ "Finalizer", "Reference Handler", "Signal Dispatcher" ].freeze
+      THREADS_COUNT_DEFAULT       = 3.freeze
+      IGNORE_IDLE_THREADS_DEFAULT = true.freeze
+
+      attr_reader :top_count, :ignore, :dump
+
+      def initialize(options={})
+        @options   = options
+        @dump = options.fetch(:dump, JRMonitor.threads.generate({}))
+        @top_count = options.fetch(:threads, THREADS_COUNT_DEFAULT)
+        @ignore    = options.fetch(:ignore_idle_threads, IGNORE_IDLE_THREADS_DEFAULT)
+      end
+
+      def each(&block)
+        i=0
+        dump.each_pair do |thread_name, _hash|
+          break if i >= top_count
+          if ignore
+            next if idle_thread?(thread_name, _hash)
+          end
+          block.call(thread_name, _hash)
+          i += 1
+        end
+      end
+
+      def idle_thread?(thread_name, data)
+        idle = false
+        if SKIPPED_THREADS.include?(thread_name)
+          # these are likely JVM dependent
+          idle = true
+        elsif thread_name.match(/Ruby-\d+-JIT-\d+/)
+          # This are internal JRuby JIT threads, 
+          # see java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor for details.
+          idle = true
+        elsif thread_name.match(/pool-\d+-thread-\d+/)
+          # This are threads used by the internal JRuby implementation to dispatch
+          # calls and tasks, see prg.jruby.internal.runtime.methods.DynamicMethod.call
+          idle = true
+        else
+          data["thread.stacktrace"].each do |trace|
+            if trace.start_with?("java.util.concurrent.ThreadPoolExecutor.getTask")
+              idle = true
+              break
+            end
+          end
+        end
+        idle
+      end
+    end
+  end
+end
diff --git a/lib/logstash/util/unicode_trimmer.rb b/logstash-core/lib/logstash/util/unicode_trimmer.rb
similarity index 100%
rename from lib/logstash/util/unicode_trimmer.rb
rename to logstash-core/lib/logstash/util/unicode_trimmer.rb
diff --git a/logstash-core/lib/logstash/util/worker_threads_default_printer.rb b/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
new file mode 100644
index 00000000000..b35058ac24e
--- /dev/null
+++ b/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
@@ -0,0 +1,29 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+
+# This class exists to format the settings for default worker threads
+module LogStash module Util class WorkerThreadsDefaultPrinter
+
+  def initialize(settings)
+    @setting = settings.fetch('pipeline.workers', 0)
+    @default = settings.fetch('default-pipeline-workers', 0)
+  end
+
+  def visit(collector)
+    visit_setting(collector)
+    visit_default(collector)
+  end
+
+  def visit_setting(collector)
+    return if @setting == 0
+    collector.push("User set pipeline workers: #{@setting}")
+  end
+
+  def visit_default(collector)
+    return if @default == 0
+    collector.push "Default pipeline workers: #{@default}"
+  end
+
+end end end
+
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
new file mode 100644
index 00000000000..a8822ca0af5
--- /dev/null
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+
+module LogStash; module Util
+  class WrappedSynchronousQueue
+    java_import java.util.concurrent.SynchronousQueue
+    java_import java.util.concurrent.TimeUnit
+
+    def initialize()
+      @queue = java.util.concurrent.SynchronousQueue.new()
+    end
+
+    # Push an object to the queue if the queue is full
+    # it will block until the object can be added to the queue.
+    #
+    # @param [Object] Object to add to the queue
+    def push(obj)
+      @queue.put(obj)
+    end
+    alias_method(:<<, :push)
+
+    # Offer an object to the queue, wait for the specified amout of time.
+    # If adding to the queue was successfull it wil return true, false otherwise.
+    #
+    # @param [Object] Object to add to the queue
+    # @param [Integer] Time in milliseconds to wait before giving up
+    # @return [Boolean] True if adding was successfull if not it return false
+    def offer(obj, timeout_ms)
+      @queue.offer(obj, timeout_ms, TimeUnit::MILLISECONDS)
+    end
+
+    # Blocking
+    def take
+      @queue.take()
+    end
+
+    # Block for X millis
+    def poll(millis)
+      @queue.poll(millis, TimeUnit::MILLISECONDS)
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/version.rb b/logstash-core/lib/logstash/version.rb
new file mode 100644
index 00000000000..676dde5ac32
--- /dev/null
+++ b/logstash-core/lib/logstash/version.rb
@@ -0,0 +1,14 @@
+# encoding: utf-8
+
+# The version of the logstash package (not the logstash-core gem version).
+#
+# Note to authors: this should not include dashes because 'gem' barfs if
+# you include a dash in the version string.
+
+# TODO: (colin) the logstash-core gem uses it's own version number in logstash-core/lib/logstash-core/version.rb
+#       there are some dependencies in logstash-core on the LOGSTASH_VERSION constant this is why
+#       the logstash version is currently defined here in logstash-core/lib/logstash/version.rb but
+#       eventually this file should be in the root logstash lib fir and dependencies in logstash-core should be
+#       fixed.
+
+LOGSTASH_VERSION = "5.0.0.dev"
diff --git a/logstash-core/lib/logstash/webserver.rb b/logstash-core/lib/logstash/webserver.rb
new file mode 100644
index 00000000000..6cf8366e3ba
--- /dev/null
+++ b/logstash-core/lib/logstash/webserver.rb
@@ -0,0 +1,65 @@
+# encoding: utf-8
+require "puma"
+require "puma/server"
+require "logstash/api/rack_app"
+
+module LogStash 
+  class WebServer
+    extend Forwardable
+
+    attr_reader :logger, :status, :config, :options, :cli_options, :runner, :binder, :events, :http_host, :http_port, :http_environment
+
+    def_delegator :@runner, :stats
+
+    DEFAULT_HOST = "127.0.0.1".freeze
+    DEFAULT_PORT = 9600.freeze
+    DEFAULT_ENVIRONMENT = 'production'.freeze
+
+    def initialize(logger, options={})
+      @logger      = logger
+      @http_host    = options[:http_host] || DEFAULT_HOST
+      @http_port    = options[:http_port] || DEFAULT_PORT
+      @http_environment = options[:http_environment] || DEFAULT_ENVIRONMENT
+      @options     = {}
+      @cli_options = options.merge({ :rackup => ::File.join(::File.dirname(__FILE__), "api", "init.ru"),
+                                     :binds => ["tcp://#{http_host}:#{http_port}"],
+                                     :debug => logger.debug?,
+                                     # Prevent puma from queueing request when not able to properly handling them,
+                                     # fixed https://github.com/elastic/logstash/issues/4674. See
+                                     # https://github.com/puma/puma/pull/640 for mode internal details in PUMA.
+                                     :queue_requests => false
+      })
+      @status      = nil
+    end
+
+    def run
+      log "=== puma start: #{Time.now} ==="
+
+      stop # Just in case
+
+      app = LogStash::Api::RackApp.app(logger, http_environment)
+      @server = ::Puma::Server.new(app)
+      @server.add_tcp_listener(http_host, http_port)
+
+      @server.run.join
+    end
+
+    def log(str)
+      logger.debug(str)
+    end
+
+    def error(str)
+      logger.error(str)
+    end
+    
+    # Empty method, this method is required because of the puma usage we make through
+    # the Single interface, https://github.com/puma/puma/blob/master/lib/puma/single.rb#L82
+    # for more details. This can always be implemented when we want to keep track of this
+    # bit of data.
+    def write_state; end
+
+    def stop(options={})
+      @server.stop(true) if @server
+    end
+  end
+end
diff --git a/locales/en.yml b/logstash-core/locales/en.yml
similarity index 64%
rename from locales/en.yml
rename to logstash-core/locales/en.yml
index f89fb254fed..ed38a34f968 100644
--- a/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -1,12 +1,13 @@
 # YAML notes
 #   |- means 'scalar block' useful for formatted text
-#   > means 'scalar block' but it chomps all newlines. Useful 
+#   > means 'scalar block' but it chomps all newlines. Useful
 #     for unformatted text.
 en:
   oops: |-
-    The error reported is: 
-      %{error}
+    An unexpected error occurred!
   logstash:
+    error: >-
+      Error: %{error}
     environment:
       jruby-required:  >-
         JRuby is required
@@ -37,7 +38,7 @@ en:
       output-worker-unsupported-with-message: >-
         %{plugin} output plugin: setting 'workers => %{worker_count}' is not
         supported by this plugin. I will continue working as if you had not set
-        this setting.
+        this setting. Reason: %{message}
     plugin:
       deprecated_milestone: >-
         %{plugin} plugin is using the 'milestone' method to declare the version
@@ -57,25 +58,47 @@ en:
     agent:
       sighup: >-
         SIGHUP received.
-      missing-configuration: >-
-        No configuration file was specified. Perhaps you forgot to provide
-        the '-f yourlogstash.conf' flag?
-      error: >-
-        Error: %{error}
       sigint: >-
-        SIGINT received. Shutting down the pipeline.
+        SIGINT received. Shutting down the agent.
       sigterm: >-
-        SIGTERM received. Shutting down the pipeline.
+        SIGTERM received. Shutting down the agent.
       slow_shutdown: |-
         Received shutdown signal, but pipeline is still waiting for in-flight events
         to be processed. Sending another ^C will force quit Logstash, but this may cause
         data loss.
       forced_sigint: >-
         SIGINT received. Terminating immediately..
+      non_reloadable_config_reload: >-
+        Unable to reload configuration because it does not support dynamic reloading
+      non_reloadable_config_register: |-
+        Logstash is not able to start since configuration auto reloading was enabled but the configuration contains plugins that don't support it. Quitting...
+    web_api:
+      hot_threads:
+        title: |-
+          ::: {%{hostname}}
+          Hot threads at %{time}, busiestThreads=%{top_count}:
+        thread_title: |-
+            %{percent_of_cpu_time} % of cpu usage by %{thread_state} thread named '%{thread_name}'
+    runner:
+      short-help: |-
+        usage:
+          bin/logstash -f CONFIG_PATH [-t] [-r] [--quiet|verbose|debug] [-w COUNT] [-l LOG]
+          bin/logstash -e CONFIG_STR [-t] [--quiet|verbose|debug] [-w COUNT] [-l LOG]
+          bin/logstash -i SHELL [--quiet|verbose|debug]
+          bin/logstash -V [--verbose|debug]
+          bin/logstash --help
+      invalid-configuration: >-
+        The given configuration is invalid. Reason: %{error}
+      missing-configuration: >-
+        No configuration file was specified. Perhaps you forgot to provide
+        the '-f yourlogstash.conf' flag?
+      reload-without-config-path: >-
+        Configuration reloading also requires passing a configuration path with '-f yourlogstash.conf'
+      invalid-shell: >-
+        Invalid option for interactive Ruby shell. Use either "irb" or "pry"
       configtest-flag-information: |-
-        You may be interested in the '--configtest' flag which you can
-        use to validate logstash's configuration before you choose
-        to restart a running system.
+        You may be interested in the '--configtest' flag which you can use to validate
+        logstash's configuration before you choose to restart a running system.
       configuration:
         obsolete: >-
           The setting `%{name}` in plugin `%{plugin}` is obsolete and is no
@@ -118,18 +141,18 @@ en:
           after %{after}
         invalid_plugin_register: >-
           Cannot register %{plugin} %{type} plugin.
-          The error reported is: 
+          The error reported is:
             %{error}
         plugin_path_missing: >-
           You specified a plugin path that does not exist: %{path}
         no_plugins_found: |-
           Could not find any plugins in "%{path}"
-          I tried to find files matching the following, but found none: 
+          I tried to find files matching the following, but found none:
             %{plugin_glob}
         log_file_failed: |-
           Failed to open %{path} for writing: %{error}
 
-          This is often a permissions issue, or the wrong 
+          This is often a permissions issue, or the wrong
           path was specified?
       flag:
         # Note: Wrap these at 55 chars so they display nicely when clamp emits
@@ -155,8 +178,25 @@ en:
           the empty string for the '-e' flag.
         configtest: |+
           Check configuration for valid syntax and then exit.
-        filterworkers: |+
-          Sets the number of filter workers to run.
+        http_host: Web API binding host
+        http_port: Web API http port
+        pipeline-workers: |+
+          Sets the number of pipeline workers to run.
+        pipeline-batch-size: |+
+          Size of batches the pipeline is to work in.
+        pipeline-batch-delay: |+
+          When creating pipeline batches, how long to wait while polling
+          for the next event.
+        path_settings: |+
+          Directory containing logstash.yml file. This can also be
+          set through the LS_SETTINGS_DIR environment variable.
+        auto_reload: |+
+          Monitor configuration changes and reload
+          whenever it is changed.
+          NOTE: use SIGHUP to manually reload the config
+        reload_interval: |+
+          How frequently to poll the configuration location
+          for changes, in seconds.
         log: |+
           Write logstash internal logs to the given
           file. Without this flag, logstash will emit
@@ -178,12 +218,29 @@ en:
           'PATH/logstash/TYPE/NAME.rb' where TYPE is
           'inputs' 'filters', 'outputs' or 'codecs'
           and NAME is the name of the plugin.
-        quiet: |+
-          Quieter logstash logging. This causes only 
-          errors to be emitted.
-        verbose: |+
-          More verbose logging. This causes 'info' 
-          level logs to be emitted.
-        debug: |+
-          Most verbose logging. This causes 'debug'
-          level logs to be emitted.
+        log_level: |+
+          Set the log level for logstash. Possible values are:
+            - quiet => :error
+            - verbose => :info
+            - debug => :debug
+            - warn => :warn
+        unsafe_shutdown: |+
+          Force logstash to exit during shutdown even
+          if there are still inflight events in memory.
+          By default, logstash will refuse to quit until all
+          received events have been pushed to the outputs.
+        rubyshell: |+
+          Drop to shell instead of running as normal.
+          Valid shells are "irb" and "pry"
+        node_name: |+
+          Specify the name of this logstash instance, if no value is given
+          it will default to the current hostname.
+        agent: |+
+          Specify an alternate agent plugin name.
+        config_debug: |+
+          Print the compiled config ruby code out as a debug log (you must also have --debug enabled).
+          WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result
+          in plaintext passwords appearing in your logs!
+        log_format: |+
+          Specify if Logstash should write its own logs in JSON form (one
+          event per line) or in plain text (using Ruby's Object#inspect)
diff --git a/logstash-core.gemspec b/logstash-core/logstash-core.gemspec
similarity index 70%
rename from logstash-core.gemspec
rename to logstash-core/logstash-core.gemspec
index 35937a060a1..f63617656f8 100644
--- a/logstash-core.gemspec
+++ b/logstash-core/logstash-core.gemspec
@@ -1,30 +1,36 @@
 # -*- encoding: utf-8 -*-
 lib = File.expand_path('../lib', __FILE__)
 $LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
-require 'logstash/version'
+require 'logstash-core/version'
 
 Gem::Specification.new do |gem|
-  gem.authors       = ["Jordan Sissel", "Pete Fritchman", "Elasticsearch"]
-  gem.email         = ["jls@semicomplete.com", "petef@databits.net", "info@elasticsearch.com"]
+  gem.authors       = ["Elastic"]
+  gem.email         = ["info@elastic.co"]
   gem.description   = %q{The core components of logstash, the scalable log and event management tool}
   gem.summary       = %q{logstash-core - The core components of logstash}
   gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
   gem.license       = "Apache License (2.0)"
 
-  gem.files         = Dir.glob(["logstash-core.gemspec", "lib/logstash-core.rb", "lib/logstash/**/*.rb", "spec/**/*.rb", "locales/*"])
+  gem.files         = Dir.glob(["logstash-core.gemspec", "lib/**/*.rb", "spec/**/*.rb", "locales/*", "lib/logstash/api/init.ru"])
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core"
   gem.require_paths = ["lib"]
-  gem.version       = LOGSTASH_VERSION
+  gem.version       = LOGSTASH_CORE_VERSION
 
-  gem.add_runtime_dependency "cabin", "~> 0.7.0" #(Apache 2.0 license)
+  gem.add_runtime_dependency "logstash-core-event-java", "~> 5.0.0.dev"
+
+  gem.add_runtime_dependency "cabin", "~> 0.8.0" #(Apache 2.0 license)
   gem.add_runtime_dependency "pry", "~> 0.10.1"  #(Ruby license)
   gem.add_runtime_dependency "stud", "~> 0.0.19" #(Apache 2.0 license)
   gem.add_runtime_dependency "clamp", "~> 0.6.5" #(MIT license) for command line args/flags
   gem.add_runtime_dependency "filesize", "0.0.4" #(MIT license) for :bytes config validator
   gem.add_runtime_dependency "gems", "~> 0.8.3"  #(MIT license)
-  gem.add_runtime_dependency "concurrent-ruby", "0.9.2"
-  gem.add_runtime_dependency "jruby-openssl", ">= 0.9.11" # Required to support TLSv1.2
+  gem.add_runtime_dependency "concurrent-ruby", "1.0.0"
+  gem.add_runtime_dependency "sinatra", '~> 1.4', '>= 1.4.6'
+  gem.add_runtime_dependency 'puma', '~> 2.16'
+  gem.add_runtime_dependency "jruby-openssl", "0.9.16" # >= 0.9.13 Required to support TLSv1.2
+  gem.add_runtime_dependency "chronic_duration", "0.10.6"
+  gem.add_runtime_dependency "jrmonitor", '~> 0.4.2'
 
   # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
   # investigated what the cause might be. -Jordan
@@ -35,6 +41,7 @@ Gem::Specification.new do |gem|
 
   # filetools and rakelib
   gem.add_runtime_dependency "minitar", "~> 0.5.4"
+  gem.add_runtime_dependency "rubyzip", "~> 1.1.7"
   gem.add_runtime_dependency "thread_safe", "~> 0.3.5" #(Apache 2.0 license)
 
   if RUBY_PLATFORM == 'java'
diff --git a/logstash-core/spec/api/lib/api/node_spec.rb b/logstash-core/spec/api/lib/api/node_spec.rb
new file mode 100644
index 00000000000..3fc6ad97752
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/node_spec.rb
@@ -0,0 +1,64 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/node"
+require "logstash/json"
+
+describe LogStash::Api::Modules::Node do
+
+  include Rack::Test::Methods
+
+  def app()
+    described_class
+  end
+
+  describe "#hot threads" do
+
+    before(:all) do
+      do_request { get "/hot_threads" }
+    end
+
+    it "respond OK" do
+      expect(last_response).to be_ok
+    end
+
+    it "should return a JSON object" do
+      expect{ LogStash::Json.load(last_response.body) }.not_to raise_error
+    end
+
+    context "#threads count" do
+
+      before(:all) do
+        do_request { get "/hot_threads?threads=5" }
+      end
+
+      let(:payload) { LogStash::Json.load(last_response.body) }
+
+      it "should return a json payload content type" do
+        expect(last_response.content_type).to eq("application/json")
+      end
+
+      it "should return information for <= # requested threads" do
+        expect(payload["threads"].count).to be <= 5
+      end
+    end
+
+    context "when asking for human output" do
+
+      before(:all) do
+        do_request { get "/hot_threads?human" }
+      end
+
+      let(:payload) { last_response.body }
+
+      it "should return a text/plain content type" do
+        expect(last_response.content_type).to eq("text/plain;charset=utf-8")
+      end
+
+      it "should return a plain text payload" do
+        expect{ JSON.parse(payload) }.to raise_error
+      end
+    end
+
+  end
+end
diff --git a/logstash-core/spec/api/lib/api/node_stats_spec.rb b/logstash-core/spec/api/lib/api/node_stats_spec.rb
new file mode 100644
index 00000000000..03bf7fd6152
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/node_stats_spec.rb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/node_stats"
+require "logstash/json"
+
+describe LogStash::Api::Modules::NodeStats do
+  include Rack::Test::Methods
+  extend ResourceDSLMethods
+
+  def app() # Used by Rack::Test::Methods
+    described_class
+  end
+
+  # DSL describing response structure
+  root_structure = {
+    "events"=>{
+      "in"=>Numeric,
+      "filtered"=>Numeric,
+      "out"=>Numeric
+    },
+    "jvm"=>{
+      "threads"=>{
+        "count"=>Numeric,
+        "peak_count"=>Numeric
+      }
+    },
+    "process"=>{
+      "peak_open_file_descriptors"=>Numeric,
+      "max_file_descriptors"=>Numeric,
+      "open_file_descriptors"=>Numeric,
+      "mem"=>{
+        "total_virtual_in_bytes"=>Numeric
+      },
+      "cpu"=>{
+        "total_in_millis"=>Numeric,
+        "percent"=>Numeric
+      }
+    }
+  }
+
+  test_api_and_resources(root_structure)
+end
diff --git a/logstash-core/spec/api/lib/api/plugins_spec.rb b/logstash-core/spec/api/lib/api/plugins_spec.rb
new file mode 100644
index 00000000000..216811d0a9b
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/plugins_spec.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/plugins"
+require "logstash/json"
+
+describe LogStash::Api::Modules::Plugins do
+
+  include Rack::Test::Methods
+
+  def app()
+    described_class
+  end
+
+  before(:all) do
+    get "/"
+  end
+
+  let(:payload) { LogStash::Json.load(last_response.body) }
+
+  it "respond to plugins resource" do
+    expect(last_response).to be_ok
+  end
+
+  it "return valid json content type" do
+    expect(last_response.content_type).to eq("application/json"), "Did not get json, got #{last_response.content_type} / #{last_response.body}"
+  end
+
+  context "#schema" do
+    it "return the expected schema" do
+      expect(payload.keys).to include("plugins", "total")
+      payload["plugins"].each do |plugin|
+        expect(plugin.keys).to include("name", "version")
+      end
+    end
+  end
+
+  context "#values" do
+
+    it "return totals of plugins" do
+      expect(payload["total"]).to eq(payload["plugins"].count)
+    end
+
+    it "return a list of available plugins" do
+      payload["plugins"].each do |plugin|
+        expect(plugin).to be_available?
+      end
+    end
+
+    it "return non empty version values" do
+      payload["plugins"].each do |plugin|
+        expect(plugin["version"]).not_to be_empty
+      end
+    end
+
+  end
+end
diff --git a/logstash-core/spec/api/lib/api/root_spec.rb b/logstash-core/spec/api/lib/api/root_spec.rb
new file mode 100644
index 00000000000..2395c9edc5d
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/root_spec.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/root"
+require "logstash/json"
+
+describe LogStash::Api::Modules::Root do
+
+  include Rack::Test::Methods
+
+  def app()
+    described_class
+  end
+
+  it "should respond to root resource" do
+    do_request { get "/" }
+    expect(last_response).to be_ok
+  end
+
+end
diff --git a/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
new file mode 100644
index 00000000000..786a37f984a
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
@@ -0,0 +1,47 @@
+module ResourceDSLMethods
+  # Convert a nested hash to a mapping of key paths to expected classes
+  def hash_to_mapping(h, path=[], mapping={})
+    h.each do |k,v|
+      if v.is_a?(Hash)
+        hash_to_mapping(v, path + [k], mapping)
+      else
+        full_path = path + [k]
+        mapping[full_path] = v
+      end
+    end
+    mapping
+  end
+
+  def test_api(expected, path)
+    context "GET #{path}" do
+      let(:payload) { LogStash::Json.load(last_response.body) }
+      
+      before(:all) do
+        do_request { get path }
+      end      
+      
+      it "should respond OK" do
+        expect(last_response).to be_ok
+      end
+      
+      hash_to_mapping(expected).each do |path,klass|
+        dotted = path.join(".")
+        
+        it "should set '#{dotted}' to be a '#{klass}'" do
+          path_value = path.reduce(payload) {|acc,v| acc[v]}
+          expect(path_value).to be_a(klass), "could not find '#{dotted}' in #{payload}"
+        end
+      end
+    end
+
+    yield if block_given? # Add custom expectations
+  end
+
+  def test_api_and_resources(expected)
+    test_api(expected, "/")
+
+    expected.keys.each do |key|
+      test_api({key => expected[key]}, "/#{key}")
+    end
+  end
+end
diff --git a/logstash-core/spec/api/lib/commands/stats.rb b/logstash-core/spec/api/lib/commands/stats.rb
new file mode 100644
index 00000000000..3059e1460f3
--- /dev/null
+++ b/logstash-core/spec/api/lib/commands/stats.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+
+describe LogStash::Api::Commands::Stats do
+
+  let(:report_method) { :run }
+  subject(:report) { do_request { report_class.new.send(report_method) } }
+
+  let(:report_class) { described_class }
+
+  describe "#events" do
+    let(:report_method) { :events }
+
+    it "return events information" do
+      expect(report.keys).to include(:in, :filtered, :out)
+    end
+  end
+  
+  describe "#hot_threads" do
+    let(:report_method) { :hot_threads }
+    
+    it "should return hot threads information as a string" do
+      expect(report.to_s).to be_a(String)
+    end
+
+    it "should return hot threads info as a hash" do
+      expect(report.to_hash).to be_a(Hash)
+    end
+  end
+
+  describe "memory stats" do
+    let(:report_method) { :memory }
+      
+    it "return hot threads information" do
+      expect(report).not_to be_empty
+    end
+
+    it "return heap information" do
+      expect(report.keys).to include(:heap_used_in_bytes)
+    end
+
+    it "return non heap information" do
+      expect(report.keys).to include(:non_heap_used_in_bytes)
+    end
+
+  end
+end
diff --git a/logstash-core/spec/api/lib/rack_app_spec.rb b/logstash-core/spec/api/lib/rack_app_spec.rb
new file mode 100644
index 00000000000..0546df9fbf6
--- /dev/null
+++ b/logstash-core/spec/api/lib/rack_app_spec.rb
@@ -0,0 +1,88 @@
+require "logstash/api/rack_app"
+require "rack/test"
+
+describe LogStash::Api::RackApp do
+  include Rack::Test::Methods
+
+  class DummyApp
+    class RaisedError < StandardError; end
+    
+    def call(env)
+      case env["PATH_INFO"]
+      when "/good-page"
+        [200, {}, ["200 OK"]]
+      when "/service-unavailable"
+        [503, {}, ["503 service unavailable"]]
+      when "/raise-error"
+        raise RaisedError, "Error raised"
+      else
+        [404, {}, ["404 Page not found"]]
+      end
+    end
+  end
+
+  let(:logger) { Cabin::Channel.get }
+
+  describe LogStash::Api::RackApp::ApiErrorHandler do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+
+    it "should let good requests through as normal" do
+      get "/good-page"
+      expect(last_response).to be_ok
+    end
+
+    it "should let through 5xx codes" do
+      get "/service-unavailable"
+      expect(last_response.status).to eql(503)
+    end
+
+    describe "raised exceptions" do
+      before do
+        allow(logger).to receive(:error).with(any_args)
+        get "/raise-error"
+      end
+      
+      it "should return a 500 error" do
+        expect(last_response.status).to eql(500)
+      end
+
+      it "should return valid JSON" do
+        expect { LogStash::Json.load(last_response.body) }.not_to raise_error
+      end
+
+      it "should log the error" do
+        expect(logger).to have_received(:error).with(LogStash::Api::RackApp::ApiErrorHandler::LOG_MESSAGE, anything).once
+      end
+    end
+  end
+
+  describe LogStash::Api::RackApp::ApiLogger do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+    
+    it "should log good requests as info" do
+      expect(logger).to receive(:info).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/good-page"
+    end
+
+    it "should log 5xx requests as warnings" do
+      expect(logger).to receive(:warn).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/service-unavailable"
+    end
+  end
+end
diff --git a/logstash-core/spec/api/spec_helper.rb b/logstash-core/spec/api/spec_helper.rb
new file mode 100644
index 00000000000..041311b9314
--- /dev/null
+++ b/logstash-core/spec/api/spec_helper.rb
@@ -0,0 +1,131 @@
+# encoding: utf-8
+API_ROOT = File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "lib", "logstash", "api"))
+
+
+
+require "logstash/devutils/rspec/spec_helper"
+
+$LOAD_PATH.unshift(File.expand_path(File.dirname(__FILE__)))
+require "lib/api/support/resource_dsl_methods"
+
+require "logstash/settings"
+require 'rack/test'
+require 'rspec'
+require "json"
+
+def read_fixture(name)
+  path = File.join(File.dirname(__FILE__), "fixtures", name)
+  File.read(path)
+end
+
+module LogStash
+  class DummyAgent < Agent
+    def fetch_config(settings)
+      "input { generator {count => 0} } output { }"
+    end
+
+    def start_webserver; end
+    def stop_webserver; end
+  end
+end
+
+##
+# Class used to wrap and manage the execution of an agent for test,
+# this helps a lot in order to have a more integrated test for the
+# web api, could be also used for other use cases if generalized enought
+##
+class LogStashRunner
+
+  attr_reader :config_str, :agent, :pipeline_settings
+
+  def initialize
+    @config_str   = "input { generator {count => 0} } output { }"
+    args = {
+      "config.reload.automatic" => false,
+      "metric.collect" => true,
+      "log.level" => "debug",
+      "node.name" => "test_agent",
+      "http.port" => rand(9600..9700),
+      "http.environment" => "test",      
+      "config.string" => @config_str,
+      "pipeline.batch.size" => 1,
+      "pipeline.workers" => 1
+    }
+    @settings = ::LogStash::SETTINGS.clone.merge(args)
+
+    @agent = LogStash::DummyAgent.new(@settings)
+  end
+
+  def start
+    agent.register_pipeline("main", @settings)
+    @runner = Thread.new(agent) do |_agent|
+      _agent.execute
+    end
+
+    wait_until_ready
+  end
+
+  def stop
+    agent.shutdown
+    Thread.kill(@runner)
+    sleep 0.1 while !@runner.stop?
+  end
+
+  private
+
+  def wait_until_ready
+    # Wait until the service and pipeline have started
+    while !(LogStash::Api::Service.instance.started? && agent.pipelines["main"].running?) do
+      sleep 0.5
+    end
+  end
+end
+
+
+##
+# Method used to wrap up a request in between of a running
+# pipeline, this makes the whole execution model easier and
+# more contained as some threads might go wild.
+##
+def do_request(&block)
+  runner = LogStashRunner.new
+  runner.start
+  ret_val = block.call
+  runner.stop
+  ret_val
+end
+
+##
+# Helper module that setups necessary mocks when doing the requests,
+# this could be just included in the test and the runner will be
+# started managed for all tests.
+##
+module LogStash; module RSpec; module RunnerConfig
+  def self.included(klass)
+    klass.before(:all) do
+      LogStashRunner.instance.start
+    end
+
+    klass.before(:each) do
+      runner = LogStashRunner.instance
+      allow(LogStash::Instrument::Collector.instance).to receive(:agent).and_return(runner.agent)
+    end
+
+    klass.after(:all) do
+      LogStashRunner.instance.stop
+    end
+  end
+end; end; end
+
+require 'rspec/expectations'
+
+RSpec::Matchers.define :be_available? do
+  match do |plugin|
+    begin
+      Gem::Specification.find_by_name(plugin["name"])
+      true
+    rescue
+      false
+    end
+  end
+end
diff --git a/spec/core/conditionals_spec.rb b/logstash-core/spec/conditionals_spec.rb
similarity index 50%
rename from spec/core/conditionals_spec.rb
rename to logstash-core/spec/conditionals_spec.rb
index dab6fc901e3..c4c68ec4c58 100644
--- a/spec/core/conditionals_spec.rb
+++ b/logstash-core/spec/conditionals_spec.rb
@@ -64,24 +64,24 @@ def conditional(expression, &block)
     CONFIG
 
     sample({"foo" => "bar"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to eq("world")
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to eq("world")
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample({"notfoo" => "bar"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to eq("hugs")
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to eq("hugs")
     end
 
     sample({"bar" => "baz"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to eq("pants")
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to eq("pants")
+      expect(subject.get("free")).to be_nil
     end
   end
 
@@ -102,31 +102,31 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => "bar", "nest" => 124) do
-      expect(subject["always"]).to be_nil
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to be_nil
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample("foo" => "bar", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to eq("world")
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to eq("world")
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample("notfoo" => "bar", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to eq("hugs")
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to eq("hugs")
     end
 
     sample("bar" => "baz", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to eq("pants")
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to eq("pants")
+      expect(subject.get("free")).to be_nil
     end
   end
 
@@ -140,7 +140,7 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => 123, "bar" => 123) do
-      expect(subject["tags"] ).to include("woot")
+      expect(subject.get("tags") ).to include("woot")
     end
   end
 
@@ -169,12 +169,12 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => "foo", "foobar" => "foobar", "greeting" => "hello world") do
-      expect(subject["tags"]).to include("field in field")
-      expect(subject["tags"]).to include("field in string")
-      expect(subject["tags"]).to include("string in field")
-      expect(subject["tags"]).to include("field in list")
-      expect(subject["tags"]).not_to include("shouldnotexist")
-      expect(subject["tags"]).to include("shouldexist")
+      expect(subject.get("tags")).to include("field in field")
+      expect(subject.get("tags")).to include("field in string")
+      expect(subject.get("tags")).to include("string in field")
+      expect(subject.get("tags")).to include("field in list")
+      expect(subject.get("tags")).not_to include("shouldnotexist")
+      expect(subject.get("tags")).to include("shouldexist")
     end
   end
 
@@ -192,107 +192,107 @@ def conditional(expression, &block)
 
     sample("foo" => "foo", "somelist" => [ "one", "two" ], "foobar" => "foobar", "greeting" => "hello world", "tags" => [ "fancypantsy" ]) do
       # verify the original exists
-      expect(subject["tags"]).to include("fancypantsy")
+      expect(subject.get("tags")).to include("fancypantsy")
 
-      expect(subject["tags"]).to include("baz")
-      expect(subject["tags"]).not_to include("foo")
-      expect(subject["tags"]).to include("notfoo")
-      expect(subject["tags"]).to include("notsomelist")
-      expect(subject["tags"]).not_to include("somelist")
-      expect(subject["tags"]).to include("no string in missing field")
+      expect(subject.get("tags")).to include("baz")
+      expect(subject.get("tags")).not_to include("foo")
+      expect(subject.get("tags")).to include("notfoo")
+      expect(subject.get("tags")).to include("notsomelist")
+      expect(subject.get("tags")).not_to include("somelist")
+      expect(subject.get("tags")).to include("no string in missing field")
     end
   end
 
   describe "operators" do
     conditional "[message] == 'sample'" do
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("different") { expect(subject["tags"] ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("different") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] != 'sample'" do
-      sample("sample") { expect(subject["tags"] ).to include("failure") }
-      sample("different") { expect(subject["tags"] ).to include("success") }
+      sample("sample") { expect(subject.get("tags") ).to include("failure") }
+      sample("different") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] < 'sample'" do
-      sample("apple") { expect(subject["tags"] ).to include("success") }
-      sample("zebra") { expect(subject["tags"] ).to include("failure") }
+      sample("apple") { expect(subject.get("tags") ).to include("success") }
+      sample("zebra") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] > 'sample'" do
-      sample("zebra") { expect(subject["tags"] ).to include("success") }
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
+      sample("zebra") { expect(subject.get("tags") ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] <= 'sample'" do
-      sample("apple") { expect(subject["tags"] ).to include("success") }
-      sample("zebra") { expect(subject["tags"] ).to include("failure") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("success") }
+      sample("zebra") { expect(subject.get("tags") ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] >= 'sample'" do
-      sample("zebra") { expect(subject["tags"] ).to include("success") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
+      sample("zebra") { expect(subject.get("tags") ).to include("success") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] =~ /sample/" do
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("some sample") { expect(subject["tags"] ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("some sample") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] !~ /sample/" do
-      sample("apple") { expect(subject["tags"]).to include("success") }
-      sample("sample") { expect(subject["tags"]).to include("failure") }
-      sample("some sample") { expect(subject["tags"]).to include("failure") }
+      sample("apple") { expect(subject.get("tags")).to include("success") }
+      sample("sample") { expect(subject.get("tags")).to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).to include("failure") }
     end
 
   end
 
   describe "negated expressions" do
     conditional "!([message] == 'sample')" do
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("different") { expect(subject["tags"]).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("different") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] != 'sample')" do
-      sample("sample") { expect(subject["tags"]).not_to include("failure") }
-      sample("different") { expect(subject["tags"]).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("failure") }
+      sample("different") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] < 'sample')" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("zebra") { expect(subject["tags"]).not_to include("failure") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] > 'sample')" do
-      sample("zebra") { expect(subject["tags"]).not_to include("success") }
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] <= 'sample')" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("zebra") { expect(subject["tags"]).not_to include("failure") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] >= 'sample')" do
-      sample("zebra") { expect(subject["tags"]).not_to include("success") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] =~ /sample/)" do
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("some sample") { expect(subject["tags"]).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("some sample") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] !~ /sample/)" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("sample") { expect(subject["tags"]).not_to include("failure") }
-      sample("some sample") { expect(subject["tags"]).not_to include("failure") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).not_to include("failure") }
     end
 
   end
@@ -300,47 +300,47 @@ def conditional(expression, &block)
   describe "value as an expression" do
     # testing that a field has a value should be true.
     conditional "[message]" do
-      sample("apple") { expect(subject["tags"]).to include("success") }
-      sample("sample") { expect(subject["tags"]).to include("success") }
-      sample("some sample") { expect(subject["tags"]).to include("success") }
+      sample("apple") { expect(subject.get("tags")).to include("success") }
+      sample("sample") { expect(subject.get("tags")).to include("success") }
+      sample("some sample") { expect(subject.get("tags")).to include("success") }
     end
 
     # testing that a missing field has a value should be false.
     conditional "[missing]" do
-      sample("apple") { expect(subject["tags"]).to include("failure") }
-      sample("sample") { expect(subject["tags"]).to include("failure") }
-      sample("some sample") { expect(subject["tags"]).to include("failure") }
+      sample("apple") { expect(subject.get("tags")).to include("failure") }
+      sample("sample") { expect(subject.get("tags")).to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).to include("failure") }
     end
   end
 
   describe "logic operators" do
     describe "and" do
       conditional "[message] and [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "[message] and ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
       conditional "![message] and [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
       conditional "![message] and ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
     end
 
     describe "or" do
       conditional "[message] or [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "[message] or ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "![message] or [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "![message] or ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
     end
   end
@@ -348,19 +348,19 @@ def conditional(expression, &block)
   describe "field references" do
     conditional "[field with space]" do
       sample("field with space" => "hurray") do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
 
     conditional "[field with space] == 'hurray'" do
       sample("field with space" => "hurray") do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
 
     conditional "[nested field][reference with][some spaces] == 'hurray'" do
       sample({"nested field" => { "reference with" => { "some spaces" => "hurray" } } }) do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
   end
@@ -385,13 +385,13 @@ def conditional(expression, &block)
       expect(subject).to be_an(Array)
       expect(subject.length).to eq(2)
 
-      expect(subject[0]["type"]).to eq("original")
-      expect(subject[0]["cond1"]).to eq("true")
-      expect(subject[0]["cond2"]).to eq(nil)
+      expect(subject[0].get("type")).to eq("original")
+      expect(subject[0].get("cond1")).to eq("true")
+      expect(subject[0].get("cond2")).to eq(nil)
 
-      expect(subject[1]["type"]).to eq("clone")
-      # expect(subject[1]["cond1"]).to eq(nil)
-      # expect(subject[1]["cond2"]).to eq("true")
+      expect(subject[1].get("type")).to eq("clone")
+      # expect(subject[1].get("cond1")).to eq(nil)
+      # expect(subject[1].get("cond2")).to eq("true")
     end
   end
 
@@ -413,16 +413,16 @@ def conditional(expression, &block)
 
     sample({"type" => "original"}) do
       # puts subject.inspect
-      expect(subject[0]["cond1"]).to eq(nil)
-      expect(subject[0]["cond2"]).to eq(nil)
+      expect(subject[0].get("cond1")).to eq(nil)
+      expect(subject[0].get("cond2")).to eq(nil)
 
-      expect(subject[1]["type"]).to eq("clone1")
-      expect(subject[1]["cond1"]).to eq("true")
-      expect(subject[1]["cond2"]).to eq(nil)
+      expect(subject[1].get("type")).to eq("clone1")
+      expect(subject[1].get("cond1")).to eq("true")
+      expect(subject[1].get("cond2")).to eq(nil)
 
-      expect(subject[2]["type"]).to eq("clone2")
-      expect(subject[2]["cond1"]).to eq(nil)
-      expect(subject[2]["cond2"]).to eq("true")
+      expect(subject[2].get("type")).to eq("clone2")
+      expect(subject[2].get("cond1")).to eq(nil)
+      expect(subject[2].get("cond2")).to eq("true")
     end
   end
 
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
new file mode 100644
index 00000000000..5a40ee9c0ec
--- /dev/null
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -0,0 +1,396 @@
+# encoding: utf-8
+require "spec_helper"
+require "stud/temporary"
+require "logstash/inputs/generator"
+require_relative "../support/mocks_classes"
+
+describe LogStash::Agent do
+
+  let(:logger) { double("logger") }
+  let(:agent_settings) { LogStash::SETTINGS }
+  let(:agent_args) { {} }
+  let(:pipeline_settings) { agent_settings.clone }
+  let(:pipeline_args) { {} }
+  let(:config_file) { Stud::Temporary.pathname }
+  let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
+
+  subject { LogStash::Agent.new(agent_settings) }
+
+  before :each do
+    [:info, :warn, :error, :fatal, :debug].each do |level|
+      allow(logger).to receive(level)
+    end
+    [:info?, :warn?, :error?, :fatal?, :debug?].each do |level|
+      allow(logger).to receive(level)
+    end
+    File.open(config_file, "w") { |f| f.puts config_file_txt }
+    agent_args.each do |key, value|
+      agent_settings.set(key, value)
+      pipeline_settings.set(key, value)
+    end
+    pipeline_args.each do |key, value|
+      pipeline_settings.set(key, value)
+    end
+    #subject.logger = logger
+  end
+
+  after :each do
+    LogStash::SETTINGS.reset
+    File.unlink(config_file)
+  end
+
+  it "fallback to hostname when no name is provided" do
+    expect(LogStash::Agent.new.node_name).to eq(Socket.gethostname)
+  end
+
+  describe "register_pipeline" do
+    let(:pipeline_id) { "main" }
+    let(:config_string) { "input { } filter { } output { }" }
+    let(:agent_args) do
+      { 
+        "config.string" => config_string,
+        "config.reload.automatic" => true,
+        "config.reload.interval" => 0.01,
+	"pipeline.workers" => 4,
+      }
+    end
+
+    it "should delegate settings to new pipeline" do
+      expect(LogStash::Pipeline).to receive(:new) do |arg1, arg2|
+        expect(arg1).to eq(config_string)
+	expect(arg2.to_hash).to include(agent_args)
+      end
+      subject.register_pipeline(pipeline_id, agent_settings)
+    end
+  end
+
+  describe "#execute" do
+    let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
+
+    before :each do
+      allow(subject).to receive(:start_webserver).and_return(false)
+      allow(subject).to receive(:stop_webserver).and_return(false)
+    end
+
+    context "when auto_reload is false" do
+      let(:agent_args) do
+        {
+          "config.reload.automatic" => false,
+          "path.config" => config_file
+        }
+      end
+      let(:pipeline_id) { "main" }
+
+      before(:each) do
+        subject.register_pipeline(pipeline_id, pipeline_settings)
+      end
+
+      context "if state is clean" do
+        before :each do
+          allow(subject).to receive(:running_pipelines?).and_return(true)
+          allow(subject).to receive(:sleep)
+          allow(subject).to receive(:clean_state?).and_return(false)
+        end
+
+        it "should not reload_state!" do
+          expect(subject).to_not receive(:reload_state!)
+          t = Thread.new { subject.execute }
+          sleep 0.1
+          Stud.stop!(t)
+          t.join
+          subject.shutdown
+        end
+      end
+
+      context "when calling reload_state!" do
+        context "with a config that contains reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { stdin {} } filter { } output { }" }
+
+          it "does not upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to_not receive(:upgrade_pipeline)
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.send(:reload_state!)
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
+        end
+
+        context "with a config that does not contain reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+
+          it "does upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to receive(:upgrade_pipeline).once.and_call_original
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.send(:reload_state!)
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+
+            subject.shutdown
+          end
+        end
+      end
+    end
+
+    context "when auto_reload is true" do
+      let(:agent_args) do
+        {
+          "config.reload.automatic" => true,
+          "config.reload.interval" => 0.01,
+          "path.config" => config_file,
+        }
+      end
+      let(:pipeline_id) { "main" }
+
+      before(:each) do
+        subject.register_pipeline(pipeline_id, pipeline_settings)
+      end
+
+      context "if state is clean" do
+        it "should periodically reload_state" do
+          allow(subject).to receive(:clean_state?).and_return(false)
+          expect(subject).to receive(:reload_state!).at_least(3).times
+          t = Thread.new { subject.execute }
+          sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+          sleep 0.1
+          Stud.stop!(t)
+          t.join
+          subject.shutdown
+        end
+      end
+
+      context "when calling reload_state!" do
+        context "with a config that contains reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { stdin {} } filter { } output { }" }
+
+          it "does not upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to_not receive(:upgrade_pipeline)
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
+        end
+
+        context "with a config that does not contain reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+
+          it "does upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to receive(:upgrade_pipeline).once.and_call_original
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
+        end
+      end
+    end
+  end
+
+  describe "#reload_state!" do
+    let(:pipeline_id) { "main" }
+    let(:first_pipeline_config) { "input { } filter { } output { }" }
+    let(:second_pipeline_config) { "input { generator {} } filter { } output { }" }
+    let(:pipeline_args) { {
+      "config.string" => first_pipeline_config,
+      "pipeline.workers" => 4
+    } }
+
+    before(:each) do
+      subject.register_pipeline(pipeline_id, pipeline_settings)
+    end
+
+    context "when fetching a new state" do
+      it "upgrades the state" do
+        expect(subject).to receive(:fetch_config).and_return(second_pipeline_config)
+        expect(subject).to receive(:upgrade_pipeline).with(pipeline_id, kind_of(LogStash::Pipeline))
+        subject.send(:reload_state!)
+      end
+    end
+    context "when fetching the same state" do
+      it "doesn't upgrade the state" do
+        expect(subject).to receive(:fetch_config).and_return(first_pipeline_config)
+        expect(subject).to_not receive(:upgrade_pipeline)
+        subject.send(:reload_state!)
+      end
+    end
+  end
+
+  describe "Environment Variables In Configs" do
+    let(:pipeline_config) { "input { generator { message => '${FOO}-bar' } } filter { } output { }" }
+    let(:agent_args) { {
+      "config.reload.automatic" => false,
+      "config.reload.interval" => 0.01,
+      "config.string" => pipeline_config
+    } }
+    let(:pipeline_id) { "main" }
+
+    context "environment variable templating" do
+      before :each do
+        @foo_content = ENV["FOO"]
+        ENV["FOO"] = "foo"
+      end
+
+      after :each do
+        ENV["FOO"] = @foo_content
+      end
+
+      it "doesn't upgrade the state" do
+        allow(subject).to receive(:fetch_config).and_return(pipeline_config)
+        subject.register_pipeline(pipeline_id, pipeline_settings)
+        expect(subject.pipelines[pipeline_id].inputs.first.message).to eq("foo-bar")
+      end
+    end
+  end
+
+  describe "#upgrade_pipeline" do
+    let(:pipeline_id) { "main" }
+    let(:pipeline_config) { "input { } filter { } output { }" }
+    let(:pipeline_args) { {
+      "config.string" => pipeline_config,
+      "pipeline.workers" => 4
+    } }
+    let(:new_pipeline_config) { "input { generator {} } output { }" }
+
+    before(:each) do
+      subject.register_pipeline(pipeline_id, pipeline_settings)
+    end
+
+    context "when the upgrade fails" do
+      before :each do
+        allow(subject).to receive(:fetch_config).and_return(new_pipeline_config)
+        allow(subject).to receive(:create_pipeline).and_return(nil)
+        allow(subject).to receive(:stop_pipeline)
+      end
+
+      it "leaves the state untouched" do
+        subject.send(:reload_state!)
+        expect(subject.pipelines[pipeline_id].config_str).to eq(pipeline_config)
+      end
+
+      context "and current state is empty" do
+        it "should not start a pipeline" do
+          expect(subject).to_not receive(:start_pipeline)
+          subject.send(:reload_state!)
+        end
+      end
+    end
+
+    context "when the upgrade succeeds" do
+      let(:new_config) { "input { generator { count => 1 } } output { }" }
+      before :each do
+        allow(subject).to receive(:fetch_config).and_return(new_config)
+        allow(subject).to receive(:stop_pipeline)
+      end
+      it "updates the state" do
+        subject.send(:reload_state!)
+        expect(subject.pipelines[pipeline_id].config_str).to eq(new_config)
+      end
+      it "starts the pipeline" do
+        expect(subject).to receive(:stop_pipeline)
+        expect(subject).to receive(:start_pipeline)
+        subject.send(:reload_state!)
+      end
+    end
+  end
+
+  describe "#fetch_config" do
+    let(:cli_config) { "filter { drop { } } " }
+    let(:agent_args) { { "config.string" => cli_config, "path.config" => config_file } }
+
+    it "should join the config string and config path content" do
+      fetched_config = subject.send(:fetch_config, agent_settings)
+      expect(fetched_config.strip).to eq(cli_config + IO.read(config_file).strip)
+    end
+  end
+
+  context "#started_at" do
+    it "return the start time when the agent is started" do
+      expect(described_class::STARTED_AT).to be_kind_of(Time)
+    end
+  end
+
+  context "#uptime" do
+    it "return the number of milliseconds since start time" do
+      expect(subject.uptime).to be >= 0
+    end
+  end
+
+  context "metrics after config reloading" do
+    let(:dummy_output) { DummyOutput.new }
+    let(:config) { "input { generator { } } output { dummyoutput { } }" }
+    let(:new_config_generator_counter) { 50 }
+    let(:new_config) { "input { generator { count => #{new_config_generator_counter} } } output { dummyoutput {} }" }
+    let(:config_path) do
+      f = Stud::Temporary.file
+      f.write(config)
+      f.close
+      f.path
+    end
+    let(:interval) { 0.2 }
+    let(:pipeline_args) do
+      {
+        "pipeline.workers" => 4,
+        "path.config" => config_path
+      }
+    end
+
+    let(:agent_args) do
+      super.merge({ "config.reload.automatic" => true,
+                    "config.reload.interval" => interval,
+                    "metric.collect" => true })
+    end 
+
+    before :each do
+      allow(DummyOutput).to receive(:new).at_least(:once).with(anything).and_return(dummy_output)
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+
+      @t = Thread.new do
+        subject.register_pipeline("main",  pipeline_settings)
+        subject.execute
+      end
+
+      sleep(2)
+    end
+
+    after :each do
+      subject.shutdown
+      Stud.stop!(@t)
+      @t.join
+    end
+
+    it "resets the metric collector" do
+      # We know that the store has more events coming in.
+      sleep(0.01) while dummy_output.events.size < new_config_generator_counter
+      snapshot = LogStash::Instrument::Collector.instance.snapshot_metric
+      expect(snapshot.metric_store.get_with_path("/stats/events")[:stats][:events][:in].value).to be > new_config_generator_counter
+
+      # update the configuration and give some time to logstash to pick it up and do the work
+      IO.write(config_path, new_config)
+
+      sleep(interval * 3) # Give time to reload the config
+      
+      # Since there is multiple threads involved with the configuration reload, 
+      # It can take some time to the stats be visible in the store but it will
+      # be eventually consistent.
+      sleep(0.01) while dummy_output.events.size < new_config_generator_counter
+
+      value = LogStash::Instrument::Collector.instance.snapshot_metric.metric_store.get_with_path("/stats/events")[:stats][:events][:in].value
+      expect(value).to eq(new_config_generator_counter)
+    end
+  end
+end
diff --git a/spec/core/config_spec.rb b/logstash-core/spec/logstash/config/config_ast_spec.rb
similarity index 78%
rename from spec/core/config_spec.rb
rename to logstash-core/spec/logstash/config/config_ast_spec.rb
index 917e0575916..fcf989fcd0c 100644
--- a/spec/core/config_spec.rb
+++ b/logstash-core/spec/logstash/config/config_ast_spec.rb
@@ -143,4 +143,39 @@
       end
     end
   end
+
+  context "when creating two instances of the same configuration" do
+
+    let(:config_string) {
+      "input { generator { } }
+       filter {
+         if [type] == 'test' { filter1 { } }
+       }
+       output {
+         output1 { }
+       }"
+    }
+
+    let(:pipeline_klass) do
+      Class.new do
+        def initialize(config)
+          grammar = LogStashConfigParser.new
+          @config = grammar.parse(config)
+          @code = @config.compile
+          eval(@code)
+        end
+        def plugin(*args);end
+      end
+    end
+
+    describe "generated conditional functionals" do
+      it "should be created per instance" do
+        instance_1 = pipeline_klass.new(config_string)
+        instance_2 = pipeline_klass.new(config_string)
+        generated_method_1 = instance_1.instance_variable_get("@generated_objects")[:cond_func_1]
+        generated_method_2 = instance_2.instance_variable_get("@generated_objects")[:cond_func_1]
+        expect(generated_method_1).to_not be(generated_method_2)
+      end
+    end
+  end
 end
diff --git a/spec/core/config_cpu_core_strategy_spec.rb b/logstash-core/spec/logstash/config/cpu_core_strategy_spec.rb
similarity index 100%
rename from spec/core/config_cpu_core_strategy_spec.rb
rename to logstash-core/spec/logstash/config/cpu_core_strategy_spec.rb
diff --git a/spec/core/config_defaults_spec.rb b/logstash-core/spec/logstash/config/defaults_spec.rb
similarity index 100%
rename from spec/core/config_defaults_spec.rb
rename to logstash-core/spec/logstash/config/defaults_spec.rb
diff --git a/logstash-core/spec/logstash/config/loader_spec.rb b/logstash-core/spec/logstash/config/loader_spec.rb
new file mode 100644
index 00000000000..b51272ee13a
--- /dev/null
+++ b/logstash-core/spec/logstash/config/loader_spec.rb
@@ -0,0 +1,36 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/config/loader"
+
+describe LogStash::Config::Loader do
+  subject { described_class.new(Cabin::Channel.get) }
+  context "when local" do
+    before { expect(subject).to receive(:local_config).with(path) }
+
+    context "unix" do
+      let(:path) { './test.conf' }
+      it 'works with relative path' do
+        subject.load_config(path)
+      end
+    end
+
+    context "windows" do
+      let(:path) { '.\test.conf' }
+      it 'work with relative windows path' do
+        subject.load_config(path)
+      end
+    end
+  end
+
+  context "when remote" do
+    context 'supported scheme' do
+      let(:path) { "http://test.local/superconfig.conf" }
+      let(:dummy_config) { 'input {}' }
+
+      before { expect(Net::HTTP).to receive(:get) { dummy_config } }
+      it 'works with http' do
+        expect(subject.load_config(path)).to eq("#{dummy_config}\n")
+      end
+    end
+  end
+end
diff --git a/spec/core/config_mixin_spec.rb b/logstash-core/spec/logstash/config/mixin_spec.rb
similarity index 57%
rename from spec/core/config_mixin_spec.rb
rename to logstash-core/spec/logstash/config/mixin_spec.rb
index 7c73b805d63..ca20d3649eb 100644
--- a/spec/core/config_mixin_spec.rb
+++ b/logstash-core/spec/logstash/config/mixin_spec.rb
@@ -96,6 +96,10 @@
       clone = subject.class.new(subject.params)
       expect(clone.password.value).to(be == secret)
     end
+
+    it "should obfuscate original_params" do
+      expect(subject.original_params['password']).to(be_a(LogStash::Util::Password))
+    end
   end
 
   describe "obsolete settings" do
@@ -151,4 +155,110 @@
       expect(subject.params).to include("password")
     end
   end
+
+  context "environment variable evaluation" do
+    let(:plugin_class) do
+      Class.new(LogStash::Filters::Base)  do
+        config_name "one_plugin"
+        config :oneString, :validate => :string, :required => false
+        config :oneBoolean, :validate => :boolean, :required => false
+        config :oneNumber, :validate => :number, :required => false
+        config :oneArray, :validate => :array, :required => false
+        config :oneHash, :validate => :hash, :required => false
+
+        def initialize(params)
+          super(params)
+        end
+      end
+    end
+
+    context "when an environment variable is not set" do
+      context "and no default is given" do
+        before do
+          # Canary. Just in case somehow this is set.
+          expect(ENV["NoSuchVariable"]).to be_nil
+        end
+
+        it "should raise a configuration error" do
+          expect do
+            plugin_class.new("oneString" => "${NoSuchVariable}")
+          end.to raise_error(LogStash::ConfigurationError)
+        end
+      end
+
+      context "and a default is given" do
+        subject do
+          plugin_class.new(
+            "oneString" => "${notExistingVar:foo}",
+            "oneBoolean" => "${notExistingVar:true}",
+            "oneArray" => [ "first array value", "${notExistingVar:foo}", "${notExistingVar:}", "${notExistingVar: }", "${notExistingVar:foo bar}" ],
+            "oneHash" => { "key" => "${notExistingVar:foo}" }
+          )
+        end
+
+        it "should use the default" do
+          expect(subject.oneString).to(be == "foo")
+          expect(subject.oneBoolean).to be_truthy
+          expect(subject.oneArray).to(be == ["first array value", "foo", "", " ", "foo bar"])
+          expect(subject.oneHash).to(be == { "key" => "foo" })
+        end
+      end
+    end
+
+    context "when an environment variable is set" do
+      before do
+        ENV["FunString"] = "fancy"
+        ENV["FunBool"] = "true"
+      end
+
+      after do
+        ENV.delete("FunString")
+        ENV.delete("FunBool")
+      end
+
+      subject do
+        plugin_class.new(
+          "oneString" => "${FunString:foo}",
+          "oneBoolean" => "${FunBool:false}",
+          "oneArray" => [ "first array value", "${FunString:foo}" ],
+          "oneHash" => { "key1" => "${FunString:foo}", "key2" => "${FunString} is ${FunBool}", "key3" => "${FunBool:false} or ${funbool:false}" }
+        )
+      end
+
+      it "should use the value in the variable" do
+        expect(subject.oneString).to(be == "fancy")
+        expect(subject.oneBoolean).to(be_truthy)
+        expect(subject.oneArray).to(be == [ "first array value", "fancy" ])
+        expect(subject.oneHash).to(be == { "key1" => "fancy", "key2" => "fancy is true", "key3" => "true or false" })
+      end
+    end
+
+    context "should support $ in values" do
+      before do
+        ENV["bar"] = "foo"
+        ENV["f$$"] = "bar"
+      end
+
+      after do
+        ENV.delete("bar")
+        ENV.delete("f$$")
+      end
+
+      subject do
+        plugin_class.new(
+          "oneString" => "${f$$:val}",
+          "oneArray" => ["foo$bar", "${bar:my$val}"]
+          # "dollar_in_env" => "${f$$:final}"
+        )
+      end
+
+      it "should support $ in values" do
+        expect(subject.oneArray).to(be == ["foo$bar", "foo"])
+      end
+
+      it "should not support $ in environment variable name" do
+        expect(subject.oneString).to(be == "${f$$:val}")
+      end
+    end
+  end
 end
diff --git a/spec/core/environment_spec.rb b/logstash-core/spec/logstash/environment_spec.rb
similarity index 100%
rename from spec/core/environment_spec.rb
rename to logstash-core/spec/logstash/environment_spec.rb
diff --git a/logstash-core/spec/logstash/filter_delegator_spec.rb b/logstash-core/spec/logstash/filter_delegator_spec.rb
new file mode 100644
index 00000000000..595fbfedc36
--- /dev/null
+++ b/logstash-core/spec/logstash/filter_delegator_spec.rb
@@ -0,0 +1,143 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/filter_delegator"
+require "logstash/instrument/null_metric"
+require "logstash/event"
+
+describe LogStash::FilterDelegator do
+  let(:logger) { double(:logger) }
+  let(:filter_id) { "my-filter" }
+  let(:config) do
+    { "host" => "127.0.0.1", "id" => filter_id }
+  end
+  let(:metric) { LogStash::Instrument::NullMetric.new }
+  let(:events) { [LogStash::Event.new, LogStash::Event.new] }
+
+  before :each do
+    allow(metric).to receive(:namespace).with(anything).and_return(metric)
+  end
+
+  let(:plugin_klass) do
+    Class.new(LogStash::Filters::Base) do
+      config_name "super_plugin"
+      config :host, :validate => :string
+      def register; end
+    end
+  end
+
+  subject { described_class.new(logger, plugin_klass, metric, config) }
+
+  it "create a plugin with the passed options" do
+    expect(plugin_klass).to receive(:new).with(config).and_return(plugin_klass.new(config))
+    described_class.new(logger, plugin_klass, metric, config)
+  end
+
+  context "when the plugin support flush" do
+    let(:plugin_klass) do
+      Class.new(LogStash::Filters::Base) do
+        config_name "super_plugin"
+        config :host, :validate => :string
+        def register; end
+        def flush(options = {}); @events ; end
+        def filter(event)
+          @events ||= []
+          @events << event
+          event.cancel
+        end
+      end
+    end
+
+    it "defines a flush method" do
+      expect(subject.respond_to?(:flush)).to be_truthy
+    end
+
+    context "when the flush return events" do
+      it "increments the out" do
+        subject.multi_filter([LogStash::Event.new])
+        expect(metric).to receive(:increment).with(:out, 1)
+        subject.flush({})
+      end
+    end
+
+    context "when the flush doesn't return anything" do
+      it "doesnt increment the out" do
+        expect(metric).not_to receive(:increment)
+        subject.flush({})
+      end
+    end
+
+    context "when the filter buffer events" do
+      it "doesn't increment out" do
+        expect(metric).to receive(:increment).with(:in, events.size)
+        expect(metric).not_to receive(:increment)
+
+        subject.multi_filter(events)
+      end
+    end
+
+    context "when the fitler create more events" do
+      let(:plugin_klass) do
+        Class.new(LogStash::Filters::Base) do
+          config_name "super_plugin"
+          config :host, :validate => :string
+          def register; end
+          def flush(options = {}); @events ; end
+
+          # naive split filter implementation
+          def filter(event)
+            event.cancel
+            2.times { yield LogStash::Event.new }
+          end
+        end
+      end
+
+      it "increments the in/out of the metric" do
+        expect(metric).to receive(:increment).with(:in, events.size)
+        expect(metric).to receive(:increment).with(:out, events.size * 2)
+
+        subject.multi_filter(events)
+      end
+    end
+  end
+
+  context "when the plugin doesnt support flush" do
+    let(:plugin_klass) do
+      Class.new(LogStash::Filters::Base) do
+        config_name "super_plugin"
+        config :host, :validate => :string
+        def register; end
+        def filter(event)
+          event
+        end
+      end
+    end
+
+    it "doesnt define a flush method" do
+      expect(subject.respond_to?(:flush)).to be_falsey
+    end
+
+    it "increments the in/out of the metric" do
+      expect(metric).to receive(:increment).with(:in, events.size)
+      expect(metric).to receive(:increment).with(:out, events.size)
+
+      subject.multi_filter(events)
+    end
+  end
+
+  context "#config_name" do
+    it "proxy the config_name to the class method" do
+      expect(subject.config_name).to eq("super_plugin")
+    end
+  end
+
+  context "delegate methods to the original plugin" do
+    # I am not testing the behavior of these methods
+    # this is done in the plugin tests. I just want to make sure
+    # the proxy delegates the methods.
+    LogStash::FilterDelegator::DELEGATED_METHODS.each do |method|
+      it "delegate method: `#{method}` to the filter" do
+        expect(subject.respond_to?(method))
+      end
+    end
+  end
+end
diff --git a/spec/filters/base_spec.rb b/logstash-core/spec/logstash/filters/base_spec.rb
similarity index 86%
rename from spec/filters/base_spec.rb
rename to logstash-core/spec/logstash/filters/base_spec.rb
index 177c44dcb8c..26c5d6c5438 100644
--- a/spec/filters/base_spec.rb
+++ b/logstash-core/spec/logstash/filters/base_spec.rb
@@ -62,7 +62,7 @@ def filter(event)
     CONFIG
 
     sample "example" do
-      insist { subject["new_field"] } == ["new_value", "new_value_2"]
+      insist { subject.get("new_field") } == ["new_value", "new_value_2"]
     end
   end
 
@@ -76,7 +76,7 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
   end
 
@@ -90,11 +90,11 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "test"]
     end
   end
 
@@ -108,19 +108,19 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1"]) do
-      insist { subject["tags"] } == ["t1", "test"]
+      insist { subject.get("tags") } == ["t1", "test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1", "t2", "t3", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "t3", "test"]
     end
   end
 
@@ -134,27 +134,27 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop", "tags" => ["t4"]) do
-      insist { subject["tags"] } == ["t4"]
+      insist { subject.get("tags") } == ["t4"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"t2\", \"t3\"]}")) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"t2\"]}")) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
   end
 
@@ -168,13 +168,13 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop", "tags" => ["t1", "goaway", "t3"], "blackhole" => "goaway") do
-      insist { subject["tags"] } == ["t1", "t3"]
+      insist { subject.get("tags") } == ["t1", "t3"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"goaway\", \"t3\"], \"blackhole\":\"goaway\"}")) do
-      insist { subject["tags"] } == ["t1", "t3"]
+      insist { subject.get("tags") } == ["t1", "t3"]
     end
   end
 
@@ -230,7 +230,7 @@ def filter(event)
 
     sample("type" => "noop", "t1" => ["t2", "t3"]) do
       insist { subject }.include?("t1")
-      insist { subject["[t1][0]"] } == "t3"
+      insist { subject.get("[t1][0]") } == "t3"
     end
   end
 
diff --git a/spec/inputs/base_spec.rb b/logstash-core/spec/logstash/inputs/base_spec.rb
similarity index 84%
rename from spec/inputs/base_spec.rb
rename to logstash-core/spec/logstash/inputs/base_spec.rb
index d87f07b49f6..a3f01fa89e1 100644
--- a/spec/inputs/base_spec.rb
+++ b/logstash-core/spec/logstash/inputs/base_spec.rb
@@ -15,50 +15,50 @@ def register; end
     input = LogStash::Inputs::NOOP.new("tags" => "value")
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value"])
+    expect(evt.get("tags")).to eq(["value"])
   end
 
   it "should add multiple tag" do
     input = LogStash::Inputs::NOOP.new("tags" => ["value1","value2"])
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value1","value2"])
+    expect(evt.get("tags")).to eq(["value1","value2"])
   end
 
   it "should allow duplicates  tag" do
     input = LogStash::Inputs::NOOP.new("tags" => ["value","value"])
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value","value"])
+    expect(evt.get("tags")).to eq(["value","value"])
   end
 
   it "should add tag with sprintf" do
     input = LogStash::Inputs::NOOP.new("tags" => "%{type}")
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["noop"])
+    expect(evt.get("tags")).to eq(["noop"])
   end
 
   it "should add single field" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"field" => "value"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["field"]).to eq("value")
+    expect(evt.get("field")).to eq("value")
   end
 
   it "should add single field with sprintf" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"%{type}" => "%{type}"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["noop"]).to eq("noop")
+    expect(evt.get("noop")).to eq("noop")
   end
 
   it "should add multiple field" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"field" => ["value1", "value2"], "field2" => "value"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["field"]).to eq(["value1","value2"])
-    expect(evt["field2"]).to eq("value")
+    expect(evt.get("field")).to eq(["value1","value2"])
+    expect(evt.get("field2")).to eq("value")
   end
 end
 
diff --git a/logstash-core/spec/logstash/inputs/metrics_spec.rb b/logstash-core/spec/logstash/inputs/metrics_spec.rb
new file mode 100644
index 00000000000..97a89facda3
--- /dev/null
+++ b/logstash-core/spec/logstash/inputs/metrics_spec.rb
@@ -0,0 +1,52 @@
+# encoding: utf-8
+require "logstash/inputs/metrics"
+require "spec_helper"
+
+describe LogStash::Inputs::Metrics do
+  before :each do
+    LogStash::Instrument::Collector.instance.clear
+  end
+
+  let(:queue) { [] }
+
+  describe "#run" do
+    it "should register itself to the collector observer" do
+      expect(LogStash::Instrument::Collector.instance).to receive(:add_observer).with(subject)
+      t = Thread.new { subject.run(queue) }
+      sleep(0.1) # give a bit of time to the thread to start
+      subject.stop
+    end
+  end
+
+  describe "#update" do
+    let(:namespaces)  { [:root, :base] }
+    let(:key)        { :foo }
+    let(:metric_store) { LogStash::Instrument::MetricStore.new }
+
+    it "should fill up the queue with received events" do
+      Thread.new { subject.run(queue) }
+      sleep(0.1)
+      subject.stop
+
+      metric_store.fetch_or_store(namespaces, key, LogStash::Instrument::MetricType::Counter.new(namespaces, key))
+      subject.update(LogStash::Instrument::Snapshot.new(metric_store))
+      expect(queue.count).to eq(1)
+    end
+  end
+
+  describe "#stop" do
+    it "should remove itself from the the collector observer" do
+      expect(LogStash::Instrument::Collector.instance).to receive(:delete_observer).with(subject)
+      t = Thread.new { subject.run(queue) }
+      sleep(0.1) # give a bit of time to the thread to start
+      subject.stop
+    end
+
+    it "should unblock the input" do
+      t = Thread.new { subject.run(queue) }
+      sleep(0.1) # give a bit of time to the thread to start
+      subject.do_stop
+      wait_for { t.status }.to be_falsey
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/collector_spec.rb b/logstash-core/spec/logstash/instrument/collector_spec.rb
new file mode 100644
index 00000000000..b96be4a5ede
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/collector_spec.rb
@@ -0,0 +1,49 @@
+# encoding: utf-8
+require "logstash/instrument/collector"
+require "spec_helper"
+
+describe LogStash::Instrument::Collector do
+  subject { LogStash::Instrument::Collector.instance }
+  describe "#push" do
+    let(:namespaces_path) { [:root, :pipelines, :pipelines01] }
+    let(:key) { :my_key }
+
+    context "when the `MetricType` exist" do
+      it "store the metric of type `counter`" do
+        subject.push(namespaces_path, key, :counter, :increment)
+      end
+    end
+
+    context "when the `MetricType` doesn't exist" do
+      let(:wrong_type) { :donotexist }
+
+      it "logs an error but dont crash" do
+        expect(subject.logger).to receive(:error)
+          .with("Collector: Cannot create concrete class for this metric type",
+        hash_including({ :type => wrong_type, :namespaces_path => namespaces_path }))
+
+          subject.push(namespaces_path, key, wrong_type, :increment)
+      end
+    end
+
+    context "when there is a conflict with the metric key" do
+      let(:conflicting_namespaces) { [namespaces_path, key].flatten }
+
+      it "logs an error but dont crash" do
+        subject.push(namespaces_path, key, :counter, :increment)
+
+        expect(subject.logger).to receive(:error)
+          .with("Collector: Cannot record metric",
+          hash_including({ :exception => instance_of(LogStash::Instrument::MetricStore::NamespacesExpectedError) }))
+
+          subject.push(conflicting_namespaces, :random_key, :counter, :increment)
+      end
+    end
+  end
+
+  describe "#snapshot_metric" do
+    it "return a `LogStash::Instrument::MetricStore`" do
+      expect(subject.snapshot_metric).to be_kind_of(LogStash::Instrument::Snapshot)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/metric_spec.rb b/logstash-core/spec/logstash/instrument/metric_spec.rb
new file mode 100644
index 00000000000..0a8a65d4338
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/metric_spec.rb
@@ -0,0 +1,110 @@
+# encoding: utf-8
+require "logstash/instrument/metric"
+require "logstash/instrument/collector"
+require_relative "../../support/matchers"
+require "spec_helper"
+
+describe LogStash::Instrument::Metric do
+  let(:collector) { [] }
+  let(:namespace) { :root }
+
+  subject { LogStash::Instrument::Metric.new(collector) }
+
+  context "#increment" do
+    it "a counter by 1" do
+      metric = subject.increment(:root, :error_rate)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 1)
+    end
+
+    it "a counter by a provided value" do
+      metric = subject.increment(:root, :error_rate, 20)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 20)
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.increment(:root, "", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.increment(:root, nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  context "#decrement" do
+    it "a counter by 1" do
+      metric = subject.decrement(:root, :error_rate)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 1)
+    end
+
+    it "a counter by a provided value" do
+      metric = subject.decrement(:root, :error_rate, 20)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 20)
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.decrement(:root, "", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.decrement(:root, nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  context "#gauge" do
+    it "set the value of a key" do
+      metric = subject.gauge(:root, :size_queue, 20)
+      expect(collector).to be_a_metric_event([:root, :size_queue], :gauge, :set, 20)
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.gauge(:root, "", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.gauge(:root, nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  context "#time" do
+    let(:sleep_time) { 2 }
+    let(:sleep_time_ms) { sleep_time * 1_000_000 }
+
+    it "records the duration" do
+      subject.time(:root, :duration_ms) { sleep(sleep_time) }
+
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5000)
+      expect(collector[0]).to match(:root)
+      expect(collector[1]).to be(:duration_ms)
+      expect(collector[2]).to be(:mean)
+    end
+
+    it "returns the value of the executed block" do
+      expect(subject.time(:root, :testing) { "hello" }).to eq("hello")
+    end
+
+    it "return a TimedExecution" do
+      execution = subject.time(:root, :duration_ms)
+      sleep(sleep_time)
+      execution.stop
+
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 0.1)
+      expect(collector[0]).to match(:root)
+      expect(collector[1]).to be(:duration_ms)
+      expect(collector[2]).to be(:mean)
+    end
+  end
+
+  context "#namespace" do
+    let(:sub_key) { :my_sub_key }
+
+    it "creates a new metric object and append the `sub_key` to the `base_key`" do
+      expect(subject.namespace(sub_key).namespace_name).to eq([sub_key])
+    end
+
+    it "uses the same collector as the creator class" do
+      child = subject.namespace(sub_key)
+      metric = child.increment(:error_rate)
+      expect(collector).to be_a_metric_event([sub_key, :error_rate], :counter, :increment, 1)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/metric_store_spec.rb b/logstash-core/spec/logstash/instrument/metric_store_spec.rb
new file mode 100644
index 00000000000..4371977355b
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/metric_store_spec.rb
@@ -0,0 +1,163 @@
+# encoding: utf-8
+require "logstash/instrument/metric_store"
+require "logstash/instrument/metric_type/base"
+
+describe LogStash::Instrument::MetricStore do
+  let(:namespaces) { [ :root, :pipelines, :pipeline_01 ] }
+  let(:key) { :events_in }
+  let(:counter) { LogStash::Instrument::MetricType::Counter.new(namespaces, key) }
+
+  context "when the metric object doesn't exist" do
+    it "store the object" do
+      expect(subject.fetch_or_store(namespaces, key, counter)).to eq(counter)
+    end
+
+    it "support a block as argument" do
+      expect(subject.fetch_or_store(namespaces, key) { counter }).to eq(counter)
+    end
+  end
+
+  context "when the metric object exist in the namespace"  do
+    let(:new_counter) { LogStash::Instrument::MetricType::Counter.new(namespaces, key) }
+
+    it "return the object" do
+      subject.fetch_or_store(namespaces, key, counter)
+      expect(subject.fetch_or_store(namespaces, key, new_counter)).to eq(counter)
+    end
+  end
+
+  context "when the namespace end node isn't a map" do
+    let(:conflicting_namespaces) { [:root, :pipelines, :pipeline_01, :events_in] }
+
+    it "raise an exception" do
+      subject.fetch_or_store(namespaces, key, counter)
+      expect { subject.fetch_or_store(conflicting_namespaces, :new_key, counter) }.to raise_error(LogStash::Instrument::MetricStore::NamespacesExpectedError)
+    end
+  end
+
+  context "retrieving events" do
+    let(:metric_events) {
+      [
+        [[:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch"], :event_in, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_in, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_out, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline02], :processed_events_out, :increment],
+      ]
+    }
+
+    before :each do
+      # Lets add a few metrics in the store before trying to find them
+      metric_events.each do |namespaces, metric_key, action|
+        metric = subject.fetch_or_store(namespaces, metric_key, LogStash::Instrument::MetricType::Counter.new(namespaces, metric_key))
+        metric.execute(action)
+      end
+    end
+
+    describe "#get" do
+      context "when the path exist" do
+        it "retrieves end of of a branch" do
+          metrics = subject.get(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch")
+          expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => anything)))))))
+        end
+
+        it "retrieves branch" do
+          metrics = subject.get(:node, :sashimi, :pipelines, :pipeline01)
+          expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything)))))
+        end
+
+        it "allow to retrieve a specific metrics" do
+          metrics = subject.get(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch", :event_in)
+          expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => a_hash_including(:event_in => be_kind_of(LogStash::Instrument::MetricType::Base)))))))))
+        end
+
+        context "with filtered keys" do
+          it "allows to retrieve multiple keys on the same level" do
+            metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01,pipeline02")
+            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
+          end
+
+          it "supports space in the keys" do
+            metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01, pipeline02 ")
+            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
+          end
+
+          it "retrieves only the requested keys" do
+            metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01,pipeline02", :processed_events_in)
+            expect(metrics[:node][:sashimi][:pipelines].keys).to include(:pipeline01, :pipeline02)
+          end
+        end
+
+        context "when the path doesnt exist" do
+          it "raise an exception" do
+            expect { subject.get(:node, :sashimi, :dontexist) }.to raise_error(LogStash::Instrument::MetricStore::MetricNotFound, /dontexist/)
+          end
+        end
+      end
+
+      describe "#get_with_path" do
+        context "when the path exist" do
+          it "removes the first `/`" do
+            metrics = subject.get_with_path("/node/sashimi/")
+            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => anything)))
+          end
+
+          it "retrieves end of of a branch" do
+            metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01/plugins/logstash-output-elasticsearch")
+            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => anything)))))))
+          end
+
+          it "retrieves branch" do
+            metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01")
+            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything)))))
+          end
+
+          it "allow to retrieve a specific metrics" do
+            metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01/plugins/logstash-output-elasticsearch/event_in")
+            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => a_hash_including(:event_in => be_kind_of(LogStash::Instrument::MetricType::Base)))))))))
+          end
+
+          context "with filtered keys" do
+            it "allows to retrieve multiple keys on the same level" do
+              metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01,pipeline02/plugins/logstash-output-elasticsearch/event_in")
+              expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
+            end
+
+            it "supports space in the keys" do
+              metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01, pipeline02 /plugins/logstash-output-elasticsearch/event_in")
+              expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
+            end
+
+            it "retrieves only the requested keys" do
+              metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01,pipeline02", :processed_events_in)
+              expect(metrics[:node][:sashimi][:pipelines].keys).to include(:pipeline01, :pipeline02)
+            end
+          end
+        end
+      end
+
+      context "when the path doesnt exist" do
+        it "raise an exception" do
+          expect { subject.get_with_path("node/sashimi/dontexist, pipeline02 /plugins/logstash-output-elasticsearch/event_in") }.to raise_error(LogStash::Instrument::MetricStore::MetricNotFound, /dontexist/)
+        end
+      end
+    end
+
+    describe "#each" do
+      it "retrieves all the metric" do
+        expect(subject.each.size).to eq(metric_events.size)
+      end
+
+      it "returns metric types" do
+        metrics = []
+        subject.each { |i| metrics << i }
+        expect(metrics.size).to eq(metric_events.size)
+      end
+
+      it "retrieves all the metrics from a specific branch" do
+        metrics = []
+        subject.each("node/sashimi/pipelines/pipeline01") { |i| metrics << i }
+        expect(metrics.size).to eq(3)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
new file mode 100644
index 00000000000..b51aebc792d
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "logstash/instrument/metric_type/counter"
+require "spec_helper"
+
+describe LogStash::Instrument::MetricType::Counter do
+  let(:namespaces) { [:root, :pipelines, :pipeline_01] }
+  let(:key) { :mykey }
+
+  subject { LogStash::Instrument::MetricType::Counter.new(namespaces, key) }
+
+  describe "#increment" do
+    it "increment the counter" do
+      expect{ subject.increment }.to change { subject.value }.by(1)
+    end
+  end
+
+  describe "#decrement" do
+    it "decrement the counter" do
+      expect{ subject.decrement }.to change { subject.value }.by(-1)
+    end
+  end
+
+  context "When serializing to JSON" do
+    it "serializes the value" do
+      expect(LogStash::Json.dump(subject)).to eq("0")
+    end
+  end
+
+  context "When creating a hash " do
+    it "creates the hash from all the values" do
+      metric_hash = {
+        "key" => key,
+        "namespaces" => namespaces,
+        "value" => 0,
+        "type" => "counter"
+      }
+      expect(subject.to_hash).to match(metric_hash)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
new file mode 100644
index 00000000000..0481f6d283b
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "logstash/instrument/metric_type/gauge"
+require "logstash/json"
+require "spec_helper"
+
+describe LogStash::Instrument::MetricType::Gauge do
+  let(:namespaces) { [:root, :pipelines, :pipeline_01] }
+  let(:key) { :mykey }
+  let(:value) { "hello" }
+
+  subject { described_class.new(namespaces, key) }
+
+  before :each do
+    subject.execute(:set, value)
+  end
+
+  describe "#execute" do
+    it "set the value of the gauge" do
+      expect(subject.value).to eq(value)
+    end
+  end
+
+  context "When serializing to JSON" do
+    it "serializes the value" do
+      expect(LogStash::Json.dump(subject)).to eq("\"#{value}\"")
+    end
+  end
+
+  context "When creating a hash " do
+    it "creates the hash from all the values" do
+      metric_hash = {
+        "key" => key,
+        "namespaces" => namespaces,
+        "value" => value,
+        "type" => "gauge"
+      }
+      expect(subject.to_hash).to match(metric_hash)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb b/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
new file mode 100644
index 00000000000..6ba84168df9
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require "logstash/instrument/namespaced_metric"
+require "logstash/instrument/metric"
+require_relative "../../support/matchers"
+require "spec_helper"
+
+describe LogStash::Instrument::NamespacedMetric do
+  let(:namespace) { :stats }
+  let(:collector) { [] }
+  let(:metric) { LogStash::Instrument::Metric.new(collector) }
+
+  subject { described_class.new(metric, namespace) }
+
+  it "defines the same interface as `Metric`" do
+    expect(described_class).to implement_interface_of(LogStash::Instrument::Metric)
+  end
+
+  it "returns a TimedException when we call without a block" do
+    expect(subject.time(:duration_ms)).to be_kind_of(LogStash::Instrument::Metric::TimedExecution)
+  end
+
+  it "returns the value of the block" do
+    expect(subject.time(:duration_ms) { "hello" }).to eq("hello")
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/null_metric_spec.rb b/logstash-core/spec/logstash/instrument/null_metric_spec.rb
new file mode 100644
index 00000000000..ec55d341be4
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/null_metric_spec.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require "logstash/instrument/null_metric"
+require "logstash/instrument/namespaced_metric"
+require_relative "../../support/matchers"
+
+describe LogStash::Instrument::NullMetric do
+  it "defines the same interface as `Metric`" do
+    expect(described_class).to implement_interface_of(LogStash::Instrument::NamespacedMetric)
+  end
+
+  describe "#time" do
+    it "returns the value of the block without recording any metrics" do
+      expect(subject.time(:execution_time) { "hello" }).to eq("hello")
+    end
+
+    it "return a TimedExecution" do
+      execution = subject.time(:do_something)
+      expect { execution.stop }.not_to raise_error
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb
new file mode 100644
index 00000000000..649e711d119
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb
@@ -0,0 +1,45 @@
+require 'spec_helper'
+require 'logstash/instrument/periodic_poller/jvm'
+
+describe LogStash::Instrument::PeriodicPoller::JVM do
+  let(:metric) { LogStash::Instrument::Metric.new }
+  let(:options) { {} }
+  subject(:jvm) { described_class.new(metric, options) }
+  
+  it "should initialize cleanly" do
+    expect { jvm }.not_to raise_error
+  end
+
+  describe "collections" do
+    subject(:collection) { jvm.collect }
+    
+    it "should run cleanly" do
+      expect { collection }.not_to raise_error
+    end
+
+    describe "metrics" do
+      let(:snapshot_store) { metric.collector.snapshot_metric.metric_store }
+      subject(:jvm_metrics) { snapshot_store.get_shallow(:jvm, :process) }
+
+      # Make looking up metric paths easy when given varargs of keys
+      # e.g. mval(:parent, :child)
+      def mval(*metric_path)
+        metric_path.reduce(jvm_metrics) {|acc,k| acc[k]}.value
+      end          
+
+      [
+        :max_file_descriptors,
+        :open_file_descriptors,
+        :peak_open_file_descriptors,
+        [:mem, :total_virtual_in_bytes],
+        [:cpu, :total_in_millis],
+        [:cpu, :percent]
+      ].each do |path|
+        path = Array(path)
+        it "should have a value for #{path} that is Numeric" do
+          expect(mval(*path)).to be_a(Numeric)
+        end
+      end
+    end
+  end
+end
diff --git a/spec/lib/logstash/java_integration_spec.rb b/logstash-core/spec/logstash/java_integration_spec.rb
similarity index 100%
rename from spec/lib/logstash/java_integration_spec.rb
rename to logstash-core/spec/logstash/java_integration_spec.rb
diff --git a/spec/util/json_spec.rb b/logstash-core/spec/logstash/json_spec.rb
similarity index 77%
rename from spec/util/json_spec.rb
rename to logstash-core/spec/logstash/json_spec.rb
index f0304f219c8..056325ac91b 100644
--- a/spec/util/json_spec.rb
+++ b/logstash-core/spec/logstash/json_spec.rb
@@ -18,6 +18,9 @@
   let(:multi) {
     [
       {:ruby => "foo bar baz", :json => "\"foo bar baz\""},
+      {:ruby => "foo   ", :json => "\"foo   \""},
+      {:ruby => " ", :json => "\" \""},
+      {:ruby => "   ", :json => "\"   \""},
       {:ruby => "1", :json => "\"1\""},
       {:ruby => {"a" => true}, :json => "{\"a\":true}"},
       {:ruby => {"a" => nil}, :json => "{\"a\":null}"},
@@ -56,6 +59,20 @@
         expect(LogStash::Json.dump(array)).to eql(json_array)
       end
 
+      context "pretty print" do
+
+        let(:hash) { { "foo" => "bar", :zoo => 2 } }
+
+        it "should serialize with pretty print" do
+          pprint_json = LogStash::Json.dump(hash, :pretty => true)
+          expect(pprint_json).to include("\n")
+        end
+
+        it "should by default do no pretty print" do
+          pprint_json = LogStash::Json.dump(hash)
+          expect(pprint_json).not_to include("\n")
+        end
+      end
     end
 
   else
@@ -93,4 +110,16 @@
   it "should raise Json::ParserError on invalid json" do
     expect{LogStash::Json.load("abc")}.to raise_error LogStash::Json::ParserError
   end
+
+  it "should return nil on empty string" do
+    o = LogStash::Json.load("")
+    expect(o).to be_nil
+  end
+
+  it "should return nil on blank string" do
+    o = LogStash::Json.load(" ")
+    expect(o).to be_nil
+    o = LogStash::Json.load("  ")
+    expect(o).to be_nil
+  end
 end
diff --git a/logstash-core/spec/logstash/output_delegator_spec.rb b/logstash-core/spec/logstash/output_delegator_spec.rb
new file mode 100644
index 00000000000..04e4b54a020
--- /dev/null
+++ b/logstash-core/spec/logstash/output_delegator_spec.rb
@@ -0,0 +1,150 @@
+# encoding: utf-8
+require "logstash/output_delegator"
+require 'spec_helper'
+
+describe LogStash::OutputDelegator do
+  let(:logger) { double("logger") }
+  let(:events) { 7.times.map { LogStash::Event.new }}
+  let(:default_worker_count) { 1 }
+
+  subject { described_class.new(logger, out_klass, default_worker_count, LogStash::Instrument::NullMetric.new) }
+
+  context "with a plain output plugin" do
+    let(:out_klass) { double("output klass") }
+    let(:out_inst) { double("output instance") }
+
+
+    before(:each) do
+      allow(out_klass).to receive(:new).with(any_args).and_return(out_inst)
+      allow(out_klass).to receive(:threadsafe?).and_return(false)
+      allow(out_klass).to receive(:workers_not_supported?).and_return(false)
+      allow(out_klass).to receive(:name).and_return("example")
+      allow(out_inst).to receive(:register)
+      allow(out_inst).to receive(:multi_receive)
+      allow(out_inst).to receive(:metric=).with(any_args)
+      allow(out_inst).to receive(:id).and_return("a-simple-plugin")
+      allow(out_inst).to receive(:plugin_unique_name).and_return("hello-123")
+      allow(logger).to receive(:debug).with(any_args)
+    end
+
+    it "should initialize cleanly" do
+      expect { subject }.not_to raise_error
+    end
+
+    context "after having received a batch of events" do
+      before do
+        subject.register
+        subject.multi_receive(events)
+      end
+
+      it "should pass the events through" do
+        expect(out_inst).to have_received(:multi_receive).with(events)
+      end
+
+      it "should increment the number of events received" do
+        expect(subject.events_received).to eql(events.length)
+      end
+    end
+
+
+    describe "closing" do
+      before do
+        subject.register
+      end
+
+      it "should register all workers on register" do
+        expect(out_inst).to have_received(:register)
+      end
+
+      it "should close all workers when closing" do
+        expect(out_inst).to receive(:do_close)
+        subject.do_close
+      end
+    end
+
+    describe "concurrency and worker support" do
+      before do
+        allow(out_inst).to receive(:id).and_return("a-simple-plugin")
+        allow(out_inst).to receive(:metric=).with(any_args)
+        allow(out_klass).to receive(:workers_not_supported?).and_return(false)
+      end
+
+      describe "non-threadsafe outputs that allow workers" do
+        let(:default_worker_count) { 3 }
+
+        before do
+          allow(out_klass).to receive(:threadsafe?).and_return(false)
+          subject.register
+        end
+
+        it "should instantiate multiple workers" do
+          expect(subject.workers.length).to eql(default_worker_count)
+        end
+
+        it "should send received events to the worker" do
+          expect(out_inst).to receive(:multi_receive).with(events)
+          subject.multi_receive(events)
+        end
+      end
+
+      describe "threadsafe outputs" do
+        before do
+          allow(out_klass).to receive(:threadsafe?).and_return(true)
+          subject.register
+        end
+
+        it "should return true when threadsafe? is invoked" do
+          expect(subject.threadsafe?).to eql(true)
+        end
+
+        it "should define a threadsafe_worker" do
+          expect(subject.send(:threadsafe_worker)).to eql(out_inst)
+        end
+
+        it "should utilize threadsafe_multi_receive" do
+          expect(subject.send(:threadsafe_worker)).to receive(:multi_receive).with(events)
+          subject.multi_receive(events)
+        end
+
+        it "should not utilize the worker queue" do
+          expect(subject.send(:worker_queue)).to be_nil
+        end
+
+        it "should send received events to the worker" do
+          expect(out_inst).to receive(:multi_receive).with(events)
+          subject.multi_receive(events)
+        end
+
+        it "should close all workers when closing" do
+          expect(out_inst).to receive(:do_close)
+          subject.do_close
+        end
+      end
+    end
+  end
+
+  # This may seem suspiciously similar to the class in outputs/base_spec
+  # but, in fact, we need a whole new class because using this even once
+  # will immutably modify the base class
+  class LogStash::Outputs::NOOPDelLegacyNoWorkers < ::LogStash::Outputs::Base
+    LEGACY_WORKERS_NOT_SUPPORTED_REASON = "legacy reason"
+
+    def register
+      workers_not_supported(LEGACY_WORKERS_NOT_SUPPORTED_REASON)
+    end
+  end
+
+  describe "legacy output workers_not_supported" do
+    let(:default_worker_count) { 2 }
+    let(:out_klass) { LogStash::Outputs::NOOPDelLegacyNoWorkers }
+
+    before(:each) do
+      allow(logger).to receive(:debug).with(any_args)
+    end
+
+    it "should only setup one worker" do
+      subject.register
+      expect(subject.worker_count).to eql(1)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/outputs/base_spec.rb b/logstash-core/spec/logstash/outputs/base_spec.rb
new file mode 100644
index 00000000000..44d49a60b99
--- /dev/null
+++ b/logstash-core/spec/logstash/outputs/base_spec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "spec_helper"
+
+# use a dummy NOOP output to test Outputs::Base
+class LogStash::Outputs::NOOP < LogStash::Outputs::Base
+  config_name "noop"
+  milestone 2
+
+  config :dummy_option, :validate => :string
+
+  def register; end
+
+  def receive(event)
+    return output?(event)
+  end
+end
+
+class LogStash::Outputs::NOOPLegacyNoWorkers < ::LogStash::Outputs::Base
+  LEGACY_WORKERS_NOT_SUPPORTED_REASON = "legacy reason"
+
+  def register
+    workers_not_supported(LEGACY_WORKERS_NOT_SUPPORTED_REASON)
+  end
+end
+
+describe "LogStash::Outputs::Base#new" do
+  it "should instantiate cleanly" do
+    params = { "dummy_option" => "potatoes", "codec" => "json", "workers" => 2 }
+    worker_params = params.dup; worker_params["workers"] = 1
+
+    expect do
+      LogStash::Outputs::NOOP.new(params.dup)
+    end.not_to raise_error
+  end
+
+  it "should move workers_not_supported declarations up to the class level" do
+    LogStash::Outputs::NOOPLegacyNoWorkers.new.register
+    expect(LogStash::Outputs::NOOPLegacyNoWorkers.workers_not_supported?).to eql(true)
+  end
+end
diff --git a/spec/logstash/patches_spec.rb b/logstash-core/spec/logstash/patches_spec.rb
similarity index 100%
rename from spec/logstash/patches_spec.rb
rename to logstash-core/spec/logstash/patches_spec.rb
diff --git a/logstash-core/spec/logstash/pipeline_reporter_spec.rb b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
new file mode 100644
index 00000000000..1aeef0e73c3
--- /dev/null
+++ b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
@@ -0,0 +1,89 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/pipeline"
+require "logstash/pipeline_reporter"
+
+class DummyOutput < LogStash::Outputs::Base
+  config_name "dummyoutput"
+  milestone 2
+
+  attr_reader :num_closes, :events
+
+  def initialize(params={})
+    super
+    @num_closes = 0
+    @events = []
+  end
+
+  def register
+  end
+
+  def receive(event)
+    @events << event
+  end
+
+  def close
+    @num_closes += 1
+  end
+end
+
+#TODO: Figure out how to add more tests that actually cover inflight events
+#This will require some janky multithreading stuff
+describe LogStash::PipelineReporter do
+  let(:generator_count) { 5 }
+  let(:config) do
+    "input { generator { count => #{generator_count} } } output { dummyoutput {} } "
+  end
+  let(:pipeline) { LogStash::Pipeline.new(config)}
+  let(:reporter) { pipeline.reporter }
+
+  before do
+    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_call_original
+    allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
+
+    @pre_snapshot = reporter.snapshot
+    pipeline.run
+    @post_snapshot = reporter.snapshot
+  end
+
+  after do
+    pipeline.shutdown
+  end
+
+  describe "events filtered" do
+    it "should start at zero" do
+      expect(@pre_snapshot.events_filtered).to eql(0)
+    end
+
+    it "should end at the number of generated events" do
+      expect(@post_snapshot.events_filtered).to eql(generator_count)
+    end
+  end
+
+  describe "events consumed" do
+    it "should start at zero" do
+      expect(@pre_snapshot.events_consumed).to eql(0)
+    end
+
+    it "should end at the number of generated events" do
+      expect(@post_snapshot.events_consumed).to eql(generator_count)
+    end
+  end
+
+  describe "inflight count" do
+    it "should be zero before running" do
+      expect(@pre_snapshot.inflight_count).to eql(0)
+    end
+
+    it "should be zero after running" do
+      expect(@post_snapshot.inflight_count).to eql(0)
+    end
+  end
+
+  describe "output states" do
+    it "should include the count of received events" do
+      expect(@post_snapshot.output_info.first[:events_received]).to eql(generator_count)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
new file mode 100644
index 00000000000..0fb6be8368d
--- /dev/null
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -0,0 +1,664 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/inputs/generator"
+require "logstash/filters/multiline"
+require_relative "../support/mocks_classes"
+
+class DummyInput < LogStash::Inputs::Base
+  config_name "dummyinput"
+  milestone 2
+
+  def register
+  end
+
+  def run(queue)
+  end
+
+  def close
+  end
+end
+
+class DummyInputGenerator < LogStash::Inputs::Base
+  config_name "dummyinputgenerator"
+  milestone 2
+
+  def register
+  end
+
+  def run(queue)
+    queue << Logstash::Event.new while !stop?
+  end
+
+  def close
+  end
+end
+
+class DummyCodec < LogStash::Codecs::Base
+  config_name "dummycodec"
+  milestone 2
+
+  def decode(data)
+    data
+  end
+
+  def encode(event)
+    event
+  end
+
+  def close
+  end
+end
+
+class DummyOutputMore < DummyOutput
+  config_name "dummyoutputmore"
+end
+
+class DummyFilter < LogStash::Filters::Base
+  config_name "dummyfilter"
+  milestone 2
+
+  def register() end
+
+  def filter(event) end
+
+  def threadsafe?() false; end
+
+  def close() end
+end
+
+class DummySafeFilter < LogStash::Filters::Base
+  config_name "dummysafefilter"
+  milestone 2
+
+  def register() end
+
+  def filter(event) end
+
+  def threadsafe?() true; end
+
+  def close() end
+end
+
+class TestPipeline < LogStash::Pipeline
+  attr_reader :outputs, :settings, :logger
+end
+
+describe LogStash::Pipeline do
+  let(:worker_thread_count)     { 5 }
+  let(:safe_thread_count)       { 1 }
+  let(:override_thread_count)   { 42 }
+  let(:pipeline_settings_obj) { LogStash::SETTINGS }
+  let(:pipeline_settings) { {} }
+
+  before :each do
+    pipeline_workers_setting = LogStash::SETTINGS.get_setting("pipeline.workers")
+    allow(pipeline_workers_setting).to receive(:default).and_return(worker_thread_count)
+    pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+  end
+
+  after :each do
+    pipeline_settings_obj.reset
+  end
+
+  describe "defaulting the pipeline workers based on thread safety" do
+    before(:each) do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummysafefilter").and_return(DummySafeFilter)
+    end
+
+    context "when there are some not threadsafe filters" do
+      let(:test_config_with_filters) {
+        <<-eos
+        input {
+          dummyinput {}
+        }
+
+        filter {
+          dummyfilter {}
+        }
+
+        output {
+          dummyoutput {}
+        }
+        eos
+      }
+
+      describe "debug compiled" do
+        let(:logger) { double("pipeline logger").as_null_object }
+
+        before do
+          expect(Cabin::Channel).to receive(:get).with(LogStash).and_return(logger).at_least(:once)
+          allow(logger).to receive(:debug?).and_return(true)
+        end
+
+        it "should not receive a debug message with the compiled code" do
+          pipeline_settings_obj.set("config.debug", false)
+          expect(logger).not_to receive(:debug).with(/Compiled pipeline/, anything)
+          pipeline = TestPipeline.new(test_config_with_filters)
+        end
+
+        it "should print the compiled code if config.debug is set to true" do
+          pipeline_settings_obj.set("config.debug", true)
+          expect(logger).to receive(:debug).with(/Compiled pipeline/, anything)
+          pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
+        end
+      end
+
+      context "when there is no command line -w N set" do
+        it "starts one filter thread" do
+          msg = "Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads"
+          pipeline = TestPipeline.new(test_config_with_filters)
+          expect(pipeline.logger).to receive(:warn).with(msg,
+            {:count_was=>worker_thread_count, :filters=>["dummyfilter"]})
+          pipeline.run
+          expect(pipeline.worker_threads.size).to eq(safe_thread_count)
+          pipeline.shutdown
+        end
+      end
+
+      context "when there is command line -w N set" do
+        let(:pipeline_settings) { {"pipeline.workers" => override_thread_count } }
+        it "starts multiple filter thread" do
+          msg = "Warning: Manual override - there are filters that might" +
+                " not work with multiple worker threads"
+          pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
+          expect(pipeline.logger).to receive(:warn).with(msg,
+            {:worker_threads=> override_thread_count, :filters=>["dummyfilter"]})
+          pipeline.run
+          expect(pipeline.worker_threads.size).to eq(override_thread_count)
+          pipeline.shutdown
+        end
+      end
+    end
+
+    context "when there are threadsafe filters only" do
+      let(:test_config_with_filters) {
+        <<-eos
+        input {
+          dummyinput {}
+        }
+
+        filter {
+          dummysafefilter {}
+        }
+
+        output {
+          dummyoutput {}
+        }
+        eos
+      }
+
+      it "starts multiple filter threads" do
+        pipeline = TestPipeline.new(test_config_with_filters)
+        pipeline.run
+        expect(pipeline.worker_threads.size).to eq(worker_thread_count)
+        pipeline.shutdown
+      end
+    end
+  end
+
+  context "close" do
+    before(:each) do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    end
+
+
+    let(:test_config_without_output_workers) {
+      <<-eos
+      input {
+        dummyinput {}
+      }
+
+      output {
+        dummyoutput {}
+      }
+      eos
+    }
+
+    let(:test_config_with_output_workers) {
+      <<-eos
+      input {
+        dummyinput {}
+      }
+
+      output {
+        dummyoutput {
+          workers => 2
+        }
+      }
+      eos
+    }
+
+    context "output close" do
+      it "should call close of output without output-workers" do
+        pipeline = TestPipeline.new(test_config_without_output_workers)
+        pipeline.run
+
+        expect(pipeline.outputs.size ).to eq(1)
+        expect(pipeline.outputs.first.workers.size ).to eq(::LogStash::SETTINGS.get("pipeline.output.workers"))
+        expect(pipeline.outputs.first.workers.first.num_closes ).to eq(1)
+        pipeline.shutdown
+      end
+
+      it "should call output close correctly with output workers" do
+        pipeline = TestPipeline.new(test_config_with_output_workers)
+        pipeline.run
+
+        expect(pipeline.outputs.size ).to eq(1)
+        # We even close the parent output worker, even though it doesn't receive messages
+
+        output_delegator = pipeline.outputs.first
+        output = output_delegator.workers.first
+
+        expect(output.num_closes).to eq(1)
+        output_delegator.workers.each do |plugin|
+          expect(plugin.num_closes ).to eq(1)
+        end
+        pipeline.shutdown
+      end
+    end
+  end
+
+  context "compiled flush function" do
+    describe "flusher thread" do
+      before(:each) do
+        allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+        allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+        allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      end
+
+      let(:config) { "input { dummyinput {} } output { dummyoutput {} }"}
+
+      it "should start the flusher thread only after the pipeline is running" do
+        pipeline = TestPipeline.new(config)
+
+        expect(pipeline).to receive(:transition_to_running).ordered.and_call_original
+        expect(pipeline).to receive(:start_flusher).ordered.and_call_original
+
+        pipeline.run
+        pipeline.shutdown
+      end
+    end
+
+    context "cancelled events should not propagate down the filters" do
+      config <<-CONFIG
+        filter {
+          multiline {
+           pattern => "hello"
+           what => next
+          }
+          multiline {
+           pattern => "hello"
+           what => next
+          }
+        }
+      CONFIG
+
+      sample("hello") do
+        expect(subject.get("message")).to eq("hello")
+      end
+    end
+
+    context "new events should propagate down the filters" do
+      config <<-CONFIG
+        filter {
+          clone {
+            clones => ["clone1"]
+          }
+          multiline {
+            pattern => "bar"
+            what => previous
+          }
+        }
+      CONFIG
+
+      sample(["foo", "bar"]) do
+        expect(subject.size).to eq(2)
+
+        expect(subject[0].get("message")).to eq("foo\nbar")
+        expect(subject[0].get("type")).to be_nil
+        expect(subject[1].get("message")).to eq("foo\nbar")
+        expect(subject[1].get("type")).to eq("clone1")
+      end
+    end
+  end
+
+  describe "max inflight warning" do
+    let(:config) { "input { dummyinput {} } output { dummyoutput {} }" }
+    let(:batch_size) { 1 }
+    let(:pipeline_settings) { { "pipeline.batch.size" => batch_size, "pipeline.workers" => 1 } }
+    let(:pipeline) { LogStash::Pipeline.new(config, pipeline_settings_obj) }
+    let(:logger) { pipeline.logger }
+    let(:warning_prefix) { /CAUTION: Recommended inflight events max exceeded!/ }
+
+    before(:each) do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(logger).to receive(:warn)
+      thread = Thread.new { pipeline.run }
+      pipeline.shutdown
+      thread.join
+    end
+
+    it "should not raise a max inflight warning if the max_inflight count isn't exceeded" do
+      expect(logger).not_to have_received(:warn).with(warning_prefix)
+    end
+
+    context "with a too large inflight count" do
+      let(:batch_size) { LogStash::Pipeline::MAX_INFLIGHT_WARN_THRESHOLD + 1 }
+
+      it "should raise a max inflight warning if the max_inflight count is exceeded" do
+        expect(logger).to have_received(:warn).with(warning_prefix)
+      end
+    end
+  end
+
+  context "compiled filter funtions" do
+
+    context "new events should propagate down the filters" do
+      config <<-CONFIG
+        filter {
+          clone {
+            clones => ["clone1", "clone2"]
+          }
+          mutate {
+            add_field => {"foo" => "bar"}
+          }
+        }
+      CONFIG
+
+      sample("hello") do
+        expect(subject.size).to eq(3)
+
+        expect(subject[0].get("message")).to eq("hello")
+        expect(subject[0].get("type")).to be_nil
+        expect(subject[0].get("foo")).to eq("bar")
+
+        expect(subject[1].get("message")).to eq("hello")
+        expect(subject[1].get("type")).to eq("clone1")
+        expect(subject[1].get("foo")).to eq("bar")
+
+        expect(subject[2].get("message")).to eq("hello")
+        expect(subject[2].get("type")).to eq("clone2")
+        expect(subject[2].get("foo")).to eq("bar")
+      end
+    end
+  end
+
+  context "metrics" do
+    config <<-CONFIG
+    input { }
+    filter { }
+    output { }
+    CONFIG
+
+    it "uses a `NullMetric` object if `metric.collect` is set to false" do
+      settings = double("LogStash::SETTINGS")
+
+      allow(settings).to receive(:get_value).with("pipeline.id").and_return("main")
+      allow(settings).to receive(:get_value).with("metric.collect").and_return(false)
+      allow(settings).to receive(:get_value).with("config.debug").and_return(false)
+
+      pipeline = LogStash::Pipeline.new(config, settings)
+      expect(pipeline.metric).to be_kind_of(LogStash::Instrument::NullMetric)
+    end
+  end
+
+  context "Multiples pipelines" do
+    before do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinputgenerator").and_return(DummyInputGenerator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputmore").and_return(DummyOutputMore)
+    end
+
+    let(:pipeline1) { LogStash::Pipeline.new("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+    let(:pipeline2) { LogStash::Pipeline.new("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutputmore {}}") }
+
+    it "should handle evaluating different config" do
+      expect(pipeline1.output_func(LogStash::Event.new)).not_to include(nil)
+      expect(pipeline1.filter_func(LogStash::Event.new)).not_to include(nil)
+      expect(pipeline2.output_func(LogStash::Event.new)).not_to include(nil)
+      expect(pipeline1.filter_func(LogStash::Event.new)).not_to include(nil)
+    end
+  end
+
+  context "Periodic Flush" do
+    let(:number_of_events) { 100 }
+    let(:config) do
+      <<-EOS
+      input {
+        generator {
+          count => #{number_of_events}
+        }
+      }
+      filter {
+        multiline {
+          pattern => "^NeverMatch"
+          negate => true
+          what => "previous"
+        }
+      }
+      output {
+        dummyoutput {}
+      }
+      EOS
+    end
+    let(:output) { DummyOutput.new }
+
+    before do
+      allow(DummyOutput).to receive(:new).with(any_args).and_return(output)
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    end
+
+    it "flushes the buffered contents of the filter" do
+      Thread.abort_on_exception = true
+      pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+      # give us a bit of time to flush the events
+      wait(5).for do
+        next unless output && output.events && output.events.first
+        output.events.first.get("message").split("\n").count
+      end.to eq(number_of_events)
+      pipeline.shutdown
+    end
+  end
+
+  context "Multiple pipelines" do
+    before do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    end
+
+    let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+    let(:pipeline2) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+
+    it "should handle evaluating different config" do
+      # When the functions are compiled from the AST it will generate instance
+      # variables that are unique to the actual config, the intances are pointing
+      # to conditionals and/or plugins.
+      #
+      # Before the `defined_singleton_method`, the definition of the method was
+      # not unique per class, but the `instance variables` were unique per class.
+      #
+      # So the methods were trying to access instance variables that did not exist
+      # in the current instance and was returning an array containing nil values for
+      # the match.
+      expect(pipeline1.output_func(LogStash::Event.new)).not_to include(nil)
+      expect(pipeline1.filter_func(LogStash::Event.new)).not_to include(nil)
+      expect(pipeline2.output_func(LogStash::Event.new)).not_to include(nil)
+      expect(pipeline1.filter_func(LogStash::Event.new)).not_to include(nil)
+    end
+  end
+
+  context "#started_at" do
+    let(:config) do
+      <<-EOS
+      input {
+        generator {}
+      }
+      EOS
+    end
+
+    subject { described_class.new(config) }
+
+    it "returns nil when the pipeline isnt started" do
+      expect(subject.started_at).to be_nil
+    end
+
+    it "return when the pipeline started working" do
+      t = Thread.new { subject.run }
+      sleep(0.1)
+      expect(subject.started_at).to be < Time.now
+      subject.shutdown
+    end
+  end
+
+  context "#uptime" do
+    let(:config) do
+      <<-EOS
+      input {
+        generator {}
+      }
+      EOS
+    end
+    subject { described_class.new(config) }
+
+    context "when the pipeline is not started" do
+      it "returns 0" do
+        expect(subject.uptime).to eq(0)
+      end
+    end
+
+    context "when the pipeline is started" do
+      it "return the duration in milliseconds" do
+        t = Thread.new { subject.run }
+        sleep(0.1)
+        expect(subject.uptime).to be > 0
+        subject.shutdown
+      end
+    end
+  end
+
+  context "when collecting metrics in the pipeline" do
+    let(:pipeline_settings) { { "pipeline.id" => pipeline_id } }
+    subject { described_class.new(config, pipeline_settings_obj) }
+    let(:pipeline_id) { "main" }
+    let(:metric) { LogStash::Instrument::Metric.new }
+    let(:number_of_events) { 1000 }
+    let(:multiline_id) { "my-multiline" }
+    let(:multiline_id_other) { "my-multiline_other" }
+    let(:dummy_output_id) { "my-dummyoutput" }
+    let(:generator_id) { "my-generator" }
+    let(:config) do
+      <<-EOS
+      input {
+        generator {
+           count => #{number_of_events}
+           id => "#{generator_id}"
+        }
+      }
+      filter {
+         multiline {
+              id => "#{multiline_id}"
+              pattern => "hello"
+              what => next
+          }
+          multiline {
+               id => "#{multiline_id_other}"
+               pattern => "hello"
+               what => next
+           }
+      }
+      output {
+        dummyoutput {
+          id => "#{dummy_output_id}"
+        }
+      }
+      EOS
+    end
+    let(:dummyoutput) { DummyOutput.new({ "id" => dummy_output_id }) }
+
+    before :each do
+      allow(DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+
+      # Reset the metric store
+      LogStash::Instrument::Collector.instance.clear
+
+      Thread.new { subject.run }
+      # make sure we have received all the generated events
+      sleep 1 while dummyoutput.events.size < number_of_events
+    end
+
+    after :each do
+      subject.shutdown
+    end
+
+    context "global metric" do
+      let(:collected_metric) { LogStash::Instrument::Collector.instance.snapshot_metric.metric_store.get_with_path("stats/events") }
+
+      it "populates the differents" do
+        expect(collected_metric[:stats][:events][:in].value).to eq(number_of_events)
+        expect(collected_metric[:stats][:events][:filtered].value).to eq(number_of_events)
+        expect(collected_metric[:stats][:events][:out].value).to eq(number_of_events)
+      end
+    end
+
+    context "pipelines" do
+      let(:collected_metric) { LogStash::Instrument::Collector.instance.snapshot_metric.metric_store.get_with_path("stats/pipelines/") }
+
+      it "populates the pipelines core metrics" do
+        expect(collected_metric[:stats][:pipelines][:main][:events][:in].value).to eq(number_of_events)
+        expect(collected_metric[:stats][:pipelines][:main][:events][:filtered].value).to eq(number_of_events)
+        expect(collected_metric[:stats][:pipelines][:main][:events][:out].value).to eq(number_of_events)
+      end
+
+      it "populates the filter metrics" do
+        [multiline_id, multiline_id_other].map(&:to_sym).each do |id|
+          [:in, :out].each do |metric_key|
+            plugin_name = "multiline_#{id}".to_sym
+            expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:events][metric_key].value).to eq(number_of_events)
+          end
+        end
+      end
+
+      it "populates the output metrics" do
+        plugin_name = "dummyoutput_#{dummy_output_id}".to_sym
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:out].value).to eq(number_of_events)
+      end
+    end
+  end
+
+  context "Pipeline object" do
+    before do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    end
+
+    let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+    let(:pipeline2) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+
+    it "should not add ivars" do
+       expect(pipeline1.instance_variables).to eq(pipeline2.instance_variables)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/plugin_spec.rb b/logstash-core/spec/logstash/plugin_spec.rb
new file mode 100644
index 00000000000..2a5040551ff
--- /dev/null
+++ b/logstash-core/spec/logstash/plugin_spec.rb
@@ -0,0 +1,273 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/plugin"
+require "logstash/outputs/base"
+require "logstash/codecs/base"
+require "logstash/inputs/base"
+require "logstash/filters/base"
+
+describe LogStash::Plugin do
+  it "should fail lookup on inexisting type" do
+    #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
+    expect { LogStash::Plugin.lookup("badbadtype", "badname") }.to raise_error(LogStash::PluginLoadingError)
+  end
+
+  it "should fail lookup on inexisting name" do
+    #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
+    expect { LogStash::Plugin.lookup("filter", "badname") }.to raise_error(LogStash::PluginLoadingError)
+  end
+
+  it "should fail on bad plugin class" do
+    LogStash::Filters::BadSuperClass = Class.new
+    expect { LogStash::Plugin.lookup("filter", "bad_super_class") }.to raise_error(LogStash::PluginLoadingError)
+  end
+
+  it "should fail on missing config_name method" do
+    LogStash::Filters::MissingConfigName = Class.new(LogStash::Filters::Base)
+    expect { LogStash::Plugin.lookup("filter", "missing_config_name") }.to raise_error(LogStash::PluginLoadingError)
+  end
+
+  it "should lookup an already defined plugin class" do
+    class LogStash::Filters::LadyGaga < LogStash::Filters::Base
+      config_name "lady_gaga"
+    end
+    expect(LogStash::Plugin.lookup("filter", "lady_gaga")).to eq(LogStash::Filters::LadyGaga)
+  end
+
+  describe "plugin signup in the registry" do
+
+    let(:registry) { LogStash::Registry.instance }
+
+    it "should be present in the registry" do
+      class LogStash::Filters::MyPlugin < LogStash::Filters::Base
+        config_name "my_plugin"
+      end
+      path     = "logstash/filters/my_plugin"
+      expect(registry.registered?(path)).to eq(true)
+    end
+  end
+
+  describe "#inspect" do
+    class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
+      config_name "param1"
+      config :num, :validate => :number, :default => 20
+      config :str, :validate => :string, :default => "test"
+    end
+    subject { LogStash::Filters::MyTestFilter.new("num" => 1, "str" => "hello") }
+
+    it "should print the class of the filter" do
+      expect(subject.inspect).to match(/^<LogStash::Filters::MyTestFilter/)
+    end
+    it "should list config options and values" do
+      expect(subject.inspect).to match(/num=>1, str=>"hello"/)
+    end
+  end
+
+  context "when validating the plugin version" do
+    let(:plugin_name) { 'logstash-filter-stromae' }
+    subject do
+      Class.new(LogStash::Filters::Base) do
+        config_name 'stromae'
+      end
+    end
+
+    it "doesn't warn the user if the version is superior or equal to 1.0.0" do
+      allow(Gem::Specification).to receive(:find_by_name)
+        .with(plugin_name)
+        .and_return(double(:version => Gem::Version.new('1.0.0')))
+
+      expect_any_instance_of(Cabin::Channel).not_to receive(:info)
+      subject.validate({})
+    end
+
+    it 'warns the user if the plugin version is between 0.9.x and 1.0.0' do
+      allow(Gem::Specification).to receive(:find_by_name)
+        .with(plugin_name)
+        .and_return(double(:version => Gem::Version.new('0.9.1')))
+
+      expect_any_instance_of(Cabin::Channel).to receive(:info)
+        .with(/Using version 0.9.x/)
+
+      subject.validate({})
+    end
+
+    it 'warns the user if the plugin version is inferior to 0.9.x' do
+      allow(Gem::Specification).to receive(:find_by_name)
+        .with(plugin_name)
+        .and_return(double(:version => Gem::Version.new('0.1.1')))
+
+      expect_any_instance_of(Cabin::Channel).to receive(:info)
+        .with(/Using version 0.1.x/)
+      subject.validate({})
+    end
+
+    it "doesnt show the version notice more than once" do
+      one_notice = Class.new(LogStash::Filters::Base) do
+        config_name "stromae"
+      end
+
+      allow(Gem::Specification).to receive(:find_by_name)
+        .with(plugin_name)
+        .and_return(double(:version => Gem::Version.new('0.1.1')))
+
+      expect_any_instance_of(Cabin::Channel).to receive(:info)
+        .once
+        .with(/Using version 0.1.x/)
+
+      one_notice.validate({})
+      one_notice.validate({})
+    end
+
+    it "warns the user if we can't find a defined version" do
+      expect_any_instance_of(Cabin::Channel).to receive(:warn)
+        .once
+        .with(/plugin doesn't have a version/)
+
+      subject.validate({})
+    end
+
+
+    it 'logs a warning if the plugin use the milestone option' do
+      expect_any_instance_of(Cabin::Channel).to receive(:debug)
+        .with(/stromae plugin is using the 'milestone' method/)
+
+      class LogStash::Filters::Stromae < LogStash::Filters::Base
+        config_name "stromae"
+        milestone 2
+      end
+    end
+  end
+
+  describe "subclass initialize" do
+    let(:args) { Hash.new }
+
+    [
+      StromaeCodec = Class.new(LogStash::Codecs::Base) do
+        config_name "stromae"
+        config :foo_tag, :validate => :string, :default => "bar"
+      end,
+      StromaeFilter = Class.new(LogStash::Filters::Base) do
+        config_name "stromae"
+        config :foo_tag, :validate => :string, :default => "bar"
+      end,
+      StromaeInput = Class.new(LogStash::Inputs::Base) do
+        config_name "stromae"
+        config :foo_tag, :validate => :string, :default => "bar"
+      end,
+      StromaeOutput = Class.new(LogStash::Outputs::Base) do
+        config_name "stromae"
+        config :foo_tag, :validate => :string, :default => "bar"
+      end
+    ].each do |klass|
+
+      it "subclass #{klass.name} does not modify params" do
+        klass.new(args)
+        expect(args).to be_empty
+      end
+    end
+
+    context "codec initialization" do
+
+      class LogStash::Codecs::Noop < LogStash::Codecs::Base
+        config_name "noop"
+
+        config :format, :validate => :string
+        def register; end
+      end
+
+      it "should only register once" do
+        args   = { "codec" => LogStash::Codecs::Noop.new("format" => ".") }
+        expect_any_instance_of(LogStash::Codecs::Noop).to receive(:register).once
+        LogStash::Plugin.new(args)
+      end
+
+    end
+  end
+
+  describe "#id" do
+    plugin_types = [
+      LogStash::Filters::Base,
+      LogStash::Codecs::Base,
+      LogStash::Outputs::Base,
+      LogStash::Inputs::Base
+    ]
+
+    plugin_types.each do |plugin_type|
+      let(:plugin) do
+        Class.new(plugin_type) do
+          config_name "simple_plugin"
+
+          config :host, :validate => :string
+          config :export, :validte => :boolean
+
+          def register; end
+        end
+      end
+
+      let(:config) do
+        {
+          "host" => "127.0.0.1",
+          "export" => true
+        }
+      end
+
+      subject { plugin.new(config) }
+
+      context "plugin type is #{plugin_type}" do
+        context "when there is not ID configured for the output" do
+          it "it uses a UUID to identify this plugins" do
+            expect(subject.id).not_to eq(nil)
+          end
+
+          it "will be different between instance of plugins" do
+            expect(subject.id).not_to eq(plugin.new(config).id)
+          end
+        end
+
+        context "When a user provide an ID for the plugin" do
+          let(:id) { "ABC" }
+          let(:config) { super.merge("id" => id) }
+
+          it "uses the user provided ID" do
+            expect(subject.id).to eq(id)
+          end
+        end
+      end
+    end
+  end
+
+  describe "#plugin_unique_name" do
+    let(:plugin) do
+      Class.new(LogStash::Filters::Base,) do
+        config_name "simple_plugin"
+        config :host, :validate => :string
+
+        def register; end
+      end
+    end
+
+    let(:config) do
+      {
+        "host" => "127.0.0.1"
+      }
+    end
+
+    context "when the id is provided" do
+      let(:my_id) { "mysuper-plugin" }
+      let(:config) { super.merge({ "id" => my_id })}
+      subject { plugin.new(config) }
+
+      it "return a human readable ID" do
+        expect(subject.plugin_unique_name).to eq("simple_plugin_#{my_id}")
+      end
+    end
+
+    context "when the id is not provided provided" do
+      subject { plugin.new(config) }
+
+      it "return a human readable ID" do
+        expect(subject.plugin_unique_name).to match(/^simple_plugin_/)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/plugins/registry_spec.rb b/logstash-core/spec/logstash/plugins/registry_spec.rb
new file mode 100644
index 00000000000..f109f68653a
--- /dev/null
+++ b/logstash-core/spec/logstash/plugins/registry_spec.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/plugins/registry"
+require "logstash/inputs/base"
+
+# use a dummy NOOP input to test plugin registry
+class LogStash::Inputs::Dummy < LogStash::Inputs::Base
+  config_name "dummy"
+
+  def register; end
+
+end
+
+describe LogStash::Registry do
+
+  let(:registry) { described_class.instance }
+
+  context "when loading installed plugins" do
+
+    let(:plugin) { double("plugin") }
+
+    it "should return the expected class" do
+      klass = registry.lookup("input", "stdin")
+      expect(klass).to eq(LogStash::Inputs::Stdin)
+    end
+
+    it "should raise an error if can not find the plugin class" do
+      expect(LogStash::Registry::Plugin).to receive(:new).with("input", "elastic").and_return(plugin)
+      expect(plugin).to receive(:path).and_return("logstash/input/elastic").twice
+      expect(plugin).to receive(:installed?).and_return(true)
+      expect { registry.lookup("input", "elastic") }.to raise_error(LoadError)
+    end
+
+    it "should load from registry is already load" do
+      registry.lookup("input", "stdin")
+      expect(registry).to receive(:registered?).and_return(true).once
+      registry.lookup("input", "stdin")
+      internal_registry = registry.instance_variable_get("@registry")
+      expect(internal_registry).to include("logstash/inputs/stdin" => LogStash::Inputs::Stdin)
+    end
+  end
+
+  context "when loading code defined plugins" do
+    it "should return the expected class" do
+      klass = registry.lookup("input", "dummy")
+      expect(klass).to eq(LogStash::Inputs::Dummy)
+    end
+  end
+
+  context "when plugin is not installed and not defined" do
+    it "should raise an error" do
+      expect { registry.lookup("input", "elastic") }.to raise_error(LoadError)
+    end
+  end
+
+end
+
diff --git a/logstash-core/spec/logstash/runner_spec.rb b/logstash-core/spec/logstash/runner_spec.rb
new file mode 100644
index 00000000000..7283bf3aa86
--- /dev/null
+++ b/logstash-core/spec/logstash/runner_spec.rb
@@ -0,0 +1,267 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/runner"
+require "stud/task"
+require "stud/trap"
+require "stud/temporary"
+require "logstash/util/java_version"
+require "logstash/logging/json"
+require "json"
+
+class NullRunner
+  def run(args); end
+end
+
+describe LogStash::Runner do
+
+  subject { LogStash::Runner }
+  let(:channel) { Cabin::Channel.new }
+
+  before :each do
+    allow(Cabin::Channel).to receive(:get).with(LogStash).and_return(channel)
+    allow(channel).to receive(:subscribe).with(any_args)
+    allow(channel).to receive(:log) {}
+    allow(LogStash::ShutdownWatcher).to receive(:logger).and_return(channel)
+  end
+
+  after :each do
+    LogStash::SETTINGS.reset
+  end
+
+  after :all do
+    LogStash::ShutdownWatcher.logger = nil
+  end
+
+  describe "argument precedence" do
+    let(:config) { "input {} output {}" }
+    let(:cli_args) { ["-e", config, "-w", "20"] }
+    let(:settings_yml_hash) { { "pipeline.workers" => 2 } }
+
+    before :each do
+      allow(LogStash::SETTINGS).to receive(:read_yaml).and_return(settings_yml_hash)
+    end
+
+    after :each do
+      LogStash::SETTINGS.reset
+    end
+
+    it "favors the last occurence of an option" do
+      expect(LogStash::Agent).to receive(:new) do |settings|
+        expect(settings.get("config.string")).to eq(config)
+        expect(settings.get("pipeline.workers")).to eq(20)
+      end
+      subject.run("bin/logstash", cli_args)
+    end
+  end
+
+  describe "argument parsing" do
+    subject { LogStash::Runner.new("") }
+    before :each do
+      allow(Cabin::Channel.get(LogStash)).to receive(:terminal)
+    end
+    context "when -e is given" do
+
+      let(:args) { ["-e", "input {} output {}"] }
+      let(:agent) { double("agent") }
+
+      before do
+        allow(agent).to receive(:logger=).with(anything)
+        allow(agent).to receive(:shutdown)
+        allow(agent).to receive(:register_pipeline)
+      end
+
+      it "should execute the agent" do
+        expect(subject).to receive(:create_agent).and_return(agent)
+        expect(agent).to receive(:execute).once
+        subject.run(args)
+      end
+    end
+
+    context "with no arguments" do
+      let(:args) { [] }
+
+      before(:each) do
+        allow(LogStash::Util::JavaVersion).to receive(:warn_on_bad_java_version)
+      end
+
+      it "should show help" do
+        expect($stderr).to receive(:puts).once
+        expect(subject).to receive(:signal_usage_error).once.and_call_original
+        expect(subject).to receive(:show_short_help).once
+        subject.run(args)
+      end
+    end
+  end
+
+  context "--pluginpath" do
+    subject { LogStash::Runner.new("") }
+    let(:single_path) { "/some/path" }
+    let(:multiple_paths) { ["/some/path1", "/some/path2"] }
+
+    it "should add single valid dir path to the environment" do
+      expect(File).to receive(:directory?).and_return(true)
+      expect(LogStash::Environment).to receive(:add_plugin_path).with(single_path)
+      subject.configure_plugin_paths(single_path)
+    end
+
+    it "should fail with single invalid dir path" do
+      expect(File).to receive(:directory?).and_return(false)
+      expect(LogStash::Environment).not_to receive(:add_plugin_path)
+      expect{subject.configure_plugin_paths(single_path)}.to raise_error(Clamp::UsageError)
+    end
+
+    it "should add multiple valid dir path to the environment" do
+      expect(File).to receive(:directory?).exactly(multiple_paths.size).times.and_return(true)
+      multiple_paths.each{|path| expect(LogStash::Environment).to receive(:add_plugin_path).with(path)}
+      subject.configure_plugin_paths(multiple_paths)
+    end
+  end
+
+  context "--auto-reload" do
+    subject { LogStash::Runner.new("") }
+    context "when -f is not given" do
+
+      let(:args) { ["-r", "-e", "input {} output {}"] }
+
+      it "should exit immediately" do
+        expect(subject).to receive(:signal_usage_error).and_call_original
+        expect(subject).to receive(:show_short_help)
+        expect(subject.run(args)).to eq(1)
+      end
+    end
+  end
+
+  describe "--log.format=json" do
+    subject { LogStash::Runner.new("") }
+    let(:logfile) { Stud::Temporary.file }
+    let(:args) { [ "--log.format", "json", "-l", logfile.path, "-e", "input {} output{}" ] }
+
+    after do
+      logfile.close
+      File.unlink(logfile.path)
+    end
+
+    before do
+      expect(channel).to receive(:subscribe).with(kind_of(LogStash::Logging::JSON)).and_call_original
+      subject.run(args)
+
+      # Log file should have stuff in it.
+      expect(logfile.stat.size).to be > 0
+    end
+
+    it "should log in valid json. One object per line." do
+      logfile.each_line do |line|
+        expect(line).not_to be_empty
+        expect { JSON.parse(line) }.not_to raise_error
+      end
+    end
+  end
+
+  describe "--config.test_and_exit" do
+    subject { LogStash::Runner.new("") }
+    let(:args) { ["-t", "-e", pipeline_string] }
+
+    context "with a good configuration" do
+      let(:pipeline_string) { "input { } filter { } output { }" }
+      it "should exit successfuly" do
+        expect(channel).to receive(:terminal)
+        expect(subject.run(args)).to eq(0)
+      end
+    end
+
+    context "with a bad configuration" do
+      let(:pipeline_string) { "rlwekjhrewlqrkjh" }
+      it "should fail by returning a bad exit code" do
+        expect(channel).to receive(:fatal)
+        expect(subject.run(args)).to eq(1)
+      end
+    end
+  end
+  describe "pipeline settings" do
+    let(:pipeline_string) { "input { stdin {} } output { stdout {} }" }
+    let(:main_pipeline_settings) { { :pipeline_id => "main" } }
+    let(:pipeline) { double("pipeline") }
+
+    before(:each) do
+      allow_any_instance_of(LogStash::Agent).to receive(:execute).and_return(true)
+      task = Stud::Task.new { 1 }
+      allow(pipeline).to receive(:run).and_return(task)
+      allow(pipeline).to receive(:shutdown)
+    end
+
+    context "when :pipeline_workers is not defined by the user" do
+      it "should not pass the value to the pipeline" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+	  expect(settings.set?("pipeline.workers")).to be(false)
+        end
+        args = ["-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    context "when :pipeline_workers is defined by the user" do
+      it "should pass the value to the pipeline" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+	  expect(settings.set?("pipeline.workers")).to be(true)
+	  expect(settings.get("pipeline.workers")).to be(2)
+        end
+
+        args = ["-w", "2", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    describe "config.debug" do
+      it "should set 'config.debug' to false by default" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.get("config.debug")).to eq(false)
+        end
+        args = ["--log.level", "debug", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+
+      it "should allow overriding config.debug" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.get("config.debug")).to eq(true)
+        end
+        args = ["--log.level", "debug", "--config.debug",  "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+  end
+
+  describe "--log.level" do
+    before :each do
+      allow_any_instance_of(subject).to receive(:show_version)
+    end
+    context "when not set" do
+      it "should set log level to warn" do
+        args = ["--version"]
+        subject.run("bin/logstash", args)
+        expect(channel.level).to eq(:warn)
+      end
+    end
+    context "when setting to debug" do
+      it "should set log level to debug" do
+        args = ["--log.level", "debug",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(channel.level).to eq(:debug)
+      end
+    end
+    context "when setting to verbose" do
+      it "should set log level to info" do
+        args = ["--log.level", "verbose",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(channel.level).to eq(:info)
+      end
+    end
+    context "when setting to quiet" do
+      it "should set log level to error" do
+        args = ["--log.level", "quiet",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(channel.level).to eq(:error)
+      end
+    end
+  end
+
+end
diff --git a/logstash-core/spec/logstash/setting_spec.rb b/logstash-core/spec/logstash/setting_spec.rb
new file mode 100644
index 00000000000..33d1572b256
--- /dev/null
+++ b/logstash-core/spec/logstash/setting_spec.rb
@@ -0,0 +1,130 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting do
+  let(:logger) { double("logger") }
+  describe "#value" do
+    context "when using a default value" do
+      context "when no value is set" do
+        subject { described_class.new("number", Numeric, 1) }
+        it "should return the default value" do
+          expect(subject.value).to eq(1)
+        end
+      end
+
+      context "when a value is set" do
+        subject { described_class.new("number", Numeric, 1) }
+        let(:new_value) { 2 }
+        before :each do
+          subject.set(new_value)
+        end
+        it "should return the set value" do
+          expect(subject.value).to eq(new_value)
+        end
+      end
+    end
+
+    context "when not using a default value" do
+      context "when no value is set" do
+        subject { described_class.new("number", Numeric, nil, false) }
+        it "should return the default value" do
+          expect(subject.value).to eq(nil)
+        end
+      end
+
+      context "when a value is set" do
+        subject { described_class.new("number", Numeric, nil, false) }
+        let(:new_value) { 2 }
+        before :each do
+          subject.set(new_value)
+        end
+        it "should return the set value" do
+          expect(subject.value).to eq(new_value)
+        end
+      end
+    end
+  end
+
+  describe "#set?" do
+    context "when there is not value set" do
+      subject { described_class.new("number", Numeric, 1) }
+      it "should return false" do
+        expect(subject.set?).to be(false)
+      end
+    end
+    context "when there is a value set" do
+      subject { described_class.new("number", Numeric, 1) }
+      before :each do
+        subject.set(2)
+      end
+      it "should return false" do
+        expect(subject.set?).to be(true)
+      end
+    end
+  end
+  describe "#set" do
+    subject { described_class.new("number", Numeric, 1) }
+    it "should change the value of a setting" do
+      expect(subject.value).to eq(1)
+      subject.set(4)
+      expect(subject.value).to eq(4)
+    end
+    context "when executed for the first time" do
+      it "should change the result of set?" do
+        expect(subject.set?).to eq(false)
+        subject.set(4)
+        expect(subject.set?).to eq(true)
+      end
+    end
+
+    context "when the argument's class does not match @klass" do
+      it "should throw an exception" do
+        expect { subject.set("not a number") }.to raise_error
+      end
+    end
+  end
+
+  describe "#reset" do
+    subject { described_class.new("number", Numeric, 1) }
+    context "if value is already set" do
+      before :each do
+        subject.set(2)
+      end
+      it "should reset value to default" do
+        subject.reset
+        expect(subject.value).to eq(1)
+      end
+      it "should reset set? to false" do
+        expect(subject.set?).to eq(true)
+        subject.reset
+        expect(subject.set?).to eq(false)
+      end
+    end
+  end
+
+  describe "validator_proc" do
+    let(:default_value) { "small text" }
+    subject { described_class.new("mytext", String, default_value) {|v| v.size < 20 } }
+    context "when validation fails" do
+      let(:new_value) { "very very very very very big text" }
+      it "should raise an exception" do
+        expect { subject.set(new_value) }.to raise_error
+      end
+      it "should not change the value" do
+        subject.set(new_value) rescue nil
+        expect(subject.value).to eq(default_value)
+      end
+    end
+    context "when validation is successful" do
+      let(:new_value) { "smaller text" }
+      it "should not raise an exception" do
+        expect { subject.set(new_value) }.to_not raise_error
+      end
+      it "should change the value" do
+        subject.set(new_value)
+        expect(subject.value).to eq(new_value)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings_spec.rb b/logstash-core/spec/logstash/settings_spec.rb
new file mode 100644
index 00000000000..050da87f4ac
--- /dev/null
+++ b/logstash-core/spec/logstash/settings_spec.rb
@@ -0,0 +1,62 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Settings do
+  let(:numeric_setting_name) { "number" }
+  let(:numeric_setting) { LogStash::Setting.new(numeric_setting_name, Numeric, 1) }
+  describe "#register" do
+    context "if setting has already been registered" do
+      before :each do
+        subject.register(numeric_setting)
+      end
+      it "should raise an exception" do
+        expect { subject.register(numeric_setting) }.to raise_error
+      end
+    end
+    context "if setting hasn't been registered" do
+      it "should not raise an exception" do
+        expect { subject.register(numeric_setting) }.to_not raise_error
+      end
+    end
+  end
+  describe "#get_setting" do
+    context "if setting has been registered" do
+      before :each do
+        subject.register(numeric_setting)
+      end
+      it "should return the setting" do
+        expect(subject.get_setting(numeric_setting_name)).to eq(numeric_setting)
+      end
+    end
+    context "if setting hasn't been registered" do
+      it "should raise an exception" do
+        expect { subject.get_setting(numeric_setting_name) }.to raise_error
+      end
+    end
+  end
+  describe "#get_subset" do
+    let(:numeric_setting_1) { LogStash::Setting.new("num.1", Numeric, 1) }
+    let(:numeric_setting_2) { LogStash::Setting.new("num.2", Numeric, 2) }
+    let(:numeric_setting_3) { LogStash::Setting.new("num.3", Numeric, 3) }
+    let(:string_setting_1) { LogStash::Setting.new("string.1", String, "hello") }
+    before :each do
+      subject.register(numeric_setting_1)
+      subject.register(numeric_setting_2)
+      subject.register(numeric_setting_3)
+      subject.register(string_setting_1)
+    end
+
+    it "supports regex" do
+      expect(subject.get_subset(/num/).get_setting("num.3")).to eq(numeric_setting_3)
+      expect { subject.get_subset(/num/).get_setting("string.1") }.to raise_error
+    end
+
+    it "returns a copy of settings" do
+      subset = subject.get_subset(/num/)
+      subset.set("num.2", 1000)
+      expect(subject.get("num.2")).to eq(2)
+      expect(subset.get("num.2")).to eq(1000)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/shutdown_watcher_spec.rb b/logstash-core/spec/logstash/shutdown_watcher_spec.rb
new file mode 100644
index 00000000000..118e126ea5d
--- /dev/null
+++ b/logstash-core/spec/logstash/shutdown_watcher_spec.rb
@@ -0,0 +1,117 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/shutdown_watcher"
+
+describe LogStash::ShutdownWatcher do
+  let(:channel) { Cabin::Channel.new }
+
+  let(:check_every) { 0.01 }
+  let(:check_threshold) { 100 }
+  subject { LogStash::ShutdownWatcher.new(pipeline, check_every) }
+  let(:pipeline) { double("pipeline") }
+  let(:reporter) { double("reporter") }
+  let(:reporter_snapshot) { double("reporter snapshot") }
+  report_count = 0
+
+  before :each do
+    LogStash::ShutdownWatcher.logger = channel
+
+    allow(pipeline).to receive(:reporter).and_return(reporter)
+    allow(pipeline).to receive(:thread).and_return(Thread.current)
+    allow(reporter).to receive(:snapshot).and_return(reporter_snapshot)
+    allow(reporter_snapshot).to receive(:o_simple_hash).and_return({})
+
+    allow(subject).to receive(:pipeline_report_snapshot).and_wrap_original do |m, *args|
+      report_count += 1
+      m.call(*args)
+    end
+  end
+
+  after :each do
+    report_count = 0
+  end
+
+  context "when pipeline is stalled" do
+    let(:increasing_count) { (1..5000).to_a }
+    before :each do
+      allow(reporter_snapshot).to receive(:inflight_count).and_return(*increasing_count)
+      allow(reporter_snapshot).to receive(:stalling_threads) { { } }
+    end
+
+    describe ".unsafe_shutdown = true" do
+      let(:abort_threshold) { subject.abort_threshold }
+      let(:report_every) { subject.report_every }
+
+      before :each do
+        subject.class.unsafe_shutdown = true
+      end
+
+      it "should force the shutdown" do
+        expect(subject).to receive(:force_exit).once
+        subject.start
+      end
+
+      it "should do exactly \"abort_threshold\" stall checks" do
+        allow(subject).to receive(:force_exit)
+        expect(subject).to receive(:shutdown_stalled?).exactly(abort_threshold).times.and_call_original
+        subject.start
+      end
+
+      it "should do exactly \"abort_threshold\"*\"report_every\" stall checks" do
+        allow(subject).to receive(:force_exit)
+        expect(subject).to receive(:pipeline_report_snapshot).exactly(abort_threshold*report_every).times.and_call_original
+        subject.start
+      end
+    end
+
+    describe ".unsafe_shutdown = false" do
+
+      before :each do
+        subject.class.unsafe_shutdown = false
+      end
+
+      it "shouldn't force the shutdown" do
+        expect(subject).to_not receive(:force_exit)
+        thread = Thread.new(subject) {|subject| subject.start }
+        sleep 0.1 until report_count > check_threshold
+        thread.kill
+      end
+    end
+  end
+
+  context "when pipeline is not stalled" do
+    let(:decreasing_count) { (1..5000).to_a.reverse }
+    before :each do
+      allow(reporter_snapshot).to receive(:inflight_count).and_return(*decreasing_count)
+      allow(reporter_snapshot).to receive(:stalling_threads) { { } }
+    end
+
+    describe ".unsafe_shutdown = true" do
+
+      before :each do
+        subject.class.unsafe_shutdown = true
+      end
+
+      it "should force the shutdown" do
+        expect(subject).to_not receive(:force_exit)
+        thread = Thread.new(subject) {|subject| subject.start }
+        sleep 0.1 until report_count > check_threshold
+        thread.kill
+      end
+    end
+
+    describe ".unsafe_shutdown = false" do
+
+      before :each do
+        subject.class.unsafe_shutdown = false
+      end
+
+      it "shouldn't force the shutdown" do
+        expect(subject).to_not receive(:force_exit)
+        thread = Thread.new(subject) {|subject| subject.start }
+        sleep 0.1 until report_count > check_threshold
+        thread.kill
+      end
+    end
+  end
+end
diff --git a/spec/util/buftok_spec.rb b/logstash-core/spec/logstash/util/buftok_spec.rb
similarity index 100%
rename from spec/util/buftok_spec.rb
rename to logstash-core/spec/logstash/util/buftok_spec.rb
diff --git a/spec/util/charset_spec.rb b/logstash-core/spec/logstash/util/charset_spec.rb
similarity index 100%
rename from spec/util/charset_spec.rb
rename to logstash-core/spec/logstash/util/charset_spec.rb
diff --git a/logstash-core/spec/logstash/util/duration_formatter_spec.rb b/logstash-core/spec/logstash/util/duration_formatter_spec.rb
new file mode 100644
index 00000000000..44c0eb64632
--- /dev/null
+++ b/logstash-core/spec/logstash/util/duration_formatter_spec.rb
@@ -0,0 +1,11 @@
+# encoding: utf-8
+require "logstash/util/duration_formatter"
+require "spec_helper"
+
+describe LogStash::Util::DurationFormatter do
+  let(:duration) { 3600 * 1000 } # in milliseconds
+
+  it "returns a human format" do
+    expect(subject.human_format(duration)).to eq("1h")
+  end
+end
diff --git a/spec/util/java_version_spec.rb b/logstash-core/spec/logstash/util/java_version_spec.rb
similarity index 89%
rename from spec/util/java_version_spec.rb
rename to logstash-core/spec/logstash/util/java_version_spec.rb
index ca73860b363..930ce064d5c 100644
--- a/spec/util/java_version_spec.rb
+++ b/logstash-core/spec/logstash/util/java_version_spec.rb
@@ -19,10 +19,18 @@
     expect(mod.bad_java_version?("1.6.0")).to be_truthy
   end
 
+  it "should mark java 7 version as bad" do
+    expect(mod.bad_java_version?("1.7.0_51")).to be_truthy
+  end
+  
+  it "should mark java version 8 as good" do
+    expect(mod.bad_java_version?("1.8.0")).to be_falsey
+  end
+  
   it "should mark a good standard java version as good" do
-    expect(mod.bad_java_version?("1.7.0_51")).to be_falsey
+    expect(mod.bad_java_version?("1.8.0_65")).to be_falsey
   end
-
+  
   it "should mark a good beta version as good" do
     expect(mod.bad_java_version?("1.8.0-beta")).to be_falsey
   end
diff --git a/spec/util/plugin_version_spec.rb b/logstash-core/spec/logstash/util/plugin_version_spec.rb
similarity index 100%
rename from spec/util/plugin_version_spec.rb
rename to logstash-core/spec/logstash/util/plugin_version_spec.rb
diff --git a/spec/util/unicode_trimmer_spec.rb b/logstash-core/spec/logstash/util/unicode_trimmer_spec.rb
similarity index 100%
rename from spec/util/unicode_trimmer_spec.rb
rename to logstash-core/spec/logstash/util/unicode_trimmer_spec.rb
diff --git a/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
new file mode 100644
index 00000000000..871952482aa
--- /dev/null
+++ b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/util/wrapped_synchronous_queue"
+
+describe LogStash::Util::WrappedSynchronousQueue do
+ context "#offer" do
+   context "queue is blocked" do
+     it "fails and give feedback" do
+       expect(subject.offer("Bonjour", 2)).to be_falsey
+     end
+   end
+
+   context "queue is not blocked" do
+     before do
+       @consumer = Thread.new { loop { subject.take } }
+       sleep(0.1)
+     end
+
+     after do
+       @consumer.kill
+     end
+     
+     it "inserts successfully" do
+       expect(subject.offer("Bonjour", 20)).to be_truthy
+     end
+   end
+ end
+end
diff --git a/spec/util_spec.rb b/logstash-core/spec/logstash/util_spec.rb
similarity index 68%
rename from spec/util_spec.rb
rename to logstash-core/spec/logstash/util_spec.rb
index 82e75092675..4cd56139fbf 100644
--- a/spec/util_spec.rb
+++ b/logstash-core/spec/logstash/util_spec.rb
@@ -3,8 +3,18 @@
 
 require "logstash/util"
 
+class ClassNameTest
+end
+
+module TestingClassName
+  class TestKlass
+  end
+end
+
 describe LogStash::Util do
 
+  subject { described_class }
+
   context "stringify_keys" do
     it "should convert hash symbol keys to strings" do
       expect(LogStash::Util.stringify_symbols({:a => 1, "b" => 2})).to eq({"a" => 1, "b" => 2})
@@ -32,4 +42,22 @@
       expect(LogStash::Util.stringify_symbols([:a, [1, :b]])).to eq(["a", [1, "b"]])
     end
   end
+
+  describe ".class_name" do
+    context "when the class is a top level class" do
+      let(:klass) { ClassNameTest.new }
+
+      it "returns the name of the class" do
+        expect(subject.class_name(klass)).to eq("ClassNameTest")
+      end
+    end
+
+    context "when the class is nested inside modules" do
+      let(:klass) { TestingClassName::TestKlass.new }
+
+      it "returns the name of the class" do
+        expect(subject.class_name(klass)).to eq("TestKlass")
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/static/i18n_spec.rb b/logstash-core/spec/static/i18n_spec.rb
new file mode 100644
index 00000000000..b2cd76377d2
--- /dev/null
+++ b/logstash-core/spec/static/i18n_spec.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require "spec_helper"
+require "i18n"
+
+I18N_T_REGEX = Regexp.new('I18n.t.+?"(.+?)"')
+
+describe I18n do
+  context "when using en.yml" do
+    glob_path = File.join(LogStash::Environment::LOGSTASH_HOME, "logstash-*", "lib", "**", "*.rb")
+
+    Dir.glob(glob_path).each do |file_name|
+
+      context "in file \"#{file_name}\"" do
+        File.foreach(file_name) do |line|
+          next unless (match = line.match(I18N_T_REGEX))
+          line = $INPUT_LINE_NUMBER
+          key = match[1]
+          it "in line #{line} the \"#{key}\" key should exist" do
+            expect(I18n.exists?(key)).to be_truthy
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/support/matchers.rb b/logstash-core/spec/support/matchers.rb
new file mode 100644
index 00000000000..88ea508b02d
--- /dev/null
+++ b/logstash-core/spec/support/matchers.rb
@@ -0,0 +1,30 @@
+# encoding: utf-8
+require "rspec"
+require "rspec/expectations"
+
+RSpec::Matchers.define :be_a_metric_event do |namespace, type, *args|
+  match do
+    namespace == Array(actual[0]).concat(Array(actual[1])) &&
+      type == actual[2] &&
+      args == actual[3..-1]
+  end
+end
+
+# Match to test `NullObject` pattern
+RSpec::Matchers.define :implement_interface_of do |type, key, value|
+  match do |actual|
+    all_instance_methods_implemented?
+  end
+
+  def missing_methods
+    expected.instance_methods.select { |method| !actual.instance_methods.include?(method) }
+  end
+
+  def all_instance_methods_implemented?
+    expected.instance_methods.all? { |method| actual.instance_methods.include?(method) }
+  end
+
+  failure_message do
+    "Expecting `#{expected}` to implements instance methods of `#{actual}`, missing methods: #{missing_methods.join(",")}"
+  end
+end
diff --git a/logstash-core/spec/support/mocks_classes.rb b/logstash-core/spec/support/mocks_classes.rb
new file mode 100644
index 00000000000..c481e8be21e
--- /dev/null
+++ b/logstash-core/spec/support/mocks_classes.rb
@@ -0,0 +1,26 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+
+class DummyOutput < LogStash::Outputs::Base
+  config_name "dummyoutput"
+  milestone 2
+
+  attr_reader :num_closes, :events
+
+  def initialize(params={})
+    super
+    @num_closes = 0
+    @events = []
+  end
+
+  def register
+  end
+
+  def receive(event)
+    @events << event
+  end
+
+  def close
+    @num_closes = 1
+  end
+end
diff --git a/logstash-event.gemspec b/logstash-event.gemspec
deleted file mode 100644
index ea6cce87e1a..00000000000
--- a/logstash-event.gemspec
+++ /dev/null
@@ -1,41 +0,0 @@
-# -*- encoding: utf-8 -*-
-Gem::Specification.new do |gem|
-  gem.authors       = ["Jordan Sissel"]
-  gem.email         = ["jls@semicomplete.com"]
-  gem.description   = %q{Library that contains the classes required to create LogStash events}
-  gem.summary       = %q{Library that contains the classes required to create LogStash events}
-  gem.homepage      = "https://github.com/logstash/logstash"
-  gem.license       = "Apache License (2.0)"
-
-  gem.files = %w{
-    lib/logstash-event.rb
-    lib/logstash/environment.rb
-    lib/logstash/errors.rb
-    lib/logstash/event.rb
-    lib/logstash/java_integration.rb
-    lib/logstash/json.rb
-    lib/logstash/namespace.rb
-    lib/logstash/timestamp.rb
-    lib/logstash/version.rb
-    lib/logstash/util.rb
-    lib/logstash/util/accessors.rb
-    LICENSE
-  }
-
-  gem.test_files    = ["spec/core/event_spec.rb"]
-  gem.name          = "logstash-event"
-  gem.require_paths = ["lib"]
-  gem.version       = "1.3.0"
-
-  gem.add_runtime_dependency "cabin"
-  gem.add_development_dependency "rspec"
-  gem.add_development_dependency "guard"
-  gem.add_development_dependency "guard-rspec"
-
-  if RUBY_PLATFORM == 'java'
-    gem.platform = RUBY_PLATFORM
-    gem.add_runtime_dependency "jrjackson"
-  else
-    gem.add_runtime_dependency "oj"
-  end
-end
diff --git a/pkg/centos/after-install.sh b/pkg/centos/after-install.sh
index 224904e4b32..1c4e7d6a6b7 100644
--- a/pkg/centos/after-install.sh
+++ b/pkg/centos/after-install.sh
@@ -1,6 +1,9 @@
-/sbin/chkconfig --add logstash
-
-chown -R logstash:logstash /opt/logstash
+chown -R logstash:logstash /usr/share/logstash
 chown logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
 chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.log:|path.log: /var/log/logstash/logstash.log|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/centos/before-install.sh b/pkg/centos/before-install.sh
index 5a852488ff3..78fc0b77d49 100644
--- a/pkg/centos/before-install.sh
+++ b/pkg/centos/before-install.sh
@@ -5,6 +5,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -r -g logstash -d /opt/logstash \
+  useradd -r -g logstash -d /usr/share/logstash \
     -s /sbin/nologin -c "logstash" logstash
 fi
diff --git a/pkg/centos/before-remove.sh b/pkg/centos/before-remove.sh
index 5109888475f..6687ee896e5 100644
--- a/pkg/centos/before-remove.sh
+++ b/pkg/centos/before-remove.sh
@@ -1,6 +1,32 @@
+# CentOS/RHEL and SuSE
 if [ $1 -eq 0 ]; then
-  /sbin/service logstash stop >/dev/null 2>&1 || true
-  /sbin/chkconfig --del logstash
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
+
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
diff --git a/pkg/debian/after-install.sh b/pkg/debian/after-install.sh
index 18b4ea8c32c..362a4ea1e58 100644
--- a/pkg/debian/after-install.sh
+++ b/pkg/debian/after-install.sh
@@ -1,7 +1,12 @@
 #!/bin/sh
 
-chown -R logstash:logstash /opt/logstash
+chown -R logstash:logstash /usr/share/logstash
 chown logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
 chmod 755 /etc/logstash
 chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.log:|path.log: /var/log/logstash/logstash.log|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/debian/before-install.sh b/pkg/debian/before-install.sh
index 45ef4e40f1f..03cf86125a9 100644
--- a/pkg/debian/before-install.sh
+++ b/pkg/debian/before-install.sh
@@ -7,6 +7,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -M -r -g logstash -d /var/lib/logstash \
+  useradd -M -r -g logstash -d /usr/share/logstash \
     -s /usr/sbin/nologin -c "LogStash Service User" logstash
 fi
diff --git a/pkg/debian/before-remove.sh b/pkg/debian/before-remove.sh
index a3f911e60ea..16347f266fc 100644
--- a/pkg/debian/before-remove.sh
+++ b/pkg/debian/before-remove.sh
@@ -1,13 +1,38 @@
 #!/bin/sh
-
+# Debian
 if [ $1 = "remove" ]; then
-  service logstash stop >/dev/null 2>&1 || true
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /usr/sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
 
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
 
-  if getent group logstash >/dev/null ; then
+  if getent group logstash > /dev/null ; then
     groupdel logstash
   fi
 fi
diff --git a/pkg/jvm.options b/pkg/jvm.options
new file mode 100644
index 00000000000..2568d6d4f5a
--- /dev/null
+++ b/pkg/jvm.options
@@ -0,0 +1,74 @@
+## JVM configuration
+
+# Xms represents the initial size of total heap space
+# Xmx represents the maximum size of total heap space
+
+-Xms256m
+-Xmx1g
+
+################################################################
+## Expert settings
+################################################################
+##
+## All settings below this section are considered
+## expert settings. Don't tamper with them unless
+## you understand what you are doing
+##
+################################################################
+
+## GC configuration
+-XX:+UseParNewGC
+-XX:+UseConcMarkSweepGC
+-XX:CMSInitiatingOccupancyFraction=75
+-XX:+UseCMSInitiatingOccupancyOnly
+
+## optimizations
+
+# disable calls to System#gc
+-XX:+DisableExplicitGC
+
+## locale
+# Set the locale language
+#-Duser.language=en
+
+# Set the locale country
+#-Duser.country=US
+
+# Set the locale variant, if any
+#-Duser.variant=
+
+## basic
+
+# set the I/O temp directory
+#-Djava.io.tmpdir=$HOME
+
+# set to headless, just in case
+-Djava.awt.headless=true
+
+# ensure UTF-8 encoding by default (e.g. filenames)
+-Dfile.encoding=UTF-8
+
+# use our provided JNA always versus the system one
+#-Djna.nosys=true
+
+## heap dumps
+
+# generate a heap dump when an allocation from the Java heap fails
+# heap dumps are created in the working directory of the JVM
+-XX:+HeapDumpOnOutOfMemoryError
+
+# specify an alternative path for heap dumps
+# ensure the directory exists and has sufficient space
+#-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof
+
+## GC logging
+#-XX:+PrintGCDetails
+#-XX:+PrintGCTimeStamps
+#-XX:+PrintGCDateStamps
+#-XX:+PrintClassHistogram
+#-XX:+PrintTenuringDistribution
+#-XX:+PrintGCApplicationStoppedTime
+
+# log GC status to a file with time stamps
+# ensure the directory exists
+#-Xloggc:${LS_GC_LOG_FILE}
diff --git a/pkg/logstash.default b/pkg/logstash.default
deleted file mode 100644
index bf8ab6ca7df..00000000000
--- a/pkg/logstash.default
+++ /dev/null
@@ -1,40 +0,0 @@
-###############################
-# Default settings for logstash
-###############################
-
-# Override Java location
-#JAVACMD=/usr/bin/java
-
-# Set a home directory
-#LS_HOME=/var/lib/logstash
-
-# Arguments to pass to logstash agent
-#LS_OPTS=""
-
-# Arguments to pass to java
-#LS_HEAP_SIZE="500m"
-#LS_JAVA_OPTS="-Djava.io.tmpdir=$HOME"
-
-# pidfiles aren't used for upstart; this is for sysv users.
-#LS_PIDFILE=/var/run/logstash.pid
-
-# user id to be invoked as; for upstart: edit /etc/init/logstash.conf
-#LS_USER=logstash
-
-# logstash logging
-#LS_LOG_FILE=/var/log/logstash/logstash.log
-#LS_USE_GC_LOGGING="true"
-
-# logstash configuration directory
-#LS_CONF_DIR=/etc/logstash/conf.d
-
-# Open file limit; cannot be overridden in upstart
-#LS_OPEN_FILES=16384
-
-# Nice level
-#LS_NICE=19
-
-# If this is set to 1, then when `stop` is called, if the process has
-# not exited within a reasonable time, SIGKILL will be sent next.
-# The default behavior is to simply log a message "program stop failed; still running"
-KILL_ON_STOP_TIMEOUT=0
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
deleted file mode 100755
index fdfc8902ac4..00000000000
--- a/pkg/logstash.sysv
+++ /dev/null
@@ -1,199 +0,0 @@
-#!/bin/sh
-# Init script for logstash
-# Maintained by Elasticsearch
-# Generated by pleaserun.
-# Implemented based on LSB Core 3.1:
-#   * Sections: 20.2, 20.3
-#
-### BEGIN INIT INFO
-# Provides:          logstash
-# Required-Start:    $remote_fs $syslog
-# Required-Stop:     $remote_fs $syslog
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description:
-# Description:        Starts Logstash as a daemon.
-### END INIT INFO
-
-PATH=/sbin:/usr/sbin:/bin:/usr/bin
-export PATH
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-name=logstash
-pidfile="/var/run/$name.pid"
-
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="500m"
-LS_LOG_DIR=/var/log/logstash
-LS_LOG_FILE="${LS_LOG_DIR}/$name.log"
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-
-
-[ -r /etc/default/$name ] && . /etc/default/$name
-[ -r /etc/sysconfig/$name ] && . /etc/sysconfig/$name
-
-program=/opt/logstash/bin/logstash
-args="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-quiet() {
-  "$@" > /dev/null 2>&1
-  return $?
-}
-
-start() {
-
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  HOME=${LS_HOME}
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
-
-  # chown doesn't grab the suplimental groups when setting the user:group - so we have to do it for it.
-  # Boy, I hope we're root here.
-  SGROUPS=$(id -Gn "$LS_USER" | tr " " "," | sed 's/,$//'; echo '')
-
-  if [ ! -z $SGROUPS ]
-  then
-	EXTRA_GROUPS="--groups $SGROUPS"
-  fi
-
-  # set ulimit as (root, presumably) first, before we drop privileges
-  ulimit -n ${LS_OPEN_FILES}
-
-  # Run the program!
-  nice -n ${LS_NICE} chroot --userspec $LS_USER:$LS_GROUP $EXTRA_GROUPS / sh -c "
-    cd $LS_HOME
-    ulimit -n ${LS_OPEN_FILES}
-    exec \"$program\" $args
-  " > "${LS_LOG_DIR}/$name.stdout" 2> "${LS_LOG_DIR}/$name.err" &
-
-  # Generate the pidfile from here. If we instead made the forked process
-  # generate it there will be a race condition between the pidfile writing
-  # and a process possibly asking for status.
-  echo $! > $pidfile
-
-  echo "$name started."
-  return 0
-}
-
-stop() {
-  # Try a few times to kill TERM the program
-  if status ; then
-    pid=`cat "$pidfile"`
-    echo "Killing $name (pid $pid) with SIGTERM"
-    kill -TERM $pid
-    # Wait for it to exit.
-    for i in 1 2 3 4 5 ; do
-      echo "Waiting $name (pid $pid) to die..."
-      status || break
-      sleep 1
-    done
-    if status ; then
-      if [ "$KILL_ON_STOP_TIMEOUT" -eq 1 ] ; then
-        echo "Timeout reached. Killing $name (pid $pid) with SIGKILL. This may result in data loss."
-        kill -KILL $pid
-        echo "$name killed with SIGKILL."
-      else
-        echo "$name stop failed; still running."
-      fi
-    else
-      echo "$name stopped."
-    fi
-  fi
-}
-
-status() {
-  if [ -f "$pidfile" ] ; then
-    pid=`cat "$pidfile"`
-    if kill -0 $pid > /dev/null 2> /dev/null ; then
-      # process by this pid is running.
-      # It may not be our pid, but that's what you get with just pidfiles.
-      # TODO(sissel): Check if this process seems to be the same as the one we
-      # expect. It'd be nice to use flock here, but flock uses fork, not exec,
-      # so it makes it quite awkward to use in this case.
-      return 0
-    else
-      return 2 # program is dead but pid file exists
-    fi
-  else
-    return 3 # program is not running
-  fi
-}
-
-force_stop() {
-  if status ; then
-    stop
-    status && kill -KILL `cat "$pidfile"`
-  fi
-}
-
-configtest() {
-  # Check if a config file exists
-  if [ ! "$(ls -A ${LS_CONF_DIR}/* 2> /dev/null)" ]; then
-    log_failure_msg "There aren't any configuration files in ${LS_CONF_DIR}"
-    exit 1
-  fi
-
-  JAVA_OPTS=${LS_JAVA_OPTS}
-  HOME=${LS_HOME}
-  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
-
-  test_args="-f ${LS_CONF_DIR} --configtest ${LS_OPTS}"
-  $program ${test_args}
-  [ $? -eq 0 ] && return 0
-  # Program not configured
-  return 6
-}
-
-case "$1" in
-  start)
-    status
-    code=$?
-    if [ $code -eq 0 ]; then
-      echo "$name is already running"
-    else
-      start
-      code=$?
-    fi
-    exit $code
-    ;;
-  stop) stop ;;
-  force-stop) force_stop ;;
-  status)
-    status
-    code=$?
-    if [ $code -eq 0 ] ; then
-      echo "$name is running"
-    else
-      echo "$name is not running"
-    fi
-    exit $code
-    ;;
-  restart)
-
-    quiet configtest
-    RET=$?
-    if [ ${RET} -ne 0 ]; then
-      echo "Configuration error. Not restarting. Re-run with configtest parameter for details"
-      exit ${RET}
-    fi
-    stop && start
-    ;;
-  configtest)
-    configtest
-    exit $?
-    ;;
-  *)
-    echo "Usage: $SCRIPTNAME {start|stop|force-stop|status|restart|configtest}" >&2
-    exit 3
-  ;;
-esac
-
-exit $?
diff --git a/pkg/logstash.sysv.debian b/pkg/logstash.sysv.debian
deleted file mode 100644
index 5795cdb991e..00000000000
--- a/pkg/logstash.sysv.debian
+++ /dev/null
@@ -1,181 +0,0 @@
-#!/bin/bash
-#
-# /etc/init.d/logstash -- startup script for LogStash.
-#
-### BEGIN INIT INFO
-# Provides:          logstash
-# Required-Start:    $all
-# Required-Stop:     $all
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description: Starts logstash
-# Description:       Starts logstash using start-stop-daemon
-### END INIT INFO
-
-set -e
-
-NAME=logstash
-DESC="Logstash Daemon"
-DEFAULT=/etc/default/$NAME
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-. /lib/lsb/init-functions
-
-if [ -r /etc/default/rcS ]; then
-   . /etc/default/rcS
-fi
-
-# The following variables can be overwritten in $DEFAULT
-PATH=/bin:/usr/bin:/sbin:/usr/sbin
-
-# See contents of file named in $DEFAULT for comments
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="500m"
-LS_LOG_FILE=/var/log/logstash/$NAME.log
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-LS_PIDFILE=/var/run/$NAME.pid
-
-# End of variables that can be overwritten in $DEFAULT
-
-# overwrite settings from default file
-if [ -f "$DEFAULT" ]; then
-   . "$DEFAULT"
-fi
-
-# Define other required variables
-PID_FILE=${LS_PIDFILE}
-DAEMON=/opt/logstash/bin/logstash
-DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-# Check DAEMON exists
-if ! test -e $DAEMON; then
-   log_failure_msg "Script $DAEMON doesn't exist"
-   exit 1
-fi
-
-case "$1" in
-   start)
-      if [ -z "$DAEMON" ]; then
-         log_failure_msg "no logstash script found - $DAEMON"
-         exit 1
-      fi
-
-      # Check if a config file exists
-      if [ ! "$(ls -A $LS_CONF_DIR/*.conf 2> /dev/null)" ]; then
-         log_failure_msg "There aren't any configuration files in $LS_CONF_DIR"
-         exit 1
-      fi
-
-      log_daemon_msg "Starting $DESC"
-
-      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-      if start-stop-daemon --test --start --pidfile "$PID_FILE" \
-         --user "$LS_USER" --exec "$JAVA" \
-      >/dev/null; then
-         # Prepare environment
-         HOME="${HOME:-$LS_HOME}"
-         LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-         ulimit -n ${LS_OPEN_FILES}
-	 cd "${LS_HOME}"
-         export PATH HOME JAVACMD LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
-
-         # Start Daemon
-         start-stop-daemon --start -b --user "$LS_USER" -c "$LS_USER":"$LS_GROUP" \
-           -d "$LS_HOME" --nicelevel "$LS_NICE" --pidfile "$PID_FILE" --make-pidfile \
-           --exec $DAEMON -- $DAEMON_OPTS
-
-         sleep 1
-
-         # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-         JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-         if start-stop-daemon --test --start --pidfile "$PID_FILE" \
-             --user "$LS_USER" --exec "$JAVA" \
-         >/dev/null; then
-
-            if [ -f "$PID_FILE" ]; then
-               rm -f "$PID_FILE"
-            fi
-
-            log_end_msg 1
-         else
-            log_end_msg 0
-         fi
-      else
-         log_progress_msg "(already running)"
-         log_end_msg 0
-      fi
-   ;;
-   stop)
-      log_daemon_msg "Stopping $DESC"
-
-      set +e
-
-      if [ -f "$PID_FILE" ]; then
-         start-stop-daemon --stop --pidfile "$PID_FILE" \
-            --user "$LS_USER" \
-            --retry=TERM/20/KILL/5 >/dev/null
-
-         if [ $? -eq 1 ]; then
-            log_progress_msg "$DESC is not running but pid file exists, cleaning up"
-         elif [ $? -eq 3 ]; then
-            PID="`cat $PID_FILE`"
-            log_failure_msg "Failed to stop $DESC (pid $PID)"
-            exit 1
-         fi
-
-         rm -f "$PID_FILE"
-      else
-         log_progress_msg "(not running)"
-      fi
-
-      log_end_msg 0
-      set -e
-   ;;
-   status)
-      set +e
-
-      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-      start-stop-daemon --test --start --pidfile "$PID_FILE" \
-         --user "$LS_USER" --exec "$JAVA" \
-      >/dev/null 2>&1
-
-      if [ "$?" = "0" ]; then
-         if [ -f "$PID_FILE" ]; then
-            log_success_msg "$DESC is not running, but pid file exists."
-            exit 1
-         else
-            log_success_msg "$DESC is not running."
-            exit 3
-         fi
-      else
-         log_success_msg "$DESC is running with pid `cat $PID_FILE`"
-      fi
-
-      set -e
-   ;;
-   restart|force-reload)
-      if [ -f "$PID_FILE" ]; then
-         $0 stop
-         sleep 1
-      fi
-
-      $0 start
-   ;;
-   *)
-      log_success_msg "Usage: $0 {start|stop|restart|force-reload|status}"
-      exit 1
-   ;;
-esac
-
-exit 0
diff --git a/pkg/logstash.sysv.redhat b/pkg/logstash.sysv.redhat
deleted file mode 100755
index c228e355e9b..00000000000
--- a/pkg/logstash.sysv.redhat
+++ /dev/null
@@ -1,132 +0,0 @@
-#! /bin/sh
-#
-#       /etc/rc.d/init.d/logstash
-#
-#       Starts Logstash as a daemon
-#
-# chkconfig: 2345 90 10
-# description: Starts Logstash as a daemon.
-
-### BEGIN INIT INFO
-# Provides: logstash
-# Required-Start: $local_fs $remote_fs
-# Required-Stop: $local_fs $remote_fs
-# Default-Start: 2 3 4 5
-# Default-Stop: S 0 1 6
-# Short-Description: Logstash
-# Description: Starts Logstash as a daemon.
-### END INIT INFO
-
-. /etc/rc.d/init.d/functions
-
-NAME=logstash
-DESC="Logstash Daemon"
-DEFAULT=/etc/sysconfig/$NAME
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-# The following variables can be overwritten in $DEFAULT
-PATH=/bin:/usr/bin:/sbin:/usr/sbin
-
-# See contents of file named in $DEFAULT for comments
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="500m"
-LS_LOG_FILE=/var/log/logstash/$NAME.log
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-LS_PIDFILE=/var/run/$NAME/$NAME.pid
-
-# End of variables that can be overwritten in $DEFAULT
-
-if [ -f "$DEFAULT" ]; then
-  . "$DEFAULT"
-fi
-
-# Define other required variables
-PID_FILE=${LS_PIDFILE}
-
-DAEMON="/opt/logstash/bin/logstash"
-DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-#
-# Function that starts the daemon/service
-#
-do_start()
-{
-
-  if [ -z "$DAEMON" ]; then
-    echo "not found - $DAEMON"
-    exit 1
-  fi
-
-  if pidofproc -p "$PID_FILE" >/dev/null; then
-    exit 0
-  fi
-
-  # Prepare environment
-  HOME="${HOME:-$LS_HOME}"
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  ulimit -n ${LS_OPEN_FILES}
-  cd "${LS_HOME}"
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
-  test -n "${JAVACMD}" && export JAVACMD
-
-  nice -n ${LS_NICE} runuser -s /bin/sh -c "exec $DAEMON $DAEMON_OPTS" ${LS_USER} >> $LS_LOG_FILE 2>&1 < /dev/null &
-
-  RETVAL=$?
-  local PID=$!
-  # runuser forks rather than execing our process.
-  usleep 500000
-  JAVA_PID=$(ps axo ppid,pid | awk -v "ppid=$PID" '$1==ppid {print $2}')
-  PID=${JAVA_PID:-$PID}
-  echo $PID > $PID_FILE
-  [ "$PID" = "$JAVA_PID" ] && success
-}
-
-#
-# Function that stops the daemon/service
-#
-do_stop()
-{
-    killproc -p $PID_FILE $DAEMON
-    RETVAL=$?
-    echo
-    [ $RETVAL = 0 ] && rm -f ${PID_FILE}
-}
-
-case "$1" in
-  start)
-    echo -n "Starting $DESC: "
-    do_start
-    touch /var/run/logstash/$NAME
-    ;;
-  stop)
-    echo -n "Stopping $DESC: "
-    do_stop
-    rm /var/run/logstash/$NAME
-    ;;
-  restart|reload)
-    echo -n "Restarting $DESC: "
-    do_stop
-    do_start
-    ;;
-  status)
-    echo -n "$DESC"
-    status -p $PID_FILE
-    exit $?
-    ;;
-  *)
-    echo "Usage: $SCRIPTNAME {start|stop|status|restart}" >&2
-    exit 3
-    ;;
-esac
-
-echo
-exit 0
diff --git a/pkg/logstash.upstart.ubuntu b/pkg/logstash.upstart.ubuntu
deleted file mode 100644
index 68730ffe949..00000000000
--- a/pkg/logstash.upstart.ubuntu
+++ /dev/null
@@ -1,47 +0,0 @@
-# logstash - agent instance
-#
-
-description     "logstash agent"
-
-start on virtual-filesystems
-stop on runlevel [06]
-
-# Respawn it if the process exits
-respawn
-
-# We're setting high here, we'll re-limit below.
-limit nofile 65550 65550
-
-setuid logstash
-setgid logstash
-
-# You need to chdir somewhere writable because logstash needs to unpack a few
-# temporary files on startup.
-console log
-script
-  # Defaults
-  PATH=/bin:/usr/bin
-  LS_HOME=/var/lib/logstash
-  LS_HEAP_SIZE="500m"
-  LS_LOG_FILE=/var/log/logstash/logstash.log
-  LS_USE_GC_LOGGING=""
-  LS_CONF_DIR=/etc/logstash/conf.d
-  LS_OPEN_FILES=16384
-  LS_NICE=19
-  LS_OPTS=""
-
-  # Override our defaults with user defaults:
-  [ -f /etc/default/logstash ] && . /etc/default/logstash
-
-  HOME="${HOME:-$LS_HOME}"
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  # Reset filehandle limit
-  ulimit -n ${LS_OPEN_FILES}
-  cd "${LS_HOME}"
-
-  # Export variables
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
-  test -n "${JAVACMD}" && export JAVACMD
-
-  exec nice -n ${LS_NICE} /opt/logstash/bin/logstash agent -f "${LS_CONF_DIR}" -l "${LS_LOG_FILE}" ${LS_OPTS}
-end script
diff --git a/pkg/logstash.yml b/pkg/logstash.yml
new file mode 100644
index 00000000000..a04760246a6
--- /dev/null
+++ b/pkg/logstash.yml
@@ -0,0 +1,106 @@
+# Settings file in YAML
+#
+# Settings can be specified either in hierarchical form, e.g.:
+#
+#   pipeline:
+#     batch:
+#       size: 125
+#       delay: 5
+#
+# Or as flat keys:
+#
+#   pipeline.batch.size: 125
+#   pipeline.batch.delay: 5
+#
+# ------------  Node identity ------------
+#
+# Use a descriptive name for the node:
+#
+# node.name: test
+#
+# If omitted the node name will default to the machine's host name
+#
+# ------------ Pipeline Settings --------------
+#
+# Set the number of workers that will, in parallel, execute the filters+outputs
+# stage of the pipeline.
+#
+# This defaults to half the number of the host's CPU cores.
+#
+# pipeline.workers: 2
+#
+# How many workers should be used per output plugin instance
+#
+# pipeline.output.workers: 1
+#
+# How many events to retrieve from inputs before sending to filters+workers
+#
+# pipeline.batch.size: 125
+#
+# How long to wait before dispatching an undersized batch to filters+workers
+# Value is in seconds.
+#
+# pipeline.batch.delay: 5
+#
+# Force Logstash to exit during shutdown even if there are still inflight
+# events in memory. By default, logstash will refuse to quit until all
+# received events have been pushed to the outputs.
+#
+# WARNING: enabling this can lead to data loss during shutdown
+#
+# pipeline.unsafe_shutdown: false
+#
+# ------------ Pipeline Configuration Settings --------------
+#
+# Where to fetch the pipeline configuration for the main pipeline
+#
+# path.config:
+#
+# Pipeline configuration string for the main pipeline
+#
+# config.string:
+#
+# At startup, test if the configuration is valid and exit (dry run)
+#
+# config.test_and_exit: false
+#
+# Periodically check if the configuration has changed and reload the pipeline
+# This can also be triggered manually through the SIGHUP signal
+#
+# config.reload.automatic: false
+#
+# How often to check if the pipeline configuration has changed (in seconds)
+#
+# config.reload.interval: 3
+#
+# Show fully compiled configuration as debug log message
+# NOTE: --log.level must be 'debug'
+#
+# config.debug: false
+#
+# ------------ Metrics Settings --------------
+#
+# Bind address for the metrics REST endpoint
+#
+# http.host: "127.0.0.1"
+#
+# Bind port for the metrics REST endpoint
+#
+# http.port: 9600
+#
+# ------------ Debugging Settings --------------
+#
+# Options for log.level:
+#   * warn => warn (default)
+#   * quiet => error
+#   * verbose => info
+#   * debug => debug
+#
+# log.level: warn
+# log.format: plain (or 'json')
+# path.log:
+#
+# ------------ Other Settings --------------
+#
+# Where to find custom plugins
+# path.plugins: []
diff --git a/pkg/startup.options b/pkg/startup.options
new file mode 100644
index 00000000000..dcb850e66df
--- /dev/null
+++ b/pkg/startup.options
@@ -0,0 +1,52 @@
+################################################################################
+# These settings are ONLY used by $LS_HOME/bin/system-install to create a custom
+# startup script for Logstash.  It should automagically use the init system
+# (systemd, upstart, sysv, etc.) that your Linux distribution uses.
+#
+# After changing anything here, you need to re-run $LS_HOME/bin/system-install
+# as root to push the changes to the init script.
+################################################################################
+
+# Override Java location
+JAVACMD=/usr/bin/java
+
+# Set a home directory
+LS_HOME=/usr/share/logstash
+
+# logstash settings directory, the path which contains logstash.yml
+LS_SETTINGS_DIR=/etc/logstash
+
+# Arguments to pass to logstash
+LS_OPTS="--path.settings ${LS_SETTINGS_DIR}"
+
+# Arguments to pass to java
+LS_JAVA_OPTS=""
+
+# pidfiles aren't used the same way for upstart and systemd; this is for sysv users.
+LS_PIDFILE=/var/run/logstash.pid
+
+# user and group id to be invoked as
+LS_USER=logstash
+LS_GROUP=logstash
+
+# Enable GC logging by uncommenting the appropriate lines in the GC logging
+# section in jvm.options
+LS_GC_LOG_FILE=/var/log/logstash/gc.log
+
+# Open file limit
+LS_OPEN_FILES=16384
+
+# Nice level
+LS_NICE=19
+
+# Change these to have the init script named and described differently
+# This is useful when running multiple instances of Logstash on the same
+# physical box or vm
+SERVICE_NAME="logstash"
+SERVICE_DESCRIPTION="logstash"
+
+# If you need to run a command or script before launching Logstash, put it
+# between the lines beginning with `read` and `EOM`, and uncomment those lines.
+###
+## read -r -d '' PRESTART << EOM
+## EOM
diff --git a/pkg/ubuntu/after-install.sh b/pkg/ubuntu/after-install.sh
index bcecadf8af7..d4827cb7e3a 100644
--- a/pkg/ubuntu/after-install.sh
+++ b/pkg/ubuntu/after-install.sh
@@ -1,6 +1,11 @@
 #!/bin/sh
 
-chown -R logstash:logstash /opt/logstash
+chown -R logstash:logstash /usr/share/logstash
 chown logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
 chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.log:|path.log: /var/log/logstash/logstash.log|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/ubuntu/before-install.sh b/pkg/ubuntu/before-install.sh
index 45ef4e40f1f..03cf86125a9 100644
--- a/pkg/ubuntu/before-install.sh
+++ b/pkg/ubuntu/before-install.sh
@@ -7,6 +7,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -M -r -g logstash -d /var/lib/logstash \
+  useradd -M -r -g logstash -d /usr/share/logstash \
     -s /usr/sbin/nologin -c "LogStash Service User" logstash
 fi
diff --git a/pkg/ubuntu/before-remove.sh b/pkg/ubuntu/before-remove.sh
index a3f911e60ea..0384e74ffca 100644
--- a/pkg/ubuntu/before-remove.sh
+++ b/pkg/ubuntu/before-remove.sh
@@ -1,13 +1,38 @@
 #!/bin/sh
-
+# Ubuntu
 if [ $1 = "remove" ]; then
-  service logstash stop >/dev/null 2>&1 || true
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /usr/sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
 
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
 
-  if getent group logstash >/dev/null ; then
+  if getent group logstash > /dev/null ; then
     groupdel logstash
   fi
 fi
diff --git a/qa/.gitignore b/qa/.gitignore
new file mode 100644
index 00000000000..2fc713beaf3
--- /dev/null
+++ b/qa/.gitignore
@@ -0,0 +1,4 @@
+Gemfile.lock
+acceptance/.vagrant
+.vagrant
+.vm_ssh_config
diff --git a/qa/Gemfile b/qa/Gemfile
new file mode 100644
index 00000000000..1919a50f272
--- /dev/null
+++ b/qa/Gemfile
@@ -0,0 +1,5 @@
+source "https://rubygems.org"
+gem "runner-tool", :git => "https://github.com/purbon/runner-tool.git"
+gem "rspec", "~> 3.1.0"
+gem "rake"
+gem "pry", :group => :test
diff --git a/qa/README.md b/qa/README.md
new file mode 100644
index 00000000000..3d4b152d4ee
--- /dev/null
+++ b/qa/README.md
@@ -0,0 +1,198 @@
+## Acceptance test Framework
+
+Welcome to the acceptance test framework for logstash, in this small
+README we're going to describe it's features and the necessary steps you will need to
+follow to setup your environment.
+
+### Setup your environment
+
+In summary this test framework is composed of:
+
+* A collection of rspec helpers and matchers that make creating tests
+  easy.
+* This rspecs helpers execute commands over SSH to a set of machines.
+* The tests are run, for now, as vagrant (virtualbox provided) machines.
+
+As of this, you need to have installed:
+
+* The latest version vagrant (=> 1.8.1)
+* Virtualbox as VM provider (=> 5.0)
+
+Is important to notice that the first time you set everything up, or when a
+new VM is added, there is the need to download the box (this will
+take a while depending on your internet speed).
+
+### Running Tests
+
+It is possible to run the full suite of the acceptance test with the codebase by 
+running the command `ci/ci_acceptance.sh`, this command will generate the artifacts, bootstrap
+the VM and run the tests.
+
+
+This test are based on a collection of Vagrant defined VM's where the
+different test are going to be executed, so first setup necessary is to
+have vagrant properly available, see https://www.vagrantup.com/ for
+details on how to install it.
+
+_Inside the `qa` directory_
+
+First of all execute the command `bundle` this will pull the necessary
+dependencies in your environment, after this is done, this is the collection of task available for you:
+
+```
+skywalker% rake -T
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+rake qa:vm:halt[platform]           # Halt all VM's involved in the acceptance test round
+rake qa:vm:setup[platform]          # Bootstrap all the VM's used for this tests
+rake qa:vm:ssh_config               # Generate a valid ssh-config
+```
+
+Important to be aware that using any of this commands:
+
+```
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+```
+
+will bootstrap all selected machines. If you're willing to run on single
+platform you should use
+
+```
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+```
+
+this will not do any bootstrap, so you are required to previously
+boostrap the VM yourself by doing `vagrant up`. This is like this
+because this command is only here for developers, not for automated
+CI's.
+
+
+### How to run tests
+
+In this framework we're using ssh to connect to a collection of Vagrant
+machines, so first and most important is to generate a valid ssh config
+file, this could be done running `rake qa:vm:ssh_config`. When this task
+is finished a file named `.vm_ssh_config` will be generated with all the
+necessary information to connect with the different machines.
+
+Now is time to run your test and to do that we have different options:
+
+* rake qa:acceptance:all              # Run all acceptance
+* rake qa:acceptance:debian           # Run acceptance test in debian machines
+* rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+* rake qa:acceptance:suse             # Run acceptance test in suse machines
+* rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+
+Generally speaking this are complex tests so they take a long time to
+finish completly, if you look for faster feedback see at the end of this
+README how to run fewer tests.
+
+## Architecture of the Framework
+
+If you wanna know more about how this framework works, here is your
+section of information.
+
+### Directory structure
+
+* ```acceptance/``` here it goes all the specs definitions.
+* ```config```  inside you can find all config files, for now only the
+  platform definition.
+* ```rspec``` here stay all framework parts necessary to get the test
+  running, you will find the commands, the rspec matchers and a
+collection of useful helpers for your test.
+* ```sys``` a collection of bash scripts used to bootstrap the machines.
+* ```vagrant``` classes and modules used to help us running vagrant.
+
+### The platform configuration file
+
+Located inside the config directory there is the platforms.json which is used to define the different platforms we test with.
+Important bits here are:
+
+* `latest` key defines the latest published version of LS release which is used to test the package upgrade scenario.
+* inside the `platforms` key you will find the list of current available
+  OS we tests with, this include the box name, their type and if they
+have to go under specific bootstrap scripts (see ```specific: true ```
+in the platform definition).
+
+This file is the one that you will use to know about differnt OS's
+testes, add new ones, etc..
+
+### I want to add a test, whad should I do?
+
+To add a test you basically should start by the acceptance directory,
+here you will find an already created tests, most important locations
+here are:
+
+* ```lib``` here is where the tests are living. If a test is not going
+  to be reused it should be created here.
+* ```shared_examples``` inside that directory should be living all tests
+  that could be reused in different scenarios, like you can see the CLI
+ones.
+
+but we want to write tests, here is an example of how do they look like,
+including the different moving parts we encounter in the framework.
+
+
+```
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    ##
+    # ServiceTester::Artifact is the component used to interact with the
+    # destination machineri and the one that keep the necessary logic
+    # for it.
+    ##
+
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+
+    ## your test code goes here.
+  end
+```
+
+this is important because as you know we test with different machines,
+so the build out artifact will be the component necessary to run the
+actions with the destination machine.
+
+but this is the main parts, to run your test you need the framework
+located inside the ```rspec``` directory. Here you will find a
+collection of commands, properly organized per operating system, that
+will let you operate and get your tests done. But don't freak out, we
+got all logic necessary to select the right one for your test.
+
+You'll probably find enough supporting classes for different platforms, but if not, feel free to add it.
+
+FYI, this is how a command looks like:
+
+```
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+  end
+  ```
+this is how we run operations and wrap them as ruby code.
+
+### Running a test (detailed level)
+
+There is also the possibility to run your tests with more granularity by
+using the `rspec` command, this will let you for example run a single
+tests, a collection of them using filtering, etc.
+
+Check https://relishapp.com/rspec/rspec-core/v/3-4/docs/command-line for more details, but here is a quick cheat sheet to run them:
+
+# Run the examples that get "is installed" in their description
+
+*  bundle exec rspec acceptance/spec -e "is installed" 
+
+# Run the example desfined at line 11
+
+*  bundle exec rspec acceptance/spec/lib/artifact_operation_spec.rb:11
diff --git a/qa/Rakefile b/qa/Rakefile
new file mode 100644
index 00000000000..ffcde89f07a
--- /dev/null
+++ b/qa/Rakefile
@@ -0,0 +1,68 @@
+require "rspec"
+require "rspec/core/runner"
+require "rspec/core/rake_task"
+require_relative "vagrant/helpers"
+require_relative "platform_config"
+
+platforms = PlatformConfig.new
+
+task :spec    => 'spec:all'
+task :default => :spec
+
+namespace :qa do
+
+  namespace :vm do
+
+    desc "Generate a valid ssh-config"
+    task :ssh_config do
+      require "json"
+      raw_ssh_config    = LogStash::VagrantHelpers.fetch_config.stdout.split("\n");
+      parsed_ssh_config = LogStash::VagrantHelpers.parse(raw_ssh_config)
+      File.write(".vm_ssh_config", parsed_ssh_config.to_json)
+    end
+
+    desc "Bootstrap all the VM's used for this tests"
+    task :setup, :platform do |t, args|
+      config   = PlatformConfig.new
+      machines = config.select_names_for(args[:platform])
+
+      message  = "bootstraping all VM's defined in acceptance/Vagrantfile"
+      message  = "#{message} for #{args[:platform]}: #{machines}" if !args[:platform].nil?
+
+      LogStash::VagrantHelpers.destroy(machines)
+      LogStash::VagrantHelpers.bootstrap(machines)
+    end
+
+    desc "Halt all VM's involved in the acceptance test round"
+    task :halt, :platform do |t, args|
+      config   = PlatformConfig.new
+      machines = config.select_names_for(args[:platform])
+      message = "halting all VM's defined inside Vagrantfile"
+      message  = "#{message} for #{args[:platform]}: #{machines}" if !args[:platform].nil?
+      puts message
+
+      LogStash::VagrantHelpers.halt(machines)
+    end
+  end
+
+  namespace :acceptance do
+    desc "Run all acceptance"
+    task :all do
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/*_spec.rb"]]))
+    end
+
+    platforms.types.each do |type|
+      desc "Run acceptance test in #{type} machines"
+      task type do
+        ENV['LS_TEST_PLATFORM']=type
+        exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/*_spec.rb"]]))
+      end
+    end
+
+    desc "Run one single machine acceptance test"
+    task :single, :machine do |t, args|
+      ENV['LS_VAGRANT_HOST']  = args[:machine]
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/**/*_spec.rb"]]))
+    end
+  end
+end
diff --git a/qa/Vagrantfile b/qa/Vagrantfile
new file mode 100644
index 00000000000..b7da73064f6
--- /dev/null
+++ b/qa/Vagrantfile
@@ -0,0 +1,28 @@
+# -*- mode: ruby -*-
+# vi: set ft=ruby :
+require_relative "./platform_config.rb"
+
+Vagrant.configure(2) do |config|
+  platforms = PlatformConfig.new
+
+  platforms.each do |platform|
+    config.vm.define platform.name do |machine|
+      machine.vm.box = platform.box
+      machine.vm.provider "virtualbox" do |v|
+        v.memory = 2096
+        v.cpus = 4
+      end
+      machine.vm.synced_folder "../build", "/logstash-build", create: true
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.privileged
+        sh.privileged = true
+      end
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.non_privileged
+        sh.privileged = false
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/config_helper.rb b/qa/acceptance/spec/config_helper.rb
new file mode 100644
index 00000000000..3d9730c2d2d
--- /dev/null
+++ b/qa/acceptance/spec/config_helper.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "json"
+
+module SpecsHelper
+
+  def self.configure(vagrant_boxes)
+    setup_config = JSON.parse(File.read(File.join(File.dirname(__FILE__), "..", "..", ".vm_ssh_config")))
+    boxes        = vagrant_boxes.inject({}) do |acc, v|
+      acc[v.name] = v.type
+      acc
+    end
+    ServiceTester.configure do |config|
+      config.servers = []
+      config.lookup  = {}
+      setup_config.each do |host_info|
+        next unless boxes.keys.include?(host_info["host"])
+        url = "#{host_info["hostname"]}:#{host_info["port"]}"
+        config.servers << url
+        config.lookup[url] = {"host" => host_info["host"], "type" => boxes[host_info["host"]] }
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/lib/artifact_operation_spec.rb b/qa/acceptance/spec/lib/artifact_operation_spec.rb
new file mode 100644
index 00000000000..fdebf23de31
--- /dev/null
+++ b/qa/acceptance/spec/lib/artifact_operation_spec.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require_relative '../shared_examples/installed'
+require_relative '../shared_examples/running'
+require_relative '../shared_examples/updated'
+
+# This tests verify that the generated artifacts could be used properly in a relase, implements https://github.com/elastic/logstash/issues/5070
+describe "artifacts operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "installable", logstash
+    it_behaves_like "updated", logstash
+  end
+end
diff --git a/qa/acceptance/spec/lib/cli_operation_spec.rb b/qa/acceptance/spec/lib/cli_operation_spec.rb
new file mode 100644
index 00000000000..2a0fbaa2176
--- /dev/null
+++ b/qa/acceptance/spec/lib/cli_operation_spec.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require_relative "../spec_helper"
+require_relative "../shared_examples/cli/logstash/version"
+require_relative "../shared_examples/cli/logstash-plugin/install"
+require_relative "../shared_examples/cli/logstash-plugin/list"
+require_relative "../shared_examples/cli/logstash-plugin/uninstall"
+require_relative "../shared_examples/cli/logstash-plugin/update"
+
+# This is the collection of test for the CLI interface, this include the plugin manager behaviour, 
+# it also include the checks for other CLI options.
+describe "CLI operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "logstash version", logstash
+    it_behaves_like "logstash install", logstash
+    it_behaves_like "logstash list", logstash
+    it_behaves_like "logstash uninstall", logstash
+    it_behaves_like "logstash update", logstash
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
new file mode 100644
index 00000000000..13cb52036a2
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash install" do |logstash|
+  before(:each) do
+    logstash.install(LOGSTASH_VERSION)
+  end
+
+  after(:each) do
+    logstash.uninstall
+  end
+
+  describe "on #{logstash.hostname}" do
+    context "with a direct internet connection" do
+      context "when the plugin exist" do
+        context "from a local `.GEM` file" do
+          let(:gem_name) { "logstash-filter-qatest-0.1.1.gem" }
+          let(:gem_path_on_vagrant) { "/tmp/#{gem_name}" }
+          before(:each) do
+            logstash.download("https://rubygems.org/gems/#{gem_name}", gem_path_on_vagrant)
+          end
+
+          after(:each) { logstash.delete_file(gem_path_on_vagrant) }
+
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install #{gem_path_on_vagrant}")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-dns")
+          end
+        end
+
+        context "when fetching a gem from rubygems" do
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest")
+          end
+
+          it "allow to install a specific version" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install --version 0.1.0 logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest", "0.1.0")
+          end
+        end
+      end
+
+      context "when the plugin doesnt exist" do
+        it "fails to install and report an error" do
+          command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify logstash-output-impossible-plugin")
+          expect(command.stderr).to match(/Plugin not found, aborting/)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
new file mode 100644
index 00000000000..64e6cd1be1a
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash list" do |logstash|
+  describe "logstash-plugin list on #{logstash.hostname}" do
+    before(:all) do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after(:all) do
+      logstash.uninstall
+    end
+
+    context "without a specific plugin" do
+      it "display a list of plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "display a list of installed plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --installed")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "list the plugins with their versions" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose")
+        result.stdout.split("\n").each do |plugin|
+          expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+\)/)
+        end
+      end
+    end
+
+    context "with a specific plugin" do
+      let(:plugin_name) { "logstash-input-stdin" }
+      it "list the plugin and display the plugin name" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name}$/)
+      end
+
+      it "list the plugin with his version" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
new file mode 100644
index 00000000000..d12bbb954c0
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash uninstall" do |logstash|
+  describe "logstash uninstall on #{logstash.hostname}" do
+    before :each do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    context "when the plugin isn't installed" do
+      it "fails to uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stderr).to match(/ERROR: Uninstall Aborted, message: This plugin has not been previously installed, aborting/)
+      end
+    end
+
+    # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
+    xcontext "when the plugin is installed" do
+      it "succesfully uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+        expect(logstash).to have_installed?("logstash-filter-qatest")
+
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stdout).to match(/^Uninstalling logstash-filter-qatest/)
+        expect(logstash).not_to have_installed?("logstash-filter-qatest")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
new file mode 100644
index 00000000000..3aaaa30523a
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash update" do |logstash|
+  describe "logstash update on #{logstash.hostname}" do
+    before :each do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    let(:plugin_name) { "logstash-filter-qatest" }
+    let(:previous_version) { "0.1.0" }
+
+    before do
+      logstash.run_command_in_path("bin/logstash-plugin install --version #{previous_version} #{plugin_name}")
+      # Logstash wont update when we have a pinned versionin the gemfile so we remove them
+      logstash.replace_in_gemfile(',\s"0.1.0"', "")
+      expect(logstash).to have_installed?(plugin_name, previous_version)
+    end
+
+    context "update a specific plugin" do
+      it "has executed succesfully" do
+        cmd = logstash.run_command_in_path("bin/logstash-plugin update #{plugin_name}")
+        expect(cmd.stdout).to match(/Updating #{plugin_name}/)
+        expect(logstash).not_to have_installed?(plugin_name, previous_version)
+      end
+    end
+
+    context "update all the plugins" do
+      it "has executed succesfully" do
+        logstash.run_command_in_path("bin/logstash-plugin update")
+        expect(logstash).to have_installed?(plugin_name, "0.1.1")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash/version.rb b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
new file mode 100644
index 00000000000..97a2027064b
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash version" do |logstash|
+  describe "logstash --version" do
+    before :all do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after :all do
+      logstash.uninstall
+    end
+
+    context "on #{logstash.hostname}" do
+      it "returns the right logstash version" do
+        result = logstash.run_command_in_path("bin/logstash --path.settings=/etc/logstash --version")
+        expect(result).to run_successfully_and_output(/#{LOGSTASH_VERSION}/)
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/installed.rb b/qa/acceptance/spec/shared_examples/installed.rb
new file mode 100644
index 00000000000..94089232174
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/installed.rb
@@ -0,0 +1,25 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if a package is possible to be installed without errors.
+RSpec.shared_examples "installable" do |logstash|
+
+  before(:each) do
+    logstash.install(LOGSTASH_VERSION)
+  end
+
+  it "is installed on #{logstash.hostname}" do
+    expect(logstash).to be_installed
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+  it "is removable on #{logstash.hostname}" do
+    logstash.uninstall
+    expect(logstash).to be_removed
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/running.rb b/qa/acceptance/spec/shared_examples/running.rb
new file mode 100644
index 00000000000..4c051dc8f29
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/running.rb
@@ -0,0 +1,17 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# Test if an installed package can actually be started and runs OK.
+RSpec.shared_examples "runnable" do |logstash|
+
+  before(:each) do
+    logstash.install(LOGSTASH_VERSION)
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+end
diff --git a/qa/acceptance/spec/shared_examples/updated.rb b/qa/acceptance/spec/shared_examples/updated.rb
new file mode 100644
index 00000000000..87d39b1acb3
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/updated.rb
@@ -0,0 +1,22 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if the current package could used to update from the latest version released.
+RSpec.shared_examples "updated" do |logstash|
+
+  before (:all) { logstash.snapshot }
+  after  (:all) { logstash.restore }
+
+  it "can update on #{logstash.hostname}" do
+    logstash.install(LOGSTASH_LATEST_VERSION, "./")
+    expect(logstash).to be_installed
+    logstash.install(LOGSTASH_VERSION)
+    expect(logstash).to be_installed
+  end
+
+  it "can run on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+end
diff --git a/qa/acceptance/spec/spec_helper.rb b/qa/acceptance/spec/spec_helper.rb
new file mode 100644
index 00000000000..8cceefd5c13
--- /dev/null
+++ b/qa/acceptance/spec/spec_helper.rb
@@ -0,0 +1,32 @@
+# encoding: utf-8
+require 'runner-tool'
+require_relative '../../rspec/helpers'
+require_relative '../../rspec/matchers'
+require_relative 'config_helper'
+require_relative "../../platform_config"
+
+ROOT = File.expand_path(File.join(File.dirname(__FILE__), '..', '..', '..'))
+$LOAD_PATH.unshift File.join(ROOT, 'logstash-core/lib')
+
+RunnerTool.configure
+
+RSpec.configure do |c|
+  c.include ServiceTester
+end
+
+platform = ENV['LS_TEST_PLATFORM'] || 'all'
+
+config                  = PlatformConfig.new
+LOGSTASH_LATEST_VERSION = config.latest
+
+default_vagrant_boxes = ( platform == 'all' ? config.platforms : config.filter_type(platform) )
+
+selected_boxes = if ENV.include?('LS_VAGRANT_HOST') then
+                   config.platforms.select { |p| p.name  == ENV['LS_VAGRANT_HOST'] }
+                 else
+                   default_vagrant_boxes
+                 end
+
+SpecsHelper.configure(selected_boxes)
+
+puts "[Acceptance specs] running on #{ServiceTester.configuration.hosts}" if !selected_boxes.empty?
diff --git a/qa/config/platforms.json b/qa/config/platforms.json
new file mode 100644
index 00000000000..09a7d6ce481
--- /dev/null
+++ b/qa/config/platforms.json
@@ -0,0 +1,18 @@
+{ 
+  "latest": "5.0.0-alpha2",
+  "platforms" : {
+    "ubuntu-1204": { "box": "elastic/ubuntu-12.04-x86_64", "type": "debian" },
+    "ubuntu-1404": { "box": "elastic/ubuntu-14.04-x86_64", "type": "debian", "specific": true },
+    "ubuntu-1604": { "box": "elastic/ubuntu-16.04-x86_64", "type": "debian" },
+    "centos-6": { "box": "elastic/centos-6-x86_64", "type": "redhat" },
+    "centos-7": { "box": "elastic/centos-7-x86_64", "type": "redhat" },
+    "oel-6": { "box": "elastic/oraclelinux-6-x86_64", "type": "redhat" },
+    "oel-7": { "box": "elastic/oraclelinux-7-x86_64", "type": "redhat" },
+    "fedora-22": { "box": "elastic/fedora-22-x86_64", "type": "redhat" },
+    "fedora-23": { "box": "elastic/fedora-23-x86_64", "type": "redhat" },
+    "debian-8": { "box": "elastic/debian-8-x86_64", "type": "debian", "specific":  true },
+    "opensuse-13": { "box": "elastic/opensuse-13-x86_64", "type": "suse" },
+    "sles-11": { "box": "elastic/sles-11-x86_64", "type": "suse", "specific": true },
+    "sles-12": { "box": "elastic/sles-12-x86_64", "type": "suse", "specific": true }
+  }
+}
diff --git a/qa/platform_config.rb b/qa/platform_config.rb
new file mode 100644
index 00000000000..42e4b197404
--- /dev/null
+++ b/qa/platform_config.rb
@@ -0,0 +1,77 @@
+# encoding: utf-8
+require "json"
+require "ostruct"
+
+# This is a wrapper to encapsulate the logic behind the different platforms we test with, 
+# this is done here in order to simplify the necessary configuration for bootstrap and interactions
+# necessary later on in the tests phases.
+#
+class PlatformConfig
+
+  # Abstract the idea of a platform, aka an OS
+  class Platform
+
+    attr_reader :name, :box, :type, :bootstrap
+
+    def initialize(name, data)
+      @name = name
+      @box  = data["box"]
+      @type = data["type"]
+      configure_bootstrap_scripts(data)
+    end
+
+    private
+
+    def configure_bootstrap_scripts(data)
+      @bootstrap = OpenStruct.new(:privileged     => "sys/#{type}/bootstrap.sh",
+                                  :non_privileged => "sys/#{type}/user_bootstrap.sh")
+      ##
+      # for now the only specific boostrap scripts are ones need
+      # with privileged access level, whenever others are also
+      # required we can update this section as well with the same pattern.
+      ##
+      @bootstrap.privileged = "sys/#{type}/#{name}/bootstrap.sh" if data["specific"]
+    end
+  end
+
+  DEFAULT_CONFIG_LOCATION = File.join(File.dirname(__FILE__), "config", "platforms.json").freeze
+
+  attr_reader :platforms, :latest
+
+  def initialize(config_path = DEFAULT_CONFIG_LOCATION)
+    @config_path = config_path
+    @platforms = []
+
+    data = JSON.parse(File.read(@config_path))
+    data["platforms"].each do |k, v|
+      @platforms << Platform.new(k, v)
+    end
+    @platforms.sort! { |a, b| a.name <=> b.name }
+    @latest = data["latest"]
+  end
+
+  def find!(platform_name)
+    result = @platforms.find { |platform| platform.name == platform_name }.first
+    if result.nil?
+      raise "Cannot find platform named: #{platform_name} in @config_path"
+    else
+      return result
+    end
+  end
+
+  def each(&block)
+    @platforms.each(&block)
+  end
+
+  def filter_type(type_name)
+    @platforms.select { |platform| platform.type == type_name }
+  end
+
+  def select_names_for(platform=nil)
+    !platform.nil? ? filter_type(platform).map{ |p| p.name } : ""
+  end
+
+  def types
+    @platforms.collect(&:type).uniq.sort
+  end
+end
diff --git a/qa/rspec/commands.rb b/qa/rspec/commands.rb
new file mode 100644
index 00000000000..c4e739198e8
--- /dev/null
+++ b/qa/rspec/commands.rb
@@ -0,0 +1,132 @@
+# encoding: utf-8
+require_relative "./commands/debian"
+require_relative "./commands/ubuntu"
+require_relative "./commands/redhat"
+require_relative "./commands/suse"
+require_relative "./commands/centos/centos-6"
+require_relative "./commands/oel/oel-6"
+require_relative "./commands/ubuntu/ubuntu-1604"
+require_relative "./commands/suse/sles-11"
+
+require "forwardable"
+
+module ServiceTester
+
+  # An artifact is the component being tested, it's able to interact with
+  # a destination machine by holding a client and is basically provides all 
+  # necessary abstractions to make the test simple.
+  class Artifact
+
+    extend Forwardable
+    def_delegators :@client, :installed?, :removed?, :running?
+
+    attr_reader :host, :client
+
+    def initialize(host, options={})
+      @host    = host
+      @options = options
+      @client  = CommandsFactory.fetch(options["type"], options["host"])
+    end
+
+    def hostname
+      @options["host"]
+    end
+
+    def name
+      "logstash"
+    end
+
+    def hosts
+      [@host]
+    end
+
+    def snapshot
+      client.snapshot(@options["host"])
+    end
+
+    def restore
+      client.restore(@options["host"])
+    end
+
+    def start_service
+      client.start_service(name, host)
+    end
+
+    def stop_service
+      client.stop_service(name, host)
+    end
+
+    def install(version, base=ServiceTester::Base::LOCATION)
+      package = client.package_for(version, base)
+      client.install(package, host)
+    end
+
+    def uninstall
+      client.uninstall(name, host)
+    end
+
+    def run_command_in_path(cmd)
+      client.run_command_in_path(cmd, host)
+    end
+
+    def run_command(cmd)
+      client.run_command(cmd, host)
+    end
+
+    def plugin_installed?(name, version = nil)
+      client.plugin_installed?(host, name, version)
+    end
+
+    def download(from, to)
+      client.download(from, to , host)
+    end
+    
+    def replace_in_gemfile(pattern, replace)
+      client.replace_in_gemfile(pattern, replace, host)
+    end
+
+    def delete_file(path)
+      client.delete_file(path, host)
+    end
+
+    def to_s
+      "Artifact #{name}@#{host}"
+    end
+  end
+
+  # Factory of commands used to select the right clients for a given type of OS and host name,
+  # this give you as much granularity as required.
+  class CommandsFactory
+
+    def self.fetch(type, host)
+      case type
+      when "debian"
+        if host.start_with?("ubuntu")
+          if host == "ubuntu-1604"
+            return Ubuntu1604Commands.new
+          else
+            return UbuntuCommands.new
+          end
+        else
+          return DebianCommands.new
+        end
+      when "suse"
+        if host == "sles-11"
+          return Sles11Commands.new
+        else
+          return SuseCommands.new
+        end
+      when "redhat"
+        if host == "centos-6"
+          return Centos6Commands.new
+        elsif host == "oel-6"
+          return Oel6Commands.new
+        else
+          return RedhatCommands.new
+        end
+      else
+        return
+      end
+    end
+  end
+end
diff --git a/qa/rspec/commands/base.rb b/qa/rspec/commands/base.rb
new file mode 100644
index 00000000000..cd2acc2bfc9
--- /dev/null
+++ b/qa/rspec/commands/base.rb
@@ -0,0 +1,68 @@
+# encoding: utf-8
+require_relative "../../vagrant/helpers"
+require_relative "system_helpers"
+
+module ServiceTester
+
+  class Base
+    LOCATION="/logstash-build".freeze
+    LOGSTASH_PATH="/usr/share/logstash/".freeze
+
+    def snapshot(host)
+      LogStash::VagrantHelpers.save_snapshot(host)
+    end
+
+    def restore(host)
+      LogStash::VagrantHelpers.restore_snapshot(host)
+    end
+
+    def start_service(service, host=nil)
+      service_manager(service, "start", host)
+    end
+
+    def stop_service(service, host=nil)
+      service_manager(service, "stop", host)
+    end
+
+    def run_command(cmd, host)
+      hosts = (host.nil? ? servers : Array(host))
+
+      response = nil
+      at(hosts, {in: :serial}) do |_host|
+        response = sudo_exec!(cmd)
+      end
+      response
+    end
+
+    def replace_in_gemfile(pattern, replace, host)
+      gemfile = File.join(LOGSTASH_PATH, "Gemfile")
+      cmd = "/bin/env sed -i.sedbak 's/#{pattern}/#{replace}/' #{gemfile}"
+      run_command(cmd, host)
+    end
+
+    def run_command_in_path(cmd, host)
+      run_command("#{File.join(LOGSTASH_PATH, cmd)}", host)
+    end
+
+    def plugin_installed?(host, plugin_name, version = nil)
+      if version.nil?
+        cmd = run_command_in_path("bin/logstash-plugin list", host)
+        search_token = plugin_name
+      else
+        cmd = run_command_in_path("bin/logstash-plugin list --verbose", host)
+        search_token ="#{plugin_name} (#{version})"
+      end
+
+      plugins_list = cmd.stdout.split("\n")
+      plugins_list.include?(search_token)
+    end
+
+    def download(from, to, host)
+      run_command("wget #{from} -O #{to}", host)
+    end
+
+    def delete_file(path, host)
+      run_command("rm -rf #{path}", host)
+    end
+  end
+end
diff --git a/qa/rspec/commands/centos/centos-6.rb b/qa/rspec/commands/centos/centos-6.rb
new file mode 100644
index 00000000000..371590490e6
--- /dev/null
+++ b/qa/rspec/commands/centos/centos-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Centos6Commands < RedhatCommands
+      include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/debian.rb b/qa/rspec/commands/debian.rb
new file mode 100644
index 00000000000..710ffc48d57
--- /dev/null
+++ b/qa/rspec/commands/debian.rb
@@ -0,0 +1,50 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class DebianCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+    end
+
+    def package_for(version, base=ServiceTester::Base::LOCATION)
+      File.join(base, "logstash-#{version}.deb")
+    end
+
+    def install(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("dpkg -i  #{package}")
+      end
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("dpkg -r #{package}")
+        sudo_exec!("dpkg --purge #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s #{package}")
+        stdout = cmd.stderr
+      end
+      (
+        stdout.match(/^Package `#{package}' is not installed and no info is available.$/) ||
+        stdout.match(/^dpkg-query: package '#{package}' is not installed and no information is available$/)
+      )
+    end
+  end
+end
diff --git a/qa/rspec/commands/oel/oel-6.rb b/qa/rspec/commands/oel/oel-6.rb
new file mode 100644
index 00000000000..ed92a8ce11d
--- /dev/null
+++ b/qa/rspec/commands/oel/oel-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Oel6Commands < RedhatCommands
+    include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/redhat.rb b/qa/rspec/commands/redhat.rb
new file mode 100644
index 00000000000..8f0cf753c23
--- /dev/null
+++ b/qa/rspec/commands/redhat.rb
@@ -0,0 +1,49 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class RedhatCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("yum list installed  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Installed Packages$/)
+      stdout.match(/^logstash.noarch/)
+    end
+
+    def package_for(version, base=ServiceTester::Base::LOCATION)
+      File.join(base, "logstash-#{version}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = {}
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("yum install -y  #{package}")
+        errors[_host] = cmd.stderr unless cmd.stderr.empty?
+      end
+      errors
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("yum remove -y #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("yum list installed #{package}")
+        stdout = cmd.stderr
+      end
+      stdout.match(/^Error: No matching Packages to list$/)
+    end
+  end
+end
diff --git a/qa/rspec/commands/suse.rb b/qa/rspec/commands/suse.rb
new file mode 100644
index 00000000000..2a214a53b7e
--- /dev/null
+++ b/qa/rspec/commands/suse.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class SuseCommands < Base
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^i | logstash | An extensible logging pipeline | package$/)
+    end
+
+    def package_for(version, base=ServiceTester::Base::LOCATION)
+      File.join(base, "logstash-#{version}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = {}
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive install  #{package}")
+        errors[_host] = cmd.stderr unless cmd.stderr.empty?
+      end
+      errors
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive remove #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd    = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/No packages found/)
+    end
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} started.$/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/suse/sles-11.rb b/qa/rspec/commands/suse/sles-11.rb
new file mode 100644
index 00000000000..80dd94dd719
--- /dev/null
+++ b/qa/rspec/commands/suse/sles-11.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../suse"
+
+module ServiceTester
+  class Sles11Commands < SuseCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("/etc/init.d/#{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} is running$/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("/etc/init.d/#{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/system_helpers.rb b/qa/rspec/commands/system_helpers.rb
new file mode 100644
index 00000000000..8cb8922946b
--- /dev/null
+++ b/qa/rspec/commands/system_helpers.rb
@@ -0,0 +1,42 @@
+require_relative "base"
+
+module ServiceTester
+  module SystemD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      (
+        stdout.match(/Active: active \(running\)/) &&
+        stdout.match(/#{package}.service - #{package}/)
+      )
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+  end
+
+  module InitD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("initctl status #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} start\/running/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("initctl #{action} #{service}")
+      end
+    end 
+  end
+end
diff --git a/qa/rspec/commands/ubuntu.rb b/qa/rspec/commands/ubuntu.rb
new file mode 100644
index 00000000000..1d1ae75f96d
--- /dev/null
+++ b/qa/rspec/commands/ubuntu.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require_relative "debian"
+
+module ServiceTester
+  class UbuntuCommands < DebianCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^#{package} start\/running/)
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/ubuntu/ubuntu-1604.rb b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
new file mode 100644
index 00000000000..ae26bc09f28
--- /dev/null
+++ b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../ubuntu"
+
+module ServiceTester
+  class Ubuntu1604Commands < UbuntuCommands
+      include ::ServiceTester::SystemD
+  end
+end
diff --git a/qa/rspec/helpers.rb b/qa/rspec/helpers.rb
new file mode 100644
index 00000000000..a939fa7dfca
--- /dev/null
+++ b/qa/rspec/helpers.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require_relative "commands"
+
+module ServiceTester
+
+  class Configuration
+    attr_accessor :servers, :lookup
+    def initialize
+      @servers  = []
+      @lookup   = {}
+    end
+
+    def hosts
+      lookup.values.map { |val| val["host"] }
+    end
+  end
+
+  class << self
+    attr_accessor :configuration
+  end
+
+  def self.configure
+    self.configuration ||= Configuration.new
+    yield(configuration) if block_given?
+  end
+
+  def servers
+    ServiceTester.configuration.servers
+  end
+
+  def select_client
+    CommandsFactory.fetch(current_example.metadata[:platform])
+  end
+
+  def current_example
+    RSpec.respond_to?(:current_example) ? RSpec.current_example : self.example
+  end
+end
diff --git a/qa/rspec/matchers.rb b/qa/rspec/matchers.rb
new file mode 100644
index 00000000000..4da583262f2
--- /dev/null
+++ b/qa/rspec/matchers.rb
@@ -0,0 +1,4 @@
+# encoding: utf-8
+require_relative "./matchers/be_installed"
+require_relative "./matchers/be_running"
+require_relative "./matchers/cli_matchers"
diff --git a/qa/rspec/matchers/be_installed.rb b/qa/rspec/matchers/be_installed.rb
new file mode 100644
index 00000000000..4de70ae21ed
--- /dev/null
+++ b/qa/rspec/matchers/be_installed.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_installed do
+  match do |subject|
+    subject.installed?(subject.hosts, subject.name)
+  end
+end
+
+RSpec::Matchers.define :be_removed do
+  match do |subject|
+    subject.removed?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/be_running.rb b/qa/rspec/matchers/be_running.rb
new file mode 100644
index 00000000000..dc687e1b11d
--- /dev/null
+++ b/qa/rspec/matchers/be_running.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_running do
+  match do |subject|
+    subject.running?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/cli_matchers.rb b/qa/rspec/matchers/cli_matchers.rb
new file mode 100644
index 00000000000..e31aa050af3
--- /dev/null
+++ b/qa/rspec/matchers/cli_matchers.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+RSpec::Matchers.define :be_successful do
+  match do |actual|
+    actual.exit_status == 0
+  end
+end
+
+RSpec::Matchers.define :fail_and_output do |expected_output|
+  match do |actual|
+    actual.exit_status == 1 && actual.stderr =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :run_successfully_and_output do |expected_output|
+  match do |actual|
+    (actual.exit_status == 0 || actual.exit_status.nil?) && actual.stdout =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :have_installed? do |name,*args|
+  match do |actual|
+    version = args.first
+    actual.plugin_installed?(name, version)
+  end
+end
+
+RSpec::Matchers.define :install_successfully do
+  match do |cmd|
+    expect(cmd).to run_successfully_and_output(/Installation successful/)
+  end
+end
diff --git a/test/windows/acceptance/logstash_release_acceptance.ps1 b/qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
similarity index 96%
rename from test/windows/acceptance/logstash_release_acceptance.ps1
rename to qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
index da812277543..397b490781d 100644
--- a/test/windows/acceptance/logstash_release_acceptance.ps1
+++ b/qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
@@ -6,7 +6,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 $LS_CONFIG="test.conf"
 $LS_BRANCH=$env:LS_BRANCH
diff --git a/test/windows/acceptance/logstash_release_default_plugins.ps1 b/qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
similarity index 96%
rename from test/windows/acceptance/logstash_release_default_plugins.ps1
rename to qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
index f4cf177e718..77b121fc6d3 100644
--- a/test/windows/acceptance/logstash_release_default_plugins.ps1
+++ b/qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
@@ -6,7 +6,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 # - Ruby 7 or newer
 
 $ruby = $env:RUBY_HOME  + "\jruby.exe"
diff --git a/test/windows/event_log/logstash_event_log_plugin_integration.ps1 b/qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
similarity index 98%
rename from test/windows/event_log/logstash_event_log_plugin_integration.ps1
rename to qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
index a267c05e72d..710daf8573a 100644
--- a/test/windows/event_log/logstash_event_log_plugin_integration.ps1
+++ b/qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
@@ -8,7 +8,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 Add-Type -assembly "system.io.compression.filesystem"
 
diff --git a/test/windows/integration/logstash_simple_integration.ps1 b/qa/scripts/windows/integration/logstash_simple_integration.ps1
similarity index 98%
rename from test/windows/integration/logstash_simple_integration.ps1
rename to qa/scripts/windows/integration/logstash_simple_integration.ps1
index 73fe2fcff7e..fb861cd2e98 100644
--- a/test/windows/integration/logstash_simple_integration.ps1
+++ b/qa/scripts/windows/integration/logstash_simple_integration.ps1
@@ -8,7 +8,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 Add-Type -assembly "system.io.compression.filesystem"
 
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
new file mode 100644
index 00000000000..1e9fe168abe
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
@@ -0,0 +1,25 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.1'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
+
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
new file mode 100644
index 00000000000..82e3be79baf
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.0'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
diff --git a/qa/sys/debian/bootstrap.sh b/qa/sys/debian/bootstrap.sh
new file mode 100644
index 00000000000..29c2c840346
--- /dev/null
+++ b/qa/sys/debian/bootstrap.sh
@@ -0,0 +1,7 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
diff --git a/qa/sys/debian/debian-8/bootstrap.sh b/qa/sys/debian/debian-8/bootstrap.sh
new file mode 100644
index 00000000000..d1a23d54430
--- /dev/null
+++ b/qa/sys/debian/debian-8/bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+echo "deb http://http.debian.net/debian jessie-backports main" >> /etc/apt/sources.list
+apt-get update
+apt-get install -y openjdk-8-jdk
diff --git a/qa/sys/debian/ubuntu-1404/bootstrap.sh b/qa/sys/debian/ubuntu-1404/bootstrap.sh
new file mode 100644
index 00000000000..728b0c3d13f
--- /dev/null
+++ b/qa/sys/debian/ubuntu-1404/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
+update-ca-certificates -f
diff --git a/qa/sys/debian/user_bootstrap.sh b/qa/sys/debian/user_bootstrap.sh
new file mode 100644
index 00000000000..8d9fcb70c1d
--- /dev/null
+++ b/qa/sys/debian/user_bootstrap.sh
@@ -0,0 +1,6 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}_all.deb"
+wget -q https://download.elastic.co/logstash/logstash/packages/debian/$LOGSTASH_FILENAME
+mv $LOGSTASH_FILENAME "logstash-${VERSION}.deb" # necessary patch until new version with the standard name format are released
diff --git a/qa/sys/redhat/bootstrap.sh b/qa/sys/redhat/bootstrap.sh
new file mode 100644
index 00000000000..5976f47722f
--- /dev/null
+++ b/qa/sys/redhat/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+yum update
+yum install -y java-1.8.0-openjdk-devel.x86_64
diff --git a/qa/sys/redhat/user_bootstrap.sh b/qa/sys/redhat/user_bootstrap.sh
new file mode 100644
index 00000000000..db964babc63
--- /dev/null
+++ b/qa/sys/redhat/user_bootstrap.sh
@@ -0,0 +1,6 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.noarch.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
+mv $LOGSTASH_FILENAME "logstash-${VERSION}.rpm" # necessary patch until new version with the standard name format are released
diff --git a/qa/sys/suse/bootstrap.sh b/qa/sys/suse/bootstrap.sh
new file mode 100644
index 00000000000..4dba83eb9ea
--- /dev/null
+++ b/qa/sys/suse/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+zypper --non-interactive list-updates
+zypper --non-interactive --no-gpg-checks --quiet install --no-recommends java-1_8_0-openjdk-devel
diff --git a/qa/sys/suse/sles-11/bootstrap.sh b/qa/sys/suse/sles-11/bootstrap.sh
new file mode 100644
index 00000000000..654be5d7ec0
--- /dev/null
+++ b/qa/sys/suse/sles-11/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+ln -s /usr/sbin/update-alternatives /usr/sbin/alternatives
+curl -L 'https://edelivery.oracle.com/otn-pub/java/jdk/8u77-b03/jdk-8u77-linux-x64.rpm' -H 'Accept-Encoding: gzip, deflate, sdch' -H 'Accept-Language: en-US,en;q=0.8' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0' -H 'Cookie: oraclelicense=accept-securebackup-cookie;' -H 'Connection: keep-alive' --compressed -o oracle_jdk_1.8.rpm
+zypper -q -n --non-interactive install oracle_jdk_1.8.rpm
diff --git a/qa/sys/suse/sles-12/bootstrap.sh b/qa/sys/suse/sles-12/bootstrap.sh
new file mode 100644
index 00000000000..3ed7a04ed15
--- /dev/null
+++ b/qa/sys/suse/sles-12/bootstrap.sh
@@ -0,0 +1,9 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+zypper addrepo http://download.opensuse.org/repositories/Java:Factory/SLE_12/Java:Factory.repo || true
+zypper --no-gpg-checks --non-interactive refresh
+zypper --non-interactive list-updates
+zypper --non-interactive --no-gpg-checks --quiet install --no-recommends java-1_8_0-openjdk-devel
diff --git a/qa/sys/suse/user_bootstrap.sh b/qa/sys/suse/user_bootstrap.sh
new file mode 100644
index 00000000000..77653c4e980
--- /dev/null
+++ b/qa/sys/suse/user_bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.noarch.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
+mv $LOGSTASH_FILENAME "logstash-${VERSION}.rpm" # necessary patch until new version with the standard name format are released
diff --git a/qa/vagrant/command.rb b/qa/vagrant/command.rb
new file mode 100644
index 00000000000..740514df8e7
--- /dev/null
+++ b/qa/vagrant/command.rb
@@ -0,0 +1,45 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+
+module LogStash
+  class CommandExecutor
+    class CommandError < StandardError; end
+
+    class CommandResponse
+      attr_reader :stdin, :stdout, :stderr, :exitstatus
+
+      def initialize(stdin, stdout, stderr, exitstatus)
+        @stdin = stdin
+        @stdout = stdout
+        @stderr = stderr
+        @exitstatus = exitstatus
+      end
+
+      def success?
+        exitstatus == 0
+      end
+    end
+
+    def self.run(cmd)
+      # This block is require to be able to launch a ruby subprocess
+      # that use bundler.
+      Bundler.with_clean_env do
+        Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr|
+          CommandResponse.new(stdin, stdout.read.chomp, stderr.read.chomp, wait_thr.value.exitstatus)
+        end
+      end
+    end
+
+    # This method will raise an exception if the `CMD`
+    # was not run successfully and will display the content of STDERR
+    def self.run!(cmd)
+      response = run(cmd)
+
+      unless response.success?
+        raise CommandError, "CMD: #{cmd} STDERR: #{response.stderr}"
+      end
+      response
+    end
+  end
+end
diff --git a/qa/vagrant/helpers.rb b/qa/vagrant/helpers.rb
new file mode 100644
index 00000000000..1b2a63929a7
--- /dev/null
+++ b/qa/vagrant/helpers.rb
@@ -0,0 +1,52 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+require_relative "command"
+
+module LogStash
+  class VagrantHelpers
+
+    def self.halt(machines="")
+      CommandExecutor.run!("vagrant halt #{machines.join(' ')}")
+    end
+
+    def self.destroy(machines="")
+      CommandExecutor.run!("vagrant destroy --force #{machines.join(' ')}") 
+    end
+
+    def self.bootstrap(machines="")
+      CommandExecutor.run!("vagrant up #{machines.join(' ')}")
+    end
+
+    def self.save_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot save #{machine} #{machine}-snapshot")
+    end
+
+    def self.restore_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot restore #{machine} #{machine}-snapshot")
+    end
+
+    def self.fetch_config
+      machines = CommandExecutor.run!("vagrant status").stdout.split("\n").select { |l| l.include?("running") }.map { |r| r.split(' ')[0]}
+      CommandExecutor.run!("vagrant ssh-config #{machines.join(' ')}")
+    end
+
+    def self.parse(lines)
+      hosts, host = [], {}
+      lines.each do |line|
+        if line.match(/Host\s(.*)$/)
+          host = { :host => line.gsub("Host","").strip }
+        elsif line.match(/HostName\s(.*)$/)
+          host[:hostname] = line.gsub("HostName","").strip
+        elsif line.match(/Port\s(.*)$/)
+          host[:port]     = line.gsub("Port","").strip
+        elsif line.empty?
+          hosts << host
+          host = {}
+        end
+      end
+      hosts << host
+      hosts
+    end
+  end
+end
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index e2637f09995..581d8c798cb 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -9,10 +9,19 @@ namespace "artifact" do
       "NOTICE.TXT",
       "CONTRIBUTORS",
       "bin/**/*",
+      "config/**/*",
       "lib/bootstrap/**/*",
       "lib/pluginmanager/**/*",
+      "lib/systeminstall/**/*",
       "patterns/**/*",
       "vendor/??*/**/*",
+      # To include ruby-maven's hidden ".mvn" directory, we need to
+      # do add the line below. This directory contains a file called
+      # "extensions.xml", which loads the ruby DSL for POMs.
+      # Failing to include this file results in updates breaking for
+      # plugins which use jar-dependencies.
+      # See more in https://github.com/elastic/logstash/issues/4818
+      "vendor/??*/**/.mvn/**/*",
       "Gemfile",
       "Gemfile.jruby-1.9.lock",
     ]
@@ -46,6 +55,20 @@ namespace "artifact" do
     end.flatten.uniq
   end
 
+  task "all" => ["prepare"] do
+    Rake::Task["artifact:deb"].invoke
+    Rake::Task["artifact:rpm"].invoke
+    Rake::Task["artifact:zip"].invoke
+    Rake::Task["artifact:tar"].invoke
+  end
+
+  task "all-all-plugins" => ["prepare-all"] do
+    Rake::Task["artifact:deb"].invoke
+    Rake::Task["artifact:rpm"].invoke
+    Rake::Task["artifact:zip"].invoke
+    Rake::Task["artifact:tar"].invoke
+  end
+
   # We create an empty bundle config file
   # This will allow the deb and rpm to create a file
   # with the correct user group and permission.
@@ -54,27 +77,85 @@ namespace "artifact" do
     File.open(".bundle/config", "w") { }
   end
 
-  # locate the "gem "logstash-core" ..." line in Gemfile, and if the :path => "." option if specified
+  # locate the "gem "logstash-core" ..." line in Gemfile, and if the :path => "..." option if specified
   # build and install the local logstash-core gem otherwise just do nothing, bundler will deal with it.
   task "install-logstash-core" do
+    # regex which matches a Gemfile gem definition for the logstash-core gem and captures the :path option
+    gem_line_regex = /^\s*gem\s+["']logstash-core["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
+
     lines = File.readlines("Gemfile")
-    matches = lines.select{|line| line[/^gem\s+["']logstash-core["']/i]}
+    matches = lines.select{|line| line[gem_line_regex]}
     abort("ERROR: Gemfile format error, need a single logstash-core gem specification") if matches.size != 1
-    if matches.first =~ /:path\s*=>\s*["']\.["']/
-      Rake::Task["plugin:install-local-logstash-core-gem"].invoke
+
+    path = matches.first[gem_line_regex, 1]
+
+    if path
+      Rake::Task["plugin:install-local-core-gem"].invoke("logstash-core", path)
     else
       puts("[artifact:install-logstash-core] using logstash-core from Rubygems")
     end
   end
 
-  task "prepare" => ["bootstrap", "plugin:install-default", "install-logstash-core", "clean-bundle-config"]
+  # # locate the "gem "logstash-core-event*" ..." line in Gemfile, and if the :path => "." option if specified
+  # # build and install the local logstash-core-event* gem otherwise just do nothing, bundler will deal with it.
+  task "install-logstash-core-event" do
+    # regex which matches a Gemfile gem definition for the logstash-core-event* gem and captures the gem name and :path option
+    gem_line_regex = /^\s*gem\s+["'](logstash-core-event[^"^']*)["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
+
+    lines = File.readlines("Gemfile")
+    matches = lines.select{|line| line[gem_line_regex]}
+    abort("ERROR: Gemfile format error, need a single logstash-core-event gem specification") if matches.size != 1
+
+    name = matches.first[gem_line_regex, 1]
+    path = matches.first[gem_line_regex, 2]
 
-  desc "Build a tar.gz of logstash with all dependencies"
+    if path
+      Rake::Task["plugin:install-local-core-gem"].invoke(name, path)
+    else
+      puts("[artifact:install-logstash-core] using #{name} from Rubygems")
+    end
+  end
+
+  # locate the "gem "logstash-core-plugin-api" ..." line in Gemfile, and if the :path => "..." option if specified
+  # build and install the local logstash-core-plugin-api gem otherwise just do nothing, bundler will deal with it.
+  task "install-logstash-core-plugin-api" do
+    # regex which matches a Gemfile gem definition for the logstash-core gem and captures the :path option
+    gem_line_regex = /^\s*gem\s+["']logstash-core-plugin-api["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
+
+    lines = File.readlines("Gemfile")
+    matches = lines.select{|line| line[gem_line_regex]}
+    abort("ERROR: Gemfile format error, need a single logstash-core-plugin-api gem specification") if matches.size != 1
+
+    path = matches.first[gem_line_regex, 1]
+
+    if path
+      Rake::Task["plugin:install-local-core-gem"].invoke("logstash-core-plugin-api", path)
+    else
+      puts("[artifact:install-logstash-core-plugin-api] using logstash-core from Rubygems")
+    end
+  end
+
+  task "prepare" => ["bootstrap", "plugin:install-default", "install-logstash-core", "install-logstash-core-event", "install-logstash-core-plugin-api", "clean-bundle-config"]
+
+  task "prepare-all" => ["bootstrap", "plugin:install-all", "install-logstash-core", "install-logstash-core-event", "install-logstash-core-plugin-api", "clean-bundle-config"]
+
+  desc "Build a tar.gz of default logstash plugins with all dependencies"
   task "tar" => ["prepare"] do
+    puts("[artifact:tar] Building tar.gz of default plugins")
+    build_tar
+  end
+
+  desc "Build a tar.gz of all logstash plugins from logstash-plugins github repo"
+  task "tar-all-plugins" => ["prepare-all"] do
+    puts("[artifact:tar] Building tar.gz of all plugins")
+    build_tar "-all-plugins"
+  end
+
+  def build_tar(tar_suffix = nil)
     require "zlib"
     require "archive/tar/minitar"
     require "logstash/version"
-    tarpath = "build/logstash-#{LOGSTASH_VERSION}.tar.gz"
+    tarpath = "build/logstash#{tar_suffix}-#{LOGSTASH_VERSION}.tar.gz"
     puts("[artifact:tar] building #{tarpath}")
     gz = Zlib::GzipWriter.new(File.new(tarpath, "wb"), Zlib::BEST_COMPRESSION)
     tar = Archive::Tar::Minitar::Output.new(gz)
@@ -106,9 +187,21 @@ namespace "artifact" do
     puts "Complete: #{tarpath}"
   end
 
+  desc "Build a zip of default logstash plugins with all dependencies"
   task "zip" => ["prepare"] do
+    puts("[artifact:zip] Building zip of default plugins")
+    build_zip
+  end
+
+  desc "Build a zip of all logstash plugins from logstash-plugins github repo"
+  task "zip-all-plugins" => ["prepare-all"] do
+    puts("[artifact:zip] Building zip of all plugins")
+    build_zip "-all-plugins"
+  end
+
+  def build_zip(zip_suffix = "")
     require 'zip'
-    zippath = "build/logstash-#{LOGSTASH_VERSION}.zip"
+    zippath = "build/logstash#{zip_suffix}-#{LOGSTASH_VERSION}.zip"
     puts("[artifact:zip] building #{zippath}")
     File.unlink(zippath) if File.exists?(zippath)
     Zip::File.open(zippath, Zip::File::CREATE) do |zipfile|
@@ -130,7 +223,10 @@ namespace "artifact" do
 
     files.each do |path|
       next if File.directory?(path)
-      dir.input("#{path}=/opt/logstash/#{path}")
+      # Omit any config dir from /usr/share/logstash for packages, since we're
+      # using /etc/logstash below
+      next if path.start_with?("config/")
+      dir.input("#{path}=/usr/share/logstash/#{path}")
     end
 
     basedir = File.join(File.dirname(__FILE__), "..")
@@ -148,16 +244,23 @@ namespace "artifact" do
       dir.input("#{empty}/=/etc/logstash/conf.d")
     end
 
+    # produce: logstash-5.0.0-alpha1.deb"
+    # produce: logstash-5.0.0-alpha1.rpm
+    package_filename = "logstash-#{LOGSTASH_VERSION}.TYPE"
+
     case platform
       when "redhat", "centos"
         File.join(basedir, "pkg", "logrotate.conf").tap do |path|
           dir.input("#{path}=/etc/logrotate.d/logstash")
         end
-        File.join(basedir, "pkg", "logstash.default").tap do |path|
-          dir.input("#{path}=/etc/sysconfig/logstash")
+        File.join(basedir, "pkg", "startup.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
-          dir.input("#{path}=/etc/init.d/logstash")
+        File.join(basedir, "pkg", "jvm.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
+        end
+        File.join(basedir, "pkg", "logstash.yml").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
         require "fpm/package/rpm"
         out = dir.convert(FPM::Package::RPM)
@@ -165,25 +268,31 @@ namespace "artifact" do
         out.attributes[:rpm_use_file_permissions] = true
         out.attributes[:rpm_user] = "root"
         out.attributes[:rpm_group] = "root"
-        out.config_files << "etc/sysconfig/logstash"
+        out.attributes[:rpm_os] = "linux"
         out.config_files << "etc/logrotate.d/logstash"
-        out.config_files << "/etc/init.d/logstash"
+        out.config_files << "/etc/logstash/startup.options"
+        out.config_files << "/etc/logstash/jvm.options"
+        out.config_files << "/etc/logstash/logstash.yml"
       when "debian", "ubuntu"
-        File.join(basedir, "pkg", "logstash.default").tap do |path|
-          dir.input("#{path}=/etc/default/logstash")
+        File.join(basedir, "pkg", "startup.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
+        end
+        File.join(basedir, "pkg", "jvm.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
-          dir.input("#{path}=/etc/init.d/logstash")
+        File.join(basedir, "pkg", "logstash.yml").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
         require "fpm/package/deb"
         out = dir.convert(FPM::Package::Deb)
         out.license = "Apache 2.0"
         out.attributes[:deb_user] = "root"
         out.attributes[:deb_group] = "root"
-        out.attributes[:deb_suggests] = "java7-runtime-headless"
-        out.config_files << "/etc/default/logstash"
+        out.attributes[:deb_suggests] = "java8-runtime-headless"
         out.config_files << "/etc/logrotate.d/logstash"
-        out.config_files << "/etc/init.d/logstash"
+        out.config_files << "/etc/logstash/startup.options"
+        out.config_files << "/etc/logstash/jvm.options"
+        out.config_files << "/etc/logstash/logstash.yml"
     end
 
     # Packaging install/removal scripts
@@ -201,7 +310,7 @@ namespace "artifact" do
     # TODO(sissel): Invoke Pleaserun to generate the init scripts/whatever
 
     out.name = "logstash"
-    out.version = LOGSTASH_VERSION
+    out.version = LOGSTASH_VERSION.gsub(/[.-]([[:alpha:]])/, '~\1')
     out.architecture = "all"
     # TODO(sissel): Include the git commit hash?
     out.iteration = "1" # what revision?
@@ -233,7 +342,7 @@ namespace "artifact" do
 
     out.attributes[:force?] = true # overwrite the rpm/deb/etc being created
     begin
-      path = File.join(basedir, "build", out.to_s)
+      path = File.join(basedir, "build", out.to_s(package_filename))
       x = out.output(path)
       puts "Completed: #{path}"
     ensure
@@ -247,10 +356,9 @@ namespace "artifact" do
     package("centos", "5")
   end
 
-  desc "Build an RPM of logstash with all dependencies"
+  desc "Build a DEB of logstash with all dependencies"
   task "deb" => ["prepare"] do
     puts("[artifact:deb] building deb package")
     package("ubuntu", "12.04")
   end
 end
-
diff --git a/rakelib/benchmark.rake b/rakelib/benchmark.rake
index 148922f6531..29bfd2c2844 100644
--- a/rakelib/benchmark.rake
+++ b/rakelib/benchmark.rake
@@ -1,7 +1,7 @@
 namespace :benchmark do
   desc "Run benchmark code in benchmark/*.rb"
   task :run => ["test:setup"] do
-    path = File.join(LogStash::Environment::LOGSTASH_HOME, "benchmark", "*.rb")
+    path = File.join(LogStash::Environment::LOGSTASH_HOME, "tools/benchmark", "*.rb")
     Dir.glob(path).each { |f| require f }
   end
 end
diff --git a/rakelib/compile.rake b/rakelib/compile.rake
index df572de21bc..aa20eb7091b 100644
--- a/rakelib/compile.rake
+++ b/rakelib/compile.rake
@@ -8,8 +8,14 @@ end
 
 namespace "compile" do
   desc "Compile the config grammar"
-  task "grammar" => "lib/logstash/config/grammar.rb"
+
+  task "grammar" => "logstash-core/lib/logstash/config/grammar.rb"
+
+  task "logstash-core-event-java" do
+    puts("Building logstash-core-event-java using gradle")
+    system("logstash-core-event-java/gradlew", "jar", "-p", "./logstash-core-event-java")
+  end
 
   desc "Build everything"
-  task "all" => "grammar"
+  task "all" => ["grammar", "logstash-core-event-java"]
 end
diff --git a/rakelib/default_plugins.rb b/rakelib/default_plugins.rb
index 4b0e3e2d35e..52ba5c14005 100644
--- a/rakelib/default_plugins.rb
+++ b/rakelib/default_plugins.rb
@@ -4,7 +4,6 @@ module RakeLib
     # plugins included by default in the logstash distribution
     DEFAULT_PLUGINS = %w(
       logstash-input-heartbeat
-      logstash-output-zeromq
       logstash-codec-collectd
       logstash-output-xmpp
       logstash-codec-dots
@@ -19,11 +18,8 @@ module RakeLib
       logstash-codec-msgpack
       logstash-codec-multiline
       logstash-codec-netflow
-      logstash-codec-oldlogstashjson
       logstash-codec-plain
       logstash-codec-rubydebug
-      logstash-filter-anonymize
-      logstash-filter-checksum
       logstash-filter-clone
       logstash-filter-csv
       logstash-filter-date
@@ -35,7 +31,6 @@ module RakeLib
       logstash-filter-json
       logstash-filter-kv
       logstash-filter-metrics
-      logstash-filter-multiline
       logstash-filter-mutate
       logstash-filter-ruby
       logstash-filter-sleep
@@ -48,7 +43,6 @@ module RakeLib
       logstash-filter-xml
       logstash-input-couchdb_changes
       logstash-input-elasticsearch
-      logstash-input-eventlog
       logstash-input-exec
       logstash-input-file
       logstash-input-ganglia
@@ -56,8 +50,10 @@ module RakeLib
       logstash-input-generator
       logstash-input-graphite
       logstash-input-http
+      logstash-input-http_poller
       logstash-input-imap
       logstash-input-irc
+      logstash-input-jdbc
       logstash-input-log4j
       logstash-input-lumberjack
       logstash-input-pipe
@@ -73,27 +69,18 @@ module RakeLib
       logstash-input-udp
       logstash-input-unix
       logstash-input-xmpp
-      logstash-input-zeromq
       logstash-input-kafka
       logstash-input-beats
       logstash-output-cloudwatch
       logstash-output-csv
       logstash-output-elasticsearch
-      logstash-output-email
-      logstash-output-exec
       logstash-output-file
-      logstash-output-ganglia
-      logstash-output-gelf
       logstash-output-graphite
-      logstash-output-hipchat
       logstash-output-http
       logstash-output-irc
-      logstash-output-juggernaut
-      logstash-output-lumberjack
+      logstash-output-kafka
       logstash-output-nagios
-      logstash-output-nagios_nsca
       logstash-output-null
-      logstash-output-opentsdb
       logstash-output-pagerduty
       logstash-output-pipe
       logstash-output-rabbitmq
@@ -105,7 +92,7 @@ module RakeLib
       logstash-output-stdout
       logstash-output-tcp
       logstash-output-udp
-      logstash-output-kafka
+      logstash-output-webhdfs
     )
 
     # plugins required to run the logstash core specs
@@ -142,7 +129,13 @@ module RakeLib
       /^logstash-output-webhdfs$/,
       /^logstash-input-rackspace$/,
       /^logstash-output-rackspace$/,
-      /^logstash-input-dynamodb$/
+      /^logstash-input-dynamodb$/,
+      /^logstash-filter-language$/,
+      /^logstash-input-heroku$/,
+      /^logstash-output-google_cloud_storage$/,
+      /^logstash-input-journald$/,
+      /^logstash-input-log4j2$/,
+      /^logstash-codec-cloudtrail$/
     ])
 
 
diff --git a/rakelib/docs.rake b/rakelib/docs.rake
index 9ba82c334f5..c7eb1d32152 100644
--- a/rakelib/docs.rake
+++ b/rakelib/docs.rake
@@ -1,5 +1,6 @@
 namespace "docs" do
 
+  desc "Generate documentation for all plugins"
   task "generate" do
     Rake::Task['plugin:install-all'].invoke
     Rake::Task['docs:generate-docs'].invoke
@@ -10,16 +11,15 @@ namespace "docs" do
     require "bootstrap/environment"
     pattern = "#{LogStash::Environment.logstash_gem_home}/gems/logstash-*/lib/logstash/{input,output,filter,codec}s/*.rb"
     list    = Dir.glob(pattern).join(" ")
-    cmd     = "bin/logstash docgen -o asciidoc_generated #{list}"
+    cmd     = "bin/bundle exec ruby docs/asciidocgen.rb -o asciidoc_generated #{list}"
     system(cmd)
   end
 
   task "generate-index" do
     list = [ 'inputs', 'outputs', 'filters', 'codecs' ]
     list.each do |type|
-      cmd = "ruby docs/asciidoc_index.rb asciidoc_generated #{type}"
+      cmd = "bin/bundle exec ruby docs/asciidoc_index.rb asciidoc_generated #{type}"
       system(cmd)
     end
   end
-
 end
diff --git a/rakelib/package.rake b/rakelib/package.rake
new file mode 100644
index 00000000000..73885a013d2
--- /dev/null
+++ b/rakelib/package.rake
@@ -0,0 +1,13 @@
+namespace "package" do
+
+  task "bundle" do
+    system("bin/logstash-plugin", "pack")
+    raise(RuntimeError, $!.to_s) unless $?.success?
+  end
+
+  desc "Build a package with the default plugins, including dependencies, to be installed offline"
+  task "plugins-default" => ["test:install-default", "bundle"]
+
+  desc "Build a package with all the plugins, including dependencies, to be installed offline"
+  task "plugins-all" => ["test:install-all", "bundle"]
+end
diff --git a/rakelib/plugin.rake b/rakelib/plugin.rake
index 9c2065c1f56..0bc1b708906 100644
--- a/rakelib/plugin.rake
+++ b/rakelib/plugin.rake
@@ -1,15 +1,16 @@
 require_relative "default_plugins"
+require 'rubygems'
 
 namespace "plugin" do
 
   def install_plugins(*args)
-    system("bin/plugin", "install", *args)
+    system("bin/logstash-plugin", "install", *args)
     raise(RuntimeError, $!.to_s) unless $?.success?
   end
 
   task "install-development-dependencies" do
     puts("[plugin:install-development-dependencies] Installing development dependencies of all installed plugins")
-    install_plugins("--development")
+    install_plugins("--development",  "--preserve")
 
     task.reenable # Allow this task to be run again
   end
@@ -17,66 +18,94 @@ namespace "plugin" do
   task "install", :name do |task, args|
     name = args[:name]
     puts("[plugin:install] Installing plugin: #{name}")
-    install_plugins("--no-verify", name)
+    install_plugins("--no-verify", "--preserve", name)
 
     task.reenable # Allow this task to be run again
   end # task "install"
 
   task "install-default" do
     puts("[plugin:install-default] Installing default plugins")
-    install_plugins("--no-verify", *LogStash::RakeLib::DEFAULT_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::DEFAULT_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-core" do
     puts("[plugin:install-core] Installing core plugins")
-    install_plugins("--no-verify", *LogStash::RakeLib::CORE_SPECS_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::CORE_SPECS_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-jar-dependencies" do
     puts("[plugin:install-jar-dependencies] Installing jar_dependencies plugins for testing")
-    install_plugins("--no-verify", *LogStash::RakeLib::TEST_JAR_DEPENDENCIES_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::TEST_JAR_DEPENDENCIES_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-vendor" do
     puts("[plugin:install-jar-dependencies] Installing vendor plugins for testing")
-    install_plugins("--no-verify", *LogStash::RakeLib::TEST_VENDOR_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::TEST_VENDOR_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-all" do
     puts("[plugin:install-all] Installing all plugins from https://github.com/logstash-plugins")
-    install_plugins("--no-verify", *LogStash::RakeLib.fetch_all_plugins)
+    p = *LogStash::RakeLib.fetch_all_plugins
+    # Install plugin one by one, ignoring plugins that have issues. Otherwise, one bad plugin will
+    # blow up the entire install process.
+    # TODO Push this downstream to #install_plugins
+    p.each do |plugin|
+      begin
+        install_plugins("--no-verify", "--preserve", plugin)
+      rescue
+        puts "Unable to install #{plugin}. Skipping"
+        next
+      end
+    end
 
     task.reenable # Allow this task to be run again
   end
 
-  task "clean-logstash-core-gem" do
-    Dir["logstash-core*.gem"].each do |gem|
+  task "clean-local-core-gem", [:name, :path] do |task, args|
+    name = args[:name]
+    path = args[:path]
+
+    Dir[File.join(path, "#{name}*.gem")].each do |gem|
+      puts("[plugin:clean-local-core-gem] Cleaning #{gem}")
       rm(gem)
     end
 
     task.reenable # Allow this task to be run again
   end
 
-  task "build-logstash-core-gem" => [ "clean-logstash-core-gem" ] do
-    puts("[plugin:build-logstash-core-gem] Building logstash-core.gemspec")
+  task "build-local-core-gem", [:name, :path]  do |task, args|
+    name = args[:name]
+    path = args[:path]
+
+    Rake::Task["plugin:clean-local-core-gem"].invoke(name, path)
 
-    system("gem build logstash-core.gemspec")
+    puts("[plugin:build-local-core-gem] Building #{File.join(path, name)}.gemspec")
+
+    Dir.chdir(path) do
+      spec = Gem::Specification.load("#{name}.gemspec")
+      Gem::Package.build(spec)
+    end
 
     task.reenable # Allow this task to be run again
   end
 
-  task "install-local-logstash-core-gem" => [ "build-logstash-core-gem" ] do
-    gems = Dir["logstash-core*.gem"]
-    abort("ERROR: logstash-core gem not found") if gems.size != 1
-    puts("[plugin:install-local-logstash-core-gem] Installing #{gems.first}")
+  task "install-local-core-gem", [:name, :path] do |task, args|
+    name = args[:name]
+    path = args[:path]
+
+    Rake::Task["plugin:build-local-core-gem"].invoke(name, path)
+
+    gems = Dir[File.join(path, "#{name}*.gem")]
+    abort("ERROR: #{name} gem not found in #{path}") if gems.size != 1
+    puts("[plugin:install-local-core-gem] Installing #{gems.first}")
     install_plugins("--no-verify", gems.first)
 
     task.reenable # Allow this task to be run again
diff --git a/rakelib/test.rake b/rakelib/test.rake
index 9c25d819589..fb2dad658ab 100644
--- a/rakelib/test.rake
+++ b/rakelib/test.rake
@@ -19,18 +19,36 @@ namespace "test" do
     require 'ci/reporter/rake/rspec_loader'
   end
 
+  def core_specs
+    # note that regardless if which logstash-core-event-* gem is live, we will always run the
+    # logstash-core-event specs since currently this is the most complete Event and Timestamp specs
+    # which actually defines the Event contract and should pass regardless of the actuall underlying
+    # implementation.
+    specs = ["spec/unit/**/*_spec.rb", "logstash-core/spec/**/*_spec.rb", "logstash-core-event/spec/**/*_spec.rb"]
+
+    # figure if the logstash-core-event-java gem is loaded and if so add its specific specs in the core specs to run
+    begin
+      require "logstash-core-event-java/version"
+      specs << "logstash-core-event-java/spec/**/*_spec.rb"
+    rescue LoadError
+      # logstash-core-event-java gem is not live, ignore and skip specs
+    end
+
+    Rake::FileList[*specs]
+  end
+
   desc "run core specs"
   task "core" => ["setup"] do
-    exit(RSpec::Core::Runner.run([Rake::FileList["spec/**/*_spec.rb"]]))
+    exit(RSpec::Core::Runner.run([core_specs]))
   end
 
   desc "run core specs in fail-fast mode"
   task "core-fail-fast" => ["setup"] do
-    exit(Spec::Core::Runner.run(["--fail-fast", Rake::FileList["spec/**/*_spec.rb"]]))
+    exit(RSpec::Core::Runner.run(["--fail-fast", core_specs]))
   end
 
   desc "run core specs on a single file"
-  task "core-single-file", [:specfile] => ["setup"] do |t,args|
+  task "core-single-file", [:specfile] => ["setup"] do |t, args|
     exit(RSpec::Core::Runner.run([Rake::FileList[args.specfile]]))
   end
 
@@ -87,6 +105,28 @@ namespace "test" do
     task.reenable
   end
 
+  task "integration" => ["setup"] do
+    require "fileutils" 
+
+    source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
+    integration_path = File.join(source, "integration_run")
+    FileUtils.rm_rf(integration_path)
+
+    exit(RSpec::Core::Runner.run([Rake::FileList["spec/integration/**/*_spec.rb"]]))
+  end
+
+  namespace "integration" do
+    task "local" => ["setup"] do
+      require "fileutils"
+
+      source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
+      integration_path = File.join(source, "integration_run")
+      FileUtils.mkdir_p(integration_path)
+
+      puts "[integration_spec] configuring local environment for running test in #{integration_path}, if you want to change this behavior delete the directory."
+      exit(RSpec::Core::Runner.run([Rake::FileList["spec/integration/**/*_spec.rb"]]))
+    end
+  end
 end
 
 task "test" => [ "test:core" ]
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
index f5ea8eb903c..b5cac5a353c 100644
--- a/rakelib/vendor.rake
+++ b/rakelib/vendor.rake
@@ -1,6 +1,6 @@
 namespace "vendor" do
   VERSIONS = {
-    "jruby" => { "version" => "1.7.22", "sha1" => "6b9e310a04ad8173d0d6dbe299da04c0ef85fc15" },
+    "jruby" => { "version" => "1.7.25", "sha1" => "cd15aef419f97cff274491e53fcfb8b88ec36785" },
   }
 
   def vendor(*args)
diff --git a/rakelib/z_rubycheck.rake b/rakelib/z_rubycheck.rake
index ed22ed016c7..bf077b8700b 100644
--- a/rakelib/z_rubycheck.rake
+++ b/rakelib/z_rubycheck.rake
@@ -32,7 +32,7 @@ if ENV['USE_RUBY'] != '1'
     # if required at this point system gems can be installed using the system_gem task, for example:
     # Rake::Task["vendor:system_gem"].invoke(jruby, "ffi", "1.9.6")
 
-    exec(jruby, "-S", rake, *ARGV)
+    exec(jruby, "-J-Xmx1g", "-S", rake, *ARGV)
   end
 end
 
diff --git a/require-analyze.rb b/require-analyze.rb
deleted file mode 100644
index f69d858aa45..00000000000
--- a/require-analyze.rb
+++ /dev/null
@@ -1,22 +0,0 @@
-require "csv"
-
-#0.003,psych/nodes/mapping,/Users/jls/.rvm/rubies/jruby-1.7.8/lib/ruby/shared/psych/nodes.rb:6:in `(root)'
-
-durations = {}
-durations.default = 0
-
-CSV.foreach(ARGV[0]) do |duration, path, source|
-  source, line, where = source.split(":")
-  #{"0.002"=>"/Users/jls/projects/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.3/lib/clamp.rb"}
-  if source.include?("jruby/1.9/gems")
-    # Get the gem name
-    source = source.gsub(/.*\/jruby\/1.9\/gems/, "")[/[^\/]+/]
-  elsif source.include?("/lib/logstash/")
-    source = source.gsub(/^.*(\/lib\/logstash\/)/, "/lib/logstash/")
-  end
-  durations[source] += duration.to_f
-end
-
-durations.sort_by { |k,v| v }.each do |k,v| 
-  puts "#{v} #{k}"
-end
diff --git a/spec/bootstrap/environment_spec.rb b/spec/bootstrap/environment_spec.rb
new file mode 100644
index 00000000000..f31cfcd38e8
--- /dev/null
+++ b/spec/bootstrap/environment_spec.rb
@@ -0,0 +1,6 @@
+# encoding: utf-8
+require "spec_helper"
+require "bootstrap/environment"
+
+describe LogStash::Environment do
+end
diff --git a/spec/core/pipeline_spec.rb b/spec/core/pipeline_spec.rb
deleted file mode 100644
index d0021d4a396..00000000000
--- a/spec/core/pipeline_spec.rb
+++ /dev/null
@@ -1,196 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-
-class DummyInput < LogStash::Inputs::Base
-  config_name "dummyinput"
-  milestone 2
-
-  def register
-  end
-
-  def run(queue)
-  end
-
-  def close
-  end
-end
-
-class DummyCodec < LogStash::Codecs::Base
-  config_name "dummycodec"
-  milestone 2
-
-  def decode(data)
-    data
-  end
-
-  def encode(event)
-    event
-  end
-
-  def close
-  end
-end
-
-class DummyOutput < LogStash::Outputs::Base
-  config_name "dummyoutput"
-  milestone 2
-
-  attr_reader :num_closes
-
-  def initialize(params={})
-    super
-    @num_closes = 0
-  end
-
-  def register
-  end
-
-  def receive(event)
-  end
-
-  def close
-    @num_closes += 1
-  end
-end
-
-class TestPipeline < LogStash::Pipeline
-  attr_reader :outputs
-end
-
-describe LogStash::Pipeline do
-
-context "close" do
-
-  before(:each) do
-    allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
-    allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
-  end
-
-    let(:test_config_without_output_workers) {
-      <<-eos
-      input {
-        dummyinput {}
-      }
-
-      output {
-        dummyoutput {}
-      }
-      eos
-    }
-
-    let(:test_config_with_output_workers) {
-      <<-eos
-      input {
-        dummyinput {}
-      }
-
-      output {
-        dummyoutput {
-          workers => 2
-        }
-      }
-      eos
-    }
-
-    context "output close" do
-      it "should call close of output without output-workers" do
-        pipeline = TestPipeline.new(test_config_without_output_workers)
-        pipeline.run
-
-        expect(pipeline.outputs.size ).to eq(1)
-        expect(pipeline.outputs.first.worker_plugins.size ).to eq(1)
-        expect(pipeline.outputs.first.worker_plugins.first.num_closes ).to eq(1)
-      end
-
-      it "should call output close correctly with output workers" do
-        pipeline = TestPipeline.new(test_config_with_output_workers)
-        pipeline.run
-
-        expect(pipeline.outputs.size ).to eq(1)
-        expect(pipeline.outputs.first.num_closes).to eq(0)
-        pipeline.outputs.first.worker_plugins.each do |plugin|
-          expect(plugin.num_closes ).to eq(1)
-        end
-      end
-    end
-  end
-
-  context "compiled flush function" do
-
-    context "cancelled events should not propagate down the filters" do
-      config <<-CONFIG
-        filter {
-          multiline {
-           pattern => "hello"
-           what => next
-          }
-          multiline {
-           pattern => "hello"
-           what => next
-          }
-        }
-      CONFIG
-
-      sample("hello") do
-        expect(subject["message"]).to eq("hello")
-      end
-    end
-
-    context "new events should propagate down the filters" do
-      config <<-CONFIG
-        filter {
-          clone {
-            clones => ["clone1"]
-          }
-          multiline {
-            pattern => "bar"
-            what => previous
-          }
-        }
-      CONFIG
-
-      sample(["foo", "bar"]) do
-        expect(subject.size).to eq(2)
-
-        expect(subject[0]["message"]).to eq("foo\nbar")
-        expect(subject[0]["type"]).to be_nil
-        expect(subject[1]["message"]).to eq("foo\nbar")
-        expect(subject[1]["type"]).to eq("clone1")
-      end
-    end
-  end
-
-  context "compiled filter funtions" do
-
-    context "new events should propagate down the filters" do
-      config <<-CONFIG
-        filter {
-          clone {
-            clones => ["clone1", "clone2"]
-          }
-          mutate {
-            add_field => {"foo" => "bar"}
-          }
-        }
-      CONFIG
-
-      sample("hello") do
-        expect(subject.size).to eq(3)
-
-        expect(subject[0]["message"]).to eq("hello")
-        expect(subject[0]["type"]).to be_nil
-        expect(subject[0]["foo"]).to eq("bar")
-
-        expect(subject[1]["message"]).to eq("hello")
-        expect(subject[1]["type"]).to eq("clone1")
-        expect(subject[1]["foo"]).to eq("bar")
-
-        expect(subject[2]["message"]).to eq("hello")
-        expect(subject[2]["type"]).to eq("clone2")
-        expect(subject[2]["foo"]).to eq("bar")
-      end
-    end
-
-  end
-end
diff --git a/spec/core/plugin_spec.rb b/spec/core/plugin_spec.rb
deleted file mode 100644
index 8248c37b75e..00000000000
--- a/spec/core/plugin_spec.rb
+++ /dev/null
@@ -1,123 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/plugin"
-
-describe LogStash::Plugin do
-  it "should fail lookup on inexisting type" do
-    expect_any_instance_of(Cabin::Channel).to receive(:debug).once
-    expect { LogStash::Plugin.lookup("badbadtype", "badname") }.to raise_error(LogStash::PluginLoadingError)
-  end
-
-  it "should fail lookup on inexisting name" do
-    expect_any_instance_of(Cabin::Channel).to receive(:debug).once
-    expect { LogStash::Plugin.lookup("filter", "badname") }.to raise_error(LogStash::PluginLoadingError)
-  end
-
-  it "should fail on bad plugin class" do
-    LogStash::Filters::BadSuperClass = Class.new
-    expect { LogStash::Plugin.lookup("filter", "bad_super_class") }.to raise_error(LogStash::PluginLoadingError)
-  end
-
-  it "should fail on missing config_name method" do
-    LogStash::Filters::MissingConfigName = Class.new(LogStash::Filters::Base)
-    expect { LogStash::Plugin.lookup("filter", "missing_config_name") }.to raise_error(LogStash::PluginLoadingError)
-  end
-
-  it "should lookup an already defined plugin class" do
-    class LogStash::Filters::LadyGaga < LogStash::Filters::Base
-      config_name "lady_gaga"
-    end
-    expect(LogStash::Plugin.lookup("filter", "lady_gaga")).to eq(LogStash::Filters::LadyGaga)
-  end
-
-  describe "#inspect" do
-    class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
-      config_name "param1"
-      config :num, :validate => :number, :default => 20
-      config :str, :validate => :string, :default => "test"
-    end
-    subject { LogStash::Filters::MyTestFilter.new("num" => 1, "str" => "hello") }
-
-    it "should print the class of the filter" do
-      expect(subject.inspect).to match(/^<LogStash::Filters::MyTestFilter/)
-    end
-    it "should list config options and values" do
-      expect(subject.inspect).to match(/num=>1, str=>"hello"/)
-    end
-  end
-
-  context "when validating the plugin version" do
-    let(:plugin_name) { 'logstash-filter-stromae' }
-    subject do
-      Class.new(LogStash::Filters::Base) do
-        config_name 'stromae'
-      end
-    end
-
-    it "doesn't warn the user if the version is superior or equal to 1.0.0" do
-      allow(Gem::Specification).to receive(:find_by_name)
-        .with(plugin_name)
-        .and_return(double(:version => Gem::Version.new('1.0.0')))
-
-      expect_any_instance_of(Cabin::Channel).not_to receive(:info)
-      subject.validate({})
-    end
-
-    it 'warns the user if the plugin version is between 0.9.x and 1.0.0' do
-      allow(Gem::Specification).to receive(:find_by_name)
-        .with(plugin_name)
-        .and_return(double(:version => Gem::Version.new('0.9.1')))
-
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
-        .with(/Using version 0.9.x/)
-
-      subject.validate({})
-    end
-
-    it 'warns the user if the plugin version is inferior to 0.9.x' do
-      allow(Gem::Specification).to receive(:find_by_name)
-        .with(plugin_name)
-        .and_return(double(:version => Gem::Version.new('0.1.1')))
-
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
-        .with(/Using version 0.1.x/)
-      subject.validate({})
-    end
-
-    it "doesnt show the version notice more than once" do
-      one_notice = Class.new(LogStash::Filters::Base) do
-        config_name "stromae"
-      end
-
-      allow(Gem::Specification).to receive(:find_by_name)
-        .with(plugin_name)
-        .and_return(double(:version => Gem::Version.new('0.1.1')))
-
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
-        .once
-        .with(/Using version 0.1.x/)
-
-      one_notice.validate({})
-      one_notice.validate({})
-    end
-
-    it "warns the user if we can't find a defined version" do
-      expect_any_instance_of(Cabin::Channel).to receive(:warn)
-        .once
-        .with(/plugin doesn't have a version/)
-
-      subject.validate({})
-    end
-    
-
-    it 'logs a warning if the plugin use the milestone option' do
-      expect_any_instance_of(Cabin::Channel).to receive(:warn)
-        .with(/stromae plugin is using the 'milestone' method/)
-
-      class LogStash::Filters::Stromae < LogStash::Filters::Base
-        config_name "stromae"
-        milestone 2
-      end
-    end
-  end
-end
diff --git a/spec/core/runner_spec.rb b/spec/core/runner_spec.rb
deleted file mode 100644
index b61cab7bf30..00000000000
--- a/spec/core/runner_spec.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/runner"
-require "stud/task"
-
-class NullRunner
-  def run(args); end
-end
-
-describe LogStash::Runner do
-
-  context "argument parsing" do
-    it "should run agent" do
-      expect(Stud::Task).to receive(:new).once.and_return(nil)
-      args = ["agent", "-e", ""]
-      expect(subject.run(args)).to eq(nil)
-    end
-
-    it "should run agent help" do
-      expect(subject).to receive(:show_help).once.and_return(nil)
-      args = ["agent", "-h"]
-      expect(subject.run(args).wait).to eq(0)
-    end
-
-    it "should show help with no arguments" do
-      expect($stderr).to receive(:puts).once.and_return("No command given")
-      expect($stderr).to receive(:puts).once
-      args = []
-      expect(subject.run(args).wait).to eq(1)
-    end
-
-    it "should show help for unknown commands" do
-      expect($stderr).to receive(:puts).once.and_return("No such command welp")
-      expect($stderr).to receive(:puts).once
-      args = ["welp"]
-      expect(subject.run(args).wait).to eq(1)
-    end
-
-  end
-end
diff --git a/spec/core/timestamp_spec.rb b/spec/core/timestamp_spec.rb
deleted file mode 100644
index 17f403ca009..00000000000
--- a/spec/core/timestamp_spec.rb
+++ /dev/null
@@ -1,84 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/timestamp"
-
-describe LogStash::Timestamp do
-
-  it "should parse its own iso8601 output" do
-    t = Time.now
-    ts = LogStash::Timestamp.new(t)
-    expect(LogStash::Timestamp.parse_iso8601(ts.to_iso8601).to_i).to eq(t.to_i)
-  end
-
-  it "should coerce iso8601 string" do
-    t = Time.now
-    ts = LogStash::Timestamp.new(t)
-    expect(LogStash::Timestamp.coerce(ts.to_iso8601).to_i).to eq(t.to_i)
-  end
-
-  it "should coerce Time" do
-    t = Time.now
-    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
-  end
-
-  it "should coerce Timestamp" do
-    t = LogStash::Timestamp.now
-    expect(LogStash::Timestamp.coerce(t).to_i).to eq(t.to_i)
-  end
-
-  it "should raise on invalid string coerce" do
-    expect{LogStash::Timestamp.coerce("foobar")}.to raise_error LogStash::TimestampParserError
-  end
-
-  it "should return nil on invalid object coerce" do
-    expect(LogStash::Timestamp.coerce(:foobar)).to be_nil
-  end
-
-  it "should support to_json" do
-    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json).to eq("\"2014-09-23T08:00:00.000Z\"")
-  end
-
-  it "should support to_json and ignore arguments" do
-    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :argumnents => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
-  end
-
-  it "should support timestamp comparaison" do
-   current = LogStash::Timestamp.new(Time.now) 
-   future = LogStash::Timestamp.new(Time.now + 100)
-
-   expect(future > current).to eq(true)
-   expect(future < current).to eq(false)
-   expect(current == current).to eq(true)
-
-   expect(current <=> current).to eq(0)
-   expect(current <=> future).to eq(-1)
-   expect(future <=> current).to eq(1)
-  end
-
-  it "should allow unary operation +" do
-    current = Time.now
-    t = LogStash::Timestamp.new(current) + 10
-    expect(t).to eq(current + 10)
-  end
-
-  describe "subtraction" do
-    it "should work on a timestamp object" do
-      t = Time.now
-      current = LogStash::Timestamp.new(t)
-      future = LogStash::Timestamp.new(t + 10)
-      expect(future - current).to eq(10)
-    end
-
-    it "should work on with time object" do
-      current = Time.now
-      t = LogStash::Timestamp.new(current + 10)
-      expect(t - current).to eq(10)
-    end
-
-    it "should work with numeric value" do
-      current = Time.now
-      t = LogStash::Timestamp.new(current + 10)
-      expect(t - 10).to eq(current)
-    end
-  end
-end
diff --git a/spec/coverage_helper.rb b/spec/coverage_helper.rb
index 6ecb2f570eb..009f7fb5419 100644
--- a/spec/coverage_helper.rb
+++ b/spec/coverage_helper.rb
@@ -5,20 +5,19 @@ module CoverageHelper
 
   ##
   # Skip list used to avoid loading certain patterns within
-  # the logstash directories, this patterns are excluded becuause
+  # the logstash directories, this patterns are excluded because
   # of potential problems or because they are going to be loaded
   # in another way.
   ##
   SKIP_LIST = Regexp.union([
     /^lib\/bootstrap\/rspec.rb$/,
-    /^lib\/logstash\/util\/prctl.rb$/
+    /^logstash-core\/lib\/logstash\/util\/prctl.rb$/
   ])
 
   def self.eager_load
-    Dir.glob("lib/**/*.rb") do |file|
+    Dir.glob("{logstash-core{/,-event/},}lib/**/*.rb") do |file|
       next if file =~ SKIP_LIST
       require file
     end
   end
-
 end
diff --git a/spec/integration/logstash_config/file_input_to_file_output_spec.rb b/spec/integration/logstash_config/file_input_to_file_output_spec.rb
new file mode 100644
index 00000000000..b9a4fcfd5c9
--- /dev/null
+++ b/spec/integration/logstash_config/file_input_to_file_output_spec.rb
@@ -0,0 +1,41 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+require "stud/temporary"
+
+describe "File input to File output" do
+  let(:number_of_events) { IO.readlines(sample_log).size }
+  let(:sample_log) { File.expand_path(File.join(File.dirname(__FILE__), "..", "support", "sample.log")) }
+  let(:output_file) { Stud::Temporary.file.path }
+  let(:config) { 
+<<EOS
+    input {
+       file {
+         path => \"#{sample_log}\"
+         stat_interval => 0
+         start_position => \"beginning\"
+         sincedb_path => \"/dev/null\"
+       }
+      }
+    output {
+      file {
+        path => \"#{output_file}\"
+      }
+    }
+EOS
+  }
+
+  before :all do
+    command("bin/logstash-plugin install logstash-input-file logstash-output-file")
+  end
+
+  it "writes events to file" do
+    cmd = "bin/logstash -e '#{config}'"
+    launch_logstash(cmd)
+
+    expect(File.exist?(output_file)).to eq(true)
+
+    # on shutdown the events arent flushed to disk correctly
+    # Known issue https://github.com/logstash-plugins/logstash-output-file/issues/12
+    expect(IO.readlines(output_file).size).to be_between(number_of_events - 10, number_of_events).inclusive
+  end
+end
diff --git a/spec/integration/plugin_manager/logstash_spec.rb b/spec/integration/plugin_manager/logstash_spec.rb
new file mode 100644
index 00000000000..3f839f060f2
--- /dev/null
+++ b/spec/integration/plugin_manager/logstash_spec.rb
@@ -0,0 +1,11 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+require_relative "../../../logstash-core/lib/logstash/version"
+
+describe "bin/logstash" do
+  it "returns the logstash version" do
+    result = command("bin/logstash --version")
+    expect(result.exit_status).to eq(0)
+    expect(result.stdout).to match(/^logstash\s#{LOGSTASH_VERSION}/)
+  end
+end
diff --git a/spec/integration/plugin_manager/plugin_install_spec.rb b/spec/integration/plugin_manager/plugin_install_spec.rb
new file mode 100644
index 00000000000..db31bc95740
--- /dev/null
+++ b/spec/integration/plugin_manager/plugin_install_spec.rb
@@ -0,0 +1,41 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+require "fileutils"
+
+context "bin/logstash-plugin install" do
+  context "with a local gem" do
+    let(:gem_name) { "logstash-input-wmi" }
+    let(:local_gem) { gem_fetch(gem_name) }
+
+    it "install the gem succesfully" do
+      result = command("bin/logstash-plugin install --no-verify #{local_gem}")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout).to match(/^Installing\s#{gem_name}\nInstallation\ssuccessful$/)
+    end
+  end
+
+  context "when the plugin exist" do
+    let(:plugin_name) { "logstash-input-drupal_dblog" }
+
+    it "sucessfully install" do
+      result = command("bin/logstash-plugin install #{plugin_name}")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout).to match(/^Validating\s#{plugin_name}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
+    end
+
+    it "allow to install a specific version" do
+      version = "2.0.2"
+      result = command("bin/logstash-plugin install --version 2.0.2 #{plugin_name}")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout).to match(/^Validating\s#{plugin_name}-#{version}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
+    end
+  end
+
+  context "when the plugin doesn't exist" do
+    it "fails to install" do
+      result = command("bin/logstash-plugin install --no-verify logstash-output-impossible-plugin")
+      expect(result.exit_status).to eq(1)
+      expect(result.stderr).to match(/Installation Aborted, message: Could not find gem/)
+    end
+  end
+end
diff --git a/spec/integration/plugin_manager/plugin_list_spec.rb b/spec/integration/plugin_manager/plugin_list_spec.rb
new file mode 100644
index 00000000000..d2ae7807f1c
--- /dev/null
+++ b/spec/integration/plugin_manager/plugin_list_spec.rb
@@ -0,0 +1,41 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+
+describe "bin/logstash-plugin list" do
+  context "without a specific plugin" do
+    it "display a list of plugins" do
+      result = command("bin/logstash-plugin list")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout.split("\n").size).to be > 1
+    end
+
+    it "display a list of installed plugins" do
+      result = command("bin/logstash-plugin list --installed")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout.split("\n").size).to be > 1
+    end
+
+    it "list the plugins with their versions" do
+      result = command("bin/logstash-plugin list --verbose")
+      result.stdout.split("\n").each do |plugin|
+        expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+\)/)
+      end
+      expect(result.exit_status).to eq(0)
+    end
+  end
+
+  context "with a specific plugin" do
+    let(:plugin_name) { "logstash-input-stdin" }
+    it "list the plugin and display the plugin name" do
+      result = command("bin/logstash-plugin list #{plugin_name}")
+      expect(result.stdout).to match(/^#{plugin_name}$/)
+      expect(result.exit_status).to eq(0)
+    end
+
+    it "list the plugin with his version" do
+      result = command("bin/logstash-plugin list --verbose #{plugin_name}")
+      expect(result.stdout).to match(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
+      expect(result.exit_status).to eq(0)
+    end
+  end
+end
diff --git a/spec/integration/plugin_manager/plugin_new_spec.rb b/spec/integration/plugin_manager/plugin_new_spec.rb
new file mode 100644
index 00000000000..a961cea4bcf
--- /dev/null
+++ b/spec/integration/plugin_manager/plugin_new_spec.rb
@@ -0,0 +1,53 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+require "fileutils"
+
+describe "bin/logstash-plugin generate" do
+
+  shared_examples "bin/logstash-plugin generate" do
+    let(:plugin_name)      { "dummy" }
+    let(:full_plugin_name) { "logstash-#{plugin_type}-#{plugin_name}" }
+
+    describe "plugin creation" do
+
+      before(:each) do
+        FileUtils.rm_rf(full_plugin_name)
+      end
+
+      after(:each) do
+        FileUtils.rm_rf(full_plugin_name)
+      end
+
+      it "generate a new plugin" do
+        result = command("bin/logstash-plugin generate --type #{plugin_type} --name #{plugin_name}")
+        expect(result.exit_status).to eq(0)
+        expect(result.stdout).to match(/Creating #{full_plugin_name}/)
+        expect(Dir.exist?("#{full_plugin_name}")).to eq(true)
+      end
+
+      it "raise an error if the plugin is already generated" do
+        command("bin/logstash-plugin generate --type #{plugin_type} --name #{plugin_name}")
+        result = command("bin/logstsh-plugin generate --type #{plugin_type} --name #{plugin_name}")
+        expect(result.exit_status).to eq(1)
+      end
+    end
+  end
+
+  describe "bin/logstash-plugin generate input" do
+    it_behaves_like "bin/logstash-plugin generate" do
+      let(:plugin_type) { "input" }
+    end
+  end
+
+  describe "bin/logstash-plugin generate filter" do
+    it_behaves_like "bin/logstash-plugin generate" do
+      let(:plugin_type) { "filter" }
+    end
+  end
+
+  describe "bin/logstash-plugin generate output" do
+    it_behaves_like "bin/logstash-plugin generate" do
+      let(:plugin_type) { "output" }
+    end
+  end
+end
diff --git a/spec/integration/plugin_manager/plugin_uninstall_spec.rb b/spec/integration/plugin_manager/plugin_uninstall_spec.rb
new file mode 100644
index 00000000000..df3c6e4396e
--- /dev/null
+++ b/spec/integration/plugin_manager/plugin_uninstall_spec.rb
@@ -0,0 +1,24 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+
+describe "bin/logstash-plugin uninstall" do
+  context "when the plugin isn't installed" do
+    it "fails to uninstall it" do
+      result = command("bin/logstash-plugin uninstall logstash-filter-cidr")
+      expect(result.stderr).to match(/ERROR: Uninstall Aborted, message: This plugin has not been previously installed, aborting/)
+      expect(result.exit_status).to eq(1)
+    end
+  end
+
+  context "when the plugin is installed" do
+      it "succesfully uninstall it" do
+      # make sure we have the plugin installed.
+      command("bin/logstash-plugin install logstash-filter-ruby")
+
+      result = command("bin/logstash-plugin uninstall logstash-filter-ruby")
+
+      expect(result.stdout).to match(/^Uninstalling logstash-filter-ruby/)
+      expect(result.exit_status).to eq(0)
+    end
+  end
+end
diff --git a/spec/integration/plugin_manager/plugin_update_spec.rb b/spec/integration/plugin_manager/plugin_update_spec.rb
new file mode 100644
index 00000000000..549a9babc80
--- /dev/null
+++ b/spec/integration/plugin_manager/plugin_update_spec.rb
@@ -0,0 +1,32 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+
+describe "update" do
+  let(:plugin_name) { "logstash-input-stdin" }
+  let(:previous_version) { "2.0.1" }
+
+  before do
+    command("bin/logstash-plugin install --version #{previous_version} #{plugin_name}")
+    cmd = command("bin/logstash-plugin list --verbose #{plugin_name}")
+    expect(cmd.stdout).to match(/#{plugin_name} \(#{previous_version}\)/)
+  end
+
+  context "update a specific plugin" do
+    subject { command("bin/logstash-plugin update #{plugin_name}") }
+
+    it "has executed succesfully" do
+      expect(subject.exit_status).to eq(0)
+      expect(subject.stdout).to match(/Updating #{plugin_name}/)
+    end
+  end
+
+  context "update all the plugins" do
+    subject { command("bin/logstash-plugin update") }
+
+    it "has executed succesfully" do
+      expect(subject.exit_status).to eq(0)
+      cmd = command("bin/logstash-plugin list --verbose #{plugin_name}").stdout
+      expect(cmd).to match(/logstash-input-stdin \(#{LogStashTestHelpers.latest_version(plugin_name)}\)/)
+    end
+  end
+end
diff --git a/spec/integration/spec_helper.rb b/spec/integration/spec_helper.rb
new file mode 100644
index 00000000000..065c812411f
--- /dev/null
+++ b/spec/integration/spec_helper.rb
@@ -0,0 +1,37 @@
+# encoding: utf-8
+require_relative "support/integration_test_helpers"
+require_relative "../../logstash-core/lib/logstash/environment"
+require "fileutils"
+
+if LogStash::Environment.windows?
+  puts "[integration] Theses integration test are specifically made to be run on under linux/unix"
+  puts "[integration] Please see our windows version of the tests https://github.com/elastic/logstash/tree/master/test/windows"
+end
+
+# Configure the test environment
+source = File.expand_path(File.join(File.dirname(__FILE__), "..", ".."))
+integration_path = File.join(source, "integration_run")
+
+puts "[integration_spec] configure environment"
+
+if Dir.exists?(integration_path)
+  # We copy the current logstash into a temporary directory
+  # since the tests are a bit destructive
+  FileUtils.mkdir_p(integration_path)
+  rsync_cmd = "rsync -a --delete --exclude 'rspec' --exclude '#{File.basename(integration_path)}' --exclude 'integration_spec' --exclude '.git' #{source} #{integration_path}"
+
+  puts "[integration_spec] Rsync source code into: #{integration_path}"
+  system(rsync_cmd)
+  puts "[integration_spec] Finish rsync"
+
+  LOGSTASH_TEST_PATH = File.join(integration_path, "logstash")
+else
+  LOGSTASH_TEST_PATH = File.expand_path(File.join(File.dirname(__FILE__), ".."))
+end
+
+puts "[integration_spec] Running the test in #{LOGSTASH_TEST_PATH}"
+puts "[integration_spec] Running specs"
+
+RSpec.configure do |config|
+  config.order = "random"
+end
diff --git a/spec/integration/support/integration_test_helpers.rb b/spec/integration/support/integration_test_helpers.rb
new file mode 100644
index 00000000000..aad90f8f07a
--- /dev/null
+++ b/spec/integration/support/integration_test_helpers.rb
@@ -0,0 +1,89 @@
+# encoding: utf-8
+require "json"
+require "open3"
+require "open-uri"
+require "stud/temporary"
+require "fileutils"
+require "bundler"
+require "gems"
+
+class CommandResponse
+  attr_reader :stdin, :stdout, :stderr, :exit_status
+
+  def initialize(cmd, stdin, stdout, stderr, exit_status)
+    @stdin = stdin
+    @stdout = stdout
+    @stderr = stderr
+    @exit_status = exit_status
+    @cmd = cmd
+  end
+
+  def to_debug
+    "DEBUG: stdout: #{stdout}, stderr: #{stderr}, exit_status: #{exit_status}"
+  end
+
+  def to_s
+    @cmd
+  end
+end
+
+def command(cmd, path = nil)
+  # http://bundler.io/v1.3/man/bundle-exec.1.html
+  # see shelling out.
+  #
+  # Since most of the integration test are environment destructive
+  # its better to run them in a cloned directory.
+  path = LOGSTASH_TEST_PATH if path == nil
+
+  Bundler.with_clean_env do
+    Dir.chdir(path) do
+      Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr|
+          CommandResponse.new(cmd,
+            stdin,
+            stdout.read.chomp,
+            stderr.read.chomp,
+            wait_thr.value.exitstatus)
+      end
+    end
+  end
+end
+
+def gem_fetch(name)
+  tmp = Stud::Temporary.directory
+  FileUtils.mkdir_p(tmp)
+
+  c = command("gem fetch #{name}", tmp)
+
+  if c.exit_status == 1
+    raise RuntimeError, "Can't fetch gem #{name}"
+  end
+
+  return Dir.glob(File.join(tmp, "#{name}*.gem")).first
+end
+
+# This is a bit hacky since JRuby doesn't support fork,
+# we use popen4 which return the pid of the process and make sure we kill it
+# after letting it run for a few seconds.
+def launch_logstash(cmd, path = nil)
+  path = LOGSTASH_TEST_PATH if path == nil
+  pid = 0
+
+  Thread.new do
+    Bundler.with_clean_env do
+      Dir.chdir(path) do
+        pid, input, output, error = IO.popen4(cmd) #jruby only
+      end
+    end
+  end
+  sleep(30)
+  begin
+    Process.kill("INT", pid)
+  rescue
+  end
+end
+
+module LogStashTestHelpers
+  def self.latest_version(name)
+    Gems.versions(name).first["number"] 
+  end
+end
diff --git a/spec/integration/support/sample.log b/spec/integration/support/sample.log
new file mode 100644
index 00000000000..8f304b59c45
--- /dev/null
+++ b/spec/integration/support/sample.log
@@ -0,0 +1,50 @@
+83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1" 200 171717 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/highlight/highlight.js HTTP/1.1" 200 26185 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/zoom-js/zoom.js HTTP/1.1" 200 7697 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/plugin/notes/notes.js HTTP/1.1" 200 2892 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/sad-medic.png HTTP/1.1" 200 430406 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Bold.ttf HTTP/1.1" 200 38720 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Regular.ttf HTTP/1.1" 200 41820 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1" 200 52878 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard.png HTTP/1.1" 200 321631 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/Dreamhost_logo.svg HTTP/1.1" 200 2126 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard2.png HTTP/1.1" 200 394967 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/apache-icon.gif HTTP/1.1" 200 8095 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/nagios-sms5.png HTTP/1.1" 200 78075 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/redis.png HTTP/1.1" 200 25230 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/elasticsearch.png HTTP/1.1" 200 8026 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/logstashbook.png HTTP/1.1" 200 54662 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/github-contributions.png HTTP/1.1" 200 34245 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/css/print/paper.css HTTP/1.1" 200 4254 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/1983_delorean_dmc-12-pic-38289.jpeg HTTP/1.1" 200 220562 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/simple-inputs-filters-outputs.jpg HTTP/1.1" 200 1168622 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/tiered-outputs-to-inputs.jpg HTTP/1.1" 200 1079983 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:53 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+24.236.252.67 - - [26/Aug/2014:21:14:10 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:26.0) Gecko/20100101 Firefox/26.0"
+93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1" 200 18848 "http://www.google.ro/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCwQFjAB&url=http%3A%2F%2Fwww.semicomplete.com%2Farticles%2Fdynamic-dns-with-dhcp%2F&ei=W88AU4n9HOq60QXbv4GwBg&usg=AFQjCNEF1X4Rs52UYQyLiySTQxa97ozM4g&bvm=bv.61535280,d.d2k" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+66.249.73.135 - - [26/Aug/2014:21:15:03 +0000] "GET /blog/tags/ipv6 HTTP/1.1" 200 12251 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+50.16.19.13 - - [26/Aug/2014:21:15:15 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "http://www.semicomplete.com/blog/tags/puppet?flav=rss20" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
+66.249.73.185 - - [26/Aug/2014:21:15:23 +0000] "GET / HTTP/1.1" 200 37932 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+110.136.166.128 - - [26/Aug/2014:21:16:11 +0000] "GET /projects/xdotool/ HTTP/1.1" 200 12292 "http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&sqi=2&ved=0CFYQFjAE&url=http%3A%2F%2Fwww.semicomplete.com%2Fprojects%2Fxdotool%2F&ei=6cwAU_bRHo6urAeI0YD4Ag&usg=AFQjCNE3V_aCf3-gfNcbS924S6jZ6FqffA&bvm=bv.61535280,d.bmk" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+46.105.14.53 - - [26/Aug/2014:21:16:17 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+123.125.71.35 - - [26/Aug/2014:21:16:31 +0000] "GET /blog/tags/release HTTP/1.1" 200 40693 "-" "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+50.150.204.184 - - [26/Aug/2014:21:17:06 +0000] "GET /images/googledotcom.png HTTP/1.1" 200 65748 "http://www.google.com/search?q=https//:google.com&source=lnms&tbm=isch&sa=X&ei=4-r8UvDrKZOgkQe7x4CICw&ved=0CAkQ_AUoAA&biw=320&bih=441" "Mozilla/5.0 (Linux; U; Android 4.0.4; en-us; LG-MS770 Build/IMM76I) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30"
+207.241.237.225 - - [26/Aug/2014:21:17:35 +0000] "GET /blog/tags/examples HTTP/1.0" 200 9208 "http://www.semicomplete.com/blog/tags/C" "Mozilla/5.0 (compatible; archive.org_bot +http://www.archive.org/details/archive.org_bot)"
+200.49.190.101 - - [26/Aug/2014:21:17:39 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
+200.49.190.100 - - [26/Aug/2014:21:17:37 +0000] "GET /blog/tags/web HTTP/1.1" 200 44019 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
+200.49.190.101 - - [26/Aug/2014:21:17:41 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
+200.49.190.101 - - [26/Aug/2014:21:17:48 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
+66.249.73.185 - - [26/Aug/2014:21:18:48 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+66.249.73.135 - - [26/Aug/2014:21:18:55 +0000] "GET /blog/tags/munin HTTP/1.1" 200 9746 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+66.249.73.135 - - [26/Aug/2014:21:19:16 +0000] "GET /blog/tags/firefox?flav=rss20 HTTP/1.1" 200 16021 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
diff --git a/spec/logstash/agent_spec.rb b/spec/logstash/agent_spec.rb
deleted file mode 100644
index 54f994d88f4..00000000000
--- a/spec/logstash/agent_spec.rb
+++ /dev/null
@@ -1,62 +0,0 @@
-# encoding: utf-8
-require 'spec_helper'
-
-describe LogStash::Agent do
-  subject { LogStash::Agent.new('') }
-  let(:dummy_config) { 'input {}' }
-
-  context "when loading the configuration" do
-    context "when local" do
-      before { expect(subject).to receive(:local_config).with(path) }
-
-      context "unix" do
-        let(:path) { './test.conf' }
-        it 'works with relative path' do
-          subject.load_config(path)
-        end
-      end
-
-      context "windows" do
-        let(:path) { '.\test.conf' }
-        it 'work with relative windows path' do
-          subject.load_config(path)
-        end
-      end
-    end
-
-    context "when remote" do
-      context 'supported scheme' do
-        let(:path) { "http://test.local/superconfig.conf" }
-
-        before { expect(Net::HTTP).to receive(:get) { dummy_config } }
-        it 'works with http' do
-          expect(subject.load_config(path)).to eq("#{dummy_config}\n")
-        end
-      end
-    end
-  end
-
-  context "--pluginpath" do
-    let(:single_path) { "/some/path" }
-    let(:multiple_paths) { ["/some/path1", "/some/path2"] }
-
-    it "should add single valid dir path to the environment" do
-      expect(File).to receive(:directory?).and_return(true)
-      expect(LogStash::Environment).to receive(:add_plugin_path).with(single_path)
-      subject.configure_plugin_paths(single_path)
-    end
-
-    it "should fail with single invalid dir path" do
-      expect(File).to receive(:directory?).and_return(false)
-      expect(LogStash::Environment).not_to receive(:add_plugin_path)
-      expect{subject.configure_plugin_paths(single_path)}.to raise_error(LogStash::ConfigurationError)
-    end
-
-    it "should add multiple valid dir path to the environment" do
-      expect(File).to receive(:directory?).exactly(multiple_paths.size).times.and_return(true)
-      multiple_paths.each{|path| expect(LogStash::Environment).to receive(:add_plugin_path).with(path)}
-      subject.configure_plugin_paths(multiple_paths)
-    end
-  end
-end
-
diff --git a/spec/outputs/base_spec.rb b/spec/outputs/base_spec.rb
deleted file mode 100644
index 841ba424df9..00000000000
--- a/spec/outputs/base_spec.rb
+++ /dev/null
@@ -1,26 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-
-# use a dummy NOOP output to test Outputs::Base
-class LogStash::Outputs::NOOP < LogStash::Outputs::Base
-  config_name "noop"
-  milestone 2
-
-  config :dummy_option, :validate => :string
-
-  def register; end
-
-  def receive(event)
-    return output?(event)
-  end
-end
-
-describe "LogStash::Outputs::Base#worker_setup" do
-  it "should create workers using original parameters except workers = 1" do
-    params = { "dummy_option" => "potatoes", "codec" => "json", "workers" => 2 }
-    worker_params = params.dup; worker_params["workers"] = 1
-    output = LogStash::Outputs::NOOP.new(params.dup)
-    expect(LogStash::Outputs::NOOP).to receive(:new).twice.with(worker_params).and_call_original
-    output.worker_setup
-  end
-end
diff --git a/spec/pluginmanager/util_spec.rb b/spec/pluginmanager/util_spec.rb
deleted file mode 100644
index 6a14beeb950..00000000000
--- a/spec/pluginmanager/util_spec.rb
+++ /dev/null
@@ -1,42 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "pluginmanager/util"
-require "gems"
-
-describe LogStash::PluginManager do
-
-  let(:plugin_name) { "logstash-output-elasticsearch" }
-
-  let(:version_data) do
-    [ { "authors"=>"Elastic", "built_at"=>"2015-08-11T00:00:00.000Z", "description"=>"Output events to elasticsearch",
-        "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"2.0.0.pre",
-        "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>true,
-        "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"},
-    { "authors"=>"Elastic", "built_at"=>"2015-08-10T00:00:00.000Z", "description"=>"Output events to elasticsearch",
-      "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"1.0.7",
-      "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>false,
-      "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"},
-    { "authors"=>"Elastic", "built_at"=>"2015-08-09T00:00:00.000Z", "description"=>"Output events to elasticsearch",
-      "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"1.0.4",
-      "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>false,
-      "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"} ]
-  end
-
-  before(:each) do
-    allow(Gems).to receive(:versions).with(plugin_name).and_return(version_data)
-  end
-
-  context "fetch plugin info" do
-
-    it "should search for the last version infomation non prerelease" do
-      version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name)
-      expect(version_info["number"]).to eq("1.0.7")
-    end
-
-
-    it "should search for the last version infomation with prerelease" do
-      version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name, :pre => true)
-      expect(version_info["number"]).to eq("2.0.0.pre")
-    end
-  end
-end
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index 5428fd8fd90..bb2ccba5c7d 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -5,6 +5,35 @@
 CoverageHelper.eager_load if ENV['COVERAGE']
 
 require "logstash/devutils/rspec/spec_helper"
+require "logstash/logging/json"
+
+class JSONIOThingy < IO
+  def initialize; end
+  def flush; end
+
+  def puts(payload)
+    # Ensure that all log payloads are valid json.
+    LogStash::Json.load(payload)
+  end
+end
+
+RSpec.configure do |c|
+  c.before do
+    # Force Cabin to always have a JSON subscriber.  The main purpose of this
+    # is to catch crashes in json serialization for our logs. JSONIOThingy
+    # exists to validate taht what LogStash::Logging::JSON emits is always
+    # valid JSON.
+    jsonvalidator = JSONIOThingy.new
+    allow(Cabin::Channel).to receive(:new).and_wrap_original do |m, *args|
+      logger = m.call(*args)
+      logger.level = :debug
+      logger.subscribe(LogStash::Logging::JSON.new(jsonvalidator))
+
+      logger
+    end
+  end
+
+end
 
 def installed_plugins
   Gem::Specification.find_all.select { |spec| spec.metadata["logstash_plugin"] }.map { |plugin| plugin.name }
diff --git a/spec/lib/logstash/bundler_spec.rb b/spec/unit/bootstrap/bundler_spec.rb
similarity index 100%
rename from spec/lib/logstash/bundler_spec.rb
rename to spec/unit/bootstrap/bundler_spec.rb
diff --git a/spec/license_spec.rb b/spec/unit/license_spec.rb
similarity index 77%
rename from spec/license_spec.rb
rename to spec/unit/license_spec.rb
index 6a0ec7ba4b0..9425cfc9111 100644
--- a/spec/license_spec.rb
+++ b/spec/unit/license_spec.rb
@@ -12,6 +12,7 @@
     Regexp.union([ /mit/,
                    /apache*/,
                    /bsd/,
+                   /artistic 2.*/,
                    /ruby/,
                    /lgpl/])
   }
@@ -24,7 +25,11 @@
     [
       # Skipped because of already included and bundled within JRuby so checking here is redundant.
       # Need to take action about jruby licenses to enable again or keep skeeping.
-      "jruby-openssl"
+      "jruby-openssl",
+      # Skipped because version 2.6.2 which we use has multiple licenses: MIT, ARTISTIC 2.0, GPL-2
+      # See https://rubygems.org/gems/mime-types/versions/2.6.2
+      # version 3.0 of mime-types (which is only compatible with Ruby 2.0) is MIT licensed
+      "mime-types"
     ]
   end
 
@@ -47,7 +52,8 @@
         next unless runtime_spec
         next if skipped_dependencies.include?(runtime_spec.name)
         runtime_spec.licenses.each do |license|
-          expect(license.downcase).to match(expected_licenses)
+          expect(license.downcase).to match(expected_licenses), 
+            lambda { "Runtime license check failed for gem #{runtime_spec.name} with version #{runtime_spec.version}" }
         end
       end
     end
diff --git a/spec/util/gemfile_spec.rb b/spec/unit/plugin_manager/gemfile_spec.rb
similarity index 100%
rename from spec/util/gemfile_spec.rb
rename to spec/unit/plugin_manager/gemfile_spec.rb
diff --git a/spec/unit/plugin_manager/install_spec.rb b/spec/unit/plugin_manager/install_spec.rb
new file mode 100644
index 00000000000..40eb3dfe408
--- /dev/null
+++ b/spec/unit/plugin_manager/install_spec.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require 'spec_helper'
+require 'pluginmanager/main'
+
+describe LogStash::PluginManager::Install do
+  let(:cmd) { LogStash::PluginManager::Install.new("install") }
+
+  before(:each) do
+    expect(cmd).to receive(:validate_cli_options!).and_return(nil)
+  end
+
+  context "when validating plugins" do
+    let(:sources) { ["https://rubygems.org", "http://localhost:9292"] }
+
+    before(:each) do
+      expect(cmd).to receive(:plugins_gems).and_return([["dummy", nil]])
+      expect(cmd).to receive(:install_gems_list!).and_return(nil)
+      expect(cmd).to receive(:remove_unused_locally_installed_gems!).and_return(nil)
+      cmd.verify = true
+    end
+
+    it "should load all the sources defined in the Gemfile" do
+      expect(cmd.gemfile.gemset).to receive(:sources).and_return(sources)
+      expect(LogStash::PluginManager).to receive(:logstash_plugin?).with("dummy", nil, {:rubygems_source => sources}).and_return(true)
+      cmd.execute
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/update_spec.rb b/spec/unit/plugin_manager/update_spec.rb
new file mode 100644
index 00000000000..5498f9dea0c
--- /dev/null
+++ b/spec/unit/plugin_manager/update_spec.rb
@@ -0,0 +1,39 @@
+# encoding: utf-8
+require 'spec_helper'
+require 'pluginmanager/main'
+
+describe LogStash::PluginManager::Update do
+  let(:cmd)     { LogStash::PluginManager::Update.new("update") }
+  let(:sources) { cmd.gemfile.gemset.sources }
+
+  before(:each) do
+    expect(cmd).to receive(:find_latest_gem_specs).and_return({})
+    allow(cmd).to receive(:warn_local_gems).and_return(nil)
+    expect(cmd).to receive(:display_updated_plugins).and_return(nil)
+    expect_any_instance_of(LogStash::Bundler).to receive(:invoke!).with(:clean => true)
+  end
+
+  it "pass all gem sources to the bundle update command" do
+    sources = cmd.gemfile.gemset.sources
+    expect_any_instance_of(LogStash::Bundler).to receive(:invoke!).with(:update => [], :rubygems_source => sources)
+    cmd.execute
+  end
+
+  context "when skipping validation" do
+    let(:cmd)    { LogStash::PluginManager::Update.new("update") }
+    let(:plugin) { OpenStruct.new(:name => "dummy", :options => {} ) }
+
+    before(:each) do
+      expect(cmd.gemfile).to receive(:find).with(plugin).and_return(plugin)
+      expect(cmd.gemfile).to receive(:save).and_return(nil)
+      expect(cmd).to receive(:plugins_to_update).and_return([plugin])
+      expect_any_instance_of(LogStash::Bundler).to receive(:invoke!).with(:update => [plugin], :rubygems_source => sources).and_return(nil)
+    end
+
+    it "skips version verification when ask for it" do
+      cmd.verify = false
+      expect(cmd).to_not receive(:validates_version)
+      cmd.execute
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/util_spec.rb b/spec/unit/plugin_manager/util_spec.rb
new file mode 100644
index 00000000000..10824e56adc
--- /dev/null
+++ b/spec/unit/plugin_manager/util_spec.rb
@@ -0,0 +1,71 @@
+#encoding: utf-8
+require 'spec_helper'
+require 'pluginmanager/util'
+require 'gems'
+
+describe LogStash::PluginManager do
+
+  describe "fetching plugin information" do
+    let(:plugin_name) { "logstash-output-elasticsearch" }
+
+    let(:version_data) do
+      [ { "authors"=>"Elastic", "built_at"=>"2015-08-11T00:00:00.000Z", "description"=>"Output events to elasticsearch",
+          "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"2.0.0.pre",
+          "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>true,
+          "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"},
+      { "authors"=>"Elastic", "built_at"=>"2015-08-10T00:00:00.000Z", "description"=>"Output events to elasticsearch",
+        "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"1.0.7",
+        "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>false,
+        "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"},
+      { "authors"=>"Elastic", "built_at"=>"2015-08-09T00:00:00.000Z", "description"=>"Output events to elasticsearch",
+        "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"1.0.4",
+        "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>false,
+        "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"} ]
+    end
+
+    before(:each) do
+      allow(Gems).to receive(:versions).with(plugin_name).and_return(version_data)
+    end
+
+    context "fetch plugin info" do
+      it "should search for the last version infomation non prerelease" do
+        version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name)
+        expect(version_info["number"]).to eq("1.0.7")
+      end
+
+
+      it "should search for the last version infomation with prerelease" do
+        version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name, :pre => true)
+        expect(version_info["number"]).to eq("2.0.0.pre")
+      end
+    end
+  end
+
+  describe "a logstash_plugin validation" do
+    let(:plugin)  { "foo" }
+    let(:version) { "9.0.0.0" }
+
+    let(:sources) { ["http://source.01", "http://source.02"] }
+    let(:options) { {:rubygems_source => sources} }
+
+    let(:gemset)  { double("gemset") }
+    let(:gemfile) { double("gemfile") }
+    let(:dep)     { double("dep") }
+    let(:fetcher) { double("fetcher") }
+
+    before(:each) do
+      allow(gemfile).to  receive(:gemset).and_return(gemset)
+      allow(gemset).to   receive(:sources).and_return(sources)
+      expect(fetcher).to receive(:spec_for_dependency).and_return([[],[]])
+    end
+
+    it "should load all available sources" do
+      expect(subject).to receive(:plugin_file?).and_return(false)
+      expect(Gem::Dependency).to receive(:new).and_return(dep)
+      expect(Gem::SpecFetcher).to receive(:fetcher).and_return(fetcher)
+
+      subject.logstash_plugin?(plugin, version, options)
+      expect(Gem.sources.map { |source| source }).to eq(sources)
+    end
+  end
+end
diff --git a/spec/unit/util/compress_spec.rb b/spec/unit/util/compress_spec.rb
new file mode 100644
index 00000000000..47bab9e995a
--- /dev/null
+++ b/spec/unit/util/compress_spec.rb
@@ -0,0 +1,121 @@
+# encoding: utf-8
+require "spec_helper"
+require 'ostruct'
+require "bootstrap/util/compress"
+
+describe LogStash::Util::Zip do
+
+  subject { Class.new { extend LogStash::Util::Zip } }
+
+  context "#extraction" do
+
+    let(:source) { File.join(File.expand_path("."), "source_file.zip") }
+    let(:target) { File.expand_path("target_dir") }
+
+    it "raise an exception if the target dir exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.extract(source, target) }.to raise_error
+    end
+
+    let(:zip_file) do
+      [ "foo", "bar", "zoo" ].inject([]) do |acc, name|
+        acc << OpenStruct.new(:name => name)
+        acc
+      end
+    end
+
+    it "extract the list of entries from a zip file" do
+      allow(Zip::File).to receive(:open).with(source).and_yield(zip_file)
+      expect(FileUtils).to receive(:mkdir_p).exactly(3).times
+      expect(zip_file).to receive(:extract).exactly(3).times
+      subject.extract(source, target)
+    end
+  end
+
+  context "#compression" do
+
+    let(:target) { File.join(File.expand_path("."), "target_file.zip") }
+    let(:source) { File.expand_path("source_dir") }
+
+    it "raise an exception if the target file exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.compress(source, target) }.to raise_error
+    end
+
+    let(:dir_files) do
+      [ "foo", "bar", "zoo" ]
+    end
+
+    let(:zip_file) { Class.new }
+
+    it "add a dir to a zip file" do
+      allow(Zip::File).to receive(:open).with(target, ::Zip::File::CREATE).and_yield(zip_file)
+      allow(Dir).to receive(:glob).and_return(dir_files)
+      expect(zip_file).to receive(:add).exactly(3).times
+      subject.compress(source, target)
+    end
+  end
+end
+
+describe LogStash::Util::Tar do
+
+  subject { Class.new { extend LogStash::Util::Tar } }
+
+  context "#extraction" do
+
+    let(:source) { File.join(File.expand_path("."), "source_file.tar.gz") }
+    let(:target) { File.expand_path("target_dir") }
+
+    it "raise an exception if the target dir exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.extract(source, target) }.to raise_error
+    end
+
+    let(:gzip_file) { Class.new }
+
+    let(:tar_file) do
+      [ "foo", "bar", "zoo" ].inject([]) do |acc, name|
+        acc << OpenStruct.new(:full_name => name)
+        acc
+      end
+    end
+
+    it "extract the list of entries from a tar.gz file" do
+      allow(Zlib::GzipReader).to receive(:open).with(source).and_yield(gzip_file)
+      allow(Gem::Package::TarReader).to receive(:new).with(gzip_file).and_yield(tar_file)
+
+      expect(FileUtils).to receive(:mkdir).with(target)
+      expect(File).to receive(:open).exactly(3).times
+      subject.extract(source, target)
+    end
+  end
+
+  context "#compression" do
+
+    let(:target) { File.join(File.expand_path("."), "target_file.tar.gz") }
+    let(:source) { File.expand_path("source_dir") }
+
+    it "raise an exception if the target file exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.compress(source, target) }.to raise_error
+    end
+
+    let(:dir_files) do
+      [ "foo", "bar", "zoo" ]
+    end
+
+    let(:tar_file) { Class.new }
+    let(:tar)      { Class.new }
+
+    it "add a dir to a tgz file" do
+      allow(Stud::Temporary).to receive(:file).and_yield(tar_file)
+      allow(Gem::Package::TarWriter).to receive(:new).with(tar_file).and_yield(tar)
+      allow(Dir).to receive(:glob).and_return(dir_files)
+      expect(File).to receive(:stat).exactly(3).times.and_return(OpenStruct.new(:mode => "rw"))
+      expect(tar).to receive(:add_file).exactly(3).times
+      expect(tar_file).to receive(:rewind)
+      expect(subject).to receive(:gzip).with(target, tar_file)
+      subject.compress(source, target)
+    end
+  end
+end
diff --git a/spec/util/retryable_spec.rb b/spec/unit/util/retryable_spec.rb
similarity index 100%
rename from spec/util/retryable_spec.rb
rename to spec/unit/util/retryable_spec.rb
diff --git a/spec/util/defaults_printer_spec.rb b/spec/util/defaults_printer_spec.rb
deleted file mode 100644
index 3e50a7032cb..00000000000
--- a/spec/util/defaults_printer_spec.rb
+++ /dev/null
@@ -1,49 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/util/defaults_printer"
-
-describe LogStash::Util::DefaultsPrinter do
-  shared_examples "a defaults printer" do
-    it 'the .print method returns a defaults description' do
-      expect(actual_block.call).to eq(expected)
-    end
-  end
-
-  let(:workers)  { 1 }
-  let(:expected) { "Default settings used: Filter workers: #{workers}" }
-  let(:settings) { {} }
-
-  describe 'class methods API' do
-    let(:actual_block) do
-      -> {described_class.print(settings)}
-    end
-
-    context 'when the settings hash is empty' do
-      it_behaves_like "a defaults printer"
-    end
-
-    context 'when the settings hash has content' do
-      let(:workers) { 42 }
-      let(:settings) { {'filter-workers' => workers} }
-
-      it_behaves_like "a defaults printer"
-    end
-  end
-
-  describe 'instance method API' do
-    let(:actual_block) do
-      -> {described_class.new(settings).print}
-    end
-
-    context 'when the settings hash is empty' do
-      it_behaves_like "a defaults printer"
-    end
-
-    context 'when the settings hash has content' do
-      let(:workers) { 13 }
-      let(:settings) { {'filter-workers' => workers} }
-
-      it_behaves_like "a defaults printer"
-    end
-  end
-end
diff --git a/spec/util/worker_threads_default_printer_spec.rb b/spec/util/worker_threads_default_printer_spec.rb
deleted file mode 100644
index 348b9d263e1..00000000000
--- a/spec/util/worker_threads_default_printer_spec.rb
+++ /dev/null
@@ -1,26 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/util/worker_threads_default_printer"
-
-describe LogStash::Util::WorkerThreadsDefaultPrinter do
-  let(:settings) { {} }
-  let(:collector) { [] }
-
-  subject { described_class.new(settings) }
-
-  context 'when the settings hash is empty' do
-    it 'the #visit method returns a string with 1 filter worker' do
-      subject.visit(collector)
-      expect(collector.first).to eq("Filter workers: 1")
-    end
-  end
-
-  context 'when the settings hash has content' do
-    let(:settings) { {'filter-workers' => 42} }
-
-    it 'the #visit method returns a string with 42 filter workers' do
-      subject.visit(collector)
-      expect(collector.first).to eq("Filter workers: 42")
-    end
-  end
-end
diff --git a/tools/Gemfile.beaker b/tools/Gemfile.beaker
deleted file mode 100644
index 97a67a20ade..00000000000
--- a/tools/Gemfile.beaker
+++ /dev/null
@@ -1,11 +0,0 @@
-source 'https://rubygems.org'
-
-gem 'beaker', '2.27.0'
-gem 'beaker-rspec'
-gem 'pry'
-gem 'docker-api', '~> 1.0'
-gem 'rubysl-securerandom'
-gem 'rspec_junit_formatter'
-gem 'rspec', '~> 3.1'
-gem 'rake'
-gem 'fog-google', '~> 0.0.9'
diff --git a/tools/benchmark/collector.rb b/tools/benchmark/collector.rb
new file mode 100644
index 00000000000..b8bbdeb1090
--- /dev/null
+++ b/tools/benchmark/collector.rb
@@ -0,0 +1,3 @@
+# encoding: utf-8
+require "benchmark/ips"
+require "logstash/instrument/collector"
diff --git a/benchmark/event_sprintf.rb b/tools/benchmark/event_sprintf.rb
similarity index 98%
rename from benchmark/event_sprintf.rb
rename to tools/benchmark/event_sprintf.rb
index 718d1fd6149..9bce01d6b2f 100644
--- a/benchmark/event_sprintf.rb
+++ b/tools/benchmark/event_sprintf.rb
@@ -1,3 +1,4 @@
+# encoding: utf-8
 require "benchmark/ips"
 require "lib/logstash/event"
 
diff --git a/tools/release.sh b/tools/release.sh
deleted file mode 100644
index 435196a95cb..00000000000
--- a/tools/release.sh
+++ /dev/null
@@ -1,67 +0,0 @@
-#!/bin/bash
-
-logstash=$PWD
-contrib=$PWD/../logstash-contrib/
-
-workdir="$PWD/build/release/"
-mkdir -p $workdir
-
-# circuit breaker to fail if there's something silly wrong.
-if [ -z "$workdir" ] ; then
-  echo "workdir is empty?!"
-  exit 1
-fi
-
-if [ ! -d "$contrib" ] ; then
-  echo "Missing: $contrib"
-  echo "Maybe git clone it?"
-  exit 1
-fi
-
-set -e
-
-prepare() {
-  rsync -a --delete $logstash/{bin,docs,lib,spec,Makefile,gembag.rb,logstash.gemspec,tools,locales,patterns,LICENSE,README.md} $contrib/{lib,spec} $workdir
-  rm -f $logstash/.VERSION.mk
-  make -C $logstash .VERSION.mk
-  make -C $logstash tarball package
-  make -C $contrib tarball package
-  cp $logstash/.VERSION.mk $workdir
-  rm -f $workdir/build/pkg
-  rm -f $workdir/build/*.{zip,rpm,gz,deb} || true
-}
-
-docs() {
-  make -C $workdir build
-  (cd $contrib; find lib/logstash -type f -name '*.rb') > $workdir/build/contrib_plugins
-  make -C $workdir -j 4 docs
-}
-
-tests() {
-  make -C $logstash test QUIET=
-  make -C $logstash tarball test QUIET=
-}
-
-packages() {
-  for path in $logstash $contrib ; do
-    rm -f $path/build/*.tar.gz
-    rm -f $path/build/*.zip
-    echo "Building packages: $path"
-    make -C $path tarball
-    for dir in build pkg . ; do
-      [ ! -d "$path/$dir" ] && continue
-      (cd $path/$dir;
-        for i in *.gz *.rpm *.deb *.zip *.jar ; do
-          [ ! -f "$i" ] && continue
-          echo "Copying $path/$dir/$i"
-          cp $i $workdir/build
-        done
-      )
-    done
-  done
-}
-
-prepare
-tests
-docs
-packages
diff --git a/tools/upload.sh b/tools/upload.sh
deleted file mode 100644
index 72684486c8c..00000000000
--- a/tools/upload.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-
-basedir=$(dirname $0)/../
-bucket=download.elasticsearch.org
-
-s3cmd put -P $basedir/build/release/build/*.gz s3://${bucket}/logstash/logstash/
-s3cmd put -P $basedir/build/release/build/*.rpm s3://${bucket}/logstash/logstash/packages/centos/
-s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/debian
-s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/ubuntu
