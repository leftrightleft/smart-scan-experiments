diff --git a/CHANGELOG b/CHANGELOG
index ca77cd8c3e0..917bf91c761 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -1,8 +1,9 @@
 1.1.10 (????)
   ## general
-  - on linux, all threads will set their process names so you can identify
+  - On linux, all threads will set their process names so you can identify
     threads in tools like top(1).
   - Java 5 is no longer supported (You must use Java 6 or newer).
+  - Windows line terminators (CRLF) are now accepted in config files.
 
   ## inputs
   - improvement: loggly: supports http proxying now (#276, patch by Richard
@@ -16,6 +17,8 @@
     warn you if you use it. (LOGSTASH-803)
   - feature: grok: Adds tag_on_failure setting so you can prevent grok from
     tagging events on failure. (#328, patch by Neil Prosser)
+  - new: uaparser: parses user agent strings in to structured data based on
+    BrowserScope data (#347, patch by Dan Everton)
 
   ## outputs
   - fix bug in mongo output that would fail to load bson_java support
diff --git a/Makefile b/Makefile
index 1e95017bb92..b669260ff7c 100644
--- a/Makefile
+++ b/Makefile
@@ -125,10 +125,6 @@ vendor/bundle: | vendor $(JRUBY)
 	@echo "=> Installing gems to $@..."
 	@#$(QUIET)GEM_HOME=$(GEM_HOME) $(JRUBY_CMD) --1.9 $(GEM_HOME)/bin/bundle install --deployment
 	$(QUIET)GEM_HOME=./vendor/bundle/jruby/1.9/ GEM_PATH= $(JRUBY_CMD) --1.9 ./gembag.rb logstash.gemspec
-	@# rubygems is rejecting pushes right now, so use a temporary locatin for lumberjack
-	$(QUIET)GEM_HOME=./vendor/bundle/jruby/1.9/ GEM_PATH= $(JRUBY_CMD) --1.9 -S gem uninstall jls-lumberjack
-	$(QUIET)rm jls-lumberjack*gem; wget http://jls.objects.dreamhost.com/gems/lumberjack/jls-lumberjack-0.0.16.gem
-	$(QUIET)GEM_HOME=./vendor/bundle/jruby/1.9/ GEM_PATH= $(JRUBY_CMD) --1.9 -S gem install --local jls-lumberjack-0.0.16.gem
 	@# Purge old version of json
 	#$(QUIET)GEM_HOME=./vendor/bundle/jruby/1.9/ GEM_PATH= $(JRUBY_CMD) --1.9 -S gem uninstall json -v 1.6.5
 	@# Purge old versions of gems installed because gembag doesn't do 
@@ -136,7 +132,7 @@ vendor/bundle: | vendor $(JRUBY)
 	$(QUIET)GEM_HOME=./vendor/bundle/jruby/1.9/ GEM_PATH= $(JRUBY_CMD) --1.9 -S gem uninstall addressable -v 2.2.8
 	@# uninstall the newer ffi (1.1.5 vs 1.3.1) because that's what makes
 	@#dependencies happy (launchy wants ffi 1.1.x)
-	$(QUIET)GEM_HOME=./vendor/bundle/jruby/1.9/ GEM_PATH= $(JRUBY_CMD) --1.9 -S gem uninstall ffi -v 1.3.1
+	#$(QUIET)GEM_HOME=./vendor/bundle/jruby/1.9/ GEM_PATH= $(JRUBY_CMD) --1.9 -S gem uninstall ffi -v 1.3.1
 	@# Purge any junk that fattens our jar without need!
 	@# The riak gem includes previous gems in the 'pkg' dir. :(
 	-rm -rf $@/jruby/1.9/gems/riak-client-1.0.3/pkg
@@ -208,6 +204,7 @@ build/flatgems: | build vendor/bundle
 	@# all the gem specs.
 	rsync -av $(VENDOR_DIR)/gems/jruby-openssl-*/lib/shared/jopenssl.jar $@/lib
 	rsync -av $(VENDOR_DIR)/gems/sys-uname-*/lib/unix/ $@/lib
+	rsync -av $(VENDOR_DIR)/gems/user_agent_parser-*/vendor/ua-parser $@/vendor
 
 flatjar-test:
 	GEM_HOME= GEM_PATH= java -jar build/logstash-$(VERSION)-flatjar.jar rspec $(TESTS)
@@ -227,7 +224,7 @@ flatjar: build/logstash-$(VERSION)-flatjar.jar
 build/jar: | build build/flatgems build/monolith
 	$(QUIET)mkdir build/jar
 	$(QUIET)rsync -av --delete build/flatgems/lib/ build/monolith/ build/ruby/ patterns build/jar/
-	$(QUIET)rsync -av --delete build/flatgems/data build/jar/
+	$(QUIET)rsync -av --delete build/flatgems/data build/flatgems/vendor build/jar/
 	$(QUIET)(cd lib; rsync -av --delete logstash/web/public ../build/jar/logstash/web/public)
 	$(QUIET)(cd lib; rsync -av --delete logstash/web/views ../build/jar/logstash/web/views)
 	$(QUIET)(cd lib; rsync -av --delete logstash/certs ../build/jar/logstash/certs)
diff --git a/docs/flags.md b/docs/flags.md
index fc911342689..29657e52182 100644
--- a/docs/flags.md
+++ b/docs/flags.md
@@ -10,7 +10,9 @@ The logstash agent has the following flags (also try using the '--help' flag)
 
 <dl>
 <dt> -f, --config CONFIGFILE </dt>
-<dd> Load the logstash config from a specific file, directory, or a wildcard. If given a directory or wildcard, config files will be read in order lexigraphically.  </dd>
+<dd> Load the logstash config from a specific file, directory, or a
+wildcard. If given a directory or wildcard, config files will be read
+from the directory in alphabetical order. </dd>
 <dt> -e CONFIGSTRING </dt>
 <dd> Use the given string as the configuration data. Same syntax as the
 config file. If not input is specified, 'stdin { type => stdin }' is
diff --git a/docs/index.html.erb b/docs/index.html.erb
index 1348613aeec..6c61341bf22 100644
--- a/docs/index.html.erb
+++ b/docs/index.html.erb
@@ -20,7 +20,13 @@ layout: content_right
     <li> <a href="tutorials/getting-started-centralized"> getting started (centralized) </a> </li>
     <li> <a href="tutorials/10-minute-walkthrough"> 10-minute walkthrough</a> - a simple walkthrough to show you how to configure the logstash agent to process events and even old logs. </li>
     <li> <a href="tutorials/metrics-from-logs"> Gathering metrics from logs </a> - take metrics from logs and ship them to graphite, ganglia, and more. </li>
-    <li> <a href="tutorials/just-enough-amqp-for-logstash">Just enough AMQP for Logstash </a> - Get a quick primer on AMQP and how to use it in Logstash! </li>
+    <li> <a href="tutorials/just-enough-rabbitmq-for-logstash">Just enough RabbitMQ for Logstash </a> - Get a quick primer on RabbitMQ and how to use it in Logstash! </li>
+  </ul>
+
+  <h3> books and articles </h3>
+
+  <ul>
+    <li> <a href="http://www.logstashbook.com">The LogStash Book </a> - An introductory LogStash book. </li>
   </ul>
 
   <h3> plugin documentation </h3>
diff --git a/docs/logging-tool-comparisons.md b/docs/logging-tool-comparisons.md
index d24d112cbc5..a39fea0546e 100644
--- a/docs/logging-tool-comparisons.md
+++ b/docs/logging-tool-comparisons.md
@@ -22,7 +22,7 @@ It provides you a simple event pipeline for taking events and logs from any
 input, manipulating them with filters, and sending them to any output. Inputs
 can be files, network, message brokers, etc. Filters are date and string
 parsers, grep-like, etc. Outputs are data stores (elasticsearch, mongodb, etc),
-message systems (amqp, stomp, etc), network (tcp, syslog), etc.
+message systems (rabbitmq, stomp, etc), network (tcp, syslog), etc.
 
 It also provides a web interface for doing search and analytics on your
 logs.
diff --git a/docs/tutorials/just-enough-amqp-for-logstash.md b/docs/tutorials/just-enough-rabbitmq-for-logstash.md
similarity index 85%
rename from docs/tutorials/just-enough-amqp-for-logstash.md
rename to docs/tutorials/just-enough-rabbitmq-for-logstash.md
index c6de06d1725..060fa6f0ac2 100644
--- a/docs/tutorials/just-enough-amqp-for-logstash.md
+++ b/docs/tutorials/just-enough-rabbitmq-for-logstash.md
@@ -1,10 +1,10 @@
 ---
-title: Just Enough AMQP - logstash
+title: Just Enough RabbitMQ - logstash
 layout: content_right
 ---
 
-While configuring your AMQP broker is out of scope for logstash, it's important
-to understand how logstash uses AMQP. To do that, we need to understand a
+While configuring your RabbitMQ broker is out of scope for logstash, it's important
+to understand how logstash uses RabbitMQ. To do that, we need to understand a
 little about AMQP.
 
 You should also consider reading
@@ -35,9 +35,8 @@ routing key.  Routing keys are discussed below.
 
 ## Broker
 
-A broker is simply the AMQP server software. There are several brokers but the
-most common (and arguably popular) is [RabbitMQ](http://www.rabbitmq.com).
-Some others are Apache Qpid (and the commercial version - RedHat MRG)
+A broker is simply the AMQP server software. There are several brokers, but this
+tutorial will cover the most common (and arguably popular), [RabbitMQ](http://www.rabbitmq.com).
 
 # Routing Keys
 
@@ -112,19 +111,19 @@ support routing keys. Topic exchanges do support them.  Just like a fanout
 exchange, all bound queues see all messages with the additional filter of the
 routing key.
 
-# AMQP in logstash
+# RabbitMQ in logstash
 
 As stated earlier, in Logstash, Outputs publish to Exchanges. Inputs read from
-Queues that are bound to Exchanges.  Logstash uses the `bunny` AMQP library for
+Queues that are bound to Exchanges.  Logstash uses the `bunny` RabbitMQ library for
 interaction with a broker. Logstash endeavors to expose as much of the
 configuration for both exchanges and queues.  There are many different tunables
 that you might be concerned with setting - including things like message
 durability or persistence of declared queues/exchanges.  See the relevant input
-and output documentation for AMQP for a full list of tunables.
+and output documentation for RabbitMQ for a full list of tunables.
 
 # Sample configurations, tips, tricks and gotchas
 
-There are several examples in the logstash source directory of AMQP usage,
+There are several examples in the logstash source directory of RabbitMQ usage,
 however a few general rules might help eliminate any issues.
 
 ## Check your bindings
@@ -136,9 +135,9 @@ sender agent
 
     input { stdin { type = "test" } }
     output {
-      amqp {
-        name => "test_exchange"
-        host => "my_amqp_server"
+      rabbitmq {
+        exchange => "test_exchange"
+        host => "my_rabbitmq_server"
         exchange_type => "fanout"
       }
     }
@@ -146,9 +145,9 @@ sender agent
 receiver agent
 
     input {
-      amqp {
-        name => "test_queue"
-        host => "my_amqp_server"
+      rabbitmq {
+        queue => "test_queue"
+        host => "my_rabbitmq_server"
         exchange => "test_exchange" # This matches the exchange declared above
       }
     }
@@ -157,15 +156,15 @@ receiver agent
 ## Message persistence
 
 By default, logstash will attempt to ensure that you don't lose any messages.
-This is reflected in the AMQP default settings as well.  However there are
-cases where you might not want this. A good example is where AMQP is not your
+This is reflected in the RabbitMQ default settings as well.  However there are
+cases where you might not want this. A good example is where RabbitMQ is not your
 primary method of shipping.
 
-In the following example, we use AMQP as a sniffing interface. Our primary
-destination is the embedded ElasticSearch instance. We have a secondary AMQP
+In the following example, we use RabbitMQ as a sniffing interface. Our primary
+destination is the embedded ElasticSearch instance. We have a secondary RabbitMQ
 output that we use for duplicating messages. However we disable persistence and
 durability on this interface so that messages don't pile up waiting for
-delivery. We only use AMQP when we want to watch messages in realtime.
+delivery. We only use RabbitMQ when we want to watch messages in realtime.
 Additionally, we're going to leverage routing keys so that we can optionally
 filter incoming messages to subsets of hosts. The exercise of getting messages
 to this logstash agent are left up to the user.
@@ -176,9 +175,9 @@ to this logstash agent are left up to the user.
 
     output {
       elasticsearch { embedded => true }
-      amqp {
-        name => "logtail"
-        host => "my_amqp_server"
+      rabbitmq {
+        exchange => "logtail"
+        host => "my_rabbitmq_server"
         exchange_type => "topic" # We use topic here to enable pub/sub with routing keys
         key => "logs.%{host}"
         durable => false # If rabbitmq restarts, the exchange disappears.
diff --git a/etc/agent.lgtm.conf b/etc/agent.lgtm.conf
index 431f94602b9..ffa67dfc94c 100644
--- a/etc/agent.lgtm.conf
+++ b/etc/agent.lgtm.conf
@@ -17,17 +17,17 @@ input {
 
 output {
   # This will be your durable shipping mechanism
-  amqp {
-    host => "myamqpserver"
+  rabbitmq {
+    host => "myrabbitmqserver"
     exchange_type => "fanout"
-    name => "rawlogs"
+    exchange => "rawlogs"
   }
   # This is an optional non-durable shipping mechanism
   # With this, you can sniff logs from your own code
-  amqp {
+  rabbitmq {
     host => "127.0.0.1"
     exchange_type => "topic"
-    name => "logsniff"
+    exchange => "logsniff"
     durable => false
     persistent => false
     # The following is optional
diff --git a/etc/examples/esriver.conf b/etc/examples/esriver.conf
index 69c8ff65548..10494b53c1f 100644
--- a/etc/examples/esriver.conf
+++ b/etc/examples/esriver.conf
@@ -16,6 +16,6 @@ output {
   stdout { }
   elasticsearch_river {
     es_host => "localhost"
-    amqp_host => "localhost"
+    rabbitmq_host => "localhost"
   }
 }
diff --git a/etc/examples/indexer.conf b/etc/examples/indexer.conf
index 2b06058b05f..0ec8753d1b3 100644
--- a/etc/examples/indexer.conf
+++ b/etc/examples/indexer.conf
@@ -1,10 +1,10 @@
 input {
-  amqp {
+  rabbitmq {
     host => "127.0.0.1"
     user => "guest"
     pass => "guest"
     exchange => "logstash"
-    name => "testing"
+	queue => "testing"
     type => "all"
   }
 
diff --git a/lib/logstash/agent.rb b/lib/logstash/agent.rb
index bf96476ef43..e3311ae6e38 100644
--- a/lib/logstash/agent.rb
+++ b/lib/logstash/agent.rb
@@ -33,6 +33,9 @@ class LogStash::Agent
   public
   def initialize
     log_to(STDERR)
+    # default log level for now.
+    @logger.level = :warn 
+
     @config_path = nil
     @config_string = nil
     @is_yaml = false
@@ -152,8 +155,8 @@ def parse_options(args)
     # Load any plugins that we have flags for.
     # TODO(sissel): The --<plugin> flag support currently will load
     # any matching plugins input, output, or filter. This means, for example,
-    # that the 'amqp' input *and* output plugin will be loaded if you pass
-    # --amqp-foo flag. This might cause confusion, but it seems reasonable for
+    # that the 'rabbitmq' input *and* output plugin will be loaded if you pass
+    # --rabbitmq-foo flag. This might cause confusion, but it seems reasonable for
     # now that any same-named component will have the same flags.
     plugins = []
     args.each do |arg|
@@ -332,7 +335,11 @@ def run(args, &block)
     @logger.info("Start thread")
     @thread = Thread.new do
       LogStash::Util::set_thread_name(self.class.name)
-      run_with_config(config, &block)
+      begin
+        run_with_config(config, &block)
+      rescue => e
+        @logger.warn(e.to_s)
+      end
     end
 
     return remaining
diff --git a/lib/logstash/agent2.rb b/lib/logstash/agent2.rb
index a66b4785a25..793b9950094 100644
--- a/lib/logstash/agent2.rb
+++ b/lib/logstash/agent2.rb
@@ -1,61 +1,211 @@
 require "logstash/config/file"
-require "clamp"
+require "logstash/plugin"
+require "logstash/pipeline"
+require "clamp" # gem 'clamp'
+require "cabin" # gem 'cabin'
+require "sys/uname" # gem 'sys-uname'
 
 class LogStash::Agent2 < Clamp::Command
+  class ConfigurationError < StandardError; end
+
   option ["-f", "--config"], "CONFIG_PATH",
-    "Load the logstash config from a specific file or directory. " \
-    "If a direcory is given, all files in that directory will " \
-    "be concatonated in lexicographical order and then parsed as " \
-    "a single config file. You can also specify wildcards (globs)" \
-    "and any matched files will be loaded in the order described above",
+    I18n.t("logstash.agent.flag.config"),
     :attribute_name => :config_path
 
   option "-e", "CONFIG_STRING",
-    "Use the given string as the configuration data. Same syntax as " \
-    "the config file. If not input is specified, then " \
-    "'stdin { type => stdin }' is the default input. If no output is " \
-    "specified, then 'stdout { debug => true }}' is default output.",
+    I18n.t("logstash.agent.flag.config-string"),
     :attribute_name => :config_string
 
   option ["-w", "--filterworkers"], "COUNT",
-    "Sets the number of filter workers to run.",
+    I18n.t("logstash.agent.flag.filterworkers"),
     :attribute_name => :filter_workers, :default => 1, &:to_i
 
   option "--watchdog-timeout", "SECONDS", 
-    "Set the filter watchdog timeout (in seconds). This timeout is used" \
-    " to detect stuck filters; stuck filters usually symptoms of bugs. " \
-    "When a filter takes longer than TIMEOUT seconds, it will cause " \
-    "logstash to abort.", :default => 10, &:to_f
+    I18n.t("logstash.agent.flag.watchdog-timeout"),
+    :default => 10, &:to_f
 
   option ["-l", "--log"], "FILE",
-    "Write logstash internal logs to the given file. Without this flag, " \
-    "logstash will emit logs to standard output."
+    I18n.t("logstash.agent.flag.log"),
+    :attribute_name => :log_file
 
   verbosity = 0
-  option "-v", :flag, "Increase verbosity of logstash internal logs. " \
-    "Specifying once will show 'informational' logs. Specifying twice " \
-    "will show 'debug' logs.", :attribute_name => :verbosity do
+  option "-v", :flag, 
+    I18n.t("logstash.agent.flag.verbosity"),
+    :default => :warn, :attribute_name => :verbosity do
     verbosity += 1
-  end
+
+    if verbosity == 1
+      next :info
+    else
+      next :debug
+    end
+  end # -v
 
   option ["-V", "--version"], :flag,
-    "Emit the version of logstash and its friends" do
-    # TODO(sissel): This should emit the version of JRuby and ElasticSearch as
-    # well. Perhaps also the versions of all gems?
-    require "logstash/version"
-    puts "logstash #{LOGSTASH_VERSION}"
-    exit(0)
-  end
+    I18n.t("logstash.agent.flag.version")
 
   plugin_paths = []
   option ["-p", "--pluginpath"] , "PATH",
-    "A colon-delimited path of where to find plugins. Plugins are expected " \
-    "to be in a specific directory hierarchy: PATH/logstash/TYPE/NAME.rb - " \
-    "where TYPE is 'input' 'filter' or 'output' and NAME is the name of the" \
-    "plugin.", :attribute_name => :plugin_path  do |value|
+    I18n.t("logstash.agent.flag.pluginpath"),
+    :attribute_name => :plugin_paths do |value|
     plugin_paths << value unless plugin_paths.include?(value)
-  end
+    next plugin_paths
+  end # -p / --pluginpath
 
+  # Emit a warning message.
+  def warn(message)
+    # For now, all warnings are fatal.
+    raise ConfigurationError, message
+  end # def warn
+
+  # Emit a failure message and abort.
+  def fail(message)
+    raise ConfigurationError, message
+  end # def fail
+
+  # Run the agent. This method is invoked after clamp parses the
+  # flags given to this program.
   def execute
+    @logger = Cabin::Channel.get(LogStash)
+
+    if version?
+      show_version
+      return 0
+    end
+
+    configure
+
+    begin
+      pipeline = LogStash::Pipeline.new(@config_string)
+    rescue LoadError => e
+      fail("Configuration problem.")
+    end
+
+    # Make SIGINT shutdown the pipeline.
+    trap_id = Stud::trap("INT") do
+      @logger.warn(I18n.t("logstash.agent.interrupted"))
+      pipeline.shutdown
+    end
+
+    # TODO(sissel): Get pipeline completion status.
+    pipeline.run
+    return 0
+  rescue ConfigurationError, LogStash::Plugin::ConfigurationError => e
+    puts I18n.t("logstash.agent.error", :error => e)
+    return 1
+  rescue => e
+    puts I18n.t("unexpected-exception", :error => e)
+    puts e.backtrace if @logger.debug?
+    return 1
+    #puts e.backtrace
+  ensure
+    Stud::untrap("INT", trap_id) unless trap_id.nil?
   end # def execute
+
+  def show_version
+    show_version_logstash
+
+    if RUBY_PLATFORM == "java"
+      show_version_java
+      show_version_jruby
+      show_version_elasticsearch
+    end
+
+    # Was the -v or --v flag given? Show all gems, too.
+    show_gems if [:info, :debug].include?(verbosity?)
+  end # def show_version
+
+  def show_version_logstash
+    require "logstash/version"
+    puts "logstash #{LOGSTASH_VERSION}"
+  end # def show_version_logstash
+
+  def show_version_jruby
+    puts "jruby #{JRUBY_VERSION} (ruby #{RUBY_VERSION})"
+  end # def show_version_jruby
+
+  def show_version_elasticsearch
+    # Not running in the jar, assume elasticsearch jars are
+    # in ../../vendor/jar/...
+    if __FILE__ !~ /^(?:jar:)?file:/
+      jarpath = File.join(File.dirname(__FILE__), "../../vendor/jar/elasticsearch*/lib/*.jar")
+      Dir.glob(jarpath).each do |jar|
+        require jar
+      end
+    end
+
+    org.elasticsearch.Version::main([])
+  end # def show_version_elasticsearch
+
+  def show_version_java
+    properties = java.lang.System.getProperties
+    puts "java #{properties["java.version"]} (#{properties["java.vendor"]})"
+    puts "jvm #{properties["java.vm.name"]} / #{properties["java.vm.version"]}"
+  end # def show_version_java
+
+  def show_gems
+    require "rubygems"
+    Gem::Specification.each do |spec|
+      puts "gem #{spec.name} #{spec.version}"
+    end
+  end # def show_gems
+
+  # Do any start-time configuration.
+  #
+  # Log file stuff, plugin path checking, etc.
+  def configure
+    configure_logging(log_file)
+    configure_plugin_path(plugin_paths) if !plugin_paths.nil?
+  end # def configure
+
+  # Point logging at a specific path.
+  def configure_logging(path)
+    # Set with the -v (or -vv...) flag
+    @logger.level = verbosity?
+    if !log_file.nil?
+      # TODO(sissel): Implement file output/rotation in Cabin.
+      # TODO(sissel): Catch exceptions, report sane errors.
+      begin
+        file = File.new(path, "a")
+      rescue => e
+        fail(I18n.t("logstash.agent.configuration.log_file_failed",
+                    :path => path, :error => e))
+      end
+
+      puts "Sending all output to #{path}."
+      @logger.subscribe(file)
+    else
+      @logger.subscribe(STDOUT)
+    end
+
+    # TODO(sissel): redirect stdout/stderr to the log as well
+    # http://jira.codehaus.org/browse/JRUBY-7003
+  end # def configure_logging
+
+  # Validate and add any paths to the list of locations
+  # logstash will look to find plugins.
+  def configure_plugin_path(paths)
+    # Append any plugin paths to the ruby search path
+    paths.each do |path|
+      # Verify the path exists
+      if !Dir.exists?(path)
+        warn(I18n.t("logstash.agent.configuration.plugin_path_missing",
+                    :path => path))
+
+      end
+
+      # TODO(sissel): Verify the path looks like the correct form.
+      # aka, there must be file in path/logstash/{filters,inputs,outputs}/*.rb
+      plugin_glob = File.join(path, "logstash", "{inputs,filters,outputs}", "*.rb")
+      if Dir.glob(plugin_glob).empty?
+        warn(I18n.t("logstash.agent.configuration.no_plugins_found",
+                    :path => path, :plugin_glob => plugin_glob))
+      end
+
+      # We push plugin paths to the front of the LOAD_PATH so that folks
+      # can override any core logstash plugins if they need to.
+      @logger.debug("Adding plugin path", :path => path)
+      $LOAD_PATH.unshift(path)
+    end
+  end # def configure_plugin_path
 end # class LogStash::Agent2
diff --git a/lib/logstash/certs/cacert.pem b/lib/logstash/certs/cacert.pem
index 1717404114b..99b310bce91 100644
--- a/lib/logstash/certs/cacert.pem
+++ b/lib/logstash/certs/cacert.pem
@@ -1,7 +1,7 @@
 ##
 ## ca-bundle.crt -- Bundle of CA Root Certificates
 ##
-## Certificate data from Mozilla as of: Fri Sep  2 23:34:57 2011
+## Certificate data from Mozilla as of: Sat Dec 29 20:03:40 2012
 ##
 ## This is a bundle of X.509 certificates of public Certificate Authorities
 ## (CA). These were automatically extracted from Mozilla's root certificates
@@ -14,42 +14,7 @@
 ## Just configure this file as the SSLCACertificateFile.
 ##
 
-# ***** BEGIN LICENSE BLOCK *****
-# Version: MPL 1.1/GPL 2.0/LGPL 2.1
-#
-# The contents of this file are subject to the Mozilla Public License Version
-# 1.1 (the "License"); you may not use this file except in compliance with
-# the License. You may obtain a copy of the License at
-# http://www.mozilla.org/MPL/
-#
-# Software distributed under the License is distributed on an "AS IS" basis,
-# WITHOUT WARRANTY OF ANY KIND, either express or implied. See the License
-# for the specific language governing rights and limitations under the
-# License.
-#
-# The Original Code is the Netscape security libraries.
-#
-# The Initial Developer of the Original Code is
-# Netscape Communications Corporation.
-# Portions created by the Initial Developer are Copyright (C) 1994-2000
-# the Initial Developer. All Rights Reserved.
-#
-# Contributor(s):
-#
-# Alternatively, the contents of this file may be used under the terms of
-# either the GNU General Public License Version 2 or later (the "GPL"), or
-# the GNU Lesser General Public License Version 2.1 or later (the "LGPL"),
-# in which case the provisions of the GPL or the LGPL are applicable instead
-# of those above. If you wish to allow use of your version of this file only
-# under the terms of either the GPL or the LGPL, and not to allow others to
-# use your version of this file under the terms of the MPL, indicate your
-# decision by deleting the provisions above and replace them with the notice
-# and other provisions required by the GPL or the LGPL. If you do not delete
-# the provisions above, a recipient may use your version of this file under
-# the terms of any one of the MPL, the GPL or the LGPL.
-#
-# ***** END LICENSE BLOCK *****
-# @(#) $RCSfile: certdata.txt,v $ $Revision: 1.79 $ $Date: 2011/09/02 19:40:56 $
+# @(#) $RCSfile: certdata.txt,v $ $Revision: 1.87 $ $Date: 2012/12/29 16:32:45 $
 
 GTE CyberTrust Global Root
 ==========================
@@ -166,38 +131,6 @@ up/1902lMXucKS1M/mQ+7LZT/uqb7YLbdHVLB3luHtgZg3Pe9T7Qtd7nS2h9Qy4qIOF+oHhEngj1
 mPnHfxsb1gYgAlihw6ID
 -----END CERTIFICATE-----
 
-Verisign Class 1 Public Primary Certification Authority
-=======================================================
------BEGIN CERTIFICATE-----
-MIICPTCCAaYCEQDNun9W8N/kvFT+IqyzcqpVMA0GCSqGSIb3DQEBAgUAMF8xCzAJBgNVBAYTAlVT
-MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE3MDUGA1UECxMuQ2xhc3MgMSBQdWJsaWMgUHJpbWFy
-eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTAeFw05NjAxMjkwMDAwMDBaFw0yODA4MDEyMzU5NTla
-MF8xCzAJBgNVBAYTAlVTMRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE3MDUGA1UECxMuQ2xhc3Mg
-MSBQdWJsaWMgUHJpbWFyeSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTCBnzANBgkqhkiG9w0BAQEF
-AAOBjQAwgYkCgYEA5Rm/baNWYS2ZSHH2Z965jeu3noaACpEO+jglr0aIguVzqKCbJF0NH8xlbgyw
-0FaEGIeaBpsQoXPftFg5a27B9hXVqKg/qhIGjTGsf7A01480Z4gJzRQR4k5FVmkfeAKA2txHkSm7
-NsljXMXg1y2He6G3MrB7MLoqLzGq7qNn2tsCAwEAATANBgkqhkiG9w0BAQIFAAOBgQBMP7iLxmjf
-7kMzDl3ppssHhE16M/+SG/Q2rdiVIjZoEWx8QszznC7EBz8UsA9P/5CSdvnivErpj82ggAr3xSnx
-giJduLHdgSOjeyUVRjB5FvjqBUuUfx3CHMjjt/QQQDwTw18fU+hI5Ia0e6E1sHslurjTjqs/OJ0A
-NACY89FxlA==
------END CERTIFICATE-----
-
-Verisign Class 2 Public Primary Certification Authority
-=======================================================
------BEGIN CERTIFICATE-----
-MIICPDCCAaUCEC0b/EoXjaOR6+f/9YtFvgswDQYJKoZIhvcNAQECBQAwXzELMAkGA1UEBhMCVVMx
-FzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAyIFB1YmxpYyBQcmltYXJ5
-IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MB4XDTk2MDEyOTAwMDAwMFoXDTI4MDgwMTIzNTk1OVow
-XzELMAkGA1UEBhMCVVMxFzAVBgNVBAoTDlZlcmlTaWduLCBJbmMuMTcwNQYDVQQLEy5DbGFzcyAy
-IFB1YmxpYyBQcmltYXJ5IENlcnRpZmljYXRpb24gQXV0aG9yaXR5MIGfMA0GCSqGSIb3DQEBAQUA
-A4GNADCBiQKBgQC2WoujDWojg4BrzzmH9CETMwZMJaLtVRKXxaeAufqDwSCg+i8VDXyhYGt+eSz6
-Bg86rvYbb7HS/y8oUl+DfUvEerf4Zh+AVPy3wo5ZShRXRtGak75BkQO7FYCTXOvnzAhsPz6zSvz/
-S2wj1VCCJkQZjiPDceoZJEcEnnW/yKYAHwIDAQABMA0GCSqGSIb3DQEBAgUAA4GBAIobK/o5wXTX
-XtgZZKJYSi034DNHD6zt96rbHuSLBlxgJ8pFUs4W7z8GZOeUaHxgMxURaa+dYo2jA1Rrpr7l7gUY
-YAS/QoD90KioHgE796Ncr6Pc5iaAIzy4RHT3Cq5Ji2F4zCS/iIqnDupzGUH9TQPwiNHleI2lKk/2
-lw0Xd8rY
------END CERTIFICATE-----
-
 Verisign Class 3 Public Primary Certification Authority
 =======================================================
 -----BEGIN CERTIFICATE-----
@@ -271,25 +204,6 @@ MA0GCSqGSIb3DQEBBQUAA4GBAFFNzb5cy5gZnBWyATl4Lk0PZ3BwmcYQWpSkU01UbSuvDV1Ai2TT
 Oaxxp5EJb+RxBrO6WVcmeQD2+A2iMzAo1KpYoJ2daZH9
 -----END CERTIFICATE-----
 
-Verisign Class 4 Public Primary Certification Authority - G2
-============================================================
------BEGIN CERTIFICATE-----
-MIIDAjCCAmsCEDKIjprS9esTR/h/xCA3JfgwDQYJKoZIhvcNAQEFBQAwgcExCzAJBgNVBAYTAlVT
-MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE8MDoGA1UECxMzQ2xhc3MgNCBQdWJsaWMgUHJpbWFy
-eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEcyMTowOAYDVQQLEzEoYykgMTk5OCBWZXJpU2ln
-biwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MR8wHQYDVQQLExZWZXJpU2lnbiBUcnVz
-dCBOZXR3b3JrMB4XDTk4MDUxODAwMDAwMFoXDTI4MDgwMTIzNTk1OVowgcExCzAJBgNVBAYTAlVT
-MRcwFQYDVQQKEw5WZXJpU2lnbiwgSW5jLjE8MDoGA1UECxMzQ2xhc3MgNCBQdWJsaWMgUHJpbWFy
-eSBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eSAtIEcyMTowOAYDVQQLEzEoYykgMTk5OCBWZXJpU2ln
-biwgSW5jLiAtIEZvciBhdXRob3JpemVkIHVzZSBvbmx5MR8wHQYDVQQLExZWZXJpU2lnbiBUcnVz
-dCBOZXR3b3JrMIGfMA0GCSqGSIb3DQEBAQUAA4GNADCBiQKBgQC68OTP+cSuhVS5B1f5j8V/aBH4
-xBewRNzjMHPVKmIquNDMHO0oW369atyzkSTKQWI8/AIBvxwWMZQFl3Zuoq29YRdsTjCG8FE3KlDH
-qGKB3FtKqsGgtG7rL+VXxbErQHDbWk2hjh+9Ax/YA9SPTJlxvOKCzFjomDqG04Y48wApHwIDAQAB
-MA0GCSqGSIb3DQEBBQUAA4GBAIWMEsGnuVAVess+rLhDityq3RS6iYF+ATwjcSGIL4LcY/oCRaxF
-WdcqWERbt5+BO5JoPeI3JPV7bI92NZYJqFmduc4jq3TWg/0ycyfYaT5DdPauxYma51N86Xv2S/PB
-ZYPejYqcPIiNOVn8qj8ijaHBZlCBckztImRPT8qAkbYp
------END CERTIFICATE-----
-
 GlobalSign Root CA
 ==================
 -----BEGIN CERTIFICATE-----
@@ -958,48 +872,6 @@ YQa7FkKMcPcw++DbZqMAAb3mLNqRX6BGi01qnD093QVG/na/oAo85ADmJ7f/hC3euiInlhBx6yLt
 398znM/jra6O1I7mT1GvFpLgXPYHDw==
 -----END CERTIFICATE-----
 
-TC TrustCenter, Germany, Class 2 CA
-===================================
------BEGIN CERTIFICATE-----
-MIIDXDCCAsWgAwIBAgICA+owDQYJKoZIhvcNAQEEBQAwgbwxCzAJBgNVBAYTAkRFMRAwDgYDVQQI
-EwdIYW1idXJnMRAwDgYDVQQHEwdIYW1idXJnMTowOAYDVQQKEzFUQyBUcnVzdENlbnRlciBmb3Ig
-U2VjdXJpdHkgaW4gRGF0YSBOZXR3b3JrcyBHbWJIMSIwIAYDVQQLExlUQyBUcnVzdENlbnRlciBD
-bGFzcyAyIENBMSkwJwYJKoZIhvcNAQkBFhpjZXJ0aWZpY2F0ZUB0cnVzdGNlbnRlci5kZTAeFw05
-ODAzMDkxMTU5NTlaFw0xMTAxMDExMTU5NTlaMIG8MQswCQYDVQQGEwJERTEQMA4GA1UECBMHSGFt
-YnVyZzEQMA4GA1UEBxMHSGFtYnVyZzE6MDgGA1UEChMxVEMgVHJ1c3RDZW50ZXIgZm9yIFNlY3Vy
-aXR5IGluIERhdGEgTmV0d29ya3MgR21iSDEiMCAGA1UECxMZVEMgVHJ1c3RDZW50ZXIgQ2xhc3Mg
-MiBDQTEpMCcGCSqGSIb3DQEJARYaY2VydGlmaWNhdGVAdHJ1c3RjZW50ZXIuZGUwgZ8wDQYJKoZI
-hvcNAQEBBQADgY0AMIGJAoGBANo46O0yAClxgwENv4wB3NrGrTmkqYov1YtcaF9QxmL1Zr3KkSLs
-qh1R1z2zUbKDTl3LSbDwTFXlay3HhQswHJJOgtTKAu33b77c4OMUuAVT8pr0VotanoWT0bSCVq5N
-u6hLVxa8/vhYnvgpjbB7zXjJT6yLZwzxnPv8V5tXXE8NAgMBAAGjazBpMA8GA1UdEwEB/wQFMAMB
-Af8wDgYDVR0PAQH/BAQDAgGGMDMGCWCGSAGG+EIBCAQmFiRodHRwOi8vd3d3LnRydXN0Y2VudGVy
-LmRlL2d1aWRlbGluZXMwEQYJYIZIAYb4QgEBBAQDAgAHMA0GCSqGSIb3DQEBBAUAA4GBAIRS+yjf
-/x91AbwBvgRWl2p0QiQxg/lGsQaKic+WLDO/jLVfenKhhQbOhvgFjuj5Jcrag4wGrOs2bYWRNAQ2
-9ELw+HkuCkhcq8xRT3h2oNmsGb0q0WkEKJHKNhAngFdb0lz1wlurZIFjdFH0l7/NEij3TWZ/p/Ac
-ASZ4smZHcFFk
------END CERTIFICATE-----
-
-TC TrustCenter, Germany, Class 3 CA
-===================================
------BEGIN CERTIFICATE-----
-MIIDXDCCAsWgAwIBAgICA+swDQYJKoZIhvcNAQEEBQAwgbwxCzAJBgNVBAYTAkRFMRAwDgYDVQQI
-EwdIYW1idXJnMRAwDgYDVQQHEwdIYW1idXJnMTowOAYDVQQKEzFUQyBUcnVzdENlbnRlciBmb3Ig
-U2VjdXJpdHkgaW4gRGF0YSBOZXR3b3JrcyBHbWJIMSIwIAYDVQQLExlUQyBUcnVzdENlbnRlciBD
-bGFzcyAzIENBMSkwJwYJKoZIhvcNAQkBFhpjZXJ0aWZpY2F0ZUB0cnVzdGNlbnRlci5kZTAeFw05
-ODAzMDkxMTU5NTlaFw0xMTAxMDExMTU5NTlaMIG8MQswCQYDVQQGEwJERTEQMA4GA1UECBMHSGFt
-YnVyZzEQMA4GA1UEBxMHSGFtYnVyZzE6MDgGA1UEChMxVEMgVHJ1c3RDZW50ZXIgZm9yIFNlY3Vy
-aXR5IGluIERhdGEgTmV0d29ya3MgR21iSDEiMCAGA1UECxMZVEMgVHJ1c3RDZW50ZXIgQ2xhc3Mg
-MyBDQTEpMCcGCSqGSIb3DQEJARYaY2VydGlmaWNhdGVAdHJ1c3RjZW50ZXIuZGUwgZ8wDQYJKoZI
-hvcNAQEBBQADgY0AMIGJAoGBALa0wTUFLg2N7KBAahwOJ6ZQkmtQGwfeLud2zODa/ISoXoxjaitN
-2U4CdhHBC/KNecoAtvGwDtf7pBc9r6tpepYnv68zoZoqWarEtTcI8hKlMbZD9TKWcSgoq40oht+7
-7uMMfTDWw1Krj10nnGvAo+cFa1dJRLNu6mTP0o56UHd3AgMBAAGjazBpMA8GA1UdEwEB/wQFMAMB
-Af8wDgYDVR0PAQH/BAQDAgGGMDMGCWCGSAGG+EIBCAQmFiRodHRwOi8vd3d3LnRydXN0Y2VudGVy
-LmRlL2d1aWRlbGluZXMwEQYJYIZIAYb4QgEBBAQDAgAHMA0GCSqGSIb3DQEBBAUAA4GBABY9xs3B
-u4VxhUafPiCPUSiZ7C1FIWMjWwS7TJC4iJIETb19AaM/9uzO8d7+feXhPrvGq14L3T2WxMup1Pkm
-5gZOngylerpuw3yCGdHHsbHD2w2Om0B8NwvxXej9H5CIpQ5ON2QhqE6NtJ/x3kit1VYYUimLRzQS
-CdS7kjXvD9s0
------END CERTIFICATE-----
-
 Certum Root CA
 ==============
 -----BEGIN CERTIFICATE-----
@@ -3719,3 +3591,305 @@ QtNoURi+VJq/REG6Sb4gumlc7rh3zc5sH62Dlhh9DrUUOYTxKOkto557HnpyWoOzeW/vtPzQCqVY
 T0bf+215WfKEIlKuD8z7fDvnaspHYcN6+NOSBB+4IIThNlQWx0DeO4pz3N/GCUzf7Nr/1FNCocny
 Yh0igzyXxfkZYiesZSLX0zzG5Y6yU8xJzrww/nsOM5D77dIUkR8Hrw==
 -----END CERTIFICATE-----
+
+Security Communication RootCA2
+==============================
+-----BEGIN CERTIFICATE-----
+MIIDdzCCAl+gAwIBAgIBADANBgkqhkiG9w0BAQsFADBdMQswCQYDVQQGEwJKUDElMCMGA1UEChMc
+U0VDT00gVHJ1c3QgU3lzdGVtcyBDTy4sTFRELjEnMCUGA1UECxMeU2VjdXJpdHkgQ29tbXVuaWNh
+dGlvbiBSb290Q0EyMB4XDTA5MDUyOTA1MDAzOVoXDTI5MDUyOTA1MDAzOVowXTELMAkGA1UEBhMC
+SlAxJTAjBgNVBAoTHFNFQ09NIFRydXN0IFN5c3RlbXMgQ08uLExURC4xJzAlBgNVBAsTHlNlY3Vy
+aXR5IENvbW11bmljYXRpb24gUm9vdENBMjCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB
+ANAVOVKxUrO6xVmCxF1SrjpDZYBLx/KWvNs2l9amZIyoXvDjChz335c9S672XewhtUGrzbl+dp++
++T42NKA7wfYxEUV0kz1XgMX5iZnK5atq1LXaQZAQwdbWQonCv/Q4EpVMVAX3NuRFg3sUZdbcDE3R
+3n4MqzvEFb46VqZab3ZpUql6ucjrappdUtAtCms1FgkQhNBqyjoGADdH5H5XTz+L62e4iKrFvlNV
+spHEfbmwhRkGeC7bYRr6hfVKkaHnFtWOojnflLhwHyg/i/xAXmODPIMqGplrz95Zajv8bxbXH/1K
+EOtOghY6rCcMU/Gt1SSwawNQwS08Ft1ENCcadfsCAwEAAaNCMEAwHQYDVR0OBBYEFAqFqXdlBZh8
+QIH4D5csOPEK7DzPMA4GA1UdDwEB/wQEAwIBBjAPBgNVHRMBAf8EBTADAQH/MA0GCSqGSIb3DQEB
+CwUAA4IBAQBMOqNErLlFsceTfsgLCkLfZOoc7llsCLqJX2rKSpWeeo8HxdpFcoJxDjrSzG+ntKEj
+u/Ykn8sX/oymzsLS28yN/HH8AynBbF0zX2S2ZTuJbxh2ePXcokgfGT+Ok+vx+hfuzU7jBBJV1uXk
+3fs+BXziHV7Gp7yXT2g69ekuCkO2r1dcYmh8t/2jioSgrGK+KwmHNPBqAbubKVY8/gA3zyNs8U6q
+tnRGEmyR7jTV7JqR50S+kDFy1UkC9gLl9B/rfNmWVan/7Ir5mUf/NVoCqgTLiluHcSmRvaS0eg29
+mvVXIwAHIRc/SjnRBUkLp7Y3gaVdjKozXoEofKd9J+sAro03
+-----END CERTIFICATE-----
+
+EC-ACC
+======
+-----BEGIN CERTIFICATE-----
+MIIFVjCCBD6gAwIBAgIQ7is969Qh3hSoYqwE893EATANBgkqhkiG9w0BAQUFADCB8zELMAkGA1UE
+BhMCRVMxOzA5BgNVBAoTMkFnZW5jaWEgQ2F0YWxhbmEgZGUgQ2VydGlmaWNhY2lvIChOSUYgUS0w
+ODAxMTc2LUkpMSgwJgYDVQQLEx9TZXJ2ZWlzIFB1YmxpY3MgZGUgQ2VydGlmaWNhY2lvMTUwMwYD
+VQQLEyxWZWdldSBodHRwczovL3d3dy5jYXRjZXJ0Lm5ldC92ZXJhcnJlbCAoYykwMzE1MDMGA1UE
+CxMsSmVyYXJxdWlhIEVudGl0YXRzIGRlIENlcnRpZmljYWNpbyBDYXRhbGFuZXMxDzANBgNVBAMT
+BkVDLUFDQzAeFw0wMzAxMDcyMzAwMDBaFw0zMTAxMDcyMjU5NTlaMIHzMQswCQYDVQQGEwJFUzE7
+MDkGA1UEChMyQWdlbmNpYSBDYXRhbGFuYSBkZSBDZXJ0aWZpY2FjaW8gKE5JRiBRLTA4MDExNzYt
+SSkxKDAmBgNVBAsTH1NlcnZlaXMgUHVibGljcyBkZSBDZXJ0aWZpY2FjaW8xNTAzBgNVBAsTLFZl
+Z2V1IGh0dHBzOi8vd3d3LmNhdGNlcnQubmV0L3ZlcmFycmVsIChjKTAzMTUwMwYDVQQLEyxKZXJh
+cnF1aWEgRW50aXRhdHMgZGUgQ2VydGlmaWNhY2lvIENhdGFsYW5lczEPMA0GA1UEAxMGRUMtQUND
+MIIBIjANBgkqhkiG9w0BAQEFAAOCAQ8AMIIBCgKCAQEAsyLHT+KXQpWIR4NA9h0X84NzJB5R85iK
+w5K4/0CQBXCHYMkAqbWUZRkiFRfCQ2xmRJoNBD45b6VLeqpjt4pEndljkYRm4CgPukLjbo73FCeT
+ae6RDqNfDrHrZqJyTxIThmV6PttPB/SnCWDaOkKZx7J/sxaVHMf5NLWUhdWZXqBIoH7nF2W4onW4
+HvPlQn2v7fOKSGRdghST2MDk/7NQcvJ29rNdQlB50JQ+awwAvthrDk4q7D7SzIKiGGUzE3eeml0a
+E9jD2z3Il3rucO2n5nzbcc8tlGLfbdb1OL4/pYUKGbio2Al1QnDE6u/LDsg0qBIimAy4E5S2S+zw
+0JDnJwIDAQABo4HjMIHgMB0GA1UdEQQWMBSBEmVjX2FjY0BjYXRjZXJ0Lm5ldDAPBgNVHRMBAf8E
+BTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQUoMOLRKo3pUW/l4Ba0fF4opvpXY0wfwYD
+VR0gBHgwdjB0BgsrBgEEAfV4AQMBCjBlMCwGCCsGAQUFBwIBFiBodHRwczovL3d3dy5jYXRjZXJ0
+Lm5ldC92ZXJhcnJlbDA1BggrBgEFBQcCAjApGidWZWdldSBodHRwczovL3d3dy5jYXRjZXJ0Lm5l
+dC92ZXJhcnJlbCAwDQYJKoZIhvcNAQEFBQADggEBAKBIW4IB9k1IuDlVNZyAelOZ1Vr/sXE7zDkJ
+lF7W2u++AVtd0x7Y/X1PzaBB4DSTv8vihpw3kpBWHNzrKQXlxJ7HNd+KDM3FIUPpqojlNcAZQmNa
+Al6kSBg6hW/cnbw/nZzBh7h6YQjpdwt/cKt63dmXLGQehb+8dJahw3oS7AwaboMMPOhyRp/7SNVe
+l+axofjk70YllJyJ22k4vuxcDlbHZVHlUIiIv0LVKz3l+bqeLrPK9HOSAgu+TGbrIP65y7WZf+a2
+E/rKS03Z7lNGBjvGTq2TWoF+bCpLagVFjPIhpDGQh2xlnJ2lYJU6Un/10asIbvPuW/mIPX64b24D
+5EI=
+-----END CERTIFICATE-----
+
+Hellenic Academic and Research Institutions RootCA 2011
+=======================================================
+-----BEGIN CERTIFICATE-----
+MIIEMTCCAxmgAwIBAgIBADANBgkqhkiG9w0BAQUFADCBlTELMAkGA1UEBhMCR1IxRDBCBgNVBAoT
+O0hlbGxlbmljIEFjYWRlbWljIGFuZCBSZXNlYXJjaCBJbnN0aXR1dGlvbnMgQ2VydC4gQXV0aG9y
+aXR5MUAwPgYDVQQDEzdIZWxsZW5pYyBBY2FkZW1pYyBhbmQgUmVzZWFyY2ggSW5zdGl0dXRpb25z
+IFJvb3RDQSAyMDExMB4XDTExMTIwNjEzNDk1MloXDTMxMTIwMTEzNDk1MlowgZUxCzAJBgNVBAYT
+AkdSMUQwQgYDVQQKEztIZWxsZW5pYyBBY2FkZW1pYyBhbmQgUmVzZWFyY2ggSW5zdGl0dXRpb25z
+IENlcnQuIEF1dGhvcml0eTFAMD4GA1UEAxM3SGVsbGVuaWMgQWNhZGVtaWMgYW5kIFJlc2VhcmNo
+IEluc3RpdHV0aW9ucyBSb290Q0EgMjAxMTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEB
+AKlTAOMupvaO+mDYLZU++CwqVE7NuYRhlFhPjz2L5EPzdYmNUeTDN9KKiE15HrcS3UN4SoqS5tdI
+1Q+kOilENbgH9mgdVc04UfCMJDGFr4PJfel3r+0ae50X+bOdOFAPplp5kYCvN66m0zH7tSYJnTxa
+71HFK9+WXesyHgLacEnsbgzImjeN9/E2YEsmLIKe0HjzDQ9jpFEw4fkrJxIH2Oq9GGKYsFk3fb7u
+8yBRQlqD75O6aRXxYp2fmTmCobd0LovUxQt7L/DICto9eQqakxylKHJzkUOap9FNhYS5qXSPFEDH
+3N6sQWRstBmbAmNtJGSPRLIl6s5ddAxjMlyNh+UCAwEAAaOBiTCBhjAPBgNVHRMBAf8EBTADAQH/
+MAsGA1UdDwQEAwIBBjAdBgNVHQ4EFgQUppFC/RNhSiOeCKQp5dgTBCPuQSUwRwYDVR0eBEAwPqA8
+MAWCAy5ncjAFggMuZXUwBoIELmVkdTAGggQub3JnMAWBAy5ncjAFgQMuZXUwBoEELmVkdTAGgQQu
+b3JnMA0GCSqGSIb3DQEBBQUAA4IBAQAf73lB4XtuP7KMhjdCSk4cNx6NZrokgclPEg8hwAOXhiVt
+XdMiKahsog2p6z0GW5k6x8zDmjR/qw7IThzh+uTczQ2+vyT+bOdrwg3IBp5OjWEopmr95fZi6hg8
+TqBTnbI6nOulnJEWtk2C4AwFSKls9cz4y51JtPACpf1wA+2KIaWuE4ZJwzNzvoc7dIsXRSZMFpGD
+/md9zU1jZ/rzAxKWeAaNsWftjj++n08C9bMJL/NMh98qy5V8AcysNnq/onN694/BtZqhFLKPM58N
+7yLcZnuEvUUXBj08yrl3NI/K6s8/MT7jiOOASSXIl7WdmplNsDz4SgCbZN2fOUvRJ9e4
+-----END CERTIFICATE-----
+
+Actalis Authentication Root CA
+==============================
+-----BEGIN CERTIFICATE-----
+MIIFuzCCA6OgAwIBAgIIVwoRl0LE48wwDQYJKoZIhvcNAQELBQAwazELMAkGA1UEBhMCSVQxDjAM
+BgNVBAcMBU1pbGFuMSMwIQYDVQQKDBpBY3RhbGlzIFMucC5BLi8wMzM1ODUyMDk2NzEnMCUGA1UE
+AwweQWN0YWxpcyBBdXRoZW50aWNhdGlvbiBSb290IENBMB4XDTExMDkyMjExMjIwMloXDTMwMDky
+MjExMjIwMlowazELMAkGA1UEBhMCSVQxDjAMBgNVBAcMBU1pbGFuMSMwIQYDVQQKDBpBY3RhbGlz
+IFMucC5BLi8wMzM1ODUyMDk2NzEnMCUGA1UEAwweQWN0YWxpcyBBdXRoZW50aWNhdGlvbiBSb290
+IENBMIICIjANBgkqhkiG9w0BAQEFAAOCAg8AMIICCgKCAgEAp8bEpSmkLO/lGMWwUKNvUTufClrJ
+wkg4CsIcoBh/kbWHuUA/3R1oHwiD1S0eiKD4j1aPbZkCkpAW1V8IbInX4ay8IMKx4INRimlNAJZa
+by/ARH6jDuSRzVju3PvHHkVH3Se5CAGfpiEd9UEtL0z9KK3giq0itFZljoZUj5NDKd45RnijMCO6
+zfB9E1fAXdKDa0hMxKufgFpbOr3JpyI/gCczWw63igxdBzcIy2zSekciRDXFzMwujt0q7bd9Zg1f
+YVEiVRvjRuPjPdA1YprbrxTIW6HMiRvhMCb8oJsfgadHHwTrozmSBp+Z07/T6k9QnBn+locePGX2
+oxgkg4YQ51Q+qDp2JE+BIcXjDwL4k5RHILv+1A7TaLndxHqEguNTVHnd25zS8gebLra8Pu2Fbe8l
+EfKXGkJh90qX6IuxEAf6ZYGyojnP9zz/GPvG8VqLWeICrHuS0E4UT1lF9gxeKF+w6D9Fz8+vm2/7
+hNN3WpVvrJSEnu68wEqPSpP4RCHiMUVhUE4Q2OM1fEwZtN4Fv6MGn8i1zeQf1xcGDXqVdFUNaBr8
+EBtiZJ1t4JWgw5QHVw0U5r0F+7if5t+L4sbnfpb2U8WANFAoWPASUHEXMLrmeGO89LKtmyuy/uE5
+jF66CyCU3nuDuP/jVo23Eek7jPKxwV2dpAtMK9myGPW1n0sCAwEAAaNjMGEwHQYDVR0OBBYEFFLY
+iDrIn3hm7YnzezhwlMkCAjbQMA8GA1UdEwEB/wQFMAMBAf8wHwYDVR0jBBgwFoAUUtiIOsifeGbt
+ifN7OHCUyQICNtAwDgYDVR0PAQH/BAQDAgEGMA0GCSqGSIb3DQEBCwUAA4ICAQALe3KHwGCmSUyI
+WOYdiPcUZEim2FgKDk8TNd81HdTtBjHIgT5q1d07GjLukD0R0i70jsNjLiNmsGe+b7bAEzlgqqI0
+JZN1Ut6nna0Oh4lScWoWPBkdg/iaKWW+9D+a2fDzWochcYBNy+A4mz+7+uAwTc+G02UQGRjRlwKx
+K3JCaKygvU5a2hi/a5iB0P2avl4VSM0RFbnAKVy06Ij3Pjaut2L9HmLecHgQHEhb2rykOLpn7VU+
+Xlff1ANATIGk0k9jpwlCCRT8AKnCgHNPLsBA2RF7SOp6AsDT6ygBJlh0wcBzIm2Tlf05fbsq4/aC
+4yyXX04fkZT6/iyj2HYauE2yOE+b+h1IYHkm4vP9qdCa6HCPSXrW5b0KDtst842/6+OkfcvHlXHo
+2qN8xcL4dJIEG4aspCJTQLas/kx2z/uUMsA1n3Y/buWQbqCmJqK4LL7RK4X9p2jIugErsWx0Hbhz
+lefut8cl8ABMALJ+tguLHPPAUJ4lueAI3jZm/zel0btUZCzJJ7VLkn5l/9Mt4blOvH+kQSGQQXem
+OR/qnuOf0GZvBeyqdn6/axag67XH/JJULysRJyU3eExRarDzzFhdFPFqSBX/wge2sY0PjlxQRrM9
+vwGYT7JZVEc+NHt4bVaTLnPqZih4zR0Uv6CPLy64Lo7yFIrM6bV8+2ydDKXhlg==
+-----END CERTIFICATE-----
+
+Trustis FPS Root CA
+===================
+-----BEGIN CERTIFICATE-----
+MIIDZzCCAk+gAwIBAgIQGx+ttiD5JNM2a/fH8YygWTANBgkqhkiG9w0BAQUFADBFMQswCQYDVQQG
+EwJHQjEYMBYGA1UEChMPVHJ1c3RpcyBMaW1pdGVkMRwwGgYDVQQLExNUcnVzdGlzIEZQUyBSb290
+IENBMB4XDTAzMTIyMzEyMTQwNloXDTI0MDEyMTExMzY1NFowRTELMAkGA1UEBhMCR0IxGDAWBgNV
+BAoTD1RydXN0aXMgTGltaXRlZDEcMBoGA1UECxMTVHJ1c3RpcyBGUFMgUm9vdCBDQTCCASIwDQYJ
+KoZIhvcNAQEBBQADggEPADCCAQoCggEBAMVQe547NdDfxIzNjpvto8A2mfRC6qc+gIMPpqdZh8mQ
+RUN+AOqGeSoDvT03mYlmt+WKVoaTnGhLaASMk5MCPjDSNzoiYYkchU59j9WvezX2fihHiTHcDnlk
+H5nSW7r+f2C/revnPDgpai/lkQtV/+xvWNUtyd5MZnGPDNcE2gfmHhjjvSkCqPoc4Vu5g6hBSLwa
+cY3nYuUtsuvffM/bq1rKMfFMIvMFE/eC+XN5DL7XSxzA0RU8k0Fk0ea+IxciAIleH2ulrG6nS4zt
+o3Lmr2NNL4XSFDWaLk6M6jKYKIahkQlBOrTh4/L68MkKokHdqeMDx4gVOxzUGpTXn2RZEm0CAwEA
+AaNTMFEwDwYDVR0TAQH/BAUwAwEB/zAfBgNVHSMEGDAWgBS6+nEleYtXQSUhhgtx67JkDoshZzAd
+BgNVHQ4EFgQUuvpxJXmLV0ElIYYLceuyZA6LIWcwDQYJKoZIhvcNAQEFBQADggEBAH5Y//01GX2c
+GE+esCu8jowU/yyg2kdbw++BLa8F6nRIW/M+TgfHbcWzk88iNVy2P3UnXwmWzaD+vkAMXBJV+JOC
+yinpXj9WV4s4NvdFGkwozZ5BuO1WTISkQMi4sKUraXAEasP41BIy+Q7DsdwyhEQsb8tGD+pmQQ9P
+8Vilpg0ND2HepZ5dfWWhPBfnqFVO76DH7cZEf1T1o+CP8HxVIo8ptoGj4W1OLBuAZ+ytIJ8MYmHV
+l/9D7S3B2l0pKoU/rGXuhg8FjZBf3+6f9L/uHfuY5H+QK4R4EA5sSVPvFVtlRkpdr7r7OnIdzfYl
+iB6XzCGcKQENZetX2fNXlrtIzYE=
+-----END CERTIFICATE-----
+
+StartCom Certification Authority
+================================
+-----BEGIN CERTIFICATE-----
+MIIHhzCCBW+gAwIBAgIBLTANBgkqhkiG9w0BAQsFADB9MQswCQYDVQQGEwJJTDEWMBQGA1UEChMN
+U3RhcnRDb20gTHRkLjErMCkGA1UECxMiU2VjdXJlIERpZ2l0YWwgQ2VydGlmaWNhdGUgU2lnbmlu
+ZzEpMCcGA1UEAxMgU3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwHhcNMDYwOTE3MTk0
+NjM3WhcNMzYwOTE3MTk0NjM2WjB9MQswCQYDVQQGEwJJTDEWMBQGA1UEChMNU3RhcnRDb20gTHRk
+LjErMCkGA1UECxMiU2VjdXJlIERpZ2l0YWwgQ2VydGlmaWNhdGUgU2lnbmluZzEpMCcGA1UEAxMg
+U3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAw
+ggIKAoICAQDBiNsJvGxGfHiflXu1M5DycmLWwTYgIiRezul38kMKogZkpMyONvg45iPwbm2xPN1y
+o4UcodM9tDMr0y+v/uqwQVlntsQGfQqedIXWeUyAN3rfOQVSWff0G0ZDpNKFhdLDcfN1YjS6LIp/
+Ho/u7TTQEceWzVI9ujPW3U3eCztKS5/CJi/6tRYccjV3yjxd5srhJosaNnZcAdt0FCX+7bWgiA/d
+eMotHweXMAEtcnn6RtYTKqi5pquDSR3l8u/d5AGOGAqPY1MWhWKpDhk6zLVmpsJrdAfkK+F2PrRt
+2PZE4XNiHzvEvqBTViVsUQn3qqvKv3b9bZvzndu/PWa8DFaqr5hIlTpL36dYUNk4dalb6kMMAv+Z
+6+hsTXBbKWWc3apdzK8BMewM69KN6Oqce+Zu9ydmDBpI125C4z/eIT574Q1w+2OqqGwaVLRcJXrJ
+osmLFqa7LH4XXgVNWG4SHQHuEhANxjJ/GP/89PrNbpHoNkm+Gkhpi8KWTRoSsmkXwQqQ1vp5Iki/
+untp+HDH+no32NgN0nZPV/+Qt+OR0t3vwmC3Zzrd/qqc8NSLf3Iizsafl7b4r4qgEKjZ+xjGtrVc
+UjyJthkqcwEKDwOzEmDyei+B26Nu/yYwl/WL3YlXtq09s68rxbd2AvCl1iuahhQqcvbjM4xdCUsT
+37uMdBNSSwIDAQABo4ICEDCCAgwwDwYDVR0TAQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYD
+VR0OBBYEFE4L7xqkQFulF2mHMMo0aEPQQa7yMB8GA1UdIwQYMBaAFE4L7xqkQFulF2mHMMo0aEPQ
+Qa7yMIIBWgYDVR0gBIIBUTCCAU0wggFJBgsrBgEEAYG1NwEBATCCATgwLgYIKwYBBQUHAgEWImh0
+dHA6Ly93d3cuc3RhcnRzc2wuY29tL3BvbGljeS5wZGYwNAYIKwYBBQUHAgEWKGh0dHA6Ly93d3cu
+c3RhcnRzc2wuY29tL2ludGVybWVkaWF0ZS5wZGYwgc8GCCsGAQUFBwICMIHCMCcWIFN0YXJ0IENv
+bW1lcmNpYWwgKFN0YXJ0Q29tKSBMdGQuMAMCAQEagZZMaW1pdGVkIExpYWJpbGl0eSwgcmVhZCB0
+aGUgc2VjdGlvbiAqTGVnYWwgTGltaXRhdGlvbnMqIG9mIHRoZSBTdGFydENvbSBDZXJ0aWZpY2F0
+aW9uIEF1dGhvcml0eSBQb2xpY3kgYXZhaWxhYmxlIGF0IGh0dHA6Ly93d3cuc3RhcnRzc2wuY29t
+L3BvbGljeS5wZGYwEQYJYIZIAYb4QgEBBAQDAgAHMDgGCWCGSAGG+EIBDQQrFilTdGFydENvbSBG
+cmVlIFNTTCBDZXJ0aWZpY2F0aW9uIEF1dGhvcml0eTANBgkqhkiG9w0BAQsFAAOCAgEAjo/n3JR5
+fPGFf59Jb2vKXfuM/gTFwWLRfUKKvFO3lANmMD+x5wqnUCBVJX92ehQN6wQOQOY+2IirByeDqXWm
+N3PH/UvSTa0XQMhGvjt/UfzDtgUx3M2FIk5xt/JxXrAaxrqTi3iSSoX4eA+D/i+tLPfkpLst0OcN
+Org+zvZ49q5HJMqjNTbOx8aHmNrs++myziebiMMEofYLWWivydsQD032ZGNcpRJvkrKTlMeIFw6T
+tn5ii5B/q06f/ON1FE8qMt9bDeD1e5MNq6HPh+GlBEXoPBKlCcWw0bdT82AUuoVpaiF8H3VhFyAX
+e2w7QSlc4axa0c2Mm+tgHRns9+Ww2vl5GKVFP0lDV9LdJNUso/2RjSe15esUBppMeyG7Oq0wBhjA
+2MFrLH9ZXF2RsXAiV+uKa0hK1Q8p7MZAwC+ITGgBF3f0JBlPvfrhsiAhS90a2Cl9qrjeVOwhVYBs
+HvUwyKMQ5bLmKhQxw4UtjJixhlpPiVktucf3HMiKf8CdBUrmQk9io20ppB+Fq9vlgcitKj1MXVuE
+JnHEhV5xJMqlG2zYYdMa4FTbzrqpMrUi9nNBCV24F10OD5mQ1kfabwo6YigUZ4LZ8dCAWZvLMdib
+D4x3TrVoivJs9iQOLWxwxXPR3hTQcY+203sC9uO41Alua551hDnmfyWl8kgAwKQB2j8=
+-----END CERTIFICATE-----
+
+StartCom Certification Authority G2
+===================================
+-----BEGIN CERTIFICATE-----
+MIIFYzCCA0ugAwIBAgIBOzANBgkqhkiG9w0BAQsFADBTMQswCQYDVQQGEwJJTDEWMBQGA1UEChMN
+U3RhcnRDb20gTHRkLjEsMCoGA1UEAxMjU3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3JpdHkg
+RzIwHhcNMTAwMTAxMDEwMDAxWhcNMzkxMjMxMjM1OTAxWjBTMQswCQYDVQQGEwJJTDEWMBQGA1UE
+ChMNU3RhcnRDb20gTHRkLjEsMCoGA1UEAxMjU3RhcnRDb20gQ2VydGlmaWNhdGlvbiBBdXRob3Jp
+dHkgRzIwggIiMA0GCSqGSIb3DQEBAQUAA4ICDwAwggIKAoICAQC2iTZbB7cgNr2Cu+EWIAOVeq8O
+o1XJJZlKxdBWQYeQTSFgpBSHO839sj60ZwNq7eEPS8CRhXBF4EKe3ikj1AENoBB5uNsDvfOpL9HG
+4A/LnooUCri99lZi8cVytjIl2bLzvWXFDSxu1ZJvGIsAQRSCb0AgJnooD/Uefyf3lLE3PbfHkffi
+Aez9lInhzG7TNtYKGXmu1zSCZf98Qru23QumNK9LYP5/Q0kGi4xDuFby2X8hQxfqp0iVAXV16iul
+Q5XqFYSdCI0mblWbq9zSOdIxHWDirMxWRST1HFSr7obdljKF+ExP6JV2tgXdNiNnvP8V4so75qbs
+O+wmETRIjfaAKxojAuuKHDp2KntWFhxyKrOq42ClAJ8Em+JvHhRYW6Vsi1g8w7pOOlz34ZYrPu8H
+vKTlXcxNnw3h3Kq74W4a7I/htkxNeXJdFzULHdfBR9qWJODQcqhaX2YtENwvKhOuJv4KHBnM0D4L
+nMgJLvlblnpHnOl68wVQdJVznjAJ85eCXuaPOQgeWeU1FEIT/wCc976qUM/iUUjXuG+v+E5+M5iS
+FGI6dWPPe/regjupuznixL0sAA7IF6wT700ljtizkC+p2il9Ha90OrInwMEePnWjFqmveiJdnxMa
+z6eg6+OGCtP95paV1yPIN93EfKo2rJgaErHgTuixO/XWb/Ew1wIDAQABo0IwQDAPBgNVHRMBAf8E
+BTADAQH/MA4GA1UdDwEB/wQEAwIBBjAdBgNVHQ4EFgQUS8W0QGutHLOlHGVuRjaJhwUMDrYwDQYJ
+KoZIhvcNAQELBQADggIBAHNXPyzVlTJ+N9uWkusZXn5T50HsEbZH77Xe7XRcxfGOSeD8bpkTzZ+K
+2s06Ctg6Wgk/XzTQLwPSZh0avZyQN8gMjgdalEVGKua+etqhqaRpEpKwfTbURIfXUfEpY9Z1zRbk
+J4kd+MIySP3bmdCPX1R0zKxnNBFi2QwKN4fRoxdIjtIXHfbX/dtl6/2o1PXWT6RbdejF0mCy2wl+
+JYt7ulKSnj7oxXehPOBKc2thz4bcQ///If4jXSRK9dNtD2IEBVeC2m6kMyV5Sy5UGYvMLD0w6dEG
+/+gyRr61M3Z3qAFdlsHB1b6uJcDJHgoJIIihDsnzb02CVAAgp9KP5DlUFy6NHrgbuxu9mk47EDTc
+nIhT76IxW1hPkWLIwpqazRVdOKnWvvgTtZ8SafJQYqz7Fzf07rh1Z2AQ+4NQ+US1dZxAF7L+/Xld
+blhYXzD8AK6vM8EOTmy6p6ahfzLbOOCxchcKK5HsamMm7YnUeMx0HgX4a/6ManY5Ka5lIxKVCCIc
+l85bBu4M4ru8H0ST9tg4RQUh7eStqxK2A6RCLi3ECToDZ2mEmuFZkIoohdVddLHRDiBYmxOlsGOm
+7XtH/UVVMKTumtTm4ofvmMkyghEpIrwACjFeLQ/Ajulrso8uBtjRkcfGEvRM/TAXw8HaOFvjqerm
+obp573PYtlNXLfbQ4ddI
+-----END CERTIFICATE-----
+
+Buypass Class 2 Root CA
+=======================
+-----BEGIN CERTIFICATE-----
+MIIFWTCCA0GgAwIBAgIBAjANBgkqhkiG9w0BAQsFADBOMQswCQYDVQQGEwJOTzEdMBsGA1UECgwU
+QnV5cGFzcyBBUy05ODMxNjMzMjcxIDAeBgNVBAMMF0J1eXBhc3MgQ2xhc3MgMiBSb290IENBMB4X
+DTEwMTAyNjA4MzgwM1oXDTQwMTAyNjA4MzgwM1owTjELMAkGA1UEBhMCTk8xHTAbBgNVBAoMFEJ1
+eXBhc3MgQVMtOTgzMTYzMzI3MSAwHgYDVQQDDBdCdXlwYXNzIENsYXNzIDIgUm9vdCBDQTCCAiIw
+DQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBANfHXvfBB9R3+0Mh9PT1aeTuMgHbo4Yf5FkNuud1
+g1Lr6hxhFUi7HQfKjK6w3Jad6sNgkoaCKHOcVgb/S2TwDCo3SbXlzwx87vFKu3MwZfPVL4O2fuPn
+9Z6rYPnT8Z2SdIrkHJasW4DptfQxh6NR/Md+oW+OU3fUl8FVM5I+GC911K2GScuVr1QGbNgGE41b
+/+EmGVnAJLqBcXmQRFBoJJRfuLMR8SlBYaNByyM21cHxMlAQTn/0hpPshNOOvEu/XAFOBz3cFIqU
+CqTqc/sLUegTBxj6DvEr0VQVfTzh97QZQmdiXnfgolXsttlpF9U6r0TtSsWe5HonfOV116rLJeff
+awrbD02TTqigzXsu8lkBarcNuAeBfos4GzjmCleZPe4h6KP1DBbdi+w0jpwqHAAVF41og9JwnxgI
+zRFo1clrUs3ERo/ctfPYV3Me6ZQ5BL/T3jjetFPsaRyifsSP5BtwrfKi+fv3FmRmaZ9JUaLiFRhn
+Bkp/1Wy1TbMz4GHrXb7pmA8y1x1LPC5aAVKRCfLf6o3YBkBjqhHk/sM3nhRSP/TizPJhk9H9Z2vX
+Uq6/aKtAQ6BXNVN48FP4YUIHZMbXb5tMOA1jrGKvNouicwoN9SG9dKpN6nIDSdvHXx1iY8f93ZHs
+M+71bbRuMGjeyNYmsHVee7QHIJihdjK4TWxPAgMBAAGjQjBAMA8GA1UdEwEB/wQFMAMBAf8wHQYD
+VR0OBBYEFMmAd+BikoL1RpzzuvdMw964o605MA4GA1UdDwEB/wQEAwIBBjANBgkqhkiG9w0BAQsF
+AAOCAgEAU18h9bqwOlI5LJKwbADJ784g7wbylp7ppHR/ehb8t/W2+xUbP6umwHJdELFx7rxP462s
+A20ucS6vxOOto70MEae0/0qyexAQH6dXQbLArvQsWdZHEIjzIVEpMMpghq9Gqx3tOluwlN5E40EI
+osHsHdb9T7bWR9AUC8rmyrV7d35BH16Dx7aMOZawP5aBQW9gkOLo+fsicdl9sz1Gv7SEr5AcD48S
+aq/v7h56rgJKihcrdv6sVIkkLE8/trKnToyokZf7KcZ7XC25y2a2t6hbElGFtQl+Ynhw/qlqYLYd
+DnkM/crqJIByw5c/8nerQyIKx+u2DISCLIBrQYoIwOula9+ZEsuK1V6ADJHgJgg2SMX6OBE1/yWD
+LfJ6v9r9jv6ly0UsH8SIU653DtmadsWOLB2jutXsMq7Aqqz30XpN69QH4kj3Io6wpJ9qzo6ysmD0
+oyLQI+uUWnpp3Q+/QFesa1lQ2aOZ4W7+jQF5JyMV3pKdewlNWudLSDBaGOYKbeaP4NK75t98biGC
+wWg5TbSYWGZizEqQXsP6JwSxeRV0mcy+rSDeJmAc61ZRpqPq5KM/p/9h3PFaTWwyI0PurKju7koS
+CTxdccK+efrCh2gdC/1cacwG0Jp9VJkqyTkaGa9LKkPzY11aWOIv4x3kqdbQCtCev9eBCfHJxyYN
+rJgWVqA=
+-----END CERTIFICATE-----
+
+Buypass Class 3 Root CA
+=======================
+-----BEGIN CERTIFICATE-----
+MIIFWTCCA0GgAwIBAgIBAjANBgkqhkiG9w0BAQsFADBOMQswCQYDVQQGEwJOTzEdMBsGA1UECgwU
+QnV5cGFzcyBBUy05ODMxNjMzMjcxIDAeBgNVBAMMF0J1eXBhc3MgQ2xhc3MgMyBSb290IENBMB4X
+DTEwMTAyNjA4Mjg1OFoXDTQwMTAyNjA4Mjg1OFowTjELMAkGA1UEBhMCTk8xHTAbBgNVBAoMFEJ1
+eXBhc3MgQVMtOTgzMTYzMzI3MSAwHgYDVQQDDBdCdXlwYXNzIENsYXNzIDMgUm9vdCBDQTCCAiIw
+DQYJKoZIhvcNAQEBBQADggIPADCCAgoCggIBAKXaCpUWUOOV8l6ddjEGMnqb8RB2uACatVI2zSRH
+sJ8YZLya9vrVediQYkwiL944PdbgqOkcLNt4EemOaFEVcsfzM4fkoF0LXOBXByow9c3EN3coTRiR
+5r/VUv1xLXA+58bEiuPwKAv0dpihi4dVsjoT/Lc+JzeOIuOoTyrvYLs9tznDDgFHmV0ST9tD+leh
+7fmdvhFHJlsTmKtdFoqwNxxXnUX/iJY2v7vKB3tvh2PX0DJq1l1sDPGzbjniazEuOQAnFN44wOwZ
+ZoYS6J1yFhNkUsepNxz9gjDthBgd9K5c/3ATAOux9TN6S9ZV+AWNS2mw9bMoNlwUxFFzTWsL8TQH
+2xc519woe2v1n/MuwU8XKhDzzMro6/1rqy6any2CbgTUUgGTLT2G/H783+9CHaZr77kgxve9oKeV
+/afmiSTYzIw0bOIjL9kSGiG5VZFvC5F5GQytQIgLcOJ60g7YaEi7ghM5EFjp2CoHxhLbWNvSO1UQ
+RwUVZ2J+GGOmRj8JDlQyXr8NYnon74Do29lLBlo3WiXQCBJ31G8JUJc9yB3D34xFMFbG02SrZvPA
+Xpacw8Tvw3xrizp5f7NJzz3iiZ+gMEuFuZyUJHmPfWupRWgPK9Dx2hzLabjKSWJtyNBjYt1gD1iq
+j6G8BaVmos8bdrKEZLFMOVLAMLrwjEsCsLa3AgMBAAGjQjBAMA8GA1UdEwEB/wQFMAMBAf8wHQYD
+VR0OBBYEFEe4zf/lb+74suwvTg75JbCOPGvDMA4GA1UdDwEB/wQEAwIBBjANBgkqhkiG9w0BAQsF
+AAOCAgEAACAjQTUEkMJAYmDv4jVM1z+s4jSQuKFvdvoWFqRINyzpkMLyPPgKn9iB5btb2iUspKdV
+cSQy9sgL8rxq+JOssgfCX5/bzMiKqr5qb+FJEMwx14C7u8jYog5kV+qi9cKpMRXSIGrs/CIBKM+G
+uIAeqcwRpTzyFrNHnfzSgCHEy9BHcEGhyoMZCCxt8l13nIoUE9Q2HJLw5QY33KbmkJs4j1xrG0aG
+Q0JfPgEHU1RdZX33inOhmlRaHylDFCfChQ+1iHsaO5S3HWCntZznKWlXWpuTekMwGwPXYshApqr8
+ZORK15FTAaggiG6cX0S5y2CBNOxv033aSF/rtJC8LakcC6wc1aJoIIAE1vyxjy+7SjENSoYc6+I2
+KSb12tjE8nVhz36udmNKekBlk4f4HoCMhuWG1o8O/FMsYOgWYRqiPkN7zTlgVGr18okmAWiDSKIz
+6MkEkbIRNBE+6tBDGR8Dk5AM/1E9V/RBbuHLoL7ryWPNbczk+DaqaJ3tvV2XcEQNtg413OEMXbug
+UZTLfhbrES+jkkXITHHZvMmZUldGL1DPvTVp9D0VzgalLA8+9oG6lLvDu79leNKGef9JOxqDDPDe
+eOzI8k1MGt6CKfjBWtrt7uYnXuhF0J0cUahoq0Tj0Itq4/g7u9xN12TyUb7mqqta6THuBrxzvxNi
+Cp/HuZc=
+-----END CERTIFICATE-----
+
+T-TeleSec GlobalRoot Class 3
+============================
+-----BEGIN CERTIFICATE-----
+MIIDwzCCAqugAwIBAgIBATANBgkqhkiG9w0BAQsFADCBgjELMAkGA1UEBhMCREUxKzApBgNVBAoM
+IlQtU3lzdGVtcyBFbnRlcnByaXNlIFNlcnZpY2VzIEdtYkgxHzAdBgNVBAsMFlQtU3lzdGVtcyBU
+cnVzdCBDZW50ZXIxJTAjBgNVBAMMHFQtVGVsZVNlYyBHbG9iYWxSb290IENsYXNzIDMwHhcNMDgx
+MDAxMTAyOTU2WhcNMzMxMDAxMjM1OTU5WjCBgjELMAkGA1UEBhMCREUxKzApBgNVBAoMIlQtU3lz
+dGVtcyBFbnRlcnByaXNlIFNlcnZpY2VzIEdtYkgxHzAdBgNVBAsMFlQtU3lzdGVtcyBUcnVzdCBD
+ZW50ZXIxJTAjBgNVBAMMHFQtVGVsZVNlYyBHbG9iYWxSb290IENsYXNzIDMwggEiMA0GCSqGSIb3
+DQEBAQUAA4IBDwAwggEKAoIBAQC9dZPwYiJvJK7genasfb3ZJNW4t/zN8ELg63iIVl6bmlQdTQyK
+9tPPcPRStdiTBONGhnFBSivwKixVA9ZIw+A5OO3yXDw/RLyTPWGrTs0NvvAgJ1gORH8EGoel15YU
+NpDQSXuhdfsaa3Ox+M6pCSzyU9XDFES4hqX2iys52qMzVNn6chr3IhUciJFrf2blw2qAsCTz34ZF
+iP0Zf3WHHx+xGwpzJFu5ZeAsVMhg02YXP+HMVDNzkQI6pn97djmiH5a2OK61yJN0HZ65tOVgnS9W
+0eDrXltMEnAMbEQgqxHY9Bn20pxSN+f6tsIxO0rUFJmtxxr1XV/6B7h8DR/Wgx6zAgMBAAGjQjBA
+MA8GA1UdEwEB/wQFMAMBAf8wDgYDVR0PAQH/BAQDAgEGMB0GA1UdDgQWBBS1A/d2O2GCahKqGFPr
+AyGUv/7OyjANBgkqhkiG9w0BAQsFAAOCAQEAVj3vlNW92nOyWL6ukK2YJ5f+AbGwUgC4TeQbIXQb
+fsDuXmkqJa9c1h3a0nnJ85cp4IaH3gRZD/FZ1GSFS5mvJQQeyUapl96Cshtwn5z2r3Ex3XsFpSzT
+ucpH9sry9uetuUg/vBa3wW306gmv7PO15wWeph6KU1HWk4HMdJP2udqmJQV0eVp+QD6CSyYRMG7h
+P0HHRwA11fXT91Q+gT3aSWqas+8QPebrb9HIIkfLzM8BMZLZGOMivgkeGj5asuRrDFR6fUNOuIml
+e9eiPZaGzPImNC1qkp2aGtAw4l1OBLBfiyB+d8E9lYLRRpo7PHi4b6HQDWSieB4pTpPDpFQUWw==
+-----END CERTIFICATE-----
+
+EE Certification Centre Root CA
+===============================
+-----BEGIN CERTIFICATE-----
+MIIEAzCCAuugAwIBAgIQVID5oHPtPwBMyonY43HmSjANBgkqhkiG9w0BAQUFADB1MQswCQYDVQQG
+EwJFRTEiMCAGA1UECgwZQVMgU2VydGlmaXRzZWVyaW1pc2tlc2t1czEoMCYGA1UEAwwfRUUgQ2Vy
+dGlmaWNhdGlvbiBDZW50cmUgUm9vdCBDQTEYMBYGCSqGSIb3DQEJARYJcGtpQHNrLmVlMCIYDzIw
+MTAxMDMwMTAxMDMwWhgPMjAzMDEyMTcyMzU5NTlaMHUxCzAJBgNVBAYTAkVFMSIwIAYDVQQKDBlB
+UyBTZXJ0aWZpdHNlZXJpbWlza2Vza3VzMSgwJgYDVQQDDB9FRSBDZXJ0aWZpY2F0aW9uIENlbnRy
+ZSBSb290IENBMRgwFgYJKoZIhvcNAQkBFglwa2lAc2suZWUwggEiMA0GCSqGSIb3DQEBAQUAA4IB
+DwAwggEKAoIBAQDIIMDs4MVLqwd4lfNE7vsLDP90jmG7sWLqI9iroWUyeuuOF0+W2Ap7kaJjbMeM
+TC55v6kF/GlclY1i+blw7cNRfdCT5mzrMEvhvH2/UpvObntl8jixwKIy72KyaOBhU8E2lf/slLo2
+rpwcpzIP5Xy0xm90/XsY6KxX7QYgSzIwWFv9zajmofxwvI6Sc9uXp3whrj3B9UiHbCe9nyV0gVWw
+93X2PaRka9ZP585ArQ/dMtO8ihJTmMmJ+xAdTX7Nfh9WDSFwhfYggx/2uh8Ej+p3iDXE/+pOoYtN
+P2MbRMNE1CV2yreN1x5KZmTNXMWcg+HCCIia7E6j8T4cLNlsHaFLAgMBAAGjgYowgYcwDwYDVR0T
+AQH/BAUwAwEB/zAOBgNVHQ8BAf8EBAMCAQYwHQYDVR0OBBYEFBLyWj7qVhy/zQas8fElyalL1BSZ
+MEUGA1UdJQQ+MDwGCCsGAQUFBwMCBggrBgEFBQcDAQYIKwYBBQUHAwMGCCsGAQUFBwMEBggrBgEF
+BQcDCAYIKwYBBQUHAwkwDQYJKoZIhvcNAQEFBQADggEBAHv25MANqhlHt01Xo/6tu7Fq1Q+e2+Rj
+xY6hUFaTlrg4wCQiZrxTFGGVv9DHKpY5P30osxBAIWrEr7BSdxjhlthWXePdNl4dp1BUoMUq5KqM
+lIpPnTX/dqQGE5Gion0ARD9V04I8GtVbvFZMIi5GQ4okQC3zErg7cBqklrkar4dBGmoYDQZPxz5u
+uSlNDUmJEYcyW+ZLBMjkXOZ0c5RdFpgTlf7727FE5TpwrDdr5rMzcijJs1eg9gIWiAYLtqZLICjU
+3j2LrTcFU3T+bsy8QxdxXvnFzBqpYe73dgzzcvRyrc9yAjYHR8/vGVCJYMzpJJUPwssd8m92kMfM
+dcGWxZ0=
+-----END CERTIFICATE-----
diff --git a/lib/logstash/config/file.rb b/lib/logstash/config/file.rb
index 5d7e7a312c1..becb4cbc528 100644
--- a/lib/logstash/config/file.rb
+++ b/lib/logstash/config/file.rb
@@ -10,7 +10,7 @@ class LogStash::Config::File
   def initialize(path=nil, string=nil)
     @path = path
     @string = string
-    @logger = LogStash::Logger.new(STDERR)
+    @logger = Cabin::Channel.get(LogStash)
 
     if (path.nil? and string.nil?) or (!path.nil? and !string.nil?)
        raise "Must give path or string, not both or neither"
@@ -42,7 +42,7 @@ def parse
       tryload o[:type], :base
       type = registry[o[:type]]
 
-      # Load the plugin itself (inputs/file, outputs/amqp, etc)
+      # Load the plugin itself (inputs/file, outputs/rabbitmq, etc)
       # TODO(sissel): Error handling
       tryload o[:type], o[:plugin]
       plugin = registry[o[:plugin]]
@@ -83,7 +83,7 @@ def each(&block)
     @config.each do |type, plugin_config_array|
       # plugin_config_array has arrays of each component config:
       # input {
-      #   amqp { ... }
+      #   rabbitmq { ... }
       #   file { ... }
       #   file { ... }
       # }
diff --git a/lib/logstash/config/grammar.rb b/lib/logstash/config/grammar.rb
index 3e7181d5a11..2d584df8b1f 100644
--- a/lib/logstash/config/grammar.rb
+++ b/lib/logstash/config/grammar.rb
@@ -35,14 +35,14 @@ class << self
 	private :_logstash_config_key_offsets, :_logstash_config_key_offsets=
 end
 self._logstash_config_key_offsets = [
-	0, 0, 12, 17, 18, 19, 29, 30, 
-	31, 43, 48, 49, 50, 62, 65, 70, 
-	75, 76, 77, 78, 94, 97, 109, 121, 
-	122, 123, 126, 126, 138, 148, 148, 149, 
-	150, 153, 153, 155, 169, 183, 198, 201, 
-	207, 213, 214, 215, 227, 227, 228, 229, 
-	232, 232, 234, 242, 255, 255, 256, 257, 
-	266, 267, 268
+	0, 0, 13, 19, 21, 23, 34, 36, 
+	38, 51, 57, 59, 61, 74, 78, 84, 
+	90, 92, 94, 95, 112, 116, 129, 142, 
+	144, 146, 150, 150, 163, 174, 174, 176, 
+	178, 182, 182, 184, 199, 214, 230, 234, 
+	241, 248, 250, 252, 265, 265, 267, 269, 
+	273, 273, 275, 284, 298, 298, 300, 302, 
+	312, 314, 316
 ]
 
 class << self
@@ -50,41 +50,47 @@ class << self
 	private :_logstash_config_trans_keys, :_logstash_config_trans_keys=
 end
 self._logstash_config_trans_keys = [
-	32, 35, 95, 123, 9, 10, 48, 57, 
-	65, 90, 97, 122, 32, 35, 123, 9, 
-	10, 10, 10, 32, 35, 95, 125, 9, 
-	10, 65, 90, 97, 122, 10, 10, 32, 
+	13, 32, 35, 95, 123, 9, 10, 48, 
+	57, 65, 90, 97, 122, 13, 32, 35, 
+	123, 9, 10, 10, 13, 10, 13, 13, 
+	32, 35, 95, 125, 9, 10, 65, 90, 
+	97, 122, 10, 13, 10, 13, 13, 32, 
 	35, 95, 123, 9, 10, 48, 57, 65, 
-	90, 97, 122, 32, 35, 123, 9, 10, 
-	10, 10, 32, 34, 35, 39, 95, 125, 
-	9, 10, 65, 90, 97, 122, 10, 34, 
-	92, 32, 35, 61, 9, 10, 32, 35, 
-	61, 9, 10, 10, 10, 62, 32, 34, 
-	35, 39, 43, 45, 91, 95, 9, 10, 
-	48, 57, 65, 90, 97, 122, 10, 34, 
-	92, 32, 34, 35, 39, 95, 125, 9, 
-	10, 65, 90, 97, 122, 32, 34, 35, 
+	90, 97, 122, 13, 32, 35, 123, 9, 
+	10, 10, 13, 10, 13, 13, 32, 34, 
+	35, 39, 95, 125, 9, 10, 65, 90, 
+	97, 122, 10, 13, 34, 92, 13, 32, 
+	35, 61, 9, 10, 13, 32, 35, 61, 
+	9, 10, 10, 13, 10, 13, 62, 13, 
+	32, 34, 35, 39, 43, 45, 91, 95, 
+	9, 10, 48, 57, 65, 90, 97, 122, 
+	10, 13, 34, 92, 13, 32, 34, 35, 
 	39, 95, 125, 9, 10, 65, 90, 97, 
-	122, 10, 10, 10, 39, 92, 32, 35, 
-	61, 95, 9, 10, 48, 57, 65, 90, 
-	97, 122, 32, 35, 95, 125, 9, 10, 
-	65, 90, 97, 122, 10, 10, 10, 39, 
-	92, 48, 57, 32, 34, 35, 39, 95, 
-	125, 9, 10, 48, 57, 65, 90, 97, 
-	122, 32, 34, 35, 39, 95, 125, 9, 
-	10, 48, 57, 65, 90, 97, 122, 32, 
+	122, 13, 32, 34, 35, 39, 95, 125, 
+	9, 10, 65, 90, 97, 122, 10, 13, 
+	10, 13, 10, 13, 39, 92, 13, 32, 
+	35, 61, 95, 9, 10, 48, 57, 65, 
+	90, 97, 122, 13, 32, 35, 95, 125, 
+	9, 10, 65, 90, 97, 122, 10, 13, 
+	10, 13, 10, 13, 39, 92, 48, 57, 
+	13, 32, 34, 35, 39, 95, 125, 9, 
+	10, 48, 57, 65, 90, 97, 122, 13, 
+	32, 34, 35, 39, 95, 125, 9, 10, 
+	48, 57, 65, 90, 97, 122, 13, 32, 
 	34, 35, 39, 43, 45, 95, 9, 10, 
-	48, 57, 65, 90, 97, 122, 10, 34, 
-	92, 32, 35, 44, 93, 9, 10, 32, 
-	35, 44, 93, 9, 10, 10, 10, 32, 
-	34, 35, 39, 95, 125, 9, 10, 65, 
-	90, 97, 122, 10, 10, 10, 39, 92, 
-	48, 57, 32, 35, 44, 93, 9, 10, 
-	48, 57, 32, 35, 44, 93, 95, 9, 
-	10, 48, 57, 65, 90, 97, 122, 10, 
-	10, 32, 35, 95, 9, 10, 65, 90, 
-	97, 122, 10, 10, 32, 35, 95, 9, 
-	10, 65, 90, 97, 122, 0
+	48, 57, 65, 90, 97, 122, 10, 13, 
+	34, 92, 13, 32, 35, 44, 93, 9, 
+	10, 13, 32, 35, 44, 93, 9, 10, 
+	10, 13, 10, 13, 13, 32, 34, 35, 
+	39, 95, 125, 9, 10, 65, 90, 97, 
+	122, 10, 13, 10, 13, 10, 13, 39, 
+	92, 48, 57, 13, 32, 35, 44, 93, 
+	9, 10, 48, 57, 13, 32, 35, 44, 
+	93, 95, 9, 10, 48, 57, 65, 90, 
+	97, 122, 10, 13, 10, 13, 13, 32, 
+	35, 95, 9, 10, 65, 90, 97, 122, 
+	10, 13, 10, 13, 13, 32, 35, 95, 
+	9, 10, 65, 90, 97, 122, 0
 ]
 
 class << self
@@ -92,14 +98,14 @@ class << self
 	private :_logstash_config_single_lengths, :_logstash_config_single_lengths=
 end
 self._logstash_config_single_lengths = [
-	0, 4, 3, 1, 1, 4, 1, 1, 
-	4, 3, 1, 1, 6, 3, 3, 3, 
-	1, 1, 1, 8, 3, 6, 6, 1, 
-	1, 3, 0, 4, 4, 0, 1, 1, 
-	3, 0, 0, 6, 6, 7, 3, 4, 
-	4, 1, 1, 6, 0, 1, 1, 3, 
-	0, 0, 4, 5, 0, 1, 1, 3, 
-	1, 1, 3
+	0, 5, 4, 2, 2, 5, 2, 2, 
+	5, 4, 2, 2, 7, 4, 4, 4, 
+	2, 2, 1, 9, 4, 7, 7, 2, 
+	2, 4, 0, 5, 5, 0, 2, 2, 
+	4, 0, 0, 7, 7, 8, 4, 5, 
+	5, 2, 2, 7, 0, 2, 2, 4, 
+	0, 0, 5, 6, 0, 2, 2, 4, 
+	2, 2, 4
 ]
 
 class << self
@@ -122,14 +128,14 @@ class << self
 	private :_logstash_config_index_offsets, :_logstash_config_index_offsets=
 end
 self._logstash_config_index_offsets = [
-	0, 0, 9, 14, 16, 18, 26, 28, 
-	30, 39, 44, 46, 48, 58, 62, 67, 
-	72, 74, 76, 78, 91, 95, 105, 115, 
-	117, 119, 123, 124, 133, 141, 142, 144, 
-	146, 150, 151, 153, 164, 175, 187, 191, 
-	197, 203, 205, 207, 217, 218, 220, 222, 
-	226, 227, 229, 236, 246, 247, 249, 251, 
-	258, 260, 262
+	0, 0, 10, 16, 19, 22, 31, 34, 
+	37, 47, 53, 56, 59, 70, 75, 81, 
+	87, 90, 93, 95, 109, 114, 125, 136, 
+	139, 142, 147, 148, 158, 167, 168, 171, 
+	174, 179, 180, 182, 194, 206, 219, 224, 
+	231, 238, 241, 244, 255, 256, 259, 262, 
+	267, 268, 270, 278, 289, 290, 293, 296, 
+	304, 307, 310
 ]
 
 class << self
@@ -137,40 +143,46 @@ class << self
 	private :_logstash_config_trans_targs, :_logstash_config_trans_targs=
 end
 self._logstash_config_trans_targs = [
-	2, 3, 1, 5, 2, 1, 1, 1, 
-	0, 2, 3, 5, 2, 0, 2, 4, 
-	2, 4, 5, 6, 8, 58, 5, 8, 
-	8, 0, 5, 7, 5, 7, 9, 10, 
+	2, 2, 3, 1, 5, 2, 1, 1, 
+	1, 0, 2, 2, 3, 5, 2, 0, 
+	2, 2, 4, 2, 2, 4, 5, 5, 
+	6, 8, 58, 5, 8, 8, 0, 5, 
+	5, 7, 5, 5, 7, 9, 9, 10, 
 	8, 12, 9, 8, 8, 8, 0, 9, 
-	10, 12, 9, 0, 9, 11, 9, 11, 
-	12, 13, 53, 25, 27, 28, 12, 27, 
-	27, 0, 0, 14, 52, 13, 15, 16, 
-	18, 15, 0, 15, 16, 18, 15, 0, 
-	15, 17, 15, 17, 19, 0, 19, 20, 
-	30, 32, 34, 34, 37, 36, 19, 35, 
-	36, 36, 0, 0, 21, 29, 20, 22, 
-	13, 23, 25, 27, 28, 22, 27, 27, 
-	0, 22, 13, 23, 25, 27, 28, 22, 
-	27, 27, 0, 22, 24, 22, 24, 0, 
-	14, 26, 25, 25, 15, 16, 18, 27, 
-	15, 27, 27, 27, 0, 5, 6, 8, 
-	58, 5, 8, 8, 0, 20, 19, 31, 
-	19, 31, 0, 21, 33, 32, 32, 35, 
-	0, 22, 13, 23, 25, 27, 28, 22, 
-	35, 27, 27, 0, 22, 13, 23, 25, 
-	36, 28, 22, 36, 36, 36, 0, 37, 
+	9, 10, 12, 9, 0, 9, 9, 11, 
+	9, 9, 11, 12, 12, 13, 53, 25, 
+	27, 28, 12, 27, 27, 0, 0, 0, 
+	14, 52, 13, 15, 15, 16, 18, 15, 
+	0, 15, 15, 16, 18, 15, 0, 15, 
+	15, 17, 15, 15, 17, 19, 0, 19, 
+	19, 20, 30, 32, 34, 34, 37, 36, 
+	19, 35, 36, 36, 0, 0, 0, 21, 
+	29, 20, 22, 22, 13, 23, 25, 27, 
+	28, 22, 27, 27, 0, 22, 22, 13, 
+	23, 25, 27, 28, 22, 27, 27, 0, 
+	22, 22, 24, 22, 22, 24, 0, 0, 
+	14, 26, 25, 25, 15, 15, 16, 18, 
+	27, 15, 27, 27, 27, 0, 5, 5, 
+	6, 8, 58, 5, 8, 8, 0, 20, 
+	19, 19, 31, 19, 19, 31, 0, 0, 
+	21, 33, 32, 32, 35, 0, 22, 22, 
+	13, 23, 25, 27, 28, 22, 35, 27, 
+	27, 0, 22, 22, 13, 23, 25, 36, 
+	28, 22, 36, 36, 36, 0, 37, 37, 
 	38, 45, 47, 49, 49, 51, 37, 50, 
-	51, 51, 0, 0, 39, 44, 38, 40, 
-	41, 37, 43, 40, 0, 40, 41, 37, 
-	43, 40, 0, 40, 42, 40, 42, 22, 
-	13, 23, 25, 27, 28, 22, 27, 27, 
-	0, 38, 37, 46, 37, 46, 0, 39, 
-	48, 47, 47, 50, 0, 40, 41, 37, 
-	43, 40, 50, 0, 40, 41, 37, 43, 
-	51, 40, 51, 51, 51, 0, 13, 12, 
-	54, 12, 54, 55, 56, 1, 55, 1, 
-	1, 0, 55, 57, 55, 57, 55, 56, 
-	1, 55, 1, 1, 0, 0
+	51, 51, 0, 0, 0, 39, 44, 38, 
+	40, 40, 41, 37, 43, 40, 0, 40, 
+	40, 41, 37, 43, 40, 0, 40, 40, 
+	42, 40, 40, 42, 22, 22, 13, 23, 
+	25, 27, 28, 22, 27, 27, 0, 38, 
+	37, 37, 46, 37, 37, 46, 0, 0, 
+	39, 48, 47, 47, 50, 0, 40, 40, 
+	41, 37, 43, 40, 50, 0, 40, 40, 
+	41, 37, 43, 51, 40, 51, 51, 51, 
+	0, 13, 12, 12, 54, 12, 12, 54, 
+	55, 55, 56, 1, 55, 1, 1, 0, 
+	55, 55, 57, 55, 55, 57, 55, 55, 
+	56, 1, 55, 1, 1, 0, 0
 ]
 
 class << self
@@ -178,40 +190,46 @@ class << self
 	private :_logstash_config_trans_actions, :_logstash_config_trans_actions=
 end
 self._logstash_config_trans_actions = [
-	5, 5, 0, 30, 5, 0, 0, 0, 
-	19, 0, 0, 13, 0, 19, 1, 1, 
-	0, 0, 0, 0, 1, 0, 0, 1, 
-	1, 19, 1, 1, 0, 0, 5, 5, 
+	5, 5, 5, 0, 30, 5, 0, 0, 
+	0, 19, 0, 0, 0, 13, 0, 19, 
+	1, 1, 1, 0, 0, 0, 0, 0, 
+	0, 1, 0, 0, 1, 1, 19, 1, 
+	1, 1, 0, 0, 0, 5, 5, 5, 
 	0, 5, 5, 0, 0, 0, 19, 0, 
-	0, 0, 0, 19, 1, 1, 0, 0, 
-	0, 39, 0, 39, 39, 0, 0, 39, 
-	39, 19, 19, 0, 0, 0, 7, 7, 
-	7, 7, 19, 0, 0, 0, 0, 19, 
-	1, 1, 0, 0, 0, 19, 0, 1, 
-	0, 1, 1, 1, 9, 1, 0, 1, 
-	1, 1, 19, 19, 0, 0, 0, 33, 
-	59, 33, 59, 59, 33, 33, 59, 59, 
-	19, 0, 1, 0, 1, 1, 0, 0, 
-	1, 1, 19, 1, 1, 0, 0, 19, 
-	0, 0, 0, 0, 5, 5, 5, 0, 
-	5, 0, 0, 0, 19, 11, 11, 42, 
-	11, 11, 42, 42, 19, 0, 1, 1, 
-	0, 0, 19, 0, 0, 0, 0, 0, 
-	19, 24, 51, 24, 51, 51, 24, 24, 
-	0, 51, 51, 19, 27, 55, 27, 55, 
-	0, 27, 27, 0, 0, 0, 19, 0, 
+	0, 0, 0, 0, 19, 1, 1, 1, 
+	0, 0, 0, 0, 0, 39, 0, 39, 
+	39, 0, 0, 39, 39, 19, 19, 19, 
+	0, 0, 0, 7, 7, 7, 7, 7, 
+	19, 0, 0, 0, 0, 0, 19, 1, 
+	1, 1, 0, 0, 0, 0, 19, 0, 
+	0, 1, 0, 1, 1, 1, 9, 1, 
+	0, 1, 1, 1, 19, 19, 19, 0, 
+	0, 0, 33, 33, 59, 33, 59, 59, 
+	33, 33, 59, 59, 19, 0, 0, 1, 
+	0, 1, 1, 0, 0, 1, 1, 19, 
+	1, 1, 1, 0, 0, 0, 19, 19, 
+	0, 0, 0, 0, 5, 5, 5, 5, 
+	0, 5, 0, 0, 0, 19, 11, 11, 
+	11, 42, 11, 11, 42, 42, 19, 0, 
+	1, 1, 1, 0, 0, 0, 19, 19, 
+	0, 0, 0, 0, 0, 19, 24, 24, 
+	51, 24, 51, 51, 24, 24, 0, 51, 
+	51, 19, 27, 27, 55, 27, 55, 0, 
+	27, 27, 0, 0, 0, 19, 0, 0, 
 	1, 0, 1, 1, 1, 1, 0, 1, 
-	1, 1, 19, 19, 0, 0, 0, 7, 
-	7, 7, 7, 7, 19, 0, 0, 0, 
-	0, 0, 19, 1, 1, 0, 0, 36, 
-	63, 36, 63, 63, 36, 36, 63, 63, 
-	19, 0, 1, 1, 0, 0, 19, 0, 
-	0, 0, 0, 0, 19, 3, 3, 3, 
-	3, 3, 0, 19, 5, 5, 5, 5, 
-	0, 5, 0, 0, 0, 19, 0, 1, 
-	1, 0, 0, 0, 0, 1, 0, 1, 
-	1, 19, 1, 1, 0, 0, 15, 15, 
-	45, 15, 45, 45, 19, 0
+	1, 1, 19, 19, 19, 0, 0, 0, 
+	7, 7, 7, 7, 7, 7, 19, 0, 
+	0, 0, 0, 0, 0, 19, 1, 1, 
+	1, 0, 0, 0, 36, 36, 63, 36, 
+	63, 63, 36, 36, 63, 63, 19, 0, 
+	1, 1, 1, 0, 0, 0, 19, 19, 
+	0, 0, 0, 0, 0, 19, 3, 3, 
+	3, 3, 3, 3, 0, 19, 5, 5, 
+	5, 5, 5, 0, 5, 0, 0, 0, 
+	19, 0, 1, 1, 1, 0, 0, 0, 
+	0, 0, 0, 1, 0, 1, 1, 19, 
+	1, 1, 1, 0, 0, 0, 15, 15, 
+	15, 45, 15, 45, 45, 19, 0
 ]
 
 class << self
@@ -268,7 +286,7 @@ def parse(string)
 
     # BEGIN RAGEL INIT
     
-# line 272 "grammar.rb"
+# line 290 "grammar.rb"
 begin
 	p ||= 0
 	pe ||= data.length
@@ -281,7 +299,7 @@ def parse(string)
     begin 
       # BEGIN RAGEL EXEC 
       
-# line 285 "grammar.rb"
+# line 303 "grammar.rb"
 begin
 	_klen, _trans, _keys, _acts, _nacts = nil
 	_goto_level = 0
@@ -476,7 +494,7 @@ def parse(string)
             $stderr.puts "Error at line #{self.line(string, p)}, column #{self.column(string, p)}: #{string[p .. -1].inspect}"
             # TODO(sissel): Note what we were expecting?
           		end
-# line 480 "grammar.rb"
+# line 498 "grammar.rb"
 			end # action switch
 		end
 	end
@@ -532,7 +550,7 @@ def parse(string)
             $stderr.puts "Error at line #{self.line(string, p)}, column #{self.column(string, p)}: #{string[p .. -1].inspect}"
             # TODO(sissel): Note what we were expecting?
           		end
-# line 536 "grammar.rb"
+# line 554 "grammar.rb"
 		end # eof action switch
 	end
 	if _trigger_goto
diff --git a/lib/logstash/config/grammar.rl b/lib/logstash/config/grammar.rl
index 5b05a0e2ccf..cfb27779df0 100644
--- a/lib/logstash/config/grammar.rl
+++ b/lib/logstash/config/grammar.rl
@@ -89,21 +89,21 @@ require "logstash/namespace"
   }
 
   #%{ e = @tokenstack.pop; puts "Comment: #{string[e ... p]}" };
-  comment = "#" (any - [\n])* >mark ; 
-  ws = ([ \t\n] | comment)** ;
+  comment = "#" (any - [\r\n])* >mark ; 
+  ws = ([ \t\r\n] | comment)** ;
   #ws = ([ \t\n])** ;
 
   # TODO(sissel): Support floating point values?
   numeric = ( ("+" | "-")?  [0-9] :>> [0-9]** ) >mark %stack_numeric;
   quoted_string = ( 
-    ( "\"" ( ( (any - [\\"\n]) | "\\" any )* ) "\"" )
-    | ( "'" ( ( (any - [\\'\n]) | "\\" any )* ) "'" ) 
+    ( "\"" ( ( (any - [\\"\r\n]) | "\\" any )* ) "\"" )
+    | ( "'" ( ( (any - [\\'\r\n]) | "\\" any )* ) "'" ) 
   ) >mark %stack_quoted_string ;
   naked_string = ( [A-Za-z_] :>> [A-Za-z0-9_]* ) >mark %stack_string ;
   string = ( quoted_string | naked_string ) ;
 
   # TODO(sissel): allow use of this.
-  regexp_literal = ( "/" ( ( (any - [\\'\n]) | "\\" any )* ) "/" )  ;
+  regexp_literal = ( "/" ( ( (any - [\\'\r\n]) | "\\" any )* ) "/" )  ;
 
   array = ( "[" ws ( string | numeric ) ws ("," ws (string | numeric ) ws)* "]" ) >array_init %array_push;
   # TODO(sissel): Implement hash syntax { key => value, ... }
diff --git a/lib/logstash/config/mixin.rb b/lib/logstash/config/mixin.rb
index 7792007fb26..be49da2b4cf 100644
--- a/lib/logstash/config/mixin.rb
+++ b/lib/logstash/config/mixin.rb
@@ -4,6 +4,7 @@
 require "logstash/logging"
 require "logstash/util/password"
 require "logstash/version"
+require "i18n"
 
 # This module is meant as a mixin to classes wishing to be configurable from
 # config files
@@ -44,9 +45,12 @@ def self.included(base)
   def config_init(params)
     # Validation will modify the values inside params if necessary.
     # For example: converting a string to a number, etc.
+    
+    # store the plugin type, turns LogStash::Inputs::Base into 'input'
+    @plugin_type = self.class.ancestors.find { |a| a.name =~ /::Base$/ }.config_name
     if !self.class.validate(params)
-      @logger.error("Config validation failed.")
-      exit 1
+      raise LogStash::Plugin::ConfigurationError,
+        I18n.t("logstash.agent.configuration.invalid_plugin_settings")
     end
 
     # warn about deprecated variable use
@@ -157,8 +161,10 @@ def inherited(subclass)
     end # def inherited
 
     def validate(params)
-      @plugin_name = [superclass.config_name, config_name].join("/")
-      @logger = LogStash::Logger.new(STDOUT)
+      @plugin_name = config_name #[superclass.config_name, config_name].join("/")
+      @plugin_type = ancestors.find { |a| a.name =~ /::Base$/ }.config_name
+      #.name.split("::")[1].downcase.gsub(/s$/,"")
+      @logger = Cabin::Channel.get(LogStash)
       is_valid = true
 
       is_valid &&= validate_plugin_status
@@ -221,8 +227,9 @@ def validate_check_required_parameter_names(params)
         elsif config_key.is_a?(String)
           next if params.keys.member?(config_key)
         end
-        @logger.error("Missing required parameter '#{config_key}' for " \
-                      "#{@plugin_name}")
+        @logger.error(I18n.t("logstash.agent.configuration.setting_missing",
+                             :setting => config_key, :plugin => @plugin_name,
+                             :type => @plugin_type))
         is_valid = false
       end
 
@@ -257,7 +264,9 @@ def validate_check_parameter_values(params)
             # Used for converting values in the config to proper objects.
             params[key] = result if !result.nil?
           else
-            @logger.error("Failed config #{@plugin_name}/#{key}: #{result} (#{value.inspect})")
+            @logger.error(I18n.t("logstash.agent.configuration.setting_invalid",
+                                 :plugin => @plugin_name, :type => @plugin_type,
+                                 :value => value, :value_type => config_val))
           end
           #puts "Result: #{key} / #{result.inspect} / #{success}"
           is_valid &&= success
diff --git a/lib/logstash/config/test.conf b/lib/logstash/config/test.conf
index 5b28b4192c1..af69223e761 100644
--- a/lib/logstash/config/test.conf
+++ b/lib/logstash/config/test.conf
@@ -1,5 +1,5 @@
 input {
-  amqp {
+  rabbitmq {
     port => 12345 
     tag => [ a, b, c ]
   }
diff --git a/lib/logstash/event.rb b/lib/logstash/event.rb
index b32068c6de0..d1d76cbfb36 100644
--- a/lib/logstash/event.rb
+++ b/lib/logstash/event.rb
@@ -1,302 +1,12 @@
-require "json"
-require "time"
-require "date"
-require "logstash/time_addon"
-require "logstash/namespace"
-require "uri"
-
 # General event type. 
-# Basically a light wrapper on top of a hash.
 #
-# TODO(sissel): properly handle lazy properties like parsed time formats, urls,
-# etc, as necessary.
+# Basically a light wrapper on top of a hash.
 class LogStash::Event
-  public
-  def initialize(data=nil)
-    @cancelled = false
-
-    @data = {
-      "@source" => "unknown",
-      "@tags" => [],
-      "@fields" => {},
-    }
-    @data.merge!(data) unless data.nil?
-    @data["@timestamp"] ||= LogStash::Time.now
-  end # def initialize
-
-  if defined?(RUBY_ENGINE) && RUBY_ENGINE == "jruby"
-    @@date_parser = Java::org.joda.time.format.ISODateTimeFormat.dateTimeParser.withOffsetParsed
+  if ENV["LOGSTASH_SCHEMA"] == "1"
+    require "logstash/event_v1"
+    include LogStash::EventV1
   else
-    # TODO(sissel): LOGSTASH-217
-    @@date_parser ||= nil
-  end
-
-  public
-  def self.from_json(json)
-    return LogStash::Event.new(JSON.parse(json))
-  end # def self.from_json
-
-  public
-  def cancel
-    @cancelled = true
-  end # def cancel
-
-  public
-  def uncancel
-    @cancelled = false
-  end # def uncancel
-
-  public
-  def cancelled?
-    return @cancelled
-  end # def cancelled?
-
-  # Create a deep-ish copy of this event.
-  public
-  def clone
-    newdata = @data.clone
-    newdata["@fields"] = {}
-    fields.each do |k,v|
-      newdata["@fields"][k] = v.clone
-    end
-    return LogStash::Event.new(newdata)
-  end # def clone
-
-  public
-  def to_s
-    return self.sprintf("%{@timestamp} %{@source}: %{@message}")
-  end # def to_s
-
-  public
-  def timestamp; @data["@timestamp"]; end # def timestamp
-  def timestamp=(val); @data["@timestamp"] = val; end # def timestamp=
-
-  public
-  def unix_timestamp
-    if RUBY_ENGINE != "jruby"
-      # This is really slow. See LOGSTASH-217
-      # For some reason, ::Time.parse isn't present even after 'require "time"'
-      # so use DateTime.parse
-      return ::DateTime.parse(timestamp).to_time.to_f
-    else
-      time = @@date_parser.parseDateTime(timestamp)
-      return time.getMillis.to_f / 1000
-    end
-  end
-
-  def ruby_timestamp
-    return ::DateTime.parse(timestamp).to_time
-  end  
-  
-  
-  public
-  def source; @data["@source"]; end # def source
-  def source=(val)
-    uri = URI.parse(val) rescue nil
-    val = uri if uri
-    if val.is_a?(URI)
-      @data["@source"] = val.to_s
-      @data["@source_host"] = val.host if @data["@source_host"].nil?
-      @data["@source_path"] = val.path
-    else
-      @data["@source"] = val
-    end
-  end # def source=
-
-  public
-  def source_host; @data["@source_host"]; end # def source_host
-  def source_host=(val); @data["@source_host"] = val; end # def source_host=
-
-  public
-  def source_path; @data["@source_path"]; end # def source_path
-  def source_path=(val); @data["@source_path"] = val; end # def source_path=
-
-  public
-  def message; @data["@message"]; end # def message
-  def message=(val); @data["@message"] = val; end # def message=
-
-  public
-  def type; @data["@type"]; end # def type
-  def type=(val); @data["@type"] = val; end # def type=
-
-  public
-  def tags; @data["@tags"]; end # def tags
-  def tags=(val); @data["@tags"] = val; end # def tags=
-
-  def id; @data["@id"]; end # def id
-  def id=(val); @data["@id"] = val; end # def id=
-
-  # field-related access
-  public
-  def [](key)
-    # If the key isn't in fields and it starts with an "@" sign, get it out of data instead of fields
-    if ! @data["@fields"].has_key?(key) and key.slice(0,1) == "@"
-      return @data[key]
-    elsif key.index(/(?<!\\)\./)
-      value = nil
-      obj = @data["@fields"]
-      # "." is what ES uses to access structured data, so adopt that
-      # idea here, too.  "foo.bar" will access key "bar" under hash "foo".
-      key.split(/(?<!\\)\./).each do |segment|
-        segment.gsub!(/\\\./, ".")
-        if (obj.is_a?(Array) || (obj.is_a?(Hash) && !obj.member?(segment)) )
-          # try to safely cast segment to integer for the 0 in foo.0.bar
-          begin
-            segment = Integer(segment)
-          rescue Exception
-            #not an int, do nothing, segment remains a string
-          end
-        end
-        if obj
-          value = obj[segment] rescue nil
-          obj = obj[segment] rescue nil
-        else
-          value = nil
-          break
-        end
-      end # key.split.each
-      return value
-    else
-      return @data["@fields"][key.gsub(/\\\./, ".")]
-    end
-  end # def []
-  
-  public
-  def []=(key, value)
-    if @data.has_key?(key)
-      @data[key] = value
-    else
-      @data["@fields"][key] = value
-    end
-  end # def []=
-
-  def fields; return @data["@fields"] end # def fields
-  
-  public
-  def to_json(*args); return @data.to_json(*args) end # def to_json
-  def to_hash; return @data end # def to_hash
-
-  public
-  def overwrite(event)
-    @data = event.to_hash
+    require "logstash/event_v0"
+    include LogStash::EventV0
   end
-
-  public
-  def include?(key)
-    return !self[key].nil?
-  end # def include?
-
-  # Append an event to this one.
-  public
-  def append(event)
-    if event.message
-      if self.message
-        self.message += "\n" + event.message 
-      else
-        self.message = event.message
-      end
-    end
-    self.tags |= event.tags
-
-    # Append all fields
-    event.fields.each do |name, value|
-      if self.fields.include?(name)
-        if !self.fields[name].is_a?(Array)
-          self.fields[name] = [self.fields[name]]
-        end
-        if value.is_a?(Array)
-          self.fields[name] |= value
-        else
-          self.fields[name] << value unless self.fields[name].include?(value)
-        end
-      else
-        self.fields[name] = value
-      end
-    end # event.fields.each
-  end # def append
-
-  # Remove a field. Returns the value of that field when deleted
-  public
-  def remove(field)
-    if @data.has_key?(field)
-      return @data.delete(field)
-    else
-      return @data["@fields"].delete(field)
-    end
-  end # def remove
-
-  # sprintf. This could use a better method name.
-  # The idea is to take an event and convert it to a string based on 
-  # any format values, delimited by %{foo} where 'foo' is a field or
-  # metadata member.
-  #
-  # For example, if the event has @type == "foo" and @source == "bar"
-  # then this string:
-  #   "type is %{@type} and source is %{@source}"
-  # will return
-  #   "type is foo and source is bar"
-  #
-  # If a %{name} value is an array, then we will join by ','
-  # If a %{name} value does not exist, then no substitution occurs.
-  #
-  # TODO(sissel): It is not clear what the value of a field that 
-  # is an array (or hash?) should be. Join by comma? Something else?
-  public
-  def sprintf(format)
-    if format.index("%").nil?
-      return format
-    end
-
-    return format.gsub(/%\{[^}]+\}/) do |tok|
-      # Take the inside of the %{ ... }
-      key = tok[2 ... -1]
-
-      if key == "+%s"
-        # Got %{+%s}, support for unix epoch time
-        if RUBY_ENGINE != "jruby"
-          # This is really slow. See LOGSTASH-217
-          Time.parse(self.timestamp).to_i
-        else
-          datetime = @@date_parser.parseDateTime(self.timestamp)
-          (datetime.getMillis / 1000).to_i
-        end
-      elsif key[0,1] == "+"
-        # We got a %{+TIMEFORMAT} so use joda to format it.
-        if RUBY_ENGINE != "jruby"
-          # This is really slow. See LOGSTASH-217
-          datetime = Date.parse(self.timestamp)
-          format = key[1 .. -1]
-          datetime.strftime(format)
-        else
-          datetime = @@date_parser.parseDateTime(self.timestamp)
-          format = key[1 .. -1]
-          datetime.toString(format) # return requested time format
-        end
-      else
-        # Use an event field.
-        value = self[key]
-
-        case value
-        when nil
-          tok # leave the %{foo} if this field does not exist in this event.
-        when Array
-          value.join(",") # Join by ',' if value is an array
-        when Hash
-          value.to_json # Convert hashes to json
-        else
-          value # otherwise return the value
-        end
-      end
-    end
-  end # def sprintf
-
-  public
-  def ==(other)
-    #puts "#{self.class.name}#==(#{other.inspect})"
-    if !other.is_a?(self.class)
-      return false
-    end
-
-    return other.to_hash == self.to_hash
-  end # def ==
 end # class LogStash::Event
diff --git a/lib/logstash/event_v0.rb b/lib/logstash/event_v0.rb
new file mode 100644
index 00000000000..d33422bfc8e
--- /dev/null
+++ b/lib/logstash/event_v0.rb
@@ -0,0 +1,310 @@
+require "json"
+require "time"
+require "date"
+require "logstash/time_addon"
+require "logstash/namespace"
+require "uri"
+
+# General event type. 
+# Basically a light wrapper on top of a hash.
+#
+# TODO(sissel): properly handle lazy properties like parsed time formats, urls,
+# etc, as necessary.
+module LogStash::EventV0
+  public
+  def initialize(data=nil)
+    @cancelled = false
+
+    @data = {
+      "@source" => "unknown",
+      "@tags" => [],
+      "@fields" => {},
+    }
+    @data.merge!(data) unless data.nil?
+    @data["@timestamp"] ||= LogStash::Time.now
+  end # def initialize
+
+  if defined?(RUBY_ENGINE) && RUBY_ENGINE == "jruby"
+    @@date_parser = Java::org.joda.time.format.ISODateTimeFormat.dateTimeParser.withOffsetParsed
+  else
+    # TODO(sissel): LOGSTASH-217
+    @@date_parser ||= nil
+  end
+
+  public
+  def cancel
+    @cancelled = true
+  end # def cancel
+
+  public
+  def uncancel
+    @cancelled = false
+  end # def uncancel
+
+  public
+  def cancelled?
+    return @cancelled
+  end # def cancelled?
+
+  # Create a deep-ish copy of this event.
+  public
+  def clone
+    newdata = @data.clone
+    newdata["@fields"] = {}
+    fields.each do |k,v|
+      newdata["@fields"][k] = v.clone
+    end
+    return LogStash::Event.new(newdata)
+  end # def clone
+
+  public
+  def to_s
+    return self.sprintf("%{@timestamp} %{@source}: %{@message}")
+  end # def to_s
+
+  public
+  def timestamp; @data["@timestamp"]; end # def timestamp
+  def timestamp=(val); @data["@timestamp"] = val; end # def timestamp=
+
+  public
+  def unix_timestamp
+    if RUBY_ENGINE != "jruby"
+      # This is really slow. See LOGSTASH-217
+      # For some reason, ::Time.parse isn't present even after 'require "time"'
+      # so use DateTime.parse
+      return ::DateTime.parse(timestamp).to_time.to_f
+    else
+      time = @@date_parser.parseDateTime(timestamp)
+      return time.getMillis.to_f / 1000
+    end
+  end
+
+  def ruby_timestamp
+    return ::DateTime.parse(timestamp).to_time
+  end  
+  
+  
+  public
+  def source; @data["@source"]; end # def source
+  def source=(val)
+    uri = URI.parse(val) rescue nil
+    val = uri if uri
+    if val.is_a?(URI)
+      @data["@source"] = val.to_s
+      @data["@source_host"] = val.host if @data["@source_host"].nil?
+      @data["@source_path"] = val.path
+    else
+      @data["@source"] = val
+    end
+  end # def source=
+
+  public
+  def source_host; @data["@source_host"]; end # def source_host
+  def source_host=(val); @data["@source_host"] = val; end # def source_host=
+
+  public
+  def source_path; @data["@source_path"]; end # def source_path
+  def source_path=(val); @data["@source_path"] = val; end # def source_path=
+
+  public
+  def message; @data["@message"]; end # def message
+  def message=(val); @data["@message"] = val; end # def message=
+
+  public
+  def type; @data["@type"]; end # def type
+  def type=(val); @data["@type"] = val; end # def type=
+
+  public
+  def tags; @data["@tags"]; end # def tags
+  def tags=(val); @data["@tags"] = val; end # def tags=
+
+  def id; @data["@id"]; end # def id
+  def id=(val); @data["@id"] = val; end # def id=
+
+  # field-related access
+  public
+  def [](key)
+    # If the key isn't in fields and it starts with an "@" sign, get it out of data instead of fields
+    if ! @data["@fields"].has_key?(key) and key.slice(0,1) == "@"
+      return @data[key]
+    elsif key.index(/(?<!\\)\./)
+      value = nil
+      obj = @data["@fields"]
+      # "." is what ES uses to access structured data, so adopt that
+      # idea here, too.  "foo.bar" will access key "bar" under hash "foo".
+      key.split(/(?<!\\)\./).each do |segment|
+        segment.gsub!(/\\\./, ".")
+        if (obj.is_a?(Array) || (obj.is_a?(Hash) && !obj.member?(segment)) )
+          # try to safely cast segment to integer for the 0 in foo.0.bar
+          begin
+            segment = Integer(segment)
+          rescue Exception
+            #not an int, do nothing, segment remains a string
+          end
+        end
+        if obj
+          value = obj[segment] rescue nil
+          obj = obj[segment] rescue nil
+        else
+          value = nil
+          break
+        end
+      end # key.split.each
+      return value
+    else
+      return @data["@fields"][key.gsub(/\\\./, ".")]
+    end
+  end # def []
+  
+  public
+  def []=(key, value)
+    if @data.has_key?(key)
+      @data[key] = value
+    else
+      @data["@fields"][key] = value
+    end
+  end # def []=
+
+  def fields; return @data["@fields"] end # def fields
+  
+  public
+  def to_json(*args); return @data.to_json(*args) end # def to_json
+  def to_hash; return @data end # def to_hash
+
+  public
+  def overwrite(event)
+    @data = event.to_hash
+  end
+
+  public
+  def include?(key)
+    return !self[key].nil?
+  end # def include?
+
+  # Append an event to this one.
+  public
+  def append(event)
+    if event.message
+      if self.message
+        self.message += "\n" + event.message 
+      else
+        self.message = event.message
+      end
+    end
+    self.tags |= event.tags
+
+    # Append all fields
+    event.fields.each do |name, value|
+      if self.fields.include?(name)
+        if !self.fields[name].is_a?(Array)
+          self.fields[name] = [self.fields[name]]
+        end
+        if value.is_a?(Array)
+          self.fields[name] |= value
+        else
+          self.fields[name] << value unless self.fields[name].include?(value)
+        end
+      else
+        self.fields[name] = value
+      end
+    end # event.fields.each
+  end # def append
+
+  # Remove a field. Returns the value of that field when deleted
+  public
+  def remove(field)
+    if @data.has_key?(field)
+      return @data.delete(field)
+    else
+      return @data["@fields"].delete(field)
+    end
+  end # def remove
+
+  # sprintf. This could use a better method name.
+  # The idea is to take an event and convert it to a string based on 
+  # any format values, delimited by %{foo} where 'foo' is a field or
+  # metadata member.
+  #
+  # For example, if the event has @type == "foo" and @source == "bar"
+  # then this string:
+  #   "type is %{@type} and source is %{@source}"
+  # will return
+  #   "type is foo and source is bar"
+  #
+  # If a %{name} value is an array, then we will join by ','
+  # If a %{name} value does not exist, then no substitution occurs.
+  #
+  # TODO(sissel): It is not clear what the value of a field that 
+  # is an array (or hash?) should be. Join by comma? Something else?
+  public
+  def sprintf(format)
+    if format.index("%").nil?
+      return format
+    end
+
+    return format.gsub(/%\{[^}]+\}/) do |tok|
+      # Take the inside of the %{ ... }
+      key = tok[2 ... -1]
+
+      if key == "+%s"
+        # Got %{+%s}, support for unix epoch time
+        if RUBY_ENGINE != "jruby"
+          # This is really slow. See LOGSTASH-217
+          Time.parse(self.timestamp).to_i
+        else
+          datetime = @@date_parser.parseDateTime(self.timestamp)
+          (datetime.getMillis / 1000).to_i
+        end
+      elsif key[0,1] == "+"
+        # We got a %{+TIMEFORMAT} so use joda to format it.
+        if RUBY_ENGINE != "jruby"
+          # This is really slow. See LOGSTASH-217
+          datetime = Date.parse(self.timestamp)
+          format = key[1 .. -1]
+          datetime.strftime(format)
+        else
+          datetime = @@date_parser.parseDateTime(self.timestamp)
+          format = key[1 .. -1]
+          datetime.toString(format) # return requested time format
+        end
+      else
+        # Use an event field.
+        value = self[key]
+
+        case value
+        when nil
+          tok # leave the %{foo} if this field does not exist in this event.
+        when Array
+          value.join(",") # Join by ',' if value is an array
+        when Hash
+          value.to_json # Convert hashes to json
+        else
+          value # otherwise return the value
+        end
+      end
+    end
+  end # def sprintf
+
+  public
+  def ==(other)
+    #puts "#{self.class.name}#==(#{other.inspect})"
+    if !other.is_a?(self.class)
+      return false
+    end
+
+    return other.to_hash == self.to_hash
+  end # def ==
+
+  # Add class methods on inclusion.
+  def self.included(klass)
+    klass.extend(ClassMethods)
+  end # def included
+
+  module ClassMethods
+    public
+    def from_json(json)
+      return self.new(JSON.parse(json))
+    end # def from_json
+  end
+
+end # module LogStash::EventV0
diff --git a/lib/logstash/event_v1.rb b/lib/logstash/event_v1.rb
new file mode 100644
index 00000000000..047caf64923
--- /dev/null
+++ b/lib/logstash/event_v1.rb
@@ -0,0 +1,196 @@
+require "json"
+require "time"
+require "date"
+require "logstash/time_addon"
+require "logstash/namespace"
+require "uri"
+
+# the logstash event object.
+#
+# An event is simply a tuple of (timestamp, data).
+# The 'timestamp' is an ISO8601 timestamp. Data is anything - any message,
+# context, references, etc that are relevant to this event.
+#
+# Internally, this is represented as a hash with only two guaranteed fields.
+#
+# * "@timestamp" - an ISO8601 timestamp representing the time the event
+#   occurred at.
+# * "@version" - the version of the schema. Currently "1"
+#
+# They are prefixed with an "@" symbol to avoid clashing with your
+# own custom fields. 
+#
+# When serialized, this is represented in JSON. For example:
+#
+#     {
+#       "@timestamp": "2013-02-09T20:39:26.234Z",
+#       "@version": "1",
+#       message: "hello world"
+#     }
+module LogStash::EventV1
+  class DeprecatedMethod < StandardError; end
+
+  public
+  def initialize(data={})
+    @cancelled = false
+
+    @data = data
+    @data["@timestamp"] = LogStash::Time.now if !@data.include?("@timestamp")
+    @data["@version"] = "1" if !@data.include?("@version")
+  end # def initialize
+
+  public
+  def self.from_json(json)
+    return self.new(JSON.parse(json))
+  end # def self.from_json
+
+  public
+  def cancel
+    @cancelled = true
+  end # def cancel
+
+  public
+  def uncancel
+    @cancelled = false
+  end # def uncancel
+
+  public
+  def cancelled?
+    return @cancelled
+  end # def cancelled?
+
+  # Create a deep-ish copy of this event.
+  public
+  def clone
+    return self.class.new(@data.clone)
+  end # def clone
+
+  public
+  def to_s
+    return self.sprintf("%{@timestamp} %{source_host} %{message}")
+  end # def to_s
+
+  public
+  def timestamp; return @data["@timestamp"]; end # def timestamp
+  def timestamp=(val); return @data["@timestamp"] = val; end # def timestamp=
+
+  def unix_timestamp
+    raise DeprecatedMethod
+  end # def unix_timestamp
+
+  def ruby_timestamp
+    raise DeprecatedMethod
+  end # def unix_timestamp
+  
+  # field-related access
+  public
+  def [](key)
+    # TODO(sissel): Implement
+  end # def []
+  
+  public
+  def []=(key, value)
+    # TODO(sissel): Implement
+  end # def []=
+
+  public
+  def fields
+    raise DeprecatedMethod
+  end
+  
+  public
+  def to_json(*args)
+    return @data.to_json(*args) 
+  end # def to_json
+
+  def to_hash
+    raise DeprecatedMethod
+  end # def to_hash
+
+  public
+  def overwrite(event)
+    @data = event.to_hash
+  end
+
+  public
+  def include?(key)
+    return !self[key].nil?
+  end # def include?
+
+  # Append an event to this one.
+  public
+  def append(event)
+    raise NotImplementedError, "LogStash::EventV1#append needs implementing"
+  end
+
+  # Remove a field. Returns the value of that field when deleted
+  public
+  def remove(field)
+    return @data.delete(field)
+  end # def remove
+
+  # sprintf. This could use a better method name.
+  # The idea is to take an event and convert it to a string based on 
+  # any format values, delimited by %{foo} where 'foo' is a field or
+  # metadata member.
+  #
+  # For example, if the event has @type == "foo" and @source == "bar"
+  # then this string:
+  #   "type is %{@type} and source is %{@source}"
+  # will return
+  #   "type is foo and source is bar"
+  #
+  # If a %{name} value is an array, then we will join by ','
+  # If a %{name} value does not exist, then no substitution occurs.
+  #
+  # TODO(sissel): It is not clear what the value of a field that 
+  # is an array (or hash?) should be. Join by comma? Something else?
+  public
+  def sprintf(format)
+    if format.index("%").nil?
+      return format
+    end
+
+    return format.gsub(/%\{[^}]+\}/) do |tok|
+      # Take the inside of the %{ ... }
+      key = tok[2 ... -1]
+
+      if key == "+%s"
+        # Got %{+%s}, support for unix epoch time
+        if RUBY_ENGINE != "jruby"
+          # This is really slow. See LOGSTASH-217
+          Time.parse(self.timestamp).to_i
+        else
+          datetime = @@date_parser.parseDateTime(self.timestamp)
+          (datetime.getMillis / 1000).to_i
+        end
+      elsif key[0,1] == "+"
+        # We got a %{+TIMEFORMAT} so use joda to format it.
+        if RUBY_ENGINE != "jruby"
+          # This is really slow. See LOGSTASH-217
+          datetime = Date.parse(self.timestamp)
+          format = key[1 .. -1]
+          datetime.strftime(format)
+        else
+          datetime = @@date_parser.parseDateTime(self.timestamp)
+          format = key[1 .. -1]
+          datetime.toString(format) # return requested time format
+        end
+      else
+        # Use an event field.
+        value = self[key]
+
+        case value
+        when nil
+          tok # leave the %{foo} if this field does not exist in this event.
+        when Array
+          value.join(",") # Join by ',' if value is an array
+        when Hash
+          value.to_json # Convert hashes to json
+        else
+          value # otherwise return the value
+        end
+      end
+    end
+  end # def sprintf
+end # module LogStash::EventV1
diff --git a/lib/logstash/filters/useragent.rb b/lib/logstash/filters/useragent.rb
new file mode 100644
index 00000000000..f58afd64540
--- /dev/null
+++ b/lib/logstash/filters/useragent.rb
@@ -0,0 +1,87 @@
+require "logstash/filters/base"
+require "logstash/namespace"
+require "tempfile"
+
+# Parse user agent strings into structured data based on BrowserScope data
+#
+# UserAgent filter, adds information about user agent like family, operating
+# system, version, and device
+#
+# Logstash releases ship with the regexes.yaml database made available from
+# ua-parser with an Apache 2.0 license. For more details on ua-parser, see
+# <https://github.com/tobie/ua-parser/>.
+class LogStash::Filters::UserAgent < LogStash::Filters::Base
+  config_name "useragent"
+  plugin_status "experimental"
+
+  # The field containing the user agent string. If this field is an
+  # array, only the first value will be used.
+  config :source, :validate => :string, :required => true
+
+  # The name of the field to assign the UA data hash to
+  config :target, :validate => :string, :default => "ua"
+
+  # regexes.yaml file to use
+  #
+  # If not specified, this will default to the regexes.yaml that ships
+  # with logstash.
+  config :regexes, :validate => :string
+
+  public
+  def register
+    require 'user_agent_parser'
+    if @regexes.nil?
+      begin
+        @parser = UserAgentParser::Parser.new()
+      rescue Exception => e
+        begin
+          # Running from a flatjar which has a different layout
+          jar_path = [__FILE__.split("!").first, "/vendor/ua-parser/regexes.yaml"].join("!")
+          tmp_file = Tempfile.new('logstash-uaparser-regexes')
+          tmp_file.write(File.read(jar_path))
+          tmp_file.close # this file is reaped when ruby exits
+          @parser = UserAgentParser::Parser.new(tmp_file.path)
+        rescue => ex
+          raise "Failed to cache, due to: #{ex}\n#{ex.backtrace}"
+        end
+      end
+    else
+      @logger.info("Using user agent regexes", :regexes => @regexes)
+      @parser = UserAgentParser::Parser.new(@regexes)
+    end
+  end #def register
+
+  public
+  def filter(event)
+    return unless filter?(event)
+    ua_data = nil
+
+    useragent = event[@source]
+    useragent = useragent.first if useragent.is_a? Array
+
+    begin
+      ua_data = @parser.parse(useragent)
+    rescue Exception => e
+      @logger.error("Uknown error while parsing user agent data", :exception => e, :field => @source, :event => event)
+    end
+
+    if !ua_data.nil?
+        event[@target] = {} if event[@target].nil?
+
+        event[@target]["name"] = ua_data.name
+        event[@target]["os"] = ua_data.os if not ua_data.os.nil?
+        event[@target]["device"] = ua_data.device if not ua_data.device.nil?
+
+        if not ua_data.version.nil?
+          ua_version = ua_data.version
+
+          event[@target]["major"] = ua_version.major
+          event[@target]["minor"] = ua_version.minor
+        end
+
+      filter_matched(event)
+    end
+
+  end # def filter
+end # class LogStash::Filters::UserAgent
+
diff --git a/lib/logstash/inputs/amqp.rb b/lib/logstash/inputs/amqp.rb
index 56f4066f104..07814852b74 100644
--- a/lib/logstash/inputs/amqp.rb
+++ b/lib/logstash/inputs/amqp.rb
@@ -1,161 +1,11 @@
-require "logstash/inputs/threadable"
-require "logstash/namespace"
-require "cgi" # for CGI.escape
-
-# Pull events from an AMQP exchange.
-#
-# <b> NOTE: THIS IS ONLY KNOWN TO WORK WITH RECENT RELEASES OF RABBITMQ. Any
-# other amqp broker will not work with this plugin. I do not know why. If you
-# need support for brokers other than rabbitmq, please file bugs here:
-# <https://github.com/ruby-amqp/bunny> </b>
-#
-# The default settings will create an entirely transient queue and listen for all messages by default.
-# If you need durability or any other advanced settings, please set the appropriate options
-class LogStash::Inputs::Amqp < LogStash::Inputs::Threadable
+require "logstash/inputs/rabbitmq"
 
+class LogStash::Inputs::AMQP < LogStash::Inputs::RabbitMQ
   config_name "amqp"
-  plugin_status "unsupported"
-
-  # Your amqp broker's custom arguments. For mirrored queues in RabbitMQ: [ "x-ha-policy", "all" ]
-  config :arguments, :validate => :array, :default => []
-
-  # Your amqp server address
-  config :host, :validate => :string, :required => true
-
-  # The AMQP port to connect on
-  config :port, :validate => :number, :default => 5672
-
-  # Your amqp username
-  config :user, :validate => :string, :default => "guest"
-
-  # Your amqp password
-  config :password, :validate => :password, :default => "guest"
-
-  # The name of the queue. Depricated due to conflicts with puppet naming convention.
-  # Replaced by 'queue' variable. See LOGSTASH-755
-  config :name, :validate => :string, :deprecated => true
-
-  # The name of the queue.
-  config :queue, :validate => :string, :default => ""
-
-  # The name of the exchange to bind the queue. This is analogous to the 'amqp
-  # output' [config 'name'](../outputs/amqp)
-  config :exchange, :validate => :string, :required => true
-
-  # The routing key to use. This is only valid for direct or fanout exchanges
-  #
-  # * Routing keys are ignored on topic exchanges.
-  # * Wildcards are not valid on direct exchanges.
-  config :key, :validate => :string, :default => "logstash"
-
-  # The vhost to use. If you don't know what this is, leave the default.
-  config :vhost, :validate => :string, :default => "/"
-
-  # Passive queue creation? Useful for checking queue existance without modifying server state
-  config :passive, :validate => :boolean, :default => false
-
-  # Is this queue durable? (aka; Should it survive a broker restart?)
-  config :durable, :validate => :boolean, :default => false
-
-  # Should the queue be deleted on the broker when the last consumer
-  # disconnects? Set this option to 'false' if you want the queue to remain
-  # on the broker, queueing up messages until a consumer comes along to
-  # consume them.
-  config :auto_delete, :validate => :boolean, :default => true
-
-  # Is the queue exclusive? (aka: Will other clients connect to this named queue?)
-  config :exclusive, :validate => :boolean, :default => true
-
-  # Prefetch count. Number of messages to prefetch
-  config :prefetch_count, :validate => :number, :default => 1
-
-  # Enable message acknowledgement
-  config :ack, :validate => :boolean, :default => true
-
-  # Enable or disable debugging
-  config :debug, :validate => :boolean, :default => false
-
-  # Enable or disable SSL
-  config :ssl, :validate => :boolean, :default => false
-
-  # Validate SSL certificate
-  config :verify_ssl, :validate => :boolean, :default => false
-
-  public
-  def initialize(params)
-    super
-
-    @format ||= "json_event"
-
-  end # def initialize
-
-  public
+  plugin_status "beta"
   def register
-
-    if @name
-      if @queue
-        @logger.error("'name' and 'queue' are the same setting, but 'name' is deprecated. Please use only 'queue'")
-      end
-      @queue = @name
-    end   
-
-    @logger.info("Registering input #{@url}")
-    require "bunny" # rubygem 'bunny'
-    @vhost ||= "/"
-    @port ||= 5672
-    @key ||= "#"
-    @amqpsettings = {
-      :vhost => @vhost,
-      :host => @host,
-      :port => @port,
-    }
-    @amqpsettings[:user] = @user if @user
-    @amqpsettings[:pass] = @password.value if @password
-    @amqpsettings[:logging] = @debug
-    @amqpsettings[:ssl] = @ssl if @ssl
-    @amqpsettings[:verify_ssl] = @verify_ssl if @verify_ssl
-    @amqpurl = "amqp://"
-    if @user
-      @amqpurl << @user if @user
-      @amqpurl << ":#{CGI.escape(@password.to_s)}" if @password
-      @amqpurl << "@"
-    end
-    @amqpurl += "#{@host}:#{@port}#{@vhost}/#{@queue}"
+    @logger.warn("The 'amqp' input plugin has been renamed to 'rabbitmq'. " \
+                 "Please update your configuration appropriately.")
+    super
   end # def register
-
-  def run(queue)
-    begin
-      @logger.debug("Connecting with AMQP settings #{@amqpsettings.inspect} to set up queue #{@queue.inspect}")
-      @bunny = Bunny.new(@amqpsettings)
-      return if terminating?
-      @bunny.start
-      @bunny.qos({:prefetch_count => @prefetch_count})
-
-      @arguments_hash = Hash[*@arguments]
-
-      @bunnyqueue = @bunny.queue(@queue, {:durable => @durable, :auto_delete => @auto_delete, :exclusive => @exclusive, :arguments => @arguments_hash })
-      @bunnyqueue.bind(@exchange, :key => @key)
-
-      @bunnyqueue.subscribe({:ack => @ack}) do |data|
-        e = to_event(data[:payload], @amqpurl)
-        if e
-          queue << e
-        end
-      end # @bunnyqueue.subscribe
-
-    rescue *[Bunny::ConnectionError, Bunny::ServerDownError] => e
-      @logger.error("AMQP connection error, will reconnect: #{e}")
-      # Sleep for a bit before retrying.
-      # TODO(sissel): Write 'backoff' method?
-      sleep(1)
-      retry
-    end # begin/rescue
-  end # def run
-
-  def teardown
-    @bunnyqueue.unsubscribe unless @durable == true
-    @bunnyqueue.delete unless @durable == true
-    @bunny.close if @bunny
-    finished
-  end # def teardown
-end # class LogStash::Inputs::Amqp
+end # class LogStash::Inputs::AMQP
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index a6e5a7393e1..cb0b894da0e 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -12,12 +12,17 @@ class LogStash::Inputs::Base < LogStash::Plugin
   # Label this input with a type.
   # Types are used mainly for filter activation.
   #
-  #
   # If you create an input with type "foobar", then only filters
   # which also have type "foobar" will act on them.
   #
   # The type is also stored as part of the event itself, so you
   # can also use the type to search for in the web interface.
+  #
+  # If you try to set a type on an event that already has one (for
+  # example when you send an event from a shipper to an indexer) then
+  # a new input will not override the existing type. A type set at 
+  # the shipper stays with that event for its life even
+  # when sent to another LogStash server.
   config :type, :validate => :string, :required => true
 
   # Set this to true to enable debugging on an input.
@@ -101,8 +106,8 @@ def to_event(raw, source)
           event.message = raw
         end
       rescue => e
-        ## TODO(sissel): Instead of dropping the event, should we treat it as
-        ## plain text and try to do the best we can with it?
+        # Instead of dropping the event, should we treat it as
+        # plain text and try to do the best we can with it?
         @logger.warn("Trouble parsing json input, falling back to plain text",
                      :input => raw, :source => source, :exception => e)
         event.message = raw
@@ -118,8 +123,8 @@ def to_event(raw, source)
           event.message ||= event.sprintf(@message_format)
         end
       rescue => e
-        ## TODO(sissel): Instead of dropping the event, should we treat it as
-        ## plain text and try to do the best we can with it?
+        # Instead of dropping the event, should we treat it as
+        # plain text and try to do the best we can with it?
         @logger.warn("Trouble parsing json input, falling back to plain text",
                      :input => raw, :source => source, :exception => e)
         event.message = raw
diff --git a/lib/logstash/inputs/gemfire.rb b/lib/logstash/inputs/gemfire.rb
index 2b38c404008..36b0c037bbc 100644
--- a/lib/logstash/inputs/gemfire.rb
+++ b/lib/logstash/inputs/gemfire.rb
@@ -234,4 +234,4 @@ def afterRegionDestroy(event)
   def afterRegionInvalidate(event)
     @logger.debug("afterRegionInvalidate #{event}")
   end
-end # class LogStash::Inputs::Amqp
+end # class LogStash::Inputs::Gemfire
diff --git a/lib/logstash/inputs/log4j.rb b/lib/logstash/inputs/log4j.rb
index 250c2dfebc5..39ce7630613 100644
--- a/lib/logstash/inputs/log4j.rb
+++ b/lib/logstash/inputs/log4j.rb
@@ -55,30 +55,25 @@ def handle_socket(socket, output_queue, event_source)
       loop do
         # NOTE: event_raw is org.apache.log4j.spi.LoggingEvent
         event_obj = ois.readObject()
-        event_data = {
-          "@type" => type,
-          "@tags" => tags,
-          "@source" => event_source,
-          "@source_host" => socket.peer,
-          "@source_path" => event_obj.getLoggerName(),
-          "@fields" => { "priority" => event_obj.getLevel().toString(), "logger_name" => event_obj.getLoggerName(), 
-                         "thread" => event_obj.getThreadName(), "class" => event_obj.getLocationInformation().getClassName(),
-                         "file" => event_obj.getLocationInformation().getFileName() + ":" + event_obj.getLocationInformation().getLineNumber(),
-                         "method" => event_obj.getLocationInformation().getMethodName()
-          },
-          "@message" => event_obj.getRenderedMessage() 
-        }
-        event_data["@fields"]["NDC"] = event_obj.getNDC() if event_obj.getNDC()
-        event_data["@fields"]["stack_trace"] = event_obj.getThrowableStrRep().to_a.join("\n") if event_obj.getThrowableInformation()
+        e = to_event(event_obj.getRenderedMessage(), event_source)
+        e.source_host = socket.peer
+        e.source_path = event_obj.getLoggerName()
+        e["priority"] = event_obj.getLevel().toString()
+        e["logger_name"] = event_obj.getLoggerName()
+        e["thread"] = event_obj.getThreadName()
+        e["class"] = event_obj.getLocationInformation().getClassName()
+        e["file"] = event_obj.getLocationInformation().getFileName() + ":" + event_obj.getLocationInformation().getLineNumber(),
+        e["method"] = event_obj.getLocationInformation().getMethodName()
+        e["NDC"] = event_obj.getNDC() if event_obj.getNDC()
+        e["stack_trace"] = event_obj.getThrowableStrRep().to_a.join("\n") if event_obj.getThrowableInformation()
         
         # Add the MDC context properties to '@fields'
         if event_obj.getProperties()
           event_obj.getPropertyKeySet().each do |key|
-            event_data["@fields"][key] = event_obj.getProperty(key)
+            e[key] = event_obj.getProperty(key)
           end  
         end  
 
-        e = ::LogStash::Event.new event_data
         if e
           output_queue << e
         end
diff --git a/lib/logstash/inputs/rabbitmq.rb b/lib/logstash/inputs/rabbitmq.rb
new file mode 100644
index 00000000000..ade38c47424
--- /dev/null
+++ b/lib/logstash/inputs/rabbitmq.rb
@@ -0,0 +1,160 @@
+require "logstash/inputs/threadable"
+require "logstash/namespace"
+require "cgi" # for CGI.escape
+
+# Pull events from a RabbitMQ exchange.
+#
+# The default settings will create an entirely transient queue and listen for all messages by default.
+# If you need durability or any other advanced settings, please set the appropriate options
+#
+# This has been tested with Bunny 0.9.x, which supports RabbitMQ 2.x and 3.x. You can
+# find links to both here:
+#
+# * RabbitMQ - <http://www.rabbitmq.com/>
+# * Bunny - <https://github.com/ruby-amqp/bunny>
+class LogStash::Inputs::RabbitMQ < LogStash::Inputs::Threadable
+
+  config_name "rabbitmq"
+  plugin_status "beta"
+
+  # Custom arguments. For example, mirrored queues in RabbitMQ 2.x:  [ "x-ha-policy", "all" ]
+  # RabbitMQ 3.x mirrored queues are set by policy. More information can be found
+  # here: http://www.rabbitmq.com/blog/2012/11/19/breaking-things-with-rabbitmq-3-0/
+  config :arguments, :validate => :array, :default => []
+
+  # Your rabbitmq server address
+  config :host, :validate => :string, :required => true
+
+  # The rabbitmq port to connect on
+  config :port, :validate => :number, :default => 5672
+
+  # Your rabbitmq username
+  config :user, :validate => :string, :default => "guest"
+
+  # Your rabbitmq password
+  config :password, :validate => :password, :default => "guest"
+
+  # The name of the queue.
+  config :queue, :validate => :string, :default => ""
+
+  # The name of the exchange to bind the queue.
+  config :exchange, :validate => :string, :required => true
+
+  # The routing key to use. This is only valid for direct or fanout exchanges
+  #
+  # * Routing keys are ignored on topic exchanges.
+  # * Wildcards are not valid on direct exchanges.
+  config :key, :validate => :string, :default => "logstash"
+
+  # The vhost to use. If you don't know what this is, leave the default.
+  config :vhost, :validate => :string, :default => "/"
+
+  # Passive queue creation? Useful for checking queue existance without modifying server state
+  config :passive, :validate => :boolean, :default => false
+
+  # Is this queue durable? (aka; Should it survive a broker restart?)
+  config :durable, :validate => :boolean, :default => false
+
+  # Should the queue be deleted on the broker when the last consumer
+  # disconnects? Set this option to 'false' if you want the queue to remain
+  # on the broker, queueing up messages until a consumer comes along to
+  # consume them.
+  config :auto_delete, :validate => :boolean, :default => true
+
+  # Is the queue exclusive? (aka: Will other clients connect to this named queue?)
+  config :exclusive, :validate => :boolean, :default => true
+
+  # Prefetch count. Number of messages to prefetch
+  config :prefetch_count, :validate => :number, :default => 1
+
+  # Enable message acknowledgement
+  config :ack, :validate => :boolean, :default => true
+
+  # Enable or disable debugging
+  config :debug, :validate => :boolean, :default => false
+
+  # Enable or disable SSL
+  config :ssl, :validate => :boolean, :default => false
+
+  # Validate SSL certificate
+  config :verify_ssl, :validate => :boolean, :default => false
+  
+  # Maximum permissible size of a frame (in bytes) to negotiate with clients
+  config :frame_max, :validate => :number, :default => 131072
+
+  public
+  def initialize(params)
+    super
+
+    @format ||= "json_event"
+
+  end # def initialize
+
+  public
+  def register   
+
+    @logger.info("Registering input #{@url}")
+    require "bunny"
+    
+    @vhost ||= "/"
+    @port ||= 5672
+    @key ||= "#"
+    
+    @rabbitmq_settings = {
+      :vhost => @vhost,
+      :host => @host,
+      :port => @port,
+    }
+    
+    @rabbitmq_settings[:user] = @user if @user
+    @rabbitmq_settings[:pass] = @password.value if @password
+    @rabbitmq_settings[:logging] = @debug
+    @rabbitmq_settings[:ssl] = @ssl if @ssl
+    @rabbitmq_settings[:verify_ssl] = @verify_ssl if @verify_ssl
+    @rabbitmq_settings[:frame_max] = @frame_max if @frame_max
+    
+    @rabbitmq_url = "amqp://"
+    if @user
+      @rabbitmq_url << @user if @user
+      @rabbitmq_url << ":#{CGI.escape(@password.to_s)}" if @password
+      @rabbitmq_url << "@"
+    end
+    @rabbitmq_url += "#{@host}:#{@port}#{@vhost}/#{@queue}"
+  end # def register
+
+  def run(queue)
+    begin
+      @logger.debug("Connecting with RabbitMQ settings #{@rabbitmq_settings.inspect} to set up queue #{@queue.inspect}")
+      @bunny = Bunny.new(@rabbitmq_settings)
+      return if terminating?
+      @bunny.start
+      @bunny.qos({:prefetch_count => @prefetch_count})
+
+      @arguments_hash = Hash[*@arguments]
+
+      @bunnyqueue = @bunny.queue(@queue, {:durable => @durable, :auto_delete => @auto_delete, :exclusive => @exclusive, :arguments => @arguments_hash })
+      @bunnyqueue.bind(@exchange, :key => @key)
+
+      @bunnyqueue.subscribe({:ack => @ack}) do |data|
+        e = to_event(data[:payload], @rabbitmq_url)
+        if e
+          queue << e
+        end
+      end # @bunnyqueue.subscribe
+
+    rescue *[Bunny::ConnectionError, Bunny::ServerDownError] => e
+      @logger.error("RabbitMQ connection error, will reconnect: #{e}")
+      # Sleep for a bit before retrying.
+      # TODO(sissel): Write 'backoff' method?
+      sleep(1)
+      retry
+    end # begin/rescue
+  end # def run
+
+  def teardown
+    @bunnyqueue.unsubscribe unless @durable == true
+    @bunnyqueue.delete unless @durable == true
+    @bunny.close if @bunny
+    finished
+  end # def teardown
+end # class LogStash::Inputs::RabbitMQ
diff --git a/lib/logstash/inputs/redis.rb b/lib/logstash/inputs/redis.rb
index c92c7891c8e..44fe1de6799 100644
--- a/lib/logstash/inputs/redis.rb
+++ b/lib/logstash/inputs/redis.rb
@@ -44,6 +44,9 @@ class LogStash::Inputs::Redis < LogStash::Inputs::Threadable
   # TODO: change required to true
   config :data_type, :validate => [ "list", "channel", "pattern_channel" ], :required => false
 
+  # How many events to return from redis using EVAL
+  config :batch_count, :validate => :number, :default => 1
+
   public
   def initialize(params)
     super
@@ -88,15 +91,39 @@ def identity
 
   private
   def connect
-    Redis.new(
+    redis = Redis.new(
       :host => @host,
       :port => @port,
       :timeout => @timeout,
       :db => @db,
       :password => @password.nil? ? nil : @password.value
     )
+    load_batch_script(redis) if @data_type == 'list' && (@batch_count > 1)
+    return redis
   end # def connect
 
+  private
+  def load_batch_script(redis)
+    #A redis lua EVAL script to fetch a count of keys
+    #in case count is bigger than current items in queue whole queue will be returned without extra nil values
+    redis_script = <<EOF
+          local i = tonumber(ARGV[1])
+          local res = {}
+          local length = redis.call('llen',KEYS[1])
+          if length < i then i = length end
+          while (i > 0) do
+            local item = redis.call("lpop", KEYS[1])
+            if (not item) then
+              break
+            end
+            table.insert(res, item)
+            i = i-1
+          end
+          return res
+EOF
+    @redis_script_sha = redis.script(:load, redis_script)
+  end
+
   private
   def queue_event(msg, output_queue)
     begin
@@ -110,8 +137,47 @@ def queue_event(msg, output_queue)
 
   private
   def list_listener(redis, output_queue)
-    response = redis.blpop @key, 0
-    queue_event response[1], output_queue
+
+    # blpop returns the 'key' read from as well as the item result
+    # we only care about the result (2nd item in the list).
+    item = redis.blpop(@key, 0)[1]
+
+    # blpop failed or .. something?
+    # TODO(sissel): handle the error
+    return if item.nil?
+    queue_event(item, output_queue)
+
+    # If @batch_count is 1, there's no need to continue.
+    return if @batch_count == 1
+
+    begin
+      redis.evalsha(@redis_script_sha, [@key], [@batch_count-1]).each do |item|
+        queue_event(item, output_queue)
+      end
+
+      # Below is a commented-out implementation of 'batch fetch'
+      # using pipelined LPOP calls. This in practice has been observed to
+      # perform exactly the same in terms of event throughput as
+      # the evalsha method. Given that the EVALSHA implementation uses
+      # one call to redis instead of N (where N == @batch_count) calls,
+      # I decided to go with the 'evalsha' method of fetching N items
+      # from redis in bulk.
+      #redis.pipelined do
+        #error, item = redis.lpop(@key)
+        #(@batch_count-1).times { redis.lpop(@key) }
+      #end.each do |item|
+        #queue_event(item, output_queue) if item
+      #end
+      # --- End commented out implementation of 'batch fetch'
+    rescue Redis::CommandError => e
+      if e.to_s =~ /NOSCRIPT/ then
+        @logger.warn("Redis may have been restarted, reloading redis batch EVAL script", :exception => e);
+        load_batch_script(redis)
+        retry
+      else
+        raise e
+      end
+    end
   end
 
   private
diff --git a/lib/logstash/inputs/snmptrap.rb b/lib/logstash/inputs/snmptrap.rb
new file mode 100644
index 00000000000..07e2a1de928
--- /dev/null
+++ b/lib/logstash/inputs/snmptrap.rb
@@ -0,0 +1,70 @@
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "snmp"
+
+# Read snmp trap messages as events
+#
+# Resulting @message looks like :
+#   #<SNMP::SNMPv1_Trap:0x6f1a7a4 @varbind_list=[#<SNMP::VarBind:0x2d7bcd8f @value="teststring", 
+#   @name=[1.11.12.13.14.15]>], @timestamp=#<SNMP::TimeTicks:0x1af47e9d @value=55>, @generic_trap=6, 
+#   @enterprise=[1.2.3.4.5.6], @source_ip="127.0.0.1", @agent_addr=#<SNMP::IpAddress:0x29a4833e @value="\xC0\xC1\xC2\xC3">, 
+#   @specific_trap=99>
+#
+# TODO : work out how to break it down into field.keys.   looks like varbind_list can have multiple entries which might 
+#        mean multiple events per trap ?
+
+class LogStash::Inputs::Snmptrap < LogStash::Inputs::Base
+  config_name "snmptrap"
+  plugin_status "experimental"
+
+  # The address to listen on
+  config :host, :validate => :string, :default => "0.0.0.0"
+
+  # The port to listen on. Remember that ports less than 1024 (privileged
+  # ports) may require root to use. hence the default of 1062.
+  config :port, :validate => :number, :default => 1062
+
+  # SNMP Community String to listen for.
+  config :community, :validate => :string, :default => "public"
+
+
+  def initialize(*args)
+    super(*args)
+  end # def initialize
+
+  public
+  def register
+    @snmptrap = nil
+  end # def register
+
+  public
+  def run(output_queue)
+    LogStash::Util::set_thread_name("input|snmptrap|#{@community}")
+    begin
+      # snmp trap server
+      snmptrap_listener(output_queue)
+    rescue => e
+      @logger.warn("SNMP Trap listener died", :exception => e, :backtrace => e.backtrace)
+      sleep(5)
+      retry
+    end # begin
+  end # def run
+
+  private
+  def snmptrap_listener(output_queue)
+    @logger.info("It's a Trap!", :host => @host, :port => @port, :community => @community)
+    @snmptrap = SNMP::TrapListener.new(:Port => @port, :Community => @community, :Host => @host) 
+    loop do
+      @snmptrap.on_trap_default do |trap|
+        begin
+          event = to_event(trap.inspect, trap.source_ip)
+          @logger.info("SNMP Trap received: ", :trap_object => trap.inspect)
+          output_queue << event if event
+        rescue => event
+          @logger.error("Failed to create event", :trap_object => trap.inspect)
+        end
+      end
+    end
+  end # def snmptrap_listener
+
+end # class LogStash::Inputs::Snmptrap
diff --git a/lib/logstash/inputs/stdin.rb b/lib/logstash/inputs/stdin.rb
index 2953d8e615b..dc35db6ac3f 100644
--- a/lib/logstash/inputs/stdin.rb
+++ b/lib/logstash/inputs/stdin.rb
@@ -18,16 +18,16 @@ def register
   end # def register
 
   def run(queue)
-    loop do
-       begin
-         e = to_event($stdin.readline.chomp, "stdin://#{@host}/")
-       rescue EOFError => ex
-         break
-       end
-      if e
-        queue << e
+    while true
+      begin
+        e = to_event($stdin.readline.chomp, "stdin://#{@host}/")
+      rescue EOFError => ex
+        # stdin closed, finish
+        break
       end
-    end # loop
+      queue << e if e
+    end # while true
+    finished
   end # def run
 
   public
diff --git a/lib/logstash/inputs/zenoss.rb b/lib/logstash/inputs/zenoss.rb
index 5d37e5b8ec6..1f35ab18dbe 100644
--- a/lib/logstash/inputs/zenoss.rb
+++ b/lib/logstash/inputs/zenoss.rb
@@ -1,25 +1,25 @@
 require "date"
-require "logstash/inputs/amqp"
+require "logstash/inputs/rabbitmq"
 require "zlib"
 
 # Read Zenoss events from the zenoss.zenevents fanout exchange.
 #
-class LogStash::Inputs::Zenoss < LogStash::Inputs::Amqp
+class LogStash::Inputs::Zenoss < LogStash::Inputs::RabitMQ
 
   config_name "zenoss"
   plugin_status "experimental"
 
-  # Your amqp server address
+  # Your rabbitmq server address
   config :host, :validate => :string, :default => "localhost"
 
-  # Your amqp username
+  # Your rabbitmq username
   config :user, :validate => :string, :default => "zenoss"
 
-  # Your amqp password
+  # Your rabbitmq password
   config :password, :validate => :password, :default => "zenoss"
 
-  # The name of the exchange to bind the queue. This is analogous to the 'amqp
-  # output' [config 'name'](../outputs/amqp)
+  # The name of the exchange to bind the queue. This is analogous to the 'rabbitmq
+  # output' [config 'name'](../outputs/rabbitmq)
   config :exchange, :validate => :string, :default => "zenoss.zenevents"
 
   # The routing key to use. This is only valid for direct or fanout exchanges
@@ -41,8 +41,8 @@ def run(queue)
     begin
       zep = Org::Zenoss::Protobufs::Zep
 
-      @logger.debug("Connecting with AMQP settings #{@amqpsettings.inspect}")
-      @bunny = Bunny.new(@amqpsettings)
+      @logger.debug("Connecting with RabbitMQ settings #{@rabbitmq_settings.inspect}")
+      @bunny = Bunny.new(@rabbitmq_settings)
       return if terminating?
       @bunny.start
       @bunny.qos({:prefetch_count => @prefetch_count})
@@ -77,7 +77,7 @@ def run(queue)
 
         # LogStash event properties.
         event = LogStash::Event.new({
-          "@source" => @amqpurl,
+          "@source" => @rabbitmq_url,
           "@type" => @type,
           "@timestamp" => timestamp,
           "@source_host" => occurrence.actor.element_title,
@@ -130,7 +130,7 @@ def run(queue)
       end # @queue.subscribe
 
     rescue *[Bunny::ConnectionError, Bunny::ServerDownError] => e
-      @logger.error("AMQP connection error, will reconnect: #{e}")
+      @logger.error("RabbitMQ connection error, will reconnect: #{e}")
       # Sleep for a bit before retrying.
       # TODO(sissel): Write 'backoff' method?
       sleep(1)
diff --git a/lib/logstash/logging.rb b/lib/logstash/logging.rb
index 7ed79da14e7..eb92d5af162 100644
--- a/lib/logstash/logging.rb
+++ b/lib/logstash/logging.rb
@@ -2,28 +2,42 @@
 require "cabin"
 require "logger"
 
-class LogStash::Logger < Cabin::Channel
+class LogStash::Logger 
   attr_accessor :target
 
   public
   def initialize(*args)
     super()
 
+    #self[:program] = File.basename($0)
+    #subscribe(::Logger.new(*args))
+    @target = args[0]
+    @channel = Cabin::Channel.get(LogStash)
+    @channel.subscribe(@target)
+ 
     # Set default loglevel to WARN unless $DEBUG is set (run with 'ruby -d')
     @level = $DEBUG ? :debug : :warn
     if ENV["LOGSTASH_DEBUG"]
-      self.level = :debug
+      @level = :debug
     end
 
-    #self[:program] = File.basename($0)
-    #subscribe(::Logger.new(*args))
-    @target = args[0]
-    subscribe(@target)
-
     # Direct metrics elsewhere.
-    metrics.channel = Cabin::Channel.new
+    @channel.metrics.channel = Cabin::Channel.new
   end # def initialize
 
+  # Delegation
+  def level=(value) @channel.level = value; end
+  def debug(*args); @channel.debug(*args); end
+  def debug?(*args); @channel.debug?(*args); end
+  def info(*args); @channel.info(*args); end
+  def info?(*args); @channel.info?(*args); end
+  def warn(*args); @channel.warn(*args); end
+  def warn?(*args); @channel.warn?(*args); end
+  def error(*args); @channel.error(*args); end
+  def error?(*args); @channel.error?(*args); end
+  def fatal(*args); @channel.fatal(*args); end
+  def fatal?(*args); @channel.fatal?(*args); end
+
   def setup_log4j(logger="")
     require "java"
 
diff --git a/lib/logstash/multiqueue.rb b/lib/logstash/multiqueue.rb
index c04522a4461..ec686f13bfa 100644
--- a/lib/logstash/multiqueue.rb
+++ b/lib/logstash/multiqueue.rb
@@ -1,12 +1,12 @@
 require "logstash/namespace"
-require "logstash/logging"
+require "cabin"
 
 class LogStash::MultiQueue
   attr_accessor :logger
 
   public
   def initialize(*queues)
-    @logger = LogStash::Logger.new(STDOUT)
+    @logger = Cabin::Channel.get(LogStash)
     @mutex = Mutex.new
     @queues = queues
   end # def initialize
diff --git a/lib/logstash/outputs/amqp.rb b/lib/logstash/outputs/amqp.rb
index 676d62ea81b..2ecb05612c8 100644
--- a/lib/logstash/outputs/amqp.rb
+++ b/lib/logstash/outputs/amqp.rb
@@ -1,157 +1,11 @@
-require "logstash/outputs/base"
-require "logstash/namespace"
-
-# Push events to an AMQP exchange.
-#
-# <b> NOTE: THIS IS ONLY KNOWN TO WORK WITH RECENT RELEASES OF RABBITMQ. Any
-# other amqp broker will not work with this plugin. I do not know why. If you
-# need support for brokers other than rabbitmq, please file bugs here:
-# <https://github.com/ruby-amqp/bunny> </b>
-class LogStash::Outputs::Amqp < LogStash::Outputs::Base
-  MQTYPES = [ "fanout", "direct", "topic" ]
+require "logstash/outputs/rabbitmq"
 
+class LogStash::Outputs::AMQP < LogStash::Outputs::RabbitMQ
   config_name "amqp"
-  plugin_status "unsupported"
-
-  # Your amqp server address
-  config :host, :validate => :string, :required => true
-
-  # The AMQP port to connect on
-  config :port, :validate => :number, :default => 5672
-
-  # Your amqp username
-  config :user, :validate => :string, :default => "guest"
-
-  # Your amqp password
-  config :password, :validate => :password, :default => "guest"
-
-  # The exchange type (fanout, topic, direct)
-  config :exchange_type, :validate => [ "fanout", "direct", "topic"], :required => true
-
-  # The name of the exchange. Depricated due to conflicts with puppet naming convention.
-  # Replaced by 'exchange' variable. See LOGSTASH-755
-  config :name, :validate => :string, :deprecated => true
-
-  # The name of the exchange
-  config :exchange, :validate => :string # TODO(sissel): Make it required when 'name' is gone
-
-  # Key to route to by default. Defaults to 'logstash'
-  #
-  # * Routing keys are ignored on fanout exchanges.
-  config :key, :validate => :string, :default => "logstash"
-
-  # The vhost to use
-  config :vhost, :validate => :string, :default => "/"
-
-  # Is this exchange durable? (aka; Should it survive a broker restart?)
-  config :durable, :validate => :boolean, :default => true
-
-  # Should messages persist to disk on the AMQP broker until they are read by a
-  # consumer?
-  config :persistent, :validate => :boolean, :default => true
-
-  # Enable or disable debugging
-  config :debug, :validate => :boolean, :default => false
-
-  # Enable or disable SSL
-  config :ssl, :validate => :boolean, :default => false
-
-  # Validate SSL certificate
-  config :verify_ssl, :validate => :boolean, :default => false
-
-  public
+  plugin_status "beta"
   def register
-    require "bunny" # rubygem 'bunny'
-
-    if @name
-      if @exchange
-        @logger.error("'name' and 'exchange' are the same setting, but 'name' is deprecated. Please use only 'exchange'")
-      end
-      @exchange = @name
-    end
-
-    @logger.info("Registering output", :plugin => self)
-    connect
+    @logger.warn("The 'amqp' output plugin has been renamed to 'rabbitmq'. " \
+                 "Please update your configuration appropriately.")
+    super
   end # def register
-
-  public
-  def connect
-    amqpsettings = {
-      :vhost => @vhost,
-      :host => @host,
-      :port => @port,
-      :logging => @debug,
-    }
-    amqpsettings[:user] = @user if @user
-    amqpsettings[:pass] = @password.value if @password
-    amqpsettings[:ssl] = @ssl if @ssl
-    amqpsettings[:verify_ssl] = @verify_ssl if @verify_ssl
-
-    begin
-      @logger.debug("Connecting to AMQP", :settings => amqpsettings,
-                    :exchange_type => @exchange_type, :name => @exchange)
-      @bunny = Bunny.new(amqpsettings)
-      @bunny.start
-    rescue => e
-      if terminating?
-        return
-      else
-        @logger.error("AMQP connection error (during connect), will reconnect",
-                      :exception => e, :backtrace => e.backtrace)
-        sleep(1)
-        retry
-      end
-    end
-
-    @logger.debug("Declaring exchange", :name => @exchange, :type => @exchange_type,
-                  :durable => @durable)
-    @bunnyexchange = @bunny.exchange(@exchange, :type => @exchange_type.to_sym, :durable => @durable)
-
-    @logger.debug("Binding exchange", :name => @exchange, :key => @key)
-  end # def connect
-
-  public
-  def receive(event)
-    return unless output?(event)
-
-    @logger.debug("Sending event", :destination => to_s, :event => event,
-                  :key => key)
-    key = event.sprintf(@key) if @key
-    begin
-      receive_raw(event.to_json, key)
-    rescue JSON::GeneratorError => e
-      @logger.warn("Trouble converting event to JSON", :exception => e,
-                   :event => event)
-      return
-    end
-  end # def receive
-
-  public
-  def receive_raw(message, key=@key)
-    begin
-      if @bunnyexchange
-        @logger.debug(["Publishing message", { :destination => to_s, :message => message, :key => key }])
-        @bunnyexchange.publish(message, :persistent => @persistent, :key => key)
-      else
-        @logger.warn("Tried to send message, but not connected to amqp yet.")
-      end
-    rescue *[Bunny::ServerDownError, Errno::ECONNRESET] => e
-      @logger.error("AMQP connection error (during publish), will reconnect: #{e}")
-      connect
-      retry
-    end
-  end
-
-  public
-  def to_s
-    return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@exchange}\##{@key}"
-  end
-
-  public
-  def teardown
-    @bunny.close rescue nil
-    @bunny = nil
-    @bunnyexchange = nil
-    finished
-  end # def teardown
-end # class LogStash::Outputs::Amqp
+end # class LogStash::Outputs::AMQP
diff --git a/lib/logstash/outputs/elasticsearch_http.rb b/lib/logstash/outputs/elasticsearch_http.rb
index 83e427c8d27..72b1f0c6a54 100644
--- a/lib/logstash/outputs/elasticsearch_http.rb
+++ b/lib/logstash/outputs/elasticsearch_http.rb
@@ -69,11 +69,25 @@ def receive(event)
   def receive_single(event, index, type)
     success = false
     while !success
-      response = @agent.post!("http://#{@host}:#{@port}/#{index}/#{type}",
-                              :body => event.to_json)
-      # We must read the body to free up this connection for reuse.
-      body = "";
-      response.read_body { |chunk| body += chunk }
+      begin
+        response = @agent.post!("http://#{@host}:#{@port}/#{index}/#{type}",
+                                :body => event.to_json)
+      rescue EOFError
+        @logger.warn("EOF while writing request or reading response header "
+                     "from elasticsearch", :host => @host, :port => @port
+        next # try again
+      end
+
+
+      begin
+        # We must read the body to free up this connection for reuse.
+        body = "";
+        response.read_body { |chunk| body += chunk }
+      rescue EOFError
+        @logger.warn("EOF while reading response body from elasticsearch",
+                     :host => @host, :port => @port
+        next # try again
+      end
 
       if response.status != 201
         @logger.error("Error writing to elasticsearch",
@@ -107,13 +121,25 @@ def flush
     # as documented here: 
     # http://www.elasticsearch.org/guide/reference/api/bulk.html
     #  "NOTE: the final line of data must end with a newline character \n."
-    response = @agent.post!("http://#{@host}:#{@port}/_bulk",
-                            :body => @queue.join("\n") + "\n")
+    begin
+      response = @agent.post!("http://#{@host}:#{@port}/_bulk",
+                              :body => @queue.join("\n") + "\n")
+    rescue EOFError
+      @logger.warn("EOF while writing request or reading response header "
+                   "from elasticsearch", :host => @host, :port => @port
+      return # abort this flush
+    end
 
     # Consume the body for error checking
     # This will also free up the connection for reuse.
     body = ""
-    response.read_body { |chunk| body += chunk }
+    begin
+      response.read_body { |chunk| body += chunk }
+    rescue EOFError
+      @logger.warn("EOF while reading response body from elasticsearch",
+                   :host => @host, :port => @port
+      return # abort this flush
+    end
 
     if response.status != 200
       @logger.error("Error writing (bulk) to elasticsearch",
diff --git a/lib/logstash/outputs/elasticsearch_river.rb b/lib/logstash/outputs/elasticsearch_river.rb
index 7e5a7aae51c..c806b47ef29 100644
--- a/lib/logstash/outputs/elasticsearch_river.rb
+++ b/lib/logstash/outputs/elasticsearch_river.rb
@@ -5,8 +5,8 @@
 require "net/http"
 
 # This output lets you store logs in elasticsearch. It's similar to the
-# 'elasticsearch' output but improves performance by using an AMQP server,
-# such as rabbitmq, to send data to elasticsearch.
+# 'elasticsearch' output but improves performance by using a queue server,
+# rabbitmq, to send data to elasticsearch.
 #
 # Upon startup, this output will automatically contact an elasticsearch cluster
 # and configure it to read from the queue to which we write.
@@ -41,43 +41,42 @@ class LogStash::Outputs::ElasticSearchRiver < LogStash::Outputs::Base
 
   # ElasticSearch river configuration: bulk timeout in milliseconds
   config :es_bulk_timeout_ms, :validate => :number, :default => 100
+  
+  # ElasticSearch river configuration: is ordered?
+  config :es_ordered, :validate => :boolean, :default => false
 
-  # Hostname of AMQP server
-  config :amqp_host, :validate => :string, :required => true
+  # Hostname of RabbitMQ server
+  config :rabbitmq_host, :validate => :string, :required => true
 
-  # Port of AMQP server
-  config :amqp_port, :validate => :number, :default => 5672
+  # Port of RabbitMQ server
+  config :rabbitmq_port, :validate => :number, :default => 5672
 
-  # AMQP user
+  # RabbitMQ user
   config :user, :validate => :string, :default => "guest"
 
-  # AMQP password
+  # RabbitMQ password
   config :password, :validate => :string, :default => "guest"
 
-  # AMQP vhost
+  # RabbitMQ vhost
   config :vhost, :validate => :string, :default => "/"
 
-  # AMQP queue name. Depricated due to conflicts with puppet naming convention.
-  # Replaced by 'queue' variable. See LOGSTASH-755
-  config :name, :validate => :string, :deprecated => true
-
-  # AMQP queue name
+  # RabbitMQ queue name
   config :queue, :validate => :string, :default => "elasticsearch"
   
-  # AMQP exchange name
+  # RabbitMQ exchange name
   config :exchange, :validate => :string, :default => "elasticsearch"
 
   # The exchange type (fanout, topic, direct)
   config :exchange_type, :validate => [ "fanout", "direct", "topic"],
          :default => "direct"
 
-  # AMQP routing key
+  # RabbitMQ routing key
   config :key, :validate => :string, :default => "elasticsearch"
 
-  # AMQP durability setting. Also used for ElasticSearch setting
+  # RabbitMQ durability setting. Also used for ElasticSearch setting
   config :durable, :validate => :boolean, :default => true
 
-  # AMQP persistence setting
+  # RabbitMQ persistence setting
   config :persistent, :validate => :boolean, :default => true
 
   # The document ID for the index. Useful for overwriting existing entries in
@@ -106,12 +105,12 @@ def register
 
   protected
   def prepare_river
-    require "logstash/outputs/amqp"
+    require "logstash/outputs/rabbitmq"
 
     # Configure the message plugin
     params = {
-      "host" => [@amqp_host],
-      "port" => [@amqp_port],
+      "host" => [@rabbitmq_host],
+      "port" => [@rabbitmq_port],
       "user" => [@user],
       "password" => [@password],
       "exchange_type" => [@exchange_type],
@@ -122,7 +121,7 @@ def prepare_river
       "persistent" => [@persistent.to_s],
       "debug" => [@debug.to_s],
     }.reject {|k,v| v.first.nil?}
-    @mq = LogStash::Outputs::Amqp.new(params)
+    @mq = LogStash::Outputs::RabbitMQ.new(params)
     @mq.register
 
     # Set up the river
@@ -132,13 +131,17 @@ def prepare_river
       # Name the river by our hostname
       require "socket"
       hostname = Socket.gethostname
-      api_path = "/_river/logstash-#{hostname.gsub('.','_')}/_meta"
-      @status_path = "/_river/logstash-#{hostname.gsub('.','_')}/_status"
+      
+      # Replace spaces with hyphens and remove all non-alpha non-dash non-underscore characters
+      river_name = "#{hostname} #{@queue}".gsub(' ', '-').gsub(/[^\w-]/, '')
+      
+      api_path = "/_river/logstash-#{river_name}/_meta"
+      @status_path = "/_river/logstash-#{river_name}/_status"
 
       river_config = {"type" => "rabbitmq",
                       "rabbitmq" => {
-                                "host" => @amqp_host=="localhost" ? hostname : @amqp_host,
-                                "port" => @amqp_port,
+                                "host" => @rabbitmq_host=="localhost" ? hostname : @rabbitmq_host,
+                                "port" => @rabbitmq_port,
                                 "user" => @user,
                                 "pass" => @password,
                                 "vhost" => @vhost,
@@ -147,10 +150,11 @@ def prepare_river
                                 "routing_key" => @key,
                                 "exchange_type" => @exchange_type,
                                 "exchange_durable" => @durable.to_s,
-                                "queue_durable" => @durable.to_s,
+                                "queue_durable" => @durable.to_s
                                },
                       "index" => {"bulk_size" => @es_bulk_size,
                                  "bulk_timeout" => "#{@es_bulk_timeout_ms}ms",
+                                 "ordered" => @es_ordered
                                 },
                      }
       @logger.info("ElasticSearch using river", :config => river_config)
diff --git a/lib/logstash/outputs/graphite.rb b/lib/logstash/outputs/graphite.rb
index 20742763956..e09248d3662 100644
--- a/lib/logstash/outputs/graphite.rb
+++ b/lib/logstash/outputs/graphite.rb
@@ -18,6 +18,12 @@ class LogStash::Outputs::Graphite < LogStash::Outputs::Base
   # The port to connect on your graphite server.
   config :port, :validate => :number, :default => 2003
 
+  # Interval between reconnect attempts to carboon
+  config :reconnect_interval, :validate => :number, :default => 2
+
+  # Should metrics be resend on failure?
+  config :resend_on_failure, :validate => :boolean, :default => false
+
   # The metric(s) to use. This supports dynamic strings like %{@source_host}
   # for metric names and also for values. This is a hash field with key 
   # of the metric name, value of the metric value. Example:
@@ -26,12 +32,23 @@ class LogStash::Outputs::Graphite < LogStash::Outputs::Base
   #
   # The value will be coerced to a floating point value. Values which cannot be
   # coerced will zero (0)
-  config :metrics, :validate => :hash, :required => true
+  config :metrics, :validate => :hash, :default => {}
+
+  # Indicate that the event @fields should be treated as metrics and will be sent as is to graphite
+  config :fields_are_metrics, :validate => :boolean, :default => false
+
+  # Include only regex matched metric names
+  config :include_metrics, :validate => :array, :default => []
+
+  # Exclude regex matched metric names, by default exclude unresolved %{field} strings
+  config :exclude_metrics, :validate => :array, :default => [ "%\{[^}]+\}" ]
 
   # Enable debug output
   config :debug, :validate => :boolean, :default => false
 
   def register
+    @include_metrics.collect!{|regexp| Regexp.new(regexp)}
+    @exclude_metrics.collect!{|regexp| Regexp.new(regexp)}
     connect
   end # def register
 
@@ -42,7 +59,7 @@ def connect
     rescue Errno::ECONNREFUSED => e
       @logger.warn("Connection refused to graphite server, sleeping...",
                    :host => @host, :port => @port)
-      sleep(2)
+      sleep(@reconnect_interval)
       retry
     end
   end # def connect
@@ -50,31 +67,45 @@ def connect
   public
   def receive(event)
     return unless output?(event)
-
+    
     # Graphite message format: metric value timestamp\n
 
-    # Catch exceptions like ECONNRESET and friends, reconnect on failure.
-    @metrics.each do |metric, value|
-      @logger.debug("processing", :metric => metric, :value => value)
+    messages = []
+    timestamp = event.sprintf("%{+%s}")
 
-      message = [event.sprintf(metric), event.sprintf(value).to_f,
-                 event.sprintf("%{+%s}")].join(" ")
+    if @fields_are_metrics
+      @logger.debug("got metrics event", :metrics => event.fields)
+      event.fields.each do |metric,value|
+        next unless @include_metrics.any? {|regexp| metric.match(regexp)}
+        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
+        messages << "#{metric} #{value.to_f} #{timestamp}"
+      end
+    else
+      @metrics.each do |metric, value|
+        @logger.debug("processing", :metric => metric, :value => value)
+        metric = event.sprintf(metric)
+        next unless @include_metrics.any? {|regexp| metric.match(regexp)}
+        next if @exclude_metrics.any? {|regexp| metric.match(regexp)}
+        messages << "#{event.sprintf(metric)} #{event.sprintf(value).to_f} #{timestamp}"
+      end
+    end
 
-      @logger.debug("Sending carbon message", :message => message, :host => @host, :port => @port)
+    unless messages.empty?
+      message = messages.join("\n")
+      @logger.debug("Sending carbon messages", :messages => messages, :host => @host, :port => @port)
 
+      # Catch exceptions like ECONNRESET and friends, reconnect on failure.
       # TODO(sissel): Test error cases. Catch exceptions. Find fortune and glory.
       begin
         @socket.puts(message)
       rescue Errno::EPIPE, Errno::ECONNRESET => e
         @logger.warn("Connection to graphite server died",
                      :exception => e, :host => @host, :port => @port)
-        sleep(2)
+        sleep(@reconnect_interval)
         connect
+        retry if @resend_on_failure
       end
-
-      # TODO(sissel): resend on failure 
-      # TODO(sissel): Make 'resend on failure' tunable; sometimes it's OK to
-      # drop metrics.
-    end # @metrics.each
+    end
+    
   end # def receive
-end # class LogStash::Outputs::Statsd
+end # class LogStash::Outputs::Statsd
\ No newline at end of file
diff --git a/lib/logstash/outputs/lumberjack.rb b/lib/logstash/outputs/lumberjack.rb
index f43edbfb687..d7c2e00f90e 100644
--- a/lib/logstash/outputs/lumberjack.rb
+++ b/lib/logstash/outputs/lumberjack.rb
@@ -31,10 +31,10 @@ def receive(event)
           "host" => event.source_host, 
           "file" => event.source_path,
           "type" => event.type
-        }.merge(event.fields)
+        }.merge(event["@fields"])
       )
     rescue Exception => e
-      @logger.log("Client write error", :e => e, :backtrace => e.backtrace)
+      @logger.error("Client write error", :e => e, :backtrace => e.backtrace)
       connect
       retry
     end
diff --git a/lib/logstash/outputs/rabbitmq.rb b/lib/logstash/outputs/rabbitmq.rb
new file mode 100644
index 00000000000..9c0355f77f3
--- /dev/null
+++ b/lib/logstash/outputs/rabbitmq.rb
@@ -0,0 +1,152 @@
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# Push events to a RabbitMQ exchange.
+#
+# This has been tested with Bunny 0.9.x, which supports RabbitMQ 2.x and 3.x. You can
+# find links to both here:
+#
+# * RabbitMQ - <http://www.rabbitmq.com/>
+# * Bunny - <https://github.com/ruby-amqp/bunny>
+class LogStash::Outputs::RabbitMQ < LogStash::Outputs::Base
+  MQTYPES = [ "fanout", "direct", "topic" ]
+
+  config_name "rabbitmq"
+  plugin_status "beta"
+
+  # Your rabbitmq server address
+  config :host, :validate => :string, :required => true
+
+  # The rabbitmq port to connect on
+  config :port, :validate => :number, :default => 5672
+
+  # Your rabbitmq username
+  config :user, :validate => :string, :default => "guest"
+
+  # Your rabbitmq password
+  config :password, :validate => :password, :default => "guest"
+
+  # The name of the exchange
+  config :exchange, :validate => :string, :required => true
+  
+  # The exchange type (fanout, topic, direct)
+  config :exchange_type, :validate => [ "fanout", "direct", "topic"], :required => true
+
+  # Key to route to by default. Defaults to 'logstash'
+  #
+  # * Routing keys are ignored on fanout exchanges.
+  config :key, :validate => :string, :default => "logstash"
+
+  # The vhost to use
+  config :vhost, :validate => :string, :default => "/"
+
+  # Is this exchange durable? (aka; Should it survive a broker restart?)
+  config :durable, :validate => :boolean, :default => true
+
+  # Should messages persist to disk on the rabbitmq broker until they are read by a
+  # consumer?
+  config :persistent, :validate => :boolean, :default => true
+
+  # Enable or disable debugging
+  config :debug, :validate => :boolean, :default => false
+
+  # Enable or disable SSL
+  config :ssl, :validate => :boolean, :default => false
+
+  # Validate SSL certificate
+  config :verify_ssl, :validate => :boolean, :default => false
+
+  # Maximum permissible size of a frame (in bytes) to negotiate with clients
+  config :frame_max, :validate => :number, :default => 131072
+
+  public
+  def register
+    require "bunny"
+
+    @logger.info("Registering output", :plugin => self)
+    connect
+  end # def register
+
+  public
+  def connect
+    
+    rabbitmq_settings = {
+      :vhost => @vhost,
+      :host => @host,
+      :port => @port,
+      :logging => @debug,
+    }
+    rabbitmq_settings[:user] = @user if @user
+    rabbitmq_settings[:pass] = @password.value if @password
+    rabbitmq_settings[:ssl] = @ssl if @ssl
+    rabbitmq_settings[:verify_ssl] = @verify_ssl if @verify_ssl
+    rabbitmq_settings[:frame_max] = @frame_max if @frame_max
+
+    begin
+      @logger.debug("Connecting to RabbitMQ", :settings => rabbitmq_settings,
+                    :exchange_type => @exchange_type, :name => @exchange)
+      @bunny = Bunny.new(rabbitmq_settings)
+      @bunny.start
+    rescue => e
+      if terminating?
+        return
+      else
+        @logger.error("RabbitMQ connection error (during connect), will reconnect",
+                      :exception => e, :backtrace => e.backtrace)
+        sleep(1)
+        retry
+      end
+    end
+
+    @logger.debug("Declaring exchange", :name => @exchange, :type => @exchange_type,
+                  :durable => @durable)
+    @bunnyexchange = @bunny.exchange(@exchange, :type => @exchange_type.to_sym, :durable => @durable)
+
+    @logger.debug("Binding exchange", :name => @exchange, :key => @key)
+  end # def connect
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    @logger.debug("Sending event", :destination => to_s, :event => event,
+                  :key => key)
+    key = event.sprintf(@key) if @key
+    begin
+      receive_raw(event.to_json, key)
+    rescue JSON::GeneratorError => e
+      @logger.warn("Trouble converting event to JSON", :exception => e,
+                   :event => event)
+      return
+    end
+  end # def receive
+
+  public
+  def receive_raw(message, key=@key)
+    begin
+      if @bunnyexchange
+        @logger.debug(["Publishing message", { :destination => to_s, :message => message, :key => key }])
+        @bunnyexchange.publish(message, :persistent => @persistent, :key => key)
+      else
+        @logger.warn("Tried to send message, but not connected to rabbitmq yet.")
+      end
+    rescue *[Bunny::ServerDownError, Errno::ECONNRESET] => e
+      @logger.error("RabbitMQ connection error (during publish), will reconnect: #{e}")
+      connect
+      retry
+    end
+  end
+
+  public
+  def to_s
+    return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@exchange}\##{@key}"
+  end
+
+  public
+  def teardown
+    @bunny.close rescue nil
+    @bunny = nil
+    @bunnyexchange = nil
+    finished
+  end # def teardown
+end # class LogStash::Outputs::RabbitMQ
diff --git a/lib/logstash/outputs/statsd.rb b/lib/logstash/outputs/statsd.rb
index b19f8867ae1..43334b4122c 100644
--- a/lib/logstash/outputs/statsd.rb
+++ b/lib/logstash/outputs/statsd.rb
@@ -4,6 +4,11 @@
 # statsd is a server for aggregating counters and other metrics to ship to
 # graphite.
 #
+# The most basic coverage of this plugin is that the 'namespace', 'sender', and
+# 'metric' names are combined into the full metric path like so:
+#
+#     namespace.sender.metric
+#
 # The general idea is that you send statsd count or latency data and every few
 # seconds it will emit the aggregated values to graphite (aggregates like
 # average, max, stddev, etc)
diff --git a/lib/logstash/pipeline.rb b/lib/logstash/pipeline.rb
index e21afe5de79..28be931c757 100644
--- a/lib/logstash/pipeline.rb
+++ b/lib/logstash/pipeline.rb
@@ -1,30 +1,73 @@
 require "logstash/config/file"
-require "logstash/agent" # only needed for now for parse_config
+#require "logstash/agent" # only needed for now for parse_config
 require "logstash/namespace"
+require "thread" # stdlib
+require "stud/trap"
 
 class LogStash::Pipeline
-  class ShutdownSignal; end
+  class ShutdownSignal < StandardError; end
 
   def initialize(configstr)
     # hacks for now to parse a config string
     config = LogStash::Config::File.new(nil, configstr)
-    agent = LogStash::Agent.new
-    @inputs, @filters, @outputs = agent.instance_eval { parse_config(config) }
+    @inputs, @filters, @outputs = parse_config(config)
 
-    @inputs.each(&:register)
-    @filters.each(&:register)
-    @outputs.each(&:register)
-
-    @input_to_filter = SizedQueue(16)
-    @filter_to_output = SizedQueue(16)
+    @input_to_filter = SizedQueue.new(20)
+    @filter_to_output = SizedQueue.new(20)
 
     # If no filters, pipe inputs directly to outputs
     if @filters.empty?
-      input_to_filter = filter_to_output
+      @input_to_filter = @filter_to_output
     end
-  end
+
+    @logger = Cabin::Channel.get(LogStash)
+    (@inputs + @filters + @outputs).each do |plugin|
+      plugin.logger = @logger
+    end
+
+    @inputs.each(&:register)
+    @filters.each(&:register)
+    @outputs.each(&:register)
+  end # def initialize
+
+  # Parses a config and returns [inputs, filters, outputs]
+  def parse_config(config)
+    # TODO(sissel): Move this method to config/file.rb
+    inputs = []
+    filters = []
+    outputs = []
+    config.parse do |plugin|
+      # 'plugin' is a has containing:
+      #   :type => the base class of the plugin (LogStash::Inputs::Base, etc)
+      #   :plugin => the class of the plugin (LogStash::Inputs::File, etc)
+      #   :parameters => hash of key-value parameters from the config.
+      type = plugin[:type].config_name  # "input" or "filter" etc...
+      klass = plugin[:plugin]
+
+      # Create a new instance of a plugin, called like:
+      # -> LogStash::Inputs::File.new( params )
+      instance = klass.new(plugin[:parameters])
+      instance.logger = @logger
+
+      case type
+        when "input"
+          inputs << instance
+        when "filter"
+          filters << instance
+        when "output"
+          outputs << instance
+        else
+          msg = "Unknown config type '#{type}'"
+          @logger.error(msg)
+          raise msg
+      end # case type
+    end # config.parse
+    return inputs, filters, outputs
+  end # def parse_config
 
   def run
+    start = Time.now
+
     # one thread per input
     @input_threads = @inputs.collect do |input|
       Thread.new(input) do |input|
@@ -38,9 +81,31 @@ def run
 
     # one outputworker thread
 
-    # Now monitor input threads state
-    # if all inputs are terminated, send shutdown signal to @input_to_filter
-  end
+    @output_thread = Thread.new do 
+      outputworker
+    end
+
+    @logger.info("Pipeline started")
+    
+    @input_threads.each(&:join)
+
+    # All input plugins have completed, send a shutdown signal.
+    duration = Time.now - start
+    puts "Duration: #{duration}"
+
+    @input_to_filter.push(ShutdownSignal)
+
+    # Wait for filters to stop
+    @filter_threads.each(&:join) if @filter_threads
+
+    # Wait for the outputs to stop
+    @output_thread.join
+
+    @logger.info("Pipeline shutdown complete.")
+
+    # exit code
+    return 0
+  end # def run
 
   def inputworker(plugin)
     begin
@@ -48,17 +113,18 @@ def inputworker(plugin)
     rescue ShutdownSignal
       plugin.teardown
     rescue => e
-      @logger.error("Exception in plugin #{plugin.class}, restarting plugin.",
-                    "plugin" => plugin.inspect, "exception" => e)
+      @logger.error(I18n.t("logstash.pipeline.worker-error",
+                           :plugin => plugin.inspect, :error => e))
+      puts e.backtrace
       plugin.teardown
       retry
     end
-  end # def 
+  end # def inputworker
 
   def filterworker
     begin
       while true
-        event << @input_to_filter
+        event = @input_to_filter.pop
         break if event == ShutdownSignal
 
         # Apply filters, in order, to the event.
@@ -67,57 +133,46 @@ def filterworker
         end
         next if event.cancelled?
 
-        @filter_to_output << event
+        @filter_to_output.push(event)
       end
     rescue => e
       @logger.error("Exception in plugin #{plugin.class}",
                     "plugin" => plugin.inspect, "exception" => e)
     end
+
     @filters.each(&:teardown)
   end # def filterworker
 
   def outputworker
-    begin
-      while true
-        event << @filter_to_output
-        break if event == ShutdownSignal
+    while true
+      event = @filter_to_output.pop
+      break if event == ShutdownSignal
 
-        @outputs.each do |output|
+      @outputs.each do |output|
+        begin
           output.receive(event)
+        rescue => e
+          @logger.error("Exception in plugin #{plugin.class}",
+                        "plugin" => plugin.inspect, "exception" => e)
         end
-      end
-    rescue => e
-      @logger.error("Exception in plugin #{plugin.class}",
-                    "plugin" => plugin.inspect, "exception" => e)
-    end
+      end # @outputs.each
+    end # while true
     @outputs.each(&:teardown)
   end # def filterworker
-end # class Pipeline
-
-def twait(thread)
-  begin
-    puts :waiting => thread[:name]
-    thread.join
-    puts :donewaiting => thread[:name]
-  rescue => e
-    puts thread => e
-  end
-end
-
-def shutdown(input, filter, output)
-  input.each do |i|
-    i.raise("SHUTDOWN")
-  end
-
-  #filter.raise("SHUTDOWN")
-  #twait(filter)
-  output.raise("SHUTDOWN")
-  twait(output)
-end
-
-trap("INT") do
-  puts "SIGINT"; shutdown(input_threads, filter_thread, output_thread)
-  exit 1
-end
 
+  # Shutdown this pipeline.
+  #
+  # This method is intended to be called from another thread
+  def shutdown
+    @input_threads.each do |thread|
+      # Interrupt all inputs
+      @logger.info("Sending shutdown signal to input thread",
+                   :thread => thread)
+      thread.raise(ShutdownSignal)
+      thread.wakeup # in case it's in blocked IO or sleeping
+    end
 
+    # No need to send the ShutdownSignal to the filters/outputs nor to wait for
+    # the inputs to finish, because in the #run method we wait for that anyway.
+  end # def shutdown
+end # class Pipeline
diff --git a/lib/logstash/plugin.rb b/lib/logstash/plugin.rb
index 48eff2da474..a45d0f962f5 100644
--- a/lib/logstash/plugin.rb
+++ b/lib/logstash/plugin.rb
@@ -1,8 +1,11 @@
 require "logstash/namespace"
 require "logstash/logging"
 require "logstash/config/mixin"
+require "cabin"
 
 class LogStash::Plugin
+  class ConfigurationError < StandardError; end
+
   attr_accessor :params
   attr_accessor :logger
 
@@ -20,8 +23,7 @@ def eql?(other)
   public
   def initialize(params=nil)
     @params = params
-    @logger = LogStash::Logger.new(STDOUT)
-    @logger.level = $DEBUG ? :debug : :warn
+    @logger = Cabin::Channel.get(LogStash)
   end
 
   # This method is called when someone or something wants this plugin to shut
@@ -47,6 +49,8 @@ def shutdown(queue)
   # forever.
   public
   def finished
+    # TODO(sissel): I'm not sure what I had planned for this shutdown_queue
+    # thing
     if @shutdown_queue
       @logger.info("Sending shutdown event to agent queue", :plugin => self)
       @shutdown_queue << self
@@ -103,4 +107,16 @@ def clear_watchdog
     Thread.current[:watchdog] = nil
     Thread.current[:watchdog_state] = nil
   end
+
+  public
+  def inspect
+    if !@config.nil?
+      description = @config \
+        .select { |k,v| !v.nil? && (v.respond_to?(:empty?) && !v.empty?) } \
+        .collect { |k,v| "#{k}=>#{v.inspect}" }
+      return "<#{self.class.name} #{description.join(", ")}>"
+    else
+      return "<#{self.class.name} --->"
+    end
+  end
 end # class LogStash::Plugin
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index 243ba30a9db..b8fe06b6f8c 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -1,8 +1,13 @@
-require "rubygems"
 require "logstash/namespace"
 require "logstash/program"
 require "logstash/util"
 require "logstash/JRUBY-6970"
+require "stud/trap"
+
+require "i18n" # gem 'i18n'
+I18n.load_path << File.expand_path(
+  File.join(File.dirname(__FILE__), "../../locales/en.yml")
+)
 
 if ENV["PROFILE_BAD_LOG_CALLS"]
   # Set PROFILE_BAD_LOG_CALLS=1 in your environment if you want
@@ -43,6 +48,7 @@ class LogStash::Runner
   include LogStash::Program
 
   def main(args)
+    @startup_interruption_trap = Stud::trap("INT") { puts "Interrupted"; exit 0 }
     LogStash::Util::set_thread_name(self.class.name)
     $: << File.join(File.dirname(__FILE__), "..")
 
@@ -51,11 +57,6 @@ def main(args)
       exit(1)
     end
 
-    #if (RUBY_ENGINE rescue nil) != "jruby"
-      #$stderr.puts "JRuby is required to use this."
-      #exit(1)
-    #end
-
     if RUBY_VERSION < "1.9.2"
       $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
       $stderr.puts "Options for fixing this: "
@@ -64,7 +65,7 @@ def main(args)
       return 1
     end
 
-    #require "java"
+    Stud::untrap("INT", @startup_interruption_trap)
 
     @runners = []
     while !args.empty?
diff --git a/lib/logstash/util.rb b/lib/logstash/util.rb
index 511a4656934..5e5ec54853c 100644
--- a/lib/logstash/util.rb
+++ b/lib/logstash/util.rb
@@ -1,10 +1,18 @@
 require "logstash/namespace"
 require "ffi" # gem ffi
-require "sys/uname" # gem sys-uname
 
 module LogStash::Util
   PR_SET_NAME = 15
-  UNAME = Sys::Uname.uname.sysname
+
+  # This can throw an exception, if it does, we're probably not on linux.
+  # It certainly throws an exception on Windows; I don't know how
+  # to work around it other than this hack.
+  begin
+    require "sys/uname" # gem sys-uname
+    UNAME = Sys::Uname.uname.sysname
+  rescue LoadError, FFI::NotFoundError
+    UNAME = "unknown"
+  end
 
   module LibC
     extend FFI::Library
diff --git a/locales/en.yml b/locales/en.yml
new file mode 100644
index 00000000000..5763371780f
--- /dev/null
+++ b/locales/en.yml
@@ -0,0 +1,108 @@
+# YAML notes
+#   |- means 'scalar block' useful for formatted text
+#   > means 'scalar block' but it chomps all newlines. Useful 
+#     for unformatted text.
+en:
+  unexpected-exception: |-
+    +-------------------------------------------------------
+    | An unexpected error occurred. This is probably a bug.
+    | You can find help with this problem in a few places:
+    | 
+    | * chat: #logstash IRC channel on freenode irc. To get
+    |         to IRC on the web, go here: http://goo.gl/TI4Ro
+    | * email: logstash-users@googlegroups.com 
+    | * bug system: https://logstash.jira.com/
+    |
+    +-------------------------------------------------------
+    The error reported is: 
+      %{error}
+  logstash:
+    pipeline:
+      worker-error: |-
+        A plugin had an unrecoverable error. Will restart this plugin.
+          Plugin: %{plugin}
+          Error: %{error}
+    agent:
+      error: |-
+        Error: %{error}
+      interrupted: |-
+        Interrupt received. Shutting down the pipeline.
+      configuration:
+        setting_missing: |-
+          Missing a required setting for the %{plugin} %{type} plugin:
+
+            %{type} {
+              %{plugin} {
+                %{setting} => # MISSING
+                ...
+              }
+            }
+        setting_invalid: |-
+          Invalid setting for %{plugin} %{type} plugin:
+
+            %{type} {
+              %{plugin} {
+                # This setting must be a %{value_type}
+                %{setting} => %{value}
+                ...
+              }
+            }
+        invalid_plugin_settings: |-
+          Something is wrong with your configuration.
+        plugin_path_missing: |-
+          You specified a plugin path that does not exist: %{path}
+        no_plugins_found: |-
+          Could not find any plugins in "%{path}"
+          I tried to find files matching the following, but found none: 
+            %{plugin_glob}
+        log_file_failed: |-
+          Failed to open %{path} for writing: %{error}
+
+          This is often a permissions issue, or the wrong 
+          path was specified?
+      flag:
+        # Wrap these at 45 chars so they display nicely when clamp emits them in
+        # an 80-character terminal
+        config: |-
+          Load the logstash config from a specific file
+          or directory.  If a direcory is given, all
+          files in that directory will be concatonated
+          in lexicographical order and then parsed as a
+          single config file. You can also specify
+          wildcards (globs) and any matched files will
+          be loaded in the order described above.
+        config-string: |-
+          Use the given string as the configuration
+          data. Same syntax as the config file. If not
+          input is specified, then 'stdin { type =|-
+          stdin }' is the default input. If no output
+          is specified, then 'stdout { debug => true
+          }}' is default output.
+        filterworkers: |-
+          Sets the number of filter workers to run.
+        watchdog-timeout: |-
+          Set the filter watchdog timeout (in seconds).
+          This timeout is used to detect stuck filters;
+          stuck filters usually symptoms of bugs.
+          When a filter takes longer than TIMEOUT
+          seconds, it will cause logstash to abort.
+        log: |-
+          Write logstash internal logs to the given
+          file. Without this flag, logstash will emit
+          logs to standard output.
+        verbosity: |-
+          Increase verbosity of logstash internal logs.
+          Specifying once will show 'informational'
+          logs. Specifying twice will show 'debug'
+          logs.
+        version: |-
+          Emit the version of logstash and its friends,
+          then exit.
+        pluginpath: |-
+          A path of where to find plugins. This flag
+          can be given multiple times to include
+          multiple paths. Plugins are expected to be
+          in a specific directory hierarchy:
+          'PATH/logstash/TYPE/NAME.rb' where TYPE is
+          'input' 'filter' or 'output' and NAME is the
+          name of the plugin.
diff --git a/logstash.gemspec b/logstash.gemspec
index db7ba4d96ce..0a8a964b564 100644
--- a/logstash.gemspec
+++ b/logstash.gemspec
@@ -24,6 +24,7 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "stud"
   gem.add_runtime_dependency "sys-uname" # for platform detection
   gem.add_runtime_dependency "clamp" # for command line args/flags
+  gem.add_runtime_dependency "i18n"
 
   # Web dependencies
   gem.add_runtime_dependency "ftw", ["~> 0.0.26"]
@@ -37,7 +38,7 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "aws-sdk"
   gem.add_runtime_dependency "heroku"
   gem.add_runtime_dependency "addressable", ["~> 2.2.6"]
-  gem.add_runtime_dependency "bunny"
+  gem.add_runtime_dependency "bunny", [">= 0.9.0.pre6"]
   gem.add_runtime_dependency "ffi"
   gem.add_runtime_dependency "ffi-rzmq", ["0.9.3"]
   gem.add_runtime_dependency "filewatch", ["0.5.1"]
@@ -62,6 +63,8 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "php-serialize" # For input drupal_dblog
   gem.add_runtime_dependency "murmurhash3"
   gem.add_runtime_dependency "rufus-scheduler"
+  #gem.add_runtime_dependency "user_agent_parser", [">= 2.0.0"]
+  gem.add_runtime_dependency "snmp"
 
   if RUBY_PLATFORM == 'java'
     gem.platform = RUBY_PLATFORM
diff --git a/patterns/haproxy b/patterns/haproxy
index a1feb67aec4..e10fd9706f0 100644
--- a/patterns/haproxy
+++ b/patterns/haproxy
@@ -31,7 +31,7 @@ HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:captured_response_headers}
 # HAPROXYCAPTUREDRESPONSEHEADERS %{DATA:response_header_content_type}\|%{DATA:response_header_content_encoding}\|%{DATA:response_header_cache_control}\|%{DATA:response_header_last_modified}
 
 # parse a haproxy 'httplog' line 
-HAPROXYHTTP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version}")?
+HAPROXYHTTP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_request}/%{INT:time_queue}/%{INT:time_backend_connect}/%{INT:time_backend_response}/%{NOTSPACE:time_duration} %{INT:http_status_code} %{NOTSPACE:bytes_read} %{DATA:captured_request_cookie} %{DATA:captured_response_cookie} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue} (\{%{HAPROXYCAPTUREDREQUESTHEADERS}\})?( )?(\{%{HAPROXYCAPTUREDRESPONSEHEADERS}\})?( )?"(<BADREQ>|(%{WORD:http_verb} (%{URIPROTO:http_proto}://)?(?:%{USER:http_user}(?::[^@]*)?@)?(?:%{URIHOST:http_host})?(?:%{URIPATHPARAM:http_request})?( HTTP/%{NUMBER:http_version})?))?"
 
 # parse a haproxy 'tcplog' line
 HAPROXYTCP %{SYSLOGTIMESTAMP:syslog_timestamp} %{IPORHOST:syslog_server} %{SYSLOGPROG}: %{IP:client_ip}:%{INT:client_port} \[%{HAPROXYDATE:accept_date}\] %{NOTSPACE:frontend_name} %{NOTSPACE:backend_name}/%{NOTSPACE:server_name} %{INT:time_queue}/%{INT:time_backend_connect}/%{NOTSPACE:time_duration} %{NOTSPACE:bytes_read} %{NOTSPACE:termination_state} %{INT:actconn}/%{INT:feconn}/%{INT:beconn}/%{INT:srvconn}/%{NOTSPACE:retries} %{INT:srv_queue}/%{INT:backend_queue}
diff --git a/patterns/mcollective b/patterns/mcollective
new file mode 100644
index 00000000000..648b172eeda
--- /dev/null
+++ b/patterns/mcollective
@@ -0,0 +1 @@
+MCOLLECTIVEAUDIT %{TIMESTAMP_ISO8601:timestamp}:
