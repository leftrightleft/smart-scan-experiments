diff --git a/.github/ISSUE_TEMPLATE.md b/.github/ISSUE_TEMPLATE.md
new file mode 100644
index 00000000000..9d85c7cfddb
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE.md
@@ -0,0 +1,11 @@
+Please post all product and debugging questions on our [forum](https://discuss.elastic.co/c/logstash). Your questions will reach our wider community members there, and if we confirm that there is a bug, then we can open a new issue here.
+
+Logstash Plugins are located in a different organization: https://github.com/logstash-plugins. For bugs on specific Logstash plugins, for example, if Redis Output has a defect, please open it in the respective Redis Output repository. 
+
+For all general issues, please provide the following details for fast resolution:
+
+- Version:
+- Operating System:
+- Config File (if you have sensitive info, please remove it):
+- Sample Data:
+- Steps to Reproduce:
diff --git a/.github/PULL_REQUEST_TEMPLATE.md b/.github/PULL_REQUEST_TEMPLATE.md
new file mode 100644
index 00000000000..a1538275aec
--- /dev/null
+++ b/.github/PULL_REQUEST_TEMPLATE.md
@@ -0,0 +1 @@
+Thanks for contributing to Logstash! If you haven't already signed our CLA, here's a handy link: https://www.elastic.co/contributor-agreement/
diff --git a/.gitignore b/.gitignore
index 9360eb39bb9..f47560fc5eb 100644
--- a/.gitignore
+++ b/.gitignore
@@ -1,5 +1,6 @@
 .*.swp
-*.gem
+/*.gem
+logstash*/*.gem
 pkg/*.deb
 pkg/*.rpm
 *.class
@@ -28,3 +29,13 @@ rspec.xml
 .vendor
 integration_run
 .mvn/
+qa/.vm_ssh_config
+qa/.vagrant
+qa/acceptance/.vagrant
+qa/Gemfile.lock
+*.ipr
+*.iws
+*.iml
+.idea
+logs
+qa/integration/services/installed/
diff --git a/.tailor b/.tailor
deleted file mode 100644
index 5e883dba31d..00000000000
--- a/.tailor
+++ /dev/null
@@ -1,8 +0,0 @@
-Tailor.config do |config|
-  config.file_set '*.rb' do |style|
-    style.indentation_spaces 2, :level => :off
-    style.max_line_length 80, :level => :off
-    style.allow_trailing_line_spaces true, :level => :off
-    style.spaces_after_comma false, :level => :off
-  end
-end
diff --git a/.travis.yml b/.travis.yml
new file mode 100644
index 00000000000..94fc6b19337
--- /dev/null
+++ b/.travis.yml
@@ -0,0 +1,29 @@
+sudo: false
+language: ruby
+cache:
+  directories:
+    - vendor/bundle
+    - ~/.gradle/
+rvm:
+  - jruby-1.7.25
+jdk:
+  - oraclejdk8
+env:
+  - INTEGRATION=false SPEC_OPTS="--order rand --format documentation"
+  - INTEGRATION=true SPEC_OPTS="--order rand --format documentation"
+  - INTEGRATION=false FEATURE_FLAG=persistent_queues SPEC_OPTS="--order rand --format documentation"
+  - INTEGRATION=true FEATURE_FLAG=persistent_queues SPEC_OPTS="--order rand --format documentation"
+before_install:
+  # Force bundler 1.12.5 because version 1.13 has issues, see https://github.com/fastlane/fastlane/issues/6065#issuecomment-246044617
+  - gem uninstall -i /home/travis/.rvm/gems/jruby-1.7.25@global bundler
+  - gem install bundler -v 1.12.5 --no-rdoc --no-ri --no-document --quiet
+install:
+  - rake test:install-core
+script:
+  - |+
+      if [ "$INTEGRATION" == "true" ]; then
+        ci/travis_integration_install.sh
+        ci/travis_integration_run.sh;
+      else
+        rake test:core
+      fi
diff --git a/CHANGELOG.md b/CHANGELOG.md
index 5f8bf0bb62c..75777fdaec9 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,236 @@
+## 5.0.0-beta1 (Sep 21, 2016)
+ - Migrated Logstash's internal logging framework to Log4j2. This enhancement provides the following features:
+   - Support changing logging level dynamically at runtime through REST endpoints. New APIs have been exposed 
+     under `_node/logging` to update log levels. Also a new endpoint `_node/logging` was added to return all 
+     existing loggers.
+   - Configurable file rotation policy for logs. Default is per-day.
+   - Support component-level or plugin level log settings.
+   - Unify logging across Logstash's Java and Ruby code.
+   - Logs are now placed in `LS_HOME/logs` dir configurable via `path.logs` setting.
+ - Breaking change: Set default log severity level to `INFO` instead of `WARN` to match Elasticsearch.
+ - Show meaningful error message with an unknown CLI command ([#5748](https://github.com/elastic/logstash/issues/5748))
+ - Monitoring API enhancements
+   - Added `duration_in_millis` metric under `/_node/stats/pipeline/events`
+   - Added JVM GC stats under `/_node/stats/jvm`
+   - Removed the `/_node/mem` resource as it's been properly moved under `/_node/jvm/mem`
+   - Added config reload stats under new resource type `_node/stats/pipeline/reloads`
+   - Added config reload enabled/disabled info to `/_node/pipeline`
+   - Added JVM GC strategy info under `/_node/jvm`
+   - Ensure `?human` option works correctly for `hot_threads` API.
+   - Make sure a non-existing API endpoint correctly returns 404 and a structured error message.
+ - Plugin Developers: Improved nomenclature and methods for 'threadsafe' outputs. Removed `workers_not_supported` 
+    method ([#5662](https://github.com/elastic/logstash/issues/5662))
+
+### Output
+  - Elasticsearch
+    - Breaking Change: Index template for 5.0 has been changed to reflect Elasticsearch's mapping 
+      changes. Most importantly, the subfield for string multi-fields has changed from `.raw` to `.keyword` 
+      to match ES default behavior ([#386](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/386))
+    - Users installing ES 5.x and LS 5.x This change will not affect you and you will continue to use 
+      the ES defaults. Users upgrading from LS 2.x to LS 5.x with ES 5.x LS will not force upgrade the template, 
+      if logstash template already exists. This means you will still use .raw for sub-fields coming from 2.x. 
+      If you choose to use the new template, you will have to reindex your data after the new template is 
+      installed.
+    - Added `check_connection_timeout` parameter which has a default of 10m
+
+## 5.0.0-alpha5 (Aug 2, 2016)
+ - Introduced a performance optimization called bi-values to store both JRuby and Java object types which will
+   benefit plugins written in Ruby.
+ - Added support for specifying a comma-separated list of resources to monitoring APIs. This can be used to 
+   filter API response ([#5609](https://github.com/elastic/logstash/issues/5609))
+ - `/_node/hot_threads?human=true` human option now returns a human readable format, not JSON.
+ - Pipeline stats from `/_node/stats/pipeline` is also included in the parent `/_node/stats` 
+   resource for completeness.
+ 
+### Input
+ - Beats
+   - Reimplemented input in Java and to use asynchronous IO library Netty. These changes resulted in 
+     up to 50% gains in throughput performance while preserving the original functionality ([#92](https://github.com/logstash-plugins/logstash-input-beats/issues/92)).
+ - JDBC
+   - Added support for providing encoding charset for strings not in UTF-8 format. `columns_charset` allows 
+     you to override this encoding setting per-column ([#143](https://github.com/logstash-plugins/logstash-input-jdbc/issues/143))
+ - HTTP Poller
+   - Added meaningful error messages on missing trust/key-store password. Document the creation of a custom keystore.
+
+### Filter
+ - CSV
+   - Added `autodetect_column_names` option to read column names from header.
+ - Throttle
+   - Reimplemented plugin to work with multiple threads, support asynchronous input and properly 
+     tracks past events ([#4](https://github.com/logstash-plugins/logstash-filter-throttle/issues/4))
+
+### Output
+ - Elasticsearch
+   - Added ability to choose different default template based on ES versions ([#401](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/401))
+ - Kafka
+   - Input is a shareable instance across multiple pipeline workers. This ensures efficient use of resources like 
+     broker TCP connections, internal producer buffers, etc ([#79](https://github.com/logstash-plugins/logstash-output-kafka/pull/79))
+   - Added feature to allow regex patterns in topics so you can subscribe to multiple ones.
+
+## 5.0.0-alpha4 (June 28, 2016)
+ - Created a new `LS_HOME/data` directory to store plugin states, Logstash instance UUID and more. This directory 
+   location is configurable via `path.data` ([#5404](https://github.com/elastic/logstash/issues/5404)).
+ - Made `bin/logstash -V/--version` fast on Unix platforms.
+ - Monitoring API: Added hostname, http_address, version as static fields for all APIs ([#5450](https://github.com/elastic/logstash/issues/5450)).
+ - Added time tracking (wall-clock) to all individual filter and output instances. The goal is to help identify 
+   what plugin configurations are consuming the most time. Exposed via `/_node/stats/pipeline`.
+ - Added ` /_node` API which provides static information for OS, JVM and pipeline settings.  
+ - Moved  `_plugins` api to `_node/plugins` endpoint.
+ - Moved `hot_thread` API report to `_node/hot_thread` endpoint.
+ - Add new `:list` property to configuration parameters. This will allow the user to specify one or more values.
+ - Add new URI config validator/type. This allows plugin like the Elasticsearch output to safely URIs for 
+   their configuration. Any password information in the URI will be masked when logged.
+ 
+### Input
+ - Kafka
+   - Added support for Kafka broker 0.10.
+ - HTTP
+   - Fixed a bug where HTTP input plugin blocked stats API ([#51](https://github.com/logstash-plugins/logstash-input-http/issues/51)). 
+ 
+### Output
+ - Elasticsearch
+   - ES output is now fully threadsafe. This means internal resources can be shared among multiple 
+     `output { elasticsearch {} }` instances.
+   - Sniffing improvements so any current connections don't have to be closed/reopened after a sniff round.
+   - Introduced a connection pool to efficiently reuse connections to ES backends.
+   - Added exponential backoff to connection retries with a ceiling of `retry_max_interval` which is the most time to 
+     wait between retries, and `retry_initial_interval` which is the initial amount of time to wait. 
+     `retry_initial_interval` will be increased exponentially between retries until a request succeeds.
+     
+ - Kafka
+   - Added support for Kafka broker 0.10
+   
+### Filter
+ - Grok
+   - Added stats counter on grok matches and failures. This is exposed in `_node/stats/pipeline`
+ - Date
+   - Added stats counter on grok matches and failures. This is exposed in `_node/stats/pipeline`  
+
+## 5.0.0-alpha3 (May 31, 2016)
+ - Breaking Change: Introduced a new way to configure application settings for Logstash through a settings.yml file.
+   This file is typically located in `LS_HOME/config`, or `/etc/logstash` when installed via packages. Logstash will not be 
+   able to start without this file, so please make sure to pass in `path.settings` if you are starting Logstash manually after 
+   installing it via a package (RPM, DEB) ([#4401](https://github.com/elastic/logstash/issues/4401)).
+ - Breaking Change: Most of the long form options (https://www.elastic.co/guide/en/logstash/5.0/command-line-flags.html) 
+   have been renamed to adhere to the yml dot notation to be used in the settings file. Short form options have not been
+   changed ([#4401](https://github.com/elastic/logstash/issues/4401)).
+ - Breaking Change: When Logstash is installed via DEB, RPM packages, it uses /usr/share and /var to install binaries and 
+   config files respectively. Previously it used to install in /opt directory. This change was done to make the user experience 
+   consistent with other Elastic products ([#5101](https://github.com/elastic/logstash/issues/5101)).
+ - Breaking Change: For plugin developers, the Event class has a [new API](https://github.com/elastic/logstash/issues/5141) 
+   to access its data. You will no longer be able to directly use the Event class through the ruby hash paradigm. All the 
+   plugins packaged with Logstash has been updated to use the new API and their versions bumped to the next major.
+ - Added support for systemd so you can now manage Logstash as a service on most Linux distributions ([#5012](https://github.com/elastic/logstash/issues/5012)).
+ - Added new subcommand `generate` to `logstash-plugins` script that bootstraps a new plugin with the right directory structure
+   and all the required files.
+ - Logstash can now emit its log in structured, json format. Specify `--log.format=json` in the settings file or via 
+   the command line ([#1569](https://github.com/elastic/logstash/issues/1569)).
+ - Added more operational information to help run Logstash in production. `_node/stats` now shows file descriptors 
+   and cpu information.
+ - Fixed a bug where Logstash would not shutdown when CTRL-C was used, when using stdin input in configuration ([#1769](https://github.com/elastic/logstash/issues/1769)).
+   
+### Input
+ - RabbitMQ: Removed `verify_ssl` option which was never used previously. To validate SSL certs use the 
+   `ssl_certificate_path` and `ssl_certificate_password` config options ([#82](https://github.com/logstash-plugins/logstash-input-rabbitmq/issues/82)).
+ - Stdin: This plugin is now non-blocking so you can use CTRL-C to stop Logstash.
+ - JDBC: Added `jdbc_password_filepath` parameter for reading password from an external file ([#120](https://github.com/logstash-plugins/logstash-input-jdbc/issues/120)).
+ 
+### Filter
+ - XML:
+   - Breaking: New configuration `suppress_empty` which defaults to `true`. Changed default behaviour of the plugin 
+     in favor of avoiding mapping conflicts when reaching elasticsearch ([#24](https://github.com/logstash-plugins/logstash-filter-xml/issues/24)).
+   - New configuration `force_content`. By default the filter expands attributes differently from content in xml 
+     elements. This option allows you to force text content and attributes to always parse to a hash value ([#16](https://github.com/logstash-plugins/logstash-filter-xml/issues/16)).
+   - Fixed a bug that ensure `target` is set when storing xml content in the event (`store_xml => true`).
+
+## 5.0.0-alpha2 (May 3, 2016
+### general
+ - Added `--preserve` option to `bin/logstash-plugin` install command. This allows us to preserve gem options 
+   which are already specified in `Gemfile`, which would have been previously overwritten.
+ - When running any plugin related commands you can now use DEBUG=1, to give the user a bit more 
+   information about what bundler is doing.
+ - Added reload support to the init script so you can do `service logstash reload`
+ - Fixed use of KILL_ON_STOP_TIMEOUT variable in init scripts which allows Logstash to force stop (#4991).
+ - Upgrade to JRuby 1.7.25.
+ - Filenames for Debian and RPM artifacts have been renamed to match Elasticsearch's naming scheme. The metadata 
+   is still the same, so upgrades will not be affected. If you have automated downloads for Logstash, please make
+   sure you have the updated URLs with the new names ([#5100](https://github.com/elastic/logstash/issues/5100)).  
+
+### Input
+ - Kafka: Fixed an issue where Snappy and LZ4 compression were not working.
+
+### Filter
+ - GeoIP: Added support for GeoIP2 city database and support for IPv6 lookups ([#23](https://github.com/logstash-plugins/logstash-filter-geoip/issues/23))
+
+### Output
+ - Elasticsearch: Added support for specifying ingest pipelines ([#410](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/410))
+ - Kafka: Fixed an issue where Snappy and LZ4 compression were not working ([#50](https://github.com/logstash-plugins/logstash-output-kafka/issues/50)).  
+
+
+## 5.0.0-alpha1 (April 5, 2016)
+### general
+ - Added APIs to monitor the Logstash pipeline. You can now query information/stats about event 
+   flow, JVM, and hot_threads.
+ - Added dynamic config, a new feature to track config file for changes and restart the 
+   pipeline (same process) with updated config changes. This feature can be enabled in two 
+   ways: Passing a CLI long-form option `--auto-reload` or with short-form `-r`. Another 
+   option, `--reload-interval <seconds>` controls how often LS should check the config files 
+   for changes. Alternatively, if you don't start with the CLI option, you can send SIGHUP 
+   or `kill -1` signal to LS to reload the config file, and restart the pipeline ([#4513](https://github.com/elastic/logstash/issues/4513)).
+ - Added support to evaluate environment variables inside the Logstash config. You can also specify a 
+   default if the variable is not defined. The syntax is `${myVar:default}` ([#3944](https://github.com/elastic/logstash/issues/3944)).
+ - Improved throughput performance across the board (up by 2x in some configs) by implementing Event 
+   representation in Java. Event is the main object that encapsulates data as it flows through 
+   Logstash and provides APIs for the plugins to perform processing. This change also enables 
+   faster serialization for future persistence work ([4191](https://github.com/elastic/logstash/issues/4191)).
+ - Added ability to configure custom garbage collection log file using `$LS_LOG_DIR`.
+ - `bin/plugin` in renamed to `bin/logstash-plugin`. This was renamed to prevent `PATH` being polluted 
+   when other components of the Elastic stack are installed on the same instance ([#4891](https://github.com/elastic/logstash/pull/4891)).
+ - Fixed a bug where new pipeline might break plugins by calling the `register` method twice causing 
+   undesired behavior ([#4851](https://github.com/elastic/logstash/issues/4851)).
+ - Made `JAVA_OPTS` and `LS_JAVA_OPTS` work consistently on Windows  ([#4758](https://github.com/elastic/logstash/pull/4758)).
+ - Fixed bug where specifying JMX parameters in `LS_JAVA_OPTS` caused Logstash not to restart properly
+   ([#4319](https://github.com/elastic/logstash/issues/4319)).
+ - Fixed a bug where upgrading plugins with Manticore threw an error and sometimes corrupted installation ([#4818](https://github.com/elastic/logstash/issues/4818)).
+ - Removed milestone warning that was displayed when the `--pluginpath` option was used to load plugins ([#4562](https://github.com/elastic/logstash/issues/4562)).
+ - Upgraded to JRuby 1.7.24.
+ - Reverted default output workers to 1. Perviously we had made output workers the same as number of pipeline
+   workers ([#4877](https://github.com/elastic/logstash/issues/4877)).
+   
+### input
+ - Beats
+   - Enhanced to verify client certificates against CA ([#8](https://github.com/logstash-plugins/logstash-input-beats/issues/8)).
+ - RabbitMQ
+   - Breaking Change: Metadata is now disabled by default because it was regressing performance.
+   - Improved performance by using an internal queue and bulk ACKs.
+ - Redis
+   - Increased the batch_size to 100 by default. This provides a big jump in throughput and 
+     reduction in CPU utilization ([#25](https://github.com/logstash-plugins/logstash-input-redis/issues/25)).
+ - JDBC
+   - Added retry connection feature ([#91](https://github.com/logstash-plugins/logstash-input-jdbc/issues/91)).
+ - Kafka
+   - Breaking: Added support for 0.9 consumer API. This plugin now supports SSL based encryption. This release 
+     changed a lot of configuration, so it is not backward compatible. Also, this version will not work with 
+     Kafka 0.8 broker
+   
+### filter
+  - DNS: 
+    - Improved performance by adding caches to both successful and failed requests.
+    - Added support for retrying with the `:max_retries` setting.
+    - Lowered the default value of timeout from 2 to 0.5 seconds.
+
+### output   
+  - Elasticsearch
+    - Bumped minimum manticore version to 0.5.4 which fixes a memory leak when sniffing 
+      is used ([#392](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/392)).
+    - Fixed bug when updating documents with doc_as_upsert and scripting.
+    - Made error messages more verbose and easier to parse by humans.
+    - Retryable failures are now logged at the info level instead of warning.
+  - Kafka
+    - Breaking: Added support for 0.9 API. This plugin now supports SSL based encryption. This release 
+      changed a lot of configuration, so it is not backward compatible. Also, this version will not work with 
+      Kafka 0.8 broker      
+
 ## 1.5.5 (Oct 29, 2015)
 ### general
  - Update to JRuby 1.7.22
@@ -140,7 +373,7 @@ pre-releases.
   - Lumberjack: This input was not handling backpressure properly from downstream plugins and
     would continue to accept data, eventually running out of memory. We added a circuit breaker to stop
     accepting new connections when we detect this situation. Please note that `max_clients` setting 
-    intoduced in v0.1.9 has been deprecated. This setting temporarily solved the problem by configuring
+    introduced in v0.1.9 has been deprecated. This setting temporarily solved the problem by configuring
     an upper limit to the number of LSF connections (#12).
   - Http: Added new input to receive data via http(s).
   - File: Fixed a critical bug where new files being added to a dir being watched would crash LS.
@@ -263,7 +496,7 @@ pre-releases.
     - Added IAM roles support so you can securely read and write events from S3 without providing your
       AWS credentials (#1575). 
     - Added support for using temporary credentials obtained from AWS STS (#1946)
-    - AWS credentials can be specfied through environment variables (#1619)  
+    - AWS credentials can be specified through environment variables (#1619)  
   - RabbitMQ: 
     - Fixed march_hare client uses incorrect connection url (LOGSTASH-2276)
     - Use Bunny 1.5.0+ (#1894)
@@ -326,7 +559,7 @@ pre-releases.
       the ability to capture failed requests from Elasticsearch and retry them. Error codes like 
       429 (too many requests) will now be retried by default for 3 times. The number of retries and the
       interval between consecutive retries can be configured (#1631)
-    - Logstash does not create a "message.raw" by default whic is usually not_analyzed; this
+    - Logstash does not create a "message.raw" by default which is usually not_analyzed; this
       helps save disk space (#11)
     - Added sniffing config to be able to list machines in the cluster while using the transport client (#22) 
     - Deprecate the usage of index_type configuration. Added document_type to be consistent
@@ -373,7 +606,7 @@ pre-releases.
   - bugfix: file: fixed debian 7 path issue
 
 ### codecs
-  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
+  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming inputs
   - improvement: collectd: add support for NaN values
 
 ### outputs
@@ -522,7 +755,7 @@ pre-releases.
 ### outputs
   - bugfix: elasticsearch: flush any buffered events on logstash shutdown
     (#1175)
-  - feature: riemann: Automatically map event fields to rieman event fields
+  - feature: riemann: Automatically map event fields to riemann event fields
     (logstash-contrib#15, Byron Pezan)
   - bugfix: lumberjack: fix off-by-one errors causing writes to another
     logstash agent to block indefinitely
@@ -789,7 +1022,7 @@ pre-releases.
     (LOGSTASH-1423, #692, #739; Jordan Sissel, Bernd Ahlers)
 
 ### patterns
-  - improvement: added IPV6 suppot to IP pattern (#623)
+  - improvement: added IPV6 support to IP pattern (#623)
 
 ## 1.2.1 (September 7, 2013)
 ### general
@@ -1172,7 +1405,7 @@ pre-releases.
    matched. (LOGSTASH-705)
  - feature: kv: Adds field_split, value_split, prefix, and container
    settings. (#225, patch by Alex Wheeler)
- - bugfix: mutate: rename on a nonexistant field now does nothing as expected.
+ - bugfix: mutate: rename on a nonexistent field now does nothing as expected.
    (LOGSTASH-757)
  - bugfix: grok: don't tag an event with _grokparsefailure if it's already so
    (#248, patch by Greg Brockman)
@@ -1228,7 +1461,7 @@ pre-releases.
    Gemfile will be purged in the near future.
  - amqp plugins are now marked 'unsupported' as there is no active maintainer
    nor is there source of active support in the community. If you're interested
-   in maintainership, please email the mailling list or contact Jordan!
+   in maintainership, please email the mailing list or contact Jordan!
 
 ### inputs
  - irc: now stores irc nick
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index d325328823b..e9eab8029ac 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -10,7 +10,7 @@ That said, some basic guidelines, which you are free to ignore :)
 
 ## Want to learn?
 
-Want to lurk about and see what others are doing with Logstash? 
+Want to lurk about and see what others are doing with Logstash?
 
 * The irc channel (#logstash on irc.freenode.org) is a good place for this
 * The [forum](https://discuss.elastic.co/c/logstash) is also
@@ -18,7 +18,7 @@ Want to lurk about and see what others are doing with Logstash?
 
 ## Got Questions?
 
-Have a problem you want Logstash to solve for you? 
+Have a problem you want Logstash to solve for you?
 
 * You can ask a question in the [forum](https://discuss.elastic.co/c/logstash)
 * Alternately, you are welcome to join the IRC channel #logstash on
@@ -36,12 +36,32 @@ If you think you found a bug, it probably is a bug.
 * If it is specific to a plugin, please file it in the respective repository under [logstash-plugins](https://github.com/logstash-plugins)
 * or ask the [forum](https://discuss.elastic.co/c/logstash).
 
+## Issue Prioritization
+The Logstash team takes time to digest, consider solutions, and weigh applicability of issues to both the broad
+Logstash user base and our own goals for the project. Through this process, we assign issues a priority using GitHub
+labels. Below is a description of priority labels.
+
+* P1: A high priority issue that affects almost all Logstash users. Bugs that would cause data loss, security
+issues and test failures. Workarounds for P1s generally donâ€™t exist without a code change. A P1 issue is usually
+stop the world kinda scenario, so we need to make sure P1s are properly triaged and being worked upon.
+* P2: A broadly applicable, high visibility issue that enhances Logstash usability for a majority of users.
+* P3: Nice-to-have bug fixes or functionality.  Workarounds for P3s generally exist.
+* P4: Anything not in above, catch-all label.
+
 # Contributing Documentation and Code Changes
 
-If you have a bugfix or new feature that you would like to contribute to
-logstash, and you think it will take more than a few minutes to produce the fix
-(ie; write code), it is worth discussing the change with the Logstash users and developers first! You can reach us via [GitHub](https://github.com/elastic/logstash/issues), the [forum](https://discuss.elastic.co/c/logstash), or via IRC (#logstash on freenode irc)
-Please note that Pull Requests without tests will not be merged. If you would like to contribute but do not have experience with writing tests, please ping us on IRC/forum or create a PR and ask our help.
+If you have a bugfix or new feature that you would like to contribute to Logstash, and you think it will take
+more than a few minutes to produce the fix (ie; write code), it is worth discussing the change with the Logstash
+users and developers first! You can reach us via [GitHub](https://github.com/elastic/logstash/issues), the [forum](https://discuss.elastic.co/c/logstash), or via IRC (#logstash on freenode irc)
+Please note that Pull Requests without tests will not be merged. If you would like to contribute but do not have
+experience with writing tests, please ping us on IRC/forum or create a PR and ask our help.
+
+If you would like to contribute to Logstash, but don't know where to start, you can use the GitHub labels "adoptme"
+and "low hanging fruit". Issues marked with these labels are relatively easy, and provides a good starting
+point to contribute to Logstash.
+
+See: https://github.com/elastic/logstash/labels/adoptme
+https://github.com/elastic/logstash/labels/low%20hanging%20fruit
 
 ## Contributing to plugins
 
@@ -61,5 +81,3 @@ Check our [documentation](https://www.elastic.co/guide/en/logstash/current/contr
    request](https://help.github.com/articles/using-pull-requests). In the pull
    request, describe what your changes do and mention any bugs/issues related
    to the pull request.
-
-
diff --git a/Gemfile b/Gemfile
index 679cbf93403..88b06d12dad 100644
--- a/Gemfile
+++ b/Gemfile
@@ -2,25 +2,118 @@
 # If you modify this file manually all comments and formatting will be lost.
 
 source "https://rubygems.org"
-gem "logstash-core", "3.0.0.dev", :path => "./logstash-core"
-# gem "logstash-core-event", "3.0.0.dev", :path => "./logstash-core-event"
-gem "logstash-core-event-java", "3.0.0.dev", :path => "./logstash-core-event-java"
-gem "logstash-core-plugin-api", "1.0.0", :path => "./logstash-core-plugin-api"
+gem "logstash-core", :path => "./logstash-core"
+gem "logstash-core-plugin-api", :path => "./logstash-core-plugin-api"
+gem "paquet", "~> 0.2.0"
+gem "ruby-progressbar", "~> 1.8.1"
+gem "builder", "~> 3.2.2"
 gem "file-dependencies", "0.1.6"
 gem "ci_reporter_rspec", "1.0.0", :group => :development
 gem "simplecov", :group => :development
-gem "coveralls", :group => :development
-# Tins 1.7 requires the ruby 2.0 platform to install,
-# this gem is a dependency of term-ansi-color which is a dependency of coveralls.
-# 1.6 is the last supported version on jruby.
 gem "tins", "1.6", :group => :development
 gem "rspec", "~> 3.1.0", :group => :development
-gem "logstash-devutils", "~> 0.0.15", :group => :development
+gem "logstash-devutils", :group => :development
 gem "benchmark-ips", :group => :development
 gem "octokit", "3.8.0", :group => :build
-gem "stud", "~> 0.0.21", :group => :build
+gem "stud", "~> 0.0.22", :group => :build
 gem "fpm", "~> 1.3.3", :group => :build
 gem "rubyzip", "~> 1.1.7", :group => :build
 gem "gems", "~> 0.8.3", :group => :build
 gem "rack-test", :require => "rack/test", :group => :development
 gem "flores", "~> 0.0.6", :group => :development
+gem "term-ansicolor", "~> 1.3.2", :group => :development
+gem "docker-api", "1.31.0", :group => :development
+gem "rest-client", "1.8.0", :group => :development
+gem "pleaserun", "~>0.0.28"
+gem "logstash-input-heartbeat"
+gem "logstash-codec-collectd"
+gem "logstash-output-xmpp"
+gem "logstash-codec-cef"
+gem "logstash-codec-dots"
+gem "logstash-codec-edn"
+gem "logstash-codec-edn_lines"
+gem "logstash-codec-fluent"
+gem "logstash-codec-es_bulk"
+gem "logstash-codec-graphite"
+gem "logstash-codec-json"
+gem "logstash-codec-json_lines"
+gem "logstash-codec-line"
+gem "logstash-codec-msgpack"
+gem "logstash-codec-multiline"
+gem "logstash-codec-netflow"
+gem "logstash-codec-plain"
+gem "logstash-codec-rubydebug"
+gem "logstash-filter-clone"
+gem "logstash-filter-csv"
+gem "logstash-filter-date"
+gem "logstash-filter-dns"
+gem "logstash-filter-drop"
+gem "logstash-filter-fingerprint"
+gem "logstash-filter-geoip"
+gem "logstash-filter-grok"
+gem "logstash-filter-json"
+gem "logstash-filter-kv"
+gem "logstash-filter-metrics"
+gem "logstash-filter-mutate"
+gem "logstash-filter-ruby"
+gem "logstash-filter-sleep"
+gem "logstash-filter-split"
+gem "logstash-filter-syslog_pri"
+gem "logstash-filter-throttle"
+gem "logstash-filter-urldecode"
+gem "logstash-filter-useragent"
+gem "logstash-filter-uuid"
+gem "logstash-filter-xml"
+gem "logstash-input-couchdb_changes"
+gem "logstash-input-elasticsearch"
+gem "logstash-input-exec"
+gem "logstash-input-file"
+gem "logstash-input-ganglia"
+gem "logstash-input-gelf"
+gem "logstash-input-generator"
+gem "logstash-input-graphite"
+gem "logstash-input-http"
+gem "logstash-input-http_poller"
+gem "logstash-input-imap"
+gem "logstash-input-irc"
+gem "logstash-input-jdbc"
+gem "logstash-input-log4j"
+gem "logstash-input-lumberjack"
+gem "logstash-input-pipe"
+gem "logstash-input-rabbitmq"
+gem "logstash-input-redis"
+gem "logstash-input-s3"
+gem "logstash-input-snmptrap"
+gem "logstash-input-sqs"
+gem "logstash-input-stdin"
+gem "logstash-input-syslog"
+gem "logstash-input-tcp"
+gem "logstash-input-twitter"
+gem "logstash-input-udp"
+gem "logstash-input-unix"
+gem "logstash-input-xmpp"
+gem "logstash-input-kafka", "~> 5"
+gem "logstash-input-beats", "~> 3.0", ">= 3.1.18"
+gem "logstash-output-cloudwatch"
+gem "logstash-output-csv"
+gem "logstash-output-elasticsearch"
+gem "logstash-output-file"
+gem "logstash-output-graphite"
+gem "logstash-output-http"
+gem "logstash-output-irc"
+gem "logstash-output-kafka", "~> 5"
+gem "logstash-output-nagios"
+gem "logstash-output-null"
+gem "logstash-output-pagerduty"
+gem "logstash-output-pipe"
+gem "logstash-output-rabbitmq"
+gem "logstash-output-redis"
+gem "logstash-output-s3"
+gem "logstash-output-sns"
+gem "logstash-output-sqs"
+gem "logstash-output-statsd"
+gem "logstash-output-stdout"
+gem "logstash-output-tcp"
+gem "logstash-output-udp"
+gem "logstash-output-webhdfs"
+gem "logstash-filter-dissect"
diff --git a/Gemfile.jruby-1.9.lock b/Gemfile.jruby-1.9.lock
index 69163d4626a..ac09246b37b 100644
--- a/Gemfile.jruby-1.9.lock
+++ b/Gemfile.jruby-1.9.lock
@@ -1,21 +1,21 @@
 PATH
   remote: ./logstash-core
   specs:
-    logstash-core (3.0.0.dev-java)
-      cabin (~> 0.8.0)
+    logstash-core (5.4.3-java)
       chronic_duration (= 0.10.6)
       clamp (~> 0.6.5)
-      concurrent-ruby (= 1.0.0)
+      concurrent-ruby (~> 1.0, >= 1.0.5)
       filesize (= 0.0.4)
       gems (~> 0.8.3)
       i18n (= 0.6.9)
-      jrjackson (~> 0.3.7)
-      jruby-monitoring (~> 0.1)
-      jruby-openssl (= 0.9.13)
-      logstash-core-event-java (~> 3.0.0.dev)
+      jar-dependencies
+      jrjackson (~> 0.4.0)
+      jrmonitor (~> 0.4.2)
+      jruby-openssl (= 0.9.16)
       minitar (~> 0.5.4)
       pry (~> 0.10.1)
-      puma (~> 2.15, >= 2.15.3)
+      puma (~> 2.16)
+      ruby-maven (~> 3.3.9)
       rubyzip (~> 1.1.7)
       sinatra (~> 1.4, >= 1.4.6)
       stud (~> 0.0.19)
@@ -23,11 +23,10 @@ PATH
       treetop (< 1.5.0)
 
 PATH
-  remote: ./logstash-core-event-java
+  remote: ./logstash-core-plugin-api
   specs:
-    logstash-core-event-java (3.0.0.dev-java)
-      jar-dependencies
-      ruby-maven (~> 3.3.9)
+    logstash-core-plugin-api (2.1.12-java)
+      logstash-core (= 5.4.3)
 
 GEM
   remote: https://rubygems.org/
@@ -35,11 +34,26 @@ GEM
     addressable (2.3.8)
     arr-pm (0.0.10)
       cabin (> 0)
-    backports (3.6.7)
-    benchmark-ips (2.3.0)
-    builder (3.2.2)
-    cabin (0.8.1)
-    childprocess (0.5.9)
+    atomic (1.1.99-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.8.0)
+    aws-sdk (2.3.22)
+      aws-sdk-resources (= 2.3.22)
+    aws-sdk-core (2.3.22)
+      jmespath (~> 1.0)
+    aws-sdk-resources (2.3.22)
+      aws-sdk-core (= 2.3.22)
+    aws-sdk-v1 (1.67.0)
+      json (~> 1.4)
+      nokogiri (~> 1)
+    backports (3.8.0)
+    benchmark-ips (2.7.2)
+    bindata (2.4.0)
+    buftok (0.2.0)
+    builder (3.2.3)
+    cabin (0.9.0)
+    childprocess (0.7.0)
       ffi (~> 1.0, >= 1.0.11)
     chronic_duration (0.10.6)
       numerizer (~> 0.1.1)
@@ -48,27 +62,38 @@ GEM
     ci_reporter_rspec (1.0.0)
       ci_reporter (~> 2.0)
       rspec (>= 2.14, < 4)
+    cinch (2.3.3)
     clamp (0.6.5)
-    coderay (1.1.0)
-    concurrent-ruby (1.0.0-java)
-    coveralls (0.8.10)
-      json (~> 1.8)
-      rest-client (>= 1.6.8, < 2)
-      simplecov (~> 0.11.0)
-      term-ansicolor (~> 1.3)
-      thor (~> 0.19.1)
-      tins (~> 1.6.0)
-    diff-lcs (1.2.5)
+    coderay (1.1.1)
+    concurrent-ruby (1.0.5-java)
+    diff-lcs (1.3)
     docile (1.1.5)
-    domain_name (0.5.20160128)
+    docker-api (1.31.0)
+      excon (>= 0.38.0)
+      json
+    domain_name (0.5.20170404)
       unf (>= 0.0.5, < 1.0.0)
+    dotenv (2.2.1)
+    edn (1.1.1)
+    elasticsearch (5.0.4)
+      elasticsearch-api (= 5.0.4)
+      elasticsearch-transport (= 5.0.4)
+    elasticsearch-api (5.0.4)
+      multi_json
+    elasticsearch-transport (5.0.4)
+      faraday
+      multi_json
+    equalizer (0.0.10)
+    excon (0.57.0)
     faraday (0.9.2)
       multipart-post (>= 1.2, < 3)
-    ffi (1.9.10-java)
+    ffi (1.9.18-java)
     file-dependencies (0.1.6)
       minitar
     filesize (0.0.4)
-    flores (0.0.6)
+    filewatch (0.9.0)
+    fivemat (1.3.5)
+    flores (0.0.7)
     fpm (1.3.3)
       arr-pm (~> 0.0.9)
       backports (>= 2.6.2)
@@ -77,48 +102,438 @@ GEM
       clamp (~> 0.6)
       ffi
       json (>= 1.7.7)
+    gelfd (0.2.0)
     gem_publisher (1.5.0)
     gems (0.8.3)
-    http-cookie (1.0.2)
+    hitimes (1.2.5-java)
+    http (0.9.9)
+      addressable (~> 2.3)
+      http-cookie (~> 1.0)
+      http-form_data (~> 1.0.1)
+      http_parser.rb (~> 0.6.0)
+    http-cookie (1.0.3)
       domain_name (~> 0.5)
+    http-form_data (1.0.1)
+    http_parser.rb (0.6.0-java)
     i18n (0.6.9)
     insist (1.0.0)
-    jar-dependencies (0.3.2)
-    jrjackson (0.3.8)
-    jruby-monitoring (0.3.0)
-    jruby-openssl (0.9.13-java)
-    json (1.8.3-java)
-    kramdown (1.9.0)
-    logstash-devutils (0.0.18-java)
+    jar-dependencies (0.3.11)
+    jls-grok (0.11.4)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.26)
+      concurrent-ruby
+    jmespath (1.3.1)
+    jrjackson (0.4.2-java)
+    jrmonitor (0.4.2)
+    jruby-openssl (0.9.16-java)
+    jruby-stdin-channel (0.2.0-java)
+    json (1.8.6-java)
+    kramdown (1.13.2)
+    logstash-codec-cef (4.1.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-collectd (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-dots (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn (3.0.2)
+      edn
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn_lines (3.0.2)
+      edn
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-es_bulk (3.0.3)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-fluent (3.1.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-graphite (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json_lines (3.0.2)
+      logstash-codec-line (>= 2.1.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-line (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-msgpack (3.0.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-multiline (3.0.3)
+      jls-grok (~> 0.11.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-codec-netflow (3.4.0)
+      bindata (>= 1.5.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-plain (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-rubydebug (3.0.2)
+      awesome_print
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-devutils (1.3.3-java)
+      fivemat
       gem_publisher
       insist (= 1.0.0)
       kramdown
+      logstash-core-plugin-api (>= 2.0, <= 2.99)
       minitar
       rake
-      rspec (~> 3.1.0)
+      rspec (~> 3.0)
       rspec-wait
       stud (>= 0.0.20)
+    logstash-filter-clone (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-csv (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-date (3.1.5)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-dissect (1.0.8)
+      jar-dependencies
+      logstash-core-plugin-api (>= 2.1.1, <= 2.99)
+    logstash-filter-dns (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+    logstash-filter-drop (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-fingerprint (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      murmurhash3
+    logstash-filter-geoip (4.1.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-grok (3.4.1)
+      jls-grok (~> 0.11.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+      stud (~> 0.0.22)
+    logstash-filter-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-kv (4.0.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-metrics (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      metriks
+      thread_safe
+    logstash-filter-mutate (3.1.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-ruby (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+    logstash-filter-sleep (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-split (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-syslog_pri (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-throttle (4.0.1)
+      atomic
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe
+    logstash-filter-urldecode (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-useragent (3.1.1-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-uuid (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-xml (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      nokogiri
+      xml-simple
+    logstash-input-beats (3.1.18-java)
+      concurrent-ruby (~> 1.0)
+      jar-dependencies (~> 0.3.4)
+      logstash-codec-multiline (>= 2.0.5)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe (~> 0.3.5)
+    logstash-input-couchdb_changes (3.1.1)
+      json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22)
+    logstash-input-elasticsearch (4.0.3)
+      elasticsearch (>= 5.0.3, < 6.0.0)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-exec (3.1.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-file (4.0.0)
+      addressable
+      filewatch (~> 0.8, >= 0.8.1)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-ganglia (3.1.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-gelf (3.0.2)
+      gelfd (= 0.2.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-generator (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-graphite (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-tcp
+    logstash-input-heartbeat (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-input-http (3.0.4)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      puma (~> 2.16, >= 2.16.0)
+      rack (~> 1)
+      stud
+    logstash-input-http_poller (3.3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 5.0.0, < 6.0.0)
+      rufus-scheduler (~> 3.0.9)
+      stud (~> 0.0.22)
+    logstash-input-imap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      mail (~> 2.6.3)
+      mime-types (= 2.6.2)
+      stud (~> 0.0.22)
+    logstash-input-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-jdbc (4.2.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      rufus-scheduler
+      sequel
+      tzinfo
+      tzinfo-data
+    logstash-input-kafka (5.1.7)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1.0)
+    logstash-input-log4j (3.0.5-java)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-lumberjack (3.1.1)
+      concurrent-ruby
+      jls-lumberjack (~> 0.0.26)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-rabbitmq (5.2.3)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.2.2, < 5.0.0)
+    logstash-input-redis (3.1.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+    logstash-input-s3 (3.1.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-input-snmptrap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snmp
+    logstash-input-sqs (3.0.3)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-input-stdin (3.2.2)
+      concurrent-ruby
+      jruby-stdin-channel
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-syslog (3.2.0)
+      concurrent-ruby
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+      logstash-filter-grok
+      stud (>= 0.0.22, < 0.1.0)
+      thread_safe
+    logstash-input-tcp (4.1.0)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-twitter (3.0.4)
+      http-form_data (<= 1.0.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      public_suffix (<= 1.4.6)
+      stud (>= 0.0.22, < 0.1)
+      twitter (= 5.15.0)
+    logstash-input-udp (3.1.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-unix (3.0.3)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-xmpp (3.1.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-mixin-aws (4.2.1)
+      aws-sdk (~> 2.3.0)
+      aws-sdk-v1 (>= 1.61.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-mixin-http_client (5.2.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.2, < 1.0.0)
+    logstash-mixin-rabbitmq_connection (4.3.0-java)
+      march_hare (~> 3.0.0)
+      stud (~> 0.0.22)
+    logstash-output-cloudwatch (3.0.4)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      rufus-scheduler (~> 3.0.9)
+    logstash-output-csv (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-json
+      logstash-input-generator
+      logstash-output-file
+    logstash-output-elasticsearch (7.3.5-java)
+      cabin (~> 0.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.4, < 1.0.0)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-file (4.0.1)
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-core-plugin-api (>= 2.0.0, < 2.99)
+    logstash-output-graphite (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-http (4.3.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 5.1.0, < 6.0.0)
+    logstash-output-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-kafka (5.1.6)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-nagios (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-null (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pagerduty (3.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-rabbitmq (4.0.7-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.3.0, < 5.0.0)
+    logstash-output-redis (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+      stud
+    logstash-output-s3 (4.0.7)
+      concurrent-ruby
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.22)
+    logstash-output-sns (4.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-sqs (4.0.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-statsd (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-generator
+      statsd-ruby (= 1.2.0)
+    logstash-output-stdout (3.1.0)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60.1, < 2.99)
+    logstash-output-tcp (4.0.0)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-output-udp (3.0.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-webhdfs (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snappy (= 0.0.12)
+      webhdfs
+    logstash-output-xmpp (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-patterns-core (4.1.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    lru_redux (1.1.0)
+    mail (2.6.6)
+      mime-types (>= 1.16, < 4)
+    manticore (0.6.1-java)
+    march_hare (3.0.0-java)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
     method_source (0.8.2)
-    mime-types (2.99)
+    metriks (0.9.9.8)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.6.2)
     minitar (0.5.4)
+    msgpack-jruby (1.4.1-java)
+    multi_json (1.12.1)
     multipart-post (2.0.0)
+    murmurhash3 (0.1.6-java)
+    mustache (0.99.8)
+    naught (1.1.0)
     netrc (0.11.0)
+    nokogiri (1.8.0-java)
     numerizer (0.1.1)
     octokit (3.8.0)
       sawyer (~> 0.6.0, >= 0.5.3)
+    paquet (0.2.1)
+    pleaserun (0.0.29)
+      cabin (> 0)
+      clamp
+      dotenv
+      insist
+      mustache (= 0.99.8)
+      stud
     polyglot (0.3.5)
-    pry (0.10.3-java)
+    pry (0.10.4-java)
       coderay (~> 1.1.0)
       method_source (~> 0.8.1)
       slop (~> 3.4)
       spoon (~> 0.0)
+    public_suffix (1.4.6)
     puma (2.16.0-java)
-    rack (1.6.4)
+    rack (1.6.8)
     rack-protection (1.5.3)
       rack
     rack-test (0.6.3)
       rack (>= 1.0)
-    rake (10.5.0)
+    rake (12.0.0)
+    redis (3.3.3)
     rest-client (1.8.0)
       http-cookie (>= 1.0.2, < 2.0)
       mime-types (>= 1.16, < 3.0)
@@ -135,57 +550,183 @@ GEM
     rspec-mocks (3.1.3)
       rspec-support (~> 3.1.0)
     rspec-support (3.1.2)
-    rspec-wait (0.0.8)
-      rspec (>= 2.11, < 3.5)
-    ruby-maven (3.3.9)
-      ruby-maven-libs (~> 3.3.1)
-    ruby-maven-libs (3.3.3)
+    rspec-wait (0.0.9)
+      rspec (>= 3, < 4)
+    ruby-maven (3.3.12)
+      ruby-maven-libs (~> 3.3.9)
+    ruby-maven-libs (3.3.9)
+    ruby-progressbar (1.8.1)
     rubyzip (1.1.7)
+    rufus-scheduler (3.0.9)
+      tzinfo
     sawyer (0.6.0)
       addressable (~> 2.3.5)
       faraday (~> 0.8, < 0.10)
-    simplecov (0.11.2)
+    sequel (4.47.0)
+    simple_oauth (0.3.1)
+    simplecov (0.14.1)
       docile (~> 1.1.0)
-      json (~> 1.8)
+      json (>= 1.8, < 3)
       simplecov-html (~> 0.10.0)
-    simplecov-html (0.10.0)
-    sinatra (1.4.7)
+    simplecov-html (0.10.1)
+    sinatra (1.4.8)
       rack (~> 1.5)
       rack-protection (~> 1.4)
       tilt (>= 1.3, < 3)
     slop (3.6.0)
-    spoon (0.0.4)
+    snappy (0.0.12-java)
+      snappy-jars (~> 1.1.0)
+    snappy-jars (1.1.0.1.2-java)
+    snmp (1.2.0)
+    spoon (0.0.6)
       ffi
+    statsd-ruby (1.2.0)
     stud (0.0.22)
     term-ansicolor (1.3.2)
       tins (~> 1.0)
-    thor (0.19.1)
-    thread_safe (0.3.5-java)
-    tilt (2.0.2)
+    thread_safe (0.3.6-java)
+    tilt (2.0.7)
     tins (1.6.0)
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
+    twitter (5.15.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (= 0.0.10)
+      faraday (~> 0.9.0)
+      http (>= 0.4, < 0.10)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.3)
+      thread_safe (~> 0.1)
+    tzinfo-data (1.2017.2)
+      tzinfo (>= 1.0.0)
     unf (0.1.4-java)
+    webhdfs (0.8.0)
+      addressable
+    xml-simple (1.1.5)
+    xmpp4r (0.5)
 
 PLATFORMS
   java
 
 DEPENDENCIES
   benchmark-ips
+  builder (~> 3.2.2)
   ci_reporter_rspec (= 1.0.0)
-  coveralls
+  docker-api (= 1.31.0)
   file-dependencies (= 0.1.6)
   flores (~> 0.0.6)
   fpm (~> 1.3.3)
   gems (~> 0.8.3)
-  logstash-core (= 3.0.0.dev)!
-  logstash-core-event-java (= 3.0.0.dev)!
-  logstash-devutils (~> 0.0.15)
+  logstash-codec-cef
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-es_bulk
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-core!
+  logstash-core-plugin-api!
+  logstash-devutils
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dissect
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-uuid
+  logstash-filter-xml
+  logstash-input-beats (~> 3.0, >= 3.1.18)
+  logstash-input-couchdb_changes
+  logstash-input-elasticsearch
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-heartbeat
+  logstash-input-http
+  logstash-input-http_poller
+  logstash-input-imap
+  logstash-input-irc
+  logstash-input-jdbc
+  logstash-input-kafka (~> 5)
+  logstash-input-log4j
+  logstash-input-lumberjack
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-input-xmpp
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-file
+  logstash-output-graphite
+  logstash-output-http
+  logstash-output-irc
+  logstash-output-kafka (~> 5)
+  logstash-output-nagios
+  logstash-output-null
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-statsd
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-webhdfs
+  logstash-output-xmpp
   octokit (= 3.8.0)
+  paquet (~> 0.2.0)
+  pleaserun (~> 0.0.28)
   rack-test
+  rest-client (= 1.8.0)
   rspec (~> 3.1.0)
+  ruby-progressbar (~> 1.8.1)
   rubyzip (~> 1.1.7)
   simplecov
-  stud (~> 0.0.21)
+  stud (~> 0.0.22)
+  term-ansicolor (~> 1.3.2)
   tins (= 1.6)
diff --git a/LICENSE b/LICENSE
index 43976b73b2b..40f7cd2fb3e 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,4 +1,4 @@
-Copyright (c) 2012â€“2016 Elasticsearch <http://www.elastic.co>
+Copyright (c) 2012â€“2017 Elasticsearch <http://www.elastic.co>
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
diff --git a/Makefile b/Makefile
deleted file mode 100644
index db263b80aff..00000000000
--- a/Makefile
+++ /dev/null
@@ -1,2 +0,0 @@
-%: 
-	rake $@
diff --git a/NOTICE.TXT b/NOTICE.TXT
index 0b8a9475f05..d9714074c61 100644
--- a/NOTICE.TXT
+++ b/NOTICE.TXT
@@ -1,5 +1,599 @@
-Elasticsearch
-Copyright 2012-2015 Elasticsearch
+Logstash
+Copyright 2012-2017 Elasticsearch
+
+This product includes software developed by The Apache Software Foundation (http://www.apache.org/).
+
+==========================================================================
+Third party libraries bundled by the Logstash project:
+
+==========================================================================
+RubyGem: edn Version: 1.1.1
+Copyright (c) 2012 Relevance Inc & Clinton N. Dreisbach
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: awesome_print Version: 1.8.0
+Copyright (c) 2010-2013 Michael Dvorkin
+http://www.dvorkin.net
+%w(mike dvorkin.net) * "@" || "twitter.com/mid"
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: jar-dependencies Version: 0.3.11
+Copyright (c) 2014 Christian Meier
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: lru_redux Version: 1.1.0
+Copyright (c) 2013 Sam Saffron
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: elasticsearch Version: 5.0.4
+   Copyright 2013 Elasticsearch
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+==========================================================================
+RubyGem: addressable Version: 2.3.8
+
+                              Apache License
+                        Version 2.0, January 2004
+                     http://www.apache.org/licenses/
+
+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+1. Definitions.
+
+   "License" shall mean the terms and conditions for use, reproduction,
+   and distribution as defined by Sections 1 through 9 of this document.
+
+   "Licensor" shall mean the copyright owner or entity authorized by
+   the copyright owner that is granting the License.
+
+   "Legal Entity" shall mean the union of the acting entity and all
+   other entities that control, are controlled by, or are under common
+   control with that entity. For the purposes of this definition,
+   "control" means (i) the power, direct or indirect, to cause the
+   direction or management of such entity, whether by contract or
+   otherwise, or (ii) ownership of fifty percent (50%) or more of the
+   outstanding shares, or (iii) beneficial ownership of such entity.
+
+   "You" (or "Your") shall mean an individual or Legal Entity
+   exercising permissions granted by this License.
+
+   "Source" form shall mean the preferred form for making modifications,
+   including but not limited to software source code, documentation
+   source, and configuration files.
+
+   "Object" form shall mean any form resulting from mechanical
+   transformation or translation of a Source form, including but
+   not limited to compiled object code, generated documentation,
+   and conversions to other media types.
+
+   "Work" shall mean the work of authorship, whether in Source or
+   Object form, made available under the License, as indicated by a
+   copyright notice that is included in or attached to the work
+   (an example is provided in the Appendix below).
+
+   "Derivative Works" shall mean any work, whether in Source or Object
+   form, that is based on (or derived from) the Work and for which the
+   editorial revisions, annotations, elaborations, or other modifications
+   represent, as a whole, an original work of authorship. For the purposes
+   of this License, Derivative Works shall not include works that remain
+   separable from, or merely link (or bind by name) to the interfaces of,
+   the Work and Derivative Works thereof.
+
+   "Contribution" shall mean any work of authorship, including
+   the original version of the Work and any modifications or additions
+   to that Work or Derivative Works thereof, that is intentionally
+   submitted to Licensor for inclusion in the Work by the copyright owner
+   or by an individual or Legal Entity authorized to submit on behalf of
+   the copyright owner. For the purposes of this definition, "submitted"
+   means any form of electronic, verbal, or written communication sent
+   to the Licensor or its representatives, including but not limited to
+   communication on electronic mailing lists, source code control systems,
+   and issue tracking systems that are managed by, or on behalf of, the
+   Licensor for the purpose of discussing and improving the Work, but
+   excluding communication that is conspicuously marked or otherwise
+   designated in writing by the copyright owner as "Not a Contribution."
+
+   "Contributor" shall mean Licensor and any individual or Legal Entity
+   on behalf of whom a Contribution has been received by Licensor and
+   subsequently incorporated within the Work.
+
+2. Grant of Copyright License. Subject to the terms and conditions of
+   this License, each Contributor hereby grants to You a perpetual,
+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+   copyright license to reproduce, prepare Derivative Works of,
+   publicly display, publicly perform, sublicense, and distribute the
+   Work and such Derivative Works in Source or Object form.
+
+3. Grant of Patent License. Subject to the terms and conditions of
+   this License, each Contributor hereby grants to You a perpetual,
+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+   (except as stated in this section) patent license to make, have made,
+   use, offer to sell, sell, import, and otherwise transfer the Work,
+   where such license applies only to those patent claims licensable
+   by such Contributor that are necessarily infringed by their
+   Contribution(s) alone or by combination of their Contribution(s)
+   with the Work to which such Contribution(s) was submitted. If You
+   institute patent litigation against any entity (including a
+   cross-claim or counterclaim in a lawsuit) alleging that the Work
+   or a Contribution incorporated within the Work constitutes direct
+   or contributory patent infringement, then any patent licenses
+   granted to You under this License for that Work shall terminate
+   as of the date such litigation is filed.
+
+4. Redistribution. You may reproduce and distribute copies of the
+   Work or Derivative Works thereof in any medium, with or without
+   modifications, and in Source or Object form, provided that You
+   meet the following conditions:
+
+   (a) You must give any other recipients of the Work or
+       Derivative Works a copy of this License; and
+
+   (b) You must cause any modified files to carry prominent notices
+       stating that You changed the files; and
+
+   (c) You must retain, in the Source form of any Derivative Works
+       that You distribute, all copyright, patent, trademark, and
+       attribution notices from the Source form of the Work,
+       excluding those notices that do not pertain to any part of
+       the Derivative Works; and
+
+   (d) If the Work includes a "NOTICE" text file as part of its
+       distribution, then any Derivative Works that You distribute must
+       include a readable copy of the attribution notices contained
+       within such NOTICE file, excluding those notices that do not
+       pertain to any part of the Derivative Works, in at least one
+       of the following places: within a NOTICE text file distributed
+       as part of the Derivative Works; within the Source form or
+       documentation, if provided along with the Derivative Works; or,
+       within a display generated by the Derivative Works, if and
+       wherever such third-party notices normally appear. The contents
+       of the NOTICE file are for informational purposes only and
+       do not modify the License. You may add Your own attribution
+       notices within Derivative Works that You distribute, alongside
+       or as an addendum to the NOTICE text from the Work, provided
+       that such additional attribution notices cannot be construed
+       as modifying the License.
+
+   You may add Your own copyright statement to Your modifications and
+   may provide additional or different license terms and conditions
+   for use, reproduction, or distribution of Your modifications, or
+   for any such Derivative Works as a whole, provided Your use,
+   reproduction, and distribution of the Work otherwise complies with
+   the conditions stated in this License.
+
+5. Submission of Contributions. Unless You explicitly state otherwise,
+   any Contribution intentionally submitted for inclusion in the Work
+   by You to the Licensor shall be under the terms and conditions of
+   this License, without any additional terms or conditions.
+   Notwithstanding the above, nothing herein shall supersede or modify
+   the terms of any separate license agreement you may have executed
+   with Licensor regarding such Contributions.
+
+6. Trademarks. This License does not grant permission to use the trade
+   names, trademarks, service marks, or product names of the Licensor,
+   except as required for reasonable and customary use in describing the
+   origin of the Work and reproducing the content of the NOTICE file.
+
+7. Disclaimer of Warranty. Unless required by applicable law or
+   agreed to in writing, Licensor provides the Work (and each
+   Contributor provides its Contributions) on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+   implied, including, without limitation, any warranties or conditions
+   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+   PARTICULAR PURPOSE. You are solely responsible for determining the
+   appropriateness of using or redistributing the Work and assume any
+   risks associated with Your exercise of permissions under this License.
+
+8. Limitation of Liability. In no event and under no legal theory,
+   whether in tort (including negligence), contract, or otherwise,
+   unless required by applicable law (such as deliberate and grossly
+   negligent acts) or agreed to in writing, shall any Contributor be
+   liable to You for damages, including any direct, indirect, special,
+   incidental, or consequential damages of any character arising as a
+   result of this License or out of the use or inability to use the
+   Work (including but not limited to damages for loss of goodwill,
+   work stoppage, computer failure or malfunction, or any and all
+   other commercial damages or losses), even if such Contributor
+   has been advised of the possibility of such damages.
+
+9. Accepting Warranty or Additional Liability. While redistributing
+   the Work or Derivative Works thereof, You may choose to offer,
+   and charge a fee for, acceptance of support, warranty, indemnity,
+   or other liability obligations and/or rights consistent with this
+   License. However, in accepting such obligations, You may act only
+   on Your own behalf and on Your sole responsibility, not on behalf
+   of any other Contributor, and only if You agree to indemnify,
+   defend, and hold each Contributor harmless for any liability
+   incurred by, or claims asserted against, such Contributor by reason
+   of your accepting any such warranty or additional liability.
+
+END OF TERMS AND CONDITIONS
+
+APPENDIX: How to apply the Apache License to your work.
+
+   To apply the Apache License to your work, attach the following
+   boilerplate notice, with the fields enclosed by brackets "[]"
+   replaced with your own identifying information. (Don't include
+   the brackets!)  The text should be enclosed in the appropriate
+   comment syntax for the file format. We also recommend that a
+   file or class name and description of purpose be included on the
+   same "printed page" as the copyright notice for easier
+   identification within third-party archives.
+
+Copyright [yyyy] [name of copyright owner]
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+==========================================================================
+RubyGem: rufus-scheduler Version: 3.0.9
+
+Copyright (c) 2005-2014, John Mettraux, jmettraux@gmail.com
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
+
+
+==========================================================================
+RubyGem: mail Version: 2.6.6
+Copyright (c) 2009-2017 Mikel Lindsaar
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+'Software'), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: cinch Version: 2.3.3
+Copyright (c) 2010 Lee Jarvis, Dominik Honnef
+Copyright (c) 2011 Dominik Honnef
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: sequel Version: 4.47.0
+Copyright (c) 2007-2008 Sharon Rosner
+Copyright (c) 2008-2017 Jeremy Evans
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to
+deal in the Software without restriction, including without limitation the
+rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+sell copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+  
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+   
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER 
+IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: tzinfo Version: 1.2.3
+Copyright (c) 2005-2017 Philip Ross
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of 
+this software and associated documentation files (the "Software"), to deal in 
+the Software without restriction, including without limitation the rights to 
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
+of the Software, and to permit persons to whom the Software is furnished to do 
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all 
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR 
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER 
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, 
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN 
+THE SOFTWARE.
+
+==========================================================================
+RubyGem: tzinfo-data Version: 1.2017.2
+Copyright (c) 2005-2017 Philip Ross
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of 
+this software and associated documentation files (the "Software"), to deal in 
+the Software without restriction, including without limitation the rights to 
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
+of the Software, and to permit persons to whom the Software is furnished to do 
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all 
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR 
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER 
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, 
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN 
+THE SOFTWARE.
+
+==========================================================================
+RubyGem: redis Version: 3.3.3
+Copyright (c) 2009 Ezra Zygmuntowicz
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+==========================================================================
+RubyGem: twitter Version: 5.15.0
+Copyright (c) 2006-2015 Erik Michaels-Ober, John Nunemaker, Wynn Netherland, Steve Richert, Steve Agalloco
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: http-form_data Version: 1.0.1
+Copyright (c) 2015 Aleksey V Zapparov
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: public_suffix Version: 1.4.6
+Copyright (c) 2009-2014 Simone Carletti <weppos@weppos.net>
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: cabin Version: 0.9.0
+Copyright 2011 Jordan Sissel
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+
+==========================================================================
+RubyGem: statsd-ruby Version: 1.2.0
+Copyright (c) 2011, 2012, 2013 Rein Henrichs
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
\ No newline at end of file
diff --git a/README.md b/README.md
index 92229c33dcb..b5555f4f8cb 100644
--- a/README.md
+++ b/README.md
@@ -1,35 +1,42 @@
-# Logstash [![Code Climate](https://codeclimate.com/github/elasticsearch/logstash/badges/gpa.svg)](https://codeclimate.com/github/elasticsearch/logstash) [![Coverage Status](https://coveralls.io/repos/elasticsearch/logstash/badge.svg?branch=origin%2Fmaster)](https://coveralls.io/r/elasticsearch/logstash?branch=origin%2Fmaster)
+# Logstash
 
 ### Build status
 
-| Branch | master | 2.x | 2.1
+| Test | master | 5.0 | 2.4 |
 |---|---|---|---|
-| core | [![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_2x/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_2x/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_21/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_21/) |
-| integration | [![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_2x/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_2x/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_21/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_21/) |
+| core | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=master)](https://travis-ci.org/elastic/logstash) | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=5.0)](https://travis-ci.org/elastic/logstash) | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=2.4)](https://travis-ci.org/elastic/logstash) |
 
-Logstash is a tool for managing events and logs. You can use it to collect
-logs, parse them, and store them for later use (like, for searching).  If you
-store them in [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html),
-you can view and analyze them with [Kibana](https://www.elastic.co/guide/en/kibana/current/index.html).
+Logstash is part of the [Elastic Stack](https://www.elastic.co/products) along with Beats, Elasticsearch and Kibana. Logstash is an open source, server-side data processing pipeline that ingests data from a multitude of sources simultaneously, transforms it, and then sends it to your favorite "stash." (Ours is Elasticsearch, naturally.). Logstash has over 200 plugins, and you can write your own very easily as well.
 
-It is fully free and fully open source. The license is Apache 2.0, meaning you
-are pretty much free to use it however you want in whatever way.
+The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
 
 For more info, see <https://www.elastic.co/products/logstash>
 
+## Documentation and Getting Started
+
+You can find the documentation and getting started guides for Logstash 
+on the [elastic.co site](https://www.elastic.co/guide/en/logstash/current/getting-started-with-logstash.html)
+
+## Downloads
+
+You can download Logstash binaries, as well as debian/rpm packages for the
+supported platforms, from [downloads page](https://www.elastic.co/downloads/logstash).
+
+## Need Help?
+
+- [Logstash Forum](https://discuss.elastic.co/c/logstash)
+- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)
+- [#logstash on freenode IRC](https://webchat.freenode.net/?channels=logstash)
+- [Logstash Product Information](https://www.elastic.co/products/logstash)
+- [Elastic Support](https://www.elastic.co/subscriptions)
+
 ## Logstash Plugins
-### AKA "Where'd that plugin go??"
 
-Since version **1.5.0 beta1 (and current master)** of Logstash, *all* plugins have been separated into their own
-repositories under the [logstash-plugins](https://github.com/logstash-plugins) github organization. Each plugin is now a self-contained Ruby gem which
-gets published to RubyGems.org. Logstash has added plugin infrastructure to easily maintain the lifecyle of the plugin.
-For more details and rationale behind these changes, see our [blogpost](https://www.elastic.co/blog/plugin-ecosystem-changes/).
+Logstash plugins are hosted in separate repositories under the [logstash-plugins](https://github.com/logstash-plugins) github organization. Each plugin is a self-contained Ruby gem which gets published to RubyGems.org.
 
-[Elasticsearch logstash-contrib repo](https://github.com/elastic/logstash-contrib) is deprecated. We
-have moved all of the plugins that existed there into their own repositories. We are migrating all of the pull requests
-and issues from logstash-contrib to the new repositories.
+### Writing your own Plugin
 
-For more info on developing and testing these plugins, please see the [README](https://github.com/logstash-plugins/logstash-output-elasticsearch/blob/master/README.md) on *any* plugin repository.
+Logstash is known for its extensibility. There are hundreds of plugins for Logstash and you can write your own very easily! For more info on developing and testing these plugins, please see the [working with plugins section](https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html)
 
 ### Plugin Issues and Pull Requests
 
@@ -39,48 +46,62 @@ For example, if you have to report an issue/enhancement for the Elasticsearch ou
 
 Logstash core will continue to exist under this repository and all related issues and pull requests can be submitted here.
 
-## Need Help?
+## Developing Logstash Core
 
-- [#logstash on freenode IRC](https://webchat.freenode.net/?channels=logstash)
-- [logstash-users on Google Groups](https://groups.google.com/d/forum/logstash-users)
-- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)
-- [Logstash Product Information](https://www.elastic.co/products/logstash)
-- [Elastic Support](https://www.elastic.co/subscriptions)
+### Prerequisites
 
-## Developing
+* Install JDK version 8
+* Install JRuby 1.7.x.
+* Install `rake` and `bundler` tool using `gem install rake` and `gem install bundler` respectively.
 
-Logstash uses [JRuby](http://jruby.org/) which gets embedded in the `vendor/jruby/` directory. It is recommended but not mandatory that you also use JRuby as your local Ruby interpreter and for this you should consider using a Ruby version manager such as [RVM](https://rvm.io/) or [rbenv](https://github.com/sstephenson/rbenv). It is possible to run the rake tasks and the `bin/` commands without having JRuby locally installed in which case the embedded JRuby will be used automatically. If you have a local JRuby installed you can force logstash to use your local JRuby instead of the embedded JRuby with the `USE_RUBY=1` environment variable.
+**On Windows** make sure to set the `JAVA_HOME` environment variable to the path to your JDK installation directory. For example `set JAVA_HOME=<JDK_PATH>`
 
-To get started, make sure you have a local JRuby or Ruby version 1.9.x or above with the `rake` tool installed.
+**Vendored JRuby**: Logstash uses [JRuby](http://jruby.org/) which gets embedded in the `vendor/jruby/` directory. It is recommended to use a Ruby version manager such as [RVM](https://rvm.io/) or [rbenv](https://github.com/sstephenson/rbenv).
 
-**On Windows** make sure to set the `JAVA_HOME` environment variable to the path to your JDK installation directory. For example `set JAVA_HOME=<JDK_PATH>`
+* To run Logstash from the repo you must first bootstrap the environment:
 
-To run logstash from the repo you must bootstrap the environment
+```sh
+rake bootstrap
+```
+    
+* You can then use `bin/logstash` to start Logstash, but there are no plugins installed. Logstash ships with default plugins. To install those, you can run:
 
-    rake bootstrap
+```sh
+rake plugin:install-default
+```
 
-or bootstrap & install the core plugins required to run the tests
+* Alternatively, you can only install the core plugins required to run the tests
 
-    rake test:install-core
+```sh
+rake test:install-core
+```
+
+To verify your environment, run
+
+```sh
+bin/logstash -e 'input { stdin { } } output { stdout {} }'
+```
 
-To verify your environment, run `bin/logstash version` which should look like this
+This should start Logstash with stdin input waiting for you to enter an event
 
-    $ bin/logstash version
-    logstash 2.0.0.dev
+```sh
+hello world
+2016-11-11T01:22:14.405+0000 0.0.0.0 hello world
+```
 
-If you are seeing errors that look like
+**Drip Launcher**
 
-    $ rake bootstrap
-    Installing minitar >= 0 because the build process needs it.
-    [bootstrap] Fetching and installing gem: minitar (>= 0)
-    rake aborted!
-    LoadError: no such file to load -- archive/tar/minitar
-    /Users/<user>/projects/logstash/rakelib/vendor.rake:17:in `untar'
-    /Users/<user>/projects/logstash/rakelib/vendor.rake:86:in `(root)'
-    Tasks: TOP => bootstrap => vendor:all => vendor:jruby
-    (See full trace by running task with --trace)
+[Drip](https://github.com/ninjudd/drip) is a tool that solves the slow JVM startup problem while developing Logstash. The drip script is intended to be a drop-in replacement for the java command. We recommend using drip during development, in particular for running tests. Using drip, the first invocation of a command will not be faster but the subsequent commands will be swift.
+
+To tell logstash to use drip, either set the `USE_DRIP=1` environment variable or set `` JAVACMD=`which drip` ``.
 
-then you may need to update your version of rubygems. Run `gem -v` to see the version of rubygems installed. Version 2.5.2 or higher should work. To update rubygems run `gem update --system` (you may need to run with `sudo` if you're using your system Ruby environment).
+Example:
+
+    USE_DRIP=1 bin/rspec
+
+**Caveats**
+
+Drip does not work with STDIN. You cannot use drip for running configs which use the stdin plugin.
 
 ## Testing
 
@@ -116,43 +137,24 @@ You can install the default set of plugins included in the logstash package or a
 Note that if a plugin is installed using the plugin manager `bin/logstash-plugin install ...` do not forget to also install the plugins development dependencies using the following command after the plugin installation:
 
     bin/logstash-plugin install --development
+    
+## Building Artifacts
 
-## Developing plugins
-
-The documentation for developing plugins can be found in the plugins README, see our example plugins:
-
-- <https://github.com/logstash-plugins/logstash-input-example>
-- <https://github.com/logstash-plugins/logstash-filter-example>
-- <https://github.com/logstash-plugins/logstash-output-example>
-- <https://github.com/logstash-plugins/logstash-codec-example>
-
-## Drip Launcher
-
-[Drip](https://github.com/ninjudd/drip) is a tool that solves the slow JVM startup problem. The drip script is intended to be a drop-in replacement for the java command. We recommend using drip during development, in particular for running tests. Using drip, the first invocation of a command will not be faster but the subsequent commands will be swift.
-
-To tell logstash to use drip, either set the `USE_DRIP=1` environment variable or set `` JAVACMD=`which drip` ``.
-
-Examples:
-
-    USE_DRIP=1 bin/rspec
-    USE_DRIP=1 bin/rspec
-
-**Caveats**
-
-Drip does not work with STDIN. You cannot use drip for running configs which use the stdin plugin.
-
-
-## Building
+You can build a Logstash snapshot package as tarball or zip file
 
-You can build a logstash package as tarball or zip file
+```sh
+rake artifact:tar
+rake artifact:zip
+```
 
-    rake artifact:tar
-    rake artifact:zip
+This will create the artifact `LS_HOME/build` directory
 
 You can also build .rpm and .deb, but the [fpm](https://github.com/jordansissel/fpm) tool is required.
 
-    rake artifact:rpm
-    rake artifact:deb
+```sh
+rake artifact:rpm
+rake artifact:deb
+```
 
 ## Project Principles
 
@@ -172,4 +174,4 @@ see that here.
 It is more important to me that you are able to contribute.
 
 For more information about contributing, see the
-[CONTRIBUTING](CONTRIBUTING.md) file.
+[CONTRIBUTING](./CONTRIBUTING.md) file.
diff --git a/ROADMAP.md b/ROADMAP.md
new file mode 100644
index 00000000000..9583b301af1
--- /dev/null
+++ b/ROADMAP.md
@@ -0,0 +1,15 @@
+## Logstash Roadmap
+
+[Logstash](https://www.elastic.co/products/logstash "Logstash") is an open-source data collection engine, developed directly on Github and distributed under the Apache License 2.0. The roadmap is defined by the core themes of performance, resiliency, and manageability, along with the overall plugin ecosystem. User requirements are the main driving force behind our development efforts. If a user has a bad time, itâ€™s a bug!  All submitted [issues](https://github.com/elastic/logstash/issues/new "New Issue"), suggestions, or ideas are greatly encouraged and appreciated.
+
+### Logstash Core
+
+With an open development model, the core roadmap can generally be distilled in Github. Long term roadmap features can be viewed with the "[roadmap](https://github.com/elastic/logstash/labels/roadmap "Roadmap Features")" label. The features and bug fixes targeted for an upcoming release can be viewed with the specific release label, e.g. "[v5.0.0](https://github.com/elastic/logstash/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3Av5.0.0 "Logstash 5.0 Release")". Please note that feature timing and priorities are susceptible to change and therefore not guaranteed.
+
+### Logstash Plugins
+
+The [Logstash Plugins](https://github.com/logstash-plugins "Logstash Plugins") ecosystem enables the innovation of additional integrations and processing capabilities for the core engine. Plugins in this organization are either maintained by Elastic or the community. Community maintained plugins have a special banner in the documentation page, e.g. [Salesforce input](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-salesforce.html "Salesforce Input Plugin"). Many awesome humans have taken the roadmap into their own hands by becoming community maintainers. If youâ€™re actively working with specific community plugins and would like to get more involved, feel free to reach out in this [forum thread](https://discuss.elastic.co/t/logstash-plugins-community-maintainers/35953 "Community Maintainers").
+
+## Latest Changes
+
+For a list of latest changes across Logstash and its plugins, see the [release notes](https://www.elastic.co/guide/en/logstash/current/releasenotes.html "Current Release Notes").
diff --git a/Rakefile b/Rakefile
index 12b4a262110..b853e472a51 100644
--- a/Rakefile
+++ b/Rakefile
@@ -15,7 +15,9 @@ Packaging?
   `rake artifact:deb`  to build an deb
 
 Developing?
-  `rake bootstrap`     installs any dependencies for doing Logstash development
-  `rake vendor:clean`  clean vendored dependencies used for Logstash development
+  `rake bootstrap`          installs any dependencies for doing Logstash development
+  `rake test:install-core`  installs any dependencies for testing Logstasch core
+  `rake test:core`          to run Logstasch core tests
+  `rake vendor:clean`       clean vendored dependencies used for Logstash development
 HELP
 end
diff --git a/acceptance_spec/acceptance/install_spec.rb b/acceptance_spec/acceptance/install_spec.rb
deleted file mode 100644
index 45efc1bf6fb..00000000000
--- a/acceptance_spec/acceptance/install_spec.rb
+++ /dev/null
@@ -1,95 +0,0 @@
-require_relative '../spec_helper_acceptance'
-
-branch = ENV['LS_BRANCH'] || 'master'
-build_url = 'https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash'
-
-describe "Logstash class:" do
-
-  case fact('osfamily')
-  when 'RedHat'
-    core_package_name    = 'logstash'
-    service_name         = 'logstash'
-    core_url             = "#{build_url}/#{branch}/nightly/JDK7/logstash-latest-SNAPSHOT.rpm"
-    pid_file             = '/var/run/logstash.pid'
-  when 'Debian'
-    core_package_name    = 'logstash'
-    service_name         = 'logstash'
-    core_url             = "#{build_url}/#{branch}/nightly/JDK7/logstash-latest-SNAPSHOT.deb"
-    pid_file             = '/var/run/logstash.pid'
-  end
-
-  context "Install Nightly core package" do
-
-    it 'should run successfully' do
-      pp = "class { 'logstash': package_url => '#{core_url}', java_install => true }
-            logstash::configfile { 'basic_config': content => 'input { tcp { port => 2000 } } output { stdout { } } ' }
-           "
-
-      # Run it twice and test for idempotency
-      apply_manifest(pp, :catch_failures => true)
-      sleep 20
-      expect(apply_manifest(pp, :catch_failures => true).exit_code).to be_zero
-
-    end
-
-    describe package(core_package_name) do
-      it { should be_installed }
-    end
-
-    describe service(service_name) do
-      it { should be_enabled }
-      it { should be_running }
-    end
-
-    describe file(pid_file) do
-      it { should be_file }
-      its(:content) { should match /[0-9]+/ }
-    end
-
-    describe port(2000) do
-      it {
-        sleep 30
-        should be_listening
-      }
-    end
-
-  end
-
-  context "ensure we are still running" do
-
-    describe service(service_name) do
-      it {
-        sleep 30
-        should be_running
-      }
-    end
-
-    describe port(2000) do
-      it { should be_listening }
-    end
-
-  end
-
-  describe "module removal" do
-
-    it 'should run successfully' do
-      pp = "class { 'logstash': ensure => 'absent' }"
-
-      # Run it twice and test for idempotency
-      apply_manifest(pp, :catch_failures => true)
-
-    end
-
-    describe service(service_name) do
-      it { should_not be_enabled }
-      it { should_not be_running }
-    end
-
-    describe package(core_package_name) do
-      it { should_not be_installed }
-    end
-
-  end
-
-end
-
diff --git a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml b/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
deleted file mode 100644
index 4f33d28f788..00000000000
--- a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  centos-6-x64:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: el-6-x86_64
-    image: electrical/centos:6.4
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'yum install -y wget ntpdate rubygems ruby-augeas ruby-devel augeas-devel logrotate'
-      - 'touch /etc/sysconfig/network'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml b/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
deleted file mode 100644
index 7b2df7423e5..00000000000
--- a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  debian-6:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: debian-6-amd64
-    image: electrical/debian:6.0.8
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
-      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml b/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
deleted file mode 100644
index 704b6c7d424..00000000000
--- a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  debian-7:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: debian-7-amd64
-    image: electrical/debian:7.3
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
-      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
deleted file mode 100644
index 4d6879e74b0..00000000000
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-HOSTS:
-  ubuntu-12-04:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: ubuntu-12.04-amd64
-    image: electrical/ubuntu:12.04
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl logrotate'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
deleted file mode 100644
index 0f6e1772f29..00000000000
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-HOSTS:
-  ubuntu-14-04:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: ubuntu-14.04-amd64
-    image: electrical/ubuntu:14.04
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq ruby ruby1.9.1-dev libaugeas-dev libaugeas-ruby lsb-release wget net-tools curl logrotate'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/spec_helper_acceptance.rb b/acceptance_spec/spec_helper_acceptance.rb
deleted file mode 100644
index 752743b4aa5..00000000000
--- a/acceptance_spec/spec_helper_acceptance.rb
+++ /dev/null
@@ -1,70 +0,0 @@
-require 'beaker-rspec'
-require 'pry'
-require 'securerandom'
-
-files_dir = ENV['files_dir'] || '/home/jenkins/puppet'
-
-proxy_host = ENV['BEAKER_PACKAGE_PROXY'] || ''
-
-if !proxy_host.empty?
-  gem_proxy = "http_proxy=#{proxy_host}" unless proxy_host.empty?
-
-  hosts.each do |host|
-    on host, "echo 'export http_proxy='#{proxy_host}'' >> /root/.bashrc"
-    on host, "echo 'export https_proxy='#{proxy_host}'' >> /root/.bashrc"
-    on host, "echo 'export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,#{host.name}\"' >> /root/.bashrc"
-  end
-else
-  gem_proxy = ''
-end
-
-hosts.each do |host|
-  # Install Puppet
-  if host.is_pe?
-    install_pe
-  else
-    puppetversion = ENV['VM_PUPPET_VERSION']
-    on host, "#{gem_proxy} gem install puppet --no-ri --no-rdoc --version '~> #{puppetversion}'"
-    on host, "mkdir -p #{host['distmoduledir']}"
-
-    if fact('osfamily') == 'Suse'
-      install_package host, 'rubygems ruby-devel augeas-devel libxml2-devel'
-      on host, "#{gem_proxy} gem install ruby-augeas --no-ri --no-rdoc"
-    end
-
-  end
-
-  # on debian/ubuntu nodes ensure we get the latest info
-  # Can happen we have stalled data in the images
-  if fact('osfamily') == 'Debian'
-    on host, "apt-get update"
-  end
-
-end
-
-RSpec.configure do |c|
-  # Project root
-  proj_root = File.expand_path(File.join(File.dirname(__FILE__), '..'))
-
-  # Readable test descriptions
-  c.formatter = :documentation
-
-  # Configure all nodes in nodeset
-  c.before :suite do
-    # Install module and dependencies
-
-    hosts.each do |host|
-
-      on host, puppet('module','install','elasticsearch-logstash'), { :acceptable_exit_codes => [0,1] }
-
-      if fact('osfamily') == 'Debian'
-        scp_to(host, "#{files_dir}/puppetlabs-apt-1.4.2.tar.gz", '/tmp/puppetlabs-apt-1.4.2.tar.gz')
-        on host, puppet('module','install','/tmp/puppetlabs-apt-1.4.2.tar.gz'), { :acceptable_exit_codes => [0,1] }
-      end
-      if fact('osfamily') == 'Suse'
-        on host, puppet('module','install','darin-zypprepo'), { :acceptable_exit_codes => [0,1] }
-      end
-
-    end
-  end
-end
diff --git a/bin/cpdump b/bin/cpdump
new file mode 100755
index 00000000000..5bbca5122c8
--- /dev/null
+++ b/bin/cpdump
@@ -0,0 +1,11 @@
+#!/usr/bin/env bin/ruby
+
+require_relative "../lib/bootstrap/environment"
+LogStash::Bundler.setup!({:without => [:build, :development]})
+require "logstash-core"
+require "logstash/environment"
+require "logstash/settings"
+
+io = Java::OrgLogstashCommonIo::FileCheckpointIO.new(LogStash::SETTINGS.get_value("path.queue"))
+cp = io.read(ARGV[0])
+puts("checkpoint #{cp.toString}")
diff --git a/bin/lock b/bin/lock
new file mode 100755
index 00000000000..a8a0529a943
--- /dev/null
+++ b/bin/lock
@@ -0,0 +1,9 @@
+#!/usr/bin/env bin/ruby
+
+require_relative "../lib/bootstrap/environment"
+LogStash::Bundler.setup!({:without => [:build, :development]})
+require "logstash-core"
+
+lock = Java::OrgLogstash::FileLockFactory.getDefault.obtainLock(ARGV[0], ".lock")
+puts("locking " + File.join(ARGV[0], ".lock"))
+sleep
diff --git a/bin/logstash b/bin/logstash
index 978bc8e112b..ef2a5d2b70d 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -6,12 +6,12 @@
 # Usage:
 #   bin/logstash <command> [arguments]
 #
-# See 'bin/logstash help' for a list of commands.
+# See 'bin/logstash --help' for a list of commands.
 #
 # Supported environment variables:
-#   LS_HEAP_SIZE="xxx" size for the -Xmx${LS_HEAP_SIZE} maximum Java heap size option, default is "1g"
+#   LS_JVM_OPTS="xxx" path to file with JVM options
 #   LS_JAVA_OPTS="xxx" to append extra options to the defaults JAVA_OPTS provided by logstash
-#   JAVA_OPTS="xxx" to *completely override* the defauls set of JAVA_OPTS provided by logstash
+#   JAVA_OPTS="xxx" to *completely override* the default set of JAVA_OPTS provided by logstash
 #
 # Development environment variables:
 #   USE_RUBY=1 to force use the local "ruby" command to launch logstash instead of using the vendored JRuby
@@ -21,16 +21,16 @@
 unset CDPATH
 # This unwieldy bit of scripting is to try to catch instances where Logstash
 # was launched from a symlink, rather than a full path to the Logstash binary
-if [ -L $0 ]; then
+if [ -L "$0" ]; then
   # Launched from a symlink
   # --Test for the readlink binary
-  RL=$(which readlink)
+  RL="$(which readlink)"
   if [ $? -eq 0 ]; then
     # readlink exists
-    SOURCEPATH=$($RL $0)
+    SOURCEPATH="$($RL $0)"
   else
     # readlink not found, attempt to parse the output of stat
-    SOURCEPATH=$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\â€˜//' -e 's/\â€™//')
+    SOURCEPATH="$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\â€˜//' -e 's/\â€™//')"
     if [ $? -ne 0 ]; then
       # Failed to execute or parse stat
       echo "Failed to find source library at path $(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
@@ -40,10 +40,17 @@ if [ -L $0 ]; then
   fi
 else
   # Not a symlink
-  SOURCEPATH=$0
+  SOURCEPATH="$0"
 fi
 
 . "$(cd `dirname $SOURCEPATH`/..; pwd)/bin/logstash.lib.sh"
 setup
 
-ruby_exec "${LOGSTASH_HOME}/lib/bootstrap/environment.rb" "logstash/runner.rb" "$@"
+if [ "$1" = "-V" ] || [ "$1" = "--version" ];
+then
+  LOGSTASH_VERSION_FILE="${LOGSTASH_HOME}/logstash-core/lib/logstash/version.rb"
+  LOGSTASH_VERSION="$(sed -ne 's/^LOGSTASH_VERSION = "\([^*]*\)"$/\1/p' $LOGSTASH_VERSION_FILE)"
+  echo "logstash $LOGSTASH_VERSION"
+else
+  ruby_exec "${LOGSTASH_HOME}/lib/bootstrap/environment.rb" "logstash/runner.rb" "$@"
+fi
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index c2058e5d5c7..76aa335f5ef 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -1,16 +1,16 @@
 unset CDPATH
 # This unwieldy bit of scripting is to try to catch instances where Logstash
 # was launched from a symlink, rather than a full path to the Logstash binary
-if [ -L $0 ]; then
+if [ -L "$0" ]; then
   # Launched from a symlink
   # --Test for the readlink binary
-  RL=$(which readlink)
+  RL="$(which readlink)"
   if [ $? -eq 0 ]; then
     # readlink exists
-    SOURCEPATH=$($RL $0)
+    SOURCEPATH="$($RL $0)"
   else
     # readlink not found, attempt to parse the output of stat
-    SOURCEPATH=$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\â€˜//' -e 's/\â€™//')
+    SOURCEPATH="$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\â€˜//' -e 's/\â€™//')"
     if [ $? -ne 0 ]; then
       # Failed to execute or parse stat
       echo "Failed to set LOGSTASH_HOME from $(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
@@ -20,14 +20,39 @@ if [ -L $0 ]; then
   fi
 else
   # Not a symlink
-  SOURCEPATH=$0
+  SOURCEPATH="$0"
 fi
 
-LOGSTASH_HOME=$(cd `dirname $SOURCEPATH`/..; pwd)
+LOGSTASH_HOME="$(cd `dirname $SOURCEPATH`/..; pwd)"
 export LOGSTASH_HOME
+SINCEDB_DIR="${LOGSTASH_HOME}"
+export SINCEDB_DIR
+
+# This block will iterate over the command-line args Logstash was started with
+# It will find the argument _after_ --path.settings and use that to attempt
+# to derive the location of an acceptable jvm.options file
+# It will do nothing if this is not found.
+# This fix is for #6379
+if [ -z "$LS_JVM_OPTS" ]; then
+  found=0
+  for i in "$@"; do
+     if [ $found -eq 1 ]; then
+       if [ -r "${i}/jvm.options" ]; then
+         export LS_JVM_OPTS="${i}/jvm.options"
+         break
+       fi
+     fi
+     if [ "$i" = "--path.settings" ]; then
+       found=1
+     fi
+  done
+fi
 
-# Defaults you can override with environment variables
-LS_HEAP_SIZE="${LS_HEAP_SIZE:=1g}"
+parse_jvm_options() {
+  if [ -f "$1" ]; then
+    echo "$(grep "^-" "$1" | tr '\n' ' ')"
+  fi
+}
 
 setup_java() {
   if [ -z "$JAVACMD" ] ; then
@@ -40,7 +65,7 @@ setup_java() {
 
   # Resolve full path to the java command.
   if [ ! -f "$JAVACMD" ] ; then
-    JAVACMD=$(which $JAVACMD 2>/dev/null)
+    JAVACMD="$(which $JAVACMD 2>/dev/null)"
   fi
 
   if [ ! -x "$JAVACMD" ] ; then
@@ -50,21 +75,27 @@ setup_java() {
 
   if [ "$JAVA_OPTS" ] ; then
     echo "WARNING: Default JAVA_OPTS will be overridden by the JAVA_OPTS defined in the environment. Environment JAVA_OPTS are $JAVA_OPTS"  1>&2
-  else
-    # There are no JAVA_OPTS set from the client, we set a predefined
-    # set of options that think are good in general
-    JAVA_OPTS="-XX:+UseParNewGC"
-    JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
-    JAVA_OPTS="$JAVA_OPTS -Djava.awt.headless=true"
+  fi
+
+  # Set a default GC log file for use by jvm.options _before_ it's called.
+  if [ -z "$LS_GC_LOG_FILE" ] ; then
+    LS_GC_LOG_FILE="./logstash-gc.log"
+  fi
 
-    JAVA_OPTS="$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75"
-    JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly"
-    # Causes the JVM to dump its heap on OutOfMemory.
-    JAVA_OPTS="$JAVA_OPTS -XX:+HeapDumpOnOutOfMemoryError"
-    # The path to the heap dump location
-    # This variable needs to be isolated since it may contain spaces
-    HEAP_DUMP_PATH="-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof"
+  # Set the initial JVM options from the jvm.options file.  Look in
+  # /etc/logstash first, and break if that file is found readable there.
+  if [ -z "$LS_JVM_OPTS" ]; then
+      for jvm_options in /etc/logstash/jvm.options \
+                        "$LOGSTASH_HOME"/config/jvm.options;
+                         do
+          if [ -r "$jvm_options" ]; then
+              LS_JVM_OPTS=$jvm_options
+              break
+          fi
+      done
   fi
+  # use the defaults, first, then override with anything provided
+  LS_JAVA_OPTS="$(parse_jvm_options "$LS_JVM_OPTS") $LS_JAVA_OPTS"
 
   if [ "$LS_JAVA_OPTS" ] ; then
     # The client set the variable LS_JAVA_OPTS, choosing his own
@@ -72,23 +103,6 @@ setup_java() {
     JAVA_OPTS="$JAVA_OPTS $LS_JAVA_OPTS"
   fi
 
-  if [ "$LS_HEAP_SIZE" ] ; then
-    JAVA_OPTS="$JAVA_OPTS -Xmx${LS_HEAP_SIZE}"
-  fi
-
-  if [ "$LS_USE_GC_LOGGING" ] ; then
-    if [ -z "$LS_GC_LOG_FILE" ] ; then
-      LS_GC_LOG_FILE="./logstash-gc.log"
-    fi
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCDetails"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCTimeStamps"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintClassHistogram"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintTenuringDistribution"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCApplicationStoppedTime"
-    JAVA_OPTS="$JAVA_OPTS -Xloggc:${LS_GC_LOG_FILE}"
-    echo "Writing garbage collection logs to ${LS_GC_LOG_FILE}"
-  fi
-
   export JAVACMD
   export JAVA_OPTS
 }
@@ -100,7 +114,7 @@ setup_drip() {
 
   # resolve full path to the drip command.
   if [ ! -f "$JAVACMD" ] ; then
-    JAVACMD=$(which $JAVACMD 2>/dev/null)
+    JAVACMD="$(which $JAVACMD 2>/dev/null)"
   fi
 
   if [ ! -x "$JAVACMD" ] ; then
@@ -113,7 +127,11 @@ setup_drip() {
   if [ "$USE_RUBY" = "1" ] ; then
     export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify"
   else
-    JAVA_OPTS="$JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+    if [ -z "$JAVA_OPTS" ] ; then
+      LS_JAVA_OPTS="$LS_JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+    else
+      JAVA_OPTS="$JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+    fi
   fi
   export JAVACMD
   export DRIP_INIT_CLASS="org.jruby.main.DripMain"
@@ -137,16 +155,9 @@ setup_ruby() {
   VENDORED_JRUBY=
 }
 
-jruby_opts() {
-  printf "%s" "--1.9"
-  for i in $JAVA_OPTS ; do
-    printf "%s" " -J$i"
-  done
-}
-
 setup() {
   # first check if we want to use drip, which can be used in vendored jruby mode
-  # and also when setting USE_RUBY=1 if the ruby interpretor is in fact jruby
+  # and also when setting USE_RUBY=1 if the ruby interpreter is in fact jruby
   if [ "$JAVACMD" ] ; then
     if [ "$(basename $JAVACMD)" = "drip" ] ; then
       DRIP_JAVACMD=1
@@ -179,8 +190,8 @@ ruby_exec() {
     # $VENDORED_JRUBY is non-empty so use the vendored JRuby
 
     if [ "$DEBUG" ] ; then
-      echo "DEBUG: exec ${JRUBY_BIN} $(jruby_opts) "-J$HEAP_DUMP_PATH" $@"
+      echo "DEBUG: exec ${JRUBY_BIN} $@"
     fi
-    exec "${JRUBY_BIN}" $(jruby_opts) "-J$HEAP_DUMP_PATH" "$@"
+    exec "${JRUBY_BIN}" "$@"
   fi
 }
diff --git a/bin/ruby b/bin/ruby
new file mode 100755
index 00000000000..b1fd647ce26
--- /dev/null
+++ b/bin/ruby
@@ -0,0 +1,24 @@
+#!/bin/sh
+# Run a ruby script using the logstash jruby launcher
+#
+# Usage:
+#   bin/ruby [arguments]
+#
+# Supported environment variables:
+#   LS_JVM_OPTS="xxx" path to file with JVM options
+#   LS_JAVA_OPTS="xxx" to append extra options to the defaults JAVA_OPTS provided by logstash
+#   JAVA_OPTS="xxx" to *completely override* the default set of JAVA_OPTS provided by logstash
+#
+# Development environment variables:
+#   USE_RUBY=1 to force use the local "ruby" command to launch logstash instead of using the vendored JRuby
+#   DEBUG=1 to output debugging information
+
+# use faster starting JRuby options see https://github.com/jruby/jruby/wiki/Improving-startup-time
+export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1"
+
+unset CDPATH
+
+. "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+setup
+
+ruby_exec "$@"
diff --git a/bin/setup.bat b/bin/setup.bat
index 40993179168..6d336f0b945 100644
--- a/bin/setup.bat
+++ b/bin/setup.bat
@@ -15,6 +15,12 @@ goto finally
 
 :setup_jruby
 REM setup_java()
+IF NOT DEFINED JAVA_HOME (
+  FOR %%I IN (java.exe) DO set JAVA_EXE=%%~$PATH:I
+)
+if defined JAVA_EXE set JAVA_HOME=%JAVA_EXE:\bin\java.exe=%
+if defined JAVA_EXE echo Using JAVA_HOME=%JAVA_HOME% retrieved from PATH
+
 if not defined JAVA_HOME goto missing_java_home
 REM ***** JAVA options *****
 
@@ -24,7 +30,9 @@ if "%LS_HEAP_SIZE%" == "" (
 
 IF NOT "%JAVA_OPTS%" == "" (
     ECHO JAVA_OPTS was set to [%JAVA_OPTS%]. Logstash will trust these options, and not set any defaults that it might usually set
-) ELSE (
+    goto opts_defined
+)
+
     SET JAVA_OPTS=%JAVA_OPTS% -Xmx%LS_HEAP_SIZE%
 
     REM Enable aggressive optimizations in the JVM
@@ -51,8 +59,9 @@ IF NOT "%JAVA_OPTS%" == "" (
     SET JAVA_OPTS=%JAVA_OPTS% -XX:+HeapDumpOnOutOfMemoryError
     REM The path to the heap dump location, note directory must exists and have enough
     REM space for a full heap dump.
-    SET JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="$LS_HOME/heapdump.hprof"
-)
+    SET JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="%LS_HOME%/heapdump.hprof"
+:opts_defined
+
 
 IF NOT "%LS_JAVA_OPTS%" == "" (
     ECHO LS_JAVA_OPTS was set to [%LS_JAVA_OPTS%]. This will be appended to the JAVA_OPTS [%JAVA_OPTS%]
diff --git a/bin/system-install b/bin/system-install
new file mode 100755
index 00000000000..f85a536b080
--- /dev/null
+++ b/bin/system-install
@@ -0,0 +1,97 @@
+#!/bin/bash
+
+unset CDPATH
+. "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+setup
+
+if [ -z "$1" ]; then
+  if [ -r /etc/logstash/startup.options ]; then
+    OPTIONS_PATH=/etc/logstash/startup.options
+  elif [ -r "${LOGSTASH_HOME}"/config/startup.options ]; then
+    OPTIONS_PATH="${LOGSTASH_HOME}"/config/startup.options
+  fi
+elif [ "$1" == "-h" ] || [ "$1" == "--help" ]; then
+  echo "Usage: system-install [OPTIONSFILE] [STARTUPTYPE] [VERSION]"
+  echo
+  echo "NOTE: These arguments are ordered, and co-dependent"
+  echo
+  echo "OPTIONSFILE: Full path to a startup.options file"
+  echo "OPTIONSFILE is required if STARTUPTYPE is specified, but otherwise looks first"
+  echo "in $LOGSTASH_HOME/config/startup.options and then /etc/logstash/startup.options"
+  echo "Last match wins"
+  echo
+  echo "STARTUPTYPE: e.g. sysv, upstart, systemd, etc."
+  echo "OPTIONSFILE is required to specify a STARTUPTYPE."
+  echo
+  echo "VERSION: The specified version of STARTUPTYPE to use.  The default is usually"
+  echo "preferred here, so it can safely be omitted."
+  echo "Both OPTIONSFILE & STARTUPTYPE are required to specify a VERSION."
+  echo
+  echo "For more information, see https://github.com/jordansissel/pleaserun"
+  exit 0
+else
+  if [ -r "$1" ]; then
+    echo "Using provided startup.options file: ${1}"
+    OPTIONS_PATH="$1"
+  else
+    echo "$1 is not a file path"
+    echo "To manually specify a startup style, put the path to startup.options as the "
+    echo "first argument, followed by the startup style (sysv, upstart, systemd)"
+    exit 1
+  fi
+fi
+
+# Read in the env vars in the selected startup.options file...
+. "${OPTIONS_PATH}"
+
+old_IFS=$IFS
+IFS=$'\n'
+lines=($(grep -v ^# ${OPTIONS_PATH} | tr -d '"' | grep -v '^LS_OPTS=' | grep \= | grep -v '\=$' | grep -v '\=\"\"$'))
+IFS=$old_IFS
+
+ENV_VAR_ARGS=()
+
+for line in ${lines[@]}; do
+  var=$(echo $line | awk -F\= '{print $1}')
+  if [ "x${!var}" != "x" ]; then
+    ENV_VAR_ARGS+=('--environment-variables')
+    ENV_VAR_ARGS+=("${var}=${!var}")
+  fi
+done
+
+# bin/logstash-plugin is a short lived ruby script thus we can use aggressive "faster starting JRuby options"
+# see https://github.com/jruby/jruby/wiki/Improving-startup-time
+export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify -X-C -Xcompile.invokedynamic=false"
+
+tempfile=$(mktemp)
+if [ "x${PRESTART}" == "x" ]; then
+  opts=("--log" "$tempfile" "--overwrite" "--install" "--name" "${SERVICE_NAME}" "--user" "${LS_USER}" "--group" "${LS_GROUP}" "--description" "${SERVICE_DESCRIPTION}" "--nice" "${LS_NICE}" "--limit-open-files" "${LS_OPEN_FILES}")
+else
+  opts=("--log" "$tempfile" "--overwrite" "--install" "--name" "${SERVICE_NAME}" "--user" "${LS_USER}" "--group" "${LS_GROUP}" "--description" "${SERVICE_DESCRIPTION}" "--nice" "${LS_NICE}" "--limit-open-files" "${LS_OPEN_FILES}" "--prestart" "${PRESTART}")
+fi
+
+if [[ $2 ]]; then
+  echo "Manually creating startup for specified platform: ${2}"
+  opts+=('--platform')
+  opts+=($2)
+fi
+
+if [[ $3 ]]; then
+  echo "Manually creating startup for specified platform (${2}) version: ${3}"
+  opts+=('--version')
+  opts+=($3)
+fi
+
+allopts=("${ENV_VAR_ARGS[@]}" "${opts[@]}")
+program="$(cd `dirname $0`/..; pwd)/bin/logstash"
+
+$(ruby_exec "${LOGSTASH_HOME}/lib/systeminstall/pleasewrap.rb" "${allopts[@]}" ${program} ${LS_OPTS})
+exit_code=$?
+
+if [ $exit_code -ne 0 ]; then
+  cat $tempfile
+  echo "Unable to install system startup script for Logstash."
+else
+  echo "Successfully created system startup script for Logstash"
+fi
+rm $tempfile
diff --git a/bot/check_pull_changelog.rb b/bot/check_pull_changelog.rb
deleted file mode 100644
index 7e8ac7e1f21..00000000000
--- a/bot/check_pull_changelog.rb
+++ /dev/null
@@ -1,89 +0,0 @@
-require "octokit"
-##
-# This script will validate that any pull request submitted against a github 
-# repository will contains changes to CHANGELOG file.
-#
-# If not the case, an helpful text will be commented on the pull request
-# If ok, a thanksful message will be commented also containing a @mention to 
-# acts as a trigger for review notification by a human.
-## 
-
-
-@bot="" # Put here your bot github username
-@password="" # Put here your bot github password
-
-@repository="logstash/logstash"
-@mention="@jordansissel"
-
-@missing_changelog_message = <<MISSING_CHANGELOG
-Hello, I'm #{@bot}, I'm here to help you accomplish your pull request submission quest
-
-You still need to accomplish these tasks:
-
-* Please add a changelog information
-
-Also note that your pull request name will appears in the details section 
-of the release notes, so please make it clear
-MISSING_CHANGELOG
-
-@ok_changelog_message = <<OK_CHANGELOG
-You successfully completed the pre-requisite quest (aka updating CHANGELOG)
-
-Also note that your pull request name will appears in the details section 
-of the release notes, so please make it clear, if not already done.
-
-#{@mention} Dear master, would you please have a look to this humble request
-OK_CHANGELOG
-
-#Connect to Github
-@client=Octokit::Client.new(:login => @bot, :password => @password)
-
-
-#For each open pull
-Octokit.pull_requests(@repository).each do |pull|
-  #Get botComment
-  botComment = nil
-  @client.issue_comments(@repository, pull.number, {
-    :sort => "created",
-    :direction => "desc"
-  }).each do |comment|
-    if comment.user.login == @bot
-      botComment = comment
-      break
-    end
-  end
-
-  if !botComment.nil? and botComment.body.start_with?("[BOT-OK]")
-    #Pull already validated by bot, nothing to do
-    puts "Pull request #{pull.number}, already ok for bot"
-  else
-    #Firt encounter, or previous [BOT-WARN] status
-    #Check for changelog
-    warnOnMissingChangeLog = true
-    @client.pull_request_files(@repository, pull.number).each do |changedFile|
-      if changedFile.filename  == "CHANGELOG"
-        if changedFile.additions.to_i > 0
-          #Changelog looks good
-          warnOnMissingChangeLog = false
-        else
-          #No additions, means crazy deletion
-          warnOnMissingChangeLog = true
-        end
-      end
-    end
-    if warnOnMissingChangeLog
-      if botComment.nil?
-        puts "Pull request #{pull.number}, adding bot warning"
-        @client.add_comment(@repository, pull.number, "[BOT-WARN] #{@missing_changelog_message}")
-      else
-        puts "Pull request #{pull.number}, already warned, no changes yet"
-      end
-    else
-      if !botComment.nil?
-        @client.delete_comment(@repository,botComment.id)
-      end
-      puts "Pull request #{pull.number}, adding bot ok"
-      @client.add_comment(@repository, pull.number, "[BOT-OK] #{@ok_changelog_message}")
-    end
-  end
-end
diff --git a/build.gradle b/build.gradle
new file mode 100644
index 00000000000..4475bd5d68b
--- /dev/null
+++ b/build.gradle
@@ -0,0 +1 @@
+apply plugin: 'idea'
diff --git a/ci/ci_acceptance.sh b/ci/ci_acceptance.sh
new file mode 100755
index 00000000000..5665fe5cc4a
--- /dev/null
+++ b/ci/ci_acceptance.sh
@@ -0,0 +1,49 @@
+#!/usr/bin/env bash
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+SELECTED_TEST_SUITE=$1
+
+if [[ $SELECTED_TEST_SUITE == $"redhat" ]]; then
+  echo "Generating the RPM, make sure you start with a clean environment before generating other packages."
+  rake artifact:rpm
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["redhat"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:redhat
+  bundle exec rake qa:vm:halt["redhat"]
+elif [[ $SELECTED_TEST_SUITE == $"debian" ]]; then
+  echo "Generating the DEB, make sure you start with a clean environment before generating other packages."
+  rake artifact:deb
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["debian"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:debian
+  bundle exec rake qa:vm:halt["debian"]
+elif [[ $SELECTED_TEST_SUITE == $"all" ]]; then
+  echo "Building Logstash artifacts"
+  rake artifact:all
+
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:all
+  bundle exec rake qa:vm:halt
+  cd ..
+fi
diff --git a/ci/ci_docs.sh b/ci/ci_docs.sh
new file mode 100755
index 00000000000..80c77505fac
--- /dev/null
+++ b/ci/ci_docs.sh
@@ -0,0 +1,23 @@
+#!/usr/bin/env bash
+set -e
+
+output_dir=$1
+
+if [[ -z $output_dir ]]; then
+  echo "Docs will be generated in default directory in LS_HOME/build/docs"
+else
+  echo "Docs will be generated in directory $output_dir"
+fi
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+mkdir -p build/docs
+rm -rf build/docs/*
+
+grep -q -F "logstash-docgen" Gemfile || echo 'gem "logstash-docgen", :path => "./tools/logstash-docgen"' >> Gemfile
+rake bootstrap
+rake test:install-core
+rake docs:generate-plugins[$output_dir]
diff --git a/ci/ci_docs_master.sh b/ci/ci_docs_master.sh
new file mode 100755
index 00000000000..d337d2894fa
--- /dev/null
+++ b/ci/ci_docs_master.sh
@@ -0,0 +1,15 @@
+#!/usr/bin/env bash
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+mkdir -p build/docs
+rm -rf build/docs/*
+rm -rf tools/logstash-docgen/source
+
+cd tools/logstash-docgen
+bundle install
+bin/logstash-docgen --all --target ../../build/docs
diff --git a/ci/ci_integration.sh b/ci/ci_integration.sh
index 139408fefc9..d8b6fa6b886 100755
--- a/ci/ci_integration.sh
+++ b/ci/ci_integration.sh
@@ -1,3 +1,24 @@
-#!/bin/sh
-rake test:install-default
-rake test:integration
+#!/usr/bin/env bash
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+echo "Running integration tests from qa/integration"
+if [[ ! -d "build" ]]; then
+  mkdir build
+fi  
+rm -rf build/*  
+echo "Building logstash tar file in build/"
+rake artifact:tar
+cd build
+echo "Extracting logstash tar file in build/"
+tar xf *.tar.gz
+
+cd ../qa/integration
+# to install test dependencies
+bundle install
+# runs all tests
+rspec
diff --git a/ci/ci_setup.sh b/ci/ci_setup.sh
index fea695cb2c5..9974055658d 100755
--- a/ci/ci_setup.sh
+++ b/ci/ci_setup.sh
@@ -1,8 +1,9 @@
 #!/usr/bin/env bash
+set -e
 
 ##
 # Note this setup needs a system ruby to be available, this can not
-# be done here as is higly system dependant.
+# be done here as is highly system dependant.
 ##
 
 #squid proxy work, so if there is a proxy it can be cached.
@@ -13,6 +14,11 @@ rm -rf vendor       # make sure there are no vendorized dependencies
 rm -rf .bundle
 rm -rf spec/reports # no stale spec reports from previous executions
 
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
 # Setup the environment
 rake bootstrap # Bootstrap your logstash instance
 
diff --git a/ci/ci_test.sh b/ci/ci_test.sh
index c0eadda6424..a7f62d151bb 100755
--- a/ci/ci_test.sh
+++ b/ci/ci_test.sh
@@ -1,10 +1,16 @@
 #!/usr/bin/env bash
+set -e
 
 ##
 # Keep in mind to run ci/ci_setup.sh if you need to setup/clean up your environment before
 # running the test suites here.
 ##
 
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
 SELECTED_TEST_SUITE=$1
 
 if [[ $SELECTED_TEST_SUITE == $"core-fail-fast" ]]; then
diff --git a/ci/travis_integration_install.sh b/ci/travis_integration_install.sh
new file mode 100755
index 00000000000..27d3a097f6e
--- /dev/null
+++ b/ci/travis_integration_install.sh
@@ -0,0 +1,26 @@
+#!/usr/bin/env bash
+set -e
+
+# This file sets up the environment for travis integration tests
+
+
+if [[ "$INTEGRATION" != "true" ]]; then
+    exit
+fi
+
+echo "Setting up integration tests"
+if [[ ! -d "build" ]]; then
+    mkdir build
+fi
+rm -rf build/*
+echo "Building logstash tar file in build/"
+rake artifact:tar
+cd build
+echo "Extracting logstash tar file in build/"
+tar xf *.tar.gz
+
+cd ../qa/integration
+pwd
+echo $BUNDLE_GEMFILE
+# to install test dependencies
+bundle install --gemfile="./Gemfile"
diff --git a/ci/travis_integration_run.sh b/ci/travis_integration_run.sh
new file mode 100755
index 00000000000..28b8cc9f760
--- /dev/null
+++ b/ci/travis_integration_run.sh
@@ -0,0 +1,16 @@
+#!/usr/bin/env bash
+set -e
+
+if [[ "$INTEGRATION" != "true" ]]; then
+    exit
+fi
+
+echo "Running integration tests from qa/integration directory"
+cd qa/integration
+
+# The offline specs can break the online ones
+# due to some sideeffects of the seccomp policy interfering with
+# the docker daemon
+# See prepare_offline_pack_spec.rb for details
+rspec --tag ~offline
+rspec --tag offline
diff --git a/config/jvm.options b/config/jvm.options
new file mode 100644
index 00000000000..68abc1ad17e
--- /dev/null
+++ b/config/jvm.options
@@ -0,0 +1,74 @@
+## JVM configuration
+
+# Xms represents the initial size of total heap space
+# Xmx represents the maximum size of total heap space
+
+-Xms256m
+-Xmx1g
+
+################################################################
+## Expert settings
+################################################################
+##
+## All settings below this section are considered
+## expert settings. Don't tamper with them unless
+## you understand what you are doing
+##
+################################################################
+
+## GC configuration
+-XX:+UseParNewGC
+-XX:+UseConcMarkSweepGC
+-XX:CMSInitiatingOccupancyFraction=75
+-XX:+UseCMSInitiatingOccupancyOnly
+
+## optimizations
+
+# disable calls to System#gc
+-XX:+DisableExplicitGC
+
+## Locale
+# Set the locale language
+#-Duser.language=en
+
+# Set the locale country
+#-Duser.country=US
+
+# Set the locale variant, if any
+#-Duser.variant=
+
+## basic
+
+# set the I/O temp directory
+#-Djava.io.tmpdir=$HOME
+
+# set to headless, just in case
+-Djava.awt.headless=true
+
+# ensure UTF-8 encoding by default (e.g. filenames)
+-Dfile.encoding=UTF-8
+
+# use our provided JNA always versus the system one
+#-Djna.nosys=true
+
+## heap dumps
+
+# generate a heap dump when an allocation from the Java heap fails
+# heap dumps are created in the working directory of the JVM
+-XX:+HeapDumpOnOutOfMemoryError
+
+# specify an alternative path for heap dumps
+# ensure the directory exists and has sufficient space
+#-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof
+
+## GC logging
+#-XX:+PrintGCDetails
+#-XX:+PrintGCTimeStamps
+#-XX:+PrintGCDateStamps
+#-XX:+PrintClassHistogram
+#-XX:+PrintTenuringDistribution
+#-XX:+PrintGCApplicationStoppedTime
+
+# log GC status to a file with time stamps
+# ensure the directory exists
+#-Xloggc:${LS_GC_LOG_FILE}
diff --git a/config/log4j2.properties b/config/log4j2.properties
new file mode 100644
index 00000000000..ac9273b64a1
--- /dev/null
+++ b/config/log4j2.properties
@@ -0,0 +1,83 @@
+status = error
+name = LogstashPropertiesConfig
+
+appender.console.type = Console
+appender.console.name = plain_console
+appender.console.layout.type = PatternLayout
+appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n
+
+appender.json_console.type = Console
+appender.json_console.name = json_console
+appender.json_console.layout.type = JSONLayout
+appender.json_console.layout.compact = true
+appender.json_console.layout.eventEol = true
+
+appender.rolling.type = RollingFile
+appender.rolling.name = plain_rolling
+appender.rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.rolling.policies.type = Policies
+appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.rolling.policies.time.interval = 1
+appender.rolling.policies.time.modulate = true
+appender.rolling.layout.type = PatternLayout
+appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %-.10000m%n
+
+appender.json_rolling.type = RollingFile
+appender.json_rolling.name = json_rolling
+appender.json_rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.json_rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.json_rolling.policies.type = Policies
+appender.json_rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.json_rolling.policies.time.interval = 1
+appender.json_rolling.policies.time.modulate = true
+appender.json_rolling.layout.type = JSONLayout
+appender.json_rolling.layout.compact = true
+appender.json_rolling.layout.eventEol = true
+
+
+rootLogger.level = ${sys:ls.log.level}
+rootLogger.appenderRef.console.ref = ${sys:ls.log.format}_console
+rootLogger.appenderRef.rolling.ref = ${sys:ls.log.format}_rolling
+
+# Slowlog
+
+appender.console_slowlog.type = Console
+appender.console_slowlog.name = plain_console_slowlog
+appender.console_slowlog.layout.type = PatternLayout
+appender.console_slowlog.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n
+
+appender.json_console_slowlog.type = Console
+appender.json_console_slowlog.name = json_console_slowlog
+appender.json_console_slowlog.layout.type = JSONLayout
+appender.json_console_slowlog.layout.compact = true
+appender.json_console_slowlog.layout.eventEol = true
+
+appender.rolling_slowlog.type = RollingFile
+appender.rolling_slowlog.name = plain_rolling_slowlog
+appender.rolling_slowlog.fileName = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}.log
+appender.rolling_slowlog.filePattern = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.rolling_slowlog.policies.type = Policies
+appender.rolling_slowlog.policies.time.type = TimeBasedTriggeringPolicy
+appender.rolling_slowlog.policies.time.interval = 1
+appender.rolling_slowlog.policies.time.modulate = true
+appender.rolling_slowlog.layout.type = PatternLayout
+appender.rolling_slowlog.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %.10000m%n
+
+appender.json_rolling_slowlog.type = RollingFile
+appender.json_rolling_slowlog.name = json_rolling_slowlog
+appender.json_rolling_slowlog.fileName = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}.log
+appender.json_rolling_slowlog.filePattern = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.json_rolling_slowlog.policies.type = Policies
+appender.json_rolling_slowlog.policies.time.type = TimeBasedTriggeringPolicy
+appender.json_rolling_slowlog.policies.time.interval = 1
+appender.json_rolling_slowlog.policies.time.modulate = true
+appender.json_rolling_slowlog.layout.type = JSONLayout
+appender.json_rolling_slowlog.layout.compact = true
+appender.json_rolling_slowlog.layout.eventEol = true
+
+logger.slowlog.name = slowlog
+logger.slowlog.level = trace
+logger.slowlog.appenderRef.console_slowlog.ref = ${sys:ls.log.format}_console_slowlog
+logger.slowlog.appenderRef.rolling_slowlog.ref = ${sys:ls.log.format}_rolling_slowlog
+logger.slowlog.additivity = false
diff --git a/config/logstash.yml b/config/logstash.yml
new file mode 100644
index 00000000000..1cdec6f74d6
--- /dev/null
+++ b/config/logstash.yml
@@ -0,0 +1,161 @@
+# Settings file in YAML
+#
+# Settings can be specified either in hierarchical form, e.g.:
+#
+#   pipeline:
+#     batch:
+#       size: 125
+#       delay: 5
+#
+# Or as flat keys:
+#
+#   pipeline.batch.size: 125
+#   pipeline.batch.delay: 5
+#
+# ------------  Node identity ------------
+#
+# Use a descriptive name for the node:
+#
+# node.name: test
+#
+# If omitted the node name will default to the machine's host name
+#
+# ------------ Data path ------------------
+#
+# Which directory should be used by logstash and its plugins
+# for any persistent needs. Defaults to LOGSTASH_HOME/data
+#
+# path.data:
+#
+# ------------ Pipeline Settings --------------
+#
+# Set the number of workers that will, in parallel, execute the filters+outputs
+# stage of the pipeline.
+#
+# This defaults to the number of the host's CPU cores.
+#
+# pipeline.workers: 2
+#
+# How many workers should be used per output plugin instance
+#
+# pipeline.output.workers: 1
+#
+# How many events to retrieve from inputs before sending to filters+workers
+#
+# pipeline.batch.size: 125
+#
+# How long to wait before dispatching an undersized batch to filters+workers
+# Value is in milliseconds.
+#
+# pipeline.batch.delay: 5
+#
+# Force Logstash to exit during shutdown even if there are still inflight
+# events in memory. By default, logstash will refuse to quit until all
+# received events have been pushed to the outputs.
+#
+# WARNING: enabling this can lead to data loss during shutdown
+#
+# pipeline.unsafe_shutdown: false
+#
+# ------------ Pipeline Configuration Settings --------------
+#
+# Where to fetch the pipeline configuration for the main pipeline
+#
+# path.config:
+#
+# Pipeline configuration string for the main pipeline
+#
+# config.string:
+#
+# At startup, test if the configuration is valid and exit (dry run)
+#
+# config.test_and_exit: false
+#
+# Periodically check if the configuration has changed and reload the pipeline
+# This can also be triggered manually through the SIGHUP signal
+#
+# config.reload.automatic: false
+#
+# How often to check if the pipeline configuration has changed (in seconds)
+#
+# config.reload.interval: 3
+#
+# Show fully compiled configuration as debug log message
+# NOTE: --log.level must be 'debug'
+#
+# config.debug: false
+#
+# ------------ Queuing Settings --------------
+#
+# Internal queuing model, "memory" for legacy in-memory based queuing and
+# "persisted" for disk-based acked queueing. Defaults is memory
+#
+# queue.type: memory
+#
+# If using queue.type: persisted, the directory path where the data files will be stored.
+# Default is path.data/queue
+#
+# path.queue:
+#
+# If using queue.type: persisted, the page data files size. The queue data consists of
+# append-only data files separated into pages. Default is 250mb
+#
+# queue.page_capacity: 250mb
+#
+# If using queue.type: persisted, the maximum number of unread events in the queue.
+# Default is 0 (unlimited)
+#
+# queue.max_events: 0
+#
+# If using queue.type: persisted, the total capacity of the queue in number of bytes.
+# If you would like more unacked events to be buffered in Logstash, you can increase the
+# capacity using this setting. Please make sure your disk drive has capacity greater than
+# the size specified here. If both max_bytes and max_events are specified, Logstash will pick
+# whichever criteria is reached first
+# Default is 1024mb or 1gb
+#
+# queue.max_bytes: 1024mb
+#
+# If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint
+# Default is 1024, 0 for unlimited
+#
+# queue.checkpoint.acks: 1024
+#
+# If using queue.type: persisted, the maximum number of written events before forcing a checkpoint
+# Default is 1024, 0 for unlimited
+#
+# queue.checkpoint.writes: 1024
+#
+# If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page
+# Default is 1000, 0 for no periodic checkpoint.
+#
+# queue.checkpoint.interval: 1000
+#
+# ------------ Metrics Settings --------------
+#
+# Bind address for the metrics REST endpoint
+#
+# http.host: "127.0.0.1"
+#
+# Bind port for the metrics REST endpoint, this option also accept a range
+# (9600-9700) and logstash will pick up the first available ports.
+#
+# http.port: 9600-9700
+#
+# ------------ Debugging Settings --------------
+#
+# Options for log.level:
+#   * fatal
+#   * error
+#   * warn
+#   * info (default)
+#   * debug
+#   * trace
+#
+# log.level: info
+# path.logs:
+#
+# ------------ Other Settings --------------
+#
+# Where to find custom plugins
+# path.plugins: []
diff --git a/config/startup.options b/config/startup.options
new file mode 100644
index 00000000000..aa271fb3e5c
--- /dev/null
+++ b/config/startup.options
@@ -0,0 +1,53 @@
+################################################################################
+# These settings are ONLY used by $LS_HOME/bin/system-install to create a custom
+# startup script for Logstash and is not used by Logstash itself. It should
+# automagically use the init system (systemd, upstart, sysv, etc.) that your
+# Linux distribution uses.
+#
+# After changing anything here, you need to re-run $LS_HOME/bin/system-install
+# as root to push the changes to the init script.
+################################################################################
+
+# Override Java location
+#JAVACMD=/usr/bin/java
+
+# Set a home directory
+LS_HOME=/usr/share/logstash
+
+# logstash settings directory, the path which contains logstash.yml
+LS_SETTINGS_DIR="${LS_HOME}/config"
+
+# Arguments to pass to logstash
+LS_OPTS="--path.settings ${LS_SETTINGS_DIR}"
+
+# Arguments to pass to java
+LS_JAVA_OPTS=""
+
+# pidfiles aren't used the same way for upstart and systemd; this is for sysv users.
+LS_PIDFILE=/var/run/logstash.pid
+
+# user and group id to be invoked as
+LS_USER=logstash
+LS_GROUP=logstash
+
+# Enable GC logging by uncommenting the appropriate lines in the GC logging
+# section in jvm.options
+LS_GC_LOG_FILE=/var/log/logstash/gc.log
+
+# Open file limit
+LS_OPEN_FILES=16384
+
+# Nice level
+LS_NICE=19
+
+# Change these to have the init script named and described differently
+# This is useful when running multiple instances of Logstash on the same
+# physical box or vm
+SERVICE_NAME="logstash"
+SERVICE_DESCRIPTION="logstash"
+
+# If you need to run a command or script before launching Logstash, put it
+# between the lines beginning with `read` and `EOM`, and uncomment those lines.
+###
+## read -r -d '' PRESTART << EOM
+## EOM
diff --git a/patterns/.gitkeep b/data/.gitkeep
similarity index 100%
rename from patterns/.gitkeep
rename to data/.gitkeep
diff --git a/docs/asciidoc_index.rb b/docs/asciidoc_index.rb
deleted file mode 100644
index be1b94403f8..00000000000
--- a/docs/asciidoc_index.rb
+++ /dev/null
@@ -1,35 +0,0 @@
-#!/usr/bin/env ruby
-
-require "erb"
-
-if ARGV.size != 2
-  $stderr.puts "No path given to search for plugin docs"
-  $stderr.puts "Usage: #{$0} plugin_doc_dir type"
-  exit 1
-end
-
-
-def plugins(glob)
-  plugins=Hash.new []
-  files = Dir.glob(glob)
-  files.collect { |f| File.basename(f).gsub(".asciidoc", "") }.each {|plugin|
-    first_letter = plugin[0,1]
-    plugins[first_letter] += [plugin]
-  }
-  return Hash[plugins.sort]
-end # def plugins
-
-basedir = ARGV[0]
-type = ARGV[1]
-
-docs = plugins(File.join(basedir, "#{type}/*.asciidoc"))
-template_path = File.join(File.dirname(__FILE__), "index-#{type}.asciidoc.erb")
-template = File.new(template_path).read
-erb = ERB.new(template, nil, "-")
-
-path = "#{basedir}/#{type}.asciidoc"
-
-File.open(path, "w") do |out|
-  html = erb.result(binding)
-  out.puts(html)
-end
diff --git a/docs/asciidocgen.rb b/docs/asciidocgen.rb
deleted file mode 100644
index 5392e6a6440..00000000000
--- a/docs/asciidocgen.rb
+++ /dev/null
@@ -1,266 +0,0 @@
-require "rubygems"
-require "erb"
-require "optparse"
-
-$: << Dir.pwd
-$: << File.join(File.dirname(__FILE__), "..", "lib")
-$: << File.join(File.dirname(__FILE__), "..", "rakelib")
-
-require_relative "../lib/bootstrap/environment" #needed for LogStash::Environment constants LOGSTASH_HOME
-require "logstash/config/mixin"
-require "logstash/inputs/base"
-require "logstash/codecs/base"
-require "logstash/filters/base"
-require "logstash/outputs/base"
-require "logstash/version"
-require "rakelib/default_plugins"
-
-class LogStashConfigAsciiDocGenerator
-  COMMENT_RE = /^ *#(?: (.*)| *$)/
-
-  def initialize
-    @rules = {
-      COMMENT_RE => lambda { |m| add_comment(m[1]) },
-      /^ *class.*< *(::)?LogStash::(Outputs|Filters|Inputs|Codecs)::(Base|Threadable)/ => \
-        lambda { |m| set_class_description },
-      /^ *config +[^=].*/ => lambda { |m| add_config(m[0]) },
-      /^ *milestone .*/ => lambda { |m| set_milestone(m[0]) },
-      /^ *config_name .*/ => lambda { |m| set_config_name(m[0]) },
-      /^ *flag[( ].*/ => lambda { |m| add_flag(m[0]) },
-      /^ *(class|def|module) / => lambda { |m| clear_comments },
-    }
-    @supported_plugins = *::LogStash::RakeLib::DEFAULT_PLUGINS
-  end
-
-  def parse(string)
-    clear_comments
-    buffer = ""
-    string.split(/\r\n|\n/).each do |line|
-      # Join long lines
-      if line =~ COMMENT_RE
-        # nothing
-      else
-        # Join extended lines
-        if line =~ /(, *$)|(\\$)|(\[ *$)/
-          buffer += line.gsub(/\\$/, "")
-          next
-        end
-      end
-
-      line = buffer + line
-      buffer = ""
-
-      @rules.each do |re, action|
-        m = re.match(line)
-        if m
-          action.call(m)
-        end
-      end # RULES.each
-    end # string.split("\n").each
-  end # def parse
-
-  def set_class_description
-    @class_description = @comments.join("\n")
-    clear_comments
-  end # def set_class_description
-
-  def add_comment(comment)
-    return if comment == "encoding: utf-8"
-    @comments << comment
-  end # def add_comment
-
-  def add_config(code)
-    # I just care about the 'config :name' part
-    code = code.sub(/,.*/, "")
-
-    # call the code, which calls 'config' in this class.
-    # This will let us align comments with config options.
-    name, opts = eval(code)
-
-    # TODO(sissel): This hack is only required until regexp configs
-    # are gone from logstash.
-    name = name.to_s unless name.is_a?(Regexp)
-
-    description = @comments.join("\n")
-    @attributes[name][:description] = description
-    clear_comments
-  end # def add_config
-
-  def add_flag(code)
-    # call the code, which calls 'config' in this class.
-    # This will let us align comments with config options.
-    #p :code => code
-    fixed_code = code.gsub(/ do .*/, "")
-    #p :fixedcode => fixed_code
-    name, description = eval(fixed_code)
-    @flags[name] = description
-    clear_comments
-  end # def add_flag
-
-  def set_config_name(code)
-    name = eval(code)
-    @name = name
-  end # def set_config_name
-
-  def set_milestone(code)
-    @milestone = eval(code)
-  end
-
-  # pretend to be the config DSL and just get the name
-  def config(name, opts={})
-    return name, opts
-  end # def config
-
-  # Pretend to support the flag DSL
-  def flag(*args, &block)
-    name = args.first
-    description = args.last
-    return name, description
-  end # def config
-
-  # pretend to be the config dsl's 'config_name' method
-  def config_name(name)
-    return name
-  end # def config_name
-
-  # pretend to be the config dsl's 'milestone' method
-  def milestone(m)
-    return m
-  end # def milestone
-
-  def clear_comments
-    @comments.clear
-  end # def clear_comments
-
-  def generate(file, settings)
-    @class_description = ""
-    @milestone = ""
-    @comments = []
-    @attributes = Hash.new { |h,k| h[k] = {} }
-    @flags = {}
-
-    # local scoping for the monkeypatch belowg
-    attributes = @attributes
-
-    # Monkeypatch the 'config' method to capture
-    # Note, this monkeypatch requires us do the config processing
-    # one at a time.
-    # LogStash::Config::Mixin::DSL.instance_eval do
-    #   define_method(:config) do |name, opts={}|
-    #     p name => opts
-    #     attributes[name].merge!(opts)
-    #   end
-    # end
-
-    # Loading the file will trigger the config dsl which should
-    # collect all the config settings.
-
-    # include the plugin lib dir for loading specific files
-
-    $: << File.join(File.dirname(file), "..", "..")
-    # include the lib dir of the plugin it self for any local dependencies
-    load file
-
-    # Get the correct base path
-    base = File.join(::LogStash::Environment::LOGSTASH_HOME,'logstash-core/lib/logstash', file.split("/")[-2])
-
-    # parse base first
-    parse(File.new(File.join(base, "base.rb"), "r").read)
-
-    # Now parse the real library
-    code = File.new(file).read
-
-    # inputs either inherit from Base or Threadable.
-    if code =~ /\< ::LogStash::Inputs::Threadable/
-      parse(File.new(File.join(base, "threadable.rb"), "r").read)
-    end
-
-    if code =~ /include ::LogStash::PluginMixins/
-      mixin = code.gsub(/.*include ::LogStash::PluginMixins::(\w+)\s.*/m, '\1')
-      mixin.gsub!(/(.)([A-Z])/, '\1_\2')
-      mixin.downcase!
-      #parse(File.new(File.join(base, "..", "plugin_mixins", "#{mixin}.rb")).read)
-      #TODO: RP make this work better with the naming
-      mixinfile = Dir.glob(File.join(::LogStash::Environment.logstash_gem_home,'gems',"logstash-mixin-#{mixin.split('_').first}-*",'lib/logstash/plugin_mixins', "#{mixin}.rb")).first
-      parse(File.new(mixinfile).read)
-    end
-
-    parse(code)
-
-    puts "Generating docs for #{file}"
-
-    if @name.nil?
-      $stderr.puts "Missing 'config_name' setting in #{file}?"
-      return nil
-    end
-
-    klass = ::LogStash::Config::Registry.registry[@name]
-    if klass.ancestors.include?(::LogStash::Inputs::Base)
-      section = "input"
-    elsif klass.ancestors.include?(::LogStash::Filters::Base)
-      section = "filter"
-    elsif klass.ancestors.include?(::LogStash::Outputs::Base)
-      section = "output"
-    elsif klass.ancestors.include?(::LogStash::Codecs::Base)
-      section = "codec"
-    end
-
-    plugin_name = "logstash-" + section + "-" + @name
-    default_plugin = @supported_plugins.include?(plugin_name)
-
-    template_file = File.join(File.dirname(__FILE__), "plugin-doc.asciidoc.erb")
-    template = ERB.new(File.new(template_file).read, nil, "-")
-
-    is_contrib_plugin = !default_plugin
-
-    # descriptions are assumed to be markdown
-    description = @class_description
-
-    klass.get_config.each do |name, settings|
-      @attributes[name].merge!(settings)
-      default = klass.get_default(name)
-      unless default.nil?
-        @attributes[name][:default] = default
-      end
-    end
-    sorted_attributes = @attributes.sort { |a,b| a.first.to_s <=> b.first.to_s }
-    klassname = ::LogStash::Config::Registry.registry[@name].to_s
-    name = @name
-
-    synopsis_file = File.join(File.dirname(__FILE__), "plugin-synopsis.asciidoc.erb")
-    synopsis = ERB.new(File.new(synopsis_file).read, nil, "-").result(binding)
-
-    if settings[:output]
-      dir = File.join(settings[:output], section + "s")
-      path = File.join(dir, "#{name}.asciidoc")
-      Dir.mkdir(settings[:output]) if !File.directory?(settings[:output])
-      Dir.mkdir(dir) if !File.directory?(dir)
-      File.open(path, "w") do |out|
-        html = template.result(binding)
-        html.gsub!("%VERSION%", LOGSTASH_VERSION)
-        html.gsub!("%PLUGIN%", @name)
-        out.puts(html)
-      end
-    else
-      puts template.result(binding)
-    end
-  end # def generate
-
-end # class LogStashConfigDocGenerator
-
-if __FILE__ == $0
-  opts = OptionParser.new
-  settings = {}
-  opts.on("-o DIR", "--output DIR",
-          "Directory to output to; optional. If not specified,"\
-          "we write to stdout.") do |val|
-    settings[:output] = val
-  end
-
-  args = opts.parse(ARGV)
-
-  args.each do |arg|
-    gen = LogStashConfigAsciiDocGenerator.new
-    gen.generate(arg, settings)
-  end
-end
diff --git a/docs/index-codecs.asciidoc.erb b/docs/index-codecs.asciidoc.erb
deleted file mode 100644
index 528ee1d39d0..00000000000
--- a/docs/index-codecs.asciidoc.erb
+++ /dev/null
@@ -1,43 +0,0 @@
-[[codec-plugins]]
-== Codec plugins
-
-A codec plugin changes the data representation of an event. Codecs are essentially stream filters that can operate as part of an input or output.
-
-The following codec plugins are available:
-
-<%-
-full_list=[]
-letters=[]
-docs.each do |doc|
-letter = doc[0]
-letters << letter
--%>
-<<plugins-codecs-letters-<%= letter %>, <%=letter %>>>
-<%- end -%>
-
-<%-
-cols=3
-rows=(docs.count/cols)+1
-item=0
-r=0
--%>
-[cols="asciidoc,asciidoc,asciidoc"]
-|=======================================================================
-<%- while r < rows do -%>
-<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-codecs-letters-<%=letters[item] %>]] <% end %>
-<%- letter = letters[item];
-arr = docs[letter]
-if ! arr.nil?
-arr.each do |plugin_item|
-full_list << plugin_item
-%>* <<plugins-codecs-<%=plugin_item -%>,<%=plugin_item -%>>>
-<%- end 
-end -%>
-<%- item+=1; c+=1; end; r+=1 end -%>
-|=======================================================================
-
-<%-
-full_list.each do |plugin|
--%>
-include::codecs/<%=plugin %>.asciidoc[]
-<%- end -%>
diff --git a/docs/index-filters.asciidoc.erb b/docs/index-filters.asciidoc.erb
deleted file mode 100644
index bcfab03925c..00000000000
--- a/docs/index-filters.asciidoc.erb
+++ /dev/null
@@ -1,43 +0,0 @@
-[[filter-plugins]]
-== Filter plugins
-
-A filter plugin performs intermediary processing on an event. Filters are often applied conditionally depending on the characteristics of the event. 
-
-The following filter plugins are available:
-
-<%-
-full_list=[]
-letters=[]
-docs.each do |doc|
-letter = doc[0]
-letters << letter
--%>
-<<plugins-filters-letters-<%= letter %>, <%=letter %>>>
-<%- end -%>
-
-<%-
-cols=3
-rows=(docs.count/cols)+1
-item=0
-r=0
--%>
-[cols="asciidoc,asciidoc,asciidoc"]
-|=======================================================================
-<%- while r < rows do -%>
-<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-filters-letters-<%=letters[item] %>]] <% end %>
-<%- letter = letters[item];
-arr = docs[letter]
-if ! arr.nil?
-arr.each do |plugin_item|
-full_list << plugin_item
-%>* <<plugins-filters-<%=plugin_item -%>,<%=plugin_item -%>>>
-<%- end 
-end -%>
-<%- item+=1; c+=1; end; r+=1 end -%>
-|=======================================================================
-
-<%-
-full_list.each do |plugin|
--%>
-include::filters/<%=plugin %>.asciidoc[]
-<%- end -%>
diff --git a/docs/index-inputs.asciidoc.erb b/docs/index-inputs.asciidoc.erb
deleted file mode 100644
index ade4fc9b249..00000000000
--- a/docs/index-inputs.asciidoc.erb
+++ /dev/null
@@ -1,43 +0,0 @@
-[[input-plugins]]
-== Input plugins
-
-An input plugin enables a specific source of events to be read by Logstash.
-
-The following input plugins are available:
-
-<%-
-full_list=[]
-letters=[]
-docs.each do |doc|
-letter = doc[0]
-letters << letter
--%>
-<<plugins-inputs-letters-<%= letter %>, <%=letter %>>>
-<%- 
-end
-
-cols=3
-rows=(docs.count/cols)+1
-item=0
-r=0
--%>
-[cols="asciidoc,asciidoc,asciidoc"]
-|=======================================================================
-<%- while r < rows do -%>
-<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-inputs-letters-<%=letters[item] %>]] <% end %>
-<%- letter = letters[item];
-arr = docs[letter]
-if ! arr.nil?
-arr.each do |plugin_item|
-full_list << plugin_item
-%>* <<plugins-inputs-<%=plugin_item -%>,<%=plugin_item -%>>>
-<%- end 
-end -%>
-<%- item+=1; c+=1; end; r+=1 end -%>
-|=======================================================================
-
-<%-
-full_list.each do |plugin|
--%>
-include::inputs/<%=plugin %>.asciidoc[]
-<%- end -%>
diff --git a/docs/index-outputs.asciidoc.erb b/docs/index-outputs.asciidoc.erb
deleted file mode 100644
index 2cab51fa478..00000000000
--- a/docs/index-outputs.asciidoc.erb
+++ /dev/null
@@ -1,43 +0,0 @@
-[[output-plugins]]
-== Output plugins
-
-An output plugin sends event data to a particular destination. Outputs are the final stage in the event pipeline.
-
-The following output plugins are available:
-
-<%-
-full_list=[]
-letters=[]
-docs.each do |doc|
-letter = doc[0]
-letters << letter
--%>
-<<plugins-outputs-letters-<%= letter %>, <%=letter %>>>
-<%- end -%>
-
-<%-
-cols=3
-rows=(docs.count/cols)+1
-item=0
-r=0
--%>
-[cols="asciidoc,asciidoc,asciidoc"]
-|=======================================================================
-<%- while r < rows do -%>
-<%- c=0; while c < cols do -%>|<% if letters[item].nil? %>&nbsp; <% else %>[[plugins-outputs-letters-<%=letters[item] %>]] <% end %>
-<%- letter = letters[item];
-arr = docs[letter]
-if ! arr.nil?
-arr.each do |plugin_item|
-full_list << plugin_item
-%>* <<plugins-outputs-<%=plugin_item -%>,<%=plugin_item -%>>>
-<%- end 
-end -%>
-<%- item+=1; c+=1; end; r+=1 end -%>
-|=======================================================================
-
-<%-
-full_list.each do |plugin|
--%>
-include::outputs/<%=plugin %>.asciidoc[]
-<%- end -%>
diff --git a/docs/index-shared1.asciidoc b/docs/index-shared1.asciidoc
new file mode 100644
index 00000000000..30c17bd322c
--- /dev/null
+++ b/docs/index-shared1.asciidoc
@@ -0,0 +1,91 @@
+
+:branch:                5.4
+:major-version:         5.x
+:logstash_version:      5.4.2
+:elasticsearch_version: 5.4.2
+:kibana_version:        5.4.2
+:docker-image:          docker.elastic.co/logstash/logstash:{logstash_version}
+
+//////////
+release-state can be: released | prerelease | unreleased
+//////////
+:release-state:  released
+
+:jdk:                   1.8.0
+:guide:                 https://www.elastic.co/guide/en/elasticsearch/guide/current/
+:ref:                   https://www.elastic.co/guide/en/elasticsearch/reference/5.4/
+:xpack-ref:             https://www.elastic.co/guide/en/x-pack/5.4/
+:kibana-ref:            https://www.elastic.co/guide/en/kibana/5.4/
+:logstash:              https://www.elastic.co/guide/en/logstash/5.4/
+:libbeat:               https://www.elastic.co/guide/en/beats/libbeat/5.4/
+:filebeat:              https://www.elastic.co/guide/en/beats/filebeat/5.4/
+:metricbeat:            https://www.elastic.co/guide/en/beats/metricbeat/5.4/
+:lsissue:               https://github.com/elastic/logstash/issues/
+:security:              X-Pack Security
+:stack:                 https://www.elastic.co/guide/en/elastic-stack/current/
+
+:xpack:                 X-Pack
+:es:                    Elasticsearch
+:kib:                   Kibana
+
+[[introduction]]
+== Logstash Introduction
+
+Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically
+unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all
+your data for diverse advanced downstream analytics and visualization use cases.
+
+While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
+type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many
+native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater
+volume and variety of data.
+
+// The pass blocks here point to the correct repository for the edit links in the guide.
+
+// Introduction
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/introduction.asciidoc
+include::static/introduction.asciidoc[]
+
+// Glossary and core concepts go here
+
+// Getting Started with Logstash
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/getting-started-with-logstash.asciidoc
+include::static/getting-started-with-logstash.asciidoc[]
+
+// Advanced LS Pipelines
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/advanced-pipeline.asciidoc
+include::static/advanced-pipeline.asciidoc[]
+
+// Processing Pipeline
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/life-of-an-event.asciidoc
+include::static/life-of-an-event.asciidoc[]
+
+// Lostash setup
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/setting-up-logstash.asciidoc
+include::static/setting-up-logstash.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/settings-file.asciidoc
+include::static/settings-file.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/running-logstash-command-line.asciidoc
+include::static/running-logstash-command-line.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/running-logstash.asciidoc
+include::static/running-logstash.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/docker.asciidoc
+include::static/docker.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/logging.asciidoc
+include::static/logging.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/persistent-queues.asciidoc
+include::static/persistent-queues.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/shutdown.asciidoc
+include::static/shutdown.asciidoc[]
diff --git a/docs/index-shared2.asciidoc b/docs/index-shared2.asciidoc
new file mode 100644
index 00000000000..cf586ef4ac2
--- /dev/null
+++ b/docs/index-shared2.asciidoc
@@ -0,0 +1,102 @@
+
+// Breaking Changes
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/breaking-changes.asciidoc
+include::static/breaking-changes.asciidoc[]
+
+// Upgrading Logstash
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/upgrading.asciidoc
+include::static/upgrading.asciidoc[]
+
+// Configuring Logstash
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/configuration.asciidoc
+include::static/configuration.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/reloading-config.asciidoc
+include::static/reloading-config.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/managing-multiline-events.asciidoc
+include::static/managing-multiline-events.asciidoc[]
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/glob-support.asciidoc
+include::static/glob-support.asciidoc[]
+
+// Working with Filebeat Modules
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/filebeat-modules.asciidoc
+include::static/filebeat-modules.asciidoc[]
+
+// Transforming Data
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/transforming-data.asciidoc
+include::static/transforming-data.asciidoc[]
+
+// Deploying & Scaling
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/deploying.asciidoc
+include::static/deploying.asciidoc[]
+
+// Troubleshooting performance
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/performance-checklist.asciidoc
+include::static/performance-checklist.asciidoc[]
+
+// Monitoring APIs
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/monitoring-apis.asciidoc
+include::static/monitoring-apis.asciidoc[]
+
+// Working with Plugins
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/plugin-manager.asciidoc
+include::static/plugin-manager.asciidoc[]
+
+// These files do their own pass blocks
+
+include::{plugins-repo-dir}/plugins/inputs.asciidoc[]
+include::{plugins-repo-dir}/plugins/outputs.asciidoc[]
+include::{plugins-repo-dir}/plugins/filters.asciidoc[]
+include::{plugins-repo-dir}/plugins/codecs.asciidoc[]
+
+:edit_url:
+
+// Contributing to Logstash
+
+include::static/contributing-to-logstash.asciidoc[]
+
+include::static/input.asciidoc[]
+include::static/codec.asciidoc[]
+include::static/filter.asciidoc[]
+include::static/output.asciidoc[]
+
+// Contributing a Patch to a Logstash Plugin
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/contributing-patch.asciidoc
+include::static/contributing-patch.asciidoc[]
+
+// Logstash Community Maintainer Guide
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/maintainer-guide.asciidoc
+include::static/maintainer-guide.asciidoc[]
+
+// A space is necessary here ^^^
+
+
+// Submitting a Plugin
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/submitting-a-plugin.asciidoc
+include::static/submitting-a-plugin.asciidoc[]
+
+
+// Glossary of Terms
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/glossary.asciidoc
+include::static/glossary.asciidoc[]
+
+
+// Release Notes
+
+:edit_url: https://github.com/elastic/logstash/edit/5.4/docs/static/releasenotes.asciidoc
+include::static/releasenotes.asciidoc[]
diff --git a/docs/index.asciidoc b/docs/index.asciidoc
new file mode 100644
index 00000000000..f267b470d9c
--- /dev/null
+++ b/docs/index.asciidoc
@@ -0,0 +1,7 @@
+[[logstash-reference]]
+= Logstash Reference
+
+:plugins-repo-dir:      {docdir}/../../logstash-docs/docs
+
+include::index-shared1.asciidoc[]
+include::index-shared2.asciidoc[]
diff --git a/docs/index.html.erb b/docs/index.html.erb
deleted file mode 100644
index 859f4f158df..00000000000
--- a/docs/index.html.erb
+++ /dev/null
@@ -1,57 +0,0 @@
----
-title: logstash docs index
-layout: content_right
----
-<div id="doc_index_container">
-
-  <h3> For Users </h3>
-  <ul>
-    <li> <a href="https://download.elasticsearch.org/logstash/logstash/logstash-%VERSION%.tar.gz"> download logstash %VERSION% </a> </li>
-    <li> <a href="release-notes"> upgrade/release notes </a> </li>
-    <li> <a href="https://github.com/elasticsearch/logstash/blob/v%VERSION%/CHANGELOG"> view changelog </a> </li>
-    <li> <a href="contrib-plugins"> contrib plugins</a> </li>
-    <li> <a href="repositories"> package repositories</a> </li>
-    <li> <a href="configuration"> configuration file overview </a> </li>
-    <li> <a href="configuration#conditionals">conditionals</a> </li>
-    <li> <a href="configuration#fieldreferences">referring to fields [like][this]</a> </li>
-    <li> <a href="configuration#sprintf">using the %{fieldname} syntax</a> </li>
-
-    <li> <a href="life-of-an-event"> the life of an event in logstash </a> </li>
-    <li> <a href="flags"> command-line flags </a> </li>
-  </ul>
-
-  <h3> For Developers </h3>
-  <ul>
-    <li> <a href="extending"> writing your own plugins </a> </li>
-  </ul>
-
-  <h3> Tutorials and Use Cases </h3>
-
-  <ul>
-    <li> <a href="tutorials/getting-started-with-logstash"> Getting started with Logstash </a> - New to Logstash? Start here! </li>
-    <li> <a href="tutorials/metrics-from-logs"> Metrics from logs </a> - take metrics from logs and ship them to graphite, ganglia, and more. </li>
-    <li> <a href="tutorials/just-enough-rabbitmq-for-logstash">Just enough RabbitMQ knowledge for Logstash </a> - Get a quick primer on RabbitMQ and how to use it in Logstash! </li>
-  </ul>
-
-  <h3> books and articles </h3>
-
-  <ul>
-    <li> <a href="http://www.logstashbook.com">The Logstash Book </a> - An introductory Logstash book. </li>
-  </ul>
-
-  <h3> plugin documentation </h3>
-<% docs.each do |type, paths| -%>
-  <div class="doc_index_section">
-    <h3><%= type %></h3>
-    <ul>
-<% paths.each do |path| -%>
-<%   name = File.basename(path).gsub(".html", "") -%>
-      <li>
-      <a href="<%= "#{type}/#{name}" %>"><%= name %></a>
-      </li>
-<% end -%>
-    </ul>
-  </div>
-<% end -%>
-</div>
-<div class="clear"></div>
diff --git a/docs/plugin-doc.asciidoc.erb b/docs/plugin-doc.asciidoc.erb
deleted file mode 100644
index 55fd81cff48..00000000000
--- a/docs/plugin-doc.asciidoc.erb
+++ /dev/null
@@ -1,68 +0,0 @@
-<%- plugin_name = name -%>
-[[plugins-<%= section %>s-<%= name %>]]
-=== <%= name %>
-
-<% unless default_plugin %>
-NOTE: This is a community-maintained plugin! It does not ship with Logstash by default, but it is easy to install by running `bin/logstash-plugin install logstash-<%= section %>-<%= plugin_name %>`.
-<% end %>
-
-<%= description %>
-
-&nbsp;
-
-==== Synopsis
-
-<% if sorted_attributes.count > 0 -%>
-This plugin supports the following configuration options:
-<% else -%>
-This plugin has no configuration options.
-<% end -%>
-
-<%= synopsis -%>
-
-<% if sorted_attributes.count > 0 -%>
-
-==== Details
-
-&nbsp;
-
-<% sorted_attributes.each do |name, config| -%>
-<%
-     next if config[:obsolete]
-     if name.is_a?(Regexp)
-       name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
-       is_regexp = true
-     else
-       is_regexp = false
-     end
--%>
-[[plugins-<%= section%>s-<%= plugin_name%>-<%= name%>]]
-===== `<%= name %>` <%= " (DEPRECATED)" if config[:deprecated] %>
-
-<% if config[:required] -%>
-  * This is a required setting.
-<% end -%>
-<% if config[:deprecated] -%>
-  * DEPRECATED WARNING: This configuration item is deprecated and may not be available in future versions.
-<% end -%>
-<% if is_regexp -%>
-  * The configuration attribute name here is anything that matches the above regular expression.
-<% end -%>
-<% if config[:validate].is_a?(Symbol) -%>
-  * Value type is <<<%= config[:validate] %>,<%= config[:validate] %>>>
-<% elsif config[:validate].nil? -%>
-  <li> Value type is <<string,string>>
-<% elsif config[:validate].is_a?(Array) -%>
-  * Value can be any of: `<%= config[:validate].join('`, `') %>`
-<% end -%>
-<% if config.include?(:default) -%>
-  * Default value is `<%= config[:default].inspect %>`
-<% else -%>
-  * There is no default value for this setting.
-<% end -%>
-
-<%= config[:description] %>
-
-<% end -%>
-
-<% end -%>
diff --git a/docs/plugin-doc.html.erb b/docs/plugin-doc.html.erb
deleted file mode 100644
index c236314e0af..00000000000
--- a/docs/plugin-doc.html.erb
+++ /dev/null
@@ -1,80 +0,0 @@
----
-title: logstash docs for <%= section %>s/<%= name %>
-layout: content_right
----
-<h2><%= name %></h2>
-<h3>Milestone: <a href="../plugin-milestones"><%= @milestone %></a></h3>
-<% if is_contrib_plugin -%>
-<% end -%>
-
-<%= description %>
-
-<h3> Synopsis </h3>
-
-A sample configuration file is shown here:
-
-<pre><code><% if section == "codec" -%>
-# with an input plugin:
-# you can also use this codec with an output.
-input { 
-<% if name == "json_lines" -%>
-  udp {
-    port =&gt; 1234
-<% else -%>
-  file {
-<% end -%>
-    codec =&gt; <%= synopsis -%>
-  }
-}
-<% else -%>
-<%= section %> {
-  <%= synopsis -%>
-}
-<% end -%></code></pre>
-
-<h3> Details </h3>
-
-<% sorted_attributes.each do |name, config| -%>
-<%
-     if name.is_a?(Regexp)
-       name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
-       is_regexp = true
-     else
-       is_regexp = false
-     end
--%>
-<h4> 
-  <a name="<%= name %>">
-    <%= name %><%= " (required setting)" if config[:required] %>
-    <%= " <strong>DEPRECATED</strong>" if config[:deprecated] %>
-</a>
-</h4>
-
-<ul>
-<% if config[:deprecated] -%>
-  <li> DEPRECATED WARNING: This configuration item is deprecated and may not be included in later versions.</li>
-<% end -%>
-<% if is_regexp -%>
-  <li> The configuration attribute name here is anything that matches the above regular expression. </li>
-<% end -%>
-<% if config[:validate].is_a?(Symbol) -%>
-  <li> Value type is <a href="../configuration#<%= config[:validate] %>"><%= config[:validate] %></a> </li>
-<% elsif config[:validate].nil? -%>
-  <li> Value type is <a href="../configuration#string">string</a> </li>
-<% elsif config[:validate].is_a?(Array) -%>
-  <li> Value can be any of: <%= config[:validate].map(&:inspect).join(", ") %> </li>
-<% end -%>
-<% if config.include?(:default) -%>
-  <li> Default value is <%= config[:default].inspect %> </li>
-<% else -%>
-  <li> There is no default value for this setting. </li>
-<% end -%>
-</ul>
-
-<%= config[:description] %>
-
-<% end -%>
-
-<hr>
-
-This is documentation from <a href="https://github.com/logstash/logstash/blob/v<%= LOGSTASH_VERSION %>/<%= file %>"><%= file %></a>
diff --git a/docs/plugin-synopsis.asciidoc.erb b/docs/plugin-synopsis.asciidoc.erb
deleted file mode 100644
index 6b1d047c4d6..00000000000
--- a/docs/plugin-synopsis.asciidoc.erb
+++ /dev/null
@@ -1,57 +0,0 @@
-<%- plugin_name = name -%>
-
-<% if sorted_attributes.count > 0 -%>
-Required configuration options:
-<% else -%>
-Complete configuration example:
-<% end -%>
-
-[source,json]
---------------------------
-<%= name %> {
-<% if sorted_attributes.count > 0 -%>
-<% sorted_attributes.each do |name, config|
-   next if config[:deprecated]
-   next if !config[:required]
--%>
-<%= "  " if section == "codec" %>    <%= name %> => ...
-<% end -%>
-<%= "  " if section == "codec" %><% ; end -%>}
---------------------------
-
-<% if sorted_attributes.count > 0 %>
-
-Available configuration options:
-
-[cols="<,<,<,<m",options="header",]
-|=======================================================================
-|Setting |Input type|Required|Default value
-<% sorted_attributes.each do |name, config|
-   next if config[:obsolete]
-   next if config[:deprecated]
-   if config[:validate].is_a?(Array)
-     annotation = "|<<string,string>>, one of `#{config[:validate].inspect}`"
-   elsif config[:validate] == :path
-     annotation = "|a valid filesystem path"
-   else
-     annotation = "|<<#{config[:validate]},#{config[:validate]}>>"
-   end
-
-   if name.is_a?(Regexp)
-     name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
-   end
-   if config[:required]
-     annotation += "|Yes"
-   else
-     annotation += "|No"
-   end
-   if config.include?(:default)
-     annotation += "|`#{config[:default].inspect}`"
-   else
-     annotation += "|"
-   end
--%>
-| <<plugins-<%= section %>s-<%=plugin_name%>-<%= name %>>> <%= annotation %>
-<% end -%>
-|=======================================================================
-<% end %>
diff --git a/docs/plugin-synopsis.html.erb b/docs/plugin-synopsis.html.erb
deleted file mode 100644
index 92465227fa8..00000000000
--- a/docs/plugin-synopsis.html.erb
+++ /dev/null
@@ -1,24 +0,0 @@
-<%= name %> {
-<% sorted_attributes.each do |name, config|
-   next if config[:deprecated]
-   if config[:validate].is_a?(Array) 
-     annotation = "string, one of #{config[:validate].inspect}"
-   elsif config[:validate] == :path
-     annotation = "a valid filesystem path"
-   else 
-     annotation = "#{config[:validate]}"
-   end
-
-   if name.is_a?(Regexp)
-     name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
-   end
-   if config[:required]
-     annotation += " (required)"
-   else
-     annotation += " (optional)"
-   end
-   annotation += ", default: #{config[:default].inspect}" if config.include?(:default)
--%>
-<%= "  " if section == "codec" %>    <a href="#<%= name %>"><%= name %></a> => ... # <%= annotation %>
-<% end -%>
-<%= "  " if section == "codec" %>  }
diff --git a/docs/static/advanced-pipeline.asciidoc b/docs/static/advanced-pipeline.asciidoc
index 7e541ce5eea..5f6c69c4116 100644
--- a/docs/static/advanced-pipeline.asciidoc
+++ b/docs/static/advanced-pipeline.asciidoc
@@ -1,22 +1,79 @@
 [[advanced-pipeline]]
-=== Setting Up an Advanced Logstash Pipeline
+=== Parsing Logs with Logstash
 
-A Logstash pipeline in most use cases has one or more input, filter, and output plugins. The scenarios in this section
-build Logstash configuration files to specify these plugins and discuss what each plugin is doing.
+In <<first-event>>, you created a basic Logstash pipeline to test your Logstash setup. In the real world, a Logstash
+pipeline is a bit more complex: it typically has one or more input, filter, and output plugins.  
 
-The Logstash configuration file defines your _Logstash pipeline_. When you start a Logstash instance, use the
-`-f <path/to/file>` option to specify the configuration file that defines that instanceâ€™s pipeline.
+In this section, you create a Logstash pipeline that uses Filebeat to take Apache web logs as input, parses those
+logs to create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Rather than
+defining the pipeline configuration at the command line, you'll define the pipeline in a config file. 
 
-A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
-plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
-the data to a destination.
+To get started, go https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here] to
+download the sample data set used in this example. Unpack the file.
 
-image::static/images/basic_logstash_pipeline.png[]
 
-The following text represents the skeleton of a configuration pipeline:
+[[configuring-filebeat]]
+==== Configuring Filebeat to Send Log Lines to Logstash
+
+Before you create the Logstash pipeline, you'll configure Filebeat to send log lines to Logstash.  
+The https://github.com/elastic/beats/tree/master/filebeat[Filebeat] client is a lightweight, resource-friendly tool
+that collects logs from files on the server and forwards these logs to your Logstash instance for processing.
+Filebeat is designed for reliability and low latency. Filebeat has a light resource footprint on the host machine,
+and the {logstash}plugins-inputs-beats.html[`Beats input`] plugin minimizes the resource demands on the Logstash
+instance.
+
+NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your
+Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
+same machine.
+
+The default Logstash installation includes the {logstash}plugins-inputs-beats.html[`Beats input`] plugin. The Beats
+input plugin enables Logstash to receive events from the Elastic Beats framework, which means that any Beat written
+to work with the Beats framework, such as Packetbeat and Metricbeat, can also send event data to Logstash. 
+
+To install Filebeat on your data source machine, download the appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page]. You can also refer to
+{filebeat}filebeat-getting-started.html[Getting Started with Filebeat] in the Beats documentation for additional
+installation instructions.
+
+After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
+directory, and replace the contents with the following lines. Make sure `paths` points to the example Apache log file,
+`logstash-tutorial.log`, that you downloaded earlier: 
+
+[source,yaml]
+--------------------------------------------------------------------------------
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /path/to/file/logstash-tutorial.log <1>
+output.logstash:
+  hosts: ["localhost:5043"]
+--------------------------------------------------------------------------------
+
+<1> Absolute path to the file or files that Filebeat processes.
+
+Save your changes. 
+
+To keep the configuration simple, you won't specify TLS/SSL settings as you would in a real world
+scenario.
+
+At the data source machine, run Filebeat with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
+sudo ./filebeat -e -c filebeat.yml -d "publish"
+--------------------------------------------------------------------------------
+
+Filebeat will attempt to connect on port 5043. Until Logstash starts with an active Beats plugin, there
+wonâ€™t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
+
+==== Configuring Logstash for Filebeat Input
+
+Next, you create a Logstash configuration pipeline that uses the Beats input plugin to receive
+events from Beats.
+
+The following text represents the skeleton of a configuration pipeline:
+
+[source,json]
+--------------------------------------------------------------------------------
 # The # character at the beginning of a line indicates a comment. Use
 # comments to describe your configuration.
 input {
@@ -30,58 +87,110 @@ output {
 }
 --------------------------------------------------------------------------------
 
-This skeleton is non-functional, because the input and output sections donâ€™t have any valid options defined. The
-examples in this tutorial build configuration files to address specific use cases.
+This skeleton is non-functional, because the input and output sections donâ€™t have any valid options defined. 
 
-Paste the skeleton into a file named `first-pipeline.conf` in your home Logstash directory.
+To get started, copy and paste the skeleton configuration pipeline into a file named `first-pipeline.conf` in your home
+Logstash directory. 
 
-[[parsing-into-es]]
-==== Parsing Apache Logs into Elasticsearch
-
-This example creates a Logstash pipeline that takes Apache web logs as input, parses those logs to create specific,
-named fields from the logs, and writes the parsed data to an Elasticsearch cluster.
+Next, configure your Logstash instance to use the Beats input plugin by adding the following lines to the `input` section
+of the `first-pipeline.conf` file:
 
-You can download the sample data set used in this example
-https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here]. Unpack this file.
+[source,json]
+--------------------------------------------------------------------------------
+    beats {
+        port => "5043"
+    }
+--------------------------------------------------------------------------------
 
-[float]
-[[configuring-file-input]]
-==== Configuring Logstash for File Input
+You'll configure Logstash to write to Elasticsearch later. For now, you can add the following line
+to the `output` section so that the output is printed to stdout when you run Logstash: 
 
-To start your Logstash pipeline, configure the Logstash instance to read from a file using the
-{logstash}plugins-inputs-file.html[file] input plugin.
+[source,json]
+--------------------------------------------------------------------------------
+    stdout { codec => rubydebug }
+--------------------------------------------------------------------------------
 
-Edit the `first-pipeline.conf` file to add the following text:
+When you're done, the contents of `first-pipeline.conf` should look like this:
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
-    file {
-        path => "/path/to/logstash-tutorial.log"
-        start_position => beginning <1>
-        ignore_older => 0 <2>
+    beats {
+        port => "5043"
     }
 }
+# The filter part of this file is commented out to indicate that it is
+# optional.
+# filter {
+#
+# }
+output {
+    stdout { codec => rubydebug }
+}
+--------------------------------------------------------------------------------
+
+To verify your configuration, run the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f first-pipeline.conf --config.test_and_exit
+--------------------------------------------------------------------------------
+
+The `--config.test_and_exit` option parses your configuration file and reports any errors.
+
+If the configuration file passes the configuration test, start Logstash with the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f first-pipeline.conf --config.reload.automatic
+--------------------------------------------------------------------------------
+
+The `--config.reload.automatic` option enables automatic config reloading so that you don't have to stop and restart Logstash
+every time you modify the configuration file.
+
+If your pipeline is working correctly, you should see a series of events like the following written to the console:
+
+[source,json]
 --------------------------------------------------------------------------------
+{
+    "@timestamp" => 2016-10-11T20:54:06.733Z,
+        "offset" => 325,
+      "@version" => "1",
+          "beat" => {
+        "hostname" => "My-MacBook-Pro.local",
+            "name" => "My-MacBook-Pro.local"
+    },
+    "input_type" => "log",
+          "host" => "My-MacBook-Pro.local",
+        "source" => "/path/to/file/logstash-tutorial.log",
+       "message" => "83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "type" => "log",
+          "tags" => [
+        [0] "beats_input_codec_plain_applied"
+    ]
+}
+...
 
-<1> The default behavior of the file input plugin is to monitor a file for new information, in a manner similar to the
-UNIX `tail -f` command. To change this default behavior and process the entire file, we need to specify the position
-where Logstash starts processing the file.
-<2> The default behavior of the file input plugin is to ignore files whose last modification is greater than 86400s. To change this default behavior and process the tutorial file (which date can be much older than a day), we need to specify to not ignore old files.
+--------------------------------------------------------------------------------
 
-Replace `/path/to/` with the actual path to the location of `logstash-tutorial.log` in your file system.
 
 [float]
 [[configuring-grok-filter]]
-===== Parsing Web Logs with the Grok Filter Plugin
+==== Parsing Web Logs with the Grok Filter Plugin
+
+Now you have a working pipeline that reads log lines from Filebeat. However you'll notice that the format of the log messages
+is not ideal. You want to parse the log messages to create specific, named fields from the logs.
+To do this, you'll use the `grok` filter plugin.
 
 The {logstash}plugins-filters-grok.html[`grok`] filter plugin is one of several plugins that are available by default in
 Logstash. For details on how to manage Logstash plugins, see the <<working-with-plugins,reference documentation>> for
 the plugin manager.
 
-Because the `grok` filter plugin looks for patterns in the incoming log data, configuration requires you to make
-decisions about how to identify the patterns that are of interest to your use case. A representative line from the web
-server log sample looks like this:
+The `grok` filter plugin enables you to parse the unstructured log data into something structured and queryable.
+
+Because the `grok` filter plugin looks for patterns in the incoming log data, configuring the plugin requires you to
+make decisions about how to identify the patterns that are of interest to your use case. A representative line from the
+web server log sample looks like this:
 
 [source,shell]
 --------------------------------------------------------------------------------
@@ -90,8 +199,7 @@ HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-
 Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
 --------------------------------------------------------------------------------
 
-The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. In this tutorial, use
-the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
+The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. To parse the data, you can use the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
 
 [horizontal]
 *Information*:: *Field Name*
@@ -107,7 +215,7 @@ Bytes served:: `bytes`
 Referrer URL:: `referrer`
 User agent:: `agent`
 
-Edit the `first-pipeline.conf` file to add the following text:
+Edit the `first-pipeline.conf` file and replace the entire `filter` section with the following text:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -118,47 +226,85 @@ filter {
 }
 --------------------------------------------------------------------------------
 
-After processing, the sample line has the following JSON representation:
+When you're done, the contents of `first-pipeline.conf` should look like this:
 
 [source,json]
 --------------------------------------------------------------------------------
-{
-"clientip" : "83.149.9.216",
-"ident" : ,
-"auth" : ,
-"timestamp" : "04/Jan/2015:05:13:42 +0000",
-"verb" : "GET",
-"request" : "/presentations/logstash-monitorama-2013/images/kibana-search.png",
-"httpversion" : "HTTP/1.1",
-"response" : "200",
-"bytes" : "203023",
-"referrer" : "http://semicomplete.com/presentations/logstash-monitorama-2013/",
-"agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+input {
+    beats {
+        port => "5043"
+    }
+}
+filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+}
+output {
+    stdout { codec => rubydebug }
 }
 --------------------------------------------------------------------------------
 
-[float]
-[[indexing-parsed-data-into-elasticsearch]]
-===== Indexing Parsed Data into Elasticsearch
+Save your changes. Because you've enabled automatic config reloading, you don't have to restart Logstash to 
+pick up your changes. However, you do need to force Filebeat to read the log file from scratch. To do this,
+go to the terminal window where Filebeat is running and press Ctrl+C to shut down Filebeat. Then delete the
+Filebeat registry file. For example, run:
 
-Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
-Elasticsearch cluster. Edit the `first-pipeline.conf` file to add the following text after the `input` section:
+[source,shell]
+--------------------------------------------------------------------------------
+sudo rm data/registry
+--------------------------------------------------------------------------------
+
+Since Filebeat stores the state of each file it harvests in the registry, deleting the registry file forces
+Filebeat to read all the files it's harvesting from scratch.
+
+Next, restart Filebeat with the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+sudo ./filebeat -e -c filebeat.yml -d "publish"
+--------------------------------------------------------------------------------
+
+After processing the log file with the grok pattern, the events will have the following JSON representation:
 
 [source,json]
 --------------------------------------------------------------------------------
-output {
-    elasticsearch {
-    }
+{
+        "request" => "/presentations/logstash-monitorama-2013/images/kibana-search.png",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+         "offset" => 325,
+           "auth" => "-",
+          "ident" => "-",
+     "input_type" => "log",
+           "verb" => "GET",
+         "source" => "/path/to/file/logstash-tutorial.log",
+        "message" => "83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+           "type" => "log",
+           "tags" => [
+        [0] "beats_input_codec_plain_applied"
+    ],
+       "referrer" => "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
+     "@timestamp" => 2016-10-11T21:04:36.167Z,
+       "response" => "200",
+          "bytes" => "203023",
+       "clientip" => "83.149.9.216",
+       "@version" => "1",
+           "beat" => {
+        "hostname" => "My-MacBook-Pro.local",
+            "name" => "My-MacBook-Pro.local"
+    },
+           "host" => "My-MacBook-Pro.local",
+    "httpversion" => "1.1",
+      "timestamp" => "04/Jan/2015:05:13:42 +0000"
 }
 --------------------------------------------------------------------------------
 
-With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes Logstash
-and Elasticsearch to be running on the same instance. You can specify a remote Elasticsearch instance using `hosts`
-configuration like `hosts => "es-machine:9092"`.
+Notice that the event includes the original message, but the log message is also broken down into specific fields.
+
 
 [float]
 [[configuring-geoip-plugin]]
-===== Enhancing Your Data with the Geoip Filter Plugin
+==== Enhancing Your Data with the Geoip Filter Plugin
 
 In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing
 data. As an example, the {logstash}plugins-filters-geoip.html[`geoip`] plugin looks up IP addresses, derives geographic
@@ -169,32 +315,26 @@ of the `first-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
-geoip {
-    source => "clientip"
-}
+    geoip {
+        source => "clientip"
+    }
 --------------------------------------------------------------------------------
 
-The `geoip` plugin configuration requires data that is already defined as separate fields. Make sure that the `geoip`
-section is after the `grok` section of the configuration file.
+The `geoip` plugin configuration requires you to specify the name of the source field that contains the IP address to look up. In this example, the `clientip` field contains the IP address.
 
-Specify the name of the field that contains the IP address to look up. In this tutorial, the field name is `clientip`.
+Since filters are evaluated in sequence, make sure that the `geoip` section is after the `grok` section of 
+the configuration file and that both the `grok` and `geoip` sections are nested within the `filter` section. 
 
-[float]
-[[testing-initial-pipeline]]
-===== Testing Your Initial Pipeline
-
-At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
-like this:
+When you're done, the contents of `first-pipeline.conf` should look like this:
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
-    file {
-        path => "/Users/palecur/logstash-1.5.2/logstash-tutorial-dataset"
-        start_position => beginning
+    beats {
+        port => "5043"
     }
 }
-filter {
+ filter {
     grok {
         match => { "message" => "%{COMBINEDAPACHELOG}"}
     }
@@ -203,236 +343,372 @@ filter {
     }
 }
 output {
-    elasticsearch {}
-    stdout {}
+    stdout { codec => rubydebug }
 }
 --------------------------------------------------------------------------------
 
-To verify your configuration, run the following command:
+Save your changes. To force Filebeat to read the log file from scratch, as you did earlier, shut down Filebeat (press Ctrl+C), 
+delete the registry file, and then restart Filebeat with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf --configtest
+sudo ./filebeat -e -c filebeat.yml -d "publish"
 --------------------------------------------------------------------------------
 
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
+Notice that the event now contains geographic location information:
 
-[source,shell]
+[source,json]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf
+{
+        "request" => "/presentations/logstash-monitorama-2013/images/kibana-search.png",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "geoip" => {
+              "timezone" => "Europe/Moscow",
+                    "ip" => "83.149.9.216",
+              "latitude" => 55.7522,
+        "continent_code" => "EU",
+             "city_name" => "Moscow",
+         "country_code2" => "RU",
+          "country_name" => "Russia",
+              "dma_code" => nil,
+         "country_code3" => "RU",
+           "region_name" => "Moscow",
+              "location" => [
+            [0] 37.6156,
+            [1] 55.7522
+        ],
+           "postal_code" => "101194",
+             "longitude" => 37.6156,
+           "region_code" => "MOW"
+    },
+    ...
 --------------------------------------------------------------------------------
 
-Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin:
 
-[source,shell]
+[float]
+[[indexing-parsed-data-into-elasticsearch]]
+==== Indexing Your Data into Elasticsearch
+
+Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
+Elasticsearch cluster. Edit the `first-pipeline.conf` file and replace the entire `output` section with the following
+text:
+
+[source,json]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=response=200'
+output {
+    elasticsearch {
+        hosts => [ "localhost:9200" ]
+    }
+}
 --------------------------------------------------------------------------------
 
-Replace $DATE with the current date, in YYYY.MM.DD format.
+With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes that
+Logstash and Elasticsearch are running on the same instance. You can specify a remote Elasticsearch instance by using
+the `hosts` configuration to specify something like `hosts => [ "es-machine:9092" ]`.
 
-Since our sample has just one 200 HTTP response, we get one hit back:
+At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
+something like this:
 
 [source,json]
 --------------------------------------------------------------------------------
-{"took":2,
-"timed_out":false,
-"_shards":{"total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.5351382,
-  "hits":[{"_index":"logstash-2015.07.30",
-    "_type":"logs",
-    "_id":"AU7gqOky1um3U6ZomFaF",
-    "_score":1.5351382,
-    "_source":{"message":"83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
-      "@version":"1",
-      "@timestamp":"2015-07-30T20:30:41.265Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"83.149.9.216",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:13:45 +0000",
-      "verb":"GET",
-      "request":"/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"52878",
-      "referrer":"\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
-      "agent":"\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\""
-      }
-    }]
-  }
+input {
+    beats {
+        port => "5043"
+    }
+}
+ filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+    geoip {
+        source => "clientip"
+    }
+}
+output {
+    elasticsearch {
+        hosts => [ "localhost:9200" ]
+    }
 }
 --------------------------------------------------------------------------------
 
-Try another search for the geographic information derived from the IP address:
+Save your changes. To force Filebeat to read the log file from scratch, as you did earlier, shut down Filebeat (press Ctrl+C), 
+delete the registry file, and then restart Filebeat with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=geoip.city_name=Buffalo'
+sudo ./filebeat -e -c filebeat.yml -d "publish"
 --------------------------------------------------------------------------------
 
-Replace $DATE with the current date, in YYYY.MM.DD format.
+[float]
+[[testing-initial-pipeline]]
+===== Testing Your Pipeline
 
-Only one of the log entries comes from Buffalo, so the query produces a single response:
+Now that the Logstash pipeline is configured to index the data into an
+Elasticsearch cluster, you can query Elasticsearch.
+
+Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin. 
+Replace $DATE with the current date, in YYYY.MM.DD format:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=response=200'
+--------------------------------------------------------------------------------
+
+NOTE: The date used in the index name is based on UTC, not the timezone where Logstash is running.
+If the query returns `index_not_found_exception`, make sure that `logstash-$DATE` reflects the actual
+name of the index. To see a list of available indexes, use this query: `curl 'localhost:9200/_cat/indices?v'`. 
+
+You should get multiple hits back. For example:
 
 [source,json]
 --------------------------------------------------------------------------------
-{"took":3,
-"timed_out":false,
-"_shards":{
-  "total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.03399,
-  "hits":[{"_index":"logstash-2015.07.31",
-    "_type":"logs",
-    "_id":"AU7mK3CVSiMeBsJ0b_EP",
-    "_score":1.03399,
-    "_source":{
-      "message":"108.174.55.234 - - [04/Jan/2015:05:27:45 +0000] \"GET /?flav=rss20 HTTP/1.1\" 200 29941 \"-\" \"-\"",
-      "@version":"1",
-      "@timestamp":"2015-07-31T22:11:22.347Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"108.174.55.234",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:27:45 +0000",
-      "verb":"GET",
-      "request":"/?flav=rss20",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"29941",
-      "referrer":"\"-\"",
-      "agent":"\"-\"",
-      "geoip":{
-        "ip":"108.174.55.234",
-        "country_code2":"US",
-        "country_code3":"USA",
-        "country_name":"United States",
-        "continent_code":"NA",
-        "region_name":"NY",
-        "city_name":"Buffalo",
-        "postal_code":"14221",
-        "latitude":42.9864,
-        "longitude":-78.7279,
-        "dma_code":514,
-        "area_code":716,
-        "timezone":"America/New_York",
-        "real_region_name":"New York",
-        "location":[-78.7279,42.9864]
+{
+  "took" : 21,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 98,
+    "max_score" : 3.745223,
+    "hits" : [
+      {
+        "_index" : "logstash-2016.10.11",
+        "_type" : "log",
+        "_id" : "AVe14gMiYMkU36o_eVsA",
+        "_score" : 3.745223,
+        "_source" : {
+          "request" : "/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
+          "agent" : "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "geoip" : {
+            "timezone" : "Europe/Moscow",
+            "ip" : "83.149.9.216",
+            "latitude" : 55.7522,
+            "continent_code" : "EU",
+            "city_name" : "Moscow",
+            "country_code2" : "RU",
+            "country_name" : "Russia",
+            "dma_code" : null,
+            "country_code3" : "RU",
+            "region_name" : "Moscow",
+            "location" : [
+              37.6156,
+              55.7522
+            ],
+            "postal_code" : "101194",
+            "longitude" : 37.6156,
+            "region_code" : "MOW"
+          },
+          "offset" : 2932,
+          "auth" : "-",
+          "ident" : "-",
+          "input_type" : "log",
+          "verb" : "GET",
+          "source" : "/path/to/file/logstash-tutorial.log",
+          "message" : "83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "type" : "log",
+          "tags" : [
+            "beats_input_codec_plain_applied"
+          ],
+          "referrer" : "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
+          "@timestamp" : "2016-10-11T22:34:25.317Z",
+          "response" : "200",
+          "bytes" : "52878",
+          "clientip" : "83.149.9.216",
+          "@version" : "1",
+          "beat" : {
+            "hostname" : "My-MacBook-Pro.local",
+            "name" : "My-MacBook-Pro.local"
+          },
+          "host" : "My-MacBook-Pro.local",
+          "httpversion" : "1.1",
+          "timestamp" : "04/Jan/2015:05:13:45 +0000"
+        }
       }
-    }
-  }]
- }
-}
+    }, 
+    ...
+    
+--------------------------------------------------------------------------------
+
+Try another search for the geographic information derived from the IP address.
+Replace $DATE with the current date, in YYYY.MM.DD format:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=geoip.city_name=Buffalo'
 --------------------------------------------------------------------------------
 
+A few log entries come from Buffalo, so the query produces the following response:
+
+[source,json]
+--------------------------------------------------------------------------------
+{
+  "took" : 3,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 3,
+    "max_score" : 2.6390574,
+    "hits" : [
+      {
+        "_index" : "logstash-2016.10.11",
+        "_type" : "log",
+        "_id" : "AVe14gMjYMkU36o_eVtO",
+        "_score" : 2.6390574,
+        "_source" : {
+          "request" : "/?flav=rss20",
+          "agent" : "\"-\"",
+          "geoip" : {
+            "timezone" : "America/New_York",
+            "ip" : "108.174.55.234",
+            "latitude" : 42.9864,
+            "continent_code" : "NA",
+            "city_name" : "Buffalo",
+            "country_code2" : "US",
+            "country_name" : "United States",
+            "dma_code" : 514,
+            "country_code3" : "US",
+            "region_name" : "New York",
+            "location" : [
+              -78.7279,
+              42.9864
+            ],
+            "postal_code" : "14221",
+            "longitude" : -78.7279,
+            "region_code" : "NY"
+          },
+          "offset" : 21471,
+          "auth" : "-",
+          "ident" : "-",
+          "input_type" : "log",
+          "verb" : "GET",
+          "source" : "/path/to/file/logstash-tutorial.log",
+          "message" : "108.174.55.234 - - [04/Jan/2015:05:27:45 +0000] \"GET /?flav=rss20 HTTP/1.1\" 200 29941 \"-\" \"-\"",
+          "type" : "log",
+          "tags" : [
+            "beats_input_codec_plain_applied"
+          ],
+          "referrer" : "\"-\"",
+          "@timestamp" : "2016-10-11T22:34:25.318Z",
+          "response" : "200",
+          "bytes" : "29941",
+          "clientip" : "108.174.55.234",
+          "@version" : "1",
+          "beat" : {
+            "hostname" : "My-MacBook-Pro.local",
+            "name" : "My-MacBook-Pro.local"
+          },
+          "host" : "My-MacBook-Pro.local",
+          "httpversion" : "1.1",
+          "timestamp" : "04/Jan/2015:05:27:45 +0000"
+        }
+      },
+     ...
+     
+--------------------------------------------------------------------------------
+
+If you are using Kibana to visualize your data, you can also explore the Filebeat data in Kibana:
+
+image::static/images/kibana-filebeat-data.png[Discovering Filebeat data in Kibana]
+
+See the {filebeat}filebeat-getting-started.html[Filebeat getting started docs] for info about loading the Kibana
+index pattern for Filebeat.
+
+You've successfully created a pipeline that uses Filebeat to take Apache web logs as input, parses those logs to
+create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Next, you
+learn how to create a pipeline that uses multiple input and output plugins.
+
 [[multiple-input-output-plugins]]
-==== Multiple Input and Output Plugins
+=== Stitching Together Multiple Input and Output Plugins
 
 The information you need to manage often comes from several disparate sources, and use cases can require multiple
 destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these
 requirements.
 
-This example creates a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
+In this section, you create a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
 sends the information to an Elasticsearch cluster as well as writing the information directly to a file.
 
 [float]
 [[twitter-configuration]]
-==== Reading from a Twitter feed
+==== Reading from a Twitter Feed
 
-To add a Twitter feed, you need several pieces of information:
+To add a Twitter feed, you use the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin. To
+configure the plugin, you need several pieces of information:
 
-* A _consumer_ key, which uniquely identifies your Twitter app, which is Logstash in this case.
+* A _consumer key_, which uniquely identifies your Twitter app.
 * A _consumer secret_, which serves as the password for your Twitter app.
-* One or more _keywords_ to search in the incoming feed.
+* One or more _keywords_ to search in the incoming feed. The example shows using "cloud" as a keyword, but you can use whatever you want.
 * An _oauth token_, which identifies the Twitter account using this app.
 * An _oauth token secret_, which serves as the password of the Twitter account.
 
-Visit https://dev.twitter.com/apps to set up a Twitter account and generate your consumer key and secret, as well as
-your OAuth token and secret.
+Visit https://dev.twitter.com/apps[https://dev.twitter.com/apps] to set up a Twitter account and generate your consumer
+key and secret, as well as your access token and secret. See the docs for the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin if you're not sure how to generate these keys. 
 
-Use this information to add the following lines to the `input` section of the `first-pipeline.conf` file:
+Like you did earlier when you worked on <<advanced-pipeline>>, create a config file (called `second-pipeline.conf`) that
+contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make
+sure you pass in the correct config file name when you run Logstash. 
+
+Add the following lines to the `input` section of the `second-pipeline.conf` file, substituting your values for the 
+placeholder values shown here:
 
 [source,json]
 --------------------------------------------------------------------------------
-twitter {
-    consumer_key =>
-    consumer_secret =>
-    keywords =>
-    oauth_token =>
-    oauth_token_secret =>
-}
+    twitter {
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
+    }
 --------------------------------------------------------------------------------
 
 [float]
 [[configuring-lsf]]
-==== The Filebeat Client
+==== Configuring Filebeat to Send Log Lines to Logstash
 
-The https://github.com/elastic/beats/tree/master/filebeat[filebeat] client is a lightweight, resource-friendly tool that
-collects logs from files on the server and forwards these logs to your Logstash instance for processing. The
-Filebeat client uses the secure Beats protocol to communicate with your Logstash instance. The
-lumberjack protocol is designed for reliability and low latency. Filebeat uses the computing resources of
-the machine hosting the source data, and the {logstash}plugins-inputs-beats.html[Beats input] plugin minimizes the
-resource demands on the Logstash instance.
+As you learned earlier in <<configuring-filebeat>>, the https://github.com/elastic/beats/tree/master/filebeat[Filebeat]
+client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your
+Logstash instance for processing.
 
-NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your
-Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
-same machine.
-
-Default Logstash configuration includes the {logstash}plugins-inputs-beats.html[Beats input plugin], which is
-designed to be resource-friendly. To install Filebeat on your data source machine, download the
-appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page].
-
-Create a configuration file for Filebeat similar to the following example:
+After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
+directory, and replace the contents with the following lines. Make sure `paths` points to your syslog: 
 
 [source,shell]
 --------------------------------------------------------------------------------
-filebeat:
-  prospectors:
-    -
-      paths:
-        - "/path/to/sample-log" <2>
-      fields:
-        type: syslog
-output:
-  logstash:
-    hosts: ["localhost:5043"]
-  tls:
-    certificate: /path/to/ssl-certificate.crt <2>
-    certificate_key: /path/to/ssl-certificate.key
-    certificate_authorities: /path/to/ssl-certificate.crt
-    timeout: 15
-
-<1> Path to the file or files that Filebeat processes.
-<2> Path to the SSL certificate for the Logstash instance.
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/*.log <1>
+  fields:
+    type: syslog <2>
+output.logstash:
+  hosts: ["localhost:5043"]
 --------------------------------------------------------------------------------
 
-Save this configuration file as `filebeat.yml`.
+<1> Absolute path to the file or files that Filebeat processes.
+<2> Adds a field called `type` with the value `syslog` to the event.
+
+Save your changes. 
+
+To keep the configuration simple, you won't specify TLS/SSL settings as you would in a real world
+scenario.
 
 Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the `input` section
-of the `first-pipeline.conf` file:
+of the `second-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
-beats {
-    port => "5043"
-    ssl => true
-    ssl_certificate => "/path/to/ssl-cert" <1>
-    ssl_key => "/path/to/ssl-key" <2>
-}
+    beats {
+        port => "5043"
+    }
 --------------------------------------------------------------------------------
 
-<1> Path to the SSL certificate that the Logstash instance uses to authenticate itself to Filebeat.
-<2> Path to the key for the SSL certificate.
-
 [float]
 [[logstash-file-output]]
 ==== Writing Logstash Data to a File
@@ -441,23 +717,23 @@ You can configure your Logstash pipeline to write data directly to a file with t
 {logstash}plugins-outputs-file.html[`file`] output plugin.
 
 Configure your Logstash instance to use the `file` output plugin by adding the following lines to the `output` section
-of the `first-pipeline.conf` file:
+of the `second-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
-file {
-    path => /path/to/target/file
-}
+    file {
+        path => "/path/to/target/file"
+    }
 --------------------------------------------------------------------------------
 
 [float]
 [[multiple-es-nodes]]
-==== Writing to multiple Elasticsearch nodes
+==== Writing to Multiple Elasticsearch Nodes
 
 Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as
 providing redundant points of entry into the cluster when a particular node is unavailable.
 
-To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the output section of the `first-pipeline.conf` file to read:
+To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the `output` section of the `second-pipeline.conf` file to read:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -470,29 +746,26 @@ output {
 
 Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the `hosts`
 parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
-default port for Elasticsearch is `9200` and can be omitted in the configuration above.
+the default port for Elasticsearch is `9200` and can be omitted in the configuration above.
 
 [float]
 [[testing-second-pipeline]]
 ===== Testing the Pipeline
 
-At this point, your `first-pipeline.conf` file looks like this:
+At this point, your `second-pipeline.conf` file looks like this: 
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
     twitter {
-        consumer_key =>
-        consumer_secret =>
-        keywords =>
-        oauth_token =>
-        oauth_token_secret =>
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
     }
     beats {
         port => "5043"
-        ssl => true
-        ssl_certificate => "/path/to/ssl-cert"
-        ssl_key => "/path/to/ssl-key"
     }
 }
 output {
@@ -500,7 +773,7 @@ output {
         hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
     }
     file {
-        path => /path/to/target/file
+        path => "/path/to/target/file"
     }
 }
 --------------------------------------------------------------------------------
@@ -515,98 +788,48 @@ At the data source machine, run Filebeat with the following command:
 sudo ./filebeat -e -c filebeat.yml -d "publish"
 --------------------------------------------------------------------------------
 
-Filebeat will attempt to connect on port 5403. Until Logstash starts with an active Beats plugin, there
+Filebeat will attempt to connect on port 5043. Until Logstash starts with an active Beats plugin, there
 wonâ€™t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
 
 To verify your configuration, run the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf --configtest
+bin/logstash -f second-pipeline.conf --config.test_and_exit
 --------------------------------------------------------------------------------
 
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
+The `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file
+passes the configuration test, start Logstash with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf
+bin/logstash -f second-pipeline.conf
 --------------------------------------------------------------------------------
 
 Use the `grep` utility to search in the target file to verify that information is present:
 
 [source,shell]
 --------------------------------------------------------------------------------
-grep Mozilla /path/to/target/file
+grep syslog /path/to/target/file
 --------------------------------------------------------------------------------
 
 Run an Elasticsearch query to find the same information in the Elasticsearch cluster:
 
 [source,shell]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-2015.07.30/_search?q=agent=Mozilla'
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=fields.type:syslog'
 --------------------------------------------------------------------------------
 
-[[stalled-shutdown]]
-=== Stalled Shutdown Detection
-
-Shutting down a running Logstash instance involves the following steps:
-
-* Stop all input, filter and output plugins
-* Process all in-flight events
-* Terminate the Logstash process
-
-The following conditions affect the shutdown process:
-
-* An input plugin receiving data at a slow pace.
-* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy
-query.
-* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+Replace $DATE with the current date, in YYYY.MM.DD format.
 
-These situations make the duration and success of the shutdown process unpredictable.
+To see data from the Twitter feed, try this query:
 
-Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
-This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
-worker threads.
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'http://localhost:9200/logstash-$DATE/_search?pretty&q=client:iphone'
+--------------------------------------------------------------------------------
 
-To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--allow-unsafe-shutdown` flag when
-you start Logstash.
+Again, remember to replace $DATE with the current date, in YYYY.MM.DD format. 
 
-[[shutdown-stall-example]]
-==== Stall Detection Example
 
-In this example, slow filter execution prevents the pipeline from clean shutdown. By starting Logstash with the
-`--allow-unsafe-shutdown` flag, quitting with *Ctrl+C* results in an eventual shutdown that loses 20 events.
 
-========
-[source,shell]
-% bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } \
-                     output { stdout { codec => dots } }' -w 1 --allow-unsafe-shutdown
-Default settings used: Filter workers: 1
-Logstash startup completed
-^CSIGINT received. Shutting down the pipeline. {:level=>:warn}
-Received shutdown signal, but pipeline is still waiting for in-flight events
-to be processed. Sending another ^C will force quit Logstash, but this may cause
-data loss. {:level=>:warn}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
-The shutdown process appears to be stalled due to busy or blocked plugins. Check
-    the logs for more information.
-{:level=>:error}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
-Forcefully quitting logstash.. {:level=>:fatal}
-========
-
-When `--allow-unsafe-shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index 64b15808d43..4ee6ee8e5c9 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -1,72 +1,310 @@
 [[breaking-changes]]
 == Breaking changes
 
-**Breaking changes in 2.2**
-Although 2.2 is fully compatible with configurations from older versions, there are some architectural 
-changes to the pipeline that users need to take into consideration before deploying in production. 
-These changes are not strictly "breaking" in the semantic versioning sense, but they make Logstash behave differently 
-in runtime, and can also affect performance. We have compiled such a list in the <<_upgrading_logstash_to_2.2>> section. 
-Please review it before deploying 2.2 version.
+This section discusses the changes that you need to be aware of when migrating your application to Logstash 5.0 from the previous major release of Logstash (2.x).
 
-**Changes in 2.0**
+[float]
+=== Changes in Logstash Core
 
-Version 2.0 of Logstash has some changes that are incompatible with previous versions of Logstash. This section discusses
-what you need to be aware of when migrating to this version.
+These changes can impact any instance of Logstash and are plugin agnostic, but only if you are using the features that are impacted.
 
 [float]
-== Elasticsearch Output Default
+==== Application Settings
+
+[IMPORTANT]
+Logstash 5.0 introduces a new way to <<logstash-settings-file, configure application settings>> for Logstash through a
+`logstash.yml` file.
 
-Starting with the 2.0 release of Logstash, the default Logstash output for Elasticsearch is HTTP. To use the `node` or
-`transport` protocols, download the https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-elasticsearch_java.html[Elasticsearch Java plugin]. The
-Logstash HTTP output to Elasticsearch now supports sniffing.
+This file is typically located in `${LS_HOME}/config`, or `/etc/logstash` when installed via packages. Logstash will not be 
+able to start without this file, so please make sure to pass in `--path.settings` if you are starting Logstash manually
+after installing it via a package (RPM, DEB).
 
-NOTE: The `elasticsearch_java` plugin has two versions specific to the version of the underlying Elasticsearch cluster.
-Be sure to specify the correct value for the `--version` option during installation:
-* For Elasticsearch versions before 2.0, use the command
-`bin/plugin install --version 1.5.x logstash-output-elasticsearch_java`
-* For Elasticsearch versions 2.0 and after, use the command
-`bin/plugin install --version 2.0.0 logstash-output-elasticsearch_java`
+[source,bash]
+----------------------------------
+bin/logstash --path.settings /path/to/logstash.yml
+----------------------------------
 
 [float]
-=== Configuration Changes
+==== URL Changes for DEB/RPM Packages
 
-The Elasticsearch output plugin configuration has the following changes:
+The previous `packages.elastic.co` URL has been altered to `artifacts.elastic.co`. 
+Ensure you update your repository files before running the upgrade process, or 
+your operating system may not see the new packages.
 
-* The `host` configuration option is now `hosts`, allowing you to specify multiple hosts and associated ports in the
-`myhost:9200` format
-* New options: `bind_host`, `bind_port`, `cluster`, `embedded`, `embedded_http_port`, `port`, `sniffing_delay`
-* The `max_inflight_requests` option, which was deprecated in the 1.5 release, is now removed
-* Since the `hosts` option allows specification of ports for the hosts, the redundant `port` option is now removed
-* The `node_name` and `protocol` options have been moved to the `elasticsearch_java` plugin
+[float]
+==== Release Packages
 
-The following deprecated configuration settings are removed in this release:
+When Logstash 5.0 is installed via DEB or RPM packages, it now uses `/usr/share/logstash` to
+install binaries. Previously it used to install in `/opt/logstash` directory. This change was done to make the user experience consistent with other products in the Elastic Stack.
 
-* input plugin configuration settings: `debug`, `format`, `charset`, `message_format`
-* output plugin configuration settings: `type`, `tags`, `exclude_tags`.
-* filter plugin configuration settings: `type`, `tags`, `exclude_tags`.
+[cols="3", options="header"]
+|===
+| |DEB |RPM
+|Logstash 2.x
+|`/opt/logstash`
+|`/opt/logstash`
+|Logstash 5.0 
+|`/usr/share/logstash`
+|`/usr/share/logstash`
+|===
 
-Configuration files with these settings present are invalid and prevent Logstash from starting.
+A complete directory layout is described in <<dir-layout>>. This will likely impact any scripts that you may have written
+to support installing or manipulating Logstash, such as via Puppet.
 
 [float]
-=== Kafka Output Configuration Changes
+==== Default Logging Level
+
+The default log severity level changed to `INFO` instead of `WARN` to match Elasticsearch. Existing logs
+(in core and plugins) were too noisy at the `INFO` level, so we audited our log messages and switched some of them to
+`DEBUG` level.
+
+You can use the new `logstash.yml` file to configure the `log.level` setting or continue to pass the new
+`--log.level` command line flag.
 
-The 2.0 release of Logstash includes a new version of the Kafka output plugin with significant configuration changes.
-Please compare the documentation pages for the
-https://www.elastic.co/guide/en/logstash/1.5/plugins-outputs-kafka.html[Logstash 1.5] and
-https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-kafka.html[Logstash 2.0] versions of the Kafka output plugin
-and update your configuration files accordingly.
+[source,bash]
+----------------------------------
+bin/logstash --log.level warn
+----------------------------------
 
 [float]
-=== Metrics Filter Changes
-Prior implementations of the metrics filter plugin used dotted field names. Elasticsearch does not allow field names to
-have dots, beginning with version 2.0, so a change was made to use sub-fields instead of dots in this plugin. Please note
-that these changes make version 3.0.0 of the metrics filter plugin incompatible with previous releases.
+==== Plugin Manager Renamed
+
+`bin/plugin` has been renamed to `bin/logstash-plugin`. This occurred in Logstash 2.3 and it was mainly prevent `PATH` being
+polluted when other components of the Elastic Stack are installed on the same machine. Also, this provides a foundation
+for future change which will allow Elastic Stack packs to be installed via this script.
+
+Logstash 5.0 also adds a `remove` option, which is an alias for the now-deprecated `uninstall` option.
+
+As with earlier releases, the updated script allows both online and offline plugin installation. For example, to install a
+plugin named â€œmy-pluginâ€, itâ€™s as simple as running:
+
+[source,bash]
+----------------------------------
+bin/logstash-plugin install my-plugin
+----------------------------------
 
+Similar to the package changes, this is likely to impact and scripts that have been written to follow Logstash
+installations.
+
+Like earlier releases of Logstash, most plugins are bundled directly with Logstash, so no additional action is required
+while upgrading from earlier Logstash releases. However, if you are attempting to install a non-bundled plugin, then make
+sure that it supports Logstash 5.0 before upgrading!
+
+[float]
+==== Logstash with All Plugins Download
+
+The Logstash All Plugins download option has been removed. For users previously using this option as a convenience for
+offline plugin management purposes (air-gapped environments), please see the <<offline-plugins>> documentation page.
+
+There were 17 plugins removed from 5.0 default bundle. These plugins can still be installed manually for use:
+
+* logstash-codec-oldlogstashjson
+* logstash-input-eventlog
+* logstash-input-log4j
+* logstash-input-zeromq
+* logstash-filter-anonymize
+* logstash-filter-checksum
+* logstash-filter-multiline
+* logstash-output-email
+* logstash-output-exec
+* logstash-output-ganglia
+* logstash-output-gelf
+* logstash-output-hipchat
+* logstash-output-juggernaut
+* logstash-output-lumberjack
+* logstash-output-nagios_nsca
+* logstash-output-opentsdb
+* logstash-output-zeromq
 
 [float]
-=== Filter Worker Default Change
+==== Command Line Interface
+
+Some CLI Options changed in Logstash 5.0. If you were using the â€œlong formâ€ of the <<command-line-flags,options>>,
+then this will impact the way that you launch Logstash. They were changed to match the `logstash.yml` format used to
+simplify future setup, as well as behave in the same way as other products in the Elastic Stack. For example, hereâ€™s two
+before-and-after examples. In Logstash 2.x, you may have run something:
+
+[source,bash]
+----------------------------------
+bin/logstash --config my.conf --pipeline-workers 8 <1>
+bin/logstash -f my.conf -w 8 <2>
+----------------------------------
+<1> Long form options `config` and `pipeline-workers` are used here.
+<2> Short form options `f` and `w` (aliases for the former` are used here.
+
+But, in Logstash 5.0, this becomes:
+
+[source,bash]
+----------------------------------
+bin/logstash --path.config my.conf --pipeline.workers 8 <1>
+bin/logstash -f my.conf -w 8 <2>
+----------------------------------
+<1> Long form options are changed to reflect the new options.
+<2> Short form options are unchanged.
+
+NOTE: None of the short form options have changed!
+
+[float]
+==== RSpec testing script
+
+The `rspec` script is no longer bundled with Logstash release artifacts. This script has been used previously to 
+run unit tests for validating Logstash configurations. While this was useful to some users, this mechanism assumed that Logstash users 
+were familiar with the RSpec framework, which is a Ruby testing framework.
+
+
+[float]
+=== Breaking Changes in Plugins
+
+[float]
+==== Elasticsearch Output `workers` Setting Removed
+
+Starting with Logstash 5.0, the `workers` setting in the Elasticsearch output
+plugin is no longer supported. Pipelines that specify this setting will no
+longer start up. You need to specify the `pipeline.workers` setting at the
+pipeline level instead. For more information about setting
+`pipeline.workers`, see <<logstash-settings-file>>.
+
+[float]
+==== Elasticsearch Output Index Template
+
+The index template for Elasticsearch 5.0 has been changed to reflect
+https://www.elastic.co/guide/en/elasticsearch/reference/5.0/breaking_50_mapping_changes.html[Elasticsearch's mapping changes]. Most
+importantly, the subfield for string multi-fields has changed from `.raw` to `.keyword` to match Elasticsearch's default
+behavior. The impact of this change to various user groups is detailed below:
+
+** New Logstash 5.0 and Elasticsearch 5.0 users: Multi-fields (often called sub-fields) use `.keyword` from the
+outset. In Kibana, you can use `my_field.keyword` to perform aggregations against text-based fields, in the same way that it 
+used to be `my_field.raw`.
+** Existing users with custom templates: Using a custom template means that you control the template completely, and our 
+template changes do not impact you.
+** Existing users with default template: Logstash does not force you to upgrade templates if one already exists. If you
+intend to move to the new template and want to use `.keyword`, you will most likely want to reindex existing data so that it
+also uses the `.keyword` field, unless you are able to transition from `.raw` to `.keyword`. Elasticsearch's
+{ref}docs-reindex.html[reindexing API] can help move your data from using `.raw` subfields to `.keyword`, thereby avoiding any
+transition time. You _can_ use a custom template to get both `.raw` and `.keyword` so that you can wait until all `.raw` data
+has stopped existing before transitioning to only using `.keyword`; this will waste some storage space and memory, but it does
+help users to avoid having to relearn operations.
+
+[float]
+[[plugin-versions]]
+==== Plugin Versions
+
+Logstash is unique amongst the Elastic Stack with respect to its plugins. Unlike Elasticsearch and Kibana, which both 
+require plugins to be targeted to a specific release, Logstashâ€™s plugin ecosystem provides more flexibility so that it can
+support outside ecosystems _within the same release_. Unfortunately, 
+that flexibility can cause issues when handling upgrades.
+
+Non-standard plugins must always be checked for compatibility, but some bundled plugins are upgraded in order to remain 
+compatible with the tools or frameworks that they use for communication. For example, the
+<<plugins-inputs-kafka, Kafka Input>> and <<plugins-outputs-kafka, Kafka Output>> plugins serve as a primary example of 
+such compatibility changes. The latest version of the Kafka plugins is only compatible with Kafka 0.10, but as the 
+compatibility matrices show: earlier plugin versions are required for earlier versions of Kafka (e.g., Kafka 0.9).
+
+Automatic upgrades generally lead to improved features and support, but network layer changes like those above may make part
+of your architecture incompatible. You should always test your Logstash configurations in a test environment before
+deploying to production, which would catch these kinds of issues. If you do face such an issue, then you should also check
+the specific pluginâ€™s page to see how to get a compatible, older plugin version if necessary.
+
+For example, if you upgrade to Logstash 5.0, but you want to run against Kafka 0.9, then you need to remove the
+bundled plugin(s) that only work with Kafka 0.10 and replace them:
+
+[source,bash]
+----------------------------------
+bin/logstash-plugin remove logstash-input-kafka
+bin/logstash-plugin remove logstash-output-kafka
+bin/logstash-plugin install --version 4.0.0 logstash-input-kafka
+bin/logstash-plugin install --version 4.0.1 logstash-output-kafka
+----------------------------------
+
+The version numbers were found by checking the compatibility matrix for the individual plugins.
+
+[float]
+==== Kafka Input Configuration Changes
+
+As described in the section <<plugin-versions, above>>, the Kafka plugin has been updated to bring in new consumer features. 
+In addition, to the plugin being incompatible with 0.8.x version of the Kafka broker, _most_ of the config options have 
+been changed to match the new consumer configurations from the Kafka Java consumer. Here's a list of important config options that have changed:
+
+* `topic_id` is renamed to `topics` and accepts an array of topics to consume from.
+* `zk_connect` has been dropped; you should use `bootstrap_servers`. There is no need for the consumer to go through ZooKeeper.
+* `consumer_id` is renamed to `client_id`.
+
+We recommend users of the Kafka plugin to check the documentation for the latest <<plugins-inputs-kafka, config options>>.
+
+[float]
+==== File Input
+
+The <<plugins-inputs-file, File Input>> `SinceDB` file is now saved at `<path.data>/plugins/inputs/file` location,
+where `path.data` is the path defined in the new `logstash.yml` file.
+
+[cols="2", options="header"]
+|===
+| |Default `sincedb_path`
+|Logstash 2.x
+|`$HOME/.sincedb*`
+|Logstash 5.0 
+|`<path.data>/plugins/inputs/file`
+|===
+
+If you have manually specified `sincedb_path` as part of the configuration, this change will not affect you.
+If you are moving from Logstash 2.x to Logstash 5.0, and you would like to use the existing SinceDB file,
+then it must be copied over to `path.data` manually to use the save state (or the path needs to be changed to point to it).
+
+[float]
+==== GeoIP Filter
+
+The GeoIP filter has been updated to use MaxMind's GeoIP2 database. Previous GeoIP version is now considered legacy 
+by MaxMind. As a result of this, `.dat` version files are no longer supported, and only `.mmdb` format is supported. 
+The new database will not include ASN data in the basic free database file.
+
+Previously, when the filter encountered an IP address for which there were no results in the database, the event
+would just pass through the filter without modification. It will now add a `_geoip_lookup_failure` tag to the
+event which will allow for some subsequent stage of the pipeline to identify those events and perform some other
+operation. To simply get the same behavior as the earlier versions, just add a filter conditional on that tag
+which then drops the tag from the event.
+
+[float]
+=== Ruby Filter and Custom Plugin Developers
+
+With the migration to the new <<event-api>>, we have changed how you can access internal data compared to previous release. 
+The `event` object no longer returns a reference to the data. Instead, it returns a copy. This might change how you perform
+manipulation of your data, especially when working with nested hashes. When working with nested hashes, itâ€™s recommended that 
+you use the <<logstash-config-field-references, `field reference` syntax>> instead of using multiple square brackets.
+
+As part of this change, Logstash has introduced new Getter/Setter APIs for accessing information in the `event` object.
+
+**Examples:**
+
+Prior to Logstash 5.0, you may have used Ruby filters like so:
+
+[source, js]
+----------------------------------
+filter {
+  ruby {
+    code => "event['name'] = 'Logstash'"
+  }
+  ruby {
+    code => "event['product']['version'] = event['major'] + '.' + event['minor']"
+  }
+}
+----------------------------------
+
+The above syntax, which uses the `event` object as a reference, is no longer supported in
+Logstash 5.0. Fortunately, the change to make it work is very simple:
+
+[source, js]
+----------------------------------
+filter {
+  ruby {
+    code => "event.set('name', 'Logstash')"
+  }
+  ruby {
+    code => "event.set('[product][version]', event.get('major') + '.' + event.get('minor'))"
+  }
+}
+----------------------------------
+
+NOTE: Moving from the old syntax to the new syntax, it can be easy to miss that `['product']['version']` became
+`'[product][version]'`. The quotes moved from inside of the square brackets to outside of the square brackets!
 
-Starting with the 2.0 release of Logstash, the default value of the `filter_workers` configuration option for filter
-plugins is half of the available CPU cores, instead of 1. This change increases parallelism in filter execution for
-resource-intensive filtering operations. You can continue to use the `-w` flag to manually set the value for this option,
-as in previous releases.
+The <<event-api>> documentation describes the available syntax in great detail.
diff --git a/docs/static/command-line-flags.asciidoc b/docs/static/command-line-flags.asciidoc
deleted file mode 100644
index 2b08c8a7d2e..00000000000
--- a/docs/static/command-line-flags.asciidoc
+++ /dev/null
@@ -1,74 +0,0 @@
-[[command-line-flags]]
-=== Command-line flags
-
-Logstash has the following flags. You can use the `--help` flag to display this information.
-
-[source,shell]
-----------------------------------
--f, --config CONFIGFILE
- Load the Logstash config from a specific file, directory, or a wildcard. If
- given a directory or wildcard, config files will be read from the directory in
- alphabetical order.
-
--e CONFIGSTRING
- Use the given string as the configuration data. Same syntax as the config file.
- If not input is specified, 'stdin { type => stdin }' is default. If no output
- is specified, 'stdout { codec => rubydebug }}' is default.
-
--w, --filterworkers COUNT
- Sets the number of pipeline workers (threads) to run for filter and output
- processing (default: number of cores).
- If you find that events are backing up, or that the CPU is not saturated, consider increasing
- this number to better utilize machine processing power.
-
--b, --pipeline-batch-size SIZE
- This parameter defines the maximum number of events an individual worker thread will collect
- before attempting to execute its filters and outputs. Default is 125 events.
- Larger batch sizes are generally more efficient, but come at the cost of increased memory
- overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
- variable to effectively use the option.
-
--u, --pipeline-batch-delay DELAY_IN_MS
- When creating pipeline event batches, how long to wait while polling for the next event.
- Default is 5ms.
-
--l, --log FILE
- Log to a given path. Default is to log to stdout
-
---verbose
- Increase verbosity to the first level (info), less verbose.
-
---debug
- Increase verbosity to the last level (trace), more verbose.
-
--V, --version
-  Display the version of Logstash.
-
--p, --pluginpath
-  A path of where to find plugins. This flag can be given multiple times to include
-  multiple paths. Plugins are expected to be in a specific directory hierarchy:
-  'PATH/logstash/TYPE/NAME.rb' where TYPE is 'inputs' 'filters', 'outputs' or 'codecs'
-  and NAME is the name of the plugin.
-
--t, --configtest
-  Checks configuration and then exit. Note that grok patterns are not checked for
-  correctness with this flag.
-  Logstash can read multiple config files from a directory. If you combine this
-  flag with `--debug`, Logstash will log the combined config file, annotating the
-  individual config blocks with the source file it came from.
-  
--r, --[no-]auto-reload
-  Monitor configuration changes and reload the configuration whenever it is changed.
-  
---reload-interval RELOAD_INTERVAL
-  Specifies how often Logstash checks the config files for changes. The default is every 3 seconds.
-
---http-host WEB_API_HTTP_HOST 
-  Web API binding host (default: "127.0.0.1")
-
---http-port WEB_API_HTTP_PORT
-  Web API http port (default: 9600)
-
--h, --help
-  Print help
-----------------------------------
diff --git a/docs/static/configuration.asciidoc b/docs/static/configuration.asciidoc
index 72aaeae5731..370bf373183 100644
--- a/docs/static/configuration.asciidoc
+++ b/docs/static/configuration.asciidoc
@@ -83,25 +83,38 @@ The settings you can configure vary according to the plugin type. For informatio
 === Value Types
 
 A plugin can require that the value for a setting be a
-certain type, such as boolean or hash. The following value
+certain type, such as boolean, list, or hash. The following value
 types are supported.
 
 [[array]]
-[float]
 ==== Array
 
-An array can be a single string value or multiple values. If you specify the same
-setting multiple times, it appends to the array.
+This type is now mostly deprecated in favor of using a standard type like `string` with the plugin defining the `:list => true` property for better type checking. It is still needed to handle lists of hashes or mixed types where type checking is not desired. 
+
+Example:
+
+[source,js]
+----------------------------------
+  users => [ {id => 1, name => bob}, {id => 2, name => jane} ]
+----------------------------------
+
+[[list]]
+[float]
+==== Lists
+
+Not a type in and of itself, but a property types can have.
+This makes it possible to type check multiple values.
+Plugin authors can enable list checking by specifying `:list => true` when declaring an argument.
 
 Example:
 
 [source,js]
 ----------------------------------
   path => [ "/var/log/messages", "/var/log/*.log" ]
-  path => "/data/mysql/mysql.log"
+  uris => [ "http://elastic.co", "http://example.net" ]
 ----------------------------------
 
-This example configures `path` to be an array that contains an element for each of the three strings.
+This example configures `path`, which is a `string` to be a list that contains an element for each of the three strings. It also will configure the `uris` parameter to be a list of URIs, failing if any of the URIs provided are not valid.
 
 
 [[boolean]]
@@ -203,6 +216,21 @@ Example:
   my_password => "password"
 ----------------------------------
 
+[[uri]]
+[float]
+==== URI
+
+A URI can be anything from a full URL like 'http://elastic.co/' to a simple identifier
+like 'foobar'. If the URI contains a password such as 'http://user:pass@example.net' the password
+portion of the URI will not be logged or printed.
+
+Example:
+[source,js]
+----------------------------------
+  my_uri => "http://foo:bar@example.net"
+----------------------------------
+
+
 [[path]]
 [float]
 ==== Path
@@ -250,7 +278,7 @@ input { # comments can appear at the end of a line, too
 ----------------------------------
 
 [[event-dependent-configuration]]
-=== Event Dependent Configuration
+=== Accessing Event Data and Fields in the Configuration
 
 The logstash agent is a processing pipeline with 3 stages: inputs -> filters ->
 outputs. Inputs generate events, filters modify them, outputs ship them
@@ -366,7 +394,7 @@ What's an expression? Comparison tests, boolean logic, and so on!
 You can use the following comparison operators:
 
 * equality: `==`,  `!=`,  `<`,  `>`,  `<=`, `>=`
-* regexp: `=~`, `!~`
+* regexp: `=~`, `!~` (checks a pattern on the right against a string value on the left)
 * inclusion: `in`, `not in`
 
 The supported boolean operators are:
@@ -387,7 +415,7 @@ For example, the following conditional uses the mutate filter to remove the fiel
 ----------------------------------
 filter {
   if [action] == "login" {
-    mutate { remove => "secret" }
+    mutate { remove_field => "secret" }
   }
 }
 ----------------------------------
@@ -406,7 +434,7 @@ output {
 }
 ----------------------------------
 
-The `in` conditional enables you to compare against the value of a field:
+You can use the `in` operator to test whether a field contains a specific string, key, or (for lists) element:
 
 [source,js]
 ----------------------------------
@@ -433,7 +461,7 @@ filter {
 ----------------------------------
 
 You use the `not in` conditional the same way. For example,
-you could use `not in` to only route events to elasticsearch
+you could use `not in` to only route events to Elasticsearch
 when `grok` is successful:
 
 [source,js]
@@ -445,13 +473,20 @@ output {
 }
 ----------------------------------
 
+You can check for the existence of a specific field, but there's currently no way to differentiate between a field that
+doesn't exist versus a field that's simply false. The expression `if [foo]` returns `false` when:
+
+* `[foo]` doesn't exist in the event,
+* `[foo]` exists in the event, but is false, or
+* `[foo]` exists in the event, but is null
+
 For more complex examples, see <<using-conditionals, Using Conditionals>>.
 
 [float]
 [[metadata]]
 ==== The @metadata field
 
-In Logstash 1.5 there is a new, special field, called `@metadata`.  The contents
+In Logstash 1.5 and later, there is a special field called `@metadata`.  The contents
 of `@metadata` will not be part of any of your events at output time, which
 makes it great to use for conditionals, or extending and building event fields
 with field reference and sprintf formatting.
@@ -484,15 +519,16 @@ Let's see what comes out:
 ----------------------------------
 
 $ bin/logstash -f ../test.conf
-Logstash startup completed
+Pipeline main started
 asdf
 {
-       "message" => "asdf",
+    "@timestamp" => 2016-06-30T02:42:51.496Z,
       "@version" => "1",
-    "@timestamp" => "2015-03-18T23:09:29.595Z",
           "host" => "example.com",
-          "show" => "This data will be in the output"
+          "show" => "This data will be in the output",
+       "message" => "asdf"
 }
+
 ----------------------------------
 
 The "asdf" typed in became the `message` field contents, and the conditional
@@ -513,18 +549,18 @@ Let's see what the output looks like with this change:
 [source,ruby]
 ----------------------------------
 $ bin/logstash -f ../test.conf
-Logstash startup completed
+Pipeline main started
 asdf
 {
-       "message" => "asdf",
-      "@version" => "1",
-    "@timestamp" => "2015-03-18T23:10:19.859Z",
-          "host" => "example.com",
-          "show" => "This data will be in the output",
+    "@timestamp" => 2016-06-30T02:46:48.565Z,
      "@metadata" => {
            "test" => "Hello",
         "no_show" => "This data will not be in the output"
-    }
+    },
+      "@version" => "1",
+          "host" => "example.com",
+          "show" => "This data will be in the output",
+       "message" => "asdf"
 }
 ----------------------------------
 
@@ -565,13 +601,13 @@ configuration a sample date string and see what comes out:
 [source,ruby]
 ----------------------------------
 $ bin/logstash -f ../test.conf
-Logstash startup completed
+Pipeline main started
 02/Mar/2014:15:36:43 +0100
 {
-       "message" => "02/Mar/2014:15:36:43 +0100",
+    "@timestamp" => 2014-03-02T14:36:43.000Z,
       "@version" => "1",
-    "@timestamp" => "2014-03-02T14:36:43.000Z",
-          "host" => "example.com"
+          "host" => "example.com",
+       "message" => "02/Mar/2014:15:36:43 +0100"
 }
 ----------------------------------
 
@@ -600,86 +636,59 @@ output {
 ----------------------------------
 
 [[environment-variables]]
-=== Using Environment Variables in Configuration
+=== Using Environment Variables in the Configuration
+
 ==== Overview
 
-* You can set environment variable references into Logstash plugins configuration using `${var}` or `$var`.
-* Each reference will be replaced by environment variable value at Logstash startup.
+* You can set environment variable references in the configuration for Logstash plugins by using `${var}`.
+* At Logstash startup, each reference will be replaced by the value of the environment variable.
 * The replacement is case-sensitive.
 * References to undefined variables raise a Logstash configuration error.
-* A default value can be given by using the form `${var:default value}`.
-* You can add environment variable references in any plugin option type : string, number, boolean, array or hash.
-* Environment variables are immutable. If you update the environment variable, you'll have to restart Logstash to pick the updated value.
+* You can give a default value by using the form `${var:default value}`. Logstash uses the default value if the
+environment variable is undefined.
+* You can add environment variable references in any plugin option type : string, number, boolean, array, or hash.
+* Environment variables are immutable. If you update the environment variable, you'll have to restart Logstash to pick up the updated value.
 
 ==== Examples
 
-[cols="a,a,a"]
-|==================================
-|Logstash config source	|Environment 	|Logstash config result
+The following examples show you how to use environment variables to set the values of some commonly used
+configuration options.
 
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => "$TCP_PORT"
-  }
-}
-----
+===== Setting the TCP Port 
+
+Here's an example that uses an environment variable to set the TCP port:
 
-|
-[source,shell]
-----
-export TCP_PORT=12345
-----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => 12345
-  }
-}
-----
-|
 [source,ruby]
-----
+----------------------------------
 input {
   tcp {
     port => "${TCP_PORT}"
   }
 }
-----
+----------------------------------
+
+Now let's set the value of `TCP_PORT`:
 
-|
 [source,shell]
 ----
 export TCP_PORT=12345
 ----
-|
+
+At startup, Logstash uses the following configuration: 
+
 [source,ruby]
-----
+----------------------------------
 input {
   tcp {
     port => 12345
   }
 }
-----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => "${TCP_PORT}"
-  }
-}
-----
+----------------------------------
+
+If the `TCP_PORT` environment variable is not set, Logstash returns a configuration error.
+
+You can fix this problem by specifying a default value: 
 
-|
-No TCP_PORT defined
-|
-Raise a logstash configuration error
-|
 [source,ruby]
 ----
 input {
@@ -689,9 +698,8 @@ input {
 }
 ----
 
-|
-No TCP_PORT defined
-|
+Now, instead of returning a configuration error if the variable is undefined, Logstash uses the default:
+
 [source,ruby]
 ----
 input {
@@ -700,31 +708,13 @@ input {
   }
 }
 ----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => "${TCP_PORT:54321}"
-  }
-}
-----
 
-|
-[source,shell]
-----
-export TCP_PORT=12345
-----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => 12345
-  }
-}
-----
-|
+If the environment variable is defined, Logstash uses the value specified for the variable instead of the default. 
+
+===== Setting the Value of a Tag
+
+Here's an example that uses an environment variable to set the value of a tag:
+
 [source,ruby]
 ----
 filter {
@@ -734,12 +724,15 @@ filter {
 }
 ----
 
-|
+Let's set the value of `ENV_TAG`:
+
 [source,shell]
 ----
 export ENV_TAG="tag2"
 ----
-|
+
+At startup, Logstash uses the following configuration: 
+
 [source,ruby]
 ----
 filter {
@@ -748,7 +741,11 @@ filter {
   }
 }
 ----
-|
+
+===== Setting a File Path
+
+Here's an example that uses an environment variable to set the path to a log file:
+
 [source,ruby]
 ----
 filter {
@@ -759,12 +756,16 @@ filter {
   }
 }
 ----
-|
+
+Let's set the value of `HOME`:
+
 [source,shell]
 ----
 export HOME="/path"
 ----
-|
+
+At startup, Logstash uses the following configuration: 
+
 [source,ruby]
 ----
 filter {
@@ -775,7 +776,7 @@ filter {
   }
 }
 ----
-|==================================
+
 
 [[config-examples]]
 === Logstash Configuration Examples
@@ -951,7 +952,7 @@ This example labels all events using the `type` field, but doesn't actually pars
 Similarly, you can use conditionals to direct events to particular outputs. For example, you could:
 
 * alert nagios of any apache events with status 5xx
-* record any 4xx status to elasticsearch
+* record any 4xx status to Elasticsearch
 * record all status code hits via statsd
 
 To tell nagios about any http event that has a 5xx status code, you
diff --git a/docs/static/contributing-patch.asciidoc b/docs/static/contributing-patch.asciidoc
index 470841574f2..c126e0bdc1f 100644
--- a/docs/static/contributing-patch.asciidoc
+++ b/docs/static/contributing-patch.asciidoc
@@ -15,7 +15,7 @@ Plugins are subclasses of a Logstash base class. A plugin's base class defines c
 ==== Input Plugins
 
 Input plugins ingest data from an external source. Input plugins are always associated with a codec. An input plugin 
-always has an associated codec plugin. Input and codec plugins operate in conjuction to create a Logstash event and add 
+always has an associated codec plugin. Input and codec plugins operate in conjunction to create a Logstash event and add 
 that event to the processing queue. An input codec is a subclass of the `LogStash::Inputs::Base` class.
 
 .Input API
@@ -276,7 +276,7 @@ describe LogStash::Outputs::ZeroMQ do
   end
 end
 
-. To add the missing test, use an instance of the ZeroMQ output and a substitute logger. This examle uses an RSpec feature 
+. To add the missing test, use an instance of the ZeroMQ output and a substitute logger. This example uses an RSpec feature 
 called _test doubles_ as the substitute logger.
 +
 Add the following lines to `zeromq_spec.rb`, after `describe LogStash::Outputs::ZeroMQ do` and before `context "when in 
@@ -326,7 +326,7 @@ end
 To run this test:
 
 . Open a terminal window
-. Mavigate to the cloned plugin folder
+. Navigate to the cloned plugin folder
 . The first time you run the test, run the command `bundle install`
 . Run the command `bundle exec rspec`
 
diff --git a/docs/static/contributing-to-logstash.asciidoc b/docs/static/contributing-to-logstash.asciidoc
index 238b26fa4fe..5c72062018e 100644
--- a/docs/static/contributing-to-logstash.asciidoc
+++ b/docs/static/contributing-to-logstash.asciidoc
@@ -14,7 +14,7 @@ Since plugins can now be developed and deployed independently of the Logstash
 core, there are documents which guide you through the process of coding and
 deploying your own plugins:
 
-
+* <<plugin-generator,Generating a New Plugin>>
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html[How to write a Logstash input plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_codec_plugin.html[How to write a Logstash codec plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_filter_plugin.html[How to write a Logstash filter plugin]
@@ -24,9 +24,9 @@ deploying your own plugins:
 * <<submitting-plugin,Submitting a Plugin>>
 
 [float]
-==== Plugin API Changes added[2.0]
+==== Plugin Shutdown APIs
 
-The 2.0 release of Logstash changes how input plugins shut down to increase shutdown reliability. There are three methods
+Starting in Logstash 2.0, we changed how input plugins shut down to increase shutdown reliability. There are three methods
 for plugin shutdown: `stop`, `stop?`, and `close`.
 
 * Call the `stop` method from outside the plugin thread. This method signals the plugin to stop.
@@ -37,7 +37,7 @@ exit. The `close` method is a a new name for the method known as `teardown` in p
 The `shutdown`, `finished`, `finished?`, `running?`, and `terminating?` methods are redundant and no longer present in the
 Plugin Base class.
 
-Sample code for the new plugin shutdown APIs is https://github.com/logstash-plugins/logstash-input-example/blob/master/lib/logstash/inputs/example.rb[available].
+Sample code for the plugin shutdown APIs is https://github.com/logstash-plugins/logstash-input-example/blob/master/lib/logstash/inputs/example.rb[available].
 
 [float]
 === Extending Logstash core
diff --git a/docs/static/deploying.asciidoc b/docs/static/deploying.asciidoc
index e57cd85554b..93bc2c47fe2 100644
--- a/docs/static/deploying.asciidoc
+++ b/docs/static/deploying.asciidoc
@@ -1,146 +1,254 @@
 [[deploying-and-scaling]]
-=== Deploying and Scaling Logstash
+== Deploying and Scaling Logstash
 
-As your use case for Logstash evolves, the preferred architecture at a given scale will change. This section discusses
-a range of Logstash architectures in increasing order of complexity, starting from a minimal installation and adding
-elements to the system. The example deployments in this section write to an Elasticsearch cluster, but Logstash can
-write to a large variety of {logstash}output-plugins.html[endpoints].
+The Elastic Stack is used for tons of use cases, from operational log and
+metrics analytics, to enterprise and application search. Making sure your data
+gets scalably, durably, and securely transported to Elasticsearch is extremely
+important, especially for mission critical environments.
+
+The goal of this document is to highlight the most common architecture patterns
+for Logstash and how to effectively scale as your deployment grows. The focus
+will be around the operational log, metrics, and security analytics use cases
+because they tend to require larger scale deployments. The deploying and scaling
+recommendations provided here may vary based on your own requirements.
 
 [float]
-[[deploying-minimal-install]]
-==== The Minimal Installation
+[[deploying-getting-started]]
+=== Getting Started
 
-The minimal Logstash installation has one Logstash instance and one Elasticsearch instance. These instances are
-directly connected. Logstash uses an {logstash}input-plugins.html[_input plugin_] to ingest data and an
-Elasticsearch {logstash}output-plugins.html[_output plugin_] to index the data in Elasticsearch, following the Logstash
-{logstash}pipeline.html[_processing pipeline_]. A Logstash instance has a fixed pipeline constructed at startup,
-based on the instanceâ€™s configuration file. You must specify an input plugin. Output defaults to `stdout`, and the
-filtering section of the pipeline, which is discussed in the next section, is optional.
+For first time users, if you simply want to tail a log file to grasp the power
+of the Elastic Stack, we recommend trying
+{filebeat}filebeat-modules-overview.html[Filebeat Modules]. Filebeat Modules
+enable you to quickly collect, parse, and index popular log types and view
+pre-built Kibana dashboards within minutes.
+{metricbeat}metricbeat-modules.html[Metricbeat Modules] provide a similar
+experience, but with metrics data. In this context, Beats will ship data
+directly to Elasticsearch where {ref}ingest.html[Ingest Nodes] will process
+and index your data.
 
-image::static/images/deploy_1.png[]
+image::static/images/deploy1.png[]
 
 [float]
-[[deploying-filter-threads]]
-==== Using Filters
+==== Introducing Logstash
+What are the main benefits for integrating Logstash into your architecture?
+
+* Scale through ingestion spikes - Logstash has an adaptive disk-based
+buffering system that will absorb incoming throughput, therefore mitigating
+backpressure
+* Ingest from other data sources like databases, S3, or messaging queues
+* Emit data to multiple destinations like S3, HDFS, or write to a file
+* Compose more sophisticated processing pipelines with conditional dataflow logic
 
-Log data is typically unstructured, often contains extraneous information that isnâ€™t relevant to your use case, and
-sometimes is missing relevant information that can be derived from the log contents. You can use a
-{logstash}filter-plugins.html[filter plugin] to parse the log into fields, remove unnecessary information, and derive
-additional information from the existing fields. For example, filters can derive geolocation information from an IP
-address and add that information to the logs, or parse and structure arbitrary text with the
-{logstash}plugins-filters-grok.html[grok] filter.
+[float]
+[[scaling-ingest]]
+=== Scaling Ingest
 
-Adding a filter plugin can significantly affect performance, depending on the amount of computation the filter plugin
-performs, as well as on the volume of the logs being processed. The `grok` filterâ€™s regular expression computation is
-particularly resource-intensive. One way to address this increased demand for computing resources is to use
-parallel processing on multicore machines. Use the `-w` switch to set the number of execution threads for Logstash
-filtering tasks. For example the `bin/logstash -w 8` command uses eight different threads for filter processing.
+Beats and Logstash make ingest awesome. Together, they provide a comprehensive
+solution that is scalable and resilient. What can you expect?
 
-image::static/images/deploy_2.png[]
+* Horizontal scalability, high availability, and variable load handling
+* Message durability with at-least-once delivery guarantees
+* End-to-end secure transport with authentication and wire encryption
 
 [float]
-[[deploying-filebeat]]
-==== Using Filebeat
+==== Beats and Logstash
+
+Beats run across thousands of edge host servers, collecting, tailing, and
+shipping logs to Logstash. Logstash serves as the centralized streaming
+engine for data unification and enrichment. The
+<<plugins-inputs-beats,Beats input plugin>> exposes a secure,
+acknowledgement-based endpoint for Beats to send data to Logstash.
+
+image::static/images/deploy2.png[]
+
+NOTE: Enabling persistent queues is strongly recommended, and these
+architecture characteristics assume that they are enabled. We encourage you to
+review the <<persistent-queues>> documentation for feature benefits and more
+details on resiliency.
+
+[float]
+==== Scalability
+
+Logstash is horizontally scalable and can form groups of nodes running the same
+pipeline. Logstashâ€™s adaptive buffering capabilities will facilitate smooth
+streaming even through variable throughput loads. If the Logstash layer becomes
+an ingestion bottleneck, simply add more nodes to scale out. Here are a few
+general recommendations:
+
+* Beats should {filebeat}load-balancing.html[load balance] across a group of
+Logstash nodes.
+* A minimum of two Logstash nodes are recommended for high availability.
+* Itâ€™s common to deploy just one Beats input per Logstash node, but multiple
+Beats inputs can also be deployed per Logstash node to expose independent
+endpoints for different data sources.
+
+[float]
+==== Resiliency
+
+When using https://www.elastic.co/products/beats/filebeat[Filebeat] or
+https://www.elastic.co/products/beats/winlogbeat[Winlogbeat] for log collection
+within this ingest flow, *at-least-once delivery* is guaranteed. Both the
+communication protocols, from Filebeat or Winlogbeat to Logstash, and from
+Logstash to Elasticsearch, are synchronous and support acknowledgements. The
+other Beats donâ€™t yet have support for acknowledgements.
 
-https://www.elastic.co/guide/en/beats/filebeat/current/index.html[Filebeat] is a lightweight, resource-friendly tool
-written in Go that collects logs from files on the server and forwards these logs to other machines for processing.
-Filebeat uses the https://www.elastic.co/guide/en/beats/libbeat/current/index.html[Beats] protocol to communicate with a
-centralized Logstash instance. Configure the Logstash instances that receive Beats data to use the
-{logstash}plugins-inputs-beats.html[Beats input plugin].
+Logstash persistent queues provide protection across node failures. For
+disk-level resiliency in Logstash, itâ€™s important to ensure disk redundancy.
+For on-premise deployments, it's recommended that you configure RAID. When
+running in the cloud or a containerized environment, itâ€™s recommended that you
+use persistent disks with replication strategies that reflect your data SLAs.
 
-Filebeat uses the computing resources of the machine hosting the source data, and the Beats input plugin minimizes the
-resource demands on the Logstash instance, making this architecture attractive for use cases with resource constraints.
+NOTE: Make sure `queue.checkpoint.writes: 1` is set for at-least-once
+guarantees. For more details, see the
+<<durability-persistent-queues,persistent queue durability>> documentation.
 
-image::static/images/deploy_3.png[]
+[float]
+==== Processing
+
+Logstash will commonly extract fields with <<plugins-filters-grok,grok>> or
+<<plugins-filters-dissect,dissect>>, augment
+<<plugins-filters-geoip,geographical>> info, and can further enrich events with
+<<plugins-filters-translate,file>>, <<plugins-filters-jdbc_streaming,database>>,
+or <<plugins-filters-elasticsearch,Elasticsearch>> lookup datasets. Be aware
+that processing complexity can affect overall throughput and CPU utilization.
+Make sure to check out the other <<filter-plugins,available filter plugins>>.
 
 [float]
-[[deploying-larger-cluster]]
-==== Scaling to a Larger Elasticsearch Cluster
+==== Secure Transport
 
-Typically, Logstash does not communicate with a single Elasticsearch node, but with a cluster that comprises several
-nodes. By default, Logstash uses the HTTP protocol to move data into the cluster.
+Enterprise-grade security is available across the entire delivery chain.
 
-You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the
-indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR
-files and has no performance disadvantages compared to the transport or node protocols. You can secure communications
-that use the HTTP REST APIs with the {shield}[Shield] plugin, which supports SSL and HTTP basic authentication.
+* Wire encryption is recommended for both the transport from
+{filebeat}configuring-ssl-logstash.html[Beats to Logstash] and from
+{xpack-ref}logstash.html[Logstash to Elasticsearch].
+* Thereâ€™s a wealth of security options when communicating with Elasticsearch
+including basic authentication, TLS, PKI, LDAP, AD, and other custom realms.
+To enable Elasticsearch security, consult the
+{xpack-ref}xpack-security.html[X-Pack documentation].
 
-When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically
-load-balance indexing requests across a
-specified set of hosts in the Elasticsearch cluster. Specifying multiple Elasticsearch nodes also provides high availability for the Elasticsearch cluster by routing traffic to active Elasticsearch nodes.
+[float]
+==== Monitoring
 
-You can also use the Elasticsearch Java APIs to serialize the data into a binary representation, using
-the transport protocol. The transport protocol can sniff the endpoint of the request and select an
-arbitrary client or data node in the Elasticsearch cluster.
+When running Logstash 5.2 or greater,
+the https://www.elastic.co/products/x-pack/monitoring[Monitoring UI] provides
+deep visibility into your deployment metrics, helping observe performance and
+alleviate bottlenecks as you scale. Monitoring is an X-Pack feature under the
+Basic License and is therefore *free to use*. To get started, consult the
+{xpack-ref}monitoring-logstash.html[X-Pack Monitoring documentation].
 
-Using the HTTP or transport protocols keep your Logstash instances separate from the Elasticsearch cluster. The node
-protocol, by contrast, has the machine running the Logstash instance join the Elasticsearch cluster, running an
-Elasticsearch instance. The data that needs indexing propagates from this node to the rest of the cluster. Since the
-machine is part of the cluster, the cluster topology is available, making the node protocol a good fit for use cases
-that use a relatively small number of persistent connections.
+If external monitoring is preferred, there are <<monitoring,Monitoring APIs>>
+that return point-in-time metrics snapshots.
 
-You can also use a third-party hardware or software load balancer to handle connections between Logstash and
-external applications.
+[float]
+[[adding-other-sources]]
+=== Adding Other Popular Sources
 
-NOTE: Make sure that your Logstash configuration does not connect directly to Elasticsearch dedicated
-{ref}modules-node.html[master nodes], which perform dedicated cluster management. Connect Logstash to client or data
-nodes to protect the stability of your Elasticsearch cluster.
+Users may have other mechanisms of collecting logging data, and itâ€™s easy to
+integrate and centralize them into the Elastic Stack. Letâ€™s walk through a few
+scenarios:
 
-image::static/images/deploy_4.png[]
+image::static/images/deploy3.png[]
 
 [float]
-[[deploying-message-queueing]]
-==== Managing Throughput Spikes with Message Queueing
+==== TCP, UDP, and HTTP Protocols
 
-When the data coming into a Logstash pipeline exceeds the Elasticsearch cluster's ability to ingest the data, you can
-use a message queue as a buffer. By default, Logstash throttles incoming events when
-indexer consumption rates fall below incoming data rates. Since this throttling can lead to events being buffered at
-the data source, preventing backpressure with message queues becomes an important part of managing your deployment.
+The TCP, UDP, and HTTP protocols are common ways to feed data into Logstash.
+Logstash can expose endpoint listeners with the respective
+<<plugins-inputs-tcp,TCP>>, <<plugins-inputs-udp,UDP>>, and
+<<plugins-inputs-http,HTTP>> input plugins. The data sources enumerated below
+are typically ingested through one of these three protocols.
 
-Adding a message queue to your Logstash deployment also provides a level of protection from data loss. When a Logstash
-instance that has consumed data from the message queue fails, the data can be replayed from the message queue to an
-active Logstash instance.
+NOTE: The UDP protocol does not support application-level acknowledgements, so
+connectivity issues may result in data loss.
 
-Several third-party message queues exist, such as Redis, Kafka, or RabbitMQ. Logstash provides input and output plugins
-to integrate with several of these third-party message queues. When your Logstash deployment has a message queue
-configured, Logstash functionally exists in two phases: shipping instances, which handles data ingestion and storage in
-the message queue, and indexing instances, which retrieve the data from the message queue, apply any configured
-filtering, and write the filtered data to an Elasticsearch index.
+For high availability scenarios, a third-party hardware or software load
+balancer, like HAProxy, should be added to fan out traffic to a group of
+Logstash nodes.
 
-image::static/images/deploy_5.png[]
+[float]
+==== Network and Security Data
+
+Although Beats may already satisfy your data ingest use case, network and
+security datasets come in a variety of forms. Letâ€™s touch on a few other
+ingestion points.
+
+* Network wire data - collect and analyze network traffic with
+https://www.elastic.co/products/beats/packetbeat[Packetbeat].
+* Netflow v5/v9/v10 - Logstash understands data from Netflow/IPFIX exporters
+with the <<plugins-codecs-netflow,Netflow codec>>.
+* Nmap - Logstash accepts and parses Nmap XML data with the
+<<plugins-codecs-nmap,Nmap codec>>.
+* SNMP trap - Logstash has a native <<plugins-inputs-snmptrap,SNMP trap input>>.
+* CEF - Logstash accepts and parses CEF data from systems like Arcsight
+SmartConnectors with the <<plugins-codecs-cef,CEF codec>>. See this
+https://www.elastic.co/blog/integrating-elastic-stack-with-arcsight-siem-part-1[blog series]
+for more details.
 
 [float]
-[[deploying-logstash-ha]]
-==== Multiple Connections for Logstash High Availability
+==== Centralized Syslog Servers
 
-To make your Logstash deployment more resilient to individual instance failures, you can set up a load balancer between
-your data source machines and the Logstash cluster. The load balancer handles the individual connections to the
-Logstash instances to ensure continuity of data ingestion and processing even when an individual instance is unavailable.
+Existing syslog server technologies like rsyslog and syslog-ng generally send
+syslog over to Logstash TCP or UDP endpoints for extraction, processing, and
+persistence. If the data format conforms to RFC3164, it can be fed directly
+to the <<plugins-inputs-syslog,Logstash syslog input>>.
 
-image::static/images/deploy_6.png[]
+[float]
+==== Infrastructure & Application Data and IoT
 
-The architecture in the previous diagram is unable to process input from a specific type, such as an RSS feed or a
-file, if the Logstash instance dedicated to that input type becomes unavailable. For more robust input processing,
-configure each Logstash instance for multiple inputs, as in the following diagram:
+Infrastructure and application metrics can be collected with
+https://www.elastic.co/products/beats/metricbeat[Metricbeat], but applications
+can also send webhooks to a Logstash HTTP input or have metrics polled from an
+HTTP endpoint with the <<plugins-inputs-http_poller,HTTP poller input plugin>>.
 
-image::static/images/deploy_7.png[]
+For applications that log with log4j2, itâ€™s recommended to use the
+SocketAppender to send JSON to the Logstash TCP input. Alternatively, log4j2
+can also log to a file for collection with FIlebeat. Usage of the log4j1
+SocketAppender is not recommended.
 
-This architecture parallelizes the Logstash workload based on the inputs you configure. With more inputs, you can add
-more Logstash instances to scale horizontally. Separate parallel pipelines also increases the reliability of your stack
-by eliminating single points of failure.
+IoT devices like Rasberry Pis, smartphones, and connected vehicles often send
+telemetry data through one of these protocols.
 
 [float]
-[[deploying-scaling]]
-==== Scaling Logstash
+[[integrating-with-messaging-queues]]
+=== Integrating with Messaging Queues
+
+If you are leveraging message queuing technologies as part of your existing
+infrastructure, getting that data into the Elastic Stack is easy. For existing
+users who are utilizing an external queuing layer like Redis or RabbitMQ just
+for data buffering with Logstash, itâ€™s recommended to use Logstash persistent
+queues instead of an external queuing layer. This will help with overall ease
+of management by removing an unnecessary layer of complexity in your ingest
+architecture.
 
-A mature Logstash deployment typically has the following pipeline:
+For users who want to integrate data from existing Kafka deployments or require
+the underlying usage of ephemeral storage, Kafka can serve as a data hub where
+Beats can persist to and Logstash nodes can consume from.
 
-* The _input_ tier consumes data from the source, and consists of Logstash instances with the proper input plugins.
-* The _message queue_ serves as a buffer to hold ingested data and serve as failover protection.
-* The _filter_ tier applies parsing and other processing to the data consumed from the message queue.
-* The _indexing_ tier moves the processed data into Elasticsearch.
+image::static/images/deploy4.png[]
 
-Any of these layers can be scaled by adding computing resources. Examine the performance of these components regularly
-as your use case evolves and add resources as needed. When Logstash routinely throttles incoming events, consider
-adding storage for your message queue. Alternately, increase the Elasticsearch cluster's rate of data consumption by
-adding more Logstash indexing instances.
+The other TCP, UDP, and HTTP sources can persist to Kafka with Logstash as a
+conduit to achieve high availability in lieu of a load balancer. A group of
+Logstash nodes can then consume from topics with the
+<<plugins-inputs-kafka,Kafka input>> to further transform and enrich the data in
+transit.
+
+[float]
+==== Resiliency and Recovery
+
+When Logstash consumes from Kafka, persistent queues should be enabled and will
+add transport resiliency to mitigate the need for reprocessing during Logstash
+node failures. In this context, itâ€™s recommended to use the default persistent
+queue disk allocation size `queue.max_bytes: 1GB`.
+
+If Kafka is configured to retain data for an extended period of time, data can
+be reprocessed from Kafka in the case of disaster recovery and reconciliation.
+
+[float]
+==== Other Messaging Queue Integrations
+
+Although an additional queuing layer is not required, Logstash can consume from
+a myriad of other message queuing technologies like
+<<plugins-inputs-rabbitmq,RabbitMQ>> and <<plugins-inputs-redis,Redis>>. It also
+supports ingestion from hosted queuing services like
+<<plugins-inputs-google_pubsub,Pub/Sub>>, <<plugins-inputs-kinesis,Kinesis>>, and
+<<plugins-inputs-sqs,SQS>>.
diff --git a/docs/static/docker.asciidoc b/docs/static/docker.asciidoc
new file mode 100644
index 00000000000..e470a285d39
--- /dev/null
+++ b/docs/static/docker.asciidoc
@@ -0,0 +1,163 @@
+[[docker]]
+=== Running Logstash on Docker
+Docker images for Logstash are available from the Elastic Docker
+registry.
+
+The base image is https://hub.docker.com/_/centos/[centos:7] and the source
+code can be found on
+https://github.com/elastic/logstash-docker/tree/{branch}[GitHub].
+
+The images are shipped with https://www.elastic.co/products/x-pack[X-Pack]
+installed.
+
+=== Pulling the image
+Obtaining Logstash for Docker is as simple as issuing a +docker
+pull+ command against the Elastic Docker registry.
+
+ifeval::["{release-state}"=="unreleased"]
+
+However, version {logstash_version} of Logstash has not yet been
+released, so no Docker image is currently available for this version.
+
+endif::[]
+
+ifeval::["{release-state}"!="unreleased"]
+
+The Docker image for Logstash {logstash_version} can be retrieved with
+the following command:
+
+["source","sh",subs="attributes"]
+--------------------------------------------
+docker pull {docker-image}
+--------------------------------------------
+
+endif::[]
+
+==== Configuring Logstash for Docker
+
+Logstash differentiates between two types of configuration:
+<<config-setting-files,Settings and Pipeline Configuration>>.
+
+===== Pipeline Configuration
+
+It is essential to place your pipeline configuration where it can be
+found by Logstash. By default, the container will look in
++/usr/share/logstash/pipeline/+ for pipeline configuration files.
+
+In this example we use a bind-mounted volume to provide the
+configuration via the +docker run+ command:
+
+["source","sh",subs="attributes"]
+--------------------------------------------
+docker run --rm -it -v ~/pipeline/:/usr/share/logstash/pipeline/ {docker-image}
+--------------------------------------------
+
+Every file in the host directory +~/pipeline/+ will then be parsed
+by Logstash as pipeline configuration.
+
+If you don't provide configuration to Logstash, it will run with a
+minimal config that listens for messages from the
+<<plugins-inputs-beats,Beats input plugin>> and echoes any that are
+received to `stdout`. In this case, the startup logs will be similar
+to the following:
+
+["source","text"]
+--------------------------------------------
+Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties.
+[2016-10-26T05:11:34,992][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=>"0.0.0.0:5044"}
+[2016-10-26T05:11:35,068][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
+[2016-10-26T05:11:35,078][INFO ][org.logstash.beats.Server] Starting server on port: 5044
+[2016-10-26T05:11:35,078][INFO ][logstash.pipeline        ] Pipeline main started
+[2016-10-26T05:11:35,105][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
+--------------------------------------------
+
+This is the default configuration for the image, defined in
++/usr/share/logstash/pipeline/logstash.conf+.  If this is the
+behaviour that you are observing, ensure that your pipeline
+configuration is being picked up correctly, and that you are replacing
+either +logstash.conf+ or the entire +pipeline+ directory.
+
+===== Settings
+
+The image provides several methods for configuring settings. The conventional
+approach is to provide a custom `logstash.yml` file, but it's
+also possible to use environment variables to define settings.
+
+[[docker-bind-mount-settings]]
+==== Bind-mounted settings files
+
+Settings files can also be provided through bind-mounts. Logstash
+expects to find them at +/usr/share/logstash/config/+.
+
+It's possible to provide an entire directory containing all needed
+files:
+
+["source","sh",subs="attributes"]
+--------------------------------------------
+docker run --rm -it -v ~/settings/:/usr/share/logstash/config/ {docker-image}
+--------------------------------------------
+
+Alternatively, a single file can be mounted:
+
+["source","sh",subs="attributes"]
+--------------------------------------------
+docker run --rm -it -v ~/settings/logstash.yml:/usr/share/logstash/config/logstash.yml {docker-image}
+--------------------------------------------
+
+NOTE: Bind-mounted configuration files will retain the same permissions and
+ownership within the container that they have on the host system. Be sure
+to set permissions such that the files will be readable and, ideally, not
+writeable by the container's +logstash+ user (UID 1000).
+
+===== Custom Images
+
+Bind-mounted configuration is not the only option, naturally. If you
+prefer the _Immutable Infrastructure_ approach, you can prepare a
+custom image containing your configuration by using a +Dockerfile+
+like this one:
+
+["source","dockerfile",subs="attributes"]
+--------------------------------------------
+FROM {docker-image}
+RUN rm -f /usr/share/logstash/pipeline/logstash.conf
+ADD pipeline/ /usr/share/logstash/pipeline/
+ADD config/ /usr/share/logstash/config/
+--------------------------------------------
+
+Be sure to replace or delete `logstash.conf` in your custom image, so
+that you don't retain the example config from the base image.
+
+==== Environment variable configuration
+
+Under Docker, Logstash settings can be configured via environment
+variables. When the container starts, a helper process checks the environment
+for variables that can be mapped to Logstash settings. Settings that are found
+in the environment are merged into `logstash.yml` as the container starts up.
+
+For compatibility with container orchestration systems, these environment
+variables are written in all capitals, with underscores as word
+separators
+
+Some example translations are shown here:
+
+.Example Docker Environment Variables
+[horizontal]
+**Environment Variable**:: **Logstash Setting**
+`PIPELINE_WORKERS`:: `pipeline.workers`
+`LOG_LEVEL`:: `log.level`
+`XPACK_MONITORING_ENABLED`:: `xpack.monitoring.enabled`
+
+In general, any setting listed in the <<logstash-settings-file, settings
+documentation>> can be configured with this technique.
+
+NOTE: Defining settings with environment variables causes `logstash.yml` to
+be modified in place. This behaviour is likely undesirable if `logstash.yml` was
+bind-mounted from the host system. Thus, it is not reccomended to
+combine the bind-mount technique with the environment variable technique. It
+is best to choose a single method for defining Logstash settings.
+
+==== Logging Configuration
+
+Under Docker, Logstash logs go to standard output by default. To
+change this behaviour, use any of the techniques above to replace the
+file at +/usr/share/logstash/config/log4j2.properties+.
diff --git a/docs/static/event-api.asciidoc b/docs/static/event-api.asciidoc
new file mode 100644
index 00000000000..2311e2b7e0c
--- /dev/null
+++ b/docs/static/event-api.asciidoc
@@ -0,0 +1,120 @@
+[[event-api]]
+=== Event API
+
+This section is targeted for plugin developers and users of Logstash's Ruby filter. Below we document recent 
+changes (starting with version 5.0) in the way users have been accessing Logstash's event based data in 
+custom plugins and in the Ruby filter. Note that <<event-dependent-configuration>> 
+data flow in Logstash's config files -- using <<logstash-config-field-references>> -- is 
+not affected by this change, and will continue to use existing syntax.
+
+[float]
+==== Event Object
+
+Event is the main object that encapsulates data flow internally in Logstash and provides an API for the plugin 
+developers to interact with the event's content. Typically, this API is used in plugins and in a Ruby filter to 
+retrieve data and use it for transformations. Event object contains the original data sent to Logstash and any additional 
+fields created during Logstash's filter stages.
+
+In 5.0, we've re-implemented the Event class and its supporting classes in pure Java. Since Event is a critical component 
+in data processing,  a rewrite in Java improves performance and provides efficient serialization when storing data on disk. For the most part, this change aims at keeping backward compatibility and is transparent to the users. To this extent we've updated and published most of the plugins in Logstash's ecosystem to adhere to the new API changes. However, if you are maintaining a custom plugin, or have a Ruby filter, this change will affect you. The aim of this guide is to describe the new API and provide examples to migrate to the new changes.
+
+[float]
+==== Event API
+
+Prior to version 5.0, developers could access and manipulate event data by directly using Ruby hash syntax. For 
+example, `event[field] = foo`. While this is powerful, our goal is to abstract the internal implementation details 
+and provide well-defined getter and setter APIs.
+
+**Get API**
+
+The getter is a read-only access of field-based data in an Event.
+
+**Syntax:** `event.get(field)`
+
+**Returns:** Value for this field or nil if the field does not exist. Returned values could be a string, 
+numeric or timestamp scalar value.
+
+`field` is a structured field sent to Logstash or created after the transformation process. `field` can also 
+be a nested field reference such as `[field][bar]`.
+
+Examples:
+
+[source,ruby]
+--------------------------------------------------
+event.get("foo" ) # => "baz"
+event.get("[foo]") # => "zab"
+event.get("[foo][bar]") # => 1
+event.get("[foo][bar]") # => 1.0
+event.get("[foo][bar]") # =>  [1, 2, 3]
+event.get("[foo][bar]") # => {"a" => 1, "b" => 2}
+event.get("[foo][bar]") # =>  {"a" => 1, "b" => 2, "c" => [1, 2]}
+--------------------------------------------------
+
+Accessing @metadata
+
+[source,ruby]
+--------------------------------------------------
+event.get("[@metadata][foo]") # => "baz"
+--------------------------------------------------
+
+**Set API**
+
+This API can be used to mutate data in an Event. 
+
+**Syntax:** `event.set(field, value)`
+
+**Returns:**  The current Event  after the mutation, which can be used for chainable calls.
+
+Examples:
+
+[source,ruby]
+--------------------------------------------------
+event.set("foo", "baz")
+event.set("[foo]", "zab")
+event.set("[foo][bar]", 1)
+event.set("[foo][bar]", 1.0)
+event.set("[foo][bar]", [1, 2, 3])
+event.set("[foo][bar]", {"a" => 1, "b" => 2})
+event.set("[foo][bar]", {"a" => 1, "b" => 2, "c" => [1, 2]})
+event.set("[@metadata][foo]", "baz")
+--------------------------------------------------
+
+Mutating a collection after setting it in the Event has an undefined behaviour and is not allowed.
+
+[source,ruby]
+--------------------------------------------------
+h = {"a" => 1, "b" => 2, "c" => [1, 2]}
+event.set("[foo][bar]", h)
+
+h["c"] = [3, 4]
+event.get("[foo][bar][c]") # => undefined
+
+Suggested way of mutating collections:
+
+h = {"a" => 1, "b" => 2, "c" => [1, 2]}
+event.set("[foo][bar]", h)
+
+h["c"] = [3, 4]
+event.set("[foo][bar]", h)
+
+# Alternatively,
+event.set("[foo][bar][c]", [3, 4]) 
+--------------------------------------------------
+
+[float]
+==== Ruby Filter
+
+The <<plugins-filters-ruby,Ruby Filter>> can be used to execute any ruby code and manipulate event data using the 
+API described above. For example, using the new API:
+
+[source,ruby]
+--------------------------------------------------
+filter {
+  ruby {
+    code => 'event.set("lowercase_field", event.get("message").downcase)'
+  }  
+}    
+--------------------------------------------------
+
+This filter will lowercase the `message` field, and set it to a new field called `lowercase_field`
+
diff --git a/docs/static/filebeat-modules.asciidoc b/docs/static/filebeat-modules.asciidoc
new file mode 100644
index 00000000000..c9f41aa27a5
--- /dev/null
+++ b/docs/static/filebeat-modules.asciidoc
@@ -0,0 +1,279 @@
+[[filebeat-modules]]
+
+== Working with Filebeat Modules
+
+Starting with version 5.3, Filebeat comes packaged with pre-built 
+{filebeat}filebeat-modules.html[modules] that contain the configurations needed
+to collect, parse, enrich, and visualize data from various log file formats.
+Each Filebeat module consists of one or more filesets that contain ingest node
+pipelines, Elasticsearch templates, Filebeat prospector configurations, and
+Kibana dashboards.
+
+Filebeat modules are a great way to get started, but you might find that ingest
+pipelines don't offer the processing power that you require. If that's the case,
+you'll need to use Logstash.
+
+[float]
+[[graduating-to-Logstash]]
+=== Graduating to Logstash
+
+You may need to graduate to using Logstash instead of ingest pipelines if you
+want to:
+
+* Use multiple outputs. Ingest pipelines were designed to only support
+Elasticsearch as an output, but you may want to use more than one output. For
+example, you may want to archive your incoming data to S3 as well as indexing
+it in Elasticsearch.
+* Use the <<persistent-queues,persistent queue>> feature to handle spikes when
+ingesting data (from Beats and other sources).
+* Take advantage of the richer transformation capabilities in Logstash, such as
+external lookups.
+
+Currently, we don't provide an automatic migration path from ingest pipelines
+to Logstash pipelines (but that's coming). For now, you can follow the steps in
+this section to configure Filebeat and build Logstash pipeline configurations
+that are equivalent to the ingest node pipelines available with the Filebeat
+modules. Then you'll be able to use the same dashboards available with Filebeat
+to visualize your data in Kibana.
+
+Follow the steps in this section to build and run Logstash configurations that
+provide capabilities similar to Filebeat modules.
+
+. Load the Filebeat index pattern and sample Kibana dashboards. To do this, you
+need to run the Filebeat module with the Elasticsearch output enabled and
+specify the `-setup` flag. 
++
+For example, to load the sample dashboards for Nginx, run:
++
+[source,shell]
+----------------------------------------------------------------------
+./filebeat -e -modules=nginx -setup -E "output.elasticsearch.hosts=["http://localhost:9200"]"
+----------------------------------------------------------------------
++
+A connection to Elasticsearch is required for this one-time setup step because
+Filebeat needs to create the index pattern and load the sample dashboards into the
+Kibana index. 
++
+After the template and dashboards are loaded, you'll see the message
+`INFO Elasticsearch template with name 'filebeat' loaded`. You can shut
+down Filebeat.
+
+. Configure Filebeat to send log lines to Logstash.
++
+See <<logstash-config-for-filebeat-modules>> for detailed examples.
+
+. Create a Logstash pipeline configuration that reads from the Beats input and
+parses the log events.
++
+See <<logstash-config-for-filebeat-modules>> for detailed examples.
+
+. Start Filebeat. For example, to start Filebeat in the foreground, use:
++
+[source,shell]
+----------------------------------------------------------------------
+sudo ./filebeat -e -c filebeat.yml -d "publish"
+----------------------------------------------------------------------
++
+NOTE: Depending on how you've installed Filebeat, you might see errors
+related to file ownership or permissions when you try to run Filebeat modules.
+See {libbeat}/config-file-permissions.html[Config File Ownership and Permissions]
+in the _Beats Platform Reference_ if you encounter errors related to file
+ownership or permissions.
++
+See {filebeat}/filebeat-starting.html[Starting Filebeat] for more info.
+
+. Start Logstash, passing in the pipeline configuration file that parses the
+log. For example:
++
+[source,shell]
+----------------------------------------------------------------------
+bin/logstash -f mypipeline.conf
+----------------------------------------------------------------------
++
+You'll see the following message when Logstash is running and listening for
+input from Beats: 
++
+[source,shell]
+----------------------------------------------------------------------
+[2017-03-17T16:31:40,319][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=>"127.0.0.1:5044"}
+[2017-03-17T16:31:40,350][INFO ][logstash.pipeline        ] Pipeline main started
+----------------------------------------------------------------------
+
+. To visualize the data in Kibana, launch the Kibana web interface by pointing
+your browser to port 5601. For example,
+http://127.0.0.1:5601[http://127.0.0.1:5601]. 
+
+[[logstash-config-for-filebeat-modules]]
+=== Configuration Examples
+
+The examples in this section show you how to configure Filebeat and build
+Logstash pipelines that parse:
+
+* <<parsing-apache2>>
+* <<parsing-mysql>>
+* <<parsing-nginx>>
+* <<parsing-system>>
+
+Of course, the paths that you specify in the Filebeat config depend on the location
+of the logs you are harvesting. The examples show common default locations.
+
+[[parsing-apache2]]
+==== Apache 2 Logs
+
+Here are some configuration examples for shipping and parsing Apache 2 access and
+error logs.
+
+===== Apache 2 Access Logs
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/apache2/access/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/apache2/access/pipeline.conf[]
+----------------------------------------------------------------------------
+
+===== Apache 2 Error Logs
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/apache2/error/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/apache2/error/pipeline.conf[]
+----------------------------------------------------------------------------
+
+[[parsing-mysql]]
+==== MySQL Logs
+
+Here are some configuration examples for shipping and parsing MySQL error and
+slowlog logs.
+
+===== MySQL Error Logs
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/mysql/error/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/mysql/error/pipeline.conf[]
+----------------------------------------------------------------------------
+
+===== MySQL Slowlog
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/mysql/slowlog/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/mysql/slowlog/pipeline.conf[]
+----------------------------------------------------------------------------
+
+[[parsing-nginx]]
+==== Nginx Logs
+
+Here are some configuration examples for shipping and parsing Nginx access and
+error logs.
+
+===== Nginx Access Logs
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/nginx/access/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/nginx/access/pipeline.conf[]
+----------------------------------------------------------------------------
+
+
+===== Nginx Error Logs
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/nginx/error/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/nginx/error/pipeline.conf[]
+----------------------------------------------------------------------------
+
+[[parsing-system]]
+==== System Logs
+
+Here are some configuration examples for shipping and parsing system
+logs.
+
+===== System Authorization Logs
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/system/auth/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/system/auth/pipeline.conf[]
+----------------------------------------------------------------------------
+
+===== Syslog
+
+Example Filebeat config:
+
+[source,yml]
+----------------------------------------------------------------------
+include::filebeat_modules/system/syslog/filebeat.yml[]
+----------------------------------------------------------------------
+
+
+Example Logstash pipeline config:
+
+[source,json]
+----------------------------------------------------------------------------
+include::filebeat_modules/system/syslog/pipeline.conf[]
+----------------------------------------------------------------------------
diff --git a/docs/static/filebeat_modules/apache2/access/filebeat.yml b/docs/static/filebeat_modules/apache2/access/filebeat.yml
new file mode 100644
index 00000000000..335d5fd6ad9
--- /dev/null
+++ b/docs/static/filebeat_modules/apache2/access/filebeat.yml
@@ -0,0 +1,8 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/apache2/access.log* 
+    - /var/log/apache2/other_vhosts_access.log*
+  exclude_files: [".gz$"]
+output.logstash:
+  hosts: ["localhost:5044"]
diff --git a/docs/static/filebeat_modules/apache2/access/pipeline.conf b/docs/static/filebeat_modules/apache2/access/pipeline.conf
new file mode 100644
index 00000000000..b33d32855ae
--- /dev/null
+++ b/docs/static/filebeat_modules/apache2/access/pipeline.conf
@@ -0,0 +1,39 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \[%{HTTPDATE:[apache2][access][time]}\] \"%{WORD:[apache2][access][method]} %{DATA:[apache2][access][url]} HTTP/%{NUMBER:[apache2][access][http_version]}\" %{NUMBER:[apache2][access][response_code]} %{NUMBER:[apache2][access][body_sent][bytes]}( \"%{DATA:[apache2][access][referrer]}\")?( \"%{DATA:[apache2][access][agent]}\")?",
+        "%{IPORHOST:[apache2][access][remote_ip]} - %{DATA:[apache2][access][user_name]} \\[%{HTTPDATE:[apache2][access][time]}\\] \"-\" %{NUMBER:[apache2][access][response_code]} -" ] }
+      remove_field => "message"
+   }
+   mutate {
+      add_field => { "read_timestamp" => "%{@timestamp}" }
+   }
+   date {
+      match => [ "[apache2][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
+      remove_field => "[apache2][access][time]"
+   }
+   useragent {
+      source => "[apache2][access][agent]"
+      target => "[apache2][access][user_agent]"
+      remove_field => "[apache2][access][agent]"
+   }
+   geoip {
+      source => "[apache2][access][remote_ip]"
+      target => "[apache2][access][geoip]"
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/filebeat_modules/apache2/error/filebeat.yml b/docs/static/filebeat_modules/apache2/error/filebeat.yml
new file mode 100644
index 00000000000..d82217f6a2f
--- /dev/null
+++ b/docs/static/filebeat_modules/apache2/error/filebeat.yml
@@ -0,0 +1,8 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/apache2/error.log*
+  exclude_files: [".gz$"]
+output.logstash:
+  hosts: ["localhost:5044"]
+  
\ No newline at end of file
diff --git a/docs/static/filebeat_modules/apache2/error/pipeline.conf b/docs/static/filebeat_modules/apache2/error/pipeline.conf
new file mode 100644
index 00000000000..16d7d96bef0
--- /dev/null
+++ b/docs/static/filebeat_modules/apache2/error/pipeline.conf
@@ -0,0 +1,33 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{LOGLEVEL:[apache2][error][level]}\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message]}",
+        "\[%{APACHE_TIME:[apache2][error][timestamp]}\] \[%{DATA:[apache2][error][module]}:%{LOGLEVEL:[apache2][error][level]}\] \[pid %{NUMBER:[apache2][error][pid]}(:tid %{NUMBER:[apache2][error][tid]})?\]( \[client %{IPORHOST:[apache2][error][client]}\])? %{GREEDYDATA:[apache2][error][message1]}" ] }
+      pattern_definitions => {
+        "APACHE_TIME" => "%{DAY} %{MONTH} %{MONTHDAY} %{TIME} %{YEAR}"
+      }
+      remove_field => "message"
+   }
+   mutate {
+      rename => { "[apache2][error][message1]" => "[apache2][error][message]" }
+   }
+   date {
+      match => [ "[apache2][error][timestamp]", "EEE MMM dd H:m:s YYYY", "EEE MMM dd H:m:s.SSSSSS YYYY" ]
+      remove_field => "[apache2][error][timestamp]"
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/filebeat_modules/mysql/error/filebeat.yml b/docs/static/filebeat_modules/mysql/error/filebeat.yml
new file mode 100644
index 00000000000..5958cc02785
--- /dev/null
+++ b/docs/static/filebeat_modules/mysql/error/filebeat.yml
@@ -0,0 +1,8 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/mysql/error.log*
+    - /var/log/mysqld.log*
+  exclude_files: [".gz$"]
+output.logstash:
+  hosts: ["localhost:5044"]
diff --git a/docs/static/filebeat_modules/mysql/error/pipeline.conf b/docs/static/filebeat_modules/mysql/error/pipeline.conf
new file mode 100644
index 00000000000..7275a5825e3
--- /dev/null
+++ b/docs/static/filebeat_modules/mysql/error/pipeline.conf
@@ -0,0 +1,37 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["%{LOCALDATETIME:[mysql][error][timestamp]} (\[%{DATA:[mysql][error][level]}\] )?%{GREEDYDATA:[mysql][error][message]}",
+        "%{TIMESTAMP_ISO8601:[mysql][error][timestamp]} %{NUMBER:[mysql][error][thread_id]} \[%{DATA:[mysql][error][level]}\] %{GREEDYDATA:[mysql][error][message1]}",
+        "%{GREEDYDATA:[mysql][error][message2]}"] }
+      pattern_definitions => {
+        "LOCALDATETIME" => "[0-9]+ %{TIME}"
+      }
+      remove_field => "message"
+   }
+   mutate {
+      rename => { "[mysql][error][message1]" => "[mysql][error][message]" }
+   }
+   mutate {
+      rename => { "[mysql][error][message2]" => "[mysql][error][message]" }
+   }
+   date {
+      match => [ "[mysql][error][timestamp]", "ISO8601", "YYMMdd H:m:s" ]
+      remove_field => "[apache2][access][time]"
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/filebeat_modules/mysql/slowlog/filebeat.yml b/docs/static/filebeat_modules/mysql/slowlog/filebeat.yml
new file mode 100644
index 00000000000..28e725511e8
--- /dev/null
+++ b/docs/static/filebeat_modules/mysql/slowlog/filebeat.yml
@@ -0,0 +1,12 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/mysql/mysql-slow.log*
+    - /var/lib/mysql/hostname-slow.log
+  exclude_files: [".gz$"]
+  multiline:
+    pattern: "^# User@Host: "
+    negate: true
+    match: after
+output.logstash:
+  hosts: ["localhost:5044"]
diff --git a/docs/static/filebeat_modules/mysql/slowlog/pipeline.conf b/docs/static/filebeat_modules/mysql/slowlog/pipeline.conf
new file mode 100644
index 00000000000..9f1de81dda0
--- /dev/null
+++ b/docs/static/filebeat_modules/mysql/slowlog/pipeline.conf
@@ -0,0 +1,31 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["^# User@Host: %{USER:[mysql][slowlog][user]}(\[[^\]]+\])? @ %{HOSTNAME:[mysql][slowlog][host]} \[(IP:[mysql][slowlog][ip])?\](\s*Id:\s* %{NUMBER:[mysql][slowlog][id]})?\n# Query_time: %{NUMBER:[mysql][slowlog][query_time][sec]}\s* Lock_time: %{NUMBER:[mysql][slowlog][lock_time][sec]}\s* Rows_sent: %{NUMBER:[mysql][slowlog][rows_sent]}\s* Rows_examined: %{NUMBER:[mysql][slowlog][rows_examined]}\n(SET timestamp=%{NUMBER:[mysql][slowlog][timestamp]};\n)?%{GREEDYMULTILINE:[mysql][slowlog][query]}"] }
+      pattern_definitions => {
+        "GREEDYMULTILINE" => "(.|\n)*"
+      }
+      remove_field => "message"
+   }
+   date {
+      match => [ "[mysql][slowlog][timestamp]", "UNIX" ]
+   }
+   mutate {
+      gsub => ["[mysql][slowlog][query]", "\n# Time: [0-9]+ [0-9][0-9]:[0-9][0-9]:[0-9][0-9](\\.[0-9]+)?$", ""]
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/filebeat_modules/nginx/access/filebeat.yml b/docs/static/filebeat_modules/nginx/access/filebeat.yml
new file mode 100644
index 00000000000..150b65128de
--- /dev/null
+++ b/docs/static/filebeat_modules/nginx/access/filebeat.yml
@@ -0,0 +1,7 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/nginx/access.log*
+  exclude_files: [".gz$"]
+output.logstash:
+  hosts: ["localhost:5044"]
diff --git a/docs/static/filebeat_modules/nginx/access/pipeline.conf b/docs/static/filebeat_modules/nginx/access/pipeline.conf
new file mode 100644
index 00000000000..ed079101dca
--- /dev/null
+++ b/docs/static/filebeat_modules/nginx/access/pipeline.conf
@@ -0,0 +1,38 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["%{IPORHOST:[nginx][access][remote_ip]} - %{DATA:[nginx][access][user_name]} \[%{HTTPDATE:[nginx][access][time]}\] \"%{WORD:[nginx][access][method]} %{DATA:[nginx][access][url]} HTTP/%{NUMBER:[nginx][access][http_version]}\" %{NUMBER:[nginx][access][response_code]} %{NUMBER:[nginx][access][body_sent][bytes]} \"%{DATA:[nginx][access][referrer]}\" \"%{DATA:[nginx][access][agent]}\""] }
+      remove_field => "message"
+   }
+   mutate {
+      rename => { "@timestamp" => "read_timestamp" }
+   }
+   date {
+      match => [ "[nginx][access][time]", "dd/MMM/YYYY:H:m:s Z" ]
+      remove_field => "[nginx][access][time]"
+   }
+   useragent {
+      source => "[nginx][access][agent]"
+      target => "[nginx][access][user_agent]"
+      remove_field => "[nginx][access][agent]"
+   }
+   geoip {
+      source => "[nginx][access][remote_ip]"
+      target => "[nginx][access][geoip]"
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/filebeat_modules/nginx/error/filebeat.yml b/docs/static/filebeat_modules/nginx/error/filebeat.yml
new file mode 100644
index 00000000000..77dfe3d413d
--- /dev/null
+++ b/docs/static/filebeat_modules/nginx/error/filebeat.yml
@@ -0,0 +1,7 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/nginx/error.log*
+  exclude_files: [".gz$"]
+output.logstash:
+  hosts: ["localhost:5044"]
diff --git a/docs/static/filebeat_modules/nginx/error/pipeline.conf b/docs/static/filebeat_modules/nginx/error/pipeline.conf
new file mode 100644
index 00000000000..64cc09599de
--- /dev/null
+++ b/docs/static/filebeat_modules/nginx/error/pipeline.conf
@@ -0,0 +1,29 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["%{DATA:[nginx][error][time]} \[%{DATA:[nginx][error][level]}\] %{NUMBER:[nginx][error][pid]}#%{NUMBER:[nginx][error][tid]}: (\*%{NUMBER:[nginx][error][connection_id]} )?%{GREEDYDATA:[nginx][error][message]}"] }
+      remove_field => "message"
+   }
+   mutate {
+      rename => { "@timestamp" => "read_timestamp" }
+   }
+   date {
+      match => [ "[nginx][error][time]", "YYYY/MM/dd H:m:s" ]
+      remove_field => "[nginx][error][time]"
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/filebeat_modules/system/auth/filebeat.yml b/docs/static/filebeat_modules/system/auth/filebeat.yml
new file mode 100644
index 00000000000..ec1c7e738de
--- /dev/null
+++ b/docs/static/filebeat_modules/system/auth/filebeat.yml
@@ -0,0 +1,11 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/auth.log*
+    - /var/log/secure*
+  exclude_files: [".gz$"]
+  multiline:
+    pattern: "^\\s"
+    match: after
+output.logstash:
+  hosts: ["localhost:5044"]
diff --git a/docs/static/filebeat_modules/system/auth/pipeline.conf b/docs/static/filebeat_modules/system/auth/pipeline.conf
new file mode 100644
index 00000000000..fb356b2aba7
--- /dev/null
+++ b/docs/static/filebeat_modules/system/auth/pipeline.conf
@@ -0,0 +1,38 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} %{DATA:[system][auth][ssh][method]} for (invalid user )?%{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]} port %{NUMBER:[system][auth][ssh][port]} ssh2(: %{GREEDYDATA:[system][auth][ssh][signature]})?",
+               "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: %{DATA:[system][auth][ssh][event]} user %{DATA:[system][auth][user]} from %{IPORHOST:[system][auth][ssh][ip]}",
+               "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sshd(?:\[%{POSINT:[system][auth][pid]}\])?: Did not receive identification string from %{IPORHOST:[system][auth][ssh][dropped_ip]}",
+               "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} sudo(?:\[%{POSINT:[system][auth][pid]}\])?: \s*%{DATA:[system][auth][user]} :( %{DATA:[system][auth][sudo][error]} ;)? TTY=%{DATA:[system][auth][sudo][tty]} ; PWD=%{DATA:[system][auth][sudo][pwd]} ; USER=%{DATA:[system][auth][sudo][user]} ; COMMAND=%{GREEDYDATA:[system][auth][sudo][command]}",
+               "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} groupadd(?:\[%{POSINT:[system][auth][pid]}\])?: new group: name=%{DATA:system.auth.groupadd.name}, GID=%{NUMBER:system.auth.groupadd.gid}",
+               "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} useradd(?:\[%{POSINT:[system][auth][pid]}\])?: new user: name=%{DATA:[system][auth][user][add][name]}, UID=%{NUMBER:[system][auth][user][add][uid]}, GID=%{NUMBER:[system][auth][user][add][gid]}, home=%{DATA:[system][auth][user][add][home]}, shell=%{DATA:[system][auth][user][add][shell]}$",
+               "%{SYSLOGTIMESTAMP:[system][auth][timestamp]} %{SYSLOGHOST:[system][auth][hostname]} %{DATA:[system][auth][program]}(?:\[%{POSINT:[system][auth][pid]}\])?: %{GREEDYMULTILINE:[system][auth][message]}"] }
+      pattern_definitions => {
+        "GREEDYMULTILINE"=> "(.|\n)*"
+      }
+      remove_field => "message"
+   }
+   date {
+      match => [ "[system][auth][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+   }
+   geoip {
+      source => "[system][auth][ssh][ip]"
+      target => "[system][auth][ssh][geoip]"
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/filebeat_modules/system/syslog/filebeat.yml b/docs/static/filebeat_modules/system/syslog/filebeat.yml
new file mode 100644
index 00000000000..103106feff1
--- /dev/null
+++ b/docs/static/filebeat_modules/system/syslog/filebeat.yml
@@ -0,0 +1,11 @@
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/messages*
+    - /var/log/syslog*
+  exclude_files: [".gz$"]
+  multiline:
+    pattern: "^\\s"
+    match: after
+output.logstash:
+  hosts: ["localhost:5044"]
diff --git a/docs/static/filebeat_modules/system/syslog/pipeline.conf b/docs/static/filebeat_modules/system/syslog/pipeline.conf
new file mode 100644
index 00000000000..578da12b7a1
--- /dev/null
+++ b/docs/static/filebeat_modules/system/syslog/pipeline.conf
@@ -0,0 +1,26 @@
+input {
+  beats {
+    # The port to listen on for filebeat connections.
+    port => 5044
+    # The IP address to listen for filebeat connections.
+    host => "0.0.0.0"
+  }
+}
+filter {
+   grok {
+      match => { "message" => ["%{SYSLOGTIMESTAMP:[system][syslog][timestamp]} %{SYSLOGHOST:[system][syslog][hostname]} %{DATA:[system][syslog][program]}(?:\[%{POSINT:[system][syslog][pid]}\])?: %{GREEDYMULTILINE:[system][syslog][message]}"] }
+      pattern_definitions => { "GREEDYMULTILINE" => "(.|\n)*" }
+      remove_field => "message"
+   }
+   date {
+      match => [ "[system][syslog][timestamp]", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+   }
+}
+output {
+  elasticsearch {
+    hosts => localhost
+    manage_template => false
+    index => "%{[@metadata][beat]}-%{+YYYY.MM.dd}"
+    document_type => "%{[@metadata][type]}"
+  }
+}
diff --git a/docs/static/getting-started-with-logstash.asciidoc b/docs/static/getting-started-with-logstash.asciidoc
index 2b2b244a0b7..16f14ec228c 100644
--- a/docs/static/getting-started-with-logstash.asciidoc
+++ b/docs/static/getting-started-with-logstash.asciidoc
@@ -2,13 +2,20 @@
 == Getting Started with Logstash
 
 This section guides you through the process of installing Logstash and verifying that everything is running properly.
-Later sections deal with increasingly complex configurations to address selected use cases.
+After learning how to stash your first event, you go on to create a more advanced pipeline that takes Apache web logs as
+input, parses the logs, and writes the parsed data to an Elasticsearch cluster. Then you learn how to stitch together multiple input and output plugins to unify data from a variety of disparate sources.
+
+This section includes the following topics:
+
+* <<installing-logstash>>
+* <<first-event>>
+* <<advanced-pipeline>>
+* <<multiple-input-output-plugins>>
 
-[float]
 [[installing-logstash]]
-=== Install Logstash
+=== Installing Logstash
 
-NOTE: Logstash requires Java 7 or later. Use the
+NOTE: Logstash requires Java 8. Java 9 is not supported. Use the
 http://www.oracle.com/technetwork/java/javase/downloads/index.html[official Oracle distribution] or an open-source
 distribution such as http://openjdk.java.net/[OpenJDK].
 
@@ -20,33 +27,188 @@ java -version
 On systems with Java installed, this command produces output similar to the following:
 
 [source,shell]
-java version "1.7.0_45"
-Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
-Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
+java version "1.8.0_65"
+Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
+Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)
+
+On some Linux systems, you may also need to have the `JAVA_HOME` environment
+exported before attempting the install, particularly if you installed Java from
+a tarball.  This is because Logstash uses Java during installation to
+automatically detect your environment and install the correct startup method
+(SysV init scripts, Upstart, or systemd).  If Logstash is unable to find the
+JAVA_HOME environment variable during package installation time, you may get an
+error message, and Logstash will be unable to start properly.
 
 [float]
 [[installing-binary]]
-==== Installing from a downloaded binary
+=== Installing from a Downloaded Binary
 
 Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
-Unpack the file. On supported Linux operating systems, you can <<package-repositories,use a package manager>> to
-install Logstash.
+Unpack the file. Do not install Logstash into a directory path that contains colon (:) characters.
+
+On supported Linux operating systems, you can use a package manager to install Logstash.
+
+[float]
+[[package-repositories]]
+=== Installing from Package Repositories
+
+We also have repositories available for APT and YUM based distributions. Note
+that we only provide binary packages, but no source packages, as the packages
+are created as part of the Logstash build.
+
+We have split the Logstash package repositories by version into separate urls
+to avoid accidental upgrades across major versions. For all {major-version}.y
+releases use {major-version} as version number.
+
+We use the PGP key
+https://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
+Elastic's Signing Key, with fingerprint
+
+    4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
+
+to sign all our packages. It is available from https://pgp.mit.edu.
+
+[float]
+==== APT
+
+ifeval::["{release-state}"=="unreleased"]
+
+Version {logstash_version} of Logstash has not yet been released.
+
+endif::[]
+
+ifeval::["{release-state}"!="unreleased"]
+
+Download and install the Public Signing Key:
+
+[source,sh]
+--------------------------------------------------
+wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
+--------------------------------------------------
+
+You may need to install the `apt-transport-https` package on Debian before proceeding:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get install apt-transport-https
+--------------------------------------------------
+
+Save the repository definition to  +/etc/apt/sources.list.d/elastic-{major-version}.list+:
+
+["source","sh",subs="attributes,callouts"]
+--------------------------------------------------
+echo "deb https://artifacts.elastic.co/packages/{major-version}/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-{major-version}.list
+--------------------------------------------------
+
+[WARNING]
+==================================================
+Use the `echo` method described above to add the Logstash repository.  Do not
+use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not
+provide a source package. If you have added the `deb-src` entry, you will see an
+error like the following:
+
+    Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)
+
+Just delete the `deb-src` entry from the `/etc/apt/sources.list` file and the
+installation should work as expected.
+==================================================
+
+Run `sudo apt-get update` and the repository is ready for use. You can install
+it with:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get update && sudo apt-get install logstash
+--------------------------------------------------
+
+See <<running-logstash,Running Logstash>> for details about managing Logstash as a system service.
+
+endif::[]
+
+[float]
+==== YUM
+
+ifeval::["{release-state}"=="unreleased"]
+
+Version {logstash_version} of Logstash has not yet been released.
+
+endif::[]
+
+ifeval::["{release-state}"!="unreleased"]
+
+Download and install the public signing key:
+
+[source,sh]
+--------------------------------------------------
+rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
+--------------------------------------------------
+
+Add the following in your `/etc/yum.repos.d/` directory
+in a file with a `.repo` suffix, for example `logstash.repo`
+
+["source","sh",subs="attributes,callouts"]
+--------------------------------------------------
+[logstash-{major-version}]
+name=Elastic repository for {major-version} packages
+baseurl=https://artifacts.elastic.co/packages/{major-version}/yum
+gpgcheck=1
+gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
+enabled=1
+autorefresh=1
+type=rpm-md
+--------------------------------------------------
+
+And your repository is ready for use. You can install it with:
+
+[source,sh]
+--------------------------------------------------
+sudo yum install logstash
+--------------------------------------------------
+
+WARNING: The repositories do not work with older rpm based distributions
+         that still use RPM v3, like CentOS5.
+
+See the <<running-logstash,Running Logstash>> document for managing Logstash as a system service.
+
+endif::[]
+
+==== Docker
+
+An image is available for running Logstash as a Docker container. It is
+available from the Elastic Docker registry. See <<docker>> for
+details on how to configure and run Logstash Docker containers.
 
 [[first-event]]
-=== Stashing Your First Event: Basic Logstash Example
+=== Stashing Your First Event
 
-To test your Logstash installation, run the most basic Logstash pipeline:
+First, let's test your Logstash installation by running the most basic _Logstash pipeline_.
 
-[source,shell]
+A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
+plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
+the data to a destination.
+
+//TODO: REPLACE WITH NEW IMAGE
+
+image::static/images/basic_logstash_pipeline.png[]
+
+To test your Logstash installation, run the most basic Logstash pipeline. For
+example:
+
+["source","sh",subs="attributes"]
+--------------------------------------------------
 cd logstash-{logstash_version}
 bin/logstash -e 'input { stdin { } } output { stdout {} }'
+--------------------------------------------------
+
+NOTE: The location of the `bin` directory varies by platform. See <<dir-layout>>
+to find the location of `bin\logstash` on your system.
 
 The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the
 command line lets you quickly test configurations without having to edit a file between iterations.
-This pipeline takes input from the standard input, `stdin`, and moves that input to the standard output, `stdout`, in a
-structured format.
+The pipeline in the example takes input from the standard input, `stdin`, and moves that input to the standard output,
+`stdout`, in a structured format.
 
-Once "Logstash startup completed" is displayed, type hello world at the command prompt to see Logstash respond:
+After starting Logstash, wait until you see "Pipeline main started" and then enter `hello world` at the command prompt:
 
 [source,shell]
 hello world
@@ -55,5 +217,4 @@ hello world
 Logstash adds timestamp and IP address information to the message. Exit Logstash by issuing a *CTRL-D* command in the
 shell where Logstash is running.
 
-The <<advanced-pipeline,Advanced Tutorial>> expands the capabilities of your Logstash instance to cover broader
-use cases.
+Congratulations! You've created and run a basic Logstash pipeline. Next, you learn how to create a more realistic pipeline.
diff --git a/docs/static/glob-support.asciidoc b/docs/static/glob-support.asciidoc
new file mode 100644
index 00000000000..7f45279e29b
--- /dev/null
+++ b/docs/static/glob-support.asciidoc
@@ -0,0 +1,50 @@
+[[glob-support]]
+=== Glob Pattern Support
+
+Logstash supports the following patterns wherever glob patterns are allowed:
+
+*`*`*::
+Match any file. You can also use an `*` to restrict other values in the glob.
+For example, `*conf` matches all files that end in `conf`. `*apache*` matches
+any files with `apache` in the name. This pattern does not match hidden files
+(dot files) on Unix-like operating systems. To match dot files, use a pattern
+like `{*,.*}`.
+
+*`**`*::
+Match directories recursively.
+
+*`?`*::
+Match any one character.
+
+*`[set]`*::
+Match any one character in a set. For example, `[a-z]`. Also supports set negation
+(`[^a-z]`).
+
+*`{p,q}`*::
+Match either literal `p` or literal `q`. The matching literal can be more than one
+character, and you can specify more than two literals. This pattern is the equivalent
+to using alternation with the vertical bar in regular expressions (`foo|bar`).
+
+*`\`*::
+Escape the next metacharacter. This means that you cannot use a backslash in Windows
+as part of a glob. The pattern `c:\foo*` will not work, so use `foo*` instead.
+
+[float]
+[[example-glob-patterns]]
+==== Example Patterns
+
+Here are some common examples of glob patterns: 
+
+`"/path/to/*.conf"`::
+Matches config files ending in `.conf` in the specified path.
+
+`"/var/log/*.log"`::
+Matches log files ending in `.log` in the specified path.
+
+`"/var/log/**/*.log`::
+Matches log files ending in `.log` in subdirectories under the specified path.
+
+`"/path/to/logs/{app1,app2,app3}/data.log"`::
+Matches app log files in the `app1`, `app2`, and `app3` subdirectories under the
+specified path.
+
diff --git a/docs/static/glossary.asciidoc b/docs/static/glossary.asciidoc
new file mode 100644
index 00000000000..0fbea1cfa36
--- /dev/null
+++ b/docs/static/glossary.asciidoc
@@ -0,0 +1,79 @@
+[[glossary]]
+== Glossary of Terms
+
+[[glossary-metadata]]@metadata ::
+  A special field for storing content that you don't want to include in output <<glossary-event,events>>. For example, the `@metadata`
+  field is useful for creating transient fields for use in <<glossary-conditional,conditional>> statements.
+    
+[[glossary-codec-plugin]]codec plugin::
+  A Logstash <<glossary-plugin,plugin>> that changes the data representation of an <<glossary-event,event>>. Codecs are essentially stream filters that can operate as part of an input or output. Codecs enable you to separate the transport of messages from the serialization process. Popular codecs include json, msgpack, and plain (text).
+  
+[[glossary-conditional]]conditional::
+  A control flow that executes certain actions based on whether a statement (also called a condition) is true or false. Logstash supports `if`, `else if`, and `else` statements. You can use conditional statements to apply filters and send events to a specific output based on conditions that you specify. 
+    
+[[glossary-event]]event::
+	A single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the Logstash <<glossary-pipeline,pipeline>>.
+    
+[[glossary-field]]field::
+  An <<glossary-event,event>> property. For example, each event in an apache access log has properties, such as a status
+  code (200, 404), request path ("/", "index.html"), HTTP verb (GET, POST), client IP address, and so on. Logstash uses
+  the term "fields" to refer to these properties.
+  
+[[glossary-field-reference]]field reference::
+  A reference to an event <<glossary-field,field>>. This reference may appear in an output block or filter block in the
+  Logstash config file. Field references are typically wrapped in square (`[]`) brackets, for example `[fieldname]`. If
+  you are referring to a top-level field, you can omit the `[]` and simply use the field name. To refer to a nested
+  field, you specify the full path to that field: `[top-level field][nested field]`.
+
+[[glossary-filter-plugin]]filter plugin::
+  A Logstash <<glossary-plugin,plugin>> that performs intermediary processing on an <<glossary-event,event>>. Typically, filters act upon
+  event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to
+  configuration rules. Filters are often applied conditionally depending on the characteristics of the event. Popular
+  filter plugins include grok, mutate, drop, clone, and geoip. Filter stages are optional.
+  
+[[glossary-gem]]gem::
+  A self-contained package of code that's hosted on https://rubygems.org[RubyGems.org]. Logstash <<glossary-plugin,plugins>> are packaged as
+  Ruby Gems. You can use the Logstash <<glossary-plugin-manager,plugin manager>> to manage Logstash gems.
+  
+[[glossary-hot-thread]]hot thread::
+  A Java thread that has high CPU usage and executes for a longer than normal period of time.
+  
+[[glossary-input-plugin]]input plugin::
+  A Logstash <<glossary-plugin,plugin>> that reads <<glossary-event,event>> data from a specific source. Input plugins are the first stage in the Logstash event processing <<glossary-pipeline,pipeline>>. Popular input plugins include file, syslog, redis, and beats.
+  
+[[glossary-indexer]]indexer::
+	A Logstash instance that is tasked with interfacing with an Elasticsearch cluster in order to index <<glossary-event,event>> data.
+    
+[[glossary-message-broker]]message broker::
+  Also referred to as a _message buffer_ or _message queue_, a message broker is external software (such as Redis, Kafka, or RabbitMQ) that stores messages from the Logstash shipper instance as an intermediate store, waiting to be processed by the Logstash indexer instance.
+ 
+[[glossary-output-plugin]]output plugin::
+  A Logstash <<glossary-plugin,plugin>> that writes <<glossary-event,event>> data to a specific destination. Outputs are the final stage in
+  the event <<glossary-pipeline,pipeline>>. Popular output plugins include elasticsearch, file, graphite, and
+  statsd.  
+  
+[[glossary-pipeline]]pipeline::
+  A term used to describe the flow of <<glossary-event,events>> through the Logstash workflow. A pipeline typically consists of a series of
+  input, filter, and output stages. <<glossary-input-plugin,Input>> stages get data from a source and generate events,
+  <<glossary-filter-plugin,filter>> stages, which are optional, modify the event data, and
+  <<glossary-output-plugin,output>> stages write the data to a destination. Inputs and outputs support <<glossary-codec-plugin,codecs>> that enable you to encode or decode the data as it enters or exits the pipeline without having to use
+  a separate filter. 
+  
+[[glossary-plugin]]plugin::
+  A self-contained software package that implements one of the stages in the Logstash event processing
+  <<glossary-pipeline,pipeline>>. The list of available plugins includes <<glossary-input-plugin,input plugins>>,
+  <<glossary-output-plugin,output plugins>>, <<glossary-codec-plugin,codec plugins>>, and
+  <<glossary-filter-plugin,filter plugins>>. The plugins are implemented as Ruby <<glossary-gem,gems>> and hosted on
+  https://rubygems.org[RubyGems.org]. You define the stages of an event processing <<glossary-pipeline,pipeline>> by configuring plugins. 
+ 
+[[glossary-plugin-manager]]plugin manager::
+  Accessed via the `bin/logstash-plugin` script, the plugin manager enables you to manage the lifecycle of
+  <<glossary-plugin,plugins>> in your Logstash deployment. You can install, remove, and upgrade plugins by using the
+  plugin manager Command Line Interface (CLI).
+
+[[shipper]]shipper::
+	An instance of Logstash that send events to another instance of Logstash, or some other application.
+    
+[[worker]]worker::
+	The filter thread model used by Logstash, where each worker receives an <<glossary-event,event>> and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive.
+
diff --git a/docs/static/images/basic_logstash_pipeline.png b/docs/static/images/basic_logstash_pipeline.png
index d1b31401a49..6d04890dfe3 100644
Binary files a/docs/static/images/basic_logstash_pipeline.png and b/docs/static/images/basic_logstash_pipeline.png differ
diff --git a/docs/static/images/deploy1.png b/docs/static/images/deploy1.png
new file mode 100644
index 00000000000..f6b97cb6547
Binary files /dev/null and b/docs/static/images/deploy1.png differ
diff --git a/docs/static/images/deploy2.png b/docs/static/images/deploy2.png
new file mode 100644
index 00000000000..2e7f48627ab
Binary files /dev/null and b/docs/static/images/deploy2.png differ
diff --git a/docs/static/images/deploy3.png b/docs/static/images/deploy3.png
new file mode 100644
index 00000000000..0769dd6e7a5
Binary files /dev/null and b/docs/static/images/deploy3.png differ
diff --git a/docs/static/images/deploy4.png b/docs/static/images/deploy4.png
new file mode 100644
index 00000000000..b0fd6b0e9aa
Binary files /dev/null and b/docs/static/images/deploy4.png differ
diff --git a/docs/static/images/deploy_3.png b/docs/static/images/deploy_3.png
index cda4337fa9d..96bc119c3e0 100644
Binary files a/docs/static/images/deploy_3.png and b/docs/static/images/deploy_3.png differ
diff --git a/docs/static/images/kibana-filebeat-data.png b/docs/static/images/kibana-filebeat-data.png
new file mode 100644
index 00000000000..b3c2d62a068
Binary files /dev/null and b/docs/static/images/kibana-filebeat-data.png differ
diff --git a/docs/static/images/logstash.png b/docs/static/images/logstash.png
index fc0caca56fe..802f8426fb0 100644
Binary files a/docs/static/images/logstash.png and b/docs/static/images/logstash.png differ
diff --git a/docs/static/include/pluginbody.asciidoc b/docs/static/include/pluginbody.asciidoc
index a51c4f74aa9..5cc65ce418a 100644
--- a/docs/static/include/pluginbody.asciidoc
+++ b/docs/static/include/pluginbody.asciidoc
@@ -1,4 +1,4 @@
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/include/pluginbody.asciidoc ?>]
+
 
 === How to write a Logstash {plugintype} plugin
 
@@ -9,10 +9,6 @@ implementation as a starting point. (If you're unfamiliar with
 Ruby, you can find an excellent quickstart guide at
 https://www.ruby-lang.org/en/documentation/quickstart/[].)
 
-NOTE: As of Logstash 1.5, all plugins are self-contained Ruby gems. This change
-makes it possible to develop and release plugins separately. In previous
-versions, plugins were part of the core Logstash distribution.
-
 ==== Get started
 
 {getstarted}
@@ -29,9 +25,17 @@ Each Logstash plugin lives in its own GitHub repository. To create a new reposit
 ** **Initialize this repository with a README** -- enables you to immediately clone the repository to your computer.
 . Click **Create Repository**.
 
+==== Use the plugin generator tool
+
+You can now create your own Logstash plugin in seconds! The `generate` subcommand of `bin/logstash-plugin` creates the foundation 
+for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you 
+can start adding custom code to process data with Logstash.
+
+For more information, see <<plugin-generator>>
+
 ==== Copy the {plugintype} code
 
-Build your local repository:
+Alternatively, you can use the examples repo we host on github.com
 
 . **Clone your plugin.** Replace `GITUSERNAME` with your github username, and
 `MYPLUGINNAME` with your plugin name.
@@ -195,7 +199,7 @@ class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
 
   public
   def encode(event)
-    @on_event.call(event, event["message"].to_s + @append + NL)
+    @on_event.call(event, event.get("message").to_s + @append + NL)
   end # def encode
 
 end # class LogStash::{pluginclass}::{pluginnamecap}
@@ -246,7 +250,7 @@ class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
     if @message
       # Replace the event message with our message as configured in the
       # config file.
-      event["message"] = @message
+      event.set("message", @message)
     end
 
     # filter_matched should go in the last line of our successful code
@@ -275,40 +279,31 @@ require "logstash/namespace"
 class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
   config_name "example"
 
-  # If declared logstash will only allow a single instance of this plugin
-  # to exist, regardless of how many CPU cores logstash detects. This is best
-  # used in cases like the File output, where separate threads writing to a single
-  # File would only cause problems.
-  #
-  # respond_to? check needed for backwards compatibility with < 2.2 Logstashes
-  declare_workers_not_supported! if self.respond_to?(:declare_workers_not_supported!)
-
-  # If declared threadsafe logstash will only ever create one
-  # instance of this plugin per pipeline.
-  # That instance will be shared across all workers
-  # It is up to the plugin author to correctly write concurrent code!
+  # This sets the concurrency behavior of this plugin. By default it is :legacy, which was the standard
+  # way concurrency worked before Logstash 2.4
+  # 
+  # You should explicitly set it to either :single or :shared as :legacy will be removed in Logstash 6.0
+  # 
+  # When configured as :single a single instance of the Output will be shared among the
+  # pipeline worker threads. Access to the `#multi_receive/#multi_receive_encoded/#receive` method will be synchronized
+  # i.e. only one thread will be active at a time making threadsafety much simpler.
+  # 
+  # You can set this to :shared if your output is threadsafe. This will maximize
+  # concurrency but you will need to make appropriate uses of mutexes in `#multi_receive/#receive`.
   #
-  # respond_to? check needed for backwards compatibility with < 2.2 Logstashes
-  declare_threadsafe! if self.respond_to?(:declare_threadsafe!)
-
+  # Only the `#multi_receive/#multi_receive_encoded` methods need to actually be threadsafe, the other methods
+  # will only be executed in a single thread
+  concurrency :single
+  
   public
-  def register
-    # Does the same thing as declare_workers_not_supported!
-    # But works in < 2.2 logstashes
-    # workers_not_supported
+  def register    
   end # def register
 
   public
   # Takes an array of events
+  # Must be threadsafe if `concurrency :shared` is set
   def multi_receive(events)
   end # def multi_receive
-
-  public
-  # Needed for logstash < 2.2 compatibility
-  # Takes events one at a time
-  def receive(event)
-  end # def receive
-
 end # class LogStash::{pluginclass}::{pluginnamecap}
 ----------------------------------
 endif::multi_receive_method[]
@@ -474,14 +469,14 @@ There are several configuration attributes:
 
 * `:validate` - allows you to enforce passing a particular data type to Logstash
 for this configuration option, such as `:string`, `:password`, `:boolean`,
-`:number`, `:array`, `:hash`, `:path` (a file-system path), `:codec` (since
-1.2.0), `:bytes` (starting in 1.5.0).  Note that this also works as a coercion
+`:number`, `:array`, `:hash`, `:path` (a file-system path), `uri` (starting in 5.0.0), `:codec` (since
+1.2.0), `:bytes` (starting in 1.5.0).  Note that this also works as a coercion 
 in that if I specify "true" for boolean (even though technically a string), it
 will become a valid boolean in the config.  This coercion works for the
 `:number` type as well where "1.2" becomes a float and "22" is an integer.
 * `:default` - lets you specify a default value for a parameter
-* `:required` - whether or not this parameter is mandatory (a Boolean `true` or
-`false`)
+* `:required` - whether or not this parameter is mandatory (a Boolean `true` or `false`)
+* `:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable. 
 * `:deprecated` - informational (also a Boolean `true` or `false`)
 * `:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.
 
@@ -534,7 +529,7 @@ ifndef::blockfilter[]
     if @message
       # Replace the event message with our message as configured in the
       # config file.
-      event["message"] = @message
+      event.set("message", @message)
     end
 
   # filter_matched should go in the last line of our successful code
@@ -542,15 +537,27 @@ ifndef::blockfilter[]
 end # def filter
 ----------------------------------
 The plugin's `filter` method is where the actual filtering work takes place!
-Inside the `filter` method you can refer to the event data using the `event`
-hash. Configuration variables are now in scope as instance variables, like
+Inside the `filter` method you can refer to the event data using the `Event`
+object. Event is the main object that encapsulates data flow internally in Logstash 
+and provides an <<event-api, API>> for the plugin developers to interact with the 
+event's content.
+
+The `filter` method should also handle any <<event-dependent-configuration, event dependent configuration>> by 
+explicitly calling the `sprintf` method available in Event class. For example:
+
+[source,ruby]
+----------------------------------
+field_foo = event.sprintf(field)
+----------------------------------
+
+Note that configuration variables are now in scope as instance variables, like
 `@message`
 
 [source,ruby]
 ----------------------------------
   filter_matched(event)
 ----------------------------------
-Calling the `filter_matched` method upon succesful execution of the plugin will
+Calling the `filter_matched` method upon successful execution of the plugin will
 ensure that any fields or tags added through the Logstash configuration for this
 filter will be handled correctly. For example, any `add_field`, `remove_field`,
 `add_tag` and/or `remove_tag` actions will be performed at this time.
@@ -603,7 +610,7 @@ ifndef::blockcodec[]
 ----------------------------------
   public
   def encode(event)
-    @on_event.call(event, event["message"].to_s + @append + NL)
+    @on_event.call(event, event.get("message").to_s + @append + NL)
   end # def encode
 ----------------------------------
 The `encode` method takes an event and serializes it (_encodes_) into another
@@ -658,7 +665,7 @@ Here's another example `run` method:
         data = $stdin.sysread(16384)
         @codec.decode(data) do |event|
           decorate(event)
-          event["host"] = @host if !event.include?("host")
+          event.set("host", @host) if !event.include?("host")
           queue << event
         end
       rescue IOError, EOFError, LogStash::ShutdownSignal
@@ -860,13 +867,13 @@ Gem::Specification.new do |s|
   s.metadata = { "logstash_plugin" => "true", "logstash_group" => "{plugintype}" }
 
   # Gem dependencies
-  s.add_runtime_dependency 'logstash-core', '>= 1.4.0', '< 2.0.0'
+  s.add_runtime_dependency "logstash-core-plugin-api", ">= 1.60", "<= 2.99"
   s.add_development_dependency 'logstash-devutils'
 end
 ----------------------------------
 
 It is appropriate to change these values to fit your plugin. In particular,
-`s.name` and `s.summary` shoud reflect your plugin's name and behavior.
+`s.name` and `s.summary` should reflect your plugin's name and behavior.
 
 `s.licenses` and `s.version` are also important and will come into play when
 you are ready to publish your plugin.
@@ -879,28 +886,7 @@ please make sure to have this line in your gemspec:
 * `s.licenses = ['Apache License (2.0)']`
 
 The gem version, designated by `s.version`, helps track changes to plugins over
-time.
-
-**Version messaging from Logstash**
-
-If you start Logstash with the `--verbose` flag, you will see messages like
-these to indicate the relative maturity indicated by the plugin version number:
-
-** **0.1.x**
-+
------
-This plugin isn't well supported by the community and likely has no maintainer.
------
-
-** **0.9.x**
-+
------
-This plugin should work but would benefit from use by folks like you. Please let us know if you find bugs or have suggestions on how to improve this plugin.
------
-
-** **1.x.x**
-You will no longer see a message indicating potential code immaturity when a
-plugin reaches version 1.0.0
+time. You should use http://semver.org/[semver versioning] strategy for version numbers. 
 
 ==== Runtime & Development Dependencies
 
@@ -918,16 +904,15 @@ Logstash plugins:
 [subs="attributes"]
 ----------------------------------
   # Gem dependencies
-  s.add_runtime_dependency 'logstash-core', '>= 1.4.0', '< 2.0.0'
+  s.add_runtime_dependency "logstash-core-plugin-api", ">= 1.60", "<= 2.99"
   s.add_development_dependency 'logstash-devutils'
 ----------------------------------
-This gemspec has a runtime dependency on the core Logstash gem and requires that
-it have a version number greater than or equal to version 1.4.0 `'>= 1.4.0'`,
-and less than version 2.0 `'< 2.0.0'`.
+This gemspec has a runtime dependency on the logstash-core-plugin-api and requires that
+it have a version number greater than or equal to version 1.60 and less than or equal to version 2.99.
 =========================
 
 
-IMPORTANT: All plugins have a runtime dependency on the `logstash` core gem, and
+IMPORTANT: All plugins have a runtime dependency on the `logstash-core-plugin-api` gem, and
 a development dependency on `logstash-devutils`.
 
 ==== Jar dependencies
@@ -941,7 +926,7 @@ added in the gemspec file in this manner:
 [subs="attributes"]
 ----------------------------------
   # Jar dependencies
-  s.requirements << "jar 'org.elasticsearch:elasticsearch', '1.4.0'"
+  s.requirements << "jar 'org.elasticsearch:elasticsearch', '5.0.0'"
   s.add_runtime_dependency 'jar-dependencies'
 ----------------------------------
 
@@ -1314,4 +1299,4 @@ the Logstash repository. When the acceptance guidelines are completed, we will
 facilitate the move to the logstash-plugins organization using the recommended
 https://help.github.com/articles/transferring-a-repository/#transferring-from-a-user-to-an-organization[github process].
 
-pass::[<?edit_url?>]
+
diff --git a/docs/static/introduction.asciidoc b/docs/static/introduction.asciidoc
index 9b69959d6a6..671b8b354bb 100644
--- a/docs/static/introduction.asciidoc
+++ b/docs/static/introduction.asciidoc
@@ -27,11 +27,11 @@ Collect more, so you can know more. Logstash welcomes data of all shapes and siz
 Where it all started.
 
 * Handle all types of logging data
-** Easily ingest a multitude of web logs like <<parsing-into-es,Apache>>, and application
+** Easily ingest a multitude of web logs like <<advanced-pipeline,Apache>>, and application
 logs like <<plugins-inputs-log4j,log4j>> for Java
 ** Capture many other log formats like <<plugins-inputs-syslog,syslog>>,
 <<plugins-inputs-eventlog,Windows event logs>>, networking and firewall logs, and more
-* Enjoy complementary secure log forwarding capabilities with https://github.com/elastic/beats/tree/master/filebeat[Filebeat]
+* Enjoy complementary secure log forwarding capabilities with https://www.elastic.co/products/beats/filebeat[Filebeat]
 * Collect metrics from <<plugins-inputs-ganglia,Ganglia>>, <<plugins-codecs-collectd,collectd>>,
 <<plugins-codecs-netflow,NetFlow>>, <<plugins-inputs-jmx,JMX>>, and many other infrastructure
 and application platforms over <<plugins-inputs-tcp,TCP>> and <<plugins-inputs-udp,UDP>>
@@ -42,12 +42,10 @@ and application platforms over <<plugins-inputs-tcp,TCP>> and <<plugins-inputs-u
 Unlock the World Wide Web.
 
 * Transform <<plugins-inputs-http,HTTP requests>> into events
-(https://www.elastic.co/blog/introducing-logstash-input-http-plugin[blog])
 ** Consume from web service firehoses like <<plugins-inputs-twitter,Twitter>> for social sentiment analysis
 ** Webhook support for GitHub, HipChat, JIRA, and countless other applications
-** Enables many https://www.elastic.co/guide/en/watcher/current/logstash-integration.html[Watcher] alerting use cases
+** Enables many https://www.elastic.co/products/x-pack/alerting[Watcher] alerting use cases
 * Create events by polling <<plugins-inputs-http_poller,HTTP endpoints>> on demand
-(https://www.elastic.co/blog/introducing-logstash-http-poller[blog])
 ** Universally capture health, performance, metrics, and other types of data from web application interfaces
 ** Perfect for scenarios where the control of polling is preferred over receiving
 
@@ -57,10 +55,9 @@ Unlock the World Wide Web.
 Discover more value from the data you already own.
 
 * Better understand your data from any relational database or NoSQL store with a
-<<plugins-inputs-jdbc,JDBC>> interface (https://www.elastic.co/blog/logstash-jdbc-input-plugin[blog])
-* Unify diverse data streams from messaging queues like Apache <<plugins-outputs-kafka,Kafka>>
-(https://www.elastic.co/blog/logstash-kafka-intro[blog]), <<plugins-outputs-rabbitmq,RabbitMQ>>,
-<<plugins-outputs-sqs,Amazon SQS>>, and <<plugins-outputs-zeromq,ZeroMQ>>
+<<plugins-inputs-jdbc,JDBC>> interface 
+* Unify diverse data streams from messaging queues like Apache <<plugins-outputs-kafka,Kafka>>,
+<<plugins-outputs-rabbitmq,RabbitMQ>>, <<plugins-outputs-sqs,Amazon SQS>>, and <<plugins-outputs-zeromq,ZeroMQ>>
 
 [float]
 === Sensors and IoT
@@ -71,9 +68,6 @@ Explore an expansive breadth of other data.
 harnessing data from connected sensors.
 * Logstash is the common event collection backbone for ingestion of data shipped from mobile devices to intelligent
 homes, connected vehicles, healthcare sensors, and many other industry specific applications.
-* https://www.elastic.co/elasticon/2015/sf/if-it-moves-measure-it-logging-iot-with-elk[Watch] as Logstash, in
-conjunction with the broader ELK stack, centralizes and enriches sensor data to gain deeper knowledge regarding a
-residential home.
 
 [float]
 == Easily Enrich Everything
@@ -87,12 +81,14 @@ structure out of unstructured data. Enjoy a wealth of integrated patterns aimed
 networking, and other types of event formats.
 * Expand your horizons by deciphering <<plugins-filters-geoip,geo coordinates>> from IP addresses, normalizing
 <<plugins-filters-date,date>> complexity, simplifying <<plugins-filters-kv,key-value pairs>> and
-<<plugins-filters-csv,CSV>> data, <<plugins-filters-anonymize,anonymizing>> sensitive information, and further
-enriching your data with <<plugins-filters-translate,local lookups>> or Elasticsearch
+<<plugins-filters-csv,CSV>> data, <<plugins-filters-fingerprint,fingerprinting>> (anonymizing) sensitive information,
+and further enriching your data with <<plugins-filters-translate,local lookups>> or Elasticsearch
 <<plugins-filters-elasticsearch,queries>>.
 * Codecs are often used to ease the processing of common event structures like <<plugins-codecs-json,JSON>>
 and <<plugins-codecs-multiline,multiline>> events.
 
+See <<transformation>> for an overview of some of the popular data processing plugins.
+
 [float]
 == Choose Your Stash
 
diff --git a/docs/static/life-of-an-event.asciidoc b/docs/static/life-of-an-event.asciidoc
index 3a5f72055c6..cef095f496f 100644
--- a/docs/static/life-of-an-event.asciidoc
+++ b/docs/static/life-of-an-event.asciidoc
@@ -1,5 +1,5 @@
 [[pipeline]]
-=== Logstash Processing Pipeline
+== How Logstash Works
 
 The Logstash event processing pipeline has three stages: inputs -> filters ->
 outputs. Inputs generate events, filters modify them, and outputs ship them
@@ -8,7 +8,7 @@ the data as it enters or exits the pipeline without having to use a separate
 filter.
 
 [float]
-==== Inputs
+=== Inputs
 You use inputs to get data into Logstash. Some of the more commonly-used inputs
 are:
 
@@ -25,7 +25,7 @@ For more information about the available inputs, see
 <<input-plugins,Input Plugins>>.
 
 [float]
-==== Filters
+=== Filters
 Filters are intermediary processing devices in the Logstash pipeline. You can
 combine filters with conditionals to perform an action on an event if it meets
 certain criteria. Some useful filters include:
@@ -45,7 +45,7 @@ For more information about the available filters, see
 <<filter-plugins,Filter Plugins>>.
 
 [float]
-==== Outputs
+=== Outputs
 Outputs are the final phase of the Logstash pipeline. An event can pass through
 multiple outputs, but once all output processing is complete, the event has
 finished its execution. Some commonly used outputs include:
@@ -55,7 +55,7 @@ your data in an efficient, convenient, and easily queryable format...
 Elasticsearch is the way to go. Period. Yes, we're biased :)
 * *file*: write event data to a file on disk.
 * *graphite*: send event data to graphite, a popular open source tool for
-storing and graphing metrics. http://graphite.wikidot.com/
+storing and graphing metrics. http://graphite.readthedocs.io/en/latest/
 * *statsd*: send event data to statsd, a service that "listens for statistics,
 like counters and timers, sent over UDP and sends aggregates to one or more
 pluggable backend services". If you're already using statsd, this could be
@@ -65,7 +65,7 @@ For more information about the available outputs, see
 <<output-plugins,Output Plugins>>.
 
 [float]
-==== Codecs
+=== Codecs
 Codecs are basically stream filters that can operate as part of an input or
 output. Codecs enable you to easily separate the transport of your messages from
 the serialization process. Popular codecs include `json`, `msgpack`, and `plain`
@@ -78,81 +78,17 @@ stacktrace messages into a single event.
 For more information about the available codecs, see
 <<codec-plugins,Codec Plugins>>.
 
-[float]
-=== Fault Tolerance
-
-Logstash keeps all events in main memory during processing. Logstash responds to a SIGTERM by attempting to halt inputs and waiting for pending events to finish processing before shutting down. When the pipeline cannot be flushed due to a stuck output or filter, Logstash waits indefinitely. For example, when a pipeline sends output to a database that is unreachable by the Logstash instance, the instance waits indefinitely after receiving a SIGTERM.
-
-To enable Logstash to detect these situations and terminate with a stalled pipeline, use the `--allow-unsafe-shutdown` flag.
-
-WARNING: Unsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason result in data loss. Shut down Logstash safely whenever possible.
-
-[float]
-==== Execution Model
-
-The Logstash pipeline coordinates the execution of inputs, filters, and outputs. The following schematic sketches the data flow of a pipeline:
-
-[source,js]
----------------------------------------------------
-input threads | pipeline worker threads
----------------------------------------------------
-
-Pipelines in the current release of Logstash process filtering and output in the same thread. Prior to the 2.2 release of Logstash, filtering and output were distinct stages handled by distinct threads.
-This change to the Logstash architecture improves performance and enables future persistence capabilities. The new pipeline substantially improves thread liveness, decreases resource usage, and increases throughput. The current Logstash pipeline is a micro-batching pipeline, which is inherently more efficient than a one-at-a-time approach. These efficiencies come in many places, two of the more prominent ones being a reduction in contention and a consequent improvement in thread liveness. These efficiencies are especially noticeable on many-core machines.
-
-Each `input {}` statement in the Logstash configuration file runs in its own thread. Inputs write events to a common Java https://docs.oracle.com/javase/7/docs/api/java/util/concurrent/SynchronousQueue.html[SynchronousQueue]. This queue holds no events, instead transferring each pushed event to a free worker, blocking if all workers are busy. Each pipeline worker thread takes a batch of events off this queue, creating a buffer per worker, runs the batch of  events through the configured filters, then runs the filtered events through any outputs. The size of the batch and number of pipeline worker threads are configurable. The following pseudocode illustrates the process flow:
-
-[source,ruby]
-synchronous_queue = SynchronousQueue.new
-inputs.each do |input|
-  Thread.new do
-    input.run(synchronous_queue)
-  end
-end
-num_pipeline_workers.times do
-  Thread.new do
-    while true
-      batch = take_batch(synchronous_queue, batch_size, batch_delay)
-      output_batch(filter_batch(batch))
-    end
-  end
-end
-wait_for_threads_to_terminate()
-
-There are three configurable options in the pipeline, `--pipeline-workers`, `--pipeline-batch-size`, and `--pipeline-batch-delay`.
-The `--pipeline-workers` or `-w` parameter determines how many threads to run for filter and output processing. If you find that events are backing up, or that the CPU is not saturated, consider increasing the value of this parameter to make better use of available processing power. Good results can even be found increasing this number past the number of available processors as these threads may spend significant time in an I/O wait state when writing to external systems. Legal values for this parameter are positive integers.
-
-The `--pipeline-batch-size` or `-b` parameter defines the maximum number of events an individual worker thread collects before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Some hardware configurations require you to increase JVM heap size by setting the `LS_HEAP_SIZE` variable to avoid performance degradation with this option. Values of this parameter in excess of the optimum range cause performance degradation due to frequent garbage collection or JVM crashes related to out-of-memory exceptions. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, issues https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[bulk requests] for each batch received. Tuning the `-b` parameter adjusts the size of bulk requests sent to Elasticsearch.
-
-The `--pipeline-batch-delay` option rarely needs to be tuned. This option adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that Logstash waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, Logstash beings to execute filters and outputs.The maximum time that Logstash waits between receiving an event and processing that event in a filter is the product of the `pipeline_batch_delay` and  `pipeline_batch_size` settings.
-
-[float]
-==== Notes on Pipeline Configuration and Performance
-
-The total number of inflight events is determined by the product of the  `pipeline_workers` and `pipeline_batch_size` parameters. This product is referred to as the _inflight count_.  Keep the value of the inflight count in mind as you adjust the `pipeline_workers` and `pipeline_batch_size` parameters. Pipelines that intermittently receive large events at irregular intervals require sufficient memory to handle these spikes. Configure the `LS_HEAP_SIZE` option accordingly.
-
-The Logstash defaults are chosen to provide fast, safe performance for most users. To increase performance, increase the number of pipeline workers or the batch size, taking into account the following suggestions:
-
-Measure each change to make sure it increases, rather than decreases, performance.
-Ensure that you leave enough memory available to cope with a sudden increase in event size. For example, an application that generates exceptions that are represented as large blobs of text.
-The number of workers may be set higher than the number of CPU cores since outputs often spend idle time in I/O wait conditions.
-
-Threads in Java have names and you can use the `jstack`, `top`, and the VisualVM graphical tools to figure out which resources a given thread uses.
-
-On Linux platforms, Logstash labels all the threads it can with something descriptive. For example, inputs show up as `[base]<inputname`, filter/output workers show up as `[base]>workerN`, where N is an integer.  Where possible, other threads are also labeled to help you identify their purpose.
-
-[float]
-==== Profiling the Heap
-
-When tuning Logstash you may have to adjust the heap size. You can use the https://visualvm.java.net/[VisualVM] tool to profile the heap. The *Monitor* pane in particular is useful for checking whether your heap allocation is sufficient for the current workload. The screenshots below show sample *Monitor* panes. The first pane examines a Logstash instance configured with too many inflight events. The second pane examines a Logstash instance configured with an appropriate amount of inflight events. Note that the specific batch sizes used here are most likely not applicable to your specific workload, as the memory demands of Logstash vary in large part based on the type of messages you are sending.
-
-image::static/images/pipeline_overload.png[]
+[[execution-model]]
+=== Execution Model
 
-image::static/images/pipeline_correct_load.png[]
+The Logstash event processing pipeline coordinates the execution of inputs,
+filters, and outputs. 
 
-In the first example we see that the CPU isnâ€™t being used very efficiently. In fact, the JVM is often times having to stop the VM for â€œfull GCsâ€. Full garbage collections are a common symptom of excessive memory pressure. This is visible in the spiky pattern on the CPU chart. In the more efficiently configured example, the GC graph pattern is more smooth, and the CPU is used in a more uniform manner. You can also see that there is ample headroom between the allocated heap size, and the maximum allowed, giving the JVM GC a lot of room to work with.
+Each input stage in the Logstash pipeline runs in its own thread. Inputs write events to a common Java https://docs.oracle.com/javase/8/docs/api/java/util/concurrent/SynchronousQueue.html[SynchronousQueue]. This queue holds no events, instead transferring each pushed event to a free worker, blocking if all workers are busy. Each pipeline worker thread takes a batch of events off this queue, creating a buffer per worker, runs the batch of events through the configured filters, then runs the filtered events through any outputs. The size of the batch and number of pipeline worker threads are configurable (see <<tuning-logstash>>). 
 
-Examining the in-depth GC statistics with a tool similar to the excellent https://visualvm.java.net/plugins.html[VisualGC] plugin shows that the over-allocated VM spends very little time in the efficient Eden GC, compared to the time spent in the more resource-intensive Old Gen â€œFullâ€ GCs.
+By default, Logstash uses in-memory bounded queues between pipeline stages
+(input â†’ filter and filter â†’ output) to buffer events. If Logstash terminates
+unsafely, any events that are stored in memory will be lost. To prevent data
+loss, you can enable Logstash to persist in-flight events to disk. See
+<<persistent-queues>> for more information.
 
-NOTE: As long as the GC pattern is acceptable, heap sizes that occasionally increase to the maximum are acceptable. Such heap size spikes happen in response to a burst of large events passing through the pipeline. In general practice, maintain a gap between the used amount of heap memory and the maximum.
-This document is not a comprehensive guide to JVM GC tuning. Read the official http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html[Oracle guide] for more information on the topic. We also recommend reading http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance].
diff --git a/docs/static/logging.asciidoc b/docs/static/logging.asciidoc
new file mode 100644
index 00000000000..bdf569cec16
--- /dev/null
+++ b/docs/static/logging.asciidoc
@@ -0,0 +1,126 @@
+[[logging]]
+=== Logging
+
+Logstash emits internal logs during its operation, which are placed in `LS_HOME/logs` (or `/var/log/logstash` for
+DEB/RPM). The default logging level is `INFO`. Logstash's logging framework is based on
+http://logging.apache.org/log4j/2.x/[Log4j 2 framework], and much of its functionality is exposed directly to users.
+
+When debugging problems, particularly problems with plugins, it can be helpful to increase the logging level to `DEBUG` 
+to emit more verbose messages. Previously, you could only set a log level that applied to the entire Logstash product. 
+Starting with 5.0, you can configure logging for a particular subsystem in Logstash. For example, if you are 
+debugging issues with Elasticsearch Output, you can increase log levels just for that component. This way 
+you can reduce noise due to excessive logging and focus on the problem area effectively.
+
+==== Log file location
+
+You can specify the log file location using `--path.logs` setting.
+
+==== Log4j 2 Configuration
+
+Logstash ships with a `log4j2.properties` file with out-of-the-box settings. You  can modify this file directly to change the 
+rotation policy, type, and other https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers[log4j2 configuration]. 
+You must restart Lostash to apply any changes that you make to this file.
+
+==== Slowlog
+
+Slow-log for Logstash adds the ability to log when a specific event takes an abnormal amount of time to make its way
+through the pipeline. Just like the normal application log, you can find slow-logs in your `--path.logs` directory.
+Slowlog is configured in the `logstash.yml` settings file with the following options:
+
+[source,yaml]
+------------------------------
+slowlog.threshold.warn (default: -1)
+slowlog.threshold.info (default: -1)
+slowlog.threshold.debug (default: -1)
+slowlog.threshold.trace (default: -1)
+------------------------------
+
+By default, these values are set to `-1nanos` to represent an infinite threshold where no slowlog will be invoked. These `slowlog.threshold`
+fields are configured using a time-value format which enables a wide range of trigger intervals. The positive numeric ranges
+can be specified using the following time units: `nanos` (nanoseconds), `micros` (microseconds), `ms` (milliseconds), `s` (second), `m` (minute),
+`h` (hour), `d` (day).
+
+Here is an example:
+
+[source,yaml]
+------------------------------
+slowlog.threshold.warn: 2s
+slowlog.threshold.info: 1s
+slowlog.threshold.debug: 500ms
+slowlog.threshold.trace: 100ms
+------------------------------
+
+In the above configuration, events that take longer than two seconds to be processed within a filter will be logged.
+The logs will include the full event and filter configuration that are responsible for the slowness.
+
+==== Logging APIs
+
+You could modify the `log4j2.properties` file and restart your Logstash, but that is both tedious and leads to unnecessary 
+downtime. Instead, you can dynamically update logging levels through the logging API. These settings are effective 
+immediately and do not need a restart.
+
+NOTE: By default, the logging API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
+instance, you need to launch Logstash with the `--http.port` flag specified to bind to a different port. See
+<<command-line-flags>> for more information.
+
+To update logging levels, take the subsystem/module you are interested in and prepend 
+`logger.` to it. For example:
+
+[source,js]
+--------------------------------------------------
+curl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/json' -d'
+{
+    "logger.logstash.outputs.elasticsearch" : "DEBUG"
+}
+'
+--------------------------------------------------
+
+While this setting is in effect, Logstash will begin to emit DEBUG-level logs for __all__ the Elasticsearch outputs 
+specified in your configuration. Please note this new setting is transient and will not survive a restart.
+
+Persistent changes should be added to `log4j2.properties`. For example:
+
+[source,yaml]
+--------------------------------------------------
+logger.elasticsearchoutput.name = logstash.outputs.elasticsearch
+logger.elasticsearchoutput.level = debug
+--------------------------------------------------
+
+To retrieve a list of logging subsystems available at runtime, you can do a `GET` request to `_node/logging`
+
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/logging?pretty'
+--------------------------------------------------
+
+Example response:
+
+["source","js"]
+--------------------------------------------------
+{
+...
+  "loggers" : {
+    "logstash.agent" : "INFO",
+    "logstash.api.service" : "INFO",
+    "logstash.basepipeline" : "INFO",
+    "logstash.codecs.plain" : "INFO",
+    "logstash.codecs.rubydebug" : "INFO",
+    "logstash.filters.grok" : "INFO",
+    "logstash.inputs.beats" : "INFO",
+    "logstash.instrument.periodicpoller.jvm" : "INFO",
+    "logstash.instrument.periodicpoller.os" : "INFO",
+    "logstash.instrument.periodicpoller.persistentqueue" : "INFO",
+    "logstash.outputs.stdout" : "INFO",
+    "logstash.pipeline" : "INFO",
+    "logstash.plugins.registry" : "INFO",
+    "logstash.runner" : "INFO",
+    "logstash.shutdownwatcher" : "INFO",
+    "org.logstash.Event" : "INFO",
+    "slowlog.logstash.codecs.plain" : "TRACE",
+    "slowlog.logstash.codecs.rubydebug" : "TRACE",
+    "slowlog.logstash.filters.grok" : "TRACE",
+    "slowlog.logstash.inputs.beats" : "TRACE",
+    "slowlog.logstash.outputs.stdout" : "TRACE"
+  }
+}
+--------------------------------------------------
diff --git a/docs/static/logstash-docs-home.asciidoc b/docs/static/logstash-docs-home.asciidoc
deleted file mode 100644
index 19bd3281184..00000000000
--- a/docs/static/logstash-docs-home.asciidoc
+++ /dev/null
@@ -1,30 +0,0 @@
-[[logstash-docs-home]]
-== Logstash Documentation
-Pretty self-explanatory, really
-
-=== Downloads and Releases
-* http://www.elasticsearch.org/overview/logstash/download/[Download Logstash 1.4.2]
-* http://www.elasticsearch.org/blog/apt-and-yum-repositories/[package repositories]
-* http://www.elasticsearch.org/blog/logstash-1-4-2/[release notes]
-* https://github.com/elasticsearch/logstash/blob/master/CHANGELOG[view changelog]
-* https://github.com/elasticsearch/puppet-logstash[Puppet Module]
-
-=== Plugins
-* http://elasticsearch.org/#[contrib plugins]
-* http://elasticsearch.org/#[writing your own plugins]
-* http://elasticsearch.org/#[Inputs] / http://elasticsearch.org/#[Filters] / http://elasticsearch.org/#[Outputs]
-* http://elasticsearch.org/#[Codecs]
-* http://elasticsearch.org/#[(more)]
-
-=== HOWTOs, References, Information
-* http://elasticsearch.org/#[Getting Started with Logstash]
-* http://elasticsearch.org/#[Configuration file overview]
-* http://elasticsearch.org/#[Command-line flags]
-* http://elasticsearch.org/#[The life of an event in Logstash]
-* http://elasticsearch.org/#[Using conditional logic]
-* http://elasticsearch.org/#[Glossary]
-* http://elasticsearch.org/#[(more)]
-
-=== About / Videos / Blogs
-* http://elasticsearch.org/#[Videos]
-* http://elasticsearch.org/#[Blogs]
diff --git a/docs/static/maintainer-guide.asciidoc b/docs/static/maintainer-guide.asciidoc
index 5c8253ca8e3..7d3e0c9a029 100644
--- a/docs/static/maintainer-guide.asciidoc
+++ b/docs/static/maintainer-guide.asciidoc
@@ -1,15 +1,17 @@
 [[community-maintainer]]
-== Logstash Plugins Community Maintainer Guide
+=== Logstash Plugins Community Maintainer Guide
 
 This document, to be read by new Maintainers, should explain their responsibilities.  It was inspired by the
 http://rfc.zeromq.org/spec:22[C4] document from the ZeroMQ project.  This document is subject to change and suggestions
 through Pull Requests and issues are strongly encouraged.
 
+[float]
 === Contribution Guidelines
 
 For general guidance around contributing to Logstash Plugins, see the
 https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html[_Contributing to Logstash_] section.
 
+[float]
 === Document Goals
 
 To help make the Logstash plugins community participation easy with positive feedback.
@@ -24,6 +26,7 @@ To support the natural life cycle of a plugin.
 To codify the roles and responsibilities of: Maintainers and Contributors with specific focus on patch testing, code
 review, merging and release.
 
+[float]
 === Development Workflow
 
 All Issues and Pull Requests must be tracked using the Github issue tracker.
@@ -32,6 +35,7 @@ The plugin uses the http://www.apache.org/licenses/LICENSE-2.0[Apache 2.0 licens
 patch introduces code which has an incompatible license. Patch ownership and copyright is defined in the Elastic
 https://www.elastic.co/contributor-agreement[Contributor License Agreement] (CLA).
 
+[float]
 ==== Terminology
 
 A "Contributor" is a role a person assumes when providing a patch. Contributors will not have commit access to the
@@ -41,6 +45,7 @@ before a patch can be reviewed. Contributors can add themselves to the plugin Co
 A "Maintainer" is a role a person assumes when maintaining a plugin and keeping it healthy, including triaging issues, and
 reviewing and merging patches.
 
+[float]
 ==== Patch Requirements
 
 A patch is a minimal and accurate answer to exactly one identified and agreed upon problem. It must conform to the
@@ -57,6 +62,7 @@ and any additional lines as necessary for change explanation and rationale.
 A patch is mergeable when it satisfies the above requirements and has been reviewed positively by at least one other
 person.
 
+[float]
 ==== Development Process
 
 A user will log an issue on the issue tracker describing the problem they face or observe with as much detail as possible.
@@ -79,26 +85,61 @@ Maintainers should involve the core team if help is needed to reach consensus.
 
 Review non-source changes such as documentation in the same way as source code changes.
 
+[float]
 ==== Branch Management
 
 The plugin has a master branch that always holds the latest in-progress version and should always build.  Topic branches
 should kept to the minimum.
 
+[float]
 ==== Changelog Management
 
 Every plugin should have a changelog (CHANGELOG.md).  If not, please create one.  When changes are made to a plugin, make sure to include a changelog entry under the respective version to document the change.  The changelog should be easily understood from a user point of view.  As we iterate and release plugins rapidly, users use the changelog as a mechanism for deciding whether to update.
 
-Changes that are not user facing should be tagged as â€œInternal: â€.  For example:
-
-* "Internal: Refactored specs for better testing"
-* "Default timeout configuration changed from 10s to 5s"
-
+Changes that are not user facing should be tagged as `internal:`.  For example:
+
+[source,markdown]
+ - internal: Refactored specs for better testing
+ - config: Default timeout configuration changed from 10s to 5s
+
+[float]
+===== Detailed format of CHANGELOG.md
+
+Sharing a similar format of CHANGELOG.md in plugins ease readability for users.
+Please see following annotated example and see a concrete example in https://raw.githubusercontent.com/logstash-plugins/logstash-filter-date/master/CHANGELOG.md[logstash-filter-date].
+
+[source,markdown]
+----
+## 1.0.x                              // <1> <2>
+ - change description                 // <3>
+ - tag: change description            // <3> <4>
+ - tag1,tag2: change description      // <3> <5>
+ - tag: Multi-line description        // <3> <6>
+   must be indented and can use
+   additional markdown syntax
+                                      // <7>
+## 1.0.0                              // <8>
+[...]
+
+----
+<1> Latest version is the first line of CHANGELOG.md
+<2> Each version identifier should be a level-2 header using `##`
+<3> One change description is described as a list item using a dash `-` aligned under the version identifier
+<4> One change can be tagged by a word and suffixed by `:`. +
+    Common tags are `bugfix`, `feature`, `doc`, `test` or `internal`.
+<5> One change can have multiple tags separated by a comma and suffixed by `:`
+<6> A multi-line change description must be properly indented
+<7> Please take care to *separate versions with an empty line*
+<8> Previous version identifier
+
+[float]
 ==== Continuous Integration
 
 Plugins are setup with automated continuous integration (CI) environments and there should be a corresponding badge on each Github page.  If itâ€™s missing, please contact the Logstash core team.
 
 Every Pull Request opened automatically triggers a CI run.  To conduct a manual run, comment â€œJenkins, please test this.â€ on the Pull Request.
 
+[float]
 === Versioning Plugins
 
 Logstash core and its plugins have separate product development lifecycles. Hence the versioning and release strategy for
@@ -115,11 +156,13 @@ Plugin releases follows a three-placed numbering scheme X.Y.Z. where X denotes a
 compatibility with existing configuration or functionality. Y denotes releases which includes features which are backward
 compatible. Z denotes releases which includes bug fixes and patches.
 
+[float]
 ==== Changing the version
 
 Version can be changed in the Gemspec, which needs to be associated with a changelog entry. Following this, we can publish
 the gem to RubyGem.org manually. At this point only the core developers can publish a gem.
 
+[float]
 ==== Labeling
 
 Labeling is a critical aspect of maintaining plugins. All issues in GitHub should be labeled correctly so it can:
@@ -139,6 +182,7 @@ details.
 * `needs tests`: Patch has no tests, and cannot be accepted without unit/integration tests
 * `docs`: Documentation related issue/PR
 
+[float]
 === Logging
 
 Although itâ€™s important not to bog down performance with excessive logging, debug level logs can be immensely helpful when
@@ -148,6 +192,7 @@ as users will be forever gracious.
 [source,shell]
 @logger.debug("Logstash loves debug logs!", :actions => actions)
 
+[float]
 === Contributor License Agreement (CLA) Guidance
 
 [qanda]
@@ -161,10 +206,12 @@ Please make sure the CLA is signed by every Contributor prior to reviewing PRs a
      signs the CLA after a PR is submitted, they can refresh the automated CLA checker by pushing another
      comment on the PR after 5 minutes of signing.
 
+[float]
 === Need Help?
 
 Ping @logstash-core on Github to get the attention of the Logstash core team.
 
+[float]
 === Community Administration
 
 The core team is there to support the plugin Maintainers and overall ecosystem.
diff --git a/docs/static/managing-multiline-events.asciidoc b/docs/static/managing-multiline-events.asciidoc
index 7245b3c50c1..79b22e29a3a 100644
--- a/docs/static/managing-multiline-events.asciidoc
+++ b/docs/static/managing-multiline-events.asciidoc
@@ -1,16 +1,23 @@
 [[multiline]]
 === Managing Multiline Events
 
-Several use cases generate events that span multiple lines of text. In order to correctly handle these multline events,
+Several use cases generate events that span multiple lines of text. In order to correctly handle these multiline events,
 Logstash needs to know how to tell which lines are part of a single event.
 
 Multiline event processing is complex and relies on proper event ordering. The best way to guarantee ordered log
-processing is to implement the processing as early in the pipeline as possible. The preferred tool in the Logstash
-pipeline is the {logstash}plugins-codecs-multiline.html[multiline codec], which merges lines from a single input using
+processing is to implement the processing as early in the pipeline as possible.
+
+The <<plugins-codecs-multiline>> codec is the preferred tool for handling multiline events
+in the Logstash pipeline. The multiline codec merges lines from a single input using
 a simple set of rules.
 
+IMPORTANT: If you are using a Logstash input plugin that supports multiple hosts, such as
+the <<plugins-inputs-beats>> input plugin, you should not use the
+<<plugins-codecs-multiline>> codec to handle multiline events. Doing so may result in the 
+mixing of streams and corrupted event data. In this situation, you need to handle multiline
+events before sending the event data to Logstash. 
 
-The most important aspects of configuring either multiline plugin are the following:
+The most important aspects of configuring the multiline codec are the following:
 
 * The `pattern` option specifies a regular expression. Lines that match the specified regular expression are considered
 either continuations of a previous line or the start of a new multiline event. You can use
@@ -20,16 +27,10 @@ value in the `pattern` option are part of the previous line. The `next` value sp
 in the `pattern` option are part of the following line.* The `negate` option applies the multiline codec to lines that
 _do not_ match the regular expression specified in the `pattern` option.
 
-See the full documentation for the {logstash}plugins-codecs-multiline.html[multiline codec] or the
-{logstash}plugins-filters-multiline.html[multiline filter] plugin for more information on configuration options.
-
-NOTE: For more complex needs, the {logstash}plugins-filters-multiline.html[multiline filter] performs a similar task at
-the filter stage of processing, where the Logstash instance aggregates multiple inputs.
-The multiline filter plugin is not thread-safe. Avoid using multiple filter workers with the multiline filter. You can
-track the progress of upgrades to the functionality of the multiline codec at
-https://github.com/logstash-plugins/logstash-codec-multiline/issues/10[this Github issue].
+See the full documentation for the <<plugins-codecs-multiline>> codec plugin for more information
+on configuration options.
 
-==== Examples of Multiline Plugin Configuration
+==== Examples of Multiline Codec Configuration
 
 The examples in this section cover the following use cases:
 
diff --git a/docs/static/monitoring-apis.asciidoc b/docs/static/monitoring-apis.asciidoc
index 97b2a57c3c4..98f246a402c 100644
--- a/docs/static/monitoring-apis.asciidoc
+++ b/docs/static/monitoring-apis.asciidoc
@@ -4,10 +4,34 @@
 Logstash provides the following monitoring APIs to retrieve runtime metrics
 about Logstash:
 
-* <<root-resource-api>>
-* <<stats-info-api>>
-* <<hot-threads-api>>
+* <<node-info-api>>
 * <<plugins-api>>
+* <<node-stats-api>>
+* <<hot-threads-api>>
+
+
+You can use the root resource to retrieve general information about the Logstash instance, including
+the host and version.
+
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/?pretty'
+--------------------------------------------------
+
+Example response:
+
+["source","js",subs="attributes"]
+--------------------------------------------------
+{
+   "host": "skywalker",
+   "version": "{logstash_version}",
+   "http_address": "127.0.0.1:9600"
+}
+--------------------------------------------------
+
+NOTE: By default, the monitoring API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
+instance, you need to launch Logstash with the `--http.port` flag specified to bind to a different port. See
+<<command-line-flags>> for more information.
 
 [float]
 [[monitoring-common-options]]
@@ -19,14 +43,12 @@ The following options can be applied to all of the Logstash monitoring APIs.
 ==== Pretty Results
 
 When appending `?pretty=true` to any request made, the JSON returned
-will be pretty formatted (use it for debugging only!). Another option is
-to set `?format=yaml` which will cause the result to be returned in the
-(sometimes) more readable yaml format.
+will be pretty formatted (use it for debugging only!).
 
 [float]
 ==== Human-Readable Output
 
-NOTE: For Alpha 1, the `human` option is supported for the <<hot-threads-api>>
+NOTE: For Logstash {logstash_version}, the `human` option is supported for the <<hot-threads-api>>
 only. When you specify `human=true`, the results are returned in plain text instead of
 JSON format. The default is false.
 
@@ -39,50 +61,93 @@ being consumed by a monitoring tool, rather than intended for human
 consumption.  The default for the `human` flag is
 `false`.
 
-[[plugins-api]]
-=== Plugins API
+[[node-info-api]]
+=== Node Info API
 
-experimental[]
+The node info API retrieves information about the node.
 
-The plugins API gets information about all Logstash plugins that are currently installed.
-This API basically returns the output of running the `bin/logstash-plugin list --verbose` command.
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/<types>'
+--------------------------------------------------
+
+Where `<types>` is optional and specifies the types of node info you want to return.
+
+You can limit the info that's returned by combining any of the following types in a comma-separated list:
+
+[horizontal]
+<<node-pipeline-info,`pipeline`>>::
+Gets pipeline-specific information and settings.
+<<node-os-info,`os`>>::
+Gets node-level info about the OS.
+<<node-jvm-info,`jvm`>>::
+Gets node-level JVM info, including info about threads.
+
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
+[[node-pipeline-info]]
+==== Pipeline Info
+
+The following request returns a JSON document that shows pipeline info, such as the number of workers,
+batch size, and batch delay:
 
 [source,js]
 --------------------------------------------------
-GET /_plugins
+curl -XGET 'localhost:9600/_node/pipeline?pretty'
 --------------------------------------------------
 
-The output is a JSON document.
+If you want to view additional information about the pipeline, such as stats for each configured input, filter,
+or output stage, see the <<pipeline-stats>> section under the <<node-stats-api>>.
 
 Example response:
 
-[source,js]
+["source","js",subs="attributes"]
 --------------------------------------------------
-"total": 102
-"plugins" : [
-  {
-      "name": "logstash-output-pagerduty"
-      "version": "2.0.2"
-  },
-  {
-      "name": "logstash-output-elasticsearch"
-      "version": "2.1.2"
+{
+  "pipeline": {
+    "workers": 8,
+    "batch_size": 125,
+    "batch_delay": 5,
+    "config_reload_automatic": true,
+    "config_reload_interval": 3
+    "id" : "main"
   }
-....
-] 
 --------------------------------------------------
 
-[[root-resource-api]]
-=== Root Resource API
+[[node-os-info]]
+==== OS Info
 
-experimental[]
+The following request returns a JSON document that shows the OS name, architecture, version, and
+available processors:
 
-The root resource API retrieves general information about the Logstash instance, including
-the host name and version information.
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/os?pretty'
+--------------------------------------------------
+
+Example response:
 
 [source,js]
 --------------------------------------------------
-GET /
+{
+  "os": {
+    "name": "Mac OS X",
+    "arch": "x86_64",
+    "version": "10.12.4",
+    "available_processors": 8
+  }
+--------------------------------------------------
+
+[[node-jvm-info]]
+==== JVM Info
+
+The following request returns a JSON document that shows node-level JVM stats, such as the JVM process id, version,
+VM info, memory usage, and info about garbage collectors:
+
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/jvm?pretty'
 --------------------------------------------------
 
 Example response:
@@ -90,56 +155,177 @@ Example response:
 [source,js]
 --------------------------------------------------
 {
-   "hostname": "skywalker",
-    "version" : {
-        "number" : "2.1.0",       
-    }
+  "jvm": {
+    "pid": 59616,
+    "version": "1.8.0_65",
+    "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
+    "vm_version": "1.8.0_65",
+    "vm_vendor": "Oracle Corporation",
+    "start_time_in_millis": 1484251185878,
+    "mem": {
+      "heap_init_in_bytes": 268435456,
+      "heap_max_in_bytes": 1037959168,
+      "non_heap_init_in_bytes": 2555904,
+      "non_heap_max_in_bytes": 0
+    },
+    "gc_collectors": [
+      "ParNew",
+      "ConcurrentMarkSweep"
+    ]
   }
+}
 --------------------------------------------------
 
+[[plugins-api]]
+=== Plugins Info API
+
+The plugins info API gets information about all Logstash plugins that are currently installed.
+This API basically returns the output of running the `bin/logstash-plugin list --verbose` command.
+
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/plugins?pretty'
+--------------------------------------------------
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
 Logstash monitoring APIs.
 
-[[stats-info-api]]
-=== Stats Info API
+The output is a JSON document.
 
-experimental[]
+Example response:
+
+["source","js",subs="attributes"]
+--------------------------------------------------
+{
+  "total": 93,
+  "plugins": [
+    {
+      "name": "logstash-codec-cef",
+      "version": "4.1.2"
+    },
+    {
+      "name": "logstash-codec-collectd",
+      "version": "3.0.3"
+    },
+    {
+      "name": "logstash-codec-dots",
+      "version": "3.0.2"
+    },
+    {
+      "name": "logstash-codec-edn",
+      "version": "3.0.2"
+    },
+    .
+    .
+    .
+  ]
+--------------------------------------------------
 
-The stats info API retrieves runtime stats about Logstash. 
+[[node-stats-api]]
+=== Node Stats API
 
-// COMMENTED OUT until Logstash supports multiple pipelines: To retrieve all stats for the Logstash instance, use the `_node/stats` endpoint:
+The node stats API retrieves runtime stats about Logstash.
 
 [source,js]
 --------------------------------------------------
-GET /_node/stats/<types>
+curl -XGET 'localhost:9600/_node/stats/<types>'
 --------------------------------------------------
 
-////
-COMMENTED OUT until Logstash supports multiple pipelines: To retrieve all stats per pipeline, use the `_pipelines/stats` endpoint:
+Where `<types>` is optional and specifies the types of stats you want to return.
+
+By default, all stats are returned. You can limit the info that's returned by combining any of the following types in a comma-separated list:
+
+[horizontal]
+<<jvm-stats,`jvm`>>::
+Gets JVM stats, including stats about threads, memory usage, garbage collectors,
+and uptime.
+<<process-stats,`process`>>::
+Gets process stats, including stats about file descriptors, memory consumption, and CPU usage.
+<<pipeline-stats,`pipeline`>>::
+Gets runtime stats about the Logstash pipeline.
+<<reload-stats,`reloads`>>::
+Gets runtime stats about config reload successes and failures.
+<<os-stats,`os`>>::
+Gets runtime stats about cgroups when Logstash is running in a container.
+
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
+[[jvm-stats]]
+==== JVM Stats
+
+The following request returns a JSON document containing JVM stats:
 
 [source,js]
 --------------------------------------------------
-GET /_pipelines/stats/<types>
+curl -XGET 'localhost:9600/_node/stats/jvm?pretty'
 --------------------------------------------------
-////
 
-Where `<types>` is optional and specifies the types of stats you want to return.
+Example response:
 
-By default, all stats are returned. You can limit this by combining any of the following types: 
+[source,js]
+--------------------------------------------------
+{
+  "jvm": {
+    "threads": {
+      "count": 35,
+      "peak_count": 36
+    },
+    "mem": {
+      "heap_used_in_bytes": 318691184,
+      "heap_used_percent": 15,
+      "heap_committed_in_bytes": 519045120,
+      "heap_max_in_bytes": 2075918336,
+      "non_heap_used_in_bytes": 189382304,
+      "non_heap_committed_in_bytes": 200728576,
+      "pools": {
+        "survivor": {
+          "peak_used_in_bytes": 8912896,
+          "used_in_bytes": 9538656,
+          "peak_max_in_bytes": 35782656,
+          "max_in_bytes": 71565312,
+          "committed_in_bytes": 17825792
+        },
+        "old": {
+          "peak_used_in_bytes": 106946320,
+          "used_in_bytes": 181913072,
+          "peak_max_in_bytes": 715849728,
+          "max_in_bytes": 1431699456,
+          "committed_in_bytes": 357957632
+        },
+        "young": {
+          "peak_used_in_bytes": 71630848,
+          "used_in_bytes": 127239456,
+          "peak_max_in_bytes": 286326784,
+          "max_in_bytes": 572653568,
+          "committed_in_bytes": 143261696
+        }
+      }
+    },
+    "gc": {
+      "collectors": {
+        "old": {
+          "collection_time_in_millis": 58,
+          "collection_count": 2
+        },
+        "young": {
+          "collection_time_in_millis": 338,
+          "collection_count": 26
+        }
+      }
+    },
+    "uptime_in_millis": 382701
+  }
+--------------------------------------------------
 
-[horizontal]
-`events`::
-	Gets event information since startup. 
-`jvm`::
-	Gets JVM stats, including stats about garbage collection. 
+[[process-stats]]
+==== Process Stats
 
-For example, the following request returns a JSON document that shows the number of events
-that were input, filtered, and output by Logstash since startup:
+The following request returns a JSON document containing process stats:
 
 [source,js]
 --------------------------------------------------
-GET /_node/stats/events
+curl -XGET 'localhost:9600/_node/stats/process?pretty'
 --------------------------------------------------
 
 Example response:
@@ -147,140 +333,311 @@ Example response:
 [source,js]
 --------------------------------------------------
 {
-    "events": {
-        "in": 91,
-        "filtered": 91,
-        "out": 91
+  "process": {
+    "open_file_descriptors": 164,
+    "peak_open_file_descriptors": 166,
+    "max_file_descriptors": 10240,
+    "mem": {
+      "total_virtual_in_bytes": 5399474176
+    },
+    "cpu": {
+      "total_in_millis": 72810537000,
+      "percent": 0,
+      "load_average": {
+        "1m": 2.41943359375
+      }
     }
+  }
 }
 --------------------------------------------------
 
-The following request returns a JSON document containing JVM stats:
+[[pipeline-stats]]
+==== Pipeline Stats
+
+The following request returns a JSON document containing pipeline stats,
+including:
+
+* the number of events that were input, filtered, or output by the pipeline
+* stats for each configured filter or output stage
+* info about config reload successes and failures
+(when <<reloading-config,config reload>> is enabled)
+* info about the persistent queue (when
+<<persistent-queues,persistent queues>> are enabled)
 
 [source,js]
 --------------------------------------------------
-GET /_node/stats/jvm
+curl -XGET 'localhost:9600/_node/stats/pipeline?pretty'
 --------------------------------------------------
 
 Example response:
 
 [source,js]
 --------------------------------------------------
-"jvm":{  
-   "timestamp":1453233447702,
-   "uptime_in_millis":211125811,
-   "mem":{  
-      "heap_used_in_bytes":58442576,
-      "heap_used_percent":5,
-      "heap_committed_in_bytes":259522560,
-      "heap_max_in_bytes":1037959168,
-      "non_heap_used_in_bytes":56332256,
-      "non_heap_committed_in_bytes":57475072,
-      "pools":{  
-         "young":{  
-            "used_in_bytes":41672000,
-            "max_in_bytes":286326784,
-            "peak_used_in_bytes":71630848,
-            "peak_max_in_bytes":286326784
-         },
-         "survivor":{  
-            "used_in_bytes":260552,
-            "max_in_bytes":35782656,
-            "peak_used_in_bytes":8912896,
-            "peak_max_in_bytes":35782656
-         },
-         "old":{  
-            "used_in_bytes":16510024,
-            "max_in_bytes":715849728,
-            "peak_used_in_bytes":16510024,
-            "peak_max_in_bytes":715849728
-         }
+{
+  "pipeline" : {
+    "events" : {
+      "duration_in_millis" : 1955,
+      "in" : 100,
+      "filtered" : 100,
+      "out" : 100,
+      "queue_push_duration_in_millis" : 71
+    },
+    "plugins" : {
+      "inputs" : [ {
+        "id" : "729b0efdc657715a4a59103ab2643c010fc46e77-1",
+        "events" : {
+          "out" : 100,
+          "queue_push_duration_in_millis" : 71
+        },
+        "name" : "beats"
+      } ],
+      "filters" : [ {
+        "id" : "729b0efdc657715a4a59103ab2643c010fc46e77-2",
+        "events" : {
+          "duration_in_millis" : 64,
+          "in" : 100,
+          "out" : 100
+        },
+        "matches" : 100,
+        "patterns_per_field" : {
+          "message" : 1
+        },
+        "name" : "grok"
+      } ],
+      "outputs" : [ {
+        "id" : "729b0efdc657715a4a59103ab2643c010fc46e77-3",
+        "events" : {
+          "duration_in_millis" : 1724,
+          "in" : 100,
+          "out" : 100
+        },
+        "name" : "stdout"
+      } ]
+    },
+    "reloads" : {
+      "last_error" : null,
+      "successes" : 2,
+      "last_success_timestamp" : "2017-05-25T02:40:40.974Z",
+      "last_failure_timestamp" : null,
+      "failures" : 0
+    },
+    "queue" : {
+      "events" : 0,
+      "type" : "persisted",
+      "capacity" : {
+        "page_capacity_in_bytes" : 262144000,
+        "max_queue_size_in_bytes" : 8589934592,
+        "max_unread_events" : 0
+      },
+      "data" : {
+        "path" : "/path/to/data/queue",
+        "free_space_in_bytes" : 89280552960,
+        "storage_type" : "hfs"
       }
-   }
+    },
+    "id" : "main"
+  }
+}
+--------------------------------------------------
+
+[[reload-stats]]
+==== Reload Stats
+
+The following request returns a JSON document that shows info about config reload successes and failures.
+
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/stats/reloads?pretty'
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "reloads": {
+    "successes": 0,
+    "failures": 0
+  }
+}
+--------------------------------------------------
+
+[[os-stats]]
+==== OS Stats
+
+When Logstash is running in a container, the following request returns a JSON document that
+contains cgroup information to give you a more accurate view of CPU load, including whether
+the container is being throttled. 
+
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/stats/os?pretty'
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "os" : {
+    "cgroup" : { 
+      "cpuacct" : {
+        "control_group" : "/elastic1",
+        "usage_nanos" : 378477588075
+                },
+      "cpu" : {
+        "control_group" : "/elastic1",
+        "cfs_period_micros" : 1000000,
+        "cfs_quota_micros" : 800000,
+        "stat" : {
+          "number_of_elapsed_periods" : 4157,
+          "number_of_times_throttled" : 460,
+          "time_throttled_nanos" : 581617440755
+        }
+      }    
+    }
+  }
 --------------------------------------------------
 
-See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
 
 [[hot-threads-api]]
 === Hot Threads API
 
-experimental[]
-
 The hot threads API gets the current hot threads for Logstash. A hot thread is a
 Java thread that has high CPU usage and executes for a longer than normal period
 of time.
 
 [source,js]
 --------------------------------------------------
-GET /_node/hot_threads
+curl -XGET 'localhost:9600/_node/hot_threads?pretty'
 --------------------------------------------------
 
 The output is a JSON document that contains a breakdown of the top hot threads for
-Logstash. The parameters allowed are:
-
-[horizontal]
-`threads`:: 	        The number of hot threads to return. The default is 3. 
-`human`:: 	            If true, returns plain text instead of JSON format. The default is false. 
-`ignore_idle_threads`:: If true, does not return idle threads. The default is true.
+Logstash.
 
 Example response:
 
 [source,js]
 --------------------------------------------------
 {
-  "hostname" : "Example-MBP-2",
-  "time" : "2016-03-08T17:58:18-08:00",
-  "busiest_threads" : 3,
-  "threads" : [ {
-    "name" : "LogStash::Runner",
-    "percent_of_cpu_time" : 16.93,
-    "state" : "timed_waiting",
-    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\tjava.lang.Thread.join(Thread.java:1253)\n\t\torg.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)\n\t\torg.jruby.RubyThread.join(RubyThread.java:697)\n\t\torg.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)\n\t\torg.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)\n\t\torg.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)\n"
-  }, {
-    "name" : "Api Webserver",
-    "percent_of_cpu_time" : 0.39,
-    "state" : "timed_waiting",
-    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\tjava.lang.Thread.join(Thread.java:1253)\n\t\torg.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)\n\t\torg.jruby.RubyThread.join(RubyThread.java:697)\n\t\torg.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)\n\t\torg.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)\n\t\torg.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)\n"
-  }, {
-    "name" : "Ruby-0-Thread-13",
-    "percent_of_cpu_time" : 0.15,
-    "state" : "timed_waiting",
-    "path" : "/Users/suyog/ws/elastic/logstash/build/logstash-3.0.0.dev/vendor/local_gems/f5685da5/logstash-core-3.0.0.dev-java/lib/logstash/pipeline.rb:496",
-    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\torg.jruby.RubyThread.sleep(RubyThread.java:1002)\n\t\torg.jruby.RubyKernel.sleep(RubyKernel.java:803)\n\t\torg.jruby.RubyKernel$INVOKER$s$0$1$sleep.call(RubyKernel$INVOKER$s$0$1$sleep.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:667)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:206)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:168)\n\t\torg.jruby.ast.FCallOneArgNode.interpret(FCallOneArgNode.java:36)\n\t\torg.jruby.ast.NewlineNode.interpret(NewlineNode.java:105)\n\t\torg.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n"
-  } ]
+    "time": "2017-01-12T12:09:45-08:00",
+    "busiest_threads": 3,
+    "threads": [
+      {
+        "name": "LogStash::Runner",
+        "percent_of_cpu_time": 1.07,
+        "state": "timed_waiting",
+        "traces": [
+          "java.lang.Object.wait(Native Method)",
+          "java.lang.Thread.join(Thread.java:1253)",
+          "org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)",
+          "org.jruby.RubyThread.join(RubyThread.java:697)",
+          "org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)",
+          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)",
+          "org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)",
+          "org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)",
+          "org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)",
+          "org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)"
+        ]
+      },
+      {
+        "name": "[main]>worker7",
+        "percent_of_cpu_time": 0.71,
+        "state": "waiting",
+        "traces": [
+          "sun.misc.Unsafe.park(Native Method)",
+          "java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:897)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1222)",
+          "java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)",
+          "org.jruby.RubyThread.lockInterruptibly(RubyThread.java:1470)",
+          "org.jruby.ext.thread.Mutex.lock(Mutex.java:91)",
+          "org.jruby.ext.thread.Mutex.synchronize(Mutex.java:147)",
+          "org.jruby.ext.thread.Mutex$INVOKER$i$0$0$synchronize.call(Mutex$INVOKER$i$0$0$synchronize.gen)"
+        ]
+      },
+      {
+        "name": "[main]>worker3",
+        "percent_of_cpu_time": 0.71,
+        "state": "waiting",
+        "traces": [
+          "sun.misc.Unsafe.park(Native Method)",
+          "java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:897)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1222)",
+          "java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)",
+          "org.jruby.RubyThread.lockInterruptibly(RubyThread.java:1470)",
+          "org.jruby.ext.thread.Mutex.lock(Mutex.java:91)",
+          "org.jruby.ext.thread.Mutex.synchronize(Mutex.java:147)",
+          "org.jruby.ext.thread.Mutex$INVOKER$i$0$0$synchronize.call(Mutex$INVOKER$i$0$0$synchronize.gen)"
+        ]
+      }
+    ]
+  }
+}
 --------------------------------------------------
 
+The parameters allowed are:
+
+[horizontal]
+`threads`:: 	        The number of hot threads to return. The default is 3.
+`human`:: 	            If true, returns plain text instead of JSON format. The default is false.
+`ignore_idle_threads`:: If true, does not return idle threads. The default is true.
+
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
 You can use the `?human` parameter to return the document in a human-readable format.
 
 [source,js]
 --------------------------------------------------
-GET /_node/hot_threads?human=true
+curl -XGET 'localhost:9600/_node/hot_threads?human=true'
 --------------------------------------------------
 
-Example of a human-readable response: 
+Example of a human-readable response:
 
 [source,js]
 --------------------------------------------------
-::: {Ringo Kid}{Gv3UrzR3SqmPQIgfG4qJMA}{127.0.0.1}{127.0.0.1:9300}
-   Hot threads at 2016-01-13T16:55:49.988Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:
-
-    0.0% (216micros out of 500ms) cpu usage by thread 'elasticsearch[Ringo Kid][transport_client_timer][T#1]{Hashed wheel timer #1}'
-     10/10 snapshots sharing following 5 elements
-       java.lang.Thread.sleep(Native Method)
-       org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
-       org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
-       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
-       java.lang.Thread.run(Thread.java:745)
+ ::: {}
+ Hot threads at 2017-01-12T12:10:15-08:00, busiestThreads=3: 
+ ================================================================================
+ 1.02 % of cpu usage, state: timed_waiting, thread name: 'LogStash::Runner' 
+	java.lang.Object.wait(Native Method)
+	java.lang.Thread.join(Thread.java:1253)
+	org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)
+	org.jruby.RubyThread.join(RubyThread.java:697)
+	org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)
+	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)
+	org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)
+	org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)
+	org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)
+	org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)
+ --------------------------------------------------------------------------------
+ 0.71 % of cpu usage, state: waiting, thread name: '[main]>worker7' 
+	sun.misc.Unsafe.park(Native Method)
+	java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
+	java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
+	java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:897)
+	java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1222)
+	java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)
+	org.jruby.RubyThread.lockInterruptibly(RubyThread.java:1470)
+	org.jruby.ext.thread.Mutex.lock(Mutex.java:91)
+	org.jruby.ext.thread.Mutex.synchronize(Mutex.java:147)
+	org.jruby.ext.thread.Mutex$INVOKER$i$0$0$synchronize.call(Mutex$INVOKER$i$0$0$synchronize.gen)
+ --------------------------------------------------------------------------------
+ 0.71 % of cpu usage, state: timed_waiting, thread name: '[main]>worker3' 
+	sun.misc.Unsafe.park(Native Method)
+	java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
+	java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
+	java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
+	java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
+	sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
+	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
+	java.lang.reflect.Method.invoke(Method.java:497)
+	org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:466)
+	org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:324)
 
-    0.0% (216micros out of 500ms) cpu usage by thread 'elasticsearch[Ringo Kid][transport_client_timer][T#1]{Hashed wheel timer #1}'
-     10/10 snapshots sharing following 5 elements
-       java.lang.Thread.sleep(Native Method)
-       org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
-       org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
-       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
-       java.lang.Thread.run(Thread.java:745)
 --------------------------------------------------
 
-See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
diff --git a/docs/static/offline-plugins.asciidoc b/docs/static/offline-plugins.asciidoc
index e099faec8a8..b67f34169bb 100644
--- a/docs/static/offline-plugins.asciidoc
+++ b/docs/static/offline-plugins.asciidoc
@@ -1,21 +1,101 @@
 [[offline-plugins]]
 === Offline Plugin Management
 
-The Logstash <<working-with-plugins,plugin manager>> was introduced in the 1.5 release. This section discusses setting up
-local repositories of plugins for use on systems without access to the Internet.
+The Logstash <<working-with-plugins,plugin manager>> provides support for preparing offline plugin packs that you can
+use to install Logstash plugins on systems that don't have Internet access. 
 
-The procedures in this section require a staging machine running Logstash that has access to a public or private Rubygems
-server. This staging machine downloads and packages the files used for offline installation.
+This procedure requires a staging machine running Logstash that has access to a public or
+<<private-rubygem,private Rubygems>> server. The staging machine downloads and packages all the files and dependencies
+required for offline installation.
 
-See the <<private-rubygem,Private Gem Repositories>> section for information on setting up your own private
-Rubygems server.
+NOTE: If you used offline plugin management prior to Logstash 5.2, you used the `pack` and `unpack` subcommands. Those
+subcommands are now deprecated, but the procedure for using them is still available in the documentation 
+<<building-offline-packages-deprecated,here>>.
 
-Users who can work with a larger Logstash artifact size can use the *Logstash (All Plugins)* download link from the
-https://www.elastic.co/downloads/logstash[Logstash product page] to download Logstash bundled with the latest version of
-all available plugins. You can distribute this bundle to all nodes without further plugin staging.
+[[building-offline-packs]]
+[float]
+=== Building Offline Plugin Packs
+
+An _offline plugin pack_ is a compressed file that contains all the plugins your offline Logstash installation requires,
+along with the dependencies for those plugins.
+
+To build an offline plugin pack:
+
+. Make sure all the plugins that you want to package are installed on the staging server and that the staging server can
+access the Internet.
+
+. Run the `bin/logstash-plugin prepare-offline-pack` subcommand to package the plugins and dependencies:
++
+[source, shell]
+-------------------------------------------------------------------------------
+bin/logstash-plugin prepare-offline-pack --output OUTPUT [PLUGINS] --overwrite
+-------------------------------------------------------------------------------
++
+where:
++
+* `OUTPUT` specifies the zip file where the compressed plugin pack will be written. The default file is
++/LOGSTASH_HOME/logstash-offline-plugins-{logstash_version}.zip+. If you are using 5.2.x and 5.3.0, this location should be a zip file whose contents will be overwritten.
+* `[PLUGINS]` specifies one or more plugins that you want to include in the pack.
+* `--overwrite` specifies if you want to override an existing file at the location
+
+Examples:
+
+["source","sh",subs="attributes"]
+-------------------------------------------------------------------------------
+bin/logstash-plugin prepare-offline-pack logstash-input-beats <1>
+bin/logstash-plugin prepare-offline-pack logstash-filter-* <2>
+bin/logstash-plugin prepare-offline-pack logstash-filter-* logstash-input-beats <3>
+-------------------------------------------------------------------------------
+<1> Packages the Beats input plugin and any dependencies.
+<2> Uses a wildcard to package all filter plugins and any dependencies.
+<3> Packages all filter plugins, the Beats input plugin, and any dependencies.
+
+NOTE: Downloading all dependencies for the specified plugins may take some time, depending on the plugins listed.
+
+[[installing-offline-packs]]
+[float]
+=== Installing Offline Plugin Packs
+
+To install an offline plugin pack:
+
+. Move the compressed bundle to the machine where you want to install the plugins.
 
+. Run the `bin/logstash-plugin install` subcommand and pass in the file URI of
+the offline plugin pack. 
++
+["source","sh",subs="attributes"]
+.Windows example:
+-------------------------------------------------------------------------------
+bin/logstash-plugin install file:///c:/path/to/logstash-offline-plugins-{logstash_version}.zip
+-------------------------------------------------------------------------------
++
+["source","sh",subs="attributes"]
+.Linux example:
+-------------------------------------------------------------------------------
+bin/logstash-plugin install file:///path/to/logstash-offline-plugins-{logstash_version}.zip
+-------------------------------------------------------------------------------
++
+This command expects a file URI, so make sure you use forward slashes and
+specify the full path to the pack.
+
+[float]
+=== Updating Offline Plugins
+
+To update offline plugins, you update the plugins on the staging server and then use the same process that you followed to
+build and install the plugin pack:
+
+. On the staging server, run the `bin/logstash-plugin update` subcommand to update the plugins. See <<updating-plugins>>.
+
+. Create a new version of the plugin pack. See <<building-offline-packs>>.
+
+. Install the new version of the plugin pack. See <<installing-offline-packs>>.
+
+
+[[building-offline-packages-deprecated]]
 [float]
-=== Building the Offline Package
+=== Building the Offline Package (Deprecated Procedure)
+
+deprecated[5.2, Starting with Logstash 5.2, the `pack` and `unpack` commands are deprecated and replaced by the `prepare-offline-pack` and `install` commands]
 
 Working with offline plugins requires you to create an _offline package_, which is a compressed file that contains all of
 the plugins your offline Logstash installation requires, along with the dependencies for those plugins.
@@ -34,7 +114,9 @@ NOTE: Downloading all dependencies for the specified plugins may take some time,
 `bin/logstash-plugin unpack` subcommand to make the packaged plugins available.
 
 [float]
-=== Install or Update a local plugin
+=== Install or Update a local plugin (Deprecated Procedure)
+
+deprecated[5.2]
 
 To install or update a local plugin, use the `--local` option with the install and update commands, as in the following
 examples:
@@ -58,6 +140,8 @@ examples:
 [[managing-packs]]
 === Managing Plugin Packs
 
+deprecated[5.2]
+
 The `pack` and `unpack` subcommands for `bin/logstash-plugin` take the following options:
 
 [horizontal]
diff --git a/docs/static/performance-checklist.asciidoc b/docs/static/performance-checklist.asciidoc
new file mode 100644
index 00000000000..58c741dfcee
--- /dev/null
+++ b/docs/static/performance-checklist.asciidoc
@@ -0,0 +1,103 @@
+[[performance-tuning]]
+== Performance Tuning
+
+This section includes the following information about tuning Logstash
+performance: 
+
+* <<performance-troubleshooting>>
+* <<tuning-logstash>> 
+
+[[performance-troubleshooting]]
+=== Performance Troubleshooting Guide
+
+You can use this troubleshooting guide to quickly diagnose and resolve Logstash performance problems. Advanced knowledge of pipeline internals is not required to understand this guide. However, the <<pipeline,pipeline documentation>> is recommended reading if you want to go beyond this guide.
+
+You may be tempted to jump ahead and change settings like `pipeline.workers` (`-w`) as a first attempt to improve performance. In our experience, changing this setting makes it more difficult to troubleshoot performance problems because you increase the number of variables in play. Instead, make one change at a time and measure the results. Starting at the end of this list is a sure-fire way to create a confusing situation.
+
+[float]
+==== Performance Checklist
+
+. *Check the performance of input sources and output destinations:*
++
+* Logstash is only as fast as the services it connects to. Logstash can only consume and produce data as fast as its input and output destinations can!
+
+. *Check system statistics:*
++
+* CPU
+** Note whether the CPU is being heavily used. On Linux/Unix, you can run `top -H` to see process statistics broken out by thread, as well as total CPU statistics.
+** If CPU usage is high, skip forward to the section about checking the JVM heap and then read the section about tuning Logstash worker settings.
+* Memory
+** Be aware of the fact that Logstash runs on the Java VM. This means that Logstash will always use the maximum amount of memory you allocate to it. 
+** Look for other applications that use large amounts of memory and may be causing Logstash to swap to disk. This can happen if the total memory used by applications exceeds physical memory.
+* I/O Utilization
+** Monitor disk I/O to check for disk saturation. 
+*** Disk saturation can happen if youâ€™re using Logstash plugins (such as the file output) that may saturate your storage. 
+*** Disk saturation can also happen if you're encountering a lot of errors that force Logstash to generate large error logs.
+*** On Linux, you can use iostat, dstat, or something similar to monitor disk I/O.
+** Monitor network I/O for network saturation.
+*** Network saturation can happen if youâ€™re using inputs/outputs that perform a lot of network operations. 
+*** On Linux, you can use a tool like dstat or iftop to monitor your network.
+
+. *Check the JVM heap:*
++
+* Often times CPU utilization can go through the roof if the heap size is too low, resulting in the JVM constantly garbage collecting.
+* A quick way to check for this issue is to double the heap size and see if performance improves. Do not increase the heap size past the amount of physical memory. Leave at least 1GB free for the OS and other processes.
+* You can make more accurate measurements of the JVM heap by using either the `jmap` command line utility distributed with Java or by using VisualVM. For more info, see <<profiling-the-heap>>.
+
+. *Tune Logstash worker settings:*
++
+* Begin by scaling up the number of pipeline workers by using the `-w` flag. This will increase the number of threads available for filters and outputs. It is safe to scale this up to a multiple of CPU cores, if need be, as the threads can become idle on I/O.
+* Each output can only be active in a single pipeline worker thread by default. You can increase this by changing the `workers` setting in the configuration block for each output. Never make this value larger than the number of pipeline workers.
+* You may also tune the output batch size. For many outputs, such as the Elasticsearch output, this setting will correspond to the size of I/O operations. In the case of the Elasticsearch output, this setting corresponds to the batch size.
+
+[[tuning-logstash]]
+=== Tuning and Profiling Logstash Performance
+
+The Logstash defaults are chosen to provide fast, safe performance for most
+users. However if you notice performance issues, you may need to modify
+some of the defaults. Logstash provides the following configurable options
+for tuning pipeline performance: `pipeline.workers`, `pipeline.batch.size`, and `pipeline.batch.delay`. For more information about setting these options, see <<logstash-settings-file>>.
+
+Make sure you've read the <<performance-troubleshooting>> before modifying these options.
+
+* The `pipeline.workers` setting determines how many threads to run for filter and output processing. If you find that events are backing up, or that the CPU is not saturated, consider increasing the value of this parameter to make better use of available processing power. Good results can even be found increasing this number past the number of available processors as these threads may spend significant time in an I/O wait state when writing to external systems. Legal values for this parameter are positive integers.
+
+* The `pipeline.batch.size` setting defines the maximum number of events an individual worker thread collects before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Some hardware configurations require you to increase JVM heap size by setting the `LS_HEAP_SIZE` variable to avoid performance degradation with this option. Values of this parameter in excess of the optimum range cause performance degradation due to frequent garbage collection or JVM crashes related to out-of-memory exceptions. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, issues https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[bulk requests] for each batch received. Tuning the `pipeline.batch.size` setting adjusts the size of bulk requests sent to Elasticsearch.
+
+* The `pipeline.batch.delay` setting rarely needs to be tuned. This setting adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that Logstash waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, Logstash begins to execute filters and outputs.The maximum time that Logstash waits between receiving an event and processing that event in a filter is the product of the `pipeline.batch.delay` and  `pipeline.batch.size` settings.
+
+[float]
+==== Notes on Pipeline Configuration and Performance
+
+If you plan to modify the default pipeline settings, take into account the
+following suggestions:
+
+* The total number of inflight events is determined by the product of the  `pipeline.workers` and `pipeline.batch.size` settings. This product is referred to as the _inflight count_.  Keep the value of the inflight count in mind as you adjust the `pipeline.workers` and `pipeline.batch.size` settings. Pipelines that intermittently receive large events at irregular intervals require sufficient memory to handle these spikes. Configure the `LS_HEAP_SIZE` variable accordingly.
+
+* Measure each change to make sure it increases, rather than decreases, performance.
+
+* Ensure that you leave enough memory available to cope with a sudden increase in event size. For example, an application that generates exceptions that are represented as large blobs of text.
+
+* The number of workers may be set higher than the number of CPU cores since outputs often spend idle time in I/O wait conditions.
+
+* Threads in Java have names and you can use the `jstack`, `top`, and the VisualVM graphical tools to figure out which resources a given thread uses.
+
+* On Linux platforms, Logstash labels all the threads it can with something descriptive. For example, inputs show up as `[base]<inputname`, filter/output workers show up as `[base]>workerN`, where N is an integer.  Where possible, other threads are also labeled to help you identify their purpose.
+
+[float]
+[[profiling-the-heap]]
+==== Profiling the Heap
+
+When tuning Logstash you may have to adjust the heap size. You can use the https://visualvm.java.net/[VisualVM] tool to profile the heap. The *Monitor* pane in particular is useful for checking whether your heap allocation is sufficient for the current workload. The screenshots below show sample *Monitor* panes. The first pane examines a Logstash instance configured with too many inflight events. The second pane examines a Logstash instance configured with an appropriate amount of inflight events. Note that the specific batch sizes used here are most likely not applicable to your specific workload, as the memory demands of Logstash vary in large part based on the type of messages you are sending.
+
+image::static/images/pipeline_overload.png[]
+
+image::static/images/pipeline_correct_load.png[]
+
+In the first example we see that the CPU isnâ€™t being used very efficiently. In fact, the JVM is often times having to stop the VM for â€œfull GCsâ€. Full garbage collections are a common symptom of excessive memory pressure. This is visible in the spiky pattern on the CPU chart. In the more efficiently configured example, the GC graph pattern is more smooth, and the CPU is used in a more uniform manner. You can also see that there is ample headroom between the allocated heap size, and the maximum allowed, giving the JVM GC a lot of room to work with.
+
+Examining the in-depth GC statistics with a tool similar to the excellent https://visualvm.java.net/plugins.html[VisualGC] plugin shows that the over-allocated VM spends very little time in the efficient Eden GC, compared to the time spent in the more resource-intensive Old Gen â€œFullâ€ GCs.
+
+NOTE: As long as the GC pattern is acceptable, heap sizes that occasionally increase to the maximum are acceptable. Such heap size spikes happen in response to a burst of large events passing through the pipeline. In general practice, maintain a gap between the used amount of heap memory and the maximum.
+This document is not a comprehensive guide to JVM GC tuning. Read the official http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html[Oracle guide] for more information on the topic. We also recommend reading http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance].
+
diff --git a/docs/static/persistent-queues.asciidoc b/docs/static/persistent-queues.asciidoc
new file mode 100644
index 00000000000..36ae6f2a425
--- /dev/null
+++ b/docs/static/persistent-queues.asciidoc
@@ -0,0 +1,167 @@
+[[persistent-queues]]
+=== Persistent Queues
+
+By default, Logstash uses in-memory bounded queues between pipeline stages
+(inputs â†’ pipeline workers) to buffer events. The size of these in-memory
+queues is fixed and not configurable. If Logstash experiences a temporary
+machine failure, the contents of the in-memory queue will be lost. Temporary machine
+failures are scenarios where Logstash or its host machine are terminated
+abnormally but are capable of being restarted. 
+
+In order to protect against data loss during abnormal termination, Logstash has
+a persistent queue feature which will store the message queue on disk.
+Persistent queues provide durability of data within Logstash.
+
+Persistent queues are also useful for Logstash deployments that need large buffers.
+Instead of deploying and managing a message broker, such as Redis, RabbitMQ, or
+Apache Kafka, to facilitate a buffered publish-subscriber model, you can enable
+persistent queues to buffer events on disk and remove the message broker.
+
+In summary, the benefits of enabling persistent queues are as follows:
+
+* Absorbs bursts of events without needing an external buffering mechanism like
+Redis or Apache Kafka.
+* Provides an at-least-once delivery guarantee against message loss during
+a normal shutdown as well as when Logstash is terminated abnormally. If Logstash
+is restarted while events are in-flight, Logstash will attempt to deliver
+messages stored in the persistent queue until delivery succeeds at least once.
++
+NOTE: You must set `queue.checkpoint.writes: 1` explicitly to guarantee
+maximum durability for all input events. See <<durability-persistent-queues>>.
+
+[[persistent-queues-limitations]]
+==== Limitations of Persistent Queues
+
+The following are problems not solved by the persistent queue feature:
+
+* Input plugins that do not use a request-response protocol cannot be protected from data loss. For example: tcp, udp, zeromq push+pull, and many other inputs do not have a mechanism to acknowledge receipt to the sender. Plugins such as beats and http, which *do* have an acknowledgement capability, are well protected by this queue.
+* It does not handle permanent machine failures such as disk corruption, disk failure, and machine loss. The data persisted to disk is not replicated.
+
+[[persistent-queues-architecture]]
+==== How Persistent Queues Work
+
+The queue sits between the input and filter stages in the same
+process:
+
+input â†’ queue â†’ filter + output 
+
+When an input has events ready to process, it writes them to the queue. When
+the write to the queue is successful, the input can send an acknowledgement to
+its data source.
+
+When processing events from the queue, Logstash acknowledges events as
+completed, within the queue, only after filters and outputs have completed.
+The queue keeps a record of events that have been processed by the pipeline.
+An event is recorded as processed (in this document, called "acknowledged" or
+"ACKed") if, and only if, the event has been processed completely by the
+Logstash pipeline. 
+
+What does acknowledged mean? This means the event has been handled by all
+configured filters and outputs. For example, if you have only one output,
+Elasticsearch, an event is ACKed when the Elasticsearch output has successfully
+sent this event to Elasticsearch. 
+
+During a normal shutdown (*CTRL+C* or SIGTERM), Logstash will stop reading
+from the queue and will finish processing the in-flight events being processed
+by the filters and outputs. Upon restart, Logstash will resume processing the
+events in the persistent queue as well as accepting new events from inputs.
+
+If Logstash is abnormally terminated, any in-flight events will not have been
+ACKed and will be reprocessed by filters and outputs when Logstash is
+restarted. Logstash processes events in batches, so it is possible
+that for any given batch, some of that batch may have been successfully
+completed, but not recorded as ACKed, when an abnormal termination occurs.
+
+For more details specific behaviors of queue writes and acknowledgement, see 
+<<durability-persistent-queues>>.
+
+[[configuring-persistent-queues]]
+==== Configuring Persistent Queues
+
+To configure persistent queues, you can specify the following options in the
+Logstash <<logstash-settings-file,settings file>>:
+
+* `queue.type`: Specify `persisted` to enable persistent queues. By default, persistent queues are disabled (default: `queue.type: memory`).
+* `path.queue`: The directory path where the data files will be stored. By default, the files are stored in `path.data/queue`. 
+* `queue.page_capacity`: The maximum size of a queue page in bytes. The queue data consists of append-only files called "pages". The default size is 250mb. Changing this value is unlikely to have performance benefits.
+// Technically, I know, this isn't "maximum number of events" it's really maximum number of events not yet read by the pipeline worker. We only use this for testing and users generally shouldn't be setting this.
+* `queue.max_events`:  The maximum number of events that are allowed in the queue. The default is 0 (unlimited). This value is used internally for the Logstash test suite.
+* `queue.max_bytes`: The total capacity of the queue in number of bytes. The
+default is 1024mb (1gb). Make sure the capacity of your disk drive is greater
+than the value you specify here.
+
+If both `queue.max_events` and 
+`queue.max_bytes` are specified, Logstash uses whichever criteria is reached
+first. See <<backpressure-persistent-queue>> for behavior when these queue limits are reached.
+
+You can also specify options that control when the checkpoint file gets updated (`queue.checkpoint.acks`, `queue.checkpoint.writes`). See <<durability-persistent-queues>>.
+
+Example configuration:
+
+[source, yaml]
+queue.type: persisted
+queue.max_bytes: 4gb 
+
+[[backpressure-persistent-queue]]
+==== Handling Back Pressure
+
+When the queue is full, Logstash puts back pressure on the inputs to stall data
+flowing into Logstash. This mechanism helps Logstash control the rate of data
+flow at the input stage without overwhelming outputs like Elasticsearch.
+
+Use `queue.max_bytes` setting to configure the total capacity of the queue on
+disk. The following example sets the total capacity of the queue to 8gb:
+
+[source, yaml]
+queue.type: persisted
+queue.max_bytes: 8gb
+
+With these settings specified, Logstash will buffer events on disk until the
+size of the queue reaches 8gb. When the queue is full of unACKed events, and
+the size limit has been reached, Logstash will no longer accept new events. 
+
+Each input handles back pressure independently. For example, when the
+<<plugins-inputs-beats,beats>> input encounters back pressure, it no longer
+accepts new connections and waits until the persistent queue has space to accept
+more events. After the filter and output stages finish processing existing
+events in the queue and ACKs them, Logstash automatically starts accepting new
+events.
+
+[[durability-persistent-queues]]
+==== Controlling Durability
+
+Durability is a property of storage writes that ensures data will be available after it's written.
+
+When the persistent queue feature is enabled, Logstash will store events on
+disk. Logstash commits to disk in a mechanism called checkpointing.
+
+To discuss durability, we need to introduce a few details about how the persistent queue is implemented.
+
+First, the queue itself is a set of pages. There are two kinds of pages: head pages and tail pages. The head page is where new events are written. There is only one head page. When the head page is of a certain size (see `queue.page_capacity`), it becomes a tail page, and a new head page is created. Tail pages are immutable, and the head page is append-only. 
+Second, the queue records details about itself (pages, acknowledgements, etc) in a separate file called a checkpoint file.
+
+When recording a checkpoint, Logstash will:
+
+* Call fsync on the head page.
+* Atomically write to disk the current state of the queue.
+
+The following settings are available to let you tune durability:
+
+* `queue.checkpoint.writes`: Logstash will checkpoint after this many writes into the queue. Currently, one event counts as one write, but this may change in future releases.
+* `queue.checkpoint.acks`: Logstash will checkpoint after this many events are acknowledged. This configuration controls the durability at the processing (filter + output)
+part of Logstash.
+
+Disk writes have a resource cost. Tuning the above values higher or lower will trade durability for performance. For instance, if you want the strongest durability for all input events, you can set `queue.checkpoint.writes: 1`.
+
+The process of checkpointing is atomic, which means any update to the file is saved if successful.
+
+If Logstash is terminated, or if there is a hardware level failure, any data
+that is buffered in the persistent queue, but not yet checkpointed, is lost.
+To avoid this possibility, you can set `queue.checkpoint.writes: 1`, but keep in
+mind that this setting can severely impact performance.
+
+[[garbage-collection]]
+==== Disk Garbage Collection
+
+On disk, the queue is stored as a set of pages where each page is one file. Each page can be at most `queue.page_capacity` in size. Pages are deleted (garbage collected) after all events in that page have been ACKed. If an older page has at least one event that is not yet ACKed, that entire page will remain on disk until all events in that page are successfully processed. Each page containing unprocessed events will count against the `queue.max_bytes` byte size.
+
diff --git a/docs/static/plugin-generator.asciidoc b/docs/static/plugin-generator.asciidoc
new file mode 100644
index 00000000000..cd18d1d6713
--- /dev/null
+++ b/docs/static/plugin-generator.asciidoc
@@ -0,0 +1,19 @@
+[[plugin-generator]]
+=== Generating Plugins
+
+You can now create your own Logstash plugin in seconds! The generate subcommand of `bin/logstash-plugin` creates the foundation 
+for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you 
+can start adding custom code to process data with Logstash.
+
+**Example Usage**
+
+[source,sh]
+--------------------------------------------
+bin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins
+-------------------------------------------
+
+* `--type`: Type of plugin - input, filter, output, or codec
+* `--name`: Name for the new plugin
+* `--path`: Directory path where the new plugin structure will be created. If not specified, it will be
+created in the current directory.
+
diff --git a/docs/static/plugin-manager.asciidoc b/docs/static/plugin-manager.asciidoc
index 75d297f2a52..e42be8db5e0 100644
--- a/docs/static/plugin-manager.asciidoc
+++ b/docs/static/plugin-manager.asciidoc
@@ -2,9 +2,22 @@
 == Working with plugins
 
 Logstash has a rich collection of input, filter, codec and output plugins. Plugins are available as self-contained
-packages called gems and hosted on RubyGems.org. The plugin manager accesed via `bin/logstash-plugin` script is used to manage the
-lifecycle of plugins in your Logstash deployment. You can install, uninstall and upgrade plugins using these Command Line
-Interface (CLI) described below.
+packages called gems and hosted on RubyGems.org. The plugin manager accessed via `bin/logstash-plugin` script is used to manage the
+lifecycle of plugins in your Logstash deployment. You can install, remove and upgrade plugins using the Command Line
+Interface (CLI) invocations described below.
+
+[float]
+[[http-proxy]]
+=== Proxy configuration
+
+The majority of the plugin manager commands require access to the internet to reach https://rubygems.org[RubyGems.org].
+If your organization is behind a firewall you can set these environments variables to configure Logstash to use your proxy.
+
+[source, shell]
+----------------------------------
+export http_proxy=http://localhost:3128
+export https_proxy=http://localhost:3128
+----------------------------------
 
 [float]
 [[listing-plugins]]
@@ -17,7 +30,7 @@ available in your deployment:
 ----------------------------------
 bin/logstash-plugin list <1>
 bin/logstash-plugin list --verbose <2>
-bin/logstash-plugin list *namefragment* <3>
+bin/logstash-plugin list '*namefragment*' <3>
 bin/logstash-plugin list --group output <4>
 ----------------------------------
 <1> Will list all installed plugins
@@ -57,14 +70,17 @@ bin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem
 
 [[installing-local-plugins-path]]
 [float]
-==== Advanced: Using `--pluginpath`
+==== Advanced: Using `--path.plugins`
 
-Using the `--pluginpath` flag, you can load a plugin source code located on your file system. Typically this is used by
+Using the Logstash `--path.plugins` flag, you can load a plugin source code located on your file system. Typically this is used by
 developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
 
+The path needs to be in a  specific directory hierarchy: `PATH/logstash/TYPE/NAME.rb`, where TYPE is 'inputs' 'filters', 'outputs' or 'codecs' and NAME is the name of the plugin.
+
 [source,shell]
 ----------------------------------
-bin/logstash --pluginpath /opt/shared/lib/logstash/input/my-custom-plugin-code.rb
+# supposing the code is in /opt/shared/lib/logstash/inputs/my-custom-plugin-code.rb
+bin/logstash --path.plugins /opt/shared/lib
 ----------------------------------
 
 [[updating-plugins]]
@@ -91,7 +107,7 @@ If you need to remove plugins from your Logstash installation:
 
 [source,shell]
 ----------------------------------
-bin/logstash-plugin uninstall logstash-output-kafka
+bin/logstash-plugin remove logstash-output-kafka
 ----------------------------------
 
 [[proxy-plugins]]
@@ -111,6 +127,10 @@ bin/logstash-plugin install logstash-output-kafka
 
 Once set, plugin commands install, update can be used through this proxy.
 
+include::plugin-generator.asciidoc[]
+
 include::offline-plugins.asciidoc[]
 
-include::private-gem-repo.asciidoc[]
\ No newline at end of file
+include::private-gem-repo.asciidoc[]
+
+include::event-api.asciidoc[]
diff --git a/docs/static/releasenotes.asciidoc b/docs/static/releasenotes.asciidoc
index 899b9cc9a75..6862ca1db24 100644
--- a/docs/static/releasenotes.asciidoc
+++ b/docs/static/releasenotes.asciidoc
@@ -1,61 +1,137 @@
 [[releasenotes]]
-== Logstash 2.1 Release Notes
+== Release Notes
 
-[float]
-== General
-
-* {lsissue}2376[Issue 2376]: Added ability to install and upgrade Logstash plugins without requiring internet
-connectivity.
-* {lsissue}3576[Issue 3576]: Support alternate or private Ruby gems server to install and update plugins.
-* {lsissue}3451[Issue 3451]: Added ability to reliably shutdown Logstash when there is a stall in event processing. This
-option can be enabled by passing `--allow-unsafe-shutdown` flag while starting Logstash. Please be aware that any in-
-flight events will be lost when shutdown happens.
-* {lsissue}4222[Issue 4222]: Fixed a memory leak which could be triggered when events having a date were serialized to
-string.
-* Added JDBC input to default package.
-* {lsissue}3243[Issue 3243]: Adding `--debug` to `--configtest` now shows the configuration in blocks annotated by source
-config file. Very useful when using multiple config files in a directory.
-* {lsissue}4130[Issue 4130]: Reset default worker threads to 1 when using non thread-safe filters like multiline.
-* Fixed file permissions for the `logrotate` configuration file.
-* {lsissue}3861[Issue 3861]: Changed the default heap size from 500MB to 1GB.
-* {lsissue}3645[Issue 3645]: Fixed config check option when starting Logstash through init scripts.
+This section summarizes the changes in the following releases:
 
-[float]
-== Input Plugins
+* <<logstash-5-4-2,Logstash 5.4.2>>
+* <<logstash-5-4-1,Logstash 5.4.1>>
+* <<logstash-5-4-0,Logstash 5.4.0>>
 
-[float]
-=== Twitter
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/21[Issue 21]: Added an option to fetch data from the
-sample Twitter streaming endpoint.
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/22[Issue 22]: Added hashtags, symbols and
-user_mentions as data for the non extended tweet event.
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/20[Issue 20]: Added an option to filter per location
-and language.
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/11[Issue 11]: Added an option to stream data from a
-list of users.
+[[logstash-5-4-2]]
+=== Logstash 5.4.2 Release Notes
 
-[float]
-=== Beats
-* https://github.com/logstash-plugins/logstash-input-beats/issues/10[Issue 10]: Properly handle multiline events from
-multiple sources, originating from Filebeat.
+* We now handle connection refused errors when installing a plugin ({lsissue}6529[Issue 6529]).
+* Fixed an issue where environment variables specified in the Elasticsearch Output host configuration 
+  weren't resolved correctly ({lsissue}6696[Issue 6696]).
+* Fixed a concurrency issue in the persistent queues feature ({lsissue}7382[Issue 7382]).
 
 [float]
-=== File
-* https://github.com/logstash-plugins/logstash-input-file/issues/44[Issue 44]: Properly handle multiline events from
-multiple sources.
+==== Input Plugins
 
+*`HTTP Poller`*:
+
+* Added a top-level user/password config option that apply to all URLs by default.
+* Added support for eager authorization. The Logstash client will now send credentials in its first request 
+  rather than waiting for a 401 challenge.
+  
 [float]
-=== Eventlog
-* https://github.com/logstash-plugins/logstash-input-eventlog/issues/11[Issue 11]: Change the underlying library to
-capture Event Logs from Windows more reliably.
+==== Output Plugins
+
+*`Elasticsearch`*:
+
+* Fixed an error where a 429 response code would cause this output to crash.
+* This output now waits for all inflight requests to complete before stopping.
 
 [float]
-== Output
+==== Filter Plugins
+
+*`GeoIP`*: 
+
+* Added support for commercial databases from Maxmind.
+* Added ASN data support via the GeoIP2-ISP database.
+
+*`User Agent`*:
+
+* Improved performance by 2.5x by moving core parser logic from Ruby to Java.
+
+*`Grok`*:
+
+* Fixed an issue where having subdirectories in a pattern folder caused Logstash to crash.
+
+
+[[logstash-5-4-1]]
+=== Logstash 5.4.1 Release Notes
+
+* Fixed an issue on Windows where Logstash was unable to locate the default log4j2.properties file ({lsissue}6352[Issue 6352]).
 
 [float]
-=== Elasticsearch
-* Improved the default template to use doc_values wherever possible.
-* Improved the default template to disable fielddata on analyzed string fields.
-* https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/260[Issue 260]: Added New setting: timeout.
-This lets you control the behavior of a slow/stuck request to Elasticsearch that could be, for example, caused by network,
-firewall, or load balancer issues.
+==== Input Plugins
+
+*`Beats`*:
+
+* Fixed an issue where Logstash would incorrectly close the connection from beats due to a timeout (https://github.com/logstash-plugins/logstash-input-beats/issues/185[Issue 185]).
+
+*`JDBC`*:
+
+* This plugin now automatically reconnects on connection issues (https://github.com/logstash-plugins/logstash-input-jdbc/issues/45[Issue 45]).
+
+*`HTTP Poller`*:
+
+* Added option to specify top-level user/password options that apply to all URLs by default.
+* Added eager auth functionality. This means the client will send any credentials in its first request rather than waiting for a 401 challenge.
+
+*`Log4j`*:
+
+* This input now rejects any non-log4j log objects sent as input.
+
+*`RabbitMQ`*:
+
+* Updated the underlying RabbitMQ Java lib to v3.0.0.
+
+*`S3`*:
+
+* Fixed an issue where Logstash would crash when attempting to ingest a JSON file with a message attribute that was not a string (https://github.com/logstash-plugins/logstash-input-s3/issues/109[Issue 109]).
+
+*`Elasticsearch`*:
+
+* Fixed scrolling to use JSON bodies in the requests.
+
+==== Filter Plugins
+
+*`Date`*:
+
+* This plugin now ignores canceled events. Previously, these events were unnecessarily processed.
+
+*`Fingerprint`*:
+
+* Improved error messages that could happen during startup.
+
+*`Grok`*:
+
+* Fixed an issue where a subdirectory under the `patterns` directory could cause Logstash to crash at startup (https://github.com/logstash-plugins/logstash-filter-grok/issues/110[Issue 110]).
+* Added ability to define custom patterns within a `grok` filter by using the `pattern_definitions` config option.
+
+*`URL Decode`*:
+
+* Fixed an issue where Logstash would crash when processing Unicode input with this filter.
+
+==== Output Plugins
+
+*`S3`*:
+
+* Fixed the `restore_from_crash` option to use the same upload options as the normal uploader (https://github.com/logstash-plugins/logstash-output-s3/issues/140[Issue 140]).
+* Updated the `canned_acl` option to allow `public-read`, `public-read-write` and `authenticated-read` as possible values.
+
+*`RabbitMQ`*:
+
+* Updated the underlying RabbitMQ Java lib to v3.0.0.
+
+*`Elasticsearch`*:
+
+* Added support for customizing the sniffing path with the `sniffing_path` and `absolute_sniffing_path` config options.
+
+*`Kafka`*:
+
+* Fixed a bug when Logstash would fail to start up when `SASL_SSL` and `PLAIN` (no Kerberos) options were specified.
+
+[[logstash-5-4-0]]
+=== Logstash 5.4.0 Release Notes
+
+* The persistent queues feature is generally available (GA) now. The beta tag has been removed.
+* The `dissect` filter is now bundled in the Logstash artifact.
+* Updated the `line` and `multiline` codecs to be threadsafe when used with inputs.
+* Logstash's plugin manager now works when an HTTP proxy is used ({lsissue}6619[Issue 6619], {lsissue}6528[Issue 6528]).
+* On Windows deployments, we now search for the java executable in `%PATH%` which works well for 
+  the latest JDK 8 updates.
+* Fixed an issue where the JVM max heap size stats were reported incorrectly in the stats API ({lsissue}6608[Issue 6608]).
+* Fixed an issue where Logstash would crash when using conditionals on a nested JSON field ({lsissue}6522[Issue 6522]).
diff --git a/docs/static/reloading-config.asciidoc b/docs/static/reloading-config.asciidoc
index 22f3118aac6..7f05e040396 100644
--- a/docs/static/reloading-config.asciidoc
+++ b/docs/static/reloading-config.asciidoc
@@ -4,19 +4,19 @@
 Starting with Logstash 2.3, you can set Logstash to detect and reload configuration
 changes automatically.
 
-To enable automatic config reloading, start Logstash with the `--auto-reload` (or `-r`)
+To enable automatic config reloading, start Logstash with the `--config.reload.automatic` (or `-r`)
 command-line option specified. For example:
 
 [source,shell]
 ----------------------------------
-bin/logstash â€“f apache.config --auto-reload
+bin/logstash â€“f apache.config --config.reload.automatic
 ----------------------------------
 
-NOTE: The `--auto-reload` option is not available when you specify the `-e` flag to pass
+NOTE: The `--config.reload.automatic` option is not available when you specify the `-e` flag to pass
 in  configuration settings from the command-line.
 
 By default, Logstash checks for configuration changes every 3 seconds. To change this interval,
-use the `--reload-interval <seconds>` option,  where `seconds` specifies how often Logstash
+use the `--config.reload.interval <seconds>` option,  where `seconds` specifies how often Logstash
 checks the config files for changes. 
 
 If Logstash is already running without auto-reload enabled, you can force Logstash to
@@ -41,3 +41,6 @@ fail, the old pipeline continues to function, and the errors are propagated to t
 
 During automatic config reloading, the JVM is not restarted. The creating and swapping of
 pipelines all happens within the same process. 
+
+Changes to <<plugins-filters-grok,grok>> pattern files are also reloaded, but only when
+a change in the config file triggers a reload (or the pipeline is restarted).  
diff --git a/docs/static/repositories.asciidoc b/docs/static/repositories.asciidoc
deleted file mode 100644
index 8a3abf61d97..00000000000
--- a/docs/static/repositories.asciidoc
+++ /dev/null
@@ -1,86 +0,0 @@
-[[package-repositories]]
-== Package Repositories
-
-We also have repositories available for APT and YUM based distributions. Note
-that we only provide binary packages, but no source packages, as the packages
-are created as part of the Logstash build.
-
-We have split the Logstash package repositories by version into separate urls
-to avoid accidental upgrades across major or minor versions. For all 1.5.x
-releases use 1.5 as version number, for 1.4.x use 1.4, etc.
-
-We use the PGP key
-http://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
-Elastic's Signing Key, with fingerprint
-
-    4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
-
-to sign all our packages. It is available from http://pgp.mit.edu.
-
-[float]
-=== APT
-
-Download and install the Public Signing Key:
-
-[source,sh]
---------------------------------------------------
-wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
---------------------------------------------------
-
-Add the repository definition to your `/etc/apt/sources.list` file:
-
-["source","sh",subs="attributes,callouts"]
---------------------------------------------------
-echo "deb http://packages.elastic.co/logstash/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list
---------------------------------------------------
-
-[WARNING]
-==================================================
-Use the `echo` method described above to add the Logstash repository.  Do not
-use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not
-provide a source package. If you have added the `deb-src` entry, you will see an
-error like the following:
-
-    Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)
-
-Just delete the `deb-src` entry from the `/etc/apt/sources.list` file and the
-installation should work as expected.
-==================================================
-
-Run `sudo apt-get update` and the repository is ready for use. You can install
-it with:
-
-[source,sh]
---------------------------------------------------
-sudo apt-get update && sudo apt-get install logstash
---------------------------------------------------
-
-[float]
-=== YUM
-
-Download and install the public signing key:
-
-[source,sh]
---------------------------------------------------
-rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
---------------------------------------------------
-
-Add the following in your `/etc/yum.repos.d/` directory
-in a file with a `.repo` suffix, for example `logstash.repo`
-
-["source","sh",subs="attributes,callouts"]
---------------------------------------------------
-[logstash-{branch}]
-name=Logstash repository for {branch}.x packages
-baseurl=http://packages.elastic.co/logstash/{branch}/centos
-gpgcheck=1
-gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
-enabled=1
---------------------------------------------------
-
-And your repository is ready for use. You can install it with:
-
-[source,sh]
---------------------------------------------------
-yum install logstash
---------------------------------------------------
diff --git a/docs/static/running-logstash-command-line.asciidoc b/docs/static/running-logstash-command-line.asciidoc
new file mode 100644
index 00000000000..b1e6e08f5b5
--- /dev/null
+++ b/docs/static/running-logstash-command-line.asciidoc
@@ -0,0 +1,159 @@
+[[running-logstash-command-line]]
+=== Running Logstash from the Command Line
+
+To run Logstash from the command line, use the following command:
+
+[source,shell]
+----
+bin/logstash [options]
+----
+
+Where `options` are <<command-line-flags,command-line>> flags that you can
+specify to control Logstash execution. The location of the `bin` directory
+varies by platform. See <<dir-layout>> to find the location of `bin\logstash` on
+your system.
+
+The following example runs Logstash and loads the Logstash config defined in
+the `mypipeline.conf` file:
+
+[source,shell]
+----
+bin/logstash -f mypipeline.conf
+----
+
+Any flags that you set at the command line override the corresponding settings
+in the Logstash <<logstash-settings-file,settings file>>, but the settings file
+itself is not changed. It remains as-is for subsequent Logstash runs.
+
+Specifying command line options is useful when you are testing Logstash.
+However, in a production environment, we recommend that you use the Logstash
+<<logstash-settings-file,settings file>> to control Logstash execution. Using
+the settings file makes it easier for you to specify multiple options, and it
+provides you with a single, versionable file that you can use to start up
+Logstash consistently for each run.
+
+[[command-line-flags]]
+==== Command-Line Flags
+
+Logstash has the following flags. You can use the `--help` flag to display this information.
+
+*`--node.name NAME`*::
+  Specify the name of this Logstash instance. If no value is given it will default to the current
+  hostname.
+
+*`-f, --path.config CONFIG_PATH`*::
+Load the Logstash config from a specific file or directory. If a directory is given, all
+files in that directory will be concatenated in lexicographical order and then parsed as a
+single config file. Specifying this flag multiple times is not supported. If you specify
+this flag multiple times, Logstash uses the last occurrence (for example, `-f foo -f bar`
+is the same as `-f bar`).
++
+You can specify wildcards (<<glob-support,globs>>) and any matched files will
+be loaded in the order described above. For example, you can use the wildcard feature to
+load specific files by name: 
++
+[source,shell]
+---------------------------------------------
+bin/logstash --debug -f '/tmp/{one,two,three}'
+---------------------------------------------
++
+With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/two`, and
+`/tmp/three`, and parses them into a single config.
+
+*`-e, --config.string CONFIG_STRING`*::
+  Use the given string as the configuration data. Same syntax as the config file. If no
+  input is specified, then the following is used as the default input:
+  `input { stdin { type => stdin } }` and if no output is specified, then the
+  following is used as the default output: `output { stdout { codec => rubydebug } }`.
+  If you wish to use both defaults, please use the empty string for the `-e` flag.
+  The default is nil.
+
+*`-w, --pipeline.workers COUNT`*::
+  Sets the number of pipeline workers to run. This option sets the number of workers that will,
+  in parallel, execute the filter and output stages of the pipeline. If you find that events are
+  backing up, or that  the CPU is not saturated, consider increasing this number to better utilize
+  machine processing power. The default is the number of the host's CPU cores.
+
+*`-b, --pipeline.batch.size SIZE`*::
+  Size of batches the pipeline is to work in. This option defines the maximum number of events an
+  individual worker thread will collect from inputs before attempting to execute its filters and outputs.
+  The default is 125 events. Larger batch sizes are generally more efficient, but come at the cost of
+  increased memory overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
+  variable to effectively use the option.
+
+*`-u, --pipeline.batch.delay DELAY_IN_MS`*::
+  When creating pipeline batches, how long to wait while polling for the next event. This option defines
+  how long in milliseconds to wait while polling for the next event before dispatching an undersized batch
+  to filters and workers. The default is 250ms.
+
+*`--pipeline.unsafe_shutdown`*::
+  Force Logstash to exit during shutdown even if there are still inflight events
+  in memory. By default, Logstash will refuse to quit until all received events
+  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
+
+*`--path.data PATH`*::
+  This should point to a writable directory. Logstash will use this directory whenever it needs to store
+  data. Plugins will also have access to this path. The default is the `data` directory under
+  Logstash home.
+
+*`-p, --path.plugins PATH`*::
+  A path of where to find custom plugins. This flag can be given multiple times to include
+  multiple paths. Plugins are expected to be in a specific directory hierarchy:
+  `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`,
+  and `NAME` is the name of the plugin.
+
+*`-l, --path.logs PATH`*::
+  Directory to write Logstash internal logs to.
+
+*`--log.level LEVEL`*::
+ Set the log level for Logstash. Possible values are:
+* `fatal`: log very severe error messages that will usually be followed by the application aborting
+* `error`: log errors
+* `warn`: log warnings
+* `info`: log verbose info (this is the default)
+* `debug`: log debugging info (for developers)
+* `trace`: log finer-grained messages beyond debugging info
+
+*`--config.debug`*::
+  Show the fully compiled configuration as a debug log message (you must also have `--log.level=debug` enabled).
+  WARNING: The log message will include any 'password' options passed to plugin configs as plaintext, and may result
+  in plaintext passwords appearing in your logs!
+
+*`-i, --interactive SHELL`*::
+  Drop to shell instead of running as normal. Valid shells are "irb" and "pry".
+
+*`-V, --version`*::
+  Emit the version of Logstash and its friends, then exit.
+
+*`-t, --config.test_and_exit`*::
+  Check configuration for valid syntax and then exit. Note that grok patterns are not checked for
+  correctness with this flag. Logstash can read multiple config files from a directory. If you combine this
+  flag with `--log.level=debug`, Logstash will log the combined config file, annotating
+  each config block with the source file it came from.
+
+*`-r, --config.reload.automatic`*::
+  Monitor configuration changes and reload whenever the configuration is changed.
+  NOTE: Use SIGHUP to manually reload the config. The default is false.
+
+*`--config.reload.interval RELOAD_INTERVAL`*::
+  How frequently to poll the configuration location for changes, in seconds. The default is every 3 seconds.
+
+*`--http.host HTTP_HOST`*::
+  Web API binding host. This option specifies the bind address for the metrics REST endpoint. The default is "127.0.0.1".
+
+*`--http.port HTTP_PORT`*::
+  Web API http port. This option specifies the bind port for the metrics REST endpoint. The default is 9600-9700.
+  This setting accepts a range of the format 9600-9700. Logstash will pick up the first available port.
+
+*`--log.format FORMAT`*::
+   Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text
+   (using Ruby's Object#inspect). The default is "plain".
+
+*`--path.settings SETTINGS_DIR`*::
+  Set the directory containing the `logstash.yml` <<logstash-settings-file,settings file>> as well
+  as the log4j logging configuration. This can also be set through the LS_SETTINGS_DIR environment variable.
+  The default is the `config` directory under Logstash home.
+
+*`-h, --help`*::
+  Print help
+
diff --git a/docs/static/running-logstash.asciidoc b/docs/static/running-logstash.asciidoc
new file mode 100644
index 00000000000..a7a1e70d81c
--- /dev/null
+++ b/docs/static/running-logstash.asciidoc
@@ -0,0 +1,53 @@
+[[running-logstash]]
+=== Running Logstash as a Service on Debian or RPM
+
+Logstash is not started automatically after installation. How to start and stop Logstash depends on whether your system
+uses systemd, upstart, or SysV.
+
+Here are some common operating systems and versions, and the corresponding
+startup styles they use.  This list is intended to be informative, not exhaustive.
+
+|=======================================================================
+| Distribution | Service System |
+| Ubuntu 16.04 and newer | <<running-logstash-systemd,systemd>> |
+| Ubuntu 12.04 through 15.10 | <<running-logstash-upstart,upstart>> |
+| Debian 8 "jessie" and newer | <<running-logstash-systemd,systemd>> |
+| Debian 7 "wheezy" and older | <<running-logstash-sysv,sysv>> |
+| CentOS (and RHEL) 7 and newer | <<running-logstash-systemd,systemd>> |
+| CentOS (and RHEL) 6 | <<running-logstash-upstart,upstart>> |
+|=======================================================================
+
+[[running-logstash-systemd]]
+==== Running Logstash by Using Systemd
+
+Distributions like Debian Jessie, Ubuntu 15.10+, and many of the SUSE derivatives use systemd and the
+`systemctl` command to start and stop services. Logstash places the systemd unit files in `/etc/systemd/system` for both deb and rpm. After installing the package, you can start up Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo systemctl start logstash.service
+-------------------------------------------
+
+[[running-logstash-upstart]]
+==== Running Logstash by Using Upstart
+
+For systems that use upstart, you can start Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo initctl start logstash
+-------------------------------------------
+
+The auto-generated configuration file for upstart systems is `/etc/init/logstash.conf`.
+
+[[running-logstash-sysv]]
+==== Running Logstash by Using SysV
+
+For systems that use SysV, you can start Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo /etc/init.d/logstash start
+-------------------------------------------
+
+The auto-generated configuration file for SysV systems is `/etc/init.d/logstash`.
diff --git a/docs/static/setting-up-logstash.asciidoc b/docs/static/setting-up-logstash.asciidoc
new file mode 100644
index 00000000000..89b3be76a6f
--- /dev/null
+++ b/docs/static/setting-up-logstash.asciidoc
@@ -0,0 +1,179 @@
+[[setup-logstash]]
+== Setting Up and Running Logstash
+
+Before reading this section, see <<installing-logstash>> for basic installation instructions to get you started.
+
+This section includes additional information on how to set up and run Logstash, including:
+
+* <<dir-layout>>
+* <<config-setting-files>>
+* <<logstash-settings-file>>
+* <<running-logstash-command-line>>
+* <<running-logstash>>
+* <<docker>>
+* <<logging>>
+* <<persistent-queues>>
+* <<shutdown>>
+
+
+[[dir-layout]]
+=== Logstash Directory Layout
+
+This section describes the default directory structure that is created when you unpack the Logstash installation packages.
+
+[[zip-targz-layout]]
+==== Directory Layout of `.zip` and `.tar.gz` Archives
+
+The `.zip` and `.tar.gz` packages are entirely self-contained. All files and
+directories are, by default, contained within the home directory -- the directory
+created when unpacking the archive.
+
+This is very convenient because you don't have to create any directories to start using Logstash, and uninstalling
+Logstash is as easy as removing the home directory.  However, it is advisable to change the default locations of the
+config and the logs directories so that you do not delete important data later on.
+
+[cols="<h,<,<m,<m",options="header",]
+|=======================================================================
+| Type | Description | Default Location | Setting
+| home
+  | Home directory of the Logstash installation.
+  | `{extract.path}`- Directory created by unpacking the archive
+ d|
+
+| bin
+  | Binary scripts, including `logstash` to start Logstash
+    and `logstash-plugin` to install plugins
+  | `{extract.path}/bin`
+ d|
+
+| settings
+  | Configuration files, including `logstash.yml` and `jvm.options`
+  | `{extract.path}/config`
+  | `path.settings`
+  
+| logs
+  | Log files
+  | `{extract.path}/logs`
+  | `path.logs`
+
+| plugins
+  | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.
+  | `{extract.path}/plugins`
+  | `path.plugins`
+
+|=======================================================================
+
+[[deb-layout]]
+==== Directory Layout of Debian and RPM Packages
+
+The Debian package and the RPM package each place config files, logs, and the settings files in the appropriate
+locations for the system:
+
+[cols="<h,<,<m,<m",options="header",]
+|=======================================================================
+| Type | Description | Default Location | Setting
+| home
+  | Home directory of the Logstash installation.
+  | `/usr/share/logstash`
+ d|
+
+| bin
+  | Binary scripts including `logstash` to start Logstash
+    and `logstash-plugin` to install plugins
+  | `/usr/share/logstash/bin`
+ d|
+
+| settings
+  | Configuration files, including `logstash.yml`, `jvm.options`, and `startup.options`
+  | `/etc/logstash`
+  | `path.settings`
+
+| conf
+  | Logstash pipeline configuration files
+  | `/etc/logstash/conf.d`
+  | `path.config`
+
+| logs
+  | Log files
+  | `/var/log/logstash`
+  | `path.logs`
+
+| plugins
+  | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.
+  | `/usr/share/logstash/plugins`
+  | `path.plugins`
+
+|=======================================================================
+
+[[docker-layout]]
+==== Directory Layout of Docker Images
+
+The Docker images are created from the `.tar.gz` packages, and follow a
+similar directory layout.
+
+[cols="<h,<,<m,<m",options="header",]
+|=======================================================================
+| Type | Description | Default Location | Setting
+| home
+  | Home directory of the Logstash installation.
+  | `/usr/share/logstash`
+ d|
+
+| bin
+  | Binary scripts, including `logstash` to start Logstash
+    and `logstash-plugin` to install plugins
+  | `/usr/share/logstash/bin`
+ d|
+
+| settings
+  | Configuration files, including `logstash.yml` and `jvm.options`
+  | `/usr/share/logstash/config`
+  | `path.settings`
+
+| conf
+  | Logstash pipeline configuration files
+  | `/usr/share/logstash/pipeline`
+  | `path.config`
+
+| plugins
+  | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.
+  | `/usr/share/logstash/plugins`
+  | `path.plugins`
+
+|=======================================================================
+
+NOTE: Logstash Docker containers do not create log files by default. They log
+to standard output.
+
+[[config-setting-files]]
+=== Logstash Configuration Files
+
+Logstash has two types of configuration files: _pipeline configuration files_, which define the Logstash processing
+pipeline, and _settings files_, which specify options that control Logstash startup and execution.
+
+==== Pipeline Configuration Files
+
+You create pipeline configuration files when you define the stages of your Logstash processing pipeline. On deb and
+rpm, you place the pipeline configuration files in the `/etc/logstash/conf.d` directory. Logstash tries to load all
+files in the `/etc/logstash/conf.d directory`, so don't store any non-config files or backup files in this directory.
+
+See <<configuration>> for more info.
+
+==== Settings Files
+
+The settings files are already defined in the Logstash installation. Logstash includes the following settings files:
+
+*`logstash.yml`*::
+  Contains Logstash configuration flags. You can set flags in this file instead of passing the flags at the command
+  line. Any flags that you set at the command line override the corresponding settings in the `logstash.yml` file. See <<logstash-settings-file>> for more info.
+*`jvm.options`*::
+  Contains JVM configuration flags. Specify each flag on a separate line. You can also use this file to set the locale
+  for Logstash.
+*`startup.options` (Linux)*::
+  Contains options used by the `system-install` script in `/usr/share/logstash/bin` to build the appropriate startup
+  script for your system. When you install the Logstash package, the `system-install` script executes at the end of the
+  installation process and uses the settings specified in `startup.options` to set options such as the user, group,
+  service name, and service description. By default, Logstash services are installed under the user `logstash`. The `startup.options` file makes it easier for you to install multiple instances of the Logstash service. You can copy
+  the file and change the values for specific settings. Note that the `startup.options` file is not read at startup. If
+  you want to change the Logstash startup script (for example, to change the Logstash user or read from a different
+  configuration path), you must re-run the `system-install` script (as root) to pass in the new settings.
diff --git a/docs/static/settings-file.asciidoc b/docs/static/settings-file.asciidoc
new file mode 100644
index 00000000000..a09ab1a9b1c
--- /dev/null
+++ b/docs/static/settings-file.asciidoc
@@ -0,0 +1,174 @@
+[[logstash-settings-file]]
+=== Settings File
+
+You can set options in the Logstash settings file, `logstash.yml`, to control Logstash execution. For example,
+you can specify pipeline settings, the location of configuration files, logging options, and other settings.
+Most of the settings in the `logstash.yml` file are also available as <<command-line-flags,command-line flags>>
+when you run Logstash. Any flags that you set at the command line override the corresponding settings in the
+`logstash.yml` file.
+
+The `logstash.yml` file is written in http://yaml.org/[YAML]. Its location varies by platform (see
+<<dir-layout>>). You can specify settings in hierarchical form or use flat keys. For example, to use
+hierarchical form to set the pipeline batch size and batch delay, you specify:
+
+[source,yaml]
+-------------------------------------------------------------------------------------
+pipeline:
+  batch:
+    size: 125
+    delay: 5
+-------------------------------------------------------------------------------------
+
+To express the same values as flat keys, you specify:
+
+[source,yaml]
+-------------------------------------------------------------------------------------
+pipeline.batch.size: 125
+pipeline.batch.delay: 5
+-------------------------------------------------------------------------------------
+
+The `logstash.yml` file includes the following settings:
+
+[options="header"]
+|=======================================================================
+| Setting | Description | Default value
+
+| `node.name`
+| A descriptive name for the node.
+| Machine's hostname
+
+| `path.data`
+| The directory that Logstash and its plugins use for any persistent needs.
+|`LOGSTASH_HOME/data`
+
+| `pipeline.workers`
+| The number of workers that will, in parallel, execute the filter and output stages of the pipeline.
+  If you find that events are backing up, or that the
+  CPU is not saturated, consider increasing this number to better utilize machine processing power.
+| Number of the host's CPU cores
+
+| `pipeline.output.workers`
+| The number of workers to use per output plugin instance.
+| `1`
+
+| `pipeline.batch.size`
+| The maximum number of events an individual worker thread will collect from inputs
+  before attempting to execute its filters and outputs.
+  Larger batch sizes are generally more efficient, but come at the cost of increased memory
+  overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
+  variable to effectively use the option.
+| `125`
+
+| `pipeline.batch.delay`
+| When creating pipeline event batches, how long in milliseconds to wait before dispatching an undersized
+  batch to filters and workers.
+| `5`
+
+| `pipeline.unsafe_shutdown`
+| When set to `true`, forces Logstash to exit during shutdown even if there are still inflight events
+  in memory. By default, Logstash will refuse to quit until all received events
+  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
+| `false`
+
+| `path.config`
+| The path to the Logstash config for the main pipeline. If you specify a directory or wildcard,
+  config files are read from the directory in alphabetical order.
+| Platform-specific. See <<dir-layout>>.
+
+| `config.string`
+| A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as
+  the config file.
+| None
+
+| `config.test_and_exit`
+| When set to `true`, checks that the configuration is valid and then exits. Note that grok patterns are not checked for
+  correctness with this setting. Logstash can read multiple config files from a directory. If you combine this
+  setting with `log.level: debug`, Logstash will log the combined config file, annotating
+  each config block with the source file it came from.
+| `false`
+
+| `config.reload.automatic`
+| When set to `true`, periodically checks if the configuration has changed and reloads the configuration whenever it is changed.
+  This can also be triggered manually through the SIGHUP signal.
+| `false`
+
+| `config.reload.interval`
+| How often in seconds Logstash checks the config files for changes.
+| `3`
+
+| `config.debug`
+| When set to `true`, shows the fully compiled configuration as a debug log message. You must also set `log.level: debug`.
+  WARNING: The log message will include any 'password' options passed to plugin configs as plaintext, and may result
+  in plaintext passwords appearing in your logs!
+| `false`
+
+| `queue.type`
+| The internal queuing model to use for event buffering. Specify `memory` for legacy in-memory based queuing, or `persisted` for disk-based ACKed queueing (<<persistent-queues,persistent queues>>).
+| `memory`
+
+| `path.queue`
+| The directory path where the data files will be stored when persistent queues are enabled (`queue.type: persisted`).
+| `path.data/queue`
+
+| `queue.page_capacity`
+| The size of the page data files used when persistent queues are enabled (`queue.type: persisted`). The queue data consists of append-only data files separated into pages.
+| 250mb
+
+| `queue.max_events`
+| The maximum number of unread events in the queue when persistent queues are enabled (`queue.type: persisted`).
+| 0 (unlimited)
+
+| `queue.max_bytes`
+| The total capacity of the queue in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both `queue.max_events` and `queue.max_bytes` are specified, Logstash uses whichever criteria is reached first. 
+| 1024mb (1g)
+
+| `queue.checkpoint.acks`
+| The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.acks: 0` to set this value to unlimited.
+|1024
+
+| `queue.checkpoint.writes`
+| The maximum number of written events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.writes: 0` to set this value to unlimited.
+| 1024
+
+| `queue.checkpoint.interval`
+| The interval in milliseconds when a checkpoint is forced on the head page when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.interval: 0` for no periodic checkpoint.
+| 1000
+
+| `http.host`
+| The bind address for the metrics REST endpoint.
+| `"127.0.0.1"`
+
+| `http.port`
+| The bind port for the metrics REST endpoint.
+| `9600`
+
+| `log.level`
+a|
+The log level. Valid options are:
+
+* `fatal` 
+* `error`
+* `warn`
+* `info`
+* `debug`
+* `trace`
+
+| `info`
+
+| `log.format`
+| The log format. Set to `json` to log in JSON format, or `plain` to use `Object#.inspect`.
+| `plain`
+
+| `path.logs`
+| The directory where Logstash will write its log to.
+| `LOGSTASH_HOME/logs
+
+| `path.plugins`
+| Where to find custom plugins. You can specify this setting multiple times to include
+  multiple paths. Plugins are expected to be in a specific directory hierarchy:
+  `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`,
+  and `NAME` is the name of the plugin.
+| Platform-specific. See <<dir-layout>>.
+
+|=======================================================================
+
diff --git a/docs/static/shutdown.asciidoc b/docs/static/shutdown.asciidoc
new file mode 100644
index 00000000000..d2288a2144c
--- /dev/null
+++ b/docs/static/shutdown.asciidoc
@@ -0,0 +1,92 @@
+[[shutdown]]
+=== Shutting Down Logstash
+
+To shut down Logstash, use one of the following commands:
+
+* On systemd, use:
++
+[source,shell]
+----
+systemctl stop logstash
+----
+
+* On upstart, use: 
++
+[source,shell]
+----
+initctl stop logstash
+----
+
+* On sysv, use: 
++
+[source,shell]
+----
+/etc/init.d/logstash stop
+----
+
+* If you have the PID, use:
++
+[source,shell]
+----
+kill -TERM {logstash_pid}
+----
+
+==== What Happens During a Controlled Shutdown?
+
+When you attempt to shut down a running Logstash instance, Logstash performs several steps before it can safely shut down. It must:
+
+* Stop all input, filter and output plugins
+* Process all in-flight events
+* Terminate the Logstash process
+
+The following conditions affect the shutdown process:
+
+* An input plugin receiving data at a slow pace.
+* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy
+query.
+* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+
+These situations make the duration and success of the shutdown process unpredictable.
+
+Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
+This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
+worker threads.
+
+To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--pipeline.unsafe_shutdown` flag when
+you start Logstash.
+
+WARNING: Unsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason may result in data loss (unless you've
+enabled Logstash to use <<persistent-queues,persistent queues>>). Shut down
+Logstash safely whenever possible.
+
+[[shutdown-stall-example]]
+==== Stall Detection Example
+
+In this example, slow filter execution prevents the pipeline from performing a clean shutdown. Because Logstash is
+started with the `--pipeline.unsafe_shutdown` flag, the shutdown results in the loss of 20 events.
+
+========
+[source,shell]
+bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } 
+  output { stdout { codec => dots } }' -w 1 --pipeline.unsafe_shutdown
+Pipeline main started
+^CSIGINT received. Shutting down the agent. {:level=>:warn}
+stopping pipeline {:id=>"main", :level=>:warn}
+Received shutdown signal, but pipeline is still waiting for in-flight events
+to be processed. Sending another ^C will force quit Logstash, but this may cause
+data loss. {:level=>:warn}
+{"inflight_count"=>125, "stalling_thread_info"=>{["LogStash::Filters::Ruby", 
+{"code"=>"sleep 10000"}]=>[{"thread_id"=>19, "name"=>"[main]>worker0", 
+"current_call"=>"(ruby filter code):1:in `sleep'"}]}} {:level=>:warn}
+The shutdown process appears to be stalled due to busy or blocked plugins. 
+Check the logs for more information. {:level=>:error}
+{"inflight_count"=>125, "stalling_thread_info"=>{["LogStash::Filters::Ruby", 
+{"code"=>"sleep 10000"}]=>[{"thread_id"=>19, "name"=>"[main]>worker0", 
+"current_call"=>"(ruby filter code):1:in `sleep'"}]}} {:level=>:warn}
+{"inflight_count"=>125, "stalling_thread_info"=>{["LogStash::Filters::Ruby", 
+{"code"=>"sleep 10000"}]=>[{"thread_id"=>19, "name"=>"[main]>worker0", 
+"current_call"=>"(ruby filter code):1:in `sleep'"}]}} {:level=>:warn}
+Forcefully quitting logstash.. {:level=>:fatal}
+========
+
+When `--pipeline.unsafe_shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
\ No newline at end of file
diff --git a/docs/static/submitting-a-plugin.asciidoc b/docs/static/submitting-a-plugin.asciidoc
new file mode 100644
index 00000000000..8de77da375e
--- /dev/null
+++ b/docs/static/submitting-a-plugin.asciidoc
@@ -0,0 +1,107 @@
+[[submitting-plugin]]
+=== Submitting your plugin to RubyGems.org and the logstash-plugins repository
+
+Logstash uses http://rubygems.org[RubyGems.org] as its repository for all plugin
+artifacts. Once you have developed your new plugin, you can make it available to
+Logstash users by simply publishing it to RubyGems.org.
+
+==== Licensing
+Logstash and all its plugins are licensed under
+https://github.com/elasticsearch/logstash/blob/master/LICENSE[Apache License, version 2 ("ALv2")].
+If you make your plugin publicly available via http://rubygems.org[RubyGems.org],
+please make sure to have this line in your gemspec:
+
+* `s.licenses = ['Apache License (2.0)']`
+
+==== Publishing to http://rubygems.org[RubyGems.org]
+
+To begin, youâ€™ll need an account on RubyGems.org
+
+* https://rubygems.org/sign_up[Sign-up for a RubyGems account].
+
+After creating an account,
+http://guides.rubygems.org/rubygems-org-api/#api-authorization[obtain] an API
+key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials`
+to store your API key. These credentials will be used to publish the gem.
+Replace `username` and `password` with the credentials you created at
+RubyGems.org:
+
+[source,sh]
+----------------------------------
+curl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials
+chmod 0600 ~/.gem/credentials
+----------------------------------
+
+Before proceeding, make sure you have the right version in your gemspec file
+and commit your changes.
+
+* `s.version = '0.1.0'`
+
+To publish version 0.1.0 of your new logstash gem:
+
+[source,sh]
+----------------------------------
+bundle install
+bundle exec rake vendor
+bundle exec rspec
+bundle exec rake publish_gem
+----------------------------------
+
+[NOTE]
+========
+Executing `rake publish_gem`:
+
+. Reads the version from the gemspec file (`s.version = '0.1.0'`)
+. Checks in your local repository if a tag exists for that version. If the tag
+already exists, it aborts the process. Otherwise, it creates a new version tag
+in your local repository.
+. Builds the gem
+. Publishes the gem to RubyGems.org
+========
+
+That's it! Your plugin is published! Logstash users can now install your plugin
+by running:
+
+[source,sh]
+[subs="attributes"]
+----------------------------------
+bin/plugin install logstash-{plugintype}-mypluginname
+----------------------------------
+
+==== Contributing your source code to https://github.com/logstash-plugins[logstash-plugins]
+
+It is not required to contribute your source code to
+https://github.com/logstash-plugins[logstash-plugins] github organization, but
+we always welcome new plugins!
+
+==== Benefits
+
+Some of the many benefits of having your plugin in the logstash-plugins
+repository are:
+
+* **Discovery** Your plugin will appear in the http://www.elasticsearch.org/guide/en/logstash/current/index.html[Logstash Reference],
+where Logstash users look first for plugins and documentation.
+* **Documentation** Your plugin documentation will automatically be added to the
+ http://www.elasticsearch.org/guide/en/logstash/current/index.html[Logstash Reference].
+* **Testing** With our testing infrastructure, your plugin will be continuously
+tested against current and future releases of Logstash.  As a result, users will
+have the assurance that if incompatibilities arise, they will be quickly
+discovered and corrected.
+
+==== Acceptance Guidelines
+
+* **Code Review** Your plugin must be reviewed by members of the community for
+coherence, quality, readability, stability and security.
+* **Tests** Your plugin must contain tests to be accepted.  These tests are also
+subject to code review for scope and completeness.  It's ok if you don't know
+how to write tests -- we will guide you. We are working on publishing a guide to
+creating tests for Logstash which will make it easier.  In the meantime, you can
+refer to http://betterspecs.org/ for examples.
+
+To begin migrating your plugin to logstash-plugins, simply create a new
+https://github.com/elasticsearch/logstash/issues[issue] in
+the Logstash repository. When the acceptance guidelines are completed, we will
+facilitate the move to the logstash-plugins organization using the recommended
+https://help.github.com/articles/transferring-a-repository/#transferring-from-a-user-to-an-organization[github process].
+
+
diff --git a/docs/static/transforming-data.asciidoc b/docs/static/transforming-data.asciidoc
new file mode 100644
index 00000000000..2e4b411e3ca
--- /dev/null
+++ b/docs/static/transforming-data.asciidoc
@@ -0,0 +1,537 @@
+[[transformation]]
+== Transforming Data
+
+With over 200 plugins in the Logstash plugin ecosystem, it's sometimes
+challenging to choose the best plugin to meet your data processing needs.
+In this section, we've collected a list of popular plugins and organized them
+according to their processing capabilities:
+
+* <<core-operations>>
+* <<data-deserialization>>
+* <<field-extraction>>
+* <<lookup-enrichment>>
+
+Also see <<filter-plugins>> and <<codec-plugins>> for the full list of available
+data processing plugins.
+
+[[core-operations]]
+=== Performing Core Operations
+
+The plugins described in this section are useful for core operations, such as
+mutating and dropping events.
+
+<<plugins-filters-date,date filter>>::
+
+Parses dates from fields to use as Logstash timestamps for events.
++
+The following config parses a field called `logdate` to set the Logstash
+timestamp:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  date {
+    match => [ "logdate", "MMM dd yyyy HH:mm:ss" ]
+  }
+}   
+--------------------------------------------------------------------------------
+
+
+<<plugins-filters-drop,drop filter>>::
+
+Drops events. This filter is typically used in combination with conditionals.
++
+The following config drops `debug` level log messages:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  if [loglevel] == "debug" {
+    drop { }
+  }
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-filters-fingerprint,fingerprint filter>>::
+
+Fingerprints fields by applying a consistent hash.
++
+The following config fingerprints the `IP`, `@timestamp`, and `message` fields
+and adds the hash to a metadata field called `generated_id`:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  fingerprint {
+    source => ["IP", "@timestamp", "message"]
+    method => "SHA1"
+    key => "0123"
+    target => "[@metadata][generated_id]"
+  }
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-filters-mutate,mutate filter>>::
+
+Performs general mutations on fields. You can rename, remove, replace, and
+modify fields in your events.
++
+The following config renames the `HOSTORIP` field to `client_ip`:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  mutate {
+    rename => { "HOSTORIP" => "client_ip" }
+  }
+}
+--------------------------------------------------------------------------------
++
+The following config strips leading and trailing whitespace from the specified
+fields:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  mutate {
+    strip => ["field1", "field2"]
+  }
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-filters-ruby,ruby filter>>::
+
+Executes Ruby code.
++
+The following config executes Ruby code that cancels 90% of the events:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  ruby {
+    code => "event.cancel if rand <= 0.90"
+  }
+}
+--------------------------------------------------------------------------------
+
+
+[[data-deserialization]]
+=== Deserializing Data
+
+The plugins described in this section are useful for deserializing data into
+Logstash events.
+
+<<plugins-codecs-avro,avro codec>>::
+
+Reads serialized Avro records as Logstash events. This plugin deserializes
+individual Avro records. It is not for reading Avro files. Avro files have a
+unique format that must be handled upon input.
++
+The following config deserializes input from Kafka:
++
+[source,json]
+----------------------------------
+input {
+  kafka {
+    codec => {
+      avro => {
+        schema_uri => "/tmp/schema.avsc"
+      }
+    }
+  }
+}
+...
+----------------------------------
+
+
+<<plugins-filters-csv,csv filter>>::
+
+Parses comma-separated value data into individual fields. By default, the
+filter autogenerates field names (column1, column2, and so on), or you can specify
+a list of names. You can also change the column separator.
++
+The following config parses CSV data into the field names specified in the
+`columns` field:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  csv {
+    separator => ","
+    columns => [ "Transaction Number", "Date", "Description", "Amount Debit", "Amount Credit", "Balance" ]
+  }
+}
+--------------------------------------------------------------------------------
+
+<<plugins-codecs-fluent,fluent codec>>::
+        
+Reads the Fluentd `msgpack` schema.
++
+The following config decodes logs received from `fluent-logger-ruby`:
++
+[source,json]
+--------------------------------------------------------------------------------
+input {
+  tcp {
+    codec => fluent
+    port => 4000
+  }
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-codecs-json,json codec>>::
+
+Decodes (via inputs) and encodes (via outputs) JSON formatted content, creating
+one event per element in a JSON array.
++
+The following config decodes the JSON formatted content in a file:
++
+[source,json]
+--------------------------------------------------------------------------------
+input {
+  file {
+    path => "/path/to/myfile.json"
+    codec =>"json"
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-codecs-protobuf,protobuf codec>>::
+
+Reads protobuf encoded messages and converts them to Logstash events. Requires
+the protobuf definitions to be compiled as Ruby files. You can compile them by
+using the
+https://github.com/codekitchen/ruby-protocol-buffers[ruby-protoc compiler].
++
+The following config decodes events from a Kafka stream:
++
+[source,json]
+--------------------------------------------------------------------------------
+input
+  kafka {
+    zk_connect => "127.0.0.1"
+    topic_id => "your_topic_goes_here"
+    codec => protobuf {
+      class_name => "Animal::Unicorn"
+      include_path => ['/path/to/protobuf/definitions/UnicornProtobuf.pb.rb']
+    }
+  }
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-filters-xml,xml filter>>::
+
+Parses XML into fields.
++
+The following config parses the whole XML document stored in the `message` field:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  xml {
+    source => "message"
+  }
+}
+--------------------------------------------------------------------------------
+
+
+[[field-extraction]]
+=== Extracting Fields and Wrangling Data
+
+The plugins described in this section are useful for extracting fields and
+parsing unstructured data into fields.
+
+<<plugins-filters-dissect,dissect filter>>::
+
+Extracts unstructured event data into fields by using delimiters. The dissect
+filter does not use regular expressions and is very fast. However, if the
+structure of the data varies from line to line, the grok filter is more
+suitable.
++
+For example, let's say you have a log that contains the following message:
++
+[source,json]
+--------------------------------------------------------------------------------
+Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...
+--------------------------------------------------------------------------------
++
+The following config dissects the message:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  dissect {
+    mapping => { "message" => "%{ts} %{+ts} %{+ts} %{src} %{prog}[%{pid}]: %{msg}" }
+  }
+}
+--------------------------------------------------------------------------------
++
+After the dissect filter is applied, the event will be dissected into the following
+fields:
++
+[source,json]
+--------------------------------------------------------------------------------
+{
+  "msg"        => "Starting system activity accounting tool...",
+  "@timestamp" => 2017-04-26T19:33:39.257Z,
+  "src"        => "localhost",
+  "@version"   => "1",
+  "host"       => "localhost.localdomain",
+  "pid"        => "1",
+  "message"    => "Apr 26 12:20:02 localhost systemd[1]: Starting system activity accounting tool...",
+  "type"       => "stdin",
+  "prog"       => "systemd",
+  "ts"         => "Apr 26 12:20:02"
+}
+--------------------------------------------------------------------------------
+
+<<plugins-filters-kv,kv filter>>::
+
+Parses key-value pairs.
++
+For example, let's say you have a log message that contains the following
+key-value pairs:
++
+[source,json]
+--------------------------------------------------------------------------------
+ip=1.2.3.4 error=REFUSED
+--------------------------------------------------------------------------------
++
+The following config parses the key-value pairs into fields:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  kv { }
+}
+--------------------------------------------------------------------------------
++
+After the filter is applied, the event in the example will have these fields:
++
+* `ip: 1.2.3.4`
+* `error: REFUSED`
+
+
+<<plugins-filters-grok,grok filter>>::
+
+Parses unstructured event data into fields. This tool is perfect for syslog
+logs, Apache and other webserver logs, MySQL logs, and in general, any log
+format that is generally written for humans and not computer consumption.
+Grok works by combining text patterns into something that matches your
+logs.
++
+For example, let's say you have an HTTP request log that contains
+the following message:
++
+[source,json]
+--------------------------------------------------------------------------------
+55.3.244.1 GET /index.html 15824 0.043
+--------------------------------------------------------------------------------
++
+The following config parses the message into fields:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  grok {
+    match => { "message" => "%{IP:client} %{WORD:method} %{URIPATHPARAM:request} %{NUMBER:bytes} %{NUMBER:duration}" }
+  }
+}
+--------------------------------------------------------------------------------
++
+After the filter is applied, the event in the example will have these fields:
++
+* `client: 55.3.244.1`
+* `method: GET`
+* `request: /index.html`
+* `bytes: 15824`
+* `duration: 0.043`
+
+[[lookup-enrichment]]
+=== Enriching Data with Lookups
+
+The plugins described in this section are useful for enriching data with
+additional info, such as GeoIP and user agent info.
+
+<<plugins-filters-dns,dns filter>>::
+
+Performs a standard or reverse DNS lookup.
++
+The following config performs a reverse lookup on the address in the
+`source_host` field and replaces it with the domain name:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  dns {
+    reverse => [ "source_host" ]
+    action => "replace"
+  }
+}
+--------------------------------------------------------------------------------
+
+    
+<<plugins-filters-elasticsearch,elasticsearch>>::
+
+Copies fields from previous log events in Elasticsearch to current events. 
++
+The following config shows a complete example of how this filter might
+be used.  Whenever Logstash receives an "end" event, it uses this Elasticsearch
+filter to find the matching "start" event based on some operation identifier.
+Then it copies the `@timestamp` field from the "start" event into a new field on
+the "end" event.  Finally, using a combination of the date filter and the
+ruby filter, the code in the example calculates the time duration in hours
+between the two events.
++
+[source,json]
+--------------------------------------------------
+      if [type] == "end" {
+         elasticsearch {
+            hosts => ["es-server"]
+            query => "type:start AND operation:%{[opid]}"
+            fields => { "@timestamp" => "started" }
+         }
+         date {
+            match => ["[started]", "ISO8601"]
+            target => "[started]"
+         }
+         ruby {
+            code => 'event.set("duration_hrs", (event.get("@timestamp") - event.get("started")) / 3600) rescue nil'
+         }
+      }
+--------------------------------------------------
+
+
+<<plugins-filters-geoip,geoip filter>>::
+
+Adds geographical information about the location of IP addresses. For example: 
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  geoip {
+    source => "clientip"
+  }
+}
+--------------------------------------------------------------------------------
++    
+After the geoip filter is applied, the event will be enriched with geoip fields.
+For example:
++
+[source,json]
+--------------------------------------------------------------------------------
+          "geoip" => {
+              "timezone" => "Europe/Moscow",
+                    "ip" => "83.149.9.216",
+              "latitude" => 55.7522,
+        "continent_code" => "EU",
+             "city_name" => "Moscow",
+         "country_code2" => "RU",
+          "country_name" => "Russia",
+              "dma_code" => nil,
+         "country_code3" => "RU",
+           "region_name" => "Moscow",
+              "location" => [
+            [0] 37.6156,
+            [1] 55.7522
+        ],
+           "postal_code" => "101194",
+             "longitude" => 37.6156,
+           "region_code" => "MOW"
+    }
+--------------------------------------------------------------------------------
+
+<<plugins-filters-jdbc_streaming,jdbc_streaming>>::
+
+Enriches events with database data.
++
+The following example executes a SQL query and stores the result set in a field
+called `country_details`:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  jdbc_streaming {
+    jdbc_driver_library => "/path/to/mysql-connector-java-5.1.34-bin.jar"
+    jdbc_driver_class => "com.mysql.jdbc.Driver"
+    jdbc_connection_string => "jdbc:mysql://localhost:3306/mydatabase"
+    jdbc_user => "me"
+    jdbc_password => "secret"
+    statement => "select * from WORLD.COUNTRY WHERE Code = :code"
+    parameters => { "code" => "country_code"}
+    target => "country_details"
+  }
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-filters-translate,translate filter>>::
+
+Replaces field contents based on replacement values specified in a hash or file.
+Currently supports these file types: YAML, JSON, and CSV.
++
+The following example takes the value of the `response_code` field, translates
+it to a description based on the values specified in the dictionary, and then
+removes the `response_code` field from the event:
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  translate {
+    field => "response_code"
+    destination => "http_response"
+    dictionary => {
+      "200" => "OK"
+      "403" => "Forbidden"
+      "404" => "Not Found"
+      "408" => "Request Timeout" 
+    }
+    remove_field => "response_code"
+  }
+}
+--------------------------------------------------------------------------------
+
+
+<<plugins-filters-useragent,useragent filter>>::
+
+Parses user agent strings into fields.
++
+The following example takes the user agent string in the `agent` field, parses
+it into user agent fields, and adds the user agent fields to a new field called
+`user_agent`. It also removes the original `agent` field: 
++
+[source,json]
+--------------------------------------------------------------------------------
+filter {
+  useragent {
+    source => "agent"
+    target => "user_agent"
+    remove_field => "agent"
+  }
+}
+--------------------------------------------------------------------------------
++ 
+After the filter is applied, the event will be enriched with user agent fields.
+For example:
++
+[source,json]
+--------------------------------------------------------------------------------
+        "user_agent": {
+          "os": "Mac OS X 10.12",
+          "major": "50",
+          "minor": "0",
+          "os_minor": "12",
+          "os_major": "10",
+          "name": "Firefox",
+          "os_name": "Mac OS X",
+          "device": "Other"
+        }
+--------------------------------------------------------------------------------   
+        
+
+    
\ No newline at end of file
diff --git a/docs/static/upgrading.asciidoc b/docs/static/upgrading.asciidoc
index 6e2d8337347..b4e8ca968ab 100644
--- a/docs/static/upgrading.asciidoc
+++ b/docs/static/upgrading.asciidoc
@@ -9,18 +9,29 @@ Before upgrading Logstash:
 * Test upgrades in a development environment before upgrading your production cluster.
 ===========================================
 
+If you are installing Logstash with other components in the Elastic Stack, also see the
+{stack}index.html[Elastic Stack installation and upgrade documentation].
+
+See the following topics for information about upgrading Logstash:
+
+* <<upgrading-using-package-managers>>
+* <<upgrading-using-direct-download>>
+* <<upgrading-logstash-5.0>>
+
+[[upgrading-using-package-managers]]
 === Upgrading Using Package Managers
 
 This procedure uses <<package-repositories,package managers>> to upgrade Logstash.
 
 1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
-2. Using the directions in the _Package Repositories_ section, update your repository links to point to the 2.0 repositories
+2. Using the directions in the _Package Repositories_ section, update your repository links to point to the 5.x repositories
 instead of the previous version.
 3. Run the `apt-get upgrade logstash` or `yum update logstash` command as appropriate for your operating system.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
-some Logstash plugins have changed in the 2.0 release.
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
+some Logstash plugins have changed in the 5.x release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
+[[upgrading-using-direct-download]]
 === Upgrading Using a Direct Download
 
 This procedure downloads the relevant Logstash binaries directly from Elastic.
@@ -28,82 +39,44 @@ This procedure downloads the relevant Logstash binaries directly from Elastic.
 1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
 2. Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
 3. Unpack the installation file into your Logstash directory.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
-some Logstash plugins have changed in the 2.0 release.
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
+some Logstash plugins have changed in the 5.x release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
-=== Upgrading Logstash and Elasticsearch to 2.0
-
-If you are using Elasticsearch as an output, and wish to upgrade to Elasticsearch 2.0, please be
-aware of https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html[breaking changes]
-before you upgrade. In addition, the following steps needs to be performed after upgrading to Elasticsearch 2.0:
-
-**Mapping changes:** Users may have custom template changes, so by default a Logstash upgrade will
-leave the template as is. Even if you don't have a custom template, Logstash will not overwrite an existing
-template by default.
-
-There is one known issue (removal of https://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-object-type.html#_path_3[path]) with using GeoIP filter that needs a manual update to the template.
-
-Note: If you have custom template changes, please make sure to save it and merge any changes. You can
-get the existing template by running:
-
-[source,shell]
-curl -XGET localhost:9200/_template/logstash
-
-
-Add the following option to your Logstash config:
+[[upgrading-logstash-5.0]]
+=== Upgrading Logstash to 5.0
 
-[source,json]
-output {
-	elasticsearch {
-		template_overwrite => true
-	}
-}
+Before upgrading Logstash, remember to read the <<breaking-changes,breaking changes>>.
 
-Restart Logstash.
+If you are installing Logstash with other components in the Elastic Stack, also see the
+{stack}index.html[Elastic Stack installation and upgrade documentation].
 
-**Dots in fields:** Elasticsearch 2.0 does not allow field names to contain the `.` character.
-Further details about this change https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking_20_mapping_changes.html#_field_names_may_not_contain_dots[here]. Some plugins already have been updated to compensate
-for this breaking change, including logstash-filter-metrics and logstash-filter-elapsed.
-These plugin updates are available for Logstash 2.0. To upgrade to the latest version of these
-plugins, the command is:
+==== When to Upgrade
 
-[source,shell]
-bin/logstash-plugin update <plugin_name>
+Fresh installations can and should start with the same version across the Elastic Stack. 
 
-**Multiline Filter:** If you are using the Multiline Filter in your configuration and upgrade to Logstash 2.0,
-you will get an error. Make sure to explicitly set the number of filter workers (`-w`) to `1`. You can set the number
-of workers by passing a command line flag such as:
+Elasticsearch 5.0 does not require Logstash 5.0. An Elasticsearch 5.0 cluster will happily receive data from a
+Logstash 2.x instance via the default HTTP communication layer. This provides some flexibility to decide when to upgrade
+Logstash relative to an Elasticsearch upgrade. It may or may not be convenient for you to upgrade them together, and it
+is
+not required to be done at the same time as long as Elasticsearch is upgraded first.
 
-[source,shell]
-bin/logstash `-w 1`
+You should upgrade in a timely manner to get the performance improvements that come with Logstash 5.0, but do so in
+the way that makes the most sense for your environment.
 
-=== Upgrading Logstash to 2.2
+==== When Not to Upgrade
 
-Logstash 2.2 re-architected the pipeline stages to provide more performance and help future enhancements in resiliency.
-The new pipeline introduced micro-batches, processing groups of events at a time. The default batch size is
-125 per worker. Also, the filter and output stages are executed in the same thread, but still, as different stages.
-The CLI flag `--pipeline-workers` or `-w` control the number of execution threads, which is set by default to number of cores.
+If any Logstash plugin that you require is not compatible with Logstash 5.0, then you should wait until it is ready
+before upgrading.
 
-**Considerations for Elasticsearch Output**
-The default batch size of the pipeline is 125 events per worker. This will by default also be the bulk size
-used for the elasticsearch output. The Elasticsearch output's `flush_size` now acts only as a maximum bulk
-size (still defaulting to 500). For example, if your pipeline batch size is 3000 events, Elasticsearch
-Output will send 500 events at a time, in 6 separate bulk requests. In other words, for Elasticsearch output,
-bulk request size is chunked based on `flush_size` and `--pipeline-batch-size`. If `flush_size` is set greater
-than `--pipeline-batch-size`, it is ignored and `--pipeline-batch-size` will be used.
+Although we make great efforts to ensure compatibility, Logstash 5.0 is not completely backwards compatible. As noted
+in the Elastic Stack upgrade guide, Logstash 5.0 should not be upgraded before Elasticsearch 5.0. This is both
+practical and because some Logstash 5.0 plugins may attempt to use features of Elasticsearch 5.0 that did not exist
+in earlier versions. For example, if you attempt to send the 5.x template to a cluster before Elasticsearch 5.0, then it
+will not be able to use it and all indexing will fail likely fail. If you use your own, custom template with Logstash,
+then this issue can be ignored.
 
-The default number of output workers in Logstash 2.2 is now equal to the number of pipeline workers (`-w`)
-unless overridden in the Logstash config file. This can be problematic for some users as the
-extra workers may consume extra resources like file handles, especially in the case of the Elasticsearch
-output. Users with more than one Elasticsearch host may want to override the `workers` setting
-for the Elasticsearch output in their Logstash config to constrain that number to a low value, between 1 to 4.
+Note the Elasticsearch Output Index Template change in the <<breaking-changes>> documentation for further insight into
+this change and how it impacts operations.
 
-**Performance Tuning in 2.2**
-Since both filters and output workers are on the same thread, this could lead to threads being idle in I/O wait state.
-Thus, in 2.2, you can safely set `-w` to a number which is a multiple of the number of cores on your machine.
-A common way to tune performance is keep increasing the `-w` beyond the # of cores until performance no longer
-improves. A note of caution - make sure you also keep heapsize in mind, because the number of in-flight events
-are `#workers * batch_size * average_event size`. More in-flight events could add to memory pressure, eventually
-leading to Out of Memory errors. You can change the heapsize in Logstash by setting `LS_HEAP_SIZE`
 
diff --git a/dripmain.rb b/dripmain.rb
index 0324150871a..60bfc0435b5 100644
--- a/dripmain.rb
+++ b/dripmain.rb
@@ -3,6 +3,7 @@
 
 require_relative "lib/bootstrap/environment"
 LogStash::Bundler.setup!({:without => [:build]})
+require "logstash-core"
 
 # typical required gems and libs
 require "logstash/environment"
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar b/gradle/wrapper/gradle-wrapper.jar
similarity index 74%
rename from logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar
rename to gradle/wrapper/gradle-wrapper.jar
index 13372aef5e2..ca78035ef05 100644
Binary files a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar and b/gradle/wrapper/gradle-wrapper.jar differ
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties b/gradle/wrapper/gradle-wrapper.properties
similarity index 80%
rename from logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
rename to gradle/wrapper/gradle-wrapper.properties
index 25611753f15..36078a84d4d 100644
--- a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
+++ b/gradle/wrapper/gradle-wrapper.properties
@@ -1,6 +1,6 @@
-#Fri Jan 22 14:29:02 EST 2016
+#Wed Jun 29 13:06:17 PDT 2016
 distributionBase=GRADLE_USER_HOME
 distributionPath=wrapper/dists
 zipStoreBase=GRADLE_USER_HOME
 zipStorePath=wrapper/dists
-distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-bin.zip
+distributionUrl=https\://services.gradle.org/distributions/gradle-2.13-bin.zip
diff --git a/logstash-core-event-java/gradlew b/gradlew
similarity index 97%
rename from logstash-core-event-java/gradlew
rename to gradlew
index 9d82f789151..27309d92314 100755
--- a/logstash-core-event-java/gradlew
+++ b/gradlew
@@ -6,12 +6,30 @@
 ##
 ##############################################################################
 
-# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
-DEFAULT_JVM_OPTS=""
+# Attempt to set APP_HOME
+# Resolve links: $0 may be a link
+PRG="$0"
+# Need this for relative symlinks.
+while [ -h "$PRG" ] ; do
+    ls=`ls -ld "$PRG"`
+    link=`expr "$ls" : '.*-> \(.*\)$'`
+    if expr "$link" : '/.*' > /dev/null; then
+        PRG="$link"
+    else
+        PRG=`dirname "$PRG"`"/$link"
+    fi
+done
+SAVED="`pwd`"
+cd "`dirname \"$PRG\"`/" >/dev/null
+APP_HOME="`pwd -P`"
+cd "$SAVED" >/dev/null
 
 APP_NAME="Gradle"
 APP_BASE_NAME=`basename "$0"`
 
+# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
+DEFAULT_JVM_OPTS=""
+
 # Use the maximum available, or set MAX_FD != -1 to use that value.
 MAX_FD="maximum"
 
@@ -30,6 +48,7 @@ die ( ) {
 cygwin=false
 msys=false
 darwin=false
+nonstop=false
 case "`uname`" in
   CYGWIN* )
     cygwin=true
@@ -40,26 +59,11 @@ case "`uname`" in
   MINGW* )
     msys=true
     ;;
+  NONSTOP* )
+    nonstop=true
+    ;;
 esac
 
-# Attempt to set APP_HOME
-# Resolve links: $0 may be a link
-PRG="$0"
-# Need this for relative symlinks.
-while [ -h "$PRG" ] ; do
-    ls=`ls -ld "$PRG"`
-    link=`expr "$ls" : '.*-> \(.*\)$'`
-    if expr "$link" : '/.*' > /dev/null; then
-        PRG="$link"
-    else
-        PRG=`dirname "$PRG"`"/$link"
-    fi
-done
-SAVED="`pwd`"
-cd "`dirname \"$PRG\"`/" >/dev/null
-APP_HOME="`pwd -P`"
-cd "$SAVED" >/dev/null
-
 CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar
 
 # Determine the Java command to use to start the JVM.
@@ -85,7 +89,7 @@ location of your Java installation."
 fi
 
 # Increase the maximum file descriptors if we can.
-if [ "$cygwin" = "false" -a "$darwin" = "false" ] ; then
+if [ "$cygwin" = "false" -a "$darwin" = "false" -a "$nonstop" = "false" ] ; then
     MAX_FD_LIMIT=`ulimit -H -n`
     if [ $? -eq 0 ] ; then
         if [ "$MAX_FD" = "maximum" -o "$MAX_FD" = "max" ] ; then
diff --git a/logstash-core-event-java/gradlew.bat b/gradlew.bat
similarity index 93%
rename from logstash-core-event-java/gradlew.bat
rename to gradlew.bat
index aec99730b4e..f6d5974e72f 100644
--- a/logstash-core-event-java/gradlew.bat
+++ b/gradlew.bat
@@ -8,14 +8,14 @@
 @rem Set local scope for the variables with windows NT shell
 if "%OS%"=="Windows_NT" setlocal
 
-@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
-set DEFAULT_JVM_OPTS=
-
 set DIRNAME=%~dp0
 if "%DIRNAME%" == "" set DIRNAME=.
 set APP_BASE_NAME=%~n0
 set APP_HOME=%DIRNAME%
 
+@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
+set DEFAULT_JVM_OPTS=
+
 @rem Find java.exe
 if defined JAVA_HOME goto findJavaFromJavaHome
 
@@ -46,7 +46,7 @@ echo location of your Java installation.
 goto fail
 
 :init
-@rem Get command-line arguments, handling Windowz variants
+@rem Get command-line arguments, handling Windows variants
 
 if not "%OS%" == "Windows_NT" goto win9xME_args
 if "%@eval[2+2]" == "4" goto 4NT_args
diff --git a/integration/logstash_config/file_input_to_file_output_spec.rb b/integration/logstash_config/file_input_to_file_output_spec.rb
deleted file mode 100644
index b9a4fcfd5c9..00000000000
--- a/integration/logstash_config/file_input_to_file_output_spec.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-require "stud/temporary"
-
-describe "File input to File output" do
-  let(:number_of_events) { IO.readlines(sample_log).size }
-  let(:sample_log) { File.expand_path(File.join(File.dirname(__FILE__), "..", "support", "sample.log")) }
-  let(:output_file) { Stud::Temporary.file.path }
-  let(:config) { 
-<<EOS
-    input {
-       file {
-         path => \"#{sample_log}\"
-         stat_interval => 0
-         start_position => \"beginning\"
-         sincedb_path => \"/dev/null\"
-       }
-      }
-    output {
-      file {
-        path => \"#{output_file}\"
-      }
-    }
-EOS
-  }
-
-  before :all do
-    command("bin/logstash-plugin install logstash-input-file logstash-output-file")
-  end
-
-  it "writes events to file" do
-    cmd = "bin/logstash -e '#{config}'"
-    launch_logstash(cmd)
-
-    expect(File.exist?(output_file)).to eq(true)
-
-    # on shutdown the events arent flushed to disk correctly
-    # Known issue https://github.com/logstash-plugins/logstash-output-file/issues/12
-    expect(IO.readlines(output_file).size).to be_between(number_of_events - 10, number_of_events).inclusive
-  end
-end
diff --git a/integration/plugin_manager/logstash_spec.rb b/integration/plugin_manager/logstash_spec.rb
deleted file mode 100644
index f7047e986a2..00000000000
--- a/integration/plugin_manager/logstash_spec.rb
+++ /dev/null
@@ -1,11 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-require_relative "../../logstash-core/lib/logstash/version"
-
-describe "bin/logstash" do
-  it "returns the logstash version" do
-    result = command("bin/logstash --version")
-    expect(result.exit_status).to eq(0)
-    expect(result.stdout).to match(/^logstash\s#{LOGSTASH_VERSION}/)
-  end
-end
diff --git a/integration/plugin_manager/plugin_install_spec.rb b/integration/plugin_manager/plugin_install_spec.rb
deleted file mode 100644
index db31bc95740..00000000000
--- a/integration/plugin_manager/plugin_install_spec.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-require "fileutils"
-
-context "bin/logstash-plugin install" do
-  context "with a local gem" do
-    let(:gem_name) { "logstash-input-wmi" }
-    let(:local_gem) { gem_fetch(gem_name) }
-
-    it "install the gem succesfully" do
-      result = command("bin/logstash-plugin install --no-verify #{local_gem}")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout).to match(/^Installing\s#{gem_name}\nInstallation\ssuccessful$/)
-    end
-  end
-
-  context "when the plugin exist" do
-    let(:plugin_name) { "logstash-input-drupal_dblog" }
-
-    it "sucessfully install" do
-      result = command("bin/logstash-plugin install #{plugin_name}")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout).to match(/^Validating\s#{plugin_name}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
-    end
-
-    it "allow to install a specific version" do
-      version = "2.0.2"
-      result = command("bin/logstash-plugin install --version 2.0.2 #{plugin_name}")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout).to match(/^Validating\s#{plugin_name}-#{version}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
-    end
-  end
-
-  context "when the plugin doesn't exist" do
-    it "fails to install" do
-      result = command("bin/logstash-plugin install --no-verify logstash-output-impossible-plugin")
-      expect(result.exit_status).to eq(1)
-      expect(result.stderr).to match(/Installation Aborted, message: Could not find gem/)
-    end
-  end
-end
diff --git a/integration/plugin_manager/plugin_list_spec.rb b/integration/plugin_manager/plugin_list_spec.rb
deleted file mode 100644
index d2ae7807f1c..00000000000
--- a/integration/plugin_manager/plugin_list_spec.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-
-describe "bin/logstash-plugin list" do
-  context "without a specific plugin" do
-    it "display a list of plugins" do
-      result = command("bin/logstash-plugin list")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout.split("\n").size).to be > 1
-    end
-
-    it "display a list of installed plugins" do
-      result = command("bin/logstash-plugin list --installed")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout.split("\n").size).to be > 1
-    end
-
-    it "list the plugins with their versions" do
-      result = command("bin/logstash-plugin list --verbose")
-      result.stdout.split("\n").each do |plugin|
-        expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+\)/)
-      end
-      expect(result.exit_status).to eq(0)
-    end
-  end
-
-  context "with a specific plugin" do
-    let(:plugin_name) { "logstash-input-stdin" }
-    it "list the plugin and display the plugin name" do
-      result = command("bin/logstash-plugin list #{plugin_name}")
-      expect(result.stdout).to match(/^#{plugin_name}$/)
-      expect(result.exit_status).to eq(0)
-    end
-
-    it "list the plugin with his version" do
-      result = command("bin/logstash-plugin list --verbose #{plugin_name}")
-      expect(result.stdout).to match(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
-      expect(result.exit_status).to eq(0)
-    end
-  end
-end
diff --git a/integration/plugin_manager/plugin_uninstall_spec.rb b/integration/plugin_manager/plugin_uninstall_spec.rb
deleted file mode 100644
index df3c6e4396e..00000000000
--- a/integration/plugin_manager/plugin_uninstall_spec.rb
+++ /dev/null
@@ -1,24 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-
-describe "bin/logstash-plugin uninstall" do
-  context "when the plugin isn't installed" do
-    it "fails to uninstall it" do
-      result = command("bin/logstash-plugin uninstall logstash-filter-cidr")
-      expect(result.stderr).to match(/ERROR: Uninstall Aborted, message: This plugin has not been previously installed, aborting/)
-      expect(result.exit_status).to eq(1)
-    end
-  end
-
-  context "when the plugin is installed" do
-      it "succesfully uninstall it" do
-      # make sure we have the plugin installed.
-      command("bin/logstash-plugin install logstash-filter-ruby")
-
-      result = command("bin/logstash-plugin uninstall logstash-filter-ruby")
-
-      expect(result.stdout).to match(/^Uninstalling logstash-filter-ruby/)
-      expect(result.exit_status).to eq(0)
-    end
-  end
-end
diff --git a/integration/plugin_manager/plugin_update_spec.rb b/integration/plugin_manager/plugin_update_spec.rb
deleted file mode 100644
index 549a9babc80..00000000000
--- a/integration/plugin_manager/plugin_update_spec.rb
+++ /dev/null
@@ -1,32 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-
-describe "update" do
-  let(:plugin_name) { "logstash-input-stdin" }
-  let(:previous_version) { "2.0.1" }
-
-  before do
-    command("bin/logstash-plugin install --version #{previous_version} #{plugin_name}")
-    cmd = command("bin/logstash-plugin list --verbose #{plugin_name}")
-    expect(cmd.stdout).to match(/#{plugin_name} \(#{previous_version}\)/)
-  end
-
-  context "update a specific plugin" do
-    subject { command("bin/logstash-plugin update #{plugin_name}") }
-
-    it "has executed succesfully" do
-      expect(subject.exit_status).to eq(0)
-      expect(subject.stdout).to match(/Updating #{plugin_name}/)
-    end
-  end
-
-  context "update all the plugins" do
-    subject { command("bin/logstash-plugin update") }
-
-    it "has executed succesfully" do
-      expect(subject.exit_status).to eq(0)
-      cmd = command("bin/logstash-plugin list --verbose #{plugin_name}").stdout
-      expect(cmd).to match(/logstash-input-stdin \(#{LogStashTestHelpers.latest_version(plugin_name)}\)/)
-    end
-  end
-end
diff --git a/integration/spec_helper.rb b/integration/spec_helper.rb
deleted file mode 100644
index f4cddfa713d..00000000000
--- a/integration/spec_helper.rb
+++ /dev/null
@@ -1,37 +0,0 @@
-# encoding: utf-8
-require_relative "support/integration_test_helpers"
-require_relative "../logstash-core/lib/logstash/environment"
-require "fileutils"
-
-if LogStash::Environment.windows?
-  puts "[integration] Theses integration test are specifically made to be run on under linux/unix"
-  puts "[integration] Please see our windows version of the tests https://github.com/elastic/logstash/tree/master/test/windows"
-end
-
-# Configure the test environment
-source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-integration_path = File.join(source, "integration_run")
-
-puts "[integration_spec] configure environment"
-
-if Dir.exists?(integration_path)
-  # We copy the current logstash into a temporary directory
-  # since the tests are a bit destructive
-  FileUtils.mkdir_p(integration_path)
-  rsync_cmd = "rsync -a --delete --exclude 'rspec' --exclude '#{File.basename(integration_path)}' --exclude 'integration_spec' --exclude '.git' #{source} #{integration_path}"
-
-  puts "[integration_spec] Rsync source code into: #{integration_path}"
-  system(rsync_cmd)
-  puts "[integration_spec] Finish rsync"
-
-  LOGSTASH_TEST_PATH = File.join(integration_path, "logstash")
-else
-  LOGSTASH_TEST_PATH = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-end
-
-puts "[integration_spec] Running the test in #{LOGSTASH_TEST_PATH}"
-puts "[integration_spec] Running specs"
-
-RSpec.configure do |config|
-  config.order = "random"
-end
diff --git a/integration/support/integration_test_helpers.rb b/integration/support/integration_test_helpers.rb
deleted file mode 100644
index aad90f8f07a..00000000000
--- a/integration/support/integration_test_helpers.rb
+++ /dev/null
@@ -1,89 +0,0 @@
-# encoding: utf-8
-require "json"
-require "open3"
-require "open-uri"
-require "stud/temporary"
-require "fileutils"
-require "bundler"
-require "gems"
-
-class CommandResponse
-  attr_reader :stdin, :stdout, :stderr, :exit_status
-
-  def initialize(cmd, stdin, stdout, stderr, exit_status)
-    @stdin = stdin
-    @stdout = stdout
-    @stderr = stderr
-    @exit_status = exit_status
-    @cmd = cmd
-  end
-
-  def to_debug
-    "DEBUG: stdout: #{stdout}, stderr: #{stderr}, exit_status: #{exit_status}"
-  end
-
-  def to_s
-    @cmd
-  end
-end
-
-def command(cmd, path = nil)
-  # http://bundler.io/v1.3/man/bundle-exec.1.html
-  # see shelling out.
-  #
-  # Since most of the integration test are environment destructive
-  # its better to run them in a cloned directory.
-  path = LOGSTASH_TEST_PATH if path == nil
-
-  Bundler.with_clean_env do
-    Dir.chdir(path) do
-      Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr|
-          CommandResponse.new(cmd,
-            stdin,
-            stdout.read.chomp,
-            stderr.read.chomp,
-            wait_thr.value.exitstatus)
-      end
-    end
-  end
-end
-
-def gem_fetch(name)
-  tmp = Stud::Temporary.directory
-  FileUtils.mkdir_p(tmp)
-
-  c = command("gem fetch #{name}", tmp)
-
-  if c.exit_status == 1
-    raise RuntimeError, "Can't fetch gem #{name}"
-  end
-
-  return Dir.glob(File.join(tmp, "#{name}*.gem")).first
-end
-
-# This is a bit hacky since JRuby doesn't support fork,
-# we use popen4 which return the pid of the process and make sure we kill it
-# after letting it run for a few seconds.
-def launch_logstash(cmd, path = nil)
-  path = LOGSTASH_TEST_PATH if path == nil
-  pid = 0
-
-  Thread.new do
-    Bundler.with_clean_env do
-      Dir.chdir(path) do
-        pid, input, output, error = IO.popen4(cmd) #jruby only
-      end
-    end
-  end
-  sleep(30)
-  begin
-    Process.kill("INT", pid)
-  rescue
-  end
-end
-
-module LogStashTestHelpers
-  def self.latest_version(name)
-    Gems.versions(name).first["number"] 
-  end
-end
diff --git a/integration/support/sample.log b/integration/support/sample.log
deleted file mode 100644
index 8f304b59c45..00000000000
--- a/integration/support/sample.log
+++ /dev/null
@@ -1,50 +0,0 @@
-83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1" 200 171717 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/highlight/highlight.js HTTP/1.1" 200 26185 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/zoom-js/zoom.js HTTP/1.1" 200 7697 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/plugin/notes/notes.js HTTP/1.1" 200 2892 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/sad-medic.png HTTP/1.1" 200 430406 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Bold.ttf HTTP/1.1" 200 38720 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Regular.ttf HTTP/1.1" 200 41820 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1" 200 52878 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard.png HTTP/1.1" 200 321631 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/Dreamhost_logo.svg HTTP/1.1" 200 2126 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard2.png HTTP/1.1" 200 394967 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/apache-icon.gif HTTP/1.1" 200 8095 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/nagios-sms5.png HTTP/1.1" 200 78075 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/redis.png HTTP/1.1" 200 25230 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/elasticsearch.png HTTP/1.1" 200 8026 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/logstashbook.png HTTP/1.1" 200 54662 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/github-contributions.png HTTP/1.1" 200 34245 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/css/print/paper.css HTTP/1.1" 200 4254 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/1983_delorean_dmc-12-pic-38289.jpeg HTTP/1.1" 200 220562 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/simple-inputs-filters-outputs.jpg HTTP/1.1" 200 1168622 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/tiered-outputs-to-inputs.jpg HTTP/1.1" 200 1079983 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:53 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-24.236.252.67 - - [26/Aug/2014:21:14:10 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:26.0) Gecko/20100101 Firefox/26.0"
-93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1" 200 18848 "http://www.google.ro/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCwQFjAB&url=http%3A%2F%2Fwww.semicomplete.com%2Farticles%2Fdynamic-dns-with-dhcp%2F&ei=W88AU4n9HOq60QXbv4GwBg&usg=AFQjCNEF1X4Rs52UYQyLiySTQxa97ozM4g&bvm=bv.61535280,d.d2k" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-66.249.73.135 - - [26/Aug/2014:21:15:03 +0000] "GET /blog/tags/ipv6 HTTP/1.1" 200 12251 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-50.16.19.13 - - [26/Aug/2014:21:15:15 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "http://www.semicomplete.com/blog/tags/puppet?flav=rss20" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
-66.249.73.185 - - [26/Aug/2014:21:15:23 +0000] "GET / HTTP/1.1" 200 37932 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-110.136.166.128 - - [26/Aug/2014:21:16:11 +0000] "GET /projects/xdotool/ HTTP/1.1" 200 12292 "http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&sqi=2&ved=0CFYQFjAE&url=http%3A%2F%2Fwww.semicomplete.com%2Fprojects%2Fxdotool%2F&ei=6cwAU_bRHo6urAeI0YD4Ag&usg=AFQjCNE3V_aCf3-gfNcbS924S6jZ6FqffA&bvm=bv.61535280,d.bmk" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-46.105.14.53 - - [26/Aug/2014:21:16:17 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-123.125.71.35 - - [26/Aug/2014:21:16:31 +0000] "GET /blog/tags/release HTTP/1.1" 200 40693 "-" "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-50.150.204.184 - - [26/Aug/2014:21:17:06 +0000] "GET /images/googledotcom.png HTTP/1.1" 200 65748 "http://www.google.com/search?q=https//:google.com&source=lnms&tbm=isch&sa=X&ei=4-r8UvDrKZOgkQe7x4CICw&ved=0CAkQ_AUoAA&biw=320&bih=441" "Mozilla/5.0 (Linux; U; Android 4.0.4; en-us; LG-MS770 Build/IMM76I) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30"
-207.241.237.225 - - [26/Aug/2014:21:17:35 +0000] "GET /blog/tags/examples HTTP/1.0" 200 9208 "http://www.semicomplete.com/blog/tags/C" "Mozilla/5.0 (compatible; archive.org_bot +http://www.archive.org/details/archive.org_bot)"
-200.49.190.101 - - [26/Aug/2014:21:17:39 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
-200.49.190.100 - - [26/Aug/2014:21:17:37 +0000] "GET /blog/tags/web HTTP/1.1" 200 44019 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
-200.49.190.101 - - [26/Aug/2014:21:17:41 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
-200.49.190.101 - - [26/Aug/2014:21:17:48 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
-66.249.73.185 - - [26/Aug/2014:21:18:48 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-66.249.73.135 - - [26/Aug/2014:21:18:55 +0000] "GET /blog/tags/munin HTTP/1.1" 200 9746 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-66.249.73.135 - - [26/Aug/2014:21:19:16 +0000] "GET /blog/tags/firefox?flav=rss20 HTTP/1.1" 200 16021 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
diff --git a/lib/bootstrap/bundler.rb b/lib/bootstrap/bundler.rb
index 2948fe8aa29..2fc69e31067 100644
--- a/lib/bootstrap/bundler.rb
+++ b/lib/bootstrap/bundler.rb
@@ -31,7 +31,7 @@ def set_key(key, value, hash, file)
       # This patch makes rubygems fetch directly from the remote servers
       # the dependencies he need and might not have downloaded in a local
       # repository. This basically enabled the offline feature to work as
-      # we remove the gems from the vendor directory before packacing.
+      # we remove the gems from the vendor directory before packaging.
       ::Bundler::Source::Rubygems.module_exec do
         def cached_gem(spec)
           cached_built_in_gem(spec)
@@ -78,7 +78,7 @@ def setup!(options = {})
     # @return [String, Exception] the installation captured output and any raised exception or nil if none
     def invoke!(options = {})
       options = {:max_tries => 10, :clean => false, :install => false, :update => false, :local => false,
-                 :all => false, :package => false, :without => [:development]}.merge(options)
+                 :jobs => 12, :all => false, :package => false, :without => [:development]}.merge(options)
       options[:without] = Array(options[:without])
       options[:update] = Array(options[:update]) if options[:update]
 
@@ -103,14 +103,24 @@ def invoke!(options = {})
       ::Bundler.settings[:path] = LogStash::Environment::BUNDLE_DIR
       ::Bundler.settings[:gemfile] = LogStash::Environment::GEMFILE_PATH
       ::Bundler.settings[:without] = options[:without].join(":")
+      ::Bundler.settings[:force] = options[:force]
+
+      if !debug?
+        # Will deal with transient network errors
+        execute_bundler_with_retry(options)
+      else
+        options[:verbose] = true
+        execute_bundler(options)
+      end
+    end
 
+    def execute_bundler_with_retry(options)
       try = 0
       # capture_stdout also traps any raised exception and pass them back as the function return [output, exception]
       output, exception = capture_stdout do
         loop do
           begin
-            ::Bundler.reset!
-            ::Bundler::CLI.start(bundler_arguments(options))
+            execute_bundler(options)
             break
           rescue ::Bundler::VersionConflict => e
             $stderr.puts("Plugin version conflict, aborting")
@@ -132,12 +142,20 @@ def invoke!(options = {})
           end
         end
       end
-
       raise exception if exception
 
       return output
     end
 
+    def execute_bundler(options)
+      ::Bundler.reset!
+      ::Bundler::CLI.start(bundler_arguments(options))
+    end
+
+    def debug?
+      ENV["DEBUG"]
+    end
+
     # build Bundler::CLI.start arguments array from the given options hash
     # @param option [Hash] the invoke! options hash
     # @return [Array<String>] Bundler::CLI.start string arguments array
@@ -162,6 +180,8 @@ def bundler_arguments(options = {})
         arguments << "--all" if options[:all]
       end
 
+      arguments << "--verbose" if options[:verbose]
+
       arguments.flatten
     end
 
diff --git a/lib/bootstrap/environment.rb b/lib/bootstrap/environment.rb
index 66ed16093f0..79ff4c024df 100644
--- a/lib/bootstrap/environment.rb
+++ b/lib/bootstrap/environment.rb
@@ -5,6 +5,7 @@
 
 require_relative "bundler"
 require_relative "rubygems"
+require "pathname"
 
 module LogStash
   module Environment
@@ -16,7 +17,9 @@ module Environment
     BUNDLE_DIR = ::File.join(LOGSTASH_HOME, "vendor", "bundle")
     GEMFILE_PATH = ::File.join(LOGSTASH_HOME, "Gemfile")
     LOCAL_GEM_PATH = ::File.join(LOGSTASH_HOME, 'vendor', 'local_gems')
-    CACHE_PATH = File.join(LOGSTASH_HOME, "vendor", "cache")
+    CACHE_PATH = ::File.join(LOGSTASH_HOME, "vendor", "cache")
+    LOCKFILE = Pathname.new(::File.join(LOGSTASH_HOME, "Gemfile.jruby-1.9.lock"))
+    GEMFILE = Pathname.new(::File.join(LOGSTASH_HOME, "Gemfile"))
 
     # @return [String] the ruby version string bundler uses to craft its gem path
     def gem_ruby_version
@@ -56,20 +59,15 @@ def pattern_path(path)
   end
 end
 
-
 # when launched as a script, not require'd, (currently from bin/logstash and bin/logstash-plugin) the first
 # argument is the path of a Ruby file to require and a LogStash::Runner class is expected to be
 # defined and exposing the LogStash::Runner#main instance method which will be called with the current ARGV
 # currently lib/logstash/runner.rb and lib/pluginmanager/main.rb are called using this.
 if $0 == __FILE__
   LogStash::Bundler.setup!({:without => [:build, :development]})
+  require_relative "patches/jar_dependencies"
+
   require ARGV.shift
-  # TODO deprecate these arguments in the next major version. use -i only
-  if ARGV == ["irb"] || ARGV == ["pry"]
-    puts "Warn: option \"#{ARGV.first}\" is deprecated, use \"-i #{ARGV.first}\" or \"--interactive=#{ARGV.first}\" instead"
-    exit_status = LogStash::Runner.run("bin/logstash", ["--interactive", ARGV.first])
-  else
-    exit_status = LogStash::Runner.run("bin/logstash", ARGV)
-  end
+  exit_status = LogStash::Runner.run("bin/logstash", ARGV)
   exit(exit_status || 0)
 end
diff --git a/lib/bootstrap/patches/gems.rb b/lib/bootstrap/patches/gems.rb
new file mode 100644
index 00000000000..94154a51345
--- /dev/null
+++ b/lib/bootstrap/patches/gems.rb
@@ -0,0 +1,16 @@
+# encoding: utf-8
+require "gems"
+
+# This patch is necessary to avoid encoding problems when Net:HTTP return stuff in ASCII format, but
+# consumer libraries, like the YAML parsers expect them to be in UTF-8. As we're using UTF-8 everywhere
+# and the usage of versions is minimal in our codebase, the patch is done here. If extended usage of this
+# is done in the feature, more proper fix should be implemented, including the creation of our own lib for
+# this tasks.
+module Gems
+  module Request
+    def get(path, data = {}, content_type = 'application/x-www-form-urlencoded', request_host = host)
+      request(:get, path, data, content_type, request_host).force_encoding("UTF-8")
+    end
+  end
+end
+
diff --git a/lib/bootstrap/patches/jar_dependencies.rb b/lib/bootstrap/patches/jar_dependencies.rb
new file mode 100644
index 00000000000..2908ab73a82
--- /dev/null
+++ b/lib/bootstrap/patches/jar_dependencies.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require "jar_dependencies"
+
+def require_jar( *args )
+  return nil unless Jars.require?
+  result = Jars.require_jar( *args )
+  if result.is_a? String
+    # JAR_DEBUG=1 will now show theses
+    Jars.debug { "--- jar coordinate #{args[0..-2].join( ':' )} already loaded with version #{result} - omit version #{args[-1]}" }
+    Jars.debug { "    try to load from #{caller.join("\n\t")}" }
+    return false
+  end
+  Jars.debug { "    register #{args.inspect} - #{result == true}" }
+  result
+end
diff --git a/lib/bootstrap/rspec.rb b/lib/bootstrap/rspec.rb
index f32057c7f9c..d24c9559553 100755
--- a/lib/bootstrap/rspec.rb
+++ b/lib/bootstrap/rspec.rb
@@ -1,12 +1,14 @@
 # encoding: utf-8
 require_relative "environment"
 LogStash::Bundler.setup!({:without => [:build]})
+require "logstash-core"
 require "logstash/environment"
 
 $LOAD_PATH.unshift(File.join(LogStash::Environment::LOGSTASH_CORE, "spec"))
 
 require "rspec/core"
 require "rspec"
+require 'ci/reporter/rake/rspec_loader'
 
 status = RSpec::Core::Runner.run(ARGV.empty? ? ["spec"] : ARGV).to_i
 exit status if status != 0
diff --git a/lib/bootstrap/rubygems.rb b/lib/bootstrap/rubygems.rb
index 06e1775c380..f11b792e7e3 100644
--- a/lib/bootstrap/rubygems.rb
+++ b/lib/bootstrap/rubygems.rb
@@ -43,6 +43,16 @@ def self.reset
       end
     end
 
+    ##
+    # Take a plugin name and get the latest versions available in the gem repository.
+    # @param [String] The plugin name
+    # @return [Hash] The collection of registered versions
+    ##
+    def versions(plugin)
+      require "gems"
+      require_relative "patches/gems"
+      Gems.versions(plugin)
+    end
     # Take a gem package and extract it to a specific target
     # @param [String] Gem file, this must be a path
     # @param [String, String] Return a Gem::Package and the installed path
diff --git a/lib/bootstrap/util/compress.rb b/lib/bootstrap/util/compress.rb
index 79bd38461b4..1d9f04b2248 100644
--- a/lib/bootstrap/util/compress.rb
+++ b/lib/bootstrap/util/compress.rb
@@ -18,13 +18,13 @@ module Zip
       # @param source [String] The location of the file to extract
       # @param target [String] Where you do want the file to be extracted
       # @raise [IOError] If the target directory already exist
-      def extract(source, target)
+      def extract(source, target, pattern = nil)
         raise CompressError.new("Directory #{target} exist") if ::File.exist?(target)
         ::Zip::File.open(source) do |zip_file|
           zip_file.each do |file|
             path = ::File.join(target, file.name)
             FileUtils.mkdir_p(::File.dirname(path))
-            zip_file.extract(file, path)
+            zip_file.extract(file, path) if pattern.nil? || pattern =~ file.name
           end
         end
       end
diff --git a/lib/pluginmanager/bundler/logstash_injector.rb b/lib/pluginmanager/bundler/logstash_injector.rb
new file mode 100644
index 00000000000..45dcc83a594
--- /dev/null
+++ b/lib/pluginmanager/bundler/logstash_injector.rb
@@ -0,0 +1,89 @@
+# encoding: utf-8
+require "bootstrap/environment"
+require "bundler"
+require "bundler/definition"
+require "bundler/dependency"
+require "bundler/dsl"
+require "bundler/injector"
+require "bundler/shared_helpers"
+require "pluginmanager/gemfile"
+require "pathname"
+
+
+# This class cannot be in the logstash namespace, because of the way the DSL
+# class interact with the other libraries
+module Bundler
+  module SharedHelpers
+    def default_bundle_dir
+      Pathname.new(LogStash::Environment::LOGSTASH_HOME)
+    end
+  end
+end
+
+module Bundler
+  class LogstashInjector < ::Bundler::Injector
+    def self.inject!(new_deps, options = { :gemfile => LogStash::Environment::GEMFILE, :lockfile => LogStash::Environment::LOCKFILE })
+      gemfile = options.delete(:gemfile)
+      lockfile = options.delete(:lockfile)
+
+      bundler_format = new_deps.plugins.collect(&method(:dependency))
+      dependencies = new_deps.dependencies.collect(&method(:dependency))
+
+      injector = new(bundler_format)
+
+      # Some of the internal classes requires to be inside the LOGSTASH_HOME to find the relative
+      # path of the core gems.
+      Dir.chdir(LogStash::Environment::LOGSTASH_HOME) do
+        injector.inject(gemfile, lockfile, dependencies)
+      end
+    end
+
+    def self.dependency(plugin)
+      ::Bundler::Dependency.new(plugin.name, "=#{plugin.version}")
+    end
+
+    # This class is pretty similar to what bundler's injector class is doing
+    # but we only accept a local resolution of the dependencies instead of calling rubygems.
+    # so we removed `definition.resolve_remotely!`
+    #
+    # And managing the gemfile is down by using our own Gemfile parser, this allow us to
+    # make it work with gems that are already defined in the gemfile.
+    def inject(gemfile_path, lockfile_path, dependencies)
+      if Bundler.settings[:frozen]
+        # ensure the lock and Gemfile are synced
+        Bundler.definition.ensure_equivalent_gemfile_and_lockfile(true)
+        # temporarily remove frozen while we inject
+        frozen = Bundler.settings.delete(:frozen)
+      end
+
+      builder = Dsl.new
+      gemfile = LogStash::Gemfile.new(File.new(gemfile_path, "r+")).load
+
+      begin
+        @new_deps.each do |dependency|
+          gemfile.update(dependency.name, dependency.requirement)
+        end
+
+        # If the dependency is defined in the gemfile, lets try to update the version with the one we have
+        # with the pack.
+        dependencies.each do |dependency|
+          if gemfile.defined_in_gemfile?(dependency.name)
+            gemfile.update(dependency.name, dependency.requirement)
+          end
+        end
+
+        builder.eval_gemfile("bundler file", gemfile.generate())
+        definition = builder.to_definition(lockfile_path, {})
+        definition.lock(lockfile_path)
+        gemfile.save
+      rescue => e
+        # the error should be handled elsewhere but we need to get the original file if we dont
+        # do this logstash will be in an inconsistent state
+        gemfile.restore!
+        raise e
+      end
+    ensure
+      Bundler.settings[:frozen] = "1" if frozen
+    end
+  end
+end
diff --git a/lib/pluginmanager/bundler/logstash_uninstall.rb b/lib/pluginmanager/bundler/logstash_uninstall.rb
new file mode 100644
index 00000000000..bdaef0dbac6
--- /dev/null
+++ b/lib/pluginmanager/bundler/logstash_uninstall.rb
@@ -0,0 +1,89 @@
+# encoding: utf-8
+require "bootstrap/environment"
+require "bundler"
+require "bundler/definition"
+require "bundler/dependency"
+require "bundler/dsl"
+require "bundler/injector"
+require "pluginmanager/gemfile"
+
+# This class cannot be in the logstash namespace, because of the way the DSL
+# class interact with the other libraries
+module Bundler
+  class LogstashUninstall
+    attr_reader :gemfile_path, :lockfile_path
+
+    def initialize(gemfile_path, lockfile_path)
+      @gemfile_path = gemfile_path
+      @lockfile_path = lockfile_path
+    end
+
+    # To be uninstalled the candidate gems need to be standalone.
+    def dependants_gems(gem_name)
+      builder = Dsl.new
+      builder.eval_gemfile("original gemfile", File.read(gemfile_path))
+      definition = builder.to_definition(lockfile_path, {})
+
+      definition.specs
+        .select { |spec| spec.dependencies.collect(&:name).include?(gem_name) }
+        .collect(&:name).sort.uniq
+    end
+
+    def uninstall!(gem_name)
+      unfreeze_gemfile do
+
+        dependencies_from = dependants_gems(gem_name)
+
+        if dependencies_from.size > 0
+          display_cant_remove_message(gem_name, dependencies_from)
+          false
+        else
+          remove_gem(gem_name)
+          true
+        end
+      end
+    end
+
+    def remove_gem(gem_name)
+      builder = Dsl.new
+      file = File.new(gemfile_path, "r+")
+
+      gemfile = LogStash::Gemfile.new(file).load
+      gemfile.remove(gem_name)
+      builder.eval_gemfile("gemfile to changes", gemfile.generate)
+
+      definition = builder.to_definition(lockfile_path, {})
+      definition.lock(lockfile_path)
+      gemfile.save
+
+      LogStash::PluginManager.ui.info("Successfully removed #{gem_name}")
+    ensure
+      file.close if file
+    end
+
+    def display_cant_remove_message(gem_name, dependencies_from)
+        message =<<-eos
+Failed to remove \"#{gem_name}\" because the following plugins or libraries depend on it:
+
+* #{dependencies_from.join("\n* ")}
+        eos
+        LogStash::PluginManager.ui.info(message)
+    end
+
+    def unfreeze_gemfile
+      if Bundler.settings[:frozen]
+        Bundler.definition.ensure_equivalent_gemfile_and_lockfile(true)
+        frozen = Bundler.settings.delete(:frozen)
+      end
+      yield
+    ensure
+      Bundler.settings[:frozen] = "1" if frozen
+    end
+
+    def self.uninstall!(gem_name, options = { :gemfile => LogStash::Environment::GEMFILE, :lockfile => LogStash::Environment::LOCKFILE })
+      gemfile_path = options[:gemfile]
+      lockfile_path = options[:lockfile]
+      LogstashUninstall.new(gemfile_path, lockfile_path).uninstall!(gem_name)
+    end
+  end
+end
diff --git a/lib/pluginmanager/command.rb b/lib/pluginmanager/command.rb
index 4adc46544c1..fa4d40dc8fc 100644
--- a/lib/pluginmanager/command.rb
+++ b/lib/pluginmanager/command.rb
@@ -6,7 +6,7 @@ def gemfile
 
   # If set in debug mode we will raise an exception and display the stacktrace
   def report_exception(readable_message, exception)
-    if ENV["DEBUG"]
+    if debug?
       raise exception
     else
       signal_error("#{readable_message}, message: #{exception.message}")
@@ -14,14 +14,13 @@ def report_exception(readable_message, exception)
   end
 
   def display_bundler_output(output)
-    if ENV['DEBUG'] && output
+    if debug? && output
       # Display what bundler did in the last run
       $stderr.puts("Bundler output")
       $stderr.puts(output)
     end
   end
 
-
   # Each plugin install for a gemfile create a path with a unique id.
   # we must clear what is not currently used in the 
   def remove_unused_locally_installed_gems!
@@ -36,4 +35,8 @@ def relative_path(path)
     require "pathname"
     ::Pathname.new(path).relative_path_from(::Pathname.new(LogStash::Environment::LOGSTASH_HOME)).to_s
   end
+
+  def debug?
+    ENV["DEBUG"]
+  end
 end
diff --git a/lib/pluginmanager/custom_gem_indexer.rb b/lib/pluginmanager/custom_gem_indexer.rb
new file mode 100644
index 00000000000..1bfd86e241f
--- /dev/null
+++ b/lib/pluginmanager/custom_gem_indexer.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+require "stud/temporary"
+
+module LogStash module PluginManager
+  class CustomGemIndexer
+    GEMS_DIR = "gems"
+
+    class << self
+      # Copy the file to a specific format that `Gem::Indexer` can understand
+      # See `#update_in_memory_index`
+      def copy_to_local_source(temporary_directory)
+        local_source = Stud::Temporary.pathname
+        local_source_gems = ::File.join(local_source, GEMS_DIR)
+
+        FileUtils.mkdir_p(local_source_gems)
+        PluginManager.ui.debug("Creating the index structure format from #{temporary_directory} to #{local_source}")
+
+        Dir.glob(::File.join(temporary_directory, "**", "*.gem")).each do |file|
+          destination = ::File.join(local_source_gems, ::File.basename(file))
+          FileUtils.cp(file, destination)
+        end
+
+        local_source
+      end
+
+      # *WARNING*: Bundler need to not be activated at this point because it won't find anything that
+      # is not defined in the gemfile/lock combo
+      #
+      # This takes a folder with a special structure, will generate an index
+      # similar to what rubygems do and make them available in the local program,
+      # we use this **side effect** to validate theses gems with the current gemfile/lock.
+      # Bundler will assume they are system gems and will use them when doing resolution checks.
+      #
+      #.
+      # â”œâ”€â”€ gems
+      # â”‚Â Â  â”œâ”€â”€ addressable-2.4.0.gem
+      # â”‚Â Â  â”œâ”€â”€ cabin-0.9.0.gem
+      # â”‚Â Â  â”œâ”€â”€ ffi-1.9.14-java.gem
+      # â”‚Â Â  â”œâ”€â”€ gemoji-1.5.0.gem
+      # â”‚Â Â  â”œâ”€â”€ launchy-2.4.3-java.gem
+      # â”‚Â Â  â”œâ”€â”€ logstash-output-elasticsearch-5.2.0-java.gem
+      # â”‚Â Â  â”œâ”€â”€ logstash-output-secret-0.1.0.gem
+      # â”‚Â Â  â”œâ”€â”€ manticore-0.6.0-java.gem
+      # â”‚Â Â  â”œâ”€â”€ spoon-0.0.6.gem
+      # â”‚Â Â  â””â”€â”€ stud-0.0.22.gem
+      #
+      # Right now this work fine, but I think we could also use Bundler's SourceList classes to handle the same thing
+      def update_in_memory_index!(local_source)
+        PluginManager.ui.debug("Generating indexes in #{local_source}")
+        indexer = ::Gem::Indexer.new(local_source, { :build_modern => true})
+        indexer.ui = ::Gem::SilentUI.new unless ENV["DEBUG"]
+        indexer.generate_index
+      end
+
+      def index(path)
+        local_source = copy_to_local_source(path)
+        update_in_memory_index!(local_source)
+        local_source
+      end
+    end
+  end
+end end
diff --git a/lib/pluginmanager/errors.rb b/lib/pluginmanager/errors.rb
new file mode 100644
index 00000000000..691c8b67500
--- /dev/null
+++ b/lib/pluginmanager/errors.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+module LogStash module PluginManager
+    class PluginManagerError < StandardError; end
+    class PluginNotFoundError < PluginManagerError; end
+    class UnpackablePluginError < PluginManagerError; end
+    class FileNotFoundError < PluginManagerError; end
+    class InvalidPackError < PluginManagerError; end
+    class InstallError < PluginManagerError
+      attr_reader :original_exception
+
+      def initialize(original_exception)
+        @original_exception = original_exception
+      end
+    end
+end end
diff --git a/lib/pluginmanager/gem_installer.rb b/lib/pluginmanager/gem_installer.rb
new file mode 100644
index 00000000000..abb686d50db
--- /dev/null
+++ b/lib/pluginmanager/gem_installer.rb
@@ -0,0 +1,92 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+require "pathname"
+require "rubygems/package"
+require "fileutils"
+
+module LogStash module PluginManager
+  # Install a physical gem package to the appropriate location inside logstash
+  # - Extract the gem
+  # - Generate the specifications
+  # - Copy the data in the right folders
+  class GemInstaller
+    GEM_HOME = Pathname.new(::File.join(LogStash::Environment::BUNDLE_DIR, "jruby", "1.9"))
+    SPECIFICATIONS_DIR = "specifications"
+    GEMS_DIR = "gems"
+    CACHE_DIR = "cache"
+
+    attr_reader :gem_home
+
+    def initialize(gem_file, display_post_install_message = false, gem_home = GEM_HOME)
+      @gem_file = gem_file
+      @gem = ::Gem::Package.new(@gem_file)
+      @gem_home = Pathname.new(gem_home)
+      @display_post_install_message = display_post_install_message
+    end
+
+    def install
+      create_destination_folders
+      extract_files
+      write_specification
+      display_post_install_message
+      copy_gem_file_to_cache
+    end
+
+    def self.install(gem_file, display_post_install_message = false, gem_home = GEM_HOME)
+      self.new(gem_file, display_post_install_message, gem_home).install
+    end
+
+    private
+    def spec
+      @gem.spec
+    end
+
+    def spec_dir
+      gem_home.join(SPECIFICATIONS_DIR)
+    end
+
+    def cache_dir
+      gem_home.join(CACHE_DIR)
+    end
+
+    def spec_file
+      spec_dir.join("#{spec.full_name}.gemspec")
+    end
+
+    def gem_dir
+      gem_home.join(GEMS_DIR, spec.full_name)
+    end
+
+    def extract_files
+      @gem.extract_files gem_dir
+    end
+
+    def write_specification
+      ::File.open(spec_file, 'w') do |file|
+        spec.installed_by_version = ::Gem.rubygems_version
+        file.puts spec.to_ruby_for_cache
+        file.fsync rescue nil # Force writing to disk
+      end
+    end
+
+    def display_post_install_message
+      PluginManager.ui.info(spec.post_install_message) if display_post_install_message?
+    end
+
+    def display_post_install_message?
+      @display_post_install_message && !spec.post_install_message.nil?
+    end
+
+    def copy_gem_file_to_cache
+      destination = ::File.join(cache_dir, ::File.basename(@gem_file))
+      FileUtils.cp(@gem_file, destination)
+    end
+
+    def create_destination_folders
+      FileUtils.mkdir_p(gem_home)
+      FileUtils.mkdir_p(gem_dir)
+      FileUtils.mkdir_p(spec_dir)
+      FileUtils.mkdir_p(cache_dir)
+    end
+  end
+end end
diff --git a/lib/pluginmanager/gemfile.rb b/lib/pluginmanager/gemfile.rb
index b1648187764..86af9a40826 100644
--- a/lib/pluginmanager/gemfile.rb
+++ b/lib/pluginmanager/gemfile.rb
@@ -15,9 +15,9 @@ def initialize(io)
       @gemset = nil
     end
 
-    def load
+    def load(with_backup = true)
       @gemset ||= DSL.parse(@io.read)
-      backup
+      backup if with_backup
       self
     end
 
@@ -25,11 +25,14 @@ def save
       raise(GemfileError, "a Gemfile must first be loaded") unless @gemset
       @io.truncate(0)
       @io.rewind
-      @io.write(HEADER)
-      @io.write(@gemset.to_s)
+      @io.write(generate)
       @io.flush
     end
 
+    def generate
+      "#{HEADER}#{gemset.to_s}"
+    end
+
     def find(name)
       @gemset.find_gem(name)
     end
@@ -41,13 +44,20 @@ def add(name, *requirements)
       @gemset.add_gem(Gem.parse(name, *requirements))
     end
 
-    # update existing or add new
+    # update existing or add new and merge passed options with current gem options if it exists
     # @param name [String] gem name
     # @param *requirements params following name use the same notation as the Gemfile gem DSL statement
     def update(name, *requirements)
       @gemset.update_gem(Gem.parse(name, *requirements))
     end
 
+    # overwrite existing or add new
+    # @param name [String] gem name
+    # @param *requirements params following name use the same notation as the Gemfile gem DSL statement
+    def overwrite(name, *requirements)
+      @gemset.overwrite_gem(Gem.parse(name, *requirements))
+    end
+
     # @return [Gem] removed gem or nil if not found
     def remove(name)
       @gemset.remove_gem(name)
@@ -61,6 +71,10 @@ def restore
       @gemset = @original_backup
     end
 
+    def defined_in_gemfile?(name)
+      @gemset.find_gem(name)
+    end
+
     def restore!
       restore
       save
@@ -99,6 +113,19 @@ def add_gem(_gem)
 
     # update existing or add new
     def update_gem(_gem)
+      if old = find_gem(_gem.name)
+        # always overwrite requirements if specified
+        old.requirements = _gem.requirements unless no_constrains?(_gem.requirements)
+        # but merge options
+        old.options = old.options.merge(_gem.options)
+      else
+        @gems << _gem
+        @gems_by_name[_gem.name.downcase] = _gem
+      end
+    end
+
+    # update existing or add new
+    def overwrite_gem(_gem)
       if old = find_gem(_gem.name)
         @gems[@gems.index(old)] = _gem
       else
@@ -119,8 +146,19 @@ def remove_gem(name)
     def copy
       Marshal.load(Marshal.dump(self))
     end
+
     private
 
+    def no_constrains?(requirements)
+      return true if requirements.nil? || requirements.empty?
+
+      # check for the dummy ">= 0" version constrain or any variations thereof
+      # which is in fact a "no constrain" constrain which we should discard
+      return true if requirements.size == 1 && requirements.first.to_s.gsub(/\s+/, "") == ">=0"
+
+      false
+    end
+
     def sources_to_s
       return "" if @sources.empty?
       @sources.map{|source| "source #{source.inspect}"}.join("\n")
diff --git a/lib/pluginmanager/generate.rb b/lib/pluginmanager/generate.rb
new file mode 100644
index 00000000000..6717682e021
--- /dev/null
+++ b/lib/pluginmanager/generate.rb
@@ -0,0 +1,92 @@
+# encoding: utf-8
+require "pluginmanager/command"
+require "pluginmanager/templates/render_context"
+require "erb"
+require "ostruct"
+require "fileutils"
+require "pathname"
+
+class LogStash::PluginManager::Generate < LogStash::PluginManager::Command
+
+  TYPES = [ "input", "filter", "output", "codec" ]
+
+  option "--type", "TYPE", "Type of the plugin {input, filter, codec, output}s", :required => true
+  option "--name", "PLUGIN", "Name of the new plugin", :required => true
+  option "--path", "PATH", "Location where the plugin skeleton will be created", :default => Dir.pwd
+
+  def execute
+    validate_params
+    source = File.join(File.dirname(__FILE__), "templates", "#{type}-plugin")
+    @target_path = File.join(path, full_plugin_name)
+    FileUtils.mkdir(@target_path)
+    puts " Creating #{@target_path}"
+
+    begin
+      create_scaffold(source, @target_path)
+    rescue Errno::EACCES => exception
+      report_exception("Permission denied when executing the plugin manager", exception)
+    rescue => exception
+      report_exception("Plugin creation Aborted", exception)
+    end
+  end
+
+  private
+
+  def validate_params
+    raise(ArgumentError, "should be one of: input, filter, codec or output") unless TYPES.include?(type)
+  end
+
+  def create_scaffold(source, target)
+    transform_r(source, target)
+  end
+
+  def transform_r(source, target)
+    Dir.entries(source).each do |entry|
+      next if [ ".", ".." ].include?(entry)
+      source_entry = File.join(source, entry)
+      target_entry = File.join(target, entry)
+
+      if File.directory?(source_entry)
+        FileUtils.mkdir(target_entry) unless File.exists?(target_entry)
+        transform_r(source_entry, target_entry)
+      else
+        # copy the new file, in case of being an .erb file should render first
+        if source_entry.end_with?("erb")
+          target_entry = target_entry.gsub(/.erb$/,"").gsub("example", name)
+          File.open(target_entry, "w") { |f| f.write(render(source_entry)) }
+        else
+          FileUtils.cp(source_entry, target_entry)
+        end
+        puts "\t create #{File.join(full_plugin_name, Pathname.new(target_entry).relative_path_from(Pathname.new(@target_path)))}"
+      end
+    end
+  end
+
+  def render(source)
+    template = File.read(source)
+    renderer = ERB.new(template)
+    context  = LogStash::PluginManager::RenderContext.new(options)
+    renderer.result(context.get_binding)
+  end
+
+  def options
+    git_data = get_git_info
+    @options ||= {
+      :plugin_name => name,
+      :author => git_data.author,
+      :email  => git_data.email,
+      :min_version => "2.0",
+    }
+  end
+
+  def get_git_info
+    git = OpenStruct.new
+    git.author = %x{ git config --get user.name  }.strip rescue "your_username"
+    git.email  = %x{ git config --get user.email }.strip rescue "your_username@example.com"
+    git
+  end
+
+  def full_plugin_name
+    @full_plugin_name ||= "logstash-#{type}-#{name.downcase}"
+  end
+end
diff --git a/lib/pluginmanager/install.rb b/lib/pluginmanager/install.rb
index dae85e3dfc5..476b00ec7be 100644
--- a/lib/pluginmanager/install.rb
+++ b/lib/pluginmanager/install.rb
@@ -1,5 +1,8 @@
 # encoding: utf-8
 require "pluginmanager/command"
+require "pluginmanager/install_strategy_factory"
+require "pluginmanager/ui"
+require "pluginmanager/errors"
 require "jar-dependencies"
 require "jar_install_post_install_hook"
 require "file-dependencies/gem"
@@ -9,6 +12,7 @@ class LogStash::PluginManager::Install < LogStash::PluginManager::Command
   parameter "[PLUGIN] ...", "plugin name(s) or file", :attribute_name => :plugins_arg
   option "--version", "VERSION", "version of the plugin to install"
   option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
+  option "--preserve", :flag, "preserve current gem options", :default => false
   option "--development", :flag, "install all development dependencies of currently installed plugins", :default => false
   option "--local", :flag, "force local-only plugin installation. see bin/logstash-plugin package|unpack", :default => false
 
@@ -16,6 +20,33 @@ class LogStash::PluginManager::Install < LogStash::PluginManager::Command
   # but the argument parsing does not support it for now so currently if specifying --version only
   # one plugin name can be also specified.
   def execute
+    # Turn off any jar dependencies lookup when running with `--local`
+    ENV["JARS_SKIP"] = "true" if local?
+
+    # This is a special flow for PACK related plugins,
+    # if we dont detect an pack we will just use the normal `Bundle install` Strategy`
+    # this could be refactored into his own strategy
+    begin
+      if strategy = LogStash::PluginManager::InstallStrategyFactory.create(plugins_arg)
+        LogStash::PluginManager.ui.debug("Installing with strategy: #{strategy.class}")
+        strategy.execute
+        return
+      end
+    rescue LogStash::PluginManager::InstallError => e
+      report_exception("An error occured when installing the: #{plugins_args_human}, to have more information about the error add a DEBUG=1 before running the command.", e.original_exception)
+      return
+    rescue LogStash::PluginManager::FileNotFoundError => e
+      report_exception("File not found for: #{plugins_args_human}", e)
+      return
+    rescue LogStash::PluginManager::InvalidPackError => e
+      report_exception("Invalid pack for: #{plugins_args_human}, reason: #{e.message}", e)
+      return
+    rescue => e
+      report_exception("Something went wrong when installing #{plugins_args_human}", e)
+      return
+    end
+
+    # TODO(ph): refactor this into his own strategy
     validate_cli_options!
 
     if local_gems?
@@ -90,7 +121,15 @@ def install_gems_list!(install_list)
 
     # Add plugins/gems to the current gemfile
     puts("Installing" + (install_list.empty? ? "..." : " " + install_list.collect(&:first).join(", ")))
-    install_list.each { |plugin, version, options| gemfile.update(plugin, version, options) }
+    install_list.each do |plugin, version, options|
+      if preserve?
+        plugin_gem = gemfile.find(plugin)
+        puts("Preserving Gemfile gem options for plugin #{plugin}") if plugin_gem && !plugin_gem.options.empty?
+        gemfile.update(plugin, version, options)
+      else
+        gemfile.overwrite(plugin, version, options)
+      end
+    end
 
     # Sync gemfiles changes to disk to make them available to the `bundler install`'s API
     gemfile.save
@@ -119,6 +158,7 @@ def install_gems_list!(install_list)
   # Bundler 2.0, will have support for plugins source we could create a .gem source
   # to support it.
   def extract_local_gems_plugins
+    FileUtils.mkdir_p(LogStash::Environment::CACHE_PATH)
     plugins_arg.collect do |plugin|
       # We do the verify before extracting the gem so we dont have to deal with unused path
       if verify?
@@ -126,6 +166,9 @@ def extract_local_gems_plugins
         signal_error("Installation aborted, verification failed for #{plugin}") unless LogStash::PluginManager.logstash_plugin?(plugin, version)
       end
 
+      # Make the original .gem available for the prepare-offline-pack,
+      # paquet will lookup in the cache directory before going to rubygems.
+      FileUtils.cp(plugin, ::File.join(LogStash::Environment::CACHE_PATH, ::File.basename(plugin)))
       package, path = LogStash::Rubygems.unpack(plugin, LogStash::Environment::LOCAL_GEM_PATH)
       [package.spec.name, package.spec.version, { :path => relative_path(path) }]
     end
@@ -143,4 +186,8 @@ def local_gems?
       signal_usage_error("Mixed source of plugins, you can't mix local `.gem` and remote gems")
     end
   end
+
+  def plugins_args_human
+    plugins_arg.join(", ")
+  end
 end # class Logstash::PluginManager
diff --git a/lib/pluginmanager/install_strategy_factory.rb b/lib/pluginmanager/install_strategy_factory.rb
new file mode 100644
index 00000000000..3901492f692
--- /dev/null
+++ b/lib/pluginmanager/install_strategy_factory.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+require "pluginmanager/pack_fetch_strategy/repository"
+require "pluginmanager/pack_fetch_strategy/uri"
+
+module LogStash module PluginManager
+  class InstallStrategyFactory
+    AVAILABLES_STRATEGIES = [
+      LogStash::PluginManager::PackFetchStrategy::Uri,
+      LogStash::PluginManager::PackFetchStrategy::Repository
+    ]
+
+    def self.create(plugins_args)
+      plugin_name_or_uri = plugins_args.first
+      return false if plugin_name_or_uri.nil? || plugin_name_or_uri.strip.empty?
+
+      AVAILABLES_STRATEGIES.each do |strategy|
+        if installer = strategy.get_installer_for(plugin_name_or_uri)
+          return installer
+        end
+      end
+      return false
+    end
+  end
+end end
diff --git a/lib/pluginmanager/list.rb b/lib/pluginmanager/list.rb
index 65f28c31cc0..90c900d9d76 100644
--- a/lib/pluginmanager/list.rb
+++ b/lib/pluginmanager/list.rb
@@ -9,7 +9,7 @@ class LogStash::PluginManager::List < LogStash::PluginManager::Command
   option "--installed", :flag, "List only explicitly installed plugins using bin/logstash-plugin install ...", :default => false
   option "--verbose", :flag, "Also show plugin version number", :default => false
   option "--group", "NAME", "Filter plugins per group: input, output, filter or codec" do |arg|
-    raise(ArgumentError, "should be one of: input, output, filter or codec") unless ['input', 'output', 'filter', 'codec'].include?(arg)
+    raise(ArgumentError, "should be one of: input, output, filter or codec") unless ['input', 'output', 'filter', 'codec', 'pack'].include?(arg)
     arg
   end
 
diff --git a/lib/pluginmanager/main.rb b/lib/pluginmanager/main.rb
index 15841b107e3..598df0d38fe 100644
--- a/lib/pluginmanager/main.rb
+++ b/lib/pluginmanager/main.rb
@@ -15,23 +15,30 @@ module PluginManager
 require "pluginmanager/util"
 require "pluginmanager/gemfile"
 require "pluginmanager/install"
-require "pluginmanager/uninstall"
+require "pluginmanager/remove"
 require "pluginmanager/list"
 require "pluginmanager/update"
 require "pluginmanager/pack"
 require "pluginmanager/unpack"
+require "pluginmanager/generate"
+require "pluginmanager/prepare_offline_pack"
+require "pluginmanager/proxy_support"
+configure_proxy
 
 module LogStash
   module PluginManager
     class Error < StandardError; end
 
     class Main < Clamp::Command
-      subcommand "install", "Install a plugin", LogStash::PluginManager::Install
-      subcommand "uninstall", "Uninstall a plugin", LogStash::PluginManager::Uninstall
+      subcommand "list", "List all installed Logstash plugins", LogStash::PluginManager::List
+      subcommand "install", "Install a Logstash plugin", LogStash::PluginManager::Install
+      subcommand "remove", "Remove a Logstash plugin", LogStash::PluginManager::Remove
       subcommand "update", "Update a plugin", LogStash::PluginManager::Update
-      subcommand "pack", "Package currently installed plugins", LogStash::PluginManager::Pack
-      subcommand "unpack", "Unpack packaged plugins", LogStash::PluginManager::Unpack
-      subcommand "list", "List all installed plugins", LogStash::PluginManager::List
+      subcommand "pack", "Package currently installed plugins, Deprecated: Please use prepare-offline-pack instead", LogStash::PluginManager::Pack
+      subcommand "unpack", "Unpack packaged plugins, Deprecated: Please use prepare-offline-pack instead", LogStash::PluginManager::Unpack
+      subcommand "generate", "Create the foundation for a new plugin", LogStash::PluginManager::Generate
+      subcommand "uninstall", "Uninstall a plugin. Deprecated: Please use remove instead", LogStash::PluginManager::Remove
+      subcommand "prepare-offline-pack", "Create an archive of specified plugins to use for offline installation", LogStash::PluginManager::PrepareOfflinePack
     end
   end
 end
diff --git a/lib/pluginmanager/offline_plugin_packager.rb b/lib/pluginmanager/offline_plugin_packager.rb
new file mode 100644
index 00000000000..c4cd6cf24d6
--- /dev/null
+++ b/lib/pluginmanager/offline_plugin_packager.rb
@@ -0,0 +1,118 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+require "pluginmanager/errors"
+require "bootstrap/environment"
+require "bootstrap/util/compress"
+require "paquet"
+require "stud/temporary"
+require "fileutils"
+
+module LogStash module PluginManager
+  class SpecificationHelpers
+    WILDCARD = "*"
+    WILDCARD_INTO_RE = ".*"
+
+    def self.find_by_name_with_wildcards(pattern)
+      re = transform_pattern_into_re(pattern)
+      ::Gem::Specification.find_all.select do |specification|
+        specification.name =~ re
+      end
+    end
+
+    def self.transform_pattern_into_re(pattern)
+      Regexp.new("^#{pattern.gsub(WILDCARD, WILDCARD_INTO_RE)}$")
+    end
+  end
+
+  class OfflinePluginPackager
+    LOGSTASH_DIR = "logstash"
+    DEPENDENCIES_DIR = ::File.join(LOGSTASH_DIR, "dependencies")
+
+    # To make sure we have the maximum compatibility
+    # we will ignore theses gems and they won't be included in the pack
+    IGNORE_GEMS_IN_PACK = %w(
+      logstash-core
+      logstash-core-plugin-api
+      jar-dependencies
+    )
+
+    INVALID_PLUGINS_TO_EXPLICIT_PACK = IGNORE_GEMS_IN_PACK.collect { |name| /^#{name}/ } + [
+      /mixin/
+    ]
+
+    def initialize(plugins_to_package, target)
+      @plugins_to_package = Array(plugins_to_package)
+      @target = target
+
+      validate_plugins!
+    end
+
+    def validate_plugins!
+      @plugins_to_package.each do |plugin_name|
+        if INVALID_PLUGINS_TO_EXPLICIT_PACK.any? { |invalid_name| plugin_name =~ invalid_name }
+          raise UnpackablePluginError, "Cannot explicitly pack `#{plugin_name}` for offline installation"
+        end
+      end
+    end
+
+    def generate_temporary_path
+      Stud::Temporary.pathname
+    end
+
+    def explicitly_declared_plugins_specs
+      @plugins_to_package.collect do |plugin_pattern|
+        specs = SpecificationHelpers.find_by_name_with_wildcards(plugin_pattern)
+
+        if specs.size > 0
+          specs
+        else
+          raise LogStash::PluginManager::PluginNotFoundError, "Cannot find plugins matching: `#{plugin_pattern}`"
+        end
+      end.flatten
+    end
+
+    def execute
+      temp_path = generate_temporary_path
+      packet_gem = Paquet::Gem.new(temp_path, LogStash::Environment::CACHE_PATH)
+
+      explicit_plugins_specs = explicitly_declared_plugins_specs
+
+      explicit_plugins_specs.each do |spec|
+        packet_gem.add(spec.name)
+      end
+
+      IGNORE_GEMS_IN_PACK.each do |gem_name|
+        packet_gem.ignore(gem_name)
+      end
+
+      packet_gem.pack
+
+      prepare_package(explicit_plugins_specs, temp_path)
+      LogStash::Util::Zip.compress(temp_path, @target)
+    ensure
+      FileUtils.rm_rf(temp_path)
+    end
+
+    def prepare_package(explicit_plugins, temp_path)
+      FileUtils.mkdir_p(::File.join(temp_path, LOGSTASH_DIR))
+      FileUtils.mkdir_p(::File.join(temp_path, DEPENDENCIES_DIR))
+
+      explicit_path = ::File.join(temp_path, LOGSTASH_DIR)
+      dependencies_path = ::File.join(temp_path, DEPENDENCIES_DIR)
+
+      Dir.glob(::File.join(temp_path, "*.gem")).each do |gem_file|
+        filename = ::File.basename(gem_file)
+
+        if explicit_plugins.any? { |spec| filename =~ /^#{spec.name}/ }
+          FileUtils.mv(gem_file, ::File.join(explicit_path, filename))
+        else
+          FileUtils.mv(gem_file, ::File.join(dependencies_path, filename))
+        end
+      end
+    end
+
+    def self.package(plugins_args, target)
+      OfflinePluginPackager.new(plugins_args, target).execute
+    end
+  end
+end end
diff --git a/lib/pluginmanager/pack.rb b/lib/pluginmanager/pack.rb
index 18b46e18511..2f00728199d 100644
--- a/lib/pluginmanager/pack.rb
+++ b/lib/pluginmanager/pack.rb
@@ -8,6 +8,8 @@ class LogStash::PluginManager::Pack < LogStash::PluginManager::PackCommand
   option "--overwrite", :flag, "Overwrite a previously generated package file", :default => false
 
   def execute
+    signal_deprecation_warning_for_pack
+
     puts("Packaging plugins for offline usage")
 
     validate_target_file
diff --git a/lib/pluginmanager/pack_command.rb b/lib/pluginmanager/pack_command.rb
index 2409b212f97..3f4ed0b383c 100644
--- a/lib/pluginmanager/pack_command.rb
+++ b/lib/pluginmanager/pack_command.rb
@@ -10,4 +10,12 @@ def archive_manager
   def file_extension
     zip? ? ".zip" : ".tar.gz"
   end
+
+  def signal_deprecation_warning_for_pack
+  message =<<-EOS
+The pack and the unpack command are now deprecated and will be removed in a future version of Logstash.
+See the `prepare-offline-pack` to update your workflow. You can get documentation about this by running `bin/logstash-plugin prepare-offline-pack --help`
+  EOS
+  puts message
+  end
 end
diff --git a/lib/pluginmanager/pack_fetch_strategy/repository.rb b/lib/pluginmanager/pack_fetch_strategy/repository.rb
new file mode 100644
index 00000000000..52d1e3cee34
--- /dev/null
+++ b/lib/pluginmanager/pack_fetch_strategy/repository.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+# In the context of the plugin manager no dependencies are currently loaded.
+# So we have to manually require the version file
+require_relative "../../../logstash-core/lib/logstash/version"
+require "pluginmanager/pack_installer/remote"
+require "pluginmanager/utils/http_client"
+require "pluginmanager/ui"
+require "net/http"
+require "uri"
+
+module LogStash module PluginManager module PackFetchStrategy
+  class Repository
+    DEFAULT_PACK_URL = "https://artifacts.elastic.co/downloads/logstash-plugins"
+    PACK_EXTENSION = "zip"
+
+    class << self
+      def elastic_pack_base_uri
+        env_url = ENV["LOGSTASH_PACK_URL"]
+        (env_url.nil? || env_url.empty?) ? DEFAULT_PACK_URL : env_url
+      end
+
+      def pack_uri(plugin_name)
+        url = "#{elastic_pack_base_uri}/#{plugin_name}/#{plugin_name}-#{LOGSTASH_VERSION}.#{PACK_EXTENSION}"
+        URI.parse(url)
+      end
+
+      def get_installer_for(plugin_name)
+        uri = pack_uri(plugin_name)
+
+        PluginManager.ui.debug("Looking if package named: #{plugin_name} exists at #{uri}")
+
+        if Utils::HttpClient.remote_file_exist?(uri)
+          PluginManager.ui.debug("Found package at: #{uri}")
+          return LogStash::PluginManager::PackInstaller::Remote.new(uri)
+        else
+          PluginManager.ui.debug("Package not found at: #{uri}")
+          return nil
+        end
+      rescue SocketError, Errno::ECONNREFUSED, Errno::EHOSTUNREACH => e
+        # This probably means there is a firewall in place of the proxy is not correctly configured.
+        # So lets skip this strategy but log a meaningful errors.
+        PluginManager.ui.debug("Network error, skipping Elastic pack, exception: #{e}")
+
+        return nil
+      end
+    end
+  end
+end end end
diff --git a/lib/pluginmanager/pack_fetch_strategy/uri.rb b/lib/pluginmanager/pack_fetch_strategy/uri.rb
new file mode 100644
index 00000000000..9dfc19828b9
--- /dev/null
+++ b/lib/pluginmanager/pack_fetch_strategy/uri.rb
@@ -0,0 +1,44 @@
+# encoding: utf-8
+require "pluginmanager/utils/http_client"
+require "pluginmanager/pack_installer/local"
+require "pluginmanager/pack_installer/remote"
+require "pluginmanager/ui"
+require "net/http"
+require "uri"
+
+module LogStash module PluginManager module PackFetchStrategy
+  class Uri
+    class << self
+      def get_installer_for(plugin_name)
+        begin
+          uri =  URI.parse(plugin_name)
+
+          if local?(uri)
+            PluginManager.ui.debug("Local file: #{uri.path}")
+            return LogStash::PluginManager::PackInstaller::Local.new(uri.path)
+          elsif http?(uri)
+            PluginManager.ui.debug("Remote file: #{uri}")
+            return LogStash::PluginManager::PackInstaller::Remote.new(uri)
+          else
+            return nil
+          end
+        rescue URI::InvalidURIError,
+          URI::InvalidComponentError,
+          URI::BadURIError => e
+
+          PluginManager.ui.debug("Invalid URI for pack, uri: #{uri}")
+          return nil
+        end
+      end
+
+      private
+      def http?(uri)
+        !uri.scheme.nil? && uri.scheme.match(/^http/)
+      end
+
+      def local?(uri)
+        !uri.scheme.nil? && uri.scheme == "file"
+      end
+    end
+  end
+end end end
diff --git a/lib/pluginmanager/pack_installer/local.rb b/lib/pluginmanager/pack_installer/local.rb
new file mode 100644
index 00000000000..8912cf9f76d
--- /dev/null
+++ b/lib/pluginmanager/pack_installer/local.rb
@@ -0,0 +1,67 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+require "pluginmanager/bundler/logstash_injector"
+require "pluginmanager/gem_installer"
+require "pluginmanager/custom_gem_indexer"
+require "pluginmanager/errors"
+require "pluginmanager/pack_installer/pack"
+require "bootstrap/util/compress"
+require "rubygems/indexer"
+
+module LogStash module PluginManager module PackInstaller
+  class Local
+    PACK_EXTENSION = ".zip"
+    LOGSTASH_PATTERN_RE = /logstash\/?/
+
+    attr_reader :local_file
+
+    def initialize(local_file)
+      @local_file = local_file
+    end
+
+    def execute
+      raise PluginManager::FileNotFoundError, "Can't file local file #{local_file}" unless ::File.exist?(local_file)
+      raise PluginManager::InvalidPackError, "Invalid format, the pack must be in zip format" unless valid_format?(local_file)
+
+      PluginManager.ui.info("Installing file: #{local_file}")
+      uncompressed_path = uncompress(local_file)
+      PluginManager.ui.debug("Pack uncompressed to #{uncompressed_path}")
+      pack = LogStash::PluginManager::PackInstaller::Pack.new(uncompressed_path)
+      raise PluginManager::InvalidPackError, "The pack must contains at least one plugin" unless pack.valid?
+
+      local_source = LogStash::PluginManager::CustomGemIndexer.index(uncompressed_path)
+
+      # Try to add the gems to the current gemfile and lock file, if successful
+      # both of them will be updated. This injector is similar to Bundler's own injector class
+      # minus the support for additionals source and doing local resolution only.
+      ::Bundler::LogstashInjector.inject!(pack)
+
+      # When successful its safe to install the gem and their specifications in the bundle directory
+      pack.gems.each do |packed_gem|
+        PluginManager.ui.debug("Installing, #{packed_gem.name}, version: #{packed_gem.version} file: #{packed_gem.file}")
+        LogStash::PluginManager::GemInstaller::install(packed_gem.file, packed_gem.plugin?)
+      end
+      PluginManager.ui.info("Install successful")
+    rescue ::Bundler::BundlerError => e
+      raise PluginManager::InstallError.new(e), "An error occurred went installing plugins"
+    ensure
+      FileUtils.rm_rf(uncompressed_path) if uncompressed_path && Dir.exist?(uncompressed_path)
+      FileUtils.rm_rf(local_source) if local_source && Dir.exist?(local_source)
+    end
+
+    private
+    def uncompress(source)
+      temporary_directory = Stud::Temporary.pathname
+      LogStash::Util::Zip.extract(source, temporary_directory, LOGSTASH_PATTERN_RE)
+      temporary_directory
+    rescue Zip::Error => e
+      # OK Zip's handling of file is bit weird, if the file exist but is not a valid zip, it will raise
+      # a `Zip::Error` exception with a file not found message...
+      raise InvalidPackError, "Cannot uncompress the zip: #{source}"
+    end
+
+    def valid_format?(local_file)
+      ::File.extname(local_file).downcase == PACK_EXTENSION
+    end
+  end
+end end end
diff --git a/lib/pluginmanager/pack_installer/pack.rb b/lib/pluginmanager/pack_installer/pack.rb
new file mode 100644
index 00000000000..3090472dbf2
--- /dev/null
+++ b/lib/pluginmanager/pack_installer/pack.rb
@@ -0,0 +1,81 @@
+# encoding: utf-8
+require "pluginmanager/errors"
+
+module LogStash module PluginManager module PackInstaller
+  # A object that represent the directory structure
+  # related to the actual gems in the extracted package.
+  #
+  # Example of a valid structure, where `logstash-output-secret` is the actual
+  # plugin to be installed.
+  #.
+  # â”œâ”€â”€ dependencies
+  # â”‚Â Â  â”œâ”€â”€ addressable-2.4.0.gem
+  # â”‚Â Â  â”œâ”€â”€ cabin-0.9.0.gem
+  # â”‚Â Â  â”œâ”€â”€ ffi-1.9.14-java.gem
+  # â”‚Â Â  â”œâ”€â”€ gemoji-1.5.0.gem
+  # â”‚Â Â  â”œâ”€â”€ launchy-2.4.3-java.gem
+  # â”‚Â Â  â”œâ”€â”€ logstash-output-elasticsearch-5.2.0-java.gem
+  # â”‚Â Â  â”œâ”€â”€ manticore-0.6.0-java.gem
+  # â”‚Â Â  â”œâ”€â”€ spoon-0.0.6.gem
+  # â”‚Â Â  â””â”€â”€ stud-0.0.22.gem
+  # â””â”€â”€ logstash-output-secret-0.1.0.gem
+  class Pack
+    class GemInformation
+      EXTENSION = ".gem"
+      SPLIT_CHAR = "-"
+      JAVA_PLATFORM_RE = /-java/
+      DEPENDENCIES_DIR_RE = /dependencies/
+
+      attr_reader :file, :name, :version, :platform
+
+      def initialize(gem)
+        @file = gem
+        extracts_information
+      end
+
+      def dependency?
+        @dependency
+      end
+
+      def plugin?
+        !dependency?
+      end
+
+      private
+      def extracts_information
+        basename = ::File.basename(file, EXTENSION)
+        parts = basename.split(SPLIT_CHAR)
+
+        @dependency = ::File.dirname(file) =~ DEPENDENCIES_DIR_RE
+
+        if basename.match(JAVA_PLATFORM_RE)
+          @platform = parts.pop
+          @version = parts.pop
+          @name = parts.join(SPLIT_CHAR)
+        else
+          @platform = nil
+          @version = parts.pop
+          @name = parts.join(SPLIT_CHAR)
+        end
+      end
+    end
+
+    attr_reader :gems
+
+    def initialize(source)
+      @gems = Dir.glob(::File.join(source, "**", "*.gem")).collect { |gem| GemInformation.new(gem) }
+    end
+
+    def plugins
+      gems.select { |gem| !gem.dependency? }
+    end
+
+    def dependencies
+      gems.select { |gem| gem.dependency? }
+    end
+
+    def valid?
+      plugins.size > 0
+    end
+  end
+end end end
diff --git a/lib/pluginmanager/pack_installer/remote.rb b/lib/pluginmanager/pack_installer/remote.rb
new file mode 100644
index 00000000000..37cb52973b5
--- /dev/null
+++ b/lib/pluginmanager/pack_installer/remote.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require "pluginmanager/pack_installer/local"
+require "pluginmanager/utils/downloader"
+require "fileutils"
+
+module LogStash module PluginManager module PackInstaller
+  class Remote
+    attr_reader :remote_url, :feedback
+
+    def initialize(remote_url, feedback = Utils::Downloader::ProgressbarFeedback)
+      @remote_url = remote_url
+      @feedback = feedback
+    end
+
+    def execute
+      PluginManager.ui.info("Downloading file: #{remote_url}")
+      downloaded_file = Utils::Downloader.fetch(remote_url, feedback)
+      PluginManager.ui.debug("Downloaded package to: #{downloaded_file}")
+
+      PackInstaller::Local.new(downloaded_file).execute
+    ensure
+      FileUtils.rm_rf(downloaded_file) if downloaded_file
+    end
+  end
+end end end
diff --git a/lib/pluginmanager/prepare_offline_pack.rb b/lib/pluginmanager/prepare_offline_pack.rb
new file mode 100644
index 00000000000..0860c3b602b
--- /dev/null
+++ b/lib/pluginmanager/prepare_offline_pack.rb
@@ -0,0 +1,76 @@
+# encoding: utf-8
+require "pluginmanager/command"
+require "pluginmanager/errors"
+
+class LogStash::PluginManager::PrepareOfflinePack < LogStash::PluginManager::Command
+  parameter "[PLUGIN] ...", "plugin name(s)", :attribute_name => :plugins_arg
+  option "--output", "OUTPUT", "output zip file", :default => ::File.join(LogStash::Environment::LOGSTASH_HOME, "logstash-offline-plugins-#{LOGSTASH_VERSION}.zip")
+  option "--overwrite", :flag, "overwrite a previously generated package file", :default => false
+
+  def execute
+    validate_arguments!
+
+    # We need to start bundler, dependencies so  the plugins are available for the prepare
+    LogStash::Bundler.setup!({:without => [:build, :development]})
+
+    # manually require paquet since its an external dependency
+    require "pluginmanager/offline_plugin_packager"
+    require "paquet"
+    require "paquet/shell_ui"
+
+    # Override the shell output with the one from the plugin manager
+    # To silence some of debugs/info statements
+    Paquet.ui = Paquet::SilentUI unless debug?
+
+    if File.directory?(output)
+      signal_error("Package creation cancelled: The specified output is a directory, you must specify a filename with a zip extension, provided output: #{output}.")
+    else
+      if File.extname(output).downcase != ".zip"
+        signal_error("Package creation cancelled: You must specify the zip extension for the provided filename: #{output}.")
+      end
+
+      if ::File.exists?(output)
+        if overwrite?
+          File.delete(output)
+        else
+          signal_error("Package creation cancelled: output file destination #{output} already exists.")
+        end
+      end
+    end
+
+    LogStash::PluginManager::OfflinePluginPackager.package(plugins_arg, output)
+
+    message = <<-EOS
+Offline package created at: #{output}
+
+You can install it with this command `bin/logstash-plugin install file://#{::File.expand_path(output)}`
+    EOS
+
+    LogStash::PluginManager::ui.info(message)
+  rescue LogStash::PluginManager::UnpackablePluginError => e
+    report_exception("Offline package", e)
+  rescue LogStash::PluginManager::PluginNotFoundError => e
+    report_exception("Cannot create the offline archive", e)
+  end
+
+  def validate_arguments!
+    if plugins_arg.size == 0
+      message = <<-EOS
+You need to specify at least one plugin or use a wildcard expression.
+
+Examples:
+bin/logstash-plugin prepare-offline-pack logstash-input-beats
+bin/logstash-plugin prepare-offline-pack logstash-filter-jdbc logstash-input-beats
+bin/logstash-plugin prepare-offline-pack logstash-filter-*
+bin/logstash-plugin prepare-offline-pack logstash-filter-* logstash-input-beats
+
+You can get a list of the installed plugin by running `bin/logstash-plugin list`
+
+The prepare offline will pack the currently installed plugins and their dependencies
+for offline installation.
+EOS
+      signal_usage_error(message)
+    end
+  end
+end
+
diff --git a/lib/pluginmanager/proxy_support.rb b/lib/pluginmanager/proxy_support.rb
new file mode 100644
index 00000000000..e148ede2b7a
--- /dev/null
+++ b/lib/pluginmanager/proxy_support.rb
@@ -0,0 +1,99 @@
+# encoding: utf-8
+require "uri"
+require "java"
+require "erb"
+require "ostruct"
+require "fileutils"
+require "stud/temporary"
+require "jar-dependencies"
+
+
+# This is a bit of a hack, to make sure that all of our call pass to a specific proxies.
+# We do this before any jar-dependences check is done, meaning we have to silence him.
+module Jars
+  def self.warn(message)
+    if ENV["debug"]
+      puts message
+    end
+  end
+end
+
+SETTINGS_TEMPLATE = ::File.join(::File.dirname(__FILE__), "settings.xml.erb")
+SETTINGS_TARGET = ::File.join(Dir.home, ".m2")
+
+class ProxyTemplateData
+  attr_reader :proxies
+
+  def initialize(proxies)
+    @proxies = proxies.collect { |proxy| OpenStruct.new(proxy) }
+  end
+
+  def get_binding
+    binding
+  end
+end
+
+# Apply HTTP_PROXY and HTTPS_PROXY to the current environment
+# this will be used by any JRUBY calls
+def apply_env_proxy_settings(settings)
+  scheme = settings[:protocol].downcase
+  java.lang.System.setProperty("#{scheme}.proxyHost", settings[:host])
+  java.lang.System.setProperty("#{scheme}.proxyPort", settings[:port].to_s)
+  java.lang.System.setProperty("#{scheme}.proxyUsername", settings[:username].to_s)
+  java.lang.System.setProperty("#{scheme}.proxyPassword", settings[:password].to_s)
+end
+
+def extract_proxy_values_from_uri(proxy_uri)
+  proxy_uri = URI(proxy_uri)
+  {
+    :protocol => proxy_uri.scheme,
+    :host => proxy_uri.host,
+    :port => proxy_uri.port,
+    :username => proxy_uri.user,
+    :password => proxy_uri.password
+  }
+end
+
+def get_proxy(key)
+  ENV[key.downcase] || ENV[key.upcase]
+end
+
+def valid_proxy?(proxy)
+  !proxy.nil? && !proxy.strip.empty?
+end
+
+def configure_proxy
+  proxies = []
+  proxy = get_proxy("http_proxy")
+  if valid_proxy?(proxy)
+    proxy_settings = extract_proxy_values_from_uri(proxy)
+    proxy_settings[:protocol] = "http"
+    apply_env_proxy_settings(proxy_settings)
+    proxies << proxy_settings
+  end
+
+  proxy = get_proxy("https_proxy")
+  if valid_proxy?(proxy)
+    proxy_settings = extract_proxy_values_from_uri(proxy)
+    proxy_settings[:protocol] = "https"
+    apply_env_proxy_settings(proxy_settings)
+    proxies << proxy_settings
+  end
+
+  # I've tried overriding jar dependency environment variable to declare the settings but it doesn't seems to work.
+  # I am not sure if its because of our current setup or its a bug in the library.
+  if !proxies.empty?
+    FileUtils.mkdir_p(SETTINGS_TARGET)
+    target = ::File.join(SETTINGS_TARGET, "settings.xml")
+    template = ::File.read(SETTINGS_TEMPLATE)
+    template_content = ERB.new(template, 3).result(ProxyTemplateData.new(proxies).get_binding)
+
+    if ::File.exist?(target)
+      if template_content != ::File.read(target)
+        puts "WARNING: A maven settings file already exist at #{target}, please review the content to make sure it include your proxies configuration."
+      end
+    else
+      ::File.open(target, "w") { |f| f.write(template_content) }
+    end
+  end
+end
diff --git a/lib/pluginmanager/remove.rb b/lib/pluginmanager/remove.rb
new file mode 100644
index 00000000000..6c2efebbbcd
--- /dev/null
+++ b/lib/pluginmanager/remove.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require "pluginmanager/bundler/logstash_uninstall"
+require "pluginmanager/command"
+
+class LogStash::PluginManager::Remove < LogStash::PluginManager::Command
+  parameter "PLUGIN", "plugin name"
+
+  def execute
+    signal_error("File #{LogStash::Environment::GEMFILE_PATH} does not exist or is not writable, aborting") unless File.writable?(LogStash::Environment::GEMFILE_PATH)
+
+    ##
+    # Need to setup the bundler status to enable uninstall of plugins
+    # installed as local_gems, otherwise gem:specification is not
+    # finding the plugins
+    ##
+    LogStash::Bundler.setup!({:without => [:build, :development]})
+
+    # make sure this is an installed plugin and present in Gemfile.
+    # it is not possible to uninstall a dependency not listed in the Gemfile, for example a dependent codec
+    signal_error("This plugin has not been previously installed") unless LogStash::PluginManager.installed_plugin?(plugin, gemfile)
+
+    exit(1) unless ::Bundler::LogstashUninstall.uninstall!(plugin)
+
+    remove_unused_locally_installed_gems!
+  rescue => exception
+    report_exception("Operation aborted, cannot remove plugin.", exception)
+  end
+end
diff --git a/lib/pluginmanager/settings.xml.erb b/lib/pluginmanager/settings.xml.erb
new file mode 100644
index 00000000000..43df773d91b
--- /dev/null
+++ b/lib/pluginmanager/settings.xml.erb
@@ -0,0 +1,18 @@
+<settings xmlns="http://maven.apache.org/SETTINGS/1.0.0"
+  xmlns:xsi="http://www.w3.org/2001/XMLSchema-instance"
+  xsi:schemaLocation="http://maven.apache.org/SETTINGS/1.0.0
+  https://maven.apache.org/xsd/settings-1.0.0.xsd">
+  <proxies>
+    <% proxies.each_with_index do |proxy, idx| %>
+    <proxy>
+      <id><%=proxy.host%>.<%=idx%></id>
+      <active>true</active>
+      <protocol><%=proxy.protocol%></protocol>
+      <host><%=proxy.host%></host>
+      <port><%=proxy.port%></port>
+      <username><%=proxy.username%></username>
+      <password><%=proxy.password%></password>
+    </proxy>
+  <% end %>
+  </proxies>
+</settings>
diff --git a/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..654a05b6614
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-codec-<%= plugin_name %>
+Example codec plugin. This should help bootstrap your effort to write your own codec plugin!
diff --git a/lib/pluginmanager/templates/codec-plugin/Gemfile b/lib/pluginmanager/templates/codec-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/codec-plugin/LICENSE b/lib/pluginmanager/templates/codec-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/codec-plugin/README.md b/lib/pluginmanager/templates/codec-plugin/README.md
new file mode 100644
index 00000000000..a75e88df936
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-codec-awesome", :path => "/your/local/logstash-codec-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'codec {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-codec-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-codec-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/codec-plugin/Rakefile b/lib/pluginmanager/templates/codec-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
new file mode 100644
index 00000000000..91ed93785b8
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
@@ -0,0 +1,45 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/namespace"
+
+# This <%= @plugin_name %> codec will append a string to the message field
+# of an event, either in the decoding or encoding methods
+#
+# This is only intended to be used as an example.
+#
+# input {
+#   stdin { codec => <%= @plugin_name %> }
+# }
+#
+# or
+#
+# output {
+#   stdout { codec => <%= @plugin_name %> }
+# }
+#
+class LogStash::Codecs::<%= classify(plugin_name) %> < LogStash::Codecs::Base
+
+  # The codec name
+  config_name "<%= plugin_name %>"
+
+  # Append a string to the message
+  config :append, :validate => :string, :default => ', Hello World!'
+
+  def register
+    @lines = LogStash::Codecs::Line.new
+    @lines.charset = "UTF-8"
+  end # def register
+
+  def decode(data)
+    @lines.decode(data) do |line|
+      replace = { "message" => line.get("message").to_s + @append }
+      yield LogStash::Event.new(replace)
+    end
+  end # def decode
+
+  # Encode a single event, this returns the raw data to be returned as a String
+  def encode_sync(event)
+    event.get("message").to_s + @append + NL
+  end # def encode_sync
+
+end # class LogStash::Codecs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
new file mode 100644
index 00000000000..91e1b0600f1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-codec-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "codec" }
+
+  # Gem dependencies
+  s.add_runtime_dependency 'logstash-core-plugin-api', "~> <%= min_version %>"
+  s.add_runtime_dependency 'logstash-codec-line'
+  s.add_development_dependency 'logstash-devutils'
+end
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
new file mode 100644
index 00000000000..48cca741ab2
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
@@ -0,0 +1,3 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require "logstash/codecs/<%= plugin_name %>"
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
new file mode 100644
index 00000000000..dc64aba12c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
diff --git a/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md b/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..6b18c6221de
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-filter-<%= plugin_name %>
+Example filter plugin. This should help bootstrap your effort to write your own filter plugin!
diff --git a/lib/pluginmanager/templates/filter-plugin/Gemfile b/lib/pluginmanager/templates/filter-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/filter-plugin/LICENSE b/lib/pluginmanager/templates/filter-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/filter-plugin/README.md b/lib/pluginmanager/templates/filter-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/filter-plugin/Rakefile b/lib/pluginmanager/templates/filter-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb b/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb
new file mode 100644
index 00000000000..ca5d9f7ca3b
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# This <%= @plugin_name %> filter will replace the contents of the default 
+# message field with whatever you specify in the configuration.
+#
+# It is only intended to be used as an <%= @plugin_name %>.
+class LogStash::Filters::<%= classify(plugin_name) %> < LogStash::Filters::Base
+
+  # Setting the config_name here is required. This is how you
+  # configure this filter from your Logstash config.
+  #
+  # filter {
+  #   <%= @plugin_name %> {
+  #     message => "My message..."
+  #   }
+  # }
+  #
+  config_name "<%= plugin_name %>"
+  
+  # Replace the message with this value.
+  config :message, :validate => :string, :default => "Hello World!"
+  
+
+  public
+  def register
+    # Add instance variables 
+  end # def register
+
+  public
+  def filter(event)
+
+    if @message
+      # Replace the event message with our message as configured in the
+      # config file.
+      event.set("message", @message)
+    end
+
+    # filter_matched should go in the last line of our successful code
+    filter_matched(event)
+  end # def filter
+end # class LogStash::Filters::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb b/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb
new file mode 100644
index 00000000000..5f910dc40fb
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb
@@ -0,0 +1,23 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-filter-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_development_dependency 'logstash-devutils'
+end
diff --git a/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb b/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb
new file mode 100644
index 00000000000..5dceafe7fc4
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require "logstash/filters/<%= plugin_name %>"
+
+describe LogStash::Filters::<%= classify(plugin_name) %> do
+  describe "Set to Hello World" do
+    let(:config) do <<-CONFIG
+      filter {
+        <%= plugin_name %> {
+          message => "Hello World"
+        }
+      }
+    CONFIG
+    end
+
+    sample("message" => "some text") do
+      expect(subject).to include("message")
+      expect(subject.get('message')).to eq('Hello World')
+    end
+  end
+end
diff --git a/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb b/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb
new file mode 100644
index 00000000000..dc64aba12c1
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
diff --git a/lib/pluginmanager/templates/input-plugin/CHANGELOG.md b/lib/pluginmanager/templates/input-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..eca3db404e8
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-input-<%= plugin_name %>
+Example input plugin. This should help bootstrap your effort to write your own input plugin!
diff --git a/lib/pluginmanager/templates/input-plugin/Gemfile b/lib/pluginmanager/templates/input-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/input-plugin/LICENSE b/lib/pluginmanager/templates/input-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/input-plugin/README.md b/lib/pluginmanager/templates/input-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/input-plugin/Rakefile b/lib/pluginmanager/templates/input-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb b/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb
new file mode 100644
index 00000000000..176467ccb5c
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "stud/interval"
+require "socket" # for Socket.gethostname
+
+# Generate a repeating message.
+#
+# This plugin is intented only as an example.
+
+class LogStash::Inputs::<%= classify(plugin_name) %> < LogStash::Inputs::Base
+  config_name "<%= @plugin_name %>"
+
+  # If undefined, Logstash will complain, even if codec is unused.
+  default :codec, "plain"
+
+  # The message string to use in the event.
+  config :message, :validate => :string, :default => "Hello World!"
+
+  # Set how frequently messages should be sent.
+  #
+  # The default, `1`, means send a message every second.
+  config :interval, :validate => :number, :default => 1
+
+  public
+  def register
+    @host = Socket.gethostname
+  end # def register
+
+  def run(queue)
+    # we can abort the loop if stop? becomes true
+    while !stop?
+      event = LogStash::Event.new("message" => @message, "host" => @host)
+      decorate(event)
+      queue << event
+      # because the sleep interval can be big, when shutdown happens
+      # we want to be able to abort the sleep
+      # Stud.stoppable_sleep will frequently evaluate the given block
+      # and abort the sleep(@interval) if the return value is true
+      Stud.stoppable_sleep(@interval) { stop? }
+    end # loop
+  end # def run
+
+  def stop
+    # nothing to do in this case so it is not necessary to define stop
+    # examples of common "stop" tasks:
+    #  * close sockets (unblocking blocking reads/accepts)
+    #  * cleanup temporary files
+    #  * terminate spawned threads
+  end
+end # class LogStash::Inputs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb b/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb
new file mode 100644
index 00000000000..9f8543887b2
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb
@@ -0,0 +1,25 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-input-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = '{TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "input" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_runtime_dependency 'logstash-codec-plain'
+  s.add_runtime_dependency 'stud', '>= 0.0.22'
+  s.add_development_dependency 'logstash-devutils', '>= 0.0.16'
+end
diff --git a/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb b/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb
new file mode 100644
index 00000000000..7b8bfde8ea3
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb
@@ -0,0 +1,11 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/inputs/<%= plugin_name %>"
+
+describe LogStash::Inputs::<%= classify(plugin_name) %> do
+
+  it_behaves_like "an interruptible input plugin" do
+    let(:config) { { "interval" => 100 } }
+  end
+
+end
diff --git a/lib/pluginmanager/templates/output-plugin/CHANGELOG.md b/lib/pluginmanager/templates/output-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..2593de38fc7
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-output-<%= plugin_name %>
+Example output plugin. This should help bootstrap your effort to write your own output plugin!
diff --git a/lib/pluginmanager/templates/output-plugin/Gemfile b/lib/pluginmanager/templates/output-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/output-plugin/LICENSE b/lib/pluginmanager/templates/output-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/output-plugin/README.md b/lib/pluginmanager/templates/output-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/output-plugin/Rakefile b/lib/pluginmanager/templates/output-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb b/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb
new file mode 100644
index 00000000000..eadd499bf98
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# An <%= plugin_name %> output that does nothing.
+class LogStash::Outputs::<%= classify(plugin_name) %> < LogStash::Outputs::Base
+  config_name "<%= plugin_name %>"
+
+  public
+  def register
+  end # def register
+
+  public
+  def receive(event)
+    return "Event received"
+  end # def event
+end # class LogStash::Outputs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb b/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb
new file mode 100644
index 00000000000..db396e1ff1c
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-output-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "output" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_runtime_dependency "logstash-codec-plain"
+  s.add_development_dependency "logstash-devutils"
+end
diff --git a/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb b/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb
new file mode 100644
index 00000000000..220d967bd63
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/outputs/<%= plugin_name %>"
+require "logstash/codecs/plain"
+require "logstash/event"
+
+describe LogStash::Outputs::<%= classify(plugin_name) %> do
+  let(:sample_event) { LogStash::Event.new }
+  let(:output) { LogStash::Outputs::<%= classify(plugin_name) %>.new }
+
+  before do
+    output.register
+  end
+
+  describe "receive message" do
+    subject { output.receive(sample_event) }
+
+    it "returns a string" do
+      expect(subject).to eq("Event received")
+    end
+  end
+end
diff --git a/lib/pluginmanager/templates/render_context.rb b/lib/pluginmanager/templates/render_context.rb
new file mode 100644
index 00000000000..583c6a9ee07
--- /dev/null
+++ b/lib/pluginmanager/templates/render_context.rb
@@ -0,0 +1,20 @@
+require "erb"
+
+module LogStash::PluginManager
+  class RenderContext
+    def initialize(options = {})
+      options.each do |name, value|
+        define_singleton_method(name) { value }
+      end
+    end
+
+    def get_binding
+      binding()
+    end
+
+    def classify(klass_name)
+      klass_name.split(/-|_/).map { |e| e.capitalize }.join("")
+    end
+
+  end
+end
diff --git a/lib/pluginmanager/ui.rb b/lib/pluginmanager/ui.rb
new file mode 100644
index 00000000000..1f40265cf3c
--- /dev/null
+++ b/lib/pluginmanager/ui.rb
@@ -0,0 +1,24 @@
+# encoding: utf-8
+module LogStash module PluginManager
+  # The command line commands should be able to report but they shouldn't
+  # require an explicit logger like log4j.
+  class Shell
+    def info(message)
+      puts message
+    end
+    alias_method :error, :info
+    alias_method :warn, :info
+
+    def debug(message)
+      puts message if ENV["DEBUG"]
+    end
+  end
+
+  def self.ui
+    @ui ||= Shell.new
+  end
+
+  def self.ui=(new_ui)
+    @ui = new_ui
+  end
+end end
diff --git a/lib/pluginmanager/uninstall.rb b/lib/pluginmanager/uninstall.rb
deleted file mode 100644
index e7598a4ebc9..00000000000
--- a/lib/pluginmanager/uninstall.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# encoding: utf-8
-require "pluginmanager/command"
-
-class LogStash::PluginManager::Uninstall < LogStash::PluginManager::Command
-
-  parameter "PLUGIN", "plugin name"
-
-  def execute
-    signal_error("File #{LogStash::Environment::GEMFILE_PATH} does not exist or is not writable, aborting") unless File.writable?(LogStash::Environment::GEMFILE_PATH)
-
-    ##
-    # Need to setup the bundler status to enable uninstall of plugins
-    # installed as local_gems, otherwise gem:specification is not
-    # finding the plugins
-    ##
-    LogStash::Bundler.setup!({:without => [:build, :development]})
-
-    # make sure this is an installed plugin and present in Gemfile.
-    # it is not possible to uninstall a dependency not listed in the Gemfile, for example a dependent codec
-    signal_error("This plugin has not been previously installed, aborting") unless LogStash::PluginManager.installed_plugin?(plugin, gemfile)
-
-    # since we previously did a gemfile.find(plugin) there is no reason why
-    # remove would not work (return nil) here
-    if gemfile.remove(plugin)
-      gemfile.save
-
-      puts("Uninstalling #{plugin}")
-
-      # any errors will be logged to $stderr by invoke!
-      # output, exception = LogStash::Bundler.invoke!(:install => true, :clean => true)
-      output = LogStash::Bundler.invoke!(:install => true, :clean => true)
-
-      remove_unused_locally_installed_gems!
-    end
-  rescue => exception
-    gemfile.restore!
-    report_exception("Uninstall Aborted", exception)
-  ensure
-    display_bundler_output(output)
-  end
-end
diff --git a/lib/pluginmanager/unpack.rb b/lib/pluginmanager/unpack.rb
index 7937e7d2e24..f1f7221e171 100644
--- a/lib/pluginmanager/unpack.rb
+++ b/lib/pluginmanager/unpack.rb
@@ -8,6 +8,8 @@ class LogStash::PluginManager::Unpack < LogStash::PluginManager::PackCommand
   parameter "file", "the package file name", :attribute_name => :package_file, :required => true
 
   def execute
+    signal_deprecation_warning_for_pack
+
     puts("Unpacking #{package_file}")
 
     FileUtils.rm_rf(LogStash::Environment::CACHE_PATH)
diff --git a/lib/pluginmanager/update.rb b/lib/pluginmanager/update.rb
index fd840e91183..583b6c954e2 100644
--- a/lib/pluginmanager/update.rb
+++ b/lib/pluginmanager/update.rb
@@ -6,22 +6,28 @@
 
 class LogStash::PluginManager::Update < LogStash::PluginManager::Command
   REJECTED_OPTIONS = [:path, :git, :github]
+  # These are local gems used by LS and needs to be filtered out of other plugin gems
+  NON_PLUGIN_LOCAL_GEMS = ["logstash-core", "logstash-core-plugin-api"]
 
   parameter "[PLUGIN] ...", "Plugin name(s) to upgrade to latest version", :attribute_name => :plugins_arg
   option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
   option "--local", :flag, "force local-only plugin update. see bin/logstash-plugin package|unpack", :default => false
 
   def execute
-    local_gems = gemfile.locally_installed_gems
+    # Turn off any jar dependencies lookup when running with `--local`
+    ENV["JARS_SKIP"] = "true" if local?
+
+    # remove "system" local gems used by LS
+    local_gems = gemfile.locally_installed_gems.map(&:name) - NON_PLUGIN_LOCAL_GEMS
 
     if local_gems.size > 0
       if update_all?
-        plugins_with_path = local_gems.map(&:name)
+        plugins_with_path = local_gems
       else
-        plugins_with_path = plugins_arg & local_gems.map(&:name)
+        plugins_with_path = plugins_arg & local_gems
       end
 
-      warn_local_gems(plugins_with_path)
+      warn_local_gems(plugins_with_path) if plugins_with_path.size > 0
     end
     update_gems!
   end
@@ -40,14 +46,13 @@ def update_gems!
     previous_gem_specs_map = find_latest_gem_specs
 
     # remove any version constrain from the Gemfile so the plugin(s) can be updated to latest version
-    # calling update without requiremend will remove any previous requirements
+    # calling update without requirements will remove any previous requirements
     plugins = plugins_to_update(previous_gem_specs_map)
     # Skipping the major version validation when using a local cache as we can have situations
     # without internet connection.
     filtered_plugins = plugins.map { |plugin| gemfile.find(plugin) }
       .compact
       .reject { |plugin| REJECTED_OPTIONS.any? { |key| plugin.options.has_key?(key) } }
-      .select { |plugin| local? || (verify? ? validates_version(plugin.name) : true) }
       .each   { |plugin| gemfile.update(plugin.name) }
 
     # force a disk sync before running bundler
@@ -60,7 +65,9 @@ def update_gems!
     options = {:update => plugins, :rubygems_source => gemfile.gemset.sources}
     options[:local] = true if local?
     output = LogStash::Bundler.invoke!(options)
-    output = LogStash::Bundler.invoke!(:clean => true)
+    # We currently dont removed unused gems from the logstash installation
+    # see: https://github.com/elastic/logstash/issues/6339
+    # output = LogStash::Bundler.invoke!(:clean => true)
     display_updated_plugins(previous_gem_specs_map)
   rescue => exception
     gemfile.restore!
@@ -69,12 +76,6 @@ def update_gems!
     display_bundler_output(output)
   end
 
-  # validate if there is any major version update so then we can ask the user if he is
-  # sure to update or not.
-  def validates_version(plugin)
-    LogStash::PluginManager.update_to_major_version?(plugin)
-  end
-
   # create list of plugins to update
   def plugins_to_update(previous_gem_specs_map)
     if update_all?
diff --git a/lib/pluginmanager/util.rb b/lib/pluginmanager/util.rb
index 149ff6256d4..a521f2bac6a 100644
--- a/lib/pluginmanager/util.rb
+++ b/lib/pluginmanager/util.rb
@@ -50,29 +50,13 @@ def self.logstash_plugin?(plugin, version = nil, options={})
   # @option options [Boolean] :pre Include pre release versions in the search (default: false)
   # @return [Hash] The plugin version information as returned by rubygems
   def self.fetch_latest_version_info(plugin, options={})
-    require "gems"
     exclude_prereleases =  options.fetch(:pre, false)
-    versions = Gems.versions(plugin)
+    versions = LogStash::Rubygems.versions(plugin)
     raise ValidationError.new("Something went wrong with the validation. You can skip the validation with the --no-verify option") if !versions.is_a?(Array) || versions.empty?
     versions = versions.select { |version| !version["prerelease"] } if !exclude_prereleases
     versions.first
   end
 
-  # Let's you decide to update to the last version of a plugin if this is a major version
-  # @param [String] A plugin name
-  # @return [Boolean] True in case the update is moving forward, false otherwise
-  def self.update_to_major_version?(plugin_name)
-    plugin_version  = fetch_latest_version_info(plugin_name)
-    latest_version  = plugin_version['number'].split(".")
-    current_version = Gem::Specification.find_by_name(plugin_name).version.version.split(".")
-    if (latest_version[0].to_i > current_version[0].to_i)
-      ## warn if users want to continue
-      puts("You are updating #{plugin_name} to a new version #{latest_version.join('.')}, which may not be compatible with #{current_version.join('.')}. are you sure you want to proceed (Y/N)?")
-      return ( "y" == STDIN.gets.strip.downcase ? true : false)
-    end
-    true
-  end
-
   # @param spec [Gem::Specification] plugin gem specification
   # @return [Boolean] true if this spec is for an installable logstash plugin
   def self.logstash_plugin_gem_spec?(spec)
@@ -93,7 +77,7 @@ def self.plugin_file?(plugin)
   end
 
   # retrieve gem specs for all or specified name valid logstash plugins locally installed
-  # @param name [String] specific plugin name to find or nil for all plungins
+  # @param name [String] specific plugin name to find or nil for all plugins
   # @return [Array<Gem::Specification>] all local logstash plugin gem specs
   def self.find_plugins_gem_specs(name = nil)
     specs = name ? Gem::Specification.find_all_by_name(name) : Gem::Specification.find_all
@@ -101,7 +85,7 @@ def self.find_plugins_gem_specs(name = nil)
   end
 
   # list of all locally installed plugins specs specified in the Gemfile.
-  # note that an installed plugin dependecies like codecs will not be listed, only those
+  # note that an installed plugin dependencies like codecs will not be listed, only those
   # specifically listed in the Gemfile.
   # @param gemfile [LogStash::Gemfile] the gemfile to validate against
   # @return [Array<Gem::Specification>] list of plugin specs
@@ -113,13 +97,13 @@ def self.all_installed_plugins_gem_specs(gemfile)
 
   # @param plugin [String] plugin name
   # @param gemfile [LogStash::Gemfile] the gemfile to validate against
-  # @return [Boolean] true if the plugin is an installed logstash plugin and spefificed in the Gemfile
+  # @return [Boolean] true if the plugin is an installed logstash plugin and specified in the Gemfile
   def self.installed_plugin?(plugin, gemfile)
     !!gemfile.find(plugin) && find_plugins_gem_specs(plugin).any?
   end
 
   # @param plugin_list [Array] array of [plugin name, version] tuples
-  # @return [Array] array of [plugin name, version, ...] tuples when duplciate names have been merged and non duplicate version requirements added
+  # @return [Array] array of [plugin name, version, ...] tuples when duplicate names have been merged and non duplicate version requirements added
   def self.merge_duplicates(plugin_list)
 
     # quick & dirty naive dedup for now
diff --git a/lib/pluginmanager/utils/downloader.rb b/lib/pluginmanager/utils/downloader.rb
new file mode 100644
index 00000000000..0d520febfaa
--- /dev/null
+++ b/lib/pluginmanager/utils/downloader.rb
@@ -0,0 +1,91 @@
+# encoding: utf-8
+require "ruby-progressbar"
+require "pluginmanager/utils/http_client"
+require "pluginmanager/errors"
+require "fileutils"
+
+module LogStash module PluginManager module Utils
+  class Downloader
+    class ProgressbarFeedback
+      FORMAT = "%t [%B] %p%%"
+      TITLE = "Downloading"
+
+      attr_reader :progress_bar
+
+      def initialize(max)
+        @progress_bar = ProgressBar.create(:title => TITLE,
+                                           :starting_at => 0,
+                                           :total => max,
+                                           :format => FORMAT)
+      end
+
+      def update(status)
+        progress_bar.progress += status
+      end
+    end
+
+    class SilentFeedback
+      def initialize(max)
+      end
+
+      def update(status)
+      end
+    end
+
+    attr_reader :download_to, :remote_file_uri, :feedback_strategy
+
+    def initialize(remote_file_uri, feedback = SilentFeedback)
+      @remote_file_uri = URI(remote_file_uri)
+      @download_to = Stud::Temporary.pathname
+      @feedback_strategy = feedback
+    end
+
+    def fetch(redirect_count = 0)
+      # This is defensive programming, but in the real world we do create redirects all the time
+      raise HttpClient::RedirectionLimit, "Too many redirection, tried #{REDIRECTION_LIMIT} times" if redirect_count >= HttpClient::REDIRECTION_LIMIT
+
+      begin
+        FileUtils.mkdir_p(download_to)
+        downloaded_file = ::File.open(::File.join(download_to, ::File.basename(remote_file_uri.path)), "wb")
+
+        HttpClient.start(remote_file_uri) do |http|
+          request = Net::HTTP::Get.new(remote_file_uri.path)
+
+          http.request(request) do |response|
+            if response.code == "200"
+              download_chunks(response, downloaded_file)
+            elsif response.code == "302"
+              new_uri = response.headers["location"]
+
+              redirect_count += 1
+              downloader = self.new(new_uri, feedback_strategy)
+              downloader.fetch(redirect_count)
+            else
+              raise LogStash::PluginManager::FileNotFoundError, "Can't download #{remote_file_uri}" if response.code != "200"
+            end
+          end
+          downloaded_file.close
+          downloaded_file.path
+        end
+      rescue => e
+        downloaded_file.close rescue nil
+        FileUtils.rm_rf(download_to)
+        raise e
+      end
+    end
+
+    def self.fetch(remote_file, feedback = SilentFeedback)
+      new(remote_file, feedback).fetch
+    end
+
+    private
+    def download_chunks(response, downloaded_file)
+      feedback = feedback_strategy.new(response.content_length)
+
+      response.read_body do |chunk|
+        feedback.update(chunk.bytesize)
+        downloaded_file.write(chunk)
+      end
+    end
+  end
+end end end
diff --git a/lib/pluginmanager/utils/http_client.rb b/lib/pluginmanager/utils/http_client.rb
new file mode 100644
index 00000000000..c84bb7caa70
--- /dev/null
+++ b/lib/pluginmanager/utils/http_client.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+module LogStash module PluginManager module Utils
+  class HttpClient
+    class RedirectionLimit < RuntimeError; end
+
+    HTTPS_SCHEME = "https"
+    REDIRECTION_LIMIT = 5
+
+    # Proxies should be handled by the library
+    # https://ruby-doc.org/stdlib-2.3.1/libdoc/net/http/rdoc/Net/HTTP.html#class-Net::HTTP-label-Proxies
+    def self.start(uri)
+      uri = URI(uri)
+      Net::HTTP.start(uri.host, uri.port, http_options(uri)) { |http| yield http }
+    end
+
+    def self.http_options(uri)
+      ssl_enabled = uri.scheme == HTTPS_SCHEME
+
+      {
+        :use_ssl => ssl_enabled
+      }
+    end
+
+    # Do a HEAD request on the file to see if it exist before downloading it
+    def self.remote_file_exist?(uri, redirect_count = 0)
+      uri = URI(uri)
+
+      # This is defensive programming, but in the real world we do create redirects all the time
+      raise RedirectionLimit, "Too many redirection, tried #{REDIRECTION_LIMIT} times" if redirect_count >= REDIRECTION_LIMIT
+
+      start(uri) do |http|
+        return false if uri.path.empty?
+
+        request = Net::HTTP::Head.new(uri.path)
+        response = http.request(request)
+
+        if response.code == "302"
+          new_uri = response.headers["location"]
+          remote_file_exist?(new_uri, redirect_count + 1)
+        elsif response.code == "200"
+          true
+        else
+          false
+        end
+      end
+    end
+  end
+end end end
diff --git a/lib/systeminstall/pleasewrap.rb b/lib/systeminstall/pleasewrap.rb
new file mode 100755
index 00000000000..f7ee00bd447
--- /dev/null
+++ b/lib/systeminstall/pleasewrap.rb
@@ -0,0 +1,12 @@
+# encoding: utf-8
+$LOAD_PATH.unshift(File.expand_path(File.join(__FILE__, "..", "..")))
+
+require "bootstrap/environment"
+
+ENV["GEM_HOME"] = ENV["GEM_PATH"] = LogStash::Environment.logstash_gem_home
+Gem.use_paths(LogStash::Environment.logstash_gem_home)
+
+#libdir = File.expand_path("../lib", File.dirname(__FILE__))
+#$LOAD_PATH << libdir if File.exist?(File.join(libdir, "pleaserun", "cli.rb"))
+require "pleaserun/cli"
+exit(PleaseRun::CLI.run || 0)
diff --git a/logstash-core-event-java/.gitignore b/logstash-core-event-java/.gitignore
deleted file mode 100644
index a453cb95034..00000000000
--- a/logstash-core-event-java/.gitignore
+++ /dev/null
@@ -1,9 +0,0 @@
-*.class
-
-# build dirs
-build
-.gradle
-
-# Intellij
-.idea
-*.iml
diff --git a/logstash-core-event-java/README.md b/logstash-core-event-java/README.md
deleted file mode 100644
index 7b12d19f135..00000000000
--- a/logstash-core-event-java/README.md
+++ /dev/null
@@ -1,63 +0,0 @@
-# logstash-core-event-java
-
-## dev install
-
-1- build code with
-
-```
-$ cd logstash-core-event-java
-$ gradle build
-```
-
-A bunch of warning are expected, it should end with:
-
-```
-BUILD SUCCESSFUL
-```
-
-2- update root logstash `Gemfile` to use this gem with:
-
-```
-# gem "logstash-core-event", "x.y.z", :path => "./logstash-core-event"
-gem "logstash-core-event-java", "x.y.z", :path => "./logstash-core-event-java"
-```
-
-3- update `logstash-core/logstash-core.gemspec` with:
-
-```
-# gem.add_runtime_dependency "logstash-core-event", "x.y.z"
-gem.add_runtime_dependency "logstash-core-event-java", "x.y.z"
-```
-
-4- and install:
-
-```
-$ bin/bundle
-```
-
-- install core plugins for tests
-
-```
-$ rake test:install-core
-```
-
-## specs
-
-```
-$ bin/rspec spec
-$ bin/rspec logstash-core/spec
-$ bin/rspec logstash-core-event/spec
-$ bin/rspec logstash-core-event-java/spec
-```
-
-or
-
-```
-$ rake test:core
-```
-
-also
-
-```
-$ rake test:plugins
-```
\ No newline at end of file
diff --git a/logstash-core-event-java/build.gradle b/logstash-core-event-java/build.gradle
deleted file mode 100644
index b2a4a55ec43..00000000000
--- a/logstash-core-event-java/build.gradle
+++ /dev/null
@@ -1,107 +0,0 @@
-buildscript {
-    repositories {
-        mavenLocal()
-        mavenCentral()
-        jcenter()
-    }
-    dependencies {
-        classpath 'net.saliman:gradle-cobertura-plugin:2.2.8'
-    }
-}
-
-repositories {
-    mavenLocal()
-    mavenCentral()
-    jcenter()
-}
-
-gradle.projectsEvaluated {
-    tasks.withType(JavaCompile) {
-        options.compilerArgs << "-Xlint:deprecation"
-//        options.compilerArgs << "-Xlint:unchecked" << "-Xlint:deprecation"
-    }
-}
-
-apply plugin: 'java'
-apply plugin: 'idea'
-
-group = 'org.logstash'
-
-project.sourceCompatibility = 1.7
-
-task sourcesJar(type: Jar, dependsOn: classes) {
-    from sourceSets.main.allSource
-    classifier 'sources'
-    extension 'jar'
-}
-
-task javadocJar(type: Jar, dependsOn: javadoc) {
-    from javadoc.destinationDir
-    classifier 'javadoc'
-    extension 'jar'
-}
-
-task copyGemjar(type: Copy, dependsOn: sourcesJar) {
-    from project.jar
-    into project.file('lib/logstash-core-event-java/')
-}
-
-task cleanGemjar {
-    delete fileTree(project.file('lib/logstash-core-event-java/')) {
-        include '*.jar'
-    }
-}
-
-clean.dependsOn(cleanGemjar)
-jar.finalizedBy(copyGemjar)
-
-configurations.create('sources')
-configurations.create('javadoc')
-configurations.archives {
-    extendsFrom configurations.sources
-    extendsFrom configurations.javadoc
-}
-
-artifacts {
-    sources(sourcesJar) {
-        // Weird Gradle quirk where type will be used for the extension, but only for sources
-        type 'jar'
-    }
-
-    javadoc(javadocJar) {
-        type 'javadoc'
-    }
-}
-
-configurations {
-    provided
-}
-
-project.sourceSets {
-    main.compileClasspath += project.configurations.provided
-    main.runtimeClasspath += project.configurations.provided
-    test.compileClasspath += project.configurations.provided
-    test.runtimeClasspath += project.configurations.provided
-}
-project.javadoc.classpath += project.configurations.provided
-
-idea {
-    module {
-        scopes.PROVIDED.plus += [project.configurations.provided]
-    }
-}
-
-dependencies {
-    compile 'com.fasterxml.jackson.core:jackson-core:2.7.1'
-    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.1-1'
-    provided 'org.jruby:jruby-core:1.7.22'
-    testCompile 'junit:junit:4.12'
-    testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
-}
-
-// See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
-task wrapper(type: Wrapper) {
-    description = 'Install Gradle wrapper'
-    gradleVersion = '2.8'
-}
-
diff --git a/logstash-core-event-java/gradle.properties b/logstash-core-event-java/gradle.properties
deleted file mode 100644
index b5cdaba6a69..00000000000
--- a/logstash-core-event-java/gradle.properties
+++ /dev/null
@@ -1 +0,0 @@
-VERSION=0.0.1-SNAPSHOT
diff --git a/logstash-core-event-java/lib/logstash-core-event-java.rb b/logstash-core-event-java/lib/logstash-core-event-java.rb
deleted file mode 100644
index 29b487aa192..00000000000
--- a/logstash-core-event-java/lib/logstash-core-event-java.rb
+++ /dev/null
@@ -1 +0,0 @@
-require "logstash-core-event-java/logstash-core-event-java"
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb b/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb
deleted file mode 100644
index cf86fec4d16..00000000000
--- a/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-# encoding: utf-8
-
-require "java"
-
-module LogStash
-end
-
-require "logstash-core-event-java_jars"
-
-# local dev setup
-classes_dir = File.expand_path("../../../build/classes/main", __FILE__)
-
-if File.directory?(classes_dir)
-  # if in local dev setup, add target to classpath
-  $CLASSPATH << classes_dir unless $CLASSPATH.include?(classes_dir)
-else
-  # otherwise use included jar
-  begin
-    require "logstash-core-event-java/logstash-core-event-java.jar"
-  rescue Exception => e
-    raise("Error loading logstash-core-event-java/logstash-core-event-java.jar file, cause: #{e.message}")
-  end
-end
-
-require "jruby_event_ext"
-require "jruby_timestamp_ext"
-require "logstash/event"
-require "logstash/timestamp"
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash-core-event-java/version.rb b/logstash-core-event-java/lib/logstash-core-event-java/version.rb
deleted file mode 100644
index 6c297b7c2fd..00000000000
--- a/logstash-core-event-java/lib/logstash-core-event-java/version.rb
+++ /dev/null
@@ -1,8 +0,0 @@
-# encoding: utf-8
-
-# The version of logstash core event java gem.
-#
-# Note to authors: this should not include dashes because 'gem' barfs if
-# you include a dash in the version string.
-
-LOGSTASH_CORE_EVENT_JAVA_VERSION = "3.0.0.dev"
diff --git a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
deleted file mode 100644
index 143d7a3e068..00000000000
--- a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
+++ /dev/null
@@ -1,6 +0,0 @@
-# this is a generated file, to avoid over-writing it just delete this comment
-require 'jar_dependencies'
-
-require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.1' )
-require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
-require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.1-1' )
diff --git a/logstash-core-event-java/lib/logstash-core-event.rb b/logstash-core-event-java/lib/logstash-core-event.rb
deleted file mode 100644
index 29b487aa192..00000000000
--- a/logstash-core-event-java/lib/logstash-core-event.rb
+++ /dev/null
@@ -1 +0,0 @@
-require "logstash-core-event-java/logstash-core-event-java"
\ No newline at end of file
diff --git a/logstash-core-event-java/lib/logstash/event.rb b/logstash-core-event-java/lib/logstash/event.rb
deleted file mode 100644
index 8f6a1908901..00000000000
--- a/logstash-core-event-java/lib/logstash/event.rb
+++ /dev/null
@@ -1,26 +0,0 @@
-# encoding: utf-8
-
-require "logstash/namespace"
-require "logstash/json"
-require "logstash/string_interpolation"
-require "cabin"
-
-# transcient pipeline events for normal in-flow signaling as opposed to
-# flow altering exceptions. for now having base classes is adequate and
-# in the future it might be necessary to refactor using like a BaseEvent
-# class to have a common interface for all pileline events to support
-# eventual queueing persistence for example, TBD.
-class LogStash::ShutdownEvent; end
-class LogStash::FlushEvent; end
-
-module LogStash
-  FLUSH = LogStash::FlushEvent.new
-
-  # LogStash::SHUTDOWN is used by plugins
-  SHUTDOWN = LogStash::ShutdownEvent.new
-end
-
-# for backward compatibility, require "logstash/event" is used a lots of places so let's bootstrap the
-# Java code loading from here.
-# TODO: (colin) I think we should mass replace require "logstash/event" with require "logstash-core-event"
-require "logstash-core-event"
\ No newline at end of file
diff --git a/logstash-core-event-java/logstash-core-event-java.gemspec b/logstash-core-event-java/logstash-core-event-java.gemspec
deleted file mode 100644
index ef8d2a2bad5..00000000000
--- a/logstash-core-event-java/logstash-core-event-java.gemspec
+++ /dev/null
@@ -1,31 +0,0 @@
-# -*- encoding: utf-8 -*-
-lib = File.expand_path('../lib', __FILE__)
-$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
-require 'logstash-core-event-java/version'
-
-Gem::Specification.new do |gem|
-  gem.authors       = ["Elastic"]
-  gem.email         = ["info@elastic.co"]
-  gem.description   = %q{The core event component of logstash, the scalable log and event management tool}
-  gem.summary       = %q{logstash-core-event-java - The core event component of logstash}
-  gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
-  gem.license       = "Apache License (2.0)"
-
-  gem.files         = Dir.glob(["logstash-core-event-java.gemspec", "lib/**/*.jar", "lib/**/*.rb", "spec/**/*.rb"])
-  gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
-  gem.name          = "logstash-core-event-java"
-  gem.require_paths = ["lib"]
-  gem.version       = LOGSTASH_CORE_EVENT_JAVA_VERSION
-
-  gem.platform = "java"
-
-  gem.add_runtime_dependency "jar-dependencies"
-
-  # as of Feb 3rd 2016, the ruby-maven gem is resolved to version 3.3.3 and that version
-  # has an rdoc problem that causes a bundler exception. 3.3.9 is the current latest version
-  # which does not have this problem.
-  gem.add_runtime_dependency "ruby-maven", "~> 3.3.9"
-
-  gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.1"
-  gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.1-1"
-end
diff --git a/logstash-core-event-java/settings.gradle b/logstash-core-event-java/settings.gradle
deleted file mode 100644
index 3885bfa1686..00000000000
--- a/logstash-core-event-java/settings.gradle
+++ /dev/null
@@ -1,2 +0,0 @@
-rootProject.name = 'logstash-core-event-java'
-
diff --git a/logstash-core-event-java/spec/event_spec.rb b/logstash-core-event-java/spec/event_spec.rb
deleted file mode 100644
index 9df705418f3..00000000000
--- a/logstash-core-event-java/spec/event_spec.rb
+++ /dev/null
@@ -1,282 +0,0 @@
-# encoding: utf-8
-
-require "spec_helper"
-require "logstash/util"
-require "logstash/event"
-require "json"
-
-TIMESTAMP = "@timestamp"
-
-describe LogStash::Event do
-  context "to_json" do
-    it "should serialize simple values" do
-      e = LogStash::Event.new({"foo" => "bar", "bar" => 1, "baz" => 1.0, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
-      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":\"bar\",\"bar\":1,\"baz\":1.0,\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
-    end
-
-    it "should serialize deep hash values" do
-      e = LogStash::Event.new({"foo" => {"bar" => 1, "baz" => 1.0, "biz" => "boz"}, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
-      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":{\"bar\":1,\"baz\":1.0,\"biz\":\"boz\"},\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
-    end
-
-    it "should serialize deep array values" do
-      e = LogStash::Event.new({"foo" => ["bar", 1, 1.0], TIMESTAMP => "2015-05-28T23:02:05.350Z"})
-      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":[\"bar\",1,1.0],\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
-    end
-
-    it "should serialize deep hash from field reference assignments" do
-      e = LogStash::Event.new({TIMESTAMP => "2015-05-28T23:02:05.350Z"})
-      e["foo"] = "bar"
-      e["bar"] = 1
-      e["baz"] = 1.0
-      e["[fancy][pants][socks]"] = "shoes"
-      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\",\"foo\":\"bar\",\"bar\":1,\"baz\":1.0,\"fancy\":{\"pants\":{\"socks\":\"shoes\"}}}"))
-    end
-  end
-
-  context "[]" do
-    it "should get simple values" do
-      e = LogStash::Event.new({"foo" => "bar", "bar" => 1, "baz" => 1.0, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
-      expect(e["foo"]).to eq("bar")
-      expect(e["[foo]"]).to eq("bar")
-      expect(e["bar"]).to eq(1)
-      expect(e["[bar]"]).to eq(1)
-      expect(e["baz"]).to eq(1.0)
-      expect(e["[baz]"]).to eq(1.0)
-      expect(e[TIMESTAMP].to_s).to eq("2015-05-28T23:02:05.350Z")
-      expect(e["[#{TIMESTAMP}]"].to_s).to eq("2015-05-28T23:02:05.350Z")
-    end
-
-    it "should get deep hash values" do
-      e = LogStash::Event.new({"foo" => {"bar" => 1, "baz" => 1.0}})
-      expect(e["[foo][bar]"]).to eq(1)
-      expect(e["[foo][baz]"]).to eq(1.0)
-    end
-
-    it "should get deep array values" do
-      e = LogStash::Event.new({"foo" => ["bar", 1, 1.0]})
-      expect(e["[foo][0]"]).to eq("bar")
-      expect(e["[foo][1]"]).to eq(1)
-      expect(e["[foo][2]"]).to eq(1.0)
-      expect(e["[foo][3]"]).to be_nil
-    end
-  end
-
-  context "[]=" do
-    it "should set simple values" do
-      e = LogStash::Event.new()
-      expect(e["foo"] = "bar").to eq("bar")
-      expect(e["foo"]).to eq("bar")
-
-      e = LogStash::Event.new({"foo" => "test"})
-      expect(e["foo"] = "bar").to eq("bar")
-      expect(e["foo"]).to eq("bar")
-    end
-
-    it "should set deep hash values" do
-      e = LogStash::Event.new()
-      expect(e["[foo][bar]"] = "baz").to eq("baz")
-      expect(e["[foo][bar]"]).to eq("baz")
-      expect(e["[foo][baz]"]).to be_nil
-    end
-
-    it "should set deep array values" do
-      e = LogStash::Event.new()
-      expect(e["[foo][0]"] = "bar").to eq("bar")
-      expect(e["[foo][0]"]).to eq("bar")
-      expect(e["[foo][1]"] = 1).to eq(1)
-      expect(e["[foo][1]"]).to eq(1)
-      expect(e["[foo][2]"] = 1.0 ).to eq(1.0)
-      expect(e["[foo][2]"]).to eq(1.0)
-      expect(e["[foo][3]"]).to be_nil
-    end
-
-    it "should add key when setting nil value" do
-      e = LogStash::Event.new()
-      e["[foo]"] = nil
-      expect(e.to_hash).to include("foo" => nil)
-    end
-
-    # BigDecinal is now natively converted by JRuby, see https://github.com/elastic/logstash/pull/4838
-    it "should set BigDecimal" do
-      e = LogStash::Event.new()
-      e["[foo]"] = BigDecimal.new(1)
-      expect(e["foo"]).to be_kind_of(BigDecimal)
-      expect(e["foo"]).to eq(BigDecimal.new(1))
-    end
-  end
-
-  context "timestamp" do
-    it "getters should present a Ruby LogStash::Timestamp" do
-      e = LogStash::Event.new()
-      expect(e.timestamp.class).to eq(LogStash::Timestamp)
-      expect(e[TIMESTAMP].class).to eq(LogStash::Timestamp)
-    end
-
-    it "to_hash should inject a Ruby LogStash::Timestamp" do
-      e = LogStash::Event.new()
-
-      expect(e.to_java).to be_kind_of(Java::ComLogstash::Event)
-      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::ComLogstash::Timestamp)
-
-      expect(e.to_hash[TIMESTAMP]).to be_kind_of(LogStash::Timestamp)
-      # now make sure the original map was not touched
-      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::ComLogstash::Timestamp)
-    end
-
-    it "should set timestamp" do
-      e = LogStash::Event.new
-      now = Time.now
-      e["@timestamp"] = LogStash::Timestamp.at(now.to_i)
-      expect(e.timestamp.to_i).to eq(now.to_i)
-      expect(e["@timestamp"].to_i).to eq(now.to_i)
-    end
-  end
-
-  context "append" do
-    it "should append" do
-      event = LogStash::Event.new("message" => "hello world")
-      event.append(LogStash::Event.new("message" => "another thing"))
-      expect(event["message"]).to eq(["hello world", "another thing"])
-    end
-  end
-
-  context "tags" do
-    it "should tag" do
-      event = LogStash::Event.new("message" => "hello world")
-      expect(event["tags"]).to be_nil
-      event["tags"] = ["foo"]
-      expect(event["tags"]).to eq(["foo"])
-    end
-  end
-
-
-  # noop logger used to test the injectable logger in Event
-  # this implementation is not complete because only the warn
-  # method is used in Event.
-  module DummyLogger
-    def self.warn(message)
-      # do nothing
-    end
-  end
-
-  context "logger" do
-
-    let(:logger) { double("Logger") }
-    after(:each) {  LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER }
-
-    # the following 2 specs are using both a real module (DummyLogger)
-    # and a mock. both tests are needed to make sure the implementation
-    # supports both types of objects.
-
-    it "should set logger using a module" do
-      LogStash::Event.logger = DummyLogger
-      expect(DummyLogger).to receive(:warn).once
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
-    end
-
-    it "should set logger using a mock" do
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
-    end
-
-    it "should unset logger" do
-      # first set
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
-
-      # then unset
-      LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER
-      expect(logger).to receive(:warn).never
-      # this will produce a log line in stdout by the Java Event
-      LogStash::Event.new(TIMESTAMP => "ignore this log")
-    end
-
-
-    it "should warn on parsing error" do
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once.with(/^Error parsing/)
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
-    end
-
-    it "should warn on invalid timestamp object" do
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once.with(/^Unrecognized/)
-      LogStash::Event.new(TIMESTAMP => Array.new)
-    end
-  end
-
-  context "to_hash" do
-    let (:source_hash) {  {"a" => 1, "b" => [1, 2, 3, {"h" => 1, "i" => "baz"}], "c" => {"d" => "foo", "e" => "bar", "f" => [4, 5, "six"]}} }
-    let (:source_hash_with_matada) {  source_hash.merge({"@metadata" => {"a" => 1, "b" => 2}}) }
-    subject { LogStash::Event.new(source_hash_with_matada) }
-
-    it "should include @timestamp and @version" do
-      h = subject.to_hash
-      expect(h).to include("@timestamp")
-      expect(h).to include("@version")
-      expect(h).not_to include("@metadata")
-    end
-
-    it "should include @timestamp and @version and @metadata" do
-      h = subject.to_hash_with_metadata
-      expect(h).to include("@timestamp")
-      expect(h).to include("@version")
-      expect(h).to include("@metadata")
-    end
-
-    it "should produce valid deep Ruby hash without metadata" do
-      h = subject.to_hash
-      h.delete("@timestamp")
-      h.delete("@version")
-      expect(h).to eq(source_hash)
-    end
-
-    it "should produce valid deep Ruby hash with metadata" do
-      h = subject.to_hash_with_metadata
-      h.delete("@timestamp")
-      h.delete("@version")
-      expect(h).to eq(source_hash_with_matada)
-    end
-  end
-
-  context "from_json" do
-    let (:source_json) { "{\"foo\":1, \"bar\":\"baz\"}" }
-    let (:blank_strings) {["", "  ",  "   "]}
-    let (:bare_strings) {["aa", "  aa", "aa  "]}
-
-    it "should produce a new event from json" do
-      expect(LogStash::Event.from_json(source_json).size).to eq(1)
-
-      event = LogStash::Event.from_json(source_json)[0]
-      expect(event["[foo]"]).to eq(1)
-      expect(event["[bar]"]).to eq("baz")
-    end
-
-    it "should ignore blank strings" do
-      blank_strings.each do |s|
-        expect(LogStash::Event.from_json(s).size).to eq(0)
-      end
-    end
-
-    it "should raise TypeError on nil string" do
-      expect{LogStash::Event.from_json(nil)}.to raise_error TypeError
-    end
-
-    it "should consistently handle nil" do
-      blank_strings.each do |s|
-        expect{LogStash::Event.from_json(nil)}.to raise_error
-        expect{LogStash::Event.new(LogStash::Json.load(nil))}.to raise_error
-      end
-    end
-
-    it "should consistently handle bare string" do
-      bare_strings.each do |s|
-        expect{LogStash::Event.from_json(s)}.to raise_error LogStash::Json::ParserError
-        expect{LogStash::Event.new(LogStash::Json.load(s))}.to raise_error LogStash::Json::ParserError
-       end
-    end
-  end
-end
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Javafier.java b/logstash-core-event-java/src/main/java/com/logstash/Javafier.java
deleted file mode 100644
index f4f16266570..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/Javafier.java
+++ /dev/null
@@ -1,152 +0,0 @@
-package com.logstash;
-
-import org.jruby.RubyArray;
-import org.jruby.RubyHash;
-import org.jruby.RubyString;
-import org.jruby.RubyObject;
-import org.jruby.RubyBoolean;
-import org.jruby.RubyArray;
-import org.jruby.RubyFloat;
-import org.jruby.RubyInteger;
-import org.jruby.RubyNil;
-import org.jruby.RubyBoolean;
-import org.jruby.RubyFixnum;
-import org.jruby.RubyTime;
-import org.jruby.RubySymbol;
-import org.jruby.ext.bigdecimal.RubyBigDecimal;
-import com.logstash.ext.JrubyTimestampExtLibrary;
-import org.jruby.runtime.builtin.IRubyObject;
-import java.math.BigDecimal;
-import org.joda.time.DateTime;
-import java.util.*;
-
-public class Javafier {
-
-    private Javafier(){}
-
-    public static List<Object> deep(RubyArray a) {
-        final ArrayList<Object> result = new ArrayList();
-
-        // TODO: (colin) investagate why .toJavaArrayUnsafe() which should be faster by avoiding copying produces nil values spec errors in arrays
-        for (IRubyObject o : a.toJavaArray()) {
-            result.add(deep(o));
-        }
-        return result;
-    }
-
-    public static HashMap<String, Object> deep(RubyHash h) {
-        final HashMap result = new HashMap();
-
-        h.visitAll(new RubyHash.Visitor() {
-            @Override
-            public void visit(IRubyObject key, IRubyObject value) {
-                result.put(deep(key).toString(), deep(value));
-            }
-        });
-        return result;
-    }
-
-    public static String deep(RubyString s) {
-        return s.asJavaString();
-    }
-
-    public static long deep(RubyInteger i) {
-        return i.getLongValue();
-    }
-
-    public static long deep(RubyFixnum n) {
-        return n.getLongValue();
-    }
-
-    public static double deep(RubyFloat f) {
-        return f.getDoubleValue();
-    }
-
-    public static BigDecimal deep(RubyBigDecimal bd) {
-        return bd.getBigDecimalValue();
-    }
-
-    public static Timestamp deep(JrubyTimestampExtLibrary.RubyTimestamp t) {
-        return t.getTimestamp();
-    }
-
-    public static boolean deep(RubyBoolean b) {
-        return b.isTrue();
-    }
-
-    public static Object deep(RubyNil n) {
-        return null;
-    }
-
-    public static DateTime deep(RubyTime t) {
-        return t.getDateTime();
-    }
-
-    public static String deep(RubySymbol s) {
-        return s.asJavaString();
-    }
-
-    public static Object deep(RubyBoolean.True b) {
-        return true;
-    }
-
-    public static Object deep(RubyBoolean.False b) {
-        return false;
-    }
-
-    public static Object deep(IRubyObject o) {
-        // TODO: (colin) this enum strategy is cleaner but I am hoping that is not slower than using a instanceof cascade
-
-        RUBYCLASS clazz;
-        try {
-            clazz = RUBYCLASS.valueOf(o.getClass().getSimpleName());
-        } catch (IllegalArgumentException e) {
-            throw new IllegalArgumentException("Missing Ruby class handling for full class name=" + o.getClass().getName() + ", simple name=" + o.getClass().getSimpleName());
-        }
-
-        switch(clazz) {
-            case RubyArray: return deep((RubyArray)o);
-            case RubyHash: return deep((RubyHash)o);
-            case RubyString: return deep((RubyString)o);
-            case RubyInteger: return deep((RubyInteger)o);
-            case RubyFloat: return deep((RubyFloat)o);
-            case RubyBigDecimal: return deep((RubyBigDecimal)o);
-            case RubyTimestamp: return deep((JrubyTimestampExtLibrary.RubyTimestamp)o);
-            case RubyBoolean: return deep((RubyBoolean)o);
-            case RubyFixnum: return deep((RubyFixnum)o);
-            case RubyTime: return deep((RubyTime)o);
-            case RubySymbol: return deep((RubySymbol)o);
-            case RubyNil: return deep((RubyNil)o);
-            case True: return deep((RubyBoolean.True)o);
-            case False: return deep((RubyBoolean.False)o);
-        }
-
-        if (o.isNil()) {
-            return null;
-        }
-
-        // TODO: (colin) temporary trace to spot any unhandled types
-        System.out.println("***** WARN: UNHANDLED IRubyObject full class name=" + o.getMetaClass().getRealClass().getName() + ", simple name=" + o.getClass().getSimpleName() + " java class=" + o.getJavaClass().toString() + " toString=" + o.toString());
-
-        return o.toJava(o.getJavaClass());
-    }
-
-    enum RUBYCLASS {
-        RubyString,
-        RubyInteger,
-        RubyFloat,
-        RubyBigDecimal,
-        RubyTimestamp,
-        RubyArray,
-        RubyHash,
-        RubyBoolean,
-        RubyFixnum,
-        RubyObject,
-        RubyNil,
-        RubyTime,
-        RubySymbol,
-        True,
-        False;
-    }
-}
-
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Logger.java b/logstash-core-event-java/src/main/java/com/logstash/Logger.java
deleted file mode 100644
index fc425542715..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/Logger.java
+++ /dev/null
@@ -1,13 +0,0 @@
-package com.logstash;
-
-// minimalist Logger interface to wire a logger callback in the Event class
-// for now only warn is defined because this is the only method that's required
-// in the Event class.
-// TODO: (colin) generalize this
-
-public interface Logger {
-
-    // TODO: (colin) complete interface beyond warn when needed
-
-    void warn(String message);
-}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java b/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java
deleted file mode 100644
index 0bafab8c9da..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java
+++ /dev/null
@@ -1,65 +0,0 @@
-package com.logstash;
-
-import com.logstash.ext.JrubyTimestampExtLibrary;
-import org.jruby.Ruby;
-import org.jruby.RubyArray;
-import org.jruby.RubyHash;
-import org.jruby.ext.bigdecimal.RubyBigDecimal;
-import org.jruby.javasupport.JavaUtil;
-import org.jruby.runtime.builtin.IRubyObject;
-
-import java.math.BigDecimal;
-import java.util.*;
-
-public final class Rubyfier {
-
-    private Rubyfier(){}
-
-    public static IRubyObject deep(Ruby runtime, final Object input) {
-        if (input instanceof IRubyObject) return (IRubyObject)input;
-        if (input instanceof Map) return deepMap(runtime, (Map) input);
-        if (input instanceof List) return deepList(runtime, (List) input);
-        if (input instanceof Timestamp) return JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(runtime, (Timestamp)input);
-        if (input instanceof Collection) throw new ClassCastException("unexpected Collection type " + input.getClass());
-
-        // BigDecimal is not currenly handled by JRuby and this is the type Jackson uses for floats
-        if (input instanceof BigDecimal) return new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), (BigDecimal)input);
-
-        return JavaUtil.convertJavaToUsableRubyObject(runtime, input);
-    }
-
-    public static Object deepOnly(Ruby runtime, final Object input) {
-        if (input instanceof Map) return deepMap(runtime, (Map) input);
-        if (input instanceof List) return deepList(runtime, (List) input);
-        if (input instanceof Timestamp) return JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(runtime, (Timestamp)input);
-        if (input instanceof Collection) throw new ClassCastException("unexpected Collection type " + input.getClass());
-
-        // BigDecimal is not currenly handled by JRuby and this is the type Jackson uses for floats
-        if (input instanceof BigDecimal) return new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), (BigDecimal)input);
-
-        return input;
-    }
-
-    private static RubyArray deepList(Ruby runtime, final List list) {
-        final int length = list.size();
-        final RubyArray array = runtime.newArray(length);
-
-        for (Object item : list) {
-            // use deepOnly because RubyArray.add already calls JavaUtil.convertJavaToUsableRubyObject on item
-            array.add(deepOnly(runtime, item));
-        }
-
-        return array;
-    }
-
-    private static RubyHash deepMap(Ruby runtime, final Map<?, ?> map) {
-        RubyHash hash = RubyHash.newHash(runtime);
-
-        for (Map.Entry<?, ?> entry : map.entrySet()) {
-            // use deepOnly on value because RubyHash.put already calls JavaUtil.convertJavaToUsableRubyObject on items
-            hash.put(entry.getKey(), deepOnly(runtime, entry.getValue()));
-        }
-
-        return hash;
-    }
-}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java b/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java
deleted file mode 100644
index c12bb3e0573..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java
+++ /dev/null
@@ -1,10 +0,0 @@
-package com.logstash;
-
-public class StdioLogger implements Logger {
-
-    // TODO: (colin) complete implementation beyond warn when needed
-
-    public void warn(String message) {
-        System.out.println(message);
-    }
-}
diff --git a/logstash-core-event/lib/logstash-core-event.rb b/logstash-core-event/lib/logstash-core-event.rb
deleted file mode 100644
index b2979326dac..00000000000
--- a/logstash-core-event/lib/logstash-core-event.rb
+++ /dev/null
@@ -1 +0,0 @@
-require "logstash-core-event/logstash-core-event"
\ No newline at end of file
diff --git a/logstash-core-event/lib/logstash-core-event/logstash-core-event.rb b/logstash-core-event/lib/logstash-core-event/logstash-core-event.rb
deleted file mode 100644
index b0f773e203c..00000000000
--- a/logstash-core-event/lib/logstash-core-event/logstash-core-event.rb
+++ /dev/null
@@ -1,5 +0,0 @@
-# encoding: utf-8
-module LogStash
-end
-
-require "logstash/event"
\ No newline at end of file
diff --git a/logstash-core-event/lib/logstash-core-event/version.rb b/logstash-core-event/lib/logstash-core-event/version.rb
deleted file mode 100644
index 18e991d6b0c..00000000000
--- a/logstash-core-event/lib/logstash-core-event/version.rb
+++ /dev/null
@@ -1,8 +0,0 @@
-# encoding: utf-8
-
-# The version of logstash core event gem.
-#
-# Note to authors: this should not include dashes because 'gem' barfs if
-# you include a dash in the version string.
-
-LOGSTASH_CORE_EVENT_VERSION = "3.0.0.dev"
diff --git a/logstash-core-event/lib/logstash/event.rb b/logstash-core-event/lib/logstash/event.rb
deleted file mode 100644
index b1eb9d46cdb..00000000000
--- a/logstash-core-event/lib/logstash/event.rb
+++ /dev/null
@@ -1,278 +0,0 @@
-# encoding: utf-8
-require "time"
-require "date"
-require "cabin"
-require "logstash/namespace"
-require "logstash/util/accessors"
-require "logstash/timestamp"
-require "logstash/json"
-require "logstash/string_interpolation"
-
-# transcient pipeline events for normal in-flow signaling as opposed to
-# flow altering exceptions. for now having base classes is adequate and
-# in the future it might be necessary to refactor using like a BaseEvent
-# class to have a common interface for all pileline events to support
-# eventual queueing persistence for example, TBD.
-class LogStash::ShutdownEvent; end
-class LogStash::FlushEvent; end
-
-module LogStash
-  FLUSH = LogStash::FlushEvent.new
-
-  # LogStash::SHUTDOWN is used by plugins
-  SHUTDOWN = LogStash::ShutdownEvent.new
-end
-
-# the logstash event object.
-#
-# An event is simply a tuple of (timestamp, data).
-# The 'timestamp' is an ISO8601 timestamp. Data is anything - any message,
-# context, references, etc that are relevant to this event.
-#
-# Internally, this is represented as a hash with only two guaranteed fields.
-#
-# * "@timestamp" - an ISO8601 timestamp representing the time the event
-#   occurred at.
-# * "@version" - the version of the schema. Currently "1"
-#
-# They are prefixed with an "@" symbol to avoid clashing with your
-# own custom fields.
-#
-# When serialized, this is represented in JSON. For example:
-#
-#     {
-#       "@timestamp": "2013-02-09T20:39:26.234Z",
-#       "@version": "1",
-#       message: "hello world"
-#     }
-class LogStash::Event
-  class DeprecatedMethod < StandardError; end
-
-  CHAR_PLUS = "+"
-  TIMESTAMP = "@timestamp"
-  VERSION = "@version"
-  VERSION_ONE = "1"
-  TIMESTAMP_FAILURE_TAG = "_timestampparsefailure"
-  TIMESTAMP_FAILURE_FIELD = "_@timestamp"
-
-  METADATA = "@metadata".freeze
-  METADATA_BRACKETS = "[#{METADATA}]".freeze
-
-  # Floats outside of these upper and lower bounds are forcibly converted
-  # to scientific notation by Float#to_s
-  MIN_FLOAT_BEFORE_SCI_NOT = 0.0001
-  MAX_FLOAT_BEFORE_SCI_NOT = 1000000000000000.0
-
-  DEFAULT_LOGGER = Cabin::Channel.get(LogStash)
-  @@logger = DEFAULT_LOGGER
-
-  def initialize(data = {})
-    @cancelled = false
-    @data = data
-    @accessors = LogStash::Util::Accessors.new(data)
-    @data[VERSION] ||= VERSION_ONE
-    ts = @data[TIMESTAMP]
-    @data[TIMESTAMP] = ts ? init_timestamp(ts) : LogStash::Timestamp.now
-
-    @metadata = @data.delete(METADATA) || {}
-    @metadata_accessors = LogStash::Util::Accessors.new(@metadata)
-  end
-
-  def cancel
-    @cancelled = true
-  end
-
-  def uncancel
-    @cancelled = false
-  end
-
-  def cancelled?
-    @cancelled
-  end
-
-  # Create a deep-ish copy of this event.
-  def clone
-    copy = {}
-    @data.each do |k,v|
-      # TODO(sissel): Recurse if this is a hash/array?
-      copy[k] = begin v.clone rescue v end
-    end
-
-    self.class.new(copy)
-  end
-
-  def to_s
-    "#{timestamp.to_iso8601} #{self.sprintf("%{host} %{message}")}"
-  end
-
-  def timestamp
-    @data[TIMESTAMP]
-  end
-
-  def timestamp=(val)
-    @data[TIMESTAMP] = val
-  end
-
-  def [](fieldref)
-    if fieldref.start_with?(METADATA_BRACKETS)
-      @metadata_accessors.get(fieldref[METADATA_BRACKETS.length .. -1])
-    elsif fieldref == METADATA
-      @metadata
-    else
-      @accessors.get(fieldref)
-    end
-  end
-
-  def []=(fieldref, value)
-    if fieldref == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
-      raise TypeError, "The field '@timestamp' must be a (LogStash::Timestamp, not a #{value.class} (#{value})"
-    end
-    if fieldref.start_with?(METADATA_BRACKETS)
-      @metadata_accessors.set(fieldref[METADATA_BRACKETS.length .. -1], value)
-    elsif fieldref == METADATA
-      @metadata = value
-      @metadata_accessors = LogStash::Util::Accessors.new(@metadata)
-    else
-      @accessors.set(fieldref, value)
-    end
-  end
-
-  def to_json(*args)
-    # ignore arguments to respect accepted to_json method signature
-    LogStash::Json.dump(@data)
-  end
-
-  def to_hash
-    @data
-  end
-
-  def overwrite(event)
-    # pickup new event @data and also pickup @accessors
-    # otherwise it will be pointing on previous data
-    @data = event.instance_variable_get(:@data)
-    @accessors = event.instance_variable_get(:@accessors)
-
-    #convert timestamp if it is a String
-    if @data[TIMESTAMP].is_a?(String)
-      @data[TIMESTAMP] = LogStash::Timestamp.parse_iso8601(@data[TIMESTAMP])
-    end
-  end
-
-  def include?(fieldref)
-    if fieldref.start_with?(METADATA_BRACKETS)
-      @metadata_accessors.include?(fieldref[METADATA_BRACKETS.length .. -1])
-    elsif fieldref == METADATA
-      true
-    else
-      @accessors.include?(fieldref)
-    end
-  end
-
-  # Append an event to this one.
-  def append(event)
-    # non-destructively merge that event with ourselves.
-
-    # no need to reset @accessors here because merging will not disrupt any existing field paths
-    # and if new ones are created they will be picked up.
-    LogStash::Util.hash_merge(@data, event.to_hash)
-  end
-
-  # Remove a field or field reference. Returns the value of that field when deleted
-  def remove(fieldref)
-    @accessors.del(fieldref)
-  end
-
-  # sprintf. This could use a better method name.
-  # The idea is to take an event and convert it to a string based on
-  # any format values, delimited by %{foo} where 'foo' is a field or
-  # metadata member.
-  #
-  # For example, if the event has type == "foo" and host == "bar"
-  # then this string:
-  #   "type is %{type} and source is %{host}"
-  # will return
-  #   "type is foo and source is bar"
-  #
-  # If a %{name} value is an array, then we will join by ','
-  # If a %{name} value does not exist, then no substitution occurs.
-  def sprintf(format)
-    LogStash::StringInterpolation.evaluate(self, format)
-  end
-
-  def tag(value)
-    # Generalize this method for more usability
-    self["tags"] ||= []
-    self["tags"] << value unless self["tags"].include?(value)
-  end
-
-  def to_hash_with_metadata
-    @metadata.empty? ? to_hash : to_hash.merge(METADATA => @metadata)
-  end
-
-  def to_json_with_metadata(*args)
-    # ignore arguments to respect accepted to_json method signature
-    LogStash::Json.dump(to_hash_with_metadata)
-  end
-
-  # this is used by logstash-devutils spec_helper.rb to monkey patch the Event field setter []=
-  # and add systematic encoding validation on every field set in specs.
-  # TODO: (colin) this should be moved, probably in logstash-devutils ?
-  def self.validate_value(value)
-    case value
-    when String
-      raise("expected UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.encoding == Encoding::UTF_8
-      raise("invalid UTF-8 encoding for value=#{value}, encoding=#{value.encoding.inspect}") unless value.valid_encoding?
-      value
-    when Array
-      value.each{|v| validate_value(v)} # don't map, return original object
-      value
-    else
-      value
-    end
-  end
-
-  # depracated public methods
-  # TODO: (colin) since these depracated mothods are still exposed in 2.x we should remove them in 3.0
-
-  def unix_timestamp
-    raise DeprecatedMethod
-  end
-
-  def ruby_timestamp
-    raise DeprecatedMethod
-  end
-
-  def fields
-    raise DeprecatedMethod
-  end
-
-  # set a new logger for all Event instances
-  # there is no point in changing it at runtime for other reasons than in tests/specs.
-  # @param logger [Cabin::Channel] logger instance that will be used by all Event instances
-  def self.logger=(logger)
-    @@logger = logger
-  end
-
-  private
-
-  def logger
-    @@logger
-  end
-
-  def init_timestamp(o)
-    begin
-      timestamp = LogStash::Timestamp.coerce(o)
-      return timestamp if timestamp
-
-      logger.warn("Unrecognized #{TIMESTAMP} value, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD}field", :value => o.inspect)
-    rescue LogStash::TimestampParserError => e
-      logger.warn("Error parsing #{TIMESTAMP} string, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD} field", :value => o.inspect, :exception => e.message)
-    end
-
-    @data["tags"] ||= []
-    @data["tags"] << TIMESTAMP_FAILURE_TAG unless @data["tags"].include?(TIMESTAMP_FAILURE_TAG)
-    @data[TIMESTAMP_FAILURE_FIELD] = o
-
-    LogStash::Timestamp.now
-  end
-end
diff --git a/logstash-core-event/lib/logstash/string_interpolation.rb b/logstash-core-event/lib/logstash/string_interpolation.rb
deleted file mode 100644
index 13044e4c005..00000000000
--- a/logstash-core-event/lib/logstash/string_interpolation.rb
+++ /dev/null
@@ -1,152 +0,0 @@
-# encoding: utf-8
-require "thread_safe"
-require "forwardable"
-
-module LogStash
-  module StringInterpolation
-    extend self
-
-    # Floats outside of these upper and lower bounds are forcibly converted
-    # to scientific notation by Float#to_s
-    MIN_FLOAT_BEFORE_SCI_NOT = 0.0001
-    MAX_FLOAT_BEFORE_SCI_NOT = 1000000000000000.0
-
-    CACHE = ThreadSafe::Cache.new
-    TEMPLATE_TAG_REGEXP = /%\{[^}]+\}/
-
-    def evaluate(event, template)
-      if template.is_a?(Float) && (template < MIN_FLOAT_BEFORE_SCI_NOT || template >= MAX_FLOAT_BEFORE_SCI_NOT)
-        return ("%.15f" % template).sub(/0*$/,"")
-      end
-
-      template = template.to_s
-
-      return template if not_cachable?(template)
-
-      compiled = CACHE.get_or_default(template, nil) || CACHE.put(template, compile_template(template))
-      compiled.evaluate(event)
-    end
-
-    # clear the global compiled templates cache
-    def clear_cache
-      CACHE.clear
-    end
-
-    # @return [Fixnum] the compiled templates cache size
-    def cache_size
-      CACHE.size
-    end
-
-    private
-    def not_cachable?(template)
-      template.index("%").nil?
-    end
-
-    def compile_template(template)
-      nodes = Template.new
-
-      position = 0
-      matches = template.to_enum(:scan, TEMPLATE_TAG_REGEXP).map { |m| $~ }
-
-      matches.each do |match|
-        tag = match[0][2..-2]
-        start = match.offset(0).first
-        nodes << StaticNode.new(template[position..(start-1)]) if start > 0
-        nodes << identify(tag)
-        position = match.offset(0).last
-      end
-
-      if position < template.size
-        nodes << StaticNode.new(template[position..-1])
-      end
-
-      optimize(nodes)
-    end
-
-    def optimize(nodes)
-      nodes.size == 1 ?  nodes.first : nodes
-    end
-
-    def identify(tag)
-      if tag == "+%s"
-        EpocNode.new
-      elsif tag[0, 1] == "+"
-        DateNode.new(tag[1..-1])
-      else
-        KeyNode.new(tag)
-      end
-    end
-  end
-
-  class Template
-    extend Forwardable
-    def_delegators :@nodes, :<<, :push, :size, :first
-
-    def initialize
-      @nodes = []
-    end
-
-    def evaluate(event)
-      @nodes.collect { |node| node.evaluate(event) }.join
-    end
-  end
-
-  class EpocNode
-    def evaluate(event)
-      t = event.timestamp
-      raise LogStash::Error, "Unable to format in string \"#{@format}\", #{LogStash::Event::TIMESTAMP} field not found" unless t
-      t.to_i.to_s
-    end
-  end
-
-  class StaticNode
-    def initialize(content)
-      @content = content
-    end
-
-    def evaluate(event)
-      @content
-    end
-  end
-
-  class KeyNode
-    def initialize(key)
-      @key = key
-    end
-
-    def evaluate(event)
-      value = event[@key]
-
-      case value
-      when nil
-        "%{#{@key}}"
-      when Array
-        value.join(",")
-      when Hash
-        LogStash::Json.dump(value)
-      else
-        # Make sure we dont work on the refence of the value
-        # The Java Event implementation was always returning a string.
-        "#{value}"
-      end
-    end
-  end
-
-  class DateNode
-    def initialize(format)
-      @format = format
-      @formatter = org.joda.time.format.DateTimeFormat.forPattern(@format)
-          .withZone(org.joda.time.DateTimeZone::UTC)
-    end
-
-    def evaluate(event)
-      t = event.timestamp
-
-      raise LogStash::Error, "Unable to format in string \"#{@format}\", #{LogStash::Event::TIMESTAMP} field not found" unless t
-
-      org.joda.time.Instant.java_class.constructor(Java::long).new_instance(
-        t.tv_sec * 1000 + t.tv_usec / 1000
-      ).to_java.toDateTime.toString(@formatter)
-    end
-  end
-end
diff --git a/logstash-core-event/lib/logstash/timestamp.rb b/logstash-core-event/lib/logstash/timestamp.rb
deleted file mode 100644
index fb75c5f2538..00000000000
--- a/logstash-core-event/lib/logstash/timestamp.rb
+++ /dev/null
@@ -1,97 +0,0 @@
-# encoding: utf-8
-require "logstash/environment"
-require "logstash/json"
-require "forwardable"
-require "date"
-require "time"
-
-module LogStash
-  class TimestampParserError < StandardError; end
-
-  class Timestamp
-    extend Forwardable
-    include Comparable
-
-    def_delegators :@time, :tv_usec, :usec, :year, :iso8601, :to_i, :tv_sec, :to_f, :to_edn, :<=>, :+
-
-    attr_reader :time
-
-    ISO8601_STRFTIME = "%04d-%02d-%02dT%02d:%02d:%02d.%06d%+03d:00".freeze
-    ISO8601_PRECISION = 3
-
-    def initialize(time = Time.new)
-      @time = time.utc
-    end
-
-    def self.at(*args)
-      Timestamp.new(::Time.at(*args))
-    end
-
-    def self.parse(*args)
-      Timestamp.new(::Time.parse(*args))
-    end
-
-    def self.now
-      Timestamp.new(::Time.now)
-    end
-
-    # coerce tries different strategies based on the time object class to convert into a Timestamp.
-    # @param [String, Time, Timestamp] time the time object to try coerce
-    # @return [Timestamp, nil] Timestamp will be returned if successful otherwise nil
-    # @raise [TimestampParserError] on String with invalid format
-    def self.coerce(time)
-      case time
-      when String
-        LogStash::Timestamp.parse_iso8601(time)
-      when LogStash::Timestamp
-        time
-      when Time
-        LogStash::Timestamp.new(time)
-      else
-        nil
-      end
-    end
-
-    if LogStash::Environment.jruby?
-      JODA_ISO8601_PARSER = org.joda.time.format.ISODateTimeFormat.dateTimeParser
-      UTC = org.joda.time.DateTimeZone.forID("UTC")
-
-      def self.parse_iso8601(t)
-        millis = JODA_ISO8601_PARSER.parseMillis(t)
-        LogStash::Timestamp.at(millis / 1000, (millis % 1000) * 1000)
-      rescue => e
-        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
-      end
-
-    else
-
-      def self.parse_iso8601(t)
-        # warning, ruby's Time.parse is *really* terrible and slow.
-        LogStash::Timestamp.new(::Time.parse(t))
-      rescue => e
-        raise(TimestampParserError, "invalid timestamp string #{t.inspect}, error=#{e.inspect}")
-      end
-    end
-
-    def utc
-      @time.utc # modifies the receiver
-      self
-    end
-    alias_method :gmtime, :utc
-
-    def to_json(*args)
-      # ignore arguments to respect accepted to_json method signature
-      "\"" + to_iso8601 + "\""
-    end
-    alias_method :inspect, :to_json
-
-    def to_iso8601
-      @iso8601 ||= @time.iso8601(ISO8601_PRECISION)
-    end
-    alias_method :to_s, :to_iso8601
-
-    def -(value)
-      @time - (value.is_a?(Timestamp) ? value.time : value)
-    end
-  end
-end
diff --git a/logstash-core-event/lib/logstash/util/accessors.rb b/logstash-core-event/lib/logstash/util/accessors.rb
deleted file mode 100644
index 01c16910855..00000000000
--- a/logstash-core-event/lib/logstash/util/accessors.rb
+++ /dev/null
@@ -1,123 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/util"
-require "thread_safe"
-
-module LogStash::Util
-
-  # PathCache is a singleton which globally caches the relation between a field reference and its
-  # decomposition into a [key, path array] tuple. For example the field reference [foo][bar][baz]
-  # is decomposed into ["baz", ["foo", "bar"]].
-  module PathCache
-    extend self
-
-    # requiring libraries and defining constants is thread safe in JRuby so
-    # PathCache::CACHE will be corretly initialized, once, when accessors.rb
-    # will be first required
-    CACHE = ThreadSafe::Cache.new
-
-    def get(field_reference)
-      # the "get_or_default(x, nil) || put(x, parse(x))" is ~2x faster than "get || put" because the get call is
-      # proxied through the JRuby JavaProxy op_aref method. the correct idiom here would be to use
-      # "compute_if_absent(x){parse(x)}" but because of the closure creation, it is ~1.5x slower than
-      # "get_or_default || put".
-      # this "get_or_default || put" is obviously non-atomic which is not really important here
-      # since all threads will set the same value and this cache will stabilize very quickly after the first
-      # few events.
-      CACHE.get_or_default(field_reference, nil) || CACHE.put(field_reference, parse(field_reference))
-    end
-
-    def parse(field_reference)
-      path = field_reference.split(/[\[\]]/).select{|s| !s.empty?}
-      [path.pop, path]
-    end
-  end
-
-  # Accessors uses a lookup table to speedup access of a field reference of the form
-  # "[hello][world]" to the underlying store hash into {"hello" => {"world" => "foo"}}
-  class Accessors
-
-    # @param store [Hash] the backing data store field refereces point to
-    def initialize(store)
-      @store = store
-
-      # @lut is a lookup table between a field reference and a [target, key] tuple
-      # where target is the containing Hash or Array for key in @store.
-      # this allows us to directly access the containing object for key instead of
-      # walking the field reference path into the inner @store objects
-      @lut = {}
-    end
-
-    # @param field_reference [String] the field reference
-    # @return [Object] the value in @store for this field reference
-    def get(field_reference)
-      target, key = lookup(field_reference)
-      return nil unless target
-      target.is_a?(Array) ? target[key.to_i] : target[key]
-    end
-
-    # @param field_reference [String] the field reference
-    # @param value [Object] the value to set in @store for this field reference
-    # @return [Object] the value set
-    def set(field_reference, value)
-      target, key = lookup_or_create(field_reference)
-      target[target.is_a?(Array) ? key.to_i : key] = value
-    end
-
-    # @param field_reference [String] the field reference to remove
-    # @return [Object] the removed value in @store for this field reference
-    def del(field_reference)
-      target, key = lookup(field_reference)
-      return nil unless target
-      target.is_a?(Array) ? target.delete_at(key.to_i) : target.delete(key)
-    end
-
-    # @param field_reference [String] the field reference to test for inclusion in the store
-    # @return [Boolean] true if the store contains a value for this field reference
-    def include?(field_reference)
-      target, key = lookup(field_reference)
-      return false unless target
-
-      target.is_a?(Array) ? !target[key.to_i].nil? : target.include?(key)
-    end
-
-    private
-
-    # retrieve the [target, key] tuple associated with this field reference
-    # @param field_reference [String] the field referece
-    # @return [[Object, String]] the  [target, key] tuple associated with this field reference
-    def lookup(field_reference)
-      @lut[field_reference] ||= find_target(field_reference)
-    end
-
-    # retrieve the [target, key] tuple associated with this field reference and create inner
-    # container objects if they do not exists
-    # @param field_reference [String] the field referece
-    # @return [[Object, String]] the  [target, key] tuple associated with this field reference
-    def lookup_or_create(field_reference)
-      @lut[field_reference] ||= find_or_create_target(field_reference)
-    end
-
-    # find the target container object in store for this field reference
-    # @param field_reference [String] the field referece
-    # @return [Object] the target container object in store associated with this field reference
-    def find_target(field_reference)
-      key, path = PathCache.get(field_reference)
-      target = path.inject(@store) do |r, k|
-        return nil unless r
-        r[r.is_a?(Array) ? k.to_i : k]
-      end
-      target ? [target, key] : nil
-    end
-
-    # find the target container object in store for this field reference and create inner
-    # container objects if they do not exists
-    # @param field_reference [String] the field referece
-    # @return [Object] the target container object in store associated with this field reference
-    def find_or_create_target(accessor)
-      key, path = PathCache.get(accessor)
-      target = path.inject(@store) {|r, k| r[r.is_a?(Array) ? k.to_i : k] ||= {}}
-      [target, key]
-    end
-  end # class Accessors
-end # module LogStash::Util
diff --git a/logstash-core-event/logstash-core-event.gemspec b/logstash-core-event/logstash-core-event.gemspec
deleted file mode 100644
index 9e0a757a870..00000000000
--- a/logstash-core-event/logstash-core-event.gemspec
+++ /dev/null
@@ -1,23 +0,0 @@
-# -*- encoding: utf-8 -*-
-lib = File.expand_path('../lib', __FILE__)
-$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
-require 'logstash-core-event/version'
-
-Gem::Specification.new do |gem|
-  gem.authors       = ["Elastic"]
-  gem.email         = ["info@elastic.co"]
-  gem.description   = %q{The core event component of logstash, the scalable log and event management tool}
-  gem.summary       = %q{logstash-core-event - The core event component of logstash}
-  gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
-  gem.license       = "Apache License (2.0)"
-
-  gem.files         = Dir.glob(["logstash-core-event.gemspec", "lib/**/*.rb", "spec/**/*.rb"])
-  gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
-  gem.name          = "logstash-core-event"
-  gem.require_paths = ["lib"]
-  gem.version       = LOGSTASH_CORE_EVENT_VERSION
-
-  if RUBY_PLATFORM == 'java'
-    gem.platform = RUBY_PLATFORM
-  end
-end
diff --git a/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
index e83d1586c2e..38e2052b874 100644
--- a/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
+++ b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
@@ -1,2 +1 @@
-# encoding: utf-8
-LOGSTASH_CORE_PLUGIN_API = "1.0.0"
+LOGSTASH_CORE_PLUGIN_API = "2.1.12"
diff --git a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
index 08efcb63abf..fc2c0f8de80 100644
--- a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
+++ b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
@@ -11,13 +11,13 @@ Gem::Specification.new do |gem|
   gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
   gem.license       = "Apache License (2.0)"
 
-  gem.files         = Dir.glob(["logstash-core-event.gemspec", "lib/**/*.rb", "spec/**/*.rb"])
+  gem.files         = Dir.glob(["logstash-core-plugin-api.gemspec", "lib/**/*.rb", "spec/**/*.rb"])
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core-plugin-api"
   gem.require_paths = ["lib"]
   gem.version       = LOGSTASH_CORE_PLUGIN_API
 
-  gem.add_runtime_dependency "logstash-core", ">= 2.0.0", "<= 3.0.0.dev"
+  gem.add_runtime_dependency "logstash-core", "5.4.3"
 
   # Make sure we dont build this gem from a non jruby
   # environment.
diff --git a/logstash-core/build.gradle b/logstash-core/build.gradle
new file mode 100644
index 00000000000..4f51f82f5d6
--- /dev/null
+++ b/logstash-core/build.gradle
@@ -0,0 +1,131 @@
+import org.yaml.snakeyaml.Yaml
+
+apply plugin: 'java'
+apply plugin: 'idea'
+
+// fetch version from Logstash's master versions.yml file
+def versionMap = (Map) (new Yaml()).load(new File("$projectDir/../versions.yml").text)
+
+group = 'org.logstash'
+description = """Logstash Core Java"""
+version = versionMap['logstash-core']
+
+repositories {
+    mavenCentral()
+}
+
+buildscript {
+    repositories {
+        mavenCentral()
+    }
+    dependencies {
+        classpath 'org.yaml:snakeyaml:1.17'
+    }
+}
+
+gradle.projectsEvaluated {
+    tasks.withType(JavaCompile) {
+        options.compilerArgs << "-Xlint:deprecation"
+//        options.compilerArgs << "-Xlint:unchecked" << "-Xlint:deprecation"
+    }
+}
+
+project.sourceCompatibility = 1.8
+project.targetCompatibility = 1.8
+
+task sourcesJar(type: org.gradle.api.tasks.bundling.Jar, dependsOn: classes) {
+    from sourceSets.main.allSource
+    classifier 'sources'
+    extension 'jar'
+}
+
+task javadocJar(type: org.gradle.api.tasks.bundling.Jar, dependsOn: javadoc) {
+    from javadoc.destinationDir
+    classifier 'javadoc'
+    extension 'jar'
+}
+
+// copy jar file into the gem lib dir but without the version number in filename
+task copyGemjar(type: org.gradle.api.tasks.Copy, dependsOn: sourcesJar) {
+    from project.jar
+    into project.file('lib/logstash-core/')
+    rename(/(.+)-${project.version}.jar/, '$1.jar')
+}
+
+task cleanGemjar {
+    delete fileTree(project.file('lib/logstash-core/')) {
+        include '*.jar'
+    }
+}
+
+clean.dependsOn(cleanGemjar)
+jar.finalizedBy(copyGemjar)
+
+task gemspec_jars << {
+    File gemspec_jars = file("./gemspec_jars.rb")
+    gemspec_jars.newWriter().withWriter { w ->
+        w << "# This file is generated by Gradle as part of the build process. It extracts the build.gradle\n"
+        w << "# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec\n"
+        w << "# for the jar-dependencies requirements.\n\n"
+        configurations.runtime.allDependencies.each { dependency ->
+            w << "gem.requirements << \"jar ${dependency.group}:${dependency.name}, ${dependency.version}\"\n"
+        }
+    }
+}
+build.finalizedBy(gemspec_jars)
+
+configurations.create('sources')
+configurations.create('javadoc')
+configurations.archives {
+    extendsFrom configurations.sources
+    extendsFrom configurations.javadoc
+}
+
+artifacts {
+    sources(sourcesJar) {
+        // Weird Gradle quirk where type will be used for the extension, but only for sources
+        type 'jar'
+    }
+    javadoc(javadocJar) {
+        type 'javadoc'
+    }
+}
+
+configurations {
+    provided
+}
+
+project.sourceSets {
+    main.compileClasspath += project.configurations.provided
+    main.runtimeClasspath += project.configurations.provided
+    test.compileClasspath += project.configurations.provided
+    test.runtimeClasspath += project.configurations.provided
+}
+project.javadoc.classpath += project.configurations.provided
+
+idea {
+    module {
+        scopes.PROVIDED.plus += [project.configurations.provided]
+    }
+}
+
+dependencies {
+    compile 'org.apache.logging.log4j:log4j-api:2.6.2'
+    compile 'org.apache.logging.log4j:log4j-core:2.6.2'
+    compile 'com.fasterxml.jackson.core:jackson-core:2.7.4'
+    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.4'
+    compile 'com.fasterxml.jackson.module:jackson-module-afterburner:2.7.4'
+    compile 'com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:2.7.4'
+    testCompile 'org.apache.logging.log4j:log4j-core:2.6.2:tests'
+    testCompile 'org.apache.logging.log4j:log4j-api:2.6.2:tests'
+    testCompile 'junit:junit:4.12'
+    testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
+    provided 'org.jruby:jruby-core:1.7.25'
+}
+
+// See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
+task wrapper(type: Wrapper) {
+    description = 'Install Gradle wrapper'
+    gradleVersion = '2.8'
+}
+
diff --git a/logstash-core/gemspec_jars.rb b/logstash-core/gemspec_jars.rb
new file mode 100644
index 00000000000..2459f5d8949
--- /dev/null
+++ b/logstash-core/gemspec_jars.rb
@@ -0,0 +1,10 @@
+# This file is generated by Gradle as part of the build process. It extracts the build.gradle
+# runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec
+# for the jar-dependencies requirements.
+
+gem.requirements << "jar org.apache.logging.log4j:log4j-api, 2.6.2"
+gem.requirements << "jar org.apache.logging.log4j:log4j-core, 2.6.2"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.4"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.4"
+gem.requirements << "jar com.fasterxml.jackson.module:jackson-module-afterburner, 2.7.4"
+gem.requirements << "jar com.fasterxml.jackson.dataformat:jackson-dataformat-cbor, 2.7.4"
diff --git a/logstash-core/lib/logstash-core/logstash-core.rb b/logstash-core/lib/logstash-core/logstash-core.rb
index 74f073326eb..8c452b0d24a 100644
--- a/logstash-core/lib/logstash-core/logstash-core.rb
+++ b/logstash-core/lib/logstash-core/logstash-core.rb
@@ -1,3 +1,25 @@
 # encoding: utf-8
+
+require "java"
+
 module LogStash
 end
+
+require "logstash-core_jars"
+
+# local dev setup
+classes_dir = File.expand_path("../../../build/classes/main", __FILE__)
+resources_dir = File.expand_path("../../../build/resources/main", __FILE__)
+
+if File.directory?(classes_dir) && File.directory?(resources_dir)
+  # if in local dev setup, add target to classpath
+  $CLASSPATH << classes_dir unless $CLASSPATH.include?(classes_dir)
+  $CLASSPATH << resources_dir unless $CLASSPATH.include?(resources_dir)
+else
+  # otherwise use included jar
+  begin
+    require "logstash-core/logstash-core.jar"
+  rescue Exception => e
+    raise("Error loading logstash-core/logstash-core.jar file, cause: #{e.message}")
+  end
+end
diff --git a/logstash-core/lib/logstash-core/version.rb b/logstash-core/lib/logstash-core/version.rb
index fdc9d13f1a4..09e89aeb411 100644
--- a/logstash-core/lib/logstash-core/version.rb
+++ b/logstash-core/lib/logstash-core/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_VERSION = "3.0.0.dev"
+LOGSTASH_CORE_VERSION = "5.4.3"
diff --git a/logstash-core/lib/logstash-core_jars.rb b/logstash-core/lib/logstash-core_jars.rb
new file mode 100644
index 00000000000..8ccd269ea4b
--- /dev/null
+++ b/logstash-core/lib/logstash-core_jars.rb
@@ -0,0 +1,22 @@
+# this is a generated file, to avoid over-writing it just delete this comment
+begin
+  require 'jar_dependencies'
+rescue LoadError
+  require 'org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar'
+  require 'com/fasterxml/jackson/module/jackson-module-afterburner/2.7.4/jackson-module-afterburner-2.7.4.jar'
+  require 'org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar'
+  require 'com/fasterxml/jackson/core/jackson-core/2.7.4/jackson-core-2.7.4.jar'
+  require 'com/fasterxml/jackson/core/jackson-annotations/2.7.0/jackson-annotations-2.7.0.jar'
+  require 'com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.7.4/jackson-dataformat-cbor-2.7.4.jar'
+  require 'com/fasterxml/jackson/core/jackson-databind/2.7.4/jackson-databind-2.7.4.jar'
+end
+
+if defined? Jars
+  require_jar( 'org.apache.logging.log4j', 'log4j-core', '2.6.2' )
+  require_jar( 'com.fasterxml.jackson.module', 'jackson-module-afterburner', '2.7.4' )
+  require_jar( 'org.apache.logging.log4j', 'log4j-api', '2.6.2' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.4' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
+  require_jar( 'com.fasterxml.jackson.dataformat', 'jackson-dataformat-cbor', '2.7.4' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.4' )
+end
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 5d2fde3201d..1d3a43f7058 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -9,6 +9,7 @@
 require "logstash/instrument/metric"
 require "logstash/pipeline"
 require "logstash/webserver"
+require "logstash/event_dispatcher"
 require "stud/trap"
 require "logstash/config/loader"
 require "uri"
@@ -18,38 +19,51 @@
 LogStash::Environment.load_locale!
 
 class LogStash::Agent
+  include LogStash::Util::Loggable
   STARTED_AT = Time.now.freeze
 
-  attr_reader :metric, :node_name, :pipelines, :logger
+  attr_reader :metric, :name, :pipelines, :settings, :webserver, :dispatcher
+  attr_accessor :logger
 
   # initialize method for LogStash::Agent
   # @param params [Hash] potential parameters are:
-  #   :node_name [String] - identifier for the agent
+  #   :name [String] - identifier for the agent
   #   :auto_reload [Boolean] - enable reloading of pipelines
   #   :reload_interval [Integer] - reload pipelines every X seconds
-  #   :logger [Cabin::Channel] - logger instance
-  def initialize(params)
-    @logger = params[:logger]
-    @auto_reload = params[:auto_reload]
+  def initialize(settings = LogStash::SETTINGS)
+    @logger = self.class.logger
+    @settings = settings
+    @auto_reload = setting("config.reload.automatic")
 
     @pipelines = {}
-    @node_name = params[:node_name] || Socket.gethostname
-    @web_api_http_host = params[:web_api_http_host]
-    @web_api_http_port = params[:web_api_http_port]
+    @name = setting("node.name")
+    @http_host = setting("http.host")
+    @http_port = setting("http.port")
+    @http_environment = setting("http.environment")
+    # Generate / load the persistent uuid
+    id
 
     @config_loader = LogStash::Config::Loader.new(@logger)
-    @reload_interval = params[:reload_interval] || 3 # seconds
+    @reload_interval = setting("config.reload.interval")
     @upgrade_mutex = Mutex.new
 
-    @collect_metric = params.fetch(:collect_metric, false)
-    setup_metric_collection
+    @collect_metric = setting("metric.collect")
+
+    # Create the collectors and configured it with the library
+    configure_metrics_collectors
+
+    @pipeline_reload_metric = metric.namespace([:stats, :pipelines])
+    @instance_reload_metric = metric.namespace([:stats, :reloads])
+
+    @dispatcher = LogStash::EventDispatcher.new(self)
+    LogStash::PLUGIN_REGISTRY.hooks.register_emitter(self.class, dispatcher)
+    dispatcher.fire(:after_initialize)
   end
 
   def execute
-    @thread = Thread.current # this var is implicilty used by Stud.stop?
-    @logger.info("starting agent")
+    @thread = Thread.current # this var is implicitly used by Stud.stop?
+    @logger.debug("starting agent")
 
-    start_background_services
     start_pipelines
     start_webserver
 
@@ -61,8 +75,8 @@ def execute
       Stud.interval(@reload_interval) { reload_state! }
     else
       while !Stud.stop?
-        if clean_state? || running_pipelines?
-          sleep 0.5
+        if clean_state? || running_user_defined_pipelines?
+          sleep(0.5)
         else
           break
         end
@@ -74,18 +88,34 @@ def execute
   # @param pipeline_id [String] pipeline string identifier
   # @param settings [Hash] settings that will be passed when creating the pipeline.
   #   keys should be symbols such as :pipeline_workers and :pipeline_batch_delay
-  def register_pipeline(pipeline_id, settings)
-    pipeline = create_pipeline(settings.merge(:pipeline_id => pipeline_id, :metric => metric))
+  def register_pipeline(settings)
+    pipeline_settings = settings.clone
+    pipeline_id = pipeline_settings.get("pipeline.id")
+
+    pipeline = create_pipeline(pipeline_settings)
     return unless pipeline.is_a?(LogStash::Pipeline)
+    if @auto_reload && !pipeline.reloadable?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_register"),
+                    :pipeline_id => pipeline_id,
+                    :plugins => pipeline.non_reloadable_plugins.map(&:class))
+      return
+    end
     @pipelines[pipeline_id] = pipeline
   end
 
   def reload_state!
     @upgrade_mutex.synchronize do
-      @pipelines.each do |pipeline_id, _|
+      @pipelines.each do |pipeline_id, pipeline|
+        next if pipeline.settings.get("config.reload.automatic") == false
         begin
           reload_pipeline!(pipeline_id)
         rescue => e
+          @instance_reload_metric.increment(:failures)
+          @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
+            n.increment(:failures)
+            n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
+            n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+          end
           @logger.error(I18n.t("oops"), :message => e.message, :class => e.class.name, :backtrace => e.backtrace)
         end
       end
@@ -99,20 +129,94 @@ def uptime
     ((Time.now.to_f - STARTED_AT.to_f) * 1000.0).to_i
   end
 
+  def stop_collecting_metrics
+    @periodic_pollers.stop
+  end
+
   def shutdown
-    stop_background_services
+    stop_collecting_metrics
     stop_webserver
     shutdown_pipelines
   end
 
-  def node_uuid
-    @node_uuid ||= SecureRandom.uuid
+  def id
+    return @id if @id
+
+    uuid = nil
+    if ::File.exists?(id_path)
+      begin
+        uuid = ::File.open(id_path) {|f| f.each_line.first.chomp }
+      rescue => e
+        logger.warn("Could not open persistent UUID file!",
+                    :path => id_path,
+                    :error => e.message,
+                    :class => e.class.name)
+      end
+    end
+
+    if !uuid
+      uuid = SecureRandom.uuid
+      logger.info("No persistent UUID file found. Generating new UUID",
+                  :uuid => uuid,
+                  :path => id_path)
+      begin
+        ::File.open(id_path, 'w') {|f| f.write(uuid) }
+      rescue => e
+        logger.warn("Could not write persistent UUID file! Will use ephemeral UUID",
+                    :uuid => uuid,
+                    :path => id_path,
+                    :error => e.message,
+                    :class => e.class.name)
+      end
+    end
+
+    @id = uuid
+  end
+
+  def id_path
+    @id_path ||= ::File.join(settings.get("path.data"), "uuid")
+  end
+
+  def running_pipelines
+    @upgrade_mutex.synchronize do
+      @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }
+    end
+  end
+
+  def running_pipelines?
+    @upgrade_mutex.synchronize do
+      @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }.any?
+    end
+  end
+
+  def running_user_defined_pipelines?
+    @upgrade_mutex.synchronize do
+      @pipelines.select do |pipeline_id, _|
+        pipeline = @pipelines[pipeline_id]
+        pipeline.running? && !pipeline.system?
+      end.any?
+    end
+  end
+
+  def close_pipeline(id)
+    pipeline = @pipelines[id]
+    if pipeline
+      @logger.warn("closing pipeline", :id => id)
+      pipeline.close
+    end
+  end
+
+  def close_pipelines
+    @pipelines.each  do |id, _|
+      close_pipeline(id)
+    end
   end
 
   private
+
   def start_webserver
-    options = {:http_host => @web_api_http_host, :http_port => @web_api_http_port }
-    @webserver = LogStash::WebServer.new(@logger, options)
+    options = {:http_host => @http_host, :http_ports => @http_port, :http_environment => @http_environment }
+    @webserver = LogStash::WebServer.new(@logger, self, options)
     Thread.new(@webserver) do |webserver|
       LogStash::Util.set_thread_name("Api Webserver")
       webserver.run
@@ -123,70 +227,163 @@ def stop_webserver
     @webserver.stop if @webserver
   end
 
-  def start_background_services
-    if collect_metrics?
-      @logger.debug("Agent: Starting metric periodic pollers")
-      @periodic_pollers.start
-    end
-  end
-
-  def stop_background_services
-    if collect_metrics?
-      @logger.debug("Agent: Stopping metric periodic pollers")
-      @periodic_pollers.stop
-    end
-  end
+  def configure_metrics_collectors
+    @collector = LogStash::Instrument::Collector.new
 
-  def setup_metric_collection
-    if collect_metrics?
+    @metric = if collect_metrics?
       @logger.debug("Agent: Configuring metric collection")
-      LogStash::Instrument::Collector.instance.agent = self
-      @metric = LogStash::Instrument::Metric.new
+      LogStash::Instrument::Metric.new(@collector)
     else
-      @metric = LogStash::Instrument::NullMetric.new
+      LogStash::Instrument::NullMetric.new(@collector)
     end
 
-    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(metric)
+    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(@metric, settings.get("queue.type"), self)
+    @periodic_pollers.start
+  end
+
+  def reset_pipeline_metrics(id)
+    # selectively reset metrics we don't wish to keep after reloading
+    # these include metrics about the plugins and number of processed events
+    # we want to keep other metrics like reload counts and error messages
+    @collector.clear("stats/pipelines/#{id}/plugins")
+    @collector.clear("stats/pipelines/#{id}/events")
   end
 
   def collect_metrics?
     @collect_metric
   end
 
-  def create_pipeline(settings)
-    begin
-      config = fetch_config(settings)
-    rescue => e
-      @logger.error("failed to fetch pipeline configuration", :message => e.message)
-      return
+  def increment_reload_failures_metrics(id, message, backtrace = nil)
+    @instance_reload_metric.increment(:failures)
+    @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+      n.increment(:failures)
+      n.gauge(:last_error, { :message => message, :backtrace =>backtrace})
+      n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+    end
+    if @logger.debug?
+      @logger.error("Cannot create pipeline", :reason => message, :backtrace => backtrace)
+    else
+      @logger.error("Cannot create pipeline", :reason => message)
+    end
+  end
+
+  # create a new pipeline with the given settings and config, if the pipeline initialization failed
+  # increment the failures metrics
+  # @param settings [Settings] the setting for the new pipelines
+  # @param config [String] the configuration string or nil to fetch the configuration per settings
+  # @return [Pipeline] the new pipeline or nil if it failed
+  def create_pipeline(settings, config = nil)
+    if config.nil?
+      begin
+        config = fetch_config(settings)
+      rescue => e
+        @logger.error("failed to fetch pipeline configuration", :message => e.message)
+        return nil
+      end
     end
 
     begin
-      LogStash::Pipeline.new(config, settings)
+      LogStash::Pipeline.new(config, settings, metric)
     rescue => e
-      @logger.error("fetched an invalid config", :config => config, :reason => e.message)
-      return
+      increment_reload_failures_metrics(settings.get("pipeline.id"), e.message, e.backtrace)
+      return nil
     end
   end
 
   def fetch_config(settings)
-    @config_loader.format_config(settings[:config_path], settings[:config_string])
+    @config_loader.format_config(settings.get("path.config"), settings.get("config.string"))
   end
 
-  # since this method modifies the @pipelines hash it is
-  # wrapped in @upgrade_mutex in the parent call `reload_state!`
+  # reload_pipeline trys to reloads the pipeline with id using a potential new configuration if it changed
+  # since this method modifies the @pipelines hash it is wrapped in @upgrade_mutex in the parent call `reload_state!`
+  # @param id [String] the pipeline id to reload
   def reload_pipeline!(id)
     old_pipeline = @pipelines[id]
-    new_pipeline = create_pipeline(old_pipeline.original_settings)
-    return if new_pipeline.nil?
+    new_config = fetch_config(old_pipeline.settings)
 
-    if old_pipeline.config_str == new_pipeline.config_str
-      @logger.debug("no configuration change for pipeline",
-                    :pipeline => id, :config => old_pipeline.config_str)
-    else
-      @logger.warn("fetched new config for pipeline. upgrading..",
-                   :pipeline => id, :config => new_pipeline.config_str)
-      upgrade_pipeline(id, new_pipeline)
+    if old_pipeline.config_str == new_config
+      @logger.debug("no configuration change for pipeline", :pipeline => id)
+      return
+    end
+
+    # check if this pipeline is not reloadable. it should not happen as per the check below
+    # but keep it here as a safety net if a reloadable pipeline was reloaded with a non reloadable pipeline
+    if !old_pipeline.reloadable?
+      @logger.error("pipeline is not reloadable", :pipeline => id)
+      return
+    end
+
+    # BasePipeline#initialize will compile the config, and load all plugins and raise an exception
+    # on an invalid configuration
+    begin
+      pipeline_validator = LogStash::BasePipeline.new(new_config, old_pipeline.settings)
+    rescue => e
+      increment_reload_failures_metrics(id, e.message, e.backtrace)
+      return
+    end
+
+    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort
+    if !pipeline_validator.reloadable?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => id, :plugins => pipeline_validator.non_reloadable_plugins.map(&:class))
+      increment_reload_failures_metrics(id, "non reloadable pipeline")
+      return
+    end
+
+    # we know configis valid so we are fairly comfortable to first stop old pipeline and then start new one
+    upgrade_pipeline(id, old_pipeline.settings, new_config)
+  end
+
+  # upgrade_pipeline first stops the old pipeline and starts the new one
+  # this method exists only for specs to be able to expects this to be executed
+  # @params pipeline_id [String] the pipeline id to upgrade
+  # @params settings [Settings] the settings for the new pipeline
+  # @params new_config [String] the new pipeline config
+  def upgrade_pipeline(pipeline_id, settings, new_config)
+    @logger.warn("fetched new config for pipeline. upgrading..", :pipeline => pipeline_id, :config => new_config)
+
+    # first step: stop the old pipeline.
+    # IMPORTANT: a new pipeline with same settings should not be instantiated before the previous one is shutdown
+
+    stop_pipeline(pipeline_id)
+    reset_pipeline_metrics(pipeline_id)
+
+    # second step create and start a new pipeline now that the old one is shutdown
+
+    new_pipeline = create_pipeline(settings, new_config)
+    if new_pipeline.nil?
+      # this is a scenario where the configuration is valid (compilable) but the new pipeline refused to start
+      # and at this point NO pipeline is running
+      @logger.error("failed to create the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
+      increment_reload_failures_metrics(pipeline_id, "failed to create the reloaded pipeline")
+      return
+    end
+
+    ### at this point pipeline#close must be called if upgrade_pipeline does not succeed
+
+    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort. this should normally not
+    # happen since the check should be done in reload_pipeline! prior to get here.
+    if !new_pipeline.reloadable?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => pipeline_id, :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
+      increment_reload_failures_metrics(pipeline_id, "non reloadable pipeline")
+      new_pipeline.close
+      return
+    end
+
+    # @pipelines[pipeline_id] must be initialized before #start_pipeline below which uses it
+    @pipelines[pipeline_id] = new_pipeline
+
+    if !start_pipeline(pipeline_id)
+      @logger.error("failed to start the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
+      # do not call increment_reload_failures_metrics here since #start_pipeline already does it on failure
+      new_pipeline.close
+      return
+    end
+
+    # pipeline started successfully, update reload success metrics
+    @instance_reload_metric.increment(:successes)
+    @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
+      n.increment(:successes)
+      n.gauge(:last_success_timestamp, LogStash::Timestamp.now)
     end
   end
 
@@ -194,16 +391,32 @@ def start_pipeline(id)
     pipeline = @pipelines[id]
     return unless pipeline.is_a?(LogStash::Pipeline)
     return if pipeline.ready?
-    @logger.info("starting pipeline", :id => id)
-    Thread.new do
+    @logger.debug("starting pipeline", :id => id)
+    t = Thread.new do
       LogStash::Util.set_thread_name("pipeline.#{id}")
       begin
         pipeline.run
       rescue => e
+        @instance_reload_metric.increment(:failures)
+        @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+          n.increment(:failures)
+          n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
+          n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+        end
         @logger.error("Pipeline aborted due to error", :exception => e, :backtrace => e.backtrace)
+
+        # TODO: this is weird, why dont we return directly here? any reason we need to enter the while true loop below?!
+      end
+    end
+    while true do
+      if !t.alive?
+        return false
+      elsif pipeline.running?
+        return true
+      else
+        sleep 0.01
       end
     end
-    sleep 0.01 until pipeline.ready?
   end
 
   def stop_pipeline(id)
@@ -215,31 +428,40 @@ def stop_pipeline(id)
   end
 
   def start_pipelines
-    @pipelines.each { |id, _| start_pipeline(id) }
+    @instance_reload_metric.increment(:successes, 0)
+    @instance_reload_metric.increment(:failures, 0)
+    @pipelines.each do |id, pipeline|
+      start_pipeline(id)
+      pipeline.collect_stats
+      # no reloads yet, initialize all the reload metrics
+      init_pipeline_reload_metrics(id)
+    end
   end
 
   def shutdown_pipelines
     @pipelines.each { |id, _| stop_pipeline(id) }
   end
 
-  def running_pipelines?
-    @upgrade_mutex.synchronize do
-      @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }.any?
-    end
-  end
-
   def running_pipeline?(pipeline_id)
     thread = @pipelines[pipeline_id].thread
     thread.is_a?(Thread) && thread.alive?
   end
 
-  def upgrade_pipeline(pipeline_id, new_pipeline)
-    stop_pipeline(pipeline_id)
-    @pipelines[pipeline_id] = new_pipeline
-    start_pipeline(pipeline_id)
-  end
-
   def clean_state?
     @pipelines.empty?
   end
+
+  def setting(key)
+    @settings.get(key)
+  end
+
+  def init_pipeline_reload_metrics(id)
+    @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+      n.increment(:successes, 0)
+      n.increment(:failures, 0)
+      n.gauge(:last_error, nil)
+      n.gauge(:last_success_timestamp, nil)
+      n.gauge(:last_failure_timestamp, nil)
+    end
+  end
 end # class LogStash::Agent
diff --git a/logstash-core/lib/logstash/api/app_helpers.rb b/logstash-core/lib/logstash/api/app_helpers.rb
new file mode 100644
index 00000000000..98e6c377576
--- /dev/null
+++ b/logstash-core/lib/logstash/api/app_helpers.rb
@@ -0,0 +1,74 @@
+# encoding: utf-8
+require "logstash/json"
+require "logstash/api/errors"
+
+module LogStash::Api::AppHelpers
+  # This method handle both of the normal flow *happy path*
+  # and the display or errors, if more custom logic is added here
+  # it will make sense to separate them.
+  #
+  # See `#error` method in the `LogStash::Api::Module::Base`
+  def respond_with(data, options={})
+    as     = options.fetch(:as, :json)
+    filter = options.fetch(:filter, "")
+
+    status data.respond_to?(:status_code) ? data.status_code : 200
+
+    if as == :json
+      if api_error?(data)
+        data = generate_error_hash(data)
+      else
+        selected_fields = extract_fields(filter.to_s.strip)
+        data.select! { |k,v| selected_fields.include?(k) } unless selected_fields.empty?
+        unless options.include?(:exclude_default_metadata)
+          data = data.to_hash
+          if data.values.size == 0 && selected_fields.size > 0
+            raise LogStash::Api::NotFoundError
+          end
+          data = default_metadata.merge(data)
+        end
+      end
+
+      content_type "application/json"
+      LogStash::Json.dump(data, {:pretty => pretty?})
+    else
+      content_type "text/plain"
+      data.to_s
+    end
+  end
+
+  protected
+  def extract_fields(filter_string)
+    (filter_string.empty? ? [] : filter_string.split(",").map { |s| s.strip.to_sym })
+  end
+
+  def as_boolean(string)
+    return true   if string == true   || string =~ (/(true|t|yes|y|1)$/i)
+    return false  if string == false  || string.blank? || string =~ (/(false|f|no|n|0)$/i)
+    raise ArgumentError.new("invalid value for Boolean: \"#{string}\"")
+  end
+
+  def default_metadata
+    @factory.build(:default_metadata).all
+  end
+
+  def api_error?(error)
+    error.is_a?(LogStash::Api::ApiError)
+  end
+
+  def pretty?
+    params.has_key?("pretty")
+  end
+
+  def generate_error_hash(error)
+    {
+      :path => request.path,
+      :status => error.status_code,
+      :error => error.to_hash
+    }
+  end
+
+  def human?
+    params.has_key?("human") && (params["human"].nil? || as_boolean(params["human"]) == true)
+  end
+end
diff --git a/logstash-core/lib/logstash/api/command_factory.rb b/logstash-core/lib/logstash/api/command_factory.rb
new file mode 100644
index 00000000000..2d790b5ee1c
--- /dev/null
+++ b/logstash-core/lib/logstash/api/command_factory.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require "logstash/api/service"
+require "logstash/api/commands/system/basicinfo_command"
+require "logstash/api/commands/system/plugins_command"
+require "logstash/api/commands/stats"
+require "logstash/api/commands/node"
+require "logstash/api/commands/default_metadata"
+
+
+module LogStash
+  module Api
+    class CommandFactory
+      attr_reader :factory, :service
+
+      def initialize(service)
+        @service = service
+        @factory = {
+          :system_basic_info => ::LogStash::Api::Commands::System::BasicInfo,
+          :plugins_command => ::LogStash::Api::Commands::System::Plugins,
+          :stats => ::LogStash::Api::Commands::Stats,
+          :node => ::LogStash::Api::Commands::Node,
+          :default_metadata => ::LogStash::Api::Commands::DefaultMetadata
+        }
+      end
+
+      def build(*klass_path)
+        # Get a nested path with args like (:parent, :child)
+        klass = klass_path.reduce(factory) {|acc,v| acc[v]}
+
+        if klass
+          klass.new(service)
+        else
+          raise ArgumentError, "Class path '#{klass_path}' does not map to command!"
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/base.rb b/logstash-core/lib/logstash/api/commands/base.rb
new file mode 100644
index 00000000000..d2bef44e6fb
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/base.rb
@@ -0,0 +1,27 @@
+# encoding: utf-8
+
+module LogStash
+  module Api
+    module Commands
+      class Base
+        attr_reader :service
+        
+        def initialize(service = LogStash::Api::Service.instance)
+          @service = service
+        end
+
+        def uptime
+          service.agent.uptime
+        end
+        
+        def started_at
+          (LogStash::Agent::STARTED_AT.to_f * 1000.0).to_i
+        end
+
+        def extract_metrics(path, *keys)
+          service.extract_metrics(path, *keys)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/default_metadata.rb b/logstash-core/lib/logstash/api/commands/default_metadata.rb
new file mode 100644
index 00000000000..4436adf1350
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/default_metadata.rb
@@ -0,0 +1,30 @@
+# encoding: utf-8
+
+require "logstash/api/commands/base"
+
+module LogStash
+  module Api
+    module Commands
+      class DefaultMetadata < Commands::Base
+        def all
+          {:host => host, :version => version, :http_address => http_address,
+           :id => service.agent.id, :name => service.agent.name}
+        end
+
+        def host
+          Socket.gethostname
+        end
+
+        def version
+          LOGSTASH_CORE_VERSION
+        end
+
+        def http_address
+          @http_address ||= service.get_shallow(:http_address).value
+        rescue ::LogStash::Instrument::MetricStore::MetricNotFound, NoMethodError => e
+          nil
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/hot_threads_reporter.rb b/logstash-core/lib/logstash/api/commands/hot_threads_reporter.rb
new file mode 100644
index 00000000000..0de0baefe81
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/hot_threads_reporter.rb
@@ -0,0 +1,58 @@
+# encoding: utf-8
+
+class HotThreadsReport
+  STRING_SEPARATOR_LENGTH = 80.freeze
+  HOT_THREADS_STACK_TRACES_SIZE_DEFAULT = 10.freeze
+
+  def initialize(cmd, options)
+    @cmd = cmd
+    filter = { :stacktrace_size => options.fetch(:stacktrace_size, HOT_THREADS_STACK_TRACES_SIZE_DEFAULT) }
+    jr_dump = JRMonitor.threads.generate(filter)
+    @thread_dump = ::LogStash::Util::ThreadDump.new(options.merge(:dump => jr_dump))
+  end
+
+  def to_s
+    hash = to_hash[:hot_threads]
+    report =  "#{I18n.t("logstash.web_api.hot_threads.title", :hostname => hash[:hostname], :time => hash[:time], :top_count => @thread_dump.top_count )} \n"
+    report << '=' * STRING_SEPARATOR_LENGTH
+    report << "\n"
+    hash[:threads].each do |thread|
+      thread_report = "#{I18n.t("logstash.web_api.hot_threads.thread_title", :percent_of_cpu_time => thread[:percent_of_cpu_time], :thread_state => thread[:state], :thread_name => thread[:name])} \n"
+      thread_report << "#{thread[:path]}\n" if thread[:path]
+      thread[:traces].each do |trace|
+        thread_report << "\t#{trace}\n"
+      end
+      report << thread_report
+      report << '-' * STRING_SEPARATOR_LENGTH
+      report << "\n"
+    end
+    report
+  end
+
+  def to_hash
+    hash = { :time => Time.now.iso8601, :busiest_threads => @thread_dump.top_count, :threads => [] }
+    @thread_dump.each do |thread_name, _hash|
+      thread_name, thread_path = _hash["thread.name"].split(": ")
+      thread = { :name => thread_name,
+                 :percent_of_cpu_time => cpu_time_as_percent(_hash),
+                 :state => _hash["thread.state"]
+      }
+      thread[:path] = thread_path if thread_path
+      traces = []
+      _hash["thread.stacktrace"].each do |trace|
+        traces << trace
+      end
+      thread[:traces] = traces
+      hash[:threads] << thread
+    end
+    { :hot_threads => hash }
+  end
+
+  def cpu_time_as_percent(hash)
+    (((cpu_time(hash) / @cmd.uptime * 1.0)*10000).to_i)/100.0
+  end
+
+  def cpu_time(hash)
+    hash["cpu.time"] / 1000000.0
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/node.rb b/logstash-core/lib/logstash/api/commands/node.rb
new file mode 100644
index 00000000000..e52e6c94fb5
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/node.rb
@@ -0,0 +1,64 @@
+# encoding: utf-8
+require "logstash/api/commands/base"
+require_relative "hot_threads_reporter"
+
+module LogStash
+  module Api
+    module Commands
+      class Node < Commands::Base
+
+        def all(selected_fields=[])
+          payload = {
+            :pipeline => pipeline,
+            :os => os,
+            :jvm => jvm
+          }
+          payload.select! { |k,v| selected_fields.include?(k) } unless selected_fields.empty?
+          payload
+        end
+
+        def pipeline(pipeline_id = LogStash::SETTINGS.get("pipeline.id").to_sym)
+          stats = extract_metrics(
+            [:stats, :pipelines, pipeline_id, :config],
+            :workers, :batch_size, :batch_delay, :config_reload_automatic, :config_reload_interval
+          )
+          stats.merge(:id => pipeline_id)
+        end
+
+        def os
+          {
+            :name => java.lang.System.getProperty("os.name"),
+            :arch => java.lang.System.getProperty("os.arch"),
+            :version => java.lang.System.getProperty("os.version"),
+            :available_processors => java.lang.Runtime.getRuntime().availableProcessors()
+          }
+        end
+
+        def jvm
+          memory_bean = ManagementFactory.getMemoryMXBean()
+
+          {
+            :pid =>  ManagementFactory.getRuntimeMXBean().getName().split("@").first.to_i,
+            :version => java.lang.System.getProperty("java.version"),
+            :vm_name => java.lang.System.getProperty("java.vm.name"),
+            :vm_version => java.lang.System.getProperty("java.version"),
+            :vm_vendor => java.lang.System.getProperty("java.vendor"),
+            :vm_name => java.lang.System.getProperty("java.vm.name"),
+            :start_time_in_millis => started_at,
+            :mem => {
+              :heap_init_in_bytes => (memory_bean.getHeapMemoryUsage().getInit() < 0 ? 0 : memory_bean.getHeapMemoryUsage().getInit()),
+              :heap_max_in_bytes => (memory_bean.getHeapMemoryUsage().getMax() < 0 ? 0 : memory_bean.getHeapMemoryUsage().getMax()),
+              :non_heap_init_in_bytes => (memory_bean.getNonHeapMemoryUsage().getInit() < 0 ? 0 : memory_bean.getNonHeapMemoryUsage().getInit()),
+              :non_heap_max_in_bytes => (memory_bean.getNonHeapMemoryUsage().getMax() < 0 ? 0 : memory_bean.getNonHeapMemoryUsage().getMax())
+            },
+            :gc_collectors => ManagementFactory.getGarbageCollectorMXBeans().collect(&:getName)
+          }
+        end
+
+        def hot_threads(options={})
+          HotThreadsReport.new(self, options)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/stats.rb b/logstash-core/lib/logstash/api/commands/stats.rb
new file mode 100644
index 00000000000..c9a59f878c2
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/stats.rb
@@ -0,0 +1,117 @@
+# encoding: utf-8
+require "logstash/api/commands/base"
+require 'logstash/util/thread_dump'
+require_relative "hot_threads_reporter"
+
+java_import java.nio.file.Files
+java_import java.nio.file.Paths
+
+module LogStash
+  module Api
+    module Commands
+      class Stats < Commands::Base
+        def jvm
+          {
+            :threads => extract_metrics(
+              [:jvm, :threads],
+              :count,
+              :peak_count
+            ),
+            :mem => memory,
+            :gc => gc,
+            :uptime_in_millis => service.get_shallow(:jvm, :uptime_in_millis),
+          }
+        end
+
+        def reloads
+          service.get_shallow(:stats, :reloads)
+        end
+
+        def process
+          extract_metrics(
+            [:jvm, :process],
+            :open_file_descriptors,
+            :peak_open_file_descriptors,
+            :max_file_descriptors,
+            [:mem, [:total_virtual_in_bytes]],
+            [:cpu, [:total_in_millis, :percent, :load_average]]
+          )
+        end
+
+        def events
+          extract_metrics(
+            [:stats, :events],
+            :in, :filtered, :out, :duration_in_millis
+          )
+        end
+
+        def pipeline(pipeline_id = LogStash::SETTINGS.get("pipeline.id").to_sym)
+          stats = service.get_shallow(:stats, :pipelines, pipeline_id)
+          stats = PluginsStats.report(stats)
+          stats.merge(:id => pipeline_id)
+        end
+
+        def memory
+          memory = service.get_shallow(:jvm, :memory)
+          {
+            :heap_used_in_bytes => memory[:heap][:used_in_bytes],
+            :heap_used_percent => memory[:heap][:used_percent],
+            :heap_committed_in_bytes => memory[:heap][:committed_in_bytes],
+            :heap_max_in_bytes => memory[:heap][:max_in_bytes],
+            :heap_used_in_bytes => memory[:heap][:used_in_bytes],
+            :non_heap_used_in_bytes => memory[:non_heap][:used_in_bytes],
+            :non_heap_committed_in_bytes => memory[:non_heap][:committed_in_bytes],
+            :pools => memory[:pools].inject({}) do |acc, (type, hash)|
+              hash.delete("committed_in_bytes")
+              acc[type] = hash
+              acc
+            end
+          }
+        end
+
+        def os
+          service.get_shallow(:os)
+        rescue
+          # The only currently fetch OS information is about the linux
+          # containers.
+          {}
+        end
+
+        def gc
+          service.get_shallow(:jvm, :gc)
+        end
+
+        def hot_threads(options={})
+          HotThreadsReport.new(self, options)
+        end
+
+        module PluginsStats
+          module_function
+
+          def plugin_stats(stats, plugin_type)
+            # Turn the `plugins` stats hash into an array of [ {}, {}, ... ]
+            # This is to produce an array of data points, one point for each
+            # plugin instance.
+            return [] unless stats[:plugins] && stats[:plugins].include?(plugin_type)
+            stats[:plugins][plugin_type].collect do |id, data|
+              { :id => id }.merge(data)
+            end
+          end
+
+          def report(stats)
+            {
+              :events => stats[:events],
+              :plugins => {
+                :inputs => plugin_stats(stats, :inputs),
+                :filters => plugin_stats(stats, :filters),
+                :outputs => plugin_stats(stats, :outputs)
+              },
+              :reloads => stats[:reloads],
+              :queue => stats[:queue]
+            }
+          end
+        end # module PluginsStats
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb b/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb
new file mode 100644
index 00000000000..6eacdbd5b4b
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require 'logstash/api/commands/base'
+require "logstash/util/duration_formatter"
+require 'logstash/build'
+
+module LogStash
+  module Api
+    module Commands
+      module System
+        class BasicInfo < Commands::Base
+
+          def run
+            # Just merge this stuff with the defaults
+            BUILD_INFO
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/system/plugins_command.rb b/logstash-core/lib/logstash/api/commands/system/plugins_command.rb
new file mode 100644
index 00000000000..378f65e8598
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/system/plugins_command.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require "logstash/api/commands/base"
+
+module LogStash
+  module Api
+    module Commands
+      module System
+        class Plugins < Commands::Base
+          def run
+            { :total => plugins.count, :plugins => plugins }
+          end
+
+          private
+
+          def plugins
+            @plugins ||= find_plugins_gem_specs.map do |spec|
+              { :name => spec.name, :version => spec.version.to_s }
+            end.sort_by do |spec|
+              spec[:name]
+            end
+          end
+
+          def find_plugins_gem_specs
+            @specs ||= ::Gem::Specification.find_all.select{|spec| logstash_plugin_gem_spec?(spec)}
+          end
+
+          def logstash_plugin_gem_spec?(spec)
+            spec.metadata && spec.metadata["logstash_plugin"] == "true"
+          end
+
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/errors.rb b/logstash-core/lib/logstash/api/errors.rb
new file mode 100644
index 00000000000..e080305d9d1
--- /dev/null
+++ b/logstash-core/lib/logstash/api/errors.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    class ApiError < StandardError;
+      def initialize(message = nil)
+        super(message || "Api Error")
+      end
+
+      def status_code
+        500
+      end
+
+      def to_hash
+        { :message => to_s }
+      end
+    end
+
+    class NotFoundError < ApiError
+      def initialize
+        super("Not Found")
+      end
+
+      def status_code
+        404
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/init.ru b/logstash-core/lib/logstash/api/init.ru
deleted file mode 100644
index 7fc0c93e9b9..00000000000
--- a/logstash-core/lib/logstash/api/init.ru
+++ /dev/null
@@ -1,31 +0,0 @@
-ROOT = File.expand_path(File.dirname(__FILE__))
-$LOAD_PATH.unshift File.join(ROOT, 'lib')
-Dir.glob('lib/**').each{ |d| $LOAD_PATH.unshift(File.join(ROOT, d)) }
-
-require 'sinatra'
-require 'app/root'
-require 'app/modules/stats'
-require 'app/modules/node'
-require 'app/modules/node_stats'
-require 'app/modules/plugins'
-
-env = ENV["RACK_ENV"].to_sym
-set :environment, env
-
-set :service, LogStash::Api::Service.instance
-
-configure do
-  enable :logging
-end
-run LogStash::Api::Root
-
-namespaces = { "/_node" => LogStash::Api::Node,
-               "/_node/stats" => LogStash::Api::NodeStats,
-               "/_stats" => LogStash::Api::Stats,
-               "/_plugins" => LogStash::Api::Plugins }
-
-namespaces.each_pair do |namespace, app|
-  map(namespace) do
-    run app
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app.rb b/logstash-core/lib/logstash/api/lib/app.rb
deleted file mode 100644
index 72946ec6707..00000000000
--- a/logstash-core/lib/logstash/api/lib/app.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-# encoding: utf-8
-require "cabin"
-require "logstash/json"
-require "helpers/app_helpers"
-require "app/service"
-require "app/command_factory"
-require "logstash/util/loggable"
-
-module LogStash::Api
-  class BaseApp < ::Sinatra::Application
-
-    attr_reader :factory
-
-    if settings.environment != :production
-      set :raise_errors, true
-      set :show_exceptions, :after_handler
-    end
-
-    include LogStash::Util::Loggable
-
-    helpers AppHelpers
-
-    def initialize(app=nil)
-      super(app)
-      @factory = CommandFactory.new(settings.service)
-    end
-
-    not_found do
-      status 404
-      as   = params.has_key?("human") ? :string : :json
-      text = as == :string ? "" : {}
-      respond_with(text, :as => as)
-    end
-
-    error do
-      logger.error(env['sinatra.error'].message, :url => request.url, :ip => request.ip, :params => request.params)
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/command.rb b/logstash-core/lib/logstash/api/lib/app/command.rb
deleted file mode 100644
index 75d8f958c6b..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/command.rb
+++ /dev/null
@@ -1,29 +0,0 @@
-# encoding: utf-8
-require "app/service"
-
-module LogStash::Api
-  class Command
-
-    attr_reader :service
-
-    def initialize(service = LogStash::Api::Service.instance)
-      @service = service
-    end
-
-    def run
-      raise "Not implemented"
-    end
-
-    def hostname
-      service.agent.node_name
-    end
-
-    def uptime
-      service.agent.uptime
-    end
-
-    def started_at
-      (LogStash::Agent::STARTED_AT.to_f * 1000.0).to_i
-    end
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/command_factory.rb b/logstash-core/lib/logstash/api/lib/app/command_factory.rb
deleted file mode 100644
index 29e71e6c4f7..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/command_factory.rb
+++ /dev/null
@@ -1,29 +0,0 @@
-# encoding: utf-8
-require "app/service"
-require "app/commands/system/basicinfo_command"
-require "app/commands/stats/events_command"
-require "app/commands/stats/hotthreads_command"
-require "app/commands/stats/memory_command"
-require "app/commands/system/plugins_command"
-
-module LogStash::Api
-  class CommandFactory
-
-    attr_reader :factory, :service
-
-    def initialize(service)
-      @service = service
-      @factory = {}.merge(
-        :system_basic_info => SystemBasicInfoCommand,
-        :events_command => StatsEventsCommand,
-        :hot_threads_command => HotThreadsCommand,
-        :memory_command => JvmMemoryCommand,
-        :plugins_command => PluginsCommand
-      )
-    end
-
-    def build(klass)
-      factory[klass].new(service)
-    end
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/stats/events_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/stats/events_command.rb
deleted file mode 100644
index 78337364548..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/stats/events_command.rb
+++ /dev/null
@@ -1,13 +0,0 @@
-# encoding: utf-8
-require "app/command"
-
-class LogStash::Api::StatsEventsCommand < LogStash::Api::Command
-
-  def run
-    #return whatever is comming out of the snapshot event, this obvoiusly
-    #need to be tailored to the right metrics for this command.
-    stats =  LogStash::Json.load(service.get(:events_stats))
-    stats["stats"]["events"]
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/stats/hotthreads_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/stats/hotthreads_command.rb
deleted file mode 100644
index 0c3f4ee2ef7..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/stats/hotthreads_command.rb
+++ /dev/null
@@ -1,120 +0,0 @@
-# encoding: utf-8
-require "app/command"
-require 'monitoring'
-require "socket"
-
-class LogStash::Api::HotThreadsCommand < LogStash::Api::Command
-
-  STACK_TRACES_SIZE_DEFAULT = 10.freeze
-
-  def run(options={})
-    filter = { :stacktrace_size => options.fetch(:stacktrace_size, STACK_TRACES_SIZE_DEFAULT) }
-    hash   = JRMonitor.threads.generate(filter)
-    ThreadDump.new(hash, self, options)
-  end
-
-  private
-
-  class ThreadDump
-
-    SKIPPED_THREADS             = [ "Finalizer", "Reference Handler", "Signal Dispatcher" ].freeze
-    THREADS_COUNT_DEFAULT       = 3.freeze
-    IGNORE_IDLE_THREADS_DEFAULT = true.freeze
-
-    attr_reader :top_count, :ignore, :dump
-
-    def initialize(dump, cmd, options={})
-      @dump      = dump
-      @options   = options
-      @top_count = options.fetch(:threads, THREADS_COUNT_DEFAULT)
-      @ignore    = options.fetch(:ignore_idle_threads, IGNORE_IDLE_THREADS_DEFAULT)
-      @cmd       = cmd
-    end
-
-    def to_s
-      hash = to_hash
-      report =  "#{I18n.t("logstash.web_api.hot_threads.title", :hostname => hash[:hostname], :time => hash[:time], :top_count => top_count )} \n"
-      hash[:threads].each do |thread|
-        thread_report = ""
-        thread_report = "\t #{I18n.t("logstash.web_api.hot_threads.thread_title", :percent_of_cpu_time => thread[:percent_of_cpu_time], :thread_state => thread[:state], :thread_name => thread[:name])} \n"
-        thread_report = "\t #{thread[:percent_of_cpu_time]} % of of cpu usage by #{thread[:state]} thread named '#{thread[:name]}'\n"
-        thread_report << "\t\t #{thread[:path]}\n" if thread[:path]
-        thread[:traces].split("\n").each do |trace|
-          thread_report << "#{trace}\n"
-        end
-        report << thread_report
-      end
-      report
-    end
-
-    def to_hash
-      hash = { :hostname => hostname, :time => Time.now.iso8601, :busiest_threads => top_count, :threads => [] }
-      each do |thread_name, _hash|
-        thread_name, thread_path = _hash["thread.name"].split(": ")
-        thread = { :name => thread_name,
-                   :percent_of_cpu_time => cpu_time_as_percent(_hash),
-                   :state => _hash["thread.state"]
-        }
-        thread[:path] = thread_path if thread_path
-        traces = ""
-        _hash["thread.stacktrace"].each do |trace|
-          traces << "\t\t#{trace}\n"
-        end
-        thread[:traces] = traces unless traces.empty?
-        hash[:threads] << thread
-      end
-      hash
-    end
-
-    private
-
-    def each(&block)
-      i=0
-      dump.each_pair do |thread_name, _hash|
-        break if i >= top_count
-        if ignore
-          next if idle_thread?(thread_name, _hash)
-        end
-        block.call(thread_name, _hash)
-        i += 1
-      end
-    end
-
-    def idle_thread?(thread_name, data)
-      idle = false
-      if SKIPPED_THREADS.include?(thread_name)
-        # these are likely JVM dependent
-        idle = true
-      elsif thread_name.match(/Ruby-\d+-JIT-\d+/)
-        # This are internal JRuby JIT threads, 
-        # see java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor for details.
-        idle = true
-      elsif thread_name.match(/pool-\d+-thread-\d+/)
-        # This are threads used by the internal JRuby implementation to dispatch
-        # calls and tasks, see prg.jruby.internal.runtime.methods.DynamicMethod.call
-        idle = true
-      else
-        data["thread.stacktrace"].each do |trace|
-          if trace.start_with?("java.util.concurrent.ThreadPoolExecutor.getTask")
-            idle = true
-            break
-          end
-        end
-      end
-      idle
-    end
-
-    def hostname
-      @cmd.hostname
-    end
-
-    def cpu_time_as_percent(hash)
-      (((cpu_time(hash) / @cmd.uptime * 1.0)*10000).to_i)/100.0
-    end
-
-    def cpu_time(hash)
-      hash["cpu.time"] / 1000000.0
-    end
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/stats/memory_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/stats/memory_command.rb
deleted file mode 100644
index b6aa34f5d42..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/stats/memory_command.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "app/command"
-require 'monitoring'
-
-class LogStash::Api::JvmMemoryCommand < LogStash::Api::Command
-
-  def run
-    memory = LogStash::Json.load(service.get(:jvm_memory_stats))
-    {
-      :heap_used_in_bytes => memory["heap"]["used_in_bytes"],
-      :heap_used_percent => memory["heap"]["used_percent"],
-      :heap_committed_in_bytes => memory["heap"]["committed_in_bytes"],
-      :heap_max_in_bytes => memory["heap"]["max_in_bytes"],
-      :heap_used_in_bytes => memory["heap"]["used_in_bytes"],
-      :non_heap_used_in_bytes => memory["non_heap"]["used_in_bytes"],
-      :non_heap_committed_in_bytes => memory["non_heap"]["committed_in_bytes"],
-      :pools => memory["pools"].inject({}) do |acc, (type, hash)|
-          hash.delete("committed_in_bytes")
-          acc[type] = hash
-          acc
-    end
-    }
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/system/basicinfo_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/system/basicinfo_command.rb
deleted file mode 100644
index 0822f54fb6a..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/system/basicinfo_command.rb
+++ /dev/null
@@ -1,15 +0,0 @@
-# encoding: utf-8
-require "app/command"
-require "logstash/util/duration_formatter"
-
-class LogStash::Api::SystemBasicInfoCommand < LogStash::Api::Command
-
-  def run
-    {
-      "hostname" => hostname,
-      "version" => {
-        "number" => LOGSTASH_VERSION
-      }
-    }
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/system/plugins_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/system/plugins_command.rb
deleted file mode 100644
index 07623283ecc..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/system/plugins_command.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-# encoding: utf-8
-require "app/command"
-
-class LogStash::Api::PluginsCommand < LogStash::Api::Command
-
-  def run
-    { :total => plugins.count, :plugins => plugins }
-  end
-
-  private
-
-  def plugins
-    @plugins ||= find_plugins_gem_specs.map do |spec|
-      { :name => spec.name, :version => spec.version.to_s }
-    end.sort_by do |spec|
-      spec[:name]
-    end
-  end
-
-  def find_plugins_gem_specs
-    @specs ||= Gem::Specification.find_all.select{|spec| logstash_plugin_gem_spec?(spec)}
-  end
-
-  def logstash_plugin_gem_spec?(spec)
-    spec.metadata && spec.metadata["logstash_plugin"] == "true"
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/node.rb b/logstash-core/lib/logstash/api/lib/app/modules/node.rb
deleted file mode 100644
index 3edfb0de5a1..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/node.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Node < BaseApp
-
-    helpers AppHelpers
-
-    # return hot threads information
-    get "/hot_threads" do
-      ignore_idle_threads = params["ignore_idle_threads"] || true
-
-      options = {
-        :ignore_idle_threads => as_boolean(ignore_idle_threads),
-        :human => params.has_key?("human")
-      }
-      options[:threads] = params["threads"].to_i if params.has_key?("threads")
-
-      command = factory.build(:hot_threads_command)
-      as    = options[:human] ? :string : :json
-      respond_with(command.run(options), {:as => as})
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/node_stats.rb b/logstash-core/lib/logstash/api/lib/app/modules/node_stats.rb
deleted file mode 100644
index 8317cad3369..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/node_stats.rb
+++ /dev/null
@@ -1,51 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class NodeStats < BaseApp
-
-    helpers AppHelpers
-
-
-    # Global _stats resource where all information is 
-    # retrieved and show
-    get "/" do
-      events_command = factory.build(:events_command)
-      payload = {
-        :events => events_command.run,
-        :jvm => jvm_payload
-      }
-
-      respond_with payload
-    end
-
-    # Show all events stats information
-    # (for ingested, emitted, dropped)
-    # - #events since startup
-    # - #data (bytes) since startup
-    # - events/s
-    # - bytes/s
-    # - dropped events/s
-    # - events in the pipeline
-    get "/events" do
-      command = factory.build(:events_command)
-      respond_with({ :events => command.run })
-    end
-
-    # return hot threads information
-    get "/jvm" do
-      respond_with jvm_payload
-    end
-
-    private
-
-    def jvm_payload
-      command = factory.build(:memory_command)
-      {
-        :timestamp => command.started_at,
-        :uptime_in_millis => command.uptime,
-        :mem => command.run
-      }
-    end
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/plugins.rb b/logstash-core/lib/logstash/api/lib/app/modules/plugins.rb
deleted file mode 100644
index 93a94bf76c3..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/plugins.rb
+++ /dev/null
@@ -1,15 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Plugins < BaseApp
-
-    helpers AppHelpers
-
-    get "/" do
-      command = factory.build(:plugins_command)
-      respond_with(command.run())
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/stats.rb b/logstash-core/lib/logstash/api/lib/app/modules/stats.rb
deleted file mode 100644
index ed3aa54f789..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/stats.rb
+++ /dev/null
@@ -1,21 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Stats < BaseApp
-
-    helpers AppHelpers
-
-    # return hot threads information
-    get "/jvm" do
-      command = factory.build(:memory_command)
-      jvm_payload = {
-        :timestamp => command.started_at,
-        :uptime_in_millis => command.uptime,
-        :mem => command.run
-      }
-      respond_with({:jvm => jvm_payload})
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/root.rb b/logstash-core/lib/logstash/api/lib/app/root.rb
deleted file mode 100644
index 75a0ba6be67..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/root.rb
+++ /dev/null
@@ -1,13 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Root < BaseApp
-
-    get "/" do
-      command = factory.build(:system_basic_info)
-      respond_with command.run
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/service.rb b/logstash-core/lib/logstash/api/lib/app/service.rb
deleted file mode 100644
index 4b63593c18a..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/service.rb
+++ /dev/null
@@ -1,61 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/collector"
-require "logstash/util/loggable"
-
-class LogStash::Api::Service
-
-  include Singleton
-  include LogStash::Util::Loggable
-
-  def initialize
-    @snapshot_rotation_mutex = Mutex.new
-    @snapshot = nil
-    logger.debug("[api-service] start") if logger.debug?
-    LogStash::Instrument::Collector.instance.add_observer(self)
-  end
-
-  def stop
-    logger.debug("[api-service] stop") if logger.debug?
-    LogStash::Instrument::Collector.instance.delete_observer(self)
-  end
-
-  def agent
-    LogStash::Instrument::Collector.instance.agent
-  end
-
-  def started?
-    !@snapshot.nil? && has_counters?
-  end
-
-  def update(snapshot)
-    logger.debug("[api-service] snapshot received", :snapshot => snapshot) if logger.debug?
-    if @snapshot_rotation_mutex.try_lock
-      @snapshot = snapshot
-      @snapshot_rotation_mutex.unlock
-    end
-  end
-
-  def get(key)
-    metric_store = @snapshot.metric_store
-    if key == :jvm_memory_stats
-      data = metric_store.get_with_path("jvm/memory")[:jvm][:memory]
-    else
-      data = metric_store.get_with_path("stats/events")
-    end
-    LogStash::Json.dump(data)
-  end
-
-  private
-
-  def has_counters?
-    (["LogStash::Instrument::MetricType::Counter", "LogStash::Instrument::MetricType::Gauge"] - metric_types).empty?
-  end
-
-  def metric_types
-    types = []
-    @snapshot_rotation_mutex.synchronize do
-      types = @snapshot.metric_store.all.map { |t| t.class.to_s }
-    end
-    return types
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/stats.rb b/logstash-core/lib/logstash/api/lib/app/stats.rb
deleted file mode 100644
index 2d3f9a4f08b..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/stats.rb
+++ /dev/null
@@ -1,56 +0,0 @@
-# encoding: utf-8
-require "app"
-require "app/stats/events_command"
-require "app/stats/hotthreads_command"
-
-module LogStash::Api
-  class Stats < BaseApp
-
-    helpers AppHelpers
-
-
-    # Global _stats resource where all information is 
-    # retrieved and show
-    get "/" do
-      events_command = factory.build(:events_command)
-      memory_command = factory.build(:memory_command)
-      payload = {
-        :events => events_command.run,
-        :jvm => { :memory => memory_command.run }
-      }
-      respond_with payload
-    end
-
-    # Show all events stats information
-    # (for ingested, emitted, dropped)
-    # - #events since startup
-    # - #data (bytes) since startup
-    # - events/s
-    # - bytes/s
-    # - dropped events/s
-    # - events in the pipeline
-    get "/events" do
-      command = factory.build(:events_command)
-      respond_with({ :events => command.run })
-    end
-
-    # return hot threads information
-    get "/jvm/hot_threads" do
-      top_threads_count = params["threads"] || 3
-      ignore_idle_threads = params["ignore_idle_threads"] || true
-      options = {
-        :threads => top_threads_count.to_i,
-        :ignore_idle_threads => as_boolean(ignore_idle_threads)
-      }
-      command = factory.build(:hot_threads_command)
-      respond_with(command.run(options), :string)
-    end
-
-    # return hot threads information
-    get "/jvm/memory" do
-      command = factory.build(:memory_command)
-      respond_with({ :memory => command.run })
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/helpers/app_helpers.rb b/logstash-core/lib/logstash/api/lib/helpers/app_helpers.rb
deleted file mode 100644
index cd872edc51d..00000000000
--- a/logstash-core/lib/logstash/api/lib/helpers/app_helpers.rb
+++ /dev/null
@@ -1,23 +0,0 @@
-# encoding: utf-8
-require "logstash/json"
-
-module LogStash::Api::AppHelpers
-
-  def respond_with(data, options={})
-    as     = options.fetch(:as, :json)
-    pretty = params.has_key?("pretty")
-    if as == :json
-      content_type "application/json"
-      LogStash::Json.dump(data, {:pretty => pretty})
-    else
-      content_type "text/plain"
-      data.to_s
-    end
-  end
-
-  def as_boolean(string)
-    return true   if string == true   || string =~ (/(true|t|yes|y|1)$/i)
-    return false  if string == false  || string.blank? || string =~ (/(false|f|no|n|0)$/i)
-    raise ArgumentError.new("invalid value for Boolean: \"#{string}\"")
-  end
-end
diff --git a/logstash-core/lib/logstash/api/modules/base.rb b/logstash-core/lib/logstash/api/modules/base.rb
new file mode 100644
index 00000000000..d8ad5e0c7f9
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/base.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+require "logstash/api/app_helpers"
+require "logstash/api/command_factory"
+require "logstash/api/errors"
+
+module LogStash
+  module Api
+    module Modules
+      class Base < ::Sinatra::Base
+
+        helpers AppHelpers
+
+        # These options never change
+        # Sinatra isn't good at letting you change internal settings at runtime
+        # which is a requirement. We always propagate errors up and catch them
+        # in a custom rack handler in the RackApp class
+        set :environment, :production
+        set :raise_errors, true
+        set :show_exceptions, false
+
+        attr_reader :factory, :agent
+
+        include LogStash::Util::Loggable
+
+        helpers AppHelpers
+
+        def initialize(app=nil, agent)
+          super(app)
+          @agent = agent
+          @factory = ::LogStash::Api::CommandFactory.new(LogStash::Api::Service.new(agent))
+        end
+
+        not_found do
+          # We cannot raise here because it won't be catched by the `error` handler.
+          # So we manually create a new instance of NotFound and just pass it down.
+          respond_with(NotFoundError.new)
+        end
+
+        # This allow to have custom exception but keep a consistent
+        # format to report them.
+        error ApiError do |error|
+          respond_with(error)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/logging.rb b/logstash-core/lib/logstash/api/modules/logging.rb
new file mode 100644
index 00000000000..d18edd4e8d5
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/logging.rb
@@ -0,0 +1,52 @@
+# encoding: utf-8
+#
+java_import org.apache.logging.log4j.core.LoggerContext
+
+module LogStash
+  module Api
+    module Modules
+      class Logging < ::LogStash::Api::Modules::Base
+        # retrieve logging specific parameters from the provided settings
+        #
+        # return any unused configurations
+        def handle_logging(settings)
+          Hash[settings.map do |key, level|
+            if key.start_with?("logger.")
+              _, path = key.split("logger.")
+              LogStash::Logging::Logger::configure_logging(level, path)
+              nil
+            else
+              [key, level]
+            end
+          end]
+        end
+
+        put "/" do
+          begin
+            request.body.rewind
+            req_body = LogStash::Json.load(request.body.read)
+            remaining = handle_logging(req_body)
+            unless remaining.empty?
+              raise ArgumentError, I18n.t("logstash.web_api.logging.unrecognized_option", :option => remaining.keys.first)
+            end
+            respond_with({"acknowledged" => true})
+          rescue ArgumentError => e
+            status 400
+            respond_with({"error" => e.message})
+          end
+        end
+
+        get "/" do
+          context = LogStash::Logging::Logger::get_logging_context
+          if context.nil?
+            status 500
+            respond_with({"error" => "Logstash loggers were not initialized properly"})
+          else
+            loggers = context.getLoggers.map { |lgr| [lgr.getName, lgr.getLevel.name] }.sort
+            respond_with({"loggers" => Hash[loggers]})
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/node.rb b/logstash-core/lib/logstash/api/modules/node.rb
new file mode 100644
index 00000000000..32bf09149fa
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/node.rb
@@ -0,0 +1,36 @@
+# encoding: utf-8
+require "logstash/api/modules/base"
+require "logstash/api/errors"
+
+module LogStash
+  module Api
+    module Modules
+      class Node < ::LogStash::Api::Modules::Base
+        def node
+          factory.build(:node)
+        end
+
+        get "/hot_threads" do
+          ignore_idle_threads = params["ignore_idle_threads"] || true
+
+          options = { :ignore_idle_threads => as_boolean(ignore_idle_threads) }
+          options[:threads] = params["threads"].to_i if params.has_key?("threads")
+
+          as = human? ? :string : :json
+          respond_with(node.hot_threads(options), {:as => as})
+        end
+
+         get "/?:filter?" do
+           selected_fields = extract_fields(params["filter"].to_s.strip)
+           values = node.all(selected_fields)
+
+           if values.size == 0
+             raise NotFoundError
+           else
+             respond_with(values)
+           end
+         end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/node_stats.rb b/logstash-core/lib/logstash/api/modules/node_stats.rb
new file mode 100644
index 00000000000..f56efe81c59
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/node_stats.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class NodeStats < ::LogStash::Api::Modules::Base
+
+        before do
+          @stats = factory.build(:stats)
+        end
+
+        get "/?:filter?" do
+          payload = {
+            :jvm => jvm_payload,
+            :process => process_payload,
+            :pipeline => pipeline_payload,
+            :reloads => reloads,
+            :os => os_payload
+          }
+          respond_with(payload, {:filter => params["filter"]})
+        end
+
+        private
+        def os_payload
+          @stats.os
+        end
+
+        def events_payload
+          @stats.events
+        end
+
+        def jvm_payload
+          @stats.jvm
+        end
+
+        def reloads
+          @stats.reloads
+        end
+
+        def process_payload
+          @stats.process
+        end
+
+        def mem_payload
+          @stats.memory
+        end
+
+        def pipeline_payload
+          @stats.pipeline
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/plugins.rb b/logstash-core/lib/logstash/api/modules/plugins.rb
new file mode 100644
index 00000000000..7edd3da154a
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/plugins.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Plugins < ::LogStash::Api::Modules::Base
+
+        get "/" do
+          command = factory.build(:plugins_command)
+          respond_with(command.run())
+        end
+
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/root.rb b/logstash-core/lib/logstash/api/modules/root.rb
new file mode 100644
index 00000000000..10a414187ac
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/root.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Root < ::LogStash::Api::Modules::Base
+        get "/" do
+          command = factory.build(:system_basic_info)
+          respond_with command.run
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/stats.rb b/logstash-core/lib/logstash/api/modules/stats.rb
new file mode 100644
index 00000000000..a35c9f062b7
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/stats.rb
@@ -0,0 +1,42 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Stats < ::LogStash::Api::Modules::Base
+        def stats_command
+          factory.build(:stats)
+        end
+
+        # return hot threads information
+        get "/jvm/hot_threads" do
+          top_threads_count = params["threads"] || 3
+          ignore_idle_threads = params["ignore_idle_threads"] || true
+          options = {
+            :threads => top_threads_count.to_i,
+            :ignore_idle_threads => as_boolean(ignore_idle_threads)
+          }
+
+          respond_with(stats_command.hot_threads(options))
+        end
+
+        # return hot threads information
+        get "/jvm/memory" do
+          respond_with({ :memory => stats_command.memory })
+        end
+
+        get "/?:filter?" do
+          payload = {
+            :events => stats_command.events,
+            :jvm => {
+              :timestamp => stats_command.started_at,
+              :uptime_in_millis => stats_command.uptime,
+              :memory => stats_command.memory,
+            },
+            :os => stats_command.os
+          }
+          respond_with(payload, {:filter => params["filter"]})
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/rack_app.rb b/logstash-core/lib/logstash/api/rack_app.rb
new file mode 100644
index 00000000000..8b2d1905559
--- /dev/null
+++ b/logstash-core/lib/logstash/api/rack_app.rb
@@ -0,0 +1,113 @@
+require "sinatra"
+require "rack"
+require "logstash/api/modules/base"
+require "logstash/api/modules/node"
+require "logstash/api/modules/node_stats"
+require "logstash/api/modules/plugins"
+require "logstash/api/modules/root"
+require "logstash/api/modules/logging"
+require "logstash/api/modules/stats"
+
+module LogStash
+  module Api
+    module RackApp
+      METADATA_FIELDS = [:request_method, :path_info, :query_string, :http_version, :http_accept].freeze
+      def self.log_metadata(status, env)
+        METADATA_FIELDS.reduce({:status => status}) do |acc, field|
+          acc[field] = env[field.to_s.upcase]
+          acc
+        end
+      end
+
+      class ApiLogger
+        LOG_MESSAGE = "API HTTP Request".freeze
+
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          res = @app.call(env)
+          status, headers, body = res
+
+          if fatal_error?(status)
+            @logger.error? && @logger.error(LOG_MESSAGE, RackApp.log_metadata(status, env))
+          else
+            @logger.debug? && @logger.debug(LOG_MESSAGE, RackApp.log_metadata(status, env))
+          end
+
+          res
+        end
+
+        def fatal_error?(status)
+          status >= 500 && status < 600
+        end
+      end
+
+      class ApiErrorHandler
+        LOG_MESSAGE = "Internal API server error".freeze
+
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          @app.call(env)
+        rescue => e
+          body = RackApp.log_metadata(500, env).
+                   merge({
+                           :error => "Unexpected Internal Error",
+                           :class => e.class.name,
+                           :message => e.message,
+                           :backtrace => e.backtrace
+                         })
+
+          @logger.error(LOG_MESSAGE, body)
+
+          [500,
+           {'Content-Type' => 'application/json'},
+           [LogStash::Json.dump(body)]
+          ]
+        end
+      end
+
+      def self.app(logger, agent, environment)
+        namespaces = rack_namespaces(agent)
+
+        Rack::Builder.new do
+          # Custom logger object. Rack CommonLogger does not work with cabin
+          use ApiLogger, logger
+
+          # In test env we want errors to propagate up the chain
+          # so we get easy to understand test failures.
+          # In production / dev we don't want a bad API endpoint
+          # to crash the process
+          if environment != "test"
+            use ApiErrorHandler, logger
+          end
+
+          run LogStash::Api::Modules::Root.new(nil, agent)
+          namespaces.each_pair do |namespace, app|
+            map(namespace) do
+              # Pass down a reference to the current agent
+              # This allow the API to have direct access to the collector
+              run app.new(nil, agent)
+            end
+          end
+        end
+      end
+
+      def self.rack_namespaces(agent)
+        {
+          "/_node" => LogStash::Api::Modules::Node,
+          "/_stats" => LogStash::Api::Modules::Stats,
+          "/_node/stats" => LogStash::Api::Modules::NodeStats,
+          "/_node/plugins" => LogStash::Api::Modules::Plugins,
+          "/_node/logging" => LogStash::Api::Modules::Logging
+        }
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/service.rb b/logstash-core/lib/logstash/api/service.rb
new file mode 100644
index 00000000000..32563fc994e
--- /dev/null
+++ b/logstash-core/lib/logstash/api/service.rb
@@ -0,0 +1,34 @@
+# encoding: utf-8
+require "logstash/instrument/collector"
+require "logstash/util/loggable"
+
+module LogStash
+  module Api
+    class Service
+      include LogStash::Util::Loggable
+
+      attr_reader :agent
+
+      def initialize(agent)
+        @agent = agent
+        logger.debug("[api-service] start") if logger.debug?
+      end
+
+      def started?
+        true
+      end
+
+      def snapshot
+        agent.metric.collector.snapshot_metric
+      end
+
+      def get_shallow(*path)
+        snapshot.metric_store.get_shallow(*path)
+      end
+
+      def extract_metrics(path, *keys)
+        snapshot.metric_store.extract_metrics(path, *keys)
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/build.rb b/logstash-core/lib/logstash/build.rb
new file mode 100644
index 00000000000..fb2132c22c4
--- /dev/null
+++ b/logstash-core/lib/logstash/build.rb
@@ -0,0 +1,6 @@
+# encoding: utf-8
+
+# DO NOT EDIT
+# this file acts as a placeholder for build information when executing
+# logstash in dev mode (outside of a package build)
+BUILD_INFO = {}
diff --git a/logstash-core/lib/logstash/codecs/base.rb b/logstash-core/lib/logstash/codecs/base.rb
index 4d10950f534..cf8b582d9d5 100644
--- a/logstash-core/lib/logstash/codecs/base.rb
+++ b/logstash-core/lib/logstash/codecs/base.rb
@@ -7,12 +7,18 @@
 # This is the base class for logstash codecs.
 module LogStash::Codecs; class Base < LogStash::Plugin
   include LogStash::Config::Mixin
+
   config_name "codec"
 
+  def self.plugin_type
+    "codec"
+  end
+
   def initialize(params={})
     super
     config_init(@params)
     register if respond_to?(:register)
+    setup_multi_encode!
   end
 
   public
@@ -23,10 +29,37 @@ def decode(data)
   alias_method :<<, :decode
 
   public
+  # DEPRECATED: Prefer defining encode_sync or multi_encode
   def encode(event)
-    raise "#{self.class}#encode must be overidden"
+    encoded = multi_encode([event])
+    encoded.each {|event,data| @on_event.call(event,data) }
   end # def encode
 
+  public
+  # Relies on the codec being synchronous (which they all are!)
+  # We need a better long term design here, but this is an improvement
+  # over the current API for shared plugins
+  # It is best if the codec implements this directly
+  def multi_encode(events)
+    if @has_encode_sync              
+      events.map {|event| [event, self.encode_sync(event)]}
+    else
+      batch = Thread.current[:logstash_output_codec_batch] ||= []
+      batch.clear
+      
+      events.each {|event| self.encode(event) }
+      batch
+    end
+  end
+
+  def setup_multi_encode!
+    @has_encode_sync = self.methods.include?(:encode_sync)
+
+    on_event do |event, data|
+      Thread.current[:logstash_output_codec_batch] << [event, data]
+    end
+  end
+
   public
   def close; end;
 
diff --git a/logstash-core/lib/logstash/config/config_ast.rb b/logstash-core/lib/logstash/config/config_ast.rb
index 41ec1c599a1..16f47fc1680 100644
--- a/logstash-core/lib/logstash/config/config_ast.rb
+++ b/logstash-core/lib/logstash/config/config_ast.rb
@@ -60,20 +60,28 @@ def recursive_select_parent(results=[], klass)
 
 module LogStash; module Config; module AST
 
-  def self.defered_conditionals=(val)
-    @defered_conditionals = val
+  def self.deferred_conditionals=(val)
+    @deferred_conditionals = val
   end
 
-  def self.defered_conditionals
-    @defered_conditionals
+  def self.deferred_conditionals
+    @deferred_conditionals
   end
 
-  def self.defered_conditionals_index
-    @defered_conditionals_index
+  def self.deferred_conditionals_index
+    @deferred_conditionals_index
   end
 
-  def self.defered_conditionals_index=(val)
-    @defered_conditionals_index = val
+  def self.deferred_conditionals_index=(val)
+    @deferred_conditionals_index = val
+  end
+
+  def self.plugin_instance_index
+    @plugin_instance_index
+  end
+
+  def self.plugin_instance_index=(val)
+    @plugin_instance_index = val
   end
 
   class Node < Treetop::Runtime::SyntaxNode
@@ -84,8 +92,9 @@ def text_value_for_comments
 
   class Config < Node
     def compile
-      LogStash::Config::AST.defered_conditionals = []
-      LogStash::Config::AST.defered_conditionals_index = 0
+      LogStash::Config::AST.deferred_conditionals = []
+      LogStash::Config::AST.deferred_conditionals_index = 0
+      LogStash::Config::AST.plugin_instance_index = 0
       code = []
 
       code << <<-CODE
@@ -94,6 +103,7 @@ def compile
         @outputs = []
         @periodic_flushers = []
         @shutdown_flushers = []
+        @generated_objects = {}
       CODE
 
       sections = recursive_select(LogStash::Config::AST::PluginSection)
@@ -113,7 +123,7 @@ def compile
         definitions << "define_singleton_method :#{type}_func do |event|"
         definitions << "  targeted_outputs = []" if type == "output"
         definitions << "  events = [event]" if type == "filter"
-        definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", :event => event.to_hash)"
+        definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", \"event\" => event.to_hash)"
 
         sections.select { |s| s.plugin_type.text_value == type }.each do |s|
           definitions << s.compile.split("\n", -1).map { |e| "  #{e}" }
@@ -126,7 +136,7 @@ def compile
 
       code += definitions.join("\n").split("\n", -1).collect { |l| "  #{l}" }
 
-      code += LogStash::Config::AST.defered_conditionals
+      code += LogStash::Config::AST.deferred_conditionals
 
       return code.join("\n")
     end
@@ -137,7 +147,9 @@ class Whitespace < Node; end
   class PluginSection < Node
     # Global plugin numbering for the janky instance variable naming we use
     # like @filter_<name>_1
-    @@i = 0
+    def initialize(*args)
+      super(*args)
+    end
 
     # Generate ruby code to initialize all the plugins.
     def compile_initializer
@@ -147,31 +159,31 @@ def compile_initializer
 
 
         code << <<-CODE
-          #{name} = #{plugin.compile_initializer}
-          @#{plugin.plugin_type}s << #{name}
+          @generated_objects[:#{name}] = #{plugin.compile_initializer}
+          @#{plugin.plugin_type}s << @generated_objects[:#{name}]
         CODE
 
         # The flush method for this filter.
         if plugin.plugin_type == "filter"
 
           code << <<-CODE
-            #{name}_flush = lambda do |options, &block|
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name})
+            @generated_objects[:#{name}_flush] = lambda do |options, &block|
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}])
 
-              events = #{name}.flush(options)
+              events = @generated_objects[:#{name}].flush(options)
 
               return if events.nil? || events.empty?
 
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name}, :events => events)
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}], :events => events.map { |x| x.to_hash  })
 
               #{plugin.compile_starting_here.gsub(/^/, "  ")}
 
               events.each{|e| block.call(e)}
             end
 
-            if #{name}.respond_to?(:flush)
-              @periodic_flushers << #{name}_flush if #{name}.periodic_flush
-              @shutdown_flushers << #{name}_flush
+            if @generated_objects[:#{name}].respond_to?(:flush)
+              @periodic_flushers << @generated_objects[:#{name}_flush] if @generated_objects[:#{name}].periodic_flush
+              @shutdown_flushers << @generated_objects[:#{name}_flush]
             end
           CODE
 
@@ -192,9 +204,10 @@ def generate_variables
 
       plugins.each do |plugin|
         # Unique number for every plugin.
-        @@i += 1
+        LogStash::Config::AST.plugin_instance_index += 1
         # store things as ivars, like @filter_grok_3
-        var = "@#{plugin.plugin_type}_#{plugin.plugin_name}_#{@@i}"
+        var = :"#{plugin.plugin_type}_#{plugin.plugin_name}_#{LogStash::Config::AST.plugin_instance_index}"
+        # puts("var=#{var.inspect}")
         @variables[plugin] = var
       end
       return @variables
@@ -236,13 +249,13 @@ def compile_initializer
     def compile
       case plugin_type
       when "input"
-        return "start_input(#{variable_name})"
+        return "start_input(@generated_objects[:#{variable_name}])"
       when "filter"
         return <<-CODE
-          events = #{variable_name}.multi_filter(events)
+          events = @generated_objects[:#{variable_name}].multi_filter(events)
         CODE
       when "output"
-        return "targeted_outputs << #{variable_name}\n"
+        return "targeted_outputs << @generated_objects[:#{variable_name}]\n"
       when "codec"
         settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
         attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
@@ -389,9 +402,9 @@ def compile
       type = recursive_select_parent(PluginSection).first.plugin_type.text_value
 
       if type == "filter"
-        i = LogStash::Config::AST.defered_conditionals_index += 1
+        i = LogStash::Config::AST.deferred_conditionals_index += 1
         source = <<-CODE
-          def cond_func_#{i}(input_events)
+          @generated_objects[:cond_func_#{i}] = lambda do |input_events|
             result = []
             input_events.each do |event|
               events = [event]
@@ -402,10 +415,10 @@ def cond_func_#{i}(input_events)
             result
           end
         CODE
-        LogStash::Config::AST.defered_conditionals << source
+        LogStash::Config::AST.deferred_conditionals << source
 
         <<-CODE
-          events = cond_func_#{i}(events)
+          events = @generated_objects[:cond_func_#{i}].call(events)
         CODE
       else # Output
         <<-CODE
@@ -512,7 +525,7 @@ def compile
   end
   class Selector < RValue
     def compile
-      return "event[#{text_value.inspect}]"
+      return "event.get(#{text_value.inspect})"
     end
   end
   class SelectorElement < Node; end
diff --git a/logstash-core/lib/logstash/config/file.rb b/logstash-core/lib/logstash/config/file.rb
index fb0d939dfde..6b5ae954675 100644
--- a/logstash-core/lib/logstash/config/file.rb
+++ b/logstash-core/lib/logstash/config/file.rb
@@ -2,17 +2,15 @@
 require "logstash/namespace"
 require "logstash/config/grammar"
 require "logstash/config/config_ast"
-require "logstash/config/registry"
 require "logstash/errors"
 require "logger"
 
 class LogStash::Config::File
   include Enumerable
-  attr_accessor :logger
+  include LogStash::Util::Loggable
 
   public
   def initialize(text)
-    @logger = Cabin::Channel.get(LogStash)
     @text = text
     @config = parse(text)
   end # def initialize
diff --git a/logstash-core/lib/logstash/config/loader.rb b/logstash-core/lib/logstash/config/loader.rb
index 37179518ed5..d894bd71bee 100644
--- a/logstash-core/lib/logstash/config/loader.rb
+++ b/logstash-core/lib/logstash/config/loader.rb
@@ -3,6 +3,7 @@
 module LogStash; module Config; class Loader
   def initialize(logger)
     @logger = logger
+    @config_debug = LogStash::SETTINGS.get_value("config.debug")
   end
 
   def format_config(config_path, config_string)
@@ -11,7 +12,18 @@ def format_config(config_path, config_string)
       # Append the config string.
       # This allows users to provide both -f and -e flags. The combination
       # is rare, but useful for debugging.
-      config_string = config_string + load_config(config_path)
+      loaded_config = load_config(config_path)
+      if loaded_config.empty? && config_string.empty?
+        # If loaded config from `-f` is empty *and* if config string is empty we raise an error
+        fail(I18n.t("logstash.runner.configuration.file-not-found", :path => config_path))
+      end
+
+      # tell the user we are merging, otherwise it is very confusing
+      if !loaded_config.empty? && !config_string.empty?
+        @logger.info("Created final config by merging config string and config path", :path => config_path)
+      end
+
+      config_string = config_string + loaded_config
     else
       # include a default stdin input if no inputs given
       if config_string !~ /input *{/
@@ -51,11 +63,12 @@ def local_config(path)
     path = ::File.expand_path(path)
     path = ::File.join(path, "*") if ::File.directory?(path)
 
+    config = ""
     if Dir.glob(path).length == 0
-      fail(I18n.t("logstash.runner.configuration.file-not-found", :path => path))
+      @logger.info("No config files found in path", :path => path)
+      return config
     end
 
-    config = ""
     encoding_issue_files = []
     Dir.glob(path).sort.each do |file|
       next unless ::File.file?(file)
@@ -69,14 +82,18 @@ def local_config(path)
         encoding_issue_files << file
       end
       config << cfg + "\n"
-      @logger.debug? && @logger.debug("\nThe following is the content of a file", :config_file => file.to_s)
-      @logger.debug? && @logger.debug("\n" + cfg + "\n\n")
+      if @config_debug
+        @logger.debug? && @logger.debug("\nThe following is the content of a file", :config_file => file.to_s)
+        @logger.debug? && @logger.debug("\n" + cfg + "\n\n")
+      end
     end
     if encoding_issue_files.any?
       fail("The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}")
     end
-    @logger.debug? && @logger.debug("\nThe following is the merged configuration")
-    @logger.debug? && @logger.debug("\n" + config + "\n\n")
+    if @config_debug
+      @logger.debug? && @logger.debug("\nThe following is the merged configuration")
+      @logger.debug? && @logger.debug("\n" + config + "\n\n")
+    end
     return config
   end # def load_config
 
diff --git a/logstash-core/lib/logstash/config/mixin.rb b/logstash-core/lib/logstash/config/mixin.rb
index 1d152062222..fdeae9c4aec 100644
--- a/logstash-core/lib/logstash/config/mixin.rb
+++ b/logstash-core/lib/logstash/config/mixin.rb
@@ -1,10 +1,12 @@
 # encoding: utf-8
 require "logstash/namespace"
-require "logstash/config/registry"
+require "logstash/plugins/registry"
 require "logstash/logging"
 require "logstash/util/password"
+require "logstash/util/safe_uri"
 require "logstash/version"
 require "logstash/environment"
+require "logstash/util/environment_variables"
 require "logstash/util/plugin_version"
 require "filesize"
 
@@ -38,7 +40,7 @@ module LogStash::Config::Mixin
   PLUGIN_VERSION_1_0_0 = LogStash::Util::PluginVersion.new(1, 0, 0)
   PLUGIN_VERSION_0_9_0 = LogStash::Util::PluginVersion.new(0, 9, 0)
 
-  ENV_PLACEHOLDER_REGEX = /\$(?<name>\w+)|\$\{(?<name>\w+)(\:(?<default>[^}]*))?\}/
+  ENV_PLACEHOLDER_REGEX = /\$\{(?<name>\w+)(\:(?<default>[^}]*))?\}/
 
   # This method is called when someone does 'include LogStash::Config'
   def self.included(base)
@@ -46,15 +48,32 @@ def self.included(base)
     base.extend(LogStash::Config::Mixin::DSL)
   end
 
+  # Recursive method to replace environment variable references in parameters
+  def deep_replace(value)
+    if (value.is_a?(Hash))
+      value.each do |valueHashKey, valueHashValue|
+        value[valueHashKey.to_s] = deep_replace(valueHashValue)
+      end
+    else
+      if (value.is_a?(Array))
+        value.each_index do | valueArrayIndex|
+          value[valueArrayIndex] = deep_replace(value[valueArrayIndex])
+        end
+      else
+        return replace_env_placeholders(value)
+      end
+    end
+  end
+
   def config_init(params)
     # Validation will modify the values inside params if necessary.
     # For example: converting a string to a number, etc.
-    
+
     # Keep a copy of the original config params so that we can later
     # differentiate between explicit configuration and implicit (default)
     # configuration.
-    @original_params = params.clone
-    
+    original_params = params.clone
+
     # store the plugin type, turns LogStash::Inputs::Base into 'input'
     @plugin_type = self.class.ancestors.find { |a| a.name =~ /::Base$/ }.config_name
 
@@ -64,19 +83,20 @@ def config_init(params)
       if opts && opts[:deprecated]
         extra = opts[:deprecated].is_a?(String) ? opts[:deprecated] : ""
         extra.gsub!("%PLUGIN%", self.class.config_name)
-        @logger.warn("You are using a deprecated config setting " +
+        self.logger.warn("You are using a deprecated config setting " +
                      "#{name.inspect} set in #{self.class.config_name}. " +
                      "Deprecated settings will continue to work, " +
                      "but are scheduled for removal from logstash " +
                      "in the future. #{extra} If you have any questions " +
                      "about this, please visit the #logstash channel " +
                      "on freenode irc.", :name => name, :plugin => self)
+
       end
       if opts && opts[:obsolete]
         extra = opts[:obsolete].is_a?(String) ? opts[:obsolete] : ""
         extra.gsub!("%PLUGIN%", self.class.config_name)
         raise LogStash::ConfigurationError,
-          I18n.t("logstash.agent.configuration.obsolete", :name => name,
+          I18n.t("logstash.runner.configuration.obsolete", :name => name,
                  :plugin => self.class.config_name, :extra => extra)
       end
     end
@@ -86,7 +106,7 @@ def config_init(params)
       next if params.include?(name.to_s)
       if opts.include?(:default) and (name.is_a?(Symbol) or name.is_a?(String))
         # default values should be cloned if possible
-        # cloning prevents 
+        # cloning prevents
         case opts[:default]
           when FalseClass, TrueClass, NilClass, Numeric
             params[name.to_s] = opts[:default]
@@ -103,21 +123,10 @@ def config_init(params)
 
     # Resolve environment variables references
     params.each do |name, value|
-      if (value.is_a?(Hash))
-        value.each do |valueHashKey, valueHashValue|
-          value[valueHashKey.to_s] = replace_env_placeholders(valueHashValue)
-        end
-      else
-        if (value.is_a?(Array))
-          value.each_index do |valueArrayIndex|
-            value[valueArrayIndex] = replace_env_placeholders(value[valueArrayIndex])
-          end
-        else
-          params[name.to_s] = replace_env_placeholders(value)
-        end
-      end
+      params[name.to_s] = deep_replace(value)
     end
 
+
     if !self.class.validate(params)
       raise LogStash::ConfigurationError,
         I18n.t("logstash.runner.configuration.invalid_plugin_settings")
@@ -138,10 +147,15 @@ def config_init(params)
       next if key[0, 1] == "@"
 
       # Set this key as an instance variable only if it doesn't start with an '@'
-      @logger.debug("config #{self.class.name}/@#{key} = #{value.inspect}")
+      self.logger.debug("config #{self.class.name}/@#{key} = #{value.inspect}")
       instance_variable_set("@#{key}", value)
     end
 
+    # now that we know the parameters are valid, we can obfuscate the original copy
+    # of the parameters before storing them as an instance variable
+    self.class.secure_params!(original_params)
+    @original_params = original_params
+
     @config = params
   end # def config_init
 
@@ -149,7 +163,6 @@ def config_init(params)
   # Process following patterns : $VAR, ${VAR}, ${VAR:defaultValue}
   def replace_env_placeholders(value)
     return value unless value.is_a?(String)
-    #raise ArgumentError, "Cannot replace ENV placeholders on non-strings. Got #{value.class}" if !value.is_a?(String)
 
     value.gsub(ENV_PLACEHOLDER_REGEX) do |placeholder|
       # Note: Ruby docs claim[1] Regexp.last_match is thread-local and scoped to
@@ -163,21 +176,23 @@ def replace_env_placeholders(value)
       if replacement.nil?
         raise LogStash::ConfigurationError, "Cannot evaluate `#{placeholder}`. Environment variable `#{name}` is not set and there is no default value given."
       end
-      @logger.info? && @logger.info("Evaluating environment variable placeholder", :placeholder => placeholder, :replacement => replacement)
       replacement
     end
   end # def replace_env_placeholders
 
   module DSL
+
+    include LogStash::Util::EnvironmentVariables
+
     attr_accessor :flags
 
     # If name is given, set the name and return it.
     # If no name given (nil), return the current name.
     def config_name(name = nil)
       @config_name = name if !name.nil?
-      LogStash::Config::Registry.registry[@config_name] = self
-      return @config_name
+      @config_name
     end
+    alias_method :config_plugin, :config_name
 
     # Deprecated: Declare the version of the plugin
     # inside the gemspec.
@@ -188,8 +203,7 @@ def plugin_status(status = nil)
     # Deprecated: Declare the version of the plugin
     # inside the gemspec.
     def milestone(m = nil)
-      @logger = Cabin::Channel.get(LogStash)
-      @logger.debug(I18n.t('logstash.plugin.deprecated_milestone', :plugin => config_name))
+      self.logger.debug(I18n.t('logstash.plugin.deprecated_milestone', :plugin => config_name))
     end
 
     # Define a new configuration setting
@@ -252,7 +266,6 @@ def inherited(subclass)
     def validate(params)
       @plugin_name = config_name
       @plugin_type = ancestors.find { |a| a.name =~ /::Base$/ }.config_name
-      @logger = Cabin::Channel.get(LogStash)
       is_valid = true
 
       print_version_notice
@@ -264,6 +277,7 @@ def validate(params)
       return is_valid
     end # def validate
 
+    # TODO: Remove in 6.0
     def print_version_notice
       return if @@version_notice_given
 
@@ -272,26 +286,22 @@ def print_version_notice
 
         if plugin_version < PLUGIN_VERSION_1_0_0
           if plugin_version < PLUGIN_VERSION_0_9_0
-            @logger.info(I18n.t("logstash.plugin.version.0-1-x", 
+            self.logger.info(I18n.t("logstash.plugin.version.0-1-x",
                                 :type => @plugin_type,
                                 :name => @config_name,
                                 :LOGSTASH_VERSION => LOGSTASH_VERSION))
           else
-            @logger.info(I18n.t("logstash.plugin.version.0-9-x", 
+            self.logger.info(I18n.t("logstash.plugin.version.0-9-x",
                                 :type => @plugin_type,
                                 :name => @config_name,
                                 :LOGSTASH_VERSION => LOGSTASH_VERSION))
           end
         end
       rescue LogStash::PluginNoVersionError
-        # If we cannot find a version in the currently installed gems we
-        # will display this message. This could happen in the test, if you 
-        # create an anonymous class to test a plugin.
-        @logger.warn(I18n.t("logstash.plugin.no_version",
-                                :type => @plugin_type,
-                                :name => @config_name,
-                                :LOGSTASH_VERSION => LOGSTASH_VERSION))
-      ensure 
+        # This can happen because of one of the following:
+        # - The plugin is loaded from the plugins.path and contains no gemspec.
+        # - The plugin is defined in a universal plugin, so the loaded plugin doesn't correspond to an actual gemspec.
+      ensure
         @@version_notice_given = true
       end
     end
@@ -311,65 +321,95 @@ def validate_check_invalid_parameter_names(params)
 
       if invalid_params.size > 0
         invalid_params.each do |name|
-          @logger.error("Unknown setting '#{name}' for #{@plugin_name}")
+          self.logger.error("Unknown setting '#{name}' for #{@plugin_name}")
         end
         return false
       end # if invalid_params.size > 0
       return true
     end # def validate_check_invalid_parameter_names
 
+    def validate_check_required_parameter(config_key, config_opts, k, v)
+      if config_key.is_a?(Regexp)
+        (k =~ config_key && v)
+      elsif config_key.is_a?(String)
+        k && v
+      end
+    end
+
     def validate_check_required_parameter_names(params)
       is_valid = true
 
       @config.each do |config_key, config|
         next unless config[:required]
 
-        if config_key.is_a?(Regexp)
-          next if params.keys.select { |k| k =~ config_key }.length > 0
-        elsif config_key.is_a?(String)
-          next if params.keys.member?(config_key)
+        if config_key.is_a?(Regexp) && !params.keys.any? { |k| k =~ config_key }
+          is_valid = false
+        end
+
+        value = params[config_key]
+        if value.nil? || (config[:list] && Array(value).empty?)
+          self.logger.error(I18n.t("logstash.runner.configuration.setting_missing",
+                               :setting => config_key, :plugin => @plugin_name,
+                               :type => @plugin_type))
+          is_valid = false
         end
-        @logger.error(I18n.t("logstash.runner.configuration.setting_missing",
-                             :setting => config_key, :plugin => @plugin_name,
-                             :type => @plugin_type))
-        is_valid = false
       end
 
       return is_valid
     end
 
+    def process_parameter_value(value, config_settings)
+      config_val = config_settings[:validate]
+
+      if config_settings[:list]
+        value = Array(value) # coerce scalars to lists
+        # Empty lists are converted to nils
+        return true, nil if value.empty?
+
+        validated_items = value.map {|v| validate_value(v, config_val)}
+        is_valid = validated_items.all? {|sr| sr[0] }
+        processed_value = validated_items.map {|sr| sr[1]}
+      else
+        is_valid, processed_value = validate_value(value, config_val)
+      end
+
+      return [is_valid, processed_value]
+    end
+
     def validate_check_parameter_values(params)
-      # Filter out parametrs that match regexp keys.
+      # Filter out parameters that match regexp keys.
       # These are defined in plugins like this:
-      #   config /foo.*/ => ... 
-      is_valid = true
+      #   config /foo.*/ => ...
+      all_params_valid = true
 
       params.each do |key, value|
         @config.keys.each do |config_key|
           next unless (config_key.is_a?(Regexp) && key =~ config_key) \
                       || (config_key.is_a?(String) && key == config_key)
-          config_val = @config[config_key][:validate]
-          #puts "  Key matches."
-          success, result = validate_value(value, config_val)
-          if success 
-            # Accept coerced value if success
+
+          config_settings = @config[config_key]
+
+          is_valid, processed_value = process_parameter_value(value, config_settings)
+
+          if is_valid
+            # Accept coerced value if valid
             # Used for converting values in the config to proper objects.
-            params[key] = result if !result.nil?
+            params[key] = processed_value
           else
-            @logger.error(I18n.t("logstash.runner.configuration.setting_invalid",
+            self.logger.error(I18n.t("logstash.runner.configuration.setting_invalid",
                                  :plugin => @plugin_name, :type => @plugin_type,
                                  :setting => key, :value => value.inspect,
-                                 :value_type => config_val,
-                                 :note => result))
+                                 :value_type => config_settings[:validate],
+                                 :note => processed_value))
           end
-          #puts "Result: #{key} / #{result.inspect} / #{success}"
-          is_valid &&= success
+
+          all_params_valid &&= is_valid
 
           break # done with this param key
         end # config.each
       end # params.each
 
-      return is_valid
+      return all_params_valid
     end # def validate_check_parameter_values
 
     def validator_find(key)
@@ -389,8 +429,10 @@ def validate_value(value, validator)
       # (see LogStash::Inputs::File for example)
       result = nil
 
+      value = deep_replace(value)
+
       if validator.nil?
-        return true
+        return true, value
       elsif validator.is_a?(Array)
         value = [*value]
         if value.size > 1
@@ -402,7 +444,7 @@ def validate_value(value, validator)
         end
         result = value.first
       elsif validator.is_a?(Symbol)
-        # TODO(sissel): Factor this out into a coersion method?
+        # TODO(sissel): Factor this out into a coercion method?
         # TODO(sissel): Document this stuff.
         value = hash_or_array(value)
 
@@ -504,6 +546,12 @@ def validate_value(value, validator)
             end
 
             result = value.first.is_a?(::LogStash::Util::Password) ? value.first : ::LogStash::Util::Password.new(value.first)
+          when :uri
+            if value.size > 1
+              return false, "Expected uri (one value), got #{value.size} values?"
+            end
+
+            result = value.first.is_a?(::LogStash::Util::SafeURI) ? value.first : ::LogStash::Util::SafeURI.new(value.first)
           when :path
             if value.size > 1 # Only 1 value wanted
               return false, "Expected path (one value), got #{value.size} values?"
@@ -537,6 +585,15 @@ def validate_value(value, validator)
       return true, result
     end # def validate_value
 
+    def secure_params!(params)
+      params.each do |key, value|
+        if [:uri, :password].include? @config[key][:validate]
+          is_valid, processed_value = process_parameter_value(value, @config[key])
+          params[key] = processed_value
+        end
+      end
+    end
+
     def hash_or_array(value)
       if !value.is_a?(Hash)
         value = [*value] # coerce scalar to array if necessary
diff --git a/logstash-core/lib/logstash/config/registry.rb b/logstash-core/lib/logstash/config/registry.rb
deleted file mode 100644
index 8463716cf06..00000000000
--- a/logstash-core/lib/logstash/config/registry.rb
+++ /dev/null
@@ -1,13 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-
-# Global config registry.
-module LogStash::Config::Registry
-  @registry = Hash.new
-  class << self
-    attr_accessor :registry
-
-    # TODO(sissel): Add some helper methods here.
-  end
-end # module LogStash::Config::Registry
-  
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 79e7f24d86c..3ddca09ea65 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -1,13 +1,83 @@
 # encoding: utf-8
 require "logstash/errors"
+require "logstash/java_integration"
+require "logstash/config/cpu_core_strategy"
+require "logstash/settings"
+require "socket"
+require "stud/temporary"
 
 module LogStash
+  # In the event that we're requiring this file without bootstrap/environment.rb
+  if !defined?(LogStash::Environment::LOGSTASH_HOME)
+    module Environment
+      LOGSTASH_HOME = Stud::Temporary.directory("logstash-home")
+      Dir.mkdir(::File.join(LOGSTASH_HOME, "data"))
+    end
+  end
+
+  [
+            Setting::String.new("node.name", Socket.gethostname),
+    Setting::NullableString.new("path.config", nil, false),
+ Setting::WritableDirectory.new("path.data", ::File.join(LogStash::Environment::LOGSTASH_HOME, "data")),
+    Setting::NullableString.new("config.string", nil, false),
+           Setting::Boolean.new("config.test_and_exit", false),
+           Setting::Boolean.new("config.reload.automatic", false),
+           Setting::Numeric.new("config.reload.interval", 3), # in seconds
+           Setting::Boolean.new("metric.collect", true),
+            Setting::String.new("pipeline.id", "main"),
+            Setting::Boolean.new("pipeline.system", false),
+   Setting::PositiveInteger.new("pipeline.workers", LogStash::Config::CpuCoreStrategy.maximum),
+   Setting::PositiveInteger.new("pipeline.output.workers", 1),
+   Setting::PositiveInteger.new("pipeline.batch.size", 125),
+           Setting::Numeric.new("pipeline.batch.delay", 5), # in milliseconds
+           Setting::Boolean.new("pipeline.unsafe_shutdown", false),
+                    Setting.new("path.plugins", Array, []),
+    Setting::NullableString.new("interactive", nil, false),
+           Setting::Boolean.new("config.debug", false),
+            Setting::String.new("log.level", "info", true, ["fatal", "error", "warn", "debug", "info", "trace"]),
+           Setting::Boolean.new("version", false),
+           Setting::Boolean.new("help", false),
+            Setting::String.new("log.format", "plain", true, ["json", "plain"]),
+            Setting::String.new("http.host", "127.0.0.1"),
+            Setting::PortRange.new("http.port", 9600..9700),
+            Setting::String.new("http.environment", "production"),
+            Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked"]),
+            Setting::Boolean.new("queue.drain", false),
+            Setting::Bytes.new("queue.page_capacity", "250mb"),
+            Setting::Bytes.new("queue.max_bytes", "1024mb"),
+            Setting::Numeric.new("queue.max_events", 0), # 0 is unlimited
+            Setting::Numeric.new("queue.checkpoint.acks", 1024), # 0 is unlimited
+            Setting::Numeric.new("queue.checkpoint.writes", 1024), # 0 is unlimited
+            Setting::Numeric.new("queue.checkpoint.interval", 1000), # 0 is no time-based checkpointing
+            Setting::TimeValue.new("slowlog.threshold.warn", "-1"),
+            Setting::TimeValue.new("slowlog.threshold.info", "-1"),
+            Setting::TimeValue.new("slowlog.threshold.debug", "-1"),
+            Setting::TimeValue.new("slowlog.threshold.trace", "-1")
+  ].each {|setting| SETTINGS.register(setting) }
+
+  # Compute the default queue path based on `path.data`
+  default_queue_file_path = ::File.join(SETTINGS.get("path.data"), "queue")
+  SETTINGS.register Setting::WritableDirectory.new("path.queue", default_queue_file_path)
+  
+  SETTINGS.on_post_process do |settings|
+    # If the data path is overridden but the queue path isn't recompute the queue path
+    # We need to do this at this stage because of the weird execution order
+    # our monkey-patched Clamp follows
+    if settings.set?("path.data") && !settings.set?("path.queue")
+      settings.set_value("path.queue", ::File.join(settings.get("path.data"), "queue"))
+    end
+  end
+
   module Environment
     extend self
 
     LOGSTASH_CORE = ::File.expand_path(::File.join(::File.dirname(__FILE__), "..", ".."))
     LOGSTASH_ENV = (ENV["LS_ENV"] || 'production').to_s.freeze
 
+    LINUX_OS_RE = /linux/
+    WINDOW_OS_RE = /mswin|msys|mingw|cygwin|bccwin|wince|emc/
+    MACOS_OS_RE = /darwin/
+
     def env
       LOGSTASH_ENV
     end
@@ -70,7 +140,11 @@ def jruby?
     end
 
     def windows?
-      ::Gem.win_platform?
+      RbConfig::CONFIG['host_os'] =~ WINDOW_OS_RE
+    end
+
+    def linux?
+      RbConfig::CONFIG['host_os'] =~ LINUX_OS_RE
     end
 
     def locales_path(path)
diff --git a/logstash-core/lib/logstash/event.rb b/logstash-core/lib/logstash/event.rb
new file mode 100644
index 00000000000..6e892d27826
--- /dev/null
+++ b/logstash-core/lib/logstash/event.rb
@@ -0,0 +1,56 @@
+# encoding: utf-8
+
+require "logstash/namespace"
+require "logstash/json"
+require "jruby_event_ext"
+require "jruby_timestamp_ext"
+require "logstash/timestamp"
+require "logstash/string_interpolation"
+
+# transient pipeline events for normal in-flow signaling as opposed to
+# flow altering exceptions. for now having base classes is adequate and
+# in the future it might be necessary to refactor using like a BaseEvent
+# class to have a common interface for all pipeline events to support
+# eventual queueing persistence for example, TBD.
+module LogStash
+  class SignalEvent
+    def flush?; raise "abstract method"; end;
+    def shutdown?; raise "abstract method"; end;
+  end
+
+  class ShutdownEvent < SignalEvent
+    def flush?; false; end;
+    def shutdown?; true; end;
+  end
+
+  class FlushEvent < SignalEvent
+    def flush?; true; end;
+    def shutdown?; false; end;
+  end
+
+  class NoSignal < SignalEvent
+    def flush?; false; end;
+    def shutdown?; false; end;
+  end
+
+  FLUSH = FlushEvent.new
+  SHUTDOWN = ShutdownEvent.new
+  NO_SIGNAL = NoSignal.new
+
+  class Event
+    MSG_BRACKETS_METHOD_MISSING = "Direct event field references (i.e. event['field']) have been disabled in favor of using event get and set methods (e.g. event.get('field')). Please consult the Logstash 5.0 breaking changes documentation for more details.".freeze
+    MSG_BRACKETS_EQUALS_METHOD_MISSING = "Direct event field references (i.e. event['field'] = 'value') have been disabled in favor of using event get and set methods (e.g. event.set('field', 'value')). Please consult the Logstash 5.0 breaking changes documentation for more details.".freeze
+    RE_BRACKETS_METHOD = /^\[\]$/.freeze
+    RE_BRACKETS_EQUALS_METHOD = /^\[\]=$/.freeze
+
+    def method_missing(method_name, *arguments, &block)
+      if RE_BRACKETS_METHOD.match(method_name.to_s)
+        raise NoMethodError.new(MSG_BRACKETS_METHOD_MISSING)
+      end
+      if RE_BRACKETS_EQUALS_METHOD.match(method_name.to_s)
+        raise NoMethodError.new(MSG_BRACKETS_EQUALS_METHOD_MISSING)
+      end
+      super
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/event_dispatcher.rb b/logstash-core/lib/logstash/event_dispatcher.rb
new file mode 100644
index 00000000000..9d68cc3efa4
--- /dev/null
+++ b/logstash-core/lib/logstash/event_dispatcher.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+module LogStash
+  class EventDispatcher
+    java_import "java.util.concurrent.CopyOnWriteArraySet"
+
+    attr_reader :emitter
+
+    def initialize(emitter)
+      @emitter = emitter
+      @listeners = CopyOnWriteArraySet.new
+    end
+
+    # This operation is slow because we use a CopyOnWriteArrayList
+    # But the majority of the addition will be done at bootstrap time
+    # So add_listener shouldn't be called often at runtime.
+    #
+    # On the other hand the notification could be called really often.
+    def add_listener(listener)
+      @listeners.add(listener)
+    end
+
+    # This operation is slow because we use a `CopyOnWriteArrayList` as the backend, instead of a
+    # ConcurrentHashMap, but since we are mostly adding stuff and iterating the `CopyOnWriteArrayList`
+    # should provide a better performance.
+    #
+    # See note on add_listener, this method shouldn't be called really often.
+    def remove_listener(listener)
+      @listeners.remove(listener)
+    end
+
+    def fire(method_name, *arguments)
+      @listeners.each do |listener|
+        if listener.respond_to?(method_name)
+          listener.send(method_name, emitter, *arguments)
+        end
+      end
+    end
+    alias_method :execute, :fire
+  end
+end
diff --git a/logstash-core/lib/logstash/execution_context.rb b/logstash-core/lib/logstash/execution_context.rb
new file mode 100644
index 00000000000..ad1579f40e6
--- /dev/null
+++ b/logstash-core/lib/logstash/execution_context.rb
@@ -0,0 +1,10 @@
+# encoding: utf-8
+module LogStash
+  class ExecutionContext
+    attr_reader :pipeline_id
+
+    def initialize(pipeline_id)
+      @pipeline_id = pipeline_id
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/filter_delegator.rb b/logstash-core/lib/logstash/filter_delegator.rb
index 132d03f933e..fb0569ae67e 100644
--- a/logstash-core/lib/logstash/filter_delegator.rb
+++ b/logstash-core/lib/logstash/filter_delegator.rb
@@ -9,22 +9,24 @@ class FilterDelegator
       :threadsafe?,
       :do_close,
       :do_stop,
-      :periodic_flush
+      :periodic_flush,
+      :reloadable?
     ]
     def_delegators :@filter, *DELEGATED_METHODS
 
-    def initialize(logger, klass, metric, *args)
-      options = args.reduce({}, :merge)
-
+    def initialize(logger, klass, metric, execution_context, plugin_args)
       @logger = logger
       @klass = klass
-      @filter = klass.new(options)
+      @id = plugin_args["id"]
+      @filter = klass.new(plugin_args)
 
       # Scope the metrics to the plugin
-      namespaced_metric = metric.namespace(@filter.plugin_unique_name.to_sym)
-      @filter.metric = metric
+      namespaced_metric = metric.namespace(@id.to_sym)
+      @filter.metric = namespaced_metric
+      @filter.execution_context = execution_context
 
       @metric_events = namespaced_metric.namespace(:events)
+      namespaced_metric.gauge(:name, config_name)
 
       # Not all the filters will do bufferings
       define_flush_method if @filter.respond_to?(:flush)
@@ -37,9 +39,11 @@ def config_name
     def multi_filter(events)
       @metric_events.increment(:in, events.size)
 
+      clock = @metric_events.time(:duration_in_millis)
       new_events = @filter.multi_filter(events)
+      clock.stop
 
-      # There is no garantee in the context of filter
+      # There is no guarantee in the context of filter
       # that EVENTS_INT == EVENTS_OUT, see the aggregates and
       # the split filter
       c = new_events.count { |event| !event.cancelled? }
diff --git a/logstash-core/lib/logstash/filters/base.rb b/logstash-core/lib/logstash/filters/base.rb
index ae6616ddf01..8b1ab5e07b4 100644
--- a/logstash-core/lib/logstash/filters/base.rb
+++ b/logstash-core/lib/logstash/filters/base.rb
@@ -7,6 +7,7 @@
 require "logstash/util/decorators"
 
 class LogStash::Filters::Base < LogStash::Plugin
+  include LogStash::Util::Loggable
   include LogStash::Config::Mixin
 
   config_name "filter"
@@ -117,6 +118,10 @@ class LogStash::Filters::Base < LogStash::Plugin
   # Optional.
   config :periodic_flush, :validate => :boolean, :default => false
 
+  def self.plugin_type
+    "filter"
+  end
+
   public
   def initialize(params)
     super
@@ -134,6 +139,14 @@ def filter(event)
     raise "#{self.class}#filter must be overidden"
   end # def filter
 
+  public
+  def do_filter(event, &block)
+    time = java.lang.System.nanoTime
+    filter(event, &block)
+    @slow_logger.on_event("event processing time", @original_params, event, java.lang.System.nanoTime - time)
+  end
+
+
   # in 1.5.0 multi_filter is meant to be used in the generated filter function in LogStash::Config::AST::Plugin only
   # and is temporary until we refactor the filter method interface to accept events list and return events list,
   # just list in multi_filter see https://github.com/elastic/logstash/issues/2872.
@@ -148,7 +161,7 @@ def multi_filter(events)
     events.each do |event|
       unless event.cancelled?
         result << event
-        filter(event){|new_event| result << new_event}
+        do_filter(event){|new_event| result << new_event}
       end
     end
     result
@@ -156,7 +169,7 @@ def multi_filter(events)
 
   public
   def execute(event, &block)
-    filter(event, &block)
+    do_filter(event, &block)
   end # def execute
 
   public
@@ -182,14 +195,20 @@ def filter_matched(event)
     # note below that the tags array field needs to be updated then reassigned to the event.
     # this is important because a construct like event["tags"].delete(tag) will not work
     # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
+
+    tags = event.get("tags")
+    return unless tags
+
+    tags = Array(tags)
     @remove_tag.each do |tag|
-      tags = event["tags"]
-      break if tags.nil? || tags.empty?
+      break if tags.empty?
+
       tag = event.sprintf(tag)
       @logger.debug? and @logger.debug("filters/#{self.class.name}: removing tag", :tag => tag)
       tags.delete(tag)
-      event["tags"] = tags
     end
+
+    event.set("tags", tags)
   end # def filter_matched
 
   protected
diff --git a/logstash-core/lib/logstash/inputs/base.rb b/logstash-core/lib/logstash/inputs/base.rb
index 414cd714784..3db155fb3d0 100644
--- a/logstash-core/lib/logstash/inputs/base.rb
+++ b/logstash-core/lib/logstash/inputs/base.rb
@@ -9,7 +9,9 @@
 
 # This is the base class for Logstash inputs.
 class LogStash::Inputs::Base < LogStash::Plugin
+  include LogStash::Util::Loggable
   include LogStash::Config::Mixin
+
   config_name "input"
 
   # Add a `type` field to all events handled by this input.
@@ -48,6 +50,10 @@ class LogStash::Inputs::Base < LogStash::Plugin
   attr_accessor :params
   attr_accessor :threadable
 
+  def self.plugin_type
+    "input"
+  end
+
   public
   def initialize(params={})
     super
@@ -78,21 +84,35 @@ def stop
 
   public
   def do_stop
-    @logger.debug("stopping", :plugin => self)
+    @logger.debug("stopping", :plugin => self.class.name)
     @stop_called.make_true
     stop
   end
 
-  # stop? should never be overriden
+  # stop? should never be overridden
   public
   def stop?
     @stop_called.value
   end
+  
+  def clone
+    cloned = super
+    cloned.codec = @codec.clone if @codec
+    cloned
+  end
+
+  def execution_context=(context)
+    super
+    # There is no easy way to propage an instance variable into the codec, because the codec
+    # are created at the class level
+    @codec.execution_context = context
+    context
+  end
 
   protected
   def decorate(event)
     # Only set 'type' if not already set. This is backwards-compatible behavior
-    event["type"] = @type if @type && !event.include?("type")
+    event.set("type", @type) if @type && !event.include?("type")
 
     LogStash::Util::Decorators.add_fields(@add_field,event,"inputs/#{self.class.name}")
     LogStash::Util::Decorators.add_tags(@tags,event,"inputs/#{self.class.name}")
diff --git a/logstash-core/lib/logstash/inputs/metrics.rb b/logstash-core/lib/logstash/inputs/metrics.rb
deleted file mode 100644
index 8a8ce92dcf0..00000000000
--- a/logstash-core/lib/logstash/inputs/metrics.rb
+++ /dev/null
@@ -1,47 +0,0 @@
-# encoding: utf-8
-require "logstash/event"
-require "logstash/inputs/base"
-require "logstash/instrument/collector"
-
-module LogStash module Inputs
-  # The Metrics inputs is responable of registring itself to the collector.
-  # The collector class will periodically emits new snapshot of the system,
-  # The metrics need to take that information and transform it into
-  # a `Logstash::Event`, which can be consumed by the shipper and send to
-  # Elasticsearch
-  class Metrics < LogStash::Inputs::Base
-    config_name "metrics"
-    milestone 3
-
-    def register
-    end
-
-    def run(queue)
-      @logger.debug("Metric: input started")
-      @queue = queue
-
-      # we register to the collector after receiving the pipeline queue
-      LogStash::Instrument::Collector.instance.add_observer(self)
-
-      # Keep this plugin thread alive,
-      # until we shutdown the metric pipeline
-      sleep(1) while !stop?
-    end
-
-    def stop
-      @logger.debug("Metrics input: stopped")
-      LogStash::Instrument::Collector.instance.delete_observer(self)
-    end
-
-    def update(snapshot)
-      @logger.debug("Metrics input: received a new snapshot", :created_at => snapshot.created_at, :snapshot => snapshot, :event => snapshot.metric_store.to_event) if @logger.debug?
-
-      # The back pressure is handled in the collector's
-      # scheduled task (running into his own thread) if something append to one of the listener it will
-      # will timeout. In a sane pipeline, with a low traffic of events it shouldn't be a problems.
-      snapshot.metric_store.each do |metric|
-        @queue << LogStash::Event.new({ "@timestamp" => snapshot.created_at }.merge(metric.to_hash))
-      end
-    end
-  end
-end;end
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
index 614ba372a40..08e72599f3d 100644
--- a/logstash-core/lib/logstash/instrument/collector.rb
+++ b/logstash-core/lib/logstash/instrument/collector.rb
@@ -8,16 +8,11 @@
 require "thread"
 
 module LogStash module Instrument
-  # The Collector singleton is the single point of reference for all
+  # The Collector is the single point of reference for all
   # the metrics collection inside logstash, the metrics library will make
   # direct calls to this class.
-  #
-  # This class is an observable responsable of periodically emitting view of the system
-  # to other components like the internal metrics pipelines.
   class Collector
     include LogStash::Util::Loggable
-    include Observable
-    include Singleton
 
     SNAPSHOT_ROTATION_TIME_SECS = 1 # seconds
     SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS = 10 * 60 # seconds
@@ -27,7 +22,6 @@ class Collector
     def initialize
       @metric_store = MetricStore.new
       @agent = nil
-      start_periodic_snapshotting
     end
 
     # The metric library will call this unique interface
@@ -35,7 +29,7 @@ def initialize
     # of update the metric
     #
     # If there is a problem with the key or the type of metric we will record an error
-    # but we wont stop processing events, theses errors are not considered fatal.
+    # but we won't stop processing events, theses errors are not considered fatal.
     #
     def push(namespaces_path, key, type, *metric_type_params)
       begin
@@ -44,8 +38,6 @@ def push(namespaces_path, key, type, *metric_type_params)
         end
 
         metric.execute(*metric_type_params)
-
-        changed # we had changes coming in so we can notify the observers
       rescue MetricStore::NamespacesExpectedError => e
         logger.error("Collector: Cannot record metric", :exception => e)
       rescue NameError => e
@@ -59,51 +51,17 @@ def push(namespaces_path, key, type, *metric_type_params)
       end
     end
 
-    def clear
-      @metric_store = MetricStore.new
-    end
-
-    # Monitor the `Concurrent::TimerTask` this update is triggered on every successful or not
-    # run of the task, TimerTask implement Observable and the collector acts as
-    # the observer and will keep track if something went wrong in the execution.
-    #
-    # @param [Time] Time of execution
-    # @param [result] Result of the execution
-    # @param [Exception] Exception
-    def update(time_of_execution, result, exception)
-      return true if exception.nil?
-      logger.error("Collector: Something went wrong went sending data to the observers",
-                   :execution_time => time_of_execution,
-                   :result => result,
-                   :exception => exception)
-    end
-
     # Snapshot the current Metric Store and return it immediately,
     # This is useful if you want to get access to the current metric store without
     # waiting for a periodic call.
     #
     # @return [LogStash::Instrument::MetricStore]
     def snapshot_metric
-      Snapshot.new(@metric_store)
+      Snapshot.new(@metric_store.dup)
     end
 
-    # Configure and start the periodic task for snapshotting the `MetricStore`
-    def start_periodic_snapshotting
-      @snapshot_task = Concurrent::TimerTask.new { publish_snapshot }
-      @snapshot_task.execution_interval = SNAPSHOT_ROTATION_TIME_SECS
-      @snapshot_task.timeout_interval = SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS
-      @snapshot_task.add_observer(self)
-      @snapshot_task.execute
-    end
-
-    # Create a snapshot of the MetricStore and send it to to the registered observers
-    # The observer will receive the following signature in the update methode.
-    #
-    # `#update(created_at, metric_store)`
-    def publish_snapshot
-      created_at = Time.now
-      logger.debug("Collector: Sending snapshot to observers", :created_at => created_at) if logger.debug?
-      notify_observers(snapshot_metric)
+    def clear(keypath)
+      @metric_store.prune(keypath)
     end
   end
 end; end
diff --git a/logstash-core/lib/logstash/instrument/metric.rb b/logstash-core/lib/logstash/instrument/metric.rb
index 601c7b0ed4b..d31a8613dad 100644
--- a/logstash-core/lib/logstash/instrument/metric.rb
+++ b/logstash-core/lib/logstash/instrument/metric.rb
@@ -13,27 +13,27 @@ class MetricNoNamespaceProvided < MetricException; end
   class Metric
     attr_reader :collector
 
-    def initialize(collector = LogStash::Instrument::Collector.instance)
+    def initialize(collector)
       @collector = collector
     end
 
     def increment(namespace, key, value = 1)
-      validate_key!(key)
+      self.class.validate_key!(key)
       collector.push(namespace, key, :counter, :increment, value)
     end
 
     def decrement(namespace, key, value = 1)
-      validate_key!(key)
+      self.class.validate_key!(key)
       collector.push(namespace, key, :counter, :decrement, value)
     end
 
     def gauge(namespace, key, value)
-      validate_key!(key)
+      self.class.validate_key!(key)
       collector.push(namespace, key, :gauge, :set, value)
     end
 
     def time(namespace, key)
-      validate_key!(key)
+      self.class.validate_key!(key)
 
       if block_given?
         timer = TimedExecution.new(self, namespace, key)
@@ -46,7 +46,8 @@ def time(namespace, key)
     end
 
     def report_time(namespace, key, duration)
-      collector.push(namespace, key, :mean, :increment, duration)
+      self.class.validate_key!(key)
+      collector.push(namespace, key, :counter, :increment, duration)
     end
 
     # This method return a metric instance tied to a specific namespace
@@ -69,19 +70,19 @@ def namespace(name)
       NamespacedMetric.new(self, name)
     end
 
-    private
-    def validate_key!(key)
+    def self.validate_key!(key)
       raise MetricNoKeyProvided if key.nil? || key.empty?
     end
 
+    private
     # Allow to calculate the execution of a block of code.
     # This class support 2 differents syntax a block or the return of
-    # the object itself, but in the later case the metric wont be recorded
+    # the object itself, but in the later case the metric won't be recorded
     # Until we call `#stop`.
     #
     # @see LogStash::Instrument::Metric#time
     class TimedExecution
-      MILLISECONDS = 1_000_000.0.freeze
+      MILLISECONDS = 1_000.0.freeze
 
       def initialize(metric, namespace, key)
         @metric = metric
@@ -95,7 +96,9 @@ def start
       end
 
       def stop
-        @metric.report_time(@namespace, @key, (MILLISECONDS * (Time.now - @start_time)).to_i)
+        execution_time = (MILLISECONDS * (Time.now - @start_time)).to_i
+        @metric.report_time(@namespace, @key, execution_time)
+        execution_time
       end
     end
   end
diff --git a/logstash-core/lib/logstash/instrument/metric_store.rb b/logstash-core/lib/logstash/instrument/metric_store.rb
index 53ff0cd6668..119297c53ed 100644
--- a/logstash-core/lib/logstash/instrument/metric_store.rb
+++ b/logstash-core/lib/logstash/instrument/metric_store.rb
@@ -1,7 +1,7 @@
 # encoding: utf-8
 require "concurrent"
-require "logstash/event"
 require "logstash/instrument/metric_type"
+require "thread"
 
 module LogStash module Instrument
   # The Metric store the data structure that make sure the data is
@@ -25,6 +25,12 @@ def initialize
       # This hash has only one dimension
       # and allow fast retrieval of the metrics
       @fast_lookup = Concurrent::Map.new
+
+      # This Mutex block the critical section for the
+      # structured hash, it block the zone when we first insert a metric
+      # in the structured hash or when we query it for search or to make
+      # the result available in the API.
+      @structured_lookup_mutex = Mutex.new
     end
 
     # This method use the namespace and key to search the corresponding value of
@@ -46,16 +52,13 @@ def fetch_or_store(namespaces, key, default_value = nil)
       # BUT. If the value is not present in the `@fast_lookup` the value will be inserted and
       # `#puf_if_absent` will return nil. With this returned value of nil we assume that we don't
       # have it in the `@metric_store` for structured search so we add it there too.
-      #
-      # The problem with only using the `@metric_store` directly all the time would require us
-      # to use the mutex around the structure since its a multi-level hash, without that it wont
-      # return a consistent value of the metric and this would slow down the code and would
-      # complixity the code.
-      if found_value = @fast_lookup.put_if_absent([namespaces, key], provided_value)
+      if found_value = @fast_lookup.put_if_absent(namespaces.dup << key, provided_value)
         return found_value
       else
-        # If we cannot find the value this mean we need to save it in the store.
-        fetch_or_store_namespaces(namespaces).fetch_or_store(key, provided_value)
+        @structured_lookup_mutex.synchronize do
+          # If we cannot find the value this mean we need to save it in the store.
+          fetch_or_store_namespaces(namespaces).fetch_or_store(key, provided_value)
+        end
         return provided_value
       end
     end
@@ -70,30 +73,99 @@ def fetch_or_store(namespaces, key, default_value = nil)
     # If you use the `,` on a key the metric store will return the both values at that level
     #
     # The returned hash will keep the same structure as it had in the `Concurrent::Map`
-    # but will be a normal ruby hash. This will allow the api to easily seriliaze the content
+    # but will be a normal ruby hash. This will allow the api to easily serialize the content
     # of the map
     #
     # @param [Array] The path where values should be located
     # @return [Hash]
     def get_with_path(path)
-      key_paths = path.gsub(/^#{KEY_PATH_SEPARATOR}+/, "").split(KEY_PATH_SEPARATOR)
-      get(*key_paths)
+      get(*key_paths(path))
     end
 
     # Similar to `get_with_path` but use symbols instead of string
     #
-    # @param [Array<Symbol>
+    # @param [Array<Symbol>]
     # @return [Hash]
     def get(*key_paths)
       # Normalize the symbols access
       key_paths.map(&:to_sym)
       new_hash = Hash.new
 
-      get_recursively(key_paths, @store, new_hash)
+      @structured_lookup_mutex.synchronize do
+        get_recursively(key_paths, @store, new_hash)
+      end
 
       new_hash
     end
 
+    # Retrieve values like `get`, but don't return them fully nested.
+    # This means that if you call `get_shallow(:foo, :bar)` the result will not
+    # be nested inside of `{:foo {:bar => values}`.
+    #
+    # @param [Array<Symbol>]
+    # @return [Hash]
+    def get_shallow(*key_paths)
+      key_paths.reduce(get(*key_paths)) {|acc, p| acc[p]}
+    end
+
+
+    # Return a hash including the values of the keys given at the path given
+    # 
+    # Example Usage:
+    # extract_metrics(
+    #   [:jvm, :process],
+    #   :open_file_descriptors,
+    #   [:cpu, [:total_in_millis, :percent]]
+    #   [:pipelines, [:one, :two], :size]
+    # )
+    # 
+    # Returns:
+    # # From the jvm.process metrics namespace
+    # {
+    #   :open_file_descriptors => 123
+    #   :cpu => { :total_in_millis => 456, :percent => 789 }
+    #   :pipelines => {
+    #                   :one => {:size => 90210},
+    #                   :two => {:size => 8675309}
+    #                 }
+    # }
+    def extract_metrics(path, *keys)
+      keys.reduce({}) do |acc,k|
+        # Simplify 1-length keys
+        k = k.first if k.is_a?(Array) && k.size == 1
+
+        # If we have array values here we need to recurse
+        # There are two levels of looping here, one for the paths we might pass in
+        # one for the upcoming keys we might pass in
+        if k.is_a?(Array)
+          # We need to build up future executions to extract_metrics
+          # which means building up the path and keys arguments.
+          # We need a nested loop her to execute all permutations of these in case we hit
+          # something like [[:a,:b],[:c,:d]] which produces 4 different metrics
+          next_paths = Array(k.first)
+          next_keys = Array(k[1])
+          rest = k[2..-1]
+          next_paths.each do |next_path|
+            # If there already is a hash at this location use that so we don't overwrite it
+            np_hash = acc[next_path] || {}
+            
+            acc[next_path] = next_keys.reduce(np_hash) do |a,next_key|
+              a.merge! extract_metrics(path + [next_path], [next_key, *rest])
+            end
+          end
+        else # Scalar value
+          res = get_shallow(*path)[k]
+          acc[k] = res ? res.value : nil
+        end
+        
+        acc
+      end
+    end    
+
+    def has_metric?(*path)
+      @fast_lookup[path]
+    end
+
     # Return all the individuals Metric,
     # This call mimic a Enum's each if a block is provided
     #
@@ -110,11 +182,28 @@ def each(path = nil, &block)
     end
     alias_method :all, :each
 
+    def prune(path)
+      key_paths = key_paths(path).map(&:to_sym)
+      @structured_lookup_mutex.synchronize do
+        keys_to_delete = @fast_lookup.keys.select {|namespace| (key_paths - namespace[0..-2]).empty? }
+        keys_to_delete.each {|k| @fast_lookup.delete(k) }
+        delete_from_map(@store, key_paths)
+      end
+    end
+
+    def size
+      @fast_lookup.size
+    end
+
     private
     def get_all
       @fast_lookup.values
     end
 
+    def key_paths(path)
+      path.gsub(/^#{KEY_PATH_SEPARATOR}+/, "").split(KEY_PATH_SEPARATOR)
+    end
+
     # This method take an array of keys and recursively search the metric store structure
     # and return a filtered hash of the structure. This method also take into consideration
     # getting two different branchs.
@@ -224,5 +313,14 @@ def fetch_or_store_namespace_recursively(map, namespaces_path, idx = 0)
       new_map = map.fetch_or_store(current) { Concurrent::Map.new }
       return fetch_or_store_namespace_recursively(new_map, namespaces_path, idx + 1)
     end
+
+    def delete_from_map(map, keys)
+      key = keys.first
+      if keys.size == 1
+        map.delete(key)
+      else
+        delete_from_map(map[key], keys[1..-1]) unless map[key].nil?
+      end
+    end
   end
 end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/base.rb b/logstash-core/lib/logstash/instrument/metric_type/base.rb
index 5711c3f83b6..b473871b56e 100644
--- a/logstash-core/lib/logstash/instrument/metric_type/base.rb
+++ b/logstash-core/lib/logstash/instrument/metric_type/base.rb
@@ -1,5 +1,4 @@
 # encoding: utf-8
-require "logstash/event"
 require "logstash/util"
 
 module LogStash module Instrument module MetricType
@@ -17,10 +16,7 @@ def inspect
 
     def to_hash
       {
-        "namespaces" => namespaces,
-        "key" => key,
-        "type" => type,
-        "value" => value
+        key => value
       }
     end
 
diff --git a/logstash-core/lib/logstash/instrument/namespaced_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
index 6b0ad020e60..1f056bd0735 100644
--- a/logstash-core/lib/logstash/instrument/namespaced_metric.rb
+++ b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
@@ -24,7 +24,7 @@ def increment(key, value = 1)
       @metric.increment(namespace_name, key, value)
     end
 
-    def decrement(namespace, key, value = 1)
+    def decrement(key, value = 1)
       @metric.decrement(namespace_name, key, value)
     end
 
@@ -45,7 +45,7 @@ def collector
     end
 
     def namespace(name)
-      NamespacedMetric.new(metric, namespace_name.concat(Array(name)))
+      NamespacedMetric.new(metric, namespace_name + Array(name))
     end
 
     private
diff --git a/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
new file mode 100644
index 00000000000..c4e8e762c23
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
@@ -0,0 +1,54 @@
+# encoding: utf-8
+require "logstash/instrument/null_metric"
+
+module LogStash module Instrument
+  # This class acts a a proxy between the metric library and the user calls.
+  #
+  # This is the class that plugins authors will use to interact with the `MetricStore`
+  # It has the same public interface as `Metric` class but doesnt require to send
+  # the namespace on every call.
+  #
+  # @see Logstash::Instrument::Metric
+  class NamespacedNullMetric
+    attr_reader :namespace_name
+    # Create metric with a specific namespace
+    #
+    # @param metric [LogStash::Instrument::Metric] The metric instance to proxy
+    # @param namespace [Array] The namespace to use
+    def initialize(metric = nil, namespace_name = :null)
+      @metric = metric
+      @namespace_name = Array(namespace_name)
+    end
+
+    def increment(key, value = 1)
+    end
+
+    def decrement(key, value = 1)
+    end
+
+    def gauge(key, value)
+    end
+
+    def report_time(key, duration)
+    end
+
+    def time(key, &block)
+      if block_given?
+        yield
+      else
+        ::LogStash::Instrument::NullMetric::NullTimedExecution
+      end
+    end
+
+    def collector
+      @metric.collector
+    end
+
+    def namespace(name)
+      NamespacedNullMetric.new(metric, namespace_name + Array(name))
+    end
+
+    private
+    attr_reader :metric
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/null_metric.rb b/logstash-core/lib/logstash/instrument/null_metric.rb
index b8054b766dc..56bd0feac19 100644
--- a/logstash-core/lib/logstash/instrument/null_metric.rb
+++ b/logstash-core/lib/logstash/instrument/null_metric.rb
@@ -2,45 +2,60 @@
 require "logstash/instrument/metric"
 
 module LogStash module Instrument
- # This class is used in the context when we disable the metric collection
- # for specific plugin to replace the `NamespacedMetric` class with this one
- # which doesn't produce any metric to the collector.
- class NullMetric
-   attr_reader :namespace_name, :collector
-
-   def increment(key, value = 1)
-   end
-
-   def decrement(namespace, key, value = 1)
-   end
-
-   def gauge(key, value)
-   end
-
-   def report_time(key, duration)
-   end
-
-   # We have to manually redefine this method since it can return an
-   # object this object also has to be implemented as a NullObject
-   def time(key)
-     if block_given?
-       yield
-     else
-       NullTimedExecution
-     end
-   end
-
-   def namespace(key)
-     self.class.new
-   end
-
-   private
-   # Null implementation of the internal timer class
-   #
-   # @see LogStash::Instrument::TimedExecution`
-   class NullTimedExecution
-     def self.stop
-     end
-   end
- end
+  # This class is used in the context when we disable the metric collection
+  # for specific plugin to replace the `NamespacedMetric` class with this one
+  # which doesn't produce any metric to the collector.
+  class NullMetric
+    attr_reader :namespace_name, :collector
+
+    def initialize(collector = nil)
+      @collector = collector
+    end
+
+    def increment(namespace, key, value = 1)
+      Metric.validate_key!(key)
+    end
+
+    def decrement(namespace, key, value = 1)
+      Metric.validate_key!(key)
+    end
+
+    def gauge(namespace, key, value)
+      Metric.validate_key!(key)
+    end
+
+    def report_time(namespace, key, duration)
+      Metric.validate_key!(key)
+    end
+
+    # We have to manually redefine this method since it can return an
+    # object this object also has to be implemented as a NullObject
+    def time(namespace, key)
+      Metric.validate_key!(key)
+      if block_given?
+        yield
+      else
+        NullTimedExecution
+      end
+    end
+
+    def namespace(name)
+      raise MetricNoNamespaceProvided if name.nil? || name.empty?
+      NamespacedNullMetric.new(self, name)
+    end
+
+    def self.validate_key!(key)
+      raise MetricNoKeyProvided if key.nil? || key.empty?
+    end
+
+    private
+    # Null implementation of the internal timer class
+    #
+    # @see LogStash::Instrument::TimedExecution`
+    class NullTimedExecution
+      def self.stop
+        0
+      end
+    end
+  end
 end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/base.rb b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
index 32bfd931a9a..b66c50b58e3 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
@@ -8,10 +8,12 @@ class Base
     include LogStash::Util::Loggable
 
     DEFAULT_OPTIONS = {
-      :polling_interval => 1,
-      :polling_timeout => 60
+      :polling_interval => 5,
+      :polling_timeout => 120
     }
 
+    attr_reader :metric
+
     public
     def initialize(metric, options = {})
       @metric = metric
@@ -22,11 +24,25 @@ def initialize(metric, options = {})
     def update(time, result, exception)
       return unless exception
 
-      logger.error("PeriodicPoller: exception",
-                   :poller => self,
-                   :result => result,
-                   :exception => exception,
-                   :executed_at => time)
+      if exception.is_a?(Concurrent::TimeoutError)
+        # On a busy system this can happen, we just log it as a debug
+        # event instead of an error, Some of the JVM calls can take a long time or block.
+        logger.debug("PeriodicPoller: Timeout exception",
+                :poller => self,
+                :result => result,
+                :polling_timeout => @options[:polling_timeout],
+                :polling_interval => @options[:polling_interval],
+                :exception => exception.class,
+                :executed_at => time)
+      else
+        logger.error("PeriodicPoller: exception",
+                :poller => self,
+                :result => result,
+                :exception => exception.class,
+                :polling_timeout => @options[:polling_timeout],
+                :polling_interval => @options[:polling_interval],
+                :executed_at => time)
+      end
     end
 
     def collect
@@ -34,14 +50,16 @@ def collect
     end
 
     def start
-      logger.debug("PeriodicPoller: Starting", :poller => self,
+      logger.debug("PeriodicPoller: Starting",
                    :polling_interval => @options[:polling_interval],
                    :polling_timeout => @options[:polling_timeout]) if logger.debug?
+
+      collect # Collect data right away if possible
       @task.execute
     end
 
     def stop
-      logger.debug("PeriodicPoller: Stopping", :poller => self)
+      logger.debug("PeriodicPoller: Stopping")
       @task.shutdown
     end
 
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb b/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb
new file mode 100644
index 00000000000..28199b2a5cc
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb
@@ -0,0 +1,137 @@
+# encoding: utf-8
+require "pathname"
+require "logstash/util/loggable"
+
+# Logic from elasticsearch/core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java
+# Move to ruby to remove any existing dependency
+module LogStash module Instrument module PeriodicPoller
+  class Cgroup
+    include LogStash::Util::Loggable
+
+    CONTROL_GROUP_RE = Regexp.compile("\\d+:([^:,]+(?:,[^:,]+)?):(/.*)");
+    CONTROLLER_SEPARATOR_RE = ","
+
+    PROC_SELF_CGROUP_FILE = Pathname.new("/proc/self/cgroup")
+    PROC_CGROUP_CPU_DIR = Pathname.new("/sys/fs/cgroup/cpu")
+    PROC_CGROUP_CPUACCT_DIR = Pathname.new("/sys/fs/cgroup/cpuacct")
+
+    GROUP_CPUACCT = "cpuacct"
+    CPUACCT_USAGE_FILE = "cpuacct.usage"
+
+    GROUP_CPU = "cpu"
+    CPU_FS_PERIOD_US_FILE = "cpu.cfs_period_us"
+    CPU_FS_QUOTA_US_FILE = "cpu.cfs_quota_us"
+
+    CPU_STATS_FILE = "cpu.stat"
+
+    class << self
+      def are_cgroup_available?
+        [::File.exist?(PROC_SELF_CGROUP_FILE),
+         Dir.exist?(PROC_CGROUP_CPU_DIR),
+         Dir.exist?(PROC_CGROUP_CPUACCT_DIR)].all?
+      end
+
+      def control_groups
+        response = {}
+
+        read_proc_self_cgroup_lines.each do |line|
+          matches = CONTROL_GROUP_RE.match(line)
+          # multiples controls, same hierarchy
+          controllers = matches[1].split(CONTROLLER_SEPARATOR_RE)
+          controllers.each_with_object(response) { |controller| response[controller] = matches[2] }
+        end
+
+        response
+      end
+
+      def read_first_line(path)
+        IO.readlines(path).first
+      end
+
+      def cgroup_cpuacct_usage_nanos(control_group)
+        read_first_line(::File.join(PROC_CGROUP_CPUACCT_DIR, control_group, CPUACCT_USAGE_FILE)).to_i
+      end
+
+      def cgroup_cpu_fs_period_micros(control_group)
+        read_first_line(::File.join(PROC_CGROUP_CPUACCT_DIR, control_group, CPU_FS_PERIOD_US_FILE)).to_i
+      end
+
+      def cgroup_cpu_fs_quota_micros(control_group)
+        read_first_line(::File.join(PROC_CGROUP_CPUACCT_DIR, control_group,  CPU_FS_QUOTA_US_FILE)).to_i
+      end
+
+      def read_proc_self_cgroup_lines
+        IO.readlines(PROC_SELF_CGROUP_FILE)
+      end
+
+      class CpuStats
+        attr_reader :number_of_elapsed_periods, :number_of_times_throttled, :time_throttled_nanos
+
+        def initialize(number_of_elapsed_periods, number_of_times_throttled, time_throttled_nanos)
+          @number_of_elapsed_periods = number_of_elapsed_periods
+          @number_of_times_throttled = number_of_times_throttled
+          @time_throttled_nanos = time_throttled_nanos
+        end
+      end
+
+      def read_sys_fs_cgroup_cpuacct_cpu_stat(control_group)
+        IO.readlines(::File.join(PROC_CGROUP_CPU_DIR, control_group, CPU_STATS_FILE))
+      end
+
+      def cgroup_cpuacct_cpu_stat(control_group)
+        lines = read_sys_fs_cgroup_cpuacct_cpu_stat(control_group);
+
+        number_of_elapsed_periods = -1;
+        number_of_times_throttled = -1;
+        time_throttled_nanos = -1;
+
+        lines.each do |line|
+          fields = line.split(/\s+/)
+          case fields.first
+          when "nr_periods" then number_of_elapsed_periods = fields[1].to_i
+          when "nr_throttled" then number_of_times_throttled= fields[1].to_i
+          when "throttled_time" then time_throttled_nanos = fields[1].to_i
+          end
+        end
+
+        CpuStats.new(number_of_elapsed_periods, number_of_times_throttled, time_throttled_nanos)
+      end
+
+      def get_all
+       groups = control_groups
+       return if groups.empty?
+
+       cgroups_stats = {
+         :cpuacct => {},
+         :cpu => {}
+       }
+
+       cpuacct_group = groups[GROUP_CPUACCT]
+       cgroups_stats[:cpuacct][:control_group] = cpuacct_group
+       cgroups_stats[:cpuacct][:usage_nanos] = cgroup_cpuacct_usage_nanos(cpuacct_group)
+
+       cpu_group = groups[GROUP_CPU]
+       cgroups_stats[:cpu][:control_group] = cpu_group
+       cgroups_stats[:cpu][:cfs_period_micros] = cgroup_cpu_fs_period_micros(cpu_group)
+       cgroups_stats[:cpu][:cfs_quota_micros] = cgroup_cpu_fs_quota_micros(cpu_group)
+
+       cpu_stats = cgroup_cpuacct_cpu_stat(cpu_group)
+
+       cgroups_stats[:cpu][:stat] = {
+         :number_of_elapsed_periods => cpu_stats.number_of_elapsed_periods,
+         :number_of_times_throttled => cpu_stats.number_of_times_throttled,
+         :time_throttled_nanos => cpu_stats.time_throttled_nanos
+       }
+
+       cgroups_stats
+      rescue => e
+        logger.debug("Error, cannot retrieve cgroups information", :exception => e.class.name, :message => e.message) if logger.debug?
+        nil
+      end
+
+      def get
+        are_cgroup_available? ? get_all : nil
+      end
+    end
+  end
+end end end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
index 3b85d92efa6..e7c716f6633 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
@@ -1,25 +1,125 @@
 # encoding: utf-8
 require "logstash/instrument/periodic_poller/base"
-require 'monitoring'
+require "logstash/instrument/periodic_poller/load_average"
+require "logstash/environment"
+require "jrmonitor"
+require "set"
+
+java_import 'java.lang.management.ManagementFactory'
+java_import 'java.lang.management.OperatingSystemMXBean'
+java_import 'java.lang.management.GarbageCollectorMXBean'
+java_import 'java.lang.management.RuntimeMXBean'
+java_import 'com.sun.management.UnixOperatingSystemMXBean'
+java_import 'javax.management.MBeanServer'
+java_import 'javax.management.ObjectName'
+java_import 'javax.management.AttributeList'
+java_import 'javax.naming.directory.Attribute'
+
 
 module LogStash module Instrument module PeriodicPoller
   class JVM < Base
+    class GarbageCollectorName
+      YOUNG_GC_NAMES = Set.new(["Copy", "PS Scavenge", "ParNew", "G1 Young Generation"])
+      OLD_GC_NAMES = Set.new(["MarkSweepCompact", "PS MarkSweep", "ConcurrentMarkSweep", "G1 Old Generation"])
+
+      YOUNG = :young
+      OLD = :old
+
+      def self.get(gc_name)
+        if YOUNG_GC_NAMES.include?(gc_name)
+          YOUNG
+        elsif(OLD_GC_NAMES.include?(gc_name))
+          OLD
+        end
+      end
+    end
+
+    MEMORY_TRANSPOSE_MAP = {
+      "usage.used" => :used_in_bytes,
+      "usage.committed" => :committed_in_bytes,
+      "usage.max" => :max_in_bytes,
+      "peak.max" => :peak_max_in_bytes,
+      "peak.used" => :peak_used_in_bytes
+    }
 
     attr_reader :metric
 
     def initialize(metric, options = {})
       super(metric, options)
-      @metric = metric
+      @load_average = LoadAverage.create
     end
 
     def collect
       raw = JRMonitor.memory.generate
-      collect_heap_metrics(raw)
-      collect_non_heap_metrics(raw)
+      collect_jvm_metrics(raw)
       collect_pools_metrics(raw)
+      collect_threads_metrics
+      collect_process_metrics
+      collect_gc_stats
+      collect_load_average
+    end
+
+    def collect_gc_stats
+      garbage_collectors = ManagementFactory.getGarbageCollectorMXBeans()
+
+      garbage_collectors.each do |collector|
+        name = GarbageCollectorName.get(collector.getName())
+        if name.nil?
+          logger.error("Unknown garbage collector name", :name => name)
+        else
+          metric.gauge([:jvm, :gc, :collectors, name], :collection_count, collector.getCollectionCount())
+          metric.gauge([:jvm, :gc, :collectors, name], :collection_time_in_millis, collector.getCollectionTime())
+        end
+      end
+    end
+
+    def collect_threads_metrics
+      threads_mx = ManagementFactory.getThreadMXBean()
+
+      metric.gauge([:jvm, :threads], :count, threads_mx.getThreadCount())
+      metric.gauge([:jvm, :threads], :peak_count, threads_mx.getPeakThreadCount())
     end
 
-    private
+    def collect_process_metrics
+      process_metrics = JRMonitor.process.generate
+
+      path = [:jvm, :process]
+
+
+      open_fds = process_metrics["open_file_descriptors"]
+      if @peak_open_fds.nil? || open_fds > @peak_open_fds
+        @peak_open_fds = open_fds
+      end
+      metric.gauge(path, :open_file_descriptors, open_fds)
+      metric.gauge(path, :peak_open_file_descriptors, @peak_open_fds)
+      metric.gauge(path, :max_file_descriptors, process_metrics["max_file_descriptors"])
+
+      cpu_path = path + [:cpu]
+      cpu_metrics = process_metrics["cpu"]
+      metric.gauge(cpu_path, :percent, cpu_metrics["process_percent"])
+      metric.gauge(cpu_path, :total_in_millis, cpu_metrics["total_in_millis"])
+
+      metric.gauge(path + [:mem], :total_virtual_in_bytes, process_metrics["mem"]["total_virtual_in_bytes"])
+
+    end
+
+    def collect_load_average
+      begin
+        load_average = @load_average.get
+      rescue => e
+        logger.debug("Can't retrieve load average", :exception => e.class.name, :message => e.message)
+        load_average = nil
+      end
+
+      metric.gauge([:jvm, :process, :cpu], :load_average, load_average) unless load_average.nil?
+    end
+
+    def collect_jvm_metrics(data)
+      runtime_mx_bean = ManagementFactory.getRuntimeMXBean()
+      metric.gauge([:jvm], :uptime_in_millis, runtime_mx_bean.getUptime())
+      collect_heap_metrics(data)
+      collect_non_heap_metrics(data)
+    end
 
     def collect_heap_metrics(data)
       heap = aggregate_information_for(data["heap"].values)
@@ -68,11 +168,10 @@ def aggregate_information_for(collection)
       collection.reduce(default_information_accumulator) do |m,e|
         e = { e[0] => e[1] } if e.is_a?(Array)
         e.each_pair do |k,v|
-          m[:used_in_bytes] += v       if k.include?("used")
-          m[:committed_in_bytes] += v  if k.include?("committed")
-          m[:max_in_bytes] += v        if k.include?("max")
-          m[:peak_max_in_bytes] += v   if k.include?("peak.max")
-          m[:peak_used_in_bytes] += v  if k.include?("peak.used")
+          if MEMORY_TRANSPOSE_MAP.include?(k)
+            transpose_key = MEMORY_TRANSPOSE_MAP[k]
+            m[transpose_key] += v
+          end
         end
         m
       end
@@ -84,9 +183,8 @@ def default_information_accumulator
         :committed_in_bytes => 0,
         :max_in_bytes => 0,
         :peak_used_in_bytes => 0,
-        :peak_max_in_bytes  => 0
+        :peak_max_in_bytes => 0
       }
     end
-
   end
 end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/load_average.rb b/logstash-core/lib/logstash/instrument/periodic_poller/load_average.rb
new file mode 100644
index 00000000000..1e13f1a9ca2
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/load_average.rb
@@ -0,0 +1,49 @@
+# encoding: utf-8
+java_import "java.lang.management.ManagementFactory"
+
+module LogStash module Instrument module PeriodicPoller
+  class LoadAverage
+    class Windows
+      def self.get
+        nil
+      end
+    end
+
+    class Linux
+      LOAD_AVG_FILE = "/proc/loadavg"
+      TOKEN_SEPARATOR = " "
+
+      def self.get(content = ::File.read(LOAD_AVG_FILE))
+        load_average = content.chomp.split(TOKEN_SEPARATOR)
+
+        {
+          :"1m" => load_average[0].to_f,
+          :"5m" => load_average[1].to_f,
+          :"15m" => load_average[2].to_f
+        }
+      end
+    end
+
+    class Other
+      def self.get()
+        load_average_1m = ManagementFactory.getOperatingSystemMXBean().getSystemLoadAverage()
+
+        return nil if load_average_1m.nil?
+
+        {
+          :"1m" => load_average_1m
+        }
+      end
+    end
+
+    def self.create
+      if LogStash::Environment.windows?
+        Windows
+      elsif LogStash::Environment.linux?
+        Linux
+      else
+        Other
+      end
+    end
+  end
+end end end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/os.rb b/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
index 8ad09dfc7d7..7f2a334b5ba 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require "logstash/instrument/periodic_poller/base"
+require "logstash/instrument/periodic_poller/cgroup"
 
 module LogStash module Instrument module PeriodicPoller
   class Os < Base
@@ -8,6 +9,26 @@ def initialize(metric, options = {})
     end
 
     def collect
+      collect_cgroup
+    end
+
+    def collect_cgroup
+      if stats = Cgroup.get
+        save_metric([:os], :cgroup, stats)
+      end
+    end
+
+    # Recursive function to create the Cgroups values form the created hash
+    def save_metric(namespace, k, v)
+      if v.is_a?(Hash)
+        v.each do |new_key, new_value|
+          n = namespace.dup
+          n << k.to_sym
+          save_metric(n, new_key, new_value)
+        end
+      else
+        metric.gauge(namespace, k.to_sym, v)
+      end
     end
   end
 end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/periodic_poller_observer.rb b/logstash-core/lib/logstash/instrument/periodic_poller/periodic_poller_observer.rb
deleted file mode 100644
index 382b350968d..00000000000
--- a/logstash-core/lib/logstash/instrument/periodic_poller/periodic_poller_observer.rb
+++ /dev/null
@@ -1,19 +0,0 @@
-# encoding: utf-8
-module LogStash module Instrument module PeriodicPoller
-  class PeriodicPollerObserver
-    include LogStash::Util::Loggable
-    
-    def initialize(poller)
-      @poller = poller
-    end
-
-    def update(time, result, exception)
-      if exception
-        logger.error("PeriodicPoller exception", :poller => @poller,
-                     :result => result,
-                     :exception => exception,
-                     :executed_at => time)
-      end
-    end
-  end
-end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/pq.rb b/logstash-core/lib/logstash/instrument/periodic_poller/pq.rb
new file mode 100644
index 00000000000..d0028031f1f
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/pq.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/base"
+
+module LogStash module Instrument module PeriodicPoller
+  class PersistentQueue < Base
+    def initialize(metric, queue_type, agent, options = {})
+      super(metric, options)
+      @metric = metric
+      @queue_type = queue_type
+      @agent = agent
+    end
+
+    def collect
+      pipeline_id, pipeline = @agent.running_pipelines.first
+      unless pipeline.nil?
+        pipeline.collect_stats
+      end
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_pollers.rb b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
index 09c4feebd57..0ce6d406448 100644
--- a/logstash-core/lib/logstash/instrument/periodic_pollers.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/instrument/periodic_poller/os"
 require "logstash/instrument/periodic_poller/jvm"
+require "logstash/instrument/periodic_poller/pq"
 
 module LogStash module Instrument
   # Each PeriodPoller manager his own thread to do the poller
@@ -9,10 +10,11 @@ module LogStash module Instrument
   class PeriodicPollers
     attr_reader :metric
 
-    def initialize(metric)
+    def initialize(metric, queue_type, pipelines)
       @metric = metric
       @periodic_pollers = [PeriodicPoller::Os.new(metric),
-                          PeriodicPoller::JVM.new(metric)]
+                           PeriodicPoller::JVM.new(metric),
+                           PeriodicPoller::PersistentQueue.new(metric, queue_type, pipelines)]
     end
 
     def start
diff --git a/logstash-core/lib/logstash/instrument/snapshot.rb b/logstash-core/lib/logstash/instrument/snapshot.rb
index f46068439ad..62a12677fdb 100644
--- a/logstash-core/lib/logstash/instrument/snapshot.rb
+++ b/logstash-core/lib/logstash/instrument/snapshot.rb
@@ -1,6 +1,5 @@
 # encoding: utf-8
 require "logstash/util/loggable"
-require "logstash/event"
 
 module LogStash module Instrument
   class Snapshot
diff --git a/logstash-core/lib/logstash/instrument/wrapped_write_client.rb b/logstash-core/lib/logstash/instrument/wrapped_write_client.rb
new file mode 100644
index 00000000000..5da275c9f29
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/wrapped_write_client.rb
@@ -0,0 +1,59 @@
+# encoding: utf-8
+module LogStash module Instrument
+  class WrappedWriteClient
+    def initialize(write_client, pipeline, metric, plugin)
+      @write_client = write_client
+
+      pipeline_id = pipeline.pipeline_id.to_s.to_sym
+      plugin_type = "#{plugin.class.plugin_type}s".to_sym
+
+      @events_metrics = metric.namespace([:stats, :events])
+      @pipeline_metrics = metric.namespace([:stats, :pipelines, pipeline_id, :events])
+      @plugin_events_metrics = metric.namespace([:stats, :pipelines, pipeline_id, :plugins, plugin_type, plugin.id.to_sym, :events])
+
+      define_initial_metrics_values
+    end
+
+    def get_new_batch
+      @write_client.get_new_batch
+    end
+
+    def push(event)
+      record_metric { @write_client.push(event) }
+    end
+    alias_method(:<<, :push)
+
+    def push_batch(batch)
+      record_metric(batch.size) { @write_client.push_batch(batch) }
+    end
+
+    private
+    def record_metric(size = 1)
+      @events_metrics.increment(:in, size)
+      @pipeline_metrics.increment(:in, size)
+      @plugin_events_metrics.increment(:out, size)
+
+      clock = @events_metrics.time(:queue_push_duration_in_millis)
+
+      result = yield
+
+      # Reuse the same values for all the endpoints to make sure we don't have skew in times.
+      execution_time = clock.stop
+
+      @pipeline_metrics.report_time(:queue_push_duration_in_millis, execution_time)
+      @plugin_events_metrics.report_time(:queue_push_duration_in_millis, execution_time)
+
+      result
+    end
+
+    def define_initial_metrics_values
+      @events_metrics.increment(:in, 0)
+      @pipeline_metrics.increment(:in, 0)
+      @plugin_events_metrics.increment(:out, 0)
+
+      @events_metrics.report_time(:queue_push_duration_in_millis, 0)
+      @pipeline_metrics.report_time(:queue_push_duration_in_millis, 0)
+      @plugin_events_metrics.report_time(:queue_push_duration_in_millis, 0)
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/java_integration.rb b/logstash-core/lib/logstash/java_integration.rb
index 670ceaae650..fb5b9eb8c1b 100644
--- a/logstash-core/lib/logstash/java_integration.rb
+++ b/logstash-core/lib/logstash/java_integration.rb
@@ -1,7 +1,7 @@
 # encoding: utf-8
 require "java"
 
-# this is mainly for usage with JrJackson json parsing in :raw mode which genenerates
+# this is mainly for usage with JrJackson json parsing in :raw mode which generates
 # Java::JavaUtil::ArrayList and Java::JavaUtil::LinkedHashMap native objects for speed.
 # these object already quacks like their Ruby equivalents Array and Hash but they will
 # not test for is_a?(Array) or is_a?(Hash) and we do not want to include tests for
@@ -35,7 +35,7 @@ def self.===(other)
   # this bug makes has_key? (and all its aliases) return false for a key that has a nil value.
   # Only LinkedHashMap is patched here because patching the Map interface is not working.
   # TODO find proper fix, and submit upstream
-  # releavant JRuby files:
+  # relevant JRuby files:
   # https://github.com/jruby/jruby/blob/master/core/src/main/ruby/jruby/java/java_ext/java.util.rb
   # https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/java/proxies/MapJavaProxy.java
   def has_key?(key)
diff --git a/logstash-core/lib/logstash/logging.rb b/logstash-core/lib/logstash/logging.rb
index 1dbaa0aa932..201d706bc75 100644
--- a/logstash-core/lib/logstash/logging.rb
+++ b/logstash-core/lib/logstash/logging.rb
@@ -1,91 +1,3 @@
 # encoding: utf-8
+require "logstash/logging/logger"
 require "logstash/namespace"
-require "cabin"
-require "logger"
-
-class LogStash::Logger
-  attr_accessor :target
-
-  public
-  def initialize(*args)
-    super()
-
-    #self[:program] = File.basename($0)
-    #subscribe(::Logger.new(*args))
-    @target = args[0]
-    @channel = Cabin::Channel.get(LogStash)
-
-    # lame hack until cabin's smart enough not to doubley-subscribe something.
-    # without this subscription count check, running the test suite
-    # causes Cabin to subscribe to STDOUT maaaaaany times.
-    subscriptions = @channel.instance_eval { @subscribers.count }
-    @channel.subscribe(@target) unless subscriptions > 0
-
-    # Set default loglevel to WARN unless $DEBUG is set (run with 'ruby -d')
-    @level = $DEBUG ? :debug : :warn
-    if ENV["LOGSTASH_DEBUG"]
-      @level = :debug
-    end
-
-    # Direct metrics elsewhere.
-    @channel.metrics.channel = Cabin::Channel.new
-  end # def initialize
-
-  # Delegation
-  def level=(value) @channel.level = value; end
-  def debug(*args); @channel.debug(*args); end
-  def debug?(*args); @channel.debug?(*args); end
-  def info(*args); @channel.info(*args); end
-  def info?(*args); @channel.info?(*args); end
-  def warn(*args); @channel.warn(*args); end
-  def warn?(*args); @channel.warn?(*args); end
-  def error(*args); @channel.error(*args); end
-  def error?(*args); @channel.error?(*args); end
-  def fatal(*args); @channel.fatal(*args); end
-  def fatal?(*args); @channel.fatal?(*args); end
-
-  def self.setup_log4j(logger)
-    require "java"
-
-    properties = java.util.Properties.new
-    log4j_level = "WARN"
-    case logger.level
-      when :debug
-        log4j_level = "DEBUG"
-      when :info
-        log4j_level = "INFO"
-      when :warn
-        log4j_level = "WARN"
-    end # case level
-    properties.setProperty("log4j.rootLogger", "#{log4j_level},logstash")
-
-    # TODO(sissel): This is a shitty hack to work around the fact that
-    # LogStash::Logger isn't used anymore. We should fix that.
-    target = logger.instance_eval { @subscribers }.values.first.instance_eval { @io }
-    case target
-      when STDOUT
-        properties.setProperty("log4j.appender.logstash",
-                      "org.apache.log4j.ConsoleAppender")
-        properties.setProperty("log4j.appender.logstash.Target", "System.out")
-      when STDERR
-        properties.setProperty("log4j.appender.logstash",
-                      "org.apache.log4j.ConsoleAppender")
-        properties.setProperty("log4j.appender.logstash.Target", "System.err")
-      when target.is_a?(File)
-        properties.setProperty("log4j.appender.logstash",
-                      "org.apache.log4j.FileAppender")
-        properties.setProperty("log4j.appender.logstash.File", target.path)
-      else
-        properties.setProperty("log4j.appender.logstash", "org.apache.log4j.varia.NullAppender")
-    end # case target
-
-    properties.setProperty("log4j.appender.logstash.layout",
-                  "org.apache.log4j.PatternLayout")
-    properties.setProperty("log4j.appender.logstash.layout.conversionPattern",
-                  "log4j, [%d{yyyy-MM-dd}T%d{HH:mm:ss.SSS}] %5p: %c: %m%n")
-
-    org.apache.log4j.LogManager.resetConfiguration
-    org.apache.log4j.PropertyConfigurator.configure(properties)
-    logger.debug("log4j java properties setup", :log4j_level => log4j_level)
-  end
-end # class LogStash::Logger
diff --git a/logstash-core/lib/logstash/logging/json.rb b/logstash-core/lib/logstash/logging/json.rb
new file mode 100644
index 00000000000..f51f8051d58
--- /dev/null
+++ b/logstash-core/lib/logstash/logging/json.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/logging"
+require "logstash/json"
+
+module LogStash; module Logging; class JSON
+  def initialize(io)
+    raise ArgumentError, "Expected IO, got #{io.class.name}" unless io.is_a?(IO)
+
+    @io = io
+    @lock = Mutex.new
+  end
+
+  def <<(obj)
+    serialized = LogStash::Json.dump(obj)
+    @lock.synchronize do
+      @io.puts(serialized)
+      @io.flush
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/logging/logger.rb b/logstash-core/lib/logstash/logging/logger.rb
new file mode 100644
index 00000000000..378adaa0c82
--- /dev/null
+++ b/logstash-core/lib/logstash/logging/logger.rb
@@ -0,0 +1,127 @@
+require "logstash/java_integration"
+require "uri"
+
+module LogStash
+  module Logging
+    java_import org.apache.logging.log4j.Level
+    java_import org.apache.logging.log4j.LogManager
+    java_import org.apache.logging.log4j.core.config.Configurator
+    java_import org.apache.logging.log4j.core.config.DefaultConfiguration
+
+    class Logger
+      @@config_mutex = Mutex.new
+      @@logging_context = nil
+
+      def initialize(name)
+        @logger = LogManager.getLogger(name)
+      end
+
+      def debug?
+        @logger.is_debug_enabled
+      end
+
+      def info?
+        @logger.is_info_enabled
+      end
+
+      def error?
+        @logger.is_error_enabled
+      end
+
+      def warn?
+        @logger.is_warn_enabled
+      end
+
+      def fatal?
+        @logger.is_fatal_enabled
+      end
+
+      def trace?
+        @logger.is_trace_enabled
+      end
+
+      def debug(message, data = {})
+        @logger.debug(message, data)
+      end
+
+      def warn(message, data = {})
+        @logger.warn(message, data)
+      end
+
+      def info(message, data = {})
+        @logger.info(message, data)
+      end
+
+      def error(message, data = {})
+        @logger.error(message, data)
+      end
+
+      def fatal(message, data = {})
+        @logger.fatal(message, data)
+      end
+
+      def trace(message, data = {})
+        @logger.trace(message, data)
+      end
+
+      def self.configure_logging(level, path = LogManager::ROOT_LOGGER_NAME)
+        @@config_mutex.synchronize { Configurator.setLevel(path, Level.valueOf(level)) }
+      rescue Exception => e
+        raise ArgumentError, "invalid level[#{level}] for logger[#{path}]"
+      end
+
+      def self.initialize(config_location)
+        @@config_mutex.synchronize do
+          if @@logging_context.nil?
+            file_path = URI(config_location).path
+            if ::File.exists?(file_path)
+              logs_location = java.lang.System.getProperty("ls.logs")
+              puts "Sending Logstash's logs to #{logs_location} which is now configured via log4j2.properties"
+              @@logging_context = Configurator.initialize(nil, config_location)
+            else
+              # fall back to default config
+              puts "Could not find log4j2 configuration at path #{file_path}. Using default config which logs to console"
+              @@logging_context = Configurator.initialize(DefaultConfiguration.new)
+            end
+          end
+        end
+      end
+
+      def self.get_logging_context
+        return @@logging_context
+      end
+    end
+
+    class SlowLogger
+      def initialize(name, warn_threshold, info_threshold, debug_threshold, trace_threshold)
+        slowlog_name = ["slowlog", name].join('.')
+        @slowlogger = LogManager.getLogger(slowlog_name)
+        @warn_threshold = warn_threshold
+        @info_threshold = info_threshold
+        @debug_threshold = debug_threshold
+        @trace_threshold = trace_threshold
+      end
+
+      def as_data(plugin_params, event, took_in_nanos)
+        {
+          :plugin_params => plugin_params,
+          :took_in_nanos => took_in_nanos,
+          :took_in_millis => took_in_nanos / 1000000,
+          :event => event.to_json
+        }
+      end
+
+      def on_event(message, plugin_params, event, took_in_nanos)
+        if @warn_threshold >= 0 and took_in_nanos > @warn_threshold
+          @slowlogger.warn(message, as_data(plugin_params, event, took_in_nanos))
+        elsif @info_threshold >= 0 and took_in_nanos > @info_threshold
+          @slowlogger.info(message, as_data(plugin_params, event, took_in_nanos))
+        elsif @debug_threshold >= 0 and took_in_nanos > @debug_threshold
+          @slowlogger.debug(message, as_data(plugin_params, event, took_in_nanos))
+        elsif @trace_threshold >= 0 and took_in_nanos > @trace_threshold
+          @slowlogger.trace(message, as_data(plugin_params, event, took_in_nanos))
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/output_delegator.rb b/logstash-core/lib/logstash/output_delegator.rb
index 50a5a9d49c7..fa34187c227 100644
--- a/logstash-core/lib/logstash/output_delegator.rb
+++ b/logstash-core/lib/logstash/output_delegator.rb
@@ -1,163 +1,55 @@
-# encoding: utf-8
-require "concurrent/atomic/atomic_fixnum"
+require "logstash/output_delegator_strategy_registry"
+
+require "logstash/output_delegator_strategies/shared"
+require "logstash/output_delegator_strategies/single"
+require "logstash/output_delegator_strategies/legacy"
 
-# This class goes hand in hand with the pipeline to provide a pool of
-# free workers to be used by pipeline worker threads. The pool is
-# internally represented with a SizedQueue set the the size of the number
-# of 'workers' the output plugin is configured with.
-#
-# This plugin also records some basic statistics
 module LogStash class OutputDelegator
-  attr_reader :workers, :config, :worker_count, :threadsafe
+  attr_reader :metric, :metric_events, :strategy, :namespaced_metric, :metric_events, :id
 
-  # The *args this takes are the same format that a Outputs::Base takes. A list of hashes with parameters in them
-  # Internally these just get merged together into a single hash
-  def initialize(logger, klass, default_worker_count, metric, *args)
+  def initialize(logger, output_class, metric, execution_context, strategy_registry, plugin_args)
     @logger = logger
-    @threadsafe = klass.threadsafe?
-    @config = args.reduce({}, :merge)
-    @klass = klass
-
-    # Create an instance of the input so we can fetch the identifier
-    output = @klass.new(*args)
-
-    # Scope the metrics to the plugin
-    namespaced_metric = metric.namespace(output.plugin_unique_name.to_sym)
-    @metric_events = namespaced_metric.namespace(:events)
-
-
-    # We define this as an array regardless of threadsafety
-    # to make reporting simpler, even though a threadsafe plugin will just have
-    # a single instance
-    #
-    # Older plugins invoke the instance method Outputs::Base#workers_not_supported
-    # To detect these we need an instance to be created first :()
-    # TODO: In the next major version after 2.x remove support for this
-    @workers = [@klass.new(*args)]
-    @workers.first.register # Needed in case register calls `workers_not_supported`
-
-    # DO NOT move this statement before the instantiation of the first single instance
-    # Read the note above to understand why
-    @worker_count = calculate_worker_count(default_worker_count)
-    @logger.debug("Will start workers for output", :worker_count => @worker_count, :class => klass)
-
-    warn_on_worker_override!
-    # This queue is used to manage sharing across threads
-    @worker_queue = SizedQueue.new(@worker_count)
+    @output_class = output_class
+    @metric = metric
+    @id = plugin_args["id"]
 
-    @workers += (@worker_count - 1).times.map do
-      inst = @klass.new(*args)
-      inst.metric = @metric
-      inst.register
-      inst
-    end
+    raise ArgumentError, "No strategy registry specified" unless strategy_registry
+    raise ArgumentError, "No ID specified! Got args #{plugin_args}" unless id
 
-    @workers.each { |w| @worker_queue << w }
-
-    @events_received = Concurrent::AtomicFixnum.new(0)
-
-
-    # One might wonder why we don't use something like
-    # define_singleton_method(:multi_receive, method(:threadsafe_multi_receive)
-    # and the answer is this is buggy on Jruby 1.7.x . It works 98% of the time!
-    # The other 2% you get weird errors about rebinding to the same object
-    # Until we switch to Jruby 9.x keep the define_singleton_method parts
-    # the way they are, with a block
-    # See https://github.com/jruby/jruby/issues/3582
-    if threadsafe?
-      @threadsafe_worker = @workers.first
-      define_singleton_method(:multi_receive) do |events|
-        threadsafe_multi_receive(events)
-      end
-    else
-      define_singleton_method(:multi_receive) do |events|
-        worker_multi_receive(events)
-      end
-    end
-  end
-
-  def threadsafe?
-    !!@threadsafe
-  end
+    @namespaced_metric = metric.namespace(id.to_sym)
+    @namespaced_metric.gauge(:name, config_name)
+    @metric_events = @namespaced_metric.namespace(:events)
 
-  def warn_on_worker_override!
-    # The user has configured extra workers, but this plugin doesn't support it :(
-    if worker_limits_overriden?
-      message = @klass.workers_not_supported_message
-      warning_meta = {:plugin => @klass.config_name, :worker_count => @config["workers"]}
-      if message
-        warning_meta[:message] = message
-        @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported-with-message", warning_meta))
-      else
-        @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported", warning_meta))
-      end
-    end
+    @strategy = strategy_registry.
+                  class_for(self.concurrency).
+                  new(@logger, @output_class, @namespaced_metric, execution_context, plugin_args)
   end
 
-  def worker_limits_overriden?
-    @config["workers"] && @config["workers"] > 1 && @klass.workers_not_supported?
+  def config_name
+    @output_class.config_name
   end
 
-  def calculate_worker_count(default_worker_count)
-    if @threadsafe || @klass.workers_not_supported?
-      1
-    else
-      @config["workers"] || default_worker_count
-    end
+  def reloadable?
+    @output_class.reloadable?
   end
 
-  def config_name
-    @klass.config_name
+  def concurrency
+    @output_class.concurrency
   end
 
   def register
-    @workers.each {|w| w.register}
+    @strategy.register
   end
 
-  def threadsafe_multi_receive(events)
-    @events_received.increment(events.length)
+  def multi_receive(events)
     @metric_events.increment(:in, events.length)
-
-    @threadsafe_worker.multi_receive(events)
+    clock = @metric_events.time(:duration_in_millis)
+    @strategy.multi_receive(events)
+    clock.stop
     @metric_events.increment(:out, events.length)
   end
 
-  def worker_multi_receive(events)
-    @events_received.increment(events.length)
-    @metric_events.increment(:in, events.length)
-
-    worker = @worker_queue.pop
-    begin
-      worker.multi_receive(events)
-      @metric_events.increment(:out, events.length)
-    ensure
-      @worker_queue.push(worker)
-    end
-  end
-
   def do_close
-    @logger.debug("closing output delegator", :klass => self)
-
-    @worker_count.times do
-      worker = @worker_queue.pop
-      worker.do_close
-    end
-  end
-
-  def events_received
-    @events_received.value
+    @strategy.do_close
   end
-
-  # There's no concept of 'busy' workers for a threadsafe plugin!
-  def busy_workers
-    if @threadsafe
-      0
-    else
-      @workers.size - @worker_queue.size
-    end
-  end
-
-  private
-  # Needed for testing, so private
-  attr_reader :threadsafe_worker, :worker_queue
-end end
+end; end
diff --git a/logstash-core/lib/logstash/output_delegator_strategies/legacy.rb b/logstash-core/lib/logstash/output_delegator_strategies/legacy.rb
new file mode 100644
index 00000000000..9648c249417
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategies/legacy.rb
@@ -0,0 +1,33 @@
+# Remove this in Logstash 6.0
+module LogStash module OutputDelegatorStrategies class Legacy
+  attr_reader :worker_count, :workers
+  
+  def initialize(logger, klass, metric, execution_context, plugin_args)
+    @worker_count = (plugin_args["workers"] || 1).to_i
+    @workers = @worker_count.times.map { klass.new(plugin_args) }
+    @workers.each do |w|
+      w.metric = metric
+      w.execution_context = execution_context
+    end
+    @worker_queue = SizedQueue.new(@worker_count)
+    @workers.each {|w| @worker_queue << w}
+  end
+  
+  def register
+    @workers.each(&:register)
+  end
+  
+  def multi_receive(events)
+    worker = @worker_queue.pop
+    worker.multi_receive(events)
+  ensure
+    @worker_queue << worker if worker
+  end
+
+  def do_close
+    # No mutex needed since this is only called when the pipeline is clear
+    @workers.each(&:do_close)
+  end
+
+  ::LogStash::OutputDelegatorStrategyRegistry.instance.register(:legacy, self)
+end; end; end
diff --git a/logstash-core/lib/logstash/output_delegator_strategies/shared.rb b/logstash-core/lib/logstash/output_delegator_strategies/shared.rb
new file mode 100644
index 00000000000..4afce212ec5
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategies/shared.rb
@@ -0,0 +1,22 @@
+module LogStash module OutputDelegatorStrategies class Shared
+  def initialize(logger, klass, metric, execution_context, plugin_args)
+    @output = klass.new(plugin_args)
+    @output.metric = metric
+    @output.execution_context = execution_context
+  end
+  
+  def register
+    @output.register
+  end
+
+  def multi_receive(events)
+    @output.multi_receive(events)
+  end
+
+  def do_close    
+    @output.do_close
+  end
+
+  ::LogStash::OutputDelegatorStrategyRegistry.instance.register(:shared, self)  
+end; end; end
+
diff --git a/logstash-core/lib/logstash/output_delegator_strategies/single.rb b/logstash-core/lib/logstash/output_delegator_strategies/single.rb
new file mode 100644
index 00000000000..5b836f2b587
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategies/single.rb
@@ -0,0 +1,25 @@
+module LogStash module OutputDelegatorStrategies class Single
+  def initialize(logger, klass, metric, execution_context, plugin_args)
+    @output = klass.new(plugin_args)
+    @output.metric = metric
+    @output.execution_context = execution_context
+    @mutex = Mutex.new
+  end
+
+  def register
+    @output.register
+  end
+  
+  def multi_receive(events)
+    @mutex.synchronize do
+      @output.multi_receive(events)
+    end
+  end
+
+  def do_close
+    # No mutex needed since this is only called when the pipeline is clear
+    @output.do_close
+  end
+
+  ::LogStash::OutputDelegatorStrategyRegistry.instance.register(:single, self)
+end; end; end
diff --git a/logstash-core/lib/logstash/output_delegator_strategy_registry.rb b/logstash-core/lib/logstash/output_delegator_strategy_registry.rb
new file mode 100644
index 00000000000..dc4e0a02000
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategy_registry.rb
@@ -0,0 +1,36 @@
+module LogStash; class OutputDelegatorStrategyRegistry
+  class InvalidStrategyError < StandardError; end
+                   
+  # This is generally used as a singleton
+  # Except perhaps during testing
+  def self.instance
+    @instance ||= self.new
+  end
+
+  def initialize()
+    @map = {}
+  end
+
+  def classes
+    @map.values
+  end
+
+  def types
+    @map.keys
+  end
+  
+  def class_for(type)
+    klass = @map[type]
+
+    if !klass
+      raise InvalidStrategyError, "Could not find output delegator strategy of type '#{type}'. Valid strategies: #{@strategy_registry.types}"
+    end
+
+    klass
+  end
+
+  def register(type, klass)
+    @map[type] = klass
+  end
+
+end; end
diff --git a/logstash-core/lib/logstash/outputs/base.rb b/logstash-core/lib/logstash/outputs/base.rb
index 3f59cc0e715..8f85c251849 100644
--- a/logstash-core/lib/logstash/outputs/base.rb
+++ b/logstash-core/lib/logstash/outputs/base.rb
@@ -8,6 +8,7 @@
 require "concurrent/atomic/atomic_fixnum"
 
 class LogStash::Outputs::Base < LogStash::Plugin
+  include LogStash::Util::Loggable
   include LogStash::Config::Mixin
 
   config_name "output"
@@ -20,41 +21,40 @@ class LogStash::Outputs::Base < LogStash::Plugin
 
   # The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.
   config :codec, :validate => :codec, :default => "plain"
+  # TODO remove this in Logstash 6.0
+  # when we no longer support the :legacy type
+  # This is hacky, but it can only be herne
+  config :workers, :type => :number, :default => 1
+  
+  # Set or return concurrency type
+  def self.concurrency(type=nil)
+    if type
+      @concurrency = type
+    else
+      @concurrency || :legacy # default is :legacyo
+    end
+  end
 
-  # The number of workers to use for this output.
-  # Note that this setting may not be useful for all outputs.
-  config :workers, :validate => :number, :default => 1
-
-  attr_reader :worker_plugins, :available_workers, :workers, :worker_plugins, :workers_not_supported
-
+  # Deprecated: Favor `concurrency :shared`
   def self.declare_threadsafe!
-    declare_workers_not_supported!
-    @threadsafe = true
+    concurrency :shared
   end
 
+  # Deprecated: Favor `#concurrency`
   def self.threadsafe?
-    @threadsafe == true
+    concurrency == :shared
   end
 
+  # Deprecated: Favor `concurrency :single`
+  # Remove in Logstash 6.0.0
   def self.declare_workers_not_supported!(message=nil)
-    @workers_not_supported_message = message
-    @workers_not_supported = true
-  end
-
-  def self.workers_not_supported_message
-    @workers_not_supported_message
-  end
-
-  def self.workers_not_supported?
-    !!@workers_not_supported
+    concurrency :single
   end
 
   public
-  # TODO: Remove this in the next major version after Logstash 2.x
-  # Post 2.x it should raise an error and tell people to use the class level
-  # declaration
-  def workers_not_supported(message=nil)
-    self.class.declare_workers_not_supported!(message)
+
+  def self.plugin_type
+    "output"
   end
 
   public
@@ -62,9 +62,15 @@ def initialize(params={})
     super
     config_init(@params)
 
+    if self.workers != 1
+      raise LogStash::ConfigurationError, "You are using a plugin that doesn't support workers but have set the workers value explicitly! This plugin uses the #{concurrency} and doesn't need this option"
+    end
+
     # If we're running with a single thread we must enforce single-threaded concurrency by default
     # Maybe in a future version we'll assume output plugins are threadsafe
     @single_worker_mutex = Mutex.new
+    
+    @receives_encoded = self.methods.include?(:multi_receive_encoded)
   end
 
   public
@@ -78,9 +84,33 @@ def receive(event)
   end # def receive
 
   public
-  # To be overriden in implementations
+  # To be overridden in implementations
   def multi_receive(events)
-    events.each {|event| receive(event) }
+    if @receives_encoded
+      self.multi_receive_encoded(codec.multi_encode(events))
+    else
+      events.each {|event| receive(event) }
+    end
+  end
+
+  def workers_not_supported(message=nil)
+    raise "This plugin (#{self.class.name}) is using the obsolete '#workers_not_supported' method. If you installed this plugin specifically on this Logstash version, it is not compatible. If you are a plugin author, please see https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_output_plugin.html for more info"
+  end
+
+  def codec
+    params["codec"]
+  end
+
+  def concurrency
+    self.class.concurrency
+  end
+
+  def execution_context=(context)
+    super
+    # There is no easy way to propage an instance variable into the codec, because the codec
+    # are created at the class level
+    @codec.execution_context = context
+    context
   end
 
   private
diff --git a/logstash-core/lib/logstash/patches.rb b/logstash-core/lib/logstash/patches.rb
index 8a3b3aa85e6..283d74aa346 100644
--- a/logstash-core/lib/logstash/patches.rb
+++ b/logstash-core/lib/logstash/patches.rb
@@ -3,3 +3,4 @@
 require "logstash/patches/cabin"
 require "logstash/patches/profile_require_calls"
 require "logstash/patches/stronger_openssl_defaults"
+require "logstash/patches/exception_to_json"
diff --git a/logstash-core/lib/logstash/patches/cabin.rb b/logstash-core/lib/logstash/patches/cabin.rb
index bb44fa2a421..5d5fc712156 100644
--- a/logstash-core/lib/logstash/patches/cabin.rb
+++ b/logstash-core/lib/logstash/patches/cabin.rb
@@ -9,7 +9,7 @@
   # Basically, the following is wastes tons of effort creating objects that are
   # never used if the log level hides the log:
   #
-  #     logger.debug("something happend", :what => Happened)
+  #     logger.debug("something happened", :what => Happened)
   #
   # This is shown to be 4x faster:
   #
diff --git a/logstash-core/lib/logstash/patches/clamp.rb b/logstash-core/lib/logstash/patches/clamp.rb
new file mode 100644
index 00000000000..bdebfeb6f6e
--- /dev/null
+++ b/logstash-core/lib/logstash/patches/clamp.rb
@@ -0,0 +1,96 @@
+require 'clamp'
+require 'logstash/environment'
+
+module Clamp
+  module Attribute
+    class Instance
+      def default_from_environment
+        # we don't want uncontrolled var injection from the environment
+        # since we're establishing that settings can be pulled from only three places:
+        # 1. default settings
+        # 2. yaml file
+        # 3. cli arguments
+      end
+    end
+  end
+
+  module Option
+
+    module Declaration
+      def deprecated_option(switches, type, description, opts = {})
+        Option::Definition.new(switches, type, description, opts).tap do |option|
+          declared_options << option
+          block ||= option.default_conversion_block
+          define_deprecated_accessors_for(option, opts, &block)
+        end
+      end
+    end
+
+    module StrictDeclaration
+
+      include Clamp::Attribute::Declaration
+      include LogStash::Util::Loggable
+
+      # Instead of letting Clamp set up accessors for the options
+      # weÅ•e going to tightly controlling them through
+      # LogStash::SETTINGS
+      def define_simple_writer_for(option, &block)
+        LogStash::SETTINGS.get(option.attribute_name)
+        define_method(option.write_method) do |value|
+          value = instance_exec(value, &block) if block
+          LogStash::SETTINGS.set_value(option.attribute_name, value)
+        end
+      end
+
+      def define_reader_for(option)
+        define_method(option.read_method) do
+          LogStash::SETTINGS.get_value(option.attribute_name)
+        end
+      end
+
+      def define_deprecated_accessors_for(option, opts, &block)
+        define_deprecated_writer_for(option, opts, &block)
+      end
+
+      def define_deprecated_writer_for(option, opts, &block)
+        define_method(option.write_method) do |value|
+          self.class.logger.warn "DEPRECATION WARNING: The flag #{option.switches} has been deprecated, please use \"--#{opts[:new_flag]}=#{opts[:new_value]}\" instead."
+          LogStash::SETTINGS.set(opts[:new_flag], opts[:new_value])
+        end
+      end
+    end
+
+    class Definition
+      # Allow boolean flags to optionally receive a true/false argument
+      # to explicitly set them, i.e.
+      # --long.flag.name       => sets flag to true
+      # --long.flag.name true  => sets flag to true
+      # --long.flag.name false => sets flag to false
+      # --long.flag.name=true  => sets flag to true
+      # --long.flag.name=false => sets flag to false
+      def extract_value(switch, arguments)
+        if flag? && (arguments.first.nil? || arguments.first.match("^-"))
+          flag_value(switch)
+        else
+          arguments.shift
+        end
+      end
+    end
+  end
+
+  # Create a subclass of Clamp::Command that enforces the use of
+  # LogStash::SETTINGS for setting validation
+  class StrictCommand < Command
+    class << self
+      include ::Clamp::Option::StrictDeclaration
+    end
+
+    def handle_remaining_arguments
+      unless remaining_arguments.empty?
+        signal_usage_error "Unknown command '#{remaining_arguments.first}'"
+      end
+    end
+  end
+end
+
+
diff --git a/logstash-core/lib/logstash/patches/exception_to_json.rb b/logstash-core/lib/logstash/patches/exception_to_json.rb
new file mode 100644
index 00000000000..00de79f25f6
--- /dev/null
+++ b/logstash-core/lib/logstash/patches/exception_to_json.rb
@@ -0,0 +1,5 @@
+class Exception
+  def to_json
+    {"exception_name" => self.class.name, "message" => message}
+  end
+end
diff --git a/logstash-core/lib/logstash/patches/puma.rb b/logstash-core/lib/logstash/patches/puma.rb
new file mode 100644
index 00000000000..b8507db1875
--- /dev/null
+++ b/logstash-core/lib/logstash/patches/puma.rb
@@ -0,0 +1,44 @@
+# encoding: utf-8
+#
+# Patch to replace the usage of STDERR and STDOUT
+# see: https://github.com/elastic/logstash/issues/5912
+module LogStash
+  class NullLogger
+    def self.debug(message)
+    end
+  end
+
+  # Puma uses by default the STDERR an the STDOUT for all his error
+  # handling, the server class accept custom a events object that can accept custom io object,
+  # so I just wrap the logger into an IO like object.
+  class IOWrappedLogger
+    def initialize(new_logger)
+      @logger_lock = Mutex.new
+      @logger = new_logger
+    end
+
+    def sync=(v)
+      # noop
+    end
+
+    def logger=(logger)
+      @logger_lock.synchronize { @logger = logger }
+    end
+
+    def puts(str)
+      # The logger only accept a str as the first argument
+      @logger_lock.synchronize { @logger.debug(str.to_s) }
+    end
+    alias_method :write, :puts
+    alias_method :<<, :puts
+  end
+
+end
+
+# Reopen the puma class to create a scoped STDERR and STDOUT
+# This operation is thread safe since its done at the class level
+# and force JRUBY to flush his classes cache.
+module Puma
+  STDERR = LogStash::IOWrappedLogger.new(LogStash::NullLogger)
+  STDOUT = LogStash::IOWrappedLogger.new(LogStash::NullLogger)
+end
diff --git a/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb b/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
index ccd43ac1f86..eeb841dd874 100644
--- a/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
+++ b/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
@@ -54,7 +54,7 @@ def self.__default_options
   #
   # This monkeypatch doesn't enforce a `VERIFY_MODE` on the SSLContext,
   # SSLContext are both used for the client and the server implementation,
-  # If set the `verify_mode` to peer the server wont accept any connection,
+  # If set the `verify_mode` to peer the server won't accept any connection,
   # because it will try to verify the client certificate, this is a protocol
   # details implemented at the plugin level.
   #
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 9fcb77e9101..9eabf82b81a 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -4,111 +4,167 @@
 require "concurrent"
 require "logstash/namespace"
 require "logstash/errors"
+require "logstash-core/logstash-core"
 require "logstash/event"
 require "logstash/config/file"
 require "logstash/filters/base"
 require "logstash/inputs/base"
 require "logstash/outputs/base"
-require "logstash/config/cpu_core_strategy"
-require "logstash/util/defaults_printer"
 require "logstash/shutdown_watcher"
-require "logstash/util/wrapped_synchronous_queue"
 require "logstash/pipeline_reporter"
 require "logstash/instrument/metric"
 require "logstash/instrument/namespaced_metric"
 require "logstash/instrument/null_metric"
+require "logstash/instrument/namespaced_null_metric"
 require "logstash/instrument/collector"
+require "logstash/instrument/wrapped_write_client"
 require "logstash/output_delegator"
 require "logstash/filter_delegator"
+require "logstash/queue_factory"
+require "logstash/execution_context"
 
-module LogStash; class Pipeline
- attr_reader :inputs,
-    :filters,
-    :outputs,
-    :worker_threads,
-    :events_consumed,
-    :events_filtered,
-    :reporter,
-    :pipeline_id,
-    :metric,
-    :logger,
-    :started_at,
-    :thread,
-    :config_str,
-    :original_settings
-
-  DEFAULT_SETTINGS = {
-    :default_pipeline_workers => LogStash::Config::CpuCoreStrategy.maximum,
-    :pipeline_batch_size => 125,
-    :pipeline_batch_delay => 5, # in milliseconds
-    :flush_interval => 5, # in seconds
-    :flush_timeout_interval => 60 # in seconds
-  }
-  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
+module LogStash; class BasePipeline
+  include LogStash::Util::Loggable
 
-  def self.validate_config(config_str, settings = {})
-    begin
-      # There should be a better way to test this
-      self.new(config_str, settings)
-    rescue => e
-      e.message
-    end
-  end
+  attr_reader :config_str, :config_hash, :inputs, :filters, :outputs, :pipeline_id
 
-  def initialize(config_str, settings = {})
+  def initialize(config_str, settings = SETTINGS)
+    @logger = self.logger
     @config_str = config_str
-    @original_settings = settings
-    @logger = Cabin::Channel.get(LogStash)
-    @pipeline_id = settings[:pipeline_id] || self.object_id
-    @settings = DEFAULT_SETTINGS.clone
-    settings.each {|setting, value| configure(setting, value) }
-    @reporter = LogStash::PipelineReporter.new(@logger, self)
+    @config_hash = Digest::SHA1.hexdigest(@config_str)
+    # Every time #plugin is invoked this is incremented to give each plugin
+    # a unique id when auto-generating plugin ids
+    @plugin_counter ||= 0
+
+    @pipeline_id = settings.get_value("pipeline.id") || self.object_id
 
+    # A list of plugins indexed by id
+    @plugins_by_id = {}
     @inputs = nil
     @filters = nil
     @outputs = nil
+    @execution_context = LogStash::ExecutionContext.new(@pipeline_id)
 
-    @worker_threads = []
+    grammar = LogStashConfigParser.new
+    parsed_config = grammar.parse(config_str)
+    raise(ConfigurationError, grammar.failure_reason) if parsed_config.nil?
 
-    # Metric object should be passed upstream, multiple pipeline share the same metric
-    # and collector only the namespace will changes.
-    # If no metric is given, we use a `NullMetric` for all internal calls.
-    # We also do this to make the changes backward compatible with previous testing of the
-    # pipeline.
-    #
-    # This need to be configured before we evaluate the code to make
-    # sure the metric instance is correctly send to the plugin.
-    @metric = settings.fetch(:metric, Instrument::NullMetric.new)
+    config_code = parsed_config.compile
 
-    grammar = LogStashConfigParser.new
-    @config = grammar.parse(config_str)
-    if @config.nil?
-      raise LogStash::ConfigurationError, grammar.failure_reason
+    # config_code = BasePipeline.compileConfig(config_str)
+
+    if settings.get_value("config.debug") && @logger.debug?
+      @logger.debug("Compiled pipeline code", :code => config_code)
     end
-    # This will compile the config to ruby and evaluate the resulting code.
-    # The code will initialize all the plugins and define the
+
+    # Evaluate the config compiled code that will initialize all the plugins and define the
     # filter and output methods.
-    code = @config.compile
-    @code = code
+    begin
+      eval(config_code)
+    rescue => e
+      raise e
+    end
+  end
+
+  def plugin(plugin_type, name, *args)
+    @plugin_counter += 1
 
-    # The config code is hard to represent as a log message...
-    # So just print it.
-    @logger.debug? && @logger.debug("Compiled pipeline code:\n#{code}")
+    # Collapse the array of arguments into a single merged hash
+    args = args.reduce({}, &:merge)
+
+    id = if args["id"].nil? || args["id"].empty?
+      args["id"] = "#{@config_hash}-#{@plugin_counter}"
+    else
+      args["id"]
+    end
+
+    raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
+    @plugins_by_id[id] = true
+
+    # use NullMetric if called in the BasePipeline context otherwise use the @metric value
+    metric = @metric || Instrument::NullMetric.new
+
+    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
+    # Scope plugins of type 'input' to 'inputs'
+    type_scoped_metric = pipeline_scoped_metric.namespace("#{plugin_type}s".to_sym)
+
+    klass = Plugin.lookup(plugin_type, name)
+
+    if plugin_type == "output"
+      OutputDelegator.new(@logger, klass, type_scoped_metric, @execution_context, OutputDelegatorStrategyRegistry.instance, args)
+    elsif plugin_type == "filter"
+      FilterDelegator.new(@logger, klass, type_scoped_metric, @execution_context, args)
+    else # input
+      input_plugin = klass.new(args)
+      scoped_metric = type_scoped_metric.namespace(id.to_sym)
+      scoped_metric.gauge(:name, input_plugin.config_name)
+      input_plugin.metric = scoped_metric
+      input_plugin.execution_context = @execution_context
+      input_plugin
+    end
+  end
+
+  def reloadable?
+    non_reloadable_plugins.empty?
+  end
+
+  def non_reloadable_plugins
+    (inputs + filters + outputs).select { |plugin| !plugin.reloadable? }
+  end
+end; end
+
+module LogStash; class Pipeline < BasePipeline
+  attr_reader \
+    :worker_threads,
+    :events_consumed,
+    :events_filtered,
+    :reporter,
+    :started_at,
+    :thread,
+    :settings,
+    :metric,
+    :filter_queue_client,
+    :input_queue_client,
+    :queue
+
+  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
+
+  def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
+    # This needs to be configured before we call super which will evaluate the code to make
+    # sure the metric instance is correctly send to the plugins to make the namespace scoping work
+    @metric = if namespaced_metric
+      settings.get("metric.collect") ? namespaced_metric : Instrument::NullMetric.new(namespaced_metric.collector)
+    else
+      Instrument::NullMetric.new
+    end
+
+    @settings = settings
+    @reporter = PipelineReporter.new(@logger, self)
+    @worker_threads = []
+
+    super(config_str, settings)
 
     begin
-      eval(code)
+      @queue = LogStash::QueueFactory.create(settings)
     rescue => e
-      raise
+      @logger.error("Logstash failed to create queue", "exception" => e.message, "backtrace" => e.backtrace)
+      raise e
     end
 
-    @input_queue = LogStash::Util::WrappedSynchronousQueue.new
+    @input_queue_client = @queue.write_client
+    @filter_queue_client = @queue.read_client
+    @signal_queue = Queue.new
+    # Note that @inflight_batches as a central mechanism for tracking inflight
+    # batches will fail if we have multiple read clients here.
+    @filter_queue_client.set_events_metric(metric.namespace([:stats, :events]))
+    @filter_queue_client.set_pipeline_metric(
+        metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
+    )
+    @drain_queue =  @settings.get_value("queue.drain")
+
     @events_filtered = Concurrent::AtomicFixnum.new(0)
     @events_consumed = Concurrent::AtomicFixnum.new(0)
 
-    # We generally only want one thread at a time able to access pop/take/poll operations
-    # from this queue. We also depend on this to be able to block consumers while we snapshot
-    # in-flight buffers
-    @input_queue_pop_mutex = Mutex.new
     @input_threads = []
     # @ready requires thread safety since it is typically polled from outside the pipeline thread
     @ready = Concurrent::AtomicBoolean.new(false)
@@ -120,38 +176,29 @@ def ready?
     @ready.value
   end
 
-  def configure(setting, value)
-    @settings[setting] = value
-  end
-
   def safe_pipeline_worker_count
-    default = DEFAULT_SETTINGS[:default_pipeline_workers]
-    thread_count = @settings[:pipeline_workers] #override from args "-w 8" or config
+    default = @settings.get_default("pipeline.workers")
+    pipeline_workers = @settings.get("pipeline.workers") #override from args "-w 8" or config
     safe_filters, unsafe_filters = @filters.partition(&:threadsafe?)
+    plugins = unsafe_filters.collect { |f| f.config_name }
 
-    if unsafe_filters.any?
-      plugins = unsafe_filters.collect { |f| f.config_name }
-      case thread_count
-      when nil
-        # user did not specify a worker thread count
-        # warn if the default is multiple
-
-        if default > 1
-          @logger.warn("Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads",
-                       :count_was => default, :filters => plugins)
-        end
+    return pipeline_workers if unsafe_filters.empty?
 
-        1 # can't allow the default value to propagate if there are unsafe filters
-      when 0, 1
-        1
-      else
+    if @settings.set?("pipeline.workers")
+      if pipeline_workers > 1
         @logger.warn("Warning: Manual override - there are filters that might not work with multiple worker threads",
-                     :worker_threads => thread_count, :filters => plugins)
-        thread_count # allow user to force this even if there are unsafe filters
+                     :worker_threads => pipeline_workers, :filters => plugins)
       end
     else
-      thread_count || default
+      # user did not specify a worker thread count
+      # warn if the default is multiple
+      if default > 1
+        @logger.warn("Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads",
+                     :count_was => default, :filters => plugins)
+        return 1 # can't allow the default value to propagate if there are unsafe filters
+      end
     end
+    pipeline_workers
   end
 
   def filters?
@@ -161,13 +208,12 @@ def filters?
   def run
     @started_at = Time.now
 
-    LogStash::Util.set_thread_name("[#{pipeline_id}]-pipeline-manager")
-    @logger.terminal(LogStash::Util::DefaultsPrinter.print(@settings))
     @thread = Thread.current
+    Util.set_thread_name("[#{pipeline_id}]-pipeline-manager")
 
     start_workers
 
-    @logger.log("Pipeline #{@pipeline_id} started")
+    @logger.info("Pipeline #{@pipeline_id} started")
 
     # Block until all inputs have stopped
     # Generally this happens if SIGINT is sent and `shutdown` is called from an external thread
@@ -177,17 +223,24 @@ def run
     wait_inputs
     transition_to_stopped
 
-    @logger.info("Input plugins stopped! Will shutdown filter/output workers.")
+    @logger.debug("Input plugins stopped! Will shutdown filter/output workers.")
 
     shutdown_flusher
     shutdown_workers
 
-    @logger.log("Pipeline #{@pipeline_id} has been shutdown")
+    close
+
+    @logger.debug("Pipeline #{@pipeline_id} has been shutdown")
 
     # exit code
     return 0
   end # def run
 
+  def close
+    @filter_queue_client.close
+    @queue.close
+  end
+
   def transition_to_running
     @running.make_true
   end
@@ -204,37 +257,78 @@ def stopped?
     @running.false?
   end
 
-  def start_workers
-    @inflight_batches = {}
+  def system?
+    settings.get_value("pipeline.system")
+  end
+
+  # register_plugin simply calls the plugin #register method and catches & logs any error
+  # @param plugin [Plugin] the plugin to register
+  # @return [Plugin] the registered plugin
+  def register_plugin(plugin)
+    plugin.register
+    plugin
+  rescue => e
+    @logger.error("Error registering plugin", :plugin => plugin.inspect, :error => e.message)
+    raise e
+  end
 
+  # register_plugins calls #register_plugin on the plugins list and upon exception will call Plugin#do_close on all registered plugins
+  # @param plugins [Array[Plugin]] the list of plugins to register
+  def register_plugins(plugins)
+    registered = []
+    plugins.each { |plugin| registered << register_plugin(plugin) }
+  rescue => e
+    registered.each(&:do_close)
+    raise e
+  end
+
+  def start_workers
     @worker_threads.clear # In case we're restarting the pipeline
     begin
-      start_inputs
-      @outputs.each {|o| o.register }
-      @filters.each {|f| f.register }
+      register_plugins(@outputs)
+      register_plugins(@filters)
 
       pipeline_workers = safe_pipeline_worker_count
-      batch_size = @settings[:pipeline_batch_size]
-      batch_delay = @settings[:pipeline_batch_delay]
+      batch_size = @settings.get("pipeline.batch.size")
+      batch_delay = @settings.get("pipeline.batch.delay")
+
       max_inflight = batch_size * pipeline_workers
+
+      config_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :config])
+      config_metric.gauge(:workers, pipeline_workers)
+      config_metric.gauge(:batch_size, batch_size)
+      config_metric.gauge(:batch_delay, batch_delay)
+      config_metric.gauge(:config_reload_automatic, @settings.get("config.reload.automatic"))
+      config_metric.gauge(:config_reload_interval, @settings.get("config.reload.interval"))
+
       @logger.info("Starting pipeline",
-                   :id => self.pipeline_id,
-                   :pipeline_workers => pipeline_workers,
-                   :batch_size => batch_size,
-                   :batch_delay => batch_delay,
-                   :max_inflight => max_inflight)
+                   "id" => self.pipeline_id,
+                   "pipeline.workers" => pipeline_workers,
+                   "pipeline.batch.size" => batch_size,
+                   "pipeline.batch.delay" => batch_delay,
+                   "pipeline.max_inflight" => max_inflight)
       if max_inflight > MAX_INFLIGHT_WARN_THRESHOLD
         @logger.warn "CAUTION: Recommended inflight events max exceeded! Logstash will run with up to #{max_inflight} events in memory in your current configuration. If your message sizes are large this may cause instability with the default heap size. Please consider setting a non-standard heap size, changing the batch size (currently #{batch_size}), or changing the number of pipeline workers (currently #{pipeline_workers})"
       end
 
       pipeline_workers.times do |t|
         @worker_threads << Thread.new do
-          LogStash::Util.set_thread_name("[#{pipeline_id}]>worker#{t}")
+          Util.set_thread_name("[#{pipeline_id}]>worker#{t}")
           worker_loop(batch_size, batch_delay)
         end
       end
+
+      # inputs should be started last, after all workers
+      begin
+        start_inputs
+      rescue => e
+        # if there is any exception in starting inputs, make sure we shutdown workers.
+        # exception will already by logged in start_inputs
+        shutdown_workers
+        raise e
+      end
     ensure
-      # it is important to garantee @ready to be true after the startup sequence has been completed
+      # it is important to guarantee @ready to be true after the startup sequence has been completed
       # to potentially unblock the shutdown method which may be waiting on @ready to proceed
       @ready.make_true
     end
@@ -243,75 +337,43 @@ def start_workers
   # Main body of what a worker thread does
   # Repeatedly takes batches off the queue, filters, then outputs them
   def worker_loop(batch_size, batch_delay)
-    running = true
+    shutdown_requested = false
 
-    namespace_events = metric.namespace([:stats, :events])
-    namespace_pipeline = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
+    @filter_queue_client.set_batch_dimensions(batch_size, batch_delay)
 
-    while running
-      # To understand the purpose behind this synchronize please read the body of take_batch
-      input_batch, signal = @input_queue_pop_mutex.synchronize { take_batch(batch_size, batch_delay) }
-      running = false if signal == LogStash::SHUTDOWN
-
-      @events_consumed.increment(input_batch.size)
-      namespace_events.increment(:in, input_batch.size)
-      namespace_pipeline.increment(:in, input_batch.size)
-
-      filtered_batch = filter_batch(input_batch)
-
-      if signal # Flush on SHUTDOWN or FLUSH
-        flush_options = (signal == LogStash::SHUTDOWN) ? {:final => true} : {}
-        flush_filters_to_batch(filtered_batch, flush_options)
-      end
+    while true
+      signal = @signal_queue.empty? ? NO_SIGNAL : @signal_queue.pop
+      shutdown_requested |= signal.shutdown? # latch on shutdown signal
 
-      @events_filtered.increment(filtered_batch.size)
+      batch = @filter_queue_client.read_batch # metrics are started in read_batch
+      @events_consumed.increment(batch.size)
+      filter_batch(batch)
+      flush_filters_to_batch(batch, :final => false) if signal.flush?
+      output_batch(batch)
+      @filter_queue_client.close_batch(batch)
 
-      namespace_events.increment(:filtered, filtered_batch.size)
-      namespace_pipeline.increment(:filtered, filtered_batch.size)
-
-      output_batch(filtered_batch)
-
-      namespace_events.increment(:out, filtered_batch.size)
-      namespace_pipeline.increment(:out, filtered_batch.size)
-
-      inflight_batches_synchronize { set_current_thread_inflight_batch(nil) }
-    end
-  end
-
-  def take_batch(batch_size, batch_delay)
-    batch = []
-    # Since this is externally synchronized in `worker_look` wec can guarantee that the visibility of an insight batch
-    # guaranteed to be a full batch not a partial batch
-    set_current_thread_inflight_batch(batch)
-
-    signal = false
-    batch_size.times do |t|
-      event = (t == 0) ? @input_queue.take : @input_queue.poll(batch_delay)
-
-      if event.nil?
-        next
-      elsif event == LogStash::SHUTDOWN || event == LogStash::FLUSH
-        # We MUST break here. If a batch consumes two SHUTDOWN events
-        # then another worker may have its SHUTDOWN 'stolen', thus blocking
-        # the pipeline. We should stop doing work after flush as well.
-        signal = event
-        break
-      else
-        batch << event
-      end
+      # keep break at end of loop, after the read_batch operation, some pipeline specs rely on this "final read_batch" before shutdown.
+      break if shutdown_requested && !draining_queue?
     end
 
-    [batch, signal]
+    # we are shutting down, queue is drained if it was required, now  perform a final flush.
+    # for this we need to create a new empty batch to contain the final flushed events
+    batch = @filter_queue_client.new_batch
+    @filter_queue_client.start_metrics(batch) # explicitly call start_metrics since we dont do a read_batch here
+    flush_filters_to_batch(batch, :final => true)
+    output_batch(batch)
+    @filter_queue_client.close_batch(batch)
   end
 
   def filter_batch(batch)
-    batch.reduce([]) do |acc,e|
-      if e.is_a?(LogStash::Event)
-        filtered = filter_func(e)
-        filtered.each {|fe| acc << fe unless fe.cancelled?}
+    batch.each do |event|
+      filter_func(event).each do |e|
+        #these are both original and generated events
+        batch.merge(e) unless e.cancelled?
       end
-      acc
     end
+    @filter_queue_client.add_filtered_metrics(batch)
+    @events_filtered.increment(batch.size)
   rescue Exception => e
     # Plugins authors should manage their own exceptions in the plugin code
     # but if an exception is raised up to the worker thread they are considered
@@ -320,38 +382,32 @@ def filter_batch(batch)
     # Users need to check their configuration or see if there is a bug in the
     # plugin.
     @logger.error("Exception in pipelineworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
-                  "exception" => e, "backtrace" => e.backtrace)
-    raise
+                  "exception" => e.message, "backtrace" => e.backtrace)
+
+    raise e
   end
 
   # Take an array of events and send them to the correct output
   def output_batch(batch)
     # Build a mapping of { output_plugin => [events...]}
-    outputs_events = batch.reduce(Hash.new { |h, k| h[k] = [] }) do |acc, event|
+    output_events_map = Hash.new { |h, k| h[k] = [] }
+    batch.each do |event|
       # We ask the AST to tell us which outputs to send each event to
       # Then, we stick it in the correct bin
 
       # output_func should never return anything other than an Array but we have lots of legacy specs
       # that monkeypatch it and return nil. We can deprecate  "|| []" after fixing these specs
-      outputs_for_event = output_func(event) || []
-
-      outputs_for_event.each { |output| acc[output] << event }
-      acc
+      (output_func(event) || []).each do |output|
+        output_events_map[output].push(event)
+      end
     end
-
     # Now that we have our output to event mapping we can just invoke each output
     # once with its list of events
-    outputs_events.each { |output, events| output.multi_receive(events) }
-  end
-
-  def set_current_thread_inflight_batch(batch)
-    @inflight_batches[Thread.current] = batch
-  end
-
-  def inflight_batches_synchronize
-    @input_queue_pop_mutex.synchronize do
-      yield(@inflight_batches)
+    output_events_map.each do |output, events|
+      output.multi_receive(events)
     end
+    
+    @filter_queue_client.add_output_metrics(batch)
   end
 
   def wait_inputs
@@ -369,10 +425,11 @@ def start_inputs
     end
     @inputs += moreinputs
 
-    @inputs.each do |input|
-      input.register
-      start_input(input)
-    end
+    # first make sure we can register all input plugins
+    register_plugins(@inputs)
+
+    # then after all input plugins are successfully registered, start them
+    @inputs.each { |input| start_input(input) }
   end
 
   def start_input(plugin)
@@ -380,13 +437,14 @@ def start_input(plugin)
   end
 
   def inputworker(plugin)
-    LogStash::Util::set_thread_name("[#{pipeline_id}]<#{plugin.class.config_name}")
+    Util::set_thread_name("[#{pipeline_id}]<#{plugin.class.config_name}")
     begin
-      plugin.run(@input_queue)
+      input_queue_client = wrapped_write_client(plugin)
+      plugin.run(input_queue_client)
     rescue => e
       if plugin.stop?
         @logger.debug("Input plugin raised exception during shutdown, ignoring it.",
-                      :plugin => plugin.class.config_name, :exception => e,
+                      :plugin => plugin.class.config_name, :exception => e.message,
                       :backtrace => e.backtrace)
         return
       end
@@ -394,12 +452,12 @@ def inputworker(plugin)
       # otherwise, report error and restart
       if @logger.debug?
         @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
-                             :plugin => plugin.inspect, :error => e.to_s,
+                             :plugin => plugin.inspect, :error => e.message,
                              :exception => e.class,
                              :stacktrace => e.backtrace.join("\n")))
       else
         @logger.error(I18n.t("logstash.pipeline.worker-error",
-                             :plugin => plugin.inspect, :error => e))
+                             :plugin => plugin.inspect, :error => e.message))
       end
 
       # Assuming the failure that caused this exception is transient,
@@ -416,17 +474,17 @@ def inputworker(plugin)
   # @param before_stop [Proc] code block called before performing stop operation on input plugins
   def shutdown(&before_stop)
     # shutdown can only start once the pipeline has completed its startup.
-    # avoid potential race conditoon between the startup sequence and this
+    # avoid potential race condition between the startup sequence and this
     # shutdown method which can be called from another thread at any time
     sleep(0.1) while !ready?
 
-    # TODO: should we also check against calling shutdown multiple times concurently?
+    # TODO: should we also check against calling shutdown multiple times concurrently?
 
     before_stop.call if block_given?
 
-    @logger.info "Closing inputs"
+    @logger.debug "Closing inputs"
     @inputs.each(&:do_stop)
-    @logger.info "Closed inputs"
+    @logger.debug "Closed inputs"
   end # def shutdown
 
   # After `shutdown` is called from an external thread this is called from the main thread to
@@ -435,8 +493,8 @@ def shutdown(&before_stop)
   def shutdown_workers
     # Each worker thread will receive this exactly once!
     @worker_threads.each do |t|
-      @logger.debug("Pushing shutdown", :thread => t)
-      @input_queue.push(LogStash::SHUTDOWN)
+      @logger.debug("Pushing shutdown", :thread => t.inspect)
+      @signal_queue.push(SHUTDOWN)
     end
 
     @worker_threads.each do |t|
@@ -448,26 +506,6 @@ def shutdown_workers
     @outputs.each(&:do_close)
   end
 
-  def plugin(plugin_type, name, *args)
-    args << {} if args.empty?
-
-    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
-
-    klass = LogStash::Plugin.lookup(plugin_type, name)
-
-    if plugin_type == "output"
-      LogStash::OutputDelegator.new(@logger, klass, default_output_workers, pipeline_scoped_metric.namespace(:outputs), *args)
-    elsif plugin_type == "filter"
-      LogStash::FilterDelegator.new(@logger, klass, pipeline_scoped_metric.namespace(:filters), *args)
-    else
-      klass.new(*args)
-    end
-  end
-
-  def default_output_workers
-    @settings[:pipeline_workers] || @settings[:default_pipeline_workers]
-  end
-
   # for backward compatibility in devutils for the rspec helpers, this method is not used
   # in the pipeline anymore.
   def filter(event, &block)
@@ -476,7 +514,7 @@ def filter(event, &block)
   end
 
 
-  # perform filters flush and yeild flushed event to the passed block
+  # perform filters flush and yield flushed event to the passed block
   # @param options [Hash]
   # @option options [Boolean] :final => true to signal a final shutdown flush
   def flush_filters(options = {}, &block)
@@ -506,7 +544,7 @@ def shutdown_flusher
   def flush
     if @flushing.compare_and_set(false, true)
       @logger.debug? && @logger.debug("Pushing flush onto pipeline")
-      @input_queue.push(LogStash::FLUSH)
+      @signal_queue.push(FLUSH)
     end
   end
 
@@ -520,23 +558,24 @@ def uptime
   end
 
   # perform filters flush into the output queue
+  #
+  # @param batch [ReadClient::ReadBatch]
   # @param options [Hash]
-  # @option options [Boolean] :final => true to signal a final shutdown flush
   def flush_filters_to_batch(batch, options = {})
     flush_filters(options) do |event|
       unless event.cancelled?
         @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
-        batch << event
+        batch.merge(event)
       end
     end
 
     @flushing.set(false)
-  end # flush_filters_to_output!
+  end # flush_filters_to_batch
 
   def plugin_threads_info
     input_threads = @input_threads.select {|t| t.alive? }
     worker_threads = @worker_threads.select {|t| t.alive? }
-    (input_threads + worker_threads).map {|t| LogStash::Util.thread_info(t) }
+    (input_threads + worker_threads).map {|t| Util.thread_info(t) }
   end
 
   def stalling_threads_info
@@ -546,4 +585,51 @@ def stalling_threads_info
       .each {|t| t.delete("blocked_on") }
       .each {|t| t.delete("status") }
   end
-end end
+
+  def collect_stats
+    pipeline_metric = @metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :queue])
+    pipeline_metric.gauge(:type, settings.get("queue.type"))
+
+    if @queue.is_a?(LogStash::Util::WrappedAckedQueue) && @queue.queue.is_a?(LogStash::AckedQueue)
+      queue = @queue.queue
+      dir_path = queue.dir_path
+      file_store = Files.get_file_store(Paths.get(dir_path))
+
+      pipeline_metric.namespace([:capacity]).tap do |n|
+        n.gauge(:page_capacity_in_bytes, queue.page_capacity)
+        n.gauge(:max_queue_size_in_bytes, queue.max_size_in_bytes)
+        n.gauge(:max_unread_events, queue.max_unread_events)
+      end
+      pipeline_metric.namespace([:data]).tap do |n|
+        n.gauge(:free_space_in_bytes, file_store.get_unallocated_space)
+        n.gauge(:storage_type, file_store.type)
+        n.gauge(:path, dir_path)
+      end
+
+      pipeline_metric.gauge(:events, queue.unread_count)
+    end
+  end
+
+  # Sometimes we log stuff that will dump the pipeline which may contain
+  # sensitive information (like the raw syntax tree which can contain passwords)
+  # We want to hide most of what's in here
+  def inspect
+    {
+      :pipeline_id => @pipeline_id,
+      :settings => @settings.inspect,
+      :ready => @ready,
+      :running => @running,
+      :flushing => @flushing
+    }
+  end
+
+  private
+
+  def draining_queue?
+    @drain_queue ? !@filter_queue_client.empty? : false
+  end
+
+  def wrapped_write_client(plugin)
+    LogStash::Instrument::WrappedWriteClient.new(@input_queue_client, self, metric, plugin)
+  end
+end; end
diff --git a/logstash-core/lib/logstash/pipeline_reporter.rb b/logstash-core/lib/logstash/pipeline_reporter.rb
index c7ae6ca847c..ed73144c86c 100644
--- a/logstash-core/lib/logstash/pipeline_reporter.rb
+++ b/logstash-core/lib/logstash/pipeline_reporter.rb
@@ -39,7 +39,7 @@ def format_threads_by_plugin
     end
   end
 
-  def initialize(logger,pipeline)
+  def initialize(logger, pipeline)
     @logger = logger
     @pipeline = pipeline
   end
@@ -52,14 +52,14 @@ def snapshot
   end
 
   def to_hash
-    pipeline.inflight_batches_synchronize do |batch_map|
+    # pipeline.filter_queue_client.inflight_batches is synchronized
+    pipeline.filter_queue_client.inflight_batches do |batch_map|
       worker_states_snap = worker_states(batch_map) # We only want to run this once
       inflight_count = worker_states_snap.map {|s| s[:inflight_count] }.reduce(0, :+)
 
       {
         :events_filtered => events_filtered,
         :events_consumed => events_consumed,
-        :worker_count => pipeline.worker_threads.size,
         :inflight_count => inflight_count,
         :worker_states => worker_states_snap,
         :output_info => output_info,
@@ -83,32 +83,27 @@ def plugin_threads
     pipeline.plugin_threads
   end
 
-  # Not threadsafe! must be called within an `inflight_batches_synchronize` block
+  # Not threadsafe! ensure synchronization
   def worker_states(batch_map)
-      pipeline.worker_threads.map.with_index do |thread,idx|
-        status = thread.status || "dead"
-        inflight_count = batch_map[thread] ? batch_map[thread].size : 0
-        {
-          :status => status,
-          :alive => thread.alive?,
-          :index => idx,
-          :inflight_count => inflight_count
-        }
+    pipeline.worker_threads.map.with_index do |thread, idx|
+      status = thread.status || "dead"
+      inflight_count = batch_map[thread] ? batch_map[thread].size : 0
+      {
+        :status => status,
+        :alive => thread.alive?,
+        :index => idx,
+        :inflight_count => inflight_count
+      }
     end
   end
 
   def output_info
     pipeline.outputs.map do |output_delegator|
-      is_multi_worker = output_delegator.worker_count > 1
-
       {
         :type => output_delegator.config_name,
-        :config => output_delegator.config,
-        :is_multi_worker => is_multi_worker,
-        :events_received => output_delegator.events_received,
-        :workers => output_delegator.workers,
-        :busy_workers => output_delegator.busy_workers
+        :id => output_delegator.id,
+        :concurrency => output_delegator.concurrency,        
       }
     end
   end
-end end
\ No newline at end of file
+end end
diff --git a/logstash-core/lib/logstash/plugin.rb b/logstash-core/lib/logstash/plugin.rb
index d6c335e7279..cf5f474bbb5 100644
--- a/logstash-core/lib/logstash/plugin.rb
+++ b/logstash-core/lib/logstash/plugin.rb
@@ -3,13 +3,13 @@
 require "logstash/logging"
 require "logstash/config/mixin"
 require "logstash/instrument/null_metric"
-require "cabin"
 require "concurrent"
 require "securerandom"
 
 class LogStash::Plugin
-  attr_accessor :params
-  attr_accessor :logger
+  include LogStash::Util::Loggable
+
+  attr_accessor :params, :execution_context
 
   NL = "\n"
 
@@ -20,18 +20,20 @@ class LogStash::Plugin
   # for a specific plugin.
   config :enable_metric, :validate => :boolean, :default => true
 
-  # Add a unique `ID` to the plugin instance, this `ID` is used for tracking
-  # information for a specific configuration of the plugin.
+  # Add a unique `ID` to the plugin configuration. If no ID is specified, Logstash will generate one. 
+  # It is strongly recommended to set this ID in your configuration. This is particularly useful 
+  # when you have two or more plugins of the same type, for example, if you have 2 grok filters. 
+  # Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.
   #
-  # ```
+  # [source,ruby]
+  # ---------------------------------------------------------------------------------------------------
   # output {
   #  stdout {
-  #    id => "ABC"
+  #    id => "my_plugin_id"
   #  }
   # }
-  # ```
+  # ---------------------------------------------------------------------------------------------------
   #
-  # If you don't explicitely set this variable Logstash will generate a unique name.
   config :id, :validate => :string
 
   def hash
@@ -39,14 +41,22 @@ def hash
     self.class.name.hash
   end
 
-
   def eql?(other)
     self.class.name == other.class.name && @params == other.params
   end
 
   def initialize(params=nil)
+    @logger = self.logger
+    # need to access settings statically because plugins are initialized in config_ast with no context.
+    settings = LogStash::SETTINGS
+    @slow_logger = self.slow_logger(settings.get("slowlog.threshold.warn"),
+                                    settings.get("slowlog.threshold.info"),
+                                    settings.get("slowlog.threshold.debug"),
+                                    settings.get("slowlog.threshold.trace"))
     @params = LogStash::Util.deep_clone(params)
-    @logger = Cabin::Channel.get(LogStash)
+    # The id should always be defined normally, but in tests that might not be the case
+    # In the future we may make this more strict in the Plugin API
+    @params["id"] ||= "#{self.class.config_name}_#{SecureRandom.uuid}"
   end
 
   # Return a uniq ID for this plugin configuration, by default
@@ -56,21 +66,13 @@ def initialize(params=nil)
   #
   # @return [String] A plugin ID
   def id
-    (@params["id"].nil? || @params["id"].empty?) ? SecureRandom.uuid : @params["id"]
-  end
-
-  # Return a unique_name, This is composed by the name of
-  # the plugin and the generated ID (of the configured one)
-  #
-  # @return [String] a unique name
-  def plugin_unique_name
-    "#{config_name}_#{id}"
+    @params["id"]
   end
 
   # close is called during shutdown, after the plugin worker
   # main task terminates
   def do_close
-    @logger.debug("closing", :plugin => self)
+    @logger.debug("closing", :plugin => self.class.name)
     close
   end
 
@@ -95,6 +97,14 @@ def inspect
     end
   end
 
+  def reloadable?
+    self.class.reloadable?
+  end
+
+  def self.reloadable?
+    true
+  end
+
   def debug_info
     [self.class.to_s, original_params]
   end
@@ -104,7 +114,14 @@ def metric=(new_metric)
   end
 
   def metric
-    @metric_plugin ||= enable_metric ? @metric : LogStash::Instrument::NullMetric.new
+    # We can disable metric per plugin if we want in the configuration
+    # we will use the NullMetric in this case.
+    @metric_plugin ||= if @enable_metric
+                         # Fallback when testing plugin and no metric collector are correctly configured.
+                         @metric.nil? ? LogStash::Instrument::NamespacedNullMetric.new : @metric
+                       else
+                         LogStash::Instrument::NamespacedNullMetric.new(@metric, :null)
+                       end
   end
 
   # return the configured name of this plugin
@@ -113,56 +130,12 @@ def config_name
     self.class.config_name
   end
 
-
-  # Look up a plugin by type and name.
+  # This is keep for backward compatibility, the logic was moved into the registry class
+  # but some plugins use this method to return a specific instance on lookup
+  #
+  # Should I remove this now and make sure the pipeline invoke the Registry or I should wait for 6.0
+  # Its not really part of the public api but its used by the tests a lot to mock the plugins.
   def self.lookup(type, name)
-    path = "logstash/#{type}s/#{name}"
-
-    # first check if plugin already exists in namespace and continue to next step if not
-    begin
-      return namespace_lookup(type, name)
-    rescue NameError
-      logger.debug("Plugin not defined in namespace, checking for plugin file", :type => type, :name => name, :path => path)
-    end
-
-    # try to load the plugin file. ex.: lookup("filter", "grok") will require logstash/filters/grok
-    require(path)
-
-    # check again if plugin is now defined in namespace after the require
-    namespace_lookup(type, name)
-  rescue LoadError, NameError => e
-    raise(LogStash::PluginLoadingError, I18n.t("logstash.pipeline.plugin-loading-error", :type => type, :name => name, :path => path, :error => e.to_s))
-  end
-
-  private
-  # lookup a plugin by type and name in the existing LogStash module namespace
-  # ex.: namespace_lookup("filter", "grok") looks for LogStash::Filters::Grok
-  # @param type [String] plugin type, "input", "ouput", "filter"
-  # @param name [String] plugin name, ex.: "grok"
-  # @return [Class] the plugin class or raises NameError
-  # @raise NameError if plugin class does not exist or is invalid
-  def self.namespace_lookup(type, name)
-    type_const = "#{type.capitalize}s"
-    namespace = LogStash.const_get(type_const)
-    # the namespace can contain constants which are not for plugins classes (do not respond to :config_name)
-    # namespace.constants is the shallow collection of all constants symbols in namespace
-    # note that below namespace.const_get(c) should never result in a NameError since c is from the constants collection
-    klass_sym = namespace.constants.find { |c| is_a_plugin?(namespace.const_get(c), name) }
-    klass = klass_sym && namespace.const_get(klass_sym)
-    raise(NameError) unless klass
-    klass
-  end
-
-  # check if klass is a valid plugin for name
-  # @param klass [Class] plugin class
-  # @param name [String] plugin name
-  # @return [Boolean] true if klass is a valid plugin for name
-  def self.is_a_plugin?(klass, name)
-    klass.ancestors.include?(LogStash::Plugin) && klass.respond_to?(:config_name) && klass.config_name == name
-  end
-
-  # @return [Cabin::Channel] logger channel for class methods
-  def self.logger
-    @logger ||= Cabin::Channel.get(LogStash)
+    LogStash::PLUGIN_REGISTRY.lookup_pipeline_plugin(type, name)
   end
 end # class LogStash::Plugin
diff --git a/logstash-core/lib/logstash/plugins/hooks_registry.rb b/logstash-core/lib/logstash/plugins/hooks_registry.rb
new file mode 100644
index 00000000000..93ba026a263
--- /dev/null
+++ b/logstash-core/lib/logstash/plugins/hooks_registry.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+module LogStash module Plugins
+  # This calls allow logstash to expose the endpoints for listeners
+  class HooksRegistry
+    java_import "java.util.concurrent.ConcurrentHashMap"
+    java_import "java.util.concurrent.CopyOnWriteArrayList"
+
+    def initialize
+      @registered_emitters = ConcurrentHashMap.new
+      @registered_hooks = ConcurrentHashMap.new
+    end
+
+    def register_emitter(emitter_scope, dispatcher)
+      @registered_emitters.put(emitter_scope, dispatcher)
+      sync_hooks
+    end
+
+    def remove_emitter(emitter_scope)
+      @registered_emitters.remove(emitter_scope)
+    end
+
+    def register_hooks(emitter_scope, callback)
+      callbacks = @registered_hooks.computeIfAbsent(emitter_scope) do
+        CopyOnWriteArrayList.new
+      end
+
+      callbacks.add(callback)
+      sync_hooks
+    end
+
+    def emitters_count
+      @registered_emitters.size
+    end
+
+    def hooks_count(emitter_scope = nil)
+      if emitter_scope.nil?
+        @registered_hooks.elements().collect(&:size).reduce(0, :+)
+      else
+        callbacks = @registered_hooks.get(emitter_scope)
+        callbacks.nil? ? 0 : @registered_hooks.get(emitter_scope).size
+      end
+    end
+
+    private
+    def sync_hooks
+      @registered_emitters.each do |emitter, dispatcher|
+        listeners = @registered_hooks.get(emitter)
+
+        unless listeners.nil?
+          listeners.each do |listener|
+            dispatcher.add_listener(listener)
+          end
+        end
+      end
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/plugins/registry.rb b/logstash-core/lib/logstash/plugins/registry.rb
new file mode 100644
index 00000000000..622b5da3539
--- /dev/null
+++ b/logstash-core/lib/logstash/plugins/registry.rb
@@ -0,0 +1,248 @@
+# encoding: utf-8
+require "rubygems/package"
+require "logstash/util/loggable"
+require "logstash/plugin"
+require "logstash/plugins/hooks_registry"
+
+module LogStash module Plugins
+  class Registry
+    include LogStash::Util::Loggable
+
+    # Add a bit more sanity with when interacting with the rubygems'
+    # specifications database, most of out code interact directly with really low level
+    # components of bundler/rubygems we need to encapsulate that and this is a start.
+    class GemRegistry
+      LOGSTASH_METADATA_KEY = "logstash_plugin"
+
+      class << self
+        def installed_gems
+          ::Gem::Specification
+        end
+
+        def logstash_plugins
+          installed_gems
+            .select { |spec| spec.metadata && spec.metadata[LOGSTASH_METADATA_KEY] }
+            .collect { |spec| PluginRawContext.new(spec) }
+        end
+      end
+    end
+
+    class PluginRawContext
+      HOOK_FILE = "logstash_registry.rb"
+      NAME_DELIMITER = "-"
+
+      attr_reader :spec
+
+      def initialize(spec)
+        @spec = spec
+        @destructured_name = spec.name.split(NAME_DELIMITER)
+      end
+
+      def name
+        @destructured_name[2..-1].join(NAME_DELIMITER)
+      end
+
+      def type
+        @destructured_name[1]
+      end
+
+      # In the context of the plugin, the hook file available  need to exist in any top level
+      # required paths.
+      #
+      # Example for the logstash-output-elasticsearch we have this line in the gemspec.
+      #
+      # s.require_paths = ["lib"], so the we will expect to have a `logstash_registry.rb` file in the `lib`
+      # directory.
+      def hooks_file
+        @hook_file ||= spec.full_require_paths.collect do |path|
+          f = ::File.join(path, HOOK_FILE)
+          ::File.exist?(f) ? f : nil
+        end.compact.first
+      end
+
+      def has_hooks?
+        !hooks_file.nil?
+      end
+
+      def execute_hooks!
+        require hooks_file
+      end
+    end
+
+    class PluginSpecification
+      attr_reader :type, :name, :klass
+
+      def initialize(type, name, klass)
+        @type  = type.to_sym
+        @name  = name
+        @klass = klass
+      end
+    end
+
+    class UniversalPluginSpecification < PluginSpecification
+      def initialize(type, name, klass)
+        super(type, name, klass)
+        @instance = klass.new
+      end
+
+      def register(hooks, settings)
+        @instance.register_hooks(hooks)
+        @instance.additionals_settings(settings)
+      end
+    end
+
+    attr_reader :hooks
+
+    def initialize
+      @registry = {}
+      @hooks = HooksRegistry.new
+    end
+
+    def setup!
+      load_available_plugins
+      execute_universal_plugins
+    end
+
+    def execute_universal_plugins
+      @registry.values
+        .select { |specification| specification.is_a?(UniversalPluginSpecification) }
+        .each { |specification| specification.register(hooks, LogStash::SETTINGS) }
+    end
+
+    def load_available_plugins
+      GemRegistry.logstash_plugins.each do |plugin_context|
+        # When a plugin has a HOOK_FILE defined, its the responsibility of the plugin
+        # to register itself to the registry of available plugins.
+        #
+        # Legacy plugin will lazy register themselves
+        if plugin_context.has_hooks?
+          begin
+            logger.debug("Executing hooks", :name => plugin_context.name, :type => plugin_context.type, :hooks_file => plugin_context.hooks_file)
+            plugin_context.execute_hooks!
+          rescue => e
+            logger.error("error occured when loading plugins hooks file", :name => plugin_context.name, :type => plugin_context.type, :exception => e.message, :stacktrace => e.backtrace )
+          end
+        end
+      end
+    end
+
+    def lookup(type, plugin_name, &block)
+      plugin = get(type, plugin_name)
+      # Assume that we have a legacy plugin
+      if plugin.nil?
+        plugin = legacy_lookup(type, plugin_name)
+      end
+
+      if block_given? # if provided pass a block to do validation
+        raise LoadError, "Block validation fails for plugin named #{plugin_name} of type #{type}," unless block.call(plugin.klass, plugin_name)
+      end
+
+      return plugin.klass
+    end
+
+    # The legacy_lookup method uses the 1.5->5.0 file structure to find and match
+    # a plugin and will do a lookup on the namespace of the required class to find a matching
+    # plugin with the appropriate type.
+    def legacy_lookup(type, plugin_name)
+      begin
+        path = "logstash/#{type}s/#{plugin_name}"
+
+        begin
+          require path
+        rescue LoadError
+          # Plugin might be already defined in the current scope
+          # This scenario often happen in test when we write an adhoc class
+        end
+
+        klass = namespace_lookup(type, plugin_name)
+        plugin = lazy_add(type, plugin_name, klass)
+      rescue => e
+        logger.error("Problems loading a plugin with",
+                    :type => type,
+                    :name => plugin_name,
+                    :path => path,
+                    :error_message => e.message,
+                    :error_class => e.class,
+                    :error_backtrace => e.backtrace)
+
+        raise LoadError, "Problems loading the requested plugin named #{plugin_name} of type #{type}. Error: #{e.class} #{e.message}"
+      end
+
+      plugin
+    end
+
+    def lookup_pipeline_plugin(type, name)
+      LogStash::PLUGIN_REGISTRY.lookup(type, name) do |plugin_klass, plugin_name|
+        is_a_plugin?(plugin_klass, plugin_name)
+      end
+    rescue LoadError, NameError => e
+      logger.debug("Problems loading the plugin with", :type => type, :name => name)
+      raise(LogStash::PluginLoadingError, I18n.t("logstash.pipeline.plugin-loading-error", :type => type, :name => name, :error => e.to_s))
+    end
+
+    def lazy_add(type, name, klass)
+      logger.debug("On demand adding plugin to the registry", :name => name, :type => type, :class => klass)
+      add_plugin(type, name, klass)
+    end
+
+    def add(type, name, klass)
+      logger.debug("Adding plugin to the registry", :name => name, :type => type, :class => klass)
+      add_plugin(type, name, klass)
+    end
+
+    def get(type, plugin_name)
+      @registry[key_for(type, plugin_name)]
+    end
+
+    def exists?(type, name)
+      @registry.include?(key_for(type, name))
+    end
+
+    def size
+      @registry.size
+    end
+
+    private
+    # lookup a plugin by type and name in the existing LogStash module namespace
+    # ex.: namespace_lookup("filter", "grok") looks for LogStash::Filters::Grok
+    # @param type [String] plugin type, "input", "output", "filter"
+    # @param name [String] plugin name, ex.: "grok"
+    # @return [Class] the plugin class or raises NameError
+    # @raise NameError if plugin class does not exist or is invalid
+    def namespace_lookup(type, name)
+      type_const = "#{type.capitalize}s"
+      namespace = LogStash.const_get(type_const)
+      # the namespace can contain constants which are not for plugins classes (do not respond to :config_name)
+      # namespace.constants is the shallow collection of all constants symbols in namespace
+      # note that below namespace.const_get(c) should never result in a NameError since c is from the constants collection
+      klass_sym = namespace.constants.find { |c| is_a_plugin?(namespace.const_get(c), name) }
+      klass = klass_sym && namespace.const_get(klass_sym)
+
+      raise(NameError) unless klass
+      klass
+    end
+
+    # check if klass is a valid plugin for name
+    # @param klass [Class] plugin class
+    # @param name [String] plugin name
+    # @return [Boolean] true if klass is a valid plugin for name
+    def is_a_plugin?(klass, name)
+      klass.ancestors.include?(LogStash::Plugin) && klass.respond_to?(:config_name) && klass.config_name == name
+    end
+
+    def add_plugin(type, name, klass)
+      if !exists?(type, name)
+        specification_klass = type == :universal ? UniversalPluginSpecification : PluginSpecification
+        @registry[key_for(type, name)] = specification_klass.new(type, name, klass)
+      else
+        logger.debug("Ignoring, plugin already added to the registry", :name => name, :type => type, :klass => klass)
+      end
+    end
+
+    def key_for(type, plugin_name)
+      "#{type}-#{plugin_name}"
+    end
+  end end
+
+  PLUGIN_REGISTRY = Plugins::Registry.new
+end
diff --git a/logstash-core/lib/logstash/queue_factory.rb b/logstash-core/lib/logstash/queue_factory.rb
new file mode 100644
index 00000000000..70b215557f4
--- /dev/null
+++ b/logstash-core/lib/logstash/queue_factory.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require "fileutils"
+require "logstash/event"
+require "logstash/namespace"
+require "logstash/util/wrapped_acked_queue"
+require "logstash/util/wrapped_synchronous_queue"
+
+module LogStash
+  class QueueFactory
+    def self.create(settings)
+      queue_type = settings.get("queue.type")
+      queue_page_capacity = settings.get("queue.page_capacity")
+      queue_max_bytes = settings.get("queue.max_bytes")
+      queue_max_events = settings.get("queue.max_events")
+      checkpoint_max_acks = settings.get("queue.checkpoint.acks")
+      checkpoint_max_writes = settings.get("queue.checkpoint.writes")
+      checkpoint_max_interval = settings.get("queue.checkpoint.interval")
+
+      queue_path = ::File.join(settings.get("path.queue"), settings.get("pipeline.id"))
+
+      case queue_type
+      when "memory_acked"
+        # memory_acked is used in tests/specs
+        FileUtils.mkdir_p(queue_path)
+        LogStash::Util::WrappedAckedQueue.create_memory_based(queue_path, queue_page_capacity, queue_max_events, queue_max_bytes)
+      when "persisted"
+        # persisted is the disk based acked queue
+        FileUtils.mkdir_p(queue_path)
+        LogStash::Util::WrappedAckedQueue.create_file_based(queue_path, queue_page_capacity, queue_max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, queue_max_bytes)
+      when "memory"
+        # memory is the legacy and default setting
+        LogStash::Util::WrappedSynchronousQueue.new
+      else
+        raise ConfigurationError, "Invalid setting `#{queue_type}` for `queue.type`, supported types are: 'memory_acked', 'memory', 'persisted'"
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index a8984244a46..0863b5ea6dc 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -3,194 +3,275 @@
 Encoding.default_external = Encoding::UTF_8
 $DEBUGLIST = (ENV["DEBUG"] || "").split(",")
 
-require "clamp" # gem 'clamp'
+require "clamp"
 require "net/http"
+
+require "logstash/namespace"
+require "logstash-core/logstash-core"
 require "logstash/environment"
 
 LogStash::Environment.load_locale!
 
-require "logstash/namespace"
 require "logstash/agent"
 require "logstash/config/defaults"
-
-class LogStash::Runner < Clamp::Command
-
-  option ["-f", "--config"], "CONFIG_PATH",
+require "logstash/shutdown_watcher"
+require "logstash/patches/clamp"
+require "logstash/settings"
+require "logstash/version"
+require "logstash/plugins/registry"
+
+java_import 'org.logstash.FileLockFactory'
+
+class LogStash::Runner < Clamp::StrictCommand
+  include LogStash::Util::Loggable
+  # The `path.settings` and `path.logs` need to be defined in the runner instead of the `logstash-core/lib/logstash/environment.rb`
+  # because the `Environment::LOGSTASH_HOME` doesn't exist in the context of the `logstash-core` gem.
+  #
+  # See issue https://github.com/elastic/logstash/issues/5361
+  LogStash::SETTINGS.register(LogStash::Setting::String.new("path.settings", ::File.join(LogStash::Environment::LOGSTASH_HOME, "config")))
+  LogStash::SETTINGS.register(LogStash::Setting::String.new("path.logs", ::File.join(LogStash::Environment::LOGSTASH_HOME, "logs")))
+
+  # Node Settings
+  option ["-n", "--node.name"], "NAME",
+    I18n.t("logstash.runner.flag.name"),
+    :attribute_name => "node.name",
+    :default => LogStash::SETTINGS.get_default("node.name")
+
+  # Config Settings
+  option ["-f", "--path.config"], "CONFIG_PATH",
     I18n.t("logstash.runner.flag.config"),
-    :attribute_name => :config_path
+    :attribute_name => "path.config"
 
-  option "-e", "CONFIG_STRING",
+  option ["-e", "--config.string"], "CONFIG_STRING",
     I18n.t("logstash.runner.flag.config-string",
-           :default_input => LogStash::Config::Defaults.input,
-           :default_output => LogStash::Config::Defaults.output),
-    :default => nil, :attribute_name => :config_string
+      :default_input => LogStash::Config::Defaults.input,
+      :default_output => LogStash::Config::Defaults.output),
+    :default => LogStash::SETTINGS.get_default("config.string"),
+    :attribute_name => "config.string"
 
-  option ["-w", "--pipeline-workers"], "COUNT",
+  # Pipeline settings
+  option ["-w", "--pipeline.workers"], "COUNT",
     I18n.t("logstash.runner.flag.pipeline-workers"),
-    :attribute_name => :pipeline_workers,
-    :default => LogStash::Pipeline::DEFAULT_SETTINGS[:default_pipeline_workers]
-
-  option ["-b", "--pipeline-batch-size"], "SIZE",
-         I18n.t("logstash.runner.flag.pipeline-batch-size"),
-         :attribute_name => :pipeline_batch_size,
-         :default => LogStash::Pipeline::DEFAULT_SETTINGS[:pipeline_batch_size]
+    :attribute_name => "pipeline.workers",
+    :default => LogStash::SETTINGS.get_default("pipeline.workers")
 
-  option ["-u", "--pipeline-batch-delay"], "DELAY_IN_MS",
-         I18n.t("logstash.runner.flag.pipeline-batch-delay"),
-         :attribute_name => :pipeline_batch_delay,
-         :default => LogStash::Pipeline::DEFAULT_SETTINGS[:pipeline_batch_delay]
+  option ["-b", "--pipeline.batch.size"], "SIZE",
+    I18n.t("logstash.runner.flag.pipeline-batch-size"),
+    :attribute_name => "pipeline.batch.size",
+    :default => LogStash::SETTINGS.get_default("pipeline.batch.size")
 
-  option ["-l", "--log"], "FILE",
-    I18n.t("logstash.runner.flag.log"),
-    :attribute_name => :log_file
+  option ["-u", "--pipeline.batch.delay"], "DELAY_IN_MS",
+    I18n.t("logstash.runner.flag.pipeline-batch-delay"),
+    :attribute_name => "pipeline.batch.delay",
+    :default => LogStash::SETTINGS.get_default("pipeline.batch.delay")
 
-  # Old support for the '-v' flag'
-  option "-v", :flag,
-    I18n.t("logstash.runner.flag.verbosity"),
-    :attribute_name => :verbosity, :multivalued => true
-
-  option "--quiet", :flag, I18n.t("logstash.runner.flag.quiet")
-  option "--verbose", :flag, I18n.t("logstash.runner.flag.verbose")
-  option "--debug", :flag, I18n.t("logstash.runner.flag.debug")
+  option ["--pipeline.unsafe_shutdown"], :flag,
+    I18n.t("logstash.runner.flag.unsafe_shutdown"),
+    :attribute_name => "pipeline.unsafe_shutdown",
+    :default => LogStash::SETTINGS.get_default("pipeline.unsafe_shutdown")
 
-  option ["-V", "--version"], :flag,
-    I18n.t("logstash.runner.flag.version")
+  # Data Path Setting
+  option ["--path.data"] , "PATH",
+    I18n.t("logstash.runner.flag.datapath"),
+    :attribute_name => "path.data",
+    :default => LogStash::SETTINGS.get_default("path.data")
 
-  option ["-p", "--pluginpath"] , "PATH",
+  # Plugins Settings
+  option ["-p", "--path.plugins"] , "PATH",
     I18n.t("logstash.runner.flag.pluginpath"),
-    :multivalued => true,
-    :attribute_name => :plugin_paths
+    :multivalued => true, :attribute_name => "path.plugins",
+    :default => LogStash::SETTINGS.get_default("path.plugins")
 
-  option ["-t", "--configtest"], :flag,
-    I18n.t("logstash.runner.flag.configtest"),
-    :attribute_name => :config_test
+  # Logging Settings
+  option ["-l", "--path.logs"], "PATH",
+    I18n.t("logstash.runner.flag.log"),
+    :attribute_name => "path.logs",
+    :default => LogStash::SETTINGS.get_default("path.logs")
 
-  option "--[no-]allow-unsafe-shutdown", :flag,
-    I18n.t("logstash.runner.flag.unsafe_shutdown"),
-    :attribute_name => :unsafe_shutdown,
-    :default => false
+  option "--log.level", "LEVEL", I18n.t("logstash.runner.flag.log_level"),
+    :default => LogStash::SETTINGS.get_default("log.level"),
+    :attribute_name => "log.level"
 
+  option "--config.debug", :flag,
+    I18n.t("logstash.runner.flag.config_debug"),
+    :default => LogStash::SETTINGS.get_default("config.debug"),
+    :attribute_name => "config.debug"
+
+  # Other settings
   option ["-i", "--interactive"], "SHELL",
     I18n.t("logstash.runner.flag.rubyshell"),
-    :attribute_name => :ruby_shell
+    :attribute_name => "interactive"
 
-  option ["-n", "--node-name"], "NAME",
-    I18n.t("logstash.runner.flag.node_name"),
-    :attribute_name => :node_name
+  option ["-V", "--version"], :flag,
+    I18n.t("logstash.runner.flag.version")
+
+  option ["-t", "--config.test_and_exit"], :flag,
+    I18n.t("logstash.runner.flag.configtest"),
+    :attribute_name => "config.test_and_exit",
+    :default => LogStash::SETTINGS.get_default("config.test_and_exit")
 
-  option ["-r", "--[no-]auto-reload"], :flag,
+  option ["-r", "--config.reload.automatic"], :flag,
     I18n.t("logstash.runner.flag.auto_reload"),
-    :attribute_name => :auto_reload, :default => false
+    :attribute_name => "config.reload.automatic",
+    :default => LogStash::SETTINGS.get_default("config.reload.automatic")
 
-  option ["--reload-interval"], "RELOAD_INTERVAL",
+  option ["--config.reload.interval"], "RELOAD_INTERVAL",
     I18n.t("logstash.runner.flag.reload_interval"),
-    :attribute_name => :reload_interval, :default => 3, &:to_i
+    :attribute_name => "config.reload.interval",
+    :default => LogStash::SETTINGS.get_default("config.reload.interval")
+
+  option ["--http.host"], "HTTP_HOST",
+    I18n.t("logstash.runner.flag.http_host"),
+    :attribute_name => "http.host",
+    :default => LogStash::SETTINGS.get_default("http.host")
+
+  option ["--http.port"], "HTTP_PORT",
+    I18n.t("logstash.runner.flag.http_port"),
+    :attribute_name => "http.port",
+    :default => LogStash::SETTINGS.get_default("http.port")
+
+  option ["--log.format"], "FORMAT",
+    I18n.t("logstash.runner.flag.log_format"),
+    :attribute_name => "log.format",
+    :default => LogStash::SETTINGS.get_default("log.format")
+
+  option ["--path.settings"], "SETTINGS_DIR",
+    I18n.t("logstash.runner.flag.path_settings"),
+    :attribute_name => "path.settings",
+    :default => LogStash::SETTINGS.get_default("path.settings")
+
+  ### DEPRECATED FLAGS ###
+  deprecated_option ["--verbose"], :flag,
+    I18n.t("logstash.runner.flag.verbose"),
+    :new_flag => "log.level", :new_value => "info"
+
+  deprecated_option ["--debug"], :flag,
+    I18n.t("logstash.runner.flag.debug"),
+    :new_flag => "log.level", :new_value => "debug"
+
+  deprecated_option ["--quiet"], :flag,
+    I18n.t("logstash.runner.flag.quiet"),
+    :new_flag => "log.level", :new_value => "error"
 
-  option ["--http-host"], "WEB_API_HTTP_HOST",
-    I18n.t("logstash.web_api.flag.http_host"),
-    :attribute_name => :web_api_http_host, :default => "127.0.0.1"
-
-  option ["--http-port"], "WEB_API_HTTP_PORT",
-    I18n.t("logstash.web_api.flag.http_port"),
-    :attribute_name => :web_api_http_port, :default => 9600
-
-  def pipeline_workers=(pipeline_workers_value)
-    @pipeline_settings[:pipeline_workers] = validate_positive_integer(pipeline_workers_value)
-  end
-
-  def pipeline_batch_size=(pipeline_batch_size_value)
-    @pipeline_settings[:pipeline_batch_size] = validate_positive_integer(pipeline_batch_size_value)
-  end
+  attr_reader :agent
 
-  def pipeline_batch_delay=(pipeline_batch_delay_value)
-    @pipeline_settings[:pipeline_batch_delay] = validate_positive_integer(pipeline_batch_delay_value)
+  def initialize(*args)
+    @settings = LogStash::SETTINGS
+    super(*args)
   end
 
-  def validate_positive_integer(str_arg)
-    int_arg = str_arg.to_i
-    if str_arg !~ /^\d+$/ || int_arg < 1
-      raise ArgumentError, "Expected a positive integer, got '#{str_arg}'"
+  def run(args)
+    settings_path = fetch_settings_path(args)
+
+    @settings.set("path.settings", settings_path) if settings_path
+
+    begin
+      LogStash::SETTINGS.from_yaml(LogStash::SETTINGS.get("path.settings"))
+    rescue Errno::ENOENT
+      $stderr.puts "WARNING: Could not find logstash.yml which is typically located in $LS_HOME/config or /etc/logstash. You can specify the path using --path.settings. Continuing using the defaults"
+    rescue => e
+      # abort unless we're just looking for the help
+      unless cli_help?(args)
+        if e.kind_of?(Psych::Exception)
+          yaml_file_path = ::File.join(LogStash::SETTINGS.get("path.settings"), "logstash.yml")
+          $stderr.puts "ERROR: Failed to parse YAML file \"#{yaml_file_path}\". Please confirm if the YAML structure is valid (e.g. look for incorrect usage of whitespace or indentation). Aborting... parser_error=>#{e.message}"
+        else
+          $stderr.puts "ERROR: Failed to load settings file from \"path.settings\". Aborting... path.setting=#{LogStash::SETTINGS.get("path.settings")}, exception=#{e.class}, message=>#{e.message}"
+        end
+        return 1
+      end
     end
 
-    int_arg
-  end
-
-  attr_reader :agent
-
-  def initialize(*args)
-    @logger = Cabin::Channel.get(LogStash)
-    @pipeline_settings ||= { :pipeline_id => "main" }
-    super(*args)
+    super(*[args])
   end
 
   def execute
+    # Only when execute is have the CLI options been added to the @settings
+    # We invoke post_process to apply extra logic to them.
+    # The post_process callbacks have been added in environment.rb
+    @settings.post_process
+    
     require "logstash/util"
     require "logstash/util/java_version"
     require "stud/task"
-    require "cabin" # gem 'cabin'
-
 
     # Configure Logstash logging facility, this need to be done before everything else to
     # make sure the logger has the correct settings and the log level is correctly defined.
-    configure_logging(log_file)
+    java.lang.System.setProperty("ls.logs", setting("path.logs"))
+    java.lang.System.setProperty("ls.log.format", setting("log.format"))
+    java.lang.System.setProperty("ls.log.level", setting("log.level"))
+    unless java.lang.System.getProperty("log4j.configurationFile")
+      log4j_config_location = ::File.join(setting("path.settings"), "log4j2.properties")
+      LogStash::Logging::Logger::initialize("file:///" + log4j_config_location)
+    end
+    # override log level that may have been introduced from a custom log4j config file
+    LogStash::Logging::Logger::configure_logging(setting("log.level"))
+
+    if setting("config.debug") && !logger.debug?
+      logger.warn("--config.debug was specified, but log.level was not set to \'debug\'! No config info will be logged.")
+    end
+
+    # We configure the registry and load any plugin that can register hooks
+    # with logstash, this need to be done before any operation.
+    LogStash::PLUGIN_REGISTRY.setup!
+    @settings.validate_all
 
     LogStash::Util::set_thread_name(self.class.name)
 
     if RUBY_VERSION < "1.9.2"
-      $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
+      logger.fatal "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
       return 1
     end
 
-    # Print a warning to STDERR for bad java versions
-    LogStash::Util::JavaVersion.warn_on_bad_java_version
+    # Exit on bad java versions
+    java_version = LogStash::Util::JavaVersion.version
+    if LogStash::Util::JavaVersion.bad_java_version?(java_version)
+      logger.fatal "Java version 1.8.0 or later is required. (You are running: #{java_version})"
+      return 1
+    end
 
-    LogStash::ShutdownWatcher.unsafe_shutdown = unsafe_shutdown?
-    LogStash::ShutdownWatcher.logger = @logger
+    LogStash::ShutdownWatcher.unsafe_shutdown = setting("pipeline.unsafe_shutdown")
 
-    configure
+    configure_plugin_paths(setting("path.plugins"))
 
     if version?
       show_version
       return 0
     end
 
-    return start_shell(@ruby_shell, binding) if @ruby_shell
+    return start_shell(setting("interactive"), binding) if setting("interactive")
+
+    @settings.format_settings.each {|line| logger.debug(line) }
 
-    if config_string.nil? && config_path.nil?
+    if setting("config.string").nil? && setting("path.config").nil?
       fail(I18n.t("logstash.runner.missing-configuration"))
     end
 
-    if @auto_reload && config_path.nil?
+    if setting("config.reload.automatic") && setting("path.config").nil?
       # there's nothing to reload
       signal_usage_error(I18n.t("logstash.runner.reload-without-config-path"))
     end
 
-    if config_test?
-      config_loader = LogStash::Config::Loader.new(@logger, config_test?)
-      config_str = config_loader.format_config(config_path, config_string)
-      config_error = LogStash::Pipeline.config_valid?(config_str)
-      if config_error == true
-        @logger.terminal "Configuration OK"
+    if setting("config.test_and_exit")
+      config_loader = LogStash::Config::Loader.new(logger)
+      config_str = config_loader.format_config(setting("path.config"), setting("config.string"))
+      begin
+        LogStash::BasePipeline.new(config_str)
+        puts "Configuration OK"
+        logger.info "Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash"
         return 0
-      else
-        @logger.fatal I18n.t("logstash.error", :error => config_error)
+      rescue => e
+        logger.fatal I18n.t("logstash.runner.invalid-configuration", :error => e.message)
         return 1
       end
     end
 
-    @agent = create_agent(:logger => @logger,
-                          :auto_reload => @auto_reload,
-                          :reload_interval => @reload_interval,
-                          :collect_metric => true,
-                          :debug => debug?,
-                          :node_name => node_name,
-                          :web_api_http_host => @web_api_http_host,
-                          :web_api_http_port => @web_api_http_port)
+    # lock path.data before starting the agent
+    @data_path_lock = FileLockFactory.getDefault().obtainLock(setting("path.data"), ".lock");
 
-    @agent.register_pipeline("main", @pipeline_settings.merge({
-                          :config_string => config_string,
-                          :config_path => config_path
-                          }))
+    @agent = create_agent(@settings)
+
+    @agent.register_pipeline(@settings)
 
     # enable sigint/sigterm before starting the agent
     # to properly handle a stalled agent
@@ -200,40 +281,52 @@ def execute
     @agent_task = Stud::Task.new { @agent.execute }
 
     # no point in enabling config reloading before the agent starts
-    sighup_id = trap_sighup()
+    # also windows doesn't have SIGHUP. we can skip it
+    sighup_id = LogStash::Environment.windows? ? nil : trap_sighup()
 
     agent_return = @agent_task.wait
 
     @agent.shutdown
 
+    # flush any outstanding log messages during shutdown
+    org.apache.logging.log4j.LogManager.shutdown
+
     agent_return
 
+  rescue org.logstash.LockException => e
+    logger.fatal(I18n.t("logstash.runner.locked-data-path", :path => setting("path.data")))
+    return 1
   rescue Clamp::UsageError => e
     $stderr.puts "ERROR: #{e.message}"
     show_short_help
     return 1
   rescue => e
-    @logger.fatal I18n.t("oops", :error => e, :backtrace => e.backtrace)
+    # if logger itself is not initialized
+    if LogStash::Logging::Logger.get_logging_context.nil?
+      $stderr.puts "#{I18n.t("oops")} :error => #{e}, :backtrace => #{e.backtrace}"
+    else
+      logger.fatal(I18n.t("oops"), :error => e, :backtrace => e.backtrace)
+    end
     return 1
   ensure
     Stud::untrap("INT", sigint_id) unless sigint_id.nil?
     Stud::untrap("TERM", sigterm_id) unless sigterm_id.nil?
     Stud::untrap("HUP", sighup_id) unless sighup_id.nil?
+    FileLockFactory.getDefault().releaseLock(@data_path_lock) if @data_path_lock
     @log_fd.close if @log_fd
   end # def self.main
 
   def show_version
     show_version_logstash
 
-    if [:info, :debug].include?(verbosity?) || debug? || verbose?
+    if logger.info?
       show_version_ruby
       show_version_java if LogStash::Environment.jruby?
-      show_gems if [:debug].include?(verbosity?) || debug?
+      show_gems if logger.debug?
     end
   end # def show_version
 
   def show_version_logstash
-    require "logstash/version"
     puts "logstash #{LOGSTASH_VERSION}"
   end # def show_version_logstash
 
@@ -254,13 +347,6 @@ def show_gems
     end
   end # def show_gems
 
-  # Do any start-time configuration.
-  #
-  # Log file stuff, plugin path checking, etc.
-  def configure
-    configure_plugin_paths(plugin_paths)
-  end # def configure
-
   # add the given paths for ungemified/bare plugins lookups
   # @param paths [String, Array<String>] plugins path string or list of path strings to add
   def configure_plugin_paths(paths)
@@ -274,54 +360,6 @@ def create_agent(*args)
     LogStash::Agent.new(*args)
   end
 
-  # Point logging at a specific path.
-  def configure_logging(path)
-    @logger = Cabin::Channel.get(LogStash)
-    # Set with the -v (or -vv...) flag
-    if quiet?
-      @logger.level = :error
-    elsif verbose?
-      @logger.level = :info
-    elsif debug?
-      @logger.level = :debug
-    else
-      # Old support for the -v and -vv stuff.
-      if verbosity? && verbosity?.any?
-        # this is an array with length of how many times the flag is given
-        if verbosity?.length == 1
-          @logger.warn("The -v flag is deprecated and will be removed in a future release. You should use --verbose instead.")
-          @logger.level = :info
-        else
-          @logger.warn("The -vv flag is deprecated and will be removed in a future release. You should use --debug instead.")
-          @logger.level = :debug
-        end
-      else
-        @logger.level = :warn
-      end
-    end
-
-    if log_file
-      # TODO(sissel): Implement file output/rotation in Cabin.
-      # TODO(sissel): Catch exceptions, report sane errors.
-      begin
-        @log_fd.close if @log_fd
-        @log_fd = File.new(path, "a")
-      rescue => e
-        fail(I18n.t("logstash.runner.configuration.log_file_failed",
-                    :path => path, :error => e))
-      end
-
-      @logger.subscribe(STDOUT, :level => :fatal)
-      @logger.subscribe(@log_fd)
-      @logger.terminal "Sending logstash logs to #{path}."
-    else
-      @logger.subscribe(STDOUT)
-    end
-
-    # TODO(sissel): redirect stdout/stderr to the log as well
-    # http://jira.codehaus.org/browse/JRUBY-7003
-  end # def configure_logging
-
   # Emit a failure message and abort.
   def fail(message)
     signal_usage_error(message)
@@ -349,14 +387,14 @@ def start_shell(shell, start_binding)
 
   def trap_sighup
     Stud::trap("HUP") do
-      @logger.warn(I18n.t("logstash.agent.sighup"))
+      logger.warn(I18n.t("logstash.agent.sighup"))
       @agent.reload_state!
     end
   end
 
   def trap_sigterm
     Stud::trap("TERM") do
-      @logger.warn(I18n.t("logstash.agent.sigterm"))
+      logger.warn(I18n.t("logstash.agent.sigterm"))
       @agent_task.stop!
     end
   end
@@ -364,15 +402,43 @@ def trap_sigterm
   def trap_sigint
     Stud::trap("INT") do
       if @interrupted_once
-        @logger.fatal(I18n.t("logstash.agent.forced_sigint"))
+        logger.fatal(I18n.t("logstash.agent.forced_sigint"))
         exit
       else
-        @logger.warn(I18n.t("logstash.agent.sigint"))
-        Thread.new(@logger) {|logger| sleep 5; logger.warn(I18n.t("logstash.agent.slow_shutdown")) }
+        logger.warn(I18n.t("logstash.agent.sigint"))
+        Thread.new(logger) {|lg| sleep 5; lg.warn(I18n.t("logstash.agent.slow_shutdown")) }
         @interrupted_once = true
         @agent_task.stop!
       end
     end
   end
 
-end # class LogStash::Runner
+  def setting(key)
+    @settings.get_value(key)
+  end
+
+  # where can I find the logstash.yml file?
+  # 1. look for a "--path.settings path"
+  # 2. look for a "--path.settings=path"
+  # 3. check if the LS_SETTINGS_DIR environment variable is set
+  # 4. return nil if not found
+  def fetch_settings_path(cli_args)
+    if i=cli_args.find_index("--path.settings")
+      cli_args[i+1]
+    elsif settings_arg = cli_args.find {|v| v.match(/--path.settings=/) }
+      match = settings_arg.match(/--path.settings=(.*)/)
+      match[1]
+    elsif ENV['LS_SETTINGS_DIR']
+      ENV['LS_SETTINGS_DIR']
+    else
+      nil
+    end
+  end
+  
+  # is the user asking for CLI help subcommand?
+  def cli_help?(args)
+    # I know, double negative
+    !(["--help", "-h"] & args).empty?
+  end
+
+end
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
new file mode 100644
index 00000000000..3d9ddcef3de
--- /dev/null
+++ b/logstash-core/lib/logstash/settings.rb
@@ -0,0 +1,535 @@
+# encoding: utf-8
+require "logstash/util/loggable"
+require "fileutils"
+require "logstash/util/byte_value"
+require "logstash/util/time_value"
+
+module LogStash
+  class Settings
+
+    def initialize
+      @settings = {}
+      # Theses settings were loaded from the yaml file
+      # but we didn't find any settings to validate them,
+      # lets keep them around until we do `validate_all` at that
+      # time universal plugins could have added new settings.
+      @transient_settings = {}
+    end
+
+    def register(setting)
+      if @settings.key?(setting.name)
+        raise ArgumentError.new("Setting \"#{setting.name}\" has already been registered as #{setting.inspect}")
+      else
+        @settings[setting.name] = setting
+      end
+    end
+
+    def get_setting(setting_name)
+      setting = @settings[setting_name]
+      raise ArgumentError.new("Setting \"#{setting_name}\" hasn't been registered") if setting.nil?
+      setting
+    end
+
+    def get_subset(setting_regexp)
+      regexp = setting_regexp.is_a?(Regexp) ? setting_regexp : Regexp.new(setting_regexp)
+      settings = self.class.new
+      @settings.each do |setting_name, setting|
+        next unless setting_name.match(regexp)
+        settings.register(setting.clone)
+      end
+      settings
+    end
+
+    def set?(setting_name)
+      get_setting(setting_name).set?
+    end
+
+    def clone
+      get_subset(".*")
+    end
+
+    def get_default(setting_name)
+      get_setting(setting_name).default
+    end
+
+    def get_value(setting_name)
+      get_setting(setting_name).value
+    end
+    alias_method :get, :get_value
+
+    def set_value(setting_name, value, graceful = false)
+      get_setting(setting_name).set(value)
+    rescue ArgumentError => e
+      if graceful
+        @transient_settings[setting_name] = value
+      else
+        raise e
+      end
+    end
+    alias_method :set, :set_value
+
+    def to_hash
+      hash = {}
+      @settings.each do |name, setting|
+        hash[name] = setting.value
+      end
+      hash
+    end
+
+    def merge(hash, graceful = false)
+      hash.each {|key, value| set_value(key, value, graceful) }
+      self
+    end
+
+    def format_settings
+      output = []
+      output << "-------- Logstash Settings (* means modified) ---------"
+      @settings.each do |setting_name, setting|
+        value = setting.value
+        default_value = setting.default
+        if default_value == value # print setting and its default value
+          output << "#{setting_name}: #{value.inspect}" unless value.nil?
+        elsif default_value.nil? # print setting and warn it has been set
+          output << "*#{setting_name}: #{value.inspect}"
+        elsif value.nil? # default setting not set by user
+          output << "#{setting_name}: #{default_value.inspect}"
+        else # print setting, warn it has been set, and show default value
+          output << "*#{setting_name}: #{value.inspect} (default: #{default_value.inspect})"
+        end
+      end
+      output << "--------------- Logstash Settings -------------------"
+      output
+    end
+
+    def reset
+      @settings.values.each(&:reset)
+    end
+
+    def from_yaml(yaml_path)
+      settings = read_yaml(::File.join(yaml_path, "logstash.yml"))
+      self.merge(flatten_hash(settings), true)
+      self
+    end
+    
+    def post_process
+      if @post_process_callbacks
+        @post_process_callbacks.each do |callback|
+          callback.call(self)
+        end
+      end
+    end
+    
+    def on_post_process(&block)
+      @post_process_callbacks ||= []
+      @post_process_callbacks << block
+    end
+
+    def validate_all
+      # lets merge the transient_settings again to see if new setting were added.
+      self.merge(@transient_settings)
+
+      @settings.each do |name, setting|
+        setting.validate_value
+      end
+    end
+
+    private
+    def read_yaml(path)
+      YAML.safe_load(IO.read(path)) || {}
+    end
+
+    def flatten_hash(h,f="",g={})
+      return g.update({ f => h }) unless h.is_a? Hash
+      if f.empty?
+        h.each { |k,r| flatten_hash(r,k,g) }
+      else
+        h.each { |k,r| flatten_hash(r,"#{f}.#{k}",g) }
+      end
+      g
+    end
+  end
+
+  class Setting
+    include LogStash::Util::Loggable
+
+    attr_reader :name, :default
+
+    def initialize(name, klass, default=nil, strict=true, &validator_proc)
+      @name = name
+      unless klass.is_a?(Class)
+        raise ArgumentError.new("Setting \"#{@name}\" must be initialized with a class (received #{klass})")
+      end
+      @klass = klass
+      @validator_proc = validator_proc
+      @value = nil
+      @value_is_set = false
+      @strict = strict
+
+      validate(default) if @strict
+      @default = default
+    end
+
+    def value
+      @value_is_set ? @value : default
+    end
+
+    def set?
+      @value_is_set
+    end
+
+    def strict?
+      @strict
+    end
+
+    def set(value)
+      validate(value) if @strict
+      @value = value
+      @value_is_set = true
+      @value
+    end
+
+    def reset
+      @value = nil
+      @value_is_set = false
+    end
+
+    def to_hash
+      {
+        "name" => @name,
+        "klass" => @klass,
+        "value" => @value,
+        "value_is_set" => @value_is_set,
+        "default" => @default,
+        # Proc#== will only return true if it's the same obj
+        # so no there's no point in comparing it
+        # also thereÅ› no use case atm to return the proc
+        # so let's not expose it
+        #"validator_proc" => @validator_proc
+      }
+    end
+
+    def ==(other)
+      self.to_hash == other.to_hash
+    end
+
+    def validate_value
+      validate(value)
+    end
+
+    protected
+    def validate(input)
+      if !input.is_a?(@klass)
+        raise ArgumentError.new("Setting \"#{@name}\" must be a #{@klass}. Received: #{input} (#{input.class})")
+      end
+
+      if @validator_proc && !@validator_proc.call(input)
+        raise ArgumentError.new("Failed to validate setting \"#{@name}\" with value: #{input}")
+      end
+    end
+
+    class Coercible < Setting
+      def initialize(name, klass, default=nil, strict=true, &validator_proc)
+        @name = name
+        unless klass.is_a?(Class)
+          raise ArgumentError.new("Setting \"#{@name}\" must be initialized with a class (received #{klass})")
+        end
+        @klass = klass
+        @validator_proc = validator_proc
+        @value = nil
+        @value_is_set = false
+
+        if strict
+          coerced_default = coerce(default)
+          validate(coerced_default)
+          @default = coerced_default
+        else
+          @default = default
+        end
+      end
+      def set(value)
+        coerced_value = coerce(value)
+        validate(coerced_value)
+        @value = coerce(coerced_value)
+        @value_is_set = true
+        @value
+      end
+
+      def coerce(value)
+        raise NotImplementedError.new("Please implement #coerce for #{self.class}")
+      end
+    end
+    ### Specific settings #####
+
+    class Boolean < Coercible
+      def initialize(name, default, strict=true, &validator_proc)
+        super(name, Object, default, strict, &validator_proc)
+      end
+
+      def coerce(value)
+        case value
+        when TrueClass, "true"
+          true
+        when FalseClass, "false"
+          false
+        else
+          raise ArgumentError.new("could not coerce #{value} into a boolean")
+        end
+      end
+    end
+
+    class Numeric < Coercible
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Numeric, default, strict)
+      end
+
+      def coerce(v)
+        return v if v.is_a?(::Numeric)
+
+        # I hate these "exceptions as control flow" idioms
+        # but Ruby's `"a".to_i => 0` makes it hard to do anything else.
+        coerced_value = (Integer(v) rescue nil) || (Float(v) rescue nil)
+
+        if coerced_value.nil?
+          raise ArgumentError.new("Failed to coerce value to Numeric. Received #{v} (#{v.class})")
+        else
+          coerced_value
+        end
+      end
+    end
+
+    class Integer < Coercible
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Integer, default, strict)
+      end
+
+      def coerce(value)
+        return value unless value.is_a?(::String)
+
+        coerced_value = Integer(value) rescue nil
+
+        if coerced_value.nil?
+          raise ArgumentError.new("Failed to coerce value to Integer. Received #{value} (#{value.class})")
+        else
+          coerced_value
+        end
+      end
+    end
+
+    class PositiveInteger < Integer
+      def initialize(name, default=nil, strict=true)
+        super(name, default, strict) do |v|
+          if v > 0
+            true
+          else
+            raise ArgumentError.new("Number must be bigger than 0. Received: #{v}")
+          end
+        end
+      end
+    end
+
+    class Port < Integer
+      VALID_PORT_RANGE = 1..65535
+
+      def initialize(name, default=nil, strict=true)
+        super(name, default, strict) { |value| valid?(value) }
+      end
+
+      def valid?(port)
+        VALID_PORT_RANGE.cover?(port)
+      end
+    end
+
+    class PortRange < Coercible
+      PORT_SEPARATOR = "-"
+
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Range, default, strict=true) { |value| valid?(value) }
+      end
+
+      def valid?(range)
+        Port::VALID_PORT_RANGE.first <= range.first && Port::VALID_PORT_RANGE.last >= range.last
+      end
+
+      def coerce(value)
+        case value
+        when ::Range
+          value
+        when ::Fixnum
+          value..value
+        when ::String
+          first, last = value.split(PORT_SEPARATOR)
+          last = first if last.nil?
+          begin
+            (Integer(first))..(Integer(last))
+          rescue ArgumentError # Trap and reraise a more human error
+            raise ArgumentError.new("Could not coerce #{value} into a port range")
+          end
+        else
+          raise ArgumentError.new("Could not coerce #{value} into a port range")
+        end
+      end
+
+      def validate(value)
+        unless valid?(value)
+          raise ArgumentError.new("Invalid value \"#{value}, valid options are within the range of #{Port::VALID_PORT_RANGE.first}-#{Port::VALID_PORT_RANGE.last}")
+        end
+      end
+    end
+
+    class Validator < Setting
+      def initialize(name, default=nil, strict=true, validator_class=nil)
+        @validator_class = validator_class
+        super(name, ::Object, default, strict)
+      end
+
+      def validate(value)
+        @validator_class.validate(value)
+      end
+    end
+
+    class String < Setting
+      def initialize(name, default=nil, strict=true, possible_strings=[])
+        @possible_strings = possible_strings
+        super(name, ::String, default, strict)
+      end
+
+      def validate(value)
+        super(value)
+        unless @possible_strings.empty? || @possible_strings.include?(value)
+          raise ArgumentError.new("Invalid value \"#{value}\". Options are: #{@possible_strings.inspect}")
+        end
+      end
+    end
+
+    class NullableString < String
+      def validate(value)
+        return if value.nil?
+        super(value)
+      end
+    end
+
+    class ExistingFilePath < Setting
+      def initialize(name, default=nil, strict=true)
+        super(name, ::String, default, strict) do |file_path|
+          if !::File.exists?(file_path)
+            raise ::ArgumentError.new("File \"#{file_path}\" must exist but was not found.")
+          else
+            true
+          end
+        end
+      end
+    end
+
+    class WritableDirectory < Setting
+      def initialize(name, default=nil, strict=false)
+        super(name, ::String, default, strict)
+      end
+      
+      def validate(path)
+        super(path)
+
+        if ::File.directory?(path)
+          if !::File.writable?(path)
+            raise ::ArgumentError.new("Path \"#{path}\" must be a writable directory. It is not writable.")
+          end
+        elsif ::File.symlink?(path)
+          # TODO(sissel): I'm OK if we relax this restriction. My experience
+          # is that it's usually easier and safer to just reject symlinks.
+          raise ::ArgumentError.new("Path \"#{path}\" must be a writable directory. It cannot be a symlink.")
+        elsif ::File.exist?(path)
+          raise ::ArgumentError.new("Path \"#{path}\" must be a writable directory. It is not a directory.")
+        else
+          parent = ::File.dirname(path)
+          if !::File.writable?(parent)
+            raise ::ArgumentError.new("Path \"#{path}\" does not exist and I cannot create it because the parent path \"#{parent}\" is not writable.")
+          end
+        end
+
+        # If we get here, the directory exists and is writable.
+        true
+      end
+
+      def value
+        super.tap do |path|
+          if !::File.directory?(path)
+            # Create the directory if it doesn't exist.
+            begin
+              logger.info("Creating directory", setting: name, path: path)
+              ::FileUtils.mkdir_p(path)
+            rescue => e
+              # TODO(sissel): Catch only specific exceptions?
+              raise ::ArgumentError.new("Path \"#{path}\" does not exist, and I failed trying to create it: #{e.class.name} - #{e}")
+            end
+          end
+        end
+      end
+    end
+
+    class Bytes < Coercible
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Fixnum, default, strict=true) { |value| valid?(value) }
+      end
+
+      def valid?(value)
+        value.is_a?(Fixnum) && value >= 0
+      end
+
+      def coerce(value)
+        case value
+        when ::Numeric
+          value
+        when ::String
+          LogStash::Util::ByteValue.parse(value)
+        else
+          raise ArgumentError.new("Could not coerce '#{value}' into a bytes value")
+        end
+      end
+
+      def validate(value)
+        unless valid?(value)
+          raise ArgumentError.new("Invalid byte value \"#{value}\".")
+        end
+      end
+    end
+
+    class TimeValue < Coercible
+      def initialize(name, default, strict=true, &validator_proc)
+        super(name, ::Fixnum, default, strict, &validator_proc)
+      end
+
+      def coerce(value)
+        return value if value.is_a?(::Fixnum)
+        Util::TimeValue.from_value(value).to_nanos
+      end
+    end
+
+    class ArrayCoercible < Coercible
+      def initialize(name, klass, default, strict=true, &validator_proc)
+        @element_class = klass
+        super(name, ::Array, default, strict, &validator_proc)
+      end
+
+      def coerce(value)
+        Array(value)
+      end
+
+      protected
+      def validate(input)
+        if !input.is_a?(@klass)
+          raise ArgumentError.new("Setting \"#{@name}\" must be a #{@klass}. Received: #{input} (#{input.class})")
+        end
+
+        unless input.all? {|el| el.kind_of?(@element_class) }
+          raise ArgumentError.new("Values of setting \"#{@name}\" must be #{@element_class}. Received: #{input.map(&:class)}")
+        end
+
+        if @validator_proc && !@validator_proc.call(input)
+          raise ArgumentError.new("Failed to validate setting \"#{@name}\" with value: #{input}")
+        end
+      end
+    end
+  end
+
+
+  SETTINGS = Settings.new
+end
diff --git a/logstash-core/lib/logstash/shutdown_watcher.rb b/logstash-core/lib/logstash/shutdown_watcher.rb
index fa0d1f01fd4..10de81db1b6 100644
--- a/logstash-core/lib/logstash/shutdown_watcher.rb
+++ b/logstash-core/lib/logstash/shutdown_watcher.rb
@@ -2,6 +2,7 @@
 
 module LogStash
   class ShutdownWatcher
+    include LogStash::Util::Loggable
 
     CHECK_EVERY = 1 # second
     REPORT_EVERY = 5 # checks
@@ -25,14 +26,6 @@ def self.unsafe_shutdown?
       @unsafe_shutdown
     end
 
-    def self.logger=(logger)
-      @logger = logger
-    end
-
-    def self.logger
-      @logger ||= Cabin::Channel.get(LogStash)
-    end
-
     def self.start(pipeline, cycle_period=CHECK_EVERY, report_every=REPORT_EVERY, abort_threshold=ABORT_AFTER)
       controller = self.new(pipeline, cycle_period, report_every, abort_threshold)
       Thread.new(controller) { |controller| controller.start }
@@ -51,7 +44,7 @@ def start
         @reports << pipeline_report_snapshot
         @reports.delete_at(0) if @reports.size > @report_every # expire old report
         if cycle_number == (@report_every - 1) # it's report time!
-          logger.warn(@reports.last)
+          logger.warn(@reports.last.to_s)
 
           if shutdown_stalled?
             logger.error("The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.") if stalled_count == 0
diff --git a/logstash-core-event-java/lib/logstash/string_interpolation.rb b/logstash-core/lib/logstash/string_interpolation.rb
similarity index 67%
rename from logstash-core-event-java/lib/logstash/string_interpolation.rb
rename to logstash-core/lib/logstash/string_interpolation.rb
index 7baf091f304..2eef6dfdff8 100644
--- a/logstash-core-event-java/lib/logstash/string_interpolation.rb
+++ b/logstash-core/lib/logstash/string_interpolation.rb
@@ -6,12 +6,12 @@ module StringInterpolation
 
     # clear the global compiled templates cache
     def clear_cache
-      Java::ComLogstash::StringInterpolation.get_instance.clear_cache;
+      Java::OrgLogstash::StringInterpolation.get_instance.clear_cache;
     end
 
     # @return [Fixnum] the compiled templates cache size
     def cache_size
-      Java::ComLogstash::StringInterpolation.get_instance.cache_size;
+      Java::OrgLogstash::StringInterpolation.get_instance.cache_size;
     end
   end
 end
diff --git a/logstash-core-event-java/lib/logstash/timestamp.rb b/logstash-core/lib/logstash/timestamp.rb
similarity index 94%
rename from logstash-core-event-java/lib/logstash/timestamp.rb
rename to logstash-core/lib/logstash/timestamp.rb
index 0a4661a2d19..1aefb23b4e0 100644
--- a/logstash-core-event-java/lib/logstash/timestamp.rb
+++ b/logstash-core/lib/logstash/timestamp.rb
@@ -1,7 +1,6 @@
 # encoding: utf-8
 
 require "logstash/namespace"
-require "logstash-core-event"
 
 module LogStash
   class TimestampParserError < StandardError; end
diff --git a/logstash-core/lib/logstash/universal_plugin.rb b/logstash-core/lib/logstash/universal_plugin.rb
new file mode 100644
index 00000000000..3891bdc14d5
--- /dev/null
+++ b/logstash-core/lib/logstash/universal_plugin.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+module LogStash
+  class UniversalPlugin
+    def initialize
+    end
+
+    def register_hooks(hookManager)
+    end
+
+    def additionals_settings(settings)
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/util.rb b/logstash-core/lib/logstash/util.rb
index 88f8b999200..0f7ea7817f0 100644
--- a/logstash-core/lib/logstash/util.rb
+++ b/logstash-core/lib/logstash/util.rb
@@ -141,7 +141,7 @@ def self.hash_merge_many(*hashes)
   end # def hash_merge_many
 
 
-  # nomalize method definition based on platform.
+  # normalize method definition based on platform.
   # normalize is used to convert an object create through
   # json deserialization from JrJackson in :raw mode to pure Ruby
   # to support these pure Ruby object monkey patches.
diff --git a/logstash-core/lib/logstash/util/byte_value.rb b/logstash-core/lib/logstash/util/byte_value.rb
new file mode 100644
index 00000000000..36a8c6c83c6
--- /dev/null
+++ b/logstash-core/lib/logstash/util/byte_value.rb
@@ -0,0 +1,60 @@
+# encoding: utf-8
+require "logstash/namespace"
+
+module LogStash; module Util; module ByteValue
+  module_function
+
+  B = 1
+  KB = B << 10
+  MB = B << 20
+  GB = B << 30
+  TB = B << 40
+  PB = B << 50
+
+  def parse(text)
+    if !text.is_a?(String)
+      raise ArgumentError, "ByteValue::parse takes a String, got a `#{text.class.name}`"
+    end
+    number = text.to_f
+    factor = multiplier(text)
+
+    (number * factor).to_i
+  end
+
+  def multiplier(text)
+    case text
+      when /(?:k|kb)$/ 
+        KB
+      when /(?:m|mb)$/
+        MB
+      when /(?:g|gb)$/
+        GB
+      when /(?:t|tb)$/
+        TB
+      when /(?:p|pb)$/
+        PB
+      when /(?:b)$/
+        B
+      else 
+        raise ArgumentError, "Unknown bytes value '#{text}'"
+    end
+  end
+
+  def human_readable(number)
+    value, unit = if number > PB
+      [number / PB, "pb"]
+    elsif number > TB
+      [number / TB, "tb"]
+    elsif number > GB
+      [number / GB, "gb"]
+    elsif number > MB
+      [number / MB, "mb"]
+    elsif number > KB
+      [number / KB, "kb"]
+    else
+      [number, "b"]
+    end
+
+    format("%.2d%s", value, unit)
+  end
+end end end
diff --git a/logstash-core/lib/logstash/util/decorators.rb b/logstash-core/lib/logstash/util/decorators.rb
index 265656e5ce9..f5e4ac5dabd 100644
--- a/logstash-core/lib/logstash/util/decorators.rb
+++ b/logstash-core/lib/logstash/util/decorators.rb
@@ -6,9 +6,8 @@ module LogStash::Util
 
   # Decorators provides common manipulation on the event data.
   module Decorators
+    include LogStash::Util::Loggable
     extend self
-    
-    @logger = Cabin::Channel.get(LogStash)
 
     # fields is a hash of field => value
     # where both `field` and `value` can use sprintf syntax.
@@ -22,29 +21,33 @@ def add_fields(fields,event, pluginname)
             # note below that the array field needs to be updated then reassigned to the event.
             # this is important because a construct like event[field] << v will not work
             # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
-            a = Array(event[field])
+            a = Array(event.get(field))
             a << v
-            event[field] = a
+            event.set(field, a)
           else
-            event[field] = v
+            event.set(field, v)
           end
-          @logger.debug? and @logger.debug("#{pluginname}: adding value to field", :field => field, :value => value)
+          self.logger.debug? and self.logger.debug("#{pluginname}: adding value to field", "field" => field, "value" => value)
         end
       end
     end
 
     # tags is an array of string. sprintf syntax can be used.
-    def add_tags(tags, event, pluginname)
-      tags.each do |tag|
-        tag = event.sprintf(tag)
-        @logger.debug? and @logger.debug("#{pluginname}: adding tag", :tag => tag)
+    def add_tags(new_tags, event, pluginname)
+      return if new_tags.empty?
+
+      tags = Array(event.get("tags")) # note that Array(nil) => []
+
+      new_tags.each do |new_tag|
+        new_tag = event.sprintf(new_tag)
+        self.logger.debug? and self.logger.debug("#{pluginname}: adding tag", "tag" => new_tag)
         # note below that the tags array field needs to be updated then reassigned to the event.
         # this is important because a construct like event["tags"] << tag will not work
         # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
-        tags = event["tags"] || []
-        tags << tag
-        event["tags"] = tags
+        tags << new_tag  #unless tags.include?(new_tag)
       end
+
+      event.set("tags", tags)
     end
 
   end # module LogStash::Util::Decorators
diff --git a/logstash-core/lib/logstash/util/defaults_printer.rb b/logstash-core/lib/logstash/util/defaults_printer.rb
deleted file mode 100644
index 6dd850e1d50..00000000000
--- a/logstash-core/lib/logstash/util/defaults_printer.rb
+++ /dev/null
@@ -1,31 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/util"
-require "logstash/util/worker_threads_default_printer"
-
-
-# This class exists to format the settings for defaults used
-module LogStash module Util class DefaultsPrinter
-  def self.print(settings)
-    new(settings).print
-  end
-
-  def initialize(settings)
-    @settings = settings
-    @printers = [workers]
-  end
-
-  def print
-    collector = []
-    @printers.each do |printer|
-      printer.visit(collector)
-    end
-    "Settings: " + collector.join(', ')
-  end
-
-  private
-
-  def workers
-    WorkerThreadsDefaultPrinter.new(@settings)
-  end
-end end end
diff --git a/logstash-core/lib/logstash/util/environment_variables.rb b/logstash-core/lib/logstash/util/environment_variables.rb
new file mode 100644
index 00000000000..2b61b539a7a
--- /dev/null
+++ b/logstash-core/lib/logstash/util/environment_variables.rb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+module ::LogStash::Util::EnvironmentVariables
+
+  ENV_PLACEHOLDER_REGEX = /\${(?<name>[a-zA-Z_.][a-zA-Z0-9_.]*)(:(?<default>[^}]*))?}/
+
+  # Recursive method to replace environment variable references in parameters
+  def deep_replace(value)
+    if value.is_a?(Hash)
+      value.each do |valueHashKey, valueHashValue|
+        value[valueHashKey.to_s] = deep_replace(valueHashValue)
+      end
+    else
+      if value.is_a?(Array)
+        value.each_index do | valueArrayIndex|
+          value[valueArrayIndex] = deep_replace(value[valueArrayIndex])
+        end
+      else
+        return replace_env_placeholders(value)
+      end
+    end
+  end
+
+  # Replace all environment variable references in 'value' param by environment variable value and return updated value
+  # Process following patterns : $VAR, ${VAR}, ${VAR:defaultValue}
+  def replace_env_placeholders(value)
+    return value unless value.is_a?(String)
+
+    value.gsub(ENV_PLACEHOLDER_REGEX) do |placeholder|
+      # Note: Ruby docs claim[1] Regexp.last_match is thread-local and scoped to
+      # the call, so this should be thread-safe.
+      #
+      # [1] http://ruby-doc.org/core-2.1.1/Regexp.html#method-c-last_match
+      name = Regexp.last_match(:name)
+      default = Regexp.last_match(:default)
+
+      replacement = ENV.fetch(name, default)
+      if replacement.nil?
+        raise LogStash::ConfigurationError, "Cannot evaluate `#{placeholder}`. Environment variable `#{name}` is not set and there is no default value given."
+      end
+      replacement
+    end
+  end # def replace_env_placeholders
+end
diff --git a/logstash-core/lib/logstash/util/java_version.rb b/logstash-core/lib/logstash/util/java_version.rb
index 71225b30a0a..f73d09de10d 100644
--- a/logstash-core/lib/logstash/util/java_version.rb
+++ b/logstash-core/lib/logstash/util/java_version.rb
@@ -1,20 +1,6 @@
 # encoding: utf-8
-require 'cabin'
 
 module LogStash::Util::JavaVersion
-  def self.logger
-    @logger ||= Cabin::Channel.get(LogStash)
-  end
-
-  # Print a warning if we're on a bad version of java
-  def self.warn_on_bad_java_version
-    if self.bad_java_version?(self.version)
-      msg = "!!! Please upgrade your java version, the current version '#{self.version}' is not supported. We recommend a minimum version of Java 8"
-      STDERR.puts(msg)
-      logger.warn(msg)
-    end
-  end
-
   # Return the current java version string. Returns nil if this is a non-java platform (e.g. MRI).
   def self.version
     return nil unless LogStash::Environment.jruby?
diff --git a/logstash-core/lib/logstash/util/loggable.rb b/logstash-core/lib/logstash/util/loggable.rb
index 0add9f3e2b0..25e24cfcb69 100644
--- a/logstash-core/lib/logstash/util/loggable.rb
+++ b/logstash-core/lib/logstash/util/loggable.rb
@@ -1,29 +1,31 @@
 # encoding: utf-8
+require "logstash/logging/logger"
 require "logstash/namespace"
-require "cabin"
 
 module LogStash module Util
   module Loggable
-    class << self
-      def logger=(new_logger)
-        @logger = new_logger
+    def self.included(klass)
+
+      def klass.log4j_name
+        ruby_name = self.name || self.class.name || self.class.to_s
+        ruby_name.gsub('::', '.').downcase
       end
 
-      def logger
-        @logger ||= Cabin::Channel.get(LogStash)
+      def klass.logger
+        @logger ||= LogStash::Logging::Logger.new(log4j_name)
       end
-    end
 
-    def self.included(base)
-      class << base
-        def logger
-          Loggable.logger
-        end
+      def klass.slow_logger(warn_threshold, info_threshold, debug_threshold, trace_threshold)
+        @slow_logger ||= LogStash::Logging::SlowLogger.new(log4j_name, warn_threshold, info_threshold, debug_threshold, trace_threshold)
       end
-    end
 
-    def logger
-      Loggable.logger
+      def logger
+        self.class.logger
+      end
+
+      def slow_logger(warn_threshold, info_threshold, debug_threshold, trace_threshold)
+        self.class.slow_logger(warn_threshold, info_threshold, debug_threshold, trace_threshold)
+      end
     end
   end
 end; end
diff --git a/logstash-core/lib/logstash/util/prctl.rb b/logstash-core/lib/logstash/util/prctl.rb
index 8d8bb5c7826..ffb57201452 100644
--- a/logstash-core/lib/logstash/util/prctl.rb
+++ b/logstash-core/lib/logstash/util/prctl.rb
@@ -4,7 +4,7 @@ module LibC
   extend FFI::Library
   ffi_lib 'c'
 
-  # Ok so the 2nd arg isn't really a string... but whaatever
+  # Ok so the 2nd arg isn't really a string... but whatever
   attach_function :prctl, [:int, :string, :long, :long, :long], :int
 end
 
diff --git a/logstash-core/lib/logstash/util/retryable.rb b/logstash-core/lib/logstash/util/retryable.rb
index 04df5ce8c4a..1a932dd48c4 100644
--- a/logstash-core/lib/logstash/util/retryable.rb
+++ b/logstash-core/lib/logstash/util/retryable.rb
@@ -3,7 +3,7 @@ module LogStash
   module Retryable
     # execute retryable code block
     # @param [Hash] options retryable options
-    # @option options [Fixnum] :tries retries to perform, default 1, set to 0 for infite retries. 1 means that upon exception the block will be retried once
+    # @option options [Fixnum] :tries retries to perform, default 1, set to 0 for infinite retries. 1 means that upon exception the block will be retried once
     # @option options [Fixnum] :base_sleep seconds to sleep on first retry, default 1
     # @option options [Fixnum] :max_sleep max seconds to sleep upon exponential backoff, default 1
     # @option options [Exception] :rescue exception class list to retry on, defaults is Exception, which retries on any Exception.
diff --git a/logstash-core/lib/logstash/util/safe_uri.rb b/logstash-core/lib/logstash/util/safe_uri.rb
new file mode 100644
index 00000000000..76b50d27e9a
--- /dev/null
+++ b/logstash-core/lib/logstash/util/safe_uri.rb
@@ -0,0 +1,56 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+require "forwardable"
+
+# This class exists to quietly wrap a password string so that, when printed or
+# logged, you don't accidentally print the password itself.
+class LogStash::Util::SafeURI
+  PASS_PLACEHOLDER = "xxxxxx".freeze
+  HOSTNAME_PORT_REGEX=/\A(?<hostname>([A-Za-z0-9\.\-]+)|\[[0-9A-Fa-f\:]+\])(:(?<port>\d+))?\Z/
+  
+  extend Forwardable
+  
+  def_delegators :@uri, :coerce, :query=, :route_from, :port=, :default_port, :select, :normalize!, :absolute?, :registry=, :path, :password, :hostname, :merge, :normalize, :host, :component_ary, :userinfo=, :query, :set_opaque, :+, :merge!, :-, :password=, :parser, :port, :set_host, :set_path, :opaque=, :scheme, :fragment=, :set_query, :set_fragment, :userinfo, :hostname=, :set_port, :path=, :registry, :opaque, :route_to, :set_password, :hierarchical?, :set_user, :set_registry, :set_userinfo, :fragment, :component, :user=, :set_scheme, :absolute, :host=, :relative?, :scheme=, :user
+  
+  attr_reader :uri
+  
+  public
+  def initialize(arg)    
+    @uri = case arg
+           when String
+             arg = "//#{arg}" if HOSTNAME_PORT_REGEX.match(arg)
+             URI.parse(arg)
+           when URI
+             arg
+           else
+             raise ArgumentError, "Expected a string or URI, got a #{arg.class} creating a URL"
+           end
+  end
+
+  def to_s
+    sanitized.to_s
+  end
+
+  def inspect
+    sanitized.to_s
+  end
+
+  def sanitized
+    return uri unless uri.password # nothing to sanitize here!
+    
+    safe = uri.clone
+    safe.password = PASS_PLACEHOLDER
+    safe
+  end
+
+  def ==(other)
+    other.is_a?(::LogStash::Util::SafeURI) ? @uri == other.uri : false
+  end
+
+  def clone
+    cloned_uri = uri.clone
+    self.class.new(cloned_uri)
+  end
+end
+
diff --git a/logstash-core/lib/logstash/util/thread_dump.rb b/logstash-core/lib/logstash/util/thread_dump.rb
new file mode 100644
index 00000000000..11d1a8da066
--- /dev/null
+++ b/logstash-core/lib/logstash/util/thread_dump.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+module LogStash
+  module Util
+    class ThreadDump
+      SKIPPED_THREADS             = [ "Finalizer", "Reference Handler", "Signal Dispatcher" ].freeze
+      THREADS_COUNT_DEFAULT       = 3.freeze
+      IGNORE_IDLE_THREADS_DEFAULT = true.freeze
+
+      attr_reader :top_count, :ignore, :dump
+
+      def initialize(options={})
+        @options   = options
+        @dump = options.fetch(:dump, JRMonitor.threads.generate({}))
+        @top_count = options.fetch(:threads, THREADS_COUNT_DEFAULT)
+        @ignore    = options.fetch(:ignore_idle_threads, IGNORE_IDLE_THREADS_DEFAULT)
+      end
+
+      def each(&block)
+        i=0
+        dump.each_pair do |thread_name, _hash|
+          break if i >= top_count
+          if ignore
+            next if idle_thread?(thread_name, _hash)
+          end
+          block.call(thread_name, _hash)
+          i += 1
+        end
+      end
+
+      def idle_thread?(thread_name, data)
+        idle = false
+        if SKIPPED_THREADS.include?(thread_name)
+          # these are likely JVM dependent
+          idle = true
+        elsif thread_name.match(/Ruby-\d+-JIT-\d+/)
+          # This are internal JRuby JIT threads, 
+          # see java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor for details.
+          idle = true
+        elsif thread_name.match(/pool-\d+-thread-\d+/)
+          # This are threads used by the internal JRuby implementation to dispatch
+          # calls and tasks, see prg.jruby.internal.runtime.methods.DynamicMethod.call
+          idle = true
+        else
+          data["thread.stacktrace"].each do |trace|
+            if trace.start_with?("java.util.concurrent.ThreadPoolExecutor.getTask")
+              idle = true
+              break
+            end
+          end
+        end
+        idle
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/util/time_value.rb b/logstash-core/lib/logstash/util/time_value.rb
new file mode 100644
index 00000000000..63e19ce0264
--- /dev/null
+++ b/logstash-core/lib/logstash/util/time_value.rb
@@ -0,0 +1,70 @@
+module LogStash
+  module Util
+    class TimeValue
+      def initialize(duration, time_unit)
+        @duration = duration
+        @time_unit = time_unit
+      end
+
+      def self.from_value(value)
+        if value.is_a?(TimeValue)
+          TimeValue.new(value.duration, value.time_unit)
+        elsif value.is_a?(::String)
+          normalized = value.downcase.strip
+          if normalized.end_with?("nanos")
+            TimeValue.new(parse(normalized, 5), :nanosecond)
+          elsif normalized.end_with?("micros")
+            TimeValue.new(parse(normalized, 6), :microsecond)
+          elsif normalized.end_with?("ms")
+            TimeValue.new(parse(normalized, 2), :millisecond)
+          elsif normalized.end_with?("s")
+            TimeValue.new(parse(normalized, 1), :second)
+          elsif normalized.end_with?("m")
+            TimeValue.new(parse(normalized, 1), :minute)
+          elsif normalized.end_with?("h")
+            TimeValue.new(parse(normalized, 1), :hour)
+          elsif normalized.end_with?("d")
+            TimeValue.new(parse(normalized, 1), :day)
+          elsif normalized =~ /^-0*1/
+            TimeValue.new(-1, :nanosecond)
+          else
+            raise ArgumentError.new("invalid time unit: \"#{value}\"")
+          end
+        else
+          raise ArgumentError.new("value is not a string: #{value} [#{value.class}]")
+        end
+      end
+
+      def to_nanos
+        case @time_unit
+        when :day
+          86400000000000 * @duration
+        when :hour
+          3600000000000 * @duration
+        when :minute
+          60000000000 * @duration
+        when :second
+          1000000000 * @duration
+        when :millisecond
+          1000000 * @duration
+        when :microsecond
+          1000 * @duration
+        when :nanosecond
+          @duration
+        end
+      end
+
+      def ==(other)
+        self.duration == other.duration and self.time_unit == other.time_unit
+      end
+
+      def self.parse(value, suffix)
+        Integer(value[0..(value.size - suffix - 1)].strip)
+      end
+
+      private_class_method :parse
+      attr_reader :duration
+      attr_reader :time_unit
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/util/worker_threads_default_printer.rb b/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
index 43869162865..b35058ac24e 100644
--- a/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
+++ b/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
@@ -6,8 +6,8 @@
 module LogStash module Util class WorkerThreadsDefaultPrinter
 
   def initialize(settings)
-    @setting = settings.fetch(:pipeline_workers, 0)
-    @default = settings.fetch(:default_pipeline_workers, 0)
+    @setting = settings.fetch('pipeline.workers', 0)
+    @default = settings.fetch('default-pipeline-workers', 0)
   end
 
   def visit(collector)
diff --git a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
new file mode 100644
index 00000000000..9d34869a8dc
--- /dev/null
+++ b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
@@ -0,0 +1,389 @@
+# encoding: utf-8
+
+require "jruby_acked_queue_ext"
+require "jruby_acked_batch_ext"
+require "concurrent"
+# This is an adapted copy of the wrapped_synchronous_queue file
+# ideally this should be moved to Java/JRuby
+
+module LogStash; module Util
+  # Some specialized constructors. The calling code *does* need to know what kind it creates but
+  # not the internal implementation e.g. LogStash::AckedMemoryQueue etc.
+  # Note the use of allocate - this is what new does before it calls initialize.
+  # Note that the new method has been made private this is because there is no
+  # default queue implementation.
+  # It would be expensive to create a persistent queue in the new method
+  # to then throw it away in favor of a memory based one directly after.
+  # Especially in terms of (mmap) memory allocation and proper close sequencing.
+
+  class WrappedAckedQueue
+    class QueueClosedError < ::StandardError; end
+    class NotImplementedError < ::StandardError; end
+
+    def self.create_memory_based(path, capacity, max_events, max_bytes)
+      self.allocate.with_queue(
+        LogStash::AckedMemoryQueue.new(path, capacity, max_events, max_bytes)
+      )
+    end
+
+    def self.create_file_based(path, capacity, max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, max_bytes)
+      self.allocate.with_queue(
+        LogStash::AckedQueue.new(path, capacity, max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, max_bytes)
+      )
+    end
+
+    private_class_method :new
+
+    attr_reader :queue
+
+    def with_queue(queue)
+      @queue = queue
+      @queue.open
+      @closed = Concurrent::AtomicBoolean.new(false)
+      self
+    end
+
+    def closed?
+      @closed.true?
+    end
+
+    # Push an object to the queue if the queue is full
+    # it will block until the object can be added to the queue.
+    #
+    # @param [Object] Object to add to the queue
+    def push(obj)
+      check_closed("write")
+      @queue.write(obj)
+    end
+    alias_method(:<<, :push)
+
+    # TODO - fix doc for this noop method
+    # Offer an object to the queue, wait for the specified amount of time.
+    # If adding to the queue was successful it will return true, false otherwise.
+    #
+    # @param [Object] Object to add to the queue
+    # @param [Integer] Time in milliseconds to wait before giving up
+    # @return [Boolean] True if adding was successful if not it return false
+    def offer(obj, timeout_ms)
+      raise NotImplementedError.new("The offer method is not implemented. There is no non blocking write operation yet.")
+    end
+
+    # Blocking
+    def take
+      check_closed("read a batch")
+      # TODO - determine better arbitrary timeout millis
+      @queue.read_batch(1, 200).get_elements.first
+    end
+
+    # Block for X millis
+    def poll(millis)
+      check_closed("read")
+      @queue.read_batch(1, millis).get_elements.first
+    end
+
+    def read_batch(size, wait)
+      check_closed("read a batch")
+      @queue.read_batch(size, wait)
+    end
+
+    def write_client
+      WriteClient.new(self)
+    end
+
+    def read_client()
+      ReadClient.new(self)
+    end
+
+    def check_closed(action)
+      if closed?
+        raise QueueClosedError.new("Attempted to #{action} on a closed AckedQueue")
+      end
+    end
+
+    def close
+      @queue.close
+      @closed.make_true
+    end
+
+    class ReadClient
+      # We generally only want one thread at a time able to access pop/take/poll operations
+      # from this queue. We also depend on this to be able to block consumers while we snapshot
+      # in-flight buffers
+
+      def initialize(queue, batch_size = 125, wait_for = 250)
+        @queue = queue
+        @mutex = Mutex.new
+        # Note that @inflight_batches as a central mechanism for tracking inflight
+        # batches will fail if we have multiple read clients in the pipeline.
+        @inflight_batches = {}
+        # allow the worker thread to report the execution time of the filter + output
+        @inflight_clocks = {}
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def close
+        @queue.close
+      end
+
+      def empty?
+        @mutex.synchronize { @queue.is_fully_acked? }
+      end
+
+      def set_batch_dimensions(batch_size, wait_for)
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def set_events_metric(metric)
+        @event_metric = metric
+        define_initial_metrics_values(@event_metric)
+      end
+
+      def set_pipeline_metric(metric)
+        @pipeline_metric = metric
+        define_initial_metrics_values(@pipeline_metric)
+      end
+
+      def define_initial_metrics_values(namespaced_metric)
+        namespaced_metric.report_time(:duration_in_millis, 0)
+        namespaced_metric.increment(:filtered, 0)
+        namespaced_metric.increment(:out, 0)
+      end
+
+      def inflight_batches
+        @mutex.synchronize do
+          yield(@inflight_batches)
+        end
+      end
+
+      def current_inflight_batch
+        @inflight_batches.fetch(Thread.current, [])
+      end
+
+      # create a new empty batch
+      # @return [ReadBatch] a new empty read batch
+      def new_batch
+        ReadBatch.new(@queue, @batch_size, @wait_for)
+      end
+
+      def read_batch
+        if @queue.closed?
+          raise QueueClosedError.new("Attempt to take a batch from a closed AckedQueue")
+        end
+
+        batch = new_batch
+        @mutex.synchronize { batch.read_next }
+        start_metrics(batch)
+        batch
+      end
+
+      def start_metrics(batch)
+        @mutex.synchronize do
+          # there seems to be concurrency issues with metrics, keep it in the mutex
+          set_current_thread_inflight_batch(batch)
+          start_clock
+        end
+      end
+
+      def set_current_thread_inflight_batch(batch)
+        @inflight_batches[Thread.current] = batch
+      end
+
+      def close_batch(batch)
+        @mutex.synchronize do
+          batch.close
+
+          # there seems to be concurrency issues with metrics, keep it in the mutex
+          @inflight_batches.delete(Thread.current)
+          stop_clock(batch)
+        end
+      end
+
+      def start_clock
+        @inflight_clocks[Thread.current] = [
+          @event_metric.time(:duration_in_millis),
+          @pipeline_metric.time(:duration_in_millis)
+        ]
+      end
+
+      def stop_clock(batch)
+        unless @inflight_clocks[Thread.current].nil?
+          if batch.size > 0
+            # onl/y stop (which also records) the metrics if the batch is non-empty.
+            # start_clock is now called at empty batch creation and an empty batch could
+            # stay empty all the way down to the close_batch call.
+            @inflight_clocks[Thread.current].each(&:stop)
+          end
+          @inflight_clocks.delete(Thread.current)
+        end
+      end
+
+      def add_starting_metrics(batch)
+        return if @event_metric.nil? || @pipeline_metric.nil?
+        @event_metric.increment(:in, batch.starting_size)
+        @pipeline_metric.increment(:in, batch.starting_size)
+      end
+
+      def add_filtered_metrics(batch)
+        @event_metric.increment(:filtered, batch.filtered_size)
+        @pipeline_metric.increment(:filtered, batch.filtered_size)
+      end
+
+      def add_output_metrics(batch)
+        @event_metric.increment(:out, batch.filtered_size)
+        @pipeline_metric.increment(:out, batch.filtered_size)
+      end
+    end
+
+    class ReadBatch
+      def initialize(queue, size, wait)
+        @queue = queue
+        @size = size
+        @wait = wait
+
+        @originals = Hash.new
+
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        # @cancelled = Hash.new
+
+        @generated = Hash.new
+        @iterating_temp = Hash.new
+        @iterating = false # Atomic Boolean maybe? Although batches are not shared across threads
+        @acked_batch = nil
+      end
+
+      def read_next
+        @acked_batch = @queue.read_batch(@size, @wait)
+        return if @acked_batch.nil?
+        @acked_batch.get_elements.each { |e| @originals[e] = true }
+      end
+
+      def close
+        # this will ack the whole batch, regardless of whether some
+        # events were cancelled or failed
+        return if @acked_batch.nil?
+        @acked_batch.close
+      end
+
+      def merge(event)
+        return if event.nil? || @originals.key?(event)
+        # take care not to cause @generated to change during iteration
+        # @iterating_temp is merged after the iteration
+        if iterating?
+          @iterating_temp[event] = true
+        else
+          # the periodic flush could generate events outside of an each iteration
+          @generated[event] = true
+        end
+      end
+
+      def cancel(event)
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        raise("cancel is unsupported")
+        # @cancelled[event] = true
+      end
+
+      def each(&blk)
+        # take care not to cause @originals or @generated to change during iteration
+
+        # below the checks for @cancelled.include?(e) have been replaced by e.cancelled?
+        # TODO: for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        @iterating = true
+        @originals.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @generated.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @iterating = false
+        update_generated
+      end
+
+      def size
+        filtered_size
+      end
+
+      def starting_size
+        @originals.size
+      end
+
+      def filtered_size
+        @originals.size + @generated.size
+      end
+
+      def cancelled_size
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        raise("cancelled_size is unsupported ")
+        # @cancelled.size
+      end
+
+      def shutdown_signal_received?
+        false
+      end
+
+      def flush_signal_received?
+        false
+      end
+
+      private
+
+      def iterating?
+        @iterating
+      end
+
+      def update_generated
+        @generated.update(@iterating_temp)
+        @iterating_temp.clear
+      end
+    end
+
+    class WriteClient
+      def initialize(queue)
+        @queue = queue
+      end
+
+      def get_new_batch
+        WriteBatch.new
+      end
+
+      def push(event)
+        if @queue.closed?
+          raise QueueClosedError.new("Attempted to write an event to a closed AckedQueue")
+        end
+        @queue.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def push_batch(batch)
+        if @queue.closed?
+          raise QueueClosedError.new("Attempted to write a batch to a closed AckedQueue")
+        end
+        batch.each do |event|
+          push(event)
+        end
+      end
+    end
+
+    class WriteBatch
+      def initialize
+        @events = []
+      end
+
+      def size
+        @events.size
+      end
+
+      def push(event)
+        @events.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def each(&blk)
+        @events.each do |e|
+          blk.call(e)
+        end
+      end
+    end
+  end
+end end
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
index a8822ca0af5..9a5f39044f4 100644
--- a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -5,8 +5,8 @@ class WrappedSynchronousQueue
     java_import java.util.concurrent.SynchronousQueue
     java_import java.util.concurrent.TimeUnit
 
-    def initialize()
-      @queue = java.util.concurrent.SynchronousQueue.new()
+    def initialize
+      @queue = java.util.concurrent.SynchronousQueue.new
     end
 
     # Push an object to the queue if the queue is full
@@ -18,24 +18,288 @@ def push(obj)
     end
     alias_method(:<<, :push)
 
-    # Offer an object to the queue, wait for the specified amout of time.
-    # If adding to the queue was successfull it wil return true, false otherwise.
+    # Offer an object to the queue, wait for the specified amount of time.
+    # If adding to the queue was successful it wil return true, false otherwise.
     #
     # @param [Object] Object to add to the queue
     # @param [Integer] Time in milliseconds to wait before giving up
-    # @return [Boolean] True if adding was successfull if not it return false
+    # @return [Boolean] True if adding was successful if not it return false
     def offer(obj, timeout_ms)
       @queue.offer(obj, timeout_ms, TimeUnit::MILLISECONDS)
     end
 
     # Blocking
     def take
-      @queue.take()
+      @queue.take
     end
 
     # Block for X millis
     def poll(millis)
       @queue.poll(millis, TimeUnit::MILLISECONDS)
     end
+
+    def write_client
+      WriteClient.new(self)
+    end
+
+    def read_client
+      ReadClient.new(self)
+    end
+
+    def close
+      # ignore
+    end
+
+    class ReadClient
+      # We generally only want one thread at a time able to access pop/take/poll operations
+      # from this queue. We also depend on this to be able to block consumers while we snapshot
+      # in-flight buffers
+
+      def initialize(queue, batch_size = 125, wait_for = 250)
+        @queue = queue
+        @mutex = Mutex.new
+        # Note that @inflight_batches as a central mechanism for tracking inflight
+        # batches will fail if we have multiple read clients in the pipeline.
+        @inflight_batches = {}
+
+        # allow the worker thread to report the execution time of the filter + output
+        @inflight_clocks = {}
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def close
+        # noop, compat with acked queue read client
+      end
+
+      def empty?
+        true # synchronous queue is alway empty
+      end
+
+      def set_batch_dimensions(batch_size, wait_for)
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def set_events_metric(metric)
+        @event_metric = metric
+        define_initial_metrics_values(@event_metric)
+      end
+
+      def set_pipeline_metric(metric)
+        @pipeline_metric = metric
+        define_initial_metrics_values(@pipeline_metric)
+      end
+
+      def define_initial_metrics_values(namespaced_metric)
+        namespaced_metric.report_time(:duration_in_millis, 0)
+        namespaced_metric.increment(:filtered, 0)
+        namespaced_metric.increment(:out, 0)
+      end
+
+      def inflight_batches
+        @mutex.synchronize do
+          yield(@inflight_batches)
+        end
+      end
+
+      def current_inflight_batch
+        @inflight_batches.fetch(Thread.current, [])
+      end
+
+      # create a new empty batch
+      # @return [ReadBatch] a new empty read batch
+      def new_batch
+        ReadBatch.new(@queue, @batch_size, @wait_for)
+      end
+
+      def read_batch
+        batch = new_batch
+        @mutex.synchronize { batch.read_next }
+        start_metrics(batch)
+        batch
+      end
+
+      def start_metrics(batch)
+        @mutex.synchronize do
+          # there seems to be concurrency issues with metrics, keep it in the mutex
+          set_current_thread_inflight_batch(batch)
+          start_clock
+        end
+      end
+
+      def set_current_thread_inflight_batch(batch)
+        @inflight_batches[Thread.current] = batch
+      end
+
+      def close_batch(batch)
+        @mutex.synchronize do
+          # there seems to be concurrency issues with metrics, keep it in the mutex
+          @inflight_batches.delete(Thread.current)
+          stop_clock(batch)
+        end
+      end
+
+      def start_clock
+        @inflight_clocks[Thread.current] = [
+          @event_metric.time(:duration_in_millis),
+          @pipeline_metric.time(:duration_in_millis)
+        ]
+      end
+
+      def stop_clock(batch)
+        unless @inflight_clocks[Thread.current].nil?
+          if batch.size > 0
+            # only stop (which also records) the metrics if the batch is non-empty.
+            # start_clock is now called at empty batch creation and an empty batch could
+            # stay empty all the way down to the close_batch call.
+            @inflight_clocks[Thread.current].each(&:stop)
+          end
+          @inflight_clocks.delete(Thread.current)
+        end
+      end
+
+      def add_filtered_metrics(batch)
+        @event_metric.increment(:filtered, batch.filtered_size)
+        @pipeline_metric.increment(:filtered, batch.filtered_size)
+      end
+
+      def add_output_metrics(batch)
+        @event_metric.increment(:out, batch.filtered_size)
+        @pipeline_metric.increment(:out, batch.filtered_size)
+      end
+    end
+
+    class ReadBatch
+      def initialize(queue, size, wait)
+        @queue = queue
+        @size = size
+        @wait = wait
+
+        @originals = Hash.new
+
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        # @cancelled = Hash.new
+
+        @generated = Hash.new
+        @iterating_temp = Hash.new
+        @iterating = false # Atomic Boolean maybe? Although batches are not shared across threads
+        @acked_batch = nil
+      end
+
+      def read_next
+        @size.times do |t|
+          event = @queue.poll(@wait)
+          return if event.nil? # queue poll timed out
+
+          @originals[event] = true
+        end
+      end
+
+      def merge(event)
+        return if event.nil? || @originals.key?(event)
+        # take care not to cause @generated to change during iteration
+        # @iterating_temp is merged after the iteration
+        if iterating?
+          @iterating_temp[event] = true
+        else
+          # the periodic flush could generate events outside of an each iteration
+          @generated[event] = true
+        end
+      end
+
+      def cancel(event)
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        raise("cancel is unsupported")
+        # @cancelled[event] = true
+      end
+
+      def each(&blk)
+        # take care not to cause @originals or @generated to change during iteration
+        @iterating = true
+
+        # below the checks for @cancelled.include?(e) have been replaced by e.cancelled?
+        # TODO: for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        @originals.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @generated.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @iterating = false
+        update_generated
+      end
+
+      def size
+        filtered_size
+      end
+
+      def starting_size
+        @originals.size
+      end
+
+      def filtered_size
+        @originals.size + @generated.size
+      end
+
+      def cancelled_size
+      # TODO: disabled for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+      raise("cancelled_size is unsupported ")
+        # @cancelled.size
+      end
+
+      private
+
+      def iterating?
+        @iterating
+      end
+
+      def update_generated
+        @generated.update(@iterating_temp)
+        @iterating_temp.clear
+      end
+    end
+
+    class WriteClient
+      def initialize(queue)
+        @queue = queue
+      end
+
+      def get_new_batch
+        WriteBatch.new
+      end
+
+      def push(event)
+        @queue.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def push_batch(batch)
+        batch.each do |event|
+          push(event)
+        end
+      end
+    end
+
+    class WriteBatch
+      def initialize
+        @events = []
+      end
+
+      def size
+        @events.size
+      end
+
+      def push(event)
+        @events.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def each(&blk)
+        @events.each do |e|
+          blk.call(e)
+        end
+      end
+    end
   end
 end end
diff --git a/logstash-core/lib/logstash/version.rb b/logstash-core/lib/logstash/version.rb
index 70715b097cb..c4f2c917d32 100644
--- a/logstash-core/lib/logstash/version.rb
+++ b/logstash-core/lib/logstash/version.rb
@@ -11,4 +11,4 @@
 #       eventually this file should be in the root logstash lib fir and dependencies in logstash-core should be
 #       fixed.
 
-LOGSTASH_VERSION = "3.0.0.dev"
+LOGSTASH_VERSION = "5.4.3"
diff --git a/logstash-core/lib/logstash/webserver.rb b/logstash-core/lib/logstash/webserver.rb
index 23bcaf0b576..0531cb84395 100644
--- a/logstash-core/lib/logstash/webserver.rb
+++ b/logstash-core/lib/logstash/webserver.rb
@@ -1,98 +1,103 @@
 # encoding: utf-8
+require "logstash/api/rack_app"
 require "puma"
-require "puma/single"
-require "puma/binder"
-require "puma/configuration"
-require "puma/commonlogger"
+require "puma/server"
+require "logstash/patches/puma"
+require "concurrent"
+require "thread"
 
-module LogStash 
+module LogStash
   class WebServer
-
     extend Forwardable
 
-    attr_reader :logger, :status, :config, :options, :cli_options, :runner, :binder, :events
+    attr_reader :logger, :status, :config, :options, :runner, :binder, :events, :http_host, :http_ports, :http_environment, :agent
 
     def_delegator :@runner, :stats
 
     DEFAULT_HOST = "127.0.0.1".freeze
-    DEFAULT_PORT = 9600.freeze
-
-    def initialize(logger, options={})
-      @logger      = logger
-      http_host    = options[:http_host] || DEFAULT_HOST
-      http_port    = options[:http_port] || DEFAULT_PORT
-      @options     = {}
-      @cli_options = options.merge({ :rackup => ::File.join(::File.dirname(__FILE__), "api", "init.ru"),
-                                     :binds => ["tcp://#{http_host}:#{http_port}"],
-                                     :debug => logger.debug?,
-                                     # Prevent puma from queueing request when not able to properly handling them,
-                                     # fixed https://github.com/elastic/logstash/issues/4674. See
-                                     # https://github.com/puma/puma/pull/640 for mode internal details in PUMA.
-                                     :queue_requests => false
-      })
-      @status      = nil
-
-      parse_options
-
-      @runner  = nil
-      @events  = ::Puma::Events.strings
-      @binder  = ::Puma::Binder.new(@events)
-      @binder.import_from_env
-
-      set_environment
+    DEFAULT_PORTS = (9600..9700).freeze
+    DEFAULT_ENVIRONMENT = 'production'.freeze
+
+    def initialize(logger, agent, options={})
+      @logger = logger
+      @agent = agent
+      @http_host = options[:http_host] || DEFAULT_HOST
+      @http_ports = options[:http_ports] || DEFAULT_PORTS
+      @http_environment = options[:http_environment] || DEFAULT_ENVIRONMENT
+      @options = {}
+      @status = nil
+      @running = Concurrent::AtomicBoolean.new(false)
     end
 
     def run
-      log "=== puma start: #{Time.now} ==="
-
-      @runner = Puma::Single.new(self)
-      @status = :run
-      @runner.run
-      stop(:graceful => true)
+      logger.debug("Starting puma")
+
+      stop # Just in case
+
+      running!
+
+      http_ports.each_with_index do |port, idx|
+        begin
+          if running?
+            @port = port
+            logger.debug("Trying to start WebServer", :port => @port)
+            start_webserver(@port)
+          else
+            break # we are closing down the server so just get out of the loop
+          end
+        rescue Errno::EADDRINUSE
+          if http_ports.count == 1
+            raise Errno::EADDRINUSE.new(I18n.t("logstash.web_api.cant_bind_to_port", :port => http_ports.first))
+          elsif idx == http_ports.count-1
+            raise Errno::EADDRINUSE.new(I18n.t("logstash.web_api.cant_bind_to_port_in_range", :http_ports => http_ports))
+          end
+        end
+      end
     end
 
-    def log(str)
-      logger.debug(str)
+    def running!
+      @running.make_true
     end
 
-    def error(str)
-      logger.error(str)
+    def running?
+      @running.value
     end
 
-    # Empty method, this method is required because of the puma usage we make through
-    # the Single interface, https://github.com/puma/puma/blob/master/lib/puma/single.rb#L82
-    # for more details. This can always be implemented when we want to keep track of this
-    # bit of data.
-    def write_state; end
+    def address
+      "#{http_host}:#{@port}"
+    end
 
     def stop(options={})
-      graceful = options.fetch(:graceful, true)
+      @running.make_false
+      @server.stop(true) if @server
+    end
 
-      if graceful
-        @runner.stop_blocked
-      else
-        @runner.stop
-      end rescue nil
+    def start_webserver(port)
+      # wrap any output that puma could generate into a wrapped logger
+      # use the puma namespace to override STDERR, STDOUT in that scope.
+      Puma::STDERR.logger = logger
+      Puma::STDOUT.logger = logger
 
-      @status = :stop
-      log "=== puma shutdown: #{Time.now} ==="
-    end
+      io_wrapped_logger = LogStash::IOWrappedLogger.new(logger)
 
-    private
+      app = LogStash::Api::RackApp.app(logger, agent, http_environment)
 
-    def env
-      @options[:debug] ? "development" : "production"
-    end
+      events = ::Puma::Events.new(io_wrapped_logger, io_wrapped_logger)
+
+      @server = ::Puma::Server.new(app, events)
+      @server.add_tcp_listener(http_host, port)
+
+      logger.info("Successfully started Logstash API endpoint", :port => port)
 
-    def set_environment
-      @options[:environment] = env
-      ENV['RACK_ENV']        = env
+      set_http_address_metric("#{http_host}:#{port}")
+
+      @server.run.join
     end
 
-    def parse_options
-      @config  = ::Puma::Configuration.new(cli_options)
-      @config.load
-      @options = @config.options
+    private
+    def set_http_address_metric(value)
+      return unless @agent.metric
+      @agent.metric.gauge([], :http_address, value)
     end
   end
 end
diff --git a/logstash-core/locales/en.yml b/logstash-core/locales/en.yml
index 7797fee5730..a00ce406a99 100644
--- a/logstash-core/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -1,6 +1,6 @@
 # YAML notes
 #   |- means 'scalar block' useful for formatted text
-#   > means 'scalar block' but it chomps all newlines. Useful 
+#   > means 'scalar block' but it chomps all newlines. Useful
 #     for unformatted text.
 en:
   oops: |-
@@ -44,9 +44,6 @@ en:
         %{plugin} plugin is using the 'milestone' method to declare the version
         of the plugin this method is deprecated in favor of declaring the
         version inside the gemspec.
-      no_version: >-
-        %{name} plugin doesn't have a version. This plugin isn't well
-         supported by the community and likely has no maintainer.
       version:
         0-9-x:
          Using version 0.9.x %{type} plugin '%{name}'. This plugin should work but
@@ -68,29 +65,41 @@ en:
         data loss.
       forced_sigint: >-
         SIGINT received. Terminating immediately..
+      non_reloadable_config_reload: >-
+        Unable to reload configuration because it does not support dynamic reloading
+      non_reloadable_config_register: |-
+        Logstash is not able to start since configuration auto reloading was enabled but the configuration contains plugins that don't support it. Quitting...
     web_api:
-      flag:
-        http_host: Web API binding host
-        http_port: Web API http port
+      cant_bind_to_port: |-
+        Logstash tried to bind to port %{port}, but the port is already in use. You can specify a new port by launching logstash with the --http.port option."
+      cant_bind_to_port_in_range: |-
+        Logstash tried to bind to port range %{http_ports}, but all the ports are already in use. You can specify a new port by launching logstash with the --http.port option."
       hot_threads:
         title: |-
           ::: {%{hostname}}
-            Hot threads at %{time}, busiestThreads=%{top_count}:
+          Hot threads at %{time}, busiestThreads=%{top_count}:
         thread_title: |-
-            %{percent_of_cpu_time} % of cpu usage by %{thread_state} thread named '%{thread_name}'
+          %{percent_of_cpu_time} % of cpu usage, state: %{thread_state}, thread name: '%{thread_name}'
+      logging:
+        unrecognized_option: |-
+          unrecognized option [%{option}]
     runner:
       short-help: |-
         usage:
-          bin/logstash -f CONFIG_PATH [-t] [-r] [--quiet|verbose|debug] [-w COUNT] [-l LOG]
-          bin/logstash -e CONFIG_STR [-t] [--quiet|verbose|debug] [-w COUNT] [-l LOG]
-          bin/logstash -i SHELL [--quiet|verbose|debug]
-          bin/logstash -V [--verbose|debug]
+          bin/logstash -f CONFIG_PATH [-t] [-r] [] [-w COUNT] [-l LOG]
+          bin/logstash -e CONFIG_STR [-t] [--log.level fatal|error|warn|info|debug|trace] [-w COUNT] [-l LOG]
+          bin/logstash -i SHELL [--log.level fatal|error|warn|info|debug|trace]
+          bin/logstash -V [--log.level fatal|error|warn|info|debug|trace]
           bin/logstash --help
+      invalid-configuration: >-
+        The given configuration is invalid. Reason: %{error}
       missing-configuration: >-
         No configuration file was specified. Perhaps you forgot to provide
         the '-f yourlogstash.conf' flag?
       reload-without-config-path: >-
         Configuration reloading also requires passing a configuration path with '-f yourlogstash.conf'
+      locked-data-path: >-
+        Logstash could not be started because there is already another instance using the configured data directory.  If you wish to run multiple instances, you must change the "path.data" setting.
       invalid-shell: >-
         Invalid option for interactive Ruby shell. Use either "irb" or "pry"
       configtest-flag-information: |-
@@ -102,8 +111,7 @@ en:
           longer available. %{extra} If you have any questions about this, you
           are invited to visit https://discuss.elastic.co/c/logstash and ask.
         file-not-found: |-
-          No config files found: %{path}
-          Can you make sure this path is a logstash config file?
+          No config files found: %{path}. Can you make sure this path is a logstash config file?
         scheme-not-supported: |-
           URI scheme not supported: %{path}
           Either pass a local file path or "file|http://" URI
@@ -138,18 +146,18 @@ en:
           after %{after}
         invalid_plugin_register: >-
           Cannot register %{plugin} %{type} plugin.
-          The error reported is: 
+          The error reported is:
             %{error}
         plugin_path_missing: >-
           You specified a plugin path that does not exist: %{path}
         no_plugins_found: |-
           Could not find any plugins in "%{path}"
-          I tried to find files matching the following, but found none: 
+          I tried to find files matching the following, but found none:
             %{plugin_glob}
         log_file_failed: |-
           Failed to open %{path} for writing: %{error}
 
-          This is often a permissions issue, or the wrong 
+          This is often a permissions issue, or the wrong
           path was specified?
       flag:
         # Note: Wrap these at 55 chars so they display nicely when clamp emits
@@ -175,6 +183,8 @@ en:
           the empty string for the '-e' flag.
         configtest: |+
           Check configuration for valid syntax and then exit.
+        http_host: Web API binding host
+        http_port: Web API http port
         pipeline-workers: |+
           Sets the number of pipeline workers to run.
         pipeline-batch-size: |+
@@ -182,6 +192,11 @@ en:
         pipeline-batch-delay: |+
           When creating pipeline batches, how long to wait while polling
           for the next event.
+        path_settings: |+
+          Directory containing logstash.yml file. This can also be
+          set through the LS_SETTINGS_DIR environment variable.
+        path_logs: |+
+            Directory to Write Logstash internal logs to.
         auto_reload: |+
           Monitor configuration changes and reload
           whenever it is changed.
@@ -198,10 +213,14 @@ en:
           Specifying once will show 'informational'
           logs. Specifying twice will show 'debug'
           logs. This flag is deprecated. You should use
-          --verbose or --debug instead.
+          --log-level=info or --log-level=debug instead.
         version: |+
           Emit the version of logstash and its friends,
           then exit.
+        datapath: |+
+          This should point to a writable directory. Logstash
+          will use this directory whenever it needs to store
+          data. Plugins will also have access to this path.
         pluginpath: |+
           A path of where to find plugins. This flag
           can be given multiple times to include
@@ -210,15 +229,14 @@ en:
           'PATH/logstash/TYPE/NAME.rb' where TYPE is
           'inputs' 'filters', 'outputs' or 'codecs'
           and NAME is the name of the plugin.
-        quiet: |+
-          Quieter logstash logging. This causes only 
-          errors to be emitted.
-        verbose: |+
-          More verbose logging. This causes 'info' 
-          level logs to be emitted.
-        debug: |+
-          Most verbose logging. This causes 'debug'
-          level logs to be emitted.
+        log_level: |+
+          Set the log level for logstash. Possible values are:
+            - fatal
+            - error
+            - warn
+            - info
+            - debug
+            - trace
         unsafe_shutdown: |+
           Force logstash to exit during shutdown even
           if there are still inflight events in memory.
@@ -227,8 +245,24 @@ en:
         rubyshell: |+
           Drop to shell instead of running as normal.
           Valid shells are "irb" and "pry"
-        node_name: |+
+        name: |+
           Specify the name of this logstash instance, if no value is given
           it will default to the current hostname.
         agent: |+
           Specify an alternate agent plugin name.
+        config_debug: |+
+          Print the compiled config ruby code out as a debug log (you must also have --log.level=debug enabled).
+          WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result
+          in plaintext passwords appearing in your logs!
+        log_format: |+
+          Specify if Logstash should write its own logs in JSON form (one
+          event per line) or in plain text (using Ruby's Object#inspect)
+        debug: |+
+          Set the log level to debug.
+          DEPRECATED: use --log.level=debug instead.
+        verbose: |+
+          Set the log level to info.
+          DEPRECATED: use --log.level=info instead.
+        quiet: |+
+          Set the log level to info.
+          DEPRECATED: use --log.level=quiet instead.
diff --git a/logstash-core/logstash-core.gemspec b/logstash-core/logstash-core.gemspec
index c830291c8f7..f2d509aec9b 100644
--- a/logstash-core/logstash-core.gemspec
+++ b/logstash-core/logstash-core.gemspec
@@ -11,26 +11,25 @@ Gem::Specification.new do |gem|
   gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
   gem.license       = "Apache License (2.0)"
 
-  gem.files         = Dir.glob(["logstash-core.gemspec", "lib/**/*.rb", "spec/**/*.rb", "locales/*", "lib/logstash/api/init.ru"])
+  gem.files         = Dir.glob(["logstash-core.gemspec", "gemspec_jars.rb", "lib/**/*.rb", "spec/**/*.rb", "locales/*", "lib/logstash/api/init.ru", "lib/logstash-core/logstash-core.jar"])
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core"
   gem.require_paths = ["lib"]
   gem.version       = LOGSTASH_CORE_VERSION
 
-  gem.add_runtime_dependency "logstash-core-event-java", "~> 3.0.0.dev"
+  gem.platform = "java"
 
-  gem.add_runtime_dependency "cabin", "~> 0.8.0" #(Apache 2.0 license)
   gem.add_runtime_dependency "pry", "~> 0.10.1"  #(Ruby license)
   gem.add_runtime_dependency "stud", "~> 0.0.19" #(Apache 2.0 license)
   gem.add_runtime_dependency "clamp", "~> 0.6.5" #(MIT license) for command line args/flags
   gem.add_runtime_dependency "filesize", "0.0.4" #(MIT license) for :bytes config validator
   gem.add_runtime_dependency "gems", "~> 0.8.3"  #(MIT license)
-  gem.add_runtime_dependency "concurrent-ruby", "1.0.0"
+  gem.add_runtime_dependency "concurrent-ruby", "~> 1.0", ">= 1.0.5"
   gem.add_runtime_dependency "sinatra", '~> 1.4', '>= 1.4.6'
-  gem.add_runtime_dependency 'puma', '~> 2.16', '>= 2.16.0'
-  gem.add_runtime_dependency "jruby-openssl", "0.9.13" # Required to support TLSv1.2
+  gem.add_runtime_dependency 'puma', '~> 2.16'
+  gem.add_runtime_dependency "jruby-openssl", "0.9.16" # >= 0.9.13 Required to support TLSv1.2
   gem.add_runtime_dependency "chronic_duration", "0.10.6"
-  gem.add_runtime_dependency "jruby-monitoring", '~> 0.1'
+  gem.add_runtime_dependency "jrmonitor", '~> 0.4.2'
 
   # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
   # investigated what the cause might be. -Jordan
@@ -44,19 +43,13 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "rubyzip", "~> 1.1.7"
   gem.add_runtime_dependency "thread_safe", "~> 0.3.5" #(Apache 2.0 license)
 
-  if RUBY_PLATFORM == 'java'
-    gem.platform = RUBY_PLATFORM
-    gem.add_runtime_dependency "jrjackson", "~> 0.3.7" #(Apache 2.0 license)
-  else
-    gem.add_runtime_dependency "oj" #(MIT-style license)
-  end
-
-  if RUBY_ENGINE == "rbx"
-    # rubinius puts the ruby stdlib into gems.
-    gem.add_runtime_dependency "rubysl"
-
-    # Include racc to make the xml tests pass.
-    # https://github.com/rubinius/rubinius/issues/2632#issuecomment-26954565
-    gem.add_runtime_dependency "racc"
-  end
+  gem.add_runtime_dependency "jrjackson", "~> 0.4.0" #(Apache 2.0 license)
+
+  gem.add_runtime_dependency "jar-dependencies"
+  # as of Feb 3rd 2016, the ruby-maven gem is resolved to version 3.3.3 and that version
+  # has an rdoc problem that causes a bundler exception. 3.3.9 is the current latest version
+  # which does not have this problem.
+  gem.add_runtime_dependency "ruby-maven", "~> 3.3.9"
+
+  eval(File.read(File.expand_path("../gemspec_jars.rb", __FILE__)))
 end
diff --git a/logstash-core/settings.gradle b/logstash-core/settings.gradle
new file mode 100644
index 00000000000..4da77f25813
--- /dev/null
+++ b/logstash-core/settings.gradle
@@ -0,0 +1 @@
+rootProject.name = 'logstash-core'
diff --git a/logstash-core/spec/api/lib/api/logging_spec.rb b/logstash-core/spec/api/lib/api/logging_spec.rb
new file mode 100644
index 00000000000..214a2ad69f2
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/logging_spec.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/logging"
+require "logstash/json"
+
+describe LogStash::Api::Modules::Logging do
+  include_context "api setup"
+
+  describe "#logging" do
+
+    context "when setting a logger's log level" do
+      before(:all) do
+        @runner = LogStashRunner.new
+        @runner.start
+      end
+
+      after(:all) do
+        @runner.stop
+      end
+
+      it "should return a positive acknowledgement on success" do
+        put '/', '{"logger.logstash": "ERROR"}'
+        payload = LogStash::Json.load(last_response.body)
+        expect(payload['acknowledged']).to eq(true)
+      end
+
+      it "should throw error when level is invalid" do
+        put '/', '{"logger.logstash": "invalid"}'
+        payload = LogStash::Json.load(last_response.body)
+        expect(payload['error']).to eq("invalid level[invalid] for logger[logstash]")
+      end
+
+      it "should throw error when key logger is invalid" do
+        put '/', '{"invalid" : "ERROR"}'
+        payload = LogStash::Json.load(last_response.body)
+        expect(payload['error']).to eq("unrecognized option [invalid]")
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/api/lib/api/node_plugins_spec.rb b/logstash-core/spec/api/lib/api/node_plugins_spec.rb
new file mode 100644
index 00000000000..5389e10c418
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/node_plugins_spec.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require_relative "../../../support/shared_examples"
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/plugins"
+require "logstash/json"
+
+describe LogStash::Api::Modules::Plugins do
+  include_context "api setup"
+  include_examples "not found"
+
+  extend ResourceDSLMethods
+
+  before(:each) do
+    do_request { get "/" }
+  end
+
+  let(:payload) { LogStash::Json.load(last_response.body) }
+
+  describe "retrieving plugins" do
+    it "should return OK" do
+      expect(last_response).to be_ok
+    end
+
+    it "should return a list of plugins" do
+      expect(payload["plugins"]).to be_a(Array)
+    end
+
+    it "should return the total number of plugins" do
+      expect(payload["total"]).to be_a(Numeric)
+    end
+  end
+end
diff --git a/logstash-core/spec/api/lib/api/node_spec.rb b/logstash-core/spec/api/lib/api/node_spec.rb
index 119fca23ed2..a8f8b009f5b 100644
--- a/logstash-core/spec/api/lib/api/node_spec.rb
+++ b/logstash-core/spec/api/lib/api/node_spec.rb
@@ -1,16 +1,13 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/modules/node"
+require "logstash/api/modules/node"
 require "logstash/json"
 
-describe LogStash::Api::Node do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
+describe LogStash::Api::Modules::Node do
+  include_context "api setup"
+  include_examples "not found"
 
   describe "#hot threads" do
 
@@ -39,26 +36,116 @@ def app()
       end
 
       it "should return information for <= # requested threads" do
-        expect(payload["threads"].count).to be <= 5
+        expect(payload["hot_threads"]["threads"].count).to be <= 5
       end
     end
 
     context "when asking for human output" do
+      [
+        "/hot_threads?human",
+        "/hot_threads?human=true",
+        "/hot_threads?human=1",
+        "/hot_threads?human=t",
+      ].each do |path|
+
+        before(:all) do
+          do_request { get path }
+        end
+
+        let(:payload) { last_response.body }
+
+        it "should return a text/plain content type" do
+          expect(last_response.content_type).to eq("text/plain;charset=utf-8")
+        end
+
+        it "should return a plain text payload" do
+          expect{ JSON.parse(payload) }.to raise_error
+        end
+      end
+    end
 
+    context "When asking for human output and threads count" do
       before(:all) do
-        do_request { get "/hot_threads?human" }
+        # Make sure we have enough threads for this to work.
+        @threads = []
+        5.times { @threads << Thread.new { loop {} } }
+
+        do_request { get "/hot_threads?human=t&threads=2"}
+      end
+
+      after(:all) do
+        @threads.each { |t| t.kill } rescue nil
       end
 
       let(:payload) { last_response.body }
 
-      it "should return a text/plain content type" do
-        expect(last_response.content_type).to eq("text/plain;charset=utf-8")
+      it "should return information for <= # requested threads" do
+        expect(payload.scan(/thread name/).size).to eq(2)
       end
+    end
 
-      it "should return a plain text payload" do
-        expect{ JSON.parse(payload) }.to raise_error
+    context "when not asking for human output" do
+      [
+        "/hot_threads?human=false",
+        "/hot_threads?human=0",
+        "/hot_threads?human=f",
+      ].each do |path|
+        before(:all) do
+          do_request { get path }
+        end
+
+        it "should return a json payload content type" do
+          expect(last_response.content_type).to eq("application/json")
+        end
+
+        let(:payload) { last_response.body }
+
+        it "should return a json payload" do
+          expect{ JSON.parse(payload) }.not_to raise_error
+        end
       end
     end
 
+    describe "Generic JSON testing" do
+      extend ResourceDSLMethods
+
+      root_structure = {
+        "pipeline" => {
+          "workers" => Numeric,
+          "batch_size" => Numeric,
+          "batch_delay" => Numeric,
+          "config_reload_automatic" => Boolean,
+          "config_reload_interval" => Numeric
+        },
+        "os" => {
+          "name" => String,
+          "arch" => String,
+          "version" => String,
+          "available_processors" => Numeric
+        },
+        "jvm" => {
+          "pid" => Numeric,
+          "version" => String,
+          "vm_name" => String,
+          "vm_version" => String,
+          "vm_vendor" => String,
+          "start_time_in_millis" => Numeric,
+          "mem" => {
+            "heap_init_in_bytes" => Numeric,
+            "heap_max_in_bytes" => Numeric,
+            "non_heap_init_in_bytes" => Numeric,
+            "non_heap_max_in_bytes" => Numeric
+        },
+        "gc_collectors" => Array
+        },
+        "hot_threads"=> {
+          "time" => String,
+          "busiest_threads" => Numeric,
+          "threads" => Array
+        }
+      }
+
+      test_api_and_resources(root_structure, :exclude_from_root => ["hot_threads"])
+    end
   end
 end
diff --git a/logstash-core/spec/api/lib/api/node_stats_spec.rb b/logstash-core/spec/api/lib/api/node_stats_spec.rb
index c90d167e3a7..a4eae4d5aa3 100644
--- a/logstash-core/spec/api/lib/api/node_stats_spec.rb
+++ b/logstash-core/spec/api/lib/api/node_stats_spec.rb
@@ -1,68 +1,92 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/modules/node_stats"
+require "logstash/api/modules/node_stats"
 require "logstash/json"
 
-describe LogStash::Api::NodeStats do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
-
-  let(:payload) { LogStash::Json.load(last_response.body) }
-
-  context "#root" do
-
-    before(:all) do
-      do_request { get "/" }
-    end
-
-    it "respond OK" do
-      expect(last_response).to be_ok
-    end
-
-    ["events", "jvm"].each do |key|
-      it "contains #{key} information" do
-        expect(payload).to include(key)
-      end
-    end
-  end
-
-  context "#events" do
-
-    let(:payload) { LogStash::Json.load(last_response.body) }
-
-    before(:all) do
-      do_request { get "/events" }
-    end
-
-    it "respond OK" do
-      expect(last_response).to be_ok
-    end
-
-    it "contains events information" do
-      expect(payload).to include("events")
-    end
-  end
-
-  context "#jvm" do
-
-    let(:payload) { LogStash::Json.load(last_response.body) }
-
-    before(:all) do
-      do_request { get "/jvm" }
-    end
-
-    it "respond OK" do
-      expect(last_response).to be_ok
-    end
-
-    it "contains memory information" do
-      expect(payload).to include("mem")
-    end
-  end
-
+describe LogStash::Api::Modules::NodeStats do
+  include_context "api setup"
+  include_examples "not found"
+
+  extend ResourceDSLMethods
+
+  # DSL describing response structure
+  root_structure = {
+    "jvm"=>{
+      "uptime_in_millis" => Numeric,
+      "threads"=>{
+        "count"=>Numeric,
+        "peak_count"=>Numeric
+      },
+      "gc" => {
+        "collectors" => {
+          "young" => {
+            "collection_count" => Numeric,
+            "collection_time_in_millis" => Numeric
+          },
+          "old" => {
+            "collection_count" => Numeric,
+            "collection_time_in_millis" => Numeric
+          }
+        }
+      },
+      "mem" => {
+        "heap_used_in_bytes" => Numeric,
+        "heap_used_percent" => Numeric,
+        "heap_committed_in_bytes" => Numeric,
+        "heap_max_in_bytes" => Numeric,
+        "non_heap_used_in_bytes" => Numeric,
+        "non_heap_committed_in_bytes" => Numeric,
+        "pools" => {
+          "survivor" => {
+            "peak_used_in_bytes" => Numeric,
+            "used_in_bytes" => Numeric,
+            "peak_max_in_bytes" => Numeric,
+            "max_in_bytes" => Numeric
+          },
+          "old" => {
+            "peak_used_in_bytes" => Numeric,
+            "used_in_bytes" => Numeric,
+            "peak_max_in_bytes" => Numeric,
+            "max_in_bytes" => Numeric
+          },
+          "young" => {
+            "peak_used_in_bytes" => Numeric,
+            "used_in_bytes" => Numeric,
+            "peak_max_in_bytes" => Numeric,
+            "max_in_bytes" => Numeric
+          }
+        }
+      }
+    },
+    "process"=>{
+      "peak_open_file_descriptors"=>Numeric,
+      "max_file_descriptors"=>Numeric,
+      "open_file_descriptors"=>Numeric,
+      "mem"=>{
+        "total_virtual_in_bytes"=>Numeric
+      },
+      "cpu"=>{
+        "total_in_millis"=>Numeric,
+        "percent"=>Numeric,
+        "load_average" => { "1m" => Numeric }
+      }
+    },
+   "pipeline" => {
+     "events" => {
+        "duration_in_millis" => Numeric,
+        "in" => Numeric,
+        "filtered" => Numeric,
+        "out" => Numeric,
+        "queue_push_duration_in_millis" => Numeric
+     }
+   },
+   "reloads" => {
+     "successes" => Numeric,
+     "failures" => Numeric
+   }
+  }
+
+  test_api_and_resources(root_structure)
 end
diff --git a/logstash-core/spec/api/lib/api/plugins_spec.rb b/logstash-core/spec/api/lib/api/plugins_spec.rb
index 4e0aa66b48b..ee554dae22f 100644
--- a/logstash-core/spec/api/lib/api/plugins_spec.rb
+++ b/logstash-core/spec/api/lib/api/plugins_spec.rb
@@ -1,18 +1,15 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/modules/plugins"
+require "logstash/api/modules/plugins"
 require "logstash/json"
 
-describe LogStash::Api::Plugins do
+describe LogStash::Api::Modules::Plugins do
+  include_context "api setup"
+  include_examples "not found"
 
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
-
-  before(:all) do
+  before(:each) do
     get "/"
   end
 
@@ -23,7 +20,7 @@ def app()
   end
 
   it "return valid json content type" do
-    expect(last_response.content_type).to eq("application/json")
+    expect(last_response.content_type).to eq("application/json"), "Did not get json, got #{last_response.content_type} / #{last_response.body}"
   end
 
   context "#schema" do
@@ -52,6 +49,5 @@ def app()
         expect(plugin["version"]).not_to be_empty
       end
     end
-
   end
 end
diff --git a/logstash-core/spec/api/lib/api/root_spec.rb b/logstash-core/spec/api/lib/api/root_spec.rb
index 6bc8a4937b6..ad9dc08381a 100644
--- a/logstash-core/spec/api/lib/api/root_spec.rb
+++ b/logstash-core/spec/api/lib/api/root_spec.rb
@@ -1,20 +1,18 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/root"
+require "logstash/api/modules/root"
 require "logstash/json"
 
-describe LogStash::Api::Root do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
+describe LogStash::Api::Modules::Root do
+  include_context "api setup"
 
   it "should respond to root resource" do
     do_request { get "/" }
     expect(last_response).to be_ok
   end
 
+  include_examples "not found"
 end
+
diff --git a/logstash-core/spec/api/lib/api/stats_spec.rb b/logstash-core/spec/api/lib/api/stats_spec.rb
deleted file mode 100644
index 8dfd2617b42..00000000000
--- a/logstash-core/spec/api/lib/api/stats_spec.rb
+++ /dev/null
@@ -1,19 +0,0 @@
-# encoding: utf-8
-require_relative "../../spec_helper"
-require "sinatra"
-require "app/modules/stats"
-
-describe LogStash::Api::Stats do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
-
-  it "respond to the jvm resource" do
-    do_request { get "/jvm" }
-    expect(last_response).to be_ok
-  end
-
-end
diff --git a/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
new file mode 100644
index 00000000000..0800e731e97
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
@@ -0,0 +1,87 @@
+# Ruby doesn't have common class for boolean,
+# And to simplify the ResourceDSLMethods check it make sense to have it.
+module Boolean; end
+class TrueClass
+  include Boolean
+end
+class FalseClass
+  include Boolean
+end
+
+module ResourceDSLMethods
+  # Convert a nested hash to a mapping of key paths to expected classes
+  def hash_to_mapping(h, path=[], mapping={})
+    h.each do |k,v|
+      if v.is_a?(Hash)
+        hash_to_mapping(v, path + [k], mapping)
+      else
+        full_path = path + [k]
+        mapping[full_path] = v
+      end
+    end
+    mapping
+  end
+
+  def test_api(expected, path)
+    context "GET #{path}" do
+      let(:payload) { LogStash::Json.load(last_response.body) }
+
+      before(:all) do
+        do_request { get path }
+      end
+
+      it "should respond OK" do
+        expect(last_response).to be_ok
+      end
+
+
+      describe "the default metadata" do
+        it "should include the host" do
+          expect(payload["host"]).to eql(Socket.gethostname)
+        end
+
+        it "should include the version" do
+          expect(payload["version"]).to eql(LOGSTASH_CORE_VERSION)
+        end
+
+        it "should include the http address" do
+          expect(payload["http_address"]).to eql("#{Socket.gethostname}:#{::LogStash::WebServer::DEFAULT_PORTS.first}")
+        end
+
+        it "should include the node name" do
+          expect(payload["name"]).to eql(@runner.agent.name)
+        end
+
+        it "should include the node id" do
+          expect(payload["id"]).to eql(@runner.agent.id)
+        end
+      end
+
+      hash_to_mapping(expected).each do |resource_path,klass|
+        dotted = resource_path.join(".")
+
+        it "should set '#{dotted}' at '#{path}' to be a '#{klass}'" do
+          expect(last_response).to be_ok # fail early if need be
+          resource_path_value = resource_path.reduce(payload) do |acc,v|
+            expect(acc.has_key?(v)).to eql(true), "Expected to find value '#{v}' in structure '#{acc}', but could not. Payload was '#{payload}'"
+            acc[v]
+          end
+          expect(resource_path_value).to be_a(klass), "could not find '#{dotted}' in #{payload}"
+        end
+      end
+    end
+
+    yield if block_given? # Add custom expectations
+  end
+
+  def test_api_and_resources(expected, xopts={})
+    xopts[:exclude_from_root] ||= []
+    root_expectation = expected.clone
+    xopts[:exclude_from_root].each {|k| root_expectation.delete(k)}
+    test_api(root_expectation, "/")
+
+    expected.keys.each do |key|
+      test_api({key => expected[key]}, "/#{key}")
+    end
+  end
+end
diff --git a/logstash-core/spec/api/lib/commands/events_spec.rb b/logstash-core/spec/api/lib/commands/events_spec.rb
deleted file mode 100644
index 9bbcc3e7aa8..00000000000
--- a/logstash-core/spec/api/lib/commands/events_spec.rb
+++ /dev/null
@@ -1,17 +0,0 @@
-# encoding: utf-8
-require_relative "../../spec_helper"
-require "app/commands/stats/events_command"
-
-describe LogStash::Api::StatsEventsCommand do
-
-  context "#schema" do
-
-    let(:report) do
-      do_request { subject.run }
-    end
-
-    it "return events information" do
-      expect(report).to include("in", "filtered", "out")
-    end
-  end
-end
diff --git a/logstash-core/spec/api/lib/commands/jvm_spec.rb b/logstash-core/spec/api/lib/commands/jvm_spec.rb
deleted file mode 100644
index e3f01d00aaf..00000000000
--- a/logstash-core/spec/api/lib/commands/jvm_spec.rb
+++ /dev/null
@@ -1,45 +0,0 @@
-# encoding: utf-8
-require_relative "../../spec_helper"
-require "app/commands/stats/hotthreads_command"
-require "app/commands/stats/memory_command"
-
-describe "JVM stats" do
-
-  describe LogStash::Api::HotThreadsCommand do
-
-    let(:report) do
-      do_request { subject.run }
-    end
-
-    context "#schema" do
-      it "return hot threads information" do
-        report = do_request { subject.run }
-        expect(report.to_s).not_to be_empty
-      end
-
-    end
-  end
-
-  describe LogStash::Api::JvmMemoryCommand do
-
-    context "#schema" do
-
-      let(:report) do
-        do_request { subject.run }
-      end
-
-      it "return hot threads information" do
-        expect(report).not_to be_empty
-      end
-
-      it "return heap information" do
-        expect(report.keys).to include(:heap_used_in_bytes)
-      end
-
-      it "return non heap information" do
-        expect(report.keys).to include(:non_heap_used_in_bytes)
-      end
-
-    end
-  end
-end
diff --git a/logstash-core/spec/api/lib/commands/stats.rb b/logstash-core/spec/api/lib/commands/stats.rb
new file mode 100644
index 00000000000..3059e1460f3
--- /dev/null
+++ b/logstash-core/spec/api/lib/commands/stats.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+
+describe LogStash::Api::Commands::Stats do
+
+  let(:report_method) { :run }
+  subject(:report) { do_request { report_class.new.send(report_method) } }
+
+  let(:report_class) { described_class }
+
+  describe "#events" do
+    let(:report_method) { :events }
+
+    it "return events information" do
+      expect(report.keys).to include(:in, :filtered, :out)
+    end
+  end
+  
+  describe "#hot_threads" do
+    let(:report_method) { :hot_threads }
+    
+    it "should return hot threads information as a string" do
+      expect(report.to_s).to be_a(String)
+    end
+
+    it "should return hot threads info as a hash" do
+      expect(report.to_hash).to be_a(Hash)
+    end
+  end
+
+  describe "memory stats" do
+    let(:report_method) { :memory }
+      
+    it "return hot threads information" do
+      expect(report).not_to be_empty
+    end
+
+    it "return heap information" do
+      expect(report.keys).to include(:heap_used_in_bytes)
+    end
+
+    it "return non heap information" do
+      expect(report.keys).to include(:non_heap_used_in_bytes)
+    end
+
+  end
+end
diff --git a/logstash-core/spec/api/lib/errors_spec.rb b/logstash-core/spec/api/lib/errors_spec.rb
new file mode 100644
index 00000000000..430671402d0
--- /dev/null
+++ b/logstash-core/spec/api/lib/errors_spec.rb
@@ -0,0 +1,27 @@
+# encoding: utf-8
+require_relative "../spec_helper"
+require "logstash/api/errors"
+
+describe LogStash::Api::ApiError do
+  subject { described_class.new }
+
+  it "#status_code returns 500" do
+    expect(subject.status_code).to eq(500)
+  end
+
+  it "#to_hash return the message of the exception" do
+    expect(subject.to_hash).to include(:message => "Api Error")
+  end
+end
+
+describe LogStash::Api::NotFoundError do
+  subject { described_class.new }
+
+  it "#status_code returns 404" do
+    expect(subject.status_code).to eq(404)
+  end
+
+  it "#to_hash return the message of the exception" do
+    expect(subject.to_hash).to include(:message => "Not Found")
+  end
+end
diff --git a/logstash-core/spec/api/lib/rack_app_spec.rb b/logstash-core/spec/api/lib/rack_app_spec.rb
new file mode 100644
index 00000000000..9c6af8679e9
--- /dev/null
+++ b/logstash-core/spec/api/lib/rack_app_spec.rb
@@ -0,0 +1,90 @@
+require "logstash/api/rack_app"
+require "rack/test"
+
+describe LogStash::Api::RackApp do
+  include Rack::Test::Methods
+
+  class DummyApp
+    class RaisedError < StandardError; end
+
+    def call(env)
+      case env["PATH_INFO"]
+      when "/good-page"
+        [200, {}, ["200 OK"]]
+      when "/service-unavailable"
+        [503, {}, ["503 service unavailable"]]
+      when "/raise-error"
+        raise RaisedError, "Error raised"
+      else
+        [404, {}, ["404 Page not found"]]
+      end
+    end
+  end
+
+  let(:logger) { double("logger") }
+
+  describe LogStash::Api::RackApp::ApiErrorHandler do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+
+    it "should let good requests through as normal" do
+      get "/good-page"
+      expect(last_response).to be_ok
+    end
+
+    it "should let through 5xx codes" do
+      get "/service-unavailable"
+      expect(last_response.status).to eql(503)
+    end
+
+    describe "raised exceptions" do
+      before do
+        allow(logger).to receive(:error).with(any_args)
+        get "/raise-error"
+      end
+
+      it "should return a 500 error" do
+        expect(last_response.status).to eql(500)
+      end
+
+      it "should return valid JSON" do
+        expect { LogStash::Json.load(last_response.body) }.not_to raise_error
+      end
+
+      it "should log the error" do
+        expect(logger).to have_received(:error).with(LogStash::Api::RackApp::ApiErrorHandler::LOG_MESSAGE, anything).once
+      end
+    end
+  end
+
+  describe LogStash::Api::RackApp::ApiLogger do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+
+    it "should log good requests as info" do
+      expect(logger).to receive(:debug?).and_return(true)
+      expect(logger).to receive(:debug).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/good-page"
+    end
+
+    it "should log 5xx requests as warnings" do
+      expect(logger).to receive(:error?).and_return(true)
+      expect(logger).to receive(:error).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/service-unavailable"
+    end
+  end
+end
diff --git a/logstash-core/spec/api/spec_helper.rb b/logstash-core/spec/api/spec_helper.rb
index 90a1bb1e378..193eff2d916 100644
--- a/logstash-core/spec/api/spec_helper.rb
+++ b/logstash-core/spec/api/spec_helper.rb
@@ -1,16 +1,17 @@
 # encoding: utf-8
 API_ROOT = File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "lib", "logstash", "api"))
 
+require "stud/task"
 require "logstash/devutils/rspec/spec_helper"
-
+$LOAD_PATH.unshift(File.expand_path(File.dirname(__FILE__)))
+require "lib/api/support/resource_dsl_methods"
+require_relative "../support/mocks_classes"
+require 'rspec/expectations'
+require "logstash/settings"
 require 'rack/test'
 require 'rspec'
 require "json"
 
-ENV['RACK_ENV'] = 'test'
-
-Rack::Builder.parse_file(File.join(API_ROOT, 'init.ru'))
-
 def read_fixture(name)
   path = File.join(File.dirname(__FILE__), "fixtures", name)
   File.read(path)
@@ -18,11 +19,11 @@ def read_fixture(name)
 
 module LogStash
   class DummyAgent < Agent
-    def fetch_config(settings)
-      "input { generator {count => 0} } output { }"
+    def start_webserver
+      http_address = "#{Socket.gethostname}:#{::LogStash::WebServer::DEFAULT_PORTS.first}"
+      @webserver = Struct.new(:address).new(http_address)
+      self.metric.gauge([], :http_address, http_address)
     end
-
-    def start_webserver; end
     def stop_webserver; end
   end
 end
@@ -30,58 +31,47 @@ def stop_webserver; end
 ##
 # Class used to wrap and manage the execution of an agent for test,
 # this helps a lot in order to have a more integrated test for the
-# web api, could be also used for other use cases if generalized enought
+# web api, could be also used for other use cases if generalized enough
 ##
 class LogStashRunner
 
   attr_reader :config_str, :agent, :pipeline_settings
 
   def initialize
-    args = [
-      :logger => Cabin::Channel.get(LogStash),
-      :auto_reload => false,
-      :collect_metric => true,
-      :debug => false,
-      :node_name => "test_agent",
-      :web_api_http_port => rand(9600..9700)
-    ]
-
-    @config_str   = "input { generator {count => 0} } output { }"
-    @agent = LogStash::DummyAgent.new(*args)
-    @pipeline_settings ||= { :pipeline_id => "main",
-                             :config_str => config_str,
-                            :pipeline_batch_size => 1,
-                            :flush_interval => 1,
-                            :pipeline_workers => 1 }
+    @config_str   = "input { generator {count => 100 } } output { dummyoutput {} }"
+
+    args = {
+      "config.reload.automatic" => false,
+      "metric.collect" => true,
+      "log.level" => "debug",
+      "node.name" => "test_agent",
+      "http.port" => rand(9600..9700),
+      "http.environment" => "test",      
+      "config.string" => @config_str,
+      "pipeline.batch.size" => 1,
+      "pipeline.workers" => 1
+    }
+    @settings = ::LogStash::SETTINGS.clone.merge(args)
+
+    @agent = LogStash::DummyAgent.new(@settings)
   end
 
   def start
-    agent.register_pipeline("main", pipeline_settings)
-    @runner = Thread.new(agent) do |_agent|
-      _agent.execute
-    end
-    wait_until_snapshot_received
+    # We start a pipeline that will generate a finite number of events
+    # before starting the expectations
+    agent.register_pipeline(@settings)
+    @agent_task = Stud::Task.new { agent.execute }
+    @agent_task.wait
   end
 
   def stop
     agent.shutdown
-    Thread.kill(@runner)
-    sleep 0.1 while !@runner.stop?
-  end
-
-  private
-
-  def wait_until_snapshot_received
-    while !LogStash::Api::Service.instance.started? do
-      sleep 0.5
-    end
   end
 end
 
-
 ##
 # Method used to wrap up a request in between of a running
-# pipeline, this makes the hole execution model easier and
+# pipeline, this makes the whole execution model easier and
 # more contained as some threads might go wild.
 ##
 def do_request(&block)
@@ -92,30 +82,6 @@ def do_request(&block)
   ret_val
 end
 
-##
-# Helper module that setups necessary mocks when doing the requests,
-# this could be just included in the test and the runner will be
-# started managed for all tests.
-##
-module LogStash; module RSpec; module RunnerConfig
-  def self.included(klass)
-    klass.before(:all) do
-      LogStashRunner.instance.start
-    end
-
-    klass.before(:each) do
-      runner = LogStashRunner.instance
-      allow(LogStash::Instrument::Collector.instance).to receive(:agent).and_return(runner.agent)
-    end
-
-    klass.after(:all) do
-      LogStashRunner.instance.stop
-    end
-  end
-end; end; end
-
-require 'rspec/expectations'
-
 RSpec::Matchers.define :be_available? do
   match do |plugin|
     begin
@@ -126,3 +92,20 @@ def self.included(klass)
     end
   end
 end
+
+shared_context "api setup" do
+  before :all do
+    @runner = LogStashRunner.new
+    @runner.start
+  end
+  
+  after :all do
+    @runner.stop
+  end
+
+  include Rack::Test::Methods
+
+  def app()
+    described_class.new(nil, @runner.agent)
+  end
+end
diff --git a/logstash-core/spec/conditionals_spec.rb b/logstash-core/spec/conditionals_spec.rb
index dab6fc901e3..80a4bd7bf25 100644
--- a/logstash-core/spec/conditionals_spec.rb
+++ b/logstash-core/spec/conditionals_spec.rb
@@ -25,6 +25,19 @@ def conditional(expression, &block)
 describe "conditionals in output" do
   extend ConditionalFanciness
 
+  class DummyNullOutput < LogStash::Outputs::Base
+    config_name "dummynull"
+
+    def register
+    end
+    def multi_receive(events)
+    end
+  end
+
+  before do
+    LogStash::PLUGIN_REGISTRY.add(:output, "dummynull", DummyNullOutput)
+  end
+
   describe "simple" do
     config <<-CONFIG
       input {
@@ -35,7 +48,7 @@ def conditional(expression, &block)
       }
       output {
         if [foo] == "bar" {
-          stdout { }
+          dummynull { }
         }
       }
     CONFIG
@@ -64,24 +77,24 @@ def conditional(expression, &block)
     CONFIG
 
     sample({"foo" => "bar"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to eq("world")
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to eq("world")
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample({"notfoo" => "bar"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to eq("hugs")
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to eq("hugs")
     end
 
     sample({"bar" => "baz"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to eq("pants")
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to eq("pants")
+      expect(subject.get("free")).to be_nil
     end
   end
 
@@ -102,31 +115,31 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => "bar", "nest" => 124) do
-      expect(subject["always"]).to be_nil
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to be_nil
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample("foo" => "bar", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to eq("world")
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to eq("world")
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample("notfoo" => "bar", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to eq("hugs")
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to eq("hugs")
     end
 
     sample("bar" => "baz", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to eq("pants")
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to eq("pants")
+      expect(subject.get("free")).to be_nil
     end
   end
 
@@ -140,7 +153,7 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => 123, "bar" => 123) do
-      expect(subject["tags"] ).to include("woot")
+      expect(subject.get("tags") ).to include("woot")
     end
   end
 
@@ -169,12 +182,12 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => "foo", "foobar" => "foobar", "greeting" => "hello world") do
-      expect(subject["tags"]).to include("field in field")
-      expect(subject["tags"]).to include("field in string")
-      expect(subject["tags"]).to include("string in field")
-      expect(subject["tags"]).to include("field in list")
-      expect(subject["tags"]).not_to include("shouldnotexist")
-      expect(subject["tags"]).to include("shouldexist")
+      expect(subject.get("tags")).to include("field in field")
+      expect(subject.get("tags")).to include("field in string")
+      expect(subject.get("tags")).to include("string in field")
+      expect(subject.get("tags")).to include("field in list")
+      expect(subject.get("tags")).not_to include("shouldnotexist")
+      expect(subject.get("tags")).to include("shouldexist")
     end
   end
 
@@ -192,107 +205,107 @@ def conditional(expression, &block)
 
     sample("foo" => "foo", "somelist" => [ "one", "two" ], "foobar" => "foobar", "greeting" => "hello world", "tags" => [ "fancypantsy" ]) do
       # verify the original exists
-      expect(subject["tags"]).to include("fancypantsy")
+      expect(subject.get("tags")).to include("fancypantsy")
 
-      expect(subject["tags"]).to include("baz")
-      expect(subject["tags"]).not_to include("foo")
-      expect(subject["tags"]).to include("notfoo")
-      expect(subject["tags"]).to include("notsomelist")
-      expect(subject["tags"]).not_to include("somelist")
-      expect(subject["tags"]).to include("no string in missing field")
+      expect(subject.get("tags")).to include("baz")
+      expect(subject.get("tags")).not_to include("foo")
+      expect(subject.get("tags")).to include("notfoo")
+      expect(subject.get("tags")).to include("notsomelist")
+      expect(subject.get("tags")).not_to include("somelist")
+      expect(subject.get("tags")).to include("no string in missing field")
     end
   end
 
   describe "operators" do
     conditional "[message] == 'sample'" do
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("different") { expect(subject["tags"] ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("different") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] != 'sample'" do
-      sample("sample") { expect(subject["tags"] ).to include("failure") }
-      sample("different") { expect(subject["tags"] ).to include("success") }
+      sample("sample") { expect(subject.get("tags") ).to include("failure") }
+      sample("different") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] < 'sample'" do
-      sample("apple") { expect(subject["tags"] ).to include("success") }
-      sample("zebra") { expect(subject["tags"] ).to include("failure") }
+      sample("apple") { expect(subject.get("tags") ).to include("success") }
+      sample("zebra") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] > 'sample'" do
-      sample("zebra") { expect(subject["tags"] ).to include("success") }
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
+      sample("zebra") { expect(subject.get("tags") ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] <= 'sample'" do
-      sample("apple") { expect(subject["tags"] ).to include("success") }
-      sample("zebra") { expect(subject["tags"] ).to include("failure") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("success") }
+      sample("zebra") { expect(subject.get("tags") ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] >= 'sample'" do
-      sample("zebra") { expect(subject["tags"] ).to include("success") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
+      sample("zebra") { expect(subject.get("tags") ).to include("success") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] =~ /sample/" do
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("some sample") { expect(subject["tags"] ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("some sample") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] !~ /sample/" do
-      sample("apple") { expect(subject["tags"]).to include("success") }
-      sample("sample") { expect(subject["tags"]).to include("failure") }
-      sample("some sample") { expect(subject["tags"]).to include("failure") }
+      sample("apple") { expect(subject.get("tags")).to include("success") }
+      sample("sample") { expect(subject.get("tags")).to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).to include("failure") }
     end
 
   end
 
   describe "negated expressions" do
     conditional "!([message] == 'sample')" do
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("different") { expect(subject["tags"]).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("different") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] != 'sample')" do
-      sample("sample") { expect(subject["tags"]).not_to include("failure") }
-      sample("different") { expect(subject["tags"]).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("failure") }
+      sample("different") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] < 'sample')" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("zebra") { expect(subject["tags"]).not_to include("failure") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] > 'sample')" do
-      sample("zebra") { expect(subject["tags"]).not_to include("success") }
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] <= 'sample')" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("zebra") { expect(subject["tags"]).not_to include("failure") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success")}
     end
 
     conditional "!([message] >= 'sample')" do
-      sample("zebra") { expect(subject["tags"]).not_to include("success") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] =~ /sample/)" do
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("some sample") { expect(subject["tags"]).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("some sample") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] !~ /sample/)" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("sample") { expect(subject["tags"]).not_to include("failure") }
-      sample("some sample") { expect(subject["tags"]).not_to include("failure") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).not_to include("failure") }
     end
 
   end
@@ -300,47 +313,47 @@ def conditional(expression, &block)
   describe "value as an expression" do
     # testing that a field has a value should be true.
     conditional "[message]" do
-      sample("apple") { expect(subject["tags"]).to include("success") }
-      sample("sample") { expect(subject["tags"]).to include("success") }
-      sample("some sample") { expect(subject["tags"]).to include("success") }
+      sample("apple") { expect(subject.get("tags")).to include("success") }
+      sample("sample") { expect(subject.get("tags")).to include("success") }
+      sample("some sample") { expect(subject.get("tags")).to include("success") }
     end
 
     # testing that a missing field has a value should be false.
     conditional "[missing]" do
-      sample("apple") { expect(subject["tags"]).to include("failure") }
-      sample("sample") { expect(subject["tags"]).to include("failure") }
-      sample("some sample") { expect(subject["tags"]).to include("failure") }
+      sample("apple") { expect(subject.get("tags")).to include("failure") }
+      sample("sample") { expect(subject.get("tags")).to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).to include("failure") }
     end
   end
 
   describe "logic operators" do
     describe "and" do
       conditional "[message] and [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "[message] and ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
       conditional "![message] and [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
       conditional "![message] and ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
     end
 
     describe "or" do
       conditional "[message] or [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "[message] or ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "![message] or [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "![message] or ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
     end
   end
@@ -348,19 +361,19 @@ def conditional(expression, &block)
   describe "field references" do
     conditional "[field with space]" do
       sample("field with space" => "hurray") do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
 
     conditional "[field with space] == 'hurray'" do
       sample("field with space" => "hurray") do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
 
     conditional "[nested field][reference with][some spaces] == 'hurray'" do
       sample({"nested field" => { "reference with" => { "some spaces" => "hurray" } } }) do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
   end
@@ -385,13 +398,13 @@ def conditional(expression, &block)
       expect(subject).to be_an(Array)
       expect(subject.length).to eq(2)
 
-      expect(subject[0]["type"]).to eq("original")
-      expect(subject[0]["cond1"]).to eq("true")
-      expect(subject[0]["cond2"]).to eq(nil)
+      expect(subject[0].get("type")).to eq("original")
+      expect(subject[0].get("cond1")).to eq("true")
+      expect(subject[0].get("cond2")).to eq(nil)
 
-      expect(subject[1]["type"]).to eq("clone")
-      # expect(subject[1]["cond1"]).to eq(nil)
-      # expect(subject[1]["cond2"]).to eq("true")
+      expect(subject[1].get("type")).to eq("clone")
+      # expect(subject[1].get("cond1")).to eq(nil)
+      # expect(subject[1].get("cond2")).to eq("true")
     end
   end
 
@@ -413,17 +426,16 @@ def conditional(expression, &block)
 
     sample({"type" => "original"}) do
       # puts subject.inspect
-      expect(subject[0]["cond1"]).to eq(nil)
-      expect(subject[0]["cond2"]).to eq(nil)
+      expect(subject[0].get("cond1")).to eq(nil)
+      expect(subject[0].get("cond2")).to eq(nil)
 
-      expect(subject[1]["type"]).to eq("clone1")
-      expect(subject[1]["cond1"]).to eq("true")
-      expect(subject[1]["cond2"]).to eq(nil)
+      expect(subject[1].get("type")).to eq("clone1")
+      expect(subject[1].get("cond1")).to eq("true")
+      expect(subject[1].get("cond2")).to eq(nil)
 
-      expect(subject[2]["type"]).to eq("clone2")
-      expect(subject[2]["cond1"]).to eq(nil)
-      expect(subject[2]["cond2"]).to eq("true")
+      expect(subject[2].get("type")).to eq("clone2")
+      expect(subject[2].get("cond1")).to eq(nil)
+      expect(subject[2].get("cond2")).to eq("true")
     end
   end
-
 end
diff --git a/logstash-core/spec/logstash/acked_queue_concurrent_stress_spec.rb b/logstash-core/spec/logstash/acked_queue_concurrent_stress_spec.rb
new file mode 100644
index 00000000000..815da9a9df4
--- /dev/null
+++ b/logstash-core/spec/logstash/acked_queue_concurrent_stress_spec.rb
@@ -0,0 +1,291 @@
+# encoding: utf-8
+require "logstash/util/wrapped_acked_queue"
+require "logstash/event"
+require "logstash/instrument/namespaced_metric"
+
+describe LogStash::Util::WrappedAckedQueue, :stress_test => true do
+  let(:path) { Stud::Temporary.directory }
+
+  context "with multiple writers" do
+    let(:items) { expected_count / writers }
+    let(:page_capacity) { 1 << page_capacity_multiplier }
+    let(:queue_capacity) { page_capacity * queue_capacity_multiplier }
+
+    let(:output_strings) { [] }
+    let(:reject_memo_keys) { [:reject_memo_keys, :path, :queue, :writer_threads, :collector, :metric, :reader_threads, :output_strings] }
+
+    let(:queue) do
+      described_class.create_file_based(path, page_capacity, 0, queue_checkpoint_acks, queue_checkpoint_writes, queue_checkpoint_interval, queue_capacity)
+    end
+
+    let(:writer_threads) do
+      writer = queue.write_client
+      writers.times.map do |i|
+        Thread.new(i, items, writer) do |_i, _items, _writer|
+          publisher(_items, _writer)
+        end
+      end
+    end
+
+    let(:writers_finished) { Concurrent::AtomicBoolean.new(false) }
+
+    let(:reader_threads) do
+      reader = queue.read_client
+      reader.set_batch_dimensions(batch_size, batch_wait)
+      reader.set_events_metric(metric.namespace([:stats, :events]))
+      reader.set_pipeline_metric(metric.namespace([:stats, :pipelines, :main, :events]))
+
+      readers.times.map do |i|
+        Thread.new(i, reader, counts) do |_i, _reader, _counts|
+          begin
+            tally = 0
+            while true
+              batch = _reader.read_batch
+              break if batch.size.zero? && writers_finished.value == true && queue.queue.is_fully_acked?
+              sleep(rand * 0.01) if simulate_work
+              tally += batch.size
+              batch.close
+            end
+            _counts[_i] = tally
+            # puts("reader #{_i}, tally=#{tally}, _counts=#{_counts.inspect}")
+          rescue => e
+            p :reader_error => e
+          end
+        end
+      end
+    end
+
+    def publisher(items, writer)
+      items.times.each do |i|
+        event = LogStash::Event.new("sequence" => "#{i}".ljust(string_size))
+        writer.push(event)
+      end
+    rescue => e
+      p :publisher_error => e
+    end
+
+    let(:collector) { LogStash::Instrument::Collector.new }
+    let(:metric) { LogStash::Instrument::Metric.new(collector) }
+
+    shared_examples "a well behaved queue" do
+      it "writes, reads, closes and reopens" do
+        Thread.abort_on_exception = true
+
+        # force lazy initialization to avoid concurency issues within threads
+        counts
+        queue
+
+        # Start the threads
+        writer_threads
+        reader_threads
+
+        writer_threads.each(&:join)
+        writers_finished.make_true
+
+        reader_threads.each(&:join)
+
+        enqueued = queue.queue.unread_count
+
+        if enqueued != 0
+          output_strings << "unread events in queue: #{enqueued}"
+        end
+
+        got = counts.reduce(&:+)
+
+        if got != expected_count
+          # puts("count=#{counts.inspect}")
+          output_strings << "events read: #{got}"
+        end
+
+        sleep 0.1
+        expect { queue.close }.not_to raise_error
+        sleep 0.1
+        files = Dir.glob(path + '/*').map{|f| f.sub("#{path}/", '')}
+        if files.count != 2
+          output_strings << "File count after close mismatch expected: 2 got: #{files.count}"
+          output_strings.concat files
+        end
+
+        begin
+          queue.queue.open
+        rescue Exception => e
+          output_strings << e.message
+        end
+
+        queue.queue.close
+
+        if output_strings.any?
+          output_strings << __memoized.reject{|k,v| reject_memo_keys.include?(k)}.inspect
+        end
+
+        expect(output_strings).to eq([])
+      end
+    end
+
+    let(:writers) { 3 }
+    let(:readers) { 3 }
+    let(:simulate_work) { true }
+    let(:counts) { Concurrent::Array.new([0, 0, 0, 0, 0, 0, 0, 0]) }
+    let(:page_capacity_multiplier) { 20 }
+    let(:queue_capacity_multiplier) { 128 }
+    let(:queue_checkpoint_acks) { 1024 }
+    let(:queue_checkpoint_writes) { 1024 }
+    let(:queue_checkpoint_interval) { 1000 }
+    let(:batch_size) { 500 }
+    let(:batch_wait) { 1000 }
+    let(:expected_count) { 60000 }
+    let(:string_size) { 256 }
+
+    describe "with simulate_work ON" do
+      let(:simulate_work) { true }
+
+      context "> more writers than readers <" do
+        let(:writers) { 4 }
+        let(:readers) { 2 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> less writers than readers <" do
+        let(:writers) { 2 }
+        let(:readers) { 4 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger checkpoint acks <" do
+        let(:queue_checkpoint_acks) { 3000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller checkpoint acks <" do
+        let(:queue_checkpoint_acks) { 500 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger checkpoint writes <" do
+        let(:queue_checkpoint_writes) { 3000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller checkpoint writes <" do
+        let(:queue_checkpoint_writes) { 500 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger checkpoint interval <" do
+        let(:queue_checkpoint_interval) { 3000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller checkpoint interval <" do
+        let(:queue_checkpoint_interval) { 500 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller batch wait <" do
+        let(:batch_wait) { 125 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger batch wait <" do
+        let(:batch_wait) { 5000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller event size <" do
+        let(:string_size) { 8 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger event size <" do
+        let(:string_size) { 8192 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> small queue size limit <" do
+        let(:queue_capacity_multiplier) { 10 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> very large queue size limit <" do
+        let(:queue_capacity_multiplier) { 512 }
+        it_behaves_like "a well behaved queue"
+      end
+    end
+
+    describe "with simulate_work OFF" do
+      let(:simulate_work) { false }
+
+      context "> more writers than readers <" do
+        let(:writers) { 4 }
+        let(:readers) { 2 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> less writers than readers <" do
+        let(:writers) { 2 }
+        let(:readers) { 4 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger checkpoint acks <" do
+        let(:queue_checkpoint_acks) { 3000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller checkpoint acks <" do
+        let(:queue_checkpoint_acks) { 500 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger checkpoint writes <" do
+        let(:queue_checkpoint_writes) { 3000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller checkpoint writes <" do
+        let(:queue_checkpoint_writes) { 500 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger checkpoint interval <" do
+        let(:queue_checkpoint_interval) { 3000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller checkpoint interval <" do
+        let(:queue_checkpoint_interval) { 500 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller batch wait <" do
+        let(:batch_wait) { 125 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger batch wait <" do
+        let(:batch_wait) { 5000 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> smaller event size <" do
+        let(:string_size) { 8 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> larger event size <" do
+        let(:string_size) { 8192 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> small queue size limit <" do
+        let(:queue_capacity_multiplier) { 10 }
+        it_behaves_like "a well behaved queue"
+      end
+
+      context "> very large queue size limit <" do
+        let(:queue_capacity_multiplier) { 512 }
+        it_behaves_like "a well behaved queue"
+      end
+    end
+  end
+end
\ No newline at end of file
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index b7ad9065e04..e90c36e3350 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -1,188 +1,371 @@
 # encoding: utf-8
-require 'spec_helper'
-require 'stud/temporary'
+require "spec_helper"
+require "stud/temporary"
+require "logstash/inputs/generator"
+require_relative "../support/mocks_classes"
+require "fileutils"
+require_relative "../support/helpers"
 
 describe LogStash::Agent do
 
+  let(:agent_settings) { LogStash::SETTINGS }
+  let(:default_pipeline_id) { LogStash::SETTINGS.get("pipeline.id") }
+  let(:agent_args) { {} }
+  let(:pipeline_settings) { agent_settings.clone }
+  let(:pipeline_args) { {} }
+  let(:config_file) { Stud::Temporary.pathname }
+  let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
   let(:logger) { double("logger") }
-  let(:agent_args) { { :logger => logger } }
-  subject { LogStash::Agent.new(agent_args) }
+
+  subject { LogStash::Agent.new(agent_settings) }
 
   before :each do
-    [:info, :warn, :error, :fatal, :debug].each do |level|
-      allow(logger).to receive(level)
+    # This MUST run first, before `subject` is invoked to ensure clean state
+    clear_data_dir
+
+    File.open(config_file, "w") { |f| f.puts config_file_txt }
+    agent_args.each do |key, value|
+      agent_settings.set(key, value)
+      pipeline_settings.set(key, value)
     end
-    [:info?, :warn?, :error?, :fatal?, :debug?].each do |level|
-      allow(logger).to receive(level)
+    pipeline_args.each do |key, value|
+      pipeline_settings.set(key, value)
     end
+    allow(described_class).to receive(:logger).and_return(logger)
+    [:debug, :info, :error, :warn, :fatal, :trace].each {|level| allow(logger).to receive(level) }
+    [:debug?, :info?, :error?, :warn?, :fatal?, :trace?].each {|level| allow(logger).to receive(level) }
+  end
+
+  after :each do
+    LogStash::SETTINGS.reset
+    File.unlink(config_file)
+  end
+
+  it "fallback to hostname when no name is provided" do
+    expect(LogStash::Agent.new.name).to eq(Socket.gethostname)
   end
 
   describe "register_pipeline" do
-    let(:pipeline_id) { "main" }
-    let(:settings) { {
-      :config_string => "input { } filter { } output { }",
-      :pipeline_workers => 4
-    } }
+    let(:config_string) { "input { } filter { } output { }" }
+    let(:agent_args) do
+      {
+        "config.string" => config_string,
+        "config.reload.automatic" => true,
+        "config.reload.interval" => 0.01,
+        "pipeline.workers" => 4,
+      }
+    end
 
-    let(:agent_args) { {
-      :logger => logger,
-      :auto_reload => false,
-      :reload_interval => 0.01
-    } }
+    after(:each) do
+      subject.close_pipelines
+    end
 
     it "should delegate settings to new pipeline" do
-      expect(LogStash::Pipeline).to receive(:new).with(settings[:config_string], hash_including(settings))
-      subject.register_pipeline(pipeline_id, settings)
+      expect(LogStash::Pipeline).to receive(:new) do |arg1, arg2|
+        expect(arg1).to eq(config_string)
+        expect(arg2.to_hash).to include(agent_args)
+      end
+      subject.register_pipeline(agent_settings)
+    end
+  end
+
+  describe "#id" do
+    let(:config_file_txt) { "" }
+    let(:id_file_data) { File.open(subject.id_path) {|f| f.read } }
+
+    it "should return a UUID" do
+      expect(subject.id).to be_a(String)
+      expect(subject.id.size).to be > 0
+    end
+
+    it "should write out the persistent UUID" do
+      expect(id_file_data).to eql(subject.id)
     end
   end
 
   describe "#execute" do
-    let(:sample_config) { "input { generator { count => 100000 } } output { stdout { } }" }
-    let(:config_file) { Stud::Temporary.pathname }
+    let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
 
     before :each do
       allow(subject).to receive(:start_webserver).and_return(false)
       allow(subject).to receive(:stop_webserver).and_return(false)
-      File.open(config_file, "w") { |f| f.puts sample_config }
-    end
-
-    after :each do
-      File.unlink(config_file)
     end
 
     context "when auto_reload is false" do
-      let(:agent_args) { { :logger => logger, :auto_reload => false, :reload_interval => 0.01, :config_path => config_file } }
+      let(:agent_args) do
+        {
+          "config.reload.automatic" => false,
+          "path.config" => config_file
+        }
+      end
 
-      before :each do
-        allow(subject).to receive(:sleep)
-        allow(subject).to receive(:clean_state?).and_return(false)
-        allow(subject).to receive(:running_pipelines?).and_return(true)
+      before(:each) do
+        subject.register_pipeline(pipeline_settings)
+      end
+
+      context "when a system pipeline is running" do
+        context "when one pipeline is finite" do
+          let(:pipeline_args) {
+            {
+              "path.config" => "a",
+              "config.string" => "input { generator { count => 1000 }} output { null {} }"
+            }
+          }
+          let(:system_pipeline_settings) do
+            s = agent_settings.clone
+            s.set("path.config", "")
+            s.set("config.string", "input { generator {}} output { null {} }")
+            s.set("pipeline.id", ".monitoring")
+            s.set("pipeline.system", true)
+            s
+          end
+
+          it "stops logstash at the end of the execution of the finite pipeline" do
+            subject.register_pipeline(system_pipeline_settings)
+            expect(subject.execute).to be_nil
+          end
+        end
       end
 
       context "if state is clean" do
+        before :each do
+          allow(subject).to receive(:running_pipelines?).and_return(true)
+          allow(subject).to receive(:sleep)
+          allow(subject).to receive(:clean_state?).and_return(false)
+        end
+
         it "should not reload_state!" do
           expect(subject).to_not receive(:reload_state!)
           t = Thread.new { subject.execute }
-          sleep 0.1
+
           Stud.stop!(t)
           t.join
+          subject.shutdown
+        end
+      end
+
+      context "when calling reload_state!" do
+        context "with a pipeline with auto reloading turned off" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+          let(:pipeline_args) { { "config.reload.automatic" => false } }
+
+          it "does not try to reload the pipeline" do
+            t = Thread.new { subject.execute }
+            sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
+            expect(subject).to_not receive(:reload_pipeline!)
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.reload_state!
+
+            # TODO: refactor this. forcing an arbitrary fixed delay for thread concurrency issues is an indication of
+            # a bad test design or missing class functionality.
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
+        end
+
+        context "with a pipeline with auto reloading turned on" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+          let(:pipeline_args) { { "config.reload.automatic" => true } }
+
+          it "tries to reload the pipeline" do
+            t = Thread.new { subject.execute }
+            sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
+            expect(subject).to receive(:reload_pipeline!).once.and_call_original
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.reload_state!
+
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
         end
       end
     end
 
     context "when auto_reload is true" do
-      let(:agent_args) { { :logger => logger, :auto_reload => true, :reload_interval => 0.01 } }
+      let(:agent_args) do
+        {
+          "config.reload.automatic" => true,
+          "config.reload.interval" => 0.01,
+          "path.config" => config_file,
+        }
+      end
+
+      before(:each) do
+        subject.register_pipeline(pipeline_settings)
+      end
+
       context "if state is clean" do
         it "should periodically reload_state" do
           allow(subject).to receive(:clean_state?).and_return(false)
-          expect(subject).to receive(:reload_state!).at_least(3).times
           t = Thread.new { subject.execute }
-          sleep 0.1
+          sleep(0.01) until subject.running_pipelines? && subject.pipelines.values.first.running?
+
+          expect(subject).to receive(:reload_state!).at_least(2).times
+
+          sleep 1
+
           Stud.stop!(t)
           t.join
+          subject.shutdown
         end
       end
     end
+
+    context "when calling reload_state!" do
+    end
   end
 
   describe "#reload_state!" do
-    let(:pipeline_id) { "main" }
     let(:first_pipeline_config) { "input { } filter { } output { }" }
-    let(:second_pipeline_config) { "input { generator {} } filter { } output { }" }
-    let(:pipeline_settings) { {
-      :config_string => first_pipeline_config,
-      :pipeline_workers => 4
+    let(:second_pipeline_config) { "input { generator { count => 10000 } } filter { } output { }" }
+    let(:pipeline_args) { {
+      "config.string" => first_pipeline_config,
+      "pipeline.workers" => 4,
+      "config.reload.automatic" => true
     } }
 
     before(:each) do
-      subject.register_pipeline(pipeline_id, pipeline_settings)
+      subject.register_pipeline(pipeline_settings)
+    end
+
+    after(:each) do
+      subject.close_pipelines
     end
 
     context "when fetching a new state" do
       it "upgrades the state" do
         expect(subject).to receive(:fetch_config).and_return(second_pipeline_config)
-        expect(subject).to receive(:upgrade_pipeline).with(pipeline_id, kind_of(LogStash::Pipeline))
-        subject.send(:reload_state!)
+        expect(subject).to receive(:upgrade_pipeline).with(default_pipeline_id, kind_of(LogStash::Settings), second_pipeline_config)
+        subject.reload_state!
       end
     end
     context "when fetching the same state" do
       it "doesn't upgrade the state" do
         expect(subject).to receive(:fetch_config).and_return(first_pipeline_config)
         expect(subject).to_not receive(:upgrade_pipeline)
-        subject.send(:reload_state!)
+        subject.reload_state!
+      end
+    end
+
+    context "with a config that contains reload incompatible plugins" do
+      let(:second_pipeline_config) { "input { stdin {} } filter { } output { }" }
+
+      it "does not upgrade the new config" do
+        expect(subject).to receive(:fetch_config).and_return(second_pipeline_config)
+        expect(subject).to_not receive(:upgrade_pipeline)
+        subject.reload_state!
+      end
+    end
+  end
+
+  describe "Environment Variables In Configs" do
+    let(:pipeline_config) { "input { generator { message => '${FOO}-bar' } } filter { } output { }" }
+    let(:agent_args) { {
+      "config.reload.automatic" => false,
+      "config.reload.interval" => 0.01,
+      "config.string" => pipeline_config
+    } }
+
+    context "environment variable templating" do
+      before :each do
+        @foo_content = ENV["FOO"]
+        ENV["FOO"] = "foo"
+      end
+
+      after :each do
+        ENV["FOO"] = @foo_content
+        subject.close_pipelines
+      end
+
+      it "doesn't upgrade the state" do
+        allow(subject).to receive(:fetch_config).and_return(pipeline_config)
+        subject.register_pipeline(pipeline_settings)
+        expect(subject.pipelines[default_pipeline_id].inputs.first.message).to eq("foo-bar")
       end
     end
   end
 
   describe "#upgrade_pipeline" do
-    let(:pipeline_id) { "main" }
     let(:pipeline_config) { "input { } filter { } output { }" }
-    let(:pipeline_settings) { {
-      :config_string => pipeline_config,
-      :pipeline_workers => 4
+    let(:pipeline_args) { {
+      "config.string" => pipeline_config,
+      "pipeline.workers" => 4
     } }
     let(:new_pipeline_config) { "input { generator {} } output { }" }
 
     before(:each) do
-      subject.register_pipeline(pipeline_id, pipeline_settings)
+      subject.register_pipeline(pipeline_settings)
+    end
+
+    after(:each) do
+      # new pipelines will be created part of the upgrade process so we need
+      # to close any initialized pipelines
+      subject.close_pipelines
     end
 
     context "when the upgrade fails" do
       before :each do
         allow(subject).to receive(:fetch_config).and_return(new_pipeline_config)
         allow(subject).to receive(:create_pipeline).and_return(nil)
-        allow(subject).to receive(:stop_pipeline)
+        allow(subject).to receive(:stop_pipeline) do |id|
+          # we register_pipeline but we never execute them so we have to mock #stop_pipeline to
+          # not call Pipeline#shutdown but Pipeline#close
+          subject.close_pipeline(id)
+        end
       end
 
       it "leaves the state untouched" do
-        subject.send(:reload_state!)
-        expect(subject.pipelines[pipeline_id].config_str).to eq(pipeline_config)
+        subject.send(:"reload_pipeline!", default_pipeline_id)
+        expect(subject.pipelines[default_pipeline_id].config_str).to eq(pipeline_config)
       end
 
       context "and current state is empty" do
         it "should not start a pipeline" do
           expect(subject).to_not receive(:start_pipeline)
-          subject.send(:reload_state!)
+          subject.send(:"reload_pipeline!", default_pipeline_id)
         end
       end
     end
 
     context "when the upgrade succeeds" do
       let(:new_config) { "input { generator { count => 1 } } output { }" }
+
       before :each do
         allow(subject).to receive(:fetch_config).and_return(new_config)
-        allow(subject).to receive(:stop_pipeline)
+        allow(subject).to receive(:start_pipeline).and_return(true)
+        allow(subject).to receive(:stop_pipeline) do |id|
+          # we register_pipeline but we never execute them so we have to mock #stop_pipeline to
+          # not call Pipeline#shutdown but Pipeline#close
+          subject.close_pipeline(id)
+        end
       end
+
       it "updates the state" do
-        subject.send(:reload_state!)
-        expect(subject.pipelines[pipeline_id].config_str).to eq(new_config)
+        subject.send(:"reload_pipeline!", default_pipeline_id)
+        expect(subject.pipelines[default_pipeline_id].config_str).to eq(new_config)
       end
+
       it "starts the pipeline" do
-        expect(subject).to receive(:stop_pipeline)
-        expect(subject).to receive(:start_pipeline)
-        subject.send(:reload_state!)
+        expect(subject).to receive(:start_pipeline).and_return(true)
+        expect(subject).to receive(:stop_pipeline) do |id|
+          # we register_pipeline but we never execute them so we have to mock #stop_pipeline to
+          # not call Pipeline#shutdown but Pipeline#close
+          subject.close_pipeline(id)
+        end
+        subject.send(:"reload_pipeline!", default_pipeline_id)
       end
     end
   end
 
   describe "#fetch_config" do
-    let(:file_config) { "input { generator { count => 100 } } output { stdout { } }" }
     let(:cli_config) { "filter { drop { } } " }
-    let(:tmp_config_path) { Stud::Temporary.pathname }
-    let(:agent_args) { { :logger => logger, :config_string => "filter { drop { } } ", :config_path => tmp_config_path } }
-
-    before :each do
-      IO.write(tmp_config_path, file_config)
-    end
-
-    after :each do
-      File.unlink(tmp_config_path)
-    end
+    let(:agent_args) { { "config.string" => cli_config, "path.config" => config_file } }
 
     it "should join the config string and config path content" do
-      settings = { :config_path => tmp_config_path, :config_string => cli_config }
-      fetched_config = subject.send(:fetch_config, settings)
-      expect(fetched_config.strip).to eq(cli_config + IO.read(tmp_config_path))
+      fetched_config = subject.send(:fetch_config, agent_settings)
+      expect(fetched_config.strip).to eq(cli_config + IO.read(config_file).strip)
     end
   end
 
@@ -197,4 +380,209 @@
       expect(subject.uptime).to be >= 0
     end
   end
+
+  context "metrics after config reloading" do
+    let!(:config) { "input { generator { } } output { dummyoutput { } }" }
+
+    let!(:config_path) do
+      f = Stud::Temporary.file
+      f.write(config)
+      f.fsync
+      f.close
+      f.path
+    end
+
+    let(:pipeline_args) do
+      {
+        "pipeline.workers" => 2,
+        "path.config" => config_path
+      }
+    end
+
+    let(:agent_args) do
+      {
+        "config.reload.automatic" => false,
+        "pipeline.batch.size" => 1,
+        "metric.collect" => true
+      }
+    end
+
+    # We need to create theses dummy classes to know how many
+    # events where actually generated by the pipeline and successfully send to the output.
+    # Theses values are compared with what we store in the metric store.
+    class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
+
+    let!(:dummy_output) { LogStash::Outputs::DroppingDummyOutput.new }
+    let!(:dummy_output2) { DummyOutput2.new }
+    let(:initial_generator_threshold) { 1000 }
+    let(:pipeline_thread) do
+      Thread.new do
+        subject.register_pipeline(pipeline_settings)
+        subject.execute
+      end
+    end
+
+
+    before :each do
+      allow(LogStash::Outputs::DroppingDummyOutput).to receive(:new).at_least(:once).with(anything).and_return(dummy_output)
+      allow(DummyOutput2).to receive(:new).at_least(:once).with(anything).and_return(dummy_output2)
+
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(LogStash::Outputs::DroppingDummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput2").and_return(DummyOutput2)
+
+      @abort_on_exception = Thread.abort_on_exception
+      Thread.abort_on_exception = true
+
+      pipeline_thread
+
+      # wait for some events to reach the dummy_output
+      sleep(0.1) until dummy_output.events_received > initial_generator_threshold
+    end
+
+    after :each do
+      begin
+        subject.shutdown
+        Stud.stop!(pipeline_thread)
+        pipeline_thread.join
+      ensure
+        Thread.abort_on_exception = @abort_on_exception
+      end
+    end
+
+    context "when reloading a good config" do
+      let(:new_config_generator_counter) { 500 }
+      let(:new_config) { "input { generator { count => #{new_config_generator_counter} } } output { dummyoutput2 {} }" }
+
+      before :each do
+        File.open(config_path, "w") do |f|
+          f.write(new_config)
+          f.fsync
+        end
+
+        subject.send(:"reload_pipeline!", "main")
+
+        # wait until pipeline restarts
+        sleep(0.01) until dummy_output2.events_received > 0
+      end
+
+      it "resets the pipeline metric collector" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:events][:in].value
+        expect(value).to be <= new_config_generator_counter
+      end
+
+      it "does not reset the global event count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/events")[:stats][:events][:in].value
+        expect(value).to be > initial_generator_threshold
+      end
+
+      it "increases the successful reload count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
+        instance_value = snapshot.metric_store.get_with_path("/stats")[:stats][:reloads][:successes].value
+        expect(value).to eq(1)
+        expect(instance_value).to eq(1)
+      end
+
+      it "does not set the failure reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_failure_timestamp].value
+        expect(value).to be(nil)
+      end
+
+      it "sets the success reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_success_timestamp].value
+        expect(value).to be_a(LogStash::Timestamp)
+      end
+
+      it "does not set the last reload error" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_error].value
+        expect(value).to be(nil)
+      end
+    end
+
+    context "when reloading a bad config" do
+      let(:new_config) { "input { generator { count => " }
+      before :each do
+
+        File.open(config_path, "w") do |f|
+          f.write(new_config)
+          f.fsync
+        end
+
+        subject.send(:"reload_pipeline!", "main")
+      end
+
+      it "does not increase the successful reload count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
+        expect(value).to eq(0)
+      end
+
+      it "does not set the successful reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_success_timestamp].value
+        expect(value).to be(nil)
+      end
+
+      it "sets the failure reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_failure_timestamp].value
+        expect(value).to be_a(LogStash::Timestamp)
+      end
+
+      it "sets the last reload error" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_error].value
+        expect(value).to be_a(Hash)
+        expect(value).to include(:message, :backtrace)
+      end
+
+      it "increases the failed reload count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:failures].value
+        expect(value).to be > 0
+      end
+    end
+
+    context "when reloading a config that raises exception on pipeline.run" do
+      let(:new_config) { "input { generator { count => 10000 } }" }
+
+      class BrokenGenerator < LogStash::Inputs::Generator
+        def register
+          raise ArgumentError
+        end
+      end
+
+      before :each do
+        allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(BrokenGenerator)
+
+        File.open(config_path, "w") do |f|
+          f.write(new_config)
+          f.fsync
+        end
+      end
+
+      it "does not increase the successful reload count" do
+        expect { subject.send(:"reload_pipeline!", "main") }.to_not change {
+          snapshot = subject.metric.collector.snapshot_metric
+          reload_metrics = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads]
+          reload_metrics[:successes].value
+        }
+      end
+
+      it "increases the failures reload count" do
+        expect { subject.send(:"reload_pipeline!", "main") }.to change {
+          snapshot = subject.metric.collector.snapshot_metric
+          reload_metrics = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads]
+          reload_metrics[:failures].value
+        }.by(1)
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/codecs/base_spec.rb b/logstash-core/spec/logstash/codecs/base_spec.rb
new file mode 100644
index 00000000000..42b07b5d4ed
--- /dev/null
+++ b/logstash-core/spec/logstash/codecs/base_spec.rb
@@ -0,0 +1,74 @@
+# encoding: utf-8
+require "spec_helper"
+
+DATA_DOUBLE = "data".freeze
+
+# use a dummy NOOP output to test Outputs::Base
+class LogStash::Codecs::NOOPAsync < LogStash::Codecs::Base
+  attr_reader :last_result
+  config_name "noop_async"
+
+  def encode(event)
+    @last_result = @on_event.call(event, DATA_DOUBLE)
+  end
+end
+
+class LogStash::Codecs::NOOPSync < LogStash::Codecs::Base
+  attr_reader :last_result
+  config_name "noop_sync"
+
+  def encode_sync(event)
+    DATA_DOUBLE
+  end
+end
+
+class LogStash::Codecs::NOOPMulti < LogStash::Codecs::Base
+  attr_reader :last_result
+  config_name "noop_multi"
+
+  def encode_sync(event)
+    DATA_DOUBLE
+  end
+end
+
+describe LogStash::Codecs::Base do
+  let(:params) { {} }
+  subject(:instance) { klass.new(params.dup) }
+  let(:event) { double("event") }
+  let(:encoded_data) { DATA_DOUBLE }
+  let(:encoded_tuple) { [event, encoded_data] }
+
+  describe "encoding" do
+    shared_examples "encoder types" do |codec_class|
+      let(:klass) { codec_class }
+      
+      describe "#{codec_class}" do
+        describe "multi_encode" do
+          it "should return an array of [event,data] tuples" do
+            expect(instance.multi_encode([event,event])).to eq([encoded_tuple, encoded_tuple])
+          end
+        end
+        
+        describe "#encode" do
+          before do
+            @result = nil
+            instance.on_event do |event, data|
+              @result = [event, data]
+            end
+            instance.encode(event)
+          end
+          
+          it "should yield the correct result" do
+            expect(@result).to eq(encoded_tuple)
+          end
+        end
+      end
+    end
+
+    include_examples("encoder types", LogStash::Codecs::NOOPAsync)
+    include_examples("encoder types", LogStash::Codecs::NOOPSync)
+    include_examples("encoder types", LogStash::Codecs::NOOPMulti)
+  end
+end
+
+                          
diff --git a/logstash-core/spec/logstash/config/config_ast_spec.rb b/logstash-core/spec/logstash/config/config_ast_spec.rb
index 917e0575916..657b00523c4 100644
--- a/logstash-core/spec/logstash/config/config_ast_spec.rb
+++ b/logstash-core/spec/logstash/config/config_ast_spec.rb
@@ -143,4 +143,82 @@
       end
     end
   end
+
+  context "when using two plugin sections of the same type" do
+    let(:pipeline_klass) do
+      Class.new do
+        def initialize(config)
+          grammar = LogStashConfigParser.new
+          @config = grammar.parse(config)
+          @code = @config.compile
+          eval(@code)
+        end
+        def plugin(*args);end
+      end
+    end
+    context "(filters)" do
+      let(:config_string) {
+        "input { generator { } }
+         filter { filter1 { } }
+         filter { filter1 { } }
+         output { output1 { } }"
+      }
+
+
+      it "should create a pipeline with both sections" do
+        generated_objects = pipeline_klass.new(config_string).instance_variable_get("@generated_objects")
+        filters = generated_objects.keys.map(&:to_s).select {|obj_name| obj_name.match(/^filter.+?_\d+$/) }
+        expect(filters.size).to eq(2)
+      end
+    end
+
+    context "(filters)" do
+      let(:config_string) {
+        "input { generator { } }
+         output { output1 { } }
+         output { output1 { } }"
+      }
+
+
+      it "should create a pipeline with both sections" do
+        generated_objects = pipeline_klass.new(config_string).instance_variable_get("@generated_objects")
+        outputs = generated_objects.keys.map(&:to_s).select {|obj_name| obj_name.match(/^output.+?_\d+$/) }
+        expect(outputs.size).to eq(2)
+      end
+    end
+  end
+  context "when creating two instances of the same configuration" do
+
+    let(:config_string) {
+      "input { generator { } }
+       filter {
+         if [type] == 'test' { filter1 { } }
+       }
+       output {
+         output1 { }
+       }"
+    }
+
+    let(:pipeline_klass) do
+      Class.new do
+        def initialize(config)
+          grammar = LogStashConfigParser.new
+          @config = grammar.parse(config)
+          @code = @config.compile
+          eval(@code)
+        end
+        def plugin(*args);end
+      end
+    end
+
+    describe "generated conditional functionals" do
+      it "should be created per instance" do
+        instance_1 = pipeline_klass.new(config_string)
+        instance_2 = pipeline_klass.new(config_string)
+        generated_method_1 = instance_1.instance_variable_get("@generated_objects")[:cond_func_1]
+        generated_method_2 = instance_2.instance_variable_get("@generated_objects")[:cond_func_1]
+        expect(generated_method_1).to_not be(generated_method_2)
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/config/loader_spec.rb b/logstash-core/spec/logstash/config/loader_spec.rb
index b51272ee13a..955feb2f615 100644
--- a/logstash-core/spec/logstash/config/loader_spec.rb
+++ b/logstash-core/spec/logstash/config/loader_spec.rb
@@ -3,7 +3,9 @@
 require "logstash/config/loader"
 
 describe LogStash::Config::Loader do
-  subject { described_class.new(Cabin::Channel.get) }
+  let(:logger) { double("logger") }
+  subject { described_class.new(logger) }
+
   context "when local" do
     before { expect(subject).to receive(:local_config).with(path) }
 
diff --git a/logstash-core/spec/logstash/config/mixin_spec.rb b/logstash-core/spec/logstash/config/mixin_spec.rb
index 2a9bb8ac3d5..d5f63fc9143 100644
--- a/logstash-core/spec/logstash/config/mixin_spec.rb
+++ b/logstash-core/spec/logstash/config/mixin_spec.rb
@@ -68,6 +68,74 @@
     end
   end
 
+  context "when validating lists of items" do
+    let(:klass) do
+      Class.new(LogStash::Filters::Base)  do
+        config_name "multiuri"
+        config :uris, :validate => :uri, :list => true
+        config :strings, :validate => :string, :list => true
+        config :required_strings, :validate => :string, :list => true, :required => true
+      end
+    end
+
+    let(:uris) { ["http://example.net/1", "http://example.net/2"] }
+    let(:safe_uris) { uris.map {|str| ::LogStash::Util::SafeURI.new(str) } }
+    let(:strings) { ["I am a", "modern major general"] }
+    let(:required_strings) { ["required", "strings"] }
+
+    subject { klass.new("uris" => uris, "strings" => strings, "required_strings" => required_strings) }
+
+    it "a URI list should return an array of URIs" do
+      expect(subject.uris).to match_array(safe_uris)
+    end
+
+    it "a string list should return an array of strings" do
+      expect(subject.strings).to match_array(strings)
+    end
+
+    context "with a scalar value" do
+      let(:strings) { "foo" }
+
+      it "should return the scalar value as a single element array" do
+        expect(subject.strings).to match_array([strings])
+      end
+    end
+
+    context "with an empty list" do
+      let(:strings) { [] }
+
+      it "should return nil" do
+        expect(subject.strings).to be_nil
+      end
+    end
+
+    describe "with required => true" do
+      context "and a single element" do
+        let(:required_strings) { ["foo"] }
+
+        it "should return the single value" do
+          expect(subject.required_strings).to eql(required_strings)
+        end
+      end
+
+      context "with an empty list" do
+        let (:required_strings) { [] }
+
+        it "should raise a configuration error" do
+          expect { subject.required_strings }.to raise_error(LogStash::ConfigurationError)
+        end        
+      end
+
+      context "with no value specified" do
+        let (:required_strings) { nil }
+
+        it "should raise a configuration error" do
+          expect { subject.required_strings }.to raise_error(LogStash::ConfigurationError)
+        end
+      end          
+    end
+  end
+
   context "when validating :password" do
     let(:klass) do
       Class.new(LogStash::Filters::Base)  do
@@ -96,6 +164,99 @@
       clone = subject.class.new(subject.params)
       expect(clone.password.value).to(be == secret)
     end
+
+    it "should obfuscate original_params" do
+      expect(subject.original_params['password']).to(be_a(LogStash::Util::Password))
+    end
+  end
+
+  context "when validating :uri" do
+    let(:klass) do
+      Class.new(LogStash::Filters::Base)  do
+        config_name "fakeuri"
+        config :uri, :validate => :uri
+      end
+    end
+
+    shared_examples("safe URI") do |options|
+      options ||= {}
+      
+      subject { klass.new("uri" => uri_str) }
+
+      it "should be a SafeURI object" do
+        expect(subject.uri).to(be_a(LogStash::Util::SafeURI))
+      end
+
+      it "should correctly copy URI types" do
+        clone = subject.class.new(subject.params)
+        expect(clone.uri.to_s).to eql(uri_hidden)
+      end
+
+      it "should make the real URI object available under #uri" do
+        expect(subject.uri.uri).to be_a(::URI)
+      end
+
+      it "should obfuscate original_params" do
+        expect(subject.original_params['uri']).to(be_a(LogStash::Util::SafeURI))
+      end
+
+      if !options[:exclude_password_specs]
+        describe "passwords" do
+          it "should make password values hidden with #to_s" do
+            expect(subject.uri.to_s).to eql(uri_hidden)
+          end
+
+          it "should make password values hidden with #inspect" do
+            expect(subject.uri.inspect).to eql(uri_hidden)
+          end
+        end
+      end
+
+      context "attributes" do
+        [:scheme, :user, :password, :hostname, :path].each do |attr|
+          it "should make #{attr} available" do
+            expect(subject.uri.send(attr)).to eql(self.send(attr))
+          end
+        end
+      end
+    end
+
+    context "with a host:port combination" do
+      let(:scheme) { nil }
+      let(:user) { nil }
+      let(:password) { nil }
+      let(:hostname) { "myhostname" }
+      let(:port) { 1234 }
+      let(:path) { "" }
+      let(:uri_str) { "#{hostname}:#{port}" }
+      let(:uri_hidden) { "//#{hostname}:#{port}" }
+
+      include_examples("safe URI", :exclude_password_specs => true)
+    end
+
+    context "with a username / password" do
+      let(:scheme) { "myscheme" }
+      let(:user) { "myuser" }
+      let(:password) { "fancypants" }
+      let(:hostname) { "myhostname" }
+      let(:path) { "/my/path" }
+      let(:uri_str) { "#{scheme}://#{user}:#{password}@#{hostname}#{path}" }
+      let(:uri_hidden) { "#{scheme}://#{user}:#{LogStash::Util::SafeURI::PASS_PLACEHOLDER}@#{hostname}#{path}" }
+
+      include_examples("safe URI")
+    end
+
+    context "without a username / password" do
+      let(:scheme) { "myscheme" }
+      let(:user) { nil }
+      let(:password) { nil }
+      let(:hostname) { "myhostname" }
+      let(:path) { "/my/path" }
+      let(:uri_str) { "#{scheme}://#{hostname}#{path}" }
+      let(:uri_hidden) { "#{scheme}://#{hostname}#{path}" }
+
+      include_examples("safe URI")
+    end
   end
 
   describe "obsolete settings" do
@@ -156,11 +317,18 @@
     let(:plugin_class) do
       Class.new(LogStash::Filters::Base)  do
         config_name "one_plugin"
-        config :oneString, :validate => :string
-        config :oneBoolean, :validate => :boolean
-        config :oneNumber, :validate => :number
-        config :oneArray, :validate => :array
-        config :oneHash, :validate => :hash
+        config :oneString, :validate => :string, :required => false
+        config :oneBoolean, :validate => :boolean, :required => false
+        config :oneNumber, :validate => :number, :required => false
+        config :oneArray, :validate => :array, :required => false
+        config :oneHash, :validate => :hash, :required => false
+        config :nestedHash, :validate => :hash, :required => false
+        config :nestedArray, :validate => :hash, :required => false
+        config :deepHash, :validate => :hash, :required => false
+
+        def initialize(params)
+          super(params)
+        end
       end
     end
 
@@ -201,11 +369,13 @@
       before do
         ENV["FunString"] = "fancy"
         ENV["FunBool"] = "true"
+        ENV["SERVER_LS_TEST_ADDRESS"] = "some.host.address.tld"
       end
 
       after do
         ENV.delete("FunString")
         ENV.delete("FunBool")
+        ENV.delete("SERVER_LS_TEST_ADDRESS")
       end
 
       subject do
@@ -213,7 +383,10 @@
           "oneString" => "${FunString:foo}",
           "oneBoolean" => "${FunBool:false}",
           "oneArray" => [ "first array value", "${FunString:foo}" ],
-          "oneHash" => { "key1" => "${FunString:foo}", "key2" => "$FunString is ${FunBool}", "key3" => "${FunBool:false} or ${funbool:false}" }
+          "oneHash" => { "key1" => "${FunString:foo}", "key2" => "${FunString} is ${FunBool}", "key3" => "${FunBool:false} or ${funbool:false}" },
+          "nestedHash" => { "level1" => { "key1" => "http://${FunString}:8080/blah.txt" } },
+          "nestedArray" => { "level1" => [{ "key1" => "http://${FunString}:8080/blah.txt" }, { "key2" => "http://${FunString}:8080/foo.txt" }] },
+          "deepHash" => { "level1" => { "level2" => {"level3" => { "key1" => "http://${FunString}:8080/blah.txt" } } } }
         )
       end
 
@@ -222,9 +395,48 @@
         expect(subject.oneBoolean).to(be_truthy)
         expect(subject.oneArray).to(be == [ "first array value", "fancy" ])
         expect(subject.oneHash).to(be == { "key1" => "fancy", "key2" => "fancy is true", "key3" => "true or false" })
+        expect(subject.nestedHash).to(be == { "level1" => { "key1" => "http://fancy:8080/blah.txt" } })
+        expect(subject.nestedArray).to(be == { "level1" => [{ "key1" => "http://fancy:8080/blah.txt" }, { "key2" => "http://fancy:8080/foo.txt" }] })
+        expect(subject.deepHash).to(be == { "level1" => { "level2" => { "level3" => { "key1" => "http://fancy:8080/blah.txt" } } } })
       end
 
+      it "should validate settings after interpolating ENV variables" do
+        expect {
+          Class.new(LogStash::Filters::Base) do
+            include LogStash::Config::Mixin
+            config_name "test"
+            config :server_address, :validate => :uri
+          end.new({"server_address" => "${SERVER_LS_TEST_ADDRESS}"})
+        }.not_to raise_error
+      end
     end
-  end
 
+    context "should support $ in values" do
+      before do
+        ENV["bar"] = "foo"
+        ENV["f$$"] = "bar"
+      end
+
+      after do
+        ENV.delete("bar")
+        ENV.delete("f$$")
+      end
+
+      subject do
+        plugin_class.new(
+          "oneString" => "${f$$:val}",
+          "oneArray" => ["foo$bar", "${bar:my$val}"]
+          # "dollar_in_env" => "${f$$:final}"
+        )
+      end
+
+      it "should support $ in values" do
+        expect(subject.oneArray).to(be == ["foo$bar", "foo"])
+      end
+
+      it "should not support $ in environment variable name" do
+        expect(subject.oneString).to(be == "${f$$:val}")
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/environment_spec.rb b/logstash-core/spec/logstash/environment_spec.rb
index 581ef6ae8be..28637ea2323 100644
--- a/logstash-core/spec/logstash/environment_spec.rb
+++ b/logstash-core/spec/logstash/environment_spec.rb
@@ -53,4 +53,42 @@
       expect($LOAD_PATH).to include(path)
     end
   end
+
+
+  describe "OS detection" do
+    windows_host_os = %w(bccwin cygwin mingw mswin wince)
+    linux_host_os = %w(linux)
+
+    context "windows" do
+      windows_host_os.each do |host|
+        it "#{host} returns true" do
+          expect(RbConfig::CONFIG).to receive(:[]).with("host_os").and_return(host)
+          expect(LogStash::Environment.windows?).to be_truthy
+        end
+      end
+
+      linux_host_os.each do |host|
+        it "#{host} returns false" do
+          expect(RbConfig::CONFIG).to receive(:[]).with("host_os").and_return(host)
+          expect(LogStash::Environment.windows?).to be_falsey
+        end
+      end
+    end
+
+    context "Linux" do
+      windows_host_os.each do |host|
+        it "#{host} returns true" do
+          expect(RbConfig::CONFIG).to receive(:[]).with("host_os").and_return(host)
+          expect(LogStash::Environment.linux?).to be_falsey
+        end
+      end
+
+      linux_host_os.each do |host|
+        it "#{host} returns false" do
+          expect(RbConfig::CONFIG).to receive(:[]).with("host_os").and_return(host)
+          expect(LogStash::Environment.linux?).to be_truthy
+        end
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/event_dispatcher_spec.rb b/logstash-core/spec/logstash/event_dispatcher_spec.rb
new file mode 100644
index 00000000000..339df8189d8
--- /dev/null
+++ b/logstash-core/spec/logstash/event_dispatcher_spec.rb
@@ -0,0 +1,83 @@
+# encoding: utf-8
+#
+require "logstash/event_dispatcher"
+
+describe LogStash::EventDispatcher do
+  class DummyEmitter
+    attr_reader :dispatcher
+
+    def initialize
+      @dispatcher = LogStash::EventDispatcher.new(self)
+    end
+
+    def method_exists
+      dispatcher.fire(:method_exists)
+    end
+
+    def method_exists_with_arguments(argument1, argument2, argument3)
+      dispatcher.fire(:method_exists_with_arguments, argument1, argument2, argument3)
+    end
+
+    def method_do_not_exist
+      dispatcher.fire(:method_do_not_exist)
+    end
+  end
+
+  class CustomSpy
+    def method_exists
+    end
+
+    def method_exists_with_arguments(argument1, argument2, argument3)
+    end
+  end
+
+  let(:listener) { CustomSpy }
+  subject(:emitter) { DummyEmitter.new }
+
+  it "ignores duplicate listener" do
+    emitter.dispatcher.add_listener(listener)
+    emitter.dispatcher.add_listener(listener)
+    expect(listener).to receive(:method_exists).with(emitter).once
+    emitter.method_exists
+  end
+
+  describe "Emits events" do
+    before do
+      emitter.dispatcher.add_listener(listener)
+    end
+
+    context "when the method exist" do
+      it "calls the method without arguments" do
+        expect(listener).to receive(:method_exists).with(emitter)
+        emitter.method_exists
+      end
+
+      it "calls the method with arguments" do
+        expect(listener).to receive(:method_exists_with_arguments).with(emitter, 1, 2, 3)
+        emitter.method_exists_with_arguments(1, 2, 3)
+      end
+    end
+
+    context "when the method doesn't exist on the listener" do
+      it "should not raise an exception" do
+        expect { emitter.method_do_not_exist }.not_to raise_error
+      end
+    end
+  end
+
+  describe "Configuring listeners" do
+    it "adds a listener to an emitter" do
+      expect(listener).to receive(:method_exists).with(emitter)
+      emitter.dispatcher.add_listener(listener)
+      emitter.method_exists
+    end
+
+    it "allows to remove a listener to an emitter" do
+      expect(listener).to receive(:method_exists).with(emitter).once
+      emitter.dispatcher.add_listener(listener)
+      emitter.method_exists
+      emitter.dispatcher.remove_listener(listener)
+      emitter.method_exists
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/event_spec.rb b/logstash-core/spec/logstash/event_spec.rb
new file mode 100644
index 00000000000..c798907087a
--- /dev/null
+++ b/logstash-core/spec/logstash/event_spec.rb
@@ -0,0 +1,346 @@
+# encoding: utf-8
+
+require "spec_helper"
+require "logstash/util"
+require "logstash/event"
+require "json"
+require "java"
+
+TIMESTAMP = "@timestamp"
+
+describe LogStash::Event do
+  context "to_json" do
+    it "should serialize simple values" do
+      e = LogStash::Event.new({"foo" => "bar", "bar" => 1, "baz" => 1.0, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":\"bar\",\"bar\":1,\"baz\":1.0,\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
+    end
+
+    it "should serialize deep hash values" do
+      e = LogStash::Event.new({"foo" => {"bar" => 1, "baz" => 1.0, "biz" => "boz"}, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":{\"bar\":1,\"baz\":1.0,\"biz\":\"boz\"},\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
+    end
+
+    it "should serialize deep array values" do
+      e = LogStash::Event.new({"foo" => ["bar", 1, 1.0], TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"foo\":[\"bar\",1,1.0],\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\"}"))
+    end
+
+    it "should serialize deep hash from field reference assignments" do
+      e = LogStash::Event.new({TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      e.set("foo", "bar")
+      e.set("bar", 1)
+      e.set("baz", 1.0)
+      e.set("[fancy][pants][socks]", "shoes")
+      expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\",\"foo\":\"bar\",\"bar\":1,\"baz\":1.0,\"fancy\":{\"pants\":{\"socks\":\"shoes\"}}}"))
+    end
+  end
+
+  context "#get" do
+    it "should get simple values" do
+      e = LogStash::Event.new({"foo" => "bar", "bar" => 1, "baz" => 1.0, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(e.get("foo")).to eq("bar")
+      expect(e.get("[foo]")).to eq("bar")
+      expect(e.get("bar")).to eq(1)
+      expect(e.get("[bar]")).to eq(1)
+      expect(e.get("baz")).to eq(1.0)
+      expect(e.get("[baz]")).to eq(1.0)
+      expect(e.get(TIMESTAMP).to_s).to eq("2015-05-28T23:02:05.350Z")
+      expect(e.get("[#{TIMESTAMP}]").to_s).to eq("2015-05-28T23:02:05.350Z")
+    end
+
+    it "should get deep hash values" do
+      e = LogStash::Event.new({"foo" => {"bar" => 1, "baz" => 1.0}})
+      expect(e.get("[foo][bar]")).to eq(1)
+      expect(e.get("[foo][baz]")).to eq(1.0)
+    end
+
+    it "should get deep array values" do
+      e = LogStash::Event.new({"foo" => ["bar", 1, 1.0]})
+      expect(e.get("[foo][0]")).to eq("bar")
+      expect(e.get("[foo][1]")).to eq(1)
+      expect(e.get("[foo][2]")).to eq(1.0)
+      expect(e.get("[foo][3]")).to be_nil
+    end
+
+    context "negative array values" do
+      it "should index from the end of the array" do
+        list = ["bar", 1, 1.0]
+        e = LogStash::Event.new({"foo" => list})
+        expect(e.get("[foo][-3]")).to eq(list[-3])
+        expect(e.get("[foo][-2]")).to eq(list[-2])
+        expect(e.get("[foo][-1]")).to eq(list[-1])
+      end
+    end
+  end
+
+  context "#set" do
+    it "should set simple values" do
+      e = LogStash::Event.new()
+      expect(e.set("foo", "bar")).to eq("bar")
+      expect(e.get("foo")).to eq("bar")
+
+      e = LogStash::Event.new({"foo" => "test"})
+      expect(e.set("foo", "bar")).to eq("bar")
+      expect(e.get("foo")).to eq("bar")
+    end
+
+    it "should set deep hash values" do
+      e = LogStash::Event.new()
+      expect(e.set("[foo][bar]", "baz")).to eq("baz")
+      expect(e.get("[foo][bar]")).to eq("baz")
+      expect(e.get("[foo][baz]")).to be_nil
+    end
+
+    it "should set deep array values" do
+      e = LogStash::Event.new()
+      expect(e.set("[foo][0]", "bar")).to eq("bar")
+      expect(e.get("[foo][0]")).to eq("bar")
+      expect(e.set("[foo][1]", 1)).to eq(1)
+      expect(e.get("[foo][1]")).to eq(1)
+      expect(e.set("[foo][2]", 1.0)).to eq(1.0)
+      expect(e.get("[foo][2]")).to eq(1.0)
+      expect(e.get("[foo][3]")).to be_nil
+    end
+
+    it "should add key when setting nil value" do
+      e = LogStash::Event.new()
+      e.set("[foo]", nil)
+      expect(e.to_hash).to include("foo" => nil)
+    end
+
+    # BigDecimal is now natively converted by JRuby, see https://github.com/elastic/logstash/pull/4838
+    it "should set BigDecimal" do
+      e = LogStash::Event.new()
+      e.set("[foo]", BigDecimal.new(1))
+      expect(e.get("foo")).to be_kind_of(BigDecimal)
+      expect(e.get("foo")).to eq(BigDecimal.new(1))
+    end
+
+    it "should set RubyBignum" do
+      e = LogStash::Event.new()
+      e.set("[foo]", -9223372036854776000)
+      expect(e.get("foo")).to be_kind_of(Bignum)
+      expect(e.get("foo")).to eq(-9223372036854776000)
+    end
+
+    it "should convert Time to Timestamp" do
+      e = LogStash::Event.new()
+      time = Time.now
+      e.set("[foo]", Time.at(time.to_f))
+      expect(e.get("foo")).to be_kind_of(LogStash::Timestamp)
+      expect(e.get("foo").to_f).to be_within(0.1).of(time.to_f)
+    end
+
+    it "should set XXJavaProxy Jackson crafted" do
+      proxy = org.logstash.Util.getMapFixtureJackson()
+      # proxy is {"string": "foo", "int": 42, "float": 42.42, "array": ["bar","baz"], "hash": {"string":"quux"} }
+      e = LogStash::Event.new()
+      e.set("[proxy]", proxy)
+      expect(e.get("[proxy][string]")).to eql("foo")
+      expect(e.get("[proxy][int]")).to eql(42)
+      expect(e.get("[proxy][float]")).to eql(42.42)
+      expect(e.get("[proxy][array][0]")).to eql("bar")
+      expect(e.get("[proxy][array][1]")).to eql("baz")
+      expect(e.get("[proxy][hash][string]")).to eql("quux")
+    end
+
+    it "should set XXJavaProxy hand crafted" do
+      proxy = org.logstash.Util.getMapFixtureHandcrafted()
+      # proxy is {"string": "foo", "int": 42, "float": 42.42, "array": ["bar","baz"], "hash": {"string":"quux"} }
+      e = LogStash::Event.new()
+      e.set("[proxy]", proxy)
+      expect(e.get("[proxy][string]")).to eql("foo")
+      expect(e.get("[proxy][int]")).to eql(42)
+      expect(e.get("[proxy][float]")).to eql(42.42)
+      expect(e.get("[proxy][array][0]")).to eql("bar")
+      expect(e.get("[proxy][array][1]")).to eql("baz")
+      expect(e.get("[proxy][hash][string]")).to eql("quux")
+    end
+
+    it "should fail on non UTF-8 encoding" do
+      # e = LogStash::Event.new
+      # s1 = "\xE0 Montr\xE9al".force_encoding("ISO-8859-1")
+      # expect(s1.encoding.name).to eq("ISO-8859-1")
+      # expect(s1.valid_encoding?).to eq(true)
+      # e.set("test", s1)
+      # s2 = e.get("test")
+      # expect(s2.encoding.name).to eq("UTF-8")
+      # expect(s2.valid_encoding?).to eq(true)
+    end
+  end
+
+  context "timestamp" do
+    it "getters should present a Ruby LogStash::Timestamp" do
+      e = LogStash::Event.new()
+      expect(e.timestamp.class).to eq(LogStash::Timestamp)
+      expect(e.get(TIMESTAMP).class).to eq(LogStash::Timestamp)
+    end
+
+    it "to_hash should inject a Ruby LogStash::Timestamp" do
+      e = LogStash::Event.new()
+
+      expect(e.to_java).to be_kind_of(Java::OrgLogstash::Event)
+      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::OrgLogstash::Timestamp)
+
+      expect(e.to_hash[TIMESTAMP]).to be_kind_of(LogStash::Timestamp)
+      # now make sure the original map was not touched
+      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::OrgLogstash::Timestamp)
+    end
+
+    it "should set timestamp" do
+      e = LogStash::Event.new
+      now = Time.now
+      e.set("@timestamp", LogStash::Timestamp.at(now.to_i))
+      expect(e.timestamp.to_i).to eq(now.to_i)
+      expect(e.get("@timestamp").to_i).to eq(now.to_i)
+    end
+  end
+
+  context "append" do
+    it "should append" do
+      event = LogStash::Event.new("message" => "hello world")
+      event.append(LogStash::Event.new("message" => "another thing"))
+      expect(event.get("message")).to eq(["hello world", "another thing"])
+    end
+  end
+
+  context "tags" do
+    it "should tag" do
+      event = LogStash::Event.new("message" => "hello world")
+      expect(event.get("tags")).to be_nil
+      event.tag("foo")
+      expect(event.get("tags")).to eq(["foo"])
+    end
+  end
+
+
+  # TODO(talevy): migrate tests to Java. no reason to test logging logic in ruby when it is being
+  #               done in java land.
+
+  # context "logger" do
+
+  #   let(:logger) { double("Logger") }
+
+  #   before(:each) do
+  #     allow(LogStash::Event).to receive(:logger).and_return(logger)
+  #   end
+
+  #   it "should set logger using a module" do
+  #     expect(logger).to receive(:warn).once
+  #     LogStash::Event.new(TIMESTAMP => "invalid timestamp")
+  #   end
+
+  #   it "should warn on invalid timestamp object" do
+  #     expect(logger).to receive(:warn).once.with(/^Unrecognized/)
+  #     LogStash::Event.new(TIMESTAMP => Array.new)
+  #   end
+  # end
+
+  context "to_hash" do
+    let (:source_hash) {  {"a" => 1, "b" => [1, 2, 3, {"h" => 1, "i" => "baz"}], "c" => {"d" => "foo", "e" => "bar", "f" => [4, 5, "six"]}} }
+    let (:source_hash_with_metadata) {  source_hash.merge({"@metadata" => {"a" => 1, "b" => 2}}) }
+    subject { LogStash::Event.new(source_hash_with_metadata) }
+
+    it "should include @timestamp and @version" do
+      h = subject.to_hash
+      expect(h).to include("@timestamp")
+      expect(h).to include("@version")
+      expect(h).not_to include("@metadata")
+    end
+
+    it "should include @timestamp and @version and @metadata" do
+      h = subject.to_hash_with_metadata
+      expect(h).to include("@timestamp")
+      expect(h).to include("@version")
+      expect(h).to include("@metadata")
+    end
+
+    it "should produce valid deep Ruby hash without metadata" do
+      h = subject.to_hash
+      h.delete("@timestamp")
+      h.delete("@version")
+      expect(h).to eq(source_hash)
+    end
+
+    it "should produce valid deep Ruby hash with metadata" do
+      h = subject.to_hash_with_metadata
+      h.delete("@timestamp")
+      h.delete("@version")
+      expect(h).to eq(source_hash_with_metadata)
+    end
+  end
+
+  context "from_json" do
+    let (:source_json) { "{\"foo\":1, \"bar\":\"baz\"}" }
+    let (:blank_strings) {["", "  ",  "   "]}
+    let (:bare_strings) {["aa", "  aa", "aa  "]}
+
+    it "should produce a new event from json" do
+      expect(LogStash::Event.from_json(source_json).size).to eq(1)
+
+      event = LogStash::Event.from_json(source_json)[0]
+      expect(event.get("[foo]")).to eq(1)
+      expect(event.get("[bar]")).to eq("baz")
+    end
+
+    it "should ignore blank strings" do
+      blank_strings.each do |s|
+        expect(LogStash::Event.from_json(s).size).to eq(0)
+      end
+    end
+
+    it "should raise TypeError on nil string" do
+      expect{LogStash::Event.from_json(nil)}.to raise_error TypeError
+    end
+
+    it "should consistently handle nil" do
+      blank_strings.each do |s|
+        expect{LogStash::Event.from_json(nil)}.to raise_error
+        expect{LogStash::Event.new(LogStash::Json.load(nil))}.to raise_error
+      end
+    end
+
+    it "should consistently handle bare string" do
+      bare_strings.each do |s|
+        expect{LogStash::Event.from_json(s)}.to raise_error LogStash::Json::ParserError
+        expect{LogStash::Event.new(LogStash::Json.load(s))}.to raise_error LogStash::Json::ParserError
+       end
+    end
+  end
+
+  context "initialize" do
+
+    it "should accept Ruby Hash" do
+      e = LogStash::Event.new({"foo" => 1, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(e.get("foo")).to eq(1)
+      expect(e.timestamp.to_iso8601).to eq("2015-05-28T23:02:05.350Z")
+    end
+
+    it "should accept Java Map" do
+      h = Java::JavaUtil::HashMap.new
+      h.put("foo", 2);
+      h.put(TIMESTAMP, "2016-05-28T23:02:05.350Z");
+      e = LogStash::Event.new(h)
+
+      expect(e.get("foo")).to eq(2)
+      expect(e.timestamp.to_iso8601).to eq("2016-05-28T23:02:05.350Z")
+    end
+
+  end
+
+  context "method missing exception messages" do
+    subject { LogStash::Event.new({"foo" => "bar"}) }
+
+    it "#[] method raises a better exception message" do
+      expect { subject["foo"] }.to raise_error(NoMethodError, /Direct event field references \(i\.e\. event\['field'\]\)/)
+    end
+
+    it "#[]= method raises a better exception message" do
+      expect { subject["foo"] = "baz" }.to raise_error(NoMethodError, /Direct event field references \(i\.e\. event\['field'\] = 'value'\)/)
+    end
+
+    it "other missing method raises normal exception message" do
+      expect { subject.baz() }.to raise_error(NoMethodError, /undefined method `baz' for/)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/execution_context_spec.rb b/logstash-core/spec/logstash/execution_context_spec.rb
new file mode 100644
index 00000000000..351f3a22f9e
--- /dev/null
+++ b/logstash-core/spec/logstash/execution_context_spec.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/execution_context"
+
+describe LogStash::ExecutionContext do
+  let(:pipeline_id) { :main }
+
+  subject { described_class.new(pipeline_id) }
+
+  it "returns the `pipeline_id`" do
+    expect(subject.pipeline_id).to eq(pipeline_id)
+  end
+end
diff --git a/logstash-core/spec/logstash/filter_delegator_spec.rb b/logstash-core/spec/logstash/filter_delegator_spec.rb
index 595fbfedc36..577dfee105a 100644
--- a/logstash-core/spec/logstash/filter_delegator_spec.rb
+++ b/logstash-core/spec/logstash/filter_delegator_spec.rb
@@ -3,6 +3,7 @@
 require "logstash/filter_delegator"
 require "logstash/instrument/null_metric"
 require "logstash/event"
+require "logstash/execution_context"
 
 describe LogStash::FilterDelegator do
   let(:logger) { double(:logger) }
@@ -10,8 +11,10 @@
   let(:config) do
     { "host" => "127.0.0.1", "id" => filter_id }
   end
-  let(:metric) { LogStash::Instrument::NullMetric.new }
+  let(:collector) { [] }
+  let(:metric) { LogStash::Instrument::NamespacedNullMetric.new(collector, :null) }
   let(:events) { [LogStash::Event.new, LogStash::Event.new] }
+  let(:default_execution_context) { LogStash::ExecutionContext.new(:main) }
 
   before :each do
     allow(metric).to receive(:namespace).with(anything).and_return(metric)
@@ -25,11 +28,11 @@ def register; end
     end
   end
 
-  subject { described_class.new(logger, plugin_klass, metric, config) }
+  subject { described_class.new(logger, plugin_klass, metric, default_execution_context, config) }
 
   it "create a plugin with the passed options" do
     expect(plugin_klass).to receive(:new).with(config).and_return(plugin_klass.new(config))
-    described_class.new(logger, plugin_klass, metric, config)
+    described_class.new(logger, plugin_klass, metric, default_execution_context, config)
   end
 
   context "when the plugin support flush" do
@@ -67,15 +70,22 @@ def filter(event)
     end
 
     context "when the filter buffer events" do
-      it "doesn't increment out" do
+      before do
+        allow(metric).to receive(:increment).with(anything, anything)
+      end
+
+      it "has incremented :in" do
         expect(metric).to receive(:increment).with(:in, events.size)
-        expect(metric).not_to receive(:increment)
+        subject.multi_filter(events)
+      end
 
+      it "has not incremented :out" do
+        expect(metric).not_to receive(:increment).with(:out, anything)
         subject.multi_filter(events)
       end
     end
 
-    context "when the fitler create more events" do
+    context "when the filter create more events" do
       let(:plugin_klass) do
         Class.new(LogStash::Filters::Base) do
           config_name "super_plugin"
@@ -91,6 +101,10 @@ def filter(event)
         end
       end
 
+      before do
+        allow(metric).to receive(:increment).with(anything, anything)
+      end
+
       it "increments the in/out of the metric" do
         expect(metric).to receive(:increment).with(:in, events.size)
         expect(metric).to receive(:increment).with(:out, events.size * 2)
@@ -112,6 +126,10 @@ def filter(event)
       end
     end
 
+    before do
+      allow(metric).to receive(:increment).with(anything, anything)
+    end
+
     it "doesnt define a flush method" do
       expect(subject.respond_to?(:flush)).to be_falsey
     end
diff --git a/logstash-core/spec/logstash/filters/base_spec.rb b/logstash-core/spec/logstash/filters/base_spec.rb
index 177c44dcb8c..b3e27a333f3 100644
--- a/logstash-core/spec/logstash/filters/base_spec.rb
+++ b/logstash-core/spec/logstash/filters/base_spec.rb
@@ -62,7 +62,7 @@ def filter(event)
     CONFIG
 
     sample "example" do
-      insist { subject["new_field"] } == ["new_value", "new_value_2"]
+      insist { subject.get("new_field") } == ["new_value", "new_value_2"]
     end
   end
 
@@ -76,7 +76,7 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
   end
 
@@ -90,11 +90,44 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "test"]
+    end
+  end
+
+  describe "tags parsing with one tag as string value" do
+    config <<-CONFIG
+    filter {
+      noop {
+        add_tag => ["bar"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop") do
+      insist { subject.get("tags") } == ["bar"]
+    end
+
+    sample("type" => "noop", "tags" => "foo") do
+      insist { subject.get("tags") } == ["foo", "bar"]
+    end
+  end
+
+  describe "tags parsing with duplicate tags" do
+    config <<-CONFIG
+    filter {
+      noop {
+        add_tag => ["foo"]
+      }
+    }
+    CONFIG
+
+    sample("type" => "noop", "tags" => "foo") do
+      # this is completely weird but seems to be already expected in other specs
+      insist { subject.get("tags") } == ["foo", "foo"]
     end
   end
 
@@ -108,19 +141,19 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1"]) do
-      insist { subject["tags"] } == ["t1", "test"]
+      insist { subject.get("tags") } == ["t1", "test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1", "t2", "t3", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "t3", "test"]
     end
   end
 
@@ -133,28 +166,40 @@ def filter(event)
     }
     CONFIG
 
+    sample("type" => "noop", "tags" => "foo") do
+      insist { subject.get("tags") } == ["foo"]
+    end
+
+    sample("type" => "noop", "tags" => "t2") do
+      insist { subject.get("tags") } == []
+    end
+
+    sample("type" => "noop", "tags" => ["t2"]) do
+      insist { subject.get("tags") } == []
+    end
+
     sample("type" => "noop", "tags" => ["t4"]) do
-      insist { subject["tags"] } == ["t4"]
+      insist { subject.get("tags") } == ["t4"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"t2\", \"t3\"]}")) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"t2\"]}")) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
   end
 
@@ -168,13 +213,13 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop", "tags" => ["t1", "goaway", "t3"], "blackhole" => "goaway") do
-      insist { subject["tags"] } == ["t1", "t3"]
+      insist { subject.get("tags") } == ["t1", "t3"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"goaway\", \"t3\"], \"blackhole\":\"goaway\"}")) do
-      insist { subject["tags"] } == ["t1", "t3"]
+      insist { subject.get("tags") } == ["t1", "t3"]
     end
   end
 
@@ -203,7 +248,21 @@ def filter(event)
     end
   end
 
- describe "remove_field on deep objects" do
+  describe "remove_field on tags" do
+    config <<-CONFIG
+    filter {
+      noop {
+        remove_field => ["tags"]
+      }
+    }
+    CONFIG
+
+    sample("tags" => "foo") do
+      reject { subject }.include?("tags")
+    end
+  end
+
+  describe "remove_field on deep objects" do
     config <<-CONFIG
     filter {
       noop {
@@ -230,7 +289,7 @@ def filter(event)
 
     sample("type" => "noop", "t1" => ["t2", "t3"]) do
       insist { subject }.include?("t1")
-      insist { subject["[t1][0]"] } == "t3"
+      insist { subject.get("[t1][0]") } == "t3"
     end
   end
 
diff --git a/logstash-core/spec/logstash/inputs/base_spec.rb b/logstash-core/spec/logstash/inputs/base_spec.rb
index d87f07b49f6..0e2ada3a4f7 100644
--- a/logstash-core/spec/logstash/inputs/base_spec.rb
+++ b/logstash-core/spec/logstash/inputs/base_spec.rb
@@ -1,5 +1,7 @@
 # encoding: utf-8
 require "spec_helper"
+require "logstash/execution_context"
+require "logstash/inputs/base"
 
 # use a dummy NOOP input to test Inputs::Base
 class LogStash::Inputs::NOOP < LogStash::Inputs::Base
@@ -15,50 +17,89 @@ def register; end
     input = LogStash::Inputs::NOOP.new("tags" => "value")
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value"])
+    expect(evt.get("tags")).to eq(["value"])
   end
 
   it "should add multiple tag" do
     input = LogStash::Inputs::NOOP.new("tags" => ["value1","value2"])
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value1","value2"])
+    expect(evt.get("tags")).to eq(["value1","value2"])
   end
 
   it "should allow duplicates  tag" do
     input = LogStash::Inputs::NOOP.new("tags" => ["value","value"])
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value","value"])
+    expect(evt.get("tags")).to eq(["value","value"])
   end
 
   it "should add tag with sprintf" do
     input = LogStash::Inputs::NOOP.new("tags" => "%{type}")
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["noop"])
+    expect(evt.get("tags")).to eq(["noop"])
   end
 
   it "should add single field" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"field" => "value"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["field"]).to eq("value")
+    expect(evt.get("field")).to eq("value")
   end
 
   it "should add single field with sprintf" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"%{type}" => "%{type}"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["noop"]).to eq("noop")
+    expect(evt.get("noop")).to eq("noop")
   end
 
   it "should add multiple field" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"field" => ["value1", "value2"], "field2" => "value"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["field"]).to eq(["value1","value2"])
-    expect(evt["field2"]).to eq("value")
+    expect(evt.get("field")).to eq(["value1","value2"])
+    expect(evt.get("field2")).to eq("value")
+  end
+
+  context "execution context" do
+    let(:default_execution_context) { LogStash::ExecutionContext.new(:main) }
+    let(:klass) { LogStash::Inputs::NOOP }
+
+    subject(:instance) { klass.new({}) }
+
+    it "allow to set the context" do
+      expect(instance.execution_context).to be_nil
+      instance.execution_context = default_execution_context
+
+      expect(instance.execution_context).to eq(default_execution_context)
+    end
+
+    it "propagate the context to the codec" do
+      expect(instance.codec.execution_context).to be_nil
+      instance.execution_context = default_execution_context
+
+      expect(instance.codec.execution_context).to eq(default_execution_context)
+    end
+  end
+
+  describe "cloning" do
+    let(:input) do
+      LogStash::Inputs::NOOP.new("add_field" => {"field" => ["value1", "value2"], "field2" => "value"})
+    end
+    
+    let(:cloned) do
+      input.clone
+    end
+    
+    it "should clone the codec when cloned" do
+      expect(input.codec).not_to eq(cloned.codec)
+    end  
+    
+    it "should preserve codec params" do
+      expect(input.codec.params).to eq(cloned.codec.params)
+    end
   end
 end
 
diff --git a/logstash-core/spec/logstash/inputs/metrics_spec.rb b/logstash-core/spec/logstash/inputs/metrics_spec.rb
deleted file mode 100644
index 97a89facda3..00000000000
--- a/logstash-core/spec/logstash/inputs/metrics_spec.rb
+++ /dev/null
@@ -1,52 +0,0 @@
-# encoding: utf-8
-require "logstash/inputs/metrics"
-require "spec_helper"
-
-describe LogStash::Inputs::Metrics do
-  before :each do
-    LogStash::Instrument::Collector.instance.clear
-  end
-
-  let(:queue) { [] }
-
-  describe "#run" do
-    it "should register itself to the collector observer" do
-      expect(LogStash::Instrument::Collector.instance).to receive(:add_observer).with(subject)
-      t = Thread.new { subject.run(queue) }
-      sleep(0.1) # give a bit of time to the thread to start
-      subject.stop
-    end
-  end
-
-  describe "#update" do
-    let(:namespaces)  { [:root, :base] }
-    let(:key)        { :foo }
-    let(:metric_store) { LogStash::Instrument::MetricStore.new }
-
-    it "should fill up the queue with received events" do
-      Thread.new { subject.run(queue) }
-      sleep(0.1)
-      subject.stop
-
-      metric_store.fetch_or_store(namespaces, key, LogStash::Instrument::MetricType::Counter.new(namespaces, key))
-      subject.update(LogStash::Instrument::Snapshot.new(metric_store))
-      expect(queue.count).to eq(1)
-    end
-  end
-
-  describe "#stop" do
-    it "should remove itself from the the collector observer" do
-      expect(LogStash::Instrument::Collector.instance).to receive(:delete_observer).with(subject)
-      t = Thread.new { subject.run(queue) }
-      sleep(0.1) # give a bit of time to the thread to start
-      subject.stop
-    end
-
-    it "should unblock the input" do
-      t = Thread.new { subject.run(queue) }
-      sleep(0.1) # give a bit of time to the thread to start
-      subject.do_stop
-      wait_for { t.status }.to be_falsey
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/instrument/collector_spec.rb b/logstash-core/spec/logstash/instrument/collector_spec.rb
index b96be4a5ede..b5c9b3073de 100644
--- a/logstash-core/spec/logstash/instrument/collector_spec.rb
+++ b/logstash-core/spec/logstash/instrument/collector_spec.rb
@@ -3,7 +3,7 @@
 require "spec_helper"
 
 describe LogStash::Instrument::Collector do
-  subject { LogStash::Instrument::Collector.instance }
+  subject { LogStash::Instrument::Collector.new }
   describe "#push" do
     let(:namespaces_path) { [:root, :pipelines, :pipelines01] }
     let(:key) { :my_key }
@@ -45,5 +45,9 @@
     it "return a `LogStash::Instrument::MetricStore`" do
       expect(subject.snapshot_metric).to be_kind_of(LogStash::Instrument::Snapshot)
     end
+
+    it "returns a clone of the metric store" do
+      expect(subject.snapshot_metric).not_to eq(subject.snapshot_metric)
+    end
   end
 end
diff --git a/logstash-core/spec/logstash/instrument/metric_spec.rb b/logstash-core/spec/logstash/instrument/metric_spec.rb
index 0a8a65d4338..123a47be268 100644
--- a/logstash-core/spec/logstash/instrument/metric_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_spec.rb
@@ -67,15 +67,15 @@
 
   context "#time" do
     let(:sleep_time) { 2 }
-    let(:sleep_time_ms) { sleep_time * 1_000_000 }
+    let(:sleep_time_ms) { sleep_time * 1_000 }
 
     it "records the duration" do
       subject.time(:root, :duration_ms) { sleep(sleep_time) }
 
-      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5000)
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5)
       expect(collector[0]).to match(:root)
       expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:mean)
+      expect(collector[2]).to be(:counter)
     end
 
     it "returns the value of the executed block" do
@@ -85,12 +85,13 @@
     it "return a TimedExecution" do
       execution = subject.time(:root, :duration_ms)
       sleep(sleep_time)
-      execution.stop
+      execution_time = execution.stop
 
+      expect(execution_time).to eq(collector.last)
       expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 0.1)
       expect(collector[0]).to match(:root)
       expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:mean)
+      expect(collector[2]).to be(:counter)
     end
   end
 
diff --git a/logstash-core/spec/logstash/instrument/metric_store_spec.rb b/logstash-core/spec/logstash/instrument/metric_store_spec.rb
index 4371977355b..3b655bc3e9d 100644
--- a/logstash-core/spec/logstash/instrument/metric_store_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_store_spec.rb
@@ -53,6 +53,20 @@
       end
     end
 
+    context "#has_metric?" do
+      context "when the path exist" do
+        it "returns true" do
+          expect(subject.has_metric?(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch", :event_in)).to be_truthy
+        end
+      end
+
+      context "when the path doesn't exist" do
+        it "returns false" do
+          expect(subject.has_metric?(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-input-nothing")).to be_falsey
+        end
+      end
+    end
+
     describe "#get" do
       context "when the path exist" do
         it "retrieves end of of a branch" do
@@ -142,6 +156,73 @@
       end
     end
 
+    describe "get_shallow" do
+      it "should retrieve a path as a single value" do
+        r = subject.get_shallow(:node, :sashimi, :pipelines, :pipeline01, :processed_events_in)
+        expect(r.value).to eql(1)
+      end
+    end
+
+    describe "extract_metrics" do
+      it "should retrieve non-nested values correctly" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines, :pipeline01],
+          :processed_events_in,
+          :processed_events_out,
+        )
+        expect(r[:processed_events_in]).to eql(1)
+        expect(r[:processed_events_out]).to eql(1)
+      end
+
+      it "should retrieve nested values correctly alongside non-nested ones" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines, :pipeline01],
+          :processed_events_in,
+          [:plugins, :"logstash-output-elasticsearch", :event_in]
+        )
+       expect(r[:processed_events_in]).to eql(1)
+        expect(r[:plugins][:"logstash-output-elasticsearch"][:event_in]).to eql(1)
+      end
+
+      it "should retrieve multiple nested keys at a given location" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines],
+          [:pipeline01, [:processed_events_in, :processed_events_out]]
+        )
+
+        expect(r[:pipeline01][:processed_events_in]).to eql(1)
+        expect(r[:pipeline01][:processed_events_out]).to eql(1)
+      end
+
+      it "should retrieve a single key nested in multiple places" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines],
+          [[:pipeline01, :pipeline02], :processed_events_out]
+        )
+
+        expect(r[:pipeline01][:processed_events_out]).to eql(1)
+        expect(r[:pipeline02][:processed_events_out]).to eql(1)
+      end
+
+      it "handle overlaps of paths" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines],
+          [:pipeline01, :processed_events_in],
+          [[:pipeline01, :pipeline02], :processed_events_out]
+        )
+
+        expect(r[:pipeline01][:processed_events_in]).to eql(1)
+        expect(r[:pipeline01][:processed_events_out]).to eql(1)
+        expect(r[:pipeline02][:processed_events_out]).to eql(1)
+      end
+    end
+
+    describe "#size" do
+      it "returns the number of unique metrics" do
+        expect(subject.size).to eq(metric_events.size)
+      end
+    end
+
     describe "#each" do
       it "retrieves all the metric" do
         expect(subject.each.size).to eq(metric_events.size)
@@ -160,4 +241,35 @@
       end
     end
   end
+
+  describe "#prune" do
+    let(:metric_events) {
+      [
+        [[:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch"], :event_in, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_in, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_out, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline02], :processed_events_out, :increment],
+      ]
+    }
+
+    before :each do
+      # Lets add a few metrics in the store before trying to find them
+      metric_events.each do |namespaces, metric_key, action|
+        metric = subject.fetch_or_store(namespaces, metric_key, LogStash::Instrument::MetricType::Counter.new(namespaces, metric_key))
+        metric.execute(action)
+      end
+    end
+
+    it "should remove all keys with the same starting path as the argument" do
+      expect(subject.get(:node, :sashimi, :pipelines, :pipeline01)).to be_a(Hash)
+      subject.prune("/node/sashimi/pipelines/pipeline01")
+      expect { subject.get(:node, :sashimi, :pipelines, :pipeline01) }.to raise_error LogStash::Instrument::MetricStore::MetricNotFound
+    end
+
+    it "should keep other metrics on different path branches" do
+      expect(subject.get(:node, :sashimi, :pipelines, :pipeline02)).to be_a(Hash)
+      subject.prune("/node/sashimi/pipelines/pipeline01")
+      expect { subject.get(:node, :sashimi, :pipelines, :pipeline02) }.to_not raise_error
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
index b51aebc792d..05d9054069d 100644
--- a/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
@@ -28,12 +28,7 @@
 
   context "When creating a hash " do
     it "creates the hash from all the values" do
-      metric_hash = {
-        "key" => key,
-        "namespaces" => namespaces,
-        "value" => 0,
-        "type" => "counter"
-      }
+      metric_hash = { key => 0 }
       expect(subject.to_hash).to match(metric_hash)
     end
   end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
index 0481f6d283b..e285a8eb5cf 100644
--- a/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
@@ -29,10 +29,7 @@
   context "When creating a hash " do
     it "creates the hash from all the values" do
       metric_hash = {
-        "key" => key,
-        "namespaces" => namespaces,
-        "value" => value,
-        "type" => "gauge"
+        key => value
       }
       expect(subject.to_hash).to match(metric_hash)
     end
diff --git a/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb b/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
index 6ba84168df9..b4446afbfce 100644
--- a/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
+++ b/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
@@ -2,10 +2,11 @@
 require "logstash/instrument/namespaced_metric"
 require "logstash/instrument/metric"
 require_relative "../../support/matchers"
+require_relative "../../support/shared_examples"
 require "spec_helper"
 
 describe LogStash::Instrument::NamespacedMetric do
-  let(:namespace) { :stats }
+  let(:namespace) { :root }
   let(:collector) { [] }
   let(:metric) { LogStash::Instrument::Metric.new(collector) }
 
@@ -22,4 +23,70 @@
   it "returns the value of the block" do
     expect(subject.time(:duration_ms) { "hello" }).to eq("hello")
   end
+
+  it "its doesnt change the original `namespace` when creating a subnamespace" do
+    new_namespace = subject.namespace(:wally)
+
+    expect(subject.namespace_name).to eq([namespace])
+    expect(new_namespace.namespace_name).to eq([:root, :wally])
+  end
+
+  context "#increment" do
+    it "a counter by 1" do
+      metric = subject.increment(:error_rate)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 1)
+    end
+
+    it "a counter by a provided value" do
+      metric = subject.increment(:error_rate, 20)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 20)
+    end
+  end
+
+  context "#decrement" do
+    it "a counter by 1" do
+      metric = subject.decrement(:error_rate)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 1)
+    end
+
+    it "a counter by a provided value" do
+      metric = subject.decrement(:error_rate, 20)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 20)
+    end
+  end
+
+  context "#gauge" do
+    it "set the value of a key" do
+      metric = subject.gauge(:size_queue, 20)
+      expect(collector).to be_a_metric_event([:root, :size_queue], :gauge, :set, 20)
+    end
+  end
+
+  context "#time" do
+    let(:sleep_time) { 2 }
+    let(:sleep_time_ms) { sleep_time * 1_000 }
+
+    it "records the duration" do
+      subject.time(:duration_ms) { sleep(sleep_time) }
+
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5)
+      expect(collector[0]).to match([:root])
+      expect(collector[1]).to be(:duration_ms)
+      expect(collector[2]).to be(:counter)
+    end
+
+    it "return a TimedExecution" do
+      execution = subject.time(:duration_ms)
+      sleep(sleep_time)
+      execution_time = execution.stop
+
+      expect(execution_time).to eq(collector.last)
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 0.1)
+      expect(collector[0]).to match([:root])
+      expect(collector[1]).to be(:duration_ms)
+      expect(collector[2]).to be(:counter)
+    end
+  end
+
+  include_examples "metrics commons operations"
 end
diff --git a/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb b/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb
new file mode 100644
index 00000000000..fdd831dfbfc
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require "logstash/instrument/namespaced_null_metric"
+require "logstash/instrument/null_metric"
+require_relative "../../support/matchers"
+require "spec_helper"
+
+describe LogStash::Instrument::NamespacedNullMetric do
+  let(:namespace) { :root }
+  let(:collector) { [] }
+  let(:metric) { LogStash::Instrument::NullMetric.new(collector) }
+
+  subject { described_class.new(metric, namespace) }
+
+  it "defines the same interface as `Metric`" do
+    expect(described_class).to implement_interface_of(LogStash::Instrument::NamespacedMetric)
+  end
+
+  it "returns a TimedException when we call without a block" do
+    expect(subject.time(:duration_ms)).to be(LogStash::Instrument::NullMetric::NullTimedExecution)
+  end
+
+  it "returns the value of the block" do
+    expect(subject.time(:duration_ms) { "hello" }).to eq("hello")
+  end
+
+  it "its doesnt change the original `namespace` when creating a subnamespace" do
+    new_namespace = subject.namespace(:wally)
+
+    expect(subject.namespace_name).to eq([namespace])
+    expect(new_namespace.namespace_name).to eq([:root, :wally])
+  end
+
+end
diff --git a/logstash-core/spec/logstash/instrument/null_metric_spec.rb b/logstash-core/spec/logstash/instrument/null_metric_spec.rb
index ec55d341be4..27a861eae69 100644
--- a/logstash-core/spec/logstash/instrument/null_metric_spec.rb
+++ b/logstash-core/spec/logstash/instrument/null_metric_spec.rb
@@ -1,21 +1,23 @@
 # encoding: utf-8
 require "logstash/instrument/null_metric"
 require "logstash/instrument/namespaced_metric"
+require_relative "../../support/shared_examples"
 require_relative "../../support/matchers"
+require "spec_helper"
 
 describe LogStash::Instrument::NullMetric do
+
+  let(:key) { "test" }
+  let(:collector) { [] }
+  subject { LogStash::Instrument::NullMetric.new(collector) }
+
   it "defines the same interface as `Metric`" do
-    expect(described_class).to implement_interface_of(LogStash::Instrument::NamespacedMetric)
+    expect(described_class).to implement_interface_of(LogStash::Instrument::Metric)
   end
 
-  describe "#time" do
-    it "returns the value of the block without recording any metrics" do
-      expect(subject.time(:execution_time) { "hello" }).to eq("hello")
-    end
-
-    it "return a TimedExecution" do
-      execution = subject.time(:do_something)
-      expect { execution.stop }.not_to raise_error
+  describe "#namespace" do
+    it "return a NamespacedNullMetric" do
+      expect(subject.namespace(key)).to be_kind_of LogStash::Instrument::NamespacedNullMetric
     end
   end
 end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/base_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/base_spec.rb
new file mode 100644
index 00000000000..d0a869aa767
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/base_spec.rb
@@ -0,0 +1,32 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/base"
+require "logstash/instrument/metric"
+require "logstash/instrument/collector"
+
+describe LogStash::Instrument::PeriodicPoller::Base do
+  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+  let(:options) { {} }
+
+  subject { described_class.new(metric, options) }
+
+  describe "#update" do
+    it "logs an timeout exception to debug level" do
+      exception = Concurrent::TimeoutError.new
+      expect(subject.logger).to receive(:debug).with(anything, hash_including(:exception => exception.class))
+      subject.update(Time.now, "hola", exception)
+    end
+
+    it "logs any other exception to error level" do
+      exception = Class.new
+      expect(subject.logger).to receive(:error).with(anything, hash_including(:exception => exception.class))
+      subject.update(Time.now, "hola", exception)
+    end
+
+    it "doesnt log anything when no exception is received" do
+      exception = Concurrent::TimeoutError.new
+      expect(subject.logger).not_to receive(:debug).with(anything)
+      expect(subject.logger).not_to receive(:error).with(anything)
+      subject.update(Time.now, "hola", exception)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb
new file mode 100644
index 00000000000..639ab0d8a9d
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb
@@ -0,0 +1,148 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/cgroup"
+require "spec_helper"
+
+describe LogStash::Instrument::PeriodicPoller::Cgroup do
+  subject { described_class }
+
+  context ".are_cgroup_available?" do
+    context "all the file exist" do
+      before do
+        allow(::File).to receive(:exist?).with(subject::PROC_SELF_CGROUP_FILE).and_return(true)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPU_DIR).and_return(true)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPUACCT_DIR).and_return(true)
+      end
+
+      it "returns true" do
+        expect(subject.are_cgroup_available?).to be_truthy
+      end
+    end
+
+    context "not all the file exist" do
+      before do
+        allow(::File).to receive(:exist?).with(subject::PROC_SELF_CGROUP_FILE).and_return(true)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPU_DIR).and_return(false)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPUACCT_DIR).and_return(true)
+      end
+
+      it "returns false" do
+        expect(subject.are_cgroup_available?).to be_falsey
+      end
+    end
+  end
+
+  context ".control_groups" do
+    let(:proc_self_cgroup_content) {
+      %w(14:name=systemd,holaunlimited:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+13:pids:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+12:hugetlb:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+11:net_prio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+10:perf_event:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+9:net_cls:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+8:freezer:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+7:devices:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+6:memory:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+5:blkio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+4:cpuacct:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+3:cpu:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+2:cpuset:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+1:name=openrc:/docker) }
+
+    before do
+      allow(subject).to receive(:read_proc_self_cgroup_lines).and_return(proc_self_cgroup_content)
+    end
+
+    it "returns the control groups" do
+      expect(subject.control_groups).to match({
+        "name=systemd" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "holaunlimited" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "pids" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "hugetlb" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "net_prio" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "perf_event" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "net_cls" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "freezer" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "devices" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "memory" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "blkio" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "cpuacct" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "cpu" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "cpuset" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "name=openrc" => "/docker"
+      })
+    end
+  end
+
+  context ".get_all" do
+    context "when we can retrieve the stats" do
+      let(:cpuacct_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+      let(:cpuacct_usage) { 1982 }
+      let(:cpu_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+      let(:cfs_period_micros) { 500 }
+      let(:cfs_quota_micros) { 98 }
+      let(:cpu_stats_number_of_periods) { 1 }
+      let(:cpu_stats_number_of_time_throttled) { 2 }
+      let(:cpu_stats_time_throttled_nanos) { 3 }
+      let(:proc_self_cgroup_content) {
+        %W(14:name=systemd,holaunlimited:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+13:pids:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+12:hugetlb:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+11:net_prio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+10:perf_event:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+9:net_cls:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+8:freezer:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+7:devices:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+6:memory:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+5:blkio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+4:cpuacct:#{cpuacct_control_group}
+3:cpu:#{cpu_control_group}
+2:cpuset:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+1:name=openrc:/docker) }
+      let(:cpu_stat_file_content) {
+        [
+          "nr_periods #{cpu_stats_number_of_periods}",
+          "nr_throttled #{cpu_stats_number_of_time_throttled}",
+          "throttled_time #{cpu_stats_time_throttled_nanos}"
+        ]
+      }
+
+      before do
+        allow(subject).to receive(:read_proc_self_cgroup_lines).and_return(proc_self_cgroup_content)
+        allow(subject).to receive(:read_sys_fs_cgroup_cpuacct_cpu_stat).and_return(cpu_stat_file_content)
+
+        allow(subject).to receive(:cgroup_cpuacct_usage_nanos).with(cpuacct_control_group).and_return(cpuacct_usage)
+        allow(subject).to receive(:cgroup_cpu_fs_period_micros).with(cpu_control_group).and_return(cfs_period_micros)
+        allow(subject).to receive(:cgroup_cpu_fs_quota_micros).with(cpu_control_group).and_return(cfs_quota_micros)
+      end
+
+      it "returns all the stats" do
+        expect(subject.get_all).to match(
+          :cpuacct => {
+            :control_group => cpuacct_control_group,
+            :usage_nanos => cpuacct_usage,
+          },
+          :cpu => {
+            :control_group => cpu_control_group,
+            :cfs_period_micros => cfs_period_micros,
+            :cfs_quota_micros => cfs_quota_micros,
+            :stat => {
+                :number_of_elapsed_periods => cpu_stats_number_of_periods,
+                :number_of_times_throttled => cpu_stats_number_of_time_throttled,
+                :time_throttled_nanos => cpu_stats_time_throttled_nanos
+            }
+          }
+        )
+      end
+    end
+
+    context "when an exception is raised" do
+      before do
+        allow(subject).to receive(:control_groups).and_raise("Something went wrong")
+      end
+
+      it "returns nil" do
+        expect(subject.get_all).to be_nil
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb
new file mode 100644
index 00000000000..d233803cc19
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb
@@ -0,0 +1,150 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/instrument/periodic_poller/jvm"
+require "logstash/instrument/collector"
+require "logstash/environment"
+
+describe LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName do
+  subject { LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName }
+
+  context "when the gc is of young type" do
+    LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName::YOUNG_GC_NAMES.each do |name|
+      it "returns young for #{name}" do
+        expect(subject.get(name)).to eq(:young)
+      end
+    end
+  end
+
+  context "when the gc is of old type" do
+    LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName::OLD_GC_NAMES.each do |name|
+      it "returns old for #{name}" do
+        expect(subject.get(name)).to eq(:old)
+      end
+    end
+  end
+
+  it "returns `nil` when we dont know the gc name" do
+      expect(subject.get("UNKNOWN GC")).to be_nil
+  end
+end
+
+
+describe LogStash::Instrument::PeriodicPoller::JVM do
+  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+  let(:options) { {} }
+  subject(:jvm) { described_class.new(metric, options) }
+
+  it "should initialize cleanly" do
+    expect { jvm }.not_to raise_error
+  end
+
+  describe "load average" do
+    context "on linux" do
+      context "when an exception occur reading the file" do
+        before do
+          expect(LogStash::Environment).to receive(:windows?).and_return(false)
+          expect(LogStash::Environment).to receive(:linux?).and_return(true)
+          expect(::File).to receive(:read).with("/proc/loadavg").and_raise("Didnt work out so well")
+        end
+
+        it "doesn't raise an exception" do
+          expect { subject.collect }.not_to raise_error
+        end
+      end
+    end
+  end
+
+  describe "aggregate heap information" do
+    shared_examples "heap_information" do
+      let(:data_set) do
+        {
+          "usage.used" => 5,
+          "usage.committed" => 11,
+          "usage.max" => 21,
+          "peak.max" => 51,
+          "peak.used" => 61
+        }
+      end
+      let(:collection) { [data_set] }
+
+      it "return the right values" do
+        expect(subject.aggregate_information_for(collection)).to match({
+          :used_in_bytes => 5 * collection.size,
+          :committed_in_bytes => 11 * collection.size,
+          :max_in_bytes => 21 * collection.size,
+          :peak_max_in_bytes => 51 * collection.size,
+          :peak_used_in_bytes => 61 * collection.size
+        })
+      end
+    end
+
+    context "with only one data set in a collection" do
+      include_examples "heap_information"
+    end
+
+    context "with multiples data set in a collection" do
+      include_examples "heap_information" do
+        let(:collection) { ar = []; ar << data_set; ar << data_set; ar }
+      end
+    end
+  end
+
+  describe "collections" do
+    subject(:collection) { jvm.collect }
+    it "should run cleanly" do
+      expect { collection }.not_to raise_error
+    end
+
+    describe "metrics" do
+      before(:each) { jvm.collect }
+      let(:snapshot_store) { metric.collector.snapshot_metric.metric_store }
+      subject(:jvm_metrics) { snapshot_store.get_shallow(:jvm) }
+
+      # Make looking up metric paths easy when given varargs of keys
+      # e.g. mval(:parent, :child)
+      def mval(*metric_path)
+        metric_path.reduce(jvm_metrics) {|acc,k| acc[k]}.value
+      end
+
+      [
+        [:process, :max_file_descriptors],
+        [:process, :open_file_descriptors],
+        [:process, :peak_open_file_descriptors],
+        [:process, :mem, :total_virtual_in_bytes],
+        [:process, :cpu, :total_in_millis],
+        [:process, :cpu, :percent],
+        [:gc, :collectors, :young, :collection_count],
+        [:gc, :collectors, :young, :collection_time_in_millis],
+        [:gc, :collectors, :old, :collection_count],
+        [:gc, :collectors, :old, :collection_time_in_millis]
+      ].each do |path|
+        path = Array(path)
+        it "should have a value for #{path} that is Numeric" do
+          expect(mval(*path)).to be_a(Numeric)
+        end
+      end
+
+      context "real system" do
+        if LogStash::Environment.linux?
+          context "Linux" do
+            it "returns the load avg" do
+              expect(subject[:process][:cpu][:load_average].value).to include(:"1m" => a_kind_of(Numeric), :"5m" => a_kind_of(Numeric), :"15m" => a_kind_of(Numeric))
+            end
+          end
+        elsif LogStash::Environment.windows?
+          context "Window" do
+            it "returns nothing" do
+              expect(subject[:process][:cpu].has_key?(:load_average)).to be_falsey
+            end
+          end
+        else
+          context "Other" do
+            it "returns 1m only" do
+              expect(subject[:process][:cpu][:load_average].value).to include(:"1m" => a_kind_of(Numeric))
+            end
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/load_average_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/load_average_spec.rb
new file mode 100644
index 00000000000..7063466e303
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/load_average_spec.rb
@@ -0,0 +1,87 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/load_average"
+describe LogStash::Instrument::PeriodicPoller::LoadAverage do
+  subject { described_class.create }
+
+  context "on mocked system" do
+    context "on Linux" do
+      before do
+        expect(LogStash::Environment).to receive(:windows?).and_return(false)
+        expect(LogStash::Environment).to receive(:linux?).and_return(true)
+      end
+
+      context "when it can read the file" do
+        let(:proc_loadavg) { "0.00 0.01 0.05 3/180 29727" }
+
+        it "return the 3 load average from `/proc/loadavg`" do
+          avg_1m, avg_5m, avg_15m = proc_loadavg.chomp.split(" ")
+
+          expect(subject.get(proc_loadavg)).to include(:"1m" => avg_1m.to_f, :"5m" => avg_5m.to_f, :"15m" => avg_15m.to_f)
+        end
+      end
+    end
+
+    context "on windows" do
+      before do
+        expect(LogStash::Environment).to receive(:windows?).and_return(true)
+      end
+
+      it "Xreturns nil" do
+        expect(subject.get).to be_nil
+      end
+    end
+
+    context "on other" do
+      before do
+        expect(LogStash::Environment).to receive(:windows?).and_return(false)
+        expect(LogStash::Environment).to receive(:linux?).and_return(false)
+      end
+
+      context "when 'OperatingSystemMXBean.getSystemLoadAverage' return something" do
+        let(:load_avg) { 5 }
+
+        before do
+          expect(ManagementFactory).to receive(:getOperatingSystemMXBean).and_return(double("OperatingSystemMXBean", :getSystemLoadAverage => load_avg))
+        end
+
+        it "returns the value" do
+          expect(subject.get).to include(:"1m" => 5)
+        end
+      end
+
+      context "when 'OperatingSystemMXBean.getSystemLoadAverage' doesn't return anything" do
+        before do
+          expect(ManagementFactory).to receive(:getOperatingSystemMXBean).and_return(double("OperatingSystemMXBean", :getSystemLoadAverage => nil))
+        end
+
+        it "returns nothing" do
+          expect(subject.get).to be_nil
+        end
+      end
+    end
+  end
+
+  # Since we are running this on macos and linux I think it make sense to have real test
+  # insteadof only mock
+  context "real system" do
+    if LogStash::Environment.linux?
+      context "Linux" do
+        it "returns the load avg" do
+          expect(subject.get).to include(:"1m" => a_kind_of(Numeric), :"5m" => a_kind_of(Numeric), :"15m" => a_kind_of(Numeric))
+        end
+      end
+    elsif LogStash::Environment.windows?
+      context "window" do
+        it "returns nothing" do
+          expect(subject.get).to be_nil
+        end
+      end
+    else
+      context "Other" do
+        it "returns 1m only" do
+          expect(subject.get).to include(:"1m" => a_kind_of(Numeric))
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/os_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/os_spec.rb
new file mode 100644
index 00000000000..a8772aa6106
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/os_spec.rb
@@ -0,0 +1,85 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/os"
+require "logstash/instrument/metric"
+require "logstash/instrument/collector"
+
+describe LogStash::Instrument::PeriodicPoller::Os do
+  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+
+  context "recorded cgroup metrics (mocked cgroup env)" do
+    subject { described_class.new(metric, {})}
+
+    let(:snapshot_store) { metric.collector.snapshot_metric.metric_store }
+    let(:os_metrics) { snapshot_store.get_shallow(:os) }
+
+    let(:cpuacct_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+    let(:cpuacct_usage) { 1982 }
+    let(:cpu_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+    let(:cpu_period_micros) { 500 }
+    let(:cpu_quota_micros) { 98 }
+    let(:cpu_stats_number_of_periods) { 1 }
+    let(:cpu_stats_number_of_time_throttled) { 2 }
+    let(:cpu_stats_time_throttled_nanos) { 3 }
+    let(:proc_self_cgroup_content) {
+      %W(14:name=systemd,holaunlimited:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+13:pids:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+12:hugetlb:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+11:net_prio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+10:perf_event:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+9:net_cls:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+8:freezer:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+7:devices:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+6:memory:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+5:blkio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+4:cpuacct:#{cpuacct_control_group}
+3:cpu:#{cpu_control_group}
+2:cpuset:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+1:name=openrc:/docker) }
+    let(:cpu_stat_file_content) {
+      [
+        "nr_periods #{cpu_stats_number_of_periods}",
+        "nr_throttled #{cpu_stats_number_of_time_throttled}",
+        "throttled_time #{cpu_stats_time_throttled_nanos}"
+      ]
+    }
+
+    before do
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:are_cgroup_available?).and_return(true)
+
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:read_proc_self_cgroup_lines).and_return(proc_self_cgroup_content)
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:read_sys_fs_cgroup_cpuacct_cpu_stat).and_return(cpu_stat_file_content)
+
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:cgroup_cpuacct_usage_nanos).with(cpuacct_control_group).and_return(cpuacct_usage)
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:cgroup_cpu_fs_period_micros).with(cpu_control_group).and_return(cpu_period_micros)
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:cgroup_cpu_fs_quota_micros).with(cpu_control_group).and_return(cpu_quota_micros)
+
+      subject.collect
+    end
+
+    def mval(*metric_path)
+      metric_path.reduce(os_metrics) {|acc,k| acc[k]}.value
+    end
+
+    it "should have a value for #{[:cgroup, :cpuacc, :control_group]} that is a String" do
+      expect(mval(:cgroup, :cpuacct, :control_group)).to be_a(String)
+    end
+
+    it "should have a value for #{[:cgroup, :cpu, :control_group]} that is a String" do
+      expect(mval(:cgroup, :cpu, :control_group)).to be_a(String)
+    end
+
+    [
+      [:cgroup, :cpuacct, :usage_nanos],
+      [:cgroup, :cpu, :cfs_period_micros],
+      [:cgroup, :cpu, :cfs_quota_micros],
+      [:cgroup, :cpu, :stat, :number_of_elapsed_periods],
+      [:cgroup, :cpu, :stat, :number_of_times_throttled],
+      [:cgroup, :cpu, :stat, :time_throttled_nanos]
+    ].each do |path|
+      path = Array(path)
+      it "should have a value for #{path} that is Numeric" do
+        expect(mval(*path)).to be_a(Numeric)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/wrapped_write_client_spec.rb b/logstash-core/spec/logstash/instrument/wrapped_write_client_spec.rb
new file mode 100644
index 00000000000..d01ae4dd24b
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/wrapped_write_client_spec.rb
@@ -0,0 +1,128 @@
+# encoding: utf-8
+require "logstash/instrument/metric"
+require "logstash/instrument/wrapped_write_client"
+require "logstash/util/wrapped_synchronous_queue"
+require "logstash/event"
+require_relative "../../support/mocks_classes"
+require "spec_helper"
+
+describe LogStash::Instrument::WrappedWriteClient do
+  let!(:write_client) { queue.write_client }
+  let!(:read_client) { queue.read_client }
+  let(:pipeline) { double("pipeline", :pipeline_id => :main) }
+  let(:collector)   { LogStash::Instrument::Collector.new }
+  let(:metric) { LogStash::Instrument::Metric.new(collector) }
+  let(:plugin) { LogStash::Inputs::DummyInput.new({ "id" => myid }) }
+  let(:event) { LogStash::Event.new }
+  let(:myid) { "1234myid" }
+
+  subject { described_class.new(write_client, pipeline, metric, plugin) }
+
+  def threaded_read_client
+    Thread.new do
+      started_at = Time.now
+
+      batch_size = 0
+      loop {
+        if Time.now - started_at > 60
+          raise "Took too much time to read from the queue"
+        end
+        batch_size = read_client.read_batch.size
+
+        break if batch_size > 0
+      }
+      expect(batch_size).to eq(1)
+    end
+  end
+
+  shared_examples "queue tests" do
+    it "pushes single event to the `WriteClient`" do
+      pusher_thread = Thread.new(subject, event) do |_subject, _event|
+        _subject.push(_event)
+      end
+
+      reader_thread = threaded_read_client
+
+      [pusher_thread, reader_thread].collect(&:join)
+    end
+
+    it "pushes batch to the `WriteClient`" do
+      batch = write_client.get_new_batch
+      batch << event
+
+      pusher_thread = Thread.new(subject, batch) do |_subject, _batch|
+        _subject.push_batch(_batch)
+      end
+
+      reader_thread = threaded_read_client
+      [pusher_thread, reader_thread].collect(&:join)
+    end
+
+    context "recorded metrics" do
+      before do
+        pusher_thread = Thread.new(subject, event) do |_subject, _event|
+          _subject.push(_event)
+        end
+
+        reader_thread = threaded_read_client
+        [pusher_thread, reader_thread].collect(&:join)
+      end
+
+      let(:snapshot_store) { collector.snapshot_metric.metric_store }
+
+      let(:snapshot_metric) { snapshot_store.get_shallow(:stats) }
+
+      it "records instance level events `in`" do
+        expect(snapshot_metric[:events][:in].value).to eq(1)
+      end
+
+      it "records pipeline level `in`" do
+        expect(snapshot_metric[:pipelines][:main][:events][:in].value).to eq(1)
+      end
+
+      it "record input `out`" do
+        expect(snapshot_metric[:pipelines][:main][:plugins][:inputs][myid.to_sym][:events][:out].value).to eq(1)
+      end
+
+      context "recording of the duration of pushing to the queue" do
+        it "records at the `global events` level" do
+          expect(snapshot_metric[:events][:queue_push_duration_in_millis].value).to be_kind_of(Integer)
+        end
+
+        it "records at the `pipeline` level" do
+          expect(snapshot_metric[:pipelines][:main][:events][:queue_push_duration_in_millis].value).to be_kind_of(Integer)
+        end
+
+        it "records at the `plugin level" do
+          expect(snapshot_metric[:pipelines][:main][:plugins][:inputs][myid.to_sym][:events][:queue_push_duration_in_millis].value).to be_kind_of(Integer)
+        end
+      end
+    end
+  end
+
+  context "WrappedSynchronousQueue" do
+    let(:queue) { LogStash::Util::WrappedSynchronousQueue.new }
+
+    before do
+      read_client.set_events_metric(metric.namespace([:stats, :events]))
+      read_client.set_pipeline_metric(metric.namespace([:stats, :pipelines, :main, :events]))
+    end
+
+    include_examples "queue tests"
+  end
+
+  context "AckedMemoryQueue" do
+    let(:queue) { LogStash::Util::WrappedAckedQueue.create_memory_based("", 1024, 10, 4096) }
+
+    before do
+      read_client.set_events_metric(metric.namespace([:stats, :events]))
+      read_client.set_pipeline_metric(metric.namespace([:stats, :pipelines, :main, :events]))
+    end
+
+    after do
+      queue.close
+    end
+
+    include_examples "queue tests"
+  end
+end
diff --git a/logstash-core/spec/logstash/json_spec.rb b/logstash-core/spec/logstash/json_spec.rb
index 056325ac91b..3340c2e68ff 100644
--- a/logstash-core/spec/logstash/json_spec.rb
+++ b/logstash-core/spec/logstash/json_spec.rb
@@ -37,7 +37,7 @@
 
     ### JRuby specific
     # Former expectation in this code were removed because of https://github.com/rspec/rspec-mocks/issues/964
-    # as soon as is fix we can re introduce them if decired, however for now the completeness of the test
+    # as soon as is fix we can re introduce them if desired, however for now the completeness of the test
     # is also not affected as the conversion would not work if the expectation where not meet.
     ###
     context "jruby deserialize" do
diff --git a/logstash-core-event/spec/logstash/event_spec.rb b/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
similarity index 71%
rename from logstash-core-event/spec/logstash/event_spec.rb
rename to logstash-core/spec/logstash/legacy_ruby_event_spec.rb
index b0e4985bc07..481087ec768 100644
--- a/logstash-core-event/spec/logstash/event_spec.rb
+++ b/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
@@ -8,68 +8,68 @@
   shared_examples "all event tests" do
     context "[]=" do
       it "should raise an exception if you attempt to set @timestamp to a value type other than a Time object" do
-        expect{subject["@timestamp"] = "crash!"}.to raise_error(TypeError)
+        expect{subject.set("@timestamp", "crash!")}.to raise_error(TypeError)
       end
 
       it "should assign simple fields" do
-        expect(subject["foo"]).to be_nil
-        expect(subject["foo"] = "bar").to eq("bar")
-        expect(subject["foo"]).to eq("bar")
+        expect(subject.get("foo")).to be_nil
+        expect(subject.set("foo", "bar")).to eq("bar")
+        expect(subject.get("foo")).to eq("bar")
       end
 
       it "should overwrite simple fields" do
-        expect(subject["foo"]).to be_nil
-        expect(subject["foo"] = "bar").to eq("bar")
-        expect(subject["foo"]).to eq("bar")
+        expect(subject.get("foo")).to be_nil
+        expect(subject.set("foo", "bar")).to eq("bar")
+        expect(subject.get("foo")).to eq("bar")
 
-        expect(subject["foo"] = "baz").to eq("baz")
-        expect(subject["foo"]).to eq("baz")
+        expect(subject.set("foo", "baz")).to eq("baz")
+        expect(subject.get("foo")).to eq("baz")
       end
 
       it "should assign deep fields" do
-        expect(subject["[foo][bar]"]).to be_nil
-        expect(subject["[foo][bar]"] = "baz").to eq("baz")
-        expect(subject["[foo][bar]"]).to eq("baz")
+        expect(subject.get("[foo][bar]")).to be_nil
+        expect(subject.set("[foo][bar]", "baz")).to eq("baz")
+        expect(subject.get("[foo][bar]")).to eq("baz")
       end
 
       it "should overwrite deep fields" do
-        expect(subject["[foo][bar]"]).to be_nil
-        expect(subject["[foo][bar]"] = "baz").to eq("baz")
-        expect(subject["[foo][bar]"]).to eq("baz")
+        expect(subject.get("[foo][bar]")).to be_nil
+        expect(subject.set("[foo][bar]", "baz")).to eq("baz")
+        expect(subject.get("[foo][bar]")).to eq("baz")
 
-        expect(subject["[foo][bar]"] = "zab").to eq("zab")
-        expect(subject["[foo][bar]"]).to eq("zab")
+        expect(subject.set("[foo][bar]", "zab")).to eq("zab")
+        expect(subject.get("[foo][bar]")).to eq("zab")
       end
 
       it "allow to set the @metadata key to a hash" do
-        subject["@metadata"] = { "action" => "index" }
-        expect(subject["[@metadata][action]"]).to eq("index")
+        subject.set("@metadata", { "action" => "index" })
+        expect(subject.get("[@metadata][action]")).to eq("index")
       end
 
       it "should add key when setting nil value" do
-        subject["[baz]"] = nil
+        subject.set("[baz]", nil)
         expect(subject.to_hash).to include("baz" => nil)
       end
 
       it "should set nil element within existing array value" do
-        subject["[foo]"] = ["bar", "baz"]
+        subject.set("[foo]", ["bar", "baz"])
 
-        expect(subject["[foo][0]"] = nil).to eq(nil)
-        expect(subject["[foo]"]).to eq([nil, "baz"])
+        expect(subject.set("[foo][0]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil, "baz"])
       end
 
       it "should set nil in first element within empty array" do
-        subject["[foo]"] = []
+        subject.set("[foo]", [])
 
-        expect(subject["[foo][0]"] = nil).to eq(nil)
-        expect(subject["[foo]"]).to eq([nil])
+        expect(subject.set("[foo][0]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil])
       end
 
       it "should set nil in second element within empty array" do
-        subject["[foo]"] = []
+        subject.set("[foo]", [])
 
-        expect(subject["[foo][1]"] = nil).to eq(nil)
-        expect(subject["[foo]"]).to eq([nil, nil])
+        expect(subject.set("[foo][1]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil, nil])
       end
     end
 
@@ -79,7 +79,7 @@
         event = LogStash::Event.new({ "reference" => data })
         LogStash::Util::Decorators.add_fields({"reference_test" => "%{reference}"}, event, "dummy-plugin")
         data.downcase!
-        expect(event["reference_test"]).not_to eq(data)
+        expect(event.get("reference_test")).not_to eq(data)
       end
 
       it "should not return a Fixnum reference" do
@@ -87,7 +87,7 @@
         event = LogStash::Event.new({ "reference" => data })
         LogStash::Util::Decorators.add_fields({"reference_test" => "%{reference}"}, event, "dummy-plugin")
         data += 41
-        expect(event["reference_test"]).to eq("1")
+        expect(event.get("reference_test")).to eq("1")
       end
 
       it "should report a unix timestamp for %{+%s}" do
@@ -124,7 +124,7 @@
 
       it "should report fields with %{field} syntax" do
         expect(subject.sprintf("%{type}")).to eq("sprintf")
-        expect(subject.sprintf("%{message}")).to eq(subject["message"])
+        expect(subject.sprintf("%{message}")).to eq(subject.get("message"))
       end
 
       it "should print deep fields" do
@@ -153,35 +153,35 @@
       end
 
       it "should render nil array values as leading empty string" do
-        expect(subject["foo"] = [nil, "baz"]).to eq([nil, "baz"])
+        expect(subject.set("foo", [nil, "baz"])).to eq([nil, "baz"])
 
-        expect(subject["[foo][0]"]).to be_nil
-        expect(subject["[foo][1]"]).to eq("baz")
+        expect(subject.get("[foo][0]")).to be_nil
+        expect(subject.get("[foo][1]")).to eq("baz")
 
         expect(subject.sprintf("%{[foo]}")).to eq(",baz")
       end
 
       it "should render nil array values as middle empty string" do
-        expect(subject["foo"] = ["bar", nil, "baz"]).to eq(["bar", nil, "baz"])
+        expect(subject.set("foo", ["bar", nil, "baz"])).to eq(["bar", nil, "baz"])
 
-        expect(subject["[foo][0]"]).to eq("bar")
-        expect(subject["[foo][1]"]).to be_nil
-        expect(subject["[foo][2]"]).to eq("baz")
+        expect(subject.get("[foo][0]")).to eq("bar")
+        expect(subject.get("[foo][1]")).to be_nil
+        expect(subject.get("[foo][2]")).to eq("baz")
 
         expect(subject.sprintf("%{[foo]}")).to eq("bar,,baz")
       end
 
      it "should render nil array values as trailing empty string" do
-        expect(subject["foo"] = ["bar", nil]).to eq(["bar", nil])
+        expect(subject.set("foo", ["bar", nil])).to eq(["bar", nil])
 
-        expect(subject["[foo][0]"]).to eq("bar")
-        expect(subject["[foo][1]"]).to be_nil
+        expect(subject.get("[foo][0]")).to eq("bar")
+        expect(subject.get("[foo][1]")).to be_nil
 
         expect(subject.sprintf("%{[foo]}")).to eq("bar,")
      end
 
       it "should render deep arrays with nil value" do
-        subject["[foo]"] = [[12, nil], 56]
+        subject.set("[foo]", [[12, nil], 56])
         expect(subject.sprintf("%{[foo]}")).to eq("12,,56")
       end
 
@@ -191,25 +191,25 @@
         end
 
         it "should return unknown patterns as UTF-8" do
-          expect(subject.sprintf("%{unkown_pattern}").encoding).to eq(Encoding::UTF_8)
+          expect(subject.sprintf("%{unknown_pattern}").encoding).to eq(Encoding::UTF_8)
         end
       end
     end
 
     context "#[]" do
       it "should fetch data" do
-        expect(subject["type"]).to eq("sprintf")
+        expect(subject.get("type")).to eq("sprintf")
       end
       it "should fetch fields" do
-        expect(subject["a"]).to eq("b")
-        expect(subject['c']['d']).to eq("f")
+        expect(subject.get("a")).to eq("b")
+        expect(subject.get('c')['d']).to eq("f")
       end
       it "should fetch deep fields" do
-        expect(subject["[j][k1]"]).to eq("v")
-        expect(subject["[c][d]"]).to eq("f")
-        expect(subject['[f][g][h]']).to eq("i")
-        expect(subject['[j][k3][4]']).to eq("m")
-        expect(subject['[j][5]']).to eq(7)
+        expect(subject.get("[j][k1]")).to eq("v")
+        expect(subject.get("[c][d]")).to eq("f")
+        expect(subject.get('[f][g][h]')).to eq("i")
+        expect(subject.get('[j][k3][4]')).to eq("m")
+        expect(subject.get('[j][5]')).to eq(7)
 
       end
 
@@ -217,7 +217,7 @@
         count = 1000000
         2.times do
           start = Time.now
-          count.times { subject["[j][k1]"] }
+          count.times { subject.get("[j][k1]") }
           duration = Time.now - start
           puts "event #[] rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
         end
@@ -263,11 +263,11 @@
         )
         subject.overwrite(new_event)
 
-        expect(subject["message"]).to eq("foo bar")
-        expect(subject["type"]).to eq("new")
+        expect(subject.get("message")).to eq("foo bar")
+        expect(subject.get("type")).to eq("new")
 
         ["tags", "source", "a", "c", "f", "j"].each do |field|
-          expect(subject[field]).to be_nil
+          expect(subject.get(field)).to be_nil
         end
       end
     end
@@ -275,7 +275,7 @@
     context "#append" do
       it "should append strings to an array" do
         subject.append(LogStash::Event.new("message" => "another thing"))
-        expect(subject["message"]).to eq([ "hello world", "another thing" ])
+        expect(subject.get("message")).to eq([ "hello world", "another thing" ])
       end
 
       it "should concatenate tags" do
@@ -283,54 +283,54 @@
         # added to_a for when array is a Java Collection when produced from json input
         # TODO: we have to find a better way to handle this in tests. maybe override
         # rspec eq or == to do an explicit to_a when comparing arrays?
-        expect(subject["tags"].to_a).to eq([ "tag1", "tag2" ])
+        expect(subject.get("tags").to_a).to eq([ "tag1", "tag2" ])
       end
 
       context "when event field is nil" do
         it "should add single value as string" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq("append1")
+          expect(subject.get("field1")).to eq("append1")
         end
         it "should add multi values as array" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","append2" ]}))
-          expect(subject[ "field1" ]).to eq([ "append1","append2" ])
+          expect(subject.get("field1")).to eq([ "append1","append2" ])
         end
       end
 
       context "when event field is a string" do
-        before { subject[ "field1" ] = "original1" }
+        before { subject.set("field1", "original1") }
 
         it "should append string to values, if different from current" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
         it "should not change value, if appended value is equal current" do
           subject.append(LogStash::Event.new({"field1" => "original1"}))
-          expect(subject[ "field1" ]).to eq("original1")
+          expect(subject.get("field1")).to eq("original1")
         end
         it "should concatenate values in an array" do
           subject.append(LogStash::Event.new({"field1" => [ "append1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
         it "should join array, removing duplicates" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
       end
       context "when event field is an array" do
-        before { subject[ "field1" ] = [ "original1", "original2" ] }
+        before { subject.set("field1", [ "original1", "original2" ] )}
 
         it "should append string values to array, if not present in array" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2", "append1" ])
         end
         it "should not append string values, if the array already contains it" do
           subject.append(LogStash::Event.new({"field1" => "original1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2" ])
         end
         it "should join array, removing duplicates" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2", "append1" ])
         end
       end
 
@@ -342,7 +342,7 @@
 
       data = { "@timestamp" => "2013-12-21T07:25:06.605Z" }
       event = LogStash::Event.new(data)
-      expect(event["@timestamp"]).to be_a(LogStash::Timestamp)
+      expect(event.get("@timestamp")).to be_a(LogStash::Timestamp)
 
       duration = 0
       [warmup, count].each do |i|
@@ -391,12 +391,6 @@
     end
 
     context "timestamp initialization" do
-      let(:logger_mock) { double("logger") }
-
-      after(:each) do
-        LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER
-      end
-
       it "should coerce timestamp" do
         t = Time.iso8601("2014-06-12T00:12:17.114Z")
         expect(LogStash::Event.new("@timestamp" => t).timestamp.to_i).to eq(t.to_i)
@@ -411,20 +405,16 @@
       it "should tag for invalid value" do
         event = LogStash::Event.new("@timestamp" => "foo")
         expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq("foo")
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
 
         event = LogStash::Event.new("@timestamp" => 666)
         expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq(666)
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq(666)
       end
 
       it "should warn for invalid value" do
-        LogStash::Event.logger = logger_mock
-
-        expect(logger_mock).to receive(:warn).twice
-
         LogStash::Event.new("@timestamp" => :foo)
         LogStash::Event.new("@timestamp" => 666)
       end
@@ -432,14 +422,11 @@
       it "should tag for invalid string format" do
         event = LogStash::Event.new("@timestamp" => "foo")
         expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq("foo")
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
       end
 
       it "should warn for invalid string format" do
-        LogStash::Event.logger = logger_mock
-
-        expect(logger_mock).to receive(:warn)
         LogStash::Event.new("@timestamp" => "foo")
       end
     end
@@ -478,7 +465,7 @@
         end
 
         it "should still allow normal field access" do
-          expect(subject["hello"]).to eq("world")
+          expect(subject.get("hello")).to eq("world")
         end
       end
 
@@ -491,15 +478,15 @@
           expect(fieldref).to start_with("[@metadata]")
 
           # Set it.
-          subject[fieldref] = value
+          subject.set(fieldref, value)
         end
 
         it "should still allow normal field access" do
-          expect(subject["normal"]).to eq("normal")
+          expect(subject.get("normal")).to eq("normal")
         end
 
         it "should allow getting" do
-          expect(subject[fieldref]).to eq(value)
+          expect(subject.get(fieldref)).to eq(value)
         end
 
         it "should be hidden from .to_json" do
@@ -522,10 +509,10 @@
       context "with no metadata" do
         subject { LogStash::Event.new("foo" => "bar") }
         it "should have no metadata" do
-          expect(subject["@metadata"]).to be_empty
+          expect(subject.get("@metadata")).to be_empty
         end
         it "should still allow normal field access" do
-          expect(subject["foo"]).to eq("bar")
+          expect(subject.get("foo")).to eq("bar")
         end
 
         it "should not include the @metadata key" do
@@ -535,12 +522,27 @@
     end
 
     context "signal events" do
-      it "should define the shutdown event" do
+      it "should define the shutdown and flush event constants" do
         # the SHUTDOWN and FLUSH constants are part of the plugin API contract
         # if they are changed, all plugins must be updated
         expect(LogStash::SHUTDOWN).to be_a(LogStash::ShutdownEvent)
         expect(LogStash::FLUSH).to be_a(LogStash::FlushEvent)
       end
+
+      it "should define the shutdown event with SignalEvent as parent class" do
+        expect(LogStash::SHUTDOWN).to be_a(LogStash::SignalEvent)
+        expect(LogStash::FLUSH).to be_a(LogStash::SignalEvent)
+      end
+
+      it "should define the flush? method" do
+        expect(LogStash::SHUTDOWN.flush?).to be_falsey
+        expect(LogStash::FLUSH.flush?).to be_truthy
+      end
+
+      it "should define the shutdown? method" do
+        expect(LogStash::SHUTDOWN.shutdown?).to be_truthy
+        expect(LogStash::FLUSH.shutdown?).to be_falsey
+      end
     end
   end
 
@@ -577,7 +579,7 @@
   end
 
   describe "using hash input from deserialized json" do
-    # this is to test the case when JrJackson deserialises Json and produces
+    # this is to test the case when JrJackson deserializes Json and produces
     # native Java Collections objects for efficiency
     it_behaves_like "all event tests" do
       subject{LogStash::Event.new(LogStash::Json.load(LogStash::Json.dump(event_hash)))}
@@ -599,7 +601,36 @@
     end
 
     it "return the string containing the timestamp, the host and the message" do
-      expect(event1.to_s).to eq("#{timestamp.to_iso8601} #{event1["host"]} #{event1["message"]}")
+      expect(event1.to_s).to eq("#{timestamp.to_iso8601} #{event1.get("host")} #{event1.get("message")}")
+    end
+  end
+
+  describe "Event accessors" do
+    let(:event) { LogStash::Event.new({ "message" => "foo" }) }
+
+    it "should invalidate target caching" do
+      expect(event.get("[a][0]")).to be_nil
+
+      expect(event.set("[a][0]", 42)).to eq(42)
+      expect(event.get("[a][0]")).to eq(42)
+      expect(event.get("[a]")).to eq({"0" => 42})
+
+      expect(event.set("[a]", [42, 24])).to eq([42, 24])
+      expect(event.get("[a]")).to eq([42, 24])
+
+      expect(event.get("[a][0]")).to eq(42)
+
+      expect(event.set("[a]", [24, 42])).to eq([24, 42])
+      expect(event.get("[a][0]")).to eq(24)
+
+      expect(event.set("[a][0]", {"a "=> 99, "b" => 98})).to eq({"a "=> 99, "b" => 98})
+      expect(event.get("[a][0]")).to eq({"a "=> 99, "b" => 98})
+
+      expect(event.get("[a]")).to eq([{"a "=> 99, "b" => 98}, 42])
+      expect(event.get("[a][0]")).to eq({"a "=> 99, "b" => 98})
+      expect(event.get("[a][1]")).to eq(42)
+      expect(event.get("[a][0][b]")).to eq(98)
     end
   end
 end
+
diff --git a/logstash-core-event/spec/logstash/timestamp_spec.rb b/logstash-core/spec/logstash/legacy_ruby_timestamp_spec.rb
similarity index 97%
rename from logstash-core-event/spec/logstash/timestamp_spec.rb
rename to logstash-core/spec/logstash/legacy_ruby_timestamp_spec.rb
index 196b895c39e..33e5c644fce 100644
--- a/logstash-core-event/spec/logstash/timestamp_spec.rb
+++ b/logstash-core/spec/logstash/legacy_ruby_timestamp_spec.rb
@@ -40,10 +40,10 @@
   end
 
   it "should support to_json and ignore arguments" do
-    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :argumnents => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :arguments => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
   end
 
-  it "should support timestamp comparaison" do
+  it "should support timestamp comparison" do
    current = LogStash::Timestamp.new(Time.now) 
    future = LogStash::Timestamp.new(Time.now + 100)
 
diff --git a/logstash-core/spec/logstash/output_delegator_spec.rb b/logstash-core/spec/logstash/output_delegator_spec.rb
index 524ad779ec9..3b9d2b2c997 100644
--- a/logstash-core/spec/logstash/output_delegator_spec.rb
+++ b/logstash-core/spec/logstash/output_delegator_spec.rb
@@ -1,27 +1,38 @@
 # encoding: utf-8
 require "logstash/output_delegator"
-require 'spec_helper'
+require "logstash/execution_context"
+require "spec_helper"
 
 describe LogStash::OutputDelegator do
   let(:logger) { double("logger") }
   let(:events) { 7.times.map { LogStash::Event.new }}
-  let(:default_worker_count) { 1 }
+  let(:plugin_args) { {"id" => "foo", "arg1" => "val1"} }
+  let(:collector) { [] }
+  let(:metric) { LogStash::Instrument::NamespacedNullMetric.new(collector, :null) }
+  let(:default_execution_context) { LogStash::ExecutionContext.new(:main) }
 
-  subject { described_class.new(logger, out_klass, default_worker_count, LogStash::Instrument::NullMetric.new) }
+  subject { described_class.new(logger, out_klass, metric, default_execution_context, ::LogStash::OutputDelegatorStrategyRegistry.instance, plugin_args) }
 
   context "with a plain output plugin" do
     let(:out_klass) { double("output klass") }
     let(:out_inst) { double("output instance") }
+    let(:concurrency) { :single }
 
     before(:each) do
+      # use the same metric instance
+      allow(metric).to receive(:namespace).with(any_args).and_return(metric)
+
       allow(out_klass).to receive(:new).with(any_args).and_return(out_inst)
-      allow(out_klass).to receive(:threadsafe?).and_return(false)
-      allow(out_klass).to receive(:workers_not_supported?).and_return(false)
+      allow(out_klass).to receive(:name).and_return("example")
+      allow(out_klass).to receive(:concurrency).with(any_args).and_return concurrency
+      allow(out_klass).to receive(:config_name).and_return("dummy_plugin")
       allow(out_inst).to receive(:register)
       allow(out_inst).to receive(:multi_receive)
       allow(out_inst).to receive(:metric=).with(any_args)
+      allow(out_inst).to receive(:execution_context=).with(default_execution_context)
       allow(out_inst).to receive(:id).and_return("a-simple-plugin")
-      allow(out_inst).to receive(:plugin_unique_name).and_return("hello-123")
+
+      allow(subject.metric_events).to receive(:increment).with(any_args)
       allow(logger).to receive(:debug).with(any_args)
     end
 
@@ -29,106 +40,112 @@
       expect { subject }.not_to raise_error
     end
 
+    it "should push the name of the plugin to the metric" do
+      expect(metric).to receive(:gauge).with(:name, out_klass.config_name)
+      described_class.new(logger, out_klass, metric, default_execution_context, ::LogStash::OutputDelegatorStrategyRegistry.instance, plugin_args)
+    end
+
     context "after having received a batch of events" do
       before do
-        subject.multi_receive(events)
+        subject.register
       end
 
       it "should pass the events through" do
-        expect(out_inst).to have_received(:multi_receive).with(events)
+        expect(out_inst).to receive(:multi_receive).with(events)
+        subject.multi_receive(events)
       end
 
       it "should increment the number of events received" do
-        expect(subject.events_received).to eql(events.length)
+        expect(subject.metric_events).to receive(:increment).with(:in, events.length)
+        expect(subject.metric_events).to receive(:increment).with(:out, events.length)
+        subject.multi_receive(events)
       end
-    end
 
-    it "should register all workers on register" do
-      expect(out_inst).to receive(:register)
-      subject.register
-    end
-
-    it "should close all workers when closing" do
-      expect(out_inst).to receive(:do_close)
-      subject.do_close
+      it "should record the `duration_in_millis`" do
+        clock = spy("clock")
+        expect(subject.metric_events).to receive(:time).with(:duration_in_millis).and_return(clock)
+        expect(clock).to receive(:stop)
+        subject.multi_receive(events)
+      end
     end
 
-    describe "concurrency and worker support" do
-      describe "non-threadsafe outputs that allow workers" do
-        let(:default_worker_count) { 3 }
-
-        before do
-          allow(out_klass).to receive(:threadsafe?).and_return(false)
-          allow(out_klass).to receive(:workers_not_supported?).and_return(false)
-          allow(out_inst).to receive(:metric=).with(any_args)
-          allow(out_inst).to receive(:id).and_return("a-simple-plugin")
-        end
-
-        it "should instantiate multiple workers" do
-          expect(subject.workers.length).to eql(default_worker_count)
-        end
-
-        it "should send received events to the worker" do
-          expect(out_inst).to receive(:multi_receive).with(events)
-          subject.multi_receive(events)
-        end
+    describe "closing" do
+      before do
+        subject.register
       end
 
-      describe "threadsafe outputs" do
-        before do
-          allow(out_klass).to receive(:threadsafe?).and_return(true)
-          allow(out_inst).to receive(:metric=).with(any_args)
-          allow(out_inst).to receive(:id).and_return("a-simple-plugin")
-          allow(out_klass).to receive(:workers_not_supported?).and_return(false)
-        end
-
-        it "should return true when threadsafe? is invoked" do
-          expect(subject.threadsafe?).to eql(true)
-        end
-
-        it "should define a threadsafe_worker" do
-          expect(subject.send(:threadsafe_worker)).to eql(out_inst)
-        end
-
-        it "should utilize threadsafe_multi_receive" do
-          expect(subject.send(:threadsafe_worker)).to receive(:multi_receive).with(events)
-          subject.multi_receive(events)
-        end
-
-        it "should not utilize the worker queue" do
-          expect(subject.send(:worker_queue)).not_to receive(:pop)
-          subject.multi_receive(events)
-        end
-
-        it "should send received events to the worker" do
-          expect(out_inst).to receive(:multi_receive).with(events)
-          subject.multi_receive(events)
-        end
+      it "should register the output plugin instance on register" do
+        expect(out_inst).to have_received(:register)
       end
-    end
-  end
-
-  # This may seem suspiciously similar to the class in outputs/base_spec
-  # but, in fact, we need a whole new class because using this even once
-  # will immutably modify the base class
-  class LogStash::Outputs::NOOPDelLegacyNoWorkers < ::LogStash::Outputs::Base
-    LEGACY_WORKERS_NOT_SUPPORTED_REASON = "legacy reason"
 
-    def register
-      workers_not_supported(LEGACY_WORKERS_NOT_SUPPORTED_REASON)
+      it "should close the output plugin instance when closing" do
+        expect(out_inst).to receive(:do_close)
+        subject.do_close
+      end
     end
-  end
-
-  describe "legacy output workers_not_supported" do
-    let(:default_worker_count) { 2 }
-    let(:out_klass) { LogStash::Outputs::NOOPDelLegacyNoWorkers }
 
-    before do
-      allow(logger).to receive(:debug).with(any_args)
-    end
+    describe "concurrency strategies" do
+      it "should have :single as the default" do
+        expect(subject.concurrency).to eq :single
+      end
 
-    it "should only setup one worker" do
-      expect(subject.worker_count).to eql(1)
+      [
+        [:shared, ::LogStash::OutputDelegatorStrategies::Shared],
+        [:single, ::LogStash::OutputDelegatorStrategies::Single],
+        [:legacy, ::LogStash::OutputDelegatorStrategies::Legacy],
+      ].each do |strategy_concurrency,klass|
+        context "with strategy #{strategy_concurrency}" do
+          let(:concurrency) { strategy_concurrency }
+
+          it "should find the correct concurrency type for the output" do
+            expect(subject.concurrency).to eq(strategy_concurrency)
+          end
+          
+          it "should find the correct Strategy class for the worker" do
+            expect(subject.strategy).to be_a(klass)
+          end
+
+          it "should set the correct parameters on the instance" do
+            expect(out_klass).to have_received(:new).with(plugin_args)
+          end
+
+          it "should set the metric on the instance" do
+            expect(out_inst).to have_received(:metric=).with(metric)
+          end
+
+          [[:register], [:do_close], [:multi_receive, [[]] ] ].each do |method, args|
+            context "strategy objects" do
+              before do
+                allow(subject.strategy).to receive(method)
+              end
+              
+              it "should delegate #{method} to the strategy" do
+                subject.send(method, *args)
+                if args
+                  expect(subject.strategy).to have_received(method).with(*args)
+                else
+                  expect(subject.strategy).to have_received(method).with(no_args)
+                end
+              end
+            end
+
+            context "strategy output instances" do
+              before do
+                allow(out_inst).to receive(method)
+              end
+              
+              it "should delegate #{method} to the strategy" do
+                subject.send(method, *args)
+                if args
+                  expect(out_inst).to have_received(method).with(*args)
+                else
+                  expect(out_inst).to have_received(method).with(no_args)
+                end
+              end
+            end
+          end
+        end
+      end
     end
   end
 end
diff --git a/logstash-core/spec/logstash/outputs/base_spec.rb b/logstash-core/spec/logstash/outputs/base_spec.rb
index 44d49a60b99..650a22d9e2c 100644
--- a/logstash-core/spec/logstash/outputs/base_spec.rb
+++ b/logstash-core/spec/logstash/outputs/base_spec.rb
@@ -1,10 +1,12 @@
 # encoding: utf-8
 require "spec_helper"
+require "logstash/outputs/base"
+require "logstash/execution_context"
 
 # use a dummy NOOP output to test Outputs::Base
-class LogStash::Outputs::NOOP < LogStash::Outputs::Base
+class LogStash::Outputs::NOOPSingle < LogStash::Outputs::Base
   config_name "noop"
-  milestone 2
+  concurrency :single
 
   config :dummy_option, :validate => :string
 
@@ -15,26 +17,122 @@ def receive(event)
   end
 end
 
-class LogStash::Outputs::NOOPLegacyNoWorkers < ::LogStash::Outputs::Base
-  LEGACY_WORKERS_NOT_SUPPORTED_REASON = "legacy reason"
+class LogStash::Outputs::NOOPShared < ::LogStash::Outputs::Base
+  concurrency :shared
+  
+  def register; end
+end
+
+class LogStash::Outputs::NOOPLegacy < ::LogStash::Outputs::Base
+  def register; end
+end
+
+class LogStash::Outputs::NOOPMultiReceiveEncoded < ::LogStash::Outputs::Base
+  concurrency :single
+  
+  def register; end
 
-  def register
-    workers_not_supported(LEGACY_WORKERS_NOT_SUPPORTED_REASON)
+  def multi_receive_encoded(events_and_encoded)
   end
 end
 
 describe "LogStash::Outputs::Base#new" do
-  it "should instantiate cleanly" do
-    params = { "dummy_option" => "potatoes", "codec" => "json", "workers" => 2 }
-    worker_params = params.dup; worker_params["workers"] = 1
+  let(:params) { {} }  
+  subject(:instance) { klass.new(params.dup) }
+
+  context "single" do
+    let(:klass) { LogStash::Outputs::NOOPSingle }
+    
+    it "should instantiate cleanly" do
+      params = { "dummy_option" => "potatoes", "codec" => "json", "workers" => 2 }
+      worker_params = params.dup; worker_params["workers"] = 1
+
+      expect{ subject }.not_to raise_error
+    end
+
+    it "should set concurrency correctly" do
+      expect(subject.concurrency).to eq(:single)
+    end
+  end
+
+  context "shared" do
+    let(:klass) { LogStash::Outputs::NOOPShared }
+    
+    it "should set concurrency correctly" do
+      expect(subject.concurrency).to eq(:shared)
+    end
+  end
 
-    expect do
-      LogStash::Outputs::NOOP.new(params.dup)
-    end.not_to raise_error
+  context "legacy" do
+    let(:klass) { LogStash::Outputs::NOOPLegacy }
+    
+    it "should set concurrency correctly" do
+      expect(subject.concurrency).to eq(:legacy)
+    end
+
+    it "should default the # of workers to 1" do
+      expect(subject.workers).to eq(1)
+    end
+
+    it "should default concurrency to :legacy" do
+      expect(subject.concurrency).to eq(:legacy)
+    end
   end
 
-  it "should move workers_not_supported declarations up to the class level" do
-    LogStash::Outputs::NOOPLegacyNoWorkers.new.register
-    expect(LogStash::Outputs::NOOPLegacyNoWorkers.workers_not_supported?).to eql(true)
+  context "execution context" do
+    let(:default_execution_context) { LogStash::ExecutionContext.new(:main) }
+    let(:klass) { LogStash::Outputs::NOOPSingle }
+
+    subject(:instance) { klass.new(params.dup) }
+
+    it "allow to set the context" do
+      expect(instance.execution_context).to be_nil
+      instance.execution_context = default_execution_context
+
+      expect(instance.execution_context).to eq(default_execution_context)
+    end
+
+    it "propagate the context to the codec" do
+      expect(instance.codec.execution_context).to be_nil
+      instance.execution_context = default_execution_context
+
+      expect(instance.codec.execution_context).to eq(default_execution_context)
+    end
+  end
+
+  describe "dispatching multi_receive" do
+    let(:event) { double("event") }
+    let(:events) { [event] }
+    
+    context "with multi_receive_encoded" do
+      let(:klass) { LogStash::Outputs::NOOPMultiReceiveEncoded }
+      let(:codec) { double("codec") }
+      let(:encoded) { double("encoded") }
+      
+      before do
+        allow(codec).to receive(:multi_encode).with(events).and_return(encoded)
+        allow(instance).to receive(:codec).and_return(codec)
+        allow(instance).to receive(:multi_receive_encoded)        
+        instance.multi_receive(events)
+      end
+
+      it "should invoke multi_receive_encoded if it exists" do
+        expect(instance).to have_received(:multi_receive_encoded).with(encoded)
+      end
+    end
+
+    context "with plain #receive" do
+      let(:klass) { LogStash::Outputs::NOOPSingle }
+
+      before do
+        allow(instance).to receive(:multi_receive).and_call_original
+        allow(instance).to receive(:receive).with(event)
+        instance.multi_receive(events)
+      end
+
+      it "should receive the event by itself" do
+        expect(instance).to have_received(:receive).with(event)
+      end
+    end
   end
 end
diff --git a/logstash-core/spec/logstash/patches_spec.rb b/logstash-core/spec/logstash/patches_spec.rb
index f22db6d07e5..f97d4ea201b 100644
--- a/logstash-core/spec/logstash/patches_spec.rb
+++ b/logstash-core/spec/logstash/patches_spec.rb
@@ -2,17 +2,18 @@
 require "socket"
 require "logstash/patches"
 require "flores/pki"
+require "logstash/json"
 
 describe "OpenSSL defaults" do
   subject { OpenSSL::SSL::SSLContext.new }
 
-  # OpenSSL::SSL::SSLContext#ciphers returns an array of 
+  # OpenSSL::SSL::SSLContext#ciphers returns an array of
   # [ [ ciphername, version, bits, alg_bits ], [ ... ], ... ]
- 
+
   # List of cipher names
   let(:ciphers) { subject.ciphers.map(&:first) }
 
-  # List of cipher encryption bit strength. 
+  # List of cipher encryption bit strength.
   let(:encryption_bits) { subject.ciphers.map { |_, _, _, a| a } }
 
   it "should not include any export ciphers" do
@@ -35,7 +36,7 @@
     # https://github.com/jordansissel/ruby-flores/blob/master/spec/flores/pki_integration_spec.rb
     # since these helpers were created to fix this particular issue
     let(:csr) { Flores::PKI::CertificateSigningRequest.new }
-    # Here, I use a 1024-bit key for faster tests. 
+    # Here, I use a 1024-bit key for faster tests.
     # Please do not use such small keys in production.
     let(:key_bits) { 1024 }
     let(:key) { OpenSSL::PKey::RSA.generate(key_bits, 65537) }
@@ -88,3 +89,13 @@
     end
   end
 end
+
+describe "exceptions used json logging hashes" do
+  let(:exception) { ArgumentError.new("so you want an argument, huh?") }
+  let(:result) { [] }
+
+  it "should not raise errors" do
+    expect { result << LogStash::Json.dump({"error" => exception}) }.not_to raise_error
+    expect(result[0]).to match(/ArgumentError.*so you want an argument/)
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
new file mode 100644
index 00000000000..29130e41580
--- /dev/null
+++ b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
@@ -0,0 +1,141 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/inputs/generator"
+require "logstash/filters/multiline"
+
+class PipelinePqFileOutput < LogStash::Outputs::Base
+  config_name "pipelinepqfileoutput"
+  milestone 2
+
+  attr_reader :num_closes, :event_count
+
+  def self.make_shared
+    @concurrency = :shared
+  end
+
+  def initialize(params={})
+    super
+    @num_closes = 0
+    @event_count = 0
+    @mutex = Mutex.new
+  end
+
+  def register
+    self.class.make_shared
+  end
+
+  def receive(event)
+    @mutex.synchronize do
+      @event_count = @event_count.succ
+    end
+  end
+
+  def close
+    @num_closes = 1
+  end
+end
+
+describe LogStash::Pipeline do
+  let(:pipeline_settings_obj) { LogStash::SETTINGS }
+  let(:pipeline_id) { "main" }
+
+  let(:multiline_id) { "my-multiline" }
+  let(:output_id) { "my-pipelinepqfileoutput" }
+  let(:generator_id) { "my-generator" }
+  let(:config) do
+    <<-EOS
+    input {
+      generator {
+        count => #{number_of_events}
+        id => "#{generator_id}"
+      }
+    }
+    filter {
+      multiline {
+        id => "#{multiline_id}"
+        pattern => "hello"
+        what => next
+      }
+    }
+    output {
+      pipelinepqfileoutput {
+        id => "#{output_id}"
+      }
+    }
+    EOS
+  end
+
+   let(:pipeline_settings) { { "queue.type" => queue_type, "pipeline.workers" => worker_thread_count, "pipeline.id" => pipeline_id} }
+
+  subject { described_class.new(config, pipeline_settings_obj, metric) }
+
+  let(:counting_output) { PipelinePqFileOutput.new({ "id" => output_id }) }
+  let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
+  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+  let(:base_queue_path) { pipeline_settings_obj.get("path.queue") }
+  let(:this_queue_folder) { File.join(base_queue_path, SecureRandom.hex(8)) }
+
+  let(:worker_thread_count) { 8 } # 1 4 8
+  let(:number_of_events) { 100_000 }
+  let(:page_capacity) { 1 * 1024 * 512 } # 1 128
+  let(:max_bytes) { 1024 * 1024 * 1024 } # 1 gb
+  let(:queue_type) { "persisted" } #  "memory" "memory_acked"
+  let(:times) { [] }
+
+  let(:pipeline_thread) do
+    # subject has to be called for the first time outside the thread because it will create a race condition
+    # with the subject.ready? call since subject is lazily initialized
+    s = subject
+    Thread.new { s.run }
+  end
+
+  before :each do
+    FileUtils.mkdir_p(this_queue_folder)
+
+    pipeline_settings_obj.set("path.queue", this_queue_folder)
+    allow(PipelinePqFileOutput).to receive(:new).with(any_args).and_return(counting_output)
+    allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+    allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+    allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
+    allow(LogStash::Plugin).to receive(:lookup).with("output", "pipelinepqfileoutput").and_return(PipelinePqFileOutput)
+
+    pipeline_workers_setting = LogStash::SETTINGS.get_setting("pipeline.workers")
+    allow(pipeline_workers_setting).to receive(:default).and_return(worker_thread_count)
+    pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    pipeline_settings_obj.set("queue.page_capacity", page_capacity)
+    pipeline_settings_obj.set("queue.max_bytes", max_bytes)
+    times.push(Time.now.to_f)
+
+    pipeline_thread
+    sleep(0.1) until subject.ready?
+
+    # make sure we have received all the generated events
+    while counting_output.event_count < number_of_events do
+      sleep(0.5)
+    end
+
+    times.unshift(Time.now.to_f - times.first)
+  end
+
+  after :each do
+    subject.shutdown
+    pipeline_thread.join
+    # Dir.rm_rf(this_queue_folder)
+  end
+
+  let(:collected_metric) { metric_store.get_with_path("stats/pipelines/") }
+
+  it "populates the pipelines core metrics" do
+    _metric = collected_metric[:stats][:pipelines][:main][:events]
+    expect(_metric[:duration_in_millis].value).not_to be_nil
+    expect(_metric[:in].value).to eq(number_of_events)
+    expect(_metric[:filtered].value).to eq(number_of_events)
+    expect(_metric[:out].value).to eq(number_of_events)
+    STDOUT.puts "  queue.type: #{pipeline_settings_obj.get("queue.type")}"
+    STDOUT.puts "  queue.page_capacity: #{pipeline_settings_obj.get("queue.page_capacity") / 1024}KB"
+    STDOUT.puts "  queue.max_bytes: #{pipeline_settings_obj.get("queue.max_bytes") / 1024}KB"
+    STDOUT.puts "  workers: #{worker_thread_count}"
+    STDOUT.puts "  events: #{number_of_events}"
+    STDOUT.puts "  took: #{times.first}s"
+  end
+end
diff --git a/logstash-core/spec/logstash/pipeline_reporter_spec.rb b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
index bdd83d4ff24..68b48181996 100644
--- a/logstash-core/spec/logstash/pipeline_reporter_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
@@ -2,30 +2,7 @@
 require "spec_helper"
 require "logstash/pipeline"
 require "logstash/pipeline_reporter"
-
-class DummyOutput < LogStash::Outputs::Base
-  config_name "dummyoutput"
-  milestone 2
-
-  attr_reader :num_closes, :events
-
-  def initialize(params={})
-    super
-    @num_closes = 0
-    @events = []
-  end
-
-  def register
-  end
-
-  def receive(event)
-    @events << event
-  end
-
-  def close
-    @num_closes += 1
-  end
-end
+require_relative "../support/mocks_classes"
 
 #TODO: Figure out how to add more tests that actually cover inflight events
 #This will require some janky multithreading stuff
@@ -38,15 +15,20 @@ def close
   let(:reporter) { pipeline.reporter }
 
   before do
-    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_call_original
     allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
 
     @pre_snapshot = reporter.snapshot
+    
     pipeline.run
     @post_snapshot = reporter.snapshot
   end
 
+  after do
+    pipeline.shutdown
+  end
+
   describe "events filtered" do
     it "should start at zero" do
       expect(@pre_snapshot.events_filtered).to eql(0)
@@ -76,10 +58,4 @@ def close
       expect(@post_snapshot.inflight_count).to eql(0)
     end
   end
-
-  describe "output states" do
-    it "should include the count of received events" do
-      expect(@post_snapshot.output_info.first[:events_received]).to eql(generator_count)
-    end
-  end
-end
\ No newline at end of file
+end
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 3753ccaae00..f480e694925 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -2,6 +2,8 @@
 require "spec_helper"
 require "logstash/inputs/generator"
 require "logstash/filters/multiline"
+require_relative "../support/mocks_classes"
+require_relative "../logstash/pipeline_reporter_spec" # for DummyOutput class
 
 class DummyInput < LogStash::Inputs::Base
   config_name "dummyinput"
@@ -48,31 +50,7 @@ def close
   end
 end
 
-class DummyOutput < LogStash::Outputs::Base
-  config_name "dummyoutput"
-  milestone 2
-
-  attr_reader :num_closes, :events
-
-  def initialize(params={})
-    super
-    @num_closes = 0
-    @events = []
-  end
-
-  def register
-  end
-
-  def receive(event)
-    @events << event
-  end
-
-  def close
-    @num_closes = 1
-  end
-end
-
-class DummyOutputMore < DummyOutput
+class DummyOutputMore < ::LogStash::Outputs::DummyOutput
   config_name "dummyoutputmore"
 end
 
@@ -102,20 +80,102 @@ def threadsafe?() true; end
   def close() end
 end
 
+class DummyFlushingFilter < LogStash::Filters::Base
+  config_name "dummyflushingfilter"
+  milestone 2
+
+  def register() end
+  def filter(event) end
+  def periodic_flush
+    true
+  end
+  def flush(options)
+    return [::LogStash::Event.new("message" => "dummy_flush")]
+  end
+  def close() end
+end
+
 class TestPipeline < LogStash::Pipeline
-  attr_reader :outputs, :settings, :logger
+  attr_reader :outputs, :settings
 end
 
 describe LogStash::Pipeline do
-  let(:worker_thread_count)     { LogStash::Pipeline::DEFAULT_SETTINGS[:default_pipeline_workers] }
+  let(:worker_thread_count)     { 5 }
   let(:safe_thread_count)       { 1 }
   let(:override_thread_count)   { 42 }
+  let(:pipeline_settings_obj) { LogStash::SETTINGS }
+  let(:pipeline_settings) { {} }
+
+  before :each do
+    pipeline_workers_setting = LogStash::SETTINGS.get_setting("pipeline.workers")
+    allow(pipeline_workers_setting).to receive(:default).and_return(worker_thread_count)
+    pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+  end
+
+  after :each do
+    pipeline_settings_obj.reset
+  end
+
+
+  describe "event cancellation" do
+    # test harness for https://github.com/elastic/logstash/issues/6055
+
+    let(:output) { LogStash::Outputs::DummyOutputWithEventsArray.new }
+
+    before do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputwitheventsarray").and_return(LogStash::Outputs::DummyOutputWithEventsArray)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "drop").and_call_original
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "mutate").and_call_original
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
+      allow(LogStash::Outputs::DummyOutputWithEventsArray).to receive(:new).with(any_args).and_return(output)
+    end
+
+    let(:config) do
+      <<-CONFIG
+        input {
+          generator {
+            lines => ["1", "2", "END"]
+            count => 1
+          }
+        }
+        filter {
+          if [message] == "1" {
+            drop {}
+          }
+          mutate { add_tag => ["notdropped"] }
+        }
+        output { dummyoutputwitheventsarray {} }
+      CONFIG
+    end
+
+    it "should not propagate cancelled events from filter to output" do
+      abort_on_exception_state = Thread.abort_on_exception
+      Thread.abort_on_exception = true
+
+      pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
+      t = Thread.new { pipeline.run }
+      sleep(0.1) until pipeline.ready?
+      wait(3).for do
+        # give us a bit of time to flush the events
+        # puts("*****" + output.events.map{|e| e.message}.to_s)
+        output.events.map{|e| e.get("message")}.include?("END")
+      end.to be_truthy
+      expect(output.events.size).to eq(2)
+      expect(output.events[0].get("tags")).to eq(["notdropped"])
+      expect(output.events[1].get("tags")).to eq(["notdropped"])
+      pipeline.shutdown
+      t.join
+
+      Thread.abort_on_exception = abort_on_exception_state
+    end
+  end
 
   describe "defaulting the pipeline workers based on thread safety" do
     before(:each) do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummysafefilter").and_return(DummySafeFilter)
     end
@@ -137,6 +197,29 @@ class TestPipeline < LogStash::Pipeline
         eos
       }
 
+      describe "debug compiled" do
+        let(:logger) { double("pipeline logger").as_null_object }
+
+        before do
+          expect(TestPipeline).to receive(:logger).and_return(logger)
+          allow(logger).to receive(:debug?).and_return(true)
+        end
+
+        it "should not receive a debug message with the compiled code" do
+          pipeline_settings_obj.set("config.debug", false)
+          expect(logger).not_to receive(:debug).with(/Compiled pipeline/, anything)
+          pipeline = TestPipeline.new(test_config_with_filters)
+          pipeline.close
+        end
+
+        it "should print the compiled code if config.debug is set to true" do
+          pipeline_settings_obj.set("config.debug", true)
+          expect(logger).to receive(:debug).with(/Compiled pipeline/, anything)
+          pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
+          pipeline.close
+        end
+      end
+
       context "when there is no command line -w N set" do
         it "starts one filter thread" do
           msg = "Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads"
@@ -145,18 +228,21 @@ class TestPipeline < LogStash::Pipeline
             {:count_was=>worker_thread_count, :filters=>["dummyfilter"]})
           pipeline.run
           expect(pipeline.worker_threads.size).to eq(safe_thread_count)
+          pipeline.shutdown
         end
       end
 
       context "when there is command line -w N set" do
+        let(:pipeline_settings) { {"pipeline.workers" => override_thread_count } }
         it "starts multiple filter thread" do
-          msg = "Warning: Manual override - there are filters that might not work with multiple worker threads"
-          pipeline = TestPipeline.new(test_config_with_filters)
+          msg = "Warning: Manual override - there are filters that might" +
+                " not work with multiple worker threads"
+          pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
           expect(pipeline.logger).to receive(:warn).with(msg,
             {:worker_threads=> override_thread_count, :filters=>["dummyfilter"]})
-          pipeline.configure(:pipeline_workers, override_thread_count)
           pipeline.run
           expect(pipeline.worker_threads.size).to eq(override_thread_count)
+          pipeline.shutdown
         end
       end
     end
@@ -179,9 +265,11 @@ class TestPipeline < LogStash::Pipeline
       }
 
       it "starts multiple filter threads" do
+        skip("This test has been failing periodically since November 2016. Tracked as https://github.com/elastic/logstash/issues/6245")
         pipeline = TestPipeline.new(test_config_with_filters)
         pipeline.run
         expect(pipeline.worker_threads.size).to eq(worker_thread_count)
+        pipeline.shutdown
       end
     end
   end
@@ -190,7 +278,7 @@ class TestPipeline < LogStash::Pipeline
     before(:each) do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     end
 
 
@@ -221,29 +309,21 @@ class TestPipeline < LogStash::Pipeline
     }
 
     context "output close" do
-      it "should call close of output without output-workers" do
-        pipeline = TestPipeline.new(test_config_without_output_workers)
-        pipeline.run
+      let(:pipeline) { TestPipeline.new(test_config_without_output_workers) }
+      let(:output) { pipeline.outputs.first }
 
-        expect(pipeline.outputs.size ).to eq(1)
-        expect(pipeline.outputs.first.workers.size ).to eq(pipeline.default_output_workers)
-        expect(pipeline.outputs.first.workers.first.num_closes ).to eq(1)
+      before do
+        allow(output).to receive(:do_close)
       end
 
-      it "should call output close correctly with output workers" do
-        pipeline = TestPipeline.new(test_config_with_output_workers)
+      after do
+        pipeline.shutdown
+      end
+      
+      it "should call close of output without output-workers" do
         pipeline.run
 
-        expect(pipeline.outputs.size ).to eq(1)
-        # We even close the parent output worker, even though it doesn't receive messages
-
-        output_delegator = pipeline.outputs.first
-        output = output_delegator.workers.first
-
-        expect(output.num_closes).to eq(1)
-        output_delegator.workers.each do |plugin|
-          expect(plugin.num_closes ).to eq(1)
-        end
+        expect(output).to have_received(:do_close).once
       end
     end
   end
@@ -253,7 +333,7 @@ class TestPipeline < LogStash::Pipeline
       before(:each) do
         allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
         allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-        allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+        allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       end
 
       let(:config) { "input { dummyinput {} } output { dummyoutput {} }"}
@@ -265,6 +345,7 @@ class TestPipeline < LogStash::Pipeline
         expect(pipeline).to receive(:start_flusher).ordered.and_call_original
 
         pipeline.run
+        pipeline.shutdown
       end
     end
 
@@ -283,7 +364,7 @@ class TestPipeline < LogStash::Pipeline
       CONFIG
 
       sample("hello") do
-        expect(subject["message"]).to eq("hello")
+        expect(subject.get("message")).to eq("hello")
       end
     end
 
@@ -302,11 +383,10 @@ class TestPipeline < LogStash::Pipeline
 
       sample(["foo", "bar"]) do
         expect(subject.size).to eq(2)
-
-        expect(subject[0]["message"]).to eq("foo\nbar")
-        expect(subject[0]["type"]).to be_nil
-        expect(subject[1]["message"]).to eq("foo\nbar")
-        expect(subject[1]["type"]).to eq("clone1")
+        expect(subject[0].get("message")).to eq("foo\nbar")
+        expect(subject[0].get("type")).to be_nil
+        expect(subject[1].get("message")).to eq("foo\nbar")
+        expect(subject[1].get("type")).to eq("clone1")
       end
     end
   end
@@ -314,18 +394,24 @@ class TestPipeline < LogStash::Pipeline
   describe "max inflight warning" do
     let(:config) { "input { dummyinput {} } output { dummyoutput {} }" }
     let(:batch_size) { 1 }
-    let(:pipeline) { LogStash::Pipeline.new(config, :pipeline_batch_size => batch_size, :pipeline_workers => 1) }
+    let(:pipeline_settings) { { "pipeline.batch.size" => batch_size, "pipeline.workers" => 1 } }
+    let(:pipeline) { LogStash::Pipeline.new(config, pipeline_settings_obj) }
     let(:logger) { pipeline.logger }
-    let(:warning_prefix) { /CAUTION: Recommended inflight events max exceeded!/ }
+    let(:warning_prefix) { Regexp.new("CAUTION: Recommended inflight events max exceeded!") }
 
     before(:each) do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(logger).to receive(:warn)
-      thread = Thread.new { pipeline.run }
+
+      # pipeline must be first called outside the thread context because it lazily initialize and will create a
+      # race condition if called in the thread
+      p = pipeline
+      t = Thread.new { p.run }
+      sleep(0.1) until pipeline.ready?
       pipeline.shutdown
-      thread.join
+      t.join
     end
 
     it "should not raise a max inflight warning if the max_inflight count isn't exceeded" do
@@ -341,8 +427,7 @@ class TestPipeline < LogStash::Pipeline
     end
   end
 
-  context "compiled filter funtions" do
-
+  context "compiled filter functions" do
     context "new events should propagate down the filters" do
       config <<-CONFIG
         filter {
@@ -358,31 +443,107 @@ class TestPipeline < LogStash::Pipeline
       sample("hello") do
         expect(subject.size).to eq(3)
 
-        expect(subject[0]["message"]).to eq("hello")
-        expect(subject[0]["type"]).to be_nil
-        expect(subject[0]["foo"]).to eq("bar")
+        expect(subject[0].get("message")).to eq("hello")
+        expect(subject[0].get("type")).to be_nil
+        expect(subject[0].get("foo")).to eq("bar")
 
-        expect(subject[1]["message"]).to eq("hello")
-        expect(subject[1]["type"]).to eq("clone1")
-        expect(subject[1]["foo"]).to eq("bar")
+        expect(subject[1].get("message")).to eq("hello")
+        expect(subject[1].get("type")).to eq("clone1")
+        expect(subject[1].get("foo")).to eq("bar")
 
-        expect(subject[2]["message"]).to eq("hello")
-        expect(subject[2]["type"]).to eq("clone2")
-        expect(subject[2]["foo"]).to eq("bar")
+        expect(subject[2].get("message")).to eq("hello")
+        expect(subject[2].get("type")).to eq("clone2")
+        expect(subject[2].get("foo")).to eq("bar")
       end
     end
   end
 
   context "metrics" do
-    config <<-CONFIG
-    input { }
-    filter { }
-    output { }
-    CONFIG
+    config = "input { } filter { } output { }"
+
+    let(:settings) { LogStash::SETTINGS.clone }
+    subject { LogStash::Pipeline.new(config, settings, metric) }
+
+    after :each do
+      subject.close
+    end
+
+    context "when metric.collect is disabled" do
+      before :each do
+        settings.set("metric.collect", false)
+      end
+
+      context "if namespaced_metric is nil" do
+        let(:metric) { nil }
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+      end
+
+      context "if namespaced_metric is a Metric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
+
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
+
+      context "if namespaced_metric is a NullMetric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
+
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(::LogStash::Instrument::NullMetric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
+    end
+
+    context "when metric.collect is enabled" do
+      before :each do
+        settings.set("metric.collect", true)
+      end
+
+      context "if namespaced_metric is nil" do
+        let(:metric) { nil }
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+      end
+
+      context "if namespaced_metric is a Metric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
 
-    it "uses a `NullMetric` object if no metric is given" do
-      pipeline = LogStash::Pipeline.new(config)
-      expect(pipeline.metric).to be_kind_of(LogStash::Instrument::NullMetric)
+        it "uses a `Metric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::Metric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
+
+      context "if namespaced_metric is a NullMetric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
+
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
     end
   end
 
@@ -391,13 +552,25 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinputgenerator").and_return(DummyInputGenerator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputmore").and_return(DummyOutputMore)
     end
 
+    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
+    before :each do
+      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
+      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
+      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    end
+
     let(:pipeline1) { LogStash::Pipeline.new("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
     let(:pipeline2) { LogStash::Pipeline.new("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutputmore {}}") }
 
+    after  do
+      pipeline1.close
+      pipeline2.close
+    end
+
     it "should handle evaluating different config" do
       expect(pipeline1.output_func(LogStash::Event.new)).not_to include(nil)
       expect(pipeline1.filter_func(LogStash::Event.new)).not_to include(nil)
@@ -407,47 +580,45 @@ class TestPipeline < LogStash::Pipeline
   end
 
   context "Periodic Flush" do
-    let(:number_of_events) { 100 }
     let(:config) do
       <<-EOS
       input {
-        generator {
-          count => #{number_of_events}
-        }
+        dummy_input {}
       }
       filter {
-        multiline {
-          pattern => "^NeverMatch"
-          negate => true
-          what => "previous"
-        }
+        dummy_flushing_filter {}
       }
       output {
-        dummyoutput {}
+        dummy_output {}
       }
       EOS
     end
-    let(:output) { DummyOutput.new }
+    let(:output) { ::LogStash::Outputs::DummyOutput.new }
 
     before do
-      allow(DummyOutput).to receive(:new).with(any_args).and_return(output)
-      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(output)
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummy_input").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummy_flushing_filter").and_return(DummyFlushingFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummy_output").and_return(::LogStash::Outputs::DummyOutput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
-      allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
     end
 
-    it "flushes the buffered contents of the filter" do
+    it "flush periodically" do
       Thread.abort_on_exception = true
-      pipeline = LogStash::Pipeline.new(config, { :flush_interval => 1 })
-      Thread.new { pipeline.run }
-      sleep 0.1 while !pipeline.ready?
-      # give us a bit of time to flush the events
-      wait(5).for do
-        next unless output && output.events && output.events.first
-        output.events.first["message"].split("\n").count
-      end.to eq(number_of_events)
+
+      pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
+      t = Thread.new { pipeline.run }
+      sleep(0.1) until pipeline.ready?
+      wait(10).for do
+        # give us a bit of time to flush the events
+        output.events.empty?
+      end.to be_falsey
+
+      expect(output.events.any? {|e| e.get("message") == "dummy_flush"}).to eq(true)
+
       pipeline.shutdown
+
+      t.join
     end
   end
 
@@ -456,15 +627,22 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     end
 
     let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
     let(:pipeline2) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
 
+    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
+    before :each do
+      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
+      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
+      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    end
+
     it "should handle evaluating different config" do
       # When the functions are compiled from the AST it will generate instance
-      # variables that are unique to the actual config, the intances are pointing
+      # variables that are unique to the actual config, the instances are pointing
       # to conditionals and/or plugins.
       #
       # Before the `defined_singleton_method`, the definition of the method was
@@ -481,25 +659,31 @@ class TestPipeline < LogStash::Pipeline
   end
 
   context "#started_at" do
+    # use a run limiting count to shutdown the pipeline automatically
     let(:config) do
       <<-EOS
       input {
-        generator {}
+        generator { count => 10 }
       }
       EOS
     end
 
     subject { described_class.new(config) }
 
-    it "returns nil when the pipeline isnt started" do
-      expect(subject.started_at).to be_nil
+    context "when the pipeline is not started" do
+      after :each do
+        subject.close
+      end
+
+      it "returns nil when the pipeline isnt started" do
+        expect(subject.started_at).to be_nil
+      end
     end
 
     it "return when the pipeline started working" do
-      t = Thread.new { subject.run }
-      sleep(0.1)
+      subject.run
       expect(subject.started_at).to be < Time.now
-      t.kill rescue nil
+      subject.shutdown
     end
   end
 
@@ -514,6 +698,10 @@ class TestPipeline < LogStash::Pipeline
     subject { described_class.new(config) }
 
     context "when the pipeline is not started" do
+      after :each do
+        subject.close
+      end
+
       it "returns 0" do
         expect(subject.uptime).to eq(0)
       end
@@ -521,19 +709,27 @@ class TestPipeline < LogStash::Pipeline
 
     context "when the pipeline is started" do
       it "return the duration in milliseconds" do
-        t = Thread.new { subject.run }
+        # subject must be first call outside the thread context because of lazy initialization
+        s = subject
+        t = Thread.new { s.run }
+        sleep(0.1) until subject.ready?
+
         sleep(0.1)
         expect(subject.uptime).to be > 0
-        t.kill rescue nil
+        subject.shutdown
+        t.join
       end
     end
   end
 
   context "when collecting metrics in the pipeline" do
-    subject { described_class.new(config, { :metric => metric, :pipeline_id => pipeline_id }) }
-    let(:pipeline_id) { :main }
-    let(:metric) { LogStash::Instrument::Metric.new }
-    let(:number_of_events) { 1000 }
+    let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+
+    subject { described_class.new(config, pipeline_settings_obj, metric) }
+
+    let(:pipeline_settings) { { "pipeline.id" => pipeline_id } }
+    let(:pipeline_id) { "main" }
+    let(:number_of_events) { 420 }
     let(:multiline_id) { "my-multiline" }
     let(:multiline_id_other) { "my-multiline_other" }
     let(:dummy_output_id) { "my-dummyoutput" }
@@ -565,31 +761,42 @@ class TestPipeline < LogStash::Pipeline
       }
       EOS
     end
-    let(:dummyoutput) { DummyOutput.new({ "id" => dummy_output_id }) }
+    let(:dummyoutput) { ::LogStash::Outputs::DummyOutput.new({ "id" => dummy_output_id }) }
+    let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
+    let(:pipeline_thread) do
+      # subject has to be called for the first time outside the thread because it will create a race condition
+      # with the subject.ready? call since subject is lazily initialized
+      s = subject
+      Thread.new { s.run }
+    end
 
     before :each do
-      allow(DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
+      allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
       allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
 
-      # Reset the metric store
-      LogStash::Instrument::Collector.instance.clear
+      pipeline_thread
+      sleep(0.1) until subject.ready?
 
-      Thread.new { subject.run }
       # make sure we have received all the generated events
-      sleep 1 while dummyoutput.events.size < number_of_events
+      wait(3).for do
+        # give us a bit of time to flush the events
+        dummyoutput.events.size >= number_of_events
+      end.to be_truthy
     end
 
     after :each do
       subject.shutdown
+      pipeline_thread.join
     end
 
     context "global metric" do
-      let(:collected_metric) { LogStash::Instrument::Collector.instance.snapshot_metric.metric_store.get_with_path("stats/events") }
+      let(:collected_metric) { metric_store.get_with_path("stats/events") }
 
-      it "populates the differents" do
+      it "populates the different metrics" do
+        expect(collected_metric[:stats][:events][:duration_in_millis].value).not_to be_nil
         expect(collected_metric[:stats][:events][:in].value).to eq(number_of_events)
         expect(collected_metric[:stats][:events][:filtered].value).to eq(number_of_events)
         expect(collected_metric[:stats][:events][:out].value).to eq(number_of_events)
@@ -597,9 +804,10 @@ class TestPipeline < LogStash::Pipeline
     end
 
     context "pipelines" do
-      let(:collected_metric) { LogStash::Instrument::Collector.instance.snapshot_metric.metric_store.get_with_path("stats/pipelines/") }
+      let(:collected_metric) { metric_store.get_with_path("stats/pipelines/") }
 
       it "populates the pipelines core metrics" do
+        expect(collected_metric[:stats][:pipelines][:main][:events][:duration_in_millis].value).not_to be_nil
         expect(collected_metric[:stats][:pipelines][:main][:events][:in].value).to eq(number_of_events)
         expect(collected_metric[:stats][:pipelines][:main][:events][:filtered].value).to eq(number_of_events)
         expect(collected_metric[:stats][:pipelines][:main][:events][:out].value).to eq(number_of_events)
@@ -608,15 +816,89 @@ class TestPipeline < LogStash::Pipeline
       it "populates the filter metrics" do
         [multiline_id, multiline_id_other].map(&:to_sym).each do |id|
           [:in, :out].each do |metric_key|
-            plugin_name = "multiline_#{id}".to_sym
+            plugin_name = id.to_sym
             expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:events][metric_key].value).to eq(number_of_events)
           end
         end
       end
 
       it "populates the output metrics" do
-        plugin_name = "dummyoutput_#{dummy_output_id}".to_sym
+        plugin_name = dummy_output_id.to_sym
+
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:in].value).to eq(number_of_events)
         expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:out].value).to eq(number_of_events)
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:duration_in_millis].value).not_to be_nil
+      end
+
+      it "populates the name of the output plugin" do
+        plugin_name = dummy_output_id.to_sym
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:name].value).to eq(::LogStash::Outputs::DummyOutput.config_name)
+      end
+
+      it "populates the name of the filter plugin" do
+        [multiline_id, multiline_id_other].map(&:to_sym).each do |id|
+          plugin_name = id.to_sym
+          expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:name].value).to eq(LogStash::Filters::Multiline.config_name)
+        end
+      end
+    end
+  end
+
+  context "Pipeline object" do
+    before do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
+    end
+
+    let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+    let(:pipeline2) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+
+    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
+    before :each do
+      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
+      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
+      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    end
+
+    it "should not add ivars" do
+       expect(pipeline1.instance_variables).to eq(pipeline2.instance_variables)
+    end
+  end
+
+  describe "#system?" do
+    after do
+      pipeline.close # close the queue
+    end
+
+    let(:pipeline) { LogStash::Pipeline.new(config_string, settings) }
+    let(:config_string) { "input { generator {} } output { null {} }" }
+
+    context "when the pipeline is a system pipeline" do
+      let(:settings) do
+        s = LogStash::SETTINGS.clone
+        s.set("pipeline.system", true)
+        s.set("config.string", config_string)
+        s
+      end
+
+
+      it "returns true" do
+        expect(pipeline.system?).to be_truthy
+      end
+    end
+
+    context "when the pipeline is not a system pipeline" do
+      let(:settings) do
+        s = LogStash::SETTINGS.clone
+        s.set("pipeline.system", false)
+        s.set("config.string", config_string)
+        s
+      end
+
+      it "returns true" do
+        expect(pipeline.system?).to be_falsey
       end
     end
   end
diff --git a/logstash-core/spec/logstash/plugin_spec.rb b/logstash-core/spec/logstash/plugin_spec.rb
index 3950fbcc6e3..b6c0bf62d92 100644
--- a/logstash-core/spec/logstash/plugin_spec.rb
+++ b/logstash-core/spec/logstash/plugin_spec.rb
@@ -5,15 +5,62 @@
 require "logstash/codecs/base"
 require "logstash/inputs/base"
 require "logstash/filters/base"
+require "logstash/execution_context"
 
 describe LogStash::Plugin do
-  it "should fail lookup on inexisting type" do
-    expect_any_instance_of(Cabin::Channel).to receive(:debug).once
+  context "reloadable" do
+    context "by default" do
+      subject do
+        Class.new(LogStash::Plugin) do
+        end
+      end
+
+      it "makes .reloadable? return true" do
+        expect(subject.reloadable?).to be_truthy
+      end
+
+      it "makes #reloadable? return true" do
+        expect(subject.new({}).reloadable?).to be_truthy
+      end
+    end
+
+    context "user can overrides" do
+      subject do
+        Class.new(LogStash::Plugin) do
+          def self.reloadable?
+            false
+          end
+        end
+      end
+
+      it "makes .reloadable? return true" do
+        expect(subject.reloadable?).to be_falsey
+      end
+
+      it "makes #reloadable? return true" do
+        expect(subject.new({}).reloadable?).to be_falsey
+      end
+    end
+  end
+
+  context "#execution_context" do
+    subject { Class.new(LogStash::Plugin).new({}) }
+
+    it "can be set and get" do
+      expect(subject.execution_context).to be_nil
+      execution_context = LogStash::ExecutionContext.new(:main)
+      subject.execution_context = execution_context
+      expect(subject.execution_context).to eq(execution_context)
+    end
+  end
+
+  it "should fail lookup on nonexistent type" do
+    #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
     expect { LogStash::Plugin.lookup("badbadtype", "badname") }.to raise_error(LogStash::PluginLoadingError)
   end
 
-  it "should fail lookup on inexisting name" do
-    expect_any_instance_of(Cabin::Channel).to receive(:debug).once
+  it "should fail lookup on nonexistent name" do
+    #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
     expect { LogStash::Plugin.lookup("filter", "badname") }.to raise_error(LogStash::PluginLoadingError)
   end
 
@@ -31,6 +78,7 @@
     class LogStash::Filters::LadyGaga < LogStash::Filters::Base
       config_name "lady_gaga"
     end
+
     expect(LogStash::Plugin.lookup("filter", "lady_gaga")).to eq(LogStash::Filters::LadyGaga)
   end
 
@@ -63,7 +111,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('1.0.0')))
 
-      expect_any_instance_of(Cabin::Channel).not_to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).not_to receive(:info)
       subject.validate({})
     end
 
@@ -72,7 +120,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('0.9.1')))
 
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:info)
         .with(/Using version 0.9.x/)
 
       subject.validate({})
@@ -83,7 +131,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('0.1.1')))
 
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:info)
         .with(/Using version 0.1.x/)
       subject.validate({})
     end
@@ -97,7 +145,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('0.1.1')))
 
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:info)
         .once
         .with(/Using version 0.1.x/)
 
@@ -105,17 +153,13 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
       one_notice.validate({})
     end
 
-    it "warns the user if we can't find a defined version" do
-      expect_any_instance_of(Cabin::Channel).to receive(:warn)
-        .once
-        .with(/plugin doesn't have a version/)
-
-      subject.validate({})
+    it "doesn't raise an exception if no version is found" do
+      expect { subject.validate({}) }.not_to raise_error
     end
 
 
     it 'logs a warning if the plugin use the milestone option' do
-      expect_any_instance_of(Cabin::Channel).to receive(:debug)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:debug)
         .with(/stromae plugin is using the 'milestone' method/)
 
       class LogStash::Filters::Stromae < LogStash::Filters::Base
@@ -185,7 +229,7 @@ def register; end
           config_name "simple_plugin"
 
           config :host, :validate => :string
-          config :export, :validte => :boolean
+          config :export, :validate => :boolean
 
           def register; end
         end
@@ -223,7 +267,7 @@ def register; end
     end
   end
 
-  describe "#plugin_unique_name" do
+  describe "#id" do
     let(:plugin) do
       Class.new(LogStash::Filters::Base,) do
         config_name "simple_plugin"
@@ -245,7 +289,7 @@ def register; end
       subject { plugin.new(config) }
 
       it "return a human readable ID" do
-        expect(subject.plugin_unique_name).to eq("simple_plugin_#{my_id}")
+        expect(subject.id).to eq(my_id)
       end
     end
 
@@ -253,7 +297,81 @@ def register; end
       subject { plugin.new(config) }
 
       it "return a human readable ID" do
-        expect(subject.plugin_unique_name).to match(/^simple_plugin_/)
+        expect(subject.id).to match(/^simple_plugin_/)
+      end
+    end
+  end
+
+
+  context "When the plugin record a metric" do
+    let(:config) { {} }
+
+    [LogStash::Inputs::Base, LogStash::Filters::Base, LogStash::Outputs::Base].each do |base|
+      let(:plugin) do
+        Class.new(base) do
+          #include LogStash::Util::Loggable
+          config_name "testing"
+
+          def register
+            metric.gauge("power_level", 9000)
+          end
+        end
+      end
+
+      subject { plugin.new(config) } 
+
+      context "when no metric is set to the plugin" do
+        context "when `enable_metric` is TRUE" do
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use a `NullMetric`" do
+            expect(subject.metric).to be_kind_of(LogStash::Instrument::NamespacedNullMetric)
+          end
+        end
+
+        context "when `enable_metric` is FALSE" do
+          let(:config) { { "enable_metric" => false } }
+
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use a `NullMetric`" do
+            expect(subject.metric).to be_kind_of(LogStash::Instrument::NamespacedNullMetric)
+          end
+        end
+      end
+
+      context "When a specific metric collector is configured" do
+        context "when `enable_metric` is TRUE" do
+          let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new).namespace("dbz") }
+
+          before :each do
+            subject.metric = metric
+          end
+
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use the configured metric" do
+            expect(subject.metric).to eq(metric)
+          end
+        end
+
+        context "when `enable_metric` is FALSE" do
+          let(:config) { { "enable_metric" => false } }
+
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use a `NullMetric`" do
+            expect(subject.metric).to be_kind_of(LogStash::Instrument::NamespacedNullMetric)
+          end
+        end
       end
     end
   end
diff --git a/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb b/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
new file mode 100644
index 00000000000..7a20e16945e
--- /dev/null
+++ b/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
@@ -0,0 +1,60 @@
+# encoding: utf-8
+require "logstash/event_dispatcher"
+require "logstash/plugins/hooks_registry"
+
+describe LogStash::Plugins::HooksRegistry do
+  class DummyEmitter
+    attr_reader :dispatcher
+
+    def initialize
+      @dispatcher = LogStash::EventDispatcher.new(self)
+    end
+
+    def do_work
+      dispatcher.fire(:do_work)
+    end
+  end
+
+  class DummyListener
+    def initialize
+      @work = false
+    end
+
+    def do_work(emitter = nil)
+      @work = true
+    end
+
+    def work?
+      @work
+    end
+  end
+
+  subject { described_class.new }
+
+  let(:emitter) { DummyEmitter.new }
+  let(:listener) { DummyListener.new }
+
+  it "allow to register an emitter" do
+    expect { subject.register_emitter(emitter.class, emitter.dispatcher) }.to change { subject.emitters_count }.by(1)
+  end
+
+  it "allow to remove an emitter" do
+    subject.register_emitter(emitter.class, emitter.dispatcher)
+    expect { subject.remove_emitter(emitter.class)}.to change { subject.emitters_count }.by(-1)
+  end
+
+  it "allow to register hooks to emitters" do
+    expect { subject.register_hooks(emitter.class, listener) }.to change { subject.hooks_count }.by(1)
+    expect { subject.register_hooks(emitter.class, listener) }.to change { subject.hooks_count(emitter.class) }.by(1)
+  end
+
+  it "link the emitter class to the listener" do
+    subject.register_emitter(emitter.class, emitter.dispatcher)
+    subject.register_hooks(emitter.class, listener)
+
+    expect(listener.work?).to be_falsey
+    emitter.do_work
+
+    expect(listener.work?).to be_truthy
+  end
+end
diff --git a/logstash-core/spec/logstash/plugins/registry_spec.rb b/logstash-core/spec/logstash/plugins/registry_spec.rb
new file mode 100644
index 00000000000..f26068ea5ab
--- /dev/null
+++ b/logstash-core/spec/logstash/plugins/registry_spec.rb
@@ -0,0 +1,65 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/plugins/registry"
+require "logstash/inputs/base"
+
+# use a dummy NOOP input to test plugin registry
+class LogStash::Inputs::Dummy < LogStash::Inputs::Base
+  config_name "dummy"
+
+  def register; end
+end
+
+
+class LogStash::Inputs::NewPlugin < LogStash::Inputs::Base
+  config_name "new_plugin"
+
+  def register; end
+end
+
+describe LogStash::Plugins::Registry do
+  let(:registry) { described_class.new }
+
+  context "when loading installed plugins" do
+    let(:plugin) { double("plugin") }
+
+    it "should return the expected class" do
+      klass = registry.lookup("input", "stdin")
+      expect(klass).to eq(LogStash::Inputs::Stdin)
+    end
+
+    it "should raise an error if can not find the plugin class" do
+      expect { registry.lookup("input", "do-not-exist-elastic") }.to raise_error(LoadError)
+    end
+
+    it "should load from registry is already load" do
+      expect(registry.exists?(:input, "stdin")).to be_falsey
+      expect { registry.lookup("input", "new_plugin") }.to change { registry.size }.by(1)
+      expect { registry.lookup("input", "new_plugin") }.not_to change { registry.size }
+    end
+  end
+
+  context "when loading code defined plugins" do
+    it "should return the expected class" do
+      klass = registry.lookup("input", "dummy")
+      expect(klass).to eq(LogStash::Inputs::Dummy)
+    end
+  end
+
+  context "when plugin is not installed and not defined" do
+    it "should raise an error" do
+      expect { registry.lookup("input", "elastic") }.to raise_error(LoadError)
+    end
+  end
+
+  context "when loading plugin manually configured" do
+    it "should return the plugin" do
+      class SimplePlugin
+      end
+
+      expect { registry.lookup("filter", "simple_plugin") }.to raise_error(LoadError)
+      registry.add(:filter, "simple_plugin", SimplePlugin)
+      expect(registry.lookup("filter", "simple_plugin")).to eq(SimplePlugin)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/queue_factory_spec.rb b/logstash-core/spec/logstash/queue_factory_spec.rb
new file mode 100644
index 00000000000..9182c9c95be
--- /dev/null
+++ b/logstash-core/spec/logstash/queue_factory_spec.rb
@@ -0,0 +1,83 @@
+# encoding: utf-8
+require "logstash/queue_factory"
+require "logstash/settings"
+require "stud/temporary"
+
+describe LogStash::QueueFactory do
+  let(:pipeline_id) { "my_pipeline" }
+  let(:settings_array) do
+    [
+      LogStash::Setting::WritableDirectory.new("path.queue", Stud::Temporary.pathname),
+      LogStash::Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked"]),
+      LogStash::Setting::Bytes.new("queue.page_capacity", "250mb"),
+      LogStash::Setting::Bytes.new("queue.max_bytes", "1024mb"),
+      LogStash::Setting::Numeric.new("queue.max_events", 0),
+      LogStash::Setting::Numeric.new("queue.checkpoint.acks", 1024),
+      LogStash::Setting::Numeric.new("queue.checkpoint.writes", 1024),
+      LogStash::Setting::Numeric.new("queue.checkpoint.interval", 1000),
+      LogStash::Setting::String.new("pipeline.id", pipeline_id)
+    ]
+  end
+
+  let(:settings) do
+    s = LogStash::Settings.new
+
+    settings_array.each do |setting|
+      s.register(setting)
+    end
+    s
+  end
+
+  subject { described_class }
+
+  context "when `queue.type` is `persisted`" do
+    before do
+      settings.set("queue.type", "persisted")
+    end
+
+    it "returns a `WrappedAckedQueue`" do
+      queue =  subject.create(settings)
+      expect(queue).to be_kind_of(LogStash::Util::WrappedAckedQueue)
+      queue.close
+    end
+
+    describe "per pipeline id subdirectory creation" do
+      let(:queue_path) { ::File.join(settings.get("path.queue"), pipeline_id) }
+
+      after :each do
+        FileUtils.rmdir(queue_path)
+      end
+
+      it "creates a queue directory based on the pipeline id" do
+        expect(Dir.exist?(queue_path)).to be_falsey
+        queue = subject.create(settings)
+        expect(Dir.exist?(queue_path)).to be_truthy
+        queue.close
+      end
+    end
+  end
+
+  context "when `queue.type` is `memory_acked`" do
+    before do
+      settings.set("queue.type", "memory_acked")
+    end
+
+    it "returns a `WrappedAckedQueue`" do
+      queue =  subject.create(settings)
+      expect(queue).to be_kind_of(LogStash::Util::WrappedAckedQueue)
+      queue.close
+    end
+  end
+
+  context "when `queue.type` is `memory`" do
+    before do
+      settings.set("queue.type", "memory")
+    end
+
+    it "returns a `WrappedAckedQueue`" do
+      queue =  subject.create(settings)
+      expect(queue).to be_kind_of(LogStash::Util::WrappedSynchronousQueue)
+      queue.close
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/runner_spec.rb b/logstash-core/spec/logstash/runner_spec.rb
index f8bcd9a6f35..e64412ebdc1 100644
--- a/logstash-core/spec/logstash/runner_spec.rb
+++ b/logstash-core/spec/logstash/runner_spec.rb
@@ -3,7 +3,10 @@
 require "logstash/runner"
 require "stud/task"
 require "stud/trap"
+require "stud/temporary"
 require "logstash/util/java_version"
+require "logstash/logging/json"
+require "json"
 
 class NullRunner
   def run(args); end
@@ -12,22 +15,61 @@ def run(args); end
 describe LogStash::Runner do
 
   subject { LogStash::Runner }
-  let(:channel) { Cabin::Channel.new }
+  let(:logger) { double("logger") }
 
   before :each do
-    allow(Cabin::Channel).to receive(:get).with(LogStash).and_return(channel)
+    allow(LogStash::Runner).to receive(:logger).and_return(logger)
+    allow(logger).to receive(:debug?).and_return(true)
+    allow(logger).to receive(:subscribe).with(any_args)
+    allow(logger).to receive(:debug) {}
+    allow(logger).to receive(:log) {}
+    allow(logger).to receive(:info) {}
+    allow(logger).to receive(:fatal) {}
+    allow(logger).to receive(:warn) {}
+    allow(LogStash::ShutdownWatcher).to receive(:logger).and_return(logger)
+    allow(LogStash::Logging::Logger).to receive(:configure_logging) do |level, path|
+      allow(logger).to receive(:level).and_return(level.to_sym)
+    end
+  end
+
+  after :each do
+    LogStash::SETTINGS.reset
+  end
+
+  after :all do
+  end
+
+  describe "argument precedence" do
+    let(:config) { "input {} output {}" }
+    let(:cli_args) { ["-e", config, "-w", "20"] }
+    let(:settings_yml_hash) { { "pipeline.workers" => 2 } }
+
+    before :each do
+      allow(LogStash::SETTINGS).to receive(:read_yaml).and_return(settings_yml_hash)
+    end
+
+    after :each do
+      LogStash::SETTINGS.reset
+    end
+
+    it "favors the last occurence of an option" do
+      expect(LogStash::Agent).to receive(:new) do |settings|
+        expect(settings.get("config.string")).to eq(config)
+        expect(settings.get("pipeline.workers")).to eq(20)
+      end
+      subject.run("bin/logstash", cli_args)
+    end
   end
 
   describe "argument parsing" do
     subject { LogStash::Runner.new("") }
+
     context "when -e is given" do
 
       let(:args) { ["-e", "input {} output {}"] }
       let(:agent) { double("agent") }
-      let(:agent_logger) { double("agent logger") }
 
       before do
-        allow(agent).to receive(:logger=).with(anything)
         allow(agent).to receive(:shutdown)
         allow(agent).to receive(:register_pipeline)
       end
@@ -41,10 +83,8 @@ def run(args); end
 
     context "with no arguments" do
       let(:args) { [] }
-      let(:agent) { double("agent") }
 
       before(:each) do
-        allow(LogStash::Agent).to receive(:new).and_return(agent)
         allow(LogStash::Util::JavaVersion).to receive(:warn_on_bad_java_version)
       end
 
@@ -95,6 +135,25 @@ def run(args); end
     end
   end
 
+  describe "--config.test_and_exit" do
+    subject { LogStash::Runner.new("") }
+    let(:args) { ["-t", "-e", pipeline_string] }
+
+    context "with a good configuration" do
+      let(:pipeline_string) { "input { } filter { } output { }" }
+      it "should exit successfully" do
+        expect(subject.run(args)).to eq(0)
+      end
+    end
+
+    context "with a bad configuration" do
+      let(:pipeline_string) { "rlwekjhrewlqrkjh" }
+      it "should fail by returning a bad exit code" do
+        expect(logger).to receive(:fatal)
+        expect(subject.run(args)).to eq(1)
+      end
+    end
+  end
   describe "pipeline settings" do
     let(:pipeline_string) { "input { stdin {} } output { stdout {} }" }
     let(:main_pipeline_settings) { { :pipeline_id => "main" } }
@@ -106,24 +165,236 @@ def run(args); end
       allow(pipeline).to receive(:run).and_return(task)
       allow(pipeline).to receive(:shutdown)
     end
+    
+    context "when :path.data is defined by the user" do
+      let(:test_data_path) { "/tmp/ls-test-data" }
+      let(:test_queue_path) { test_data_path + "/" + "queue" }
+      
+      it "should set data paths" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.get("path.data")).to eq(test_data_path)
+          expect(settings.get("path.queue")).to eq(test_queue_path)
+        end
+        
+        args = ["--path.data", test_data_path, "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+      
+      context "and path.queue is manually set" do
+        let(:queue_override_path) { "/tmp/queue-override_path" }
+        
+        it "should set data paths" do
+          expect(LogStash::Agent).to receive(:new) do |settings|
+            expect(settings.get("path.data")).to eq(test_data_path)
+            expect(settings.get("path.queue")).to eq(queue_override_path)
+          end
+          
+          LogStash::SETTINGS.set("path.queue", queue_override_path)
+          
+          args = ["--path.data", test_data_path, "-e", pipeline_string]
+          subject.run("bin/logstash", args)
+        end
+      end
+    end
+    
+    context "when :http.host is defined by the user" do
+      it "should pass the value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.host")).to be(true)
+          expect(settings.get("http.host")).to eq("localhost")
+        end
+
+        args = ["--http.host", "localhost", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    context "when :http.host is not defined by the user" do
+      it "should pass the value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.host")).to be_falsey
+          expect(settings.get("http.host")).to eq("127.0.0.1")
+        end
+
+        args = ["-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    context "when :http.port is defined by the user" do
+      it "should pass a single value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.port")).to be(true)
+          expect(settings.get("http.port")).to eq(10000..10000)
+        end
+
+        args = ["--http.port", "10000", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+
+      it "should pass a range value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.port")).to be(true)
+          expect(settings.get("http.port")).to eq(10000..20000)
+        end
+
+        args = ["--http.port", "10000-20000", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    context "when no :http.port is not defined by the user" do
+      it "should use the default settings" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.port")).to be_falsey
+          expect(settings.get("http.port")).to eq(9600..9700)
+        end
+
+        args = ["-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
 
     context "when :pipeline_workers is not defined by the user" do
       it "should not pass the value to the pipeline" do
-        expect(LogStash::Pipeline).to receive(:new).once.with(pipeline_string, hash_excluding(:pipeline_workers)).and_return(pipeline)
-
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("pipeline.workers")).to be(false)
+        end
         args = ["-e", pipeline_string]
         subject.run("bin/logstash", args)
       end
     end
 
+    context "when :pipeline_workers flag is passed without a value" do
+      it "should raise an error" do
+        args = ["-e", pipeline_string, "-w"]
+        expect { subject.run("bin/logstash", args) }.to raise_error
+      end
+    end
+
     context "when :pipeline_workers is defined by the user" do
       it "should pass the value to the pipeline" do
-        main_pipeline_settings[:pipeline_workers] = 2
-        expect(LogStash::Pipeline).to receive(:new).with(pipeline_string, hash_including(main_pipeline_settings)).and_return(pipeline)
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("pipeline.workers")).to be(true)
+          expect(settings.get("pipeline.workers")).to be(2)
+        end
 
         args = ["-w", "2", "-e", pipeline_string]
         subject.run("bin/logstash", args)
       end
     end
+
+    describe "config.debug" do
+      it "should set 'config.debug' to false by default" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.get("config.debug")).to eq(false)
+        end
+        args = ["--log.level", "debug", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+
+      it "should allow overriding config.debug" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.get("config.debug")).to eq(true)
+        end
+        args = ["--log.level", "debug", "--config.debug",  "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+  end
+
+  describe "--log.level" do
+    before :each do
+      allow_any_instance_of(subject).to receive(:show_version)
+    end
+    context "when not set" do
+      it "should set log level to warn" do
+        args = ["--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:info)
+      end
+    end
+    context "when setting to debug" do
+      it "should set log level to debug" do
+        args = ["--log.level", "debug",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:debug)
+      end
+    end
+    context "when setting to verbose" do
+      it "should set log level to info" do
+        args = ["--log.level", "info",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:info)
+      end
+    end
+    context "when setting to quiet" do
+      it "should set log level to error" do
+        args = ["--log.level", "error",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:error)
+      end
+    end
+
+    context "deprecated flags" do
+      context "when using --quiet" do
+        it "should warn about the deprecated flag" do
+          expect(logger).to receive(:warn).with(/DEPRECATION WARNING/)
+          args = ["--quiet", "--version"]
+          subject.run("bin/logstash", args)
+        end
+
+        it "should still set the log level accordingly" do
+          args = ["--quiet", "--version"]
+          subject.run("bin/logstash", args)
+          expect(logger.level).to eq(:error)
+        end
+      end
+      context "when using --debug" do
+        it "should warn about the deprecated flag" do
+          expect(logger).to receive(:warn).with(/DEPRECATION WARNING/)
+          args = ["--debug", "--version"]
+          subject.run("bin/logstash", args)
+        end
+
+        it "should still set the log level accordingly" do
+          args = ["--debug", "--version"]
+          subject.run("bin/logstash", args)
+          expect(logger.level).to eq(:debug)
+        end
+      end
+      context "when using --verbose" do
+        it "should warn about the deprecated flag" do
+          expect(logger).to receive(:warn).with(/DEPRECATION WARNING/)
+          args = ["--verbose", "--version"]
+          subject.run("bin/logstash", args)
+        end
+
+        it "should still set the log level accordingly" do
+          args = ["--verbose", "--version"]
+          subject.run("bin/logstash", args)
+          expect(logger.level).to eq(:info)
+        end
+      end
+    end
+  end
+
+  describe "path.settings" do
+    subject { LogStash::Runner.new("") }
+    context "if does not exist" do
+      let(:args) { ["--path.settings", "/tmp/a/a/a/a", "-e", "input {} output {}"] }
+
+      it "should not terminate logstash" do
+        expect(subject.run(args)).to eq(nil)
+      end
+
+      context "but if --help is passed" do
+        let(:args) { ["--path.settings", "/tmp/a/a/a/a", "--help"] }
+
+        it "should show help" do
+          expect { subject.run(args) }.to raise_error(Clamp::HelpWanted)
+        end
+      end
+    end
   end
 end
diff --git a/logstash-core/spec/logstash/setting_spec.rb b/logstash-core/spec/logstash/setting_spec.rb
new file mode 100644
index 00000000000..e16c1ed1353
--- /dev/null
+++ b/logstash-core/spec/logstash/setting_spec.rb
@@ -0,0 +1,152 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting do
+  let(:logger) { double("logger") }
+  describe "#value" do
+    context "when using a default value" do
+      context "when no value is set" do
+        subject { described_class.new("number", Numeric, 1) }
+        it "should return the default value" do
+          expect(subject.value).to eq(1)
+        end
+      end
+
+      context "when a value is set" do
+        subject { described_class.new("number", Numeric, 1) }
+        let(:new_value) { 2 }
+        before :each do
+          subject.set(new_value)
+        end
+        it "should return the set value" do
+          expect(subject.value).to eq(new_value)
+        end
+      end
+    end
+
+    context "when not using a default value" do
+      context "when no value is set" do
+        subject { described_class.new("number", Numeric, nil, false) }
+        it "should return the default value" do
+          expect(subject.value).to eq(nil)
+        end
+      end
+
+      context "when a value is set" do
+        subject { described_class.new("number", Numeric, nil, false) }
+        let(:new_value) { 2 }
+        before :each do
+          subject.set(new_value)
+        end
+        it "should return the set value" do
+          expect(subject.value).to eq(new_value)
+        end
+      end
+    end
+  end
+
+  describe "#set?" do
+    context "when there is not value set" do
+      subject { described_class.new("number", Numeric, 1) }
+      it "should return false" do
+        expect(subject.set?).to be(false)
+      end
+    end
+    context "when there is a value set" do
+      subject { described_class.new("number", Numeric, 1) }
+      before :each do
+        subject.set(2)
+      end
+      it "should return false" do
+        expect(subject.set?).to be(true)
+      end
+    end
+  end
+
+  describe "#set" do
+    subject { described_class.new("number", Numeric, 1) }
+    it "should change the value of a setting" do
+      expect(subject.value).to eq(1)
+      subject.set(4)
+      expect(subject.value).to eq(4)
+    end
+    context "when executed for the first time" do
+      it "should change the result of set?" do
+        expect(subject.set?).to eq(false)
+        subject.set(4)
+        expect(subject.set?).to eq(true)
+      end
+    end
+    context "when the argument's class does not match @klass" do
+      it "should throw an exception" do
+        expect { subject.set("not a number") }.to raise_error
+      end
+    end
+    context "when strict=false" do
+      let(:strict) { false }
+      subject { described_class.new("number", Numeric, 1, strict) }
+      before do
+        expect(subject).not_to receive(:validate)
+      end
+
+      it "should not call #validate" do
+        subject.set(123)
+      end
+    end
+    context "when strict=true" do
+      let(:strict) { true }
+      subject { described_class.new("number", Numeric, 1, strict) }
+      before do
+        expect(subject).to receive(:validate)
+      end
+
+      it "should call #validate" do
+        subject.set(123)
+      end
+    end
+  end
+
+  describe "#reset" do
+    subject { described_class.new("number", Numeric, 1) }
+    context "if value is already set" do
+      before :each do
+        subject.set(2)
+      end
+      it "should reset value to default" do
+        subject.reset
+        expect(subject.value).to eq(1)
+      end
+      it "should reset set? to false" do
+        expect(subject.set?).to eq(true)
+        subject.reset
+        expect(subject.set?).to eq(false)
+      end
+    end
+  end
+
+  describe "validator_proc" do
+    let(:default_value) { "small text" }
+    subject { described_class.new("mytext", String, default_value) {|v| v.size < 20 } }
+    context "when validation fails" do
+      let(:new_value) { "very very very very very big text" }
+      it "should raise an exception" do
+        expect { subject.set(new_value) }.to raise_error
+      end
+      it "should not change the value" do
+        subject.set(new_value) rescue nil
+        expect(subject.value).to eq(default_value)
+      end
+    end
+    context "when validation is successful" do
+      let(:new_value) { "smaller text" }
+      it "should not raise an exception" do
+        expect { subject.set(new_value) }.to_not raise_error
+      end
+      it "should change the value" do
+        subject.set(new_value)
+        expect(subject.value).to eq(new_value)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/array_coercible_spec.rb b/logstash-core/spec/logstash/settings/array_coercible_spec.rb
new file mode 100644
index 00000000000..7146ff0950a
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/array_coercible_spec.rb
@@ -0,0 +1,46 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::ArrayCoercible do
+  subject { described_class.new("option", element_class, value) }
+  let(:value) { [ ] }
+  let(:element_class) { Object }
+
+  context "when given a non array value" do
+    let(:value) { "test" }
+    describe "the value" do
+      it "is converted to an array with that single element" do
+        expect(subject.value).to eq(["test"])
+      end
+    end
+  end
+
+  context "when given an array value" do
+    let(:value) { ["test"] }
+    describe "the value" do
+      it "is not modified" do
+        expect(subject.value).to eq(value)
+      end
+    end
+  end
+
+  describe "initialization" do
+    subject { described_class }
+    let(:element_class) { Fixnum }
+    context "when given values of incorrect element class" do
+      let(:value) { "test" }
+
+      it "will raise an exception" do
+        expect { described_class.new("option", element_class, value) }.to raise_error(ArgumentError)
+      end
+    end
+    context "when given values of correct element class" do
+      let(:value) { 1 }
+
+      it "will not raise an exception" do
+        expect { described_class.new("option", element_class, value) }.not_to raise_error
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/bytes_spec.rb b/logstash-core/spec/logstash/settings/bytes_spec.rb
new file mode 100644
index 00000000000..b4fe0aab765
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/bytes_spec.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::Bytes do
+  let(:multipliers) do
+    {
+      "b" => 1,
+      "kb" => 1 << 10,
+      "mb" => 1 << 20,
+      "gb" => 1 << 30,
+      "tb" => 1 << 40,
+      "pb" => 1 << 50,
+    }
+  end
+
+  let(:number) { Flores::Random.number(0..1000) }
+  let(:unit) { Flores::Random.item(multipliers.keys) }
+  let(:default) { "0b" }
+
+  subject { described_class.new("a byte value", default, false) }
+
+  describe "#set" do
+
+    # Hard-coded test just to make sure at least one known case is working
+    context "when given '10mb'" do
+      it "returns 10485760" do
+        expect(subject.set("10mb")).to be == 10485760
+      end
+    end
+
+    context "when given a string" do
+      context "which is a valid byte unit" do
+        let(:text) { "#{number}#{unit}" }
+
+        before { subject.set(text) }
+
+        it "should coerce it to a Fixnum" do
+          expect(subject.value).to be_a(Fixnum)
+        end
+      end
+
+      context "which is not a valid byte unit" do
+        values = [ "hello world", "1234", "", "-__-" ]
+        values.each do |value|
+          it "should fail" do
+            expect { subject.set(value) }.to raise_error
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/integer_spec.rb b/logstash-core/spec/logstash/settings/integer_spec.rb
new file mode 100644
index 00000000000..f7097d96153
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/integer_spec.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::Integer do
+  subject { described_class.new("a number", nil, false) }
+  describe "#set" do
+    context "when giving a number which is not an integer" do
+      it "should raise an exception" do
+        expect { subject.set(1.1) }.to raise_error(ArgumentError)
+      end
+    end
+    context "when giving a number which is an integer" do
+      it "should set the number" do
+        expect { subject.set(100) }.to_not raise_error
+        expect(subject.value).to eq(100)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/numeric_spec.rb b/logstash-core/spec/logstash/settings/numeric_spec.rb
new file mode 100644
index 00000000000..cab162fce33
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/numeric_spec.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::Numeric do
+  subject { described_class.new("a number", nil, false) }
+  describe "#set" do
+    context "when giving a string which doesn't represent a string" do
+      it "should raise an exception" do
+        expect { subject.set("not-a-number") }.to raise_error(ArgumentError)
+      end
+    end
+    context "when giving a string which represents a " do
+      context "float" do
+        it "should coerce that string to the number" do
+          subject.set("1.1")
+          expect(subject.value).to eq(1.1)
+        end
+      end
+      context "int" do
+        it "should coerce that string to the number" do
+          subject.set("1")
+          expect(subject.value).to eq(1)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/port_range_spec.rb b/logstash-core/spec/logstash/settings/port_range_spec.rb
new file mode 100644
index 00000000000..6085b026da1
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/port_range_spec.rb
@@ -0,0 +1,93 @@
+# encoding: utf-8
+#
+require "logstash/settings"
+require "spec_helper"
+
+describe LogStash::Setting::PortRange do
+
+  context "When the value is a Fixnum" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", 9000) }
+
+    it "coerces the value in a range" do
+      expect { subject }.not_to raise_error
+    end
+
+    it "returns a range" do
+      expect(subject.value).to eq(9000..9000)
+    end
+
+    it "can update the range" do
+      subject.set(10000)
+      expect(subject.value).to eq(10000..10000)
+    end
+  end
+
+  context "When the value is a string" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", "9000-10000") }
+
+    it "coerces a string range with the format (9000-10000)" do
+      expect { subject }.not_to raise_error
+    end
+
+    it "refuses when then upper port is out of range" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", "1000-95000") }.to raise_error
+    end
+
+    it "returns a range" do
+      expect(subject.value).to eq(9000..10000)
+    end
+
+    it "can update the range" do
+      subject.set("500-1000")
+      expect(subject.value).to eq(500..1000)
+    end
+  end
+
+  context "when the value is a garbage string" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", "fsdfnsdkjnfjs") }
+
+    it "raises an argument error" do
+      expect { subject }.to raise_error
+    end
+
+
+    it "raises an exception on update" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", 10000).set("dsfnsdknfksdnfjksdnfjns") }.to raise_error
+    end
+  end
+
+  context "when the value is an unknown type" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", 0.1) }
+
+
+    it "raises an argument error" do
+      expect { subject }.to raise_error
+    end
+
+
+    it "raises an exception on update" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", 10000).set(0.1) }.to raise_error
+    end
+  end
+
+  context "When value is a range" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", 9000..10000) }
+
+    it "accepts a ruby range as the default value" do
+      expect { subject }.not_to raise_error
+    end
+
+    it "can update the range" do
+      subject.set(500..1000)
+      expect(subject.value).to eq(500..1000)
+    end
+
+    it "refuses when then upper port is out of range" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", 9000..1000000) }.to raise_error
+    end
+
+    it "raise an exception on when port are out of range" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", -1000..1000) }.to raise_error
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/string_spec.rb b/logstash-core/spec/logstash/settings/string_spec.rb
new file mode 100644
index 00000000000..69d835649ee
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/string_spec.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::String do
+  let(:possible_values) { ["a", "b", "c"] }
+  subject { described_class.new("mytext", possible_values.first, true, possible_values) }
+  describe "#set" do
+    context "when a value is given outside of possible_values" do
+      it "should raise an ArgumentError" do
+        expect { subject.set("d") }.to raise_error(ArgumentError)
+      end
+    end
+    context "when a value is given within possible_values" do
+      it "should set the value" do
+        expect { subject.set("a") }.to_not raise_error
+        expect(subject.value).to eq("a")
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/time_value_spec.rb b/logstash-core/spec/logstash/settings/time_value_spec.rb
new file mode 100644
index 00000000000..e1e4940d36a
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/time_value_spec.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::TimeValue do
+  subject { described_class.new("option", "-1") }
+  describe "#set" do
+    it "should coerce the default correctly" do
+      expect(subject.value).to eq(LogStash::Util::TimeValue.new(-1, :nanosecond).to_nanos)
+    end
+
+    context "when a value is given outside of possible_values" do
+      it "should raise an ArgumentError" do
+        expect { subject.set("invalid") }.to raise_error(ArgumentError)
+      end
+    end
+    context "when a value is given as a time value" do
+      it "should set the value" do
+        subject.set("18m")
+        expect(subject.value).to eq(LogStash::Util::TimeValue.new(18, :minute).to_nanos)
+      end
+    end
+
+    context "when a value is given as a nanosecond" do
+      it "should set the value" do
+        subject.set(5)
+        expect(subject.value).to eq(LogStash::Util::TimeValue.new(5, :nanosecond).to_nanos)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/writable_directory_spec.rb b/logstash-core/spec/logstash/settings/writable_directory_spec.rb
new file mode 100644
index 00000000000..4463ca82db1
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/writable_directory_spec.rb
@@ -0,0 +1,125 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+require "tmpdir"
+require "socket" # for UNIXSocket
+
+describe LogStash::Setting::WritableDirectory do
+  let(:mode_rx) { 0555 }
+  # linux is 108, Macos is 104, so use a safe value
+  # Stud::Temporary.pathname, will exceed that size without adding anything
+  let(:parent) { File.join(Dir.tmpdir, Time.now.to_f.to_s) }
+  let(:path) { File.join(parent, "fancy") }
+
+  before { Dir.mkdir(parent) }
+  after { Dir.exist?(path) && Dir.unlink(path) rescue nil }
+  after { Dir.unlink(parent) }
+
+  shared_examples "failure" do
+    before { subject.set(path) }
+    it "should fail" do
+      expect { subject.validate_value }.to raise_error
+    end
+  end
+
+  subject do
+    # Create a new WritableDirectory setting with no default value strict
+    # disabled.
+    described_class.new("fancy.path", "", false)
+  end
+
+  describe "#value" do
+    before { subject.set(path) }
+
+    context "when the directory is missing" do
+
+      context "and the parent is writable" do
+        after { 
+          Dir.unlink(path) 
+        }
+        it "creates the directory" do
+          subject.value # need to invoke `#value` to make it do the work.
+          expect(::File.directory?(path)).to be_truthy
+        end
+      end
+
+      context "and the directory cannot be created" do
+        before { File.chmod(mode_rx, parent) }
+        it "should fail" do
+          expect { subject.value }.to raise_error
+        end
+      end
+    end
+  end
+
+  describe "#set and #validate_value" do
+    context "when the directory exists" do
+      before { Dir.mkdir(path) }
+      after { Dir.unlink(path) }
+
+      context "and is writable" do
+        before { subject.set(path) }
+        # assume this spec already created a directory that's writable... fair? :)
+        it "should return true" do
+          expect(subject.validate_value).to be_truthy
+        end
+      end
+
+      context "but is not writable" do
+        before { File.chmod(0, path) }
+        it_behaves_like "failure"
+      end
+    end
+
+    context "when the path exists" do
+      after { File.unlink(path) }
+
+      context "but is a file" do
+        before { File.new(path, "w").close }
+        it_behaves_like "failure"
+      end
+
+      context "but is a socket" do
+        let(:socket) { UNIXServer.new(path) }
+        before { socket } # realize `socket` value
+        after { socket.close }
+        it_behaves_like "failure"
+      end
+
+      context "but is a symlink" do
+        before { File::symlink("whatever", path) }
+        it_behaves_like "failure"
+      end
+    end
+
+    context "when the directory is missing" do
+      # Create a path with at least one subdirectory we can try to fiddle with permissions
+
+      context "but can be created" do
+        before do
+          # If the path doesn't exist, we want to try creating it, so let's be
+          # extra careful and make sure the path doesn't exist yet.
+          expect(File.directory?(path)).to be_falsey
+          subject.set(path)
+        end
+
+        after do
+          Dir.unlink(path)
+        end
+
+        it "should return true" do
+          expect(subject.validate_value).to be_truthy
+        end
+      end
+
+      context "and cannot be created" do
+        before do
+          # Remove write permission on the parent
+          File.chmod(mode_rx, parent)
+        end
+
+        it_behaves_like "failure"
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings_spec.rb b/logstash-core/spec/logstash/settings_spec.rb
new file mode 100644
index 00000000000..c759dfdff43
--- /dev/null
+++ b/logstash-core/spec/logstash/settings_spec.rb
@@ -0,0 +1,149 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+require "fileutils"
+
+describe LogStash::Settings do
+  let(:numeric_setting_name) { "number" }
+  let(:numeric_setting) { LogStash::Setting.new(numeric_setting_name, Numeric, 1) }
+  describe "#register" do
+    context "if setting has already been registered" do
+      before :each do
+        subject.register(numeric_setting)
+      end
+      it "should raise an exception" do
+        expect { subject.register(numeric_setting) }.to raise_error
+      end
+    end
+    context "if setting hasn't been registered" do
+      it "should not raise an exception" do
+        expect { subject.register(numeric_setting) }.to_not raise_error
+      end
+    end
+  end
+  describe "#get_setting" do
+    context "if setting has been registered" do
+      before :each do
+        subject.register(numeric_setting)
+      end
+      it "should return the setting" do
+        expect(subject.get_setting(numeric_setting_name)).to eq(numeric_setting)
+      end
+    end
+    context "if setting hasn't been registered" do
+      it "should raise an exception" do
+        expect { subject.get_setting(numeric_setting_name) }.to raise_error
+      end
+    end
+  end
+  describe "#get_subset" do
+    let(:numeric_setting_1) { LogStash::Setting.new("num.1", Numeric, 1) }
+    let(:numeric_setting_2) { LogStash::Setting.new("num.2", Numeric, 2) }
+    let(:numeric_setting_3) { LogStash::Setting.new("num.3", Numeric, 3) }
+    let(:string_setting_1) { LogStash::Setting.new("string.1", String, "hello") }
+    before :each do
+      subject.register(numeric_setting_1)
+      subject.register(numeric_setting_2)
+      subject.register(numeric_setting_3)
+      subject.register(string_setting_1)
+    end
+
+    it "supports regex" do
+      expect(subject.get_subset(/num/).get_setting("num.3")).to eq(numeric_setting_3)
+      expect { subject.get_subset(/num/).get_setting("string.1") }.to raise_error
+    end
+
+    it "returns a copy of settings" do
+      subset = subject.get_subset(/num/)
+      subset.set("num.2", 1000)
+      expect(subject.get("num.2")).to eq(2)
+      expect(subset.get("num.2")).to eq(1000)
+    end
+  end
+
+  describe "#validate_all" do
+    subject { described_class.new }
+    let(:numeric_setting_name) { "example" }
+    let(:numeric_setting) { LogStash::Setting.new(numeric_setting_name, Numeric, 1, false) }
+
+    before do
+      subject.register(numeric_setting)
+      subject.set_value(numeric_setting_name, value)
+    end
+
+    context "when any setting is invalid" do
+      let(:value) { "some string" }
+
+      it "should fail" do
+        expect { subject.validate_all }.to raise_error
+      end
+    end
+
+    context "when all settings are valid" do
+      let(:value) { 123 }
+
+      it "should succeed" do
+        expect { subject.validate_all }.not_to raise_error
+      end
+    end
+  end
+  
+  describe "post_process" do
+    subject(:settings) { described_class.new }
+    
+    before do
+      settings.on_post_process do
+        settings.set("baz", "bot")
+      end
+      settings.register(LogStash::Setting::String.new("foo", "bar"))
+      settings.register(LogStash::Setting::String.new("baz", "somedefault"))
+      settings.post_process
+    end
+    
+    it "should run the post process callbacks" do
+      expect(settings.get("baz")).to eq("bot")
+    end
+    
+    it "should preserve original settings" do
+      expect(settings.get("foo")).to eq("bar")
+    end
+  end
+
+  context "transient settings" do
+    subject do
+      settings = described_class.new
+      settings.register(LogStash::Setting::String.new("exist", "bonsoir"))
+      settings
+    end
+
+    let(:values) { { "do.not.exist.on.boot" => true, "exist" => "bonjour" } }
+    let(:yaml_path) do
+      p = Stud::Temporary.pathname
+      FileUtils.mkdir_p(p)
+
+      ::File.open(::File.join(p, "logstash.yml"), "w+") do |f|
+        f.write(YAML.dump(values))
+      end
+      p
+    end
+
+    it "allow to read yml file that contains unknown settings" do
+      expect { subject.from_yaml(yaml_path) }.not_to raise_error
+    end
+
+    context "when running #validate_all" do
+      it "merge and validate all the registered setting" do
+        subject.from_yaml(yaml_path)
+        subject.register(LogStash::Setting::Boolean.new("do.not.exist.on.boot", false))
+
+        expect { subject.validate_all }.not_to raise_error
+        expect(subject.get("do.not.exist.on.boot")).to be_truthy
+      end
+
+      it "raise an error when the settings doesn't exist" do
+        subject.from_yaml(yaml_path)
+        expect { subject.validate_all }.to raise_error(ArgumentError)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/shutdown_watcher_spec.rb b/logstash-core/spec/logstash/shutdown_watcher_spec.rb
index 118e126ea5d..2f934f59b77 100644
--- a/logstash-core/spec/logstash/shutdown_watcher_spec.rb
+++ b/logstash-core/spec/logstash/shutdown_watcher_spec.rb
@@ -3,8 +3,6 @@
 require "logstash/shutdown_watcher"
 
 describe LogStash::ShutdownWatcher do
-  let(:channel) { Cabin::Channel.new }
-
   let(:check_every) { 0.01 }
   let(:check_threshold) { 100 }
   subject { LogStash::ShutdownWatcher.new(pipeline, check_every) }
@@ -14,8 +12,6 @@
   report_count = 0
 
   before :each do
-    LogStash::ShutdownWatcher.logger = channel
-
     allow(pipeline).to receive(:reporter).and_return(reporter)
     allow(pipeline).to receive(:thread).and_return(Thread.current)
     allow(reporter).to receive(:snapshot).and_return(reporter_snapshot)
diff --git a/logstash-core-event-java/spec/timestamp_spec.rb b/logstash-core/spec/logstash/timestamp_spec.rb
similarity index 100%
rename from logstash-core-event-java/spec/timestamp_spec.rb
rename to logstash-core/spec/logstash/timestamp_spec.rb
diff --git a/logstash-core-event/spec/logstash/util/accessors_spec.rb b/logstash-core/spec/logstash/util/accessors_spec.rb
similarity index 100%
rename from logstash-core-event/spec/logstash/util/accessors_spec.rb
rename to logstash-core/spec/logstash/util/accessors_spec.rb
diff --git a/logstash-core/spec/logstash/util/byte_value_spec.rb b/logstash-core/spec/logstash/util/byte_value_spec.rb
new file mode 100644
index 00000000000..a18e4ff11b9
--- /dev/null
+++ b/logstash-core/spec/logstash/util/byte_value_spec.rb
@@ -0,0 +1,33 @@
+require "logstash/util/byte_value"
+require "flores/random"
+
+describe LogStash::Util::ByteValue do
+  let(:multipliers) do
+    {
+      "b" => 1,
+      "kb" => 1 << 10,
+      "mb" => 1 << 20,
+      "gb" => 1 << 30,
+      "tb" => 1 << 40,
+      "pb" => 1 << 50,
+    }
+  end
+
+  let(:number) { Flores::Random.number(0..100000000000) }
+  let(:unit) { Flores::Random.item(multipliers.keys) }
+  let(:text) { "#{number}#{unit}" }
+
+  describe "#parse" do
+    # Expect a whole-unit byte value. Fractions of a byte don't make sense here. :)
+    let(:expected) { (number * multipliers[unit]).to_i }
+    subject { described_class.parse(text) }
+
+    it "should return a Numeric" do
+      expect(subject).to be_a(Numeric)
+    end
+
+    it "should have an expected byte value" do
+      expect(subject).to be == expected
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/util/defaults_printer_spec.rb b/logstash-core/spec/logstash/util/defaults_printer_spec.rb
deleted file mode 100644
index b3f0576a3a9..00000000000
--- a/logstash-core/spec/logstash/util/defaults_printer_spec.rb
+++ /dev/null
@@ -1,50 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/util/defaults_printer"
-
-describe LogStash::Util::DefaultsPrinter do
-  shared_examples "a defaults printer" do
-    it 'the .print method returns a defaults description' do
-      expect(actual_block.call).to eq(expected)
-    end
-  end
-
-  let(:workers)  { 1 }
-  let(:expected) { "Settings: User set pipeline workers: #{workers}" }
-  let(:settings) { {} }
-
-  describe 'class methods API' do
-    let(:actual_block) do
-      -> {described_class.print(settings)}
-    end
-
-    context 'when the settings hash is empty' do
-      let(:expected) { "Settings: " }
-      it_behaves_like "a defaults printer"
-    end
-
-    context 'when the settings hash has content' do
-      let(:worker_queue) { 42 }
-      let(:settings) { {:pipeline_workers => workers} }
-      it_behaves_like "a defaults printer"
-    end
-  end
-
-  describe 'instance method API' do
-    let(:actual_block) do
-      -> {described_class.new(settings).print}
-    end
-
-    context 'when the settings hash is empty' do
-      let(:expected) { "Settings: " }
-      it_behaves_like "a defaults printer"
-    end
-
-    context 'when the settings hash has content' do
-      let(:workers) { 13 }
-      let(:settings) { {:pipeline_workers => workers} }
-
-      it_behaves_like "a defaults printer"
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/util/safe_uri_spec.rb b/logstash-core/spec/logstash/util/safe_uri_spec.rb
new file mode 100644
index 00000000000..b8e5e546a31
--- /dev/null
+++ b/logstash-core/spec/logstash/util/safe_uri_spec.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require "logstash/util/safe_uri"
+require "spec_helper"
+
+module LogStash module Util
+  describe SafeURI do
+    describe "#clone" do
+      subject { LogStash::Util::SafeURI.new("http://localhost:9200/uri?q=s") }
+      it "allows modifying uri parameters" do
+        cloned_safe_uri = subject.clone
+        cloned_safe_uri.path = "/cloned"
+        cloned_safe_uri.query = "a=b"
+        expect(subject.path).to eq("/uri")
+        expect(subject.query).to eq("q=s")
+        expect(cloned_safe_uri.path).to eq("/cloned")
+        expect(cloned_safe_uri.query).to eq("a=b")
+      end
+    end
+  end
+end end
diff --git a/logstash-core/spec/logstash/util/time_value_spec.rb b/logstash-core/spec/logstash/util/time_value_spec.rb
new file mode 100644
index 00000000000..bbd8d3efdd7
--- /dev/null
+++ b/logstash-core/spec/logstash/util/time_value_spec.rb
@@ -0,0 +1,59 @@
+# encoding: utf-8
+require "logstash/util/time_value"
+require "spec_helper"
+
+RSpec.shared_examples "coercion example" do |value, expected|
+  let(:value) { value }
+  let(:expected) { expected }
+  it 'coerces correctly' do
+    expect(LogStash::Util::TimeValue.from_value(value)).to eq(expected)
+  end
+end
+
+
+module LogStash module Util
+describe TimeValue do
+    it_behaves_like "coercion example", TimeValue.new(100, :hour), TimeValue.new(100, :hour)
+    it_behaves_like "coercion example", "18nanos", TimeValue.new(18, :nanosecond)
+    it_behaves_like "coercion example", "18micros", TimeValue.new(18, :microsecond)
+    it_behaves_like "coercion example", "18ms", TimeValue.new(18, :millisecond)
+    it_behaves_like "coercion example", "18s", TimeValue.new(18, :second)
+    it_behaves_like "coercion example", "18m", TimeValue.new(18, :minute)
+    it_behaves_like "coercion example", "18h", TimeValue.new(18, :hour)
+    it_behaves_like "coercion example", "18d", TimeValue.new(18, :day)
+
+    it "coerces with a space between the duration and the unit" do
+      expected = TimeValue.new(18, :hour)
+      actual = TimeValue.from_value("18      h")
+      expect(actual).to eq(expected)
+    end
+
+    it "fails to coerce non-ints" do
+      begin
+        a = TimeValue.from_value("f18 nanos")
+        fail "should not parse"
+      rescue ArgumentError => e
+        expect(e.message).to eq("invalid value for Integer(): \"f18\"")
+      end
+    end
+
+    it "fails to coerce invalid units" do
+      begin
+        a = TimeValue.from_value("18xyz")
+        fail "should not parse"
+      rescue ArgumentError => e
+        expect(e.message).to eq("invalid time unit: \"18xyz\"")
+      end
+    end
+
+    it "fails to coerce invalid value types" do
+      begin
+        a = TimeValue.from_value(32)
+        fail "should not parse"
+      rescue ArgumentError => e
+        expect(e.message).to eq("value is not a string: 32 [Fixnum]")
+      end
+    end
+end
+end
+end
diff --git a/logstash-core/spec/logstash/util/worker_threads_default_printer_spec.rb b/logstash-core/spec/logstash/util/worker_threads_default_printer_spec.rb
deleted file mode 100644
index 1842b4373ad..00000000000
--- a/logstash-core/spec/logstash/util/worker_threads_default_printer_spec.rb
+++ /dev/null
@@ -1,45 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/util/worker_threads_default_printer"
-
-describe LogStash::Util::WorkerThreadsDefaultPrinter do
-  let(:settings)  { {} }
-  let(:collector) { [] }
-
-  subject { described_class.new(settings) }
-
-  before { subject.visit(collector) }
-
-  describe "the #visit method" do
-    context 'when the settings hash is empty' do
-      it 'adds nothing to the collector' do
-        subject.visit(collector)
-        expect(collector).to eq([])
-      end
-    end
-
-    context 'when the settings hash has both user and default content' do
-      let(:settings) { {:pipeline_workers => 42, :default_pipeline_workers => 5} }
-
-      it 'adds two strings' do
-        expect(collector).to eq(["User set pipeline workers: 42", "Default pipeline workers: 5"])
-      end
-    end
-
-    context 'when the settings hash has only user content' do
-      let(:settings) { {:pipeline_workers => 42} }
-
-      it 'adds a string with user set pipeline workers' do
-        expect(collector.first).to eq("User set pipeline workers: 42")
-      end
-    end
-
-    context 'when the settings hash has only default content' do
-      let(:settings) { {:default_pipeline_workers => 5} }
-
-      it 'adds a string with default pipeline workers' do
-        expect(collector.first).to eq("Default pipeline workers: 5")
-      end
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
index 871952482aa..c24ed273a9d 100644
--- a/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
+++ b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
@@ -1,28 +1,143 @@
 # encoding: utf-8
 require "spec_helper"
 require "logstash/util/wrapped_synchronous_queue"
+require "logstash/instrument/collector"
 
 describe LogStash::Util::WrappedSynchronousQueue do
- context "#offer" do
-   context "queue is blocked" do
-     it "fails and give feedback" do
-       expect(subject.offer("Bonjour", 2)).to be_falsey
-     end
-   end
-
-   context "queue is not blocked" do
-     before do
-       @consumer = Thread.new { loop { subject.take } }
-       sleep(0.1)
-     end
-
-     after do
-       @consumer.kill
-     end
-     
-     it "inserts successfully" do
-       expect(subject.offer("Bonjour", 20)).to be_truthy
-     end
-   end
- end
+  context "#offer" do
+    context "queue is blocked" do
+      it "fails and give feedback" do
+        expect(subject.offer("Bonjour", 2)).to be_falsey
+      end
+    end
+
+    context "queue is not blocked" do
+      before do
+        @consumer = Thread.new { loop { subject.take } }
+        sleep(0.1)
+      end
+
+      after do
+        @consumer.kill
+      end
+
+      it "inserts successfully" do
+        expect(subject.offer("Bonjour", 20)).to be_truthy
+      end
+    end
+  end
+
+  describe "queue clients" do
+    context "when requesting a write client" do
+      it "returns a client" do
+        expect(subject.write_client).to be_a(LogStash::Util::WrappedSynchronousQueue::WriteClient)
+      end
+    end
+
+    context "when requesting a read client" do
+      it "returns a client" do
+        expect(subject.read_client).to be_a(LogStash::Util::WrappedSynchronousQueue::ReadClient)
+      end
+    end
+
+    class DummyQueue < Array
+      def take() shift(); end
+      def poll(*) shift(); end
+    end
+
+    describe "WriteClient | ReadClient" do
+      let(:queue) { DummyQueue.new }
+      let(:write_client) { LogStash::Util::WrappedSynchronousQueue::WriteClient.new(queue)}
+      let(:read_client)  { LogStash::Util::WrappedSynchronousQueue::ReadClient.new(queue)}
+
+      context "when reading from the queue" do
+        let(:collector) { LogStash::Instrument::Collector.new }
+
+        before do
+          read_client.set_events_metric(LogStash::Instrument::Metric.new(collector).namespace(:events))
+          read_client.set_pipeline_metric(LogStash::Instrument::Metric.new(collector).namespace(:pipeline))
+        end
+
+        context "when the queue is empty" do
+          it "doesnt record the `duration_in_millis`" do
+            batch = read_client.read_batch
+            read_client.close_batch(batch)
+            store = collector.snapshot_metric.metric_store
+
+            expect(store.get_shallow(:events, :out).value).to eq(0)
+            expect(store.get_shallow(:events, :out)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:events, :filtered).value).to eq(0)
+            expect(store.get_shallow(:events, :filtered)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:events, :duration_in_millis).value).to eq(0)
+            expect(store.get_shallow(:events, :duration_in_millis)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:pipeline, :duration_in_millis).value).to eq(0)
+            expect(store.get_shallow(:pipeline, :duration_in_millis)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:pipeline, :out).value).to eq(0)
+            expect(store.get_shallow(:pipeline, :out)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:pipeline, :filtered).value).to eq(0)
+            expect(store.get_shallow(:pipeline, :filtered)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+          end
+        end
+
+        context "when we have item in the queue" do
+          it "records the `duration_in_millis`" do
+            batch = write_client.get_new_batch
+            5.times {|i| batch.push("value-#{i}")}
+            write_client.push_batch(batch)
+
+            read_batch = read_client.read_batch
+            sleep(0.1) # simulate some work for the `duration_in_millis`
+            # TODO: this interaction should be cleaned in an upcoming PR,
+            # This is what the current pipeline does.
+            read_client.add_filtered_metrics(read_batch)
+            read_client.add_output_metrics(read_batch)
+            read_client.close_batch(read_batch)
+            store = collector.snapshot_metric.metric_store
+
+            expect(store.get_shallow(:events, :out).value).to eq(5)
+            expect(store.get_shallow(:events, :filtered).value).to eq(5)
+            expect(store.get_shallow(:events, :duration_in_millis).value).to be > 0
+            expect(store.get_shallow(:pipeline, :duration_in_millis).value).to be > 0
+            expect(store.get_shallow(:pipeline, :out).value).to eq(5)
+            expect(store.get_shallow(:pipeline, :filtered).value).to eq(5)
+          end
+        end
+      end
+
+      context "when writing to the queue" do
+        before :each do
+          read_client.set_events_metric(LogStash::Instrument::NamespacedNullMetric.new([], :null))
+          read_client.set_pipeline_metric(LogStash::Instrument::NamespacedNullMetric.new([], :null))
+        end
+
+        it "appends batches to the queue" do
+          batch = write_client.get_new_batch
+          5.times {|i| batch.push(LogStash::Event.new({"message" => "value-#{i}"}))}
+          write_client.push_batch(batch)
+          read_batch = read_client.read_batch
+          expect(read_batch.size).to eq(5)
+          i = 0
+          read_batch.each do |data|
+            expect(data.get("message")).to eq("value-#{i}")
+            # read_batch.cancel("value-#{i}") if i > 2     # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+            data.cancel if i > 2
+            read_batch.merge(LogStash::Event.new({"message" => "generated-#{i}"})) if i > 2
+            i += 1
+          end
+          # expect(read_batch.cancelled_size).to eq(2) # disabled for https://github.com/elastic/logstash/issues/6055
+          i = 0
+          read_batch.each do |data|
+            expect(data.get("message")).to eq("value-#{i}") if i < 3
+            expect(data.get("message")).to eq("generated-#{i}") if i > 2
+            i += 1
+          end
+        end
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/util_spec.rb b/logstash-core/spec/logstash/util_spec.rb
index 4cd56139fbf..df974775a75 100644
--- a/logstash-core/spec/logstash/util_spec.rb
+++ b/logstash-core/spec/logstash/util_spec.rb
@@ -37,7 +37,7 @@ class TestKlass
       expect(LogStash::Util.stringify_symbols([1, :a])).to eq([1, "a"])
     end
 
-    it "should convert innner array symbol values to strings" do
+    it "should convert inner array symbol values to strings" do
       expect(LogStash::Util.stringify_symbols({:a => [1, :b]})).to eq({"a" => [1, "b"]})
       expect(LogStash::Util.stringify_symbols([:a, [1, :b]])).to eq(["a", [1, "b"]])
     end
diff --git a/logstash-core/spec/logstash/webserver_spec.rb b/logstash-core/spec/logstash/webserver_spec.rb
new file mode 100644
index 00000000000..cbf14542d1f
--- /dev/null
+++ b/logstash-core/spec/logstash/webserver_spec.rb
@@ -0,0 +1,150 @@
+# encoding: utf-8
+# require "logstash/json"
+require "logstash/webserver"
+require_relative "../support/helpers"
+require "socket"
+require "spec_helper"
+require "open-uri"
+require "webmock/rspec"
+
+def block_ports(range)
+  servers = []
+
+  range.each do |port|
+    begin
+      server = TCPServer.new("localhost", port)
+      Thread.new do
+        client = server.accept rescue nil
+      end
+      servers << server
+    rescue => e
+      # The port can already be taken
+    end
+  end
+
+  sleep(1)
+  servers
+end
+
+def free_ports(servers)
+  servers.each do |t|
+    t.close rescue nil # the threads are blocked just kill
+  end
+end
+
+describe LogStash::WebServer do
+  before :all do
+    @abort = Thread.abort_on_exception
+    Thread.abort_on_exception = true
+    WebMock.allow_net_connect!
+  end
+
+  after :all do
+    Thread.abort_on_exception = @abort
+  end
+
+  let(:logger) { LogStash::Logging::Logger.new("testing") }
+  let(:agent) { OpenStruct.new({:webserver => webserver, :http_address => "127.0.0.1", :id => "myid", :name => "myname"}) }
+  let(:webserver) { OpenStruct.new({}) }
+
+  subject { LogStash::WebServer.new(logger,
+                                    agent,
+                                    { :http_host => "localhost", :http_ports => port_range })}
+
+  let(:port_range) { 10000..10010 }
+
+  context "when an exception occur in the server thread" do
+    let(:spy_output) { spy("stderr").as_null_object }
+
+    it "should not log to STDERR" do
+      backup_stderr = STDERR
+      backup_stdout = STDOUT
+
+      # We are redefining constants, so lets silence the warning
+      silence_warnings do
+        STDOUT = STDERR = spy_output
+      end
+
+      expect(spy_output).not_to receive(:puts).with(any_args)
+      expect(spy_output).not_to receive(:write).with(any_args)
+
+      # This trigger an infinite loop in the reactor
+      expect(IO).to receive(:select).and_raise(IOError).at_least(:once)
+
+      t = Thread.new do
+        subject.run
+      end
+
+      sleep(1)
+
+      # We cannot use stop here, since the code is stuck in an infinite loop
+      t.kill rescue nil
+
+      silence_warnings do
+        STDERR = backup_stderr
+        STDOUT = backup_stdout
+      end
+    end
+  end
+
+  context "when the port is already in use and a range is provided" do
+    after(:each) { free_ports(@servers) }
+
+    context "when we have available ports" do
+      before(:each) do
+        @servers = block_ports(10000..10005)
+      end
+
+      it "successfully find an available port" do
+        t = Thread.new do
+          subject.run
+        end
+
+        sleep(1)
+
+        response = open("http://localhost:10006").read
+        expect { LogStash::Json.load(response) }.not_to raise_error
+        expect(subject.address).to eq("localhost:10006")
+
+        subject.stop
+        t.kill rescue nil
+      end
+    end
+
+    context "when all the ports are taken" do
+      before(:each) do
+        @servers = block_ports(port_range)
+      end
+
+      it "raise an exception" do
+        expect { subject.run }.to raise_error(Errno::EADDRINUSE, /Logstash tried to bind to port range/)
+      end
+    end
+  end
+end
+
+describe LogStash::IOWrappedLogger do
+  let(:logger)  { spy("logger") }
+  let(:message) { "foobar" }
+
+  subject { described_class.new(logger) }
+
+  it "responds to puts" do
+    subject.puts(message)
+    expect(logger).to have_received(:debug).with(message)
+  end
+
+  it "responds to write" do
+    subject.write(message)
+    expect(logger).to have_received(:debug).with(message)
+  end
+
+  it "responds to <<" do
+    subject << message
+    expect(logger).to have_received(:debug).with(message)
+  end
+
+  it "responds to sync=(v)" do
+    expect{ subject.sync = true }.not_to raise_error
+  end
+end
diff --git a/logstash-core/spec/static/i18n_spec.rb b/logstash-core/spec/static/i18n_spec.rb
new file mode 100644
index 00000000000..b2cd76377d2
--- /dev/null
+++ b/logstash-core/spec/static/i18n_spec.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require "spec_helper"
+require "i18n"
+
+I18N_T_REGEX = Regexp.new('I18n.t.+?"(.+?)"')
+
+describe I18n do
+  context "when using en.yml" do
+    glob_path = File.join(LogStash::Environment::LOGSTASH_HOME, "logstash-*", "lib", "**", "*.rb")
+
+    Dir.glob(glob_path).each do |file_name|
+
+      context "in file \"#{file_name}\"" do
+        File.foreach(file_name) do |line|
+          next unless (match = line.match(I18N_T_REGEX))
+          line = $INPUT_LINE_NUMBER
+          key = match[1]
+          it "in line #{line} the \"#{key}\" key should exist" do
+            expect(I18n.exists?(key)).to be_truthy
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/support/helpers.rb b/logstash-core/spec/support/helpers.rb
new file mode 100644
index 00000000000..6d55ab80339
--- /dev/null
+++ b/logstash-core/spec/support/helpers.rb
@@ -0,0 +1,16 @@
+# encoding: utf-8
+def silence_warnings
+  warn_level = $VERBOSE
+  $VERBOSE = nil
+  yield
+ensure
+  $VERBOSE = warn_level
+end
+
+def clear_data_dir
+    data_path = agent_settings.get("path.data")
+    Dir.foreach(data_path) do |f|
+    next if f == "." || f == ".." || f == ".gitkeep"
+    FileUtils.rm_rf(File.join(data_path, f))
+  end
+end
diff --git a/logstash-core/spec/support/mocks_classes.rb b/logstash-core/spec/support/mocks_classes.rb
new file mode 100644
index 00000000000..a69b89bc821
--- /dev/null
+++ b/logstash-core/spec/support/mocks_classes.rb
@@ -0,0 +1,91 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/inputs/base"
+require "thread"
+
+module LogStash
+  module Inputs
+    class DummyInput < LogStash::Inputs::Base
+      config_name "dummyinput"
+
+      def run(queue)
+        # noop
+      end
+    end
+  end
+  module Outputs
+    class DummyOutput < LogStash::Outputs::Base
+      config_name "dummyoutput"
+      milestone 2
+
+      attr_reader :num_closes, :events
+
+      def initialize(params={})
+        super
+        @num_closes = 0
+        @events = []
+      end
+
+      def register
+      end
+
+      def receive(event)
+        @events << event
+      end
+
+      def close
+        @num_closes += 1
+      end
+    end
+
+    class DummyOutputWithEventsArray < LogStash::Outputs::Base
+      config_name "dummyoutput2"
+      milestone 2
+
+      attr_reader :events
+
+      def initialize(params={})
+        super
+        @events = []
+      end
+
+      def register
+      end
+
+      def receive(event)
+        @events << event
+      end
+
+      def close
+      end
+    end
+
+    class DroppingDummyOutput < LogStash::Outputs::Base
+      config_name "droppingdummyoutput"
+      milestone 2
+
+      attr_reader :num_closes
+
+      def initialize(params={})
+        super
+        @num_closes = 0
+        @events_received = Concurrent::AtomicFixnum.new(0)
+      end
+
+      def register
+      end
+
+      def receive(event)
+        @events_received.increment
+      end
+
+      def events_received
+        @events_received.value
+      end
+
+      def close
+        @num_closes = 1
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/support/shared_examples.rb b/logstash-core/spec/support/shared_examples.rb
new file mode 100644
index 00000000000..0218bebb53c
--- /dev/null
+++ b/logstash-core/spec/support/shared_examples.rb
@@ -0,0 +1,108 @@
+# encoding: utf-8
+# Define the common operation that both the `NullMetric` class and the Namespaced class should answer.
+shared_examples "metrics commons operations" do
+  let(:key) { "galaxy" }
+
+  describe "#increment" do
+    it "allows to increment a key with no amount" do
+      expect { subject.increment(key, 100) }.not_to raise_error
+    end
+
+    it "allow to increment a key" do
+      expect { subject.increment(key) }.not_to raise_error
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.increment("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.increment(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#decrement" do
+    it "allows to decrement a key with no amount" do
+      expect { subject.decrement(key, 100) }.not_to raise_error
+    end
+
+    it "allow to decrement a key" do
+      expect { subject.decrement(key) }.not_to raise_error
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.decrement("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.decrement(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#gauge" do
+    it "allows to set a value" do
+      expect { subject.gauge(key, "pluto") }.not_to raise_error
+    end
+
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.gauge("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.gauge(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#report_time" do
+    it "allow to record time" do
+      expect { subject.report_time(key, 1000) }.not_to raise_error
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.report_time("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.report_time(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#time" do
+    it "allow to record time with a block given" do
+      expect do
+        subject.time(key) { 1+1 }
+      end.not_to raise_error
+    end
+
+    it "returns the value of the block without recording any metrics" do
+      expect(subject.time(:execution_time) { "hello" }).to eq("hello")
+    end
+
+    it "return a TimedExecution" do
+      execution = subject.time(:do_something)
+      expect { execution.stop }.not_to raise_error
+    end
+
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.time("") {} }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.time(nil) {} }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+end
+
+shared_examples "not found" do
+  it "should return a 404 to unknown request" do
+    do_request { get "/i_want_to_believe-#{Time.now.to_i}" }
+    expect(last_response.content_type).to eq("application/json")
+    expect(last_response).not_to be_ok
+    expect(last_response.status).to eq(404)
+    expect(LogStash::Json.load(last_response.body)).to include("status" => 404)
+    expect(LogStash::Json.load(last_response.body)["path"]).not_to be_nil
+  end
+end
+
diff --git a/logstash-core/src/main/java/JrubyAckedBatchExtService.java b/logstash-core/src/main/java/JrubyAckedBatchExtService.java
new file mode 100644
index 00000000000..f31aa6089c1
--- /dev/null
+++ b/logstash-core/src/main/java/JrubyAckedBatchExtService.java
@@ -0,0 +1,14 @@
+import org.jruby.Ruby;
+import org.jruby.runtime.load.BasicLibraryService;
+import org.logstash.ackedqueue.ext.JrubyAckedBatchExtLibrary;
+
+import java.io.IOException;
+
+public class JrubyAckedBatchExtService implements BasicLibraryService {
+    public boolean basicLoad(final Ruby runtime)
+            throws IOException
+    {
+        new JrubyAckedBatchExtLibrary().load(runtime, false);
+        return true;
+    }
+}
diff --git a/logstash-core/src/main/java/JrubyAckedQueueExtService.java b/logstash-core/src/main/java/JrubyAckedQueueExtService.java
new file mode 100644
index 00000000000..8b349646e2d
--- /dev/null
+++ b/logstash-core/src/main/java/JrubyAckedQueueExtService.java
@@ -0,0 +1,16 @@
+import org.jruby.Ruby;
+import org.jruby.runtime.load.BasicLibraryService;
+import org.logstash.ackedqueue.ext.JrubyAckedQueueExtLibrary;
+import org.logstash.ackedqueue.ext.JrubyAckedQueueMemoryExtLibrary;
+
+import java.io.IOException;
+
+public class JrubyAckedQueueExtService implements BasicLibraryService {
+    public boolean basicLoad(final Ruby runtime)
+            throws IOException
+    {
+        new JrubyAckedQueueExtLibrary().load(runtime, false);
+        new JrubyAckedQueueMemoryExtLibrary().load(runtime, false);
+        return true;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/JrubyEventExtService.java b/logstash-core/src/main/java/JrubyEventExtService.java
similarity index 88%
rename from logstash-core-event-java/src/main/java/JrubyEventExtService.java
rename to logstash-core/src/main/java/JrubyEventExtService.java
index 306a45f3971..46d54f13c8a 100644
--- a/logstash-core-event-java/src/main/java/JrubyEventExtService.java
+++ b/logstash-core/src/main/java/JrubyEventExtService.java
@@ -1,4 +1,4 @@
-import com.logstash.ext.JrubyEventExtLibrary;
+import org.logstash.ext.JrubyEventExtLibrary;
 import org.jruby.Ruby;
 import org.jruby.runtime.load.BasicLibraryService;
 
diff --git a/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java b/logstash-core/src/main/java/JrubyTimestampExtService.java
similarity index 78%
rename from logstash-core-event-java/src/main/java/JrubyTimestampExtService.java
rename to logstash-core/src/main/java/JrubyTimestampExtService.java
index 32d8eb2bf98..f11e38783e0 100644
--- a/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java
+++ b/logstash-core/src/main/java/JrubyTimestampExtService.java
@@ -1,5 +1,4 @@
-import com.logstash.ext.JrubyEventExtLibrary;
-import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.logstash.ext.JrubyTimestampExtLibrary;
 import org.jruby.Ruby;
 import org.jruby.runtime.load.BasicLibraryService;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Accessors.java b/logstash-core/src/main/java/org/logstash/Accessors.java
similarity index 54%
rename from logstash-core-event-java/src/main/java/com/logstash/Accessors.java
rename to logstash-core/src/main/java/org/logstash/Accessors.java
index 4c1e597ecdc..ec65795c334 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Accessors.java
+++ b/logstash-core/src/main/java/org/logstash/Accessors.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.HashMap;
 import java.util.Map;
@@ -33,13 +33,15 @@ public Object del(String reference) {
             if (target instanceof Map) {
                 return ((Map<String, Object>) target).remove(field.getKey());
             } else if (target instanceof List) {
-                int i = Integer.parseInt(field.getKey());
-                if (i < 0 || i >= ((List) target).size()) {
+                try {
+                    int i = Integer.parseInt(field.getKey());
+                    int offset = listIndex(i, ((List) target).size());
+                    return ((List)target).remove(offset);
+                } catch (IndexOutOfBoundsException|NumberFormatException e) {
                     return null;
                 }
-                return ((List<Object>) target).remove(i);
             } else {
-                throw new ClassCastException("expecting List or Map");
+                throw newCollectionException(target);
             }
         }
         return null;
@@ -50,8 +52,13 @@ public boolean includes(String reference) {
         Object target = findTarget(field);
         if (target instanceof Map && foundInMap((Map<String, Object>) target, field.getKey())) {
             return true;
-        } else if (target instanceof List && foundInList((List<Object>) target, Integer.parseInt(field.getKey()))) {
-            return true;
+        } else if (target instanceof List) {
+            try {
+                int i = Integer.parseInt(field.getKey());
+                return (foundInList((List<Object>) target, i) ? true : false);
+            } catch (NumberFormatException e) {
+                return false;
+            }
         } else {
             return false;
         }
@@ -67,7 +74,7 @@ private Object findTarget(FieldReference field) {
         target = this.data;
         for (String key : field.getPath()) {
             target = fetch(target, key);
-            if (target == null) {
+            if (! isCollection(target)) {
                 return null;
             }
         }
@@ -80,9 +87,13 @@ private Object findTarget(FieldReference field) {
     private Object findCreateTarget(FieldReference field) {
         Object target;
 
-        if ((target = this.lut.get(field.getReference())) != null) {
-            return target;
-        }
+        // flush the @lut to prevent stale cached fieldref which may point to an old target
+        // which was overwritten with a new value. for example, if "[a][b]" is cached and we
+        // set a new value for "[a]" then reading again "[a][b]" would point in a stale target.
+        // flushing the complete @lut is suboptimal, but a hierarchical lut would be required
+        // to be able to invalidate fieldrefs from a common root.
+        // see https://github.com/elastic/logstash/pull/5132
+        this.lut.clear();
 
         target = this.data;
         for (String key : field.getPath()) {
@@ -92,13 +103,15 @@ private Object findCreateTarget(FieldReference field) {
                 if (target instanceof Map) {
                     ((Map<String, Object>)target).put(key, result);
                 } else if (target instanceof List) {
-                    int i = Integer.parseInt(key);
-                    // TODO: what about index out of bound?
-                    ((List<Object>)target).set(i, result);
-                } else if (target == null) {
-                    // do nothing
-                } else {
-                    throw new ClassCastException("expecting List or Map");
+                    try {
+                        int i = Integer.parseInt(key);
+                        // TODO: what about index out of bound?
+                        ((List<Object>)target).set(i, result);
+                    } catch (NumberFormatException e) {
+                        continue;
+                    }
+                } else if (target != null) {
+                    throw newCollectionException(target);
                 }
             }
             target = result;
@@ -110,10 +123,13 @@ private Object findCreateTarget(FieldReference field) {
     }
 
     private boolean foundInList(List<Object> target, int index) {
-        if (index < 0 || index >= target.size()) {
+        try {
+            int offset = listIndex(index, target.size());
+            return target.get(offset) != null;
+        } catch (IndexOutOfBoundsException e) {
             return false;
         }
-        return target.get(index) != null;
+
     }
 
     private boolean foundInMap(Map<String, Object> target, String key) {
@@ -125,16 +141,16 @@ private Object fetch(Object target, String key) {
             Object result = ((Map<String, Object>) target).get(key);
             return result;
         } else if (target instanceof List) {
-            int i = Integer.parseInt(key);
-            if (i < 0 || i >= ((List) target).size()) {
+            try {
+                int offset = listIndex(Integer.parseInt(key), ((List) target).size());
+                return ((List<Object>) target).get(offset);
+            } catch (IndexOutOfBoundsException|NumberFormatException e) {
                 return null;
             }
-            Object result = ((List<Object>) target).get(i);
-            return result;
         } else if (target == null) {
             return null;
-        } {
-            throw new ClassCastException("expecting List or Map");
+        } else {
+            throw newCollectionException(target);
         }
     }
 
@@ -142,23 +158,59 @@ private Object store(Object target, String key, Object value) {
         if (target instanceof Map) {
             ((Map<String, Object>) target).put(key, value);
         } else if (target instanceof List) {
-            int i = Integer.parseInt(key);
+            int i;
+            try {
+                i = Integer.parseInt(key);
+            } catch (NumberFormatException e) {
+                return null;
+            }
             int size = ((List<Object>) target).size();
             if (i >= size) {
                 // grow array by adding trailing null items
                 // this strategy reflects legacy Ruby impl behaviour and is backed by specs
-                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbritary big numbers
+                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers
                 // TODO: (colin) should be guard against this?
                 for (int j = size; j < i; j++) {
                     ((List<Object>) target).add(null);
                 }
                 ((List<Object>) target).add(value);
             } else {
-                ((List<Object>) target).set(i, value);
+                int offset = listIndex(i, ((List) target).size());
+                ((List<Object>) target).set(offset, value);
             }
         } else {
-            throw new ClassCastException("expecting List or Map");
+            throw newCollectionException(target);
         }
         return value;
     }
+
+    private boolean isCollection(Object target) {
+        if (target == null) {
+            return false;
+        }
+        return (target instanceof Map || target instanceof List);
+    }
+
+    private ClassCastException newCollectionException(Object target) {
+        return new ClassCastException("expecting List or Map, found "  + target.getClass());
+    }
+
+    /* 
+     * Returns a positive integer offset for a list of known size.
+     *
+     * @param i if positive, and offset from the start of the list. If negative, the offset from the end of the list, where -1 means the last element.
+     * @param size the size of the list.
+     * @return the positive integer offset for the list given by index i.
+     */
+    public static int listIndex(int i, int size) {
+        if (i >= size || i < -size) {
+            throw new IndexOutOfBoundsException("Index " + i + " is out of bounds for a list with size " + size);
+        }
+
+        if (i < 0) { // Offset from the end of the array.
+            return size + i;
+        } else {
+            return i;
+        }
+    }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Cloner.java b/logstash-core/src/main/java/org/logstash/Cloner.java
similarity index 88%
rename from logstash-core-event-java/src/main/java/com/logstash/Cloner.java
rename to logstash-core/src/main/java/org/logstash/Cloner.java
index 4823f10726a..d2588064d5c 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Cloner.java
+++ b/logstash-core/src/main/java/org/logstash/Cloner.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.*;
 
@@ -24,6 +24,8 @@ private static <E> List<E> deepList(final List<E> list) {
             clone = new LinkedList<E>();
         } else if (list instanceof ArrayList<?>) {
             clone = new ArrayList<E>();
+        } else if (list instanceof ConvertedList<?>) {
+            clone = new ArrayList<E>();
         } else {
             throw new ClassCastException("unexpected List type " + list.getClass());
         }
@@ -43,6 +45,8 @@ private static <K, V> Map<K, V> deepMap(final Map<K, V> map) {
             clone = new TreeMap<K, V>();
         } else if (map instanceof HashMap<?, ?>) {
             clone = new HashMap<K, V>();
+        } else if (map instanceof ConvertedMap<?, ?>) {
+            clone = new HashMap<K, V>();
         } else {
             throw new ClassCastException("unexpected Map type " + map.getClass());
         }
diff --git a/logstash-core/src/main/java/org/logstash/ConvertedList.java b/logstash-core/src/main/java/org/logstash/ConvertedList.java
new file mode 100644
index 00000000000..d4cde257288
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ConvertedList.java
@@ -0,0 +1,224 @@
+package org.logstash;
+
+import org.jruby.RubyArray;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.List;
+import java.util.ListIterator;
+import java.util.Spliterator;
+import java.util.function.Consumer;
+import java.util.function.Predicate;
+import java.util.function.UnaryOperator;
+import java.util.stream.Stream;
+
+import static org.logstash.Valuefier.convert;
+
+public class ConvertedList<T> implements List<T>, Collection<T>, Iterable<T> {
+    private final List<T> delegate;
+
+    public ConvertedList(List<T> delegate) {
+        this.delegate = delegate;
+    }
+    public ConvertedList() {
+        this.delegate = new ArrayList<>();
+    }
+
+    public static ConvertedList<Object> newFromList(List<Object> list) {
+        ConvertedList<Object> array = new ConvertedList<>();
+
+        for (Object item : list) {
+            array.add(convert(item));
+        }
+        return array;
+    }
+
+    public static ConvertedList<Object> newFromRubyArray(RubyArray a) {
+        final ConvertedList<Object> result = new ConvertedList<>();
+
+        for (IRubyObject o : a.toJavaArray()) {
+            result.add(convert(o));
+        }
+        return result;
+    }
+
+    public Object unconvert() {
+        final ArrayList<Object> result = new ArrayList<>();
+        for (Object obj : delegate) {
+            result.add(Javafier.deep(obj));
+        }
+        return result;
+    }
+
+    // delegate methods
+    @Override
+    public int size() {
+        return delegate.size();
+    }
+
+    @Override
+    public boolean isEmpty() {
+        return delegate.isEmpty();
+    }
+
+    @Override
+    public boolean contains(Object o) {
+        return delegate.contains(o);
+    }
+
+    @Override
+    public Iterator<T> iterator() {
+        return delegate.iterator();
+    }
+
+    @Override
+    public Object[] toArray() {
+        return delegate.toArray();
+    }
+
+    @Override
+    public <T1> T1[] toArray(T1[] a) {
+        return delegate.toArray(a);
+    }
+
+    @Override
+    public boolean add(T t) {
+        return delegate.add(t);
+    }
+
+    @Override
+    public boolean remove(Object o) {
+        return delegate.remove(o);
+    }
+
+    @Override
+    public boolean containsAll(Collection<?> c) {
+        return delegate.containsAll(c);
+    }
+
+    @Override
+    public boolean addAll(Collection<? extends T> c) {
+        return delegate.addAll(c);
+    }
+
+    @Override
+    public boolean addAll(int index, Collection<? extends T> c) {
+        return delegate.addAll(index, c);
+    }
+
+    @Override
+    public boolean removeAll(Collection<?> c) {
+        return delegate.removeAll(c);
+    }
+
+    @Override
+    public boolean retainAll(Collection<?> c) {
+        return delegate.retainAll(c);
+    }
+
+    @Override
+    public void replaceAll(UnaryOperator<T> operator) {
+        delegate.replaceAll(operator);
+    }
+
+    @Override
+    public void sort(Comparator<? super T> c) {
+        delegate.sort(c);
+    }
+
+    @Override
+    public void clear() {
+        delegate.clear();
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        return delegate.equals(o);
+    }
+
+    @Override
+    public int hashCode() {
+        return delegate.hashCode();
+    }
+
+    @Override
+    public T get(int index) {
+        return delegate.get(index);
+    }
+
+    @Override
+    public T set(int index, T element) {
+        return delegate.set(index, element);
+    }
+
+    @Override
+    public void add(int index, T element) {
+        delegate.add(index, element);
+    }
+
+    @Override
+    public T remove(int index) {
+        return delegate.remove(index);
+    }
+
+    @Override
+    public int indexOf(Object o) {
+        return delegate.indexOf(o);
+    }
+
+    @Override
+    public int lastIndexOf(Object o) {
+        return delegate.lastIndexOf(o);
+    }
+
+    @Override
+    public ListIterator<T> listIterator() {
+        return delegate.listIterator();
+    }
+
+    @Override
+    public ListIterator<T> listIterator(int index) {
+        return delegate.listIterator(index);
+    }
+
+    @Override
+    public List<T> subList(int fromIndex, int toIndex) {
+        return delegate.subList(fromIndex, toIndex);
+    }
+
+    @Override
+    public Spliterator<T> spliterator() {
+        return delegate.spliterator();
+    }
+
+    @Override
+    public String toString() {
+        final StringBuffer sb = new StringBuffer("ConvertedList{");
+        sb.append("delegate=").append(delegate.toString());
+        sb.append('}');
+        return sb.toString();
+    }
+
+    @Override
+    public boolean removeIf(Predicate<? super T> filter) {
+        return delegate.removeIf(filter);
+    }
+
+    @Override
+    public Stream<T> stream() {
+        return delegate.stream();
+    }
+
+    @Override
+    public Stream<T> parallelStream() {
+        return delegate.parallelStream();
+    }
+
+    @Override
+    public void forEach(Consumer<? super T> action) {
+        delegate.forEach(action);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ConvertedMap.java b/logstash-core/src/main/java/org/logstash/ConvertedMap.java
new file mode 100644
index 00000000000..ea25ba6aba3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ConvertedMap.java
@@ -0,0 +1,182 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValues;
+import org.jruby.RubyHash;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.function.BiConsumer;
+import java.util.function.BiFunction;
+import java.util.function.Function;
+
+public class ConvertedMap<K, V> implements Map<K, V> {
+
+    private final Map<K, V> delegate;
+
+    public ConvertedMap(Map<K, V> delegate) {
+        this.delegate = delegate;
+    }
+
+    public ConvertedMap() {
+        this.delegate = new HashMap<>();
+    }
+
+    public static ConvertedMap<String, Object> newFromMap(Map<String, Object> o) {
+        ConvertedMap<String, Object> cm = new ConvertedMap<>();
+        for (Map.Entry<String, Object> entry : o.entrySet()) {
+            String k = String.valueOf(BiValues.newBiValue(entry.getKey()).javaValue());
+            cm.put(k, Valuefier.convert(entry.getValue()));
+        }
+        return cm;
+    }
+
+    public static ConvertedMap<String, Object> newFromRubyHash(RubyHash o) {
+        final ConvertedMap<String, Object> result = new ConvertedMap<>();
+
+        o.visitAll(new RubyHash.Visitor() {
+            @Override
+            public void visit(IRubyObject key, IRubyObject value) {
+                String k = String.valueOf(BiValues.newBiValue(key).javaValue()) ;
+                result.put(k, Valuefier.convert(value));
+            }
+        });
+        return result;
+    }
+
+    public Object unconvert() {
+        final HashMap<K, V> result = new HashMap<>();
+        for (Map.Entry<K, V> entry : entrySet()) {
+            result.put(entry.getKey(), (V) Javafier.deep(entry.getValue()));
+        }
+        return result;
+    }
+
+    // Delegate methods
+    @Override
+    public int size() {
+        return delegate.size();
+    }
+
+    @Override
+    public boolean isEmpty() {
+        return delegate.isEmpty();
+    }
+
+    @Override
+    public boolean containsKey(Object key) {
+        return delegate.containsKey(key);
+    }
+
+    @Override
+    public boolean containsValue(Object value) {
+        return delegate.containsValue(value);
+    }
+
+    @Override
+    public V get(Object key) {
+        return delegate.get(key);
+    }
+
+    @Override
+    public V put(K key, V value) {
+        return delegate.put(key, value);
+    }
+
+    @Override
+    public V remove(Object key) {
+        return delegate.remove(key);
+    }
+
+    @Override
+    public void putAll(Map<? extends K, ? extends V> m) {
+        delegate.putAll(m);
+    }
+
+    @Override
+    public void clear() {
+        delegate.clear();
+    }
+
+    @Override
+    public Set<K> keySet() {
+        return delegate.keySet();
+    }
+
+    @Override
+    public Collection<V> values() {
+        return delegate.values();
+    }
+
+    @Override
+    public Set<Entry<K, V>> entrySet() {
+        return delegate.entrySet();
+    }
+
+    @Override
+    public V getOrDefault(Object key, V defaultValue) {
+        return delegate.getOrDefault(key, defaultValue);
+    }
+
+    @Override
+    public void forEach(BiConsumer<? super K, ? super V> action) {
+        delegate.forEach(action);
+    }
+
+    @Override
+    public void replaceAll(BiFunction<? super K, ? super V, ? extends V> function) {
+        delegate.replaceAll(function);
+    }
+
+    @Override
+    public V putIfAbsent(K key, V value) {
+        return delegate.putIfAbsent(key, value);
+    }
+
+    @Override
+    public boolean remove(Object key, Object value) {
+        return delegate.remove(key, value);
+    }
+
+    @Override
+    public boolean replace(K key, V oldValue, V newValue) {
+        return delegate.replace(key, oldValue, newValue);
+    }
+
+    @Override
+    public V replace(K key, V value) {
+        return delegate.replace(key, value);
+    }
+
+    @Override
+    public V computeIfAbsent(K key, Function<? super K, ? extends V> mappingFunction) {
+        return delegate.computeIfAbsent(key, mappingFunction);
+    }
+
+    @Override
+    public V computeIfPresent(K key, BiFunction<? super K, ? super V, ? extends V> remappingFunction) {
+        return delegate.computeIfPresent(key, remappingFunction);
+    }
+
+    @Override
+    public V compute(K key, BiFunction<? super K, ? super V, ? extends V> remappingFunction) {
+        return delegate.compute(key, remappingFunction);
+    }
+
+    @Override
+    public V merge(K key, V value, BiFunction<? super V, ? super V, ? extends V> remappingFunction) {
+        return delegate.merge(key, value, remappingFunction);
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        return delegate.equals(o);
+    }
+
+    @Override
+    public int hashCode() {
+        return delegate.hashCode();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/DLQEntry.java b/logstash-core/src/main/java/org/logstash/DLQEntry.java
new file mode 100644
index 00000000000..78624901519
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/DLQEntry.java
@@ -0,0 +1,115 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash;
+
+import org.logstash.ackedqueue.Queueable;
+
+import java.io.IOException;
+import java.io.Serializable;
+import java.nio.ByteBuffer;
+
+
+public class DLQEntry implements Cloneable, Serializable, Queueable {
+
+    private final Event event;
+    private final String pluginType;
+    private final String pluginId;
+    private final String reason;
+    private final Timestamp entryTime;
+
+    public DLQEntry(Event event, String pluginType, String pluginId, String reason) {
+        this(event, pluginType, pluginId, reason, Timestamp.now());
+    }
+
+    public DLQEntry(Event event, String pluginType, String pluginId, String reason, Timestamp entryTime) {
+        this.event = event;
+        this.pluginType = pluginType;
+        this.pluginId = pluginId;
+        this.reason = reason;
+        this.entryTime = entryTime;
+    }
+
+    @Override
+    public byte[] serialize() throws IOException {
+        byte[] entryTimeInBytes = entryTime.serialize();
+        byte[] eventInBytes = this.event.serialize();
+        byte[] pluginTypeBytes = pluginType.getBytes();
+        byte[] pluginIdBytes = pluginId.getBytes();
+        byte[] reasonBytes = reason.getBytes();
+        ByteBuffer buffer = ByteBuffer.allocate(entryTimeInBytes.length
+                + eventInBytes.length
+                + pluginTypeBytes.length
+                + pluginIdBytes.length
+                + reasonBytes.length
+                + (Integer.BYTES * 5)); // magic number represents the five byte[] + lengths
+        putLengthAndBytes(buffer, entryTimeInBytes);
+        putLengthAndBytes(buffer, eventInBytes);
+        putLengthAndBytes(buffer, pluginTypeBytes);
+        putLengthAndBytes(buffer, pluginIdBytes);
+        putLengthAndBytes(buffer, reasonBytes);
+        return buffer.array();
+    }
+
+    public static DLQEntry deserialize(byte[] bytes) throws IOException {
+        ByteBuffer buffer = ByteBuffer.allocate(bytes.length);
+        buffer.put(bytes);
+        buffer.position(0);
+
+        Timestamp entryTime = new Timestamp(new String(getLengthPrefixedBytes(buffer)));
+        Event event = Event.deserialize(getLengthPrefixedBytes(buffer));
+        String pluginType = new String(getLengthPrefixedBytes(buffer));
+        String pluginId = new String(getLengthPrefixedBytes(buffer));
+        String reason = new String(getLengthPrefixedBytes(buffer));
+
+        return new DLQEntry(event, pluginType, pluginId, reason, entryTime);
+    }
+
+    private static void putLengthAndBytes(ByteBuffer buffer, byte[] bytes) {
+        buffer.putInt(bytes.length);
+        buffer.put(bytes);
+    }
+
+    private static byte[] getLengthPrefixedBytes(ByteBuffer buffer) {
+        int length = buffer.getInt();
+        byte[] bytes = new byte[length];
+        buffer.get(bytes);
+        return bytes;
+    }
+
+    public Event getEvent() {
+        return event;
+    }
+
+    public String getPluginType() {
+        return pluginType;
+    }
+
+    public String getPluginId() {
+        return pluginId;
+    }
+
+    public String getReason() {
+        return reason;
+    }
+
+    public Timestamp getEntryTime() {
+        return entryTime;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/DateNode.java b/logstash-core/src/main/java/org/logstash/DateNode.java
similarity index 92%
rename from logstash-core-event-java/src/main/java/com/logstash/DateNode.java
rename to logstash-core/src/main/java/org/logstash/DateNode.java
index 560d9f53d3c..423cb8ecb70 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/DateNode.java
+++ b/logstash-core/src/main/java/org/logstash/DateNode.java
@@ -1,10 +1,9 @@
-package com.logstash;
+package org.logstash;
 
 import org.joda.time.DateTimeZone;
 import org.joda.time.format.DateTimeFormat;
 import org.joda.time.format.DateTimeFormatter;
 
-import java.io.IOError;
 import java.io.IOException;
 
 /**
diff --git a/logstash-core-event-java/src/main/java/com/logstash/EpochNode.java b/logstash-core/src/main/java/org/logstash/EpochNode.java
similarity index 93%
rename from logstash-core-event-java/src/main/java/com/logstash/EpochNode.java
rename to logstash-core/src/main/java/org/logstash/EpochNode.java
index 4451ffa73c4..cc228315d74 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/EpochNode.java
+++ b/logstash-core/src/main/java/org/logstash/EpochNode.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Event.java b/logstash-core/src/main/java/org/logstash/Event.java
similarity index 56%
rename from logstash-core-event-java/src/main/java/com/logstash/Event.java
rename to logstash-core/src/main/java/org/logstash/Event.java
index bf62eb4ea3b..206ce651a0a 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Event.java
+++ b/logstash-core/src/main/java/org/logstash/Event.java
@@ -1,16 +1,31 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.logstash.bivalues.NullBiValue;
+import org.logstash.bivalues.StringBiValue;
+import org.logstash.bivalues.TimeBiValue;
+import org.logstash.bivalues.TimestampBiValue;
+import org.logstash.ext.JrubyTimestampExtLibrary;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
 import org.joda.time.DateTime;
 import org.jruby.RubySymbol;
+import org.logstash.ackedqueue.Queueable;
 
 import java.io.IOException;
 import java.io.Serializable;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 
+import static org.logstash.ObjectMappers.CBOR_MAPPER;
+import static org.logstash.ObjectMappers.JSON_MAPPER;
 
-public class Event implements Cloneable, Serializable {
+
+public class Event implements Cloneable, Serializable, Queueable {
 
     private boolean cancelled;
     private Map<String, Object> data;
@@ -26,13 +41,13 @@ public class Event implements Cloneable, Serializable {
     public static final String TIMESTAMP_FAILURE_FIELD = "_@timestamp";
     public static final String VERSION = "@version";
     public static final String VERSION_ONE = "1";
+    private static final String DATA_MAP_KEY = "DATA";
+    private static final String META_MAP_KEY = "META";
+    private static final String SEQNUM_MAP_KEY = "SEQUENCE_NUMBER";
 
-    private static final Logger DEFAULT_LOGGER = new StdioLogger();
-    private static final ObjectMapper mapper = new ObjectMapper();
 
-    // logger is static since once set there is no point in changing it at runtime
-    // for other reasons than in tests/specs.
-    private transient static Logger logger = DEFAULT_LOGGER;
+    private static final Logger logger = LogManager.getLogger(Event.class);
+    private static final ObjectMapper mapper = new ObjectMapper();
 
     public Event()
     {
@@ -48,22 +63,34 @@ public Event()
 
     public Event(Map data)
     {
-        this.data = data;
+        this.data = (Map<String, Object>)Valuefier.convert(data);
+
         if (!this.data.containsKey(VERSION)) {
             this.data.put(VERSION, VERSION_ONE);
         }
 
         if (this.data.containsKey(METADATA)) {
-            this.metadata = (HashMap<String, Object>) this.data.remove(METADATA);
+            this.metadata = (Map<String, Object>) this.data.remove(METADATA);
         } else {
             this.metadata = new HashMap<String, Object>();
         }
         this.metadata_accessors = new Accessors(this.metadata);
 
         this.cancelled = false;
-        this.timestamp = initTimestamp(data.get(TIMESTAMP));
+
+        Object providedTimestamp = data.get(TIMESTAMP);
+        // keep reference to the parsedTimestamp for tagging below
+        Timestamp parsedTimestamp = initTimestamp(providedTimestamp);
+        this.timestamp = (parsedTimestamp == null) ? Timestamp.now() : parsedTimestamp;
+
         this.data.put(TIMESTAMP, this.timestamp);
         this.accessors = new Accessors(this.data);
+
+        // the tag() method has to be called after the Accessors initialization
+        if (parsedTimestamp == null) {
+            tag(TIMESTAMP_FAILURE_TAG);
+            this.setField(TIMESTAMP_FAILURE_FIELD, providedTimestamp);
+        }
     }
 
     public Map<String, Object> getData() {
@@ -75,7 +102,7 @@ public Map<String, Object> getMetadata() {
     }
 
     public void setData(Map<String, Object> data) {
-        this.data = data;
+        this.data = ConvertedMap.newFromMap(data);
     }
 
     public Accessors getAccessors() {
@@ -120,6 +147,11 @@ public void setTimestamp(Timestamp t) {
     }
 
     public Object getField(String reference) {
+        Object val = getUnconvertedField(reference);
+        return Javafier.deep(val);
+    }
+
+    public Object getUnconvertedField(String reference) {
         if (reference.equals(METADATA)) {
             return this.metadata;
         } else if (reference.startsWith(METADATA_BRACKETS)) {
@@ -134,12 +166,12 @@ public void setField(String reference, Object value) {
             // TODO(talevy): check type of timestamp
             this.accessors.set(reference, value);
         } else if (reference.equals(METADATA_BRACKETS) || reference.equals(METADATA)) {
-            this.metadata = (HashMap<String, Object>) value;
+            this.metadata = (Map<String, Object>) value;
             this.metadata_accessors = new Accessors(this.metadata);
         } else if (reference.startsWith(METADATA_BRACKETS)) {
             this.metadata_accessors.set(reference.substring(METADATA_BRACKETS.length()), value);
         } else {
-            this.accessors.set(reference, value);
+            this.accessors.set(reference, Valuefier.convert(value));
         }
     }
 
@@ -153,10 +185,53 @@ public boolean includes(String reference) {
         }
     }
 
+    public byte[] toBinary() throws IOException {
+        return toBinaryFromMap(toSerializableMap());
+    }
+
+    private Map<String, Map<String, Object>> toSerializableMap() {
+        HashMap<String, Map<String, Object>> hashMap = new HashMap<>();
+        hashMap.put(DATA_MAP_KEY, this.data);
+        hashMap.put(META_MAP_KEY, this.metadata);
+        return hashMap;
+    }
+
+    private byte[] toBinaryFromMap(Map<String, Map<String, Object>> representation) throws IOException {
+        return CBOR_MAPPER.writeValueAsBytes(representation);
+    }
+
+    private static Event fromSerializableMap(Map<String, Map<String, Object>> representation) throws IOException{
+        if (!representation.containsKey(DATA_MAP_KEY)) {
+            throw new IOException("The deserialized Map must contain the \"DATA\" key");
+        }
+        if (!representation.containsKey(META_MAP_KEY)) {
+            throw new IOException("The deserialized Map must contain the \"META\" key");
+        }
+        Map<String, Object> dataMap = representation.get(DATA_MAP_KEY);
+        dataMap.put(METADATA, representation.get(META_MAP_KEY));
+        return new Event(dataMap);
+    }
+
+    public static Event fromBinary(byte[] source) throws IOException {
+        if (source == null || source.length == 0) {
+            return new Event();
+        }
+        return fromSerializableMap(fromBinaryToMap(source));
+    }
+
+    private static Map<String, Map<String, Object>> fromBinaryToMap(byte[] source) throws IOException {
+        Object o = CBOR_MAPPER.readValue(source, HashMap.class);
+        if (o instanceof Map) {
+            return (HashMap<String, Map<String, Object>>) o;
+        } else {
+            throw new IOException("incompatible from binary object type=" + o.getClass().getName() + " , only HashMap is supported");
+        }
+    }
+
     public String toJson()
             throws IOException
     {
-        return mapper.writeValueAsString((Map<String, Object>)this.data);
+        return JSON_MAPPER.writeValueAsString(this.data);
     }
 
     public static Event[] fromJson(String json)
@@ -168,7 +243,7 @@ public static Event[] fromJson(String json)
         }
 
         Event[] result;
-        Object o = mapper.readValue(json, Object.class);
+        Object o = JSON_MAPPER.readValue(json, Object.class);
         // we currently only support Map or Array json objects
         if (o instanceof Map) {
             result = new Event[]{ new Event((Map)o) };
@@ -177,19 +252,19 @@ public static Event[] fromJson(String json)
             int i = 0;
             for (Object e : (List)o) {
                 if (!(e instanceof Map)) {
-                    throw new IOException("incompatible inner json array object type=" + e.getClass().getName() + " , only hash map is suppoted");
+                    throw new IOException("incompatible inner json array object type=" + e.getClass().getName() + " , only hash map is supported");
                 }
                 result[i++] = new Event((Map)e);
             }
         } else {
-            throw new IOException("incompatible json object type=" + o.getClass().getName() + " , only hash map or arrays are suppoted");
+            throw new IOException("incompatible json object type=" + o.getClass().getName() + " , only hash map or arrays are supported");
         }
 
         return result;
     }
 
     public Map toMap() {
-        return this.data;
+        return Cloner.deep(this.data);
     }
 
     public Event overwrite(Event e) {
@@ -205,7 +280,6 @@ public Event overwrite(Event e) {
         return this;
     }
 
-
     public Event append(Event e) {
         Util.mapMerge(this.data, e.data);
 
@@ -247,16 +321,22 @@ public String toString() {
 
     private Timestamp initTimestamp(Object o) {
         try {
-            if (o == null) {
+            if (o == null || o instanceof NullBiValue) {
                 // most frequent
                 return new Timestamp();
             } else if (o instanceof String) {
                 // second most frequent
                 return new Timestamp((String) o);
+            } else if (o instanceof StringBiValue) {
+                return new Timestamp(((StringBiValue) o).javaValue());
+            } else if (o instanceof TimeBiValue) {
+                return new Timestamp(((TimeBiValue) o).javaValue());
             } else if (o instanceof JrubyTimestampExtLibrary.RubyTimestamp) {
                 return new Timestamp(((JrubyTimestampExtLibrary.RubyTimestamp) o).getTimestamp());
             } else if (o instanceof Timestamp) {
                 return new Timestamp((Timestamp) o);
+            } else if (o instanceof TimestampBiValue) {
+                return new Timestamp(((TimestampBiValue) o).javaValue());
             } else if (o instanceof DateTime) {
                 return new Timestamp((DateTime) o);
             } else if (o instanceof Date) {
@@ -264,33 +344,58 @@ private Timestamp initTimestamp(Object o) {
             } else if (o instanceof RubySymbol) {
                 return new Timestamp(((RubySymbol) o).asJavaString());
             } else {
-                Event.logger.warn("Unrecognized " + TIMESTAMP + " value type=" + o.getClass().toString());
+                logger.warn("Unrecognized " + TIMESTAMP + " value type=" + o.getClass().toString());
             }
         } catch (IllegalArgumentException e) {
-            Event.logger.warn("Error parsing " + TIMESTAMP + " string value=" + o.toString());
+            logger.warn("Error parsing " + TIMESTAMP + " string value=" + o.toString());
         }
 
-        tag(TIMESTAMP_FAILURE_TAG);
-        this.data.put(TIMESTAMP_FAILURE_FIELD, o);
-
-        return Timestamp.now();
+        return null;
     }
 
     public void tag(String tag) {
-        List<Object> tags = (List<Object>) this.data.get("tags");
-        if (tags == null) {
+        List<Object> tags;
+        Object _tags = this.getField("tags");
+
+        // short circuit the null case where we know we won't need deduplication step below at the end
+        if (_tags == null) {
+            setField("tags", Arrays.asList(tag));
+            return;
+        }
+
+        // assign to tags var the proper List of either the existing _tags List or a new List containing whatever non-List item was in the tags field
+        if (_tags instanceof List) {
+            tags = (List<Object>) _tags;
+        } else {
+            // tags field has a value but not in a List, convert in into a List
             tags = new ArrayList<>();
-            this.data.put("tags", tags);
+            tags.add(_tags);
         }
 
+        // now make sure the tags list does not already contain the tag
+        // TODO: we should eventually look into using alternate data structures to do more efficient dedup but that will require properly defining the tagging API too
         if (!tags.contains(tag)) {
             tags.add(tag);
         }
+
+        // set that back as a proper BiValue
+        this.setField("tags", tags);
     }
 
-    // Event.logger is static since once set there is no point in changing it at runtime
-    // for other reasons than in tests/specs.
-    public static void setLogger(Logger logger) {
-        Event.logger = logger;
+    public byte[] serialize() throws IOException {
+        Map<String, Map<String, Object>> dataMap = toSerializableMap();
+        return toBinaryFromMap(dataMap);
+    }
+
+    public byte[] serializeWithoutSeqNum() throws IOException {
+        return toBinary();
+    }
+
+    public static Event deserialize(byte[] data) throws IOException {
+        if (data == null || data.length == 0) {
+            return new Event();
+        }
+        Map<String, Map<String, Object>> dataMap = fromBinaryToMap(data);
+        return fromSerializableMap(dataMap);
     }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/FieldReference.java b/logstash-core/src/main/java/org/logstash/FieldReference.java
similarity index 97%
rename from logstash-core-event-java/src/main/java/com/logstash/FieldReference.java
rename to logstash-core/src/main/java/org/logstash/FieldReference.java
index e0d7e3ee969..5e468743e39 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/FieldReference.java
+++ b/logstash-core/src/main/java/org/logstash/FieldReference.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.ArrayList;
 import java.util.Arrays;
diff --git a/logstash-core/src/main/java/org/logstash/FileLockFactory.java b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
new file mode 100644
index 00000000000..b553ab9e6e7
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
@@ -0,0 +1,121 @@
+// this class is largely inspired by Lucene FSLockFactory and friends, below is the Lucene original Apache 2.0 license:
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.logstash;
+
+import java.io.IOException;
+import java.nio.channels.FileChannel;
+import java.nio.channels.FileLock;
+import java.nio.file.FileSystems;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * FileLockFactory provides a way to obtain an exclusive file lock for a given dir path and lock name.
+ * The obtainLock() method will return a Filelock object which should be released using the releaseLock()
+ * method. Normally the returned FileLock object should not be manipulated directly. Only the obtainLock()
+ * and releaseLock() methods should be use to gain and release exclusive access.
+ * This is threadsafe and will work across threads on the same JVM and will also work across processes/JVM.
+ *
+ * TODO: because of the releaseLock() method, strictly speaking this class is not only a factory anymore,
+ * maybe we should rename it FileLockManager?
+ */
+public class FileLockFactory {
+
+    /**
+     * Singleton instance
+     */
+    public static final FileLockFactory INSTANCE = new FileLockFactory();
+
+    private FileLockFactory() {}
+
+    private static final Set<String> LOCK_HELD = Collections.synchronizedSet(new HashSet<>());
+    private static final Map<FileLock, String> LOCK_MAP =  Collections.synchronizedMap(new HashMap<>());
+
+    public static final FileLockFactory getDefault() {
+        return FileLockFactory.INSTANCE;
+    }
+
+    public FileLock obtainLock(String lockDir, String lockName) throws IOException {
+        Path dirPath = FileSystems.getDefault().getPath(lockDir);
+
+        // Ensure that lockDir exists and is a directory.
+        // note: this will fail if lockDir is a symlink
+        Files.createDirectories(dirPath);
+
+        Path lockPath = dirPath.resolve(lockName);
+
+        try {
+            Files.createFile(lockPath);
+        } catch (IOException ignore) {
+            // we must create the file to have a truly canonical path.
+            // if it's already created, we don't care. if it cant be created, it will fail below.
+        }
+
+        // fails if the lock file does not exist
+        final Path realLockPath = lockPath.toRealPath();
+
+        if (LOCK_HELD.add(realLockPath.toString())) {
+            FileChannel channel = null;
+            FileLock lock = null;
+            try {
+                channel = FileChannel.open(realLockPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE);
+                lock = channel.tryLock();
+                if (lock != null) {
+                    LOCK_MAP.put(lock, realLockPath.toString());
+                    return lock;
+                } else {
+                    throw new LockException("Lock held by another program on lock path: " + realLockPath);
+                }
+            } finally {
+                if (lock == null) { // not successful - clear up and move out
+                    try {
+                        if (channel != null) {
+                            channel.close();
+                        }
+                    } catch (Throwable t) {
+                        // suppress any channel close exceptions
+                    }
+
+                    boolean removed = LOCK_HELD.remove(realLockPath.toString());
+                    if (removed == false) {
+                        throw new LockException("Lock path was cleared but never marked as held: " + realLockPath);
+                    }
+                }
+            }
+        } else {
+            throw new LockException("Lock held by this virtual machine on lock path: " + realLockPath);
+        }
+    }
+
+    public void releaseLock(FileLock lock) throws IOException {
+        String lockPath = LOCK_MAP.remove(lock);
+        if (lockPath == null) { throw new LockException("Cannot release unobtained lock"); }
+        lock.release();
+        Boolean removed = LOCK_HELD.remove(lockPath);
+        if (removed == false) { throw new LockException("Lock path was not marked as held: " + lockPath); }
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/Javafier.java b/logstash-core/src/main/java/org/logstash/Javafier.java
new file mode 100644
index 00000000000..b12e7ec6bac
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/Javafier.java
@@ -0,0 +1,34 @@
+package org.logstash;
+
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+
+public class Javafier {
+    private static final String ERR_TEMPLATE = "Missing Ruby class handling for full class name=%s, simple name=%s";
+    /*
+    Javafier.deep() is called by getField.
+    When any value is added to the Event it should pass through Valuefier.convert.
+    deep(Object o) is the mechanism to pluck the Java value from a BiValue or convert a
+    ConvertedList and ConvertedMap back to ArrayList or HashMap.
+     */
+    private Javafier(){}
+
+    public static Object deep(Object o) {
+        if (o instanceof BiValue) {
+            return ((BiValue)o).javaValue();
+        } else if(o instanceof ConvertedMap) {
+            return ((ConvertedMap) o).unconvert();
+        }  else if(o instanceof ConvertedList) {
+            return ((ConvertedList) o).unconvert();
+        } else {
+            try {
+                return BiValues.newBiValue(o).javaValue();
+            } catch (IllegalArgumentException e) {
+                Class cls = o.getClass();
+                throw new IllegalArgumentException(String.format(ERR_TEMPLATE, cls.getName(), cls.getSimpleName()));
+            }
+        }
+    }
+}
+
diff --git a/logstash-core-event-java/src/main/java/com/logstash/KeyNode.java b/logstash-core/src/main/java/org/logstash/KeyNode.java
similarity index 91%
rename from logstash-core-event-java/src/main/java/com/logstash/KeyNode.java
rename to logstash-core/src/main/java/org/logstash/KeyNode.java
index cfc46861f69..e6a5c0bea3d 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/KeyNode.java
+++ b/logstash-core/src/main/java/org/logstash/KeyNode.java
@@ -1,6 +1,7 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.databind.ObjectMapper;
+import org.logstash.bivalues.BiValue;
 
 import java.io.IOException;
 import java.util.List;
@@ -58,6 +59,9 @@ public static String join(List<?> list, String delim) {
     private static String toString(Object value, String delim) {
         if (value == null) return "";
         if (value instanceof List) return join((List)value, delim);
+        if (value instanceof BiValue) {
+            return ((BiValue) value).toString();
+        }
         return value.toString();
     }
 }
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/LockException.java b/logstash-core/src/main/java/org/logstash/LockException.java
new file mode 100644
index 00000000000..fad548440d4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/LockException.java
@@ -0,0 +1,13 @@
+package org.logstash;
+
+import java.io.IOException;
+
+public class LockException extends IOException {
+    public LockException(String message) {
+        super(message);
+    }
+
+    public LockException(String message, Throwable cause) {
+        super(message, cause);
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/ObjectMappers.java b/logstash-core/src/main/java/org/logstash/ObjectMappers.java
new file mode 100644
index 00000000000..55cc633b685
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ObjectMappers.java
@@ -0,0 +1,19 @@
+package org.logstash;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import com.fasterxml.jackson.dataformat.cbor.CBORFactory;
+import com.fasterxml.jackson.dataformat.cbor.CBORGenerator;
+import com.fasterxml.jackson.module.afterburner.AfterburnerModule;
+
+public class ObjectMappers {
+    public static final ObjectMapper JSON_MAPPER = new ObjectMapper();
+    public static final ObjectMapper CBOR_MAPPER = new ObjectMapper(new CBORFactory());
+
+    static {
+        JSON_MAPPER.registerModule(new AfterburnerModule());
+
+        CBORFactory cborf = (CBORFactory) CBOR_MAPPER.getFactory();
+        cborf.configure(CBORGenerator.Feature.WRITE_MINIMAL_INTS, false);
+        CBOR_MAPPER.registerModule(new AfterburnerModule());
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/PathCache.java b/logstash-core/src/main/java/org/logstash/PathCache.java
similarity index 98%
rename from logstash-core-event-java/src/main/java/com/logstash/PathCache.java
rename to logstash-core/src/main/java/org/logstash/PathCache.java
index b7beff95b89..2e884470850 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/PathCache.java
+++ b/logstash-core/src/main/java/org/logstash/PathCache.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.concurrent.ConcurrentHashMap;
 
diff --git a/logstash-core/src/main/java/org/logstash/Rubyfier.java b/logstash-core/src/main/java/org/logstash/Rubyfier.java
new file mode 100644
index 00000000000..a23b65c314b
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/Rubyfier.java
@@ -0,0 +1,59 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+import org.jruby.Ruby;
+import org.jruby.RubyArray;
+import org.jruby.RubyHash;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+
+public final class Rubyfier {
+    private static final String ERR_TEMPLATE = "Missing Java class handling for full class name=%s, simple name=%s";
+    /*
+    Rubyfier.deep() is called by JrubyEventExtLibrary RubyEvent ruby_get_field,
+    ruby_remove, ruby_to_hash and ruby_to_hash_with_metadata.
+    When any value is added to the Event it should pass through Valuefier.convert.
+    Rubyfier.deep is the mechanism to pluck the Ruby value from a BiValue or convert a
+    ConvertedList and ConvertedMap back to RubyArray or RubyHash.
+    However, IRubyObjects and the RUby runtime do not belong in ConvertedMap or ConvertedList
+    so they are unconverted here.
+    */
+    private Rubyfier() {
+    }
+
+    public static IRubyObject deep(Ruby runtime, final Object input) {
+        if (input instanceof BiValue) return ((BiValue) input).rubyValue(runtime);
+        if (input instanceof Map) return deepMap(runtime, (Map) input);
+        if (input instanceof List) return deepList(runtime, (List) input);
+        if (input instanceof Collection) throw new ClassCastException("Unexpected Collection type " + input.getClass());
+
+        try {
+            return BiValues.newBiValue(input).rubyValue(runtime);
+        } catch (IllegalArgumentException e) {
+            Class cls = input.getClass();
+            throw new IllegalArgumentException(String.format(ERR_TEMPLATE, cls.getName(), cls.getSimpleName()));
+        }
+    }
+
+    private static RubyArray deepList(Ruby runtime, final List list) {
+        final int length = list.size();
+        final RubyArray array = runtime.newArray(length);
+        for (Object item : list) {
+            array.add(deep(runtime, item));
+        }
+        return array;
+    }
+
+    private static RubyHash deepMap(Ruby runtime, final Map<?, ?> map) {
+        RubyHash hash = RubyHash.newHash(runtime);
+        for (Map.Entry entry : map.entrySet()) {
+            // Note: RubyHash.put calls JavaUtil.convertJavaToUsableRubyObject on keys and values
+            hash.put(entry.getKey(), deep(runtime, entry.getValue()));
+        }
+        return hash;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StaticNode.java b/logstash-core/src/main/java/org/logstash/StaticNode.java
similarity index 93%
rename from logstash-core-event-java/src/main/java/com/logstash/StaticNode.java
rename to logstash-core/src/main/java/org/logstash/StaticNode.java
index 73b5c160440..36c2ef11123 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/StaticNode.java
+++ b/logstash-core/src/main/java/org/logstash/StaticNode.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java b/logstash-core/src/main/java/org/logstash/StringInterpolation.java
similarity index 98%
rename from logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java
rename to logstash-core/src/main/java/org/logstash/StringInterpolation.java
index 5830cc89672..2bf93561095 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java
+++ b/logstash-core/src/main/java/org/logstash/StringInterpolation.java
@@ -1,9 +1,8 @@
-package com.logstash;
+package org.logstash;
 
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.Objects;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Template.java b/logstash-core/src/main/java/org/logstash/Template.java
similarity index 96%
rename from logstash-core-event-java/src/main/java/com/logstash/Template.java
rename to logstash-core/src/main/java/org/logstash/Template.java
index a17e69b3946..418e1690824 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Template.java
+++ b/logstash-core/src/main/java/org/logstash/Template.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 import java.util.ArrayList;
diff --git a/logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java b/logstash-core/src/main/java/org/logstash/TemplateNode.java
similarity index 87%
rename from logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java
rename to logstash-core/src/main/java/org/logstash/TemplateNode.java
index 942bbc1ee03..1f7d9fbcf56 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java
+++ b/logstash-core/src/main/java/org/logstash/TemplateNode.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java b/logstash-core/src/main/java/org/logstash/Timestamp.java
similarity index 83%
rename from logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
rename to logstash-core/src/main/java/org/logstash/Timestamp.java
index 434dc93a13c..f76ede04ad6 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
+++ b/logstash-core/src/main/java/org/logstash/Timestamp.java
@@ -1,17 +1,19 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.databind.annotation.JsonSerialize;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
-import org.joda.time.LocalDateTime;
 import org.joda.time.Duration;
+import org.joda.time.LocalDateTime;
 import org.joda.time.format.DateTimeFormatter;
 import org.joda.time.format.ISODateTimeFormat;
+import org.logstash.ackedqueue.Queueable;
 
+import java.io.IOException;
 import java.util.Date;
 
-@JsonSerialize(using = TimestampSerializer.class)
-public class Timestamp implements Cloneable {
+@JsonSerialize(using = org.logstash.json.TimestampSerializer.class)
+public class Timestamp implements Cloneable, Comparable, Queueable {
 
     // all methods setting the time object must set it in the UTC timezone
     private DateTime time;
@@ -75,10 +77,20 @@ public long usec() {
         return (new Duration(JAN_1_1970.toDateTime(DateTimeZone.UTC), this.time).getMillis() % 1000) * 1000;
     }
 
+    @Override
+    public int compareTo(Object other) {
+        return getTime().compareTo(((Timestamp) other).getTime());
+    }
+
     @Override
     public Timestamp clone() throws CloneNotSupportedException {
         Timestamp clone = (Timestamp)super.clone();
         clone.setTime(this.getTime());
         return clone;
     }
+
+    @Override
+    public byte[] serialize() throws IOException {
+        return toString().getBytes();
+    }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java b/logstash-core/src/main/java/org/logstash/TimestampSerializer.java
similarity index 95%
rename from logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
rename to logstash-core/src/main/java/org/logstash/TimestampSerializer.java
index c90afdd9227..3c854c3d8b5 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
+++ b/logstash-core/src/main/java/org/logstash/TimestampSerializer.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.databind.JsonSerializer;
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Util.java b/logstash-core/src/main/java/org/logstash/Util.java
similarity index 66%
rename from logstash-core-event-java/src/main/java/com/logstash/Util.java
rename to logstash-core/src/main/java/org/logstash/Util.java
index 907fd5489b1..77a6565b7de 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Util.java
+++ b/logstash-core/src/main/java/org/logstash/Util.java
@@ -1,13 +1,44 @@
-package com.logstash;
+package org.logstash;
 
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
 import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Map;
 
+
 public class Util {
     private Util() {}
 
+    public static Object getMapFixtureJackson() throws IOException {
+        StringBuilder json = new StringBuilder();
+        json.append("{");
+        json.append("\"string\": \"foo\", ");
+        json.append("\"int\": 42, ");
+        json.append("\"float\": 42.42, ");
+        json.append("\"array\": [\"bar\",\"baz\"], ");
+        json.append("\"hash\": {\"string\":\"quux\"} }");
+
+        ObjectMapper mapper = new ObjectMapper();
+        return mapper.readValue(json.toString(), Object.class);
+    }
+
+    public static Map<String, Object> getMapFixtureHandcrafted() {
+        HashMap<String, Object> inner = new HashMap<>();
+        inner.put("string", "quux");
+        HashMap<String, Object> map = new HashMap<>();
+        map.put("string", "foo");
+        map.put("int", 42);
+        map.put("float", 42.42);
+        map.put("array", Arrays.asList("bar", "baz"));
+        map.put("hash", inner);
+        return map;
+    }
+
     public static void mapMerge(Map<String, Object> target, Map<String, Object> add) {
         for (Map.Entry<String, Object> e : add.entrySet()) {
             if (target.containsKey(e.getKey())) {
diff --git a/logstash-core/src/main/java/org/logstash/Valuefier.java b/logstash-core/src/main/java/org/logstash/Valuefier.java
new file mode 100644
index 00000000000..1739c11c312
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/Valuefier.java
@@ -0,0 +1,92 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+import org.logstash.ext.JrubyTimestampExtLibrary;
+import org.joda.time.DateTime;
+import org.jruby.RubyArray;
+import org.jruby.RubyHash;
+import org.jruby.RubyTime;
+import org.jruby.java.proxies.ArrayJavaProxy;
+import org.jruby.java.proxies.ConcreteJavaProxy;
+import org.jruby.java.proxies.JavaProxy;
+import org.jruby.java.proxies.MapJavaProxy;
+import org.jruby.javasupport.JavaUtil;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.List;
+import java.util.Map;
+
+public class Valuefier {
+    private static final String PROXY_ERR_TEMPLATE = "Missing Valuefier handling for full class name=%s, simple name=%s, wrapped object=%s";
+    private static final String ERR_TEMPLATE = "Missing Valuefier handling for full class name=%s, simple name=%s";
+
+    private Valuefier(){}
+
+    private static Object convertJavaProxy(JavaProxy jp) {
+        Object obj = JavaUtil.unwrapJavaObject(jp);
+        if (obj instanceof IRubyObject[]) {
+            ConvertedList<Object> list = new ConvertedList<>();
+            for (IRubyObject ro : ((IRubyObject[]) obj)) {
+                list.add(convert(ro));
+            }
+            return list;
+        }
+        if (obj instanceof List) {
+            return ConvertedList.newFromList((List<Object>) obj);
+        }
+        try {
+            return BiValues.newBiValue(jp);
+        } catch (IllegalArgumentException e) {
+            Class cls = obj.getClass();
+            throw new IllegalArgumentException(String.format(PROXY_ERR_TEMPLATE, cls.getName(), cls.getSimpleName(), obj.getClass().getName()), e);
+        }
+    }
+
+    public static Object convertNonCollection(Object o) {
+        try {
+            return BiValues.newBiValue(o);
+        } catch (IllegalArgumentException e) {
+            Class cls = o.getClass();
+            throw new IllegalArgumentException(String.format(ERR_TEMPLATE, cls.getName(), cls.getSimpleName()), e);
+        }
+    }
+
+    public static Object convert(Object o) throws IllegalArgumentException {
+        if (o instanceof ConvertedMap || o instanceof ConvertedList) {
+            return o;
+        }
+        if (o instanceof BiValue) {
+            return o;
+        }
+        if (o instanceof RubyHash) {
+            return ConvertedMap.newFromRubyHash((RubyHash) o);
+        }
+        if (o instanceof RubyArray) {
+            return ConvertedList.newFromRubyArray((RubyArray) o);
+        }
+        if (o instanceof Map) {
+            return ConvertedMap.newFromMap((Map<String, Object>) o);
+        }
+        if (o instanceof List) {
+            return ConvertedList.newFromList((List<Object>) o);
+        }
+        if (o instanceof MapJavaProxy){
+            return ConvertedMap.newFromMap((Map)((MapJavaProxy) o).getObject());
+        }
+        if (o instanceof ArrayJavaProxy || o instanceof ConcreteJavaProxy){
+            return convertJavaProxy((JavaProxy) o);
+        }
+        if (o instanceof RubyTime) {
+            RubyTime time = (RubyTime) o;
+            Timestamp ts = new Timestamp(time.getDateTime());
+            JrubyTimestampExtLibrary.RubyTimestamp rts = JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(time.getRuntime(), ts);
+            return convertNonCollection(rts);
+        }
+        if (o instanceof DateTime) {
+            Timestamp ts = new Timestamp((DateTime) o);
+            return convertNonCollection(ts);
+        }
+        return convertNonCollection(o);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
new file mode 100644
index 00000000000..7ff83a1c75d
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
@@ -0,0 +1,44 @@
+package org.logstash.ackedqueue;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.List;
+import java.util.concurrent.atomic.AtomicBoolean;
+
+public class Batch implements Closeable {
+
+    private final List<Queueable> elements;
+
+    private final List<Long> seqNums;
+    private final Queue queue;
+    private final AtomicBoolean closed;
+
+    public Batch(List<Queueable> elements, List<Long> seqNums, Queue q) {
+        this.elements = elements;
+        this.seqNums = seqNums;
+        this.queue = q;
+        this.closed = new AtomicBoolean(false);
+    }
+
+    // close acks the batch ackable events
+    public void close() throws IOException {
+        if (closed.getAndSet(true) == false) {
+              this.queue.ack(this.seqNums);
+        } else {
+            // TODO: how should we handle double-closing?
+            throw new IOException("double closing batch");
+        }
+    }
+
+    public int size() {
+        return elements.size();
+    }
+
+    public List<? extends Queueable> getElements() {
+        return elements;
+    }
+
+    public Queue getQueue() {
+        return queue;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Checkpoint.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Checkpoint.java
new file mode 100644
index 00000000000..6fc26dcc1bd
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Checkpoint.java
@@ -0,0 +1,62 @@
+package org.logstash.ackedqueue;
+
+public class Checkpoint {
+//    Checkpoint file structure see FileCheckpointIO
+
+    public static final int VERSION = 1;
+
+    private final int pageNum;             // local per-page page number
+    private final int firstUnackedPageNum; // queue-wide global pointer, only valid in the head checkpoint
+    private final long firstUnackedSeqNum; // local per-page unacknowledged tracking
+    private final long minSeqNum;          // local per-page minimum seqNum
+    private final int elementCount;        // local per-page element count
+
+
+    public Checkpoint(int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) {
+        this.pageNum = pageNum;
+        this.firstUnackedPageNum = firstUnackedPageNum;
+        this.firstUnackedSeqNum = firstUnackedSeqNum;
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+    }
+
+    public int getPageNum() {
+        return this.pageNum;
+    }
+
+    public int getFirstUnackedPageNum() {
+        return this.firstUnackedPageNum;
+    }
+
+    public long getFirstUnackedSeqNum() {
+        return this.firstUnackedSeqNum;
+    }
+
+    public long getMinSeqNum() {
+        return this.minSeqNum;
+    }
+
+    public int getElementCount() {
+        return this.elementCount;
+    }
+
+    // @return true if this checkpoint indicates a fulle acked page
+    public boolean isFullyAcked() {
+        return this.elementCount > 0 && this.firstUnackedSeqNum >= this.minSeqNum + this.elementCount;
+    }
+
+    // @return the highest seqNum in this page or -1 for an initial checkpoint
+    public long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+
+    public String toString() {
+        return "pageNum=" + this.pageNum + ", firstUnackedPageNum=" + this.firstUnackedPageNum + ", firstUnackedSeqNum=" + this.firstUnackedSeqNum + ", minSeqNum=" + this.minSeqNum + ", elementCount=" + this.elementCount + ", isFullyAcked=" + (this.isFullyAcked() ? "yes" : "no");
+    }
+
+    public boolean equals(Checkpoint other) {
+        if (this == other ) { return true; }
+        return (this.pageNum == other.pageNum && this.firstUnackedPageNum == other.firstUnackedPageNum && this.firstUnackedSeqNum == other.firstUnackedSeqNum && this.minSeqNum == other.minSeqNum && this.elementCount == other.elementCount);
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/FileSettings.java b/logstash-core/src/main/java/org/logstash/ackedqueue/FileSettings.java
new file mode 100644
index 00000000000..10ccd963a84
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/FileSettings.java
@@ -0,0 +1,130 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+public class FileSettings implements Settings {
+    private String dirForFiles;
+    private CheckpointIOFactory checkpointIOFactory;
+    private PageIOFactory pageIOFactory;
+    private Class elementClass;
+    private int capacity;
+    private long queueMaxBytes;
+    private int maxUnread;
+    private int checkpointMaxAcks;
+    private int checkpointMaxWrites;
+    private int checkpointMaxInterval;
+
+    private FileSettings() { this(""); }
+
+    public FileSettings(String dirPath) {
+        this.dirForFiles = dirPath;
+        this.maxUnread = 0;
+        this.checkpointMaxAcks = 1024;
+        this.checkpointMaxWrites = 1024;
+        this.checkpointMaxInterval = 1000; // millisec
+    }
+
+    @Override
+    public Settings setCheckpointIOFactory(CheckpointIOFactory factory) {
+        this.checkpointIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementIOFactory(PageIOFactory factory) {
+        this.pageIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementClass(Class elementClass) {
+        this.elementClass = elementClass;
+        return this;
+    }
+
+    @Override
+    public Settings setQueueMaxBytes(long size) {
+        this.queueMaxBytes = size;
+        return this;
+    }
+
+    @Override
+    public Settings setCapacity(int capacity) {
+        this.capacity = capacity;
+        return this;
+    }
+
+    @Override
+    public Settings setMaxUnread(int maxUnread) {
+        this.maxUnread = maxUnread;
+        return this;
+    }
+
+    @Override
+    public Settings setCheckpointMaxAcks(int checkpointMaxAcks) {
+        this.checkpointMaxAcks = checkpointMaxAcks;
+        return this;
+    }
+
+    @Override
+    public Settings setCheckpointMaxWrites(int checkpointMaxWrites) {
+        this.checkpointMaxWrites = checkpointMaxWrites;
+        return this;
+    }
+
+    @Override
+    public Settings setCheckpointMaxInterval(int checkpointMaxInterval) {
+        this.checkpointMaxInterval = checkpointMaxInterval;
+        return this;
+    }
+
+    @Override
+    public int getCheckpointMaxAcks() {
+        return checkpointMaxAcks;
+    }
+
+    @Override
+    public int getCheckpointMaxWrites() {
+        return checkpointMaxWrites;
+    }
+
+    @Override
+    public int getCheckpointMaxInterval() {
+        return checkpointMaxInterval;
+    }
+
+    @Override
+    public CheckpointIOFactory getCheckpointIOFactory() {
+        return checkpointIOFactory;
+    }
+
+    public PageIOFactory getPageIOFactory() {
+        return pageIOFactory;
+    }
+
+    @Override
+    public Class getElementClass()  {
+        return this.elementClass;
+    }
+
+    @Override
+    public String getDirPath() {
+        return dirForFiles;
+    }
+
+    @Override
+    public long getQueueMaxBytes() {
+        return queueMaxBytes;
+    }
+
+    @Override
+    public int getCapacity() {
+        return capacity;
+    }
+
+    @Override
+    public int getMaxUnread() {
+        return this.maxUnread;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
new file mode 100644
index 00000000000..9e0eff0d46d
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
@@ -0,0 +1,127 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.CheckpointIO;
+import org.logstash.ackedqueue.io.PageIO;
+
+import java.io.IOException;
+import java.util.BitSet;
+
+public class HeadPage extends Page {
+
+    // create a new HeadPage object and new page.{pageNum} empty valid data file
+    public HeadPage(int pageNum, Queue queue, PageIO pageIO) throws IOException {
+        super(pageNum, queue, 0, 0, 0, new BitSet(), pageIO);
+    }
+
+    // create a new HeadPage object from an existing checkpoint and open page.{pageNum} empty valid data file
+    // @param pageIO is expected to be open/recover/create
+    public HeadPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
+        super(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO);
+
+        assert checkpoint.getMinSeqNum() == pageIO.getMinSeqNum() && checkpoint.getElementCount() == pageIO.getElementCount() :
+                String.format("checkpoint minSeqNum=%d or elementCount=%d is different than pageIO minSeqNum=%d or elementCount=%d", checkpoint.getMinSeqNum(), checkpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
+
+        // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
+        if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
+            this.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
+        }
+    }
+
+    // verify if data size plus overhead is not greater than the page capacity
+    public boolean hasCapacity(int byteSize) {
+        return this.pageIO.persistedByteCount(byteSize) <= this.pageIO.getCapacity();
+    }
+
+    public boolean hasSpace(int byteSize) {
+        return this.pageIO.hasSpace((byteSize));
+    }
+
+    // NOTE:
+    // we have a page concern inconsistency where readBatch() takes care of the
+    // deserialization and returns a Batch object which contains the deserialized
+    // elements objects of the proper elementClass but HeadPage.write() deals with
+    // a serialized element byte[] and serialization is done at the Queue level to
+    // be able to use the Page.hasSpace() method with the serialized element byte size.
+    //
+    public void write(byte[] bytes, long seqNum, int checkpointMaxWrites) throws IOException {
+        this.pageIO.write(bytes, seqNum);
+
+        if (this.minSeqNum <= 0) {
+            this.minSeqNum = seqNum;
+            this.firstUnreadSeqNum = seqNum;
+        }
+        this.elementCount++;
+
+        // force a checkpoint if we wrote checkpointMaxWrites elements since last checkpoint
+        // the initial condition of an "empty" checkpoint, maxSeqNum() will return -1
+        if (checkpointMaxWrites > 0 && (seqNum >= this.lastCheckpoint.maxSeqNum() + checkpointMaxWrites)) {
+            // did we write more than checkpointMaxWrites elements? if so checkpoint now
+            checkpoint();
+        }
+    }
+
+    public void ensurePersistedUpto(long seqNum) throws IOException {
+        long lastCheckpointUptoSeqNum = this.lastCheckpoint.getMinSeqNum() + this.lastCheckpoint.getElementCount();
+
+        // if the last checkpoint for this headpage already included the given seqNum, no need to fsync/checkpoint
+        if (seqNum > lastCheckpointUptoSeqNum) {
+            // head page checkpoint does a data file fsync
+            checkpoint();
+        }
+    }
+
+    public TailPage behead() throws IOException {
+        checkpoint();
+
+        TailPage tailPage = new TailPage(this);
+
+        // first thing that must be done after beheading is to create a new checkpoint for that new tail page
+        // tail page checkpoint does NOT includes a fsync
+        tailPage.checkpoint();
+
+        // TODO: should we have a better deactivation strategy to avoid too rapid reactivation scenario?
+        Page firstUnreadPage = queue.firstUnreadPage();
+        if (firstUnreadPage == null || (tailPage.getPageNum() > firstUnreadPage.getPageNum())) {
+            // deactivate if this new tailPage is not where the read is occurring
+            tailPage.getPageIO().deactivate();
+        }
+
+        return tailPage;
+    }
+
+    // head page checkpoint, fsync data page if writes occured since last checkpoint
+    // update checkpoint only if it changed since lastCheckpoint
+    public void checkpoint() throws IOException {
+
+         if (this.elementCount > this.lastCheckpoint.getElementCount()) {
+             // fsync & checkpoint if data written since last checkpoint
+
+             this.pageIO.ensurePersisted();
+             forceCheckpoint();
+        } else {
+             Checkpoint checkpoint = new Checkpoint(this.pageNum, this.queue.firstUnackedPageNum(), firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+             if (! checkpoint.equals(this.lastCheckpoint)) {
+                 // checkpoint only if it changed since last checkpoint
+
+                 // non-dry code with forceCheckpoint() to avoid unnecessary extra new Checkpoint object creation
+                 CheckpointIO io = queue.getCheckpointIO();
+                 io.write(io.headFileName(), checkpoint);
+                 this.lastCheckpoint = checkpoint;
+             }
+         }
+      }
+
+    // unconditionally update head checkpoint
+    public void forceCheckpoint() throws IOException {
+        Checkpoint checkpoint = new Checkpoint(this.pageNum, this.queue.firstUnackedPageNum(), firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+        CheckpointIO io = queue.getCheckpointIO();
+        io.write(io.headFileName(), checkpoint);
+        this.lastCheckpoint = checkpoint;
+    }
+
+    public void close() throws IOException {
+        checkpoint();
+        this.pageIO.close();
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/MemorySettings.java b/logstash-core/src/main/java/org/logstash/ackedqueue/MemorySettings.java
new file mode 100644
index 00000000000..d0b5503c65c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/MemorySettings.java
@@ -0,0 +1,132 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+public class MemorySettings implements Settings {
+    private CheckpointIOFactory checkpointIOFactory;
+    private PageIOFactory pageIOFactory;
+    private Class elementClass;
+    private int capacity;
+    private long queueMaxBytes;
+    private final String dirPath;
+    private int maxUnread;
+    private int checkpointMaxAcks;
+    private int checkpointMaxWrites;
+    private int checkpointMaxInterval;
+
+    public MemorySettings() {
+        this("");
+    }
+
+    public MemorySettings(String dirPath) {
+        this.dirPath = dirPath;
+        this.maxUnread = 0;
+        this.checkpointMaxAcks = 1;
+        this.checkpointMaxWrites = 1;
+        this.checkpointMaxInterval = 0;
+    }
+
+    @Override
+    public Settings setCheckpointIOFactory(CheckpointIOFactory factory) {
+        this.checkpointIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementIOFactory(PageIOFactory factory) {
+        this.pageIOFactory = factory;
+        return this;
+    }
+
+    @Override
+    public Settings setElementClass(Class elementClass) {
+        this.elementClass = elementClass;
+        return this;
+    }
+
+    @Override
+    public Settings setCapacity(int capacity) {
+        this.capacity = capacity;
+        return this;
+    }
+
+    @Override
+    public Settings setQueueMaxBytes(long size) {
+        this.queueMaxBytes = size;
+        return this;
+    }
+
+    @Override
+    public Settings setMaxUnread(int maxUnread) {
+        this.maxUnread = maxUnread;
+        return this;
+    }
+
+    @Override
+    public Settings setCheckpointMaxAcks(int checkpointMaxAcks) {
+        this.checkpointMaxAcks = checkpointMaxAcks;
+        return this;
+    }
+
+    @Override
+    public Settings setCheckpointMaxWrites(int checkpointMaxWrites) {
+        this.checkpointMaxWrites = checkpointMaxWrites;
+        return this;
+    }
+
+    @Override
+    public Settings setCheckpointMaxInterval(int checkpointMaxInterval) {
+        this.checkpointMaxInterval = checkpointMaxInterval;
+        return this;
+    }
+
+    @Override
+    public int getCheckpointMaxAcks() {
+        return checkpointMaxAcks;
+    }
+
+    @Override
+    public int getCheckpointMaxWrites() {
+        return checkpointMaxWrites;
+    }
+
+    @Override
+    public int getCheckpointMaxInterval() {
+        return checkpointMaxInterval;
+    }
+
+    @Override
+    public CheckpointIOFactory getCheckpointIOFactory() {
+        return checkpointIOFactory;
+    }
+
+    public PageIOFactory getPageIOFactory() {
+        return pageIOFactory;
+    }
+
+    @Override
+    public Class getElementClass()  {
+        return this.elementClass;
+    }
+
+    @Override
+    public String getDirPath() {
+        return this.dirPath;
+    }
+
+    @Override
+    public long getQueueMaxBytes() {
+        return this.queueMaxBytes;
+    }
+
+    @Override
+    public int getCapacity() {
+        return this.capacity;
+    }
+
+    @Override
+    public int getMaxUnread() {
+        return this.maxUnread;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
new file mode 100644
index 00000000000..1560e78e391
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
@@ -0,0 +1,159 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.PageIO;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.BitSet;
+import java.util.List;
+import java.util.stream.Collectors;
+
+public abstract class Page implements Closeable {
+    protected final int pageNum;
+    protected long minSeqNum; // TODO: see if we can make it final?
+    protected int elementCount;
+    protected long firstUnreadSeqNum;
+    protected final Queue queue;
+    protected PageIO pageIO;
+
+    // bit 0 is minSeqNum
+    // TODO: go steal LocalCheckpointService in feature/seq_no from ES
+    // TODO: https://github.com/elastic/elasticsearch/blob/feature/seq_no/core/src/main/java/org/elasticsearch/index/seqno/LocalCheckpointService.java
+    protected BitSet ackedSeqNums;
+    protected Checkpoint lastCheckpoint;
+
+    public Page(int pageNum, Queue queue, long minSeqNum, int elementCount, long firstUnreadSeqNum, BitSet ackedSeqNums, PageIO pageIO) {
+        this.pageNum = pageNum;
+        this.queue = queue;
+
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+        this.firstUnreadSeqNum = firstUnreadSeqNum;
+        this.ackedSeqNums = ackedSeqNums;
+        this.lastCheckpoint = new Checkpoint(0, 0, 0, 0, 0);
+        this.pageIO = pageIO;
+    }
+
+    public String toString() {
+        return "pageNum=" + this.pageNum + ", minSeqNum=" + this.minSeqNum + ", elementCount=" + this.elementCount + ", firstUnreadSeqNum=" + this.firstUnreadSeqNum;
+    }
+
+    // NOTE:
+    // we have a page concern inconsistency where readBatch() takes care of the
+    // deserialization and returns a Batch object which contains the deserialized
+    // elements objects of the proper elementClass but HeadPage.write() deals with
+    // a serialized element byte[] and serialization is done at the Queue level to
+    // be able to use the Page.hasSpace() method with the serialized element byte size.
+    //
+    // @param limit the batch size limit
+    // @param elementClass the concrete element class for deserialization
+    // @return Batch batch of elements read when the number of elements can be <= limit
+    public Batch readBatch(int limit) throws IOException {
+
+        // first make sure this page is activated, activating previously activated is harmless
+        this.pageIO.activate();
+
+        SequencedList<byte[]> serialized = this.pageIO.read(this.firstUnreadSeqNum, limit);
+        List<Queueable> deserialized = serialized.getElements().stream().map(e -> this.queue.deserialize(e)).collect(Collectors.toList());
+
+        assert serialized.getSeqNums().get(0) == this.firstUnreadSeqNum :
+            String.format("firstUnreadSeqNum=%d != first result seqNum=%d", this.firstUnreadSeqNum, serialized.getSeqNums().get(0));
+
+        Batch batch = new Batch(deserialized, serialized.getSeqNums(), this.queue);
+
+        this.firstUnreadSeqNum += deserialized.size();
+
+        return batch;
+    }
+
+    public boolean isFullyRead() {
+        return unreadCount() <= 0;
+//        return this.elementCount <= 0 || this.firstUnreadSeqNum > maxSeqNum();
+    }
+
+    public boolean isFullyAcked() {
+        // TODO: it should be something similar to this when we use a proper bitset class like ES
+        // this.ackedSeqNum.firstUnackedBit >= this.elementCount;
+        // TODO: for now use a naive & inefficient mechanism with a simple Bitset
+        return this.elementCount > 0 && this.ackedSeqNums.cardinality() >= this.elementCount;
+    }
+
+    public long unreadCount() {
+        return this.elementCount <= 0 ? 0 : Math.max(0, (maxSeqNum() - this.firstUnreadSeqNum) + 1);
+    }
+
+    // update the page acking bitset. trigger checkpoint on the page if it is fully acked or if we acked more than the
+    // configured threshold checkpointMaxAcks.
+    // note that if the fully acked tail page is the first unacked page, it is not really necessary to also checkpoint
+    // the head page to update firstUnackedPageNum because it will be updated in the next upcoming head page checkpoint
+    // and in a crash condition, the Queue open recovery will detect and purge fully acked pages
+    //
+    // @param seqNums the list of same-page seqNums to ack
+    // @param checkpointMaxAcks the number of acks that will trigger a page checkpoint
+    public void ack(List<Long> seqNums, int checkpointMaxAcks) throws IOException {
+        for (long seqNum : seqNums) {
+            // TODO: eventually refactor to use new bit handling class
+
+            assert seqNum >= this.minSeqNum :
+                    String.format("seqNum=%d is smaller than minSeqnum=%d", seqNum, this.minSeqNum);
+
+            assert seqNum < this.minSeqNum + this.elementCount:
+                    String.format("seqNum=%d is greater than minSeqnum=%d + elementCount=%d = %d", seqNum, this.minSeqNum, this.elementCount, this.minSeqNum + this.elementCount);
+            int index = (int)(seqNum - this.minSeqNum);
+
+            this.ackedSeqNums.set(index);
+        }
+
+        // checkpoint if totally acked or we acked more than checkpointMaxAcks elements in this page since last checkpoint
+        // note that fully acked pages cleanup is done at queue level in Queue.ack()
+        long firstUnackedSeqNum = firstUnackedSeqNum();
+
+        if (isFullyAcked()) {
+            checkpoint();
+
+            assert firstUnackedSeqNum >= this.minSeqNum + this.elementCount - 1:
+                    String.format("invalid firstUnackedSeqNum=%d for minSeqNum=%d and elementCount=%d and cardinality=%d", firstUnackedSeqNum, this.minSeqNum, this.elementCount, this.ackedSeqNums.cardinality());
+
+        } else if (checkpointMaxAcks > 0 && (firstUnackedSeqNum >= this.lastCheckpoint.getFirstUnackedSeqNum() + checkpointMaxAcks)) {
+            // did we acked more than checkpointMaxAcks elements? if so checkpoint now
+            checkpoint();
+        }
+    }
+
+    public abstract void checkpoint() throws IOException;
+
+    public abstract void close() throws IOException;
+
+    public int getPageNum() {
+        return pageNum;
+    }
+
+    public long getMinSeqNum() {
+        return this.minSeqNum;
+    }
+
+    public int getElementCount() {
+        return elementCount;
+    }
+
+    public Queue getQueue() {
+        return queue;
+    }
+
+    public PageIO getPageIO() {
+        return pageIO;
+    }
+
+    protected long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+
+    protected long firstUnackedSeqNum() {
+        // TODO: eventually refactor to use new bithandling class
+        return this.ackedSeqNums.nextClearBit(0) + this.minSeqNum;
+    }
+
+    protected int firstUnackedPageNumFromQueue() {
+        return queue.firstUnackedPageNum();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
new file mode 100644
index 00000000000..cd961e38eaf
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -0,0 +1,714 @@
+package org.logstash.ackedqueue;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.FileLockFactory;
+import org.logstash.LockException;
+import org.logstash.ackedqueue.io.CheckpointIO;
+import org.logstash.ackedqueue.io.PageIO;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.lang.reflect.InvocationTargetException;
+import java.lang.reflect.Method;
+import java.nio.channels.FileLock;
+import java.nio.file.NoSuchFileException;
+import java.util.ArrayList;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Set;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+
+// TODO: Notes
+//
+// - time-based fsync
+//
+// - tragic errors handling
+//   - what errors cause whole queue to be broken
+//   - where to put try/catch for these errors
+
+
+public class Queue implements Closeable {
+    protected long seqNum;
+    protected HeadPage headPage;
+
+    // complete list of all non fully acked pages. note that exact sequentially by pageNum cannot be assumed
+    // because any fully acked page will be removed from this list potentially creating pageNum gaps in the list.
+    protected final List<TailPage> tailPages;
+
+    // this list serves the only purpose of quickly retrieving the first unread page, operation necessary on every read
+    // reads will simply remove the first page from the list when fully read and writes will append new pages upon beheading
+    protected final List<TailPage> unreadTailPages;
+
+    // checkpoints that were not purged in the acking code to keep contiguous checkpoint files
+    // regardless of the correcponding data file purge.
+    protected final Set<Integer> preservedCheckpoints;
+
+    protected volatile long unreadCount;
+    protected volatile long currentByteSize;
+
+    private final CheckpointIO checkpointIO;
+    private final PageIOFactory pageIOFactory;
+    private final int pageCapacity;
+    private final long maxBytes;
+    private final String dirPath;
+    private final int maxUnread;
+    private final int checkpointMaxAcks;
+    private final int checkpointMaxWrites;
+    private final int checkpointMaxInterval;
+
+    private final AtomicBoolean closed;
+
+    // deserialization
+    private final Class elementClass;
+    private final Method deserializeMethod;
+
+    // thread safety
+    private final Lock lock = new ReentrantLock();
+    private final Condition notFull  = lock.newCondition();
+    private final Condition notEmpty = lock.newCondition();
+
+    // exclusive dir access
+    private FileLock dirLock;
+    private final static String LOCK_NAME = ".lock";
+
+    private static final Logger logger = LogManager.getLogger(Queue.class);
+
+    public Queue(Settings settings) {
+        this(
+            settings.getDirPath(),
+            settings.getCapacity(),
+            settings.getQueueMaxBytes(),
+            settings.getCheckpointIOFactory().build(settings.getDirPath()),
+            settings.getPageIOFactory(),
+            settings.getElementClass(),
+            settings.getMaxUnread(),
+            settings.getCheckpointMaxWrites(),
+            settings.getCheckpointMaxAcks(),
+            settings.getCheckpointMaxInterval()
+        );
+    }
+
+    public Queue(String dirPath, int pageCapacity, long maxBytes, CheckpointIO checkpointIO, PageIOFactory pageIOFactory, Class elementClass, int maxUnread, int checkpointMaxWrites, int checkpointMaxAcks, int checkpointMaxInterval) {
+        this.dirPath = dirPath;
+        this.pageCapacity = pageCapacity;
+        this.maxBytes = maxBytes;
+        this.checkpointIO = checkpointIO;
+        this.pageIOFactory = pageIOFactory;
+        this.elementClass = elementClass;
+        this.tailPages = new ArrayList<>();
+        this.unreadTailPages = new ArrayList<>();
+        this.preservedCheckpoints = new HashSet<>();
+        this.closed = new AtomicBoolean(true); // not yet opened
+        this.maxUnread = maxUnread;
+        this.checkpointMaxAcks = checkpointMaxAcks;
+        this.checkpointMaxWrites = checkpointMaxWrites;
+        this.checkpointMaxInterval = checkpointMaxInterval;
+        this.unreadCount = 0;
+        this.currentByteSize = 0;
+
+        // retrieve the deserialize method
+        try {
+            Class[] cArg = new Class[1];
+            cArg[0] = byte[].class;
+            this.deserializeMethod = this.elementClass.getDeclaredMethod("deserialize", cArg);
+        } catch (NoSuchMethodException e) {
+            throw new QueueRuntimeException("cannot find deserialize method on class " + this.elementClass.getName(), e);
+        }
+    }
+
+    public String getDirPath() {
+        return this.dirPath;
+    }
+
+    public long getMaxBytes() {
+        return this.maxBytes;
+    }
+
+    public long getMaxUnread() {
+        return this.maxUnread;
+    }
+
+    public long getCurrentByteSize() {
+        return this.currentByteSize;
+    }
+
+    public int getPageCapacity() {
+        return this.pageCapacity;
+    }
+
+    public long getUnreadCount() {
+        return this.unreadCount;
+    }
+
+    // moved queue opening logic in open() method until we have something in place to used in-memory checkpoints for testing
+    // because for now we need to pass a Queue instance to the Page and we don't want to trigger a Queue recovery when
+    // testing Page
+    public void open() throws IOException {
+        final int headPageNum;
+
+        if (!this.closed.get()) { throw new IOException("queue already opened"); }
+
+        lock.lock();
+        try {
+            // verify exclusive access to the dirPath
+            this.dirLock = FileLockFactory.getDefault().obtainLock(this.dirPath, LOCK_NAME);
+
+            Checkpoint headCheckpoint;
+            try {
+                headCheckpoint = this.checkpointIO.read(checkpointIO.headFileName());
+            } catch (NoSuchFileException e) {
+                // if there is no head checkpoint, create a new headpage and checkpoint it and exit method
+
+                logger.debug("No head checkpoint found at: {}, creating new head page", checkpointIO.headFileName());
+
+                this.seqNum = 0;
+                headPageNum = 0;
+
+                newCheckpointedHeadpage(headPageNum);
+                this.closed.set(false);
+
+                return;
+            }
+
+            // at this point we have a head checkpoint to figure queue recovery
+
+            // reconstruct all tail pages state upto but excluding the head page
+            for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
+
+                // all tail checkpoints in the sequence should exist, if not abort mission with a NoSuchFileException
+                Checkpoint cp = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));
+
+                logger.debug("opening tail page: {}, in: {}, with checkpoint: {}", pageNum, this.dirPath, cp.toString());
+
+                PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
+                pageIO.open(cp.getMinSeqNum(), cp.getElementCount());
+
+                add(cp, new TailPage(cp, this, pageIO));
+            }
+
+            // transform the head page into a tail page only if the headpage is non-empty
+            // in both cases it will be checkpointed to track any changes in the firstUnackedPageNum when reconstructing the tail pages
+
+            logger.debug("opening head page: {}, in: {}, with checkpoint: {}", headCheckpoint.getPageNum(), this.dirPath, headCheckpoint.toString());
+
+            PageIO pageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
+            pageIO.recover(); // optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data
+
+            if (pageIO.getMinSeqNum() != headCheckpoint.getMinSeqNum() || pageIO.getElementCount() != headCheckpoint.getElementCount()) {
+                // the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes
+
+                logger.warn("recovered head data page {} is different than checkpoint, using recovered page information", headCheckpoint.getPageNum());
+                logger.debug("head checkpoint minSeqNum={} or elementCount={} is different than head pageIO minSeqNum={} or elementCount={}", headCheckpoint.getMinSeqNum(), headCheckpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
+
+                long firstUnackedSeqNum = headCheckpoint.getFirstUnackedSeqNum();
+                if (firstUnackedSeqNum < pageIO.getMinSeqNum()) {
+                    logger.debug("head checkpoint firstUnackedSeqNum={} is < head pageIO minSeqNum={}, using pageIO minSeqNum", firstUnackedSeqNum, pageIO.getMinSeqNum());
+                    firstUnackedSeqNum = pageIO.getMinSeqNum();
+                }
+                headCheckpoint = new Checkpoint(headCheckpoint.getPageNum(), headCheckpoint.getFirstUnackedPageNum(), firstUnackedSeqNum, pageIO.getMinSeqNum(), pageIO.getElementCount());
+            }
+            this.headPage = new HeadPage(headCheckpoint, this, pageIO);
+
+            if (this.headPage.getMinSeqNum() <= 0 && this.headPage.getElementCount() <= 0) {
+                // head page is empty, let's keep it as-is
+
+                this.currentByteSize += pageIO.getCapacity();
+
+                // but checkpoint it to update the firstUnackedPageNum if it changed
+                this.headPage.checkpoint();
+            } else {
+                // head page is non-empty, transform it into a tail page and create a new empty head page
+                add(headCheckpoint, this.headPage.behead());
+
+                headPageNum = headCheckpoint.getPageNum() + 1;
+                newCheckpointedHeadpage(headPageNum);
+
+                // track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum
+                if (headCheckpoint.maxSeqNum() > this.seqNum) {
+                    this.seqNum = headCheckpoint.maxSeqNum();
+                }
+            }
+
+            // only activate the first tail page
+            if (tailPages.size() > 0) {
+                this.tailPages.get(0).getPageIO().activate();
+            }
+
+            // TODO: here do directory traversal and cleanup lingering pages? could be a background operations to not delay queue start?
+
+            this.closed.set(false);
+        } catch (LockException e) {
+            throw new LockException("The queue failed to obtain exclusive access, cause: " + e.getMessage());
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // add a read tail page into this queue structures but also verify that this tail page
+    // is not fully acked in which case it will be purged
+    private void add(Checkpoint checkpoint, TailPage page) throws IOException {
+        if (checkpoint.isFullyAcked()) {
+            // first make sure any fully acked page per the checkpoint is purged if not already
+            try { page.getPageIO().purge(); } catch (NoSuchFileException e) { /* ignore */ }
+
+            // we want to keep all the "middle" checkpoints between the first unacked tail page and the head page
+            // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
+            // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first
+            // unacked tail page and the head page. we did however purge the data file to free disk resources.
+
+            if (this.tailPages.size() == 0) {
+                // this is the first tail page and it is fully acked so just purge it
+                this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));
+            } else {
+                // create a tail page with a null PageIO and add it to tail pages but not unreadTailPages
+                // since it is fully read because also fully acked
+                // TODO: I don't like this null pageIO tail page...
+                this.tailPages.add(new TailPage(checkpoint, this, null));
+            }
+        } else {
+            this.tailPages.add(page);
+            this.unreadTailPages.add(page);
+            this.unreadCount += page.unreadCount();
+            this.currentByteSize += page.getPageIO().getCapacity();
+
+            // for now deactivate all tail pages, we will only reactivate the first one at the end
+            page.getPageIO().deactivate();
+        }
+
+        // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
+        if (checkpoint.maxSeqNum() > this.seqNum) {
+            this.seqNum = checkpoint.maxSeqNum();
+        }
+    }
+
+    // create a new empty headpage for the given pageNum and immediately checkpoint it
+    // @param pageNum the page number of the new head page
+    private void newCheckpointedHeadpage(int pageNum) throws IOException {
+        PageIO headPageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
+        headPageIO.create();
+        this.headPage = new HeadPage(pageNum, this, headPageIO);
+        this.headPage.forceCheckpoint();
+        this.currentByteSize += headPageIO.getCapacity();
+    }
+
+    // @param element the Queueable object to write to the queue
+    // @return long written sequence number
+    public long write(Queueable element) throws IOException {
+        byte[] data = element.serialize();
+
+        if (! this.headPage.hasCapacity(data.length)) {
+            throw new IOException("data to be written is bigger than page capacity");
+        }
+
+        // the write strategy with regard to the isFull() state is to assume there is space for this element
+        // and write it, then after write verify if we just filled the queue and wait on the notFull condition
+        // *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case
+        // holding an element in memory while waiting for the notFull condition would mean always having the current write
+        // element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible
+        // to write the current element.
+
+        lock.lock();
+        try {
+
+            // create a new head page if the current does not have sufficient space left for data to be written
+            if (! this.headPage.hasSpace(data.length)) {
+
+                // TODO: verify queue state integrity WRT Queue.open()/recover() at each step of this process
+
+                int newHeadPageNum = this.headPage.pageNum + 1;
+
+                if (this.headPage.isFullyAcked()) {
+                    // purge the old headPage because its full and fully acked
+                    // there is no checkpoint file to purge since just creating a new TailPage from a HeadPage does
+                    // not trigger a checkpoint creation in itself
+                    TailPage tailPage = new TailPage(this.headPage);
+                    tailPage.purge();
+                } else {
+                    // beheading includes checkpoint+fsync if required
+                    TailPage tailPage = this.headPage.behead();
+
+                    this.tailPages.add(tailPage);
+                    if (! tailPage.isFullyRead()) {
+                        this.unreadTailPages.add(tailPage);
+                    }
+                }
+
+                // create new head page
+                newCheckpointedHeadpage(newHeadPageNum);
+            }
+
+            long seqNum = nextSeqNum();
+            this.headPage.write(data, seqNum, this.checkpointMaxWrites);
+            this.unreadCount++;
+            
+            notEmpty.signal();
+
+            // now check if we reached a queue full state and block here until it is not full
+            // for the next write or the queue was closed.
+            while (isFull() && !isClosed()) {
+                try {
+                    notFull.await();
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to return normally and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // this is a bit tricky in the case of the queue full condition blocking state.
+                    // TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return seqNum;
+                }
+            }
+
+            return seqNum;
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // @return true if the queue is deemed at full capacity
+    public boolean isFull() {
+        // TODO: I am not sure if having unreadCount as volatile is sufficient here. all unreadCount updates are done inside synchronized
+        // TODO: sections, I believe that to only read the value here, having it as volatile is sufficient?
+        if ((this.maxBytes > 0) && this.currentByteSize >= this.maxBytes) {
+            return true;
+        } else {
+            return ((this.maxUnread > 0) && this.unreadCount >= this.maxUnread);
+        }
+    }
+
+    // @return true if the queue is fully acked, which implies that it is fully read which works as an "empty" state.
+    public boolean isFullyAcked() {
+        lock.lock();
+        try {
+            return this.tailPages.isEmpty() ? this.headPage.isFullyAcked() : false;
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
+    public void ensurePersistedUpto(long seqNum) throws IOException{
+        lock.lock();
+        try {
+            this.headPage.ensurePersistedUpto(seqNum);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // non-blockin queue read
+    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
+    // @return Batch the batch containing 1 or more element up to the required limit or null of no elements were available
+    public Batch nonBlockReadBatch(int limit) throws IOException {
+        lock.lock();
+        try {
+            Page p = firstUnreadPage();
+            return (p == null) ? null : _readPageBatch(p, limit);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // blocking readBatch notes:
+    //   the queue close() notifies all pending blocking read so that they unblock if the queue is being closed.
+    //   this means that all blocking read methods need to verify for the queue close condition.
+    //
+    // blocking queue read until elements are available for read
+    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
+    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
+    public Batch readBatch(int limit) throws IOException {
+        Page p;
+
+        lock.lock();
+        try {
+            while ((p = firstUnreadPage()) == null && !isClosed()) {
+                try {
+                    notEmpty.await();
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to simply return null and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return null;
+                }
+            }
+
+            // need to check for close since it is a condition for exiting the while loop
+            return (isClosed()) ? null : _readPageBatch(p, limit);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    // blocking queue read until elements are available for read or the given timeout is reached.
+    // @param limit read the next batch of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
+    // @param timeout the maximum time to wait in milliseconds
+    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
+    public Batch readBatch(int limit, long timeout) throws IOException {
+        Page p;
+
+        lock.lock();
+        try {
+            // wait only if queue is empty
+            if ((p = firstUnreadPage()) == null) {
+                try {
+                    notEmpty.await(timeout, TimeUnit.MILLISECONDS);
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to simply return null and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return null;
+                }
+
+                // if after returning from wait queue is still empty, or the queue was closed return null
+                if ((p = firstUnreadPage()) == null || isClosed()) { return null; }
+            }
+
+            return _readPageBatch(p, limit);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    private Batch _readPageBatch(Page p, int limit) throws IOException {
+        boolean wasFull = isFull();
+
+        Batch b = p.readBatch(limit);
+        this.unreadCount -= b.size();
+
+        if (p.isFullyRead()) { removeUnreadPage(p); }
+        if (wasFull) { notFull.signalAll(); }
+
+        return b;
+    }
+
+    private static class TailPageResult {
+        public TailPage page;
+        public int index;
+
+        public TailPageResult(TailPage page, int index) {
+            this.page = page;
+            this.index = index;
+        }
+    }
+
+    // perform a binary search through tail pages to find in which page this seqNum falls into
+    private TailPageResult binaryFindPageForSeqnum(long seqNum) {
+        int lo = 0;
+        int hi = this.tailPages.size() - 1;
+        while (lo <= hi) {
+            int mid = lo + (hi - lo) / 2;
+            TailPage p = this.tailPages.get(mid);
+
+            if (seqNum < p.getMinSeqNum()) {
+                hi = mid - 1;
+            } else if (seqNum >= (p.getMinSeqNum() + p.getElementCount())) {
+                lo = mid + 1;
+            } else {
+                return new TailPageResult(p, mid);
+            }
+        }
+        return null;
+    }
+
+    // perform a linear search through tail pages to find in which page this seqNum falls into
+    private TailPageResult linearFindPageForSeqnum(long seqNum) {
+        for (int i = 0; i < this.tailPages.size(); i++) {
+            TailPage p = this.tailPages.get(i);
+            if (p.getMinSeqNum() > 0 && seqNum >= p.getMinSeqNum() && seqNum < p.getMinSeqNum() + p.getElementCount()) {
+                return new TailPageResult(p, i);
+            }
+        }
+        return null;
+    }
+
+    // ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from
+    // same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks
+    // acks since last checkpoint it will also trigger a checkpoint.
+    // @param seqNums the list of same-page sequence numbers to ack
+    public void ack(List<Long> seqNums) throws IOException {
+        // as a first implementation we assume that all batches are created from the same page
+        // so we will avoid multi pages acking here for now
+
+        // find the page to ack by traversing from oldest tail page
+        long firstAckSeqNum = seqNums.get(0);
+
+        lock.lock();
+        try {
+            TailPageResult result = null;
+
+            if (this.tailPages.size() > 0) {
+                // short-circuit: first check in the first tail page as it is the most likely page where acking will happen
+                TailPage p = this.tailPages.get(0);
+                if (p.getMinSeqNum() > 0 && firstAckSeqNum >= p.getMinSeqNum() && firstAckSeqNum < p.getMinSeqNum() + p.getElementCount()) {
+                    result = new TailPageResult(p, 0);
+                } else {
+                    // dual search strategy: if few tail pages search linearly otherwise perform binary search
+                    result = (this.tailPages.size() > 3) ? binaryFindPageForSeqnum(firstAckSeqNum) : linearFindPageForSeqnum(firstAckSeqNum);
+                }
+            }
+
+            if (result == null) {
+                // if not found then it is in head page
+                assert this.headPage.getMinSeqNum() > 0 && firstAckSeqNum >= this.headPage.getMinSeqNum() && firstAckSeqNum < this.headPage.getMinSeqNum() + this.headPage.getElementCount():
+                        String.format("seqNum=%d is not in head page with minSeqNum=%d", firstAckSeqNum, this.headPage.getMinSeqNum());
+
+                // page acking checkpoints fully acked pages
+                this.headPage.ack(seqNums, this.checkpointMaxAcks);
+            } else {
+                // page acking also checkpoints fully acked pages or upon reaching the checkpointMaxAcks threshold
+                result.page.ack(seqNums, this.checkpointMaxAcks);
+
+                // cleanup fully acked tail page
+                if (result.page.isFullyAcked()) {
+                    boolean wasFull = isFull();
+
+                    this.tailPages.remove(result.index);
+
+                    // remove page data file regardless if it is the first or a middle tail page to free resources
+                    result.page.purge();
+                    this.currentByteSize -= result.page.getPageIO().getCapacity();
+
+                    if (result.index != 0) {
+                        // this an in-between page, we don't purge it's checkpoint to preserve checkpoints sequence on disk
+                        // save that checkpoint so that if it becomes the first checkpoint it can be purged later on.
+                        this.preservedCheckpoints.add(result.page.getPageNum());
+                    } else {
+                        // if this is the first page also remove checkpoint file
+                        this.checkpointIO.purge(this.checkpointIO.tailFileName(result.page.getPageNum()));
+
+                        // check if there are preserved checkpoints file next to this one and delete them
+                        int nextPageNum = result.page.getPageNum() + 1;
+                        while (preservedCheckpoints.remove(nextPageNum)) {
+                            this.checkpointIO.purge(this.checkpointIO.tailFileName(nextPageNum));
+                            nextPageNum++;
+                        }
+                    }
+
+                    if (wasFull) { notFull.signalAll(); }
+                }
+
+                this.headPage.checkpoint();
+            }
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    public CheckpointIO getCheckpointIO() {
+        return this.checkpointIO;
+    }
+
+    // deserialize a byte array into the required element class.
+    // @param bytes the byte array to deserialize
+    // @return Queueable the deserialized byte array into the required Queueable interface implementation concrete class
+    public Queueable deserialize(byte[] bytes) {
+        try {
+            return (Queueable)this.deserializeMethod.invoke(this.elementClass, bytes);
+        } catch (IllegalAccessException|InvocationTargetException e) {
+            throw new QueueRuntimeException("deserialize invocation error", e);
+        }
+    }
+
+    public void close() throws IOException {
+        // TODO: review close strategy and exception handling and resiliency of first closing tail pages if crash in the middle
+
+        if (closed.getAndSet(true) == false) {
+            lock.lock();
+            try {
+                // TODO: not sure if we need to do this here since the headpage close will also call ensurePersisted
+                ensurePersistedUpto(this.seqNum);
+
+                for (TailPage p : this.tailPages) { p.close(); }
+                this.headPage.close();
+
+                // release all referenced objects
+                this.tailPages.clear();
+                this.unreadTailPages.clear();
+                this.headPage = null;
+
+                // unblock blocked reads which will return null by checking of isClosed()
+                // no data will be lost because the actual read has not been performed
+                notEmpty.signalAll();
+
+
+                // unblock blocked writes. a write is blocked *after* the write has been performed so
+                // unblocking is safe and will return from the write call
+                notFull.signalAll();
+
+            } finally {
+                try {
+                    FileLockFactory.getDefault().releaseLock(this.dirLock);
+                } catch (IOException e) {
+                    // log error and ignore
+                    logger.error("Queue close releaseLock failed, error={}", e.getMessage());
+                } finally {
+                    lock.unlock();
+                }
+            }
+        }
+    }
+
+    protected Page firstUnreadPage() throws IOException {
+        // look at head page if no unreadTailPages
+        return (this.unreadTailPages.isEmpty()) ? (this.headPage.isFullyRead() ? null : this.headPage) : this.unreadTailPages.get(0);
+    }
+
+    private void removeUnreadPage(Page p) {
+        // HeadPage is not part of the unreadTailPages, just ignore
+        if (p instanceof TailPage){
+            // the page to remove should always be the first one
+            assert this.unreadTailPages.get(0) == p : String.format("unread page is not first in unreadTailPages list");
+            this.unreadTailPages.remove(0);
+        }
+    }
+
+    protected int firstUnackedPageNum() {
+        if (this.tailPages.isEmpty()) {
+            return this.headPage.getPageNum();
+        }
+        return this.tailPages.get(0).getPageNum();
+    }
+
+    public long getAckedCount() {
+        return headPage.ackedSeqNums.cardinality() + tailPages.stream()
+                .mapToLong(page -> page.ackedSeqNums.cardinality())
+                .sum();
+    }
+
+    public long getUnackedCount() {
+        long headPageCount = (headPage.getElementCount() - headPage.ackedSeqNums.cardinality());
+        long tailPagesCount = tailPages.stream()
+                .mapToLong(page -> (page.getElementCount() - page.ackedSeqNums.cardinality())).sum();
+        return headPageCount + tailPagesCount;
+    }
+
+    protected long nextSeqNum() {
+        return this.seqNum += 1;
+    }
+
+    protected boolean isClosed() {
+        return this.closed.get();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java
new file mode 100644
index 00000000000..06b8639d5b0
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java
@@ -0,0 +1,29 @@
+package org.logstash.ackedqueue;
+
+public class QueueRuntimeException extends RuntimeException {
+
+    public static QueueRuntimeException newFormatMessage(String fmt, Object... args) {
+        return new QueueRuntimeException(
+                String.format(fmt, args)
+        );
+    }
+
+    public QueueRuntimeException() {
+    }
+
+    public QueueRuntimeException(String message) {
+        super(message);
+    }
+
+    public QueueRuntimeException(String message, Throwable cause) {
+        super(message, cause);
+    }
+
+    public QueueRuntimeException(Throwable cause) {
+        super(cause);
+    }
+
+    public QueueRuntimeException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) {
+        super(message, cause, enableSuppression, writableStackTrace);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queueable.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queueable.java
new file mode 100644
index 00000000000..2becec11d3b
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queueable.java
@@ -0,0 +1,10 @@
+package org.logstash.ackedqueue;
+
+import java.io.IOException;
+
+public interface Queueable {
+
+    byte[] serialize() throws IOException;
+
+    static Object deserialize(byte[] bytes) { throw new RuntimeException("please implement deserialize"); };
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/SequencedList.java b/logstash-core/src/main/java/org/logstash/ackedqueue/SequencedList.java
new file mode 100644
index 00000000000..8bb580fe053
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/SequencedList.java
@@ -0,0 +1,21 @@
+package org.logstash.ackedqueue;
+
+import java.util.List;
+
+public class SequencedList<E> {
+    private final List<E> elements;
+    private final List<Long> seqNums;
+
+    public SequencedList(List<E> elements, List<Long> seqNums) {
+        this.elements = elements;
+        this.seqNums = seqNums;
+    }
+
+    public List<E> getElements() {
+        return elements;
+    }
+
+    public List<Long> getSeqNums() {
+        return seqNums;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java
new file mode 100644
index 00000000000..38635845991
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java
@@ -0,0 +1,44 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+public interface Settings {
+    Settings setCheckpointIOFactory(CheckpointIOFactory factory);
+
+    Settings setElementIOFactory(PageIOFactory factory);
+
+    Settings setElementClass(Class elementClass);
+
+    Settings setCapacity(int capacity);
+
+    Settings setQueueMaxBytes(long size);
+
+    Settings setMaxUnread(int maxUnread);
+
+    Settings setCheckpointMaxAcks(int checkpointMaxAcks);
+
+    Settings setCheckpointMaxWrites(int checkpointMaxWrites);
+
+    Settings setCheckpointMaxInterval(int checkpointMaxInterval);
+
+    CheckpointIOFactory getCheckpointIOFactory();
+
+    PageIOFactory getPageIOFactory();
+
+    Class getElementClass();
+
+    String getDirPath();
+
+    int getCapacity();
+
+    long getQueueMaxBytes();
+
+    int getMaxUnread();
+
+    int getCheckpointMaxAcks();
+
+    int getCheckpointMaxWrites();
+
+    int getCheckpointMaxInterval();
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java
new file mode 100644
index 00000000000..29e62ba607a
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java
@@ -0,0 +1,48 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.CheckpointIO;
+import org.logstash.ackedqueue.io.PageIO;
+
+import java.io.IOException;
+import java.util.BitSet;
+
+public class TailPage extends Page {
+
+    // create a new TailPage object from a HeadPage object
+    public TailPage(HeadPage page) {
+        super(page.pageNum, page.queue, page.minSeqNum, page.elementCount, page.firstUnreadSeqNum, page.ackedSeqNums, page.pageIO);
+    }
+
+    // create a new TailPage object for an exiting Checkpoint and data file
+    // @param pageIO the PageIO object is expected to be open/recover/create
+    public TailPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
+        super(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO);
+
+        // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
+        if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
+            this.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
+        }
+    }
+
+    public void checkpoint() throws IOException {
+        // TODO: not concurrent for first iteration:
+
+        // since this is a tail page and no write can happen in this page, there is no point in performing a fsync on this page, just stamp checkpoint
+        CheckpointIO io = queue.getCheckpointIO();
+        this.lastCheckpoint = io.write(io.tailFileName(this.pageNum), this.pageNum, 0, firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+    }
+
+    // delete all IO files associated with this page
+    public void purge() throws IOException {
+        if (this.pageIO != null) {
+            this.pageIO.purge(); // page IO purge calls close
+        }
+    }
+
+    public void close() throws IOException {
+        checkpoint();
+        if (this.pageIO != null) {
+            this.pageIO.close();
+        }
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java
new file mode 100644
index 00000000000..cd858b5faa5
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java
@@ -0,0 +1,88 @@
+package org.logstash.ackedqueue.ext;
+
+import org.jruby.Ruby;
+import org.jruby.RubyClass;
+import org.jruby.RubyModule;
+import org.jruby.RubyObject;
+import org.jruby.RubyArray;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.logstash.ackedqueue.Batch;
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+
+import java.io.IOException;
+
+public class JrubyAckedBatchExtLibrary implements Library {
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("AckedBatch", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyAckedBatch(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.defineAnnotatedMethods(RubyAckedBatch.class);
+    }
+
+    @JRubyClass(name = "AckedBatch", parent = "Object")
+    public static class RubyAckedBatch extends RubyObject {
+        private Batch batch;
+
+        public RubyAckedBatch(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+            this.batch = null;
+        }
+
+        public RubyAckedBatch(Ruby runtime, Batch batch) {
+            super(runtime, runtime.getModule("LogStash").getClass("AckedBatch"));
+            this.batch = batch;
+        }
+
+        @SuppressWarnings("unchecked") // for the getList() calls
+        @JRubyMethod(name = "initialize", required = 3)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject events,  IRubyObject seqNums,  IRubyObject queue)
+        {
+            if (! (events instanceof RubyArray)) {
+                context.runtime.newArgumentError("expected events array");
+            }
+            if (! (seqNums instanceof RubyArray)) {
+                context.runtime.newArgumentError("expected seqNums array");
+            }
+            if (! (queue instanceof JrubyAckedQueueExtLibrary.RubyAckedQueue)) {
+                context.runtime.newArgumentError("expected queue AckedQueue");
+            }
+
+            this.batch = new Batch(((RubyArray)events).getList(), ((RubyArray)seqNums).getList(), ((JrubyAckedQueueExtLibrary.RubyAckedQueue)queue).getQueue());
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "get_elements")
+        public IRubyObject ruby_get_elements(ThreadContext context)
+        {
+            RubyArray result = context.runtime.newArray();
+            this.batch.getElements().forEach(e -> result.add(new JrubyEventExtLibrary.RubyEvent(context.runtime, (Event)e)));
+
+            return result;
+        }
+
+        @JRubyMethod(name = "close")
+        public IRubyObject ruby_close(ThreadContext context)
+        {
+            try {
+                this.batch.close();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
new file mode 100644
index 00000000000..dbf7c8e22ea
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
@@ -0,0 +1,192 @@
+package org.logstash.ackedqueue.ext;
+
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.RubyClass;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyModule;
+import org.jruby.RubyObject;
+import org.jruby.RubyBoolean;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.runtime.Arity;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.logstash.ackedqueue.Batch;
+import org.logstash.ackedqueue.FileSettings;
+import org.logstash.ackedqueue.Queue;
+import org.logstash.ackedqueue.Settings;
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.FileCheckpointIO;
+import org.logstash.ackedqueue.io.MmapPageIO;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+import java.io.IOException;
+
+public class JrubyAckedQueueExtLibrary implements Library {
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("AckedQueue", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyAckedQueue(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.defineAnnotatedMethods(RubyAckedQueue.class);
+    }
+
+    // TODO:
+    // as a simplified first prototyping implementation, the Settings class is not exposed and the queue elements
+    // are assumed to be logstash Event.
+
+    @JRubyClass(name = "AckedQueue", parent = "Object")
+    public static class RubyAckedQueue extends RubyObject {
+        private Queue queue;
+
+        public RubyAckedQueue(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+        }
+
+        public Queue getQueue() {
+            return this.queue;
+        }
+
+        // def initialize
+        @JRubyMethod(name = "initialize", optional = 7)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
+        {
+            args = Arity.scanArgs(context.runtime, args, 7, 0);
+
+            int capacity = RubyFixnum.num2int(args[1]);
+            int maxUnread = RubyFixnum.num2int(args[2]);
+            int checkpointMaxAcks = RubyFixnum.num2int(args[3]);
+            int checkpointMaxWrites = RubyFixnum.num2int(args[4]);
+            int checkpointMaxInterval = RubyFixnum.num2int(args[5]);
+            long queueMaxBytes = RubyFixnum.num2long(args[6]);
+
+            Settings s = new FileSettings(args[0].asJavaString());
+            PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
+            CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
+            s.setCapacity(capacity);
+            s.setMaxUnread(maxUnread);
+            s.setQueueMaxBytes(queueMaxBytes);
+            s.setCheckpointMaxAcks(checkpointMaxAcks);
+            s.setCheckpointMaxWrites(checkpointMaxWrites);
+            s.setCheckpointMaxInterval(checkpointMaxInterval);
+            s.setElementIOFactory(pageIOFactory);
+            s.setCheckpointIOFactory(checkpointIOFactory);
+            s.setElementClass(Event.class);
+
+            this.queue = new Queue(s);
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "max_unread_events")
+        public IRubyObject ruby_max_unread_events(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxUnread());
+        }
+
+        @JRubyMethod(name = "max_size_in_bytes")
+        public IRubyObject ruby_max_size_in_bytes(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxBytes());
+        }
+
+        @JRubyMethod(name = "page_capacity")
+        public IRubyObject ruby_page_capacity(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getPageCapacity());
+        }
+
+        @JRubyMethod(name = "dir_path")
+        public IRubyObject ruby_dir_path(ThreadContext context) {
+            return context.runtime.newString(queue.getDirPath());
+        }
+
+        @JRubyMethod(name = "current_byte_size")
+        public IRubyObject ruby_current_byte_size(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getCurrentByteSize());
+        }
+
+        @JRubyMethod(name = "acked_count")
+        public IRubyObject ruby_acked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getAckedCount());
+        }
+
+        @JRubyMethod(name = "unacked_count")
+        public IRubyObject ruby_unacked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnackedCount());
+        }
+
+        @JRubyMethod(name = "unread_count")
+        public IRubyObject ruby_unread_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnreadCount());
+        }
+
+        @JRubyMethod(name = "open")
+        public IRubyObject ruby_open(ThreadContext context)
+        {
+            try {
+                this.queue.open();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = {"write", "<<"}, required = 1)
+        public IRubyObject ruby_write(ThreadContext context, IRubyObject event)
+        {
+            if (!(event instanceof JrubyEventExtLibrary.RubyEvent)) {
+                throw context.runtime.newTypeError("wrong argument type " + event.getMetaClass() + " (expected LogStash::Event)");
+            }
+
+            long seqNum;
+            try {
+                seqNum = this.queue.write(((JrubyEventExtLibrary.RubyEvent) event).getEvent());
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.runtime.newFixnum(seqNum);
+        }
+
+        @JRubyMethod(name = "read_batch", required = 2)
+        public IRubyObject ruby_read_batch(ThreadContext context, IRubyObject limit, IRubyObject timeout)
+        {
+            Batch b;
+
+            try {
+                b = this.queue.readBatch(RubyFixnum.num2int(limit), RubyFixnum.num2int(timeout));
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            // TODO: return proper Batch object
+            return (b == null) ? context.nil : new JrubyAckedBatchExtLibrary.RubyAckedBatch(context.runtime, b);
+        }
+
+        @JRubyMethod(name = "is_fully_acked?")
+        public IRubyObject ruby_is_fully_acked(ThreadContext context)
+        {
+            return RubyBoolean.newBoolean(context.runtime, this.queue.isFullyAcked());
+        }
+
+        @JRubyMethod(name = "close")
+        public IRubyObject ruby_close(ThreadContext context)
+        {
+            try {
+                this.queue.close();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
new file mode 100644
index 00000000000..25fb10aca6a
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
@@ -0,0 +1,189 @@
+package org.logstash.ackedqueue.ext;
+
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.RubyClass;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyModule;
+import org.jruby.RubyObject;
+import org.jruby.RubyBoolean;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.runtime.Arity;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.logstash.ackedqueue.Batch;
+import org.logstash.ackedqueue.MemorySettings;
+import org.logstash.ackedqueue.Queue;
+import org.logstash.ackedqueue.Settings;
+import org.logstash.ackedqueue.io.ByteBufferPageIO;
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.MemoryCheckpointIO;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+import java.io.IOException;
+
+public class JrubyAckedQueueMemoryExtLibrary implements Library {
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("AckedMemoryQueue", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyAckedMemoryQueue(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.defineAnnotatedMethods(RubyAckedMemoryQueue.class);
+    }
+
+    // TODO:
+    // as a simplified first prototyping implementation, the Settings class is not exposed and the queue elements
+    // are assumed to be logstash Event.
+
+
+    @JRubyClass(name = "AckedMemoryQueue", parent = "Object")
+    public static class RubyAckedMemoryQueue extends RubyObject {
+        private Queue queue;
+
+        public RubyAckedMemoryQueue(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+        }
+
+        public Queue getQueue() {
+            return this.queue;
+        }
+
+        // def initialize
+        @JRubyMethod(name = "initialize", optional = 4)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
+        {
+            args = Arity.scanArgs(context.runtime, args, 4, 0);
+
+            int capacity = RubyFixnum.num2int(args[1]);
+            int maxUnread = RubyFixnum.num2int(args[2]);
+            long queueMaxBytes = RubyFixnum.num2long(args[3]);
+
+            Settings s = new MemorySettings(args[0].asJavaString());
+            PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+            CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
+            s.setCapacity(capacity);
+            s.setMaxUnread(maxUnread);
+            s.setQueueMaxBytes(queueMaxBytes);
+            s.setElementIOFactory(pageIOFactory);
+            s.setCheckpointIOFactory(checkpointIOFactory);
+            s.setElementClass(Event.class);
+
+            this.queue = new Queue(s);
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "max_unread_events")
+        public IRubyObject ruby_max_unread_events(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxUnread());
+        }
+
+        @JRubyMethod(name = "max_size_in_bytes")
+        public IRubyObject ruby_max_size_in_bytes(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxBytes());
+        }
+
+        @JRubyMethod(name = "page_capacity")
+        public IRubyObject ruby_page_capacity(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getPageCapacity());
+        }
+
+        @JRubyMethod(name = "dir_path")
+        public IRubyObject ruby_dir_path(ThreadContext context) {
+            return context.runtime.newString(queue.getDirPath());
+        }
+
+        @JRubyMethod(name = "current_byte_size")
+        public IRubyObject ruby_current_byte_size(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getCurrentByteSize());
+        }
+
+        @JRubyMethod(name = "acked_count")
+        public IRubyObject ruby_acked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getAckedCount());
+        }
+
+        @JRubyMethod(name = "unread_count")
+        public IRubyObject ruby_unread_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnreadCount());
+        }
+
+        @JRubyMethod(name = "unacked_count")
+        public IRubyObject ruby_unacked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnackedCount());
+        }
+
+        @JRubyMethod(name = "open")
+        public IRubyObject ruby_open(ThreadContext context)
+        {
+            try {
+                this.queue.getCheckpointIO().purge();
+                this.queue.open();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = {"write", "<<"}, required = 1)
+        public IRubyObject ruby_write(ThreadContext context, IRubyObject event)
+        {
+            if (!(event instanceof JrubyEventExtLibrary.RubyEvent)) {
+                throw context.runtime.newTypeError("wrong argument type " + event.getMetaClass() + " (expected LogStash::Event)");
+            }
+
+            long seqNum;
+            try {
+                seqNum = this.queue.write(((JrubyEventExtLibrary.RubyEvent) event).getEvent());
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.runtime.newFixnum(seqNum);
+        }
+
+        @JRubyMethod(name = "read_batch", required = 2)
+        public IRubyObject ruby_read_batch(ThreadContext context, IRubyObject limit, IRubyObject timeout)
+        {
+            Batch b;
+
+            try {
+                b = this.queue.readBatch(RubyFixnum.num2int(limit), RubyFixnum.num2int(timeout));
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            // TODO: return proper Batch object
+            return (b == null) ? context.nil : new JrubyAckedBatchExtLibrary.RubyAckedBatch(context.runtime, b);
+        }
+
+        @JRubyMethod(name = "is_fully_acked?")
+        public IRubyObject ruby_is_fully_acked(ThreadContext context)
+        {
+            return RubyBoolean.newBoolean(context.runtime, this.queue.isFullyAcked());
+        }
+
+        @JRubyMethod(name = "close")
+        public IRubyObject ruby_close(ThreadContext context)
+        {
+            try {
+                this.queue.close();
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/AbstractByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/AbstractByteBufferPageIO.java
new file mode 100644
index 00000000000..811522c8dc4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/AbstractByteBufferPageIO.java
@@ -0,0 +1,283 @@
+package org.logstash.ackedqueue.io;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.ackedqueue.SequencedList;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+public abstract class AbstractByteBufferPageIO implements PageIO {
+
+    public class PageIOInvalidElementException extends IOException {
+        public PageIOInvalidElementException() { super(); }
+        public PageIOInvalidElementException(String message) { super(message); }
+    }
+
+    public class PageIOInvalidVersionException extends IOException {
+        public PageIOInvalidVersionException(String message) { super(message); }
+    }
+
+    public static final byte VERSION_ONE = 1;
+    public static final int VERSION_SIZE = Byte.BYTES;
+    public static final int CHECKSUM_SIZE = Integer.BYTES;
+    public static final int LENGTH_SIZE = Integer.BYTES;
+    public static final int SEQNUM_SIZE = Long.BYTES;
+    public static final int MIN_RECORD_SIZE = SEQNUM_SIZE + CHECKSUM_SIZE;
+    public static final int HEADER_SIZE = 1;     // version byte
+    public static final int MIN_CAPACITY = VERSION_SIZE + SEQNUM_SIZE + LENGTH_SIZE + 1 + CHECKSUM_SIZE; // header overhead plus elements overhead to hold a single 1 byte element
+    public static final List<byte[]> EMPTY_READ = new ArrayList<>(0);
+
+    public static final boolean VERIFY_CHECKSUM = true;
+    public static final boolean STRICT_CAPACITY = true;
+
+    private static final Logger logger = LogManager.getLogger(AbstractByteBufferPageIO.class);
+
+    protected int capacity; // page capacity is an int per the ByteBuffer class.
+    protected final int pageNum;
+    protected final List<Integer> offsetMap; // has to be extendable
+    protected long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
+    protected int elementCount;
+    protected int head; // head is the write position and is an int per ByteBuffer class position
+    protected byte version;
+    protected Checksum checkSummer;
+
+    public AbstractByteBufferPageIO(int pageNum, int capacity) {
+        this.minSeqNum = 0;
+        this.elementCount = 0;
+        this.version = 0;
+        this.head = 0;
+        this.pageNum = pageNum;
+        this.capacity = capacity;
+        this.offsetMap = new ArrayList<>();
+        this.checkSummer = new CRC32();
+    }
+
+    // @return the concrete class buffer
+    protected abstract ByteBuffer getBuffer();
+
+    @Override
+    public void open(long minSeqNum, int elementCount) throws IOException {
+        getBuffer().position(0);
+        this.version = getBuffer().get();
+        validateVersion(this.version);
+        this.head = 1;
+
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+
+        if (this.elementCount > 0) {
+            // verify first seqNum to be same as expected minSeqNum
+            long seqNum = getBuffer().getLong();
+            if (seqNum != this.minSeqNum) { throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum)); }
+
+            // reset back position to first seqNum
+            getBuffer().position(this.head);
+
+            for (int i = 0; i < this.elementCount; i++) {
+                // verify that seqNum must be of strict + 1 increasing order
+                readNextElement(this.minSeqNum + i, !VERIFY_CHECKSUM);
+            }
+        }
+    }
+
+    // recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes
+    // to reflect what it recovered from the page
+    @Override
+    public void recover() throws IOException {
+        getBuffer().position(0);
+        this.version = getBuffer().get();
+        validateVersion(this.version);
+        this.head = 1;
+
+        // force minSeqNum to actual first element seqNum
+        this.minSeqNum = getBuffer().getLong();
+        // reset back position to first seqNum
+        getBuffer().position(this.head);
+
+        // reset elementCount to 0 and increment to octal number of valid elements found
+        this.elementCount = 0;
+
+        for (int i = 0; ; i++) {
+            try {
+                // verify that seqNum must be of strict + 1 increasing order
+                readNextElement(this.minSeqNum + i, VERIFY_CHECKSUM);
+                this.elementCount += 1;
+            } catch (PageIOInvalidElementException e) {
+                // simply stop at first invalid element
+                logger.debug("PageIO recovery element index:{}, readNextElement exception: {}", i, e.getMessage());
+                break;
+            }
+        }
+
+        // if we were not able to read any element just reset minSeqNum to zero
+        if (this.elementCount <= 0) {
+            this.minSeqNum = 0;
+        }
+    }
+
+    // we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check
+    // and if an unexpected version byte is read throw PageIOInvalidVersionException
+    private void validateVersion(byte version) throws PageIOInvalidVersionException {
+        if (version != VERSION_ONE) {
+            throw new PageIOInvalidVersionException(String.format("Expected page version=%d but found version=%d", VERSION_ONE, version));
+        }
+    }
+
+    // read and validate next element at page head
+    // @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
+    private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws PageIOInvalidElementException {
+        // if there is no room for the seqNum and length bytes stop here
+        // TODO: I know this isn't a great exception message but at the time of writing I couldn't come up with anything better :P
+        if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) { throw new PageIOInvalidElementException("cannot read seqNum and length bytes past buffer capacity"); }
+
+        int elementOffset = this.head;
+        int newHead = this.head;
+        ByteBuffer buffer = getBuffer();
+
+        long seqNum = buffer.getLong();
+        newHead += SEQNUM_SIZE;
+
+        if (seqNum != expectedSeqNum) { throw new PageIOInvalidElementException(String.format("Element seqNum %d is expected to be %d", seqNum, expectedSeqNum)); }
+
+        int length = buffer.getInt();
+        newHead += LENGTH_SIZE;
+
+        // length must be > 0
+        if (length <= 0) { throw new PageIOInvalidElementException("Element invalid length"); }
+
+        // if there is no room for the proposed data length and checksum just stop here
+        if (newHead + length + CHECKSUM_SIZE > capacity) { throw new PageIOInvalidElementException("cannot read element payload and checksum past buffer capacity"); }
+
+        if (verifyChecksum) {
+            // read data and compute checksum;
+            byte[] readBytes = new byte[length];
+            buffer.get(readBytes);
+            int checksum = buffer.getInt();
+            int computedChecksum = checksum(readBytes);
+            if (computedChecksum != checksum) { throw new PageIOInvalidElementException("Element invalid checksum"); }
+        }
+
+        // at this point we recovered a valid element
+        this.offsetMap.add(elementOffset);
+        this.head = newHead + length + CHECKSUM_SIZE;
+
+        buffer.position(this.head);
+    }
+
+    @Override
+    public void create() throws IOException {
+        getBuffer().position(0);
+        getBuffer().put(VERSION_ONE);
+        this.head = 1;
+        this.minSeqNum = 0L;
+        this.elementCount = 0;
+    }
+
+    @Override
+    public void write(byte[] bytes, long seqNum) throws IOException {
+        write(bytes, seqNum, bytes.length, checksum(bytes));
+    }
+
+    protected int write(byte[] bytes, long seqNum, int length, int checksum) {
+        // since writes always happen at head, we can just append head to the offsetMap
+        assert this.offsetMap.size() == this.elementCount :
+                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
+
+        int initialHead = this.head;
+        ByteBuffer buffer = getBuffer();
+
+        buffer.position(this.head);
+        buffer.putLong(seqNum);
+        buffer.putInt(length);
+        buffer.put(bytes);
+        buffer.putInt(checksum);
+        this.head += persistedByteCount(bytes.length);
+
+        assert this.head == buffer.position() :
+                String.format("head=%d != buffer position=%d", this.head, buffer.position());
+
+        if (this.elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(initialHead);
+        this.elementCount++;
+
+        return initialHead;
+    }
+
+    @Override
+    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
+        assert seqNum >= this.minSeqNum :
+                String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
+        assert seqNum <= maxSeqNum() :
+                String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
+
+        List<byte[]> elements = new ArrayList<>();
+        List<Long> seqNums = new ArrayList<>();
+
+        int offset = this.offsetMap.get((int)(seqNum - this.minSeqNum));
+
+        ByteBuffer buffer = getBuffer();
+        buffer.position(offset);
+
+        for (int i = 0; i < limit; i++) {
+            long readSeqNum = buffer.getLong();
+
+            assert readSeqNum == (seqNum + i) :
+                    String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
+
+            int readLength = buffer.getInt();
+            byte[] readBytes = new byte[readLength];
+            buffer.get(readBytes);
+            int checksum = buffer.getInt();
+            int computedChecksum = checksum(readBytes);
+            if (computedChecksum != checksum) {
+                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
+            }
+
+            elements.add(readBytes);
+            seqNums.add(readSeqNum);
+
+            if (seqNum + i >= maxSeqNum()) {
+                break;
+            }
+        }
+
+        return new SequencedList<byte[]>(elements, seqNums);
+    }
+
+    @Override
+    public int getCapacity() { return this.capacity; }
+
+    @Override
+    public long getMinSeqNum() { return this.minSeqNum; }
+
+    @Override
+    public int getElementCount() { return this.elementCount; }
+
+    @Override
+    public boolean hasSpace(int bytes) {
+        int bytesLeft = this.capacity - this.head;
+        return persistedByteCount(bytes) <= bytesLeft;
+    }
+
+    @Override
+    public int persistedByteCount(int byteCount) {
+        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
+    }
+
+    protected int checksum(byte[] bytes) {
+        checkSummer.reset();
+        checkSummer.update(bytes, 0, bytes.length);
+        return (int) checkSummer.getValue();
+    }
+
+    private long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferPageIO.java
new file mode 100644
index 00000000000..37481022228
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferPageIO.java
@@ -0,0 +1,56 @@
+package org.logstash.ackedqueue.io;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+public class ByteBufferPageIO extends AbstractByteBufferPageIO {
+
+    private final ByteBuffer buffer;
+
+    public ByteBufferPageIO(int pageNum, int capacity, String path) throws IOException {
+        this(capacity, new byte[0]);
+    }
+
+    public ByteBufferPageIO(int capacity) throws IOException {
+        this(capacity, new byte[0]);
+    }
+
+    public ByteBufferPageIO(int capacity, byte[] initialBytes) throws IOException {
+        super(0, capacity);
+
+        if (initialBytes.length > capacity) {
+            throw new IOException("initial bytes greater than capacity");
+        }
+
+        this.buffer = ByteBuffer.allocate(capacity);
+        this.buffer.put(initialBytes);
+    }
+
+    @Override
+    public void deactivate() { /* nothing */ }
+
+    @Override
+    public void activate() { /* nyet */ }
+
+    @Override
+    public void ensurePersisted() { /* nada */ }
+
+    @Override
+    public void purge() { /* zilch */ }
+
+    @Override
+    public void close() { /* don't look here */ }
+
+
+    @Override
+    protected ByteBuffer getBuffer() { return this.buffer; }
+
+    // below public methods only used by tests
+
+    // TODO: static method for tests - should refactor
+    public static int _persistedByteCount(int byteCount) { return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE; }
+
+    public int getWritePosition() { return this.head; }
+
+    public byte[] dump() { return this.buffer.array(); }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIO.java
new file mode 100644
index 00000000000..e60f9127351
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIO.java
@@ -0,0 +1,24 @@
+package org.logstash.ackedqueue.io;
+
+import org.logstash.ackedqueue.Checkpoint;
+import java.io.IOException;
+
+public interface CheckpointIO {
+
+    // @return Checkpoint the written checkpoint object
+    Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException;
+
+    void write(String fileName, Checkpoint checkpoint) throws IOException;
+
+    Checkpoint read(String fileName) throws IOException;
+
+    void purge(String fileName) throws IOException;
+
+    void purge() throws IOException;
+
+    // @return the head page checkpoint file name
+    String headFileName();
+
+    // @return the tail page checkpoint file name for given page number
+    String tailFileName(int pageNum);
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIOFactory.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIOFactory.java
new file mode 100644
index 00000000000..b3e43aaf80e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIOFactory.java
@@ -0,0 +1,6 @@
+package org.logstash.ackedqueue.io;
+
+@FunctionalInterface
+public interface CheckpointIOFactory {
+    CheckpointIO build(String dirPath);
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/FileCheckpointIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/FileCheckpointIO.java
new file mode 100644
index 00000000000..d51cefe6a63
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/FileCheckpointIO.java
@@ -0,0 +1,124 @@
+package org.logstash.ackedqueue.io;
+
+import org.logstash.ackedqueue.Checkpoint;
+import org.logstash.common.io.BufferedChecksumStreamInput;
+import org.logstash.common.io.BufferedChecksumStreamOutput;
+import org.logstash.common.io.ByteArrayStreamOutput;
+import org.logstash.common.io.InputStreamStreamInput;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.nio.file.Files;
+import java.nio.file.OpenOption;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import static java.nio.file.StandardOpenOption.CREATE;
+import static java.nio.file.StandardOpenOption.WRITE;
+import static java.nio.file.StandardOpenOption.TRUNCATE_EXISTING;
+import static java.nio.file.StandardOpenOption.DSYNC;
+
+public class FileCheckpointIO  implements CheckpointIO {
+//    Checkpoint file structure
+//
+//    byte version;
+//    int pageNum;
+//    int firstUnackedPageNum;
+//    long firstUnackedSeqNum;
+//    long minSeqNum;
+//    int elementCount;
+
+    public static final int BUFFER_SIZE = Short.BYTES // version
+            + Integer.BYTES  // pageNum
+            + Integer.BYTES  // firstUnackedPageNum
+            + Long.BYTES     // firstUnackedSeqNum
+            + Long.BYTES     // minSeqNum
+            + Integer.BYTES  // eventCount
+            + Integer.BYTES;    // checksum
+
+    private final String dirPath;
+    private final String HEAD_CHECKPOINT = "checkpoint.head";
+    private final String TAIL_CHECKPOINT = "checkpoint.";
+    private final OpenOption[] WRITE_OPTIONS = new OpenOption[] { WRITE, CREATE, TRUNCATE_EXISTING, DSYNC };
+
+    public FileCheckpointIO(String dirPath) {
+        this.dirPath = dirPath;
+    }
+
+    @Override
+    public Checkpoint read(String fileName) throws IOException {
+        Path path = Paths.get(dirPath, fileName);
+        InputStream is = Files.newInputStream(path);
+        return read(new BufferedChecksumStreamInput(new InputStreamStreamInput(is)));
+    }
+
+    @Override
+    public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException {
+        Checkpoint checkpoint = new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
+        write(fileName, checkpoint);
+        return checkpoint;
+    }
+
+    @Override
+    public void write(String fileName, Checkpoint checkpoint) throws IOException {
+        Path path = Paths.get(dirPath, fileName);
+        final byte[] buffer = new byte[BUFFER_SIZE];
+        write(checkpoint, buffer);
+        Files.write(path, buffer, WRITE_OPTIONS);
+    }
+
+    @Override
+    public void purge(String fileName) throws IOException {
+        Path path = Paths.get(dirPath, fileName);
+        Files.delete(path);
+    }
+
+    @Override
+    public void purge() throws IOException {
+        // TODO: dir traversal and delete all checkpoints?
+        throw new UnsupportedOperationException("purge() is not supported");
+    }
+
+    // @return the head page checkpoint file name
+    @Override
+    public String headFileName() {
+         return HEAD_CHECKPOINT;
+    }
+
+    // @return the tail page checkpoint file name for given page number
+    @Override
+    public String tailFileName(int pageNum) {
+        return TAIL_CHECKPOINT + pageNum;
+    }
+
+    private Checkpoint read(BufferedChecksumStreamInput crcsi) throws IOException {
+        int version = (int) crcsi.readShort();
+        // TODO - build reader for this version
+        int pageNum = crcsi.readInt();
+        int firstUnackedPageNum = crcsi.readInt();
+        long firstUnackedSeqNum = crcsi.readLong();
+        long minSeqNum = crcsi.readLong();
+        int elementCount = crcsi.readInt();
+
+        int calcCrc32 = (int)crcsi.getChecksum();
+        int readCrc32 = crcsi.readInt();
+        if (readCrc32 != calcCrc32) {
+            throw new IOException(String.format("Checkpoint checksum mismatch, expected: %d, actual: %d", calcCrc32, readCrc32));
+        }
+        if (version != Checkpoint.VERSION) {
+            throw new IOException("Unknown file format version: " + version);
+        }
+
+        return new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
+    }
+
+    private void write(Checkpoint checkpoint, byte[] buf) throws IOException {
+        BufferedChecksumStreamOutput output = new BufferedChecksumStreamOutput(new ByteArrayStreamOutput(buf));
+        output.writeShort((short)Checkpoint.VERSION);
+        output.writeInt(checkpoint.getPageNum());
+        output.writeInt(checkpoint.getFirstUnackedPageNum());
+        output.writeLong(checkpoint.getFirstUnackedSeqNum());
+        output.writeLong(checkpoint.getMinSeqNum());
+        output.writeInt(checkpoint.getElementCount());
+        output.writeInt((int)output.getChecksum());
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MemoryCheckpointIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MemoryCheckpointIO.java
new file mode 100644
index 00000000000..681ec9a75e9
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MemoryCheckpointIO.java
@@ -0,0 +1,81 @@
+package org.logstash.ackedqueue.io;
+
+import org.logstash.ackedqueue.Checkpoint;
+
+import java.io.IOException;
+import java.nio.file.NoSuchFileException;
+import java.util.HashMap;
+import java.util.Map;
+
+public class MemoryCheckpointIO implements CheckpointIO {
+
+    private final String HEAD_CHECKPOINT = "checkpoint.head";
+    private final String TAIL_CHECKPOINT = "checkpoint.";
+
+    private static final Map<String, Map<String, Checkpoint>> sources = new HashMap<>();
+
+    private final String dirPath;
+
+    public static void clearSources() {
+        sources.clear();
+    }
+
+    public MemoryCheckpointIO(String dirPath) {
+        this.dirPath = dirPath;
+    }
+
+    @Override
+    public Checkpoint read(String fileName) throws IOException {
+
+        Checkpoint cp = null;
+        Map<String, Checkpoint> ns = this.sources.get(dirPath);
+        if (ns != null) {
+           cp = ns.get(fileName);
+        }
+        if (cp == null) { throw new NoSuchFileException("no memory checkpoint for dirPath: " + this.dirPath + ", fileName: " + fileName); }
+        return cp;
+    }
+
+    @Override
+    public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException {
+        Checkpoint checkpoint = new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
+        write(fileName, checkpoint);
+        return checkpoint;
+    }
+
+    @Override
+    public void write(String fileName, Checkpoint checkpoint) throws IOException {
+        Map<String, Checkpoint> ns = this.sources.get(dirPath);
+        if (ns == null) {
+            ns = new HashMap<>();
+            this.sources.put(this.dirPath, ns);
+        }
+        ns.put(fileName, checkpoint);
+    }
+
+    @Override
+    public void purge(String fileName) {
+        Map<String, Checkpoint> ns = this.sources.get(dirPath);
+        if (ns != null) {
+           ns.remove(fileName);
+        }
+    }
+
+    @Override
+    public void purge() {
+        this.sources.remove(this.dirPath);
+    }
+
+    // @return the head page checkpoint file name
+    @Override
+    public String headFileName() {
+        return HEAD_CHECKPOINT;
+    }
+
+    // @return the tail page checkpoint file name for given page number
+    @Override
+    public String tailFileName(int pageNum) {
+        return TAIL_CHECKPOINT + pageNum;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
new file mode 100644
index 00000000000..931d642c272
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
@@ -0,0 +1,130 @@
+package org.logstash.ackedqueue.io;
+
+import sun.misc.Cleaner;
+import sun.nio.ch.DirectBuffer;
+
+import java.io.File;
+import java.io.IOException;
+import java.io.RandomAccessFile;
+import java.nio.MappedByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.Files;
+import java.nio.file.Paths;
+
+// TODO: this essentially a copy of ByteBufferPageIO and should be DRY'ed - temp impl to test file based stress test
+
+@SuppressWarnings("sunapi")
+public class MmapPageIO extends AbstractByteBufferPageIO {
+
+    private File file;
+
+    private FileChannel channel;
+    protected MappedByteBuffer buffer;
+
+    public MmapPageIO(int pageNum, int capacity, String dirPath) throws IOException {
+        super(pageNum, capacity);
+
+        this.file = Paths.get(dirPath, "page." + pageNum).toFile();
+    }
+
+    @Override
+    public void open(long minSeqNum, int elementCount) throws IOException {
+        mapFile(STRICT_CAPACITY);
+        super.open(minSeqNum, elementCount);
+    }
+
+    // recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes
+    // to reflect what it recovered from the page
+    @Override
+    public void recover() throws IOException {
+        mapFile(!STRICT_CAPACITY);
+        super.recover();
+    }
+
+    // memory map data file to this.buffer and read initial version byte
+    // @param strictCapacity if true verify that data file size is same as configured page capacity, if false update page capacity to actual file size
+    private void mapFile(boolean strictCapacity) throws IOException {
+        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
+
+        if (raf.length() > Integer.MAX_VALUE) {
+            throw new IOException("Page file too large " + this.file);
+        }
+        int pageFileCapacity = (int)raf.length();
+
+        if (strictCapacity && this.capacity != pageFileCapacity) {
+            throw new IOException("Page file size " + pageFileCapacity + " different to configured page capacity " + this.capacity + " for " + this.file);
+        }
+
+        // update capacity to actual raf length
+        this.capacity = pageFileCapacity;
+
+        if (this.capacity < MIN_CAPACITY) { throw new IOException(String.format("Page file size is too small to hold elements")); }
+
+        this.channel = raf.getChannel();
+        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+        raf.close();
+        this.buffer.load();
+    }
+
+    @Override
+    public void create() throws IOException {
+        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
+        this.channel = raf.getChannel();
+        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+        raf.close();
+
+        super.create();
+    }
+
+    @Override
+    public void deactivate() throws IOException {
+        close(); // close can be called multiple times
+    }
+
+    @Override
+    public void activate() throws IOException {
+        if (this.channel == null) {
+            RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
+            this.channel = raf.getChannel();
+            this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+            raf.close();
+            this.buffer.load();
+        }
+        // TODO: do we need to check is the channel is still open? not sure how it could be closed
+    }
+
+    @Override
+    public void ensurePersisted() {
+        this.buffer.force();
+    }
+
+    @Override
+    public void purge() throws IOException {
+        close();
+        Files.delete(this.file.toPath());
+    }
+
+    @Override
+    public void close() throws IOException {
+        if (this.buffer != null) {
+            this.buffer.force();
+
+            // calling the cleaner() method releases resources held by this direct buffer which would be held until GC otherwise.
+            // see https://github.com/elastic/logstash/pull/6740
+            Cleaner cleaner = ((DirectBuffer) this.buffer).cleaner();
+            if (cleaner != null) { cleaner.clean(); }
+
+        }
+        if (this.channel != null) {
+            if (this.channel.isOpen()) { this.channel.force(false); }
+            this.channel.close(); // close can be called multiple times
+        }
+        this.channel = null;
+        this.buffer = null;
+    }
+
+    @Override
+    protected MappedByteBuffer getBuffer() {
+        return this.buffer;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIO.java
new file mode 100644
index 00000000000..f76341e5d3f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIO.java
@@ -0,0 +1,61 @@
+package org.logstash.ackedqueue.io;
+
+import org.logstash.ackedqueue.SequencedList;
+
+import java.io.Closeable;
+import java.io.IOException;
+
+public interface PageIO extends Closeable {
+
+    // the concrete class should be constructed with the pageNum, capacity and dirPath attributes
+    // and open/recover/create must first be called to setup with physical data file
+    //
+    // TODO: we should probably refactor this with a factory to force the creation of a fully
+    //       initialized concrete object with either open/recover/create instead of allowing
+    //       a partially initialized object using the concrete class constructor. Not sure.
+
+    // open an existing data container and reconstruct internal state if required
+    void open(long minSeqNum, int elementCount) throws IOException;
+
+    // optimistically recover an existing data container and reconstruct internal state
+    // with the actual read/recovered data. this is only useful when reading a head page
+    // data file since tail pages are read-only.
+    void recover() throws IOException;
+
+    // create a new empty data file
+    void create() throws IOException;
+
+    // verify if the data container has space for the given number of bytes
+    boolean hasSpace(int bytes);
+
+    // write the given bytes to the data container
+    void write(byte[] bytes, long seqNum) throws IOException;
+
+    // read up to limit number of items starting at give seqNum
+    SequencedList<byte[]> read(long seqNum, int limit) throws IOException;
+
+    // @return the data container total capacity in bytes
+    int getCapacity();
+
+    // @return the actual persisted byte count (with overhead) for the given data bytes
+    int persistedByteCount(int bytes);
+
+    // signal that this data page is not active and resources can be released
+    void deactivate() throws IOException;
+
+    // signal that this data page is active will be read or written to
+    // should do nothing if page is aready active
+    void activate() throws IOException;
+
+    // issue the proper data container "fsync" sematic
+    void ensurePersisted();
+
+    // delete/unlink/remove data file
+    void purge() throws IOException;
+
+    // @return the data container elements count
+    int getElementCount();
+
+    // @return the data container min sequence number
+    long getMinSeqNum();
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIOFactory.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIOFactory.java
new file mode 100644
index 00000000000..dffe219b9dc
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIOFactory.java
@@ -0,0 +1,8 @@
+package org.logstash.ackedqueue.io;
+
+import java.io.IOException;
+
+@FunctionalInterface
+public interface PageIOFactory {
+    PageIO build(int pageNum, int capacity, String dirPath) throws IOException;
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStream.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStream.java
new file mode 100644
index 00000000000..62221c3b240
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStream.java
@@ -0,0 +1,283 @@
+package org.logstash.ackedqueue.io.wip;
+
+import org.logstash.ackedqueue.Checkpoint;
+import org.logstash.ackedqueue.SequencedList;
+import org.logstash.common.io.BufferedChecksumStreamInput;
+import org.logstash.common.io.BufferedChecksumStreamOutput;
+import org.logstash.common.io.ByteArrayStreamOutput;
+import org.logstash.common.io.ByteBufferStreamInput;
+import org.logstash.ackedqueue.io.PageIO;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+
+public class MemoryPageIOStream implements PageIO {
+    static final int CHECKSUM_SIZE = Integer.BYTES;
+    static final int LENGTH_SIZE = Integer.BYTES;
+    static final int SEQNUM_SIZE = Long.BYTES;
+    static final int MIN_RECORD_SIZE = SEQNUM_SIZE + LENGTH_SIZE + CHECKSUM_SIZE;
+    static final int VERSION_SIZE = Integer.BYTES;
+
+    private final byte[] buffer;
+    private final int capacity;
+    private int writePosition;
+    private int readPosition;
+    private int elementCount;
+    private long minSeqNum;
+    private ByteBufferStreamInput streamedInput;
+    private ByteArrayStreamOutput streamedOutput;
+    private BufferedChecksumStreamOutput crcWrappedOutput;
+    private final List<Integer> offsetMap;
+    private String dirPath = "";
+    private String headerDetails = "";
+
+    public int persistedByteCount(byte[] data) {
+        return persistedByteCount(data.length);
+    }
+
+    @Override
+    public int persistedByteCount(int length) {
+        return MIN_RECORD_SIZE + length;
+    }
+
+    public MemoryPageIOStream(int pageNum, int capacity, String dirPath) throws IOException {
+        this(capacity, new byte[capacity]);
+        this.dirPath = dirPath;
+    }
+
+    public MemoryPageIOStream(int capacity, String dirPath) throws IOException {
+        this(capacity, new byte[capacity]);
+        this.dirPath = dirPath;
+    }
+
+    public MemoryPageIOStream(int capacity) throws IOException {
+        this(capacity, new byte[capacity]);
+    }
+
+    public MemoryPageIOStream(int capacity, byte[] initialBytes) throws IOException {
+        this.capacity = capacity;
+        if (initialBytes.length > capacity) {
+            throw new IOException("initial bytes greater than capacity");
+        }
+        buffer = initialBytes;
+        offsetMap = new ArrayList<>();
+        streamedInput = new ByteBufferStreamInput(ByteBuffer.wrap(buffer));
+        streamedOutput = new ByteArrayStreamOutput(buffer);
+        crcWrappedOutput = new BufferedChecksumStreamOutput(streamedOutput);
+    }
+
+    @Override
+    public void recover() throws IOException {
+        throw new UnsupportedOperationException("recover() is not supported");
+    }
+
+    @Override
+    public void open(long minSeqNum, int elementCount) throws IOException {
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+        writePosition = verifyHeader();
+        readPosition = writePosition;
+        if (elementCount > 0) {
+            long seqNumRead;
+            BufferedChecksumStreamInput in = new BufferedChecksumStreamInput(streamedInput);
+            for (int i = 0; i < this.elementCount; i++) {
+                if (writePosition + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
+                    throw new IOException(String.format("cannot read seqNum and length bytes past buffer capacity"));
+                }
+
+                seqNumRead = in.readLong();
+
+                //verify that the buffer starts with the min sequence number
+                if (i == 0 && seqNumRead != this.minSeqNum) {
+                    String msg = String.format("Page minSeqNum mismatch, expected: %d, actual: %d", this.minSeqNum, seqNumRead);
+                    throw new IOException(msg);
+                }
+
+                in.resetDigest();
+                byte[] bytes = in.readByteArray();
+                int actualChecksum = (int) in.getChecksum();
+                int expectedChecksum = in.readInt();
+
+                if (actualChecksum != expectedChecksum) {
+                    // explode with tragic error
+                }
+
+                offsetMap.add(writePosition);
+                writePosition += persistedByteCount(bytes);
+            }
+            setReadPoint(this.minSeqNum);
+        }
+    }
+
+    @Override
+    public void create() throws IOException {
+        writePosition = addHeader();
+        readPosition = writePosition;
+        this.minSeqNum = 1L;
+        this.elementCount = 0;
+    }
+
+    @Override
+    public boolean hasSpace(int byteSize) {
+        return this.capacity >= writePosition + persistedByteCount(byteSize);
+    }
+
+    @Override
+    public void write(byte[] bytes, long seqNum) throws IOException {
+        int pos = this.writePosition;
+        int writeLength = persistedByteCount(bytes);
+        writeToBuffer(seqNum, bytes, writeLength);
+        writePosition += writeLength;
+        assert writePosition == streamedOutput.getPosition() :
+                String.format("writePosition=%d != streamedOutput position=%d", writePosition, streamedOutput.getPosition());
+        if (elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(pos);
+        elementCount++;
+    }
+
+    @Override
+    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
+        if (elementCount == 0) {
+            return new SequencedList<>(new ArrayList<>(), new ArrayList<>());
+        }
+        setReadPoint(seqNum);
+        return read(limit);
+    }
+
+    @Override
+    public int getCapacity() {
+        return capacity;
+    }
+
+    @Override
+    public void deactivate() {
+        // do nothing
+    }
+
+    @Override
+    public void activate() {
+        // do nothing
+    }
+
+    @Override
+    public void ensurePersisted() {
+        // do nothing
+    }
+
+    @Override
+    public void purge() throws IOException {
+        // do nothing
+    }
+
+    @Override
+    public void close() throws IOException {
+        // TBD
+    }
+
+    //@Override
+    public void setPageHeaderDetails(String details) {
+        headerDetails = details;
+    }
+
+    public int getWritePosition() {
+        return writePosition;
+    }
+
+    public int getElementCount() {
+        return elementCount;
+    }
+
+    public long getMinSeqNum() {
+        return minSeqNum;
+    }
+
+    // used in tests
+    public byte[] getBuffer() {
+        return buffer;
+    }
+
+    // used in tests
+    public String readHeaderDetails() throws IOException {
+        int tempPosition = readPosition;
+        streamedInput.movePosition(0);
+        int ver = streamedInput.readInt();
+        String details = new String(streamedInput.readByteArray());
+        streamedInput.movePosition(tempPosition);
+        return details;
+    }
+
+    private void setReadPoint(long seqNum) throws IOException {
+        int readPosition = offsetMap.get(calcRelativeSeqNum(seqNum));
+        streamedInput.movePosition(readPosition);
+    }
+
+    private int calcRelativeSeqNum(long seqNum) {
+        return (int) (seqNum - minSeqNum);
+    }
+
+    private int addHeader() throws IOException {
+        streamedOutput.writeInt(Checkpoint.VERSION);
+        byte[] details = headerDetails.getBytes();
+        streamedOutput.writeByteArray(details);
+        return VERSION_SIZE + LENGTH_SIZE + details.length;
+    }
+
+    private int verifyHeader() throws IOException {
+        int ver = streamedInput.readInt();
+        if (ver != Checkpoint.VERSION) {
+            String msg = String.format("Page version mismatch, expecting: %d, this version: %d", Checkpoint.VERSION, ver);
+            throw new IOException(msg);
+        }
+        int len = streamedInput.readInt();
+        streamedInput.skip(len);
+        return VERSION_SIZE + LENGTH_SIZE + len;
+    }
+
+    private void writeToBuffer(long seqNum, byte[] data, int len) throws IOException {
+        streamedOutput.setWriteWindow(writePosition, len);
+        crcWrappedOutput.writeLong(seqNum);
+        crcWrappedOutput.resetDigest();
+        crcWrappedOutput.writeByteArray(data);
+        long checksum = crcWrappedOutput.getChecksum();
+        crcWrappedOutput.writeInt((int) checksum);
+        crcWrappedOutput.flush();
+        crcWrappedOutput.close();
+    }
+
+    private SequencedList<byte[]> read(int limit) throws IOException {
+        List<byte[]> elements = new ArrayList<>();
+        List<Long> seqNums = new ArrayList<>();
+
+        int upto = available(limit);
+        for (int i = 0; i < upto; i++) {
+            long seqNum = readSeqNum();
+            byte[] data = readData();
+            skipChecksum();
+            elements.add(data);
+            seqNums.add(seqNum);
+        }
+        return new SequencedList<>(elements, seqNums);
+    }
+
+    private long readSeqNum() throws IOException {
+        return streamedInput.readLong();
+    }
+
+    private byte[] readData() throws IOException {
+        return streamedInput.readByteArray();
+    }
+
+    private void skipChecksum() throws IOException {
+        streamedInput.skip(CHECKSUM_SIZE);
+    }
+
+    private int available(int sought) {
+        if (elementCount < 1) return 0;
+        if (elementCount < sought) return elementCount;
+        return sought;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/BiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/BiValue.java
new file mode 100644
index 00000000000..7270f8e5e8e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/BiValue.java
@@ -0,0 +1,18 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.runtime.builtin.IRubyObject;
+
+public interface BiValue<R extends IRubyObject, J> {
+    IRubyObject rubyValue(Ruby runtime);
+
+    J javaValue();
+
+    R rubyValueUnconverted();
+
+    boolean hasRubyValue();
+
+    boolean hasJavaValue();
+}
+
+
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/BiValueCommon.java b/logstash-core/src/main/java/org/logstash/bivalues/BiValueCommon.java
new file mode 100644
index 00000000000..0e8748f0be4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/BiValueCommon.java
@@ -0,0 +1,110 @@
+package org.logstash.bivalues;
+
+import com.fasterxml.jackson.annotation.JsonValue;
+import org.jruby.Ruby;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.io.InvalidObjectException;
+import java.io.ObjectInputStream;
+import java.io.ObjectStreamException;
+import java.io.Serializable;
+
+public abstract class BiValueCommon<R extends IRubyObject, J> implements Serializable {
+    protected transient R rubyValue;
+    protected J javaValue;
+
+    public R rubyValue(Ruby runtime) {
+        if (hasRubyValue()) {
+            return rubyValue;
+        }
+        addRuby(runtime);
+        return rubyValue;
+    }
+
+    @JsonValue
+    public J javaValue() {
+        if (javaValue == null) {
+            addJava();
+        }
+        return javaValue;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+
+        if (hasJavaValue() && javaValue.getClass().isAssignableFrom(o.getClass())){
+            return javaValue.equals(o);
+        }
+
+        if(!(o instanceof BiValue)) {
+            return false;
+        }
+
+        BiValueCommon<?, ?> other = (BiValueCommon<?, ?>) o;
+
+        return (other.hasJavaValue() && other.javaValue().equals(javaValue)) ||
+                (other.hasRubyValue() && other.rubyValueUnconverted().equals(rubyValue));
+
+    }
+
+    @Override
+    public int hashCode() {
+        if (hasRubyValue()) {
+            return rubyValue.hashCode();
+        }
+        if (hasJavaValue()) {
+            return javaValue.hashCode();
+        }
+        return 0;
+    }
+
+    public R rubyValueUnconverted() {
+        return rubyValue;
+    }
+
+    public boolean hasRubyValue() {
+        return null != rubyValue;
+    }
+
+    public boolean hasJavaValue() {
+        return null != javaValue;
+    }
+
+    protected abstract void addRuby(Ruby runtime);
+
+    protected abstract void addJava();
+
+    @Override
+    public String toString() {
+        if (hasRubyValue()) {
+            javaValue();
+        }
+        if (javaValue == null) {
+            return "";
+        }
+        return String.valueOf(javaValue);
+    }
+
+    protected static Object newProxy(BiValue instance) {
+        return new SerializationProxy(instance);
+    }
+
+    private static class SerializationProxy implements Serializable {
+        private static final long serialVersionUID = -1749700725129586973L;
+
+        private final Object javaValue;
+
+        public SerializationProxy(BiValue o) {
+            javaValue = o.javaValue(); // ensure the javaValue is converted from a ruby one if it exists
+        }
+
+        private Object readResolve() throws ObjectStreamException {
+            return BiValues.newBiValue(javaValue);
+        }
+    }
+
+    private void readObject(ObjectInputStream stream) throws InvalidObjectException {
+        throw new InvalidObjectException("Proxy required");
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/BiValues.java b/logstash-core/src/main/java/org/logstash/bivalues/BiValues.java
new file mode 100644
index 00000000000..8529189fa06
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/BiValues.java
@@ -0,0 +1,203 @@
+package org.logstash.bivalues;
+
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.jruby.RubyBignum;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.RubyNil;
+import org.jruby.RubyString;
+import org.jruby.RubySymbol;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.jruby.java.proxies.JavaProxy;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.util.HashMap;
+
+public enum BiValues {
+    ORG_LOGSTASH_EXT_JRUBYTIMESTAMPEXTLIBRARY$RUBYTIMESTAMP(BiValueType.TIMESTAMP),
+    ORG_LOGSTASH_TIMESTAMP(BiValueType.TIMESTAMP),
+    JAVA_LANG_BOOLEAN(BiValueType.BOOLEAN),
+    JAVA_LANG_DOUBLE(BiValueType.DOUBLE),
+    JAVA_LANG_FLOAT(BiValueType.FLOAT),
+    JAVA_LANG_INTEGER(BiValueType.INT),
+    JAVA_LANG_LONG(BiValueType.LONG),
+    JAVA_LANG_STRING(BiValueType.STRING),
+    JAVA_MATH_BIGDECIMAL(BiValueType.DECIMAL),
+    JAVA_MATH_BIGINTEGER(BiValueType.BIGINT),
+    ORG_JRUBY_EXT_BIGDECIMAL_RUBYBIGDECIMAL(BiValueType.DECIMAL),
+    ORG_JRUBY_JAVA_PROXIES_CONCRETEJAVAPROXY(BiValueType.JAVAPROXY),
+    ORG_JRUBY_RUBYBIGNUM(BiValueType.BIGINT),
+    ORG_JRUBY_RUBYBOOLEAN$FALSE(BiValueType.BOOLEAN),
+    ORG_JRUBY_RUBYBOOLEAN$TRUE(BiValueType.BOOLEAN),
+    ORG_JRUBY_RUBYBOOLEAN(BiValueType.BOOLEAN),
+    ORG_JRUBY_RUBYFIXNUM(BiValueType.LONG),
+    ORG_JRUBY_RUBYFLOAT(BiValueType.DOUBLE),
+    ORG_JRUBY_RUBYINTEGER(BiValueType.LONG),
+    ORG_JRUBY_RUBYNIL(BiValueType.NULL),
+    ORG_JRUBY_RUBYSTRING(BiValueType.STRING),
+    ORG_JRUBY_RUBYSYMBOL(BiValueType.SYMBOL), // one way conversion, a Java string will use STRING
+    NULL(BiValueType.NULL);
+
+    private static HashMap<String, String> initCache() {
+        HashMap<String, String> hm = new HashMap<>();
+        hm.put("org.logstash.Timestamp", "ORG_LOGSTASH_TIMESTAMP");
+        hm.put("org.logstash.ext.JrubyTimestampExtLibrary$RubyTimestamp", "ORG_LOGSTASH_EXT_JRUBYTIMESTAMPEXTLIBRARY$RUBYTIMESTAMP");
+        hm.put("java.lang.Boolean", "JAVA_LANG_BOOLEAN");
+        hm.put("java.lang.Double", "JAVA_LANG_DOUBLE");
+        hm.put("java.lang.Float", "JAVA_LANG_FLOAT");
+        hm.put("java.lang.Integer", "JAVA_LANG_INTEGER");
+        hm.put("java.lang.Long", "JAVA_LANG_LONG");
+        hm.put("java.lang.String", "JAVA_LANG_STRING");
+        hm.put("java.math.BigDecimal", "JAVA_MATH_BIGDECIMAL");
+        hm.put("java.math.BigInteger", "JAVA_MATH_BIGINTEGER");
+        hm.put("org.jruby.RubyBignum", "ORG_JRUBY_RUBYBIGNUM");
+        hm.put("org.jruby.RubyBoolean", "ORG_JRUBY_RUBYBOOLEAN");
+        hm.put("org.jruby.RubyBoolean$False", "ORG_JRUBY_RUBYBOOLEAN$FALSE");
+        hm.put("org.jruby.RubyBoolean$True", "ORG_JRUBY_RUBYBOOLEAN$TRUE");
+        hm.put("org.jruby.RubyFixnum", "ORG_JRUBY_RUBYFIXNUM");
+        hm.put("org.jruby.RubyFloat", "ORG_JRUBY_RUBYFLOAT");
+        hm.put("org.jruby.RubyInteger", "ORG_JRUBY_RUBYINTEGER");
+        hm.put("org.jruby.RubyNil", "ORG_JRUBY_RUBYNIL");
+        hm.put("org.jruby.RubyString", "ORG_JRUBY_RUBYSTRING");
+        hm.put("org.jruby.RubySymbol", "ORG_JRUBY_RUBYSYMBOL");
+        hm.put("org.jruby.ext.bigdecimal.RubyBigDecimal", "ORG_JRUBY_EXT_BIGDECIMAL_RUBYBIGDECIMAL");
+        hm.put("org.jruby.java.proxies.ConcreteJavaProxy", "ORG_JRUBY_JAVA_PROXIES_CONCRETEJAVAPROXY");
+        return hm;
+    }
+
+    private final BiValueType biValueType;
+
+    BiValues(BiValueType biValueType) {
+        this.biValueType = biValueType;
+    }
+
+    private static final HashMap<String, String> nameCache = initCache();
+
+    private BiValue build(Object value) {
+        return biValueType.build(value);
+    }
+
+    public static BiValue newBiValue(Object o) {
+        if (o == null){
+            return NULL.build(null);
+        }
+        BiValues bvs = valueOf(fetchName(o));
+        return bvs.build(o);
+    }
+
+    private static String fetchName(Object o) {
+        String cls = o.getClass().getName();
+        if (nameCache.containsKey(cls)) {
+            return nameCache.get(cls);
+        }
+        String toCache = cls.toUpperCase().replace('.', '_');
+        // TODO[Guy] log warn that we are seeing a uncached value
+        nameCache.put(cls, toCache);
+        return toCache;
+    }
+
+    private enum BiValueType {
+        STRING {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new StringBiValue((RubyString) value);
+                }
+                return new StringBiValue((String) value);
+            }
+        },
+        SYMBOL {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new SymbolBiValue((RubySymbol) value);
+                }
+                return new SymbolBiValue((String) value);
+            }
+        },
+        LONG {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new LongBiValue((RubyInteger) value);
+                }
+                return new LongBiValue((Long) value);
+            }
+        },
+        INT {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new IntegerBiValue((RubyInteger) value);
+                }
+                return new IntegerBiValue((Integer) value);
+            }
+        },
+        DOUBLE {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new DoubleBiValue((RubyFloat) value);
+                }
+                return new DoubleBiValue((Double) value);
+            }
+        },
+        FLOAT {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new DoubleBiValue((RubyFloat) value);
+                }
+                return new FloatBiValue((Float) value);
+            }
+        },
+        DECIMAL {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new BigDecimalBiValue((RubyBigDecimal) value);
+                }
+                return new BigDecimalBiValue((BigDecimal) value);
+            }
+        },
+        BOOLEAN {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new BooleanBiValue((RubyBoolean) value);
+                }
+                return new BooleanBiValue((Boolean) value);
+            }
+        },
+        TIMESTAMP {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new TimestampBiValue((RubyTimestamp) value);
+                }
+                return new TimestampBiValue((Timestamp) value);
+            }
+        },
+        NULL {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new NullBiValue((RubyNil) value);
+                }
+                return NullBiValue.newNullBiValue();
+            }
+        },
+        BIGINT {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new BigIntegerBiValue((RubyBignum) value);
+                }
+                return new BigIntegerBiValue((BigInteger) value);
+            }
+        },
+        JAVAPROXY {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new JavaProxyBiValue((JavaProxy) value);
+                }
+                return new JavaProxyBiValue(value);
+            }
+        };
+        abstract BiValue build(Object value);
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/BigDecimalBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/BigDecimalBiValue.java
new file mode 100644
index 00000000000..a4226a9dcd4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/BigDecimalBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+
+import java.io.ObjectStreamException;
+import java.math.BigDecimal;
+
+public class BigDecimalBiValue extends BiValueCommon<RubyBigDecimal, BigDecimal> implements BiValue<RubyBigDecimal, BigDecimal> {
+
+    public BigDecimalBiValue(RubyBigDecimal rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public BigDecimalBiValue(BigDecimal javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private BigDecimalBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getBigDecimalValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/BigIntegerBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/BigIntegerBiValue.java
new file mode 100644
index 00000000000..e2d04822448
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/BigIntegerBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyBignum;
+
+import java.io.ObjectStreamException;
+import java.math.BigInteger;
+
+public class BigIntegerBiValue extends BiValueCommon<RubyBignum, BigInteger> implements BiValue<RubyBignum, BigInteger> {
+
+    public BigIntegerBiValue(RubyBignum rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public BigIntegerBiValue(BigInteger javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private BigIntegerBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = new RubyBignum(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/BooleanBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/BooleanBiValue.java
new file mode 100644
index 00000000000..e50cee9cbd3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/BooleanBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyBoolean;
+
+import java.io.ObjectStreamException;
+
+
+public class BooleanBiValue extends BiValueCommon<RubyBoolean, Boolean> implements BiValue<RubyBoolean, Boolean> {
+
+    public BooleanBiValue(RubyBoolean rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public BooleanBiValue(Boolean javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private BooleanBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyBoolean.newBoolean(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.isTrue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/DoubleBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/DoubleBiValue.java
new file mode 100644
index 00000000000..9185875f691
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/DoubleBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyFloat;
+
+import java.io.ObjectStreamException;
+
+
+public class DoubleBiValue extends BiValueCommon<RubyFloat, Double> implements BiValue<RubyFloat, Double> {
+
+    public DoubleBiValue(RubyFloat rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public DoubleBiValue(Double javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private DoubleBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyFloat.newFloat(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getDoubleValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/FloatBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/FloatBiValue.java
new file mode 100644
index 00000000000..95771b8b9ce
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/FloatBiValue.java
@@ -0,0 +1,40 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyFloat;
+
+import java.io.ObjectStreamException;
+
+
+public class FloatBiValue extends BiValueCommon<RubyFloat, Float> implements BiValue<RubyFloat, Float> {
+
+    public FloatBiValue(RubyFloat rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public FloatBiValue(Float javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private FloatBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyFloat.newFloat(runtime, (double)javaValue);
+    }
+
+    protected void addJava() {
+        double value = rubyValue.getDoubleValue();
+        if ((float) value != value) {
+            throw new ArithmeticException("Float overflow - Incorrect FloatBiValue usage: BiValues should pick DoubleBiValue for RubyFloat");
+        }
+        javaValue = (float) value;
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/IntegerBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/IntegerBiValue.java
new file mode 100644
index 00000000000..92e6da9d28a
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/IntegerBiValue.java
@@ -0,0 +1,40 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyInteger;
+import org.jruby.javasupport.JavaUtil;
+
+import java.io.ObjectStreamException;
+
+public class IntegerBiValue extends BiValueCommon<RubyInteger, Integer> implements BiValue<RubyInteger, Integer> {
+
+    public IntegerBiValue(RubyInteger rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public IntegerBiValue(int javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private IntegerBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        long value = rubyValue.getLongValue();
+        if ((int) value != value) {
+            throw new ArithmeticException("Integer overflow - Incorrect IntegerBiValue usage: BiValues should pick LongBiValue for RubyInteger");
+        }
+        javaValue = (int) value;
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/JavaProxyBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/JavaProxyBiValue.java
new file mode 100644
index 00000000000..f8f3e8771ff
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/JavaProxyBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.java.proxies.JavaProxy;
+import org.jruby.javasupport.JavaUtil;
+
+import java.io.ObjectStreamException;
+
+public class JavaProxyBiValue extends BiValueCommon<JavaProxy, Object> implements BiValue<JavaProxy, Object> {
+
+    public JavaProxyBiValue(JavaProxy rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public JavaProxyBiValue(Object javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private JavaProxyBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (JavaProxy) JavaUtil.convertJavaToUsableRubyObject(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getObject();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/LongBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/LongBiValue.java
new file mode 100644
index 00000000000..2b7c1cb8ef4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/LongBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyInteger;
+import org.jruby.javasupport.JavaUtil;
+
+import java.io.ObjectStreamException;
+
+public class LongBiValue extends BiValueCommon<RubyInteger, Long> implements BiValue<RubyInteger, Long> {
+
+    public LongBiValue(RubyInteger rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public LongBiValue(long javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private LongBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getLongValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/NullBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/NullBiValue.java
new file mode 100644
index 00000000000..4a5f4e53b77
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/NullBiValue.java
@@ -0,0 +1,45 @@
+package org.logstash.bivalues;
+
+import com.fasterxml.jackson.annotation.JsonValue;
+import org.jruby.Ruby;
+import org.jruby.RubyNil;
+
+import java.io.ObjectStreamException;
+
+public class NullBiValue extends BiValueCommon<RubyNil, Object> implements BiValue<RubyNil, Object> {
+    public static NullBiValue newNullBiValue() {
+        return new NullBiValue();
+    }
+
+    public NullBiValue(RubyNil rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    private NullBiValue() {
+        rubyValue = null;
+        javaValue = null;
+    }
+
+    @JsonValue
+    @Override
+    public Object javaValue() {
+        return null;
+    }
+
+    @Override
+    public boolean hasJavaValue() {
+        return true;
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (RubyNil) runtime.getNil();
+    }
+
+    protected void addJava() {}
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/StringBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/StringBiValue.java
new file mode 100644
index 00000000000..5369e23e7bf
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/StringBiValue.java
@@ -0,0 +1,35 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyString;
+
+import java.io.ObjectStreamException;
+
+public class StringBiValue extends BiValueCommon<RubyString, String> implements BiValue<RubyString, String> {
+
+    public StringBiValue(RubyString rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public StringBiValue(String javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private StringBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyString.newUnicodeString(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.asJavaString();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/SymbolBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/SymbolBiValue.java
new file mode 100644
index 00000000000..3cbcde3871f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/SymbolBiValue.java
@@ -0,0 +1,35 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubySymbol;
+
+import java.io.ObjectStreamException;
+
+public class SymbolBiValue extends BiValueCommon<RubySymbol, String> implements BiValue<RubySymbol, String> {
+
+    public SymbolBiValue(RubySymbol rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public SymbolBiValue(String javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private SymbolBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubySymbol.newSymbol(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.asJavaString();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/TimeBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/TimeBiValue.java
new file mode 100644
index 00000000000..bcced16503a
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/TimeBiValue.java
@@ -0,0 +1,37 @@
+package org.logstash.bivalues;
+
+import org.joda.time.DateTime;
+import org.jruby.Ruby;
+import org.jruby.RubyTime;
+
+import java.io.ObjectStreamException;
+
+
+public class TimeBiValue extends BiValueCommon<RubyTime, DateTime> implements BiValue<RubyTime, DateTime> {
+
+    public TimeBiValue(RubyTime rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public TimeBiValue(DateTime javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private TimeBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyTime.newTime(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getDateTime();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/bivalues/TimestampBiValue.java b/logstash-core/src/main/java/org/logstash/bivalues/TimestampBiValue.java
new file mode 100644
index 00000000000..b1d98ca9eee
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/bivalues/TimestampBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.jruby.Ruby;
+
+import java.io.ObjectStreamException;
+
+public class TimestampBiValue extends BiValueCommon<RubyTimestamp, Timestamp> implements BiValue<RubyTimestamp, Timestamp> {
+
+    public TimestampBiValue(RubyTimestamp rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public TimestampBiValue(Timestamp javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private TimestampBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyTimestamp.newRubyTimestamp(runtime, (Timestamp) javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getTimestamp();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksum.java b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksum.java
new file mode 100644
index 00000000000..87558d32cee
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksum.java
@@ -0,0 +1,76 @@
+package org.logstash.common.io;
+
+import java.util.zip.Checksum;
+
+/**
+ * Wraps another {@link Checksum} with an internal buffer
+ * to speed up checksum calculations.
+ */
+public class BufferedChecksum implements Checksum {
+    private final Checksum in;
+    private final byte buffer[];
+    private int upto;
+    /** Default buffer size: 256 */
+    public static final int DEFAULT_BUFFERSIZE = 256;
+
+    /**
+     * Create a new BufferedChecksum with {@link #DEFAULT_BUFFERSIZE}
+     *
+     * @param in The checksum
+     */
+    public BufferedChecksum(Checksum in) {
+        this(in, DEFAULT_BUFFERSIZE);
+    }
+
+    /**
+     * Create a new BufferedChecksum with the specified buffer size
+     *
+     * @param in The checksum
+     * @param bufferSize The buffer size in bytes
+     */
+    public BufferedChecksum(Checksum in, int bufferSize) {
+        this.in = in;
+        this.buffer = new byte[bufferSize];
+    }
+
+    @Override
+    public void update(int b) {
+        if (upto == buffer.length) {
+            flush();
+        }
+        buffer[upto++] = (byte) b;
+    }
+
+    @Override
+    public void update(byte[] b, int off, int len) {
+        if (len >= buffer.length) {
+            flush();
+            in.update(b, off, len);
+        } else {
+            if (upto + len > buffer.length) {
+                flush();
+            }
+            System.arraycopy(b, off, buffer, upto, len);
+            upto += len;
+        }
+    }
+
+    @Override
+    public long getValue() {
+        flush();
+        return in.getValue();
+    }
+
+    @Override
+    public void reset() {
+        upto = 0;
+        in.reset();
+    }
+
+    private void flush() {
+        if (upto > 0) {
+            in.update(buffer, 0, upto);
+        }
+        upto = 0;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamInput.java
new file mode 100644
index 00000000000..beed5238738
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamInput.java
@@ -0,0 +1,104 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+/**
+ * Similar to Lucene's BufferedChecksumIndexInput, however this wraps a
+ * {@link StreamInput} so anything read will update the checksum
+ */
+public final class BufferedChecksumStreamInput extends StreamInput {
+    private static final int SKIP_BUFFER_SIZE = 1024;
+    private byte[] skipBuffer;
+    private final StreamInput in;
+    private final Checksum digest;
+
+    public BufferedChecksumStreamInput(StreamInput in) {
+        this.in = in;
+        this.digest = new BufferedChecksum(new CRC32());
+    }
+
+    public BufferedChecksumStreamInput(StreamInput in, BufferedChecksumStreamInput reuse) {
+        this.in = in;
+        if (reuse == null ) {
+            this.digest = new BufferedChecksum(new CRC32());
+        } else {
+            this.digest = reuse.digest;
+            digest.reset();
+            this.skipBuffer = reuse.skipBuffer;
+        }
+    }
+
+    public long getChecksum() {
+        return this.digest.getValue();
+    }
+
+    @Override
+    public byte readByte() throws IOException {
+        final byte b = in.readByte();
+        digest.update(b);
+        return b;
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+        in.readBytes(b, offset, len);
+        digest.update(b, offset, len);
+    }
+
+    @Override
+    public void reset() throws IOException {
+        in.reset();
+        digest.reset();
+    }
+
+    @Override
+    public int read() throws IOException {
+        return readByte() & 0xFF;
+    }
+
+    @Override
+    public void close() throws IOException {
+        in.close();
+    }
+
+    @Override
+    public boolean markSupported() {
+        return in.markSupported();
+    }
+
+
+    @Override
+    public long skip(long numBytes) throws IOException {
+        if (numBytes < 0) {
+            throw new IllegalArgumentException("numBytes must be >= 0, got " + numBytes);
+        }
+        if (skipBuffer == null) {
+            skipBuffer = new byte[SKIP_BUFFER_SIZE];
+        }
+        assert skipBuffer.length == SKIP_BUFFER_SIZE;
+        long skipped = 0;
+        for (; skipped < numBytes; ) {
+            final int step = (int) Math.min(SKIP_BUFFER_SIZE, numBytes - skipped);
+            readBytes(skipBuffer, 0, step);
+            skipped += step;
+        }
+        return skipped;
+    }
+
+    @Override
+    public int available() throws IOException {
+        return in.available();
+    }
+
+    @Override
+    public synchronized void mark(int readlimit) {
+        in.mark(readlimit);
+    }
+
+    public void resetDigest() {
+        digest.reset();
+    }
+}
+
diff --git a/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamOutput.java b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamOutput.java
new file mode 100644
index 00000000000..f37b71f92bf
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/BufferedChecksumStreamOutput.java
@@ -0,0 +1,57 @@
+package org.logstash.common.io;
+
+
+import java.io.IOException;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+/**
+ * Similar to Lucene's BufferedChecksumIndexOutput, however this wraps a
+ * {@link StreamOutput} so anything written will update the checksum
+ */
+public final class BufferedChecksumStreamOutput extends StreamOutput {
+    private final StreamOutput out;
+    private final Checksum digest;
+
+    public BufferedChecksumStreamOutput(StreamOutput out) {
+        this.out = out;
+        this.digest = new BufferedChecksum(new CRC32());
+    }
+
+    public long getChecksum() {
+        return this.digest.getValue();
+    }
+
+    @Override
+    public void writeByte(byte b) throws IOException {
+        out.writeByte(b);
+        digest.update(b);
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) throws IOException {
+        out.writeBytes(b, offset, length);
+        digest.update(b, offset, length);
+    }
+
+    @Override
+    public void flush() throws IOException {
+        out.flush();
+    }
+
+    @Override
+    public void close() throws IOException {
+        out.close();
+    }
+
+    @Override
+    public void reset() throws IOException {
+        out.reset();
+        digest.reset();
+    }
+
+    public void resetDigest() {
+        digest.reset();
+    }
+}
+
diff --git a/logstash-core/src/main/java/org/logstash/common/io/ByteArrayStreamOutput.java b/logstash-core/src/main/java/org/logstash/common/io/ByteArrayStreamOutput.java
new file mode 100644
index 00000000000..6f49581e9cc
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/ByteArrayStreamOutput.java
@@ -0,0 +1,65 @@
+package org.logstash.common.io;
+
+import java.nio.ByteBuffer;
+
+public class ByteArrayStreamOutput extends StreamOutput {
+    private byte[] bytes;
+
+    private int pos;
+    private int limit;
+
+    public ByteArrayStreamOutput(byte[] bytes) {
+        reset(bytes);
+    }
+
+    public ByteArrayStreamOutput(ByteBuffer bytebuffer) {
+        reset(bytebuffer.array());
+    }
+
+    public ByteArrayStreamOutput(ByteBuffer bytebuffer, int offset, int len) {
+        reset(bytebuffer.array(), offset, len);
+    }
+
+    public ByteArrayStreamOutput(byte[] bytes, int offset, int len) {
+        reset(bytes, offset, len);
+    }
+
+    public void reset(byte[] bytes) {
+        reset(bytes, 0, bytes.length);
+    }
+
+    public void reset(byte[] bytes, int offset, int len) {
+        this.bytes = bytes;
+        pos = offset;
+        limit = offset + len;
+    }
+
+    public void setWriteWindow(int offset, int len) {
+        pos = offset;
+        limit = offset + len;
+    }
+
+    public void reset() {
+    }
+
+    public void reset(int offset) {
+        pos = offset;
+    }
+
+    public int getPosition() {
+        return pos;
+    }
+
+    @Override
+    public void writeByte(byte b) {
+        assert pos < limit :  String.format("ByteArrayStreamOutput#writeByte pos=%d !< limit=%d", pos, limit);
+        bytes[pos++] = b;
+    }
+
+    @Override
+    public void writeBytes(byte[] b, int offset, int length) {
+        assert pos + length <= limit;
+        System.arraycopy(b, offset, bytes, pos, length);
+        pos += length;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/ByteBufferStreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferStreamInput.java
new file mode 100644
index 00000000000..8afeb4eef2c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferStreamInput.java
@@ -0,0 +1,93 @@
+package org.logstash.common.io;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+public class ByteBufferStreamInput extends StreamInput {
+
+    private final ByteBuffer buffer;
+
+    public ByteBufferStreamInput(ByteBuffer buffer) {
+        this.buffer = buffer;
+    }
+
+    @Override
+    public int read() throws IOException {
+        if (!buffer.hasRemaining()) {
+            return -1;
+        }
+        return buffer.get() & 0xFF;
+    }
+
+    @Override
+    public byte readByte() throws IOException {
+        if (!buffer.hasRemaining()) {
+            throw new EOFException();
+        }
+        return buffer.get();
+    }
+
+    @Override
+    public int read(byte[] b, int off, int len) throws IOException {
+        if (!buffer.hasRemaining()) {
+            return -1;
+        }
+
+        len = Math.min(len, buffer.remaining());
+        buffer.get(b, off, len);
+        return len;
+    }
+
+    @Override
+    public long skip(long n) throws IOException {
+        if (n > buffer.remaining()) {
+            int ret = buffer.position();
+            buffer.position(buffer.limit());
+            return ret;
+        }
+        buffer.position((int) (buffer.position() + n));
+        return n;
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+        if (buffer.remaining() < len) {
+            throw new EOFException();
+        }
+        buffer.get(b, offset, len);
+    }
+
+    @Override
+    public void reset() throws IOException {
+        buffer.reset();
+    }
+
+    public void movePosition(int position) {
+        buffer.position(position);
+    }
+
+    public void rewind() throws IOException {
+        buffer.rewind();
+    }
+
+    @Override
+    public int available() throws IOException {
+        return buffer.remaining();
+    }
+
+    @Override
+    public void mark(int readlimit) {
+        buffer.mark();
+    }
+
+    @Override
+    public boolean markSupported() {
+        return true;
+    }
+
+    @Override
+    public void close() throws IOException {
+    }
+}
+
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReadManager.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReadManager.java
new file mode 100644
index 00000000000..d7b52041d0e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReadManager.java
@@ -0,0 +1,150 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.logstash.common.io;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.DLQEntry;
+import org.logstash.Timestamp;
+
+import java.io.IOException;
+import java.nio.file.FileSystems;
+import java.nio.file.Path;
+import java.nio.file.StandardWatchEventKinds;
+import java.nio.file.WatchEvent;
+import java.nio.file.WatchKey;
+import java.nio.file.WatchService;
+import java.util.concurrent.ConcurrentSkipListSet;
+import java.util.concurrent.TimeUnit;
+import java.util.function.Function;
+import java.util.stream.Collectors;
+
+import static java.nio.file.StandardWatchEventKinds.ENTRY_CREATE;
+import static java.nio.file.StandardWatchEventKinds.ENTRY_DELETE;
+import static org.logstash.common.io.DeadLetterQueueWriteManager.getSegmentPaths;
+
+public class DeadLetterQueueReadManager {
+    private static final Logger logger = LogManager.getLogger(DeadLetterQueueReadManager.class);
+
+    private RecordIOReader currentReader;
+    private final Path queuePath;
+    private final ConcurrentSkipListSet<Path> segments;
+    private final WatchService watchService;
+
+    public DeadLetterQueueReadManager(Path queuePath) throws Exception {
+        this.queuePath = queuePath;
+        this.watchService = FileSystems.getDefault().newWatchService();
+        this.queuePath.register(watchService, ENTRY_CREATE, ENTRY_DELETE);
+        this.segments = new ConcurrentSkipListSet<>((p1, p2) -> {
+            Function<Path, Integer> id = (p) -> Integer.parseInt(p.getFileName().toString().split("\\.")[0]);
+            return id.apply(p1).compareTo(id.apply(p2));
+        });
+
+        segments.addAll(getSegmentPaths(queuePath).collect(Collectors.toList()));
+    }
+
+    public void seekToNextEvent(Timestamp timestamp) throws IOException {
+        for (Path segment : segments) {
+            currentReader = new RecordIOReader(segment);
+            byte[] event = currentReader.seekToNextEventPosition(timestamp, (b) -> {
+                try {
+                    return DLQEntry.deserialize(b).getEntryTime();
+                } catch (IOException e) {
+                    return null;
+                }
+            }, Timestamp::compareTo);
+            if (event != null) {
+                return;
+            }
+        }
+        currentReader.close();
+        currentReader = null;
+    }
+
+    private long pollNewSegments(long timeout) throws IOException, InterruptedException {
+        long startTime = System.currentTimeMillis();
+        WatchKey key = watchService.poll(timeout, TimeUnit.MILLISECONDS);
+        if (key != null) {
+            for (WatchEvent<?> watchEvent : key.pollEvents()) {
+                if (watchEvent.kind() == StandardWatchEventKinds.ENTRY_CREATE) {
+                    segments.addAll(getSegmentPaths(queuePath).collect(Collectors.toList()));
+                }
+                key.reset();
+            }
+        }
+        return System.currentTimeMillis() - startTime;
+    }
+
+    public DLQEntry pollEntry(long timeout) throws IOException, InterruptedException {
+        byte[] bytes = pollEntryBytes(timeout);
+        if (bytes == null) {
+            return null;
+        }
+        return DLQEntry.deserialize(bytes);
+    }
+
+    byte[] pollEntryBytes() throws IOException, InterruptedException {
+        return pollEntryBytes(100);
+    }
+
+    byte[] pollEntryBytes(long timeout) throws IOException, InterruptedException {
+        long timeoutRemaining = timeout;
+        if (currentReader == null) {
+            timeoutRemaining -= pollNewSegments(timeout);
+            // If no new segments are found, exit
+            if (segments.isEmpty()) {
+                logger.debug("No entries found: no segment files found in dead-letter-queue directory");
+                return null;
+            }
+            currentReader = new RecordIOReader(segments.first());
+        }
+
+        byte[] event = currentReader.readEvent();
+        if (event == null && currentReader.isEndOfStream()) {
+            if (currentReader.getPath().equals(segments.last())) {
+                pollNewSegments(timeoutRemaining);
+            } else {
+                currentReader.close();
+                currentReader = new RecordIOReader(segments.higher(currentReader.getPath()));
+                return pollEntryBytes(timeoutRemaining);
+            }
+        }
+
+        return event;
+    }
+
+    public void setCurrentReaderAndPosition(Path segmentPath, long position) throws IOException {
+        currentReader = new RecordIOReader(segmentPath);
+        currentReader.seekToOffset(position);
+    }
+
+    public Path getCurrentSegment() {
+        return currentReader.getPath();
+    }
+
+    public long getCurrentPosition() throws IOException {
+        return currentReader.getChannelPosition();
+    }
+
+    public void close() throws IOException {
+        if (currentReader != null) {
+            currentReader.close();
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriteManager.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriteManager.java
new file mode 100644
index 00000000000..3a822fc2b61
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriteManager.java
@@ -0,0 +1,122 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.logstash.common.io;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.DLQEntry;
+
+import java.io.IOException;
+import java.nio.channels.FileChannel;
+import java.nio.channels.FileLock;
+import java.nio.channels.OverlappingFileLockException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.stream.Stream;
+
+import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
+
+public class DeadLetterQueueWriteManager {
+
+    private static final Logger logger = LogManager.getLogger(DeadLetterQueueWriteManager.class);
+
+    static final String SEGMENT_FILE_PATTERN = "%d.log";
+    static final String LOCK_FILE = ".lock";
+    private final long maxSegmentSize;
+    private final long maxQueueSize;
+    private final Path queuePath;
+    private final FileLock lock;
+    private RecordIOWriter currentWriter;
+    private long currentQueueSize;
+    private int currentSegmentIndex;
+
+    /**
+     *
+     * @param queuePath
+     * @param maxSegmentSize
+     * @throws IOException
+     */
+    public DeadLetterQueueWriteManager(Path queuePath, long maxSegmentSize, long maxQueueSize) throws IOException {
+        // check that only one instance of the writer is open in this configured path
+        Path lockFilePath = queuePath.resolve(LOCK_FILE);
+        boolean isNewlyCreated = lockFilePath.toFile().createNewFile();
+        FileChannel channel = FileChannel.open(lockFilePath, StandardOpenOption.WRITE);
+        try {
+            this.lock = channel.lock();
+        } catch (OverlappingFileLockException e) {
+            if (isNewlyCreated) {
+                logger.warn("Previous Dead Letter Queue Writer was not closed safely.");
+            }
+            throw new RuntimeException("uh oh, someone else is writing to this dead-letter queue");
+        }
+
+        this.queuePath = queuePath;
+        this.maxSegmentSize = maxSegmentSize;
+        this.maxQueueSize = maxQueueSize;
+        this.currentQueueSize = getStartupQueueSize();
+
+        currentSegmentIndex = getSegmentPaths(queuePath)
+                .map(s -> s.getFileName().toString().split("\\.")[0])
+                .mapToInt(Integer::parseInt)
+                .max().orElse(0);
+        this.currentWriter = nextWriter();
+    }
+
+    private long getStartupQueueSize() throws IOException {
+        return getSegmentPaths(queuePath)
+                .mapToLong((p) -> {
+                    try {
+                        return Files.size(p);
+                    } catch (IOException e) {
+                        return 0L;
+                    }
+                } )
+                .sum();
+    }
+
+    private RecordIOWriter nextWriter() throws IOException {
+        return new RecordIOWriter(queuePath.resolve(String.format(SEGMENT_FILE_PATTERN, ++currentSegmentIndex)));
+    }
+
+    static Stream<Path> getSegmentPaths(Path path) throws IOException {
+        return Files.list(path).filter((p) -> p.toString().endsWith(".log"));
+    }
+
+    public synchronized void writeEntry(DLQEntry event) throws IOException {
+        byte[] record = event.serialize();
+        int eventPayloadSize = RECORD_HEADER_SIZE + record.length;
+        if (currentQueueSize + eventPayloadSize > maxQueueSize) {
+            logger.error("cannot write event to DLQ, no space available");
+            return;
+        } else if (currentWriter.getPosition() + eventPayloadSize > maxSegmentSize) {
+            currentWriter.close();
+            currentWriter = nextWriter();
+        }
+        currentQueueSize += currentWriter.writeEvent(record);
+    }
+
+    public synchronized void close() throws IOException {
+        this.lock.release();
+        if (currentWriter != null) {
+            currentWriter.close();
+        }
+        Files.deleteIfExists(queuePath.resolve(LOCK_FILE));
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/InputStreamStreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/InputStreamStreamInput.java
new file mode 100644
index 00000000000..712e42b87b1
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/InputStreamStreamInput.java
@@ -0,0 +1,77 @@
+package org.logstash.common.io;
+
+import java.io.EOFException;
+import java.io.IOException;
+import java.io.InputStream;
+
+public class InputStreamStreamInput extends StreamInput {
+
+    private final InputStream is;
+
+    public InputStreamStreamInput(InputStream is) {
+        this.is = is;
+    }
+
+    @Override
+    public byte readByte() throws IOException {
+        int ch = is.read();
+        if (ch < 0)
+            throw new EOFException();
+        return (byte) (ch);
+    }
+
+    @Override
+    public void readBytes(byte[] b, int offset, int len) throws IOException {
+        if (len < 0)
+            throw new IndexOutOfBoundsException();
+        final int read = Streams.readFully(is, b, offset, len);
+        if (read != len) {
+            throw new EOFException();
+        }
+    }
+
+    @Override
+    public void reset() throws IOException {
+        is.reset();
+    }
+
+    @Override
+    public boolean markSupported() {
+        return is.markSupported();
+    }
+
+    @Override
+    public void mark(int readlimit) {
+        is.mark(readlimit);
+    }
+
+    @Override
+    public void close() throws IOException {
+        is.close();
+    }
+
+    @Override
+    public int available() throws IOException {
+        return is.available();
+    }
+
+    @Override
+    public int read() throws IOException {
+        return is.read();
+    }
+
+    @Override
+    public int read(byte[] b) throws IOException {
+        return is.read(b);
+    }
+
+    @Override
+    public int read(byte[] b, int off, int len) throws IOException {
+        return is.read(b, off, len);
+    }
+
+    @Override
+    public long skip(long n) throws IOException {
+        return is.skip(n);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/RecordHeader.java b/logstash-core/src/main/java/org/logstash/common/io/RecordHeader.java
new file mode 100644
index 00000000000..178f77d23f9
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/RecordHeader.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.logstash.common.io;
+
+import java.nio.ByteBuffer;
+import java.util.OptionalInt;
+
+public class RecordHeader {
+    private final RecordType type;
+    private final int size;
+    private final OptionalInt totalEventSize;
+    private final int checksum;
+
+    public RecordHeader(RecordType type, int size, OptionalInt totalEventSize, int checksum) {
+        this.type = type;
+        this.size = size;
+        this.totalEventSize = totalEventSize;
+        this.checksum = checksum;
+    }
+
+    public RecordType getType() {
+        return type;
+    }
+
+    public int getSize() {
+        return size;
+    }
+
+    public int getChecksum() {
+        return checksum;
+    }
+
+    public OptionalInt getTotalEventSize() {
+        return totalEventSize;
+    }
+
+    public static RecordHeader get(ByteBuffer currentBlock) {
+        RecordType type = RecordType.fromByte(currentBlock.get());
+
+        if (type == null) {
+            return null;
+        }
+
+        final int size = currentBlock.getInt();
+        final int totalSize = currentBlock.getInt();
+        final OptionalInt totalEventSize = (totalSize != -1) ? OptionalInt.of(totalSize) : OptionalInt.empty();
+        final int checksum = currentBlock.getInt();
+        return new RecordHeader(type, size, totalEventSize, checksum);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/RecordIOReader.java b/logstash-core/src/main/java/org/logstash/common/io/RecordIOReader.java
new file mode 100644
index 00000000000..2ea87a93010
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/RecordIOReader.java
@@ -0,0 +1,244 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.logstash.common.io;
+
+import org.logstash.Timestamp;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.ClosedByInterruptException;
+import java.nio.channels.FileChannel;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.Comparator;
+import java.util.function.Function;
+import java.util.function.Supplier;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
+import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
+import static org.logstash.common.io.RecordIOWriter.VERSION;
+import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
+
+/**
+ */
+public class RecordIOReader {
+
+    private final FileChannel channel;
+    private final ByteBuffer currentBlock;
+    private int currentBlockSizeReadFromChannel;
+    private final Path path;
+    private long channelPosition;
+
+    public RecordIOReader(Path path) throws IOException {
+        this.path = path;
+        this.channel = FileChannel.open(path, StandardOpenOption.READ);
+        this.currentBlock = ByteBuffer.allocate(BLOCK_SIZE);
+        this.currentBlockSizeReadFromChannel = 0;
+        ByteBuffer versionBuffer = ByteBuffer.allocate(1);
+        this.channel.read(versionBuffer);
+        versionBuffer.rewind();
+        if (versionBuffer.get() != VERSION) {
+            throw new RuntimeException("Invalid file. check version");
+        }
+        this.channelPosition = this.channel.position();
+    }
+
+    public Path getPath() {
+        return path;
+    }
+
+    public void seekToBlock(int bid) throws IOException {
+        seekToOffset(bid * BLOCK_SIZE + VERSION_SIZE);
+    }
+
+    public void seekToOffset(long channelOffset) throws IOException {
+        currentBlock.rewind();
+        currentBlockSizeReadFromChannel = 0;
+        channel.position(channelOffset);
+        channelPosition = channel.position();
+    }
+
+    public <T> byte[] seekToNextEventPosition(T target, Function<byte[], T> keyExtractor, Comparator<T> keyComparator) throws IOException {
+        int matchingBlock;
+        int lowBlock = 0;
+        int highBlock = (int) (channel.size() - VERSION_SIZE) / BLOCK_SIZE;
+
+        if (highBlock == 0) {
+            return null;
+        }
+
+        while (lowBlock < highBlock) {
+            int middle = (int) Math.ceil((highBlock + lowBlock) / 2.0);
+            seekToBlock(middle);
+            T found = keyExtractor.apply(readEvent());
+            int compare = keyComparator.compare(found, target);
+            if (compare > 0) {
+                highBlock = middle - 1;
+            } else if (compare < 0) {
+                lowBlock = middle;
+            } else {
+                matchingBlock = middle;
+                break;
+            }
+        }
+        matchingBlock = lowBlock;
+
+        // now sequential scan to event
+        seekToBlock(matchingBlock);
+        int currentPosition = 0;
+        int compare = -1;
+        byte[] event = null;
+        while (compare < 0) {
+            currentPosition = currentBlock.position();
+            event = readEvent();
+            if (event == null) {
+                return null;
+            }
+            compare = keyComparator.compare(keyExtractor.apply(event), target);
+        }
+        currentBlock.position(currentPosition);
+        return event;
+    }
+
+    public long getChannelPosition() throws IOException {
+        return channelPosition;
+    }
+
+    /**
+     *
+     * @param rewind
+     * @throws IOException
+     */
+    void consumeBlock(boolean rewind) throws IOException {
+        if (rewind) {
+            currentBlockSizeReadFromChannel = 0;
+            currentBlock.rewind();
+        } else if (currentBlockSizeReadFromChannel == BLOCK_SIZE) {
+            // already read enough, no need to read more
+            return;
+        }
+        int originalPosition = currentBlock.position();
+        int read = channel.read(currentBlock);
+        currentBlockSizeReadFromChannel += (read > 0) ? read : 0;
+        currentBlock.position(originalPosition);
+    }
+
+    /**
+     * basically, is last block
+     * @return
+     */
+    public boolean isEndOfStream() {
+        return currentBlockSizeReadFromChannel < BLOCK_SIZE;
+    }
+
+    /**
+     *
+     */
+     int seekToStartOfEventInBlock() throws IOException {
+         while (true) {
+             RecordType type = RecordType.fromByte(currentBlock.array()[currentBlock.arrayOffset() + currentBlock.position()]);
+             if (RecordType.COMPLETE.equals(type) || RecordType.START.equals(type)) {
+                 return currentBlock.position();
+             } else if (RecordType.END.equals(type)) {
+                 RecordHeader header = RecordHeader.get(currentBlock);
+                 currentBlock.position(currentBlock.position() + header.getSize());
+             } else {
+                 return -1;
+             }
+         }
+    }
+
+    /**
+     *
+     * @return true if ready to read event, false otherwise
+     * @throws IOException
+     */
+    boolean consumeToStartOfEvent() throws IOException {
+        // read and seek to start of event
+        consumeBlock(false);
+        while (true) {
+            int eventStartPosition = seekToStartOfEventInBlock();
+            if (eventStartPosition < 0) {
+                if (isEndOfStream()) {
+                    return false;
+                } else {
+                    consumeBlock(true);
+                }
+            } else {
+                return true;
+            }
+        }
+    }
+
+    private void maybeRollToNextBlock() throws IOException {
+        // check block position state
+        if (currentBlock.remaining() < RECORD_HEADER_SIZE + 1) {
+            consumeBlock(true);
+        }
+    }
+
+    private void getRecord(ByteBuffer buffer, RecordHeader header) throws IOException {
+        Checksum computedChecksum = new CRC32();
+        computedChecksum.update(currentBlock.array(), currentBlock.position(), header.getSize());
+
+        if ((int) computedChecksum.getValue() != header.getChecksum()) {
+            throw new RuntimeException("invalid checksum of record");
+        }
+
+        buffer.put(currentBlock.array(), currentBlock.position(), header.getSize());
+        currentBlock.position(currentBlock.position() + header.getSize());
+    }
+
+    /**
+     * @return
+     * @throws IOException
+     */
+    public byte[] readEvent() throws IOException {
+        try {
+            if (channel.isOpen() == false || consumeToStartOfEvent() == false) {
+                return null;
+            }
+            RecordHeader header = RecordHeader.get(currentBlock);
+            int cumReadSize = 0;
+            int bufferSize = header.getTotalEventSize().orElseGet(header::getSize);
+            ByteBuffer buffer = ByteBuffer.allocate(bufferSize);
+            getRecord(buffer, header);
+            cumReadSize += header.getSize();
+            while (cumReadSize < bufferSize) {
+                maybeRollToNextBlock();
+                RecordHeader nextHeader = RecordHeader.get(currentBlock);
+                getRecord(buffer, nextHeader);
+                cumReadSize += nextHeader.getSize();
+            }
+            return buffer.array();
+        } catch (ClosedByInterruptException e) {
+            return null;
+        } finally {
+            if (channel.isOpen()) {
+                channelPosition = channel.position();
+            }
+        }
+    }
+
+    public void close() throws IOException {
+        channel.close();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/RecordIOWriter.java b/logstash-core/src/main/java/org/logstash/common/io/RecordIOWriter.java
new file mode 100644
index 00000000000..2fb6acc5996
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/RecordIOWriter.java
@@ -0,0 +1,130 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.OptionalInt;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+import static org.logstash.common.io.RecordType.COMPLETE;
+import static org.logstash.common.io.RecordType.END;
+import static org.logstash.common.io.RecordType.MIDDLE;
+import static org.logstash.common.io.RecordType.START;
+
+/**
+ *
+ * File Format
+ * | â€” magic number (4bytes) --|
+ *
+ * [  32kbyte block....
+ *    --- 1 byte RecordHeader Type ---
+ *    --- 4 byte RecordHeader Size ---
+ *
+ * ]
+ * [ 32kbyte block...
+ *
+ *
+ *
+ * ]
+ */
+public class RecordIOWriter {
+
+    private final FileChannel channel;
+    private int posInBlock;
+    private int currentBlockIdx;
+
+    static final int BLOCK_SIZE = 32 * 1024; // package-private for tests
+    static final int RECORD_HEADER_SIZE = 13;
+    static final int VERSION_SIZE = 1;
+    static final char VERSION = '1';
+
+    public RecordIOWriter(Path recordsFile) throws IOException {
+        this.posInBlock = 0;
+        this.currentBlockIdx = 0;
+        recordsFile.toFile().createNewFile();
+        this.channel = FileChannel.open(recordsFile, StandardOpenOption.WRITE);
+        this.channel.write(ByteBuffer.wrap(new byte[] { VERSION }));
+    }
+
+    private int remainingInBlock() {
+        return BLOCK_SIZE - posInBlock;
+    }
+
+    int writeRecordHeader(RecordHeader header) throws IOException {
+        ByteBuffer buffer = ByteBuffer.allocate(RECORD_HEADER_SIZE);
+        buffer.put(header.getType().toByte());
+        buffer.putInt(header.getSize());
+        buffer.putInt(header.getTotalEventSize().orElse(-1));
+        buffer.putInt(header.getChecksum());
+        buffer.rewind();
+        return channel.write(buffer);
+    }
+
+    private RecordType getNextType(ByteBuffer buffer, RecordType previous) {
+        boolean fits = buffer.remaining() + RECORD_HEADER_SIZE < remainingInBlock();
+        if (previous == null) {
+            return (fits) ? COMPLETE : START;
+        }
+        if (previous == START || previous == MIDDLE) {
+            return (fits) ? END : MIDDLE;
+        }
+        return null;
+    }
+
+    public long getPosition() throws IOException {
+        return channel.position();
+    }
+
+    public long writeEvent(byte[] eventArray) throws IOException {
+        ByteBuffer eventBuffer = ByteBuffer.wrap(eventArray);
+        RecordType nextType = null;
+        ByteBuffer slice = eventBuffer.slice();
+        long startPosition = channel.position();
+        while (slice.hasRemaining()) {
+            if (posInBlock + RECORD_HEADER_SIZE + 1 > BLOCK_SIZE) {
+                channel.position((++currentBlockIdx) * BLOCK_SIZE + VERSION_SIZE);
+                posInBlock = 0;
+            }
+            nextType = getNextType(slice, nextType);
+            int originalLimit = slice.limit();
+            int nextRecordSize = Math.min(remainingInBlock() - RECORD_HEADER_SIZE, slice.remaining());
+            OptionalInt optTotalSize = (nextType == RecordType.START) ? OptionalInt.of(eventArray.length) : OptionalInt.empty();
+            slice.limit(nextRecordSize);
+
+            Checksum checksum = new CRC32();
+            checksum.update(slice.array(), slice.arrayOffset() + slice.position(), nextRecordSize);
+            posInBlock += writeRecordHeader(
+                    new RecordHeader(nextType, nextRecordSize, optTotalSize, (int) checksum.getValue()));
+            posInBlock += channel.write(slice);
+
+            slice.limit(originalLimit);
+            slice = slice.slice();
+        }
+        return channel.position() - startPosition;
+    }
+
+    public void close() throws IOException {
+        channel.close();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/RecordType.java b/logstash-core/src/main/java/org/logstash/common/io/RecordType.java
new file mode 100644
index 00000000000..3b2e90e3af8
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/RecordType.java
@@ -0,0 +1,56 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.logstash.common.io;
+
+public enum RecordType {
+    COMPLETE,
+    START,
+    MIDDLE,
+    END;
+
+    public byte toByte() {
+        switch (this) {
+            case COMPLETE:
+                return 'c';
+            case START:
+                return 's';
+            case MIDDLE:
+                return 'm';
+            case END:
+                return 'e';
+            default:
+                throw new RuntimeException("wat?");
+        }
+    }
+
+    public static RecordType fromByte(byte b) {
+        switch (b) {
+            case 'c':
+                return COMPLETE;
+            case 's':
+                return START;
+            case 'm':
+                return MIDDLE;
+            case 'e':
+                return END;
+            default:
+                return null;
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/StreamInput.java b/logstash-core/src/main/java/org/logstash/common/io/StreamInput.java
new file mode 100644
index 00000000000..2b8f22515d8
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/StreamInput.java
@@ -0,0 +1,101 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.io.InputStream;
+
+public abstract class StreamInput extends InputStream {
+    /**
+     * Reads and returns a single byte.
+     * @return byte from stream
+     * @throws IOException if error occurs while reading content
+     */
+    public abstract byte readByte() throws IOException;
+
+    /**
+     * Reads a specified number of bytes into an array at the specified offset.
+     *
+     * @param b      the array to read bytes into
+     * @param offset the offset in the array to start storing bytes
+     * @param len    the number of bytes to read
+     * @throws IOException if an error occurs while reading content
+     */
+    public abstract void readBytes(byte[] b, int offset, int len) throws IOException;
+
+    /**
+     * Reads four bytes and returns an int.
+     *
+     * @return four-byte integer value from bytes
+     * @throws IOException if an error occurs while reading content
+     */
+    public int readInt() throws IOException {
+        return ((readByte() & 0xFF) << 24) | ((readByte() & 0xFF) << 16)
+                | ((readByte() & 0xFF) << 8) | (readByte() & 0xFF);
+    }
+    
+    /**
+     * Reads an int stored in variable-length format.  Reads between one and
+     * five bytes.  Smaller values take fewer bytes.  Negative numbers
+     * will always use all 5 bytes and are therefore better serialized
+     * using {@link #readInt}
+     *
+     * @return integer value from var-int formatted bytes
+     * @throws IOException if an error occurs while reading content
+     */
+    public int readVInt() throws IOException {
+        byte b = readByte();
+        int i = b & 0x7F;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        i |= (b & 0x7F) << 7;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        i |= (b & 0x7F) << 14;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        i |= (b & 0x7F) << 21;
+        if ((b & 0x80) == 0) {
+            return i;
+        }
+        b = readByte();
+        assert (b & 0x80) == 0;
+        return i | ((b & 0x7F) << 28);
+    }
+
+    /**
+     * Reads two bytes and returns a short.
+     *
+     * @return short value from bytes
+     * @throws IOException if an error occurs while reading content
+     */
+    public short readShort() throws IOException {
+        int i = ((readByte() & 0xFF) <<  8);
+        int j = (readByte() & 0xFF);
+        return (short) (i | j);
+    }
+
+    /**
+     * Reads eight bytes and returns a long.
+     *
+     * @return long value from bytes
+     * @throws IOException if an error occurs while reading content
+     */
+    public long readLong() throws IOException {
+        return (((long) readInt()) << 32) | (readInt() & 0xFFFFFFFFL);
+    }
+
+    public byte[] readByteArray() throws IOException {
+        int length = readInt();
+        byte[] values = new byte[length];
+        for (int i = 0; i < length; i++) {
+            values[i] = readByte();
+        }
+        return values;
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/StreamOutput.java b/logstash-core/src/main/java/org/logstash/common/io/StreamOutput.java
new file mode 100644
index 00000000000..b0da079ac73
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/StreamOutput.java
@@ -0,0 +1,87 @@
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.io.OutputStream;
+
+public abstract class StreamOutput extends OutputStream {
+    @Override
+    public void write(int b) throws IOException {
+        writeByte((byte) b);
+    }
+
+    public abstract void writeByte(byte b) throws IOException;
+
+    public abstract void writeBytes(byte[] b, int offset, int length) throws IOException;
+
+    public abstract void reset() throws IOException;
+
+    /**
+     * Writes an int in a variable-length format.  Writes between one and
+     * five bytes.  Smaller values take fewer bytes.  Negative numbers
+     * will always use all 5 bytes and are therefore better serialized
+     * using {@link #writeInt}
+     *
+     * @param i The integer to write
+     * @throws IOException if an error occurs while writing content
+     */
+    public void writeVInt(int i) throws IOException {
+        while ((i & ~0x7F) != 0) {
+            writeByte((byte) ((i & 0x7f) | 0x80));
+            i >>>= 7;
+        }
+        writeByte((byte) i);
+    }
+
+    /**
+     * Writes a short as two bytes.
+     *
+     * @param i The short to write
+     * @throws IOException if an error occurs while writing content
+     */
+    public void writeShort(short i) throws IOException {
+        writeByte((byte)(i >>  8));
+        writeByte((byte) i);
+    }
+
+    /**
+     * Writes an int as four bytes.
+     *
+     * @param i The int to write
+     * @throws IOException if an error occurs while writing content
+     */
+    public void writeInt(int i) throws IOException {
+        writeByte((byte) (i >> 24));
+        writeByte((byte) (i >> 16));
+        writeByte((byte) (i >> 8));
+        writeByte((byte) i);
+    }
+
+    public void writeIntArray(int[] values) throws IOException {
+        writeVInt(values.length);
+        for (int value : values) {
+            writeInt(value);
+        }
+    }
+
+    /**
+     * Writes a long as eight bytes.
+     *
+     * @param i the long to write
+     * @throws IOException if an error occurs while writing content
+     */
+    public void writeLong(long i) throws IOException {
+        writeInt((int) (i >> 32));
+        writeInt((int) i);
+    }
+
+    /**
+     * Writes an array of bytes.
+     *
+     * @param b the bytes to write
+     * @throws IOException if an error occurs while writing content
+     */
+    public void writeByteArray(byte[] b) throws IOException {
+        writeInt(b.length);
+        writeBytes(b, 0, b.length);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/Streams.java b/logstash-core/src/main/java/org/logstash/common/io/Streams.java
new file mode 100644
index 00000000000..a156640ab3e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/Streams.java
@@ -0,0 +1,60 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.common.io;
+
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.Reader;
+
+public abstract class Streams {
+
+    public static int readFully(Reader reader, char[] dest) throws IOException {
+        return readFully(reader, dest, 0, dest.length);
+    }
+
+    public static int readFully(Reader reader, char[] dest, int offset, int len) throws IOException {
+        int read = 0;
+        while (read < len) {
+            final int r = reader.read(dest, offset + read, len - read);
+            if (r == -1) {
+                break;
+            }
+            read += r;
+        }
+        return read;
+    }
+
+    public static int readFully(InputStream reader, byte[] dest) throws IOException {
+        return readFully(reader, dest, 0, dest.length);
+    }
+
+    public static int readFully(InputStream reader, byte[] dest, int offset, int len) throws IOException {
+        int read = 0;
+        while (read < len) {
+            final int r = reader.read(dest, offset + read, len - read);
+            if (r == -1) {
+                break;
+            }
+            read += r;
+        }
+        return read;
+    }
+}
+
diff --git a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java b/logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
similarity index 82%
rename from logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
rename to logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
index 1cb630d5a75..2edd25e6645 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
+++ b/logstash-core/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
@@ -1,37 +1,20 @@
-package com.logstash.ext;
-
-import com.logstash.Logger;
-import com.logstash.Event;
-import com.logstash.PathCache;
-import com.logstash.Javafier;
-import com.logstash.Timestamp;
-import com.logstash.Rubyfier;
-import com.logstash.Javafier;
-import org.jruby.Ruby;
-import org.jruby.RubyObject;
-import org.jruby.RubyClass;
-import org.jruby.RubyModule;
-import org.jruby.RubyString;
-import org.jruby.RubyHash;
-import org.jruby.RubyBoolean;
-import org.jruby.RubyArray;
-import org.jruby.RubyFloat;
-import org.jruby.RubyInteger;
+package org.logstash.ext;
+
+import org.logstash.*;
+import org.jruby.*;
 import org.jruby.anno.JRubyClass;
 import org.jruby.anno.JRubyMethod;
 import org.jruby.exceptions.RaiseException;
+import org.jruby.java.proxies.MapJavaProxy;
 import org.jruby.javasupport.JavaUtil;
 import org.jruby.runtime.Arity;
 import org.jruby.runtime.ObjectAllocator;
 import org.jruby.runtime.ThreadContext;
 import org.jruby.runtime.builtin.IRubyObject;
 import org.jruby.runtime.load.Library;
-import org.jruby.ext.bigdecimal.RubyBigDecimal;
+
 import java.io.IOException;
 import java.util.Map;
-import java.util.HashMap;
-import java.util.List;
-
 
 public class JrubyEventExtLibrary implements Library {
 
@@ -39,10 +22,12 @@ public class JrubyEventExtLibrary implements Library {
     private static RubyClass GENERATOR_ERROR = null;
     private static RubyClass LOGSTASH_ERROR = null;
 
+    @Override
     public void load(Ruby runtime, boolean wrap) throws IOException {
         RubyModule module = runtime.defineModule("LogStash");
 
         RubyClass clazz = runtime.defineClassUnder("Event", runtime.getObject(), new ObjectAllocator() {
+            @Override
             public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
                 return new RubyEvent(runtime, rubyClass);
             }
@@ -53,7 +38,6 @@ public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
         clazz.setConstant("TIMESTAMP", runtime.newString(Event.TIMESTAMP));
         clazz.setConstant("TIMESTAMP_FAILURE_TAG", runtime.newString(Event.TIMESTAMP_FAILURE_TAG));
         clazz.setConstant("TIMESTAMP_FAILURE_FIELD", runtime.newString(Event.TIMESTAMP_FAILURE_FIELD));
-        clazz.setConstant("DEFAULT_LOGGER", runtime.getModule("Cabin").getClass("Channel").callMethod("get", runtime.getModule("LogStash")));
         clazz.setConstant("VERSION", runtime.newString(Event.VERSION));
         clazz.setConstant("VERSION_ONE", runtime.newString(Event.VERSION_ONE));
         clazz.defineAnnotatedMethods(RubyEvent.class);
@@ -73,24 +57,9 @@ public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
         }
     }
 
-    public static class ProxyLogger implements Logger {
-        private RubyObject logger;
-
-        public ProxyLogger(RubyObject logger) {
-             this.logger = logger;
-        }
-
-        // TODO: (colin) complete implementation beyond warn when needed
-
-        public void warn(String message) {
-            logger.callMethod("warn", RubyString.newString(logger.getRuntime(), message));
-        }
-    }
-
     @JRubyClass(name = "Event", parent = "Object")
     public static class RubyEvent extends RubyObject {
         private Event event;
-        private static RubyObject logger;
 
         public RubyEvent(Ruby runtime, RubyClass klass) {
             super(runtime, klass);
@@ -127,12 +96,10 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
             if (data == null || data.isNil()) {
                 this.event = new Event();
             } else if (data instanceof RubyHash) {
-                HashMap<String, Object>  newObj = Javafier.deep((RubyHash) data);
-                this.event = new Event(newObj);
-            } else if (data instanceof Map) {
-                this.event = new Event((Map) data);
-            } else if (Map.class.isAssignableFrom(data.getJavaClass())) {
-                this.event = new Event((Map)data.toJava(Map.class));
+                this.event = new Event(ConvertedMap.newFromRubyHash((RubyHash) data));
+            } else if (data instanceof MapJavaProxy) {
+                Map<String, Object> m = (Map)((MapJavaProxy)data).getObject();
+                this.event = new Event(ConvertedMap.newFromMap(m));
             } else {
                 throw context.runtime.newTypeError("wrong argument type " + data.getMetaClass() + " (expected Hash)");
             }
@@ -140,14 +107,14 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
             return context.nil;
         }
 
-        @JRubyMethod(name = "[]", required = 1)
+        @JRubyMethod(name = "get", required = 1)
         public IRubyObject ruby_get_field(ThreadContext context, RubyString reference)
         {
-            Object value = this.event.getField(reference.asJavaString());
+            Object value = this.event.getUnconvertedField(reference.asJavaString());
             return Rubyfier.deep(context.runtime, value);
         }
 
-        @JRubyMethod(name = "[]=", required = 2)
+        @JRubyMethod(name = "set", required = 2)
         public IRubyObject ruby_set_field(ThreadContext context, RubyString reference, IRubyObject value)
         {
             String r = reference.asJavaString();
@@ -158,7 +125,7 @@ public IRubyObject ruby_set_field(ThreadContext context, RubyString reference, I
                 }
                 this.event.setTimestamp(((JrubyTimestampExtLibrary.RubyTimestamp)value).getTimestamp());
             } else {
-                this.event.setField(r, Javafier.deep(value));
+                this.event.setField(r, Valuefier.convert(value));
             }
             return value;
         }
@@ -245,7 +212,7 @@ public IRubyObject ruby_to_s(ThreadContext context)
         @JRubyMethod(name = "to_hash")
         public IRubyObject ruby_to_hash(ThreadContext context) throws IOException
         {
-            return Rubyfier.deep(context.runtime, this.event.toMap());
+            return Rubyfier.deep(context.runtime, this.event.getData());
         }
 
         @JRubyMethod(name = "to_hash_with_metadata")
@@ -276,8 +243,8 @@ public IRubyObject ruby_to_json(ThreadContext context, IRubyObject[] args)
             }
         }
 
-        // @param value [String] the json string. A json object/map will convert to an array containing a single Event.
-        // and a json array will convert each element into individual Event
+        // @param value [String] the json string. A json object/map will newFromRubyArray to an array containing a single Event.
+        // and a json array will newFromRubyArray each element into individual Event
         // @return Array<Event> array of events
         @JRubyMethod(name = "from_json", required = 1, meta = true)
         public static IRubyObject ruby_from_json(ThreadContext context, IRubyObject recv, RubyString value)
@@ -312,8 +279,9 @@ public static IRubyObject ruby_validate_value(ThreadContext context, IRubyObject
         @JRubyMethod(name = "tag", required = 1)
         public IRubyObject ruby_tag(ThreadContext context, RubyString value)
         {
+            //TODO(guy) should these tags be BiValues?
             this.event.tag(((RubyString) value).asJavaString());
-            return context.runtime.getNil();
+            return context.nil;
         }
 
         @JRubyMethod(name = "timestamp")
@@ -330,14 +298,5 @@ public IRubyObject ruby_set_timestamp(ThreadContext context, IRubyObject value)
             this.event.setTimestamp(((JrubyTimestampExtLibrary.RubyTimestamp)value).getTimestamp());
             return value;
         }
-
-        // set a new logger for all Event instances
-        // there is no point in changing it at runtime for other reasons than in tests/specs.
-        @JRubyMethod(name = "logger=", required = 1, meta = true)
-        public static IRubyObject ruby_set_logger(ThreadContext context, IRubyObject recv, IRubyObject value)
-        {
-            Event.setLogger(new ProxyLogger((RubyObject)value));
-            return value;
-        }
     }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java b/logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
similarity index 93%
rename from logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java
rename to logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
index 9748a815ccb..46f2da41ca0 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java
+++ b/logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
@@ -1,6 +1,5 @@
-package com.logstash.ext;
+package org.logstash.ext;
 
-import com.logstash.*;
 import org.jruby.*;
 import org.jruby.anno.JRubyClass;
 import org.jruby.anno.JRubyMethod;
@@ -12,18 +11,27 @@
 import org.jruby.runtime.ThreadContext;
 import org.jruby.runtime.builtin.IRubyObject;
 import org.jruby.runtime.load.Library;
+import org.logstash.Timestamp;
 
 import java.io.IOException;
 
 public class JrubyTimestampExtLibrary implements Library {
+
+    private static final ObjectAllocator ALLOCATOR = new ObjectAllocator() {
+        public RubyTimestamp allocate(Ruby runtime, RubyClass rubyClass) {
+            return new RubyTimestamp(runtime, rubyClass);
+        }
+    };
+
     public void load(Ruby runtime, boolean wrap) throws IOException {
+        createTimestamp(runtime);
+    }
+
+    public static RubyClass createTimestamp(Ruby runtime) {
         RubyModule module = runtime.defineModule("LogStash");
-        RubyClass clazz = runtime.defineClassUnder("Timestamp", runtime.getObject(), new ObjectAllocator() {
-            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
-                return new RubyTimestamp(runtime, rubyClass);
-            }
-        }, module);
+        RubyClass clazz = runtime.defineClassUnder("Timestamp", runtime.getObject(), ALLOCATOR, module);
         clazz.defineAnnotatedMethods(RubyTimestamp.class);
+        return clazz;
     }
 
     @JRubyClass(name = "Timestamp", parent = "Object")
@@ -201,7 +209,7 @@ public static IRubyObject ruby_at(ThreadContext context, IRubyObject recv, IRuby
                 IRubyObject epoch = args[0];
 
                 if (epoch instanceof RubyBigDecimal) {
-                    // bug in JRuby prevents correcly parsing a BigDecimal fractional part, see https://github.com/elastic/logstash/issues/4565
+                    // bug in JRuby prevents correctly parsing a BigDecimal fractional part, see https://github.com/elastic/logstash/issues/4565
                     double usec = ((RubyBigDecimal)epoch).frac().convertToFloat().getDoubleValue() * 1000000;
                     t = (RubyTime)RubyTime.at(context, context.runtime.getTime(), ((RubyBigDecimal)epoch).to_int(), new RubyFloat(context.runtime, usec));
                 } else {
diff --git a/logstash-core/src/main/java/org/logstash/json/TimestampSerializer.java b/logstash-core/src/main/java/org/logstash/json/TimestampSerializer.java
new file mode 100644
index 00000000000..cc61e45c6e1
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/json/TimestampSerializer.java
@@ -0,0 +1,18 @@
+package org.logstash.json;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+import org.logstash.Timestamp;
+
+import java.io.IOException;
+
+public class TimestampSerializer extends JsonSerializer<Timestamp> {
+
+    @Override
+    public void serialize(Timestamp value, JsonGenerator jgen, SerializerProvider provider)
+            throws IOException
+    {
+        jgen.writeString(value.toIso8601());
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/CustomLogEvent.java b/logstash-core/src/main/java/org/logstash/log/CustomLogEvent.java
new file mode 100644
index 00000000000..f382bce75a1
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/CustomLogEvent.java
@@ -0,0 +1,37 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.log;
+
+import com.fasterxml.jackson.databind.annotation.JsonSerialize;
+import org.apache.logging.log4j.Level;
+import org.apache.logging.log4j.Marker;
+import org.apache.logging.log4j.core.config.Property;
+import org.apache.logging.log4j.core.impl.Log4jLogEvent;
+import org.apache.logging.log4j.message.Message;
+
+import java.util.List;
+
+@JsonSerialize(using = CustomLogEventSerializer.class)
+public class CustomLogEvent extends Log4jLogEvent {
+    public CustomLogEvent(final String loggerName, final Marker marker, final String loggerFQCN, final Level level,
+                          final Message message, final List<Property> properties, final Throwable t) {
+        super(loggerName, marker, loggerFQCN, level, message, properties, t);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/CustomLogEventSerializer.java b/logstash-core/src/main/java/org/logstash/log/CustomLogEventSerializer.java
new file mode 100644
index 00000000000..6432394d91f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/CustomLogEventSerializer.java
@@ -0,0 +1,48 @@
+package org.logstash.log;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonMappingException;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class CustomLogEventSerializer extends JsonSerializer<CustomLogEvent> {
+    @Override
+    public void serialize(CustomLogEvent event, JsonGenerator generator, SerializerProvider provider) throws IOException {
+        generator.writeStartObject();
+        generator.writeObjectField("level", event.getLevel());
+        generator.writeObjectField("loggerName", event.getLoggerName());
+        generator.writeObjectField("timeMillis", event.getTimeMillis());
+        generator.writeObjectField("thread", event.getThreadName());
+        generator.writeFieldName("logEvent");
+        generator.writeStartObject();
+        if (event.getMessage() instanceof StructuredMessage) {
+            StructuredMessage message = (StructuredMessage) event.getMessage();
+            generator.writeStringField("message", message.getMessage());
+            if (message.getParams() != null) {
+                for (Map.Entry<Object, Object> entry : message.getParams().entrySet()) {
+                    Object value = entry.getValue();
+                    try {
+                        generator.writeObjectField(entry.getKey().toString(), value);
+                    } catch (JsonMappingException e) {
+                        generator.writeObjectField(entry.getKey().toString(), value.toString());
+                    }
+                }
+            }
+
+        } else {
+            generator.writeStringField("message", event.getMessage().getFormattedMessage());
+        }
+
+        generator.writeEndObject();
+        generator.writeEndObject();
+    }
+
+    @Override
+    public Class<CustomLogEvent> handledType() {
+
+        return CustomLogEvent.class;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/LogstashLogEventFactory.java b/logstash-core/src/main/java/org/logstash/log/LogstashLogEventFactory.java
new file mode 100644
index 00000000000..f13c264dd21
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/LogstashLogEventFactory.java
@@ -0,0 +1,17 @@
+package org.logstash.log;
+
+import org.apache.logging.log4j.Level;
+import org.apache.logging.log4j.Marker;
+import org.apache.logging.log4j.core.LogEvent;
+import org.apache.logging.log4j.core.config.Property;
+import org.apache.logging.log4j.core.impl.LogEventFactory;
+import org.apache.logging.log4j.message.Message;
+
+import java.util.List;
+
+public class LogstashLogEventFactory implements LogEventFactory {
+    @Override
+    public LogEvent createEvent(String loggerName, Marker marker, String fqcn, Level level, Message data, List<Property> properties, Throwable t) {
+        return new CustomLogEvent(loggerName, marker, fqcn, level, data, properties, t);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/LogstashMessageFactory.java b/logstash-core/src/main/java/org/logstash/log/LogstashMessageFactory.java
new file mode 100644
index 00000000000..7cadbbf0385
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/LogstashMessageFactory.java
@@ -0,0 +1,33 @@
+package org.logstash.log;
+
+import org.apache.logging.log4j.message.Message;
+import org.apache.logging.log4j.message.MessageFactory;
+import org.apache.logging.log4j.message.ObjectMessage;
+import org.apache.logging.log4j.message.ParameterizedMessage;
+import org.apache.logging.log4j.message.SimpleMessage;
+
+import java.util.Map;
+
+public final class LogstashMessageFactory implements MessageFactory {
+
+    public static final LogstashMessageFactory INSTANCE = new LogstashMessageFactory();
+
+    @Override
+    public Message newMessage(Object message) {
+        return new ObjectMessage(message);
+    }
+
+    @Override
+    public Message newMessage(String message) {
+        return new SimpleMessage(message);
+    }
+
+    @Override
+    public Message newMessage(String message, Object... params) {
+        if (params.length == 1 && params[0] instanceof Map) {
+            return new StructuredMessage(message, params);
+        } else {
+            return new ParameterizedMessage(message, params);
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/StructuredMessage.java b/logstash-core/src/main/java/org/logstash/log/StructuredMessage.java
new file mode 100644
index 00000000000..2145acea746
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/StructuredMessage.java
@@ -0,0 +1,76 @@
+package org.logstash.log;
+
+import com.fasterxml.jackson.databind.annotation.JsonSerialize;
+import org.apache.logging.log4j.message.Message;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+@JsonSerialize(using = CustomLogEventSerializer.class)
+public class StructuredMessage implements Message {
+    private final String message;
+    private final Map<Object, Object> params;
+
+    @SuppressWarnings("unchecked")
+    public StructuredMessage(String message) {
+        this(message, (Map) null);
+    }
+
+    @SuppressWarnings("unchecked")
+    public StructuredMessage(String message, Object[] params) {
+        final Map<Object, Object> paramsMap;
+        if (params.length == 1 && params[0] instanceof Map) {
+            paramsMap = (Map) params[0];
+        } else {
+            paramsMap = new HashMap<>();
+            try {
+                for (int i = 0; i < params.length; i += 2) {
+                    paramsMap.put(params[i].toString(), params[i + 1]);
+                }
+            } catch (IndexOutOfBoundsException e) {
+                throw new IllegalArgumentException("must log key-value pairs");
+            }
+        }
+        this.message = message;
+        this.params = paramsMap;
+    }
+
+    public StructuredMessage(String message, Map<Object, Object> params) {
+        this.message = message;
+        this.params = params;
+    }
+
+    public String getMessage() {
+        return message;
+    }
+
+    public Map<Object, Object> getParams() {
+        return params;
+    }
+
+    @Override
+    public Object[] getParameters() {
+        return params.values().toArray();
+    }
+
+    @Override
+    public String getFormattedMessage() {
+        String formatted = message;
+        if (params != null && !params.isEmpty()) {
+            formatted += " " + params;
+        }
+        return formatted;
+    }
+
+    @Override
+    public String getFormat() {
+        return null;
+    }
+
+
+    @Override
+    public Throwable getThrowable() {
+        return null;
+    }
+}
diff --git a/logstash-core/src/main/resources/log4j2.component.properties b/logstash-core/src/main/resources/log4j2.component.properties
new file mode 100644
index 00000000000..18b737986ae
--- /dev/null
+++ b/logstash-core/src/main/resources/log4j2.component.properties
@@ -0,0 +1,2 @@
+Log4jLogEventFactory=org.logstash.log.LogstashLogEventFactory
+log4j2.messageFactory=org.logstash.log.LogstashMessageFactory
diff --git a/logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java b/logstash-core/src/test/java/org/logstash/AccessorsTest.java
similarity index 66%
rename from logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java
rename to logstash-core/src/test/java/org/logstash/AccessorsTest.java
index 61855abc34b..3218e96c535 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java
+++ b/logstash-core/src/test/java/org/logstash/AccessorsTest.java
@@ -1,6 +1,13 @@
-package com.logstash;
-
+package org.logstash;
+
+import org.junit.experimental.theories.DataPoint;
+import org.junit.Rule;
+import org.junit.rules.ExpectedException;
+import org.junit.experimental.theories.Theories;
+import org.junit.experimental.theories.Theory;
+import org.junit.runner.RunWith;
 import org.junit.Test;
+
 import static org.junit.Assert.*;
 
 import java.util.ArrayList;
@@ -120,6 +127,30 @@ public void testAbsentDeepListGet() throws Exception {
         assertEquals(accessors.get(reference), null);
         assertEquals(accessors.lutGet(reference), inner);
     }
+    /*
+     * Check if accessors are able to recovery from
+     * failure to convert the key (string) to integer,
+     * when it is a non-numeric value, which is not
+     * expected.
+     */
+    @Test
+    public void testInvalidIdList() throws Exception {
+        Map data = new HashMap();
+        List inner = new ArrayList();
+        data.put("map1", inner);
+        inner.add("obj1");
+        inner.add("obj2");
+
+        String reference = "[map1][IdNonNumeric]";
+
+        TestableAccessors accessors = new TestableAccessors(data);
+        assertEquals(accessors.lutGet(reference), null);
+        assertEquals(accessors.get(reference), null);
+        assertEquals(accessors.set(reference, "obj3"), null);
+        assertEquals(accessors.lutGet(reference), inner);
+        assertFalse(accessors.includes(reference));
+        assertEquals(accessors.del(reference), null);
+    }
 
     @Test
     public void testBarePut() throws Exception {
@@ -182,4 +213,62 @@ public void testNilInclude() throws Exception {
 
         assertEquals(accessors.includes("nilfield"), true);
     }
+
+    @Test
+    public void testInvalidPath() throws Exception {
+        Map data = new HashMap();
+        Accessors accessors = new Accessors(data);
+
+        assertEquals(accessors.set("[foo]", 1), 1);
+        assertEquals(accessors.get("[foo][bar]"), null);
+    }
+
+    @Test
+    public void testStaleTargetCache() throws Exception {
+        Map data = new HashMap();
+
+        Accessors accessors = new Accessors(data);
+
+        assertEquals(accessors.get("[foo][bar]"), null);
+        assertEquals(accessors.set("[foo][bar]", "baz"), "baz");
+        assertEquals(accessors.get("[foo][bar]"), "baz");
+
+        assertEquals(accessors.set("[foo]", "boom"), "boom");
+        assertEquals(accessors.get("[foo][bar]"), null);
+        assertEquals(accessors.get("[foo]"), "boom");
+    }
+
+    @RunWith(Theories.class)
+    public static class TestListIndexFailureCases {
+      private static final int size = 10;
+
+      @DataPoint
+      public static final int tooLarge = size;
+
+      @DataPoint
+      public static final int tooLarge1 = size+1;
+
+      @DataPoint
+      public static final int tooLargeNegative = -size - 1;
+
+      @Rule
+      public ExpectedException exception = ExpectedException.none();
+
+      @Theory
+      public void testListIndexOutOfBounds(int i) {
+        exception.expect(IndexOutOfBoundsException.class);
+        Accessors.listIndex(i, size);
+      }
+    }
+
+    public static class TestListIndex {
+      public void testListIndexOutOfBounds() {
+        assertEquals(Accessors.listIndex(0, 10), 0);
+        assertEquals(Accessors.listIndex(1, 10), 1);
+        assertEquals(Accessors.listIndex(9, 10), 9);
+        assertEquals(Accessors.listIndex(-1, 10), 9);
+        assertEquals(Accessors.listIndex(-9, 10), 1);
+        assertEquals(Accessors.listIndex(-10, 10), 0);
+      }
+    }
 }
diff --git a/logstash-core/src/test/java/org/logstash/DLQEntryTest.java b/logstash-core/src/test/java/org/logstash/DLQEntryTest.java
new file mode 100644
index 00000000000..2ad35b4669b
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/DLQEntryTest.java
@@ -0,0 +1,55 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash;
+
+import org.junit.Test;
+
+import java.util.Collections;
+
+import static net.javacrumbs.jsonunit.JsonAssert.assertJsonEquals;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.core.IsEqual.equalTo;
+import static org.junit.Assert.assertNotNull;
+
+public class DLQEntryTest {
+    @Test
+    public void testConstruct() throws Exception {
+        Event event = new Event(Collections.singletonMap("key", "value"));
+        DLQEntry entry = new DLQEntry(event, "type", "id", "reason");
+        assertThat(entry.getEvent(), equalTo(event));
+        assertNotNull(entry.getEntryTime());
+        assertThat(entry.getPluginType(), equalTo("type"));
+        assertThat(entry.getPluginId(), equalTo("id"));
+        assertThat(entry.getReason(), equalTo("reason"));
+    }
+
+    @Test
+    public void testSerDe() throws Exception {
+        Event event = new Event(Collections.singletonMap("key", "value"));
+        DLQEntry expected = new DLQEntry(event, "type", "id", "reason");
+        byte[] bytes = expected.serialize();
+        DLQEntry actual = DLQEntry.deserialize(bytes);
+        assertJsonEquals(actual.getEvent().toJson(), event.toJson());
+        assertThat(actual.getEntryTime().toIso8601(), equalTo(expected.getEntryTime().toIso8601()));
+        assertThat(actual.getPluginType(), equalTo("type"));
+        assertThat(actual.getPluginId(), equalTo("id"));
+        assertThat(actual.getReason(), equalTo("reason"));
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/EventTest.java b/logstash-core/src/test/java/org/logstash/EventTest.java
similarity index 61%
rename from logstash-core-event-java/src/test/java/com/logstash/EventTest.java
rename to logstash-core/src/test/java/org/logstash/EventTest.java
index 505a0593535..93ae7267992 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/EventTest.java
+++ b/logstash-core/src/test/java/org/logstash/EventTest.java
@@ -1,13 +1,83 @@
-package com.logstash;
+package org.logstash;
 
+import org.junit.Assert;
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.*;
-import static org.junit.Assert.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
 import static net.javacrumbs.jsonunit.JsonAssert.assertJsonEquals;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
 
 public class EventTest {
+    @Test
+    public void queueableInterfaceWithoutSeqNumRoundTrip() throws Exception {
+        Event e = new Event();
+        e.setField("foo", 42L);
+        e.setField("bar", 42);
+        HashMap inner = new HashMap(2);
+        inner.put("innerFoo", 42L);
+        inner.put("innerQuux", 42.42);
+        e.setField("baz", inner);
+        e.setField("[@metadata][foo]", 42L);
+        byte[] binary = e.serializeWithoutSeqNum();
+        Event er = Event.deserialize(binary);
+        assertEquals(42L, er.getField("foo"));
+        assertEquals(42, er.getField("bar"));
+        assertEquals(42L, er.getField("[baz][innerFoo]"));
+        assertEquals(42.42, er.getField("[baz][innerQuux]"));
+        assertEquals(42L, er.getField("[@metadata][foo]"));
+
+        assertEquals(e.getTimestamp().toIso8601(), er.getTimestamp().toIso8601());
+    }
+
+    @Test
+    public void queueableInterfaceRoundTrip() throws Exception {
+        Event e = new Event();
+        e.setField("foo", 42L);
+        e.setField("bar", 42);
+        HashMap inner = new HashMap(2);
+        inner.put("innerFoo", 42L);
+        inner.put("innerQuux", 42.42);
+        e.setField("baz", inner);
+        e.setField("[@metadata][foo]", 42L);
+        byte[] binary = e.serialize();
+        Event er = Event.deserialize(binary);
+        assertEquals(42L, er.getField("foo"));
+        assertEquals(42, er.getField("bar"));
+        assertEquals(42L, er.getField("[baz][innerFoo]"));
+        assertEquals(42.42, er.getField("[baz][innerQuux]"));
+        assertEquals(42L, er.getField("[@metadata][foo]"));
+
+        assertEquals(e.getTimestamp().toIso8601(), er.getTimestamp().toIso8601());
+    }
+
+    @Test
+    public void toBinaryRoundtrip() throws Exception {
+        Event e = new Event();
+        e.setField("foo", 42L);
+        e.setField("bar", 42);
+        HashMap inner = new HashMap(2);
+        inner.put("innerFoo", 42L);
+        inner.put("innerQuux", 42.42);
+        e.setField("baz", inner);
+        e.setField("[@metadata][foo]", 42L);
+        byte[] binary = e.toBinary();
+        Event er = Event.fromBinary(binary);
+        assertEquals(42L, er.getField("foo"));
+        assertEquals(42, er.getField("bar"));
+        assertEquals(42L, er.getField("[baz][innerFoo]"));
+        assertEquals(42.42, er.getField("[baz][innerQuux]"));
+        assertEquals(42L, er.getField("[@metadata][foo]"));
+
+        assertEquals(e.getTimestamp().toIso8601(), er.getTimestamp().toIso8601());
+    }
+
     @Test
     public void testBareToJson() throws Exception {
         Event e = new Event();
@@ -104,6 +174,15 @@ public void testClone() throws Exception {
         assertJsonEquals(f.toJson(), e.toJson());
     }
 
+    @Test
+    public void testToMap() throws Exception {
+        Event e = new Event();
+        Map original = e.getData();
+        Map clone = e.toMap();
+        assertFalse(original == clone);
+        assertEquals(original, clone);
+    }
+
     @Test
     public void testAppend() throws Exception {
         Map data1 = new HashMap();
@@ -116,7 +195,9 @@ public void testAppend() throws Exception {
         Event e2 = new Event(data2);
         e.append(e2);
 
-        assertEquals(Arrays.asList("original1", "original2"), e.getField("field1"));
+        assertEquals(2, ((List) e.getField("[field1]")).size());
+        assertEquals("original1", e.getField("[field1][0]"));
+        assertEquals("original2", e.getField("[field1][1]"));
     }
 
     @Test
@@ -186,4 +267,28 @@ public void testFromJsonWithInvalidJsonArray2() throws Exception {
     public void testFromJsonWithPartialInvalidJsonArray() throws Exception {
         Event.fromJson("[{\"foo\":\"bar\"}, 1]");
     }
+
+    @Test
+    public void testTagOnEmptyTagsField() throws Exception {
+        Event e = new Event();
+        e.tag("foo");
+
+        List<String> tags = (List<String>)e.getField("tags");
+        assertEquals(tags.size(), 1);
+        assertEquals(tags.get(0), "foo");
+    }
+
+    @Test
+    public void testTagOnExistingTagsField() throws Exception {
+        Map data = new HashMap();
+        data.put("tags", "foo");
+        Event e = new Event(data);
+        e.tag("bar");
+
+        List<String> tags = (List<String>)e.getField("tags");
+        assertEquals(tags.size(), 2);
+        assertEquals(tags.get(0), "foo");
+        assertEquals(tags.get(1), "bar");
+      }
+
 }
diff --git a/logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java b/logstash-core/src/test/java/org/logstash/FieldReferenceTest.java
similarity index 91%
rename from logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java
rename to logstash-core/src/test/java/org/logstash/FieldReferenceTest.java
index 73f04e3a7c4..280975f3230 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java
+++ b/logstash-core/src/test/java/org/logstash/FieldReferenceTest.java
@@ -1,11 +1,7 @@
-package com.logstash;
+package org.logstash;
 
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
 import static org.junit.Assert.*;
 
 public class FieldReferenceTest {
diff --git a/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
new file mode 100644
index 00000000000..f11c97dd2f6
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
@@ -0,0 +1,91 @@
+package org.logstash;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.nio.channels.FileLock;
+import java.nio.file.FileSystems;
+import java.nio.file.Path;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+
+public class FileLockFactoryTest {
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+    private String lockDir;
+    private final String LOCK_FILE = ".test";
+
+    private FileLock lock;
+
+    @Before
+    public void setUp() throws Exception {
+        lockDir = temporaryFolder.newFolder("lock").getPath();
+    }
+
+    @Before
+    public void lockFirst() throws Exception {
+        lock = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock.isValid(), is(equalTo(true)));
+        assertThat(lock.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void ObtainLockOnNonLocked() throws IOException {
+        // empty to just test the lone @Before lockFirst() test
+    }
+
+    @Test(expected = LockException.class)
+    public void ObtainLockOnLocked() throws IOException {
+        FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+    }
+
+    @Test
+    public void ObtainLockOnOtherLocked() throws IOException {
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, ".test2");
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void LockReleaseLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+    }
+
+    @Test
+    public void LockReleaseLockObtainLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void LockReleaseLockObtainLockRelease() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+
+        FileLockFactory.getDefault().releaseLock(lock2);
+    }
+
+    @Test(expected = LockException.class)
+    public void ReleaseNullLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(null);
+     }
+
+    @Test(expected = LockException.class)
+    public void ReleaseUnobtainedLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+        FileLockFactory.getDefault().releaseLock(lock);
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/JavafierTest.java b/logstash-core/src/test/java/org/logstash/JavafierTest.java
new file mode 100644
index 00000000000..e198bf507c5
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/JavafierTest.java
@@ -0,0 +1,19 @@
+package org.logstash;
+
+import org.jruby.RubyBignum;
+import org.junit.Test;
+
+import java.math.BigInteger;
+import static org.junit.Assert.assertEquals;
+
+public class JavafierTest extends TestBase {
+
+    @Test
+    public void testRubyBignum() {
+        RubyBignum v = RubyBignum.newBignum(ruby, "-9223372036854776000");
+
+        Object result = Javafier.deep(v);
+        assertEquals(BigInteger.class, result.getClass());
+        assertEquals( "-9223372036854776000", result.toString());
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java b/logstash-core/src/test/java/org/logstash/KeyNodeTest.java
similarity index 98%
rename from logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java
rename to logstash-core/src/test/java/org/logstash/KeyNodeTest.java
index 23cb27ac997..ea5b69c8920 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java
+++ b/logstash-core/src/test/java/org/logstash/KeyNodeTest.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import org.junit.Test;
 
diff --git a/logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java b/logstash-core/src/test/java/org/logstash/RubyfierTest.java
similarity index 75%
rename from logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java
rename to logstash-core/src/test/java/org/logstash/RubyfierTest.java
index af8ecbf0c28..df14a70729f 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java
+++ b/logstash-core/src/test/java/org/logstash/RubyfierTest.java
@@ -1,6 +1,11 @@
-package com.logstash;
-
-import org.jruby.*;
+package org.logstash;
+
+import org.jruby.RubyArray;
+import org.jruby.RubyBignum;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyFloat;
+import org.jruby.RubyHash;
+import org.jruby.RubyString;
 import org.jruby.ext.bigdecimal.RubyBigDecimal;
 import org.jruby.javasupport.JavaUtil;
 import org.jruby.runtime.builtin.IRubyObject;
@@ -8,18 +13,19 @@
 
 import java.lang.reflect.Method;
 import java.math.BigDecimal;
+import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static org.junit.Assert.*;
+import static org.junit.Assert.assertEquals;
 
-public class RubyfierTest {
+public class RubyfierTest extends TestBase {
 
     @Test
     public void testDeepWithString() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), "foo");
+        Object result = Rubyfier.deep(ruby, "foo");
         assertEquals(RubyString.class, result.getClass());
         assertEquals("foo", result.toString());
     }
@@ -30,14 +36,14 @@ public void testDeepMapWithString()
     {
         Map data = new HashMap();
         data.put("foo", "bar");
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash) Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyString.class, result.getClass());
         assertEquals("bar", result.toString());
@@ -50,16 +56,16 @@ public void testDeepListWithString()
         List data = new ArrayList();
         data.add("foo");
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyString.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals("foo", rubyArray.toJavaArray()[0].toString());
     }
 
     @Test
     public void testDeepWithInteger() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1);
+        Object result = Rubyfier.deep(ruby, 1);
         assertEquals(RubyFixnum.class, result.getClass());
         assertEquals(1L, ((RubyFixnum)result).getLongValue());
     }
@@ -70,14 +76,14 @@ public void testDeepMapWithInteger()
     {
         Map data = new HashMap();
         data.put("foo", 1);
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyFixnum.class, result.getClass());
         assertEquals(1L, ((RubyFixnum)result).getLongValue());
@@ -90,16 +96,16 @@ public void testDeepListWithInteger()
         List data = new ArrayList();
         data.add(1);
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyFixnum.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1L, ((RubyFixnum)rubyArray.toJavaArray()[0]).getLongValue());
     }
 
     @Test
     public void testDeepWithFloat() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1.0F);
+        Object result = Rubyfier.deep(ruby, 1.0F);
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
     }
@@ -110,14 +116,14 @@ public void testDeepMapWithFloat()
     {
         Map data = new HashMap();
         data.put("foo", 1.0F);
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
@@ -130,16 +136,16 @@ public void testDeepListWithFloat()
         List data = new ArrayList();
         data.add(1.0F);
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
 
     @Test
     public void testDeepWithDouble() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1.0D);
+        Object result = Rubyfier.deep(ruby, 1.0D);
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
     }
@@ -150,14 +156,14 @@ public void testDeepMapWithDouble()
     {
         Map data = new HashMap();
         data.put("foo", 1.0D);
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
@@ -170,16 +176,16 @@ public void testDeepListWithDouble()
         List data = new ArrayList();
         data.add(1.0D);
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
 
     @Test
     public void testDeepWithBigDecimal() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), new BigDecimal(1));
+        Object result = Rubyfier.deep(ruby, new BigDecimal(1));
         assertEquals(RubyBigDecimal.class, result.getClass());
         assertEquals(1.0D, ((RubyBigDecimal)result).getDoubleValue(), 0);
     }
@@ -191,14 +197,14 @@ public void testDeepMapWithBigDecimal()
         Map data = new HashMap();
         data.put("foo", new BigDecimal(1));
 
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyBigDecimal.class, result.getClass());
         assertEquals(1.0D, ((RubyBigDecimal)result).getDoubleValue(), 0);
@@ -211,10 +217,19 @@ public void testDeepListWithBigDecimal()
         List data = new ArrayList();
         data.add(new BigDecimal(1));
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyBigDecimal.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyBigDecimal)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
+
+
+    @Test
+    public void testDeepWithBigInteger() {
+        Object result = Rubyfier.deep(ruby, new BigInteger("1"));
+        assertEquals(RubyBignum.class, result.getClass());
+        assertEquals(1L, ((RubyBignum)result).getLongValue());
+    }
+
 }
diff --git a/logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java b/logstash-core/src/test/java/org/logstash/StringInterpolationTest.java
similarity index 98%
rename from logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java
rename to logstash-core/src/test/java/org/logstash/StringInterpolationTest.java
index 52d4563db4b..2559a55b360 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java
+++ b/logstash-core/src/test/java/org/logstash/StringInterpolationTest.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 
 import org.joda.time.DateTime;
@@ -51,7 +51,7 @@ public void testMissingKey() throws IOException {
     }
 
     @Test
-    public void testDateFormater() throws IOException {
+    public void testDateFormatter() throws IOException {
         Event event = getTestEvent();
         String path = "/full/%{+YYYY}";
         StringInterpolation si = StringInterpolation.getInstance();
diff --git a/logstash-core/src/test/java/org/logstash/TestBase.java b/logstash-core/src/test/java/org/logstash/TestBase.java
new file mode 100644
index 00000000000..9bbc5ff08d0
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/TestBase.java
@@ -0,0 +1,25 @@
+package org.logstash;
+
+import org.logstash.ext.JrubyTimestampExtLibrary;
+import org.jruby.CompatVersion;
+import org.jruby.Ruby;
+import org.jruby.RubyInstanceConfig;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.junit.Before;
+
+public abstract class TestBase {
+    private static boolean setupDone = false;
+    public static Ruby ruby;
+
+    @Before
+    public void setUp() throws Exception {
+        if (setupDone) return;
+
+        RubyInstanceConfig config_19 = new RubyInstanceConfig();
+        config_19.setCompatVersion(CompatVersion.RUBY1_9);
+        ruby = Ruby.newInstance(config_19);
+        RubyBigDecimal.createBigDecimal(ruby); // we need to do 'require "bigdecimal"'
+        JrubyTimestampExtLibrary.createTimestamp(ruby);
+        setupDone = true;
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java b/logstash-core/src/test/java/org/logstash/TimestampTest.java
similarity index 98%
rename from logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java
rename to logstash-core/src/test/java/org/logstash/TimestampTest.java
index 539fbe227cb..db698d43cee 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java
+++ b/logstash-core/src/test/java/org/logstash/TimestampTest.java
@@ -1,8 +1,9 @@
-package com.logstash;
+package org.logstash;
 
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 import org.junit.Test;
+
 import static org.junit.Assert.*;
 
 public class TimestampTest {
diff --git a/logstash-core/src/test/java/org/logstash/ValuefierTest.java b/logstash-core/src/test/java/org/logstash/ValuefierTest.java
new file mode 100644
index 00000000000..c80460cd454
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ValuefierTest.java
@@ -0,0 +1,113 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+import org.logstash.bivalues.TimestampBiValue;
+import org.joda.time.DateTime;
+import org.jruby.RubyClass;
+import org.jruby.RubyMatchData;
+import org.jruby.RubyString;
+import org.jruby.RubyTime;
+import org.jruby.java.proxies.ArrayJavaProxy;
+import org.jruby.java.proxies.ConcreteJavaProxy;
+import org.jruby.java.proxies.MapJavaProxy;
+import org.jruby.javasupport.Java;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.ExpectedException;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+
+import static junit.framework.TestCase.assertEquals;
+
+public class ValuefierTest extends TestBase {
+    @Test
+    public void testMapJavaProxy() {
+        Map<IRubyObject, IRubyObject> map = new HashMap<>();
+        map.put(RubyString.newString(ruby, "foo"), RubyString.newString(ruby, "bar"));
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, HashMap.class);
+        MapJavaProxy mjp = new MapJavaProxy(ruby, proxyClass);
+        mjp.setObject(map);
+
+        Object result = Valuefier.convert(mjp);
+        assertEquals(ConvertedMap.class, result.getClass());
+        ConvertedMap<String, Object> m = (ConvertedMap) result;
+        BiValue bv = BiValues.newBiValue("bar");
+        assertEquals(bv.javaValue(), ((BiValue) m.get("foo")).javaValue());
+    }
+
+    @Test
+    public void testArrayJavaProxy() {
+        IRubyObject[] array = new IRubyObject[]{RubyString.newString(ruby, "foo")};
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, String[].class);
+        ArrayJavaProxy ajp = new ArrayJavaProxy(ruby, proxyClass, array);
+
+        Object result = Valuefier.convert(ajp);
+        assertEquals(ConvertedList.class, result.getClass());
+        List<Object> a = (ConvertedList) result;
+        BiValue bv = BiValues.newBiValue("foo");
+        assertEquals(bv.javaValue(), ((BiValue) a.get(0)).javaValue());
+    }
+
+    @Test
+    public void testConcreteJavaProxy() {
+        List<IRubyObject> array = new ArrayList<>();
+        array.add(RubyString.newString(ruby, "foo"));
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, ArrayList.class);
+        ConcreteJavaProxy cjp = new ConcreteJavaProxy(ruby, proxyClass, array);
+        Object result = Valuefier.convert(cjp);
+        assertEquals(ConvertedList.class, result.getClass());
+        List<Object> a = (ConvertedList) result;
+        BiValue bv = BiValues.newBiValue("foo");
+        assertEquals(bv.javaValue(), ((BiValue) a.get(0)).javaValue());
+    }
+
+    @Test
+    public void testRubyTime() {
+        RubyTime ro = RubyTime.newTime(ruby, DateTime.now());
+        Object result = Valuefier.convert(ro);
+
+        assertEquals(TimestampBiValue.class, result.getClass());
+    }
+
+    @Test
+    public void testJodaDateTIme() {
+        DateTime jo = DateTime.now();
+        Object result = Valuefier.convert(jo);
+
+        assertEquals(TimestampBiValue.class, result.getClass());
+    }
+
+    @Rule
+    public ExpectedException exception = ExpectedException.none();
+
+    @Test
+    public void testUnhandledObject() {
+        RubyMatchData md = new RubyMatchData(ruby);
+        exception.expect(IllegalArgumentException.class);
+        exception.expectMessage("Missing Valuefier handling for full class name=org.jruby.RubyMatchData, simple name=RubyMatchData");
+        Valuefier.convert(md);
+    }
+
+    @Test
+    public void testUnhandledProxyObject() {
+        HashSet<Integer> hs = new HashSet<>();
+        hs.add(42);
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, HashSet.class);
+        ConcreteJavaProxy cjp = new ConcreteJavaProxy(ruby, proxyClass, hs);
+        BiValue result = (BiValue) Valuefier.convert(cjp);
+        assertEquals(hs, result.javaValue());
+    }
+
+    @Test
+    public void scratch() {
+        String[] parts = "foo/1_4".split("\\W|_");
+        int ord = Integer.valueOf(parts[1]);
+        assertEquals(ord, 1);
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/CheckpointTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/CheckpointTest.java
new file mode 100644
index 00000000000..83d1ac5aa49
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/CheckpointTest.java
@@ -0,0 +1,21 @@
+package org.logstash.ackedqueue;
+
+import org.junit.Test;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class CheckpointTest {
+
+    @Test
+    public void newInstance() {
+        Checkpoint checkpoint = new Checkpoint(1, 2, 3, 4, 5);
+
+        assertThat(checkpoint.getPageNum(), is(equalTo(1)));
+        assertThat(checkpoint.getFirstUnackedPageNum(), is(equalTo(2)));
+        assertThat(checkpoint.getFirstUnackedSeqNum(), is(equalTo(3L)));
+        assertThat(checkpoint.getMinSeqNum(), is(equalTo(4L)));
+        assertThat(checkpoint.getElementCount(), is(equalTo(5)));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
new file mode 100644
index 00000000000..9fca039f11e
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
@@ -0,0 +1,121 @@
+package org.logstash.ackedqueue;
+
+import org.junit.Test;
+import org.logstash.ackedqueue.io.ByteBufferPageIO;
+import org.logstash.ackedqueue.io.PageIO;
+
+import java.io.IOException;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class HeadPageTest {
+
+    @Test
+    public void newHeadPage() throws IOException {
+        Settings s = TestSettings.volatileQueueSettings(100);
+        Queue q = new Queue(s);
+        PageIO pageIO = s.getPageIOFactory().build(0, 100, "dummy");
+        pageIO.create();
+        HeadPage p = new HeadPage(0, q, pageIO);
+
+        assertThat(p.getPageNum(), is(equalTo(0)));
+        assertThat(p.isFullyRead(), is(true));
+        assertThat(p.isFullyAcked(), is(false));
+        assertThat(p.hasSpace(10), is(true));
+        assertThat(p.hasSpace(100), is(false));
+
+        q.close();
+    }
+
+    @Test
+    public void pageWrite() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings s = TestSettings.volatileQueueSettings(singleElementCapacity);
+        Queue q = new Queue(s);
+        q.open();
+        HeadPage p = q.headPage;
+
+        assertThat(p.hasSpace(element.serialize().length), is(true));
+        p.write(element.serialize(), 0, 1);
+
+        assertThat(p.hasSpace(element.serialize().length), is(false));
+        assertThat(p.isFullyRead(), is(false));
+        assertThat(p.isFullyAcked(), is(false));
+
+        q.close();
+    }
+
+    @Test
+    public void pageWriteAndReadSingle() throws IOException {
+        long seqNum = 1L;
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings s = TestSettings.volatileQueueSettings(singleElementCapacity);
+        Queue q = new Queue(s);
+        q.open();
+        HeadPage p = q.headPage;
+
+        assertThat(p.hasSpace(element.serialize().length), is(true));
+        p.write(element.serialize(), seqNum, 1);
+
+        Batch b = p.readBatch(1);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+
+        assertThat(p.hasSpace(element.serialize().length), is(false));
+        assertThat(p.isFullyRead(), is(true));
+        assertThat(p.isFullyAcked(), is(false));
+
+        q.close();
+    }
+
+    @Test
+    public void pageWriteAndReadMulti() throws IOException {
+        long seqNum = 1L;
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings s = TestSettings.volatileQueueSettings(singleElementCapacity);
+        Queue q = new Queue(s);
+        q.open();
+        HeadPage p = q.headPage;
+
+        assertThat(p.hasSpace(element.serialize().length), is(true));
+        p.write(element.serialize(), seqNum, 1);
+
+        Batch b = p.readBatch(10);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+
+        assertThat(p.hasSpace(element.serialize().length), is(false));
+        assertThat(p.isFullyRead(), is(true));
+        assertThat(p.isFullyAcked(), is(false));
+
+        q.close();
+    }
+
+    // disabled test until we figure what to do in this condition
+//    @Test
+//    public void pageViaQueueOpenForHeadCheckpointWithoutSupportingPageFiles() throws Exception {
+//        URL url = FileCheckpointIOTest.class.getResource("checkpoint.head");
+//        String dirPath = Paths.get(url.toURI()).getParent().toString();
+//        Queueable element = new StringElement("foobarbaz");
+//        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+//        Settings s = TestSettings.persistedQueueSettings(singleElementCapacity, dirPath);
+//        TestQueue q = new TestQueue(s);
+//        try {
+//            q.open();
+//        } catch (NoSuchFileException e) {
+//            assertThat(e.getMessage(), containsString("checkpoint.2"));
+//        }
+//        HeadPage p = q.getHeadPage();
+//        assertThat(p, is(equalTo(null)));
+//    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
new file mode 100644
index 00000000000..534a8088b5d
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -0,0 +1,776 @@
+package org.logstash.ackedqueue;
+
+import java.io.IOException;
+import java.nio.file.NoSuchFileException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collections;
+import java.util.List;
+import java.util.Random;
+import java.util.concurrent.Callable;
+import java.util.concurrent.ExecutionException;
+import java.util.concurrent.ExecutorService;
+import java.util.concurrent.Executors;
+import java.util.concurrent.Future;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.TimeoutException;
+import java.util.concurrent.atomic.AtomicInteger;
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.ackedqueue.io.ByteBufferPageIO;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.CoreMatchers.notNullValue;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.junit.Assert.fail;
+
+public class QueueTest {
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    private String dataPath;
+
+    @Before
+    public void setUp() throws Exception {
+        dataPath = temporaryFolder.newFolder("data").getPath();
+    }
+
+    @Test
+    public void newQueue() throws IOException {
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(10));
+        q.open();
+
+        assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));
+
+        q.close();
+    }
+
+    @Test
+    public void singleWriteRead() throws IOException {
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));
+        q.open();
+
+        Queueable element = new StringElement("foobarbaz");
+        q.write(element);
+
+        Batch b = q.nonBlockReadBatch(1);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+        assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));
+
+        q.close();
+    }
+
+    @Test
+    public void singleWriteMultiRead() throws IOException {
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));
+        q.open();
+
+        Queueable element = new StringElement("foobarbaz");
+        q.write(element);
+
+        Batch b = q.nonBlockReadBatch(2);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
+        assertThat(q.nonBlockReadBatch(2), is(equalTo(null)));
+
+        q.close();
+    }
+
+    @Test
+    public void multiWriteSamePage() throws IOException {
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));
+        q.open();
+
+        List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"));
+
+        for (Queueable e : elements) {
+            q.write(e);
+        }
+
+        Batch b = q.nonBlockReadBatch(2);
+
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(0).toString())));
+        assertThat(b.getElements().get(1).toString(), is(equalTo(elements.get(1).toString())));
+
+        b = q.nonBlockReadBatch(2);
+
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(2).toString())));
+
+        q.close();
+    }
+
+    @Test
+    public void writeMultiPage() throws IOException {
+        List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
+
+        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));
+        q.open();
+
+        for (Queueable e : elements) {
+            q.write(e);
+        }
+
+        // total of 2 pages: 1 head and 1 tail
+        assertThat(q.getTailPages().size(), is(equalTo(1)));
+
+        assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(false)));
+        assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        Batch b = q.nonBlockReadBatch(10);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+
+        assertThat(q.getTailPages().size(), is(equalTo(1)));
+
+        assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));
+        assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        b = q.nonBlockReadBatch(10);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+
+        assertThat(q.getTailPages().get(0).isFullyRead(), is(equalTo(true)));
+        assertThat(q.getTailPages().get(0).isFullyAcked(), is(equalTo(false)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        b = q.nonBlockReadBatch(10);
+        assertThat(b, is(equalTo(null)));
+
+        q.close();
+    }
+
+
+    @Test
+    public void writeMultiPageWithInOrderAcking() throws IOException {
+        List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
+
+        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));
+        q.open();
+
+        for (Queueable e : elements) {
+            q.write(e);
+        }
+
+        Batch b = q.nonBlockReadBatch(10);
+
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(q.getTailPages().size(), is(equalTo(1)));
+
+        // lets keep a ref to that tail page before acking
+        TailPage tailPage = q.getTailPages().get(0);
+
+        assertThat(tailPage.isFullyRead(), is(equalTo(true)));
+
+        // ack first batch which includes all elements from tailPages
+        b.close();
+
+        assertThat(q.getTailPages().size(), is(equalTo(0)));
+        assertThat(tailPage.isFullyRead(), is(equalTo(true)));
+        assertThat(tailPage.isFullyAcked(), is(equalTo(true)));
+
+        b = q.nonBlockReadBatch(10);
+
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(q.getHeadPage().isFullyRead(), is(equalTo(true)));
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(false)));
+
+        b.close();
+
+        assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(true)));
+
+        q.close();
+    }
+
+    @Test
+    public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
+        List<Queueable> elements1 = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"));
+        List<Queueable> elements2 = Arrays.asList(new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements1.get(0).serialize().length);
+
+        Settings settings = TestSettings.volatileQueueSettings(2 * singleElementCapacity);
+        settings.setCheckpointMaxWrites(1024); // arbitrary high enough threshold so that it's not reached (default for TestSettings is 1)
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        assertThat(q.getHeadPage().getPageNum(), is(equalTo(0)));
+        Checkpoint c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(0)));
+        assertThat(c.getMinSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+        for (Queueable e : elements1) {
+            q.write(e);
+        }
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(0)));
+        assertThat(c.getMinSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+//        assertThat(elements1.get(1).getSeqNum(), is(equalTo(2L)));
+        q.ensurePersistedUpto(2);
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(1L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+        for (Queueable e : elements2) {
+            q.write(e);
+        }
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(1)));
+        assertThat(c.getElementCount(), is(equalTo(0)));
+        assertThat(c.getMinSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(0L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(0)));
+
+        c = q.getCheckpointIO().read("checkpoint.0");
+        assertThat(c.getPageNum(), is(equalTo(0)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(1L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(1L)));
+
+        Batch b = q.nonBlockReadBatch(10);
+        b.close();
+
+        try {
+            q.getCheckpointIO().read("checkpoint.0");
+            fail("expected NoSuchFileException thrown");
+        } catch (NoSuchFileException e) {
+            // nothing
+        }
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(1)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(3L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(3L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));
+
+        b = q.nonBlockReadBatch(10);
+        b.close();
+
+        c = q.getCheckpointIO().read("checkpoint.head");
+        assertThat(c.getPageNum(), is(equalTo(1)));
+        assertThat(c.getElementCount(), is(equalTo(2)));
+        assertThat(c.getMinSeqNum(), is(equalTo(3L)));
+        assertThat(c.getFirstUnackedSeqNum(), is(equalTo(5L)));
+        assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));
+
+        q.close();
+    }
+
+    @Test
+    public void randomAcking() throws IOException {
+        Random random = new Random();
+
+        // 10 tests of random queue sizes
+        for (int loop = 0; loop < 10; loop++) {
+            int page_count = random.nextInt(10000) + 1;
+            int digits = new Double(Math.ceil(Math.log10(page_count))).intValue();
+
+            // create a queue with a single element per page
+            List<Queueable> elements = new ArrayList<>();
+            for (int i = 0; i < page_count; i++) {
+                elements.add(new StringElement(String.format("%0" + digits + "d", i)));
+            }
+            int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
+
+            TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(singleElementCapacity));
+            q.open();
+
+            for (Queueable e : elements) {
+                q.write(e);
+            }
+
+            assertThat(q.getTailPages().size(), is(equalTo(page_count - 1)));
+
+            // first read all elements
+            List<Batch> batches = new ArrayList<>();
+            for (Batch b = q.nonBlockReadBatch(1); b != null; b = q.nonBlockReadBatch(1)) {
+                batches.add(b);
+            }
+            assertThat(batches.size(), is(equalTo(page_count)));
+
+            // then ack randomly
+            Collections.shuffle(batches);
+            for (Batch b : batches) {
+                b.close();
+            }
+
+            assertThat(q.getTailPages().size(), is(equalTo(0)));
+
+            q.close();
+        }
+    }
+
+    @Test(timeout = 5000)
+    public void reachMaxUnread() throws IOException, InterruptedException, ExecutionException {
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity);
+        settings.setMaxUnread(2); // 2 so we know the first write should not block and the second should
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+
+        long seqNum = q.write(element);
+        assertThat(seqNum, is(equalTo(1L)));
+        assertThat(q.isFull(), is(false));
+
+        int ELEMENT_COUNT = 1000;
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+
+            // we expect the next write call to block so let's wrap it in a Future
+            Callable<Long> write = () -> {
+                return q.write(element);
+            };
+
+            ExecutorService executor = Executors.newFixedThreadPool(1);
+            Future<Long> future = executor.submit(write);
+
+            while (!q.isFull()) {
+                // spin wait until data is written and write blocks
+                Thread.sleep(1);
+            }
+            assertThat(q.unreadCount, is(equalTo(2L)));
+            assertThat(future.isDone(), is(false));
+
+            // read one element, which will unblock the last write
+            Batch b = q.nonBlockReadBatch(1);
+            assertThat(b.getElements().size(), is(equalTo(1)));
+
+            // future result is the blocked write seqNum for the second element
+            assertThat(future.get(), is(equalTo(2L + i)));
+            assertThat(q.isFull(), is(false));
+
+            executor.shutdown();
+        }
+
+        // since we did not ack and pages hold a single item
+        assertThat(q.getTailPages().size(), is(equalTo(ELEMENT_COUNT)));
+
+        q.close();
+    }
+
+    @Test
+    public void reachMaxUnreadWithAcking() throws IOException, InterruptedException, ExecutionException {
+        Queueable element = new StringElement("foobarbaz");
+
+        // TODO: add randomized testing on the page size (but must be > single element size)
+        Settings settings = TestSettings.volatileQueueSettings(256); // 256 is arbitrary, large enough to hold a few elements
+
+        settings.setMaxUnread(2); // 2 so we know the first write should not block and the second should
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        // perform first non-blocking write
+        long seqNum = q.write(element);
+
+        assertThat(seqNum, is(equalTo(1L)));
+        assertThat(q.isFull(), is(false));
+
+        int ELEMENT_COUNT = 1000;
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+
+            // we expect this next write call to block so let's wrap it in a Future
+            Callable<Long> write = () -> {
+                return q.write(element);
+            };
+
+            ExecutorService executor = Executors.newFixedThreadPool(1);
+            Future<Long> future = executor.submit(write);
+
+            // spin wait until data is written and write blocks
+            while (!q.isFull()) { Thread.sleep(1); }
+
+            // read one element, which will unblock the last write
+            Batch b = q.nonBlockReadBatch(1);
+            assertThat(b, is(notNullValue()));
+            assertThat(b.getElements().size(), is(equalTo(1)));
+            b.close();
+
+            // future result is the blocked write seqNum for the second element
+            assertThat(future.get(), is(equalTo(2L + i)));
+            assertThat(q.isFull(), is(false));
+
+            executor.shutdown();
+        }
+
+        // all batches are acked, no tail pages should exist
+        assertThat(q.getTailPages().size(), is(equalTo(0)));
+
+        // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page
+        assertThat(q.getHeadPage().getElementCount() > 0L, is(true));
+        assertThat(q.getHeadPage().unreadCount(), is(equalTo(1L)));
+        assertThat(q.unreadCount, is(equalTo(1L)));
+
+        q.close();
+    }
+
+    @Test(timeout = 5000)
+    public void reachMaxSizeTest() throws IOException, InterruptedException, ExecutionException {
+        Queueable element = new StringElement("0123456789"); // 10 bytes
+
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        // allow 10 elements per page but only 100 events in total
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
+
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            long seqNum = q.write(element);
+        }
+
+        assertThat(q.isFull(), is(false));
+
+        // we expect this next write call to block so let's wrap it in a Future
+        Callable<Long> write = () -> {
+            return q.write(element);
+        };
+
+        ExecutorService executor = Executors.newFixedThreadPool(1);
+        Future<Long> future = executor.submit(write);
+
+        while (!q.isFull()) { Thread.sleep(10); }
+
+        assertThat(q.isFull(), is(true));
+
+        executor.shutdown();
+        q.close();
+    }
+
+    @Test(timeout = 5000)
+    public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedException, ExecutionException {
+
+        Queueable element = new StringElement("0123456789"); // 10 bytes
+
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        // allow 10 elements per page but only 100 events in total
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
+
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        int ELEMENT_COUNT = 90; // should be able to write 90 events (9 pages) before getting full
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            long seqNum = q.write(element);
+        }
+
+        assertThat(q.isFull(), is(false));
+
+        // we expect this next write call to block so let's wrap it in a Future
+        Callable<Long> write = () -> {
+            return q.write(element);
+        };
+        ExecutorService executor = Executors.newFixedThreadPool(1);
+        Future<Long> future = executor.submit(write);
+        assertThat(future.isDone(), is(false));
+
+        while (!q.isFull()) { Thread.sleep(10); }
+        assertThat(q.isFull(), is(true));
+
+        Batch b = q.readBatch(10); // read 1 page (10 events)
+        b.close();  // purge 1 page
+
+        while (q.isFull()) { Thread.sleep(10); }
+        assertThat(q.isFull(), is(false));
+
+        // will not complete because write will not unblock until the page is purge with a batch close/acking.
+        assertThat(future.isDone(), is(false));
+
+        q.close();
+    }
+
+    @Test(timeout = 5000)
+    public void resumeWriteOnNoLongerFullQueueTest() throws IOException, InterruptedException, ExecutionException {
+        Queueable element = new StringElement("0123456789"); // 10 bytes
+
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        // allow 10 elements per page but only 100 events in total
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
+
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        int ELEMENT_COUNT = 90; // should be able to write 90 events (9 pages) before getting full
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            long seqNum = q.write(element);
+        }
+
+        assertThat(q.isFull(), is(false));
+
+        // read 1 page (10 events) here while not full yet so that the read will not singal the not full state
+        // we want the batch closing below to signal the not full state
+        Batch b = q.readBatch(10);
+
+        // we expect this next write call to block so let's wrap it in a Future
+        Callable<Long> write = () -> {
+            return q.write(element);
+        };
+        ExecutorService executor = Executors.newFixedThreadPool(1);
+        Future<Long> future = executor.submit(write);
+        assertThat(future.isDone(), is(false));
+
+        while (!q.isFull()) { Thread.sleep(10); }
+        assertThat(q.isFull(), is(true));
+        assertThat(future.isDone(), is(false));
+
+        b.close();  // purge 1 page
+
+        assertThat(future.get(), is(equalTo(ELEMENT_COUNT + 1L)));
+
+        executor.shutdown();
+        q.close();
+    }
+
+    @Test(timeout = 5000)
+    public void queueStillFullAfterPartialPageAckTest() throws IOException, InterruptedException, ExecutionException {
+
+        Queueable element = new StringElement("0123456789"); // 10 bytes
+
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        // allow 10 elements per page but only 100 events in total
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
+
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        int ELEMENT_COUNT = 90; // should be able to write 99 events before getting full
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            long seqNum = q.write(element);
+        }
+
+        assertThat(q.isFull(), is(false));
+
+        // we expect this next write call to block so let's wrap it in a Future
+        Callable<Long> write = () -> {
+            return q.write(element);
+        };
+
+        ExecutorService executor = Executors.newFixedThreadPool(1);
+        Future<Long> future = executor.submit(write);
+
+        while (!q.isFull()) { Thread.sleep(10); }
+
+        assertThat(q.isFull(), is(true));
+
+        Batch b = q.readBatch(9); // read 90% of page (9 events)
+        b.close();  // this should not purge a page
+
+        assertThat(q.isFull(), is(true)); // queue should still be full
+
+        executor.shutdown();
+        q.close();
+    }
+
+    @Test
+    public void queueStableUnderStress() throws Exception {
+        Settings settings = TestSettings.persistedQueueSettings(1000000, dataPath);
+        final ExecutorService exec = Executors.newScheduledThreadPool(2);
+        try (Queue queue = new Queue(settings)) {
+            final int count = 20_000;
+            final int concurrent = 2;
+            queue.open();
+            final Future<Integer>[] futures = new Future[concurrent];
+            for (int c = 0; c < concurrent; ++c) {
+                futures[c] = exec.submit(() -> {
+                    int i = 0;
+                    try {
+                        while (i < count / concurrent) {
+                            final Batch batch = queue.readBatch(1);
+                            for (final Queueable elem : batch.getElements()) {
+                                if (elem != null) {
+                                    ++i;
+                                }
+                            }
+                        }
+                        return i;
+                    } catch (final IOException ex) {
+                        throw new IllegalStateException(ex);
+                    }
+                });
+            }
+            for (int i = 0; i < count; ++i) {
+                try {
+                    final Queueable evnt = new StringElement("foo");
+                    queue.write(evnt);
+                } catch (final IOException ex) {
+                    throw new IllegalStateException(ex);
+                }
+            }
+            assertThat(
+                Arrays.stream(futures).map(i -> {
+                    try {
+                        return i.get(10L, TimeUnit.SECONDS);
+                    } catch (final InterruptedException | ExecutionException | TimeoutException ex) {
+                        throw new IllegalStateException(ex);
+                    }
+                }).reduce((x, y) -> x + y).orElse(0),
+                is(20_000)
+            );
+        }
+    }
+
+    @Test
+    public void testAckedCount() throws IOException {
+        Settings settings = TestSettings.persistedQueueSettings(100, dataPath);
+        Queue q = new Queue(settings);
+        q.open();
+
+        Queueable element1 = new StringElement("foobarbaz");
+        Queueable element2 = new StringElement("wowza");
+        Queueable element3 = new StringElement("third");
+        long firstSeqNum = q.write(element1);
+
+        Batch b = q.nonBlockReadBatch(1);
+        assertThat(b.getElements().size(), is(equalTo(1)));
+
+        q.close();
+
+        q = new Queue(settings);
+        q.open();
+
+        long secondSeqNum = q.write(element2);
+        long thirdSeqNum = q.write(element3);
+
+        b = q.nonBlockReadBatch(1);
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0), is(equalTo(element1)));
+
+        b = q.nonBlockReadBatch(2);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(b.getElements().get(0), is(equalTo(element2)));
+        assertThat(b.getElements().get(1), is(equalTo(element3)));
+
+        q.ack(Collections.singletonList(firstSeqNum));
+        q.close();
+
+        q = new Queue(settings);
+        q.open();
+
+        b = q.nonBlockReadBatch(2);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+
+        q.ack(Arrays.asList(secondSeqNum, thirdSeqNum));
+
+        assertThat(q.getAckedCount(), equalTo(0L));
+        assertThat(q.getUnackedCount(), equalTo(0L));
+
+        q.close();
+    }
+
+    @Test(timeout = 5000)
+    public void concurrentWritesTest() throws IOException, InterruptedException, ExecutionException {
+
+        // very small pages to maximize page creation
+        Settings settings = TestSettings.volatileQueueSettings(100);
+
+        TestQueue q = new TestQueue(settings);
+        q.open();
+
+        int ELEMENT_COUNT = 10000;
+        int WRITER_COUNT = 5;
+        AtomicInteger element_num = new AtomicInteger(0);
+
+        // we expect this next write call to block so let's wrap it in a Future
+        Callable<Integer> writer = () -> {
+            for (int i = 0; i < ELEMENT_COUNT; i++) {
+                int n = element_num.getAndIncrement();
+                q.write(new StringElement("" + n));
+            }
+            return ELEMENT_COUNT;
+        };
+
+        List<Future<Integer>> futures = new ArrayList<>();
+        ExecutorService executor = Executors.newFixedThreadPool(WRITER_COUNT);
+        for  (int i = 0; i < WRITER_COUNT; i++) {
+            futures.add(executor.submit(writer));
+        }
+
+        int BATCH_SIZE = 10;
+        int read_count = 0;
+
+        while (read_count < ELEMENT_COUNT * WRITER_COUNT) {
+            Batch b = q.readBatch(BATCH_SIZE);
+            read_count += b.size();
+            b.close();
+        }
+
+        for (Future<Integer> future : futures) {
+            int result = future.get();
+            assertThat(result, is(equalTo(ELEMENT_COUNT)));
+        }
+
+        assertThat(q.getTailPages().isEmpty(), is(true));
+        assertThat(q.isFullyAcked(), is(true));
+
+        executor.shutdown();
+        q.close();
+    }
+
+    @Test
+    public void fullyAckedHeadPageBeheadingTest() throws IOException {
+        Queueable element = new StringElement("foobarbaz1");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+
+        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));
+        q.open();
+
+        Batch b;
+        q.write(element);
+        b = q.nonBlockReadBatch(1);
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        b.close();
+
+        q.write(element);
+        b = q.nonBlockReadBatch(1);
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        b.close();
+
+        // head page should be full and fully acked
+        assertThat(q.getHeadPage().isFullyAcked(), is(true));
+        assertThat(q.getHeadPage().hasSpace(element.serialize().length), is(false));
+        assertThat(q.isFullyAcked(), is(true));
+
+        // write extra element to trigger beheading
+        q.write(element);
+
+        // since head page was fully acked it should not have created a new tail page
+
+        assertThat(q.getTailPages().isEmpty(), is(true));
+        assertThat(q.getHeadPage().getPageNum(), is(equalTo(1)));
+        assertThat(q.firstUnackedPageNum(), is(equalTo(1)));
+        assertThat(q.isFullyAcked(), is(false));
+
+        q.close();
+    }
+
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/StringElement.java b/logstash-core/src/test/java/org/logstash/ackedqueue/StringElement.java
new file mode 100644
index 00000000000..99092a90884
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/StringElement.java
@@ -0,0 +1,58 @@
+package org.logstash.ackedqueue;
+
+import java.nio.ByteBuffer;
+
+public class StringElement implements Queueable {
+    private final String content;
+
+    public StringElement(String content) {
+        this.content = content;
+    }
+
+    @Override
+    public byte[] serialize() {
+        byte[] contentBytes = this.content.getBytes();
+        ByteBuffer buffer = ByteBuffer.allocate(contentBytes.length);
+        buffer.put(contentBytes);
+        return buffer.array();
+    }
+
+    public static StringElement deserialize(byte[] bytes) {
+        ByteBuffer buffer = ByteBuffer.allocate(bytes.length);
+        buffer.put(bytes);
+
+        buffer.position(0);
+        byte[] content = new byte[bytes.length];
+        buffer.get(content);
+        return new StringElement(new String(content));
+    }
+
+    @Override
+    public String toString() {
+        return content;
+    }
+
+
+    @Override
+    public boolean equals(Object other) {
+        if (other == null) {
+            return false;
+        }
+        if (!StringElement.class.isAssignableFrom(other.getClass())) {
+            return false;
+        }
+
+        final StringElement element = (StringElement)other;
+        if ((this.content == null) ? (element.content != null) : !this.content.equals(element.content)) {
+            return false;
+        }
+        return true;
+    }
+
+    @Override
+    public int hashCode() {
+        int hash = 13;
+        hash = 53 * hash + (this.content != null ? this.content.hashCode() : 0);
+        return hash;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java b/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java
new file mode 100644
index 00000000000..16d53ef7b00
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java
@@ -0,0 +1,17 @@
+package org.logstash.ackedqueue;
+
+import java.util.List;
+
+public class TestQueue extends Queue {
+    public TestQueue(Settings settings) {
+        super(settings);
+    }
+
+    public HeadPage getHeadPage() {
+        return this.headPage;
+    }
+
+    public List<TailPage> getTailPages() {
+        return this.tailPages;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
new file mode 100644
index 00000000000..ada4c80c672
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
@@ -0,0 +1,48 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.ByteBufferPageIO;
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.FileCheckpointIO;
+import org.logstash.ackedqueue.io.MemoryCheckpointIO;
+import org.logstash.ackedqueue.io.MmapPageIO;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+public class TestSettings {
+
+    public static Settings volatileQueueSettings(int capacity) {
+        MemoryCheckpointIO.clearSources();
+        Settings s = new MemorySettings();
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+
+    public static Settings volatileQueueSettings(int capacity, long size) {
+        MemoryCheckpointIO.clearSources();
+        Settings s = new MemorySettings();
+        PageIOFactory pageIOFactory = (pageNum, pageSize, path) -> new ByteBufferPageIO(pageNum, pageSize, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setQueueMaxBytes(size);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+
+    public static Settings persistedQueueSettings(int capacity, String folder) {
+        Settings s = new FileSettings(folder);
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointMaxWrites(1);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/ByteBufferPageIOTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/ByteBufferPageIOTest.java
new file mode 100644
index 00000000000..0a356cb7c64
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/ByteBufferPageIOTest.java
@@ -0,0 +1,383 @@
+package org.logstash.ackedqueue.io;
+
+import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameter;
+import org.junit.runners.Parameterized.Parameters;
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+import org.logstash.ackedqueue.StringElement;
+import org.logstash.ackedqueue.io.AbstractByteBufferPageIO;
+import org.logstash.ackedqueue.io.ByteBufferPageIO;
+
+import java.io.IOException;
+import java.util.Arrays;
+import java.util.Collection;
+import java.util.List;
+import java.util.concurrent.Callable;
+import java.util.stream.Collectors;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class ByteBufferPageIOTest {
+
+    // convert any checked exceptions into uncheck RuntimeException
+    public static <V> V uncheck(Callable<V> callable) {
+        try {
+            return callable.call();
+        } catch (RuntimeException e) {
+            throw e;
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    private interface BufferGenerator {
+        byte[] generate() throws IOException;
+    }
+
+    private static int CAPACITY = 1024;
+    private static int MIN_CAPACITY = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(0);
+
+    private static ByteBufferPageIO newEmptyPageIO() throws IOException {
+        return newEmptyPageIO(CAPACITY);
+    }
+
+    private static ByteBufferPageIO newEmptyPageIO(int capacity) throws IOException {
+        ByteBufferPageIO io = new ByteBufferPageIO(capacity);
+        io.create();
+        return io;
+    }
+
+    private static ByteBufferPageIO newPageIO(int capacity, byte[] bytes) throws IOException {
+        return new ByteBufferPageIO(capacity, bytes);
+    }
+
+    private Queueable buildStringElement(String str) {
+        return new StringElement(str);
+    }
+
+    @Test
+    public void getWritePosition() throws IOException {
+        assertThat(newEmptyPageIO().getWritePosition(), is(equalTo(1)));
+    }
+
+    @Test
+    public void getElementCount() throws IOException {
+        assertThat(newEmptyPageIO().getElementCount(), is(equalTo(0)));
+    }
+
+    @Test
+    public void getStartSeqNum() throws IOException {
+        assertThat(newEmptyPageIO().getMinSeqNum(), is(equalTo(0L)));
+    }
+
+    @Test
+    public void hasSpace() throws IOException {
+        assertThat(newEmptyPageIO(MIN_CAPACITY).hasSpace(0), is(true));
+        assertThat(newEmptyPageIO(MIN_CAPACITY).hasSpace(1), is(false));
+    }
+
+    @Test
+    public void hasSpaceAfterWrite() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
+        long seqNum = 1L;
+
+        ByteBufferPageIO io = newEmptyPageIO(singleElementCapacity);
+
+        assertThat(io.hasSpace(element.serialize().length), is(true));
+        io.write(element.serialize(), seqNum);
+        assertThat(io.hasSpace(element.serialize().length), is(false));
+        assertThat(io.hasSpace(1), is(false));
+    }
+
+    @Test
+    public void write() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+        assertThat(io.getWritePosition(), is(equalTo(ByteBufferPageIO.HEADER_SIZE +  ByteBufferPageIO._persistedByteCount(element.serialize().length))));
+        assertThat(io.getElementCount(), is(equalTo(1)));
+        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void openValidState() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+
+        byte[] initialState = io.dump();
+        io = newPageIO(initialState.length, initialState);
+        io.open(seqNum, 1);
+        assertThat(io.getElementCount(), is(equalTo(1)));
+        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void recoversValidState() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+
+        byte[] initialState = io.dump();
+        io = newPageIO(initialState.length, initialState);
+        io.recover();
+        assertThat(io.getElementCount(), is(equalTo(1)));
+        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void recoverEmptyWriteRecover() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO io = newEmptyPageIO();
+        byte[] initialState = io.dump();
+
+        io = newPageIO(initialState.length, initialState);
+        io.recover();
+        assertThat(io.getElementCount(), is(equalTo(0)));
+
+        io.write(element.serialize(), seqNum);
+        initialState = io.dump();
+
+        io = newPageIO(initialState.length, initialState);
+        io.recover();
+        assertThat(io.getElementCount(), is(equalTo(1)));
+        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void recoverNonEmptyWriteRecover() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), 1L);
+        byte[] initialState = io.dump();
+
+        io = newPageIO(initialState.length, initialState);
+        io.recover();
+        assertThat(io.getElementCount(), is(equalTo(1)));
+
+        io.write(element.serialize(), 2L);
+        initialState = io.dump();
+
+        io = newPageIO(initialState.length, initialState);
+        io.recover();
+        assertThat(io.getElementCount(), is(equalTo(2)));
+    }
+
+    @Test(expected = IOException.class)
+    public void openUnexpectedSeqNum() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+
+        byte[] initialState = io.dump();
+        newPageIO(initialState.length, initialState);
+        io.open(1L, 1);
+    }
+
+    @RunWith(Parameterized.class)
+    public static class SingleInvalidElementTest {
+
+        private static final List<BufferGenerator> singleGenerators = Arrays.asList(
+            // invalid length
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes, 1L, 514, io.checksum(bytes));
+                return io.dump();
+            },
+
+            // invalid checksum
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes, 1L, bytes.length, 77);
+                return io.dump();
+            },
+
+            // invalid payload
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                int checksum = io.checksum(bytes);
+                bytes[1] = 0x01;
+                io.write(bytes, 1L, bytes.length, checksum);
+                return io.dump();
+            }
+        );
+
+        @Parameters
+        public static Collection<byte[]> singleElementParameters() {
+            return singleGenerators.stream().map(g -> uncheck(g::generate)).collect(Collectors.toList());
+        }
+
+        @Parameter
+        public byte[] singleElementParameter;
+
+        @Test
+        public void openInvalidSingleElement() throws IOException {
+            // none of these should generate an exception with open()
+
+            ByteBufferPageIO io = newPageIO(singleElementParameter.length, singleElementParameter);
+            io.open(1L, 1);
+
+            assertThat(io.getElementCount(), is(equalTo(1)));
+            assertThat(io.getMinSeqNum(), is(equalTo(1L)));
+        }
+
+        @Test
+        public void recoverInvalidSingleElement() throws IOException {
+            for (BufferGenerator generator : singleGenerators) {
+                byte[] bytes = generator.generate();
+                ByteBufferPageIO io = newPageIO(bytes.length, bytes);
+                io.recover();
+
+                assertThat(io.getElementCount(), is(equalTo(0)));
+            }
+        }
+    }
+
+    @RunWith(Parameterized.class)
+    public static class DoubleInvalidElementTest {
+
+        private static final List<BufferGenerator> doubleGenerators = Arrays.asList(
+            // invalid length
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+                io.write(bytes, 2L, 514, io.checksum(bytes));
+                return io.dump();
+            },
+
+            // invalid checksum
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+                io.write(bytes, 2L, bytes.length, 77);
+                return io.dump();
+            },
+
+            // invalid payload
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                int checksum = io.checksum(bytes);
+                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+                bytes[1] = 0x01;
+                io.write(bytes, 2L, bytes.length, checksum);
+                return io.dump();
+            }
+        );
+
+        @Parameters
+        public static Collection<byte[]> doubleElementParameters() {
+            return doubleGenerators.stream().map(g -> uncheck(g::generate)).collect(Collectors.toList());
+        }
+
+        @Parameter
+        public byte[] doubleElementParameter;
+
+        @Test
+        public void openInvalidDoubleElement() throws IOException {
+            // none of these will generate an exception with open()
+
+            ByteBufferPageIO io = newPageIO(doubleElementParameter.length, doubleElementParameter);
+            io.open(1L, 2);
+
+            assertThat(io.getElementCount(), is(equalTo(2)));
+            assertThat(io.getMinSeqNum(), is(equalTo(1L)));
+        }
+
+        @Test
+        public void recoverInvalidDoubleElement() throws IOException {
+            ByteBufferPageIO io = newPageIO(doubleElementParameter.length, doubleElementParameter);
+            io.recover();
+
+            assertThat(io.getElementCount(), is(equalTo(1)));
+         }
+    }
+
+    @Test(expected = AbstractByteBufferPageIO.PageIOInvalidElementException.class)
+    public void openInvalidDeqNumDoubleElement() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        ByteBufferPageIO io = newEmptyPageIO();
+        byte[] bytes = element.serialize();
+        io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+        io.write(bytes, 3L, bytes.length, io.checksum(bytes));
+
+        io = newPageIO(io.dump().length, io.dump());
+        io.open(1L, 2);
+    }
+
+    @Test
+    public void recoverInvalidDeqNumDoubleElement() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        ByteBufferPageIO io = newEmptyPageIO();
+        byte[] bytes = element.serialize();
+        io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+        io.write(bytes, 3L, bytes.length, io.checksum(bytes));
+
+        io = newPageIO(io.dump().length, io.dump());
+        io.recover();
+
+        assertThat(io.getElementCount(), is(equalTo(1)));
+    }
+
+    @Test
+    public void writeRead() throws IOException {
+        long seqNum = 42L;
+        Queueable element = buildStringElement("foobarbaz");
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+        SequencedList<byte[]> result = io.read(seqNum, 1);
+        assertThat(result.getElements().size(), is(equalTo(1)));
+        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
+        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
+        assertThat(readElement.toString(), is(equalTo(element.toString())));
+    }
+
+    @Test
+    public void writeReadMulti() throws IOException {
+        Queueable element1 = buildStringElement("foo");
+        Queueable element2 = buildStringElement("bar");
+        Queueable element3 = buildStringElement("baz");
+        Queueable element4 = buildStringElement("quux");
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element1.serialize(), 40L);
+        io.write(element2.serialize(), 41L);
+        io.write(element3.serialize(), 42L);
+        io.write(element4.serialize(), 43L);
+        int batchSize = 11;
+        SequencedList<byte[]> result = io.read(40L, batchSize);
+        assertThat(result.getElements().size(), is(equalTo(4)));
+
+        assertThat(result.getSeqNums().get(0), is(equalTo(40L)));
+        assertThat(result.getSeqNums().get(1), is(equalTo(41L)));
+        assertThat(result.getSeqNums().get(2), is(equalTo(42L)));
+        assertThat(result.getSeqNums().get(3), is(equalTo(43L)));
+
+        assertThat(StringElement.deserialize(result.getElements().get(0)).toString(), is(equalTo(element1.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(1)).toString(), is(equalTo(element2.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(2)).toString(), is(equalTo(element3.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(3)).toString(), is(equalTo(element4.toString())));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileCheckpointIOTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileCheckpointIOTest.java
new file mode 100644
index 00000000000..a6a6455a7f4
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileCheckpointIOTest.java
@@ -0,0 +1,55 @@
+package org.logstash.ackedqueue.io;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.ackedqueue.Checkpoint;
+import org.logstash.ackedqueue.io.CheckpointIO;
+import org.logstash.ackedqueue.io.FileCheckpointIO;
+
+import java.net.URL;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class FileCheckpointIOTest {
+    private String checkpointFolder;
+    private CheckpointIO io;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        checkpointFolder = temporaryFolder
+                .newFolder("checkpoints")
+                .getPath();
+        io = new FileCheckpointIO(checkpointFolder);
+    }
+
+    @Test
+    public void read() throws Exception {
+        URL url = this.getClass().getResource("checkpoint.head");
+        String dirPath = Paths.get(url.toURI()).getParent().toString();
+        io = new FileCheckpointIO(dirPath);
+        Checkpoint chk = io.read("checkpoint.head");
+        assertThat(chk.getMinSeqNum(), is(8L));
+    }
+
+    @Test
+    public void write() throws Exception {
+        io.write("checkpoint.head", 6, 2, 10L, 8L, 200);
+        io.write("checkpoint.head", 6, 2, 10L, 8L, 200);
+        Path fullFileName = Paths.get(checkpointFolder, "checkpoint.head");
+        byte[] contents = Files.readAllBytes(fullFileName);
+        URL url = this.getClass().getResource("checkpoint.head");
+        Path path = Paths.get(url.toURI());
+        byte[] compare = Files.readAllBytes(path);
+        assertThat(contents, is(equalTo(compare)));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
new file mode 100644
index 00000000000..fbc7db370f1
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
@@ -0,0 +1,56 @@
+package org.logstash.ackedqueue.io;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.ackedqueue.SequencedList;
+import org.logstash.ackedqueue.StringElement;
+import org.logstash.ackedqueue.io.MmapPageIO;
+
+import java.util.ArrayList;
+import java.util.List;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class FileMmapIOTest {
+    private String folder;
+    private MmapPageIO writeIo;
+    private MmapPageIO readIo;
+    private int pageNum;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        pageNum = 0;
+        folder = temporaryFolder
+                .newFolder("pages")
+                .getPath();
+        writeIo = new MmapPageIO(pageNum, 1024, folder);
+        readIo = new MmapPageIO(pageNum, 1024, folder);
+    }
+
+    @Test
+    public void roundTrip() throws Exception {
+        List<StringElement> list = new ArrayList<>();
+        List<StringElement> readList = new ArrayList<>();
+        writeIo.create();
+        for (int i = 1; i < 17; i++) {
+            StringElement input = new StringElement("element-" + i);
+            list.add(input);
+            writeIo.write(input.serialize(), i);
+        }
+        writeIo.close();
+        readIo.open(1, 16);
+        SequencedList<byte[]> result = readIo.read(1, 16);
+        for (byte[] bytes : result.getElements()) {
+            StringElement element = StringElement.deserialize(bytes);
+            readList.add(element);
+        }
+        assertThat(readList, is(equalTo(list)));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/MemoryCheckpointTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/MemoryCheckpointTest.java
new file mode 100644
index 00000000000..a2bcd9b41f2
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/MemoryCheckpointTest.java
@@ -0,0 +1,91 @@
+package org.logstash.ackedqueue.io;
+
+import org.junit.Before;
+import org.junit.Test;
+import static org.junit.Assert.fail;
+import org.logstash.ackedqueue.Checkpoint;
+import org.logstash.ackedqueue.MemorySettings;
+import org.logstash.ackedqueue.Settings;
+import org.logstash.ackedqueue.io.CheckpointIO;
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.MemoryCheckpointIO;
+
+import java.io.IOException;
+import java.nio.file.NoSuchFileException;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class MemoryCheckpointTest {
+
+    private CheckpointIO io;
+
+    @Before
+    public void setUp() {
+        Settings settings = new MemorySettings();
+        CheckpointIOFactory factory = (dirPath) -> new MemoryCheckpointIO(dirPath);
+        settings.setCheckpointIOFactory(factory);
+        this.io = settings.getCheckpointIOFactory().build(settings.getDirPath());
+    }
+
+    @Test
+    public void writeNewReadExisting() throws IOException {
+        io.write("checkpoint.head", 1, 2, 3, 4, 5);
+
+        Checkpoint checkpoint = io.read("checkpoint.head");
+
+        assertThat(checkpoint.getPageNum(), is(equalTo(1)));
+        assertThat(checkpoint.getFirstUnackedPageNum(), is(equalTo(2)));
+        assertThat(checkpoint.getFirstUnackedSeqNum(), is(equalTo(3L)));
+        assertThat(checkpoint.getMinSeqNum(), is(equalTo(4L)));
+        assertThat(checkpoint.getElementCount(), is(equalTo(5)));
+    }
+
+    @Test(expected = NoSuchFileException.class)
+    public void readNonexistent() throws IOException {
+        io.read("checkpoint.invalid");
+    }
+
+    @Test
+    public void readWriteDirPathNamespaced() throws IOException {
+        CheckpointIO io1 = new MemoryCheckpointIO("path1");
+        CheckpointIO io2 = new MemoryCheckpointIO("path2");
+        io1.write("checkpoint.head", 1, 0, 0, 0, 0);
+        io2.write("checkpoint.head", 2, 0, 0, 0, 0);
+
+        Checkpoint checkpoint;
+
+        checkpoint = io1.read("checkpoint.head");
+        assertThat(checkpoint.getPageNum(), is(equalTo(1)));
+
+        checkpoint = io2.read("checkpoint.head");
+        assertThat(checkpoint.getPageNum(), is(equalTo(2)));
+    }
+
+    @Test(expected = NoSuchFileException.class)
+    public void purgeDirPathNamespaced1() throws IOException {
+        CheckpointIO io1 = new MemoryCheckpointIO("path1");
+        CheckpointIO io2 = new MemoryCheckpointIO("path2");
+        io1.write("checkpoint.head", 1, 0, 0, 0, 0);
+        io2.write("checkpoint.head", 2, 0, 0, 0, 0);
+
+        io1.purge("checkpoint.head");
+
+        Checkpoint checkpoint = io1.read("checkpoint.head");
+    }
+
+    @Test
+    public void purgeDirPathNamespaced2() throws IOException {
+        CheckpointIO io1 = new MemoryCheckpointIO("path1");
+        CheckpointIO io2 = new MemoryCheckpointIO("path2");
+        io1.write("checkpoint.head", 1, 0, 0, 0, 0);
+        io2.write("checkpoint.head", 2, 0, 0, 0, 0);
+
+        io1.purge("checkpoint.head");
+
+        Checkpoint checkpoint;
+        checkpoint = io2.read("checkpoint.head");
+        assertThat(checkpoint.getPageNum(), is(equalTo(2)));
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStreamTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStreamTest.java
new file mode 100644
index 00000000000..7b3a63b6a65
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStreamTest.java
@@ -0,0 +1,188 @@
+package org.logstash.ackedqueue.io.wip;
+
+import org.junit.Test;
+import org.logstash.ackedqueue.Queueable;
+import org.logstash.ackedqueue.SequencedList;
+import org.logstash.ackedqueue.StringElement;
+import org.logstash.ackedqueue.io.wip.MemoryPageIOStream;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class MemoryPageIOStreamTest {
+
+    private final int CAPACITY = 1024;
+    private final int EMPTY_HEADER_SIZE = Integer.BYTES + Integer.BYTES;
+
+    private byte[] empty_page_with_header() {
+        byte[] result = new byte[CAPACITY];
+        // version = 1, details = ABC
+        ByteBuffer.wrap(result).put(new byte[]{0, 0, 0, 1, 0, 0, 0, 3, 65, 66, 67});
+        return result;
+    }
+
+    private MemoryPageIOStream subject() throws IOException {
+        return subject(CAPACITY);
+    }
+
+    private MemoryPageIOStream subject(int size) throws IOException {
+        MemoryPageIOStream io = new MemoryPageIOStream(size);
+        io.create();
+        return io;
+    }
+
+    private MemoryPageIOStream subject(byte[] bytes, long seqNum, int count) throws IOException {
+        MemoryPageIOStream io = new MemoryPageIOStream(bytes.length, bytes);
+        io.open(seqNum, count);
+        return io;
+    }
+
+    private Queueable buildStringElement(String str) {
+        return new StringElement(str);
+    }
+
+    @Test
+    public void getWritePosition() throws Exception {
+        assertThat(subject().getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE)));
+        assertThat(subject(empty_page_with_header(), 1L, 0).getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE + 3)));
+    }
+
+    @Test
+    public void getElementCount() throws Exception {
+        assertThat(subject().getElementCount(), is(equalTo(0)));
+        assertThat(subject(empty_page_with_header(), 1L, 0).getElementCount(), is(equalTo(0)));
+    }
+
+    @Test
+    public void getStartSeqNum() throws Exception {
+        assertThat(subject().getMinSeqNum(), is(equalTo(1L)));
+        assertThat(subject(empty_page_with_header(), 1L, 0).getMinSeqNum(), is(equalTo(1L)));
+    }
+
+    @Test
+    public void readHeaderDetails() throws Exception {
+        MemoryPageIOStream io = new MemoryPageIOStream(CAPACITY);
+        io.setPageHeaderDetails("ABC");
+        io.create();
+        assertThat(io.readHeaderDetails(), is(equalTo("ABC")));
+        assertThat(io.getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE + 3)));
+    }
+
+    @Test
+    public void hasSpace() throws Exception {
+        assertThat(subject().hasSpace(10), is(true));
+    }
+
+    @Test
+    public void write() throws Exception {
+        long seqNum = 42L;
+        Queueable element = new StringElement("foobarbaz");
+        MemoryPageIOStream subj = subject();
+        subj.write(element.serialize(), seqNum);
+        assertThat(subj.getElementCount(), is(equalTo(1)));
+        assertThat(subj.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void writeUntilFull() throws Exception {
+        long seqNum = 42L;
+        Queueable element = new StringElement("foobarbaz");
+        byte[] data = element.serialize();
+        int bufferSize = 120;
+        MemoryPageIOStream subj = subject(bufferSize);
+        while (subj.hasSpace(data.length)) {
+            subj.write(data, seqNum);
+            seqNum++;
+        }
+        int recordSize = subj.persistedByteCount(data.length);
+        int remains = bufferSize - subj.getWritePosition();
+        assertThat(recordSize, is(equalTo(25))); // element=9 + seqnum=8 + length=4 + crc=4
+        assertThat(subj.getElementCount(), is(equalTo(4)));
+        boolean noSpaceLeft = remains < recordSize;
+        assertThat(noSpaceLeft, is(true));
+    }
+
+    @Test
+    public void read() throws Exception {
+        MemoryPageIOStream subj = subject();
+        SequencedList<byte[]> result = subj.read(1L, 1);
+        assertThat(result.getElements().isEmpty(), is(true));
+    }
+
+    @Test
+    public void writeRead() throws Exception {
+        long seqNum = 42L;
+        Queueable element = buildStringElement("foobarbaz");
+        MemoryPageIOStream subj = subject();
+        subj.write(element.serialize(), seqNum);
+        SequencedList<byte[]> result = subj.read(seqNum, 1);
+        assertThat(result.getElements().size(), is(equalTo(1)));
+        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
+        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
+        assertThat(readElement.toString(), is(equalTo(element.toString())));
+    }
+
+    @Test
+    public void writeReadEmptyElement() throws Exception {
+        long seqNum = 1L;
+        Queueable element = buildStringElement("");
+        MemoryPageIOStream subj = subject();
+        subj.write(element.serialize(), seqNum);
+        SequencedList<byte[]> result = subj.read(seqNum, 1);
+        assertThat(result.getElements().size(), is(equalTo(1)));
+        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
+        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
+        assertThat(readElement.toString(), is(equalTo(element.toString())));
+    }
+
+    @Test
+    public void writeReadMulti() throws Exception {
+        Queueable element1 = buildStringElement("foo");
+        Queueable element2 = buildStringElement("bar");
+        Queueable element3 = buildStringElement("baz");
+        Queueable element4 = buildStringElement("quux");
+        MemoryPageIOStream subj = subject();
+        subj.write(element1.serialize(), 40L);
+        subj.write(element2.serialize(), 42L);
+        subj.write(element3.serialize(), 44L);
+        subj.write(element4.serialize(), 46L);
+        int batchSize = 11;
+        SequencedList<byte[]> result = subj.read(40L, batchSize);
+        assertThat(result.getElements().size(), is(equalTo(4)));
+
+        assertThat(result.getSeqNums().get(0), is(equalTo(40L)));
+        assertThat(result.getSeqNums().get(1), is(equalTo(42L)));
+        assertThat(result.getSeqNums().get(2), is(equalTo(44L)));
+        assertThat(result.getSeqNums().get(3), is(equalTo(46L)));
+
+        assertThat(StringElement.deserialize(result.getElements().get(0)).toString(), is(equalTo(element1.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(1)).toString(), is(equalTo(element2.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(2)).toString(), is(equalTo(element3.toString())));
+        assertThat(StringElement.deserialize(result.getElements().get(3)).toString(), is(equalTo(element4.toString())));
+    }
+
+    @Test
+    public void readFromFirstUnackedSeqNum() throws Exception {
+        long seqNum = 10L;
+        String[] values = new String[]{"aaa", "bbb", "ccc", "ddd", "eee", "fff", "ggg", "hhh", "iii", "jjj"};
+        MemoryPageIOStream stream = subject(300);
+        for (String val : values) {
+            Queueable element = buildStringElement(val);
+            stream.write(element.serialize(), seqNum);
+            seqNum++;
+        }
+        MemoryPageIOStream subj = subject(stream.getBuffer(), 10L, 10);
+        int batchSize = 3;
+        seqNum = 13L;
+        SequencedList<byte[]> result = subj.read(seqNum, batchSize);
+        for (int i = 0; i < 3; i++) {
+            Queueable ele = StringElement.deserialize(result.getElements().get(i));
+            assertThat(result.getSeqNums().get(i), is(equalTo(seqNum + i)));
+            assertThat(ele.toString(), is(equalTo(values[i + 3])));
+        }
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/bivalues/BiValueTest.java b/logstash-core/src/test/java/org/logstash/bivalues/BiValueTest.java
new file mode 100644
index 00000000000..c4a96e2905d
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/bivalues/BiValueTest.java
@@ -0,0 +1,215 @@
+package org.logstash.bivalues;
+
+import org.logstash.TestBase;
+import org.joda.time.DateTime;
+import org.jruby.RubyBignum;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.RubyNil;
+import org.jruby.RubyString;
+import org.jruby.RubySymbol;
+import org.jruby.RubyTime;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.junit.Test;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+public class BiValueTest extends TestBase {
+    @Test
+    public void testStringBiValueFromRuby() {
+        String s = "foo bar baz";
+        StringBiValue subject = new StringBiValue(RubyString.newString(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testStringBiValueFromJava() {
+        RubyString v = RubyString.newString(ruby, "foo bar baz");
+        StringBiValue subject = new StringBiValue("foo bar baz");
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testSymbolBiValueFromRuby() {
+        String s = "foo";
+        SymbolBiValue subject = new SymbolBiValue(RubySymbol.newSymbol(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testLongBiValueFromRuby() {
+        Long s = 123456789L;
+        LongBiValue subject = new LongBiValue(RubyFixnum.newFixnum(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testLongBiValueFromJava() {
+        RubyInteger v = RubyFixnum.newFixnum(ruby, 123456789L);
+        LongBiValue subject = new LongBiValue(123456789L);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+
+    @Test
+    public void testIntegerBiValueFromRuby() {
+        int j = 123456789;
+        IntegerBiValue subject = new IntegerBiValue(RubyFixnum.newFixnum(ruby, j));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertTrue(j - subject.javaValue() == 0);
+    }
+
+    @Test
+    public void testIntegerBiValueFromJava() {
+        RubyInteger v = RubyFixnum.newFixnum(ruby, 123456789);
+        IntegerBiValue subject = new IntegerBiValue(123456789);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testBigDecimalBiValueFromRuby() {
+        BigDecimal s = BigDecimal.valueOf(12345.678D);
+        BigDecimalBiValue subject = new BigDecimalBiValue(new RubyBigDecimal(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testBigDecimalBiValueFromJava() {
+        RubyBigDecimal v = new RubyBigDecimal(ruby, new BigDecimal(12345.678D));
+        BigDecimalBiValue subject = new BigDecimalBiValue(new BigDecimal(12345.678D));
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testDoubleBiValueFromRuby() {
+        Double s = 12345.678D;
+        DoubleBiValue subject = new DoubleBiValue(RubyFloat.newFloat(ruby, 12345.678D));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testDoubleBiValueFromJava() {
+        RubyFloat v = RubyFloat.newFloat(ruby, 12345.678D);
+        DoubleBiValue subject = new DoubleBiValue(12345.678D);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testBooleanBiValueFromRuby() {
+        BooleanBiValue subject = new BooleanBiValue(RubyBoolean.newBoolean(ruby, true));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertTrue(subject.javaValue());
+    }
+
+    @Test
+    public void testBooleanBiValueFromJava() {
+        RubyBoolean v = RubyBoolean.newBoolean(ruby, true);
+        BooleanBiValue subject = new BooleanBiValue(true);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testNullBiValueFromRuby() {
+        NullBiValue subject = new NullBiValue((RubyNil) ruby.getNil());
+        assertTrue(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(null, subject.javaValue());
+    }
+
+    @Test
+    public void testNullBiValueFromJava() {
+        NullBiValue subject = NullBiValue.newNullBiValue();
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(ruby.getNil(), subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testTimeBiValueFromRuby() {
+        DateTime t = DateTime.now();
+        RubyTime now = RubyTime.newTime(ruby, t);
+        TimeBiValue subject = new TimeBiValue(now);
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(t, subject.javaValue());
+    }
+
+    @Test
+    public void testTimeBiValueFromJava() {
+        DateTime t = DateTime.now();
+        TimeBiValue subject = new TimeBiValue(t);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(RubyTime.newTime(ruby, t), subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testBigIntegerBiValueFromRuby() {
+        BigInteger s = BigInteger.valueOf(12345678L);
+        BigIntegerBiValue subject = new BigIntegerBiValue(new RubyBignum(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testBigIntegerBiValueFromJava() {
+        RubyBignum v = new RubyBignum(ruby, BigInteger.valueOf(12345678L));
+        BigIntegerBiValue subject = new BigIntegerBiValue(BigInteger.valueOf(12345678L));
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testSerialization() throws Exception {
+        RubyBignum v = RubyBignum.newBignum(ruby, "-9223372036854776000");
+        BiValue original = BiValues.newBiValue(v);
+
+        ByteArrayOutputStream baos = new ByteArrayOutputStream();
+        ObjectOutputStream oos = new ObjectOutputStream(baos);
+        oos.writeObject(original);
+        oos.close();
+
+        ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
+        ObjectInputStream ois = new ObjectInputStream(bais);
+        BiValue copy = (BiValue) ois.readObject();
+        assertEquals(original, copy);
+        assertFalse(copy.hasRubyValue());
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/bivalues/BiValuesTest.java b/logstash-core/src/test/java/org/logstash/bivalues/BiValuesTest.java
new file mode 100644
index 00000000000..267da0567a9
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/bivalues/BiValuesTest.java
@@ -0,0 +1,280 @@
+package org.logstash.bivalues;
+
+import org.logstash.TestBase;
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.jruby.RubyBignum;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.RubyNil;
+import org.jruby.RubyString;
+import org.jruby.RubySymbol;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.jruby.javasupport.JavaUtil;
+import org.junit.Test;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class BiValuesTest extends TestBase {
+
+    @Test
+    public void testBiValuesStringRuby() {
+        String js = "double";
+        RubyString rs = RubyString.newUnicodeString(ruby, js);
+        BiValue subject = BiValues.newBiValue(rs);
+
+        assertEquals(rs, subject.rubyValueUnconverted());
+        assertEquals(rs.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(js, subject.javaValue());
+        assertEquals(String.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesStringJava() {
+        String js = "double";
+        RubyString rs = RubyString.newUnicodeString(ruby, js);
+        BiValue subject = BiValues.newBiValue(js);
+
+        assertEquals(js, subject.javaValue());
+        assertEquals(String.class, subject.javaValue().getClass());
+        assertEquals(rs, subject.rubyValue(ruby));
+        assertEquals(rs.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesSymbolRuby() {
+        String js = "double";
+        RubySymbol rs = RubySymbol.newSymbol(ruby, js);
+        BiValue subject = BiValues.newBiValue(rs);
+
+        assertEquals(rs, subject.rubyValueUnconverted());
+        assertEquals(rs.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(js, subject.javaValue());
+        assertEquals(String.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesLongRuby() {
+        long jo = 1234567L;
+        RubyInteger ro = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Long.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesLongJava() {
+        long jo = 1234567L;
+        RubyInteger ro = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Long.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesFloatRuby() {
+        double jo = 1234.567D;
+        RubyFloat ro = (RubyFloat) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Double.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesFloatJava() {
+        double jo = 1234.567D;
+        RubyFloat ro = (RubyFloat) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Double.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBigDecimalRuby() {
+        BigDecimal jo = BigDecimal.valueOf(12345.678D);
+        RubyBigDecimal ro = new RubyBigDecimal(ruby, ruby.getClass("BigDecimal"), jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigDecimal.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBigDecimalJava() {
+        BigDecimal jo = BigDecimal.valueOf(12345.678D);
+        RubyBigDecimal ro = new RubyBigDecimal(ruby, ruby.getClass("BigDecimal"), jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigDecimal.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanRubyTrue() {
+        boolean jo = true;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertTrue(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanRubyFalse() {
+        boolean jo = false;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertFalse(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanJavaTrue() {
+        boolean jo = true;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(jo);
+
+        assertTrue(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanJavaFalse() {
+        boolean jo = false;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(jo);
+
+        assertFalse(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesTimestampRuby() {
+        Timestamp jo = new Timestamp("2014-09-23T00:00:00-0800");
+        RubyTimestamp ro = RubyTimestamp.newRubyTimestamp(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Timestamp.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesTimestampJava() {
+        Timestamp jo = new Timestamp("2014-09-23T00:00:00-0800");
+        RubyTimestamp ro = RubyTimestamp.newRubyTimestamp(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Timestamp.class, subject.javaValue().getClass());
+        assertEquals(ro.toString(), subject.rubyValue(ruby).toString());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesNilRuby() {
+        RubyNil ro = (RubyNil) ruby.getNil();
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertNull(subject.javaValue());
+    }
+
+    @Test
+    public void testBiValuesNullJava() {
+        RubyNil ro = (RubyNil) ruby.getNil();
+        BiValue subject = BiValues.newBiValue(null);
+
+        assertNull(subject.javaValue());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBigIntegerRuby() {
+        BigInteger jo = BigInteger.valueOf(12345678L);
+        RubyBignum ro = new RubyBignum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(BigIntegerBiValue.class, subject.getClass());
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigInteger.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBigIntegerJava() {
+        BigInteger jo = BigInteger.valueOf(12345678L);
+        RubyBignum ro = new RubyBignum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigInteger.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    // NOTE: testBiValuesIntegerRuby will map to LongBiValue
+    @Test
+    public void testBiValuesIntegerRuby() {
+        int jo = 12345678;
+        RubyInteger ro = RubyFixnum.newFixnum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(LongBiValue.class, subject.getClass());
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(12345678L, subject.javaValue());
+        assertEquals(Long.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesIntegerJava() {
+        int jo = 12345678;
+        RubyInteger ro = RubyFixnum.newFixnum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(IntegerBiValue.class, subject.getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Integer.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/bivalues/SomeJavaObject.java b/logstash-core/src/test/java/org/logstash/bivalues/SomeJavaObject.java
new file mode 100644
index 00000000000..1ebe27c4818
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/bivalues/SomeJavaObject.java
@@ -0,0 +1,29 @@
+package org.logstash.bivalues;
+
+public class SomeJavaObject<T> {
+    private T value;
+
+    public T getValue() {
+        return value;
+    }
+
+    public SomeJavaObject(T value) {
+        this.value = value;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (!(o instanceof SomeJavaObject)) return false;
+
+        SomeJavaObject<?> that = (SomeJavaObject<?>) o;
+
+        return value != null ? value.equals(that.getValue()) : that.value == null;
+
+    }
+
+    @Override
+    public int hashCode() {
+        return value != null ? value.hashCode() : 0;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReadManagerTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReadManagerTest.java
new file mode 100644
index 00000000000..7c928fe9f3d
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReadManagerTest.java
@@ -0,0 +1,145 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.common.io;
+
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.DLQEntry;
+import org.logstash.Event;
+import org.logstash.Timestamp;
+import org.logstash.ackedqueue.StringElement;
+
+import java.io.BufferedWriter;
+import java.io.IOException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.util.Collections;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.CoreMatchers.nullValue;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+public class DeadLetterQueueReadManagerTest {
+    private Path dir;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    private static String segmentFileName(int i) {
+        return String.format(DeadLetterQueueWriteManager.SEGMENT_FILE_PATTERN, i);
+    }
+
+    @Before
+    public void setUp() throws Exception {
+        dir = temporaryFolder.newFolder().toPath();
+    }
+
+    @Test
+    public void testReadFromTwoSegments() throws Exception {
+        RecordIOWriter writer = null;
+
+        for (int i = 0; i < 5; i++) {
+            Path segmentPath = dir.resolve(segmentFileName(i));
+            writer = new RecordIOWriter(segmentPath);
+            for (int j = 0; j < 10; j++) {
+                writer.writeEvent((new StringElement("" + (i * 10 + j))).serialize());
+            }
+            if (i < 4) {
+                writer.close();
+            }
+        }
+
+        DeadLetterQueueReadManager manager = new DeadLetterQueueReadManager(dir);
+
+        for (int i = 0; i < 50; i++) {
+            String first = StringElement.deserialize(manager.pollEntryBytes()).toString();
+            assertThat(first, equalTo(String.valueOf(i)));
+        }
+
+        assertThat(manager.pollEntryBytes(), is(nullValue()));
+        assertThat(manager.pollEntryBytes(), is(nullValue()));
+        assertThat(manager.pollEntryBytes(), is(nullValue()));
+        assertThat(manager.pollEntryBytes(), is(nullValue()));
+
+        for (int j = 50; j < 60; j++) {
+            writer.writeEvent((new StringElement(String.valueOf(j))).serialize());
+        }
+
+        for (int i = 50; i < 60; i++) {
+            String first = StringElement.deserialize(manager.pollEntryBytes()).toString();
+            assertThat(first, equalTo(String.valueOf(i)));
+        }
+
+        writer.close();
+
+        Path segmentPath = dir.resolve(segmentFileName(5));
+        writer = new RecordIOWriter(segmentPath);
+
+        for (int j = 0; j < 10; j++) {
+            writer.writeEvent((new StringElement(String.valueOf(j))).serialize());
+        }
+
+
+        for (int i = 0; i < 10; i++) {
+            byte[] read = manager.pollEntryBytes();
+            while (read == null) {
+                read = manager.pollEntryBytes();
+            }
+            String first = StringElement.deserialize(read).toString();
+            assertThat(first, equalTo(String.valueOf(i)));
+        }
+
+
+        manager.close();
+    }
+
+    @Test
+    public void testSeek() throws Exception {
+        DeadLetterQueueWriteManager writeManager = new DeadLetterQueueWriteManager(dir, 10000000, 10000000);
+        Event event = new Event(Collections.emptyMap());
+        Timestamp target = null;
+        long currentEpoch = System.currentTimeMillis();
+        for (int i = 0; i < 1000; i++){
+            DLQEntry entry = new DLQEntry(event, "foo", "bar", String.valueOf(i), new Timestamp(currentEpoch++));
+            writeManager.writeEntry(entry);
+            if (i == 543) {
+                target = entry.getEntryTime();
+            }
+
+        }
+        writeManager.close();
+
+        DeadLetterQueueReadManager readManager = new DeadLetterQueueReadManager(dir);
+        readManager.seekToNextEvent(target);
+        DLQEntry entry = readManager.pollEntry(100);
+        assertThat(entry.getEntryTime().toIso8601(), equalTo(target.toIso8601()));
+        assertThat(entry.getReason(), equalTo("543"));
+    }
+
+    @Test
+    public void testInvalidDirectory()  throws Exception {
+        DeadLetterQueueReadManager readManager = new DeadLetterQueueReadManager(dir);
+        assertThat(readManager.pollEntry(100), is(nullValue()));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriteManagerTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriteManagerTest.java
new file mode 100644
index 00000000000..c9bf4cf1f67
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriteManagerTest.java
@@ -0,0 +1,95 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.common.io;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.DLQEntry;
+import org.logstash.Event;
+
+import java.nio.channels.FileChannel;
+import java.nio.channels.OverlappingFileLockException;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+
+import static junit.framework.TestCase.assertFalse;
+import static org.junit.Assert.assertTrue;
+import static org.junit.Assert.fail;
+
+public class DeadLetterQueueWriteManagerTest {
+    private Path dir;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        dir = temporaryFolder.newFolder().toPath();
+    }
+
+    @Test
+    public void testLockFileManagement() throws Exception {
+        Path lockFile = dir.resolve(".lock");
+        DeadLetterQueueWriteManager writer = new DeadLetterQueueWriteManager(dir, 1000, 1000000);
+        assertTrue(Files.exists(lockFile));
+        writer.close();
+        assertFalse(Files.exists(lockFile));
+    }
+
+    @Test
+    public void testFileLocking() throws Exception {
+        DeadLetterQueueWriteManager writer = new DeadLetterQueueWriteManager(dir, 1000, 1000000);
+        try {
+            new DeadLetterQueueWriteManager(dir, 1000, 100000);
+            fail();
+        } catch (RuntimeException e) {
+        } finally {
+            writer.close();
+        }
+    }
+
+    @Test
+    public void testUncleanCloseOfPreviousWriter() throws Exception {
+        Path lockFilePath = dir.resolve(".lock");
+        boolean created = lockFilePath.toFile().createNewFile();
+        DeadLetterQueueWriteManager writer = new DeadLetterQueueWriteManager(dir, 1000, 1000000);
+
+        FileChannel channel = FileChannel.open(lockFilePath, StandardOpenOption.WRITE);
+        try {
+            channel.lock();
+            fail();
+        } catch (OverlappingFileLockException e) {
+            assertTrue(created);
+        } finally {
+            writer.close();
+        }
+    }
+
+    @Test
+    public void testWrite() throws Exception {
+        DeadLetterQueueWriteManager writer = new DeadLetterQueueWriteManager(dir, 1000, 1000000);
+        DLQEntry entry = new DLQEntry(new Event(), "type", "id", "reason");
+        writer.writeEntry(entry);
+        writer.close();
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/RecordIOWriterTest.java b/logstash-core/src/test/java/org/logstash/common/io/RecordIOWriterTest.java
new file mode 100644
index 00000000000..d3a55ac65a3
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/RecordIOWriterTest.java
@@ -0,0 +1,190 @@
+package org.logstash.common.io;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+import org.logstash.ackedqueue.StringElement;
+
+import java.nio.ByteBuffer;
+import java.nio.file.Path;
+import java.util.Arrays;
+import java.util.Comparator;
+import java.util.function.Function;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.CoreMatchers.not;
+import static org.hamcrest.CoreMatchers.nullValue;
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
+
+public class RecordIOWriterTest {
+    private Path file;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        file = temporaryFolder.newFile("test").toPath();
+    }
+
+    @Test
+    public void testReadEmptyBlock() throws Exception {
+        RecordIOWriter writer = new RecordIOWriter(file);
+        RecordIOReader reader = new RecordIOReader(file);
+        assertThat(reader.readEvent(), is(nullValue()));
+        writer.close();
+        reader.close();
+    }
+
+    @Test
+    public void testSingleComplete() throws Exception {
+        StringElement input = new StringElement("element");
+        RecordIOWriter writer = new RecordIOWriter(file);
+        writer.writeEvent(input.serialize());
+        RecordIOReader reader = new RecordIOReader(file);
+        assertThat(StringElement.deserialize(reader.readEvent()), is(equalTo(input)));
+
+        reader.close();
+        writer.close();
+    }
+
+    @Test
+    public void testSeekToStartFromEndWithoutNextRecord() throws Exception {
+        char[] tooBig = new char[BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'c');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        writer.writeEvent(input.serialize());
+
+        RecordIOReader reader = new RecordIOReader(file);
+        reader.seekToBlock(1);
+        reader.consumeBlock(true);
+        assertThat(reader.seekToStartOfEventInBlock(), equalTo(-1));
+
+        reader.close();
+        writer.close();
+    }
+
+    @Test
+    public void testSeekToStartFromEndWithNextRecordPresent() throws Exception {
+        char[] tooBig = new char[BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'c');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        writer.writeEvent(input.serialize());
+        writer.writeEvent(input.serialize());
+
+        RecordIOReader reader = new RecordIOReader(file);
+        reader.seekToBlock(1);
+        reader.consumeBlock(true);
+        assertThat(reader.seekToStartOfEventInBlock(), equalTo(1026));
+
+        reader.close();
+        writer.close();
+    }
+
+
+    @Test
+    public void testFitsInTwoBlocks() throws Exception {
+        char[] tooBig = new char[BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'c');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        writer.writeEvent(input.serialize());
+        writer.close();
+    }
+
+    @Test
+    public void testFitsInThreeBlocks() throws Exception {
+        char[] tooBig = new char[2 * BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'r');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        writer.writeEvent(input.serialize());
+        writer.close();
+
+        RecordIOReader reader = new RecordIOReader(file);
+        StringElement element = StringElement.deserialize(reader.readEvent());
+        assertThat(element.toString().length(), equalTo(input.toString().length()));
+        assertThat(element.toString(), equalTo(input.toString()));
+        assertThat(reader.readEvent(), is(nullValue()));
+        reader.close();
+    }
+
+    @Test
+    public void testReadWhileWrite() throws Exception {
+        char[] tooBig = new char[2 * BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'r');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        RecordIOReader reader = new RecordIOReader(file);
+        byte[] inputSerialized = input.serialize();
+
+        writer.writeEvent(inputSerialized);
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        writer.writeEvent(inputSerialized);
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        writer.writeEvent(inputSerialized);
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        writer.writeEvent(inputSerialized);
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        assertThat(reader.readEvent(), is(nullValue()));
+        assertThat(reader.readEvent(), is(nullValue()));
+        assertThat(reader.readEvent(), is(nullValue()));
+        writer.writeEvent(inputSerialized);
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        writer.writeEvent(inputSerialized);
+        writer.writeEvent(inputSerialized);
+        writer.writeEvent(inputSerialized);
+        writer.writeEvent(inputSerialized);
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        assertThat(reader.readEvent(), equalTo(inputSerialized));
+        assertThat(reader.readEvent(), is(nullValue()));
+
+        writer.close();
+        reader.close();
+    }
+
+    @Test
+    public void testReadMiddle() throws Exception {
+        char[] tooBig = new char[3 * BLOCK_SIZE + 1000];
+        Arrays.fill(tooBig, 'r');
+        StringElement input = new StringElement(new String(tooBig));
+        RecordIOWriter writer = new RecordIOWriter(file);
+        RecordIOReader reader = new RecordIOReader(file);
+        byte[] inputSerialized = input.serialize();
+
+        writer.writeEvent(inputSerialized);
+        reader.seekToBlock(1);
+        assertThat(reader.readEvent(), is(nullValue()));
+        writer.writeEvent(inputSerialized);
+        reader.seekToBlock(1);
+        assertThat(reader.readEvent(), is(not(nullValue())));
+
+        writer.close();
+        reader.close();
+    }
+
+    @Test
+    public void testFind() throws Exception {
+
+        RecordIOWriter writer = new RecordIOWriter(file);
+        RecordIOReader reader = new RecordIOReader(file);
+        ByteBuffer intBuffer = ByteBuffer.wrap(new byte[4]);
+        for (int i = 0; i < 20000; i++) {
+            intBuffer.rewind();
+            intBuffer.putInt(i);
+            writer.writeEvent(intBuffer.array());
+        }
+
+        Function<byte[], Object> toInt = (b) -> ByteBuffer.wrap(b).getInt();
+        reader.seekToNextEventPosition(34, toInt, (o1, o2) -> ((Integer) o1).compareTo((Integer) o2));
+        int nextVal = (int) toInt.apply(reader.readEvent());
+        assertThat(nextVal, equalTo(34));
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/log/CustomLogEventTests.java b/logstash-core/src/test/java/org/logstash/log/CustomLogEventTests.java
new file mode 100644
index 00000000000..9604c457894
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/log/CustomLogEventTests.java
@@ -0,0 +1,113 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.log;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.apache.logging.log4j.junit.LoggerContextRule;
+import org.apache.logging.log4j.test.appender.ListAppender;
+import org.junit.ClassRule;
+import org.junit.Test;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static junit.framework.TestCase.assertEquals;
+import static junit.framework.TestCase.assertNotNull;
+
+public class CustomLogEventTests {
+    private static final ObjectMapper mapper = new ObjectMapper();
+    private static final String CONFIG = "log4j2-test1.xml";
+    private ListAppender appender;
+
+    @ClassRule
+    public static LoggerContextRule CTX = new LoggerContextRule(CONFIG);
+
+    @Test
+    public void testPatternLayout() {
+        appender = CTX.getListAppender("EventLogger").clear();
+        Logger logger = LogManager.getLogger("EventLogger");
+        logger.info("simple message");
+        logger.warn("complex message", Collections.singletonMap("foo", "bar"));
+        logger.error("my name is: {}", "foo");
+        logger.error("here is a map: {}. ok?", Collections.singletonMap(2, 5));
+        logger.warn("ignored params {}", 4, 6);
+        List<String> messages = appender.getMessages();
+        assertEquals(5, messages.size());
+        assertEquals("[INFO][EventLogger] simple message", messages.get(0));
+        assertEquals("[WARN][EventLogger] complex message {foo=bar}", messages.get(1));
+        assertEquals("[ERROR][EventLogger] my name is: foo", messages.get(2));
+        assertEquals("[ERROR][EventLogger] here is a map: {}. ok? {2=5}", messages.get(3));
+        assertEquals("[WARN][EventLogger] ignored params 4", messages.get(4));
+    }
+
+    @Test
+    @SuppressWarnings("unchecked")
+    public void testJSONLayout() throws Exception {
+        appender = CTX.getListAppender("JSONEventLogger").clear();
+        Logger logger = LogManager.getLogger("JSONEventLogger");
+        logger.info("simple message");
+        logger.warn("complex message", Collections.singletonMap("foo", "bar"));
+        logger.error("my name is: {}", "foo");
+        logger.error("here is a map: {}", Collections.singletonMap(2, 5));
+        logger.warn("ignored params {}", 4, 6, 8);
+
+        List<String> messages = appender.getMessages();
+
+        Map<String, Object> firstMessage = mapper.readValue(messages.get(0), Map.class);
+
+        assertEquals(5, firstMessage.size());
+        assertEquals("INFO", firstMessage.get("level"));
+        assertEquals("JSONEventLogger", firstMessage.get("loggerName"));
+        assertNotNull(firstMessage.get("thread"));
+        assertEquals(Collections.singletonMap("message", "simple message"), firstMessage.get("logEvent"));
+
+        Map<String, Object> secondMessage = mapper.readValue(messages.get(1), Map.class);
+
+        assertEquals(5, secondMessage.size());
+        assertEquals("WARN", secondMessage.get("level"));
+        assertEquals("JSONEventLogger", secondMessage.get("loggerName"));
+        assertNotNull(secondMessage.get("thread"));
+        Map<String, Object> logEvent = new HashMap<>();
+        logEvent.put("message", "complex message");
+        logEvent.put("foo", "bar");
+        assertEquals(logEvent, secondMessage.get("logEvent"));
+
+        Map<String, Object> thirdMessage = mapper.readValue(messages.get(2), Map.class);
+        assertEquals(5, thirdMessage.size());
+        logEvent = Collections.singletonMap("message", "my name is: foo");
+        assertEquals(logEvent, thirdMessage.get("logEvent"));
+
+        Map<String, Object> fourthMessage = mapper.readValue(messages.get(3), Map.class);
+        assertEquals(5, fourthMessage.size());
+        logEvent = new HashMap<>();
+        logEvent.put("message", "here is a map: {}");
+        logEvent.put("2", 5);
+        assertEquals(logEvent, fourthMessage.get("logEvent"));
+
+        Map<String, Object> fifthMessage = mapper.readValue(messages.get(4), Map.class);
+        assertEquals(5, fifthMessage.size());
+        logEvent = Collections.singletonMap("message", "ignored params 4");
+        assertEquals(logEvent, fifthMessage.get("logEvent"));
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/stress/Concurrent.java b/logstash-core/src/test/java/org/logstash/stress/Concurrent.java
new file mode 100644
index 00000000000..68c0d5168bd
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/stress/Concurrent.java
@@ -0,0 +1,189 @@
+package org.logstash.stress;
+
+import org.logstash.ackedqueue.*;
+import org.logstash.ackedqueue.io.ByteBufferPageIO;
+import org.logstash.ackedqueue.io.CheckpointIOFactory;
+import org.logstash.ackedqueue.io.FileCheckpointIO;
+import org.logstash.ackedqueue.io.MemoryCheckpointIO;
+import org.logstash.ackedqueue.io.MmapPageIO;
+import org.logstash.ackedqueue.io.PageIOFactory;
+
+import java.io.IOException;
+import java.time.Duration;
+import java.time.Instant;
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Collections;
+import java.util.List;
+import java.util.concurrent.ConcurrentLinkedQueue;
+import java.util.stream.Collectors;
+
+public class Concurrent {
+    final static int ELEMENT_COUNT = 2000000;
+    final static int BATCH_SIZE = 1000;
+    static Settings settings;
+
+    public static Settings memorySettings(int capacity) {
+        Settings s = new MemorySettings();
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+
+    public static Settings fileSettings(int capacity) {
+        Settings s = new MemorySettings("/tmp/queue");
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
+        CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
+        s.setCapacity(capacity);
+        s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointIOFactory(checkpointIOFactory);
+        s.setElementClass(StringElement.class);
+        return s;
+    }
+
+    public static Thread producer(Queue q, List<StringElement> input) {
+        return new Thread(() -> {
+            try {
+                for (StringElement element : input) {
+                    q.write(element);
+                }
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        });
+
+    }
+
+    public static void oneProducersOneConsumer() throws IOException, InterruptedException {
+        List<StringElement> input = new ArrayList<>();
+        List<StringElement> output = new ArrayList<>();
+
+        Instant start = Instant.now();
+
+        Queue q = new Queue(settings);
+        q.getCheckpointIO().purge();
+        q.open();
+
+        System.out.print("stating single producers and single consumers stress test... ");
+
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            input.add(new StringElement(new Integer(i).toString()));
+        }
+
+        Thread consumer = new Thread(() -> {
+            int consumedCount = 0;
+
+            try {
+                while (consumedCount < ELEMENT_COUNT) {
+                    Batch b = q.readBatch(BATCH_SIZE);
+//                    if (b.getElements().size() < BATCH_SIZE) {
+//                        System.out.println("read small batch=" + b.getElements().size());
+//                    } else {
+//                        System.out.println("read batch size=" + b.getElements().size());
+//                    }
+                    output.addAll((List<StringElement>) b.getElements());
+                    b.close();
+                    consumedCount += b.getElements().size();
+                }
+            } catch (IOException e) {
+                throw new RuntimeException(e);
+            }
+        });
+        consumer.start();
+
+        Thread producer = producer(q, input);
+        producer.start();
+
+        consumer.join();
+        q.close();
+
+        Instant end = Instant.now();
+
+        if (! input.equals(output)) {
+            System.out.println("ERROR: input and output are not equal");
+        } else {
+            System.out.println("SUCCESS, result size=" + output.size() + ", elapsed=" + Duration.between(start, end) + ", rate=" + (new Float(ELEMENT_COUNT) / Duration.between(start, end).toMillis()) * 1000);
+        }
+    }
+
+    public static void oneProducersOneMultipleConsumer() throws IOException, InterruptedException {
+        final List<StringElement> input = new ArrayList<>();
+        final Collection<StringElement> output = new ConcurrentLinkedQueue();
+        final int CONSUMERS = 5;
+        List<Thread> consumers = new ArrayList<>();
+
+        Instant start = Instant.now();
+
+        Queue q = new Queue(settings);
+        q.getCheckpointIO().purge();
+        q.open();
+
+        System.out.print("stating single producers and multiple consumers stress test... ");
+
+        for (int i = 0; i < ELEMENT_COUNT; i++) {
+            input.add(new StringElement(new Integer(i).toString()));
+        }
+
+        for (int i = 0; i < CONSUMERS; i++) {
+            consumers.add(new Thread(() -> {
+                try {
+                    while (output.size() < ELEMENT_COUNT) {
+                        Batch b = q.readBatch(BATCH_SIZE);
+//                        if (b.getElements().size() < BATCH_SIZE) {
+//                            System.out.println("read small batch=" + b.getElements().size());
+//                        } else {
+//                            System.out.println("read batch size=" + b.getElements().size());
+//                        }
+                        output.addAll((List<StringElement>) b.getElements());
+                        b.close();
+                    }
+                    // everything is read, close queue here since other consumers might be blocked trying to get next batch
+                    q.close();
+                } catch (IOException e) {
+                    throw new RuntimeException(e);
+                }
+            }));
+        }
+
+        consumers.forEach(c -> c.start());
+
+        Thread producer = producer(q, input);
+        producer.start();
+
+        // gotta hate exception handling in lambdas
+        consumers.forEach(c -> {try{c.join();} catch(InterruptedException e) {throw new RuntimeException(e);}});
+        q.close();
+
+        Instant end = Instant.now();
+
+        List<StringElement> result = output.stream().collect(Collectors.toList());
+        Collections.sort(result, (p1, p2) -> Integer.valueOf(p1.toString()).compareTo(Integer.valueOf(p2.toString())));
+
+        if (! input.equals(result)) {
+            System.out.println("ERROR: input and output are not equal");
+        } else {
+            System.out.println("SUCCESS, result size=" + output.size() + ", elapsed=" + Duration.between(start, end) + ", rate=" + (new Float(ELEMENT_COUNT) / Duration.between(start, end).toMillis()) * 1000);
+        }
+    }
+
+
+    public static void main(String[] args) throws IOException, InterruptedException {
+        System.out.println(">>> starting in-memory stress test");
+
+        settings = memorySettings(1024 * 1024); // 1MB
+        oneProducersOneConsumer();
+        oneProducersOneMultipleConsumer();
+
+        System.out.println("\n>>> starting file-based stress test in /tmp/queue");
+
+        settings = fileSettings(1024 * 1024); // 1MB
+
+        oneProducersOneConsumer();
+        oneProducersOneMultipleConsumer();
+    }
+
+}
diff --git a/logstash-core/src/test/resources/log4j-list.properties b/logstash-core/src/test/resources/log4j-list.properties
new file mode 100644
index 00000000000..435d6042ea0
--- /dev/null
+++ b/logstash-core/src/test/resources/log4j-list.properties
@@ -0,0 +1,11 @@
+status = error
+name = LogstashPropertiesConfig
+
+appender.list.type = List
+appender.list.name = List
+#appender.list.layout.type = JSONLayout
+#appender.list.layout.compact = true
+#appender.list.layout.eventEol = true
+
+rootLogger.level = info
+rootLogger.appenderRef.stdout.ref = List
diff --git a/logstash-core/src/test/resources/log4j2-test1.xml b/logstash-core/src/test/resources/log4j2-test1.xml
new file mode 100644
index 00000000000..ad81ddde21e
--- /dev/null
+++ b/logstash-core/src/test/resources/log4j2-test1.xml
@@ -0,0 +1,31 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<Configuration status="OFF" name="LoggerTest">
+    <properties>
+        <property name="filename">target/test.log</property>
+    </properties>
+    <ThresholdFilter level="trace"/>
+
+    <Appenders>
+        <List name="EventLogger">
+            <PatternLayout pattern="[%p][%c] %m"/>
+        </List>
+        <List name="JSONEventLogger">
+            <JSONLayout compact="true" eventEol="true" />
+        </List>
+    </Appenders>
+
+    <Loggers>
+        <Logger name="EventLogger" level="debug" additivity="false">
+            <AppenderRef ref="EventLogger"/>
+        </Logger>>
+
+        <Logger name="JSONEventLogger" level="debug" additivity="false">
+            <AppenderRef ref="JSONEventLogger"/>
+        </Logger>>
+
+        <Root level="trace">
+            <AppenderRef ref="EventLogger"/>
+        </Root>
+    </Loggers>
+
+</Configuration>
\ No newline at end of file
diff --git a/logstash-core/src/test/resources/org/logstash/ackedqueue/io/checkpoint.head b/logstash-core/src/test/resources/org/logstash/ackedqueue/io/checkpoint.head
new file mode 100644
index 00000000000..d189f0a7fde
Binary files /dev/null and b/logstash-core/src/test/resources/org/logstash/ackedqueue/io/checkpoint.head differ
diff --git a/pkg/centos/after-install.sh b/pkg/centos/after-install.sh
index 224904e4b32..ac226fd84b2 100644
--- a/pkg/centos/after-install.sh
+++ b/pkg/centos/after-install.sh
@@ -1,6 +1,9 @@
-/sbin/chkconfig --add logstash
-
-chown -R logstash:logstash /opt/logstash
-chown logstash /var/log/logstash
+chown -R logstash:logstash /usr/share/logstash
+chown -R logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
-chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.logs:|path.logs: /var/log/logstash|' \
+  -e 's|# path.data:|path.data: /var/lib/logstash|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/centos/before-install.sh b/pkg/centos/before-install.sh
index 5a852488ff3..78fc0b77d49 100644
--- a/pkg/centos/before-install.sh
+++ b/pkg/centos/before-install.sh
@@ -5,6 +5,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -r -g logstash -d /opt/logstash \
+  useradd -r -g logstash -d /usr/share/logstash \
     -s /sbin/nologin -c "logstash" logstash
 fi
diff --git a/pkg/centos/before-remove.sh b/pkg/centos/before-remove.sh
index 5109888475f..6687ee896e5 100644
--- a/pkg/centos/before-remove.sh
+++ b/pkg/centos/before-remove.sh
@@ -1,6 +1,32 @@
+# CentOS/RHEL and SuSE
 if [ $1 -eq 0 ]; then
-  /sbin/service logstash stop >/dev/null 2>&1 || true
-  /sbin/chkconfig --del logstash
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
+
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
diff --git a/pkg/debian/after-install.sh b/pkg/debian/after-install.sh
index 18b4ea8c32c..8a2f0767997 100644
--- a/pkg/debian/after-install.sh
+++ b/pkg/debian/after-install.sh
@@ -1,7 +1,12 @@
 #!/bin/sh
 
-chown -R logstash:logstash /opt/logstash
-chown logstash /var/log/logstash
+chown -R logstash:logstash /usr/share/logstash
+chown -R logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
 chmod 755 /etc/logstash
-chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.logs:|path.logs: /var/log/logstash|' \
+  -e 's|# path.data:|path.data: /var/lib/logstash|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/debian/before-install.sh b/pkg/debian/before-install.sh
index 45ef4e40f1f..03cf86125a9 100644
--- a/pkg/debian/before-install.sh
+++ b/pkg/debian/before-install.sh
@@ -7,6 +7,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -M -r -g logstash -d /var/lib/logstash \
+  useradd -M -r -g logstash -d /usr/share/logstash \
     -s /usr/sbin/nologin -c "LogStash Service User" logstash
 fi
diff --git a/pkg/debian/before-remove.sh b/pkg/debian/before-remove.sh
index a3f911e60ea..16347f266fc 100644
--- a/pkg/debian/before-remove.sh
+++ b/pkg/debian/before-remove.sh
@@ -1,13 +1,38 @@
 #!/bin/sh
-
+# Debian
 if [ $1 = "remove" ]; then
-  service logstash stop >/dev/null 2>&1 || true
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /usr/sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
 
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
 
-  if getent group logstash >/dev/null ; then
+  if getent group logstash > /dev/null ; then
     groupdel logstash
   fi
 fi
diff --git a/pkg/jvm.options b/pkg/jvm.options
new file mode 100644
index 00000000000..2568d6d4f5a
--- /dev/null
+++ b/pkg/jvm.options
@@ -0,0 +1,74 @@
+## JVM configuration
+
+# Xms represents the initial size of total heap space
+# Xmx represents the maximum size of total heap space
+
+-Xms256m
+-Xmx1g
+
+################################################################
+## Expert settings
+################################################################
+##
+## All settings below this section are considered
+## expert settings. Don't tamper with them unless
+## you understand what you are doing
+##
+################################################################
+
+## GC configuration
+-XX:+UseParNewGC
+-XX:+UseConcMarkSweepGC
+-XX:CMSInitiatingOccupancyFraction=75
+-XX:+UseCMSInitiatingOccupancyOnly
+
+## optimizations
+
+# disable calls to System#gc
+-XX:+DisableExplicitGC
+
+## locale
+# Set the locale language
+#-Duser.language=en
+
+# Set the locale country
+#-Duser.country=US
+
+# Set the locale variant, if any
+#-Duser.variant=
+
+## basic
+
+# set the I/O temp directory
+#-Djava.io.tmpdir=$HOME
+
+# set to headless, just in case
+-Djava.awt.headless=true
+
+# ensure UTF-8 encoding by default (e.g. filenames)
+-Dfile.encoding=UTF-8
+
+# use our provided JNA always versus the system one
+#-Djna.nosys=true
+
+## heap dumps
+
+# generate a heap dump when an allocation from the Java heap fails
+# heap dumps are created in the working directory of the JVM
+-XX:+HeapDumpOnOutOfMemoryError
+
+# specify an alternative path for heap dumps
+# ensure the directory exists and has sufficient space
+#-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof
+
+## GC logging
+#-XX:+PrintGCDetails
+#-XX:+PrintGCTimeStamps
+#-XX:+PrintGCDateStamps
+#-XX:+PrintClassHistogram
+#-XX:+PrintTenuringDistribution
+#-XX:+PrintGCApplicationStoppedTime
+
+# log GC status to a file with time stamps
+# ensure the directory exists
+#-Xloggc:${LS_GC_LOG_FILE}
diff --git a/pkg/log4j2.properties b/pkg/log4j2.properties
new file mode 100644
index 00000000000..2c3c75a9e75
--- /dev/null
+++ b/pkg/log4j2.properties
@@ -0,0 +1,28 @@
+status = error
+name = LogstashPropertiesConfig
+
+appender.rolling.type = RollingFile
+appender.rolling.name = plain_rolling
+appender.rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.rolling.policies.type = Policies
+appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.rolling.policies.time.interval = 1
+appender.rolling.policies.time.modulate = true
+appender.rolling.layout.type = PatternLayout
+appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %-.10000m%n
+
+appender.json_rolling.type = RollingFile
+appender.json_rolling.name = json_rolling
+appender.json_rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.json_rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.json_rolling.policies.type = Policies
+appender.json_rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.json_rolling.policies.time.interval = 1
+appender.json_rolling.policies.time.modulate = true
+appender.json_rolling.layout.type = JSONLayout
+appender.json_rolling.layout.compact = true
+appender.json_rolling.layout.eventEol = true
+
+rootLogger.level = ${sys:ls.log.level}
+rootLogger.appenderRef.rolling.ref = ${sys:ls.log.format}_rolling
diff --git a/pkg/logstash.default b/pkg/logstash.default
deleted file mode 100644
index c3415146761..00000000000
--- a/pkg/logstash.default
+++ /dev/null
@@ -1,41 +0,0 @@
-###############################
-# Default settings for logstash
-###############################
-
-# Override Java location
-#JAVACMD=/usr/bin/java
-
-# Set a home directory
-#LS_HOME=/var/lib/logstash
-
-# Arguments to pass to logstash agent
-#LS_OPTS=""
-
-# Arguments to pass to java
-#LS_HEAP_SIZE="1g"
-#LS_JAVA_OPTS="-Djava.io.tmpdir=$HOME"
-
-# pidfiles aren't used for upstart; this is for sysv users.
-#LS_PIDFILE=/var/run/logstash.pid
-
-# user id to be invoked as; for upstart: edit /etc/init/logstash.conf
-#LS_USER=logstash
-
-# logstash logging
-#LS_LOG_FILE=/var/log/logstash/logstash.log
-#LS_USE_GC_LOGGING="true"
-#LS_GC_LOG_FILE=/var/log/logstash/gc.log
-
-# logstash configuration directory
-#LS_CONF_DIR=/etc/logstash/conf.d
-
-# Open file limit; cannot be overridden in upstart
-#LS_OPEN_FILES=16384
-
-# Nice level
-#LS_NICE=19
-
-# If this is set to 1, then when `stop` is called, if the process has
-# not exited within a reasonable time, SIGKILL will be sent next.
-# The default behavior is to simply log a message "program stop failed; still running"
-KILL_ON_STOP_TIMEOUT=0
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
deleted file mode 100755
index d971b4a405c..00000000000
--- a/pkg/logstash.sysv
+++ /dev/null
@@ -1,198 +0,0 @@
-#!/bin/sh
-# Init script for logstash
-# Maintained by Elasticsearch
-# Generated by pleaserun.
-# Implemented based on LSB Core 3.1:
-#   * Sections: 20.2, 20.3
-#
-### BEGIN INIT INFO
-# Provides:          logstash
-# Required-Start:    $remote_fs $syslog
-# Required-Stop:     $remote_fs $syslog
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description:
-# Description:        Starts Logstash as a daemon.
-### END INIT INFO
-
-PATH=/sbin:/usr/sbin:/bin:/usr/bin
-export PATH
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-name=logstash
-pidfile="/var/run/$name.pid"
-
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="1g"
-LS_LOG_DIR=/var/log/logstash
-LS_LOG_FILE="${LS_LOG_DIR}/$name.log"
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-
-
-[ -r /etc/default/$name ] && . /etc/default/$name
-[ -r /etc/sysconfig/$name ] && . /etc/sysconfig/$name
-
-program=/opt/logstash/bin/logstash
-args="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-quiet() {
-  "$@" > /dev/null 2>&1
-  return $?
-}
-
-start() {
-
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  HOME=${LS_HOME}
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-
-  # chown doesn't grab the suplimental groups when setting the user:group - so we have to do it for it.
-  # Boy, I hope we're root here.
-  SGROUPS=$(id -Gn "$LS_USER" | tr " " "," | sed 's/,$//'; echo '')
-
-  if [ ! -z $SGROUPS ]
-  then
-	EXTRA_GROUPS="--groups $SGROUPS"
-  fi
-
-  # set ulimit as (root, presumably) first, before we drop privileges
-  ulimit -n ${LS_OPEN_FILES}
-
-  # Run the program!
-  nice -n ${LS_NICE} chroot --userspec $LS_USER:$LS_GROUP $EXTRA_GROUPS / sh -c "
-    cd $LS_HOME
-    ulimit -n ${LS_OPEN_FILES}
-    exec \"$program\" $args
-  " > "${LS_LOG_DIR}/$name.stdout" 2> "${LS_LOG_DIR}/$name.err" &
-
-  # Generate the pidfile from here. If we instead made the forked process
-  # generate it there will be a race condition between the pidfile writing
-  # and a process possibly asking for status.
-  echo $! > $pidfile
-
-  echo "$name started."
-  return 0
-}
-
-stop() {
-  # Try a few times to kill TERM the program
-  if status ; then
-    pid=`cat "$pidfile"`
-    echo "Killing $name (pid $pid) with SIGTERM"
-    kill -TERM $pid
-    # Wait for it to exit.
-    for i in 1 2 3 4 5 6 7 8 9 ; do
-      echo "Waiting $name (pid $pid) to die..."
-      status || break
-      sleep 1
-    done
-    if status ; then
-      if [[ $KILL_ON_STOP_TIMEOUT -eq 1 ]] ; then
-        echo "Timeout reached. Killing $name (pid $pid) with SIGKILL. This may result in data loss."
-        kill -KILL $pid
-        echo "$name killed with SIGKILL."
-      else
-        echo "$name stop failed; still running."
-      fi
-    else
-      echo "$name stopped."
-    fi
-  fi
-}
-
-status() {
-  if [ -f "$pidfile" ] ; then
-    pid=`cat "$pidfile"`
-    if kill -0 $pid > /dev/null 2> /dev/null ; then
-      # process by this pid is running.
-      # It may not be our pid, but that's what you get with just pidfiles.
-      # TODO(sissel): Check if this process seems to be the same as the one we
-      # expect. It'd be nice to use flock here, but flock uses fork, not exec,
-      # so it makes it quite awkward to use in this case.
-      return 0
-    else
-      return 2 # program is dead but pid file exists
-    fi
-  else
-    return 3 # program is not running
-  fi
-}
-
-force_stop() {
-  if status ; then
-    stop
-    status && kill -KILL `cat "$pidfile"`
-  fi
-}
-
-configtest() {
-  # Check if a config file exists
-  if [ ! "$(ls -A ${LS_CONF_DIR}/* 2> /dev/null)" ]; then
-    echo "There aren't any configuration files in ${LS_CONF_DIR}"
-    return 1
-  fi
-
-  HOME=${LS_HOME}
-  export PATH HOME
-
-  test_args="--configtest -f ${LS_CONF_DIR} ${LS_OPTS}"
-  $program ${test_args}
-  [ $? -eq 0 ] && return 0
-  # Program not configured
-  return 6
-}
-
-case "$1" in
-  start)
-    status
-    code=$?
-    if [ $code -eq 0 ]; then
-      echo "$name is already running"
-    else
-      start
-      code=$?
-    fi
-    exit $code
-    ;;
-  stop) stop ;;
-  force-stop) force_stop ;;
-  status)
-    status
-    code=$?
-    if [ $code -eq 0 ] ; then
-      echo "$name is running"
-    else
-      echo "$name is not running"
-    fi
-    exit $code
-    ;;
-  restart)
-
-    quiet configtest
-    RET=$?
-    if [ ${RET} -ne 0 ]; then
-      echo "Configuration error. Not restarting. Re-run with configtest parameter for details"
-      exit ${RET}
-    fi
-    stop && start
-    ;;
-  configtest)
-    configtest
-    exit $?
-    ;;
-  *)
-    echo "Usage: $SCRIPTNAME {start|stop|force-stop|status|restart|configtest}" >&2
-    exit 3
-  ;;
-esac
-
-exit $?
diff --git a/pkg/logstash.sysv.debian b/pkg/logstash.sysv.debian
deleted file mode 100644
index f83c468d81a..00000000000
--- a/pkg/logstash.sysv.debian
+++ /dev/null
@@ -1,181 +0,0 @@
-#!/bin/bash
-#
-# /etc/init.d/logstash -- startup script for LogStash.
-#
-### BEGIN INIT INFO
-# Provides:          logstash
-# Required-Start:    $all
-# Required-Stop:     $all
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description: Starts logstash
-# Description:       Starts logstash using start-stop-daemon
-### END INIT INFO
-
-set -e
-
-NAME=logstash
-DESC="Logstash Daemon"
-DEFAULT=/etc/default/$NAME
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-. /lib/lsb/init-functions
-
-if [ -r /etc/default/rcS ]; then
-   . /etc/default/rcS
-fi
-
-# The following variables can be overwritten in $DEFAULT
-PATH=/bin:/usr/bin:/sbin:/usr/sbin
-
-# See contents of file named in $DEFAULT for comments
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="1g"
-LS_LOG_FILE=/var/log/logstash/$NAME.log
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-LS_PIDFILE=/var/run/$NAME.pid
-
-# End of variables that can be overwritten in $DEFAULT
-
-# overwrite settings from default file
-if [ -f "$DEFAULT" ]; then
-   . "$DEFAULT"
-fi
-
-# Define other required variables
-PID_FILE=${LS_PIDFILE}
-DAEMON=/opt/logstash/bin/logstash
-DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-# Check DAEMON exists
-if ! test -e $DAEMON; then
-   log_failure_msg "Script $DAEMON doesn't exist"
-   exit 1
-fi
-
-case "$1" in
-   start)
-      if [ -z "$DAEMON" ]; then
-         log_failure_msg "no logstash script found - $DAEMON"
-         exit 1
-      fi
-
-      # Check if a config file exists
-      if [ ! "$(ls -A $LS_CONF_DIR/*.conf 2> /dev/null)" ]; then
-         log_failure_msg "There aren't any configuration files in $LS_CONF_DIR"
-         exit 1
-      fi
-
-      log_daemon_msg "Starting $DESC"
-
-      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-      if start-stop-daemon --test --start --pidfile "$PID_FILE" \
-         --user "$LS_USER" --exec "$JAVA" \
-      >/dev/null; then
-         # Prepare environment
-         HOME="${HOME:-$LS_HOME}"
-         LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-         ulimit -n ${LS_OPEN_FILES}
-	 cd "${LS_HOME}"
-         export PATH HOME JAVACMD LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-
-         # Start Daemon
-         start-stop-daemon --start -b --user "$LS_USER" -c "$LS_USER":"$LS_GROUP" \
-           -d "$LS_HOME" --nicelevel "$LS_NICE" --pidfile "$PID_FILE" --make-pidfile \
-           --exec $DAEMON -- $DAEMON_OPTS
-
-         sleep 1
-
-         # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-         JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-         if start-stop-daemon --test --start --pidfile "$PID_FILE" \
-             --user "$LS_USER" --exec "$JAVA" \
-         >/dev/null; then
-
-            if [ -f "$PID_FILE" ]; then
-               rm -f "$PID_FILE"
-            fi
-
-            log_end_msg 1
-         else
-            log_end_msg 0
-         fi
-      else
-         log_progress_msg "(already running)"
-         log_end_msg 0
-      fi
-   ;;
-   stop)
-      log_daemon_msg "Stopping $DESC"
-
-      set +e
-
-      if [ -f "$PID_FILE" ]; then
-         start-stop-daemon --stop --pidfile "$PID_FILE" \
-            --user "$LS_USER" \
-            --retry=TERM/20/KILL/5 >/dev/null
-
-         if [ $? -eq 1 ]; then
-            log_progress_msg "$DESC is not running but pid file exists, cleaning up"
-         elif [ $? -eq 3 ]; then
-            PID="`cat $PID_FILE`"
-            log_failure_msg "Failed to stop $DESC (pid $PID)"
-            exit 1
-         fi
-
-         rm -f "$PID_FILE"
-      else
-         log_progress_msg "(not running)"
-      fi
-
-      log_end_msg 0
-      set -e
-   ;;
-   status)
-      set +e
-
-      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-      start-stop-daemon --test --start --pidfile "$PID_FILE" \
-         --user "$LS_USER" --exec "$JAVA" \
-      >/dev/null 2>&1
-
-      if [ "$?" = "0" ]; then
-         if [ -f "$PID_FILE" ]; then
-            log_success_msg "$DESC is not running, but pid file exists."
-            exit 1
-         else
-            log_success_msg "$DESC is not running."
-            exit 3
-         fi
-      else
-         log_success_msg "$DESC is running with pid `cat $PID_FILE`"
-      fi
-
-      set -e
-   ;;
-   restart|force-reload)
-      if [ -f "$PID_FILE" ]; then
-         $0 stop
-         sleep 1
-      fi
-
-      $0 start
-   ;;
-   *)
-      log_success_msg "Usage: $0 {start|stop|restart|force-reload|status}"
-      exit 1
-   ;;
-esac
-
-exit 0
diff --git a/pkg/logstash.sysv.redhat b/pkg/logstash.sysv.redhat
deleted file mode 100755
index 07f606e8d41..00000000000
--- a/pkg/logstash.sysv.redhat
+++ /dev/null
@@ -1,132 +0,0 @@
-#! /bin/sh
-#
-#       /etc/rc.d/init.d/logstash
-#
-#       Starts Logstash as a daemon
-#
-# chkconfig: 2345 90 10
-# description: Starts Logstash as a daemon.
-
-### BEGIN INIT INFO
-# Provides: logstash
-# Required-Start: $local_fs $remote_fs
-# Required-Stop: $local_fs $remote_fs
-# Default-Start: 2 3 4 5
-# Default-Stop: S 0 1 6
-# Short-Description: Logstash
-# Description: Starts Logstash as a daemon.
-### END INIT INFO
-
-. /etc/rc.d/init.d/functions
-
-NAME=logstash
-DESC="Logstash Daemon"
-DEFAULT=/etc/sysconfig/$NAME
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-# The following variables can be overwritten in $DEFAULT
-PATH=/bin:/usr/bin:/sbin:/usr/sbin
-
-# See contents of file named in $DEFAULT for comments
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="1g"
-LS_LOG_FILE=/var/log/logstash/$NAME.log
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-LS_PIDFILE=/var/run/$NAME/$NAME.pid
-
-# End of variables that can be overwritten in $DEFAULT
-
-if [ -f "$DEFAULT" ]; then
-  . "$DEFAULT"
-fi
-
-# Define other required variables
-PID_FILE=${LS_PIDFILE}
-
-DAEMON="/opt/logstash/bin/logstash"
-DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-#
-# Function that starts the daemon/service
-#
-do_start()
-{
-
-  if [ -z "$DAEMON" ]; then
-    echo "not found - $DAEMON"
-    exit 1
-  fi
-
-  if pidofproc -p "$PID_FILE" >/dev/null; then
-    exit 0
-  fi
-
-  # Prepare environment
-  HOME="${HOME:-$LS_HOME}"
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  ulimit -n ${LS_OPEN_FILES}
-  cd "${LS_HOME}"
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-  test -n "${JAVACMD}" && export JAVACMD
-
-  nice -n ${LS_NICE} runuser -s /bin/sh -c "exec $DAEMON $DAEMON_OPTS" ${LS_USER} >> $LS_LOG_FILE 2>&1 < /dev/null &
-
-  RETVAL=$?
-  local PID=$!
-  # runuser forks rather than execing our process.
-  usleep 500000
-  JAVA_PID=$(ps axo ppid,pid | awk -v "ppid=$PID" '$1==ppid {print $2}')
-  PID=${JAVA_PID:-$PID}
-  echo $PID > $PID_FILE
-  [ "$PID" = "$JAVA_PID" ] && success
-}
-
-#
-# Function that stops the daemon/service
-#
-do_stop()
-{
-    killproc -p $PID_FILE $DAEMON
-    RETVAL=$?
-    echo
-    [ $RETVAL = 0 ] && rm -f ${PID_FILE}
-}
-
-case "$1" in
-  start)
-    echo -n "Starting $DESC: "
-    do_start
-    touch /var/run/logstash/$NAME
-    ;;
-  stop)
-    echo -n "Stopping $DESC: "
-    do_stop
-    rm /var/run/logstash/$NAME
-    ;;
-  restart|reload)
-    echo -n "Restarting $DESC: "
-    do_stop
-    do_start
-    ;;
-  status)
-    echo -n "$DESC"
-    status -p $PID_FILE
-    exit $?
-    ;;
-  *)
-    echo "Usage: $SCRIPTNAME {start|stop|status|restart}" >&2
-    exit 3
-    ;;
-esac
-
-echo
-exit 0
diff --git a/pkg/logstash.upstart.ubuntu b/pkg/logstash.upstart.ubuntu
deleted file mode 100644
index 482c53d7bf3..00000000000
--- a/pkg/logstash.upstart.ubuntu
+++ /dev/null
@@ -1,48 +0,0 @@
-# logstash - agent instance
-#
-
-description     "logstash agent"
-
-start on virtual-filesystems
-stop on runlevel [06]
-
-# Respawn it if the process exits
-respawn
-
-# We're setting high here, we'll re-limit below.
-limit nofile 65550 65550
-
-setuid logstash
-setgid logstash
-
-# You need to chdir somewhere writable because logstash needs to unpack a few
-# temporary files on startup.
-console log
-script
-  # Defaults
-  PATH=/bin:/usr/bin
-  LS_HOME=/var/lib/logstash
-  LS_HEAP_SIZE="1g"
-  LS_LOG_FILE=/var/log/logstash/logstash.log
-  LS_USE_GC_LOGGING=""
-  LS_GC_LOG_FILE=""
-  LS_CONF_DIR=/etc/logstash/conf.d
-  LS_OPEN_FILES=16384
-  LS_NICE=19
-  LS_OPTS=""
-
-  # Override our defaults with user defaults:
-  [ -f /etc/default/logstash ] && . /etc/default/logstash
-
-  HOME="${HOME:-$LS_HOME}"
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  # Reset filehandle limit
-  ulimit -n ${LS_OPEN_FILES}
-  cd "${LS_HOME}"
-
-  # Export variables
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-  test -n "${JAVACMD}" && export JAVACMD
-
-  exec nice -n ${LS_NICE} /opt/logstash/bin/logstash agent -f "${LS_CONF_DIR}" -l "${LS_LOG_FILE}" ${LS_OPTS}
-end script
diff --git a/pkg/startup.options b/pkg/startup.options
new file mode 100644
index 00000000000..dcb850e66df
--- /dev/null
+++ b/pkg/startup.options
@@ -0,0 +1,52 @@
+################################################################################
+# These settings are ONLY used by $LS_HOME/bin/system-install to create a custom
+# startup script for Logstash.  It should automagically use the init system
+# (systemd, upstart, sysv, etc.) that your Linux distribution uses.
+#
+# After changing anything here, you need to re-run $LS_HOME/bin/system-install
+# as root to push the changes to the init script.
+################################################################################
+
+# Override Java location
+JAVACMD=/usr/bin/java
+
+# Set a home directory
+LS_HOME=/usr/share/logstash
+
+# logstash settings directory, the path which contains logstash.yml
+LS_SETTINGS_DIR=/etc/logstash
+
+# Arguments to pass to logstash
+LS_OPTS="--path.settings ${LS_SETTINGS_DIR}"
+
+# Arguments to pass to java
+LS_JAVA_OPTS=""
+
+# pidfiles aren't used the same way for upstart and systemd; this is for sysv users.
+LS_PIDFILE=/var/run/logstash.pid
+
+# user and group id to be invoked as
+LS_USER=logstash
+LS_GROUP=logstash
+
+# Enable GC logging by uncommenting the appropriate lines in the GC logging
+# section in jvm.options
+LS_GC_LOG_FILE=/var/log/logstash/gc.log
+
+# Open file limit
+LS_OPEN_FILES=16384
+
+# Nice level
+LS_NICE=19
+
+# Change these to have the init script named and described differently
+# This is useful when running multiple instances of Logstash on the same
+# physical box or vm
+SERVICE_NAME="logstash"
+SERVICE_DESCRIPTION="logstash"
+
+# If you need to run a command or script before launching Logstash, put it
+# between the lines beginning with `read` and `EOM`, and uncomment those lines.
+###
+## read -r -d '' PRESTART << EOM
+## EOM
diff --git a/pkg/ubuntu/after-install.sh b/pkg/ubuntu/after-install.sh
index bcecadf8af7..8c521d50a59 100644
--- a/pkg/ubuntu/after-install.sh
+++ b/pkg/ubuntu/after-install.sh
@@ -1,6 +1,11 @@
 #!/bin/sh
 
-chown -R logstash:logstash /opt/logstash
-chown logstash /var/log/logstash
+chown -R logstash:logstash /usr/share/logstash
+chown -R logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
-chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.logs:|path.logs: /var/log/logstash|' \
+  -e 's|# path.data:|path.data: /var/lib/logstash|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/ubuntu/before-install.sh b/pkg/ubuntu/before-install.sh
index 45ef4e40f1f..03cf86125a9 100644
--- a/pkg/ubuntu/before-install.sh
+++ b/pkg/ubuntu/before-install.sh
@@ -7,6 +7,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -M -r -g logstash -d /var/lib/logstash \
+  useradd -M -r -g logstash -d /usr/share/logstash \
     -s /usr/sbin/nologin -c "LogStash Service User" logstash
 fi
diff --git a/pkg/ubuntu/before-remove.sh b/pkg/ubuntu/before-remove.sh
index a3f911e60ea..0384e74ffca 100644
--- a/pkg/ubuntu/before-remove.sh
+++ b/pkg/ubuntu/before-remove.sh
@@ -1,13 +1,38 @@
 #!/bin/sh
-
+# Ubuntu
 if [ $1 = "remove" ]; then
-  service logstash stop >/dev/null 2>&1 || true
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /usr/sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
 
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
 
-  if getent group logstash >/dev/null ; then
+  if getent group logstash > /dev/null ; then
     groupdel logstash
   fi
 fi
diff --git a/qa/.gitignore b/qa/.gitignore
new file mode 100644
index 00000000000..2fc713beaf3
--- /dev/null
+++ b/qa/.gitignore
@@ -0,0 +1,4 @@
+Gemfile.lock
+acceptance/.vagrant
+.vagrant
+.vm_ssh_config
diff --git a/qa/.rspec b/qa/.rspec
new file mode 100644
index 00000000000..23c67a21149
--- /dev/null
+++ b/qa/.rspec
@@ -0,0 +1,2 @@
+--format
+documentation
diff --git a/qa/Gemfile b/qa/Gemfile
new file mode 100644
index 00000000000..1919a50f272
--- /dev/null
+++ b/qa/Gemfile
@@ -0,0 +1,5 @@
+source "https://rubygems.org"
+gem "runner-tool", :git => "https://github.com/purbon/runner-tool.git"
+gem "rspec", "~> 3.1.0"
+gem "rake"
+gem "pry", :group => :test
diff --git a/qa/README.md b/qa/README.md
new file mode 100644
index 00000000000..27b37721f8e
--- /dev/null
+++ b/qa/README.md
@@ -0,0 +1,201 @@
+## Acceptance test Framework
+
+Welcome to the acceptance test framework for logstash, in this small
+README we're going to describe it's features and the necessary steps you will need to
+follow to setup your environment.
+
+### Setup your environment
+
+In summary this test framework is composed of:
+
+* A collection of rspec helpers and matchers that make creating tests
+  easy.
+* This rspecs helpers execute commands over SSH to a set of machines.
+* The tests are run, for now, as vagrant (virtualbox provided) machines.
+
+As of this, you need to have installed:
+
+* The latest version vagrant (=> 1.8.1)
+* Virtualbox as VM provider (=> 5.0)
+
+Is important to notice that the first time you set everything up, or when a
+new VM is added, there is the need to download the box (this will
+take a while depending on your internet speed).
+
+### Running Tests
+
+It is possible to run the full suite of the acceptance test with the codebase by 
+running the command `ci/ci_acceptance.sh`, this command will generate the artifacts, bootstrap
+the VM and run the tests.
+
+This test are based on a collection of Vagrant defined VM's where the
+different test are going to be executed, so first setup necessary is to
+have vagrant properly available, see https://www.vagrantup.com/ for
+details on how to install it.
+
+_Inside the `qa` directory_
+
+First of all execute the command `bundle` this will pull the necessary
+dependencies in your environment, after this is done, this is the collection of task available for you:
+
+```
+skywalker% rake -T
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+rake qa:vm:halt[platform]           # Halt all VM's involved in the acceptance test round
+rake qa:vm:setup[platform]          # Bootstrap all the VM's used for this tests
+rake qa:vm:ssh_config               # Generate a valid ssh-config
+```
+
+Important to be aware that using any of this commands:
+
+```
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+```
+
+before you *will have to bootstrap* all selected machines, you can do
+that using the `rake qa:vm:setup[platform]` task. This is done like this
+as bootstrap imply setting up the VM'S and this might take some time and
+you might only want to this once.
+
+In the feature we might add new rake tasks to do all at once, but for now you can use the script under
+`ci/ci_acceptance.sh` to do all at once.
+
+For local testing purposes, is recommended to not run all together, pick your target and run with the single machine command, If you're willing to run on single one, you should use:
+
+```
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+```
+
+### How to run tests
+
+If you are *running this test for first time*, you will need to setup
+your VM's first, you can do that using either `vagrant up` or `rake qa:vm:setup[platform]`. 
+
+In this framework we're using ssh to connect to a collection of Vagrant
+machines, so first and most important is to generate a valid ssh config
+file, this could be done running `rake qa:vm:ssh_config`. When this task
+is finished a file named `.vm_ssh_config` will be generated with all the
+necessary information to connect with the different machines.
+
+Now is time to run your test and to do that we have different options:
+
+* rake qa:acceptance:all              # Run all acceptance
+* rake qa:acceptance:debian           # Run acceptance test in debian machines
+* rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+* rake qa:acceptance:suse             # Run acceptance test in suse machines
+* rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+
+Generally speaking this are complex tests so they take a long time to
+finish completely, if you look for faster feedback see at the end of this
+README how to run fewer tests.
+
+## Architecture of the Framework
+
+If you wanna know more about how this framework works, here is your
+section of information.
+
+### Directory structure
+
+* ```acceptance/``` here it goes all the specs definitions.
+* ```config```  inside you can find all config files, for now only the
+  platform definition.
+* ```rspec``` here stay all framework parts necessary to get the test
+  running, you will find the commands, the rspec matchers and a
+collection of useful helpers for your test.
+* ```sys``` a collection of bash scripts used to bootstrap the machines.
+* ```vagrant``` classes and modules used to help us running vagrant.
+
+### The platform configuration file
+
+Located inside the config directory there is the platforms.json which is used to define the different platforms we test with.
+Important bits here are:
+
+* `latest` key defines the latest published version of LS release which is used to test the package upgrade scenario.
+* inside the `platforms` key you will find the list of current available
+  OS we tests with, this include the box name, their type and if they
+have to go under specific bootstrap scripts (see ```specific: true ```
+in the platform definition).
+
+This file is the one that you will use to know about differnt OS's
+testes, add new ones, etc..
+
+### I want to add a test, what should I do?
+
+To add a test you basically should start by the acceptance directory,
+here you will find an already created tests, most important locations
+here are:
+
+* ```lib``` here is where the tests are living. If a test is not going
+  to be reused it should be created here.
+* ```shared_examples``` inside that directory should be living all tests
+  that could be reused in different scenarios, like you can see the CLI
+ones.
+
+but we want to write tests, here is an example of how do they look like,
+including the different moving parts we encounter in the framework.
+
+
+```
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    ##
+    # ServiceTester::Artifact is the component used to interact with the
+    # destination machineri and the one that keep the necessary logic
+    # for it.
+    ##
+
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+
+    ## your test code goes here.
+  end
+```
+
+this is important because as you know we test with different machines,
+so the build out artifact will be the component necessary to run the
+actions with the destination machine.
+
+but this is the main parts, to run your test you need the framework
+located inside the ```rspec``` directory. Here you will find a
+collection of commands, properly organized per operating system, that
+will let you operate and get your tests done. But don't freak out, we
+got all logic necessary to select the right one for your test.
+
+You'll probably find enough supporting classes for different platforms, but if not, feel free to add it.
+
+FYI, this is how a command looks like:
+
+```
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+  end
+  ```
+this is how we run operations and wrap them as ruby code.
+
+### Running a test (detailed level)
+
+There is also the possibility to run your tests with more granularity by
+using the `rspec` command, this will let you for example run a single
+tests, a collection of them using filtering, etc.
+
+Check https://relishapp.com/rspec/rspec-core/v/3-4/docs/command-line for more details, but here is a quick cheat sheet to run them:
+
+# Run the examples that get "is installed" in their description
+
+*  bundle exec rspec acceptance/spec -e "is installed" 
+
+# Run the example defined at line 11
+
+*  bundle exec rspec acceptance/spec/lib/artifact_operation_spec.rb:11
diff --git a/qa/Rakefile b/qa/Rakefile
new file mode 100644
index 00000000000..589a230c641
--- /dev/null
+++ b/qa/Rakefile
@@ -0,0 +1,76 @@
+require "rspec"
+require "rspec/core/runner"
+require "rspec/core/rake_task"
+require_relative "vagrant/helpers"
+require_relative "platform_config"
+
+platforms = PlatformConfig.new
+
+task :spec    => 'spec:all'
+task :default => :spec
+
+namespace :qa do
+
+  namespace :vm do
+
+    def user_feedback_string_for(action, platform, machines, options={})
+      experimental_string = options["experimental"] ? "experimental" : "non experimental"
+      message  = "#{action} all #{experimental_string} VM's defined in acceptance/Vagrantfile"
+      "#{message} for #{platform}: #{machines}" if !platform.nil?
+    end
+
+    desc "Generate a valid ssh-config"
+    task :ssh_config do
+      require "json"
+      raw_ssh_config    = LogStash::VagrantHelpers.fetch_config.stdout.split("\n");
+      parsed_ssh_config = LogStash::VagrantHelpers.parse(raw_ssh_config)
+      File.write(".vm_ssh_config", parsed_ssh_config.to_json)
+    end
+
+    desc "Bootstrap all the VM's used for this tests"
+    task :setup, :platform do |t, args|
+      config   = PlatformConfig.new
+      experimental = (ENV['LS_QA_EXPERIMENTAL_OS'].to_s.downcase || "false") == "true"
+      machines = config.select_names_for(args[:platform], {"experimental" => experimental})
+
+      puts user_feedback_string_for("bootstrapping", args[:platform], machines, {"experimental" => experimental})
+
+      options = {:debug => ENV['LS_QA_DEBUG']}
+      LogStash::VagrantHelpers.destroy(machines, options)
+      LogStash::VagrantHelpers.bootstrap(machines, options)
+    end
+
+    desc "Halt all VM's involved in the acceptance test round"
+    task :halt, :platform do |t, args|
+      config   = PlatformConfig.new
+      experimental = (ENV['LS_QA_EXPERIMENTAL_OS'].to_s.downcase || "false") == "true"
+      machines = config.select_names_for(args[:platform], {"experimental" => experimental})
+
+      puts user_feedback_string_for("halting", args[:platform], machines, {"experimental" => experimental})
+      options = {:debug => ENV['LS_QA_DEBUG']}
+
+      LogStash::VagrantHelpers.halt(machines, options)
+    end
+  end
+
+  namespace :acceptance do
+    desc "Run all acceptance"
+    task :all do
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/*_spec.rb"]]))
+    end
+
+    platforms.types.each do |type|
+      desc "Run acceptance test in #{type} machines"
+      task type do
+        ENV['LS_TEST_PLATFORM']=type
+        exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/*_spec.rb"]]))
+      end
+    end
+
+    desc "Run one single machine acceptance test"
+    task :single, :machine do |t, args|
+      ENV['LS_VAGRANT_HOST']  = args[:machine]
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/**/*_spec.rb"]]))
+    end
+  end
+end
diff --git a/qa/Vagrantfile b/qa/Vagrantfile
new file mode 100644
index 00000000000..b7da73064f6
--- /dev/null
+++ b/qa/Vagrantfile
@@ -0,0 +1,28 @@
+# -*- mode: ruby -*-
+# vi: set ft=ruby :
+require_relative "./platform_config.rb"
+
+Vagrant.configure(2) do |config|
+  platforms = PlatformConfig.new
+
+  platforms.each do |platform|
+    config.vm.define platform.name do |machine|
+      machine.vm.box = platform.box
+      machine.vm.provider "virtualbox" do |v|
+        v.memory = 2096
+        v.cpus = 4
+      end
+      machine.vm.synced_folder "../build", "/logstash-build", create: true
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.privileged
+        sh.privileged = true
+      end
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.non_privileged
+        sh.privileged = false
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/config_helper.rb b/qa/acceptance/spec/config_helper.rb
new file mode 100644
index 00000000000..3d9730c2d2d
--- /dev/null
+++ b/qa/acceptance/spec/config_helper.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "json"
+
+module SpecsHelper
+
+  def self.configure(vagrant_boxes)
+    setup_config = JSON.parse(File.read(File.join(File.dirname(__FILE__), "..", "..", ".vm_ssh_config")))
+    boxes        = vagrant_boxes.inject({}) do |acc, v|
+      acc[v.name] = v.type
+      acc
+    end
+    ServiceTester.configure do |config|
+      config.servers = []
+      config.lookup  = {}
+      setup_config.each do |host_info|
+        next unless boxes.keys.include?(host_info["host"])
+        url = "#{host_info["hostname"]}:#{host_info["port"]}"
+        config.servers << url
+        config.lookup[url] = {"host" => host_info["host"], "type" => boxes[host_info["host"]] }
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/lib/artifact_operation_spec.rb b/qa/acceptance/spec/lib/artifact_operation_spec.rb
new file mode 100644
index 00000000000..f541f3a9ffa
--- /dev/null
+++ b/qa/acceptance/spec/lib/artifact_operation_spec.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require_relative '../shared_examples/installed'
+require_relative '../shared_examples/running'
+require_relative '../shared_examples/updated'
+
+# This tests verify that the generated artifacts could be used properly in a release, implements https://github.com/elastic/logstash/issues/5070
+describe "artifacts operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "installable", logstash
+    it_behaves_like "updated", logstash
+  end
+end
diff --git a/qa/acceptance/spec/lib/cli_operation_spec.rb b/qa/acceptance/spec/lib/cli_operation_spec.rb
new file mode 100644
index 00000000000..43718dc488c
--- /dev/null
+++ b/qa/acceptance/spec/lib/cli_operation_spec.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require_relative "../spec_helper"
+require_relative "../shared_examples/cli/logstash/version"
+require_relative "../shared_examples/cli/logstash-plugin/install"
+require_relative "../shared_examples/cli/logstash-plugin/list"
+require_relative "../shared_examples/cli/logstash-plugin/uninstall"
+require_relative "../shared_examples/cli/logstash-plugin/remove"
+require_relative "../shared_examples/cli/logstash-plugin/update"
+
+# This is the collection of test for the CLI interface, this include the plugin manager behaviour, 
+# it also include the checks for other CLI options.
+describe "CLI operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "logstash version", logstash
+    it_behaves_like "logstash install", logstash
+    it_behaves_like "logstash list", logstash
+    it_behaves_like "logstash uninstall", logstash
+    it_behaves_like "logstash remove", logstash
+    it_behaves_like "logstash update", logstash
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
new file mode 100644
index 00000000000..1ef1e8aa90f
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
@@ -0,0 +1,69 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash install" do |logstash|
+  before(:each) do
+    logstash.install({:version => LOGSTASH_VERSION})
+  end
+
+  after(:each) do
+    logstash.uninstall
+  end
+
+  describe "on #{logstash.hostname}" do
+    context "with a direct internet connection" do
+      context "when the plugin exist" do
+        context "from a local `.GEM` file" do
+          let(:gem_name) { "logstash-filter-qatest-0.1.1.gem" }
+          let(:gem_path_on_vagrant) { "/tmp/#{gem_name}" }
+          before(:each) do
+            logstash.download("https://rubygems.org/gems/#{gem_name}", gem_path_on_vagrant)
+          end
+
+          after(:each) { logstash.delete_file(gem_path_on_vagrant) }
+
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install #{gem_path_on_vagrant}")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-dns")
+          end
+        end
+
+        context "when fetching a gem from rubygems" do
+
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest")
+          end
+
+          it "successfully install the plugin when verification is disabled" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest")
+          end
+
+          it "fails when installing a non logstash plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install  bundler")
+            expect(command).not_to install_successfully
+          end
+
+          it "allow to install a specific version" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify --version 0.1.0 logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest", "0.1.0")
+          end
+        end
+      end
+
+      context "when the plugin doesnt exist" do
+        it "fails to install and report an error" do
+          command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify logstash-output-impossible-plugin")
+          expect(command.stderr).to match(/Plugin not found, aborting/)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
new file mode 100644
index 00000000000..5f1e00fd2df
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash list" do |logstash|
+  describe "logstash-plugin list on #{logstash.hostname}" do
+    before(:all) do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after(:all) do
+      logstash.uninstall
+    end
+
+    context "without a specific plugin" do
+      it "display a list of plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "display a list of installed plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --installed")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "list the plugins with their versions" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose")
+        result.stdout.split("\n").each do |plugin|
+          expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+(.\w+)?\)/)
+        end
+      end
+    end
+
+    context "with a specific plugin" do
+      let(:plugin_name) { "logstash-input-stdin" }
+      it "list the plugin and display the plugin name" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name}$/)
+      end
+
+      it "list the plugin with his version" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb
new file mode 100644
index 00000000000..2423ded084c
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash remove" do |logstash|
+  describe "logstash-plugin remove on #{logstash.hostname}" do
+    before :each do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    context "when the plugin isn't installed" do
+      it "fails to remove it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin remove logstash-filter-qatest")
+        expect(result.stderr).to match(/This plugin has not been previously installed/)
+      end
+    end
+
+    # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
+    xcontext "when the plugin is installed" do
+      it "successfully removes it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+        expect(logstash).to have_installed?("logstash-filter-qatest")
+
+        result = logstash.run_command_in_path("bin/logstash-plugin remove logstash-filter-qatest")
+        expect(result.stdout).to match(/^Removing logstash-filter-qatest/)
+        expect(logstash).not_to have_installed?("logstash-filter-qatest")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
new file mode 100644
index 00000000000..54ec8a6a72b
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash uninstall" do |logstash|
+  describe "logstash-plugin uninstall on #{logstash.hostname}" do
+    before :each do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    context "when the plugin isn't installed" do
+      it "fails to uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stderr).to match(/This plugin has not been previously installed/)
+      end
+    end
+
+    # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
+    xcontext "when the plugin is installed" do
+      it "successfully uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+        expect(logstash).to have_installed?("logstash-filter-qatest")
+
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stdout).to match(/^Uninstalling logstash-filter-qatest/)
+        expect(logstash).not_to have_installed?("logstash-filter-qatest")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
new file mode 100644
index 00000000000..7b9f4880c05
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash update" do |logstash|
+  describe "logstash-plugin update on #{logstash.hostname}" do
+    before :each do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    let(:plugin_name) { "logstash-filter-qatest" }
+    let(:previous_version) { "0.1.0" }
+
+    before do
+      logstash.run_command_in_path("bin/logstash-plugin install --no-verify --version #{previous_version} #{plugin_name}")
+      # Logstash won't update when we have a pinned version in the gemfile so we remove them
+      logstash.replace_in_gemfile(',[[:space:]]"0.1.0"', "")
+      expect(logstash).to have_installed?(plugin_name, previous_version)
+    end
+
+    context "update a specific plugin" do
+      it "has executed successfully" do
+        cmd = logstash.run_command_in_path("bin/logstash-plugin update --no-verify #{plugin_name}")
+        expect(cmd.stdout).to match(/Updating #{plugin_name}/)
+        expect(logstash).not_to have_installed?(plugin_name, previous_version)
+      end
+    end
+
+    context "update all the plugins" do
+      it "has executed successfully" do
+        logstash.run_command_in_path("bin/logstash-plugin update --no-verify")
+        expect(logstash).to have_installed?(plugin_name, "0.1.1")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash/version.rb b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
new file mode 100644
index 00000000000..ae6843313d7
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash version" do |logstash|
+  describe "logstash --version" do
+    before :all do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :all do
+      logstash.uninstall
+    end
+
+    context "on #{logstash.hostname}" do
+      it "returns the right logstash version" do
+        result = logstash.run_command_in_path("bin/logstash --version")
+        expect(result).to run_successfully_and_output(/#{LOGSTASH_VERSION}/)
+      end
+      context "when also using the --path.settings argument" do
+        it "returns the right logstash version" do
+          result = logstash.run_command_in_path("bin/logstash --path.settings=/etc/logstash --version")
+          expect(result).to run_successfully_and_output(/#{LOGSTASH_VERSION}/)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/installed.rb b/qa/acceptance/spec/shared_examples/installed.rb
new file mode 100644
index 00000000000..3d058134fc3
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/installed.rb
@@ -0,0 +1,26 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if a package is possible to be installed without errors.
+RSpec.shared_examples "installable" do |logstash|
+
+  before(:each) do
+    logstash.uninstall
+    logstash.install({:version => LOGSTASH_VERSION})
+  end
+
+  it "is installed on #{logstash.hostname}" do
+    expect(logstash).to be_installed
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+  it "is removable on #{logstash.hostname}" do
+    logstash.uninstall
+    expect(logstash).to be_removed
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/running.rb b/qa/acceptance/spec/shared_examples/running.rb
new file mode 100644
index 00000000000..55d18507cba
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/running.rb
@@ -0,0 +1,17 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# Test if an installed package can actually be started and runs OK.
+RSpec.shared_examples "runnable" do |logstash|
+
+  before(:each) do
+    logstash.install({:version => LOGSTASH_VERSION})
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+end
diff --git a/qa/acceptance/spec/shared_examples/updated.rb b/qa/acceptance/spec/shared_examples/updated.rb
new file mode 100644
index 00000000000..f6c9e81319a
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/updated.rb
@@ -0,0 +1,27 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if the current package could used to update from the latest version released.
+RSpec.shared_examples "updated" do |logstash|
+
+  before(:all) { logstash.uninstall }
+  after(:all)  do
+    logstash.stop_service # make sure the service is stopped
+    logstash.uninstall #remove the package to keep uniform state
+  end
+
+  before(:each) do
+    options={:version => LOGSTASH_LATEST_VERSION, :snapshot => false, :base => "./" }
+    logstash.install(options) # make sure latest version is installed
+  end
+
+  it "can be updated an run on #{logstash.hostname}" do
+    expect(logstash).to be_installed
+    # Performing the update
+    logstash.install({:version => LOGSTASH_VERSION})
+    expect(logstash).to be_installed
+    # starts the service to be sure it runs after the upgrade
+    logstash.start_service
+    expect(logstash).to be_running
+  end
+end
diff --git a/qa/acceptance/spec/spec_helper.rb b/qa/acceptance/spec/spec_helper.rb
new file mode 100644
index 00000000000..99d6bb9a141
--- /dev/null
+++ b/qa/acceptance/spec/spec_helper.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require 'runner-tool'
+require_relative '../../rspec/helpers'
+require_relative '../../rspec/matchers'
+require_relative 'config_helper'
+require_relative "../../platform_config"
+
+ROOT = File.expand_path(File.join(File.dirname(__FILE__), '..', '..', '..'))
+$LOAD_PATH.unshift File.join(ROOT, 'logstash-core/lib')
+
+RunnerTool.configure
+
+RSpec.configure do |c|
+  c.include ServiceTester
+end
+
+platform = ENV['LS_TEST_PLATFORM'] || 'all'
+experimental = (ENV['LS_QA_EXPERIMENTAL_OS'].to_s.downcase || "false") == "true"
+
+config                  = PlatformConfig.new
+LOGSTASH_LATEST_VERSION = config.latest
+
+default_vagrant_boxes = ( platform == 'all' ? config.platforms : config.filter_type(platform, {"experimental" => experimental}) )
+
+selected_boxes = if ENV.include?('LS_VAGRANT_HOST') then
+                   config.platforms.select { |p| p.name  == ENV['LS_VAGRANT_HOST'] }
+                 else
+                   default_vagrant_boxes
+                 end
+
+SpecsHelper.configure(selected_boxes)
+
+puts "[Acceptance specs] running on #{ServiceTester.configuration.hosts}" if !selected_boxes.empty?
diff --git a/qa/config/platforms.json b/qa/config/platforms.json
new file mode 100644
index 00000000000..52f8223644e
--- /dev/null
+++ b/qa/config/platforms.json
@@ -0,0 +1,18 @@
+{ 
+  "latest": "5.0.0-alpha3",
+  "platforms" : {
+    "ubuntu-1204": { "box": "elastic/ubuntu-12.04-x86_64", "type": "debian" },
+    "ubuntu-1404": { "box": "elastic/ubuntu-14.04-x86_64", "type": "debian", "specific": true },
+    "ubuntu-1604": { "box": "elastic/ubuntu-16.04-x86_64", "type": "debian", "experimental": true },
+    "centos-6": { "box": "elastic/centos-6-x86_64", "type": "redhat" },
+    "centos-7": { "box": "elastic/centos-7-x86_64", "type": "redhat" },
+    "oel-6": { "box": "elastic/oraclelinux-6-x86_64", "type": "redhat" },
+    "oel-7": { "box": "elastic/oraclelinux-7-x86_64", "type": "redhat" },
+    "fedora-22": { "box": "elastic/fedora-22-x86_64", "type": "redhat", "experimental": true },
+    "fedora-23": { "box": "elastic/fedora-23-x86_64", "type": "redhat", "experimental": true },
+    "debian-8": { "box": "elastic/debian-8-x86_64", "type": "debian", "specific":  true },
+    "opensuse-13": { "box": "elastic/opensuse-13-x86_64", "type": "suse" },
+    "sles-11": { "box": "elastic/sles-11-x86_64", "type": "suse", "specific": true },
+    "sles-12": { "box": "elastic/sles-12-x86_64", "type": "suse", "specific": true }
+  }
+}
diff --git a/qa/integration/.gitignore b/qa/integration/.gitignore
new file mode 100644
index 00000000000..01f9fbcf19f
--- /dev/null
+++ b/qa/integration/.gitignore
@@ -0,0 +1,4 @@
+/services/installed
+/fixtures/certificates
+/fixtures/offline.o
+/fixtures/offline
diff --git a/qa/integration/.rspec b/qa/integration/.rspec
new file mode 100644
index 00000000000..83a4f920292
--- /dev/null
+++ b/qa/integration/.rspec
@@ -0,0 +1,2 @@
+--default-path specs/
+--format documentation
\ No newline at end of file
diff --git a/qa/integration/Gemfile b/qa/integration/Gemfile
new file mode 100644
index 00000000000..3be9c3cd812
--- /dev/null
+++ b/qa/integration/Gemfile
@@ -0,0 +1,2 @@
+source "https://rubygems.org"
+gemspec
diff --git a/qa/integration/README.md b/qa/integration/README.md
new file mode 100644
index 00000000000..2b411cf5156
--- /dev/null
+++ b/qa/integration/README.md
@@ -0,0 +1,41 @@
+## Logstash Integration Tests aka RATS
+
+These set of tests are full integration tests as in: they can start LS from a binary, run configs using `-e` and can use any external services like Kafka, ES and S3. This framework is hybrid -- a combination of bash scripts (to mainly setup services), Ruby service files, and RSpec. All test assertions are done in RSpec.
+
+No VMs, all tests run locally.
+
+## Dependencies
+* An existing Logstash binary, defaults to `LS_HOME/build/logstash-<version>`
+* `rspec`
+
+## Preparing a test run
+
+1. If you already have a LS binary in `LS_HOME/build/logstash-<version>`, skip to step 5
+2. From Logstash git source directory or `LS_HOME` run `rake artifact:tar` to build a package
+2. Untar the newly built package
+3. `cd build`
+4. `tar xvf logstash-<version>.tar.gz`
+5. `cd LS_HOME/qa/integration`
+6. `bundle install`: This will install test dependency gems.
+
+You are now ready to run any tests from `qa/integration`.
+* Run all tests: `rspec specs/*`
+* Run single test: `rspec specs/es_output_how_spec.rb`
+
+### Directory Layout
+
+* `fixtures`: In this dir you will test settings in form of `test_name.yml`. Here you specify services to run, LS config, test specific scripts ala `.travis.yml`
+* `services`: This dir has bash scripts that download and bootstrap binaries for services. This is where services like ES will be downloaded and run from. Service can have 3 files: `<service>_setup.sh`, `<service>_teardown.sh` and `<service>`.rb. The bash scripts deal with downloading and bootstrapping, but the ruby source will trigger them from the test as a shell out (using backticks). The tests are blocked until the setup/teardown completes. For example, Elasticsearch service has `elasticsearch_setup.sh`, `elasticsearch_teardown.sh` and `elasticsearch.rb`. The service name in yml is "elasticsearch".
+* `framework`: Test framework source code.
+* `specs`: Rspec tests that use services and validates stuff
+
+### Adding a new test
+
+1. Creating a new test -- lets use as example. Call it "test_file_input" which brings up LS to read from a file and assert file contents (file output) were as expected.
+2. You'll have to create a yml file in `fixtures` called `test_file_input_spec.yml`. Here you define any external services you need and any LS config.
+3. Create a corresponding `test_file_input_spec.rb` in `specs` folder and use the `fixtures` object to get all services, config etc. The `.yml` and rspec file has to be the same name for the settings to be picked up. You can start LS inside the tests and assume all external services have already been started.
+4. Write rspec code to validate.
+
+## Future Improvements
+
+1. Perform setup and teardown from Ruby and get rid of bash files altogether.
diff --git a/qa/integration/fixtures/01_logstash_bin_smoke_spec.yml b/qa/integration/fixtures/01_logstash_bin_smoke_spec.yml
new file mode 100644
index 00000000000..dcd2ec83b46
--- /dev/null
+++ b/qa/integration/fixtures/01_logstash_bin_smoke_spec.yml
@@ -0,0 +1,15 @@
+---
+services:
+  - logstash
+config: |-
+ input {
+    tcp {
+      port => '<%=options[:port]%>'
+    }
+
+    generator { count => 5 }
+  }
+  output {
+    file { path => '<%=options[:random_file]%>' }
+  }
+
diff --git a/qa/integration/fixtures/beats_input_spec.yml b/qa/integration/fixtures/beats_input_spec.yml
new file mode 100644
index 00000000000..9fc49dd3d5f
--- /dev/null
+++ b/qa/integration/fixtures/beats_input_spec.yml
@@ -0,0 +1,36 @@
+---
+services:
+  - filebeat
+  - logstash
+config:
+  without_tls: |-
+    input {
+      beats {
+        port => 5044
+      }
+    }
+    output {}
+  tls_server_auth: |-
+    input {
+      beats {
+        ssl => true
+        port => 5044
+        ssl_certificate => '<%=options[:ssl_certificate]%>'
+        ssl_key => '<%=options[:ssl_key]%>'
+      }
+    }
+    output {}
+  tls_mutual_auth: |-
+    input {
+      beats {
+        ssl => true
+        port => 5044
+        ssl_certificate => '<%=options[:ssl_certificate]%>'
+        ssl_key => '<%=options[:ssl_key]%>'
+        ssl_verify_mode => "force_peer"
+        ssl_certificate_authorities => '<%=options[:ssl_certificate]%>'
+      }
+    }
+    output {}
+input: how_sample.input
+teardown_script:
diff --git a/qa/integration/fixtures/env_variables_config_spec.yml b/qa/integration/fixtures/env_variables_config_spec.yml
new file mode 100644
index 00000000000..008e2c95f2f
--- /dev/null
+++ b/qa/integration/fixtures/env_variables_config_spec.yml
@@ -0,0 +1,21 @@
+---
+services:
+  - logstash
+config: |-
+ input {
+    tcp {
+      port => "${TEST_ENV_TCP_PORT}"
+    }
+  }
+  filter {
+    mutate {
+      add_tag => [ "blah", "${TEST_ENV_TAG}" ]
+    }
+  }
+  output {
+    file {
+      path => "${TEST_ENV_PATH}/logstash_env_test.log"
+      flush_interval => 0
+      codec => line { format => "%{message} %{tags}" }
+    }
+  }
\ No newline at end of file
diff --git a/qa/integration/fixtures/es_output_how_spec.yml b/qa/integration/fixtures/es_output_how_spec.yml
new file mode 100644
index 00000000000..cf2c436b6be
--- /dev/null
+++ b/qa/integration/fixtures/es_output_how_spec.yml
@@ -0,0 +1,34 @@
+---
+services:
+  - logstash
+  - elasticsearch
+config: |-
+  input {
+    stdin { }
+  }
+
+  filter {
+    grok {
+      match => {
+        "message" => "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}"
+      }
+    }
+
+    date {
+      match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
+      locale => en
+    }
+    geoip {
+      source => "clientip"
+    }
+    useragent {
+      source => "agent"
+      target => "useragent"
+    }
+  }
+  output {
+    elasticsearch {}
+  }
+
+input: how_sample.input
+teardown_script:
diff --git a/qa/integration/fixtures/how_sample.input b/qa/integration/fixtures/how_sample.input
new file mode 100644
index 00000000000..bef7ff54b4e
--- /dev/null
+++ b/qa/integration/fixtures/how_sample.input
@@ -0,0 +1,37 @@
+74.125.176.147 - - [11/Sep/2014:21:50:37 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "FeedBurner/1.0 (http://www.FeedBurner.com)"
+66.249.84.55 - - [11/Sep/2014:21:50:54 +0000] "GET / HTTP/1.1" 200 37932 "-" "Mozilla/5.0 (Windows NT 6.1; rv:6.0) Gecko/20110814 Firefox/6.0 Google favicon"
+66.249.84.55 - - [11/Sep/2014:21:50:54 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 6.1; rv:6.0) Gecko/20110814 Firefox/6.0 Google favicon"
+46.105.14.53 - - [11/Sep/2014:21:51:35 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
+194.153.217.248 - - [11/Sep/2014:21:51:56 +0000] "GET /blog/geekery/debugging-java-performance.html HTTP/1.1" 200 15796 "http://logstash.net/docs/1.3.3/life-of-an-event" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:57 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/blog/geekery/debugging-java-performance.html" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:57 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/blog/geekery/debugging-java-performance.html" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:58 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:58 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/blog/geekery/debugging-java-performance.html" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:58 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+159.253.145.222 - - [11/Sep/2014:21:51:53 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+213.113.233.227 - - [11/Sep/2014:21:53:14 +0000] "GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1" 200 18848 "https://www.google.se/" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.154 Safari/537.36"
+173.192.221.211 - - [11/Sep/2014:21:53:31 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+5.10.83.95 - - [11/Sep/2014:21:54:16 +0000] "GET /files/xdotool/docs/man/?C=N;O=D HTTP/1.1" 200 955 "-" "Mozilla/5.0 (compatible; AhrefsBot/5.0; +http://ahrefs.com/robot/)"
+194.103.63.154 - - [11/Sep/2014:21:54:26 +0000] "GET /favicon.ico HTTP/1.1" 304 - "-" "Mozilla/4.0 (compatible;)"
+95.30.144.170 - - [11/Sep/2014:21:56:20 +0000] "GET /presentations/vim/ HTTP/1.0" 200 20572 "http://semicomplete.com/presentations/vim/" "Opera/9.80 (Windows NT 6.1; WOW64; U; MRA 8.0 (build 5880); ru) Presto/2.10.289 Version/12.02"
+159.253.145.222 - - [11/Sep/2014:21:56:27 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+46.105.14.53 - - [11/Sep/2014:21:56:36 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
+202.69.11.26 - - [11/Sep/2014:21:56:40 +0000] "GET /blog/tags/X11 HTTP/1.1" 200 32742 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+5.10.83.96 - - [11/Sep/2014:21:56:47 +0000] "GET /blog/geekery/insist-on-better-asserts.html HTTP/1.1" 200 9514 "-" "Mozilla/5.0 (compatible; AhrefsBot/5.0; +http://ahrefs.com/robot/)"
+202.69.11.26 - - [11/Sep/2014:21:56:48 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
+202.69.11.26 - - [11/Sep/2014:21:56:54 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
+202.69.11.26 - - [11/Sep/2014:21:56:57 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:57:06 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+174.36.228.156 - - [11/Sep/2014:21:57:28 +0000] "GET /blog HTTP/1.1" 200 37936 "-" "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.0.19; aggregator:Spinn3r (Spinn3r 3.1); http://spinn3r.com/robot) Gecko/2010040121 Firefox/3.0.19"
+108.174.55.234 - - [11/Sep/2014:21:57:42 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "-"
+173.192.221.212 - - [11/Sep/2014:21:58:16 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+217.69.133.237 - - [11/Sep/2014:21:58:29 +0000] "GET /scripts/netcat-webserver HTTP/1.1" 304 - "-" "Mozilla/5.0 (compatible; Linux x86_64; Mail.RU_Bot/2.0; +http://go.mail.ru/help/robots)"
+202.69.11.26 - - [11/Sep/2014:21:58:52 +0000] "GET /images/googledotcom.png HTTP/1.1" 200 65748 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:59:10 +0000] "GET /blog/tags/X11 HTTP/1.1" 200 32742 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:59:15 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
+202.69.11.26 - - [11/Sep/2014:21:59:17 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:59:19 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
+5.153.24.131 - - [11/Sep/2014:21:59:18 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+198.46.149.143 - - [11/Sep/2014:21:59:29 +0000] "GET /blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1" 200 9316 "-" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
+198.46.149.143 - - [11/Sep/2014:21:59:29 +0000] "GET /blog/geekery/solving-good-or-bad-problems.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1" 200 10756 "-" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
+183.60.215.50 - - [11/Sep/2014:22:00:00 +0000] "GET /scripts/netcat-webserver HTTP/1.1" 200 182 "-" "Mozilla/5.0 (compatible; EasouSpider; +http://www.easou.com/search/spider.html)"
diff --git a/qa/integration/fixtures/http_proxy_install_spec.yml b/qa/integration/fixtures/http_proxy_install_spec.yml
new file mode 100644
index 00000000000..cbfc784af81
--- /dev/null
+++ b/qa/integration/fixtures/http_proxy_install_spec.yml
@@ -0,0 +1,3 @@
+---
+services:
+  - logstash
diff --git a/qa/integration/fixtures/install_spec.yml b/qa/integration/fixtures/install_spec.yml
new file mode 100644
index 00000000000..cbfc784af81
--- /dev/null
+++ b/qa/integration/fixtures/install_spec.yml
@@ -0,0 +1,3 @@
+---
+services:
+  - logstash
diff --git a/qa/integration/fixtures/kafka_input_spec.yml b/qa/integration/fixtures/kafka_input_spec.yml
new file mode 100644
index 00000000000..a7df6098eab
--- /dev/null
+++ b/qa/integration/fixtures/kafka_input_spec.yml
@@ -0,0 +1,24 @@
+---
+services:
+  - logstash
+  - kafka
+config: |-
+  input {
+    kafka {
+      topics => "logstash_topic_plain"
+      auto_offset_reset => "earliest"
+      codec => "plain"
+      group_id => "ls10"
+    }
+  }
+  output {
+    file {
+       path => "kafka_input.output"
+       flush_interval => 0
+       codec => line { format => "%{message}" }
+    }
+  }
+
+input: how_sample.input
+actual_output: kafka_input.output 
+teardown_script:
diff --git a/qa/integration/fixtures/logstash-dummy-pack/.gitignore b/qa/integration/fixtures/logstash-dummy-pack/.gitignore
new file mode 100644
index 00000000000..0bd659d183a
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/.gitignore
@@ -0,0 +1,3 @@
+vendor/bundle
+dependencies/
+build/
diff --git a/qa/integration/fixtures/logstash-dummy-pack/Gemfile b/qa/integration/fixtures/logstash-dummy-pack/Gemfile
new file mode 100644
index 00000000000..f40ab78d3d1
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/Gemfile
@@ -0,0 +1,4 @@
+source 'https://rubygems.org'
+gemspec
+
+gem "paquet", :path => "#{File.dirname(__FILE__)}/../../../../tools/paquet"
diff --git a/qa/integration/fixtures/logstash-dummy-pack/Rakefile b/qa/integration/fixtures/logstash-dummy-pack/Rakefile
new file mode 100644
index 00000000000..bb55078e69e
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/Rakefile
@@ -0,0 +1,14 @@
+# encoding: utf-8
+require "paquet"
+
+TARGET_DIRECTORY = File.join(File.dirname(__FILE__), "dependencies")
+
+Paquet::Task.new(TARGET_DIRECTORY) do
+  pack "manticore"
+  pack "gemoji"
+
+  # Everything not defined here will be assumed to be provided
+  # by the target installation
+  ignore "logstash-core-plugin-api"
+  ignore "logstash-core"
+end
diff --git a/qa/integration/fixtures/logstash-dummy-pack/bundle.sh b/qa/integration/fixtures/logstash-dummy-pack/bundle.sh
new file mode 100755
index 00000000000..bc16b58dcfc
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/bundle.sh
@@ -0,0 +1,20 @@
+bundle install --path vendor
+bundle exec rake vendor
+bundle exec rake paquet:vendor
+rm -rf build/
+mkdir -p build/logstash-dummy-pack/logstash/
+cp -r dependencies build/logstash-dummy-pack/logstash/
+gem build logstash-output-secret.gemspec
+mv logstash-output-secret*.gem build/logstash-dummy-pack/logstash/
+
+# Generate stuff for a uber zip
+mkdir -p build/logstash-dummy-pack/elasticsearch
+touch build/logstash-dummy-pack/elasticsearch/README.md
+
+mkdir -p build/logstash-dummy-pack/kibana
+touch build/logstash-dummy-pack/kibana/README.md
+
+cd build/
+zip -r logstash-dummy-pack.zip logstash-dummy-pack
+cp *.zip ../
+cd ..
diff --git a/qa/integration/fixtures/logstash-dummy-pack/lib/logstash/outputs/secret.rb b/qa/integration/fixtures/logstash-dummy-pack/lib/logstash/outputs/secret.rb
new file mode 100644
index 00000000000..ed621ac3259
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/lib/logstash/outputs/secret.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# An secret output that does nothing.
+class LogStash::Outputs::Secret < LogStash::Outputs::Base
+  config_name "secret"
+
+  public
+  def register
+  end # def register
+
+  public
+  def receive(event)
+    return "Event received"
+  end # def event
+end # class LogStash::Outputs::Secret
diff --git a/qa/integration/fixtures/logstash-dummy-pack/logstash-dummy-pack.zip b/qa/integration/fixtures/logstash-dummy-pack/logstash-dummy-pack.zip
new file mode 100644
index 00000000000..f2dbc63c8d6
Binary files /dev/null and b/qa/integration/fixtures/logstash-dummy-pack/logstash-dummy-pack.zip differ
diff --git a/qa/integration/fixtures/logstash-dummy-pack/logstash-output-secret.gemspec b/qa/integration/fixtures/logstash-dummy-pack/logstash-output-secret.gemspec
new file mode 100644
index 00000000000..af9a1e2b192
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/logstash-output-secret.gemspec
@@ -0,0 +1,33 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-output-secret'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'Write a short summary, because Rubygems requires one.'
+  s.description   = 'Write a longer description or delete this line.'
+  s.homepage      = 'https://github.com/ph/secret'
+  s.authors       = ['Pier-Hugues Pellerin']
+  s.email         = 'phpellerin@gmail.com'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "output" }
+  s.post_install_message =<<eos
+This plugins will require the configuration of XXXXX in the logstash.yml
+
+Make sure you double check your configuration
+eos
+
+
+  # Gem dependencies
+  s.add_runtime_dependency "manticore"
+  s.add_runtime_dependency "gemoji", "< 2.0"
+
+
+  s.add_development_dependency "paquet"
+  s.add_development_dependency "rake"
+end
diff --git a/qa/integration/fixtures/logstash-dummy-pack/spec/outputs/secret_spec.rb b/qa/integration/fixtures/logstash-dummy-pack/spec/outputs/secret_spec.rb
new file mode 100644
index 00000000000..341b7a2501d
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/spec/outputs/secret_spec.rb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/outputs/secret"
+require "logstash/codecs/plain"
+require "logstash/event"
+
+describe LogStash::Outputs::Secret do
+  let(:sample_event) { LogStash::Event.new }
+  let(:output) { LogStash::Outputs::Secret.new }
+
+  before do
+    output.register
+  end
+
+  describe "receive message" do
+    subject { output.receive(sample_event) }
+
+    it "returns a string" do
+      expect(subject).to eq("Event received")
+    end
+  end
+end
diff --git a/qa/integration/fixtures/logstash-filter-qatest-0.1.1.gem b/qa/integration/fixtures/logstash-filter-qatest-0.1.1.gem
new file mode 100644
index 00000000000..3dd8ba42d1f
Binary files /dev/null and b/qa/integration/fixtures/logstash-filter-qatest-0.1.1.gem differ
diff --git a/qa/integration/fixtures/monitoring_api_spec.yml b/qa/integration/fixtures/monitoring_api_spec.yml
new file mode 100644
index 00000000000..52bff4e9e95
--- /dev/null
+++ b/qa/integration/fixtures/monitoring_api_spec.yml
@@ -0,0 +1,4 @@
+---
+name: Metrics test
+services:
+  - logstash
diff --git a/qa/integration/fixtures/offline_wrapper/.gitignore b/qa/integration/fixtures/offline_wrapper/.gitignore
new file mode 100644
index 00000000000..e4d959a4151
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/.gitignore
@@ -0,0 +1 @@
+offine
diff --git a/qa/integration/fixtures/offline_wrapper/Makefile b/qa/integration/fixtures/offline_wrapper/Makefile
new file mode 100644
index 00000000000..448c4e434fe
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/Makefile
@@ -0,0 +1,8 @@
+OBJECTS=offline offline.o
+
+default: offline
+
+clean:
+	rm -f $(OBJECTS)
+offline: offline.o
+	$(CC) -o $@ $<
diff --git a/qa/integration/fixtures/offline_wrapper/README.md b/qa/integration/fixtures/offline_wrapper/README.md
new file mode 100644
index 00000000000..34e35214aa5
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/README.md
@@ -0,0 +1,31 @@
+
+# offline with seccomp
+
+This is a little hack I wrote while trying to see if seccomp could be used to
+help me more easily test programs in "offline mode" without having to actually
+disable networking on my whole laptop.
+
+Building:
+
+```
+make offline
+```
+
+Usage:
+
+```
+./offline <program> [args]
+```
+
+Example:
+
+```
+% nc localhost 10000
+Ncat: Connection refused.
+
+% ./offline nc localhost 10000
+Ncat: Permission denied.
+
+% ./offline host google.com
+host: isc_socket_bind: permission denied
+```
diff --git a/qa/integration/fixtures/offline_wrapper/offline.c b/qa/integration/fixtures/offline_wrapper/offline.c
new file mode 100644
index 00000000000..b335729876d
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/offline.c
@@ -0,0 +1,98 @@
+#include <linux/seccomp.h>
+#include <linux/filter.h>
+#include <sys/syscall.h>
+#include <stddef.h>
+#include <sys/types.h>
+#include <errno.h>
+#include <stdio.h>
+#include <sys/prctl.h>
+#include <unistd.h>
+
+#include <sys/socket.h>
+#include <sys/un.h>
+#include <netinet/in.h>
+
+struct sock_filter reject_connect_and_bind[] = {
+  // LD|W|ABS == Load Word at ABSolute offset
+  // Load the syscall number
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, (offsetof(struct seccomp_data, nr))),
+
+  // JMP|JEQ|K Do a jump after comparing EQuality of the loaded value and a
+  // constant. If equal, jump 2 positions forward, if not equal, do not jump(zero jump).
+  // Is it the `connect` syscall?
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, __NR_connect, 2, 0),
+  // Is it the `bind` syscall?
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, __NR_bind, 1, 0),
+
+  // RET|K Return a constant.
+  // Neither bind nor connect? Allow it.
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+
+  // Fun fact. `connect` and `bind` take the same arguments, so we can process them the same way.
+  // int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen)
+  // int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen)
+
+  // Ideally, we'd load the 2nd arg (sockaddr struct) and look at the `sa_family` member to see
+  // what kind of socket address is to be used. However, BPF/seccomp doesn't allow you to 
+  // dereference pointers... so let's try relying on the sockaddr_len argument.
+
+  // Load third argument to the syscall (addrlen)
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, offsetof(struct seccomp_data, args[2])),
+
+  // Try filtering based on the sockaddr len. This isn't great, but may be better than nothing.
+  //   - Reject sockaddr_in and sockaddr_in6
+  //   - Allow everything else (unix sockets, etc)
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, sizeof(struct sockaddr_in), 1, 0),
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, sizeof(struct sockaddr_in6), 0, 1),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ERRNO|(EACCES&SECCOMP_RET_DATA)),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+};
+
+struct sock_filter reject_inet_socket[] = {
+  // LD|W|ABS == Load Word at ABSolute offset
+  // Load the syscall number
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, (offsetof(struct seccomp_data, nr))),
+
+  // Is it the `socket` syscall?
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, __NR_socket, 1, 0),
+  // Not `socket` call? Allow it.
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+
+  // Load first argument to the syscall (domain)
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, offsetof(struct seccomp_data, args[0])),
+
+  // Reject INET and INET6
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, AF_INET, 1, 0),
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, AF_INET6, 0, 1),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ERRNO|(EACCES&SECCOMP_RET_DATA)),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+};
+
+int main(int argc, char **argv) {
+  struct sock_filter *filter = reject_inet_socket;
+  unsigned short count = sizeof(reject_inet_socket) / sizeof(filter[0]);
+
+  struct sock_fprog prog = {
+    .len = count,
+    .filter = filter,
+  };
+
+  if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0)) {
+    perror("seccomp PR_SET_NO_NEW_PRIVS");
+    return 1;
+  }
+
+  if (prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &prog)) {
+    perror("seccomp");
+    return 1;
+  }
+
+  if (argc == 1) {
+    printf("Usage: %s <program> [args]\n", argv[0]);
+    return 1;
+  }
+
+  argv++;
+  execvp(argv[0], argv);
+  return 0;
+}
diff --git a/qa/integration/fixtures/persistent_queues/log4j2.properties b/qa/integration/fixtures/persistent_queues/log4j2.properties
new file mode 100644
index 00000000000..ac9273b64a1
--- /dev/null
+++ b/qa/integration/fixtures/persistent_queues/log4j2.properties
@@ -0,0 +1,83 @@
+status = error
+name = LogstashPropertiesConfig
+
+appender.console.type = Console
+appender.console.name = plain_console
+appender.console.layout.type = PatternLayout
+appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n
+
+appender.json_console.type = Console
+appender.json_console.name = json_console
+appender.json_console.layout.type = JSONLayout
+appender.json_console.layout.compact = true
+appender.json_console.layout.eventEol = true
+
+appender.rolling.type = RollingFile
+appender.rolling.name = plain_rolling
+appender.rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.rolling.policies.type = Policies
+appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.rolling.policies.time.interval = 1
+appender.rolling.policies.time.modulate = true
+appender.rolling.layout.type = PatternLayout
+appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %-.10000m%n
+
+appender.json_rolling.type = RollingFile
+appender.json_rolling.name = json_rolling
+appender.json_rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.json_rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.json_rolling.policies.type = Policies
+appender.json_rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.json_rolling.policies.time.interval = 1
+appender.json_rolling.policies.time.modulate = true
+appender.json_rolling.layout.type = JSONLayout
+appender.json_rolling.layout.compact = true
+appender.json_rolling.layout.eventEol = true
+
+
+rootLogger.level = ${sys:ls.log.level}
+rootLogger.appenderRef.console.ref = ${sys:ls.log.format}_console
+rootLogger.appenderRef.rolling.ref = ${sys:ls.log.format}_rolling
+
+# Slowlog
+
+appender.console_slowlog.type = Console
+appender.console_slowlog.name = plain_console_slowlog
+appender.console_slowlog.layout.type = PatternLayout
+appender.console_slowlog.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n
+
+appender.json_console_slowlog.type = Console
+appender.json_console_slowlog.name = json_console_slowlog
+appender.json_console_slowlog.layout.type = JSONLayout
+appender.json_console_slowlog.layout.compact = true
+appender.json_console_slowlog.layout.eventEol = true
+
+appender.rolling_slowlog.type = RollingFile
+appender.rolling_slowlog.name = plain_rolling_slowlog
+appender.rolling_slowlog.fileName = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}.log
+appender.rolling_slowlog.filePattern = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.rolling_slowlog.policies.type = Policies
+appender.rolling_slowlog.policies.time.type = TimeBasedTriggeringPolicy
+appender.rolling_slowlog.policies.time.interval = 1
+appender.rolling_slowlog.policies.time.modulate = true
+appender.rolling_slowlog.layout.type = PatternLayout
+appender.rolling_slowlog.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %.10000m%n
+
+appender.json_rolling_slowlog.type = RollingFile
+appender.json_rolling_slowlog.name = json_rolling_slowlog
+appender.json_rolling_slowlog.fileName = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}.log
+appender.json_rolling_slowlog.filePattern = ${sys:ls.logs}/logstash-slowlog-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.json_rolling_slowlog.policies.type = Policies
+appender.json_rolling_slowlog.policies.time.type = TimeBasedTriggeringPolicy
+appender.json_rolling_slowlog.policies.time.interval = 1
+appender.json_rolling_slowlog.policies.time.modulate = true
+appender.json_rolling_slowlog.layout.type = JSONLayout
+appender.json_rolling_slowlog.layout.compact = true
+appender.json_rolling_slowlog.layout.eventEol = true
+
+logger.slowlog.name = slowlog
+logger.slowlog.level = trace
+logger.slowlog.appenderRef.console_slowlog.ref = ${sys:ls.log.format}_console_slowlog
+logger.slowlog.appenderRef.rolling_slowlog.ref = ${sys:ls.log.format}_rolling_slowlog
+logger.slowlog.additivity = false
diff --git a/qa/integration/fixtures/persistent_queues/logstash.yml b/qa/integration/fixtures/persistent_queues/logstash.yml
new file mode 100644
index 00000000000..1ee962bd8ae
--- /dev/null
+++ b/qa/integration/fixtures/persistent_queues/logstash.yml
@@ -0,0 +1 @@
+queue.type: persisted
diff --git a/qa/integration/fixtures/prepare_offline_pack_spec.yml b/qa/integration/fixtures/prepare_offline_pack_spec.yml
new file mode 100644
index 00000000000..cbfc784af81
--- /dev/null
+++ b/qa/integration/fixtures/prepare_offline_pack_spec.yml
@@ -0,0 +1,3 @@
+---
+services:
+  - logstash
diff --git a/qa/integration/fixtures/reload_config_spec.yml b/qa/integration/fixtures/reload_config_spec.yml
new file mode 100644
index 00000000000..bc01103f544
--- /dev/null
+++ b/qa/integration/fixtures/reload_config_spec.yml
@@ -0,0 +1,37 @@
+---
+services:
+  - logstash
+config:
+  initial: |-
+    input {
+      tcp {
+        port => '<%=options[:port]%>'
+      }
+    }
+    output {
+      file {
+         path => '<%=options[:file]%>'
+         flush_interval => 0
+         codec => line { format => "%{message}" }
+      }
+    }
+  reload: |-
+    input {
+      tcp {
+        port => '<%=options[:port]%>'
+      }
+    }
+    filter {
+      grok {
+        match => {
+          "message" => "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}"
+        }
+      }
+    }
+    output {
+      file {
+         path => '<%=options[:file]%>'
+         flush_interval => 0
+         codec => json
+      }
+    }
\ No newline at end of file
diff --git a/qa/integration/fixtures/remove_spec.yml b/qa/integration/fixtures/remove_spec.yml
new file mode 100644
index 00000000000..cbfc784af81
--- /dev/null
+++ b/qa/integration/fixtures/remove_spec.yml
@@ -0,0 +1,3 @@
+---
+services:
+  - logstash
diff --git a/qa/integration/fixtures/settings_spec.yml b/qa/integration/fixtures/settings_spec.yml
new file mode 100644
index 00000000000..53be4a7f335
--- /dev/null
+++ b/qa/integration/fixtures/settings_spec.yml
@@ -0,0 +1,9 @@
+---
+services:
+  - logstash
+config: |-
+ input {
+    tcp {
+      port => '<%=options[:port]%>'
+    }
+  }  
\ No newline at end of file
diff --git a/qa/integration/fixtures/slowlog_spec.yml b/qa/integration/fixtures/slowlog_spec.yml
new file mode 100644
index 00000000000..0d962f7d0e4
--- /dev/null
+++ b/qa/integration/fixtures/slowlog_spec.yml
@@ -0,0 +1,15 @@
+---
+services:
+  - logstash
+config: |-
+ input {
+    generator {
+      count => 4
+    }
+ }
+ filter {
+   sleep { time => 1 every => 2 }
+ }
+ output {
+   null {}
+ }
diff --git a/qa/integration/framework/fixture.rb b/qa/integration/framework/fixture.rb
new file mode 100644
index 00000000000..ba5120e3ac1
--- /dev/null
+++ b/qa/integration/framework/fixture.rb
@@ -0,0 +1,81 @@
+require_relative "../services/service_locator"
+
+# A class that holds all fixtures for a given test file. This deals with
+# bootstrapping services, dealing with config files, inputs etc
+class Fixture
+  FIXTURES_DIR = File.expand_path(File.join("..", "..", "fixtures"), __FILE__)
+
+  attr_reader :input
+  attr_reader :actual_output
+  attr_reader :test_dir
+  attr_reader :settings
+
+  class TemplateContext
+    attr_reader :options
+
+    def initialize(options)
+      @options = options
+    end
+
+    def get_binding
+      binding
+    end
+  end
+
+  def initialize(test_file_location)
+    @test_file_location = test_file_location
+    @settings = TestSettings.new(@test_file_location)
+    @service_locator = ServiceLocator.new(@settings)
+    setup_services
+    @input = File.join(FIXTURES_DIR, @settings.get("input")) if @settings.is_set?("input")
+    @actual_output = @settings.get("actual_output")
+  end
+
+  def config(node = "root", options = nil)
+    if node == "root"
+      config = @settings.get("config")
+    else
+      config = @settings.get("config")[node]
+    end
+
+    if options != nil
+       ERB.new(config, nil, "-").result(TemplateContext.new(options).get_binding)
+    else
+      config
+    end
+  end
+
+  def get_service(name)
+    @service_locator.get_service(name)
+  end
+
+  def output_equals_expected?
+    FileUtils.identical?(@actual_output, @input)
+  end
+
+  def output_exists?
+    File.exists?(@actual_output)
+  end
+
+  def teardown
+    File.delete(@actual_output) if @settings.is_set?("actual_output") && output_exists?
+    puts "Tearing down services"
+    services = @settings.get("services")
+    services.each do |name|
+      @service_locator.get_service(name).teardown
+    end
+  end
+
+  def setup_services
+    puts "Setting up services"
+    services = @settings.get("services")
+    services.each do |name|
+     @service_locator.get_service(name).setup
+    end
+    if @settings.is_set?("setup_script")
+      puts "Setting up test specific fixtures"
+      script = File.join(FIXTURES_DIR, @settings.get("setup_script"))
+      `#{script}`
+    end
+  end
+end
diff --git a/qa/integration/framework/helpers.rb b/qa/integration/framework/helpers.rb
new file mode 100644
index 00000000000..f50da6b67aa
--- /dev/null
+++ b/qa/integration/framework/helpers.rb
@@ -0,0 +1,111 @@
+# encoding: utf-8
+# Helper module for all tests
+require "flores/random"
+require "fileutils"
+require "zip"
+require "stud/temporary"
+require "socket"
+require "ostruct"
+
+def wait_for_port(port, retry_attempts)
+  tries = retry_attempts
+  while tries > 0
+    if is_port_open?(port)
+      break
+    else
+      sleep 1
+    end
+    tries -= 1
+  end
+end
+
+def is_port_open?(port)
+  begin
+    s = TCPSocket.open("localhost", port)
+    s.close
+    return true
+  rescue Errno::ECONNREFUSED, Errno::EHOSTUNREACH
+    return false
+  end
+end
+
+def send_data(port, data)
+  socket = TCPSocket.new("127.0.0.1", port)
+  socket.puts(data)
+  socket.flush
+  socket.close
+end
+
+def config_to_temp_file(config)
+  f = Stud::Temporary.file
+  f.write(config)
+  f.close
+  f.path
+end
+
+def random_port
+  # 9600-9700 is reserved in Logstash HTTP server, so we don't want that
+  Flores::Random.integer(9701..15000)
+end
+
+class Pack
+  PLUGINS_PATH = "logstash"
+  DEPENDENCIES_PATH = File.join("logstash", "dependencies")
+  GEM_EXTENSION = ".gem"
+
+  def initialize(target)
+    @target = target
+  end
+
+  def plugins
+    @plugins ||= extract_gems_data(File.join(@target, PLUGINS_PATH))
+  end
+
+  def dependencies
+    @dependencies ||= extract_gems_data(File.join(@target, DEPENDENCIES_PATH))
+  end
+
+  def glob_gems
+    "*#{GEM_EXTENSION}"
+  end
+
+  def extract_gems_data(path)
+    Dir.glob(File.join(path, glob_gems)).collect { |gem_file| extract_gem_data_from_file(gem_file) }
+  end
+
+  def extract_gem_data_from_file(gem_file)
+    gem = File.basename(gem_file.downcase, GEM_EXTENSION)
+
+    parts = gem.split("-")
+
+    if gem.match(/java/)
+      platform = parts.pop
+      version = parts.pop
+      name = parts.join("-")
+
+      OpenStruct.new(:name => name, :version => version, :platform => platform)
+    else
+      version = parts.pop
+      name = parts.join("-")
+
+      OpenStruct.new(:name => name, :version => version, :platform => nil)
+    end
+  end
+end
+
+def extract(source, target, pattern = nil)
+  raise "Directory #{target} exist" if ::File.exist?(target)
+  Zip::File.open(source) do |zip_file|
+    zip_file.each do |file|
+      path = ::File.join(target, file.name)
+      FileUtils.mkdir_p(::File.dirname(path))
+      zip_file.extract(file, path) if pattern.nil? || pattern =~ file.name
+    end
+  end
+end
+
+def unpack(zip)
+  target = Stud::Temporary.pathname
+  extract(zip, target)
+  Pack.new(target)
+end
diff --git a/qa/integration/framework/settings.rb b/qa/integration/framework/settings.rb
new file mode 100644
index 00000000000..b5c567a049a
--- /dev/null
+++ b/qa/integration/framework/settings.rb
@@ -0,0 +1,62 @@
+require 'yaml'
+
+# All settings for a test, global and per test
+class TestSettings
+  # Setting for the entire test suite
+  INTEG_TESTS_DIR = File.expand_path(File.join("..", ".."), __FILE__)
+  # Test specific settings
+  SUITE_SETTINGS_FILE = File.join(INTEG_TESTS_DIR, "suite.yml")
+  FIXTURES_DIR = File.join(INTEG_TESTS_DIR, "fixtures")
+
+  def initialize(test_file_path)
+    test_name = File.basename(test_file_path, ".*" )
+    @tests_settings_file = File.join(FIXTURES_DIR, "#{test_name}.yml")
+    # Global suite settings
+    @suite_settings = YAML.load(ERB.new(File.new(SUITE_SETTINGS_FILE).read).result)
+    # Per test settings, where one can override stuff and define test specific config
+    @test_settings = YAML.load_file(@tests_settings_file)
+    
+    if verbose_mode?
+      puts "Test settings file: #{@tests_settings_file}"
+      puts "Suite settings file: #{SUITE_SETTINGS_FILE}"
+    end  
+
+    if is_set?("config")
+      if get("config").is_a?(Hash)
+        tmp = {}
+        get("config").each do |k, v|
+          tmp[k] = get("config")[k].gsub('\n','').split.join(" ")
+        end
+        @test_settings["config"] = tmp
+      else
+        config_string = get("config").gsub('\n','').split.join(" ")
+        @test_settings["config"] = config_string
+      end
+    end
+  end
+
+  def get(key)
+    if @test_settings.key?(key)
+      @test_settings[key]
+    else
+      @suite_settings[key]
+    end
+  end
+  
+  def verbose_mode?
+    @suite_settings["verbose_mode"]
+  end  
+
+  def is_set?(key)
+    @suite_settings.key?(key) || @test_settings.key?(key)
+  end
+
+  def feature_flag
+    @suite_settings["feature_flag"].to_s.strip
+  end
+
+  def feature_config_dir
+    feature = feature_flag
+    feature.empty? ? nil: File.join(FIXTURES_DIR, feature)
+  end
+end
diff --git a/qa/integration/integration_tests.gemspec b/qa/integration/integration_tests.gemspec
new file mode 100644
index 00000000000..8a95b4a015a
--- /dev/null
+++ b/qa/integration/integration_tests.gemspec
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name        = 'logstash-integration-tests'
+  s.version     = '0.1.0'
+  s.licenses    = ['Apache License (2.0)']
+  s.summary     = "Tests LS binary"
+  s.description = "This is a Logstash integration test helper gem"
+  s.authors     = [ "Elastic"]
+  s.email       = 'info@elastic.co'
+  s.homepage    = "http://www.elastic.co/guide/en/logstash/current/index.html"
+
+  # Files
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Gem dependencies
+  s.add_development_dependency 'elasticsearch'
+  s.add_development_dependency 'childprocess'
+  s.add_development_dependency 'rspec-wait'
+  s.add_development_dependency 'manticore'
+  s.add_development_dependency 'stud'
+  s.add_development_dependency 'pry'
+  s.add_development_dependency 'logstash-devutils'
+  s.add_development_dependency 'flores'
+  s.add_development_dependency 'rubyzip'
+end
diff --git a/qa/integration/services/elasticsearch_service.rb b/qa/integration/services/elasticsearch_service.rb
new file mode 100644
index 00000000000..66963aca5de
--- /dev/null
+++ b/qa/integration/services/elasticsearch_service.rb
@@ -0,0 +1,12 @@
+require 'elasticsearch'
+
+class ElasticsearchService < Service
+  def initialize(settings)
+    super("elasticsearch", settings)
+  end
+
+  def get_client
+    Elasticsearch::Client.new(:hosts => "localhost:9200")
+  end
+
+end
\ No newline at end of file
diff --git a/qa/integration/services/elasticsearch_setup.sh b/qa/integration/services/elasticsearch_setup.sh
new file mode 100755
index 00000000000..0f916c9cce0
--- /dev/null
+++ b/qa/integration/services/elasticsearch_setup.sh
@@ -0,0 +1,43 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${ES_VERSION+1}" ]; then
+  echo "Elasticsearch version is $ES_VERSION"
+  version=$ES_VERSION
+else
+   version=5.0.1
+fi
+
+ES_HOME=$INSTALL_DIR/elasticsearch
+
+setup_es() {
+  if [ ! -d $ES_HOME ]; then
+      local version=$1
+      download_url=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-$version.tar.gz
+      curl -sL $download_url > $INSTALL_DIR/elasticsearch.tar.gz
+      mkdir $ES_HOME
+      tar -xzf $INSTALL_DIR/elasticsearch.tar.gz --strip-components=1 -C $ES_HOME/.
+      rm $INSTALL_DIR/elasticsearch.tar.gz
+  fi
+}
+
+start_es() {
+  es_args=$@
+  $ES_HOME/bin/elasticsearch $es_args -p $ES_HOME/elasticsearch.pid > /tmp/elasticsearch.log 2>/dev/null &
+  count=120
+  echo "Waiting for elasticsearch to respond..."
+  while ! curl --silent localhost:9200 && [[ $count -ne 0 ]]; do
+      count=$(( $count - 1 ))
+      [[ $count -eq 0 ]] && return 1
+      sleep 1
+  done
+  echo "Elasticsearch is Up !"
+  return 0
+}
+
+setup_install_dir
+setup_es $version
+start_es
diff --git a/qa/integration/services/elasticsearch_teardown.sh b/qa/integration/services/elasticsearch_teardown.sh
new file mode 100755
index 00000000000..f8e4dd51139
--- /dev/null
+++ b/qa/integration/services/elasticsearch_teardown.sh
@@ -0,0 +1,15 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+ES_HOME=$INSTALL_DIR/elasticsearch
+
+stop_es() {
+    pid=$(cat $ES_HOME/elasticsearch.pid)
+    [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
+    kill -SIGTERM $pid
+}
+
+stop_es
\ No newline at end of file
diff --git a/qa/integration/services/filebeat_service.rb b/qa/integration/services/filebeat_service.rb
new file mode 100644
index 00000000000..9904499dd16
--- /dev/null
+++ b/qa/integration/services/filebeat_service.rb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+class FilebeatService < Service
+  FILEBEAT_CMD = [File.join(File.dirname(__FILE__), "installed", "filebeat", "filebeat"), "-c"]
+
+  class BackgroundProcess
+    def initialize(cmd)
+      @client_out = Stud::Temporary.file
+      @client_out.sync
+
+      @process = ChildProcess.build(*cmd)
+      @process.duplex = true
+      @process.io.stdout = @process.io.stderr = @client_out
+      ChildProcess.posix_spawn = true
+    end
+
+    def start
+      @process.start
+      sleep(0.1)
+      self
+    end
+
+    def execution_output
+      @client_out.rewind
+
+      # can be used to helper debugging when a test fails
+      @execution_output = @client_out.read
+    end
+
+    def stop
+      begin
+        @process.poll_for_exit(5)
+      rescue ChildProcess::TimeoutError
+        Process.kill("KILL", @process.pid)
+      end
+    end
+  end
+
+  def initialize(settings)
+    super("filebeat", settings)
+  end
+
+  def run(config_path)
+    cmd = FILEBEAT_CMD.dup << config_path
+    puts "Starting Filebeat with #{cmd.join(" ")}"
+    @process = BackgroundProcess.new(cmd).start
+  end
+
+  def stop
+    @process.stop
+  end
+end
diff --git a/qa/integration/services/filebeat_setup.sh b/qa/integration/services/filebeat_setup.sh
new file mode 100755
index 00000000000..e9ce2ae821a
--- /dev/null
+++ b/qa/integration/services/filebeat_setup.sh
@@ -0,0 +1,38 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${FILEBEAT_VERSION}" ]; then
+  echo "Filebeat version is $FILEBEAT_VERSION"
+  version=$FILEBEAT_VERSION
+else
+  version=5.0.1
+fi
+
+FB_HOME=$INSTALL_DIR/filebeat
+
+setup_fb() {
+    local version=$1
+    platform=`uname -s | tr '[:upper:]' '[:lower:]'`
+    architecture=`uname -m | tr '[:upper:]' '[:lower:]'`
+    download_url=https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-$version-$platform-$architecture.tar.gz
+    curl -sL $download_url > $INSTALL_DIR/filebeat.tar.gz
+    mkdir $FB_HOME
+    tar -xzf $INSTALL_DIR/filebeat.tar.gz --strip-components=1 -C $FB_HOME/.
+    rm $INSTALL_DIR/filebeat.tar.gz
+}
+
+generate_certificate() {
+    target_directory=$current_dir/../fixtures/certificates
+    mkdir -p $target_directory
+    openssl req -subj '/CN=localhost/' -x509 -days $((100 * 365)) -batch -nodes -newkey rsa:2048 -keyout $target_directory/certificate.key -out $target_directory/certificate.crt
+}
+
+setup_install_dir
+
+if [ ! -d $FB_HOME ]; then
+    generate_certificate
+    setup_fb $version
+fi
diff --git a/qa/integration/services/helpers.sh b/qa/integration/services/helpers.sh
new file mode 100644
index 00000000000..dbf970a3eba
--- /dev/null
+++ b/qa/integration/services/helpers.sh
@@ -0,0 +1,30 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+INSTALL_DIR=$current_dir/installed
+PORT_WAIT_COUNT=20
+
+setup_install_dir() {
+    if [[ ! -d "$INSTALL_DIR" ]]; then
+        mkdir $INSTALL_DIR
+    fi
+}
+
+wait_for_port() {
+    count=$PORT_WAIT_COUNT
+    port=$1
+    while ! nc -z localhost $port && [[ $count -ne 0 ]]; do
+        count=$(( $count - 1 ))
+        [[ $count -eq 0 ]] && return 1
+        sleep 0.5
+    done
+    # just in case, one more time
+    nc -z localhost $port
+}
+
+clean_install_dir() {
+    if [[ -d "$INSTALL_DIR" ]]; then
+        rm -rf $INSTALL_DIR
+    fi
+}
diff --git a/qa/integration/services/http_proxy_service.rb b/qa/integration/services/http_proxy_service.rb
new file mode 100644
index 00000000000..66c51b2484b
--- /dev/null
+++ b/qa/integration/services/http_proxy_service.rb
@@ -0,0 +1,6 @@
+# encoding: utf-8
+class Http_proxyService < Service
+  def initialize(settings)
+    super("http_proxy", settings)
+  end
+end
diff --git a/qa/integration/services/http_proxy_setup.sh b/qa/integration/services/http_proxy_setup.sh
new file mode 100755
index 00000000000..27574111d6d
--- /dev/null
+++ b/qa/integration/services/http_proxy_setup.sh
@@ -0,0 +1,25 @@
+#!/bin/bash
+set -ex
+sudo apt-get install -y squid3 net-tools
+sudo iptables -A OUTPUT -p tcp --dport 80 -m owner --uid-owner proxy -j ACCEPT
+sudo iptables -A OUTPUT -p tcp --dport 443 -m owner --uid-owner proxy -j ACCEPT
+sudo iptables -A OUTPUT -p tcp --dport 80 -m owner --uid-owner root -j ACCEPT
+sudo iptables -A OUTPUT -p tcp --dport 443 -m owner --uid-owner root -j ACCEPT
+sudo iptables -A OUTPUT -p tcp --dport 443 -j REJECT
+sudo iptables -A OUTPUT -p tcp --dport 80 -j REJECT
+
+echo "Connecting to a remote host should fails without proxy"
+curl -I --silent "http://rubygems.org" || echo "Success"
+
+echo "Connecting to a remote host with a valid proxy should succeed"
+export http_proxy=http://localhost:3128
+export https_proxy=http://localhost:3128
+export HTTP_PROXY=http://localhost:3128
+export HTTPS_PROXY=http://localhost:3128
+curl -I --silent "https://rubygems.org" || echo "Success"
+
+echo "Unset the default variables"
+unset http_proxy
+unset https_proxy
+unset HTTP_PROXY
+unset HTTPS_PROXY
diff --git a/qa/integration/services/http_proxy_teardown.sh b/qa/integration/services/http_proxy_teardown.sh
new file mode 100755
index 00000000000..db6ee2e9974
--- /dev/null
+++ b/qa/integration/services/http_proxy_teardown.sh
@@ -0,0 +1,5 @@
+#!/bin/bash
+set -ex
+
+echo "Removing all the chain"
+sudo iptables -F OUTPUT
diff --git a/qa/integration/services/kafka_service.rb b/qa/integration/services/kafka_service.rb
new file mode 100644
index 00000000000..71908f9acbb
--- /dev/null
+++ b/qa/integration/services/kafka_service.rb
@@ -0,0 +1,7 @@
+require_relative "service"
+
+class KafkaService < Service
+  def initialize(settings)
+    super("kafka", settings)
+  end
+end
diff --git a/qa/integration/services/kafka_setup.sh b/qa/integration/services/kafka_setup.sh
new file mode 100755
index 00000000000..3a3b283300d
--- /dev/null
+++ b/qa/integration/services/kafka_setup.sh
@@ -0,0 +1,68 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${KAFKA_VERSION+1}" ]; then
+    echo "KAFKA_VERSION is $KAFKA_VERSION"
+    version=$KAFKA_VERSION
+else
+    version=0.10.0.1
+fi
+
+KAFKA_HOME=$INSTALL_DIR/kafka
+KAFKA_TOPIC=logstash_topic_plain
+KAFKA_MESSAGES=37
+KAFKA_LOGS_DIR=/tmp/ls_integration/kafka-logs
+
+setup_kafka() {
+    local version=$1
+    if [ ! -d $KAFKA_HOME ]; then
+        echo "Downloading Kafka version $version"
+        curl -s -o $INSTALL_DIR/kafka.tgz "http://ftp.wayne.edu/apache/kafka/$version/kafka_2.11-$version.tgz"
+        mkdir $KAFKA_HOME && tar xzf $INSTALL_DIR/kafka.tgz -C $KAFKA_HOME --strip-components 1
+        rm $INSTALL_DIR/kafka.tgz
+    fi
+}
+
+start_kafka() {
+    echo "Starting ZooKeeper"
+    $KAFKA_HOME/bin/zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
+    wait_for_port 2181
+    echo "Starting Kafka broker"
+    $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties --override delete.topic.enable=true --override log.dirs=$KAFKA_LOGS_DIR --override log.flush.interval.ms=200
+    wait_for_port 9092
+}
+
+wait_for_messages() {
+    local count=10
+    local read_lines=0
+    
+    echo "Checking if Kafka topic has been populated with data"
+    while [[ $read_lines -ne $KAFKA_MESSAGES ]] && [[ $count -ne 0 ]]; do
+        read_lines=`$KAFKA_HOME/bin/kafka-console-consumer.sh --topic $KAFKA_TOPIC --new-consumer --bootstrap-server localhost:9092 --from-beginning --max-messages $KAFKA_MESSAGES --timeout-ms 10000 | wc -l`
+        count=$(( $count - 1 ))
+        [[ $count -eq 0 ]] && return 1
+        sleep 0.5
+        ls -lrt $KAFKA_LOGS_DIR/$KAFKA_TOPIC-0/
+    done
+    echo "Kafka topic has been populated with test data"
+}
+
+setup_install_dir
+setup_kafka $version
+start_kafka
+# Set up topics
+$KAFKA_HOME/bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic $KAFKA_TOPIC --zookeeper localhost:2181
+# check topic got created
+num_topic=`$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper localhost:2181 | grep $KAFKA_TOPIC | wc -l`
+[[ $num_topic -eq 1 ]]
+# Add test messages to the newly created topic
+cp $current_dir/../fixtures/how_sample.input $KAFKA_HOME
+[[ ! -s  how_sample.input ]]
+$KAFKA_HOME/bin/kafka-console-producer.sh --topic $KAFKA_TOPIC --broker-list localhost:9092 < $KAFKA_HOME/how_sample.input
+echo "Kafka load status code $?"
+# Wait until broker has all messages
+wait_for_messages
+echo "Kafka Setup complete"
diff --git a/qa/integration/services/kafka_teardown.sh b/qa/integration/services/kafka_teardown.sh
new file mode 100755
index 00000000000..0d10cffe844
--- /dev/null
+++ b/qa/integration/services/kafka_teardown.sh
@@ -0,0 +1,22 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+KAFKA_HOME=$INSTALL_DIR/kafka
+
+stop_kafka() {
+    echo "Stopping Kafka broker"
+    $KAFKA_HOME/bin/kafka-server-stop.sh
+    echo "Stopping zookeeper"
+    $KAFKA_HOME/bin/zookeeper-server-stop.sh
+}
+
+# delete test topic
+echo "Deleting test topic in Kafka"
+$KAFKA_HOME/bin/kafka-topics.sh --delete --topic logstash_topic_plain --zookeeper localhost:2181 --if-exists
+stop_kafka
+rm -rf /tmp/ls_integration/kafka-logs
+rm -rf /tmp/zookeeper
+
diff --git a/qa/integration/services/logstash_service.rb b/qa/integration/services/logstash_service.rb
new file mode 100644
index 00000000000..31e7e0268cb
--- /dev/null
+++ b/qa/integration/services/logstash_service.rb
@@ -0,0 +1,274 @@
+require_relative "monitoring_api"
+
+require "childprocess"
+require "bundler"
+require "socket"
+require "tempfile"
+require 'yaml'
+
+# A locally started Logstash service
+class LogstashService < Service
+
+  LS_ROOT_DIR = File.join("..", "..", "..", "..")
+  LS_VERSION_FILE = File.expand_path(File.join(LS_ROOT_DIR, "versions.yml"), __FILE__)
+  LS_BUILD_DIR = File.join(LS_ROOT_DIR, "build")
+  LS_BIN = File.join("bin", "logstash")
+  LS_CONFIG_FILE = File.join("config", "logstash.yml")
+  SETTINGS_CLI_FLAG = "--path.settings"
+
+  STDIN_CONFIG = "input {stdin {}} output { }"
+  RETRY_ATTEMPTS = 10
+
+  @process = nil
+  
+  attr_reader :logstash_home
+  attr_reader :default_settings_file
+  attr_writer :env_variables
+
+  def initialize(settings)
+    super("logstash", settings)
+
+    # if you need to point to a LS in different path
+    if @settings.is_set?("ls_home_abs_path")
+      @logstash_home = @settings.get("ls_home_abs_path")
+    else
+      # use the LS which was just built in source repo
+      ls_version_file = YAML.load_file(LS_VERSION_FILE)
+      ls_file = "logstash-" + ls_version_file["logstash"]
+      # First try without the snapshot if it's there
+      @logstash_home = File.expand_path(File.join(LS_BUILD_DIR, ls_file), __FILE__)
+      @logstash_home += "-SNAPSHOT" unless Dir.exists?(@logstash_home)
+
+      puts "Using #{@logstash_home} as LS_HOME"
+      @logstash_bin = File.join("#{@logstash_home}", LS_BIN)
+      raise "Logstash binary not found in path #{@logstash_home}" unless File.file? @logstash_bin
+    end
+    
+    @default_settings_file = File.join(@logstash_home, LS_CONFIG_FILE)
+    @monitoring_api = MonitoringAPI.new
+  end
+
+  def alive?
+    if @process.nil? || @process.exited?
+      raise "Logstash process is not up because of an errot, or it stopped"
+    else
+      @process.alive?
+    end
+  end
+  
+  def exited?
+    @process.exited?
+  end
+  
+  def exit_code
+    @process.exit_code
+  end  
+
+  # Starts a LS process in background with a given config file
+  # and shuts it down after input is completely processed
+  def start_background(config_file)
+    spawn_logstash("-e", config_file)
+  end
+
+  # Given an input this pipes it to LS. Expects a stdin input in LS
+  def start_with_input(config, input)
+    Bundler.with_clean_env do
+      `cat #{input} | #{@logstash_bin} -e \'#{config}\'`
+    end
+  end
+
+  def start_with_config_string(config)
+    spawn_logstash("-e", "#{config} ")
+  end
+
+  # Can start LS in stdin and can send messages to stdin
+  # Useful to test metrics and such
+  def start_with_stdin
+    puts "Starting Logstash #{@logstash_bin} -e #{STDIN_CONFIG}"
+    Bundler.with_clean_env do
+      out = Tempfile.new("duplex")
+      out.sync = true
+      @process = build_child_process("-e", STDIN_CONFIG)
+      # pipe STDOUT and STDERR to a file
+      @process.io.stdout = @process.io.stderr = out
+      @process.duplex = true
+      @process.start
+      wait_for_logstash
+      puts "Logstash started with PID #{@process.pid}" if alive?
+    end
+  end
+
+  def write_to_stdin(input)
+    if alive?
+      @process.io.stdin.puts(input)
+    end
+  end
+
+  # Spawn LS as a child process
+  def spawn_logstash(*args)
+    Bundler.with_clean_env do
+      @process = build_child_process(*args)
+      @env_variables.map { |k, v|  @process.environment[k] = v} unless @env_variables.nil?
+      @process.io.inherit!
+      @process.start
+      wait_for_logstash
+      puts "Logstash started with PID #{@process.pid}" if @process.alive?
+    end
+  end
+
+  def build_child_process(*args)
+    feature_config_dir = @settings.feature_config_dir
+    # if we are using a feature flag and special settings dir to enable it, use it
+    # If some tests is explicitly using --path.settings, ignore doing this, because the tests
+    # chose to overwrite it.
+    if feature_config_dir && !args.include?(SETTINGS_CLI_FLAG)
+      args << "--path.settings"
+      args << feature_config_dir
+      puts "Found feature flag. Starting LS using --path.settings #{feature_config_dir}"
+    end
+    puts "Starting Logstash: #{@logstash_bin} #{args}"
+    ChildProcess.build(@logstash_bin, *args)
+  end
+
+  def teardown
+    if !@process.nil?
+      # todo: put this in a sleep-wait loop to kill it force kill
+      @process.io.stdin.close rescue nil
+      @process.stop
+      @process = nil
+    end
+  end
+
+  # check if LS HTTP port is open
+  def is_port_open?
+    begin
+      s = TCPSocket.open("localhost", 9600)
+      s.close
+      return true
+    rescue Errno::ECONNREFUSED, Errno::EHOSTUNREACH
+      return false
+    end
+  end
+
+  def monitoring_api
+    raise "Logstash is not up, but you asked for monitoring API" unless alive?
+    @monitoring_api
+  end
+
+  # Wait until LS is started by repeatedly doing a socket connection to HTTP port
+  def wait_for_logstash
+    tries = RETRY_ATTEMPTS
+    while tries > 0
+      if is_port_open?
+        break
+      else
+        sleep 1
+      end
+      tries -= 1
+    end
+  end
+  
+  # this method only overwrites existing config with new config
+  # it does not assume that LS pipeline is fully reloaded after a 
+  # config change. It is up to the caller to validate that.
+  def reload_config(initial_config_file, reload_config_file)
+    FileUtils.cp(reload_config_file, initial_config_file)
+  end  
+  
+  def get_version
+    `#{@logstash_bin} --version`
+  end
+  
+  def get_version_yml
+    LS_VERSION_FILE
+  end   
+  
+  def process_id
+    @process.pid
+  end
+
+  def application_settings_file
+    feature_config_dir = @settings.feature_config_dir
+    unless feature_config_dir
+      @default_settings_file
+    else
+      File.join(feature_config_dir, "logstash.yml")
+    end
+  end
+
+  def plugin_cli
+    PluginCli.new(@logstash_home)
+  end
+
+  def lock_file
+    File.join(@logstash_home, "Gemfile.jruby-1.9.lock")
+  end
+
+  class PluginCli
+    class ProcessStatus < Struct.new(:exit_code, :stderr_and_stdout); end
+
+    TIMEOUT_MAXIMUM = 60 * 10 # 10mins.
+    LOGSTASH_PLUGIN = File.join("bin", "logstash-plugin")
+
+    attr_reader :logstash_plugin
+
+    def initialize(logstash_home)
+      @logstash_plugin = File.join(logstash_home, LOGSTASH_PLUGIN)
+      @logstash_home = logstash_home
+    end
+
+    def remove(plugin_name)
+      run("remove #{plugin_name}")
+    end
+
+    def prepare_offline_pack(plugins, output_zip = nil)
+      plugins = Array(plugins)
+
+      if output_zip.nil?
+        run("prepare-offline-pack #{plugins.join(" ")}")
+      else
+        run("prepare-offline-pack --output #{output_zip} #{plugins.join(" ")}")
+      end
+    end
+
+    def list(plugin_name, verbose = false)
+      run("list #{plugin_name} #{verbose ? "--verbose" : ""}")
+    end
+
+    def install(plugin_name)
+      run("install #{plugin_name}")
+    end
+
+    def run_raw(cmd_parameters, change_dir = true, environment = {})
+      out = Tempfile.new("content")
+      out.sync = true
+
+      parts = cmd_parameters.split(" ")
+      cmd = parts.shift
+
+      process = ChildProcess.build(cmd, *parts)
+      environment.each do |k, v|
+        process.environment[k] = v
+      end
+      process.io.stdout = process.io.stderr = out
+
+      Bundler.with_clean_env do
+        if change_dir
+          Dir.chdir(@logstash_home) do
+            process.start
+          end
+        else
+          process.start
+        end
+      end
+
+      process.poll_for_exit(TIMEOUT_MAXIMUM)
+      out.rewind
+      ProcessStatus.new(process.exit_code, out.read)
+    end
+
+    def run(command)
+      run_raw("#{logstash_plugin} #{command}")
+    end
+  end
+end
diff --git a/qa/integration/services/monitoring_api.rb b/qa/integration/services/monitoring_api.rb
new file mode 100644
index 00000000000..e14f56e3db9
--- /dev/null
+++ b/qa/integration/services/monitoring_api.rb
@@ -0,0 +1,35 @@
+require "manticore"
+require "json"
+
+# Convenience class to interact with the HTTP monitoring APIs
+class MonitoringAPI
+
+  def pipeline_stats
+    resp = Manticore.get("http://localhost:9600/_node/stats/pipeline").body
+    stats_response = JSON.parse(resp)
+    stats_response["pipeline"]
+  end
+
+  def event_stats
+    stats = pipeline_stats
+    stats["events"]
+  end
+
+  def version
+    request = @agent.get("http://localhost:9600/")
+    response = request.execute
+    r = JSON.parse(response.body.read)
+    r["version"]
+  end
+  
+  def node_info
+    resp = Manticore.get("http://localhost:9600/_node").body
+    JSON.parse(resp)
+  end
+
+  def node_stats
+    resp = Manticore.get("http://localhost:9600/_node/stats").body
+    JSON.parse(resp)
+  end
+
+end
diff --git a/qa/integration/services/service.rb b/qa/integration/services/service.rb
new file mode 100644
index 00000000000..63b898d30d9
--- /dev/null
+++ b/qa/integration/services/service.rb
@@ -0,0 +1,32 @@
+# Base class for a service like Kafka, ES, Logstash
+class Service
+
+  attr_reader :settings
+
+  def initialize(name, settings)
+    @name = name
+    @settings = settings
+    @setup_script = File.expand_path("../#{name}_setup.sh", __FILE__)
+    @teardown_script = File.expand_path("../#{name}_teardown.sh", __FILE__)
+  end
+
+  def setup
+    puts "Setting up #{@name} service"
+    if File.exists?(@setup_script)
+      `#{@setup_script}`
+    else
+      puts "Setup script not found for #{@name}"
+    end
+    puts "#{@name} service setup complete"
+  end
+
+  def teardown
+    puts "Tearing down #{@name} service"
+    if File.exists?(@setup_script)
+      `#{@teardown_script}`
+    else
+      puts "Teardown script not found for #{@name}"
+    end
+    puts "#{@name} service teardown complete"
+  end
+end
diff --git a/qa/integration/services/service_locator.rb b/qa/integration/services/service_locator.rb
new file mode 100644
index 00000000000..ff537ebcd2a
--- /dev/null
+++ b/qa/integration/services/service_locator.rb
@@ -0,0 +1,30 @@
+# encoding: utf-8
+require_relative "service"
+
+# This is a registry used in Fixtures so a test can get back any service class
+# at runtime
+# All new services should register here
+class ServiceLocator
+  FILE_PATTERN = "_service.rb"
+
+  def initialize(settings)
+    @services = {}
+    available_services do |name, klass|
+      @services[name] = klass.new(settings)
+    end
+  end
+
+  def get_service(name)
+    @services.fetch(name)
+  end
+
+  def available_services
+    Dir.glob(File.join(File.dirname(__FILE__), "*#{FILE_PATTERN}")).each do |f|
+      require f
+      basename = File.basename(f).gsub(/#{FILE_PATTERN}$/, "")
+      service_name = basename.downcase
+      klass = Object.const_get("#{service_name.capitalize}Service")
+      yield service_name, klass
+    end
+  end
+end
diff --git a/qa/integration/specs/01_logstash_bin_smoke_spec.rb b/qa/integration/specs/01_logstash_bin_smoke_spec.rb
new file mode 100644
index 00000000000..03a2c500dfb
--- /dev/null
+++ b/qa/integration/specs/01_logstash_bin_smoke_spec.rb
@@ -0,0 +1,183 @@
+require_relative '../framework/fixture'
+
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+require "yaml"
+require 'json'
+require 'open-uri'
+
+describe "Test Logstash instance" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+    # used in multiple LS tests
+    @ls1 = @fixture.get_service("logstash")
+    @ls2 = LogstashService.new(@fixture.settings)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+
+  after(:each) {
+    @ls1.teardown
+    @ls2.teardown
+  }
+
+  let(:file_config1) { Stud::Temporary.file.path }
+  let(:file_config2) { Stud::Temporary.file.path }
+  let(:file_config3) { Stud::Temporary.file.path }
+
+  let(:num_retries) { 50 }
+  let(:config1) { config_to_temp_file(@fixture.config("root", { :port => port1, :random_file => file_config1 })) }
+  let(:config2) { config_to_temp_file(@fixture.config("root", { :port => port2 , :random_file => file_config2 })) }
+  let(:config3) { config_to_temp_file(@fixture.config("root", { :port => port3, :random_file => file_config3 })) }
+  let(:port1) { random_port }
+  let(:port2) { random_port }
+  let(:port3) { random_port }
+
+  let(:persistent_queue_settings) { { "queue.type" => "persisted" } }
+
+  it "can start the embedded http server on default port 9600" do
+    @ls1.start_with_stdin
+    try(num_retries) do
+      expect(is_port_open?(9600)).to be(true)
+    end
+  end
+
+  context "multiple instances" do
+    it "cannot be started on the same box with the same path.data" do
+      tmp_path = Stud::Temporary.pathname
+      tmp_data_path = File.join(tmp_path, "data")
+      FileUtils.mkdir_p(tmp_data_path)
+      @ls1.spawn_logstash("-f", config1, "--path.data", tmp_data_path)
+      sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
+      expect(is_port_open?(9600)).to be true
+
+      @ls2.spawn_logstash("-f", config2, "--path.data", tmp_data_path)
+      try(num_retries) do
+        expect(@ls2.exited?).to be(true)
+      end
+      expect(@ls2.exit_code).to be(1)
+    end
+
+    it "can be started on the same box with different path.data" do
+      tmp_path_1 = Stud::Temporary.pathname
+      tmp_data_path_1 = File.join(tmp_path_1, "data")
+      FileUtils.mkdir_p(tmp_data_path_1)
+      tmp_path_2 = Stud::Temporary.pathname
+      tmp_data_path_2 = File.join(tmp_path_2, "data")
+      FileUtils.mkdir_p(tmp_data_path_2)
+      @ls1.spawn_logstash("-f", config1, "--path.data", tmp_data_path_1)
+      sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
+      expect(is_port_open?(9600)).to be true
+
+      @ls2.spawn_logstash("-f", config2, "--path.data", tmp_data_path_2)
+      sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started successfully at this point
+      expect(@ls2.exited?).to be(false)
+    end
+
+    it "can be started on the same box with automatically trying different ports for HTTP server" do
+      if @ls2.settings.feature_flag != "persistent_queues"
+        @ls1.spawn_logstash("-f", config1)
+        sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
+        expect(is_port_open?(9600)).to be true
+
+        puts "will try to start the second LS instance on 9601"
+
+        # bring up new LS instance
+        tmp_path = Stud::Temporary.pathname
+        tmp_data_path = File.join(tmp_path, "data")
+        FileUtils.mkdir_p(tmp_data_path)
+        @ls2.spawn_logstash("-f", config2, "--path.data", tmp_data_path)
+        sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started successfully at this point
+        expect(is_port_open?(9601)).to be true
+        expect(@ls1.process_id).not_to eq(@ls2.process_id)
+      else
+        # Make sure that each instance use a different `path.data`
+        path = Stud::Temporary.pathname
+        FileUtils.mkdir_p(File.join(path, "data"))
+        data = File.join(path, "data")
+        settings = persistent_queue_settings.merge({ "path.data" => data })
+        IO.write(File.join(path, "logstash.yml"), YAML.dump(settings))
+
+        @ls1.spawn_logstash("--path.settings", path, "-f", config1)
+        sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
+        expect(is_port_open?(9600)).to be true
+
+        puts "will try to start the second LS instance on 9601"
+
+        # bring up new LS instance
+        path = Stud::Temporary.pathname
+        FileUtils.mkdir_p(File.join(path, "data"))
+        data = File.join(path, "data")
+        settings = persistent_queue_settings.merge({ "path.data" => data })
+        IO.write(File.join(path, "logstash.yml"), YAML.dump(settings))
+        @ls2.spawn_logstash("--path.settings", path, "-f", config2)
+        sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started successfully at this point
+        expect(is_port_open?(9601)).to be true
+
+        expect(@ls1.process_id).not_to eq(@ls2.process_id)
+      end
+    end
+  end
+
+  it "gets the right version when asked" do
+    expected = YAML.load_file(LogstashService::LS_VERSION_FILE)
+    expect(@ls1.get_version.strip).to eq("logstash #{expected['logstash']}")
+  end
+
+  it "should still merge when -e is specified and -f has no valid config files" do
+    config_string = "input { tcp { port => #{port1} } }"
+    @ls1.spawn_logstash("-e", config_string, "-f" "/tmp/foobartest")
+    @ls1.wait_for_logstash
+
+    try(20) do
+      expect(is_port_open?(port1)).to be true
+    end
+  end
+
+  it "should not start when -e is not specified and -f has no valid config files" do
+    @ls2.spawn_logstash("-e", "", "-f" "/tmp/foobartest")
+    try(num_retries) do
+      expect(is_port_open?(9600)).to be_falsey
+    end
+  end
+
+  it "should merge config_string when both -f and -e is specified" do
+    config_string = "input { tcp { port => #{port1} } }"
+    @ls1.spawn_logstash("-e", config_string, "-f", config3)
+    @ls1.wait_for_logstash
+
+    # Both ports should be reachable
+    try(20) do
+      expect(is_port_open?(port1)).to be true
+    end
+
+    try(20) do
+      expect(is_port_open?(port3)).to be true
+    end
+  end
+
+  def get_id
+    # make sure logstash is up and running when calling this
+    JSON.parse(open("http://localhost:9600/").read)["id"]
+  end
+
+  it "should keep the same id between restarts" do
+    config_string = "input { tcp { port => #{port1} } }"
+
+    start_ls = lambda {
+      @ls1.spawn_logstash("-e", config_string, "-f", config3)
+      @ls1.wait_for_logstash
+    }
+    start_ls.call()
+    # we use a try since logstash may have started but the webserver may not yet
+    first_id = try(num_retries) { get_id }
+    @ls1.teardown
+    start_ls.call()
+    second_id = try(num_retries) { get_id }
+    expect(first_id).to eq(second_id)
+  end
+end
diff --git a/qa/integration/specs/beats_input_spec.rb b/qa/integration/specs/beats_input_spec.rb
new file mode 100644
index 00000000000..926f2cbb678
--- /dev/null
+++ b/qa/integration/specs/beats_input_spec.rb
@@ -0,0 +1,148 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require "stud/temporary"
+require "stud/try"
+require "rspec/wait"
+require "yaml"
+require "fileutils"
+
+describe "Beat Input" do
+  before(:all) do
+    @fixture = Fixture.new(__FILE__)
+  end
+
+  after :each do
+    logstash_service.teardown
+    filebeat_service.stop
+  end
+
+  before :each do
+    FileUtils.mkdir_p(File.dirname(registry_file))
+  end
+
+  let(:max_retry) { 120 }
+  let(:registry_file) { File.join(Stud::Temporary.pathname, "registry") }
+  let(:logstash_service) { @fixture.get_service("logstash") }
+  let(:filebeat_service) { @fixture.get_service("filebeat") }
+  let(:log_path) do
+    tmp_path = Stud::Temporary.file.path #get around ignore older completely
+    source = File.expand_path(@fixture.input)
+    FileUtils.cp(source, tmp_path)
+    tmp_path
+  end
+  let(:number_of_events) do
+    File.open(log_path, "r").readlines.size
+  end
+
+  shared_examples "send events" do
+    let(:filebeat_config_path) do
+      file = Stud::Temporary.file
+      file.write(YAML.dump(filebeat_config))
+      file.close
+      file.path
+    end
+
+
+    it "successfully send events" do
+      logstash_service.start_background(logstash_config)
+      process = filebeat_service.run(filebeat_config_path)
+
+      # It can take some delay for filebeat to connect to logstash and start sending data.
+      # Its possible that logstash isn't completely initialized here, we can get "Connection Refused"
+      begin
+        sleep(1) while (result = logstash_service.monitoring_api.event_stats).nil?
+      rescue
+        retry
+      end
+
+      Stud.try(max_retry.times, RSpec::Expectations::ExpectationNotMetError) do
+         result = logstash_service.monitoring_api.event_stats
+         expect(result["in"]).to eq(number_of_events)
+      end
+    end
+  end
+
+  context "Without TLS" do
+    let(:logstash_config) { @fixture.config("without_tls") }
+    let(:filebeat_config) do
+      {
+        "filebeat" => {
+          "prospectors" => [{ "paths" => [log_path], "input_type" => "log" }],
+          "registry_file" => registry_file,
+          "scan_frequency" => "1s"
+        },
+        "output" => {
+          "logstash" => { "hosts" => ["localhost:5044"] },
+          "logging" => { "level" => "debug" }
+        }
+      }
+    end
+
+    include_examples "send events"
+  end
+
+  context "With TLS" do
+    let(:certificate_directory) { File.expand_path(File.join(File.dirname(__FILE__), "..", "fixtures", "certificates")) }
+    let(:certificate) { File.join(certificate_directory, "certificate.crt") }
+    let(:ssl_key) { File.join(certificate_directory, "certificate.key") }
+    let(:certificate_authorities) { [certificate] }
+
+    context "Server auth" do
+      let(:logstash_config) { @fixture.config("tls_server_auth", { :ssl_certificate => certificate, :ssl_key => ssl_key}) }
+      let(:filebeat_config) do
+        {
+          "filebeat" => {
+            "prospectors" => [{ "paths" => [log_path], "input_type" => "log" }],
+            "registry_file" => registry_file,
+            "scan_frequency" => "1s"
+          },
+          "output" => {
+            "logstash" => {
+              "hosts" => ["localhost:5044"],
+              "tls" => {
+                "certificate_authorities" => certificate_authorities
+              },
+              "ssl" => {
+                "certificate_authorities" => certificate_authorities
+              }
+            },
+            "logging" => { "level" => "debug" }
+          }
+        }
+      end
+
+      include_examples "send events"
+    end
+
+    context "Mutual auth" do
+      let(:logstash_config) { @fixture.config("tls_mutual_auth", { :ssl_certificate => certificate, :ssl_key => ssl_key}) }
+      let(:filebeat_config) do
+        {
+          "filebeat" => {
+            "prospectors" => [{ "paths" => [log_path], "input_type" => "log" }],
+            "registry_file" => registry_file,
+            "scan_frequency" => "1s"
+          },
+          "output" => {
+            "logstash" => {
+              "hosts" => ["localhost:5044"],
+              "tls" => {
+                "certificate_authorities" => certificate_authorities,
+                "certificate" => certificate,
+                "certificate_key" => ssl_key
+              },
+              "ssl" => {
+                "certificate_authorities" => certificate_authorities,
+                "certificate" => certificate,
+                "key" => ssl_key
+              }
+            },
+            "logging" => { "level" => "debug" }
+          }
+        }
+      end
+
+      include_examples "send events"
+    end
+  end
+end
diff --git a/qa/integration/specs/cli/http_proxy_install_spec.rb b/qa/integration/specs/cli/http_proxy_install_spec.rb
new file mode 100644
index 00000000000..92c5675f508
--- /dev/null
+++ b/qa/integration/specs/cli/http_proxy_install_spec.rb
@@ -0,0 +1,65 @@
+# encoding: utf-8
+require_relative "../../framework/fixture"
+require_relative "../../framework/settings"
+require_relative "../../services/logstash_service"
+require_relative "../../services/http_proxy_service"
+require_relative "../../framework/helpers"
+require "logstash/devutils/rspec/spec_helper"
+require "stud/temporary"
+require "fileutils"
+
+# Theses tests doesn't currently work on Travis, since we need to run them in a sudo
+# environment and we do that other tests are faillings. This is probably due to IPv4 vs IPv6 settings
+# in the VM vs the container.
+#
+# We are working to bring the test to our internal Jenkins environment.
+#
+# describe "(HTTP_PROXY) CLI > logstash-plugin install", :linux => true do
+#   before :all do
+#     @fixture = Fixture.new(__FILE__)
+#     @logstash_cli = @fixture.get_service("logstash").plugin_cli
+#     @http_proxy = @fixture.get_service("http_proxy")
+#   end
+
+#   before(:all) { @http_proxy.setup }
+#   after(:all) { @http_proxy.teardown }
+
+#   before do
+#     # Make sure we don't have any settings from a previous execution
+#     FileUtils.rm_rf(File.join(Dir.home, ".m2", "settings.xml"))
+#     FileUtils.rm_rf(File.join(Dir.home, ".m2", "repository"))
+#   end
+
+#   context "when installing plugins in an airgap environment" do
+#     context "when a proxy is not configured" do
+#       it "should fail" do
+#         environment = {
+#           "http_proxy" => nil,
+#           "https_proxy" => nil,
+#           "HTTP_PROXY" => nil,
+#           "HTTPS_PROXY" => nil,
+#         }
+
+#         execute = @logstash_cli.run_raw(cmd, true, environment)
+
+#         expect(execute.stderr_and_stdout).not_to match(/Installation successful/)
+#         expect(execute.exit_code).to eq(1)
+#       end
+#     end
+
+#     context "when a proxy is configured" do
+#       it "should allow me to install a plugin" do
+#         environment = {
+#           "http_proxy" => "http://localhost:3128",
+#           "https_proxy" => "http://localhost:3128"
+#         }
+
+#         cmd = "bin/logstash-plugin install --no-verify"
+#         execute = @logstash_cli.run_raw(cmd, true, environment)
+
+#         expect(execute.stderr_and_stdout).to match(/Installation successful/)
+#         expect(execute.exit_code).to eq(0)
+#       end
+#     end
+#   end
+# end
diff --git a/qa/integration/specs/cli/install_spec.rb b/qa/integration/specs/cli/install_spec.rb
new file mode 100644
index 00000000000..d1c02037c54
--- /dev/null
+++ b/qa/integration/specs/cli/install_spec.rb
@@ -0,0 +1,97 @@
+# encoding: utf-8
+require_relative "../../framework/fixture"
+require_relative "../../framework/settings"
+require_relative "../../services/logstash_service"
+require_relative "../../framework/helpers"
+require "logstash/devutils/rspec/spec_helper"
+require "stud/temporary"
+require "fileutils"
+
+def gem_in_lock_file?(pattern, lock_file)
+  content =  File.read(lock_file)
+  content.match(pattern)
+end
+
+describe "CLI > logstash-plugin install" do
+
+  before(:all) do
+    @fixture = Fixture.new(__FILE__)
+    @logstash = @fixture.get_service("logstash")
+    @logstash_plugin = @logstash.plugin_cli
+    @pack_directory =  File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "logstash-dummy-pack"))
+  end
+
+  shared_examples "install from a pack" do
+    let(:pack) { "file://#{File.join(@pack_directory, "logstash-dummy-pack.zip")}" }
+    let(:install_command) { "bin/logstash-plugin install" }
+    let(:change_dir) { true }
+
+    # When you are on anything by linux we won't disable the internet with seccomp
+    if RbConfig::CONFIG["host_os"] == "linux"
+      context "without internet connection (linux seccomp wrapper)" do
+
+        let(:offline_wrapper_path) { File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "offline_wrapper")) }
+
+        before do
+          Dir.chdir(offline_wrapper_path) do
+            system("make clean")
+            system("make")
+          end
+        end
+
+        let(:offline_wrapper_cmd) { File.join(offline_wrapper_path, "offline") }
+
+        it "successfully install the pack" do
+          execute = @logstash_plugin.run_raw("#{offline_wrapper_cmd} #{install_command} #{pack}", change_dir)
+
+          expect(execute.stderr_and_stdout).to match(/Install successful/)
+          expect(execute.exit_code).to eq(0)
+
+          installed = @logstash_plugin.list("logstash-output-secret")
+          expect(installed.stderr_and_stdout).to match(/logstash-output-secret/)
+
+          expect(gem_in_lock_file?(/gemoji/, @logstash.lock_file)).to be_truthy
+        end
+      end
+    else
+
+      context "with internet connection" do
+        it "successfully install the pack" do
+          execute = @logstash_plugin.run_raw("#{install_command} #{pack}", change_dir)
+
+          expect(execute.stderr_and_stdout).to match(/Install successful/)
+          expect(execute.exit_code).to eq(0)
+
+          installed = @logstash_plugin.list("logstash-output-secret")
+          expect(installed.stderr_and_stdout).to match(/logstash-output-secret/)
+
+          expect(gem_in_lock_file?(/gemoji/, @logstash.lock_file)).to be_truthy
+        end
+      end
+    end
+  end
+
+  context "pack" do
+    context "when the command is run in the `$LOGSTASH_HOME`" do
+      include_examples "install from a pack"
+    end
+
+    context "when the command is run outside of the `$LOGSTASH_HOME`" do
+      include_examples "install from a pack" do
+        let(:change_dir) { false }
+        let(:install_command) { "#{@logstash.logstash_home}/bin/logstash-plugin install" }
+
+        before :all do
+          @current = Dir.pwd
+          tmp = Stud::Temporary.pathname
+          FileUtils.mkdir_p(tmp)
+          Dir.chdir(tmp)
+        end
+
+        after :all do
+          Dir.chdir(@current)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/integration/specs/cli/prepare_offline_pack_spec.rb b/qa/integration/specs/cli/prepare_offline_pack_spec.rb
new file mode 100644
index 00000000000..c828b9914e1
--- /dev/null
+++ b/qa/integration/specs/cli/prepare_offline_pack_spec.rb
@@ -0,0 +1,94 @@
+# encoding: utf-8
+require_relative "../../framework/fixture"
+require_relative "../../framework/settings"
+require_relative "../../services/logstash_service"
+require_relative "../../framework/helpers"
+require "logstash/devutils/rspec/spec_helper"
+
+
+# These are segmented into a separate tag that MUST be run separately from any docker tests
+# The reason they break the Docker API and that in turn even breaks tests not using Docker 
+# is that the Docker API has a global singleton Docker container set up as soon as it's 
+# required that acts in the background and will err out if the internet is down
+# See https://github.com/elastic/logstash/issues/7160#issue-229902725
+describe "CLI > logstash-plugin prepare-offline-pack", :offline => true do
+  before(:all) do
+    @fixture = Fixture.new(__FILE__)
+    @logstash_plugin = @fixture.get_service("logstash").plugin_cli
+  end
+
+  let(:temporary_zip_file) do
+    p = Stud::Temporary.pathname
+    FileUtils.mkdir_p(p)
+    File.join(p, "mypack.zip")
+  end
+
+  context "creating a pack for specific plugins" do
+    let(:plugins_to_pack) { %w(logstash-input-beats logstash-output-elasticsearch) }
+
+    it "successfully create a pack" do
+      execute = @logstash_plugin.prepare_offline_pack(plugins_to_pack, temporary_zip_file)
+
+      expect(execute.exit_code).to eq(0)
+      expect(execute.stderr_and_stdout).to match(/Offline package created at/)
+      expect(execute.stderr_and_stdout).to match(/#{temporary_zip_file}/)
+
+      unpacked = unpack(temporary_zip_file)
+
+      expect(unpacked.plugins.collect(&:name)).to include(*plugins_to_pack)
+      expect(unpacked.plugins.size).to eq(2)
+
+      expect(unpacked.dependencies.size).to be > 0
+    end
+  end
+
+  context "create a pack from a wildcard" do
+    let(:plugins_to_pack) { %w(logstash-filter-*) }
+
+    it "successfully create a pack" do
+      execute = @logstash_plugin.prepare_offline_pack(plugins_to_pack, temporary_zip_file)
+
+      expect(execute.exit_code).to eq(0)
+      expect(execute.stderr_and_stdout).to match(/Offline package created at/)
+      expect(execute.stderr_and_stdout).to match(/#{temporary_zip_file}/)
+
+      unpacked = unpack(temporary_zip_file)
+
+      filters = @logstash_plugin.list(plugins_to_pack.first).stderr_and_stdout.split("\n")
+
+      expect(unpacked.plugins.collect(&:name)).to include(*filters)
+      expect(unpacked.plugins.size).to eq(filters.size)
+
+      expect(unpacked.dependencies.size).to be > 0
+    end
+  end
+
+  context "create a pack with a locally installed .gem" do
+    let(:plugin_to_pack) { "logstash-filter-qatest" }
+
+    before do
+      @logstash_plugin.install(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "logstash-filter-qatest-0.1.1.gem"))
+
+      # assert that the plugins is correctly installed
+      execute = @logstash_plugin.list(plugin_to_pack)
+
+      expect(execute.stderr_and_stdout).to match(/#{plugin_to_pack}/)
+      expect(execute.exit_code).to eq(0)
+    end
+
+    it "successfully create a pack" do
+      execute = @logstash_plugin.prepare_offline_pack(plugin_to_pack, temporary_zip_file)
+
+      expect(execute.stderr_and_stdout).to match(/Offline package created at/)
+      expect(execute.stderr_and_stdout).to match(/#{temporary_zip_file}/)
+      expect(execute.exit_code).to eq(0)
+
+      unpacked = unpack(temporary_zip_file)
+
+      expect(unpacked.plugins.collect(&:name)).to include(plugin_to_pack)
+      expect(unpacked.plugins.size).to eq(1)
+
+      expect(unpacked.dependencies.size).to eq(0)
+    end
+  end
+end
diff --git a/qa/integration/specs/cli/remove_spec.rb b/qa/integration/specs/cli/remove_spec.rb
new file mode 100644
index 00000000000..95981265d5a
--- /dev/null
+++ b/qa/integration/specs/cli/remove_spec.rb
@@ -0,0 +1,91 @@
+# encoding: utf-8
+require_relative '../../framework/fixture'
+require_relative '../../framework/settings'
+require_relative '../../services/logstash_service'
+require_relative '../../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+
+describe "CLI > logstash-plugin remove" do
+  before(:all) do
+    @fixture = Fixture.new(__FILE__)
+    @logstash_plugin = @fixture.get_service("logstash").plugin_cli
+  end
+
+    if RbConfig::CONFIG["host_os"] == "linux"
+      context "without internet connection (linux seccomp wrapper)" do
+
+        let(:offline_wrapper_path) { File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "offline_wrapper")) }
+        let(:offline_wrapper_cmd) { File.join(offline_wrapper_path, "offline") }
+
+        before do
+          Dir.chdir(offline_wrapper_path) do
+            system("make clean")
+            system("make")
+          end
+        end
+
+        context "when no other plugins depends on this plugin" do
+          it "successfully remove the plugin" do
+            execute = @logstash_plugin.run_raw("#{offline_wrapper_cmd} bin/logstash-plugin remove logstash-input-twitter")
+
+            expect(execute.exit_code).to eq(0)
+            expect(execute.stderr_and_stdout).to match(/Successfully removed logstash-input-twitter/)
+
+            presence_check = @logstash_plugin.list("logstash-input-twitter")
+            expect(presence_check.exit_code).to eq(1)
+            expect(presence_check.stderr_and_stdout).to match(/ERROR: No plugins found/)
+
+            @logstash_plugin.install("logstash-input-twitter")
+          end
+        end
+
+        context "when other plugins depends on this plugin" do
+          it "refuses to remove the plugin and display the plugin that depends on it." do
+            execute = @logstash_plugin.run_raw("#{offline_wrapper_cmd} bin/logstash-plugin remove logstash-codec-json")
+
+            expect(execute.exit_code).to eq(1)
+            expect(execute.stderr_and_stdout).to match(/Failed to remove "logstash-codec-json"/)
+            expect(execute.stderr_and_stdout).to match(/logstash-input-beats/) # one of the dependency
+            expect(execute.stderr_and_stdout).to match(/logstash-output-udp/) # one of the dependency
+
+            presence_check = @logstash_plugin.list("logstash-codec-json")
+
+            expect(presence_check.exit_code).to eq(0)
+            expect(presence_check.stderr_and_stdout).to match(/logstash-codec-json/)
+          end
+        end
+
+      end
+    else
+      context "when no other plugins depends on this plugin" do
+        it "successfully remove the plugin" do
+          execute = @logstash_plugin.remove("logstash-input-twitter")
+
+          expect(execute.exit_code).to eq(0)
+          expect(execute.stderr_and_stdout).to match(/Successfully removed logstash-input-twitter/)
+
+          presence_check = @logstash_plugin.list("logstash-input-twitter")
+          expect(presence_check.exit_code).to eq(1)
+          expect(presence_check.stderr_and_stdout).to match(/ERROR: No plugins found/)
+
+          @logstash_plugin.install("logstash-input-twitter")
+        end
+      end
+
+      context "when other plugins depends on this plugin" do
+        it "refuses to remove the plugin and display the plugin that depends on it." do
+          execute = @logstash_plugin.remove("logstash-codec-json")
+
+          expect(execute.exit_code).to eq(1)
+          expect(execute.stderr_and_stdout).to match(/Failed to remove "logstash-codec-json"/)
+          expect(execute.stderr_and_stdout).to match(/logstash-input-beats/) # one of the dependency
+          expect(execute.stderr_and_stdout).to match(/logstash-output-udp/) # one of the dependency
+
+          presence_check = @logstash_plugin.list("logstash-codec-json")
+
+          expect(presence_check.exit_code).to eq(0)
+          expect(presence_check.stderr_and_stdout).to match(/logstash-codec-json/)
+        end
+      end
+    end
+end
diff --git a/qa/integration/specs/env_variables_config_spec.rb b/qa/integration/specs/env_variables_config_spec.rb
new file mode 100644
index 00000000000..d7594c90166
--- /dev/null
+++ b/qa/integration/specs/env_variables_config_spec.rb
@@ -0,0 +1,48 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+
+describe "Test Logstash configuration" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  let(:num_retries) { 50 }
+  let(:test_tcp_port) { random_port }
+  let(:test_tag) { "environment_variables_are_evil" }
+  let(:test_path) { Stud::Temporary.directory }
+  let(:sample_data) { '74.125.176.147 - - [11/Sep/2014:21:50:37 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "FeedBurner/1.0 (http://www.FeedBurner.com)"' }
+
+  it "expands environment variables in all plugin blocks" do
+    # set ENV variables before starting the service
+    test_env = {}
+    test_env["TEST_ENV_TCP_PORT"] = "#{test_tcp_port}"
+    test_env["TEST_ENV_TAG"] = test_tag
+    test_env["TEST_ENV_PATH"] = test_path
+    
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.env_variables = test_env
+    logstash_service.start_background(@fixture.config)
+    # check if TCP port env variable was resolved
+    try(num_retries) do
+      expect(is_port_open?(test_tcp_port)).to be true
+    end
+    
+    #send data and make sure all env variables are expanded by checking each stage
+    send_data(test_tcp_port, sample_data)
+    output_file = File.join(test_path, "logstash_env_test.log")
+    try(num_retries) do
+      expect(File.exists?(output_file)).to be true
+    end
+    # should have created the file using env variable with filters adding a tag based on env variable
+    try(num_retries) do
+      expect(IO.read(output_file).gsub("\n", "")).to eq("#{sample_data} blah,environment_variables_are_evil")
+    end
+  end
+end  
\ No newline at end of file
diff --git a/qa/integration/specs/es_output_how_spec.rb b/qa/integration/specs/es_output_how_spec.rb
new file mode 100644
index 00000000000..659d18af1de
--- /dev/null
+++ b/qa/integration/specs/es_output_how_spec.rb
@@ -0,0 +1,41 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+
+describe "Test Elasticsearch output" do
+
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    es_client = @fixture.get_service("elasticsearch").get_client
+    es_client.indices.delete(index: 'logstash-*')
+    @fixture.teardown
+  }
+
+  it "can ingest 37K log lines of sample apache logs" do
+    logstash_service = @fixture.get_service("logstash")
+    es_service = @fixture.get_service("elasticsearch")
+    logstash_service.start_with_input(@fixture.config, @fixture.input)
+    es_client = es_service.get_client
+    # now we test if all data was indexed by ES, but first refresh manually
+    es_client.indices.refresh
+    result = es_client.search(index: 'logstash-*', size: 0, q: '*')
+    expect(result["hits"]["total"]).to eq(37)
+    
+    # randomly checked for results and structured fields
+    result = es_client.search(index: 'logstash-*', size: 1, q: 'dynamic')
+    expect(result["hits"]["total"]).to eq(1)
+    s = result["hits"]["hits"][0]["_source"]
+    expect(s["bytes"]).to eq(18848)
+    expect(s["response"]).to eq(200)
+    expect(s["clientip"]).to eq("213.113.233.227")
+    # Use a range instead of a fixed number
+    # update on the geoip data can change the values
+    expect(s["geoip"]["longitude"]).to be_between(-180, 180)
+    expect(s["geoip"]["latitude"]).to be_between(-90, 90)
+    expect(s["verb"]).to eq("GET")
+    expect(s["useragent"]["os"]).to eq("Windows 7")
+  end
+end
diff --git a/qa/integration/specs/kafka_input_spec.rb b/qa/integration/specs/kafka_input_spec.rb
new file mode 100644
index 00000000000..70338d4cb9b
--- /dev/null
+++ b/qa/integration/specs/kafka_input_spec.rb
@@ -0,0 +1,33 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require "rspec/wait"
+require "logstash/devutils/rspec/spec_helper"
+
+describe "Test Kafka Input" do
+  let(:num_retries) { 60 }
+  let(:num_events) { 37 }
+
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+
+  it "can ingest 37 apache log lines from Kafka broker" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_background(@fixture.config)
+
+    try(num_retries) do
+      expect(@fixture.output_exists?).to be true
+    end
+
+    try(num_retries) do
+      count = File.foreach(@fixture.actual_output).inject(0) {|c, line| c+1}
+      expect(count).to eq(num_events)
+    end
+  end
+
+end
diff --git a/qa/integration/specs/monitoring_api_spec.rb b/qa/integration/specs/monitoring_api_spec.rb
new file mode 100644
index 00000000000..8024c52fb26
--- /dev/null
+++ b/qa/integration/specs/monitoring_api_spec.rb
@@ -0,0 +1,80 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require "logstash/devutils/rspec/spec_helper"
+require"stud/try"
+
+describe "Test Monitoring API" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  after(:each) {
+    @fixture.get_service("logstash").teardown
+  }
+  
+  let(:number_of_events) { 5 }
+  let(:max_retry) { 120 }
+
+  it "can retrieve event stats" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_with_stdin
+    logstash_service.wait_for_logstash
+    number_of_events.times { logstash_service.write_to_stdin("Hello world") }
+
+    Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
+      # event_stats can fail if the stats subsystem isn't ready
+      result = logstash_service.monitoring_api.event_stats rescue nil
+      expect(result).not_to be_nil
+      expect(result["in"]).to eq(number_of_events)
+    end
+  end
+
+  it "can retrieve JVM stats" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_with_stdin
+    logstash_service.wait_for_logstash
+
+    Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
+      # node_stats can fail if the stats subsystem isn't ready
+      result = logstash_service.monitoring_api.node_stats rescue nil
+      expect(result).not_to be_nil
+      expect(result["jvm"]).not_to be_nil
+      expect(result["jvm"]["uptime_in_millis"]).to be > 100
+    end
+  end
+
+  it "can retrieve queue stats" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_with_stdin
+    logstash_service.wait_for_logstash
+
+    Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
+      # node_stats can fail if the stats subsystem isn't ready
+      result = logstash_service.monitoring_api.node_stats rescue nil
+      expect(result).not_to be_nil
+      # we use fetch here since we want failed fetches to raise an exception
+      # and trigger the retry block
+      queue_stats = result.fetch("pipeline").fetch("queue")
+      expect(queue_stats).not_to be_nil
+      if logstash_service.settings.feature_flag == "persistent_queues"
+        expect(queue_stats["type"]).to eq "persisted"
+        queue_data_stats = queue_stats.fetch("data")
+        expect(queue_data_stats["free_space_in_bytes"]).not_to be_nil
+        expect(queue_data_stats["storage_type"]).not_to be_nil
+        expect(queue_data_stats["path"]).not_to be_nil
+        expect(queue_stats["events"]).not_to be_nil
+        queue_capacity_stats = queue_stats.fetch("capacity")
+        expect(queue_capacity_stats["page_capacity_in_bytes"]).not_to be_nil
+        expect(queue_capacity_stats["max_queue_size_in_bytes"]).not_to be_nil
+        expect(queue_capacity_stats["max_unread_events"]).not_to be_nil
+      else
+        expect(queue_stats["type"]).to eq("memory")
+      end
+    end
+  end
+end
diff --git a/qa/integration/specs/reload_config_spec.rb b/qa/integration/specs/reload_config_spec.rb
new file mode 100644
index 00000000000..24f37aba942
--- /dev/null
+++ b/qa/integration/specs/reload_config_spec.rb
@@ -0,0 +1,80 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+require "socket"
+require "json"
+
+describe "Test Logstash service when config reload is enabled" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  let(:timeout_seconds) { 5 }
+  let(:initial_port) { random_port }
+  let(:reload_port) { random_port }
+  let(:retry_attempts) { 10 }
+  let(:output_file1) { Stud::Temporary.file.path }
+  let(:output_file2) { Stud::Temporary.file.path }
+  let(:sample_data) { '74.125.176.147 - - [11/Sep/2014:21:50:37 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "FeedBurner/1.0 (http://www.FeedBurner.com)"' }
+  
+  let(:initial_config_file) { config_to_temp_file(@fixture.config("initial", { :port => initial_port, :file => output_file1 })) }
+  let(:reload_config_file) { config_to_temp_file(@fixture.config("reload", { :port => reload_port, :file => output_file2 })) }
+
+  it "can reload when changes are made to TCP port and grok pattern" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.spawn_logstash("-f", "#{initial_config_file}", "--config.reload.automatic", "true")
+    logstash_service.wait_for_logstash
+    wait_for_port(initial_port, retry_attempts)
+    
+    # try sending events with this
+    send_data(initial_port, sample_data)
+    Stud.try(retry_attempts.times, RSpec::Expectations::ExpectationNotMetError) do
+      expect(IO.read(output_file1).gsub("\n", "")).to eq(sample_data)
+    end
+    
+    # check metrics
+    result = logstash_service.monitoring_api.event_stats
+    expect(result["in"]).to eq(1)
+    expect(result["out"]).to eq(1)
+    
+    # do a reload
+    logstash_service.reload_config(initial_config_file, reload_config_file)
+
+    logstash_service.wait_for_logstash
+    wait_for_port(reload_port, retry_attempts)
+    
+    # make sure old socket is closed
+    expect(is_port_open?(initial_port)).to be false
+    
+    send_data(reload_port, sample_data)
+    Stud.try(retry_attempts.times, RSpec::Expectations::ExpectationNotMetError) do
+      expect(IO.read(output_file2).blank?).to be false
+    end
+    
+    # check metrics. It should be reset
+    result = logstash_service.monitoring_api.event_stats
+    expect(result["in"]).to eq(1)
+    expect(result["out"]).to eq(1)
+    
+    # check reload stats
+    reload_stats = logstash_service.monitoring_api.pipeline_stats["reloads"]
+    instance_reload_stats = logstash_service.monitoring_api.node_stats["reloads"]
+    expect(reload_stats["successes"]).to eq(1)
+    expect(reload_stats["failures"]).to eq(0)
+    expect(reload_stats["last_success_timestamp"].blank?).to be false
+    expect(reload_stats["last_error"]).to eq(nil)
+    
+    expect(instance_reload_stats["successes"]).to eq(1)
+    expect(instance_reload_stats["failures"]).to eq(0)
+    # parse the results and validate
+    re = JSON.load(File.new(output_file2))
+    expect(re["clientip"]).to eq("74.125.176.147")
+    expect(re["response"]).to eq(200)
+  end
+end
\ No newline at end of file
diff --git a/qa/integration/specs/settings_spec.rb b/qa/integration/specs/settings_spec.rb
new file mode 100644
index 00000000000..8e0972f1939
--- /dev/null
+++ b/qa/integration/specs/settings_spec.rb
@@ -0,0 +1,167 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+require "yaml"
+
+describe "Test Logstash instance whose default settings are overridden" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+    @logstash_service = @fixture.get_service("logstash")
+    @logstash_default_logs = File.join(@logstash_service.logstash_home, "logs", "logstash-plain.log")
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  before(:each) {
+    FileUtils.rm(@logstash_default_logs) if File.exists?(@logstash_default_logs)
+    # backup the application settings file -- logstash.yml
+    FileUtils.cp(@logstash_service.application_settings_file, "#{@logstash_service.application_settings_file}.original")
+  }
+  
+  after(:each) {
+    @logstash_service.teardown
+    # restore the application settings file -- logstash.yml
+    FileUtils.mv("#{@logstash_service.application_settings_file}.original", @logstash_service.application_settings_file)
+  }
+
+  let(:num_retries) { 50 }
+  let(:test_port) { random_port }
+  let(:temp_dir) { Stud::Temporary.directory("logstash-settings-test") }
+  let(:tcp_config) { @fixture.config("root", { :port => test_port }) }
+  
+  def change_setting(name, value)
+    settings = {}
+    settings[name] = value
+    overwrite_settings(settings)
+  end
+  
+  def overwrite_settings(settings)
+    IO.write(@logstash_service.application_settings_file, settings.to_yaml)
+  end
+  
+  it "should start with a new data dir" do
+    change_setting("path.data", temp_dir)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    @logstash_service.wait_for_logstash
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+  end
+  
+  it "should write logs to a new dir" do
+    change_setting("path.logs", temp_dir)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    @logstash_service.wait_for_logstash
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+    expect(File.exists?("#{temp_dir}/logstash-plain.log")).to be true
+  end
+  
+  it "should read config from the specified dir in logstash.yml" do
+    change_setting("path.config", temp_dir)
+    test_config_path = File.join(temp_dir, "test.config")
+    IO.write(test_config_path, tcp_config)
+    expect(File.exists?(test_config_path)).to be true
+    @logstash_service.spawn_logstash
+    @logstash_service.wait_for_logstash
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+  end
+  
+  it "should exit when config test_and_exit is set" do
+    s = {}
+    s["path.config"] = temp_dir
+    s["config.test_and_exit"] = true
+    s["path.logs"] = temp_dir
+    overwrite_settings(s)
+    test_config_path = File.join(temp_dir, "test.config")
+    IO.write(test_config_path, "#{tcp_config}")
+    expect(File.exists?(test_config_path)).to be true
+    @logstash_service.spawn_logstash
+    try(num_retries) do
+      expect(@logstash_service.exited?).to be true
+    end
+    expect(@logstash_service.exit_code).to eq(0)
+    
+    # now with bad config
+    IO.write(test_config_path, "#{tcp_config} filters {} ")
+    expect(File.exists?(test_config_path)).to be true
+    @logstash_service.spawn_logstash
+    try(num_retries) do
+      expect(@logstash_service.exited?).to be true
+    end
+    expect(@logstash_service.exit_code).to eq(1)
+  end
+
+  it "change pipeline settings" do
+    s = {}
+    workers = 31
+    batch_size = 1
+    s["pipeline.workers"] = workers
+    s["pipeline.batch.size"] = batch_size
+    overwrite_settings(s)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    @logstash_service.wait_for_logstash
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+
+    # now check monitoring API to validate
+    node_info = @logstash_service.monitoring_api.node_info
+    expect(node_info["pipeline"]["workers"]).to eq(workers)
+    expect(node_info["pipeline"]["batch_size"]).to eq(batch_size)
+  end
+
+  it "start on a different HTTP port" do
+    # default in 9600
+    http_port = random_port
+    change_setting("http.port", http_port)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    @logstash_service.wait_for_logstash
+    
+    try(num_retries) do
+      expect(is_port_open?(http_port)).to be true
+    end
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+    
+    expect(File.exists?(@logstash_default_logs)).to be true
+
+    resp = Manticore.get("http://localhost:#{http_port}/_node").body
+    node_info = JSON.parse(resp)
+    # should be default
+    expect(node_info["http_address"]).to eq("127.0.0.1:#{http_port}")
+  end
+
+  it "start even without a settings file specified" do
+    @logstash_service.spawn_logstash("-e", tcp_config, "--path.settings", "/tmp/fooooobbaaar")
+    @logstash_service.wait_for_logstash
+    http_port = 9600
+    try(num_retries) do
+      expect(is_port_open?(http_port)).to be true
+    end
+
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+
+    resp = Manticore.get("http://localhost:#{http_port}/_node").body
+    node_info = JSON.parse(resp)
+    expect(node_info["http_address"]).to eq("127.0.0.1:#{http_port}")
+
+    # make sure we log to console and not to any file
+    expect(File.exists?(@logstash_default_logs)).to be false
+  end
+end
diff --git a/qa/integration/specs/slowlog_spec.rb b/qa/integration/specs/slowlog_spec.rb
new file mode 100644
index 00000000000..c27f09adfae
--- /dev/null
+++ b/qa/integration/specs/slowlog_spec.rb
@@ -0,0 +1,46 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+require "yaml"
+
+describe "Test Logstash Slowlog" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+    # used in multiple LS tests
+    @ls = @fixture.get_service("logstash")
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+
+  before(:each) {
+    # backup the application settings file -- logstash.yml
+    FileUtils.cp(@ls.application_settings_file, "#{@ls.application_settings_file}.original")
+  }
+
+  after(:each) {
+    @ls.teardown
+    # restore the application settings file -- logstash.yml
+    FileUtils.mv("#{@ls.application_settings_file}.original", @ls.application_settings_file)
+  }
+
+  let(:temp_dir) { Stud::Temporary.directory("logstash-slowlog-test") }
+  let(:config) { @fixture.config("root") }
+
+  it "should write logs to a new dir" do
+    settings = {
+      "path.logs" => temp_dir,
+      "slowlog.threshold.warn" => "500ms"
+    }
+    IO.write(@ls.application_settings_file, settings.to_yaml)
+    @ls.spawn_logstash("-e", config)
+    @ls.wait_for_logstash
+    sleep 1 until @ls.exited?
+    slowlog_file = "#{temp_dir}/logstash-slowlog-plain.log"
+    expect(File.exists?(slowlog_file)).to be true
+    expect(IO.read(slowlog_file).split("\n").size).to eq(2)
+  end
+end
diff --git a/qa/integration/specs/spec_helper.rb b/qa/integration/specs/spec_helper.rb
new file mode 100644
index 00000000000..138524561d2
--- /dev/null
+++ b/qa/integration/specs/spec_helper.rb
@@ -0,0 +1,8 @@
+# encoding: utf-8
+RSpec.configure do |config|
+  if RbConfig::CONFIG["host_os"] != "linux"
+    exclude_tags = { :linux => true }
+  end
+
+  config.filter_run_excluding exclude_tags
+end
diff --git a/qa/integration/suite.yml b/qa/integration/suite.yml
new file mode 100644
index 00000000000..cdee612d4e4
--- /dev/null
+++ b/qa/integration/suite.yml
@@ -0,0 +1,7 @@
+---
+# Use this to output more debug-level information  
+verbose_mode: false  
+# Typically we use the binaries in LS_HOME/build. If you want to QA a LS in different location, 
+# use the absolute path below  
+#ls_home_abs_path: /tmp/logstash-5.0.0-alpha6
+feature_flag: <%= ENV['FEATURE_FLAG'] %>
\ No newline at end of file
diff --git a/qa/platform_config.rb b/qa/platform_config.rb
new file mode 100644
index 00000000000..6cfb9716bd6
--- /dev/null
+++ b/qa/platform_config.rb
@@ -0,0 +1,80 @@
+# encoding: utf-8
+require "json"
+require "ostruct"
+
+# This is a wrapper to encapsulate the logic behind the different platforms we test with, 
+# this is done here in order to simplify the necessary configuration for bootstrap and interactions
+# necessary later on in the tests phases.
+#
+class PlatformConfig
+
+  # Abstract the idea of a platform, aka an OS
+  class Platform
+
+    attr_reader :name, :box, :type, :bootstrap, :experimental
+
+    def initialize(name, data)
+      @name = name
+      @box  = data["box"]
+      @type = data["type"]
+      @experimental = data["experimental"] || false
+      configure_bootstrap_scripts(data)
+    end
+
+    private
+
+    def configure_bootstrap_scripts(data)
+      @bootstrap = OpenStruct.new(:privileged     => "sys/#{type}/bootstrap.sh",
+                                  :non_privileged => "sys/#{type}/user_bootstrap.sh")
+      ##
+      # for now the only specific bootstrap scripts are ones need
+      # with privileged access level, whenever others are also
+      # required we can update this section as well with the same pattern.
+      ##
+      @bootstrap.privileged = "sys/#{type}/#{name}/bootstrap.sh" if data["specific"]
+    end
+  end
+
+  DEFAULT_CONFIG_LOCATION = File.join(File.dirname(__FILE__), "config", "platforms.json").freeze
+
+  attr_reader :platforms, :latest
+
+  def initialize(config_path = DEFAULT_CONFIG_LOCATION)
+    @config_path = config_path
+    @platforms = []
+
+    data = JSON.parse(File.read(@config_path))
+    data["platforms"].each do |k, v|
+      @platforms << Platform.new(k, v)
+    end
+    @platforms.sort! { |a, b| a.name <=> b.name }
+    @latest = data["latest"]
+  end
+
+  def find!(platform_name)
+    result = @platforms.find { |platform| platform.name == platform_name }.first
+    if result.nil?
+      raise "Cannot find platform named: #{platform_name} in @config_path"
+    else
+      return result
+    end
+  end
+
+  def each(&block)
+    @platforms.each(&block)
+  end
+
+  def filter_type(type_name, options={})
+    experimental = options.fetch("experimental", false)
+    @platforms.select { |platform| platform.type == type_name && platform.experimental == experimental }
+  end
+
+  def select_names_for(platform, options={})
+    filter_options = { "experimental" => options.fetch("experimental", false) }
+    !platform.nil? ? filter_type(platform, filter_options).map{ |p| p.name } : ""
+  end
+
+  def types
+    @platforms.collect(&:type).uniq.sort
+  end
+end
diff --git a/qa/rspec/commands.rb b/qa/rspec/commands.rb
new file mode 100644
index 00000000000..f90bc74cc97
--- /dev/null
+++ b/qa/rspec/commands.rb
@@ -0,0 +1,140 @@
+# encoding: utf-8
+require_relative "./commands/debian"
+require_relative "./commands/ubuntu"
+require_relative "./commands/redhat"
+require_relative "./commands/suse"
+require_relative "./commands/centos/centos-6"
+require_relative "./commands/oel/oel-6"
+require_relative "./commands/ubuntu/ubuntu-1604"
+require_relative "./commands/suse/sles-11"
+
+require "forwardable"
+
+module ServiceTester
+
+  # An artifact is the component being tested, it's able to interact with
+  # a destination machine by holding a client and is basically provides all 
+  # necessary abstractions to make the test simple.
+  class Artifact
+
+    extend Forwardable
+    def_delegators :@client, :installed?, :removed?, :running?
+
+    attr_reader :host, :client
+
+    def initialize(host, options={})
+      @host    = host
+      @options = options
+      @client  = CommandsFactory.fetch(options["type"], options["host"])
+    end
+
+    def hostname
+      @options["host"]
+    end
+
+    def name
+      "logstash"
+    end
+
+    def hosts
+      [@host]
+    end
+
+    def snapshot
+      client.snapshot(@options["host"])
+    end
+
+    def restore
+      client.restore(@options["host"])
+    end
+
+    def start_service
+      client.start_service(name, host)
+    end
+
+    def stop_service
+      client.stop_service(name, host)
+    end
+
+    def install(options={})
+      base      = options.fetch(:base, ServiceTester::Base::LOCATION)
+      package   = client.package_for(filename(options), base)
+      client.install(package, host)
+    end
+
+    def uninstall
+      client.uninstall(name, host)
+    end
+
+    def run_command_in_path(cmd)
+      client.run_command_in_path(cmd, host)
+    end
+
+    def run_command(cmd)
+      client.run_command(cmd, host)
+    end
+
+    def plugin_installed?(name, version = nil)
+      client.plugin_installed?(host, name, version)
+    end
+
+    def download(from, to)
+      client.download(from, to , host)
+    end
+    
+    def replace_in_gemfile(pattern, replace)
+      client.replace_in_gemfile(pattern, replace, host)
+    end
+
+    def delete_file(path)
+      client.delete_file(path, host)
+    end
+
+    def to_s
+      "Artifact #{name}@#{host}"
+    end
+
+    private
+
+    def filename(options={})
+      snapshot  = options.fetch(:snapshot, true)
+      "logstash-#{options[:version]}#{(snapshot ?  "-SNAPSHOT" : "")}"
+    end
+  end
+
+  # Factory of commands used to select the right clients for a given type of OS and host name,
+  # this give you as much granularity as required.
+  class CommandsFactory
+
+    def self.fetch(type, host)
+      case type
+      when "debian"
+        if host.start_with?("ubuntu")
+          if host == "ubuntu-1604"
+            return Ubuntu1604Commands.new
+          else
+            return UbuntuCommands.new
+          end
+        else
+          return DebianCommands.new
+        end
+      when "suse"
+        if host == "sles-11"
+          return Sles11Commands.new
+        else
+          return SuseCommands.new
+        end
+      when "redhat"
+        if host == "centos-6"
+          return Centos6Commands.new
+        elsif host == "oel-6"
+          return Oel6Commands.new
+        else
+          return RedhatCommands.new
+        end
+      else
+        return
+      end
+    end
+  end
+end
diff --git a/qa/rspec/commands/base.rb b/qa/rspec/commands/base.rb
new file mode 100644
index 00000000000..3f06c2a02b1
--- /dev/null
+++ b/qa/rspec/commands/base.rb
@@ -0,0 +1,70 @@
+# encoding: utf-8
+require_relative "../../vagrant/helpers"
+require_relative "system_helpers"
+
+module ServiceTester
+
+  class InstallException < Exception; end
+
+  class Base
+    LOCATION="/logstash-build".freeze
+    LOGSTASH_PATH="/usr/share/logstash/".freeze
+
+    def snapshot(host)
+      LogStash::VagrantHelpers.save_snapshot(host)
+    end
+
+    def restore(host)
+      LogStash::VagrantHelpers.restore_snapshot(host)
+    end
+
+    def start_service(service, host=nil)
+      service_manager(service, "start", host)
+    end
+
+    def stop_service(service, host=nil)
+      service_manager(service, "stop", host)
+    end
+
+    def run_command(cmd, host)
+      hosts = (host.nil? ? servers : Array(host))
+
+      response = nil
+      at(hosts, {in: :serial}) do |_host|
+        response = sudo_exec!("JARS_SKIP='true' #{cmd}")
+      end
+      response
+    end
+
+    def replace_in_gemfile(pattern, replace, host)
+      gemfile = File.join(LOGSTASH_PATH, "Gemfile")
+      cmd = "sed -i.sedbak 's/#{pattern}/#{replace}/' #{gemfile}"
+      run_command(cmd, host)
+    end
+
+    def run_command_in_path(cmd, host)
+      run_command("#{File.join(LOGSTASH_PATH, cmd)}", host)
+    end
+
+    def plugin_installed?(host, plugin_name, version = nil)
+      if version.nil?
+        cmd = run_command_in_path("bin/logstash-plugin list", host)
+        search_token = plugin_name
+      else
+        cmd = run_command_in_path("bin/logstash-plugin list --verbose", host)
+        search_token ="#{plugin_name} (#{version})"
+      end
+
+      plugins_list = cmd.stdout.split("\n")
+      plugins_list.include?(search_token)
+    end
+
+    def download(from, to, host)
+      run_command("wget #{from} -O #{to}", host)
+    end
+
+    def delete_file(path, host)
+      run_command("rm -rf #{path}", host)
+    end
+  end
+end
diff --git a/qa/rspec/commands/centos/centos-6.rb b/qa/rspec/commands/centos/centos-6.rb
new file mode 100644
index 00000000000..371590490e6
--- /dev/null
+++ b/qa/rspec/commands/centos/centos-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Centos6Commands < RedhatCommands
+      include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/debian.rb b/qa/rspec/commands/debian.rb
new file mode 100644
index 00000000000..30fa8c8daf6
--- /dev/null
+++ b/qa/rspec/commands/debian.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class DebianCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+    end
+
+    def package_for(filename, base=ServiceTester::Base::LOCATION)
+      File.join(base, "#{filename}.deb")
+    end
+
+    def install(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      errors = []
+      at(hosts, {in: :serial}) do |_|
+        cmd = sudo_exec!("dpkg -i --force-confnew #{package}")
+        if cmd.exit_status != 0
+          errors << cmd.stderr.to_s
+        end
+      end
+      raise InstallException.new(errors.join("\n")) unless errors.empty?
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("dpkg -r #{package}")
+        sudo_exec!("dpkg --purge #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s #{package}")
+        stdout = cmd.stderr
+      end
+      (
+        stdout.match(/^Package `#{package}' is not installed and no info is available.$/) ||
+        stdout.match(/^dpkg-query: package '#{package}' is not installed and no information is available$/)
+      )
+    end
+  end
+end
diff --git a/qa/rspec/commands/oel/oel-6.rb b/qa/rspec/commands/oel/oel-6.rb
new file mode 100644
index 00000000000..ed92a8ce11d
--- /dev/null
+++ b/qa/rspec/commands/oel/oel-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Oel6Commands < RedhatCommands
+    include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/redhat.rb b/qa/rspec/commands/redhat.rb
new file mode 100644
index 00000000000..b7dce804869
--- /dev/null
+++ b/qa/rspec/commands/redhat.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class RedhatCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("yum list installed  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Installed Packages$/)
+      stdout.match(/^logstash.noarch/)
+    end
+
+    def package_for(filename, base=ServiceTester::Base::LOCATION)
+      File.join(base, "#{filename}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = []
+      exit_status = 0
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("yum install -y  #{package}")
+        exit_status += cmd.exit_status
+        errors << cmd.stderr unless cmd.stderr.empty?
+      end
+      if exit_status > 0 
+        raise InstallException.new(errors.join("\n"))
+      end
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("yum remove -y #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("yum list installed #{package}")
+        stdout = cmd.stderr
+      end
+      stdout.match(/^Error: No matching Packages to list$/)
+    end
+  end
+end
diff --git a/qa/rspec/commands/suse.rb b/qa/rspec/commands/suse.rb
new file mode 100644
index 00000000000..9b1e6eee902
--- /dev/null
+++ b/qa/rspec/commands/suse.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class SuseCommands < Base
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^i | logstash | An extensible logging pipeline | package$/)
+    end
+
+    def package_for(filename, base=ServiceTester::Base::LOCATION)
+      File.join(base, "#{filename}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = []
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive install  #{package}")
+        errors << cmd.stderr unless cmd.stderr.empty?
+      end
+      raise InstallException.new(errors.join("\n")) unless errors.empty?
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive remove #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd    = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/No packages found/)
+    end
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/Active: active \(running\)/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/suse/sles-11.rb b/qa/rspec/commands/suse/sles-11.rb
new file mode 100644
index 00000000000..80dd94dd719
--- /dev/null
+++ b/qa/rspec/commands/suse/sles-11.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../suse"
+
+module ServiceTester
+  class Sles11Commands < SuseCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("/etc/init.d/#{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} is running$/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("/etc/init.d/#{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/system_helpers.rb b/qa/rspec/commands/system_helpers.rb
new file mode 100644
index 00000000000..8cb8922946b
--- /dev/null
+++ b/qa/rspec/commands/system_helpers.rb
@@ -0,0 +1,42 @@
+require_relative "base"
+
+module ServiceTester
+  module SystemD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      (
+        stdout.match(/Active: active \(running\)/) &&
+        stdout.match(/#{package}.service - #{package}/)
+      )
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+  end
+
+  module InitD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("initctl status #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} start\/running/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("initctl #{action} #{service}")
+      end
+    end 
+  end
+end
diff --git a/qa/rspec/commands/ubuntu.rb b/qa/rspec/commands/ubuntu.rb
new file mode 100644
index 00000000000..1d1ae75f96d
--- /dev/null
+++ b/qa/rspec/commands/ubuntu.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require_relative "debian"
+
+module ServiceTester
+  class UbuntuCommands < DebianCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^#{package} start\/running/)
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/ubuntu/ubuntu-1604.rb b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
new file mode 100644
index 00000000000..ae26bc09f28
--- /dev/null
+++ b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../ubuntu"
+
+module ServiceTester
+  class Ubuntu1604Commands < UbuntuCommands
+      include ::ServiceTester::SystemD
+  end
+end
diff --git a/qa/rspec/helpers.rb b/qa/rspec/helpers.rb
new file mode 100644
index 00000000000..a939fa7dfca
--- /dev/null
+++ b/qa/rspec/helpers.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require_relative "commands"
+
+module ServiceTester
+
+  class Configuration
+    attr_accessor :servers, :lookup
+    def initialize
+      @servers  = []
+      @lookup   = {}
+    end
+
+    def hosts
+      lookup.values.map { |val| val["host"] }
+    end
+  end
+
+  class << self
+    attr_accessor :configuration
+  end
+
+  def self.configure
+    self.configuration ||= Configuration.new
+    yield(configuration) if block_given?
+  end
+
+  def servers
+    ServiceTester.configuration.servers
+  end
+
+  def select_client
+    CommandsFactory.fetch(current_example.metadata[:platform])
+  end
+
+  def current_example
+    RSpec.respond_to?(:current_example) ? RSpec.current_example : self.example
+  end
+end
diff --git a/qa/rspec/matchers.rb b/qa/rspec/matchers.rb
new file mode 100644
index 00000000000..4da583262f2
--- /dev/null
+++ b/qa/rspec/matchers.rb
@@ -0,0 +1,4 @@
+# encoding: utf-8
+require_relative "./matchers/be_installed"
+require_relative "./matchers/be_running"
+require_relative "./matchers/cli_matchers"
diff --git a/qa/rspec/matchers/be_installed.rb b/qa/rspec/matchers/be_installed.rb
new file mode 100644
index 00000000000..4de70ae21ed
--- /dev/null
+++ b/qa/rspec/matchers/be_installed.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_installed do
+  match do |subject|
+    subject.installed?(subject.hosts, subject.name)
+  end
+end
+
+RSpec::Matchers.define :be_removed do
+  match do |subject|
+    subject.removed?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/be_running.rb b/qa/rspec/matchers/be_running.rb
new file mode 100644
index 00000000000..dc687e1b11d
--- /dev/null
+++ b/qa/rspec/matchers/be_running.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_running do
+  match do |subject|
+    subject.running?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/cli_matchers.rb b/qa/rspec/matchers/cli_matchers.rb
new file mode 100644
index 00000000000..e31aa050af3
--- /dev/null
+++ b/qa/rspec/matchers/cli_matchers.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+RSpec::Matchers.define :be_successful do
+  match do |actual|
+    actual.exit_status == 0
+  end
+end
+
+RSpec::Matchers.define :fail_and_output do |expected_output|
+  match do |actual|
+    actual.exit_status == 1 && actual.stderr =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :run_successfully_and_output do |expected_output|
+  match do |actual|
+    (actual.exit_status == 0 || actual.exit_status.nil?) && actual.stdout =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :have_installed? do |name,*args|
+  match do |actual|
+    version = args.first
+    actual.plugin_installed?(name, version)
+  end
+end
+
+RSpec::Matchers.define :install_successfully do
+  match do |cmd|
+    expect(cmd).to run_successfully_and_output(/Installation successful/)
+  end
+end
diff --git a/test/windows/acceptance/logstash_release_acceptance.ps1 b/qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
similarity index 96%
rename from test/windows/acceptance/logstash_release_acceptance.ps1
rename to qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
index da812277543..397b490781d 100644
--- a/test/windows/acceptance/logstash_release_acceptance.ps1
+++ b/qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
@@ -6,7 +6,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 $LS_CONFIG="test.conf"
 $LS_BRANCH=$env:LS_BRANCH
diff --git a/test/windows/acceptance/logstash_release_default_plugins.ps1 b/qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
similarity index 96%
rename from test/windows/acceptance/logstash_release_default_plugins.ps1
rename to qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
index f4cf177e718..77b121fc6d3 100644
--- a/test/windows/acceptance/logstash_release_default_plugins.ps1
+++ b/qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
@@ -6,7 +6,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 # - Ruby 7 or newer
 
 $ruby = $env:RUBY_HOME  + "\jruby.exe"
diff --git a/test/windows/event_log/logstash_event_log_plugin_integration.ps1 b/qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
similarity index 98%
rename from test/windows/event_log/logstash_event_log_plugin_integration.ps1
rename to qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
index a267c05e72d..710daf8573a 100644
--- a/test/windows/event_log/logstash_event_log_plugin_integration.ps1
+++ b/qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
@@ -8,7 +8,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 Add-Type -assembly "system.io.compression.filesystem"
 
diff --git a/test/windows/integration/logstash_simple_integration.ps1 b/qa/scripts/windows/integration/logstash_simple_integration.ps1
similarity index 92%
rename from test/windows/integration/logstash_simple_integration.ps1
rename to qa/scripts/windows/integration/logstash_simple_integration.ps1
index 73fe2fcff7e..02cd4f41489 100644
--- a/test/windows/integration/logstash_simple_integration.ps1
+++ b/qa/scripts/windows/integration/logstash_simple_integration.ps1
@@ -8,7 +8,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 Add-Type -assembly "system.io.compression.filesystem"
 
@@ -26,7 +26,7 @@ $LS_CONFIG="test.conf"
 $LS_BRANCH=$env:LS_BRANCH
 $Logstash_path = "$Main_path\logstash"
 $Logstash_zip_file = "$Download_path\logstash.zip"
-$Logstas_URL = "https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash/$LS_BRANCH/nightly/JDK7/logstash-latest-SNAPSHOT.zip"
+$Logstash_URL = "https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash/$LS_BRANCH/nightly/JDK7/logstash-latest-SNAPSHOT.zip"
 
 ## ----------------------------------------
 
@@ -42,7 +42,7 @@ $ES_URL = "https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch
 ## Download and unzip Logstash
 
 md -Path $Logstash_path
-(New-Object System.Net.WebClient).DownloadFile($Logstas_URL, $Logstash_zip_file)
+(New-Object System.Net.WebClient).DownloadFile($Logstash_URL, $Logstash_zip_file)
 [System.IO.Compression.ZipFile]::ExtractToDirectory($Logstash_zip_file, $Download_path)
 ri $Logstash_zip_file
 mv "$Download_path\log*\*" $Logstash_path
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-0.1.1.gem b/qa/support/logstash-filter-qatest/logstash-filter-qatest-0.1.1.gem
new file mode 100644
index 00000000000..3dd8ba42d1f
Binary files /dev/null and b/qa/support/logstash-filter-qatest/logstash-filter-qatest-0.1.1.gem differ
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
new file mode 100644
index 00000000000..1e9fe168abe
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
@@ -0,0 +1,25 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.1'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
+
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
new file mode 100644
index 00000000000..82e3be79baf
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.0'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
diff --git a/qa/sys/debian/bootstrap.sh b/qa/sys/debian/bootstrap.sh
new file mode 100644
index 00000000000..29c2c840346
--- /dev/null
+++ b/qa/sys/debian/bootstrap.sh
@@ -0,0 +1,7 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
diff --git a/qa/sys/debian/debian-8/bootstrap.sh b/qa/sys/debian/debian-8/bootstrap.sh
new file mode 100644
index 00000000000..d1a23d54430
--- /dev/null
+++ b/qa/sys/debian/debian-8/bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+echo "deb http://http.debian.net/debian jessie-backports main" >> /etc/apt/sources.list
+apt-get update
+apt-get install -y openjdk-8-jdk
diff --git a/qa/sys/debian/ubuntu-1404/bootstrap.sh b/qa/sys/debian/ubuntu-1404/bootstrap.sh
new file mode 100644
index 00000000000..728b0c3d13f
--- /dev/null
+++ b/qa/sys/debian/ubuntu-1404/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
+update-ca-certificates -f
diff --git a/qa/sys/debian/user_bootstrap.sh b/qa/sys/debian/user_bootstrap.sh
new file mode 100644
index 00000000000..a99d4ff056f
--- /dev/null
+++ b/qa/sys/debian/user_bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.deb"
+wget -q https://download.elastic.co/logstash/logstash/packages/debian/$LOGSTASH_FILENAME
diff --git a/qa/sys/redhat/bootstrap.sh b/qa/sys/redhat/bootstrap.sh
new file mode 100644
index 00000000000..5976f47722f
--- /dev/null
+++ b/qa/sys/redhat/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+yum update
+yum install -y java-1.8.0-openjdk-devel.x86_64
diff --git a/qa/sys/redhat/user_bootstrap.sh b/qa/sys/redhat/user_bootstrap.sh
new file mode 100644
index 00000000000..4713b3f5c1b
--- /dev/null
+++ b/qa/sys/redhat/user_bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
diff --git a/qa/sys/suse/bootstrap.sh b/qa/sys/suse/bootstrap.sh
new file mode 100644
index 00000000000..4dba83eb9ea
--- /dev/null
+++ b/qa/sys/suse/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+zypper --non-interactive list-updates
+zypper --non-interactive --no-gpg-checks --quiet install --no-recommends java-1_8_0-openjdk-devel
diff --git a/qa/sys/suse/sles-11/bootstrap.sh b/qa/sys/suse/sles-11/bootstrap.sh
new file mode 100644
index 00000000000..654be5d7ec0
--- /dev/null
+++ b/qa/sys/suse/sles-11/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+ln -s /usr/sbin/update-alternatives /usr/sbin/alternatives
+curl -L 'https://edelivery.oracle.com/otn-pub/java/jdk/8u77-b03/jdk-8u77-linux-x64.rpm' -H 'Accept-Encoding: gzip, deflate, sdch' -H 'Accept-Language: en-US,en;q=0.8' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0' -H 'Cookie: oraclelicense=accept-securebackup-cookie;' -H 'Connection: keep-alive' --compressed -o oracle_jdk_1.8.rpm
+zypper -q -n --non-interactive install oracle_jdk_1.8.rpm
diff --git a/qa/sys/suse/sles-12/bootstrap.sh b/qa/sys/suse/sles-12/bootstrap.sh
new file mode 100644
index 00000000000..56b4d0fd7d6
--- /dev/null
+++ b/qa/sys/suse/sles-12/bootstrap.sh
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+zypper addrepo http://download.opensuse.org/repositories/Java:Factory/SLE_12/Java:Factory.repo || true
+zypper --no-gpg-checks --non-interactive refresh
+zypper --non-interactive list-updates
+ln -s /usr/sbin/update-alternatives /usr/sbin/alternatives
+curl -L 'https://edelivery.oracle.com/otn-pub/java/jdk/8u77-b03/jdk-8u77-linux-x64.rpm' -H 'Accept-Encoding: gzip, deflate, sdch' -H 'Accept-Language: en-US,en;q=0.8' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0' -H 'Cookie: oraclelicense=accept-securebackup-cookie;' -H 'Connection: keep-alive' --compressed -o oracle_jdk_1.8.rpm
+zypper -q -n --non-interactive install oracle_jdk_1.8.rpm
diff --git a/qa/sys/suse/user_bootstrap.sh b/qa/sys/suse/user_bootstrap.sh
new file mode 100644
index 00000000000..be22af8b0c1
--- /dev/null
+++ b/qa/sys/suse/user_bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
diff --git a/qa/vagrant/command.rb b/qa/vagrant/command.rb
new file mode 100644
index 00000000000..a07b71f817a
--- /dev/null
+++ b/qa/vagrant/command.rb
@@ -0,0 +1,71 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+
+module LogStash
+  class CommandExecutor
+    class CommandError < StandardError; end
+
+    class CommandResponse
+      attr_reader :stdin, :stdout, :stderr, :exitstatus
+
+      def initialize(stdin, stdout, stderr, exitstatus)
+        @stdin = stdin
+        @stdout = stdout
+        @stderr = stderr
+        @exitstatus = exitstatus
+      end
+
+      def success?
+        exitstatus == 0
+      end
+    end
+
+    def self.run(cmd, debug=false)
+      # This block is require to be able to launch a ruby subprocess
+      # that use bundler.
+      Bundler.with_clean_env do
+        stdin, stdout, stderr, wait_thr = Open3.popen3(cmd)
+        stdout_acc, stderr_acc = "", ""
+        stdout_reporter = reporter(stdout, wait_thr) do |c|
+          stdout_acc << c
+          print c if debug
+        end
+        reporter(stderr, wait_thr) do |c|
+          stderr_acc << c;
+          print c if debug
+        end
+        stdout_reporter.join
+        CommandResponse.new(stdin, stdout_acc, stderr_acc, wait_thr.value.exitstatus)
+      end
+    end
+
+    # This method will raise an exception if the `CMD`
+    # was not run successfully and will display the content of STDERR
+    def self.run!(cmd, debug=false)
+      response = run(cmd, debug)
+
+      unless response.success?
+        raise CommandError, "CMD: #{cmd} STDERR: #{response.stderr}"
+      end
+      response
+    end
+
+    private
+
+    def self.reporter(io, wait_thr, &block)
+      Thread.new(io, wait_thr) do |_io, _wait_thr|
+        while (_wait_thr.status == "run")
+          begin
+            c = _io.read(1)
+            block.call(c) if c
+          rescue IO::WaitReadable
+            IO.select([_io])
+            retry
+          end
+        end
+      end
+    end
+
+  end
+end
diff --git a/qa/vagrant/helpers.rb b/qa/vagrant/helpers.rb
new file mode 100644
index 00000000000..96ea1992a02
--- /dev/null
+++ b/qa/vagrant/helpers.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+require_relative "command"
+
+module LogStash
+  class VagrantHelpers
+
+    def self.halt(machines="", options={})
+      debug = options.fetch(:debug, false)
+      CommandExecutor.run!("vagrant halt #{machines.join(' ')}", debug)
+    end
+
+    def self.destroy(machines="", options={})
+      debug = options.fetch(:debug, false)
+      CommandExecutor.run!("vagrant destroy --force #{machines.join(' ')}", debug) 
+    end
+
+    def self.bootstrap(machines="", options={})
+      debug = options.fetch(:debug, false)
+      CommandExecutor.run!("vagrant up #{machines.join(' ')}", debug)
+    end
+
+    def self.save_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot save #{machine} #{machine}-snapshot")
+    end
+
+    def self.restore_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot restore #{machine} #{machine}-snapshot")
+    end
+
+    def self.fetch_config
+      machines = CommandExecutor.run!("vagrant status").stdout.split("\n").select { |l| l.include?("running") }.map { |r| r.split(' ')[0]}
+      CommandExecutor.run!("vagrant ssh-config #{machines.join(' ')}")
+    end
+
+    def self.parse(lines)
+      hosts, host = [], {}
+      lines.each do |line|
+        if line.match(/Host\s(.*)$/)
+          host = { :host => line.gsub("Host","").strip }
+        elsif line.match(/HostName\s(.*)$/)
+          host[:hostname] = line.gsub("HostName","").strip
+        elsif line.match(/Port\s(.*)$/)
+          host[:port]     = line.gsub("Port","").strip
+        elsif line.empty?
+          hosts << host
+          host = {}
+        end
+      end
+      hosts << host
+      hosts
+    end
+  end
+end
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index bc3d086e105..f518449740b 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -2,6 +2,9 @@ require "logstash/version"
 
 namespace "artifact" do
 
+  SNAPSHOT_BUILD = ENV["RELEASE"] != "1"
+  PACKAGE_SUFFIX = SNAPSHOT_BUILD ? "-SNAPSHOT" : ""
+
   def package_files
     [
       "LICENSE",
@@ -9,8 +12,22 @@ namespace "artifact" do
       "NOTICE.TXT",
       "CONTRIBUTORS",
       "bin/**/*",
+      "config/**/*",
+      "data",
+
       "lib/bootstrap/**/*",
       "lib/pluginmanager/**/*",
+      "lib/systeminstall/**/*",
+
+      "logstash-core/lib/**/*",
+      "logstash-core/locales/**/*",
+      "logstash-core/vendor/**/*",
+      "logstash-core/*.gemspec",
+      "logstash-core/gemspec_jars.rb",
+
+      "logstash-core-plugin-api/lib/**/*",
+      "logstash-core-plugin-api/*.gemspec",
+
       "patterns/**/*",
       "vendor/??*/**/*",
       # To include ruby-maven's hidden ".mvn" directory, we need to
@@ -33,6 +50,9 @@ namespace "artifact" do
     @exclude_paths << "**/test/files/slow-xpath.xml"
     @exclude_paths << "**/logstash-*/spec"
     @exclude_paths << "bin/bundle"
+    @exclude_paths << "bin/rspec"
+    @exclude_paths << "bin/rspec.bat"
+    @exclude_paths << "bin/lock"
 
     @exclude_paths
   end
@@ -53,6 +73,76 @@ namespace "artifact" do
     end.flatten.uniq
   end
 
+  desc "Generate rpm, deb, tar and zip artifacts"
+  task "all" => ["prepare", "build"]
+
+  desc "Build a tar.gz of default logstash plugins with all dependencies"
+  task "tar" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
+    puts("[artifact:tar] Building tar.gz of default plugins")
+    build_tar
+  end
+
+  desc "Build a zip of default logstash plugins with all dependencies"
+  task "zip" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
+    puts("[artifact:zip] Building zip of default plugins")
+    build_zip
+  end
+
+  desc "Build an RPM of logstash with all dependencies"
+  task "rpm" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
+    puts("[artifact:rpm] building rpm package")
+    package("centos", "5")
+  end
+
+  desc "Build a DEB of logstash with all dependencies"
+  task "deb" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
+    puts("[artifact:deb] building deb package")
+    package("ubuntu", "12.04")
+  end
+
+  desc "Generate logstash core gems"
+  task "gems" => ["prepare"] do
+    Rake::Task["artifact:build-logstash-core"].invoke
+    Rake::Task["artifact:build-logstash-core-plugin-api"].invoke
+  end
+
+  # "all-plugins" version of tasks
+  desc "Generate rpm, deb, tar and zip artifacts (\"all-plugins\" version)"
+  task "all-all-plugins" => ["prepare-all", "build"]
+
+  desc "Build a zip of all logstash plugins from logstash-plugins github repo"
+  task "zip-all-plugins" => ["prepare-all", "generate_build_metadata"] do
+    puts("[artifact:zip] Building zip of all plugins")
+    build_zip "-all-plugins"
+  end
+
+  desc "Build a tar.gz of all logstash plugins from logstash-plugins github repo"
+  task "tar-all-plugins" => ["prepare-all", "generate_build_metadata"] do
+    puts("[artifact:tar] Building tar.gz of all plugins")
+    build_tar "-all-plugins"
+  end
+
+  # Auxiliary tasks
+  task "build" => [:generate_build_metadata] do
+    Rake::Task["artifact:gems"].invoke unless SNAPSHOT_BUILD
+    Rake::Task["artifact:deb"].invoke
+    Rake::Task["artifact:rpm"].invoke
+    Rake::Task["artifact:zip"].invoke
+    Rake::Task["artifact:tar"].invoke
+  end
+
+  task "generate_build_metadata" do
+    return if defined?(BUILD_METADATA_FILE)
+    BUILD_METADATA_FILE = Tempfile.new('build.rb')
+    build_info = {
+      "build_date" => Time.now.iso8601,
+      "build_sha" => `git rev-parse HEAD`.chomp,
+      "build_snapshot" => SNAPSHOT_BUILD
+    }
+    metadata = [ "# encoding: utf-8", "BUILD_INFO = #{build_info}" ]
+    IO.write(BUILD_METADATA_FILE.path, metadata.join("\n"))
+  end
+
   # We create an empty bundle config file
   # This will allow the deb and rpm to create a file
   # with the correct user group and permission.
@@ -62,8 +152,8 @@ namespace "artifact" do
   end
 
   # locate the "gem "logstash-core" ..." line in Gemfile, and if the :path => "..." option if specified
-  # build and install the local logstash-core gem otherwise just do nothing, bundler will deal with it.
-  task "install-logstash-core" do
+  # build the local logstash-core gem otherwise just do nothing, bundler will deal with it.
+  task "build-logstash-core" do
     # regex which matches a Gemfile gem definition for the logstash-core gem and captures the :path option
     gem_line_regex = /^\s*gem\s+["']logstash-core["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
 
@@ -74,105 +164,107 @@ namespace "artifact" do
     path = matches.first[gem_line_regex, 1]
 
     if path
-      Rake::Task["plugin:install-local-core-gem"].invoke("logstash-core", path)
+      Rake::Task["plugin:build-local-core-gem"].invoke("logstash-core", path)
     else
-      puts("[artifact:install-logstash-core] using logstash-core from Rubygems")
+      puts "The Gemfile should reference \"logstash-core\" gem locally through :path, but found instead: #{matches}"
+      exit(1)
     end
   end
 
-  # # locate the "gem "logstash-core-event*" ..." line in Gemfile, and if the :path => "." option if specified
-  # # build and install the local logstash-core-event* gem otherwise just do nothing, bundler will deal with it.
-  task "install-logstash-core-event" do
-    # regex which matches a Gemfile gem definition for the logstash-core-event* gem and captures the gem name and :path option
-    gem_line_regex = /^\s*gem\s+["'](logstash-core-event[^"^']*)["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
+  # locate the "gem "logstash-core-plugin-api" ..." line in Gemfile, and if the :path => "..." option if specified
+  # build the local logstash-core-plugin-api gem otherwise just do nothing, bundler will deal with it.
+  task "build-logstash-core-plugin-api" do
+    # regex which matches a Gemfile gem definition for the logstash-core gem and captures the :path option
+    gem_line_regex = /^\s*gem\s+["']logstash-core-plugin-api["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
 
     lines = File.readlines("Gemfile")
     matches = lines.select{|line| line[gem_line_regex]}
-    abort("ERROR: Gemfile format error, need a single logstash-core-event gem specification") if matches.size != 1
+    abort("ERROR: Gemfile format error, need a single logstash-core-plugin-api gem specification") if matches.size != 1
 
-    name = matches.first[gem_line_regex, 1]
-    path = matches.first[gem_line_regex, 2]
+    path = matches.first[gem_line_regex, 1]
 
     if path
-      Rake::Task["plugin:install-local-core-gem"].invoke(name, path)
+      Rake::Task["plugin:build-local-core-gem"].invoke("logstash-core-plugin-api", path)
     else
-      puts("[artifact:install-logstash-core] using #{name} from Rubygems")
+      puts "The Gemfile should reference \"logstash-core-plugin-api\" gem locally through :path, but found instead: #{matches}"
+      exit(1)
     end
   end
 
-  task "prepare" => ["bootstrap", "plugin:install-default", "install-logstash-core", "install-logstash-core-event", "clean-bundle-config"]
-  task "prepare-all" => ["bootstrap", "plugin:install-all", "install-logstash-core", "install-logstash-core-event", "clean-bundle-config"]
-
-  desc "Build a tar.gz of default logstash plugins with all dependencies"
-  task "tar" => ["prepare"] do
-    puts("[artifact:tar] Building tar.gz of default plugins")
-    build_tar
+  task "prepare" do
+    if ENV['SKIP_PREPARE'] != "1"
+      ["bootstrap", "plugin:install-default", "artifact:clean-bundle-config"].each {|task| Rake::Task[task].invoke }
+    end
   end
 
-  desc "Build a tar.gz of all logstash plugins from logstash-plugins github repo"
-  task "tar-all-plugins" => ["prepare-all"] do
-    puts("[artifact:tar] Building tar.gz of all plugins")
-    build_tar "-all-plugins"
+  task "prepare-all" do
+    if ENV['SKIP_PREPARE'] != "1"
+      ["bootstrap", "plugin:install-all", "artifact:clean-bundle-config"].each {|task| Rake::Task[task].invoke }
+    end
   end
 
   def build_tar(tar_suffix = nil)
     require "zlib"
     require "archive/tar/minitar"
     require "logstash/version"
-    tarpath = "build/logstash#{tar_suffix}-#{LOGSTASH_VERSION}.tar.gz"
+    tarpath = "build/logstash#{tar_suffix}-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}.tar.gz"
     puts("[artifact:tar] building #{tarpath}")
     gz = Zlib::GzipWriter.new(File.new(tarpath, "wb"), Zlib::BEST_COMPRESSION)
     tar = Archive::Tar::Minitar::Output.new(gz)
     files.each do |path|
-      stat = File.lstat(path)
-      path_in_tar = "logstash-#{LOGSTASH_VERSION}/#{path}"
-      opts = {
-        :size => stat.size,
-        :mode => stat.mode,
-        :mtime => stat.mtime
-      }
-      if stat.directory?
-        tar.tar.mkdir(path_in_tar, opts)
-      else
-        tar.tar.add_file_simple(path_in_tar, opts) do |io|
-          File.open(path,'rb') do |fd|
-            chunk = nil
-            size = 0
-            size += io.write(chunk) while chunk = fd.read(16384)
-            if stat.size != size
-              raise "Failure to write the entire file (#{path}) to the tarball. Expected to write #{stat.size} bytes; actually write #{size}"
-            end
-          end
-        end
-      end
+      write_to_tar(tar, path, "logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}/#{path}")
     end
+
+    # add build.rb to tar
+    metadata_file_path_in_tar = File.join("logstash-core", "lib", "logstash", "build.rb")
+    path_in_tar = File.join("logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}", metadata_file_path_in_tar)
+    write_to_tar(tar, BUILD_METADATA_FILE.path, path_in_tar)
+
     tar.close
     gz.close
     puts "Complete: #{tarpath}"
   end
 
-  desc "Build a zip of default logstash plugins with all dependencies"
-  task "zip" => ["prepare"] do
-    puts("[artifact:zip] Building zip of default plugins")
-    build_zip
-  end
-
-  desc "Build a zip of all logstash plugins from logstash-plugins github repo"
-  task "zip-all-plugins" => ["prepare-all"] do
-    puts("[artifact:zip] Building zip of all plugins")
-    build_zip "-all-plugins"
+  def write_to_tar(tar, path, path_in_tar)
+    stat = File.lstat(path)
+    opts = {
+      :size => stat.size,
+      :mode => stat.mode,
+      :mtime => stat.mtime
+    }
+    if stat.directory?
+      tar.tar.mkdir(path_in_tar, opts)
+    else
+      tar.tar.add_file_simple(path_in_tar, opts) do |io|
+        File.open(path,'rb') do |fd|
+          chunk = nil
+          size = 0
+          size += io.write(chunk) while chunk = fd.read(16384)
+          if stat.size != size
+            raise "Failure to write the entire file (#{path}) to the tarball. Expected to write #{stat.size} bytes; actually write #{size}"
+          end
+        end
+      end
+    end
   end
 
   def build_zip(zip_suffix = "")
     require 'zip'
-    zippath = "build/logstash#{zip_suffix}-#{LOGSTASH_VERSION}.zip"
+    zippath = "build/logstash#{zip_suffix}-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}.zip"
     puts("[artifact:zip] building #{zippath}")
     File.unlink(zippath) if File.exists?(zippath)
     Zip::File.open(zippath, Zip::File::CREATE) do |zipfile|
       files.each do |path|
-        path_in_zip = "logstash-#{LOGSTASH_VERSION}/#{path}"
+        path_in_zip = "logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}/#{path}"
         zipfile.add(path_in_zip, path)
       end
+
+      # add build.rb to zip
+      metadata_file_path_in_zip = File.join("logstash-core", "lib", "logstash", "build.rb")
+      path_in_zip = File.join("logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}", metadata_file_path_in_zip)
+      path = BUILD_METADATA_FILE.path
+      Zip.continue_on_exists_proc = true
+      zipfile.add(path_in_zip, path)
     end
     puts "Complete: #{zippath}"
   end
@@ -185,36 +277,46 @@ namespace "artifact" do
 
     dir = FPM::Package::Dir.new
 
+    metadata_file_path = File.join("logstash-core", "lib", "logstash", "build.rb")
+    metadata_source_file_path = BUILD_METADATA_FILE.path
+    dir.input("#{metadata_source_file_path}=/usr/share/logstash/#{metadata_file_path}")
+
     files.each do |path|
       next if File.directory?(path)
-      dir.input("#{path}=/opt/logstash/#{path}")
+      # Omit any config dir from /usr/share/logstash for packages, since we're
+      # using /etc/logstash below
+      next if path.start_with?("config/")
+      dir.input("#{path}=/usr/share/logstash/#{path}")
     end
 
     basedir = File.join(File.dirname(__FILE__), "..")
 
-    File.join(basedir, "pkg", "logrotate.conf").tap do |path|
-      dir.input("#{path}=/etc/logrotate.d/logstash")
-    end
-
     # Create an empty /var/log/logstash/ directory in the package
     # This is a bit obtuse, I suppose, but it is necessary until
     # we find a better way to do this with fpm.
     Stud::Temporary.directory do |empty|
+      dir.input("#{empty}/=/usr/share/logstash/data")
       dir.input("#{empty}/=/var/log/logstash")
       dir.input("#{empty}/=/var/lib/logstash")
       dir.input("#{empty}/=/etc/logstash/conf.d")
     end
 
+    File.join(basedir, "pkg", "log4j2.properties").tap do |path|
+      dir.input("#{path}=/etc/logstash")
+    end
+    
+    package_filename = "logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}.TYPE"
+
     case platform
       when "redhat", "centos"
-        File.join(basedir, "pkg", "logrotate.conf").tap do |path|
-          dir.input("#{path}=/etc/logrotate.d/logstash")
+        File.join(basedir, "pkg", "startup.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.default").tap do |path|
-          dir.input("#{path}=/etc/sysconfig/logstash")
+        File.join(basedir, "pkg", "jvm.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
-          dir.input("#{path}=/etc/init.d/logstash")
+        File.join(basedir, "config", "logstash.yml").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
         require "fpm/package/rpm"
         out = dir.convert(FPM::Package::RPM)
@@ -222,25 +324,29 @@ namespace "artifact" do
         out.attributes[:rpm_use_file_permissions] = true
         out.attributes[:rpm_user] = "root"
         out.attributes[:rpm_group] = "root"
-        out.config_files << "etc/sysconfig/logstash"
-        out.config_files << "etc/logrotate.d/logstash"
-        out.config_files << "/etc/init.d/logstash"
+        out.attributes[:rpm_os] = "linux"
+        out.config_files << "/etc/logstash/startup.options"
+        out.config_files << "/etc/logstash/jvm.options"
+        out.config_files << "/etc/logstash/logstash.yml"
       when "debian", "ubuntu"
-        File.join(basedir, "pkg", "logstash.default").tap do |path|
-          dir.input("#{path}=/etc/default/logstash")
+        File.join(basedir, "pkg", "startup.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
-          dir.input("#{path}=/etc/init.d/logstash")
+        File.join(basedir, "pkg", "jvm.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
+        end
+        File.join(basedir, "config", "logstash.yml").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
         require "fpm/package/deb"
         out = dir.convert(FPM::Package::Deb)
         out.license = "Apache 2.0"
         out.attributes[:deb_user] = "root"
         out.attributes[:deb_group] = "root"
-        out.attributes[:deb_suggests] = "java7-runtime-headless"
-        out.config_files << "/etc/default/logstash"
-        out.config_files << "/etc/logrotate.d/logstash"
-        out.config_files << "/etc/init.d/logstash"
+        out.attributes[:deb_suggests] = "java8-runtime-headless"
+        out.config_files << "/etc/logstash/startup.options"
+        out.config_files << "/etc/logstash/jvm.options"
+        out.config_files << "/etc/logstash/logstash.yml"
     end
 
     # Packaging install/removal scripts
@@ -265,7 +371,6 @@ namespace "artifact" do
     out.url = "http://www.elasticsearch.org/overview/logstash/"
     out.description = "An extensible logging pipeline"
     out.vendor = "Elasticsearch"
-    out.dependencies << "logrotate"
 
     # Because we made a mistake in naming the RC version numbers, both rpm and deb view
     # "1.5.0.rc1" higher than "1.5.0". Setting the epoch to 1 ensures that we get a kind
@@ -282,6 +387,7 @@ namespace "artifact" do
     #   correct version of OpenJDK is impossible because there is no guarantee that
     #   is impossible for the same reasons as the Red Hat section above.
     # References:
+    # - https://github.com/elastic/logstash/issues/6275
     # - http://www.elasticsearch.org/blog/java-1-7u55-safe-use-elasticsearch-lucene/
     # - deb: https://github.com/elasticsearch/logstash/pull/1008
     # - rpm: https://github.com/elasticsearch/logstash/pull/1290
@@ -290,23 +396,11 @@ namespace "artifact" do
 
     out.attributes[:force?] = true # overwrite the rpm/deb/etc being created
     begin
-      path = File.join(basedir, "build", out.to_s)
+      path = File.join(basedir, "build", out.to_s(package_filename))
       x = out.output(path)
       puts "Completed: #{path}"
     ensure
       out.cleanup
     end
   end # def package
-
-  desc "Build an RPM of logstash with all dependencies"
-  task "rpm" => ["prepare"] do
-    puts("[artifact:rpm] building rpm package")
-    package("centos", "5")
-  end
-
-  desc "Build a DEB of logstash with all dependencies"
-  task "deb" => ["prepare"] do
-    puts("[artifact:deb] building deb package")
-    package("ubuntu", "12.04")
-  end
 end
diff --git a/rakelib/benchmark.rake b/rakelib/benchmark.rake
index 148922f6531..29bfd2c2844 100644
--- a/rakelib/benchmark.rake
+++ b/rakelib/benchmark.rake
@@ -1,7 +1,7 @@
 namespace :benchmark do
   desc "Run benchmark code in benchmark/*.rb"
   task :run => ["test:setup"] do
-    path = File.join(LogStash::Environment::LOGSTASH_HOME, "benchmark", "*.rb")
+    path = File.join(LogStash::Environment::LOGSTASH_HOME, "tools/benchmark", "*.rb")
     Dir.glob(path).each { |f| require f }
   end
 end
diff --git a/rakelib/build.rake b/rakelib/build.rake
index 2b443add8b7..45aec1f9b5f 100644
--- a/rakelib/build.rake
+++ b/rakelib/build.rake
@@ -4,3 +4,6 @@ end
 directory "build/bootstrap" => "build" do |task, args|
   mkdir_p task.name unless File.directory?(task.name)
 end
+directory "build/gems" => "build" do |task, args|
+  mkdir_p task.name unless File.directory?(task.name)
+end
diff --git a/rakelib/compile.rake b/rakelib/compile.rake
index aa20eb7091b..fd113a47664 100644
--- a/rakelib/compile.rake
+++ b/rakelib/compile.rake
@@ -11,11 +11,11 @@ namespace "compile" do
 
   task "grammar" => "logstash-core/lib/logstash/config/grammar.rb"
 
-  task "logstash-core-event-java" do
-    puts("Building logstash-core-event-java using gradle")
-    system("logstash-core-event-java/gradlew", "jar", "-p", "./logstash-core-event-java")
+  task "logstash-core-java" do
+    puts("Building logstash-core using gradle")
+    system("./gradlew", "jar")
   end
 
   desc "Build everything"
-  task "all" => ["grammar", "logstash-core-event-java"]
+  task "all" => ["grammar", "logstash-core-java"]
 end
diff --git a/rakelib/default_plugins.rb b/rakelib/default_plugins.rb
index 04e342d6199..433acedd445 100644
--- a/rakelib/default_plugins.rb
+++ b/rakelib/default_plugins.rb
@@ -1,153 +1,6 @@
 module LogStash
   module RakeLib
 
-    # plugins included by default in the logstash distribution
-    DEFAULT_PLUGINS = %w(
-      logstash-input-heartbeat
-      logstash-output-zeromq
-      logstash-codec-collectd
-      logstash-output-xmpp
-      logstash-codec-dots
-      logstash-codec-edn
-      logstash-codec-edn_lines
-      logstash-codec-fluent
-      logstash-codec-es_bulk
-      logstash-codec-graphite
-      logstash-codec-json
-      logstash-codec-json_lines
-      logstash-codec-line
-      logstash-codec-msgpack
-      logstash-codec-multiline
-      logstash-codec-netflow
-      logstash-codec-oldlogstashjson
-      logstash-codec-plain
-      logstash-codec-rubydebug
-      logstash-filter-anonymize
-      logstash-filter-checksum
-      logstash-filter-clone
-      logstash-filter-csv
-      logstash-filter-date
-      logstash-filter-dns
-      logstash-filter-drop
-      logstash-filter-fingerprint
-      logstash-filter-geoip
-      logstash-filter-grok
-      logstash-filter-json
-      logstash-filter-kv
-      logstash-filter-metrics
-      logstash-filter-multiline
-      logstash-filter-mutate
-      logstash-filter-ruby
-      logstash-filter-sleep
-      logstash-filter-split
-      logstash-filter-syslog_pri
-      logstash-filter-throttle
-      logstash-filter-urldecode
-      logstash-filter-useragent
-      logstash-filter-uuid
-      logstash-filter-xml
-      logstash-input-couchdb_changes
-      logstash-input-elasticsearch
-      logstash-input-eventlog
-      logstash-input-exec
-      logstash-input-file
-      logstash-input-ganglia
-      logstash-input-gelf
-      logstash-input-generator
-      logstash-input-graphite
-      logstash-input-http
-      logstash-input-http_poller
-      logstash-input-imap
-      logstash-input-irc
-      logstash-input-jdbc
-      logstash-input-log4j
-      logstash-input-lumberjack
-      logstash-input-pipe
-      logstash-input-rabbitmq
-      logstash-input-redis
-      logstash-input-s3
-      logstash-input-snmptrap
-      logstash-input-sqs
-      logstash-input-stdin
-      logstash-input-syslog
-      logstash-input-tcp
-      logstash-input-twitter
-      logstash-input-udp
-      logstash-input-unix
-      logstash-input-xmpp
-      logstash-input-zeromq
-      logstash-input-kafka
-      logstash-input-beats
-      logstash-output-cloudwatch
-      logstash-output-csv
-      logstash-output-elasticsearch
-      logstash-output-email
-      logstash-output-exec
-      logstash-output-file
-      logstash-output-ganglia
-      logstash-output-gelf
-      logstash-output-graphite
-      logstash-output-hipchat
-      logstash-output-http
-      logstash-output-irc
-      logstash-output-juggernaut
-      logstash-output-lumberjack
-      logstash-output-nagios
-      logstash-output-nagios_nsca
-      logstash-output-null
-      logstash-output-opentsdb
-      logstash-output-pagerduty
-      logstash-output-pipe
-      logstash-output-rabbitmq
-      logstash-output-redis
-      logstash-output-s3
-      logstash-output-sns
-      logstash-output-sqs
-      logstash-output-statsd
-      logstash-output-stdout
-      logstash-output-tcp
-      logstash-output-udp
-      logstash-output-kafka
-    )
-
-    # plugins required to run the logstash core specs
-    CORE_SPECS_PLUGINS = %w(
-      logstash-filter-clone
-      logstash-filter-mutate
-      logstash-filter-multiline
-      logstash-input-generator
-      logstash-input-stdin
-      logstash-input-tcp
-      logstash-output-stdout
-    )
-
-    TEST_JAR_DEPENDENCIES_PLUGINS = %w(
-      logstash-input-kafka
-    )
-
-    TEST_VENDOR_PLUGINS = %w(
-      logstash-codec-collectd
-    )
-
-    ALL_PLUGINS_SKIP_LIST = Regexp.union([
-      /^logstash-filter-yaml$/,
-      /jms$/,
-      /example$/,
-      /drupal/i,
-      /^logstash-output-logentries$/,
-      /^logstash-input-jdbc$/,
-      /^logstash-output-newrelic$/,
-      /^logstash-output-slack$/,
-      /^logstash-input-neo4j$/,
-      /^logstash-output-neo4j$/,
-      /^logstash-input-perfmon$/,
-      /^logstash-output-webhdfs$/,
-      /^logstash-input-rackspace$/,
-      /^logstash-output-rackspace$/,
-      /^logstash-input-dynamodb$/
-    ])
-
-
     # @return [Array<String>] list of all plugin names as defined in the logstash-plugins github organization, minus names that matches the ALL_PLUGINS_SKIP_LIST
     def self.fetch_all_plugins
       require 'octokit'
@@ -162,5 +15,27 @@ def self.is_released?(plugin)
       require 'gems'
       Gems.info(plugin) != "This rubygem could not be found."
     end
+
+    def self.fetch_plugins_for(type)
+      # Lets use the standard library here, in the context of the bootstrap the
+      # logstash-core could have failed to be installed.
+      require "json"
+      JSON.load(::File.read("rakelib/plugins-metadata.json")).select do |_, metadata|
+        metadata[type]
+      end.keys
+    end
+
+    # plugins included by default in the logstash distribution
+    DEFAULT_PLUGINS = self.fetch_plugins_for("default-plugins").freeze
+
+    # plugins required to run the logstash core specs
+    CORE_SPECS_PLUGINS = self.fetch_plugins_for("core-specs").freeze
+
+    TEST_JAR_DEPENDENCIES_PLUGINS = self.fetch_plugins_for("test-jar-dependencies").freeze
+
+    TEST_VENDOR_PLUGINS = self.fetch_plugins_for("test-vendor-plugin").freeze
+
+    ALL_PLUGINS_SKIP_LIST = Regexp.union(self.fetch_plugins_for("skip-list")).freeze
+
   end
 end
diff --git a/rakelib/dependency.rake b/rakelib/dependency.rake
index 56471a99640..323e76ff275 100644
--- a/rakelib/dependency.rake
+++ b/rakelib/dependency.rake
@@ -9,7 +9,7 @@ namespace "dependency" do
   end # task rbx-stdlib
 
   task "archive-tar-minitar" do
-    Rake::Task["gem:require"].invoke("minitar", ">= 0")
+    Rake::Task["gem:require"].invoke("minitar", "0.5.4")
   end # task archive-minitar
 
   task "stud" do
diff --git a/rakelib/docs.rake b/rakelib/docs.rake
index c7eb1d32152..437b0b0a29f 100644
--- a/rakelib/docs.rake
+++ b/rakelib/docs.rake
@@ -1,25 +1,26 @@
-namespace "docs" do
+# encoding: utf-8
+require "fileutils"
+
+DEFAULT_DOC_DIRECTORY = ::File.join(::File.dirname(__FILE__), "..", "build", "docs")
 
+namespace "docs" do
   desc "Generate documentation for all plugins"
   task "generate" do
     Rake::Task['plugin:install-all'].invoke
-    Rake::Task['docs:generate-docs'].invoke
-    Rake::Task['docs:generate-index'].invoke
+    Rake::Task['docs:generate-plugins'].invoke
   end
 
-  task "generate-docs" do
+  desc "Generate the doc for all the currently installed plugins"
+  task "generate-plugins", [:output] do |t, args|
+    args.with_defaults(:output => DEFAULT_DOC_DIRECTORY)
+
     require "bootstrap/environment"
-    pattern = "#{LogStash::Environment.logstash_gem_home}/gems/logstash-*/lib/logstash/{input,output,filter,codec}s/*.rb"
-    list    = Dir.glob(pattern).join(" ")
-    cmd     = "bin/bundle exec ruby docs/asciidocgen.rb -o asciidoc_generated #{list}"
-    system(cmd)
-  end
+    require "logstash-core/logstash-core"
+    LogStash::Bundler.setup!({:without => [:build]})
+
+    require "logstash/docgen/logstash_generator"
 
-  task "generate-index" do
-    list = [ 'inputs', 'outputs', 'filters', 'codecs' ]
-    list.each do |type|
-      cmd = "bin/bundle exec ruby docs/asciidoc_index.rb asciidoc_generated #{type}"
-      system(cmd)
-    end
+    FileUtils.mkdir_p(args[:output])
+    exit(LogStash::Docgen::LogStashGenerator.new(args[:output]).generate_plugins_docs)
   end
 end
diff --git a/rakelib/license.rake b/rakelib/license.rake
new file mode 100644
index 00000000000..fb6ba92be7a
--- /dev/null
+++ b/rakelib/license.rake
@@ -0,0 +1,84 @@
+require "rubygems/specification"
+require "bootstrap/environment"
+require_relative "default_plugins"
+require 'rubygems'
+
+namespace "license" do
+
+  SKIPPED_DEPENDENCIES = [
+    "logstash-core-plugin-api"
+  ]
+
+  GEM_INSTALL_PATH = File.join(LogStash::Environment.logstash_gem_home, "gems")
+
+  NOTICE_FILE_PATH = File.join(LogStash::Environment::LOGSTASH_HOME, "NOTICE.TXT")
+
+  desc "Generate a license/notice file for default plugin dependencies"
+  task "generate-notice-file" => ["bootstrap", "plugin:install-default"] do
+    puts("[license:generate-notice-file] Generating notice file for default plugin dependencies")
+    generate_notice_file
+  end
+
+  def generate_notice_file
+    File.open(NOTICE_FILE_PATH,'w') do |file|
+      begin
+        add_logstash_header(file)
+        add_dependencies_licenses(file)
+      rescue => e
+        raise "Unable to generate notice file, #{e}"
+      end
+    end
+  end
+
+  def add_logstash_header(notice_file)
+    copyright_year = Time.now.year
+    notice_file << "Logstash\n"
+    notice_file << "Copyright 2012-#{copyright_year} Elasticsearch\n"
+    notice_file << "\nThis product includes software developed by The Apache Software Foundation (http://www.apache.org/).\n"
+    notice_file << "\n==========================================================================\n"
+    notice_file << "Third party libraries bundled by the Logstash project:\n\n"
+  end
+
+  def add_dependencies_licenses(notice_file)
+    # to keep track of all the plugins we've traversed
+    seen_dependencies = Hash.new
+    LogStash::RakeLib::DEFAULT_PLUGINS.each do |plugin|
+      gemspec = Gem::Specification.find_all_by_name(plugin)[0]
+      gemspec.runtime_dependencies.each do |dep|
+        name = dep.name
+        next if SKIPPED_DEPENDENCIES.include?(name) || seen_dependencies.key?(name)
+        seen_dependencies[name] = true
+        # ignore all the runtime logstash-* plugin dependencies
+        next if name.start_with?("logstash")
+        path = gem_home(dep.to_spec)
+        dep.to_spec.licenses.each do |license|
+          notice = ""
+          license = ""
+          Dir.glob(File.join(path, '*LICENSE*')) do |path|
+            notice << File.read(path)
+            notice << "\n"
+          end
+          Dir.glob(File.join(path, '*NOTICE*')) do |path|
+            license << File.read(path)
+            license << "\n"
+          end
+
+          if !notice.empty? || !license.empty?
+            notice_file << "==========================================================================\n"
+            notice_file << "RubyGem: #{name} Version: #{dep.to_spec.version}\n"
+            notice_file << notice
+            notice_file << license
+          end
+        end
+      end
+    end
+  end
+
+  def gem_home(spec)
+    spec_base_name = "#{spec.name}-#{spec.version}"
+    if spec.platform == "java"
+      spec_base_name += "-java"
+    end
+    File.join(GEM_INSTALL_PATH, "#{spec_base_name}")
+  end
+end
diff --git a/rakelib/package.rake b/rakelib/package.rake
index 56606c93136..73885a013d2 100644
--- a/rakelib/package.rake
+++ b/rakelib/package.rake
@@ -1,7 +1,7 @@
 namespace "package" do
 
   task "bundle" do
-    system("bin/logstash-plugin", "package")
+    system("bin/logstash-plugin", "pack")
     raise(RuntimeError, $!.to_s) unless $?.success?
   end
 
diff --git a/rakelib/plugin.rake b/rakelib/plugin.rake
index 5620def5315..79be84172b7 100644
--- a/rakelib/plugin.rake
+++ b/rakelib/plugin.rake
@@ -1,4 +1,5 @@
 require_relative "default_plugins"
+require 'rubygems'
 
 namespace "plugin" do
 
@@ -9,7 +10,7 @@ namespace "plugin" do
 
   task "install-development-dependencies" do
     puts("[plugin:install-development-dependencies] Installing development dependencies of all installed plugins")
-    install_plugins("--development")
+    install_plugins("--development",  "--preserve")
 
     task.reenable # Allow this task to be run again
   end
@@ -17,35 +18,35 @@ namespace "plugin" do
   task "install", :name do |task, args|
     name = args[:name]
     puts("[plugin:install] Installing plugin: #{name}")
-    install_plugins("--no-verify", name)
+    install_plugins("--no-verify", "--preserve", name)
 
     task.reenable # Allow this task to be run again
   end # task "install"
 
   task "install-default" do
     puts("[plugin:install-default] Installing default plugins")
-    install_plugins("--no-verify", *LogStash::RakeLib::DEFAULT_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::DEFAULT_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-core" do
     puts("[plugin:install-core] Installing core plugins")
-    install_plugins("--no-verify", *LogStash::RakeLib::CORE_SPECS_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::CORE_SPECS_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-jar-dependencies" do
     puts("[plugin:install-jar-dependencies] Installing jar_dependencies plugins for testing")
-    install_plugins("--no-verify", *LogStash::RakeLib::TEST_JAR_DEPENDENCIES_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::TEST_JAR_DEPENDENCIES_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-vendor" do
     puts("[plugin:install-jar-dependencies] Installing vendor plugins for testing")
-    install_plugins("--no-verify", *LogStash::RakeLib::TEST_VENDOR_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::TEST_VENDOR_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
@@ -58,7 +59,7 @@ namespace "plugin" do
     # TODO Push this downstream to #install_plugins
     p.each do |plugin|
       begin
-        install_plugins("--no-verify", plugin)
+        install_plugins("--no-verify", "--preserve", plugin)
       rescue
         puts "Unable to install #{plugin}. Skipping"
         next
@@ -80,7 +81,7 @@ namespace "plugin" do
     task.reenable # Allow this task to be run again
   end
 
-  task "build-local-core-gem", [:name, :path]  do |task, args|
+  task "build-local-core-gem", [:name, :path] => ["build/gems"]  do |task, args|
     name = args[:name]
     path = args[:path]
 
@@ -88,7 +89,12 @@ namespace "plugin" do
 
     puts("[plugin:build-local-core-gem] Building #{File.join(path, name)}.gemspec")
 
-    system("cd #{path}; gem build #{name}.gemspec")
+    gem_path = nil
+    Dir.chdir(path) do
+      spec = Gem::Specification.load("#{name}.gemspec")
+      gem_path = Gem::Package.build(spec)
+    end
+    FileUtils.cp(File.join(path, gem_path), "build/gems/")
 
     task.reenable # Allow this task to be run again
   end
diff --git a/rakelib/plugins-metadata.json b/rakelib/plugins-metadata.json
new file mode 100644
index 00000000000..a2356dbb05f
--- /dev/null
+++ b/rakelib/plugins-metadata.json
@@ -0,0 +1,793 @@
+{
+	"logstash-input-heartbeat": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-collectd": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": true,
+		"skip-list": false
+	},
+	"logstash-output-xmpp": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-dots": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-edn": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-edn_lines": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-fluent": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-es_bulk": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-graphite": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-json": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-json_lines": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-line": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-msgpack": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-multiline": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-netflow": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-plain": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-codec-rubydebug": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-clone": {
+		"default-plugins": true,
+		"core-specs": true,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-csv": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-date": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-dissect": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-dns": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-drop": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-fingerprint": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-geoip": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-grok": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-json": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-kv": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-metrics": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-mutate": {
+		"default-plugins": true,
+		"core-specs": true,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-ruby": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-sleep": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-split": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-syslog_pri": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-throttle": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-urldecode": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-useragent": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-uuid": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-xml": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-couchdb_changes": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-elasticsearch": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-exec": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-file": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-ganglia": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-gelf": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-generator": {
+		"default-plugins": true,
+		"core-specs": true,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-graphite": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-http": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-http_poller": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-imap": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-irc": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-jdbc": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-log4j": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-lumberjack": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-pipe": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-rabbitmq": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-redis": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-s3": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-snmptrap": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-sqs": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-stdin": {
+		"default-plugins": true,
+		"core-specs": true,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-syslog": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-tcp": {
+		"default-plugins": true,
+		"core-specs": true,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-twitter": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-udp": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-unix": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-xmpp": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-kafka": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": true,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-input-beats": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-cloudwatch": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-csv": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-elasticsearch": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-file": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-graphite": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-http": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-irc": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-kafka": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-nagios": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-null": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-pagerduty": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-pipe": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-rabbitmq": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-redis": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-s3": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-sns": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-sqs": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-statsd": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-stdout": {
+		"default-plugins": true,
+		"core-specs": true,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-tcp": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-udp": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-output-webhdfs": {
+		"default-plugins": true,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-filter-multiline": {
+		"default-plugins": false,
+		"core-specs": true,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": false
+	},
+	"logstash-filter-yaml": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-example": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-codec-example": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-filter-example": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-output-example": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-drupal_dblog": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-output-logentries": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-output-newrelic": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-output-slack": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-neo4j": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-output-neo4j": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-perfmon": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-rackspace": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-output-rackspace": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-dynamodb": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-filter-language": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-heroku": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-output-google_cloud_storage": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-journald": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-input-log4j2": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	},
+	"logstash-codec-cloudtrail": {
+		"default-plugins": false,
+		"core-specs": false,
+		"test-jar-dependencies": false,
+		"test-vendor-plugins": false,
+		"skip-list": true
+	}
+}
diff --git a/rakelib/test.rake b/rakelib/test.rake
index 8c0d16ff4ef..00e55b3bc0b 100644
--- a/rakelib/test.rake
+++ b/rakelib/test.rake
@@ -1,4 +1,4 @@
-# we need to call exit explicity  in order to set the proper exit code, otherwise
+# we need to call exit explicitly  in order to set the proper exit code, otherwise
 # most common CI systems can not know whats up with this tests.
 
 require "pluginmanager/util"
@@ -6,13 +6,20 @@ require "pluginmanager/util"
 namespace "test" do
 
   task "setup" do
-    # Need to be run here as because if run aftewarse (after the bundler.setup task) then the report got wrong
+
+    # make sure we have a ./data/queue dir here
+    # temporary wiring until we figure proper queue initialization sequence and in test context etc.
+    mkdir "data" unless File.directory?("data")
+    mkdir "data/queue" unless File.directory?("data/queue")
+
+    # Need to be run here as because if run afterwards (after the bundler.setup task) then the report got wrong
     # numbers and misses files. There is an issue with our setup! method as this does not happen with the regular
     # bundler.setup used in regular bundler flows.
     Rake::Task["test:setup-simplecov"].invoke if ENV['COVERAGE']
 
     require "bootstrap/environment"
     LogStash::Bundler.setup!({:without => [:build]})
+    require "logstash-core"
 
     require "rspec/core/runner"
     require "rspec"
@@ -20,29 +27,25 @@ namespace "test" do
   end
 
   def core_specs
-    # note that regardless if which logstash-core-event-* gem is live, we will always run the
-    # logstash-core-event specs since currently this is the most complete Event and Timestamp specs
-    # which actually defines the Event contract and should pass regardless of the actuall underlying
-    # implementation.
-    specs = ["spec/**/*_spec.rb", "logstash-core/spec/**/*_spec.rb", "logstash-core-event/spec/**/*_spec.rb"]
-
-    # figure if the logstash-core-event-java gem is loaded and if so add its specific specs in the core specs to run
-    begin
-      require "logstash-core-event-java/version"
-      specs << "logstash-core-event-java/spec/**/*_spec.rb"
-    rescue LoadError
-      # logstash-core-event-java gem is not live, ignore and skip specs
-    end
+    specs = ["spec/unit/**/*_spec.rb", "logstash-core/spec/**/*_spec.rb"]
 
     Rake::FileList[*specs]
   end
 
-  desc "run core specs"
-  task "core" => ["setup"] do
+  desc "run all core specs"
+  task "core" => ["core-slow"]
+
+  desc "run all core specs"
+  task "core-slow" => ["setup"] do
     exit(RSpec::Core::Runner.run([core_specs]))
   end
 
-  desc "run core specs in fail-fast mode"
+  desc "run core specs excluding slower tests like stress tests"
+  task "core-fast" => ["setup"] do
+    exit(RSpec::Core::Runner.run(["--tag", "~stress_test", core_specs]))
+  end
+
+  desc "run all core specs in fail-fast mode"
   task "core-fail-fast" => ["setup"] do
     exit(RSpec::Core::Runner.run(["--fail-fast", core_specs]))
   end
@@ -55,7 +58,7 @@ namespace "test" do
   desc "run all installed plugins specs"
   task "plugins" => ["setup"] do
     plugins_to_exclude = ENV.fetch("EXCLUDE_PLUGIN", "").split(",")
-    # grab all spec files using the live plugins gem specs. this allows correclty also running the specs
+    # grab all spec files using the live plugins gem specs. this allows correctly also running the specs
     # of a local plugin dir added using the Gemfile :path option. before this, any local plugin spec would
     # not be run because they were not under the vendor/bundle/jruby/1.9/gems path
     test_files = LogStash::PluginManager.find_plugins_gem_specs.map do |spec|
@@ -91,7 +94,7 @@ namespace "test" do
         add_filter pattern
       end
 
-      add_group "bootstrap", "bootstrap/" # This module is used during bootstraping of LS
+      add_group "bootstrap", "bootstrap/" # This module is used during bootstrapping of LS
       add_group "plugin manager", "pluginmanager/" # Code related to the plugin manager
       add_group "core" do |src_file| # The LS core codebase
         /logstash\/\w+.rb/.match(src_file.filename)
@@ -104,29 +107,6 @@ namespace "test" do
     end
     task.reenable
   end
-
-  task "integration" => ["setup"] do
-    require "fileutils" 
-
-    source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-    integration_path = File.join(source, "integration_run")
-    FileUtils.rm_rf(integration_path)
-
-    exit(RSpec::Core::Runner.run([Rake::FileList["integration/**/*_spec.rb"]]))
-  end
-
-  namespace "integration" do
-    task "local" => ["setup"] do
-      require "fileutils"
-
-      source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-      integration_path = File.join(source, "integration_run")
-      FileUtils.mkdir_p(integration_path)
-
-      puts "[integration_spec] configuring local environment for running test in #{integration_path}, if you want to change this behavior delete the directory."
-      exit(RSpec::Core::Runner.run([Rake::FileList["integration/**/*_spec.rb"]]))
-    end
-  end
 end
 
 task "test" => [ "test:core" ]
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
index df29654e68e..b5cac5a353c 100644
--- a/rakelib/vendor.rake
+++ b/rakelib/vendor.rake
@@ -1,6 +1,6 @@
 namespace "vendor" do
   VERSIONS = {
-    "jruby" => { "version" => "1.7.24", "sha1" => "0c321d2192768dfec419bee6b44c7190f4db32e1" },
+    "jruby" => { "version" => "1.7.25", "sha1" => "cd15aef419f97cff274491e53fcfb8b88ec36785" },
   }
 
   def vendor(*args)
diff --git a/rakelib/version.rake b/rakelib/version.rake
new file mode 100644
index 00000000000..ec3fa8247bf
--- /dev/null
+++ b/rakelib/version.rake
@@ -0,0 +1,110 @@
+require 'yaml'
+
+def get_versions
+  yaml_versions = YAML.safe_load(IO.read("versions.yml"))
+  {
+    "logstash" => {
+      "location" => File.join("logstash-core", "lib", "logstash", "version.rb"),
+      "yaml_version" => yaml_versions["logstash"],
+      "current_version" => get_version(File.join("logstash-core", "lib", "logstash", "version.rb")),
+    },
+    "logstash-core" => {
+      "location" => File.join("logstash-core", "lib", "logstash-core", "version.rb"),
+      "yaml_version" => yaml_versions["logstash-core"],
+      "current_version" => get_version(File.join("logstash-core", "lib", "logstash-core", "version.rb")),
+    },
+    "logstash-core-plugin-api" => {
+      "location" => File.join("logstash-core-plugin-api", "lib", "logstash-core-plugin-api", "version.rb"),
+      "yaml_version" => yaml_versions["logstash-core-plugin-api"],
+      "current_version" => get_version(File.join("logstash-core-plugin-api", "lib", "logstash-core-plugin-api", "version.rb")),
+    }
+  }
+end
+
+def get_version(file)
+  text = IO.read(file)
+  version = text.match(/^[A-Z_]+ = "(.+?)"/)
+  version[1]
+end
+
+namespace :version do
+
+  desc "check if the versions.yml is out of sync with .gemspecs and other references"
+  task :check do
+    out_of_sync = get_versions.select do |component, metadata|
+      metadata["yaml_version"] != metadata["current_version"]
+    end
+    if out_of_sync.any?
+      out_of_sync.each do |component, metadata|
+        puts "#{component} is out of sync. CURRENT: #{metadata['current_version']} | YAML: #{metadata['yaml_version']}"
+      end
+      exit(1)
+    end
+  end
+
+  desc "push versions found in versions.yml to all component version locations"
+  task :sync do
+    versions = get_versions
+    # update version.rb files
+    versions.select do |component, metadata|
+      next if metadata["yaml_version"] == metadata["current_version"]
+      puts "Updating \"#{component}\" from \"#{metadata['current_version']}\" to \"#{metadata['yaml_version']}\""
+      text = IO.read(metadata["location"])
+      IO.write(metadata["location"], text.gsub(metadata["current_version"], metadata["yaml_version"]))
+    end
+
+    # ./logstash-core-plugin-api/logstash-core-plugin-api.gemspec:  gem.add_runtime_dependency "logstash-core", "5.0.0.dev"
+    logstash_core_plugin_api_gemspec = File.join("logstash-core-plugin-api", "logstash-core-plugin-api.gemspec")
+    logstash_core_version = versions['logstash-core']['yaml_version']
+    text = IO.read(logstash_core_plugin_api_gemspec)
+    IO.write(logstash_core_plugin_api_gemspec, text.sub(
+      /  gem.add_runtime_dependency \"logstash-core\", \".+?\"/,
+      "  gem.add_runtime_dependency \"logstash-core\", \"#{logstash_core_version}\""))
+  end
+
+  desc "show version of core components"
+  task :show do
+    Rake::Task["version:sync"].invoke; Rake::Task["version:sync"].reenable
+    get_versions.each do |component, metadata|
+      puts "#{component}: #{metadata['yaml_version']}"
+    end
+  end
+
+  desc "set version of logstash, logstash-core"
+  task :set, [:version] => [:validate] do |t, args|
+    hash = {}
+    get_versions.each do |component, metadata|
+      # we just assume that, usually, all components except
+      # "logstash-core-plugin-api" will be versioned together
+      # so let's skip this one and have a separate task for it
+      if component == "logstash-core-plugin-api"
+        hash[component] = metadata["yaml_version"]
+      else
+        hash[component] = args[:version]
+      end
+    end
+    IO.write("versions.yml", hash.to_yaml)
+    Rake::Task["version:sync"].invoke; Rake::Task["version:sync"].reenable
+  end
+
+  desc "set version of logstash-core-plugin-api"
+  task :set_plugin_api, [:version] => [:validate] do |t, args|
+    hash = {}
+    get_versions.each do |component, metadata|
+      if component == "logstash-core-plugin-api"
+        hash[component] = args[:version]
+      else
+        hash[component] = metadata["yaml_version"]
+      end
+    end
+    IO.write("versions.yml", hash.to_yaml)
+    Rake::Task["version:sync"].invoke; Rake::Task["version:sync"].reenable
+  end
+
+  task :validate, :version do |t, args|
+    unless Regexp.new('^\d+\.\d+\.\d+(?:-\w+\d+)?$').match(args[:version])
+      abort("Invalid version argument: \"#{args[:version]}\". Aborting...")
+    end
+  end
+
+end
diff --git a/rakelib/z_rubycheck.rake b/rakelib/z_rubycheck.rake
index ed22ed016c7..bf077b8700b 100644
--- a/rakelib/z_rubycheck.rake
+++ b/rakelib/z_rubycheck.rake
@@ -32,7 +32,7 @@ if ENV['USE_RUBY'] != '1'
     # if required at this point system gems can be installed using the system_gem task, for example:
     # Rake::Task["vendor:system_gem"].invoke(jruby, "ffi", "1.9.6")
 
-    exec(jruby, "-S", rake, *ARGV)
+    exec(jruby, "-J-Xmx1g", "-S", rake, *ARGV)
   end
 end
 
diff --git a/settings.gradle b/settings.gradle
new file mode 100644
index 00000000000..4238cd5b34f
--- /dev/null
+++ b/settings.gradle
@@ -0,0 +1,2 @@
+include ':logstash-core'
+project(':logstash-core').projectDir = new File('./logstash-core')
diff --git a/spec/bootstrap/environment_spec.rb b/spec/bootstrap/environment_spec.rb
new file mode 100644
index 00000000000..f31cfcd38e8
--- /dev/null
+++ b/spec/bootstrap/environment_spec.rb
@@ -0,0 +1,6 @@
+# encoding: utf-8
+require "spec_helper"
+require "bootstrap/environment"
+
+describe LogStash::Environment do
+end
diff --git a/spec/fixtures/dummy.gemspec b/spec/fixtures/dummy.gemspec
new file mode 100644
index 00000000000..c32bbafe6da
--- /dev/null
+++ b/spec/fixtures/dummy.gemspec
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-dummy'
+  s.version         = '0.1.1'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = Dir["lib/**/*","spec/**/*","*.gemspec","*.md","CONTRIBUTORS","Gemfile","LICENSE","NOTICE.TXT", "vendor/jar-dependencies/**/*.jar", "vendor/jar-dependencies/**/*.rb", "VERSION"]
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", ">= 1.60", "<= 2.99"
+end
diff --git a/spec/plugin_manager/install_spec.rb b/spec/plugin_manager/install_spec.rb
deleted file mode 100644
index 40eb3dfe408..00000000000
--- a/spec/plugin_manager/install_spec.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-# encoding: utf-8
-require 'spec_helper'
-require 'pluginmanager/main'
-
-describe LogStash::PluginManager::Install do
-  let(:cmd) { LogStash::PluginManager::Install.new("install") }
-
-  before(:each) do
-    expect(cmd).to receive(:validate_cli_options!).and_return(nil)
-  end
-
-  context "when validating plugins" do
-    let(:sources) { ["https://rubygems.org", "http://localhost:9292"] }
-
-    before(:each) do
-      expect(cmd).to receive(:plugins_gems).and_return([["dummy", nil]])
-      expect(cmd).to receive(:install_gems_list!).and_return(nil)
-      expect(cmd).to receive(:remove_unused_locally_installed_gems!).and_return(nil)
-      cmd.verify = true
-    end
-
-    it "should load all the sources defined in the Gemfile" do
-      expect(cmd.gemfile.gemset).to receive(:sources).and_return(sources)
-      expect(LogStash::PluginManager).to receive(:logstash_plugin?).with("dummy", nil, {:rubygems_source => sources}).and_return(true)
-      cmd.execute
-    end
-  end
-end
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index 5428fd8fd90..44919e332c7 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -6,6 +6,47 @@
 
 require "logstash/devutils/rspec/spec_helper"
 
+require "flores/rspec"
+require "flores/random"
+require "pathname"
+
+SUPPORT_DIR = Pathname.new(::File.join(::File.dirname(__FILE__), "support"))
+
+class JSONIOThingy < IO
+  def initialize; end
+  def flush; end
+
+  def puts(payload)
+    # Ensure that all log payloads are valid json.
+    LogStash::Json.load(payload)
+  end
+end
+
+RSpec.configure do |c|
+  Flores::RSpec.configure(c)
+  c.before do
+    # TODO: commented out on post-merged in master - the logger has moved to log4j
+    #
+    #
+    # Force Cabin to always have a JSON subscriber.  The main purpose of this
+    # is to catch crashes in json serialization for our logs. JSONIOThingy
+    # exists to validate taht what LogStash::Logging::JSON emits is always
+    # valid JSON.
+    # jsonvalidator = JSONIOThingy.new
+    # allow(Cabin::Channel).to receive(:new).and_wrap_original do |m, *args|
+    #   logger = m.call(*args)
+    #   logger.level = :debug
+    #   logger.subscribe(LogStash::Logging::JSON.new(jsonvalidator))
+    #
+    #   logger
+    # end
+
+    LogStash::SETTINGS.set("queue.type", "memory_acked")
+    LogStash::SETTINGS.set("queue.page_capacity", 1024 * 1024)
+    LogStash::SETTINGS.set("queue.max_events", 250)
+  end
+end
+
 def installed_plugins
   Gem::Specification.find_all.select { |spec| spec.metadata["logstash_plugin"] }.map { |plugin| plugin.name }
 end
diff --git a/spec/support/pack/empty-pack.zip b/spec/support/pack/empty-pack.zip
new file mode 100644
index 00000000000..f11de003e3f
Binary files /dev/null and b/spec/support/pack/empty-pack.zip differ
diff --git a/spec/support/pack/valid-pack.zip b/spec/support/pack/valid-pack.zip
new file mode 100644
index 00000000000..538bf0d1936
Binary files /dev/null and b/spec/support/pack/valid-pack.zip differ
diff --git a/spec/support/pack/valid-pack/logstash/valid-pack/dependencies/logstash-input-packtestdep-0.0.1.gem b/spec/support/pack/valid-pack/logstash/valid-pack/dependencies/logstash-input-packtestdep-0.0.1.gem
new file mode 100644
index 00000000000..fc51fbf46e6
Binary files /dev/null and b/spec/support/pack/valid-pack/logstash/valid-pack/dependencies/logstash-input-packtestdep-0.0.1.gem differ
diff --git a/spec/support/pack/valid-pack/logstash/valid-pack/dependencies/logstash-input-packtestdep.gemspec b/spec/support/pack/valid-pack/logstash/valid-pack/dependencies/logstash-input-packtestdep.gemspec
new file mode 100644
index 00000000000..c0d175e51e1
--- /dev/null
+++ b/spec/support/pack/valid-pack/logstash/valid-pack/dependencies/logstash-input-packtestdep.gemspec
@@ -0,0 +1,11 @@
+# coding: utf-8
+Gem::Specification.new do |spec|
+  spec.name          = "logstash-input-packtestdep"
+  spec.version       = "0.0.1"
+  spec.authors       = ["Elastic"]
+  spec.email         = ["info@elastic.co"]
+
+  spec.summary       = "a summary"
+  spec.description   = "a description"
+  spec.homepage      = "https://elastic.co"
+end
diff --git a/spec/support/pack/valid-pack/logstash/valid-pack/logstash-input-packtest-0.0.1.gem b/spec/support/pack/valid-pack/logstash/valid-pack/logstash-input-packtest-0.0.1.gem
new file mode 100644
index 00000000000..1a7f8641661
Binary files /dev/null and b/spec/support/pack/valid-pack/logstash/valid-pack/logstash-input-packtest-0.0.1.gem differ
diff --git a/spec/support/pack/valid-pack/logstash/valid-pack/logstash-input-packtest.gemspec b/spec/support/pack/valid-pack/logstash/valid-pack/logstash-input-packtest.gemspec
new file mode 100644
index 00000000000..78a7a1d48b5
--- /dev/null
+++ b/spec/support/pack/valid-pack/logstash/valid-pack/logstash-input-packtest.gemspec
@@ -0,0 +1,13 @@
+# coding: utf-8
+Gem::Specification.new do |spec|
+  spec.name          = "logstash-input-packtest"
+  spec.version       = "0.0.1"
+  spec.authors       = ["Elastic"]
+  spec.email         = ["info@elastic.co"]
+
+  spec.summary       = "a summary"
+  spec.description   = "a description"
+  spec.homepage      = "https://elastic.co"
+
+  spec.add_runtime_dependency "logstash-input-packtestdep"
+end
diff --git a/spec/bootstrap/bundler_spec.rb b/spec/unit/bootstrap/bundler_spec.rb
similarity index 100%
rename from spec/bootstrap/bundler_spec.rb
rename to spec/unit/bootstrap/bundler_spec.rb
diff --git a/spec/license_spec.rb b/spec/unit/license_spec.rb
similarity index 73%
rename from spec/license_spec.rb
rename to spec/unit/license_spec.rb
index f37f29d0431..f04e1365fe1 100644
--- a/spec/license_spec.rb
+++ b/spec/unit/license_spec.rb
@@ -1,5 +1,5 @@
 # encoding: utf-8
-require 'spec_helper'
+require_relative '../spec_helper'
 require 'rakelib/default_plugins'
 
 describe "Project licenses" do
@@ -14,7 +14,10 @@
                    /bsd/,
                    /artistic 2.*/,
                    /ruby/,
-                   /lgpl/])
+                   /lgpl/,
+                   /epl/,
+                   /elastic/i
+  ])
   }
 
   ##
@@ -25,7 +28,11 @@
     [
       # Skipped because of already included and bundled within JRuby so checking here is redundant.
       # Need to take action about jruby licenses to enable again or keep skeeping.
-      "jruby-openssl"
+      "jruby-openssl",
+      # Skipped because version 2.6.2 which we use has multiple licenses: MIT, ARTISTIC 2.0, GPL-2
+      # See https://rubygems.org/gems/mime-types/versions/2.6.2
+      # version 3.0 of mime-types (which is only compatible with Ruby 2.0) is MIT licensed
+      "mime-types"
     ]
   end
 
@@ -48,7 +55,8 @@
         next unless runtime_spec
         next if skipped_dependencies.include?(runtime_spec.name)
         runtime_spec.licenses.each do |license|
-          expect(license.downcase).to match(expected_licenses)
+          expect(license.downcase).to match(expected_licenses), 
+            lambda { "Runtime license check failed for gem #{runtime_spec.name} with version #{runtime_spec.version}" }
         end
       end
     end
diff --git a/spec/unit/plugin_manager/gem_installer_spec.rb b/spec/unit/plugin_manager/gem_installer_spec.rb
new file mode 100644
index 00000000000..616f60b7926
--- /dev/null
+++ b/spec/unit/plugin_manager/gem_installer_spec.rb
@@ -0,0 +1,61 @@
+# encoding: utf-8
+require "pluginmanager/gem_installer"
+require "pluginmanager/ui"
+require "stud/temporary"
+require "rubygems/specification"
+require "fileutils"
+require "ostruct"
+
+describe LogStash::PluginManager::GemInstaller do
+  let(:plugin_name) { "logstash-input-packtest-0.0.1" }
+  let(:simple_gem) { ::File.join(::File.dirname(__FILE__), "..", "..", "support", "pack", "valid-pack", "logstash", "valid-pack", "#{plugin_name}.gem") }
+
+  subject { described_class }
+  let(:temporary_gem_home) { p = Stud::Temporary.pathname; FileUtils.mkdir_p(p); p }
+
+  it "install the specifications in the spec dir" do
+    subject.install(simple_gem, false, temporary_gem_home)
+    spec_file = ::File.join(temporary_gem_home, "specifications", "#{plugin_name}.gemspec")
+    expect(::File.exist?(spec_file)).to be_truthy
+    expect(::File.size(spec_file)).to be > 0
+  end
+
+  it "install the gem in the gems dir" do
+    subject.install(simple_gem, false, temporary_gem_home)
+    gem_dir = ::File.join(temporary_gem_home, "gems", plugin_name)
+    expect(Dir.exist?(gem_dir)).to be_truthy
+  end
+
+  context "post_install_message" do
+    let(:message) { "Hello from the friendly pack" }
+
+    context "when present" do
+      before do
+        allow_any_instance_of(::Gem::Specification).to receive(:post_install_message).and_return(message)
+      end
+
+      context "when we want the message" do
+        it "display the message" do
+          expect(LogStash::PluginManager.ui).to receive(:info).with(message)
+          subject.install(simple_gem, true, temporary_gem_home)
+        end
+      end
+
+      context "when we dont want the message" do
+        it "doesn't display the message" do
+          expect(LogStash::PluginManager.ui).not_to receive(:info).with(message)
+          subject.install(simple_gem, false, temporary_gem_home)
+        end
+      end
+    end
+
+    context "when not present" do
+      context "when we want the message" do
+        it "doesn't display the message" do
+          expect(LogStash::PluginManager.ui).not_to receive(:info).with(message)
+          subject.install(simple_gem, true, temporary_gem_home)
+        end
+      end
+    end
+  end
+end
diff --git a/spec/pluginmanager/gemfile_spec.rb b/spec/unit/plugin_manager/gemfile_spec.rb
similarity index 100%
rename from spec/pluginmanager/gemfile_spec.rb
rename to spec/unit/plugin_manager/gemfile_spec.rb
diff --git a/spec/unit/plugin_manager/install_spec.rb b/spec/unit/plugin_manager/install_spec.rb
new file mode 100644
index 00000000000..e6f44337b5a
--- /dev/null
+++ b/spec/unit/plugin_manager/install_spec.rb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+require 'spec_helper'
+require 'pluginmanager/main'
+require "pluginmanager/pack_fetch_strategy/repository"
+
+describe LogStash::PluginManager::Install do
+  let(:cmd) { LogStash::PluginManager::Install.new("install") }
+
+  context "when validating plugins" do
+    let(:sources) { ["https://rubygems.org", "http://localhost:9292"] }
+
+    before(:each) do
+      expect(cmd).to receive(:validate_cli_options!).and_return(nil)
+      expect(cmd).to receive(:plugins_gems).and_return([["dummy", nil]])
+      expect(cmd).to receive(:install_gems_list!).and_return(nil)
+      expect(cmd).to receive(:remove_unused_locally_installed_gems!).and_return(nil)
+      cmd.verify = true
+    end
+
+    it "should load all the sources defined in the Gemfile" do
+      expect(cmd.gemfile.gemset).to receive(:sources).and_return(sources)
+      expect(LogStash::PluginManager).to receive(:logstash_plugin?).with("dummy", nil, {:rubygems_source => sources}).and_return(true)
+      cmd.execute
+    end
+  end
+
+  context "pack" do
+    let(:cmd) { LogStash::PluginManager::Install.new("install my-super-pack") }
+    before do
+      expect(cmd).to receive(:plugins_arg).and_return(["my-super-pack"]).at_least(:once)
+    end
+
+    it "reports `FileNotFoundError` exception" do
+      expect(LogStash::PluginManager::InstallStrategyFactory).to receive(:create).with(["my-super-pack"]).and_raise(LogStash::PluginManager::FileNotFoundError)
+      expect(cmd).to receive(:report_exception).with(/File not found/, be_kind_of(LogStash::PluginManager::PluginManagerError))
+      cmd.execute
+    end
+
+    it "reports `InvalidPackError` exception" do
+      expect(LogStash::PluginManager::InstallStrategyFactory).to receive(:create).with(["my-super-pack"]).and_raise(LogStash::PluginManager::InvalidPackError)
+      expect(cmd).to receive(:report_exception).with(/Invalid pack for/, be_kind_of(LogStash::PluginManager::PluginManagerError))
+      cmd.execute
+    end
+
+    it "reports any other exceptions" do
+      expect(LogStash::PluginManager::InstallStrategyFactory).to receive(:create).with(["my-super-pack"]).and_raise(StandardError)
+      expect(cmd).to receive(:report_exception).with(/Something went wrong when installing/, be_kind_of(StandardError))
+      cmd.execute
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/install_strategy_factory_spec.rb b/spec/unit/plugin_manager/install_strategy_factory_spec.rb
new file mode 100644
index 00000000000..00bdcd2b45f
--- /dev/null
+++ b/spec/unit/plugin_manager/install_strategy_factory_spec.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require "pluginmanager/install_strategy_factory"
+
+describe LogStash::PluginManager::InstallStrategyFactory do
+  subject { described_class }
+
+  context "when the plugins args is valid" do
+    let(:plugins_args) { [ "logstash-pack-mega" ] }
+
+    it "returns the first matched strategy" do
+      success = double("urifetch success")
+
+      expect(LogStash::PluginManager::PackFetchStrategy::Uri).to receive(:get_installer_for).with(plugins_args.first).and_return(success)
+      expect(subject.create(plugins_args)).to eq(success)
+    end
+
+    it "returns the matched strategy" do
+      success = double("elastic xpack success")
+
+      expect(LogStash::PluginManager::PackFetchStrategy::Repository).to receive(:get_installer_for).with(plugins_args.first).and_return(success)
+      expect(subject.create(plugins_args)).to eq(success)
+    end
+
+    it "return nil when no strategy matches" do
+      expect(LogStash::PluginManager::PackFetchStrategy::Uri).to receive(:get_installer_for).with(plugins_args.first).and_return(nil)
+      expect(LogStash::PluginManager::PackFetchStrategy::Repository).to receive(:get_installer_for).with(plugins_args.first).and_return(nil)
+      expect(subject.create(plugins_args)).to be_falsey
+    end
+  end
+
+  context "when the plugins args" do
+    context "is an empty string" do
+      let(:plugins_args) { [""] }
+
+      it "returns no strategy matched" do
+        expect(subject.create(plugins_args)).to be_falsey
+      end
+    end
+
+    context "is nil" do
+      let(:plugins_args) { [] }
+
+      it "returns no strategy matched" do
+        expect(subject.create(plugins_args)).to be_falsey
+      end
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/offline_plugin_packager_spec.rb b/spec/unit/plugin_manager/offline_plugin_packager_spec.rb
new file mode 100644
index 00000000000..367dece5d8c
--- /dev/null
+++ b/spec/unit/plugin_manager/offline_plugin_packager_spec.rb
@@ -0,0 +1,142 @@
+# encoding: utf-8
+require "pluginmanager/offline_plugin_packager"
+require "stud/temporary"
+require "stud/try"
+require "bootstrap/util/compress"
+require "fileutils"
+require "spec_helper"
+require "webmock"
+require "openssl"
+
+def retrieve_packaged_plugins(path)
+  Dir.glob(::File.join(path, "logstash", "*.gem"))
+end
+
+def retrieve_dependencies_gems(path)
+  Dir.glob(::File.join(path, "logstash", "dependencies", "*.gem"))
+end
+
+describe LogStash::PluginManager::SpecificationHelpers do
+  subject { described_class }
+
+  context "when it find gems" do
+    it "returns filtered results" do
+      expect(subject.find_by_name_with_wildcards("logstash-filter-*").all? { |spec| spec.name =~ /logstash-filter-/ }).to be_truthy
+    end
+  end
+
+  context "when it doesn't find gems" do
+    it "doesnt return gemspecs" do
+      expect(subject.find_by_name_with_wildcards("donotexistatall").size).to eq(0)
+    end
+  end
+end
+
+describe LogStash::PluginManager::OfflinePluginPackager do
+  before do
+    WebMock.allow_net_connect!
+  end
+
+  subject { described_class }
+
+  let(:temporary_dir) { Stud::Temporary.pathname }
+  let(:target) { ::File.join(temporary_dir, "my-pack.zip")}
+  let(:extract_to) { Stud::Temporary.pathname }
+  let(:retries_count) { 50 }
+  let(:retries_exceptions) { [IOError, OpenSSL::SSL::SSLError] }
+
+  context "when the plugins doesn't" do
+    let(:plugins_args) { "idotnotexist" }
+
+    it "raise an exception" do
+      expect { subject.package(plugins_args, target) }.to raise_error(LogStash::PluginManager::PluginNotFoundError)
+    end
+  end
+
+  context "when the plugins is a core gem" do
+    %W(
+    logstash-core
+    logstash-core-plugin-api).each do |plugin_name|
+      it "raise an exception with plugin: #{plugin_name}" do
+        expect { subject.package(plugin_name, target) }.to raise_error(LogStash::PluginManager::UnpackablePluginError)
+      end
+    end
+  end
+
+  context "when the plugins exist" do
+    before :all do
+      Paquet.ui = Paquet::SilentUI
+    end
+
+    before do
+      FileUtils.mkdir_p(temporary_dir)
+
+      # Because this method will reach rubygems and can be unreliable at time on CI
+      # we will retry any IOError a few times before giving up.
+      Stud.try(retries_count.times, retries_exceptions)  { subject.package(plugins_args, target) }
+
+      LogStash::Util::Zip.extract(target, extract_to)
+    end
+
+    context "one plugin specified" do
+      let(:plugins_args) { ["logstash-input-stdin"] }
+
+      it "creates a pack with the plugin" do
+        try do
+          expect(retrieve_packaged_plugins(extract_to).size).to eq(1)
+          expect(retrieve_packaged_plugins(extract_to)).to include(/logstash-input-stdin/)
+          expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+        end
+      end
+    end
+
+    context "multiples plugins" do
+      let(:plugins_args) { ["logstash-input-stdin", "logstash-input-beats"] }
+
+      it "creates pack with the plugins" do
+        try do
+          expect(retrieve_packaged_plugins(extract_to).size).to eq(2)
+
+          plugins_args.each do |plugin_name|
+            expect(retrieve_packaged_plugins(extract_to)).to include(/#{plugin_name}/)
+          end
+
+          expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+        end
+      end
+    end
+
+    context "with wildcards" do
+      let(:plugins_args) { ["logstash-filter-x*"] }
+
+      it "creates a pack with the plugins" do
+        expect(retrieve_packaged_plugins(extract_to).size).to eq(LogStash::PluginManager::SpecificationHelpers.find_by_name_with_wildcards(plugins_args.first).size)
+
+        retrieve_packaged_plugins(extract_to).each do |gem_file|
+          expect(gem_file).to match(/logstash-filter-.+/)
+        end
+
+        expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+      end
+    end
+
+    context "with wildcards and normal plugins" do
+      let(:plugins_args) { ["logstash-filter-x*", "logstash-input-beats"] }
+
+      it "creates a pack with the plugins" do
+        groups = retrieve_packaged_plugins(extract_to).group_by { |gem_file| ::File.basename(gem_file).split("-")[1] }
+
+        expect(groups["filter"].size).to eq(LogStash::PluginManager::SpecificationHelpers.find_by_name_with_wildcards(plugins_args.first).size)
+
+        groups["filter"].each do |gem_file|
+          expect(gem_file).to match(/logstash-filter-.+/)
+        end
+
+        expect(groups["input"].size).to eq(1)
+        expect(groups["input"]).to include(/logstash-input-beats/)
+
+        expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+      end
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb b/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
new file mode 100644
index 00000000000..084205bf405
--- /dev/null
+++ b/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
@@ -0,0 +1,77 @@
+# encoding: utf-8
+require "pluginmanager/pack_fetch_strategy/repository"
+require "uri"
+require "webmock/rspec"
+require "spec_helper"
+
+describe LogStash::PluginManager::PackFetchStrategy::Repository do
+  subject { described_class }
+
+  let(:plugin_name) { "hola-pack" }
+
+  context "#plugin_uri" do
+    it "generate an url from a name" do
+      matched = URI.parse("#{subject.elastic_pack_base_uri}/#{plugin_name}/#{plugin_name}-#{LOGSTASH_VERSION}.#{subject::PACK_EXTENSION}")
+      expect(subject.pack_uri(plugin_name)).to eq(matched)
+    end
+  end
+
+  context "when the remote file exist" do
+    it "is return a `RemoteInstaller`" do
+      allow(LogStash::PluginManager::Utils::HttpClient).to receive(:remote_file_exist?).with(subject.pack_uri(plugin_name)).and_return(true)
+      expect(subject.get_installer_for(plugin_name)).to be_kind_of(LogStash::PluginManager::PackInstaller::Remote)
+    end
+  end
+
+  context "when the remote file doesnt exist" do
+    it "returns false" do
+      allow(LogStash::PluginManager::Utils::HttpClient).to receive(:remote_file_exist?).with(subject.pack_uri(plugin_name)).and_return(false)
+      expect(subject.get_installer_for(plugin_name)).to be_falsey
+    end
+  end
+
+  context "when the remote host is unreachable" do
+    it "returns false and yield a debug message" do
+      # To make sure we really try to connect to a failing host we have to let it through webmock
+      host ="#{Time.now.to_i.to_s}-do-not-exist.com"
+      WebMock.disable_net_connect!(:allow => host)
+      ENV["LOGSTASH_PACK_URL"] = "http://#{host}"
+      expect(subject.get_installer_for(plugin_name)).to be_falsey
+      ENV["LOGSTASH_PACK_URL"] = nil
+    end
+  end
+
+  context "pack repository url" do
+    context "when `LOGSTASH_PACK_URL` is set in ENV" do
+      before do
+        ENV["LOGSTASH_PACK_URL"] = url
+      end
+
+      after do
+        ENV.delete("LOGSTASH_PACK_URL")
+      end
+
+      context "value is a string" do
+        let(:url) { "http://testing.dev" }
+
+        it "return the configured string" do
+          expect(subject.elastic_pack_base_uri).to eq(url)
+        end
+      end
+
+      context "value is an empty string" do
+        let(:url) { "" }
+
+        it "return the default" do
+          expect(subject.elastic_pack_base_uri).to eq(subject::DEFAULT_PACK_URL)
+        end
+      end
+    end
+
+    context "when `LOGSTASH_PACK_URL` is not set in ENV" do
+      it "return the default" do
+        expect(subject.elastic_pack_base_uri).to eq(subject::DEFAULT_PACK_URL)
+      end
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/pack_fetch_strategy/uri_spec.rb b/spec/unit/plugin_manager/pack_fetch_strategy/uri_spec.rb
new file mode 100644
index 00000000000..09effdb909c
--- /dev/null
+++ b/spec/unit/plugin_manager/pack_fetch_strategy/uri_spec.rb
@@ -0,0 +1,52 @@
+# encoding: utf-8
+require "pluginmanager/pack_fetch_strategy/uri"
+require "stud/temporary"
+
+describe LogStash::PluginManager::PackFetchStrategy::Uri do
+  subject { described_class }
+  context "when we dont have URI path" do
+    let(:plugin_path) { "logstash-input-elasticsearch" }
+
+    it "doesnt return an installer" do
+      expect(subject.get_installer_for(plugin_path)).to be_falsey
+    end
+  end
+
+  context "we have another URI scheme than file or http" do
+    let(:plugin_path) { "ftp://localhost:8888/my-pack.zip" }
+
+    it "doesnt return an installer" do
+      expect(subject.get_installer_for(plugin_path)).to be_falsey
+    end
+  end
+
+  context "we have an invalid URI scheme" do
+    let(:plugin_path) { "inv://localhost:8888/my-pack.zip" }
+
+    it "doesnt return an installer" do
+      expect(subject.get_installer_for(plugin_path)).to be_falsey
+    end
+  end
+
+  context "when we have a local path" do
+    let(:temporary_file) do
+      f = Stud::Temporary.file
+      f.write("hola")
+      f.path
+    end
+
+    let(:plugin_path) { "file://#{temporary_file}" }
+
+    it "returns a `LocalInstaller`" do
+      expect(subject.get_installer_for(plugin_path)).to be_kind_of(LogStash::PluginManager::PackInstaller::Local)
+    end
+  end
+
+  context "when we have a remote path" do
+    let(:plugin_path) { "http://localhost:8888/my-pack.zip" }
+
+    it "returns a remote installer" do
+      expect(subject.get_installer_for(plugin_path)).to be_kind_of(LogStash::PluginManager::PackInstaller::Remote)
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/pack_installer/local_spec.rb b/spec/unit/plugin_manager/pack_installer/local_spec.rb
new file mode 100644
index 00000000000..b99712a4c07
--- /dev/null
+++ b/spec/unit/plugin_manager/pack_installer/local_spec.rb
@@ -0,0 +1,68 @@
+# encoding: utf-8
+require "pluginmanager/pack_installer/local"
+require "stud/temporary"
+require "fileutils"
+
+describe LogStash::PluginManager::PackInstaller::Local do
+  subject { described_class.new(local_file) }
+
+  context "when the local file doesn't exist" do
+    let(:local_file) { ::File.join(Stud::Temporary.pathname, Time.now.to_s.to_s) }
+
+    it "raises an exception" do
+      expect { subject.execute }.to raise_error(LogStash::PluginManager::FileNotFoundError)
+    end
+  end
+
+  context "when the local file exist" do
+    context "when the file has the wrong extension" do
+      let(:local_file) { Stud::Temporary.file.path }
+
+      it "raises a InvalidPackError" do
+        expect { subject.execute }.to raise_error(LogStash::PluginManager::InvalidPackError, /Invalid format/)
+      end
+    end
+
+    context "when there is an error when the zip get uncompressed" do
+      let(:local_file) do
+        directory = Stud::Temporary.pathname
+        FileUtils.mkdir_p(directory)
+        p = ::File.join(directory, "#{Time.now.to_i.to_s}.zip")
+        FileUtils.touch(p)
+        p
+      end
+
+      it "raises a InvalidPackError" do
+        expect { subject.execute }.to raise_error(LogStash::PluginManager::InvalidPackError, /Cannot uncompress the zip/)
+      end
+    end
+
+    context "when the file doesnt have plugins in it" do
+      let(:local_file) { ::File.join(::File.dirname(__FILE__), "..", "..", "..", "support", "pack", "empty-pack.zip") }
+
+      it "raise an Invalid pack" do
+        expect { subject.execute }.to raise_error(LogStash::PluginManager::InvalidPackError, /The pack must contains at least one plugin/)
+      end
+    end
+
+    context "when the pack is valid" do
+      let(:local_file) { ::File.join(::File.dirname(__FILE__), "..", "..", "..", "support", "pack", "valid-pack.zip") }
+
+      it "install the gems" do
+        expect(::Bundler::LogstashInjector).to receive(:inject!).with(be_kind_of(LogStash::PluginManager::PackInstaller::Pack)).and_return([])
+
+        expect(::LogStash::PluginManager::GemInstaller).to receive(:install).with(/logstash-input-packtest/, anything)
+        expect(::LogStash::PluginManager::GemInstaller).to receive(:install).with(/logstash-input-packtestdep/, anything)
+
+        # Since the Gem::Indexer have side effect and we have more things loaded
+        # I have to disable it in the tests
+        mock_indexer = double("Gem::Indexer")
+        allow(mock_indexer).to receive(:ui=).with(anything)
+        expect(mock_indexer).to receive(:generate_index)
+        expect(::Gem::Indexer).to receive(:new).with(be_kind_of(String), hash_including(:build_modern => true)).and_return(mock_indexer)
+
+        expect { subject.execute }.not_to raise_error
+      end
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/pack_installer/pack_spec.rb b/spec/unit/plugin_manager/pack_installer/pack_spec.rb
new file mode 100644
index 00000000000..845f4278f61
--- /dev/null
+++ b/spec/unit/plugin_manager/pack_installer/pack_spec.rb
@@ -0,0 +1,100 @@
+# encoding: utf-8
+require "pluginmanager/pack_installer/pack"
+require "stud/temporary"
+
+describe LogStash::PluginManager::PackInstaller::Pack do
+  let(:extracted_plugin) { ::File.join(::File.dirname(__FILE__), "..", "..", "..", "support", "pack", "valid-pack") }
+
+  subject { described_class.new(extracted_plugin) }
+
+  context "when there is a plugin in the root of the pack" do
+    it "a valid pack" do
+      expect(subject.valid?).to be_truthy
+    end
+
+    it "returns the plugins" do
+      expect(subject.plugins.size).to eq(1)
+      expect(subject.plugins.collect(&:name)).to include("logstash-input-packtest")
+    end
+
+    it "returns the dependencies" do
+      expect(subject.dependencies.size).to eq(1)
+      expect(subject.dependencies.collect(&:name)).to include("logstash-input-packtestdep")
+    end
+
+    it "returns all the gems" do
+      expect(subject.gems.size).to eq(2)
+      expect(subject.gems.collect(&:name)).to include("logstash-input-packtest", "logstash-input-packtestdep")
+    end
+  end
+
+  context "when there is no plugin in the root of the pack " do
+    let(:extracted_plugin) { Stud::Temporary.pathname }
+
+    it "a invalid pack" do
+      expect(subject.valid?).to be_falsey
+    end
+  end
+end
+
+describe LogStash::PluginManager::PackInstaller::Pack::GemInformation do
+  subject { described_class.new(gem) }
+
+  shared_examples "gem information" do
+    it "returns the version" do
+      expect(subject.version).to eq("3.1.8")
+    end
+
+    it "returns the name" do
+      expect(subject.name).to eq("logstash-input-foobar")
+    end
+
+    it "returns the path of the gem" do
+      expect(subject.file).to eq(gem)
+    end
+  end
+
+  context "with a universal gem" do
+    let(:gem) { "/tmp/logstash-input-foobar-3.1.8.gem" }
+
+    include_examples "gem information"
+
+    it "returns nil for the platform" do
+      expect(subject.platform).to be_nil
+    end
+  end
+
+  context "with a java gem" do
+    let(:gem) { "/tmp/logstash-input-foobar-3.1.8-java.gem" }
+
+    include_examples "gem information"
+
+    it "returns nil for the platform" do
+      expect(subject.platform).to eq("java")
+    end
+  end
+
+  context "when its a plugin to be added to the gemfile" do
+    let(:gem) { "/tmp/logstash-input-foobar-3.1.8-java.gem" }
+
+    it "#dependency? return false" do
+      expect(subject.dependency?).to be_falsey
+    end
+
+    it "#plugin? return true" do
+      expect(subject.plugin?).to be_truthy
+    end
+  end
+
+  context "when its a dependency of a plugin" do
+    let(:gem) { "/tmp/dependencies/logstash-input-foobar-3.1.8-java.gem" }
+
+    it "#dependency? return true" do
+      expect(subject.dependency?).to be_truthy
+    end
+
+    it "#plugin? return false" do
+      expect(subject.plugin?).to be_falsey
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/pack_installer/remote_spec.rb b/spec/unit/plugin_manager/pack_installer/remote_spec.rb
new file mode 100644
index 00000000000..c6daf9ba11e
--- /dev/null
+++ b/spec/unit/plugin_manager/pack_installer/remote_spec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "pluginmanager/pack_installer/remote"
+require "webmock/rspec"
+
+describe LogStash::PluginManager::PackInstaller::Remote do
+  let(:url) { "http://localhost:8888/mypackage.zip" }
+
+  subject { described_class.new(url, LogStash::PluginManager::Utils::Downloader::SilentFeedback) }
+
+  context "when the file exist remotely" do
+    let(:content) { "around the world" }
+
+    before do
+      stub_request(:get, url).to_return(
+        { :status => 200,
+          :body => content,
+          :headers => {}}
+      )
+    end
+
+    it "download the file and do a local install" do
+      local_installer = double("LocalInstaller")
+
+      expect(local_installer).to receive(:execute)
+      expect(LogStash::PluginManager::PackInstaller::Local).to receive(:new).with(be_kind_of(String)).and_return(local_installer)
+
+      subject.execute
+    end
+  end
+
+  context "when the file doesn't exist remotely" do
+    before do
+      stub_request(:get, url).to_return({ :status => 404 })
+    end
+
+    it "raises and exception" do
+      expect { subject.execute }.to raise_error(LogStash::PluginManager::FileNotFoundError, /#{url}/)
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/prepare_offline_pack_spec.rb b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
new file mode 100644
index 00000000000..aa9376c91cf
--- /dev/null
+++ b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
@@ -0,0 +1,140 @@
+# encoding: utf-8
+require "spec_helper"
+require "pluginmanager/main"
+require "pluginmanager/prepare_offline_pack"
+require "pluginmanager/offline_plugin_packager"
+require "stud/temporary"
+require "fileutils"
+require "webmock"
+
+# This Test only handle the interaction with the OfflinePluginPackager class
+# any test for bundler will need to be done as rats test
+describe LogStash::PluginManager::PrepareOfflinePack do
+  before do
+    WebMock.allow_net_connect!
+  end
+
+  subject { described_class.new(cmd, {}) }
+
+  let(:temporary_dir) { Stud::Temporary.pathname }
+  let(:tmp_zip_file) { ::File.join(temporary_dir, "myspecial.zip") }
+  let(:offline_plugin_packager) { double("offline_plugin_packager") }
+  let(:cmd_args) { ["--output", tmp_zip_file, "logstash-input-stdin"] }
+  let(:cmd) { "prepare-offline-pack" }
+
+  before do
+    FileUtils.mkdir_p(temporary_dir)
+
+    allow(LogStash::Bundler).to receive(:invoke!).and_return(nil)
+    allow(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, anything).and_return(offline_plugin_packager)
+  end
+
+  context "when not debugging" do
+    before do
+      @before_debug_value = ENV["DEBUG"]
+      ENV["DEBUG"] = nil
+    end
+
+    after do
+      ENV["DEBUG"] = @before_debug_value
+    end
+
+    it "silences paquet ui reporter" do
+      expect(Paquet).to receive(:ui=).with(Paquet::SilentUI)
+      subject.run(cmd_args)
+    end
+
+    context "when trying to use a core gem" do
+      let(:exception) { LogStash::PluginManager::UnpackablePluginError }
+
+      before do
+        allow(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, anything).and_raise(exception)
+      end
+
+      it "catches the error" do
+        expect(subject).to receive(:report_exception).with("Offline package", be_kind_of(exception)).and_return(nil)
+        subject.run(cmd_args)
+      end
+    end
+
+    context "when trying to pack a plugin that doesnt exist" do
+      let(:exception) { LogStash::PluginManager::PluginNotFoundError }
+
+      before do
+        allow(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, anything).and_raise(exception)
+      end
+
+      it "catches the error" do
+        expect(subject).to receive(:report_exception).with("Cannot create the offline archive", be_kind_of(exception)).and_return(nil)
+        subject.run(cmd_args)
+      end
+    end
+
+
+    context "if the output is directory" do
+      let(:tmp_zip_file) { f = Stud::Temporary.pathname; FileUtils.mkdir_p(f); f }
+      let(:cmd) { "prepare-offline-pack" }
+
+      before do
+        expect(LogStash::PluginManager::OfflinePluginPackager).not_to receive(:package).with(anything)
+      end
+
+      it "fails to do any action" do
+        expect { subject.run(cmd_args) }.to raise_error Clamp::ExecutionError, /you must specify a filename/
+      end
+    end
+
+    context "if the output doesn't have a zip extension" do
+      let(:tmp_zip_file) { ::File.join(temporary_dir, "myspecial.rs") }
+
+      before do
+        expect(LogStash::PluginManager::OfflinePluginPackager).not_to receive(:package).with(anything)
+      end
+
+      it "fails to create the package" do
+        expect { subject.run(cmd_args) }.to raise_error Clamp::ExecutionError, /the zip extension/
+      end
+    end
+
+    context "if the file already exist" do
+      before do
+        FileUtils.touch(tmp_zip_file)
+      end
+
+      context "without `--overwrite`" do
+        before do
+          expect(LogStash::PluginManager::OfflinePluginPackager).not_to receive(:package).with(anything)
+        end
+
+        it "should fails" do
+          expect { subject.run(cmd_args) }.to raise_error Clamp::ExecutionError, /output file destination #{tmp_zip_file} already exist/
+        end
+      end
+
+      context "with `--overwrite`" do
+        let(:cmd_args) { ["--overwrite", "--output", tmp_zip_file, "logstash-input-stdin"] }
+
+        it "succeed" do
+          expect(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, tmp_zip_file)
+          subject.run(cmd_args)
+        end
+      end
+    end
+  end
+
+  context "when debugging" do
+    before do
+      @before_debug_value = ENV["DEBUG"]
+      ENV["DEBUG"] = "1"
+    end
+
+    after do
+      ENV["DEBUG"] = @before_debug_value
+    end
+
+    it "doesn't silence paquet ui reporter" do
+      expect(Paquet).not_to receive(:ui=).with(Paquet::SilentUI)
+      subject.run(cmd_args)
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/proxy_support_spec.rb b/spec/unit/plugin_manager/proxy_support_spec.rb
new file mode 100644
index 00000000000..228bb2a9b07
--- /dev/null
+++ b/spec/unit/plugin_manager/proxy_support_spec.rb
@@ -0,0 +1,131 @@
+# encoeing: utf-8
+require "pluginmanager/proxy_support"
+require "rexml/document"
+require "fileutils"
+require "uri"
+
+describe "Proxy support" do
+  let(:settings) { File.join(Dir.home, ".m2", "settings.xml") }
+  let(:settings_backup) { "#{settings}_bk" }
+
+  before do
+    FileUtils.mv(settings, settings_backup) if File.exist?(settings)
+    environments.each { |key, value| ENV[key] = value }
+  end
+
+  after do
+    FileUtils.mv(settings_backup, settings) if File.exist?(settings_backup)
+    environments.each { |key, _| ENV[key] = nil }
+  end
+
+  shared_examples "proxy access" do
+    let(:http_proxy) { "http://a:b@local.dev:9898" }
+    let(:https_proxy) { "https://c:d@local.dev:9898" }
+    let(:http_proxy_uri) { URI(http_proxy) }
+    let(:https_proxy_uri) { URI(https_proxy) }
+    let(:schemes) { ["http", "https"]}
+
+    let(:environments) {
+      {
+        "HTTP_PROXY" => http_proxy,
+        "HTTPS_PROXY" => https_proxy
+      }
+    }
+
+    after do
+      ["http", "https"].each do |scheme|
+        java.lang.System.clearProperty("#{scheme}.proxyHost")
+        java.lang.System.clearProperty("#{scheme}.proxyPort")
+        java.lang.System.clearProperty("#{scheme}.proxyUsername")
+        java.lang.System.clearProperty("#{scheme}.proxyPassword")
+      end
+    end
+
+    it "updates the java proxy properties" do
+      # asserts the changes
+      schemes.each do |scheme|
+        expect(java.lang.System.getProperty("#{scheme}.proxyHost")).to be_nil
+        expect(java.lang.System.getProperty("#{scheme}.proxyPort")).to be_nil
+        expect(java.lang.System.getProperty("#{scheme}.proxyUsername")).to be_nil
+        expect(java.lang.System.getProperty("#{scheme}.proxyPassword")).to be_nil
+      end
+
+      configure_proxy
+
+      schemes.each do |scheme|
+        expect(java.lang.System.getProperty("#{scheme}.proxyHost")).to eq(send("#{scheme}_proxy_uri").host)
+        expect(java.lang.System.getProperty("#{scheme}.proxyPort")).to eq(send("#{scheme}_proxy_uri").port.to_s)
+        expect(java.lang.System.getProperty("#{scheme}.proxyUsername")).to eq(send("#{scheme}_proxy_uri").user)
+        expect(java.lang.System.getProperty("#{scheme}.proxyPassword")).to eq(send("#{scheme}_proxy_uri").password)
+      end
+    end
+
+    context "when the $HOME/.m2/settings.xml doesn't exist" do
+      it "creates the settings files" do
+        expect(File.exist?(settings)).to be_falsey
+        configure_proxy
+        expect(File.exist?(settings)).to be_truthy
+      end
+
+      it "defines the proxies in the xml file" do
+        configure_proxy
+
+        content = REXML::Document.new(File.read(settings))
+
+        schemes.each_with_index do |scheme, idx|
+          target = idx + 1
+          expect(REXML::XPath.first(content, "//proxy[#{target}]/active/text()")).to be_truthy
+          expect(REXML::XPath.first(content, "//proxy[#{target}]/port/text()")).to eq(send("#{scheme}_proxy_uri").port)
+          expect(REXML::XPath.first(content, "//proxy[#{target}]/host/text()")).to eq(send("#{scheme}_proxy_uri").host)
+          expect(REXML::XPath.first(content, "//proxy[#{target}]/username/text()")).to eq(send("#{scheme}_proxy_uri").user)
+          expect(REXML::XPath.first(content, "//proxy[#{target}]/password/text()")).to eq(send("#{scheme}_proxy_uri").password)
+        end
+      end
+    end
+
+    context "when the $HOME/.m2/settings.xml exists" do
+      let(:dummy_settings) { "<settings></settings>" }
+
+      before do
+        File.open(settings, "w") do |f|
+          f.write(dummy_settings)
+        end
+      end
+
+      it "doesn't do anything to to the original file" do
+        expect(File.read(settings)).to eq(dummy_settings)
+        configure_proxy
+        expect(File.read(settings)).to eq(dummy_settings)
+      end
+    end
+  end
+
+  context "when `HTTP_PROXY` and `HTTPS_PROXY` are configured" do
+    include_examples "proxy access"
+  end
+
+  context "when `http_proxy` and `https_proxy` are configured" do
+    include_examples "proxy access" do
+      let(:environments) {
+        {
+          "http_proxy" => http_proxy,
+          "https_proxy" => https_proxy
+        }
+      }
+    end
+  end
+
+  context "when proxies are set with an empty string" do
+    let(:environments) {
+      {
+        "http_proxy" => "",
+        "https_proxy" => ""
+      }
+    }
+
+
+    it "doesn't raise an exception" do
+      expect { configure_proxy }.not_to raise_exception
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/ui_spec.rb b/spec/unit/plugin_manager/ui_spec.rb
new file mode 100644
index 00000000000..faca7b3a628
--- /dev/null
+++ b/spec/unit/plugin_manager/ui_spec.rb
@@ -0,0 +1,59 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+describe LogStash::PluginManager do
+  it "set the a default ui" do
+    expect(LogStash::PluginManager.ui).to be_kind_of(LogStash::PluginManager::Shell)
+  end
+
+  it "you can override the ui" do
+    klass = Class.new
+    LogStash::PluginManager.ui = klass
+    expect(LogStash::PluginManager.ui).to be(klass)
+    LogStash::PluginManager.ui = LogStash::PluginManager::Shell.new
+  end
+end
+
+describe LogStash::PluginManager::Shell do
+  let(:message) { "hello world" }
+
+  [:info, :error, :warn].each do |level|
+    context "Level: #{level}" do
+      it "display the message to the user" do
+        expect(subject).to receive(:puts).with(message)
+        subject.send(level, message)
+      end
+    end
+  end
+
+  context "Debug" do
+    context "when ENV['DEBUG'] is set" do
+      before do
+        @previous_value = ENV["DEBUG"]
+        ENV["DEBUG"] = "1"
+      end
+
+      it "outputs the message" do
+        expect(subject).to receive(:puts).with(message)
+        subject.debug(message)
+      end
+
+      after do
+        ENV["DEBUG"] = @previous_value
+      end
+    end
+
+    context "when ENV['DEBUG'] is not set" do
+      @previous_value = ENV["DEBUG"]
+      ENV.delete("DEBUG")
+    end
+
+    it "doesn't outputs the message" do
+      expect(subject).not_to receive(:puts).with(message)
+      subject.debug(message)
+    end
+
+    after do
+      ENV["DEBUG"] = @previous_value
+    end
+  end
+end
diff --git a/spec/plugin_manager/update_spec.rb b/spec/unit/plugin_manager/update_spec.rb
similarity index 94%
rename from spec/plugin_manager/update_spec.rb
rename to spec/unit/plugin_manager/update_spec.rb
index 5498f9dea0c..3fc4d1d62fe 100644
--- a/spec/plugin_manager/update_spec.rb
+++ b/spec/unit/plugin_manager/update_spec.rb
@@ -10,7 +10,6 @@
     expect(cmd).to receive(:find_latest_gem_specs).and_return({})
     allow(cmd).to receive(:warn_local_gems).and_return(nil)
     expect(cmd).to receive(:display_updated_plugins).and_return(nil)
-    expect_any_instance_of(LogStash::Bundler).to receive(:invoke!).with(:clean => true)
   end
 
   it "pass all gem sources to the bundle update command" do
diff --git a/spec/plugin_manager/util_spec.rb b/spec/unit/plugin_manager/util_spec.rb
similarity index 95%
rename from spec/plugin_manager/util_spec.rb
rename to spec/unit/plugin_manager/util_spec.rb
index 10824e56adc..669a186886f 100644
--- a/spec/plugin_manager/util_spec.rb
+++ b/spec/unit/plugin_manager/util_spec.rb
@@ -28,13 +28,13 @@
     end
 
     context "fetch plugin info" do
-      it "should search for the last version infomation non prerelease" do
+      it "should search for the last version information non prerelease" do
         version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name)
         expect(version_info["number"]).to eq("1.0.7")
       end
 
 
-      it "should search for the last version infomation with prerelease" do
+      it "should search for the last version information with prerelease" do
         version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name, :pre => true)
         expect(version_info["number"]).to eq("2.0.0.pre")
       end
diff --git a/spec/unit/plugin_manager/utils/downloader_spec.rb b/spec/unit/plugin_manager/utils/downloader_spec.rb
new file mode 100644
index 00000000000..e08e731af01
--- /dev/null
+++ b/spec/unit/plugin_manager/utils/downloader_spec.rb
@@ -0,0 +1,81 @@
+# encoding: utf-8
+require "pluginmanager/utils/downloader"
+require "spec_helper"
+require "webmock/rspec"
+
+describe LogStash::PluginManager::Utils::Downloader::SilentFeedback do
+  let(:max) { 500 }
+  let(:status) { max * 0.5 }
+
+  it "can create an instance" do
+    expect { described_class.new(max) }.not_to raise_error
+  end
+
+  it "can receive `#update` calls" do
+    expect { described_class.new(max).update(status) }.not_to raise_error
+  end
+end
+
+describe LogStash::PluginManager::Utils::Downloader::ProgressbarFeedback do
+  let(:max) { 500 }
+  let(:status) { max * 0.5 }
+
+  it "can create an instance" do
+    expect(ProgressBar).to receive(:create).with(hash_including(:total => max))
+    described_class.new(max)
+  end
+
+  it "can receive multiples `#update` calls" do
+    feedback = described_class.new(max)
+    expect(feedback.progress_bar).to receive(:progress=).with(status).twice
+    feedback.update(status)
+    feedback.update(status)
+  end
+end
+
+describe LogStash::PluginManager::Utils::Downloader do
+  subject { described_class }
+  let(:port) { rand(2000..5000) }
+  let(:url) { "https://localhost:#{port}/my-file.txt"}
+  let(:content) { "its halloween, halloween!" }
+
+  context "when the file exist" do
+    before do
+      stub_request(:get, url).to_return(
+        { :status => 200,
+          :body => content,
+          :headers => {}}
+      )
+    end
+
+    it "download the file to local temporary file" do
+      expect(File.read(subject.fetch(url))).to match(content)
+    end
+
+    context "when an exception occur" do
+      let(:temporary_path) { Stud::Temporary.pathname }
+
+      before do
+        expect_any_instance_of(::File).to receive(:close).at_least(:twice).and_raise("Didn't work")
+        expect(Stud::Temporary).to receive(:pathname).and_return(temporary_path)
+      end
+
+      it "deletes in progress file" do
+        expect { subject.fetch(url) }.to raise_error(RuntimeError, /Didn't work/)
+        expect(Dir.glob(::File.join(temporary_path, "**")).size).to eq(0)
+      end
+    end
+  end
+
+  context "when the file doesn't exist" do
+    before do
+      stub_request(:get, url).to_return(
+        { :status => 404 }
+      )
+    end
+
+    it "raises an exception" do
+      expect { File.read(subject.fetch(url)) }.to raise_error(LogStash::PluginManager::FileNotFoundError)
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/utils/http_client_spec.rb b/spec/unit/plugin_manager/utils/http_client_spec.rb
new file mode 100644
index 00000000000..7c922486c97
--- /dev/null
+++ b/spec/unit/plugin_manager/utils/http_client_spec.rb
@@ -0,0 +1,101 @@
+# encoding: utf-8
+require "pluginmanager/utils/http_client"
+require "uri"
+
+describe LogStash::PluginManager::Utils::HttpClient do
+  subject  { described_class }
+
+  describe ".start" do
+    context "with ssl" do
+      let(:uri) { URI.parse("https://localhost:8888") }
+
+      it "requires ssl" do
+        expect(Net::HTTP).to receive(:start).with(uri.host, uri.port, hash_including(:use_ssl => true))
+        described_class.start(uri)
+      end
+    end
+
+    context "without ssl" do
+      let(:uri) { URI.parse("http://localhost:8888") }
+
+      it "doesn't requires ssl" do
+        expect(Net::HTTP).to receive(:start).with(uri.host, uri.port, hash_including(:use_ssl => false))
+        described_class.start(uri)
+      end
+    end
+  end
+
+  describe ".remove_file_exist?" do
+    let(:mock_http) { double("Net::HTTP") }
+
+    before do
+      allow(subject).to receive(:start).with(anything).and_yield(mock_http).at_least(:once)
+    end
+
+    context "With URI with a path" do
+      let(:uri) { URI.parse("https://localhost:8080/hola") }
+
+      context "without redirect" do
+        before do
+          expect(mock_http).to receive(:request).with(kind_of(Net::HTTP::Head)).and_return(response)
+        end
+
+        context "file exist" do
+          let(:response) { instance_double("Net::HTTP::Response", :code => "200") }
+
+          it "returns true if the file exist" do
+            expect(subject.remote_file_exist?(uri)).to be_truthy
+          end
+        end
+
+        [404, 400, 401, 500].each do |code|
+          context "when the server return a #{code}" do
+            let(:response) { instance_double("Net::HTTP::Response", :code => code) }
+
+            it "returns false" do
+              expect(subject.remote_file_exist?(uri)).to be_falsey
+            end
+          end
+        end
+      end
+
+      context "with redirects" do
+        let(:redirect_response) { instance_double("Net::HTTP::Response", :code => "302", :headers => { "location" => "https://localhost:8888/new_path" }) }
+        let(:response_ok) { instance_double("Net::HTTP::Response", :code => "200") }
+
+        it "follow 1 level redirect" do
+          expect(mock_http).to receive(:request).with(kind_of(Net::HTTP::Head)).and_return(redirect_response)
+          expect(mock_http).to receive(:request).with(kind_of(Net::HTTP::Head)).and_return(response_ok)
+
+          expect(subject.remote_file_exist?(uri)).to be_truthy
+        end
+
+        it "follow up to the limit of redirect: #{described_class::REDIRECTION_LIMIT - 1}" do
+          (described_class::REDIRECTION_LIMIT - 1).times do
+            expect(mock_http).to receive(:request).with(kind_of(Net::HTTP::Head)).and_return(redirect_response)
+          end
+
+          expect(mock_http).to receive(:request).with(kind_of(Net::HTTP::Head)).and_return(response_ok)
+
+          expect(subject.remote_file_exist?(uri)).to be_truthy
+        end
+
+        it "raises a `RedirectionLimit` when too many redirection occur" do
+          described_class::REDIRECTION_LIMIT.times do
+            expect(mock_http).to receive(:request).with(kind_of(Net::HTTP::Head)).and_return(redirect_response)
+          end
+
+          expect { subject.remote_file_exist?(uri) }.to raise_error(LogStash::PluginManager::Utils::HttpClient::RedirectionLimit)
+        end
+      end
+
+      context "With URI without a path" do
+        let(:uri) { URI.parse("https://localhost:8080") }
+
+        it "return false" do
+          expect(subject.remote_file_exist?(uri)).to be_falsey
+        end
+      end
+    end
+  end
+end
diff --git a/spec/util/compress_spec.rb b/spec/unit/util/compress_spec.rb
similarity index 63%
rename from spec/util/compress_spec.rb
rename to spec/unit/util/compress_spec.rb
index 47bab9e995a..33133c09384 100644
--- a/spec/util/compress_spec.rb
+++ b/spec/unit/util/compress_spec.rb
@@ -2,6 +2,40 @@
 require "spec_helper"
 require 'ostruct'
 require "bootstrap/util/compress"
+require "stud/temporary"
+require "fileutils"
+
+def build_zip_file(structure)
+  source = Stud::Temporary.pathname
+  FileUtils.mkdir_p(source)
+
+  structure.each do |p|
+    file = ::File.basename(p)
+    path = ::File.join(source, ::File.dirname(p))
+    full_path = ::File.join(path, file)
+
+    FileUtils.mkdir_p(path)
+    ::File.open(full_path, "a") do |f|
+      f.write("Hello - #{Time.now.to_i.to_s}")
+    end
+  end
+
+  target = Stud::Temporary.pathname
+  FileUtils.mkdir_p(target)
+  target_file = ::File.join(target, "mystructure.zip")
+
+  LogStash::Util::Zip.compress(source, target_file)
+  target_file
+rescue => e
+  FileUtils.rm_rf(target) if target
+  raise e
+ensure
+  FileUtils.rm_rf(source)
+end
+
+def list_files(target)
+  Dir.glob(::File.join(target, "**", "*")).select { |f| ::File.file?(f) }.size
+end
 
 describe LogStash::Util::Zip do
 
@@ -30,10 +64,53 @@
       expect(zip_file).to receive(:extract).exactly(3).times
       subject.extract(source, target)
     end
+
+    context "patterns" do
+      # Theses tests sound duplicated but they are actually better than the other one
+      # since they do not involve any mocks.
+      subject { described_class }
+
+      let(:zip_structure) {
+        [
+          "logstash/logstash-output-secret/logstash-output-monitoring.gem",
+          "logstash/logs/more/log.log",
+          "kibana/package.json",
+          "elasticsearch/jars.jar",
+          "elasticsearch/README.md"
+        ]
+      }
+
+      let(:zip_file) { build_zip_file(zip_structure) }
+      let(:target) { Stud::Temporary.pathname }
+
+      context "when no matching pattern is supplied" do
+        it "extracts all the file" do
+          subject.extract(zip_file, target)
+
+          expect(list_files(target)).to eq(zip_structure.size)
+
+          zip_structure.each do |full_path|
+            expect(::File.exist?(::File.join(target, full_path))).to be_truthy
+          end
+        end
+      end
+
+      context "when a matching pattern is supplied" do
+        it "extracts only the relevant files" do
+          subject.extract(zip_file, target, /logstash\/?/)
+
+          expect(list_files(target)).to eq(2)
+
+          ["logstash/logstash-output-secret/logstash-output-monitoring.gem",
+           "logstash/logs/more/log.log"].each do |full_path|
+            expect(::File.exist?(::File.join(target, full_path))).to be_truthy
+          end
+        end
+      end
+    end
   end
 
   context "#compression" do
-
     let(:target) { File.join(File.expand_path("."), "target_file.zip") }
     let(:source) { File.expand_path("source_dir") }
 
@@ -62,7 +139,6 @@
   subject { Class.new { extend LogStash::Util::Tar } }
 
   context "#extraction" do
-
     let(:source) { File.join(File.expand_path("."), "source_file.tar.gz") }
     let(:target) { File.expand_path("target_dir") }
 
diff --git a/spec/util/retryable_spec.rb b/spec/unit/util/retryable_spec.rb
similarity index 100%
rename from spec/util/retryable_spec.rb
rename to spec/unit/util/retryable_spec.rb
diff --git a/tools/Gemfile.beaker b/tools/Gemfile.beaker
deleted file mode 100644
index 97a67a20ade..00000000000
--- a/tools/Gemfile.beaker
+++ /dev/null
@@ -1,11 +0,0 @@
-source 'https://rubygems.org'
-
-gem 'beaker', '2.27.0'
-gem 'beaker-rspec'
-gem 'pry'
-gem 'docker-api', '~> 1.0'
-gem 'rubysl-securerandom'
-gem 'rspec_junit_formatter'
-gem 'rspec', '~> 3.1'
-gem 'rake'
-gem 'fog-google', '~> 0.0.9'
diff --git a/benchmark/collector.rb b/tools/benchmark/collector.rb
similarity index 100%
rename from benchmark/collector.rb
rename to tools/benchmark/collector.rb
diff --git a/tools/benchmark/event_accessor.rb b/tools/benchmark/event_accessor.rb
new file mode 100644
index 00000000000..018785b51ae
--- /dev/null
+++ b/tools/benchmark/event_accessor.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "benchmark/ips"
+require "logstash/event"
+
+options = { :time => 10, :warmup => 60 }
+puts "Same Event instance"
+
+event = LogStash::Event.new("foo" => {"bar" => {"foobar" => "morebar"} })
+STDERR.puts ""
+STDERR.puts " ----------> event.get(\"[foo][bar][foobar]\") => #{event.get("[foo][bar][foobar]")}"
+STDERR.puts ""
+
+Benchmark.ips do |x|
+  x.config(options)
+
+  x.report("Deep fetch") { event.get("[foo][bar][foobar]") }
+end
diff --git a/benchmark/event_sprintf.rb b/tools/benchmark/event_sprintf.rb
similarity index 100%
rename from benchmark/event_sprintf.rb
rename to tools/benchmark/event_sprintf.rb
diff --git a/tools/logstash-docgen/.gitignore b/tools/logstash-docgen/.gitignore
new file mode 100644
index 00000000000..f33f2034685
--- /dev/null
+++ b/tools/logstash-docgen/.gitignore
@@ -0,0 +1,11 @@
+/.bundle/
+/.yardoc
+/Gemfile.lock
+/_yardoc/
+/coverage/
+/doc/
+/pkg/
+/spec/reports/
+/tmp/
+target/
+source/
diff --git a/tools/logstash-docgen/Gemfile b/tools/logstash-docgen/Gemfile
new file mode 100644
index 00000000000..b1b171bb3db
--- /dev/null
+++ b/tools/logstash-docgen/Gemfile
@@ -0,0 +1,4 @@
+source 'https://rubygems.org'
+
+# Specify your gem's dependencies in logstash-docgen.gemspec
+gemspec
diff --git a/tools/logstash-docgen/README.md b/tools/logstash-docgen/README.md
new file mode 100644
index 00000000000..b2a19abd47a
--- /dev/null
+++ b/tools/logstash-docgen/README.md
@@ -0,0 +1,86 @@
+**Features**
+
+## Generating the documentation for a Logstash instance
+
+You can now generate the documentation for a Logstash distribution.
+
+*prerequisite*:
+
+- You need to have installed the plugins inside Logstash, this is necessary to pickup the right file when generating the documentation.
+- You need to have the development gems installed. `bin/logstash-plugin --no-verify --development`
+
+If you have all of the above you can run this command:
+
+```sh
+rake doc:generate-plugins
+rake doc:generate-plugins[/tmp/the-new-doc] # Depending on the shell you are using you might need to quote the task name, like this rake "doc:generate-plugins[/tmp/new-doc]"
+```
+
+This will give you a output similar to this
+
+```sh
+logstash-input-file > SUCCESS
+logstash-input-s3 > SUCCESS
+logstash-input-kafka > FAIL
+[...]
+Exceptions: XXXX
+```
+
+The generator will try to generate the doc for all the plugins defined in the *Gemfile* and installed in Logstash, if anything goes wrong it won't
+stop the generation of the other plugin. The Task will also report any errors with stacktraces at the end, if one plugin fail the build,
+you can interrupt the process and it will output the current errors before exiting.
+
+
+## Generating the documentation for all the plugins from the organization
+
+You can now generate the documentation from master for all the plugin in the *logstash-plugins* organization.
+
+*prerequisite*
+
+- You need to go in the `tools/logstash-docgen` directory
+- You need to have the dependency installed.
+
+ To get started you can run the following commands:
+
+ ```sh
+ cd tools/logstash-docgen
+ bundle install
+ ```
+
+You can use the the `bin/logstash-docgen` command to generate any plugin that you want, this executable can generate all the plugins or specific one from their master branch.
+
+Usages:
+
+```sh
+bin/logstash-docgen --all # will generate the doc for all the plugins
+bin/logstash-docgen logstash-input-file logstash-input-s3 # generate doc for 2 plugins
+```
+
+**See:** `bin/logstash-docgen --help` for complete usage.
+
+**Notes:**
+- The nature of theses tasks require a lot of external execution to make sure all the doc are done in isolation, this process can take a long time.
+- Some plugins will be skipped, see `logstash-docgen.yml` for details.
+- The script will inject itself as a dependency on the plugin.
+
+
+## Testing the documentation as the plugin author
+
+*prerequisite*
+
+- Declare the `logstash-docgen` as development dependency
+- Add `require "logstash/docgen/plugin_doc"` to the `Rakefile`
+- Run `bundle install`
+
+After you can have access to a few rake tasks, you can list them with `bundle exec rake -T`
+
+```
+bundle exec rake doc:asciidoc # return the raw asciidoc
+bundle exec rake doc:html # Give you the raw html
+```
+
+## CI test
+
+The CI can use the `ci/docs.sh` script to correctly bootstrap and execute the docgeneration
+
+
diff --git a/tools/logstash-docgen/Rakefile b/tools/logstash-docgen/Rakefile
new file mode 100644
index 00000000000..43022f711e2
--- /dev/null
+++ b/tools/logstash-docgen/Rakefile
@@ -0,0 +1,2 @@
+require "bundler/gem_tasks"
+task :default => :spec
diff --git a/tools/logstash-docgen/bin/console b/tools/logstash-docgen/bin/console
new file mode 100755
index 00000000000..49a0d277df8
--- /dev/null
+++ b/tools/logstash-docgen/bin/console
@@ -0,0 +1,14 @@
+#!/usr/bin/env ruby
+
+require "bundler/setup"
+require "logstash/docgen"
+
+# You can add fixtures and/or initialization code here to make experimenting
+# with your gem easier. You can also use a different console, if you like.
+
+# (If you use this, don't forget to add pry to your Gemfile!)
+# require "pry"
+# Pry.start
+
+require "irb"
+IRB.start
diff --git a/tools/logstash-docgen/bin/logstash-docgen b/tools/logstash-docgen/bin/logstash-docgen
new file mode 100755
index 00000000000..e7338a7c4f6
--- /dev/null
+++ b/tools/logstash-docgen/bin/logstash-docgen
@@ -0,0 +1,6 @@
+#!/usr/bin/env ruby
+
+require "bundler/setup"
+require "logstash/docgen/runner"
+
+LogStash::Docgen::Runner.run
diff --git a/tools/logstash-docgen/bin/setup b/tools/logstash-docgen/bin/setup
new file mode 100755
index 00000000000..dce67d860af
--- /dev/null
+++ b/tools/logstash-docgen/bin/setup
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+set -euo pipefail
+IFS=$'\n\t'
+set -vx
+
+bundle install
+
+# Do any other automated setup that you need to do here
diff --git a/tools/logstash-docgen/lib/logstash/docgen.rb b/tools/logstash-docgen/lib/logstash/docgen.rb
new file mode 100644
index 00000000000..7a70dce3334
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen.rb
@@ -0,0 +1,7 @@
+require "logstash/docgen/version"
+
+module LogStash
+  module Docgen
+    # Your code goes here...
+  end
+end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/asciidoc_format.rb b/tools/logstash-docgen/lib/logstash/docgen/asciidoc_format.rb
new file mode 100644
index 00000000000..3be29229109
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/asciidoc_format.rb
@@ -0,0 +1,50 @@
+# encoding: utf-8
+require "asciidoctor"
+require "erb"
+
+module LogStash module Docgen
+  class AsciidocFormat
+    TEMPLATE_PATH = ::File.join(::File.dirname(__FILE__), "..", "..", "..", "templates")
+    TEMPLATE_FILE = ::File.join(TEMPLATE_PATH, "plugin-doc.asciidoc.erb")
+    CSS_FILE = ::File.join(TEMPLATE_PATH, "plugin-doc.css")
+
+    POST_PROCESS_KEYS = {
+      /%PLUGIN%/ => :config_name
+    }
+
+
+    def initialize(options = {})
+      @options = options
+      @template = read_template(TEMPLATE_FILE)
+    end
+
+    def generate(context)
+      erb = @template.result(context.get_binding)
+      post_process!(context, erb)
+
+      if @options.fetch(:raw, true)
+        erb
+      else
+        Asciidoctor.convert(erb,
+                            :header_footer => true,
+                            :stylesheet => CSS_FILE,
+                            :safe => 'safe')
+      end
+    end
+
+    def extension
+      "asciidoc"
+    end
+
+    private
+    def read_template(file)
+      ERB.new(::File.read(file), nil, "-")
+    end
+
+    def post_process!(context, erb)
+      POST_PROCESS_KEYS.each do |expression, method_call|
+        erb.gsub!(expression, context.send(method_call))
+      end
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/dependency_lookup.rb b/tools/logstash-docgen/lib/logstash/docgen/dependency_lookup.rb
new file mode 100644
index 00000000000..227f041096d
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/dependency_lookup.rb
@@ -0,0 +1,58 @@
+# encoding: utf-8
+require "singleton"
+require "gems"
+
+module LogStash module Docgen
+  class DependencyLookup
+    LOGSTASH_CORE_PLUGIN_API_GEM = "logstash-core-plugin-api"
+    LOGSTASH_CORE_GEM = "logstash-core"
+    PRERELEASES_RE = /\.(alpha|snapshot|pre|beta).+/
+
+    include Singleton
+
+    def supported_logstash(gemspec)
+      plugin_core_api_dep = gemspec.dependencies.select { |spec| spec.name.eql?(LOGSTASH_CORE_PLUGIN_API_GEM) }.first
+
+      core_requirements = match_core_requirements(plugin_core_api_dep.requirement)
+      clean_versions(match_core(core_requirements))
+    end
+
+    def self.supported_logstash(gemspec)
+      instance.supported_logstash(gemspec)
+    end
+
+    private
+    def match_core_requirements(requirements)
+      logstash_core_plugin_api_versions.collect do |plugin|
+        requirements.satisfied_by?(Gem::Version.new(plugin[:number]))
+        dependencies = plugin[:dependencies].select { |dependency|  dependency.first == LOGSTASH_CORE_GEM }.collect(&:last)
+      end.flatten.collect do |requirement|
+          Gem::Requirement.new(requirement.split(", "))
+      end.compact
+    end
+
+    def match_core(requirements)
+      logstash_core_versions
+        .collect { |plugin| Gem::Version.new(plugin[:number]) }
+        .select { |v| requirements.any? { |requirement| requirement.satisfied_by?(v) } }
+    end
+
+    # Remove betas/alphas and reverse sort
+    def clean_versions(gemspecs)
+      gemspecs.collect(&:to_s)
+        .collect { |v| v.gsub(PRERELEASES_RE, '') } # remove beta, alphas and snapshots
+        .uniq
+        .collect { |v| Gem::Version.new(v) }
+        .sort { |x, y| y <=> x}
+        .map(&:to_s)
+    end
+
+    def logstash_core_plugin_api_versions
+      @logstash_core_plugin_api_version ||= Gems.dependencies(LOGSTASH_CORE_PLUGIN_API_GEM)
+    end
+
+    def logstash_core_versions
+      @logstash_core_versions ||= Gems.dependencies(LOGSTASH_CORE_GEM)
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/dynamic_parser.rb b/tools/logstash-docgen/lib/logstash/docgen/dynamic_parser.rb
new file mode 100644
index 00000000000..486d71b7ca1
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/dynamic_parser.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+# This is needed because some of the plugins have weird declaration in their header files
+# so I make sure the basic namespaces are correctly setup.
+module LogStash
+  module Codecs;end
+  module Inputs;end
+  module Output;end
+end
+
+module LogStash module Docgen
+  # Since we can use ruby code to generate some of the options
+  # like the allowed values we need to actually ask the class to return the
+  # evaluated values and another process will merge the values with the extracted
+  # description. IE: Some plugins uses constant to define the list of valid values.
+  class DynamicParser
+    def initialize(context, file, klass_name)
+      @file = file
+      @klass_name = klass_name
+      @context = context
+    end
+
+    def parse
+      # If any errors is raised here it will be taken care by the `generator`,
+      # most errors should be missings jars or bad requires.
+      require @file
+
+      klass.get_config.each do |name, attributes|
+        @context.add_config_attributes(name, attributes)
+      end
+    end
+
+    # Find all the modules included by the specified class
+    # and use `source_location` to find the actual file on disk.
+    # We need to cleanup the values for evaluated modules or system module.
+    # `included_modules` will return the list of module in the order they appear.
+    # this is important because modules can override the documentation of some
+    # option.
+    def extract_sources_location
+      klass.ancestors
+        .collect { |m| m.instance_methods.collect { |method| m.instance_method(method).source_location } + m.methods.collect { |method| m.method(method).source_location } }
+        .flatten
+        .compact
+        .uniq
+        .reject { |source| !source.is_a?(String) || source == "(eval)" }
+    end
+
+    def klass
+      @klass_name.split('::').inject(Object) do |memo, name|
+        memo = memo.const_get(name); memo
+      end
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/github_generator.rb b/tools/logstash-docgen/lib/logstash/docgen/github_generator.rb
new file mode 100644
index 00000000000..2c15293bc8a
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/github_generator.rb
@@ -0,0 +1,189 @@
+# encoding: utf-8
+require "logstash/docgen/parser"
+require "logstash/docgen/task_runner"
+require "logstash/docgen/util"
+require "stud/trap"
+require "git"
+require "bundler"
+require "open3"
+require "octokit"
+require "fileutils"
+
+module LogStash module Docgen
+  # Class to encapsulate any operations needed to actually run the doc
+  # generation on a specific plugin.
+  #
+  # Since the doc generation need access to the current library/dependency and we
+  # dont want to pollute the main execution namespace with libraries that could be incompatible
+  # each execution of the doc is is own process.
+  #
+  # Its a lot slower, but we know for sure that it uses the latest dependency for each plugins.
+  class Plugin
+    class CommandException < StandardError; end
+
+    GITHUB_URI = "https://github.com/logstash-plugins/%s"
+
+    BUNDLER_CMD = "bundler install --jobs 8 --quiet --path /tmp/vendor"
+    RAKE_VENDOR_CMD = "bundle exec rake vendor"
+    RAKE_DOC_ASCIIDOC = "bundle exec rake doc:asciidoc"
+    DOCUMENT_SEPARATOR = "~~~ASCIIDOC_DOCUMENT~~~\n"
+
+    # Content needed to inject to make the generator work
+    GEMFILE_CHANGES = "gem 'logstash-docgen', :path => \"#{File.expand_path(File.join(File.dirname(__FILE__), "..", "..", ".."))}\""
+
+    # require devutils to fix an issue when the logger is not found
+    RAKEFILE_CHANGES = "
+    require 'logstash/devutils/rspec/spec_helper'
+    puts '#{DOCUMENT_SEPARATOR}'
+    require 'logstash/docgen/plugin_doc'"
+
+    attr_reader :path, :full_name
+
+    def initialize(full_name, temporary_path)
+      @full_name = full_name
+      @path = File.join(temporary_path, full_name)
+    end
+
+    def type
+      full_name.split("-")[1]
+    end
+
+    def name
+      full_name.split("-").last
+    end
+
+    def generate(destination)
+      fetch
+      inject_docgen
+      bundle_install
+      rake_vendor
+      generate_doc(destination)
+    end
+
+    private
+    def fetch
+      repository = sprintf(GITHUB_URI, full_name)
+      # by default lets just get the tip of the branch
+
+      if Dir.exist?(path)
+        g = Git.init(path)
+        g.reset
+        g.fetch
+        g.merge("origin/master")
+      else
+        g = Git.clone(repository, path, :depth => 1 )
+      end
+    end
+
+    def inject_docgen
+      File.open(File.join(path, "Gemfile"), "a") do |f|
+        f.write("\n#{GEMFILE_CHANGES}")
+      end
+
+      File.open(File.join(path, "Rakefile"), "a") do |f|
+        f.write("\n#{RAKEFILE_CHANGES}")
+      end
+    end
+
+    def rake_vendor
+      run_in_directory(RAKE_VENDOR_CMD)
+    end
+
+    def generate_doc(destination)
+      output = run_in_directory(RAKE_DOC_ASCIIDOC)
+      destination = File.join(destination, "#{type}s")
+      FileUtils.mkdir_p(destination)
+      content = output.read.split(DOCUMENT_SEPARATOR).last
+      IO.write(File.join(destination, "#{name}.asciidoc"), content)
+    end
+
+    def bundle_install
+      run_in_directory(BUNDLER_CMD)
+    end
+
+    def run_in_directory(cmd = nil, &block)
+      Dir.chdir(path) do
+        Bundler.with_clean_env do
+          stdin, stdout, stderr, wait_thr = Open3.popen3(cmd)
+          if wait_thr.value.success?
+            return stdout
+          else
+            raise CommandException.new, "cmd: #{cmd}, stdout: #{stdout.read}, stderr: #{stderr.read}"
+          end
+        end
+      end
+    end
+  end
+
+
+  # This class orchestrate all the operation between the `logstash-plugins` organization and the
+  # doc build for each plugin.
+  class DocumentationGenerator
+    LOGSTASH_PLUGINS_ORGANIZATION = "logstash-plugins"
+
+    attr_reader :plugins, :temporary_path, :config
+
+    def initialize(plugins, target, source,  c = {})
+      @temporary_path = source
+      @config = c
+
+      @target = target
+      FileUtils.mkdir_p(target)
+
+      @plugins = create_plugins(plugins)
+    end
+
+    def create_plugins(plugins)
+      plugin_names = plugins == :all ?  retrieve_all_plugins : Array(plugins)
+      plugin_names.map do |name|
+        if skip_plugin?(name)
+          puts "#{name} > #{Util.yellow("IGNORED")}"
+          nil
+        else
+          Plugin.new(name, temporary_path)
+        end
+      end.compact
+    end
+
+    def retrieve_all_plugins
+      plugins = []
+      Octokit.auto_paginate = true
+      client = Octokit::Client.new
+      client.organization_repositories(LOGSTASH_PLUGINS_ORGANIZATION).each do |repository|
+        plugins << repository[:name]
+      end
+      plugins
+    end
+
+    def skip_plugin?(name)
+      ignored_plugins.any? { |re| re.match(name) }
+    end
+
+    def ignored_plugins
+      @ignored_plugins ||= config["ignore_plugins"].map { |item| Regexp.new(item) }
+    end
+
+    def generate
+      puts "Processing, #{plugins.size} plugins: #{plugins.collect(&:name).join(", ")}"
+
+      task_runner = TaskRunner.new
+
+      # Since this process can be quite long, we allow people to interrupt it,
+      # but we should at least dump the currents errors..
+      Stud.trap("INT") do
+        puts "Process interrupted"
+        task_runner.report_failures
+
+        exit(1) # assume something went wrong
+      end
+
+      plugins.each do |plugin|
+        task_runner.run(plugin.name) do
+          plugin.generate(@target)
+        end
+      end
+
+      task_runner.report_failures
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/index.rb b/tools/logstash-docgen/lib/logstash/docgen/index.rb
new file mode 100644
index 00000000000..3568be8ea22
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/index.rb
@@ -0,0 +1,81 @@
+# encoding: utf-8
+require "erb"
+
+module LogStash module Docgen
+  class IndexContext
+    class PluginContext
+      GITHUB_URL = "https://github.com/logstash-plugins"
+
+      attr_reader :name, :type
+
+      def initialize(name, type)
+        @name = name
+      end
+
+      def full_name
+        "logstash-#{type}-#{name}"
+      end
+
+      def description
+        "WHERE I SHOULD TAKE THAT? GITHUB?"
+      end
+
+      def github_url
+        "#{GITHUB_URL}/#{full_name}"
+      end
+
+      def edit_url
+        "#{github_url}/lib/logstash/#{type}/#{name}.rb"
+      end
+    end
+
+    attr_reader :plugins
+
+    def initialize(type, plugins)
+      @plugins = plugins.sort.collect { |plugin| PluginContext.new(plugin, type) }
+    end
+
+    def get_binding
+      binding
+    end
+  end
+
+  class Index
+    ASCIIDOC_EXTENSION = ".asciidoc"
+    PLUGIN_TYPES = %w(codecs inputs filters outputs)
+
+    TEMPLATE_PATH = ::File.expand_path(::File.join(::File.dirname(__FILE__), "..", "..", "..", "templates"))
+    TEMPLATES = {
+      :inputs => ::File.read(::File.join(TEMPLATE_PATH, "index-inputs.asciidoc.erb")),
+      :codecs => ::File.read(::File.join(TEMPLATE_PATH, "index-codecs.asciidoc.erb")),
+      :filters => ::File.read(::File.join(TEMPLATE_PATH, "index-filters.asciidoc.erb")),
+      :outputs => ::File.read(::File.join(TEMPLATE_PATH, "index-outputs.asciidoc.erb"))
+    }
+
+    attr_reader :path
+
+    def initialize(path)
+      @path = path
+    end
+
+    def generate
+      PLUGIN_TYPES.each do |type|
+        plugins =  logstash_files(::File.join(path, type))
+            .sort
+            .collect { |file| ::File.basename(file, ASCIIDOC_EXTENSION) }
+
+        template = ERB.new(TEMPLATES[type.to_sym], nil, "-")
+        save(type, template.result(IndexContext.new(type, plugins).get_binding))
+      end
+    end
+
+    def logstash_files(source_path)
+      Dir.glob(::File.join(source_path, "*.asciidoc"))
+    end
+
+    def save(type, output)
+      target = ::File.join(path, "#{type}#{ASCIIDOC_EXTENSION}")
+      File.open(target, "w") { |f| f.write(output) }
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/logstash_generator.rb b/tools/logstash-docgen/lib/logstash/docgen/logstash_generator.rb
new file mode 100644
index 00000000000..93ce1faff8c
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/logstash_generator.rb
@@ -0,0 +1,77 @@
+# encoding: utf-8
+require "logstash/docgen/parser"
+require "logstash/docgen/index"
+require "logstash/docgen/util"
+require "logstash/docgen/task_runner"
+require "rubygems/specification"
+require "fileutils"
+require "stud/trap"
+
+module LogStash module Docgen
+  # This class is used to generate the documentation inside a working logstash
+  # directory, it will take all the installed gemspec and gerate the documentation for them.
+  #
+  # In practice we will install **all the plugins** before running this generator.
+  # This class is invoked inside logstash with a rake task named: `docs:generate-plugins`
+  #
+  # There is also code to generate `Index` but for now we will still handle them manually.
+  class LogStashGenerator
+    PLUGIN_RE = /logstash-(codec|input|filter|output)-(\w+)/
+
+    attr_reader :output
+
+    def initialize(output)
+      @output = output
+      FileUtils.mkdir_p(output)
+    end
+
+    def plugins_gemspec
+      Gem::Specification.select { |spec| logstash_plugin?(spec.name) }
+    end
+
+    def generate(options = {})
+      generate_plugins_docs(options)
+      generate_index(options)
+    end
+
+    def generate_plugins_docs(options = {})
+      Util.time_execution do
+        task_runner = TaskRunner.new
+
+        # Since this process can be quite long, we allow people to interrupt it,
+        # but we should at least dump the currents errors..
+        Stud.trap("INT") do
+          puts "Process interrupted"
+          task_runner.report_failures
+
+          exit(1) # assume something went wrong
+        end
+
+        plugins_gemspec.each do |spec|
+          task_runner.run(spec.name) do
+            generate_plugin_doc(plugin_root(spec), options = {})
+          end
+        end
+
+        task_runner.report_failures
+      end
+    end
+
+    def generate_index(options)
+      Index.new(output).generate
+    end
+
+    def plugin_root(spec)
+      spec.load_paths.first.gsub(/lib$/, "")
+    end
+
+    def generate_plugin_doc(path, options = {})
+      document = Docgen.generate_for_plugin(path, options)
+      document.save(output)
+    end
+
+    def logstash_plugin?(name)
+      name =~ PLUGIN_RE
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/parser.rb b/tools/logstash-docgen/lib/logstash/docgen/parser.rb
new file mode 100644
index 00000000000..720345171ac
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/parser.rb
@@ -0,0 +1,197 @@
+# encoding: utf-8
+require "logstash/docgen/dynamic_parser"
+require "logstash/docgen/static_parser"
+require "logstash/docgen/asciidoc_format"
+require "logstash/docgen/dependency_lookup"
+require "rubygems/specification"
+require "gems"
+require "open-uri"
+require "json"
+
+module LogStash module Docgen
+  class DefaultPlugins
+    DEFAULT_PLUGINS_LIST_JSON = "https://raw.githubusercontent.com/ph/logstash/de2ba3f964ae7039b7b74a4a8212beb5a76a239c/rakelib/plugins-metadata.json"
+
+    class << self
+      def default_plugins_list
+        @default_plugins_list ||= from_master_json
+      end
+
+      def include?(name)
+        default_plugins_list.include?(name)
+      end
+
+      def from_master_json
+        response = open(DEFAULT_PLUGINS_LIST_JSON)
+        JSON.parse(response.read).select { |_, values| values["default-plugins"] == true }.keys
+      end
+    end
+  end
+
+  # This class acts as the transformation point between the format
+  # and the data.
+  #
+  # At the beginning of the PoC, I was targeting multiples different format: asciidoc, manpage,
+  # since we only support 1 format now, we could probably remove it.
+  class Document
+    attr_reader :context, :format
+
+    def initialize(context, format)
+      @context = context
+      @format = format
+    end
+
+    def output
+      @output ||= format.generate(context)
+    end
+
+    def save(directory, filename = nil)
+      target_directory = ::File.join(directory, "#{context.section}s")
+      FileUtils.mkdir_p(target_directory)
+
+      filename = "#{context.config_name}.#{format.extension}" unless filename
+      target = ::File.join(target_directory, filename)
+      ::File.open(target, "w") { |f| f.write(output) }
+    end
+  end
+
+  # This class is mostly a data class that represent what will be exposed in the ERB files
+  # Less manipulation should be done in the ERB itself and most of the works should be done here
+  class PluginContext
+    ANCHOR_VERSION_RE = /\./
+    LOGSTASH_PLUGINS_ORGANIZATION = "https://github.com/logstash-plugins"
+    CANONICAL_NAME_PREFIX = "logstash"
+
+    attr_accessor :description, :config_name, :section, :name, :default_plugin, :gemspec
+
+    def initialize(options = {})
+      @config = {}
+      @options = options
+    end
+
+    def default_plugin?
+      DefaultPlugins.include?(canonical_name)
+    end
+
+    def has_description?
+      !description.nil? && !description.empty?
+    end
+
+    def version
+      gemspec.version.to_s
+    end
+
+    def release_date(format = "%B %-d, %Y")
+      @release_date ||= begin
+                          url ="https://rubygems.org/api/v1/versions/#{canonical_name}.json"
+                          response = open(url).read
+                          # HACK: One of out default plugins, the webhdfs, has a bad encoding in the gemspec
+                          # which make our parser trip with this error:
+                          #
+                          # Encoding::UndefinedConversionError message: ""\xC3"" from ASCII-8BIT to UTF-8
+                          # We dont have much choice than to force utf-8.
+                          response.encode(Encoding::UTF_8, :invalid => :replace, :undef => :replace)
+
+                          data = JSON.parse(response)
+
+                          current_version = data.select { |v| v["number"] == version }.first
+                          if current_version.nil?
+                            "N/A"
+
+                          else
+                            Time.parse(current_version["created_at"]).strftime(format)
+                          end
+                        end
+    end
+
+    def add_config_description(name, description)
+      @config[name] ||= { }
+      @config[name].merge!({ :description => description })
+    end
+
+    def add_config_attributes(name, attributes = {})
+      @config[name] ||= {}
+      @config[name].merge!(attributes)
+    end
+
+    def canonical_name
+      "#{CANONICAL_NAME_PREFIX}-#{section}-#{config_name}"
+    end
+
+    # Developer can declare options in the order they want
+    # `Hash` keys are sorted by default in the order of creation.
+    # But we force a sort options name for the documentation.
+    def config
+      Hash[@config.sort_by(&:first)]
+    end
+    alias_method :sorted_attributes, :config
+
+    def changelog_url
+      # https://github.com/logstash-plugins/logstash-input-beats/blob/master/CHANGELOG.md#310beta3
+      "#{LOGSTASH_PLUGINS_ORGANIZATION}/#{canonical_name}/blob/master/CHANGELOG.md##{anchor_version}"
+    end
+
+    def anchor_version
+      version.gsub(ANCHOR_VERSION_RE, "")
+    end
+
+    def supported_logstash(max = 5)
+      DependencyLookup.supported_logstash(gemspec)[0...max].join(", ")
+    end
+
+    # Used for making `variables` available inside
+    # ERB templates.
+    def get_binding
+      binding
+    end
+  end
+
+  # This class is the main point of entry to the parsing of the plugin ruby file,
+  # the parser need to do 3 differents actions:
+  #
+  # 1. A Static Parsed of the comment of the files
+  # 2. A Dynamic evaluation of the ruby code to get attributes and parent files
+  # 3. A static parsing of the other modules included by the specific plugin
+  class Parser
+    # This is a multipass parser
+    def self.parse(file, options = { :default_plugin => true })
+      context =  PluginContext.new(options)
+      static = StaticParser.new(context)
+
+      # Extract ancestors, classes and modules and retrieve the physical
+      # location of the code for static parsing.
+      dynamic = DynamicParser.new(context, file, static.extract_class_name(file))
+      dynamic.parse
+
+      # Static parse on the modules and parents classes
+      # will update previously parsed context
+      dynamic.extract_sources_location.each { |f| static.parse(f) }
+
+      # Static parse on the target file,
+      # Can override parents documentation. We are making the assumption that
+      # the modules are declared at the top of the class.
+      static.parse(file, true)
+      context
+    end
+  end
+
+  def self.generate_for_plugin(plugin_source_path, options = {})
+    gemspec = load_plugin_specification(plugin_source_path)
+    _, type, name = gemspec.name.split("-")
+
+    file = "#{plugin_source_path}/lib/logstash/#{type}s/#{name}.rb"
+
+    format = LogStash::Docgen::AsciidocFormat.new(options)
+
+    context = LogStash::Docgen::Parser.parse(file)
+    context.gemspec = gemspec
+    Document.new(context, format)
+  end
+
+  # Note that Gem::Specification has an internal cache.
+  def self.load_plugin_specification(plugin_source_path)
+    gemspec = Dir.glob(::File.join(plugin_source_path, "*.gemspec")).first
+    raise "Cannot find the gemspec in #{plugin_source_path}" if gemspec.nil?
+    Gem::Specification.load(gemspec)
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/plugin_doc.rb b/tools/logstash-docgen/lib/logstash/docgen/plugin_doc.rb
new file mode 100644
index 00000000000..08fd52eff3e
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/plugin_doc.rb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require "logstash/docgen/parser"
+require "stud/temporary"
+
+namespace :doc do
+  desc "Preview the raw HTML of the documentation"
+  task :html do
+    puts generate_preview({ :raw => false })
+  end
+
+  desc "Preview Asciidoc documentation"
+  task :asciidoc do
+    puts generate_preview
+  end
+end
+
+task :doc => "doc:html"
+
+
+def generate_preview(options = {})
+  LogStash::Docgen.generate_for_plugin(Dir.pwd, options).output
+end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/runner.rb b/tools/logstash-docgen/lib/logstash/docgen/runner.rb
new file mode 100644
index 00000000000..0fd08321f7e
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/runner.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require "logstash/docgen/github_generator"
+require "clamp"
+require "yaml"
+
+module LogStash module Docgen
+  class Runner < Clamp::Command
+    SAMPLE_PLUGINS = %w(logstash-input-http_poller logstash-codec-netflow logstash-codec-multiline logstash-mixin-aws)
+
+    option ["-t", "--target"], "target", "Where the generated documentation should be saved?", :default => File.join(Dir.pwd, "target")
+    option ["-a", "--all"], :flag, "generate the doc for all the plugins"
+    option ["-s", "--sample"], :flag, "Use a few plugins to test, logstash-input-sqs, logstash-input-file"
+    option ["-c", "--config"], "config", "Configuration of documentation generator", :default => File.join(Dir.pwd, "logstash-docgen.yml")
+    option ["-i", "--source"], "source", "Where we checkout the plugins, having the same source can make runs faster", :default => File.join(Dir.pwd, "source")
+
+    parameter "[PLUGIN] ...", "Specific plugin", :attribute_name => :plugins
+
+    def execute
+      if plugins.size > 0
+        with_plugins = plugins
+      elsif sample?
+        with_plugins = SAMPLE_PLUGINS
+      else
+        with_plugins = :all
+      end
+
+      DocumentationGenerator.new(with_plugins,
+                                 target,
+                                 source,
+                                 YAML.load(File.read(config))).generate
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/static_parser.rb b/tools/logstash-docgen/lib/logstash/docgen/static_parser.rb
new file mode 100644
index 00000000000..de4d9953f52
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/static_parser.rb
@@ -0,0 +1,198 @@
+# encoding: utf-8
+module LogStash::Docgen
+  # This class is parsing static content of the main class and
+  # his ancestors, the result would be the description of the plugin and the
+  # actual documentation for each of the option.
+  class StaticParser
+    COMMENTS_IGNORE = Regexp.union(
+      Regexp.new(/encoding: utf-8/i),
+      Regexp.new(/TODO:?/)
+    )
+
+    VALID_CLASS_NAME = /^LogStash::(Codecs|Inputs|Filters|Outputs)::(\w+)/
+    COMMENT_RE = /^ *#(?: (.*)| *$)/
+    MULTILINE_RE = /(, *$)|(\\$)|(\[ *$)/
+    ENDLINES_RE = /\r\n|\n/
+    CLASS_DEFINITION_RE = /^ *class\s(.*) < *(::)?LogStash::(Outputs|Filters|Inputs|Codecs)::(\w+)/
+    NEW_CLASS_DEFINITION_RE = /module (\w+) module (\w+) class\s(.*) < *(::)?LogStash::(Outputs|Filters|Inputs|Codecs)::(\w+)/
+    NEW_CLASS_DEFINITION_RE_ML = /\s*class\s(.*) < *(::)?LogStash::(Outputs|Filters|Inputs|Codecs)::(\w+)/
+    CONFIG_OPTION_RE = /^\s*((mod|base).)?config +[^=].*/
+    CONFIG_NAME_RE = /^ *config_name .*/
+    RESET_BUFFER_RE = /^require\s("|')\w+("|')/
+
+    def initialize(context)
+      @rules =  [
+        [ COMMENT_RE, :parse_comment ],
+        [ CLASS_DEFINITION_RE, :parse_class_description ],
+        [ NEW_CLASS_DEFINITION_RE_ML, :parse_new_class_description ],
+        [ CONFIG_OPTION_RE, :parse_config ],
+        [ CONFIG_NAME_RE, :parse_config_name ],
+        [ RESET_BUFFER_RE, :reset_buffer ]
+      ]
+
+      @context = context
+
+      # Extracting the class name and parsing file
+      # work on the same raw content of the file, we use this cache to make sure
+      # we dont waste resources on reading the content
+      @cached_read = {}
+
+      reset_buffer
+    end
+
+    def parse_class_description(class_definition)
+      @context.section = class_definition[3].downcase.gsub(/s$/, '')
+      @context.name = class_definition[1]
+
+      update_description
+    end
+
+    def parse_new_class_description(class_definition)
+      @context.section = class_definition[3].downcase.gsub(/s$/, '')
+      @context.name = "LogStash::#{class_definition[3]}::#{class_definition[2]}"
+
+      update_description
+    end
+
+    # This is not obvious, but if the plugin define a class before the main class it can trip the buffer
+    def update_description(match = nil)
+      return unless reading_header?
+
+      description = flush_buffer
+
+      # can only be change by the main file
+      @context.description = description if !@context.has_description? && main?
+      transition_to_reading_attributes
+    end
+
+    def parse_config_name(match)
+      if main?
+        name = match[0].match(/config_name\s++["'](\w+)['"]/)[1]
+        @context.config_name = name
+        @context.name = name
+      end
+    end
+
+    def parse_comment(match)
+      comment = match[1]
+      return if ignore_comment?(comment)
+      @buffer << comment
+    end
+
+    def parse_config(match)
+      field = match[0]
+      field_name = field.match(/config\s+:(\w+)/)[1]
+      @context.add_config_description(field_name, flush_buffer)
+    end
+
+    def parse(file, main = false)
+      @main = main
+      main ? transition_to_reading_header() : transition_to_reading_attributes()
+
+      reset_buffer
+      string = read_file(file)
+      extract_lines(string).each do |line|
+        parse_line(line)
+      end
+    end
+
+    def transition_to_reading_attributes
+      @state = :reading_attributes
+    end
+
+    def transition_to_reading_header
+      @state = :reading_header
+    end
+
+    def reading_header?
+      @state == :reading_header
+    end
+
+    def main?
+      @main
+    end
+
+    def parse_line(line)
+      @rules.each do |rule|
+        re, action = rule
+        if match = re.match(line)
+          send(action, match)
+          break
+        end
+      end
+    end
+
+    def extract_lines(string)
+      buffer = ""
+      string.split(ENDLINES_RE).collect do |line|
+        # Join extended lines
+        if !comment?(line) && multiline?(line)
+          buffer += line.chomp
+          next
+        end
+
+        line = buffer + line
+        buffer = ""
+
+        line
+      end
+    end
+
+    def ignore_comment?(comment)
+      COMMENTS_IGNORE.match(comment)
+    end
+
+    def comment?(line)
+      line =~ COMMENT_RE
+    end
+
+    def multiline?(line)
+      line =~ MULTILINE_RE
+    end
+
+    def flush_buffer
+      content = @buffer.join("\n")
+      reset_buffer
+      content
+    end
+
+    def reset_buffer(match = nil)
+      @buffer = []
+    end
+
+    def read_file(file)
+      @cached_read[file] ||= File.read(file)
+    end
+
+    # Let's try to extract a meaningful name for the classes
+    # We need to support theses format:
+    #
+    # class LogStash::Inputs::File # legacy
+    # module LogStash module inputs File
+    # module LogStash
+    #    ....
+    #    module Inputs
+    #    ...
+    #    class File # new kid on the block
+    def extract_class_name(file)
+      content = read_file(file)
+      legacy_definition = content.match(CLASS_DEFINITION_RE)
+
+      if legacy_definition.nil?
+        match_data = content.match(NEW_CLASS_DEFINITION_RE)
+        "#{match_data[1]}::#{match_data[2]}::#{match_data[3]}"
+      else
+        if valid_class_name(legacy_definition[1])
+          legacy_definition[1]
+        else
+          m = content.match(NEW_CLASS_DEFINITION_RE_ML)
+          "LogStash::#{m[3]}::#{m[1]}"
+        end
+      end
+    end
+
+    def valid_class_name(klass_name)
+      klass_name =~ VALID_CLASS_NAME
+    end
+  end
+end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/task_runner.rb b/tools/logstash-docgen/lib/logstash/docgen/task_runner.rb
new file mode 100644
index 00000000000..e74fee30a3e
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/task_runner.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require "logstash/docgen/util"
+
+module LogStash module Docgen
+  class TaskRunner
+    TERMINAL_MAX_WIDTH = 80
+
+    class Status
+      attr_reader :name, :error
+
+      def initialize(name, error = nil)
+        @name = name
+        @error = error
+      end
+
+      def success?
+        error.nil?
+      end
+    end
+
+    def initialize
+      @delayed_failures = []
+    end
+
+    def run(job_name, &block)
+      begin
+        block.call
+        report(Status.new(job_name))
+      rescue => e
+        report(Status.new(job_name, e))
+      end
+    end
+
+    def report_failures
+      if failures?
+        puts "-" * TERMINAL_MAX_WIDTH
+
+        @delayed_failures.each do |failure|
+          puts Util.red("FAILURE: #{failure.name}")
+          puts Util.red("\tException: #{failure.error.class} message: #{failure.error}")
+          puts "\t\t#{Util.red(failure.error.backtrace.join("\n\t\t"))}"
+          puts "\n"
+        end
+        return true
+      else
+        return false
+      end
+    end
+
+    def failures?
+      @delayed_failures.size > 0
+    end
+
+    def report(status)
+      if status.success?
+        puts "#{status.name} > #{Util.green("SUCCESS")}"
+      else
+        puts "#{status.name} > #{Util.red("FAIL")}"
+        @delayed_failures << status
+      end
+    end
+  end
+end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/util.rb b/tools/logstash-docgen/lib/logstash/docgen/util.rb
new file mode 100644
index 00000000000..47286cc1cd1
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/util.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+module LogStash module Docgen module Util
+  def self.time_execution(&block)
+    started_at = Time.now
+    result = block.call
+    puts "Execution took: #{Time.now - started_at}s"
+    return result
+  end
+
+  def self.red(text)
+    "\e[31m#{text}\e[0m"
+  end
+
+  def self.green(text)
+    "\e[32m#{text}\e[0m"
+  end
+
+  def self.yellow(text)
+    "\e[33m#{text}\e[0m"
+  end
+end end end
diff --git a/tools/logstash-docgen/lib/logstash/docgen/version.rb b/tools/logstash-docgen/lib/logstash/docgen/version.rb
new file mode 100644
index 00000000000..b1940f52e49
--- /dev/null
+++ b/tools/logstash-docgen/lib/logstash/docgen/version.rb
@@ -0,0 +1,5 @@
+module Logstash
+  module Docgen
+    VERSION = "0.1.0"
+  end
+end
diff --git a/tools/logstash-docgen/logstash-docgen.gemspec b/tools/logstash-docgen/logstash-docgen.gemspec
new file mode 100644
index 00000000000..961492cf3c0
--- /dev/null
+++ b/tools/logstash-docgen/logstash-docgen.gemspec
@@ -0,0 +1,37 @@
+# coding: utf-8
+lib = File.expand_path('../lib', __FILE__)
+$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
+require "logstash/docgen/version"
+
+Gem::Specification.new do |spec|
+  spec.name          = "logstash-docgen"
+  spec.version       = Logstash::Docgen::VERSION
+  spec.authors       = ["Elastic"]
+  spec.email         = ["info@elastic.co"]
+
+  spec.summary       = %q{Logstash Tooling to generate the documentation of a plugin}
+  spec.homepage      = "https://elastic.co/logstash"
+
+  spec.files         = `git ls-files -z`.split("\x0").reject { |f| f.match(%r{^(test|spec|features)/}) }
+  spec.bindir        = "bin"
+  spec.executables   = spec.files.grep(%r{^bin/}) { |f| File.basename(f) }
+  spec.require_paths = ["lib"]
+
+  spec.add_runtime_dependency "clamp"
+  spec.add_runtime_dependency "stud"
+  spec.add_runtime_dependency "git"
+  spec.add_runtime_dependency "asciidoctor"
+  spec.add_runtime_dependency "pry"
+  spec.add_runtime_dependency "addressable"
+  spec.add_runtime_dependency "octokit", "~> 3.8.0"
+
+  # gems 1.0.0 requires Ruby 2.1.9 or newer, so we pin down.
+  spec.add_runtime_dependency "gems", "0.8.3"
+
+  spec.add_development_dependency "rake", "~> 10.0"
+  spec.add_development_dependency "rspec"
+
+  # Used for the dependency lookup code
+  spec.add_development_dependency "vcr"
+  spec.add_development_dependency "webmock", "2.2.0"
+end
diff --git a/tools/logstash-docgen/logstash-docgen.yml b/tools/logstash-docgen/logstash-docgen.yml
new file mode 100644
index 00000000000..6c329006b72
--- /dev/null
+++ b/tools/logstash-docgen/logstash-docgen.yml
@@ -0,0 +1,8 @@
+---
+ignore_plugins:
+  - -mixin-
+  - logstash-input-example
+  - logstash-output-example
+  - logstash-codec-example
+  - logstash-filter-example
+  - logstash-patterns-core
diff --git a/tools/logstash-docgen/spec/fixtures/logstash-filter-dummy.gemspec b/tools/logstash-docgen/spec/fixtures/logstash-filter-dummy.gemspec
new file mode 100644
index 00000000000..39db14d14a8
--- /dev/null
+++ b/tools/logstash-docgen/spec/fixtures/logstash-filter-dummy.gemspec
@@ -0,0 +1,26 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-dummy'
+  s.version         = '0.1.1'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in logstash-docgen test"
+  s.description     = "This plugin is only used in logstash-docgen test"
+
+  s.authors       = ["Elastic"]
+  s.email         = ["info@elastic.co"]
+
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = Dir["lib/**/*","spec/**/*","*.gemspec","*.md","CONTRIBUTORS","Gemfile","LICENSE","NOTICE.TXT", "vendor/jar-dependencies/**/*.jar", "vendor/jar-dependencies/**/*.rb", "VERSION"]
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+  s.add_runtime_dependency "logstash-core-plugin-api", ">= 1.60", "<= 2.99"
+end
diff --git a/tools/logstash-docgen/spec/fixtures/plugins_source/base.rb b/tools/logstash-docgen/spec/fixtures/plugins_source/base.rb
new file mode 100644
index 00000000000..0f4a5546939
--- /dev/null
+++ b/tools/logstash-docgen/spec/fixtures/plugins_source/base.rb
@@ -0,0 +1,4 @@
+class LogStash::Inputs::Base
+  # Parent configuration
+  config :parent_config, :type => :string, :require => true
+end
diff --git a/tools/logstash-docgen/spec/fixtures/plugins_source/config_from_mixin.rb b/tools/logstash-docgen/spec/fixtures/plugins_source/config_from_mixin.rb
new file mode 100644
index 00000000000..9a9583fa36c
--- /dev/null
+++ b/tools/logstash-docgen/spec/fixtures/plugins_source/config_from_mixin.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+
+module ConfigFromMixin
+  # Config mixing description
+  config :config_mixin, :type => :string, :default => "mixin config", :required => true
+
+  # A deprecated config option
+  config :config_mixin_deprecated, :type => :string, :deprecated => true
+end
diff --git a/tools/logstash-docgen/spec/fixtures/plugins_source/new_style_header.rb b/tools/logstash-docgen/spec/fixtures/plugins_source/new_style_header.rb
new file mode 100644
index 00000000000..ec53055dd21
--- /dev/null
+++ b/tools/logstash-docgen/spec/fixtures/plugins_source/new_style_header.rb
@@ -0,0 +1,16 @@
+require_relative "base"
+
+# encoding: utf-8
+#
+# This is a new test plugins
+# with multiple line.
+
+module LogStash module Inputs class Dummy < LogStash::Inputs::Base
+  config_name "dummy"
+
+  # option 1 description
+  config :option1, :type => :boolean, :default => false
+
+  # option 2 description
+  config :option2, :type => :string, :default => "localhost"
+end
diff --git a/tools/logstash-docgen/spec/fixtures/plugins_source/old_style_header.rb b/tools/logstash-docgen/spec/fixtures/plugins_source/old_style_header.rb
new file mode 100644
index 00000000000..26852f43da1
--- /dev/null
+++ b/tools/logstash-docgen/spec/fixtures/plugins_source/old_style_header.rb
@@ -0,0 +1,19 @@
+require_relative "base"
+require_relative "config_from_mixin"
+
+# encoding: utf-8
+#
+# This is a new test plugins
+# with multiple line.
+
+class LogStash::Inputs::Dummy < LogStash::Inputs::Base
+  config_name "dummy"
+
+  include ConfigFromMixin
+
+  # option 1 description
+  config :option1, :type => :boolean, :default => false
+
+  # option 2 description
+  config :option2, :type => :string, :default => "localhost"
+end
diff --git a/tools/logstash-docgen/spec/fixtures/vcr_cassettes/logstash-core.yml b/tools/logstash-docgen/spec/fixtures/vcr_cassettes/logstash-core.yml
new file mode 100644
index 00000000000..cf0a5b39853
--- /dev/null
+++ b/tools/logstash-docgen/spec/fixtures/vcr_cassettes/logstash-core.yml
@@ -0,0 +1,1267 @@
+---
+http_interactions:
+- request:
+    method: get
+    uri: https://rubygems.org/api/v1/dependencies?gems=logstash-core-plugin-api
+    body:
+      encoding: US-ASCII
+      string: ''
+    headers:
+      Accept:
+      - "*/*"
+      User-Agent:
+      - Gems 0.8.3
+      - Ruby
+      Authorization:
+      - 953b0543ac7ee8528d2a14f263615e33
+      Connection:
+      - keep-alive
+      Keep-Alive:
+      - '30'
+      Content-Type:
+      - application/x-www-form-urlencoded
+  response:
+    status:
+      code: 200
+      message: OK
+    headers:
+      Server:
+      - nginx
+      Date:
+      - Tue, 16 Aug 2016 13:07:19 GMT
+      Content-Type:
+      - application/octet-stream; charset=utf-8
+      Transfer-Encoding:
+      - chunked
+      Connection:
+      - keep-alive
+      X-Frame-Options:
+      - SAMEORIGIN
+      X-Xss-Protection:
+      - 1; mode=block
+      X-Content-Type-Options:
+      - nosniff
+      Etag:
+      - W/"aafe8b2d5a54b17008aadfad2d80022c"
+      Cache-Control:
+      - max-age=0, private, must-revalidate
+      X-Request-Id:
+      - 6dd0326a-3a4c-4d60-8fc3-c76f07085872
+      X-Runtime:
+      - '0.006193'
+      X-Ua-Compatible:
+      - IE=Edge,chrome=1
+    body:
+      encoding: US-ASCII
+      string: !binary |-
+        BAhbIHsJOgluYW1lSSIdbG9nc3Rhc2gtY29yZS1wbHVnaW4tYXBpBjoGRVQ6
+        C251bWJlckkiCjIuMS4zBjsGVDoNcGxhdGZvcm1JIglqYXZhBjsGVDoRZGVw
+        ZW5kZW5jaWVzWwZbB0kiEmxvZ3N0YXNoLWNvcmUGOwZUSSIdPSA1LjAuMC5h
+        bHBoYTMuc25hcHNob3Q2BjsGVHsJOwBJIh1sb2dzdGFzaC1jb3JlLXBsdWdp
+        bi1hcGkGOwZUOwdJIgoyLjEuOQY7BlQ7CEkiCWphdmEGOwZUOwlbBlsHSSIS
+        bG9nc3Rhc2gtY29yZQY7BlRJIh09IDUuMC4wLmFscGhhNC5zbmFwc2hvdDMG
+        OwZUewk7AEkiHWxvZ3N0YXNoLWNvcmUtcGx1Z2luLWFwaQY7BlQ7B0kiCjIu
+        MS42BjsGVDsISSIJamF2YQY7BlQ7CVsGWwdJIhJsb2dzdGFzaC1jb3JlBjsG
+        VEkiEz0gNS4wLjAuYWxwaGEzBjsGVHsJOwBJIh1sb2dzdGFzaC1jb3JlLXBs
+        dWdpbi1hcGkGOwZUOwdJIgoyLjEuMgY7BlQ7CEkiCWphdmEGOwZUOwlbBlsH
+        SSISbG9nc3Rhc2gtY29yZQY7BlRJIh09IDUuMC4wLmFscGhhMy5zbmFwc2hv
+        dDUGOwZUewk7AEkiHWxvZ3N0YXNoLWNvcmUtcGx1Z2luLWFwaQY7BlQ7B0ki
+        CzEuMTAuMAY7BlQ7CEkiCWphdmEGOwZUOwlbBlsHSSISbG9nc3Rhc2gtY29y
+        ZQY7BlRJIhc8PSAyLjMuMiwgPj0gMi4wLjAGOwZUewk7AEkiHWxvZ3N0YXNo
+        LWNvcmUtcGx1Z2luLWFwaQY7BlQ7B0kiCjEuOC4wBjsGVDsISSIJamF2YQY7
+        BlQ7CVsGWwdJIhJsb2dzdGFzaC1jb3JlBjsGVEkiFzw9IDIuMy4xLCA+PSAy
+        LjAuMAY7BlR7CTsASSIdbG9nc3Rhc2gtY29yZS1wbHVnaW4tYXBpBjsGVDsH
+        SSIKMS41LjAGOwZUOwhJIglqYXZhBjsGVDsJWwZbB0kiEmxvZ3N0YXNoLWNv
+        cmUGOwZUSSIdPSA1LjAuMC5hbHBoYTEuc25hcHNob3QyBjsGVHsJOwBJIh1s
+        b2dzdGFzaC1jb3JlLXBsdWdpbi1hcGkGOwZUOwdJIgoyLjEuMQY7BlQ7CEki
+        CWphdmEGOwZUOwlbBlsHSSISbG9nc3Rhc2gtY29yZQY7BlRJIh09IDUuMC4w
+        LmFscGhhMy5zbmFwc2hvdDQGOwZUewk7AEkiHWxvZ3N0YXNoLWNvcmUtcGx1
+        Z2luLWFwaQY7BlQ7B0kiCzEuMTYuMAY7BlQ7CEkiCWphdmEGOwZUOwlbBlsH
+        SSISbG9nc3Rhc2gtY29yZQY7BlRJIiE8PSAyLjMuMy5zbmFwc2hvdDEsID49
+        IDIuMC4wBjsGVHsJOwBJIh1sb2dzdGFzaC1jb3JlLXBsdWdpbi1hcGkGOwZU
+        OwdJIgoxLjcuMAY7BlQ7CEkiCWphdmEGOwZUOwlbBlsHSSISbG9nc3Rhc2gt
+        Y29yZQY7BlRJIiE8PSAyLjMuMS5zbmFwc2hvdDEsID49IDIuMC4wBjsGVHsJ
+        OwBJIh1sb2dzdGFzaC1jb3JlLXBsdWdpbi1hcGkGOwZUOwdJIgoxLjEuMAY7
+        BlQ7CEkiCWphdmEGOwZUOwlbBlsHSSISbG9nc3Rhc2gtY29yZQY7BlRJIiE8
+        PSAyLjMuMC5zbmFwc2hvdDQsID49IDIuMC4wBjsGVHsJOwBJIh1sb2dzdGFz
+        aC1jb3JlLXBsdWdpbi1hcGkGOwZUOwdJIgoyLjEuNwY7BlQ7CEkiCWphdmEG
+        OwZUOwlbBlsHSSISbG9nc3Rhc2gtY29yZQY7BlRJIh09IDUuMC4wLmFscGhh
+        NC5zbmFwc2hvdDEGOwZUewk7AEkiHWxvZ3N0YXNoLWNvcmUtcGx1Z2luLWFw
+        aQY7BlQ7B0kiCzEuMTguMAY7BlQ7CEkiCWphdmEGOwZUOwlbBlsHSSISbG9n
+        c3Rhc2gtY29yZQY7BlRJIhc8PSAyLjMuMywgPj0gMi4wLjAGOwZUewk7AEki
+        HWxvZ3N0YXNoLWNvcmUtcGx1Z2luLWFwaQY7BlQ7B0kiCjIuMS40BjsGVDsI
+        SSIJamF2YQY7BlQ7CVsGWwdJIhJsb2dzdGFzaC1jb3JlBjsGVEkiHT0gNS4w
+        LjAuYWxwaGEzLnNuYXBzaG90NwY7BlR7CTsASSIdbG9nc3Rhc2gtY29yZS1w
+        bHVnaW4tYXBpBjsGVDsHSSIKMS4zLjAGOwZUOwhJIglqYXZhBjsGVDsJWwZb
+        B0kiEmxvZ3N0YXNoLWNvcmUGOwZUSSIXPD0gMi4zLjAsID49IDIuMC4wBjsG
+        VHsJOwBJIh1sb2dzdGFzaC1jb3JlLXBsdWdpbi1hcGkGOwZUOwdJIgsxLjE5
+        LjAGOwZUOwhJIglqYXZhBjsGVDsJWwZbB0kiEmxvZ3N0YXNoLWNvcmUGOwZU
+        SSIhPD0gMi4zLjQuc25hcHNob3QxLCA+PSAyLjAuMAY7BlR7CTsASSIdbG9n
+        c3Rhc2gtY29yZS1wbHVnaW4tYXBpBjsGVDsHSSILMS4yMC4wBjsGVDsISSIJ
+        amF2YQY7BlQ7CVsGWwdJIhJsb2dzdGFzaC1jb3JlBjsGVEkiFzw9IDIuMy40
+        LCA+PSAyLjAuMAY7BlR7CTsASSIdbG9nc3Rhc2gtY29yZS1wbHVnaW4tYXBp
+        BjsGVDsHSSIKMS45LjAGOwZUOwhJIglqYXZhBjsGVDsJWwZbB0kiEmxvZ3N0
+        YXNoLWNvcmUGOwZUSSIhPD0gMi4zLjIuc25hcHNob3QxLCA+PSAyLjAuMAY7
+        BlR7CTsASSIdbG9nc3Rhc2gtY29yZS1wbHVnaW4tYXBpBjsGVDsHSSILMS4x
+        Ny4wBjsGVDsISSIJamF2YQY7BlQ7CVsGWwdJIhJsb2dzdGFzaC1jb3JlBjsG
+        VEkiITw9IDIuMy4zLnNuYXBzaG90MiwgPj0gMi4wLjAGOwZUewk7AEkiHWxv
+        Z3N0YXNoLWNvcmUtcGx1Z2luLWFwaQY7BlQ7B0kiCjEuMi4wBjsGVDsISSIJ
+        amF2YQY7BlQ7CVsGWwdJIhJsb2dzdGFzaC1jb3JlBjsGVEkiITw9IDIuMy4w
+        LnNuYXBzaG90NSwgPj0gMi4wLjAGOwZUewk7AEkiHWxvZ3N0YXNoLWNvcmUt
+        cGx1Z2luLWFwaQY7BlQ7B0kiCjIuMS44BjsGVDsISSIJamF2YQY7BlQ7CVsG
+        WwdJIhJsb2dzdGFzaC1jb3JlBjsGVEkiHT0gNS4wLjAuYWxwaGE0LnNuYXBz
+        aG90MgY7BlR7CTsASSIdbG9nc3Rhc2gtY29yZS1wbHVnaW4tYXBpBjsGVDsH
+        SSIKMS40LjAGOwZUOwhJIglqYXZhBjsGVDsJWwZbB0kiEmxvZ3N0YXNoLWNv
+        cmUGOwZUSSIdPSA1LjAuMC5hbHBoYTEuc25hcHNob3QxBjsGVHsJOwBJIh1s
+        b2dzdGFzaC1jb3JlLXBsdWdpbi1hcGkGOwZUOwdJIgoxLjYuMAY7BlQ7CEki
+        CWphdmEGOwZUOwlbBlsHSSISbG9nc3Rhc2gtY29yZQY7BlRJIhM9IDUuMC4w
+        LmFscGhhMQY7BlR7CTsASSIdbG9nc3Rhc2gtY29yZS1wbHVnaW4tYXBpBjsG
+        VDsHSSIKMi4xLjUGOwZUOwhJIglqYXZhBjsGVDsJWwZbB0kiEmxvZ3N0YXNo
+        LWNvcmUGOwZUSSIdPSA1LjAuMC5hbHBoYTMuc25hcHNob3Q4BjsGVHsJOwBJ
+        Ih1sb2dzdGFzaC1jb3JlLXBsdWdpbi1hcGkGOwZUOwdJIgsyLjEuMTAGOwZU
+        OwhJIglqYXZhBjsGVDsJWwZbB0kiEmxvZ3N0YXNoLWNvcmUGOwZUSSIXPSA1
+        LjAuMC5wcmUuYWxwaGE1BjsGVHsJOwBJIh1sb2dzdGFzaC1jb3JlLXBsdWdp
+        bi1hcGkGOwZUOwdJIgoxLjAuMAY7BlQ7CEkiCWphdmEGOwZUOwlbBlsHSSIS
+        bG9nc3Rhc2gtY29yZQY7BlRJIiE8PSAyLjMuMC5zbmFwc2hvdDMsID49IDIu
+        MC4wBjsGVHsJOwBJIh1sb2dzdGFzaC1jb3JlLXBsdWdpbi1hcGkGOwZUOwdJ
+        IgsyLjEuMTEGOwZUOwhJIglqYXZhBjsGVDsJWwZbB0kiEmxvZ3N0YXNoLWNv
+        cmUGOwZUSSIdPSA1LjAuMC5hbHBoYTUuc25hcHNob3QxBjsGVA==
+    http_version:
+  recorded_at: Tue, 16 Aug 2016 13:07:19 GMT
+- request:
+    method: get
+    uri: https://rubygems.org/api/v1/dependencies?gems=logstash-core
+    body:
+      encoding: US-ASCII
+      string: ''
+    headers:
+      Accept:
+      - "*/*"
+      User-Agent:
+      - Gems 0.8.3
+      - Ruby
+      Authorization:
+      - 953b0543ac7ee8528d2a14f263615e33
+      Connection:
+      - keep-alive
+      Keep-Alive:
+      - '30'
+      Content-Type:
+      - application/x-www-form-urlencoded
+  response:
+    status:
+      code: 200
+      message: OK
+    headers:
+      Server:
+      - nginx
+      Date:
+      - Tue, 16 Aug 2016 13:07:20 GMT
+      Content-Type:
+      - application/octet-stream; charset=utf-8
+      Transfer-Encoding:
+      - chunked
+      Connection:
+      - keep-alive
+      X-Frame-Options:
+      - SAMEORIGIN
+      X-Xss-Protection:
+      - 1; mode=block
+      X-Content-Type-Options:
+      - nosniff
+      Etag:
+      - W/"f0e86058e0041b67adbc2854e6525213"
+      Cache-Control:
+      - max-age=0, private, must-revalidate
+      X-Request-Id:
+      - a37fa4e5-29a0-4eac-934b-e3cabf34ec13
+      X-Runtime:
+      - '0.019220'
+      X-Ua-Compatible:
+      - IE=Edge,chrome=1
+    body:
+      encoding: US-ASCII
+      string: !binary |-
+        BAhbYXsJOgluYW1lSSISbG9nc3Rhc2gtY29yZQY6BkVUOgtudW1iZXJJIgoy
+        LjMuMgY7BlQ6DXBsYXRmb3JtSSIJamF2YQY7BlQ6EWRlcGVuZGVuY2llc1sU
+        WwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRf
+        c2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4g
+        MS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJ
+        aTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEu
+        NS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xMwY7BlRb
+        B0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4yBjsGVFsHSSIJZ2Vt
+        cwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4w
+        LjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQG
+        OwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7
+        BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuOC4wBjsGVFsHSSIYbG9nc3Rhc2gt
+        Y29yZS1ldmVudAY7BlRJIgw9IDIuMy4yBjsGVHsJOwBJIhJsb2dzdGFzaC1j
+        b3JlBjsGVDsHSSIUMi4xLjAuc25hcHNob3QyBjsGVDsISSIJamF2YQY7BlQ7
+        CVsTWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJl
+        YWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSIN
+        fj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsH
+        SSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8
+        IDEuNS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg4+PSAwLjkuMTEG
+        OwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAwLjkuMgY7BlRbB0ki
+        CWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9
+        IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglz
+        dHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEw
+        LjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlR7CTsASSISbG9n
+        c3Rhc2gtY29yZQY7BlQ7B0kiFDIuMy4wLnNuYXBzaG90MQY7BlQ7CEkiCWph
+        dmEGOwZUOwlbFFsHSSIdbG9nc3Rhc2gtY29yZS1ldmVudC1qYXZhBjsGVEki
+        Fj0gMi4zLjAuc25hcHNob3QxBjsGVFsHSSIOanJqYWNrc29uBjsGVEkiDX4+
+        IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZU
+        WwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7
+        BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRb
+        B0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9wZW5z
+        c2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZU
+        SSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJ
+        Ig1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSIN
+        fj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0ki
+        CHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAw
+        LjguMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiCjEuNS4xBjsG
+        VDsISSIJamF2YQY7BlQ7CVsPWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4y
+        LjgGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0ki
+        DG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0g
+        MC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIg1m
+        aWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4g
+        MC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHBy
+        eQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcu
+        MAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIuMy4wLnNuYXBz
+        aG90NQY7BlQ7CEkiCWphdmEGOwZUOwlbFFsHSSIOanJqYWNrc29uBjsGVEki
+        DX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUG
+        OwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRh
+        cgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7
+        BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9w
+        ZW5zc2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkG
+        OwZUSSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZU
+        WwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZU
+        SSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRb
+        B0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+
+        PiAwLjguMAY7BlRbB0kiHWxvZ3N0YXNoLWNvcmUtZXZlbnQtamF2YQY7BlRJ
+        IhY9IDIuMy4wLnNuYXBzaG90NQY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7
+        BlQ7B0kiFDIuMy4zLnNuYXBzaG90MQY7BlQ7CEkiCWphdmEGOwZUOwlbFFsH
+        SSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3Nh
+        ZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEu
+        MS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkx
+        OG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUu
+        MAY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJ
+        IhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMG
+        OwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40
+        BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsG
+        VEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZU
+        WwdJIgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiGGxvZ3N0YXNoLWNv
+        cmUtZXZlbnQGOwZUSSIWPSAyLjMuMy5zbmFwc2hvdDEGOwZUewk7AEkiEmxv
+        Z3N0YXNoLWNvcmUGOwZUOwdJIhQyLjMuNC5zbmFwc2hvdDEGOwZUOwhJIglq
+        YXZhBjsGVDsJWxRbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRb
+        B0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXpp
+        cAY7BlRJIg1+PiAxLjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41
+        LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9w
+        BjsGVEkiDDwgMS41LjAGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDT0g
+        MC45LjEzBjsGVFsHSSIUY29uY3VycmVudC1ydWJ5BjsGVEkiDD0gMC45LjIG
+        OwZUWwdJIglnZW1zBjsGVEkiDX4+IDAuOC4zBjsGVFsHSSINZmlsZXNpemUG
+        OwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsG
+        VFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIO
+        fj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZUSSINfj4gMC44LjAGOwZUWwdJ
+        Ihhsb2dzdGFzaC1jb3JlLWV2ZW50BjsGVEkiFj0gMi4zLjQuc25hcHNob3Qx
+        BjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIKMS41LjUGOwZUOwhJ
+        IglqYXZhBjsGVDsJWxJbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNgY7
+        BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMbWlu
+        aXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYu
+        OQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5
+        LW9wZW5zc2wGOwZUSSIOPj0gMC45LjExBjsGVFsHSSIUY29uY3VycmVudC1y
+        dWJ5BjsGVEkiDX4+IDAuOS4xBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjgu
+        MwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFt
+        cAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5
+        BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsG
+        VEkiDX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIb
+        NS4wLjAuYWxwaGEyLnNuYXBzaG90MgY7BlQ7CEkiCWphdmEGOwZUOwlbGFsH
+        SSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3Nh
+        ZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEu
+        MS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkx
+        OG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUu
+        MAY7BlRbB0kiFWpydWJ5LW1vbml0b3JpbmcGOwZUSSINfj4gMC4zLjEGOwZU
+        WwdJIhVjaHJvbmljX2R1cmF0aW9uBjsGVEkiDT0gMC4xMC42BjsGVFsHSSIS
+        anJ1Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xNgY7BlRbB0kiCXB1bWEGOwZU
+        SSIXPj0gMi4xNi4wLCB+PiAyLjE2BjsGVFsHSSIMc2luYXRyYQY7BlRJIhU+
+        PSAxLjQuNiwgfj4gMS40BjsGVFsHSSIUY29uY3VycmVudC1ydWJ5BjsGVEki
+        DD0gMS4wLjAGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAuOC4zBjsGVFsHSSIN
+        ZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+
+        IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghw
+        cnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZUSSINfj4gMC44
+        LjAGOwZUWwdJIh1sb2dzdGFzaC1jb3JlLWV2ZW50LWphdmEGOwZUSSIefj4g
+        NS4wLjAuYWxwaGEyLnNuYXBzaG90MgY7BlR7CTsASSISbG9nc3Rhc2gtY29y
+        ZQY7BlQ7B0kiGDEuNS4wLnJjMy5zbmFwc2hvdDMGOwZUOwhJIglqYXZhBjsG
+        VDsJWxNbB0kiDmpyamFja3NvbgY7BlRJIgk+PSAwBjsGVFsHSSIWZmlsZS1k
+        ZXBlbmRlbmNpZXMGOwZUSSIMPSAwLjEuNgY7BlRbB0kiDG1pbml0YXIGOwZU
+        SSIJPj0gMAY7BlRbB0kiDHNpbmF0cmEGOwZUSSIJPj0gMAY7BlRbB0kiCXJh
+        Y2sGOwZUSSIJPj0gMAY7BlRbB0kiD21pbWUtdHlwZXMGOwZUSSIJPj0gMAY7
+        BlRbB0kiCGZ0dwY7BlRJIg5+PiAwLjAuNDAGOwZUWwdJIglpMThuBjsGVEki
+        DD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDX4+IDEuNC4wBjsGVFsH
+        SSINZmlsZXNpemUGOwZUSSIJPj0gMAY7BlRbB0kiCmNsYW1wBjsGVEkiCT49
+        IDAGOwZUWwdJIglzdHVkBjsGVEkiCT49IDAGOwZUWwdJIghwcnkGOwZUSSIJ
+        Pj0gMAY7BlRbB0kiCmNhYmluBjsGVEkiDT49IDAuNy4wBjsGVHsJOwBJIhJs
+        b2dzdGFzaC1jb3JlBjsGVDsHSSIYMS41LjAucmM0LnNuYXBzaG90MgY7BlQ7
+        CEkiCWphdmEGOwZUOwlbDlsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMi44
+        BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4G
+        OwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7
+        BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7
+        BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsG
+        VFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEki
+        DX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIYMS41
+        LjAucmMzLnNuYXBzaG90NgY7BlQ7CEkiCWphdmEGOwZUOwlbE1sHSSIOanJq
+        YWNrc29uBjsGVEkiCT49IDAGOwZUWwdJIhZmaWxlLWRlcGVuZGVuY2llcwY7
+        BlRJIgw9IDAuMS42BjsGVFsHSSIMbWluaXRhcgY7BlRJIgk+PSAwBjsGVFsH
+        SSIMc2luYXRyYQY7BlRJIgk+PSAwBjsGVFsHSSIJcmFjawY7BlRJIgk+PSAw
+        BjsGVFsHSSIPbWltZS10eXBlcwY7BlRJIgk+PSAwBjsGVFsHSSIIZnR3BjsG
+        VEkiDn4+IDAuMC40MAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRb
+        B0kiDHRyZWV0b3AGOwZUSSINfj4gMS40LjAGOwZUWwdJIg1maWxlc2l6ZQY7
+        BlRJIgk+PSAwBjsGVFsHSSIKY2xhbXAGOwZUSSIJPj0gMAY7BlRbB0kiCXN0
+        dWQGOwZUSSIJPj0gMAY7BlRbB0kiCHByeQY7BlRJIgk+PSAwBjsGVFsHSSIK
+        Y2FiaW4GOwZUSSINPj0gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUG
+        OwZUOwdJIhQyLjIuMC5zbmFwc2hvdDMGOwZUOwhJIglqYXZhBjsGVDsJWxRb
+        B0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9z
+        YWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAx
+        LjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglp
+        MThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41
+        LjAGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDT0gMC45LjEzBjsGVFsH
+        SSIUY29uY3VycmVudC1ydWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJIglnZW1z
+        BjsGVEkiDX4+IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAu
+        NAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7
+        BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsG
+        VFsHSSIKY2FiaW4GOwZUSSINfj4gMC43LjAGOwZUWwdJIhhsb2dzdGFzaC1j
+        b3JlLWV2ZW50BjsGVEkiF34+IDIuMi4wLnNuYXBzaG90MwY7BlR7CTsASSIS
+        bG9nc3Rhc2gtY29yZQY7BlQ7B0kiCjIuMi4yBjsGVDsISSIJamF2YQY7BlQ7
+        CVsUWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJl
+        YWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSIN
+        fj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsH
+        SSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8
+        IDEuNS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xMwY7
+        BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4yBjsGVFsHSSIJ
+        Z2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0g
+        MC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0
+        dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAu
+        MQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVFsHSSIYbG9nc3Rh
+        c2gtY29yZS1ldmVudAY7BlRJIg1+PiAyLjIuMgY7BlR7CTsASSISbG9nc3Rh
+        c2gtY29yZQY7BlQ7B0kiEDIuMC4wLmJldGEzBjsGVDsISSIJamF2YQY7BlQ7
+        CVsSWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjYGOwZUWwdJIhB0aHJl
+        YWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDG1pbml0YXIGOwZUSSIN
+        fj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0
+        cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsG
+        VEkiDj49IDAuOS4xMQY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIg1+
+        PiAwLjkuMQY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1m
+        aWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4g
+        MC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHBy
+        eQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcu
+        MAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIuMy4wLnNuYXBz
+        aG90MwY7BlQ7CEkiCWphdmEGOwZUOwlbFFsHSSIOanJqYWNrc29uBjsGVEki
+        DX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUG
+        OwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRh
+        cgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7
+        BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9w
+        ZW5zc2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkG
+        OwZUSSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZU
+        WwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZU
+        SSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRb
+        B0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+
+        PiAwLjguMAY7BlRbB0kiHWxvZ3N0YXNoLWNvcmUtZXZlbnQtamF2YQY7BlRJ
+        IhY9IDIuMy4wLnNuYXBzaG90MwY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7
+        BlQ7B0kiCjEuNS4wBjsGVDsISSIJamF2YQY7BlQ7CVsOWwdJIg5qcmphY2tz
+        b24GOwZUSSINfj4gMC4yLjgGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAu
+        NS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRv
+        cAY7BlRJIgw8IDEuNS4wBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAu
+        NAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7
+        BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsG
+        VFsHSSIKY2FiaW4GOwZUSSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNo
+        LWNvcmUGOwZUOwdJIhE1LjAuMC5hbHBoYTEGOwZUOwhJIglqYXZhBjsGVDsJ
+        WxhbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVh
+        ZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+
+        PiAxLjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJ
+        IglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwg
+        MS41LjAGOwZUWwdJIhVqcnVieS1tb25pdG9yaW5nBjsGVEkiDX4+IDAuMy4x
+        BjsGVFsHSSIVY2hyb25pY19kdXJhdGlvbgY7BlRJIg09IDAuMTAuNgY7BlRb
+        B0kiEmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJIglwdW1h
+        BjsGVEkiFz49IDIuMTYuMCwgfj4gMi4xNgY7BlRbB0kiDHNpbmF0cmEGOwZU
+        SSIVPj0gMS40LjYsIH4+IDEuNAY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7
+        BlRJIgw9IDEuMC4wBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRb
+        B0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJ
+        Ig1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsH
+        SSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+
+        IDAuOC4wBjsGVFsHSSIdbG9nc3Rhc2gtY29yZS1ldmVudC1qYXZhBjsGVEki
+        FH4+IDUuMC4wLmFscGhhMQY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7
+        B0kiETUuMC4wLmFscGhhMgY7BlQ7CEkiCWphdmEGOwZUOwlbGFsHSSIOanJq
+        YWNrc29uBjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZU
+        SSINfj4gMC4zLjUGOwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsG
+        VFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZU
+        SSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRb
+        B0kiFWpydWJ5LW1vbml0b3JpbmcGOwZUSSINfj4gMC4zLjEGOwZUWwdJIhVj
+        aHJvbmljX2R1cmF0aW9uBjsGVEkiDT0gMC4xMC42BjsGVFsHSSISanJ1Ynkt
+        b3BlbnNzbAY7BlRJIg09IDAuOS4xNgY7BlRbB0kiCXB1bWEGOwZUSSIXPj0g
+        Mi4xNi4wLCB+PiAyLjE2BjsGVFsHSSIMc2luYXRyYQY7BlRJIhU+PSAxLjQu
+        Niwgfj4gMS40BjsGVFsHSSIUY29uY3VycmVudC1ydWJ5BjsGVEkiDD0gMS4w
+        LjAGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAuOC4zBjsGVFsHSSINZmlsZXNp
+        emUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41
+        BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZU
+        SSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZUSSINfj4gMC44LjAGOwZU
+        WwdJIh1sb2dzdGFzaC1jb3JlLWV2ZW50LWphdmEGOwZUSSIUfj4gNS4wLjAu
+        YWxwaGEyBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIUMi4yLjQu
+        c25hcHNob3QyBjsGVDsISSIJamF2YQY7BlQ7CVsUWwdJIg5qcmphY2tzb24G
+        OwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAw
+        LjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxt
+        aW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAu
+        Ni45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1
+        Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xMwY7BlRbB0kiFGNvbmN1cnJlbnQt
+        cnVieQY7BlRJIgw9IDAuOS4yBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjgu
+        MwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFt
+        cAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5
+        BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsG
+        VEkiDX4+IDAuNy4wBjsGVFsHSSIYbG9nc3Rhc2gtY29yZS1ldmVudAY7BlRJ
+        Ihd+PiAyLjIuNC5zbmFwc2hvdDIGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUG
+        OwZUOwdJIhgxLjUuMC5yYzMuc25hcHNob3QxBjsGVDsISSIJamF2YQY7BlQ7
+        CVsXWwdJIg5qcmphY2tzb24GOwZUSSIJPj0gMAY7BlRbB0kiFWpydWJ5LWh0
+        dHBjbGllbnQGOwZUSSIJPj0gMAY7BlRbB0kiFmZpbGUtZGVwZW5kZW5jaWVz
+        BjsGVEkiDD0gMC4xLjYGOwZUWwdJIgxtaW5pdGFyBjsGVEkiCT49IDAGOwZU
+        WwdJIhBtYXZlbi10b29scwY7BlRJIgw9IDEuMC43BjsGVFsHSSIPcnVieS1t
+        YXZlbgY7BlRJIhA9IDMuMS4xLjAuOAY7BlRbB0kiFWphci1kZXBlbmRlbmNp
+        ZXMGOwZUSSIMPSAwLjEuNwY7BlRbB0kiDHNpbmF0cmEGOwZUSSIJPj0gMAY7
+        BlRbB0kiCXJhY2sGOwZUSSIJPj0gMAY7BlRbB0kiD21pbWUtdHlwZXMGOwZU
+        SSIJPj0gMAY7BlRbB0kiCGZ0dwY7BlRJIg5+PiAwLjAuNDAGOwZUWwdJIglp
+        MThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDX4+IDEu
+        NC4wBjsGVFsHSSINZmlsZXNpemUGOwZUSSIJPj0gMAY7BlRbB0kiCmNsYW1w
+        BjsGVEkiCT49IDAGOwZUWwdJIglzdHVkBjsGVEkiCT49IDAGOwZUWwdJIghw
+        cnkGOwZUSSIJPj0gMAY7BlRbB0kiCmNhYmluBjsGVEkiDT49IDAuNy4wBjsG
+        VHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIOMS41LjAucmMzBjsGVDsI
+        SSIJamF2YQY7BlQ7CVsTWwdJIg5qcmphY2tzb24GOwZUSSIJPj0gMAY7BlRb
+        B0kiFmZpbGUtZGVwZW5kZW5jaWVzBjsGVEkiDD0gMC4xLjYGOwZUWwdJIgxt
+        aW5pdGFyBjsGVEkiCT49IDAGOwZUWwdJIgxzaW5hdHJhBjsGVEkiCT49IDAG
+        OwZUWwdJIglyYWNrBjsGVEkiCT49IDAGOwZUWwdJIg9taW1lLXR5cGVzBjsG
+        VEkiCT49IDAGOwZUWwdJIghmdHcGOwZUSSIOfj4gMC4wLjQwBjsGVFsHSSIJ
+        aTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIg1+PiAx
+        LjQuMAY7BlRbB0kiDWZpbGVzaXplBjsGVEkiCT49IDAGOwZUWwdJIgpjbGFt
+        cAY7BlRJIgk+PSAwBjsGVFsHSSIJc3R1ZAY7BlRJIgk+PSAwBjsGVFsHSSII
+        cHJ5BjsGVEkiCT49IDAGOwZUWwdJIgpjYWJpbgY7BlRJIg0+PSAwLjcuMAY7
+        BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiCjIuMi40BjsGVDsISSIJ
+        amF2YQY7BlQ7CVsUWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZU
+        WwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6
+        aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAu
+        NS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRv
+        cAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg09
+        IDAuOS4xMwY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4y
+        BjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXpl
+        BjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7
+        BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEki
+        Dn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVFsH
+        SSIYbG9nc3Rhc2gtY29yZS1ldmVudAY7BlRJIg1+PiAyLjIuNAY7BlR7CTsA
+        SSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiDDEuNS4yLjIGOwZUOwhJIglqYXZh
+        BjsGVDsJWxBbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjIuOAY7BlRbB0ki
+        EHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMbWluaXRhcgY7
+        BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRb
+        B0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiCWdlbXMGOwZUSSIN
+        fj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsH
+        SSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+
+        IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpj
+        YWJpbgY7BlRJIg1+PiAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7
+        BlQ7B0kiFDIuMi40LnNuYXBzaG90MQY7BlQ7CEkiCXJ1YnkGOwZUOwlbFFsH
+        SSIHb2oGOwZUSSIJPj0gMAY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+
+        IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAxLjEuNwY7BlRbB0ki
+        DG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0g
+        MC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIhJq
+        cnVieS1vcGVuc3NsBjsGVEkiDT0gMC45LjEzBjsGVFsHSSIUY29uY3VycmVu
+        dC1ydWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAu
+        OC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNs
+        YW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAu
+        MTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4G
+        OwZUSSINfj4gMC43LjAGOwZUWwdJIhhsb2dzdGFzaC1jb3JlLWV2ZW50BjsG
+        VEkiF34+IDIuMi40LnNuYXBzaG90MQY7BlR7CTsASSISbG9nc3Rhc2gtY29y
+        ZQY7BlQ7B0kiCjIuMy4wBjsGVDsISSIJamF2YQY7BlQ7CVsUWwdJIg5qcmph
+        Y2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJ
+        Ig1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZU
+        WwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJ
+        Igw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsH
+        SSISanJ1Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xMwY7BlRbB0kiFGNvbmN1
+        cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4yBjsGVFsHSSIJZ2VtcwY7BlRJIg1+
+        PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJ
+        IgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4g
+        MC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNh
+        YmluBjsGVEkiDX4+IDAuOC4wBjsGVFsHSSIdbG9nc3Rhc2gtY29yZS1ldmVu
+        dC1qYXZhBjsGVEkiDD0gMi4zLjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUG
+        OwZUOwdJIg4xLjUuMC5yYzIGOwZUOwhJIglqYXZhBjsGVDsJWxdbB0kiDmpy
+        amFja3NvbgY7BlRJIgk+PSAwBjsGVFsHSSIVanJ1YnktaHR0cGNsaWVudAY7
+        BlRJIgk+PSAwBjsGVFsHSSIWZmlsZS1kZXBlbmRlbmNpZXMGOwZUSSIMPSAw
+        LjEuNgY7BlRbB0kiDG1pbml0YXIGOwZUSSIJPj0gMAY7BlRbB0kiEG1hdmVu
+        LXRvb2xzBjsGVEkiDD0gMS4wLjcGOwZUWwdJIg9ydWJ5LW1hdmVuBjsGVEki
+        ED0gMy4xLjEuMC44BjsGVFsHSSIVamFyLWRlcGVuZGVuY2llcwY7BlRJIgw9
+        IDAuMS43BjsGVFsHSSIMc2luYXRyYQY7BlRJIgk+PSAwBjsGVFsHSSIJcmFj
+        awY7BlRJIgk+PSAwBjsGVFsHSSIPbWltZS10eXBlcwY7BlRJIgk+PSAwBjsG
+        VFsHSSIIZnR3BjsGVEkiDn4+IDAuMC40MAY7BlRbB0kiCWkxOG4GOwZUSSIM
+        PSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSINfj4gMS40LjAGOwZUWwdJ
+        Ig1maWxlc2l6ZQY7BlRJIgk+PSAwBjsGVFsHSSIKY2xhbXAGOwZUSSIJPj0g
+        MAY7BlRbB0kiCXN0dWQGOwZUSSIJPj0gMAY7BlRbB0kiCHByeQY7BlRJIgk+
+        PSAwBjsGVFsHSSIKY2FiaW4GOwZUSSINPj0gMC43LjAGOwZUewk7AEkiEmxv
+        Z3N0YXNoLWNvcmUGOwZUOwdJIhs1LjAuMC5hbHBoYTQuc25hcHNob3QzBjsG
+        VDsISSIJamF2YQY7BlQ7CVsYWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4z
+        LjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0ki
+        DHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEki
+        DX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIM
+        dHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIOanJtb25pdG9yBjsGVEki
+        DX4+IDAuNC4yBjsGVFsHSSIVY2hyb25pY19kdXJhdGlvbgY7BlRJIg09IDAu
+        MTAuNgY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjkuMTYGOwZU
+        WwdJIglwdW1hBjsGVEkiDH4+IDIuMTYGOwZUWwdJIgxzaW5hdHJhBjsGVEki
+        FT49IDEuNC42LCB+PiAxLjQGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZU
+        SSIMPSAxLjAuMAY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJ
+        Ig1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSIN
+        fj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0ki
+        CHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAw
+        LjguMAY7BlRbB0kiHWxvZ3N0YXNoLWNvcmUtZXZlbnQtamF2YQY7BlRJIh09
+        IDUuMC4wLmFscGhhNC5zbmFwc2hvdDMGOwZUewk7AEkiEmxvZ3N0YXNoLWNv
+        cmUGOwZUOwdJIhs1LjAuMC5hbHBoYTQuc25hcHNob3QxBjsGVDsISSIJamF2
+        YQY7BlQ7CVsYWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJ
+        IhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAG
+        OwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40
+        BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7
+        BlRJIgw8IDEuNS4wBjsGVFsHSSIOanJtb25pdG9yBjsGVEkiDX4+IDAuNC4y
+        BjsGVFsHSSIVY2hyb25pY19kdXJhdGlvbgY7BlRJIg09IDAuMTAuNgY7BlRb
+        B0kiEmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjkuMTYGOwZUWwdJIglwdW1h
+        BjsGVEkiDH4+IDIuMTYGOwZUWwdJIgxzaW5hdHJhBjsGVEkiFT49IDEuNC42
+        LCB+PiAxLjQGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAxLjAu
+        MAY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6
+        ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUG
+        OwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJ
+        Ig5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7BlRb
+        B0kiHWxvZ3N0YXNoLWNvcmUtZXZlbnQtamF2YQY7BlRJIh09IDUuMC4wLmFs
+        cGhhNC5zbmFwc2hvdDEGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJ
+        IhQyLjEuMi5zbmFwc2hvdDEGOwZUOwhJIglqYXZhBjsGVDsJWxNbB0kiDmpy
+        amFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9zYWZlBjsG
+        VEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAxLjEuNwY7
+        BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsG
+        VEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZU
+        WwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDT0gMC45LjEzBjsGVFsHSSIUY29u
+        Y3VycmVudC1ydWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJIglnZW1zBjsGVEki
+        DX4+IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRb
+        B0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+
+        PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIK
+        Y2FiaW4GOwZUSSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUG
+        OwZUOwdJIhcxLjUuMC5yYzIuc25hcHNob3QGOwZUOwhJIglqYXZhBjsGVDsJ
+        WxdbB0kiDmpyamFja3NvbgY7BlRJIgk+PSAwBjsGVFsHSSIVanJ1YnktaHR0
+        cGNsaWVudAY7BlRJIgk+PSAwBjsGVFsHSSIWZmlsZS1kZXBlbmRlbmNpZXMG
+        OwZUSSIMPSAwLjEuNgY7BlRbB0kiDG1pbml0YXIGOwZUSSIJPj0gMAY7BlRb
+        B0kiEG1hdmVuLXRvb2xzBjsGVEkiDD0gMS4wLjcGOwZUWwdJIg9ydWJ5LW1h
+        dmVuBjsGVEkiED0gMy4xLjEuMC44BjsGVFsHSSIVamFyLWRlcGVuZGVuY2ll
+        cwY7BlRJIgw9IDAuMS43BjsGVFsHSSIMc2luYXRyYQY7BlRJIgk+PSAwBjsG
+        VFsHSSIJcmFjawY7BlRJIgk+PSAwBjsGVFsHSSIPbWltZS10eXBlcwY7BlRJ
+        Igk+PSAwBjsGVFsHSSIIZnR3BjsGVEkiDn4+IDAuMC40MAY7BlRbB0kiCWkx
+        OG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSINfj4gMS40
+        LjAGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgk+PSAwBjsGVFsHSSIKY2xhbXAG
+        OwZUSSIJPj0gMAY7BlRbB0kiCXN0dWQGOwZUSSIJPj0gMAY7BlRbB0kiCHBy
+        eQY7BlRJIgk+PSAwBjsGVFsHSSIKY2FiaW4GOwZUSSINPj0gMC43LjAGOwZU
+        ewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIhQxLjUuMi5zbmFwc2hvdDEG
+        OwZUOwhJIglqYXZhBjsGVDsJWxBbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAw
+        LjIuOAY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsH
+        SSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIM
+        PSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0ki
+        CWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9
+        IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglz
+        dHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEw
+        LjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlR7CTsASSISbG9n
+        c3Rhc2gtY29yZQY7BlQ7B0kiEDIuMC4wLmJldGEyBjsGVDsISSIJamF2YQY7
+        BlQ7CVsSWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjUGOwZUWwdJIhB0
+        aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDG1pbml0YXIGOwZU
+        SSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJ
+        Igx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIhJqcnVieS1vcGVuc3Ns
+        BjsGVEkiDj49IDAuOS4xMQY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJ
+        Ig1+PiAwLjkuMQY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJ
+        Ig1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSIN
+        fj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0ki
+        CHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAw
+        LjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiGDEuNS4wLnJj
+        My5zbmFwc2hvdDIGOwZUOwhJIglqYXZhBjsGVDsJWxNbB0kiDmpyamFja3Nv
+        bgY7BlRJIgk+PSAwBjsGVFsHSSIWZmlsZS1kZXBlbmRlbmNpZXMGOwZUSSIM
+        PSAwLjEuNgY7BlRbB0kiDG1pbml0YXIGOwZUSSIJPj0gMAY7BlRbB0kiDHNp
+        bmF0cmEGOwZUSSIJPj0gMAY7BlRbB0kiCXJhY2sGOwZUSSIJPj0gMAY7BlRb
+        B0kiD21pbWUtdHlwZXMGOwZUSSIJPj0gMAY7BlRbB0kiCGZ0dwY7BlRJIg5+
+        PiAwLjAuNDAGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0
+        cmVldG9wBjsGVEkiDX4+IDEuNC4wBjsGVFsHSSINZmlsZXNpemUGOwZUSSIJ
+        Pj0gMAY7BlRbB0kiCmNsYW1wBjsGVEkiCT49IDAGOwZUWwdJIglzdHVkBjsG
+        VEkiCT49IDAGOwZUWwdJIghwcnkGOwZUSSIJPj0gMAY7BlRbB0kiCmNhYmlu
+        BjsGVEkiDT49IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsH
+        SSIRNS4wLjAuYWxwaGEzBjsGVDsISSIJamF2YQY7BlQ7CVsYWwdJIg5qcmph
+        Y2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJ
+        Ig1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZU
+        WwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJ
+        Igw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsH
+        SSIOanJtb25pdG9yBjsGVEkiDX4+IDAuNC4yBjsGVFsHSSIVY2hyb25pY19k
+        dXJhdGlvbgY7BlRJIg09IDAuMTAuNgY7BlRbB0kiEmpydWJ5LW9wZW5zc2wG
+        OwZUSSINPSAwLjkuMTYGOwZUWwdJIglwdW1hBjsGVEkiDH4+IDIuMTYGOwZU
+        WwdJIgxzaW5hdHJhBjsGVEkiFT49IDEuNC42LCB+PiAxLjQGOwZUWwdJIhRj
+        b25jdXJyZW50LXJ1YnkGOwZUSSIMPSAxLjAuMAY7BlRbB0kiCWdlbXMGOwZU
+        SSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsG
+        VFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEki
+        Dn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJ
+        IgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiHWxvZ3N0YXNoLWNvcmUt
+        ZXZlbnQtamF2YQY7BlRJIhM9IDUuMC4wLmFscGhhMwY7BlR7CTsASSISbG9n
+        c3Rhc2gtY29yZQY7BlQ7B0kiGzUuMC4wLmFscGhhMy5zbmFwc2hvdDIGOwZU
+        OwhJIglqYXZhBjsGVDsJWxhbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMu
+        NwY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIM
+        cnVieXppcAY7BlRJIg1+PiAxLjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSIN
+        fj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0
+        cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIhVqcnVieS1tb25pdG9yaW5n
+        BjsGVEkiDX4+IDAuMy4xBjsGVFsHSSIVY2hyb25pY19kdXJhdGlvbgY7BlRJ
+        Ig09IDAuMTAuNgY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjku
+        MTYGOwZUWwdJIglwdW1hBjsGVEkiFz49IDIuMTYuMCwgfj4gMi4xNgY7BlRb
+        B0kiDHNpbmF0cmEGOwZUSSIVPj0gMS40LjYsIH4+IDEuNAY7BlRbB0kiFGNv
+        bmN1cnJlbnQtcnVieQY7BlRJIgw9IDEuMC4wBjsGVFsHSSIJZ2VtcwY7BlRJ
+        Ig1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZU
+        WwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIO
+        fj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0ki
+        CmNhYmluBjsGVEkiDX4+IDAuOC4wBjsGVFsHSSIdbG9nc3Rhc2gtY29yZS1l
+        dmVudC1qYXZhBjsGVEkiIT0gNS4wLjAucHJlLmFscGhhMy5zbmFwc2hvdDIG
+        OwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIhgxLjUuMC5yYzMuc25h
+        cHNob3Q0BjsGVDsISSIJamF2YQY7BlQ7CVsTWwdJIg5qcmphY2tzb24GOwZU
+        SSIJPj0gMAY7BlRbB0kiFmZpbGUtZGVwZW5kZW5jaWVzBjsGVEkiDD0gMC4x
+        LjYGOwZUWwdJIgxtaW5pdGFyBjsGVEkiCT49IDAGOwZUWwdJIgxzaW5hdHJh
+        BjsGVEkiCT49IDAGOwZUWwdJIglyYWNrBjsGVEkiCT49IDAGOwZUWwdJIg9t
+        aW1lLXR5cGVzBjsGVEkiCT49IDAGOwZUWwdJIghmdHcGOwZUSSIOfj4gMC4w
+        LjQwBjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRv
+        cAY7BlRJIg1+PiAxLjQuMAY7BlRbB0kiDWZpbGVzaXplBjsGVEkiCT49IDAG
+        OwZUWwdJIgpjbGFtcAY7BlRJIgk+PSAwBjsGVFsHSSIJc3R1ZAY7BlRJIgk+
+        PSAwBjsGVFsHSSIIcHJ5BjsGVEkiCT49IDAGOwZUWwdJIgpjYWJpbgY7BlRJ
+        Ig0+PSAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiGDEu
+        NS4wLnJjNC5zbmFwc2hvdDEGOwZUOwhJIglqYXZhBjsGVDsJWw5bB0kiDmpy
+        amFja3NvbgY7BlRJIg1+PiAwLjIuOAY7BlRbB0kiDG1pbml0YXIGOwZUSSIN
+        fj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0
+        cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9
+        IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglz
+        dHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEw
+        LjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlR7CTsASSISbG9n
+        c3Rhc2gtY29yZQY7BlQ7B0kiFDEuNS4yLnNuYXBzaG90MgY7BlQ7CEkiCWph
+        dmEGOwZUOwlbEFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMi44BjsGVFsH
+        SSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxtaW5pdGFy
+        BjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsG
+        VFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIJZ2VtcwY7BlRJ
+        Ig1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZU
+        WwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIO
+        fj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0ki
+        CmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3Jl
+        BjsGVDsHSSIQMS41LjAuYmV0YTIGOwZUOwhJIglqYXZhBjsGVDsJWxdbB0ki
+        DmpyamFja3NvbgY7BlRJIgk+PSAwBjsGVFsHSSIVanJ1YnktaHR0cGNsaWVu
+        dAY7BlRJIgk+PSAwBjsGVFsHSSIWZmlsZS1kZXBlbmRlbmNpZXMGOwZUSSIM
+        PSAwLjEuNgY7BlRbB0kiDG1pbml0YXIGOwZUSSIJPj0gMAY7BlRbB0kiEG1h
+        dmVuLXRvb2xzBjsGVEkiDD0gMS4wLjcGOwZUWwdJIg9ydWJ5LW1hdmVuBjsG
+        VEkiED0gMy4xLjEuMC44BjsGVFsHSSIVamFyLWRlcGVuZGVuY2llcwY7BlRJ
+        Igw9IDAuMS43BjsGVFsHSSIMc2luYXRyYQY7BlRJIgk+PSAwBjsGVFsHSSIJ
+        cmFjawY7BlRJIgk+PSAwBjsGVFsHSSIPbWltZS10eXBlcwY7BlRJIgk+PSAw
+        BjsGVFsHSSIIZnR3BjsGVEkiDn4+IDAuMC40MAY7BlRbB0kiCWkxOG4GOwZU
+        SSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSINfj4gMS40LjAGOwZU
+        WwdJIg1maWxlc2l6ZQY7BlRJIgk+PSAwBjsGVFsHSSIKY2xhbXAGOwZUSSIJ
+        Pj0gMAY7BlRbB0kiCXN0dWQGOwZUSSIJPj0gMAY7BlRbB0kiCHByeQY7BlRJ
+        Igk+PSAwBjsGVFsHSSIKY2FiaW4GOwZUSSINPj0gMC43LjAGOwZUewk7AEki
+        EmxvZ3N0YXNoLWNvcmUGOwZUOwdJIhs1LjAuMC5hbHBoYTMuc25hcHNob3Q1
+        BjsGVDsISSIJamF2YQY7BlQ7CVsYWwdJIg5qcmphY2tzb24GOwZUSSINfj4g
+        MC4zLjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRb
+        B0kiDHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsG
+        VEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsH
+        SSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIVanJ1YnktbW9uaXRv
+        cmluZwY7BlRJIg1+PiAwLjMuMQY7BlRbB0kiFWNocm9uaWNfZHVyYXRpb24G
+        OwZUSSINPSAwLjEwLjYGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDT0g
+        MC45LjE2BjsGVFsHSSIJcHVtYQY7BlRJIhc+PSAyLjE2LjAsIH4+IDIuMTYG
+        OwZUWwdJIgxzaW5hdHJhBjsGVEkiFT49IDEuNC42LCB+PiAxLjQGOwZUWwdJ
+        IhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAxLjAuMAY7BlRbB0kiCWdlbXMG
+        OwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40
+        BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsG
+        VEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZU
+        WwdJIgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiHWxvZ3N0YXNoLWNv
+        cmUtZXZlbnQtamF2YQY7BlRJIh09IDUuMC4wLmFscGhhMy5zbmFwc2hvdDUG
+        OwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIhs1LjAuMC5hbHBoYTQu
+        c25hcHNob3QyBjsGVDsISSIJamF2YQY7BlQ7CVsYWwdJIg5qcmphY2tzb24G
+        OwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAw
+        LjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxt
+        aW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAu
+        Ni45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIOanJt
+        b25pdG9yBjsGVEkiDX4+IDAuNC4yBjsGVFsHSSIVY2hyb25pY19kdXJhdGlv
+        bgY7BlRJIg09IDAuMTAuNgY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZUSSIN
+        PSAwLjkuMTYGOwZUWwdJIglwdW1hBjsGVEkiDH4+IDIuMTYGOwZUWwdJIgxz
+        aW5hdHJhBjsGVEkiFT49IDEuNC42LCB+PiAxLjQGOwZUWwdJIhRjb25jdXJy
+        ZW50LXJ1YnkGOwZUSSIMPSAxLjAuMAY7BlRbB0kiCWdlbXMGOwZUSSINfj4g
+        MC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIK
+        Y2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAu
+        MC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJp
+        bgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiHWxvZ3N0YXNoLWNvcmUtZXZlbnQt
+        amF2YQY7BlRJIh09IDUuMC4wLmFscGhhNC5zbmFwc2hvdDIGOwZUewk7AEki
+        EmxvZ3N0YXNoLWNvcmUGOwZUOwdJIhQxLjUuNC5zbmFwc2hvdDIGOwZUOwhJ
+        IglqYXZhBjsGVDsJWxBbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjIuOQY7
+        BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMbWlu
+        aXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYu
+        OQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiCWdlbXMG
+        OwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40
+        BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsG
+        VEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZU
+        WwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gt
+        Y29yZQY7BlQ7B0kiFDEuNS4zLnNuYXBzaG90MQY7BlQ7CEkiCWphdmEGOwZU
+        OwlbEFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMi45BjsGVFsHSSIQdGhy
+        ZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxtaW5pdGFyBjsGVEki
+        DX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIM
+        dHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAw
+        LjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpj
+        bGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4w
+        LjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmlu
+        BjsGVEkiDX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsH
+        SSIYMS41LjAucmMzLnNuYXBzaG90NQY7BlQ7CEkiCWphdmEGOwZUOwlbE1sH
+        SSIOanJqYWNrc29uBjsGVEkiCT49IDAGOwZUWwdJIhZmaWxlLWRlcGVuZGVu
+        Y2llcwY7BlRJIgw9IDAuMS42BjsGVFsHSSIMbWluaXRhcgY7BlRJIgk+PSAw
+        BjsGVFsHSSIMc2luYXRyYQY7BlRJIgk+PSAwBjsGVFsHSSIJcmFjawY7BlRJ
+        Igk+PSAwBjsGVFsHSSIPbWltZS10eXBlcwY7BlRJIgk+PSAwBjsGVFsHSSII
+        ZnR3BjsGVEkiDn4+IDAuMC40MAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYu
+        OQY7BlRbB0kiDHRyZWV0b3AGOwZUSSINfj4gMS40LjAGOwZUWwdJIg1maWxl
+        c2l6ZQY7BlRJIgk+PSAwBjsGVFsHSSIKY2xhbXAGOwZUSSIJPj0gMAY7BlRb
+        B0kiCXN0dWQGOwZUSSIJPj0gMAY7BlRbB0kiCHByeQY7BlRJIgk+PSAwBjsG
+        VFsHSSIKY2FiaW4GOwZUSSINPj0gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNo
+        LWNvcmUGOwZUOwdJIhQyLjIuMC5zbmFwc2hvdDIGOwZUOwhJIglqYXZhBjsG
+        VDsJWxRbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRo
+        cmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJ
+        Ig1+PiAxLjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZU
+        WwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEki
+        DDwgMS41LjAGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDT0gMC45LjEz
+        BjsGVFsHSSIUY29uY3VycmVudC1ydWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJ
+        IglnZW1zBjsGVEkiDX4+IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIM
+        PSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJ
+        c3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4x
+        MC4xBjsGVFsHSSIKY2FiaW4GOwZUSSINfj4gMC43LjAGOwZUWwdJIhhsb2dz
+        dGFzaC1jb3JlLWV2ZW50BjsGVEkiF34+IDIuMi4wLnNuYXBzaG90MgY7BlR7
+        CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiGzUuMC4wLmFscGhhMS5zbmFw
+        c2hvdDEGOwZUOwhJIglqYXZhBjsGVDsJWxhbB0kiDmpyamFja3NvbgY7BlRJ
+        Ig1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41
+        BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAxLjEuNwY7BlRbB0kiDG1pbml0
+        YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkG
+        OwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIhVqcnVieS1t
+        b25pdG9yaW5nBjsGVEkiC34+IDAuMQY7BlRbB0kiFWNocm9uaWNfZHVyYXRp
+        b24GOwZUSSINPSAwLjEwLjYGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEki
+        DT0gMC45LjEzBjsGVFsHSSIJcHVtYQY7BlRJIhc+PSAyLjE2LjAsIH4+IDIu
+        MTYGOwZUWwdJIgxzaW5hdHJhBjsGVEkiFT49IDEuNC42LCB+PiAxLjQGOwZU
+        WwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAxLjAuMAY7BlRbB0kiCWdl
+        bXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAu
+        MC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVk
+        BjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEG
+        OwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiHWxvZ3N0YXNo
+        LWNvcmUtZXZlbnQtamF2YQY7BlRJIh5+PiA1LjAuMC5hbHBoYTEuc25hcHNo
+        b3QxBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIMMS41LjIuMQY7
+        BlQ7CEkiCWphdmEGOwZUOwlbEFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAu
+        Mi44BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJ
+        IgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9
+        IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIJ
+        Z2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0g
+        MC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0
+        dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAu
+        MQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dz
+        dGFzaC1jb3JlBjsGVDsHSSIKMi4xLjEGOwZUOwhJIglqYXZhBjsGVDsJWxNb
+        B0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9z
+        YWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAx
+        LjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglp
+        MThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41
+        LjAGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDj49IDAuOS4xMQY7BlRb
+        B0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4yBjsGVFsHSSIJZ2Vt
+        cwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4w
+        LjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQG
+        OwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7
+        BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFz
+        aC1jb3JlBjsGVDsHSSIKMS41LjMGOwZUOwhJIglqYXZhBjsGVDsJWxBbB0ki
+        DmpyamFja3NvbgY7BlRJIg1+PiAwLjIuOQY7BlRbB0kiEHRocmVhZF9zYWZl
+        BjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUu
+        NAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AG
+        OwZUSSIMPCAxLjUuMAY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZU
+        WwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZU
+        SSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRb
+        B0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+
+        PiAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiDjEuNS4w
+        LnJjNAY7BlQ7CEkiCWphdmEGOwZUOwlbDlsHSSIOanJqYWNrc29uBjsGVEki
+        DX4+IDAuMi44BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRb
+        B0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIM
+        PCAxLjUuMAY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJ
+        IgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4g
+        MC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNh
+        YmluBjsGVEkiDX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsG
+        VDsHSSIbNS4wLjAuYWxwaGExLnNuYXBzaG90MgY7BlQ7CEkiCWphdmEGOwZU
+        OwlbGFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhy
+        ZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxydWJ5emlwBjsGVEki
+        DX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRb
+        B0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIM
+        PCAxLjUuMAY7BlRbB0kiFWpydWJ5LW1vbml0b3JpbmcGOwZUSSINfj4gMC4z
+        LjEGOwZUWwdJIhVjaHJvbmljX2R1cmF0aW9uBjsGVEkiDT0gMC4xMC42BjsG
+        VFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xMwY7BlRbB0kiCXB1
+        bWEGOwZUSSIXPj0gMi4xNi4wLCB+PiAyLjE2BjsGVFsHSSIMc2luYXRyYQY7
+        BlRJIhU+PSAxLjQuNiwgfj4gMS40BjsGVFsHSSIUY29uY3VycmVudC1ydWJ5
+        BjsGVEkiDD0gMS4wLjAGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAuOC4zBjsG
+        VFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsG
+        VEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZU
+        WwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZUSSIN
+        fj4gMC44LjAGOwZUWwdJIh1sb2dzdGFzaC1jb3JlLWV2ZW50LWphdmEGOwZU
+        SSIefj4gNS4wLjAuYWxwaGExLnNuYXBzaG90MgY7BlR7CTsASSISbG9nc3Rh
+        c2gtY29yZQY7BlQ7B0kiCjIuMC4xBjsGVDsISSIJamF2YQY7BlQ7CVsSWwdJ
+        Ig5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRfc2Fm
+        ZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41
+        LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9w
+        BjsGVEkiDDwgMS41LjAGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDj49
+        IDAuOS4xMQY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4y
+        BjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXpl
+        BjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7
+        BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEki
+        Dn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVHsJ
+        OwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIUMS41LjEuc25hcHNob3QxBjsG
+        VDsISSIJamF2YQY7BlQ7CVsPWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4y
+        LjgGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0ki
+        DG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0g
+        MC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIg1m
+        aWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4g
+        MC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHBy
+        eQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcu
+        MAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiDjEuNS4wLnJjMQY7
+        BlQ7CEkiCWphdmEGOwZUOwlbF1sHSSIOanJqYWNrc29uBjsGVEkiCT49IDAG
+        OwZUWwdJIhVqcnVieS1odHRwY2xpZW50BjsGVEkiCT49IDAGOwZUWwdJIhZm
+        aWxlLWRlcGVuZGVuY2llcwY7BlRJIgw9IDAuMS42BjsGVFsHSSIMbWluaXRh
+        cgY7BlRJIgk+PSAwBjsGVFsHSSIQbWF2ZW4tdG9vbHMGOwZUSSIMPSAxLjAu
+        NwY7BlRbB0kiD3J1YnktbWF2ZW4GOwZUSSIQPSAzLjEuMS4wLjgGOwZUWwdJ
+        IhVqYXItZGVwZW5kZW5jaWVzBjsGVEkiDD0gMC4xLjcGOwZUWwdJIgxzaW5h
+        dHJhBjsGVEkiCT49IDAGOwZUWwdJIglyYWNrBjsGVEkiCT49IDAGOwZUWwdJ
+        Ig9taW1lLXR5cGVzBjsGVEkiCT49IDAGOwZUWwdJIghmdHcGOwZUSSIOfj4g
+        MC4wLjQwBjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJl
+        ZXRvcAY7BlRJIg1+PiAxLjQuMAY7BlRbB0kiDWZpbGVzaXplBjsGVEkiCT49
+        IDAGOwZUWwdJIgpjbGFtcAY7BlRJIgk+PSAwBjsGVFsHSSIJc3R1ZAY7BlRJ
+        Igk+PSAwBjsGVFsHSSIIcHJ5BjsGVEkiCT49IDAGOwZUWwdJIgpjYWJpbgY7
+        BlRJIg0+PSAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0ki
+        FDEuNS4zLnNuYXBzaG90MgY7BlQ7CEkiCWphdmEGOwZUOwlbEFsHSSIOanJq
+        YWNrc29uBjsGVEkiDX4+IDAuMi45BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZU
+        SSINfj4gMC4zLjUGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsG
+        VFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJ
+        Igw8IDEuNS4wBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0ki
+        DWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+
+        PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSII
+        cHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAu
+        Ny4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIUMi4zLjAuc25h
+        cHNob3Q0BjsGVDsISSIJamF2YQY7BlQ7CVsUWwdJIg5qcmphY2tzb24GOwZU
+        SSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMu
+        NQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5p
+        dGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45
+        BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1Ynkt
+        b3BlbnNzbAY7BlRJIg09IDAuOS4xMwY7BlRbB0kiFGNvbmN1cnJlbnQtcnVi
+        eQY7BlRJIgw9IDAuOS4yBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7
+        BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7
+        BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsG
+        VFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEki
+        DX4+IDAuOC4wBjsGVFsHSSIdbG9nc3Rhc2gtY29yZS1ldmVudC1qYXZhBjsG
+        VEkiFj0gMi4zLjAuc25hcHNob3Q0BjsGVHsJOwBJIhJsb2dzdGFzaC1jb3Jl
+        BjsGVDsHSSIKMi4zLjEGOwZUOwhJIglqYXZhBjsGVDsJWxRbB0kiDmpyamFj
+        a3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEki
+        DX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAxLjEuNwY7BlRb
+        B0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEki
+        DD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJ
+        IhJqcnVieS1vcGVuc3NsBjsGVEkiDT0gMC45LjEzBjsGVFsHSSIUY29uY3Vy
+        cmVudC1ydWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJIglnZW1zBjsGVEkiDX4+
+        IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0ki
+        CmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAw
+        LjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2Fi
+        aW4GOwZUSSINfj4gMC44LjAGOwZUWwdJIhhsb2dzdGFzaC1jb3JlLWV2ZW50
+        BjsGVEkiDD0gMi4zLjEGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJ
+        Ihs1LjAuMC5hbHBoYTMuc25hcHNob3Q2BjsGVDsISSIJamF2YQY7BlQ7CVsY
+        WwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRf
+        c2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4g
+        MS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJ
+        aTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEu
+        NS4wBjsGVFsHSSIOanJtb25pdG9yBjsGVEkiDX4+IDAuNC4yBjsGVFsHSSIV
+        Y2hyb25pY19kdXJhdGlvbgY7BlRJIg09IDAuMTAuNgY7BlRbB0kiEmpydWJ5
+        LW9wZW5zc2wGOwZUSSINPSAwLjkuMTYGOwZUWwdJIglwdW1hBjsGVEkiDX4+
+        IDMuNC4wBjsGVFsHSSIMc2luYXRyYQY7BlRJIhU+PSAxLjQuNiwgfj4gMS40
+        BjsGVFsHSSIUY29uY3VycmVudC1ydWJ5BjsGVEkiDD0gMS4wLjAGOwZUWwdJ
+        IglnZW1zBjsGVEkiDX4+IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIM
+        PSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJ
+        c3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4x
+        MC4xBjsGVFsHSSIKY2FiaW4GOwZUSSINfj4gMC44LjAGOwZUWwdJIh1sb2dz
+        dGFzaC1jb3JlLWV2ZW50LWphdmEGOwZUSSIdPSA1LjAuMC5hbHBoYTMuc25h
+        cHNob3Q2BjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIQMi4wLjAu
+        YmV0YTEGOwZUOwhJIglqYXZhBjsGVDsJWxBbB0kiDmpyamFja3NvbgY7BlRJ
+        Ig1+PiAwLjIuOQY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41
+        BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4G
+        OwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7
+        BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7
+        BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZU
+        WwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+
+        PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlR7CTsA
+        SSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIuMS4wLnNuYXBzaG90MQY7BlQ7
+        CEkiCWphdmEGOwZUOwlbElsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43
+        BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxt
+        aW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAu
+        Ni45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1
+        Ynktb3BlbnNzbAY7BlRJIg4+PSAwLjkuMTEGOwZUWwdJIhRjb25jdXJyZW50
+        LXJ1YnkGOwZUSSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44
+        LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xh
+        bXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4x
+        OQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7
+        BlRJIg1+PiAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0ki
+        GzUuMC4wLmFscGhhMi5zbmFwc2hvdDEGOwZUOwhJIglqYXZhBjsGVDsJWxhb
+        B0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9z
+        YWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAx
+        LjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglp
+        MThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41
+        LjAGOwZUWwdJIhVqcnVieS1tb25pdG9yaW5nBjsGVEkiDX4+IDAuMy4xBjsG
+        VFsHSSIVY2hyb25pY19kdXJhdGlvbgY7BlRJIg09IDAuMTAuNgY7BlRbB0ki
+        EmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjkuMTYGOwZUWwdJIglwdW1hBjsG
+        VEkiFz49IDIuMTYuMCwgfj4gMi4xNgY7BlRbB0kiDHNpbmF0cmEGOwZUSSIV
+        Pj0gMS40LjYsIH4+IDEuNAY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJ
+        Igw9IDEuMC4wBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0ki
+        DWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+
+        PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSII
+        cHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAu
+        OC4wBjsGVFsHSSIdbG9nc3Rhc2gtY29yZS1ldmVudC1qYXZhBjsGVEkiHn4+
+        IDUuMC4wLmFscGhhMi5zbmFwc2hvdDEGOwZUewk7AEkiEmxvZ3N0YXNoLWNv
+        cmUGOwZUOwdJIhAxLjUuMC5yYzEuMQY7BlQ7CEkiCWphdmEGOwZUOwlbF1sH
+        SSIOanJqYWNrc29uBjsGVEkiCT49IDAGOwZUWwdJIhVqcnVieS1odHRwY2xp
+        ZW50BjsGVEkiCT49IDAGOwZUWwdJIhZmaWxlLWRlcGVuZGVuY2llcwY7BlRJ
+        Igw9IDAuMS42BjsGVFsHSSIMbWluaXRhcgY7BlRJIgk+PSAwBjsGVFsHSSIQ
+        bWF2ZW4tdG9vbHMGOwZUSSIMPSAxLjAuNwY7BlRbB0kiD3J1YnktbWF2ZW4G
+        OwZUSSIQPSAzLjEuMS4wLjgGOwZUWwdJIhVqYXItZGVwZW5kZW5jaWVzBjsG
+        VEkiDD0gMC4xLjcGOwZUWwdJIgxzaW5hdHJhBjsGVEkiCT49IDAGOwZUWwdJ
+        IglyYWNrBjsGVEkiCT49IDAGOwZUWwdJIg9taW1lLXR5cGVzBjsGVEkiCT49
+        IDAGOwZUWwdJIghmdHcGOwZUSSIOfj4gMC4wLjQwBjsGVFsHSSIJaTE4bgY7
+        BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIg1+PiAxLjQuMAY7
+        BlRbB0kiDWZpbGVzaXplBjsGVEkiCT49IDAGOwZUWwdJIgpjbGFtcAY7BlRJ
+        Igk+PSAwBjsGVFsHSSIJc3R1ZAY7BlRJIgk+PSAwBjsGVFsHSSIIcHJ5BjsG
+        VEkiCT49IDAGOwZUWwdJIgpjYWJpbgY7BlRJIg0+PSAwLjcuMAY7BlR7CTsA
+        SSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIuMC4xLnNuYXBzaG90MQY7BlQ7
+        CEkiCWphdmEGOwZUOwlbElsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43
+        BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxt
+        aW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAu
+        Ni45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1
+        Ynktb3BlbnNzbAY7BlRJIg4+PSAwLjkuMTEGOwZUWwdJIhRjb25jdXJyZW50
+        LXJ1YnkGOwZUSSINfj4gMC45LjEGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAu
+        OC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNs
+        YW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAu
+        MTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4G
+        OwZUSSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJ
+        Ihs1LjAuMC5hbHBoYTMuc25hcHNob3Q3BjsGVDsISSIJamF2YQY7BlQ7CVsY
+        WwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRf
+        c2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4g
+        MS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJ
+        aTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEu
+        NS4wBjsGVFsHSSIOanJtb25pdG9yBjsGVEkiDX4+IDAuNC4yBjsGVFsHSSIV
+        Y2hyb25pY19kdXJhdGlvbgY7BlRJIg09IDAuMTAuNgY7BlRbB0kiEmpydWJ5
+        LW9wZW5zc2wGOwZUSSINPSAwLjkuMTYGOwZUWwdJIglwdW1hBjsGVEkiDH4+
+        IDIuMTYGOwZUWwdJIgxzaW5hdHJhBjsGVEkiFT49IDEuNC42LCB+PiAxLjQG
+        OwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAxLjAuMAY7BlRbB0ki
+        CWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9
+        IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglz
+        dHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEw
+        LjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiHWxvZ3N0
+        YXNoLWNvcmUtZXZlbnQtamF2YQY7BlRJIh09IDUuMC4wLmFscGhhMy5zbmFw
+        c2hvdDcGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIgoyLjAuMAY7
+        BlQ7CEkiCWphdmEGOwZUOwlbElsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAu
+        My42BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJ
+        IgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9
+        IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIS
+        anJ1Ynktb3BlbnNzbAY7BlRJIg4+PSAwLjkuMTEGOwZUWwdJIhRjb25jdXJy
+        ZW50LXJ1YnkGOwZUSSINfj4gMC45LjEGOwZUWwdJIglnZW1zBjsGVEkiDX4+
+        IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0ki
+        CmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAw
+        LjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2Fi
+        aW4GOwZUSSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZU
+        OwdJIgoyLjEuMAY7BlQ7CEkiCWphdmEGOwZUOwlbE1sHSSIOanJqYWNrc29u
+        BjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4g
+        MC4zLjUGOwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIM
+        bWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAw
+        LjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpy
+        dWJ5LW9wZW5zc2wGOwZUSSIOPj0gMC45LjExBjsGVFsHSSIUY29uY3VycmVu
+        dC1ydWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAu
+        OC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNs
+        YW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAu
+        MTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4G
+        OwZUSSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJ
+        IgoyLjEuMwY7BlQ7CEkiCWphdmEGOwZUOwlbE1sHSSIOanJqYWNrc29uBjsG
+        VEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4z
+        LjUGOwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWlu
+        aXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYu
+        OQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5
+        LW9wZW5zc2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1
+        YnkGOwZUSSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMG
+        OwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAG
+        OwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7
+        BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJ
+        Ig1+PiAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIu
+        My4xLnNuYXBzaG90MQY7BlQ7CEkiCWphdmEGOwZUOwlbFFsHSSIOanJqYWNr
+        c29uBjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSIN
+        fj4gMC4zLjUGOwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsH
+        SSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIM
+        PSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0ki
+        EmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJy
+        ZW50LXJ1YnkGOwZUSSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4g
+        MC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIK
+        Y2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAu
+        MC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJp
+        bgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiGGxvZ3N0YXNoLWNvcmUtZXZlbnQG
+        OwZUSSIWPSAyLjMuMS5zbmFwc2hvdDEGOwZUewk7AEkiEmxvZ3N0YXNoLWNv
+        cmUGOwZUOwdJIgoxLjUuNgY7BlQ7CEkiCWphdmEGOwZUOwlbElsHSSIOanJq
+        YWNrc29uBjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZU
+        SSINfj4gMC4zLjUGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsG
+        VFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJ
+        Igw8IDEuNS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg4+PSAwLjku
+        MTEGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAwLjkuMgY7BlRb
+        B0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJ
+        Igw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJ
+        IglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAw
+        LjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlR7CTsASSIS
+        bG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIuMi4zLnNuYXBzaG90MgY7BlQ7CEki
+        CWphdmEGOwZUOwlbFFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43BjsG
+        VFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxydWJ5
+        emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAw
+        LjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0
+        b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZUSSIN
+        PSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAwLjku
+        MgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6
+        ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUG
+        OwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJ
+        Ig5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlRb
+        B0kiGGxvZ3N0YXNoLWNvcmUtZXZlbnQGOwZUSSIXfj4gMi4yLjMuc25hcHNo
+        b3QyBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIUMS41LjAuc25h
+        cHNob3QxBjsGVDsISSIJamF2YQY7BlQ7CVsOWwdJIg5qcmphY2tzb24GOwZU
+        SSINfj4gMC4yLjgGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40BjsG
+        VFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7BlRJ
+        Igw8IDEuNS4wBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRb
+        B0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+
+        PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIK
+        Y2FiaW4GOwZUSSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUG
+        OwZUOwdJIhQxLjUuNC5zbmFwc2hvdDMGOwZUOwhJIglqYXZhBjsGVDsJWxBb
+        B0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjIuOQY7BlRbB0kiEHRocmVhZF9z
+        YWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAw
+        LjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0
+        b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMG
+        OwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAG
+        OwZUSSINfj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7
+        BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJ
+        Ig1+PiAwLjcuMAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiCjEu
+        NS4yBjsGVDsISSIJamF2YQY7BlQ7CVsQWwdJIg5qcmphY2tzb24GOwZUSSIN
+        fj4gMC4yLjgGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7
+        BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsG
+        VEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZU
+        WwdJIglnZW1zBjsGVEkiDX4+IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZU
+        SSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsH
+        SSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4g
+        MC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZUSSINfj4gMC43LjAGOwZUewk7AEki
+        EmxvZ3N0YXNoLWNvcmUGOwZUOwdJIgoyLjIuMwY7BlQ7CEkiCWphdmEGOwZU
+        OwlbFFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43BjsGVFsHSSIQdGhy
+        ZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxydWJ5emlwBjsGVEki
+        DX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRb
+        B0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIM
+        PCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZUSSINPSAwLjkuMTMG
+        OwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAwLjkuMgY7BlRbB0ki
+        CWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJIgw9
+        IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJIglz
+        dHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAwLjEw
+        LjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7BlRbB0kiGGxvZ3N0
+        YXNoLWNvcmUtZXZlbnQGOwZUSSINfj4gMi4yLjMGOwZUewk7AEkiEmxvZ3N0
+        YXNoLWNvcmUGOwZUOwdJIhQyLjEuMC5zbmFwc2hvdDQGOwZUOwhJIglqYXZh
+        BjsGVDsJWxNbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0ki
+        EHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7
+        BlRJIg1+PiAxLjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQG
+        OwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsG
+        VEkiDDwgMS41LjAGOwZUWwdJIhJqcnVieS1vcGVuc3NsBjsGVEkiDj49IDAu
+        OS4xMQY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4yBjsG
+        VFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsG
+        VEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRb
+        B0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+
+        IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVHsJOwBJ
+        IhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIUMi4zLjIuc25hcHNob3QxBjsGVDsI
+        SSIJamF2YQY7BlQ7CVsUWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcG
+        OwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1
+        Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+
+        IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJl
+        ZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJ
+        Ig09IDAuOS4xMwY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAu
+        OS4yBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVz
+        aXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYu
+        NQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsG
+        VEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuOC4wBjsG
+        VFsHSSIYbG9nc3Rhc2gtY29yZS1ldmVudAY7BlRJIhY9IDIuMy4yLnNuYXBz
+        aG90MQY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIuMS4wLnNu
+        YXBzaG90MwY7BlQ7CEkiCWphdmEGOwZUOwlbE1sHSSIOanJqYWNrc29uBjsG
+        VEkiDX4+IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4z
+        LjUGOwZUWwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWlu
+        aXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYu
+        OQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5
+        LW9wZW5zc2wGOwZUSSIOPj0gMC45LjExBjsGVFsHSSIUY29uY3VycmVudC1y
+        dWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAuOC4z
+        BjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1w
+        BjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkG
+        OwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZU
+        SSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIg4y
+        LjAuMC5yYzEGOwZUOwhJIglqYXZhBjsGVDsJWxJbB0kiDmpyamFja3NvbgY7
+        BlRJIg1+PiAwLjMuNgY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAu
+        My41BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkx
+        OG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUu
+        MAY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZUSSIOPj0gMC45LjExBjsGVFsH
+        SSIUY29uY3VycmVudC1ydWJ5BjsGVEkiDX4+IDAuOS4xBjsGVFsHSSIJZ2Vt
+        cwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4w
+        LjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQG
+        OwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7
+        BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFz
+        aC1jb3JlBjsGVDsHSSIbNS4wLjAuYWxwaGEzLnNuYXBzaG90NAY7BlQ7CEki
+        CWphdmEGOwZUOwlbGFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43BjsG
+        VFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxydWJ5
+        emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+PiAw
+        LjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRyZWV0
+        b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiFWpydWJ5LW1vbml0b3JpbmcGOwZU
+        SSINfj4gMC4zLjEGOwZUWwdJIhVjaHJvbmljX2R1cmF0aW9uBjsGVEkiDT0g
+        MC4xMC42BjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xNgY7
+        BlRbB0kiCXB1bWEGOwZUSSIXPj0gMi4xNi4wLCB+PiAyLjE2BjsGVFsHSSIM
+        c2luYXRyYQY7BlRJIhU+PSAxLjQuNiwgfj4gMS40BjsGVFsHSSIUY29uY3Vy
+        cmVudC1ydWJ5BjsGVEkiDD0gMS4wLjAGOwZUWwdJIglnZW1zBjsGVEkiDX4+
+        IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0ki
+        CmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAw
+        LjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2Fi
+        aW4GOwZUSSINfj4gMC44LjAGOwZUWwdJIh1sb2dzdGFzaC1jb3JlLWV2ZW50
+        LWphdmEGOwZUSSIdPSA1LjAuMC5hbHBoYTMuc25hcHNob3Q0BjsGVHsJOwBJ
+        IhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIUMS41LjQuc25hcHNob3QxBjsGVDsI
+        SSIJamF2YQY7BlQ7CVsQWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4yLjkG
+        OwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDG1p
+        bml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42
+        LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIglnZW1z
+        BjsGVEkiDX4+IDAuOC4zBjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAu
+        NAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7
+        BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsG
+        VFsHSSIKY2FiaW4GOwZUSSINfj4gMC43LjAGOwZUewk7AEkiEmxvZ3N0YXNo
+        LWNvcmUGOwZUOwdJIgoxLjUuNAY7BlQ7CEkiCWphdmEGOwZUOwlbEFsHSSIO
+        anJqYWNrc29uBjsGVEkiDX4+IDAuMi45BjsGVFsHSSIQdGhyZWFkX3NhZmUG
+        OwZUSSINfj4gMC4zLjUGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAuNS40
+        BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRvcAY7
+        BlRJIgw8IDEuNS4wBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRb
+        B0kiDWZpbGVzaXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJ
+        Ig1+PiAwLjYuNQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsH
+        SSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+
+        IDAuNy4wBjsGVHsJOwBJIhJsb2dzdGFzaC1jb3JlBjsGVDsHSSIKMi4zLjMG
+        OwZUOwhJIglqYXZhBjsGVDsJWxRbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAw
+        LjMuNwY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsH
+        SSIMcnVieXppcAY7BlRJIg1+PiAxLjEuNwY7BlRbB0kiDG1pbml0YXIGOwZU
+        SSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJ
+        Igx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIhJqcnVieS1vcGVuc3Ns
+        BjsGVEkiDT0gMC45LjEzBjsGVFsHSSIUY29uY3VycmVudC1ydWJ5BjsGVEki
+        DD0gMC45LjIGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAuOC4zBjsGVFsHSSIN
+        ZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1wBjsGVEkiDX4+
+        IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkGOwZUWwdJIghw
+        cnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZUSSINfj4gMC44
+        LjAGOwZUWwdJIhhsb2dzdGFzaC1jb3JlLWV2ZW50BjsGVEkiDD0gMi4zLjMG
+        OwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIhs1LjAuMC5hbHBoYTMu
+        c25hcHNob3QxBjsGVDsISSIJamF2YQY7BlQ7CVsYWwdJIg5qcmphY2tzb24G
+        OwZUSSINfj4gMC4zLjcGOwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAw
+        LjMuNQY7BlRbB0kiDHJ1Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxt
+        aW5pdGFyBjsGVEkiDX4+IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAu
+        Ni45BjsGVFsHSSIMdHJlZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSIVanJ1
+        YnktbW9uaXRvcmluZwY7BlRJIg1+PiAwLjMuMQY7BlRbB0kiFWNocm9uaWNf
+        ZHVyYXRpb24GOwZUSSINPSAwLjEwLjYGOwZUWwdJIhJqcnVieS1vcGVuc3Ns
+        BjsGVEkiDT0gMC45LjE2BjsGVFsHSSIJcHVtYQY7BlRJIhc+PSAyLjE2LjAs
+        IH4+IDIuMTYGOwZUWwdJIgxzaW5hdHJhBjsGVEkiFT49IDEuNC42LCB+PiAx
+        LjQGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAxLjAuMAY7BlRb
+        B0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxlc2l6ZQY7BlRJ
+        Igw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42LjUGOwZUWwdJ
+        IglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7BlRJIg5+PiAw
+        LjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7BlRbB0kiHWxv
+        Z3N0YXNoLWNvcmUtZXZlbnQtamF2YQY7BlRJIh09IDUuMC4wLmFscGhhMy5z
+        bmFwc2hvdDEGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIgoyLjIu
+        MQY7BlQ7CEkiCWphdmEGOwZUOwlbFFsHSSIOanJqYWNrc29uBjsGVEkiDX4+
+        IDAuMy43BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZU
+        WwdJIgxydWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7
+        BlRJIg1+PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRb
+        B0kiDHRyZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9wZW5z
+        c2wGOwZUSSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZU
+        SSIMPSAwLjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJ
+        Ig1maWxlc2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSIN
+        fj4gMC42LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0ki
+        CHByeQY7BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAw
+        LjcuMAY7BlRbB0kiGGxvZ3N0YXNoLWNvcmUtZXZlbnQGOwZUSSINfj4gMi4y
+        LjEGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIgoyLjEuMgY7BlQ7
+        CEkiCWphdmEGOwZUOwlbE1sHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43
+        BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxy
+        dWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+
+        PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRy
+        ZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZU
+        SSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAw
+        LjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxl
+        c2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42
+        LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7
+        BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjcuMAY7
+        BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiCjIuMi4wBjsGVDsISSIJ
+        amF2YQY7BlQ7CVsUWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcGOwZU
+        WwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1Ynl6
+        aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+IDAu
+        NS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJlZXRv
+        cAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJIg09
+        IDAuOS4xMwY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAuOS4y
+        BjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXpl
+        BjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7
+        BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEki
+        Dn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuNy4wBjsGVFsH
+        SSIYbG9nc3Rhc2gtY29yZS1ldmVudAY7BlRJIg1+PiAyLjIuMAY7BlR7CTsA
+        SSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiFDIuMy4zLnNuYXBzaG90MgY7BlQ7
+        CEkiCWphdmEGOwZUOwlbFFsHSSIOanJqYWNrc29uBjsGVEkiDX4+IDAuMy43
+        BjsGVFsHSSIQdGhyZWFkX3NhZmUGOwZUSSINfj4gMC4zLjUGOwZUWwdJIgxy
+        dWJ5emlwBjsGVEkiDX4+IDEuMS43BjsGVFsHSSIMbWluaXRhcgY7BlRJIg1+
+        PiAwLjUuNAY7BlRbB0kiCWkxOG4GOwZUSSIMPSAwLjYuOQY7BlRbB0kiDHRy
+        ZWV0b3AGOwZUSSIMPCAxLjUuMAY7BlRbB0kiEmpydWJ5LW9wZW5zc2wGOwZU
+        SSINPSAwLjkuMTMGOwZUWwdJIhRjb25jdXJyZW50LXJ1YnkGOwZUSSIMPSAw
+        LjkuMgY7BlRbB0kiCWdlbXMGOwZUSSINfj4gMC44LjMGOwZUWwdJIg1maWxl
+        c2l6ZQY7BlRJIgw9IDAuMC40BjsGVFsHSSIKY2xhbXAGOwZUSSINfj4gMC42
+        LjUGOwZUWwdJIglzdHVkBjsGVEkiDn4+IDAuMC4xOQY7BlRbB0kiCHByeQY7
+        BlRJIg5+PiAwLjEwLjEGOwZUWwdJIgpjYWJpbgY7BlRJIg1+PiAwLjguMAY7
+        BlRbB0kiGGxvZ3N0YXNoLWNvcmUtZXZlbnQGOwZUSSIWPSAyLjMuMy5zbmFw
+        c2hvdDIGOwZUewk7AEkiEmxvZ3N0YXNoLWNvcmUGOwZUOwdJIhQyLjIuMS5z
+        bmFwc2hvdDEGOwZUOwhJIglqYXZhBjsGVDsJWxRbB0kiDmpyamFja3NvbgY7
+        BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9zYWZlBjsGVEkiDX4+IDAu
+        My41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAxLjEuNwY7BlRbB0kiDG1p
+        bml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThuBjsGVEkiDD0gMC42
+        LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAGOwZUWwdJIhJqcnVi
+        eS1vcGVuc3NsBjsGVEkiDT0gMC45LjEzBjsGVFsHSSIUY29uY3VycmVudC1y
+        dWJ5BjsGVEkiDD0gMC45LjIGOwZUWwdJIglnZW1zBjsGVEkiDX4+IDAuOC4z
+        BjsGVFsHSSINZmlsZXNpemUGOwZUSSIMPSAwLjAuNAY7BlRbB0kiCmNsYW1w
+        BjsGVEkiDX4+IDAuNi41BjsGVFsHSSIJc3R1ZAY7BlRJIg5+PiAwLjAuMTkG
+        OwZUWwdJIghwcnkGOwZUSSIOfj4gMC4xMC4xBjsGVFsHSSIKY2FiaW4GOwZU
+        SSINfj4gMC43LjAGOwZUWwdJIhhsb2dzdGFzaC1jb3JlLWV2ZW50BjsGVEki
+        F34+IDIuMi4xLnNuYXBzaG90MQY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7
+        BlQ7B0kiGzUuMC4wLmFscGhhNS5zbmFwc2hvdDEGOwZUOwhJIglqYXZhBjsG
+        VDsJWxhbB0kiDmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRo
+        cmVhZF9zYWZlBjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJ
+        Ig1+PiAxLjEuNwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZU
+        WwdJIglpMThuBjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEki
+        DDwgMS41LjAGOwZUWwdJIg5qcm1vbml0b3IGOwZUSSINfj4gMC40LjIGOwZU
+        WwdJIhVjaHJvbmljX2R1cmF0aW9uBjsGVEkiDT0gMC4xMC42BjsGVFsHSSIS
+        anJ1Ynktb3BlbnNzbAY7BlRJIg09IDAuOS4xNgY7BlRbB0kiCXB1bWEGOwZU
+        SSIMfj4gMi4xNgY7BlRbB0kiDHNpbmF0cmEGOwZUSSIVPj0gMS40LjYsIH4+
+        IDEuNAY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDEuMC4wBjsG
+        VFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsG
+        VEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRb
+        B0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+
+        IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuOC4wBjsGVFsHSSId
+        bG9nc3Rhc2gtY29yZS1ldmVudC1qYXZhBjsGVEkiHT0gNS4wLjAuYWxwaGE1
+        LnNuYXBzaG90MQY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiGzUu
+        MC4wLmFscGhhMy5zbmFwc2hvdDgGOwZUOwhJIglqYXZhBjsGVDsJWxhbB0ki
+        DmpyamFja3NvbgY7BlRJIg1+PiAwLjMuNwY7BlRbB0kiEHRocmVhZF9zYWZl
+        BjsGVEkiDX4+IDAuMy41BjsGVFsHSSIMcnVieXppcAY7BlRJIg1+PiAxLjEu
+        NwY7BlRbB0kiDG1pbml0YXIGOwZUSSINfj4gMC41LjQGOwZUWwdJIglpMThu
+        BjsGVEkiDD0gMC42LjkGOwZUWwdJIgx0cmVldG9wBjsGVEkiDDwgMS41LjAG
+        OwZUWwdJIg5qcm1vbml0b3IGOwZUSSINfj4gMC40LjIGOwZUWwdJIhVjaHJv
+        bmljX2R1cmF0aW9uBjsGVEkiDT0gMC4xMC42BjsGVFsHSSISanJ1Ynktb3Bl
+        bnNzbAY7BlRJIg09IDAuOS4xNgY7BlRbB0kiCXB1bWEGOwZUSSIMfj4gMi4x
+        NgY7BlRbB0kiDHNpbmF0cmEGOwZUSSIVPj0gMS40LjYsIH4+IDEuNAY7BlRb
+        B0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDEuMC4wBjsGVFsHSSIJZ2Vt
+        cwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVzaXplBjsGVEkiDD0gMC4w
+        LjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYuNQY7BlRbB0kiCXN0dWQG
+        OwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsGVEkiDn4+IDAuMTAuMQY7
+        BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuOC4wBjsGVFsHSSIdbG9nc3Rhc2gt
+        Y29yZS1ldmVudC1qYXZhBjsGVEkiHT0gNS4wLjAuYWxwaGEzLnNuYXBzaG90
+        OAY7BlR7CTsASSISbG9nc3Rhc2gtY29yZQY7BlQ7B0kiCjIuMy40BjsGVDsI
+        SSIJamF2YQY7BlQ7CVsUWwdJIg5qcmphY2tzb24GOwZUSSINfj4gMC4zLjcG
+        OwZUWwdJIhB0aHJlYWRfc2FmZQY7BlRJIg1+PiAwLjMuNQY7BlRbB0kiDHJ1
+        Ynl6aXAGOwZUSSINfj4gMS4xLjcGOwZUWwdJIgxtaW5pdGFyBjsGVEkiDX4+
+        IDAuNS40BjsGVFsHSSIJaTE4bgY7BlRJIgw9IDAuNi45BjsGVFsHSSIMdHJl
+        ZXRvcAY7BlRJIgw8IDEuNS4wBjsGVFsHSSISanJ1Ynktb3BlbnNzbAY7BlRJ
+        Ig09IDAuOS4xMwY7BlRbB0kiFGNvbmN1cnJlbnQtcnVieQY7BlRJIgw9IDAu
+        OS4yBjsGVFsHSSIJZ2VtcwY7BlRJIg1+PiAwLjguMwY7BlRbB0kiDWZpbGVz
+        aXplBjsGVEkiDD0gMC4wLjQGOwZUWwdJIgpjbGFtcAY7BlRJIg1+PiAwLjYu
+        NQY7BlRbB0kiCXN0dWQGOwZUSSIOfj4gMC4wLjE5BjsGVFsHSSIIcHJ5BjsG
+        VEkiDn4+IDAuMTAuMQY7BlRbB0kiCmNhYmluBjsGVEkiDX4+IDAuOC4wBjsG
+        VFsHSSIYbG9nc3Rhc2gtY29yZS1ldmVudAY7BlRJIgw9IDIuMy40BjsGVA==
+    http_version:
+  recorded_at: Tue, 16 Aug 2016 13:07:20 GMT
+recorded_with: VCR 3.0.3
diff --git a/tools/logstash-docgen/spec/logstash/docgen/dependency_lookup_spec.rb b/tools/logstash-docgen/spec/logstash/docgen/dependency_lookup_spec.rb
new file mode 100644
index 00000000000..516e1196d27
--- /dev/null
+++ b/tools/logstash-docgen/spec/logstash/docgen/dependency_lookup_spec.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+#
+require "logstash/docgen/dependency_lookup"
+require "spec_helper"
+
+describe LogStash::Docgen::DependencyLookup do
+  let(:gemspec_file) { ::File.join(::File.dirname(__FILE__), "..", "..", "fixtures", "logstash-filter-dummy.gemspec")}
+  let(:gemspec) { Gem::Specification.load(gemspec_file) }
+  subject { LogStash::Docgen::DependencyLookup }
+
+  it "doesn't include pre-release" do
+    VCR.use_cassette("logstash-core") do
+      expect(subject.supported_logstash(gemspec)).not_to include(/snapshot|beta|pre/)
+    end
+  end
+
+  it "includes only top level logstash version" do
+    VCR.use_cassette("logstash-core") do
+      expect(subject.supported_logstash(gemspec)).to include("5.0.0")
+    end
+  end
+
+  it "includes any supported versions" do
+    VCR.use_cassette("logstash-core") do
+      expect(subject.supported_logstash(gemspec)).to include("5.0.0", "2.3.4", "2.3.2")
+    end
+  end
+
+  it "doesn't include duplicates" do
+    VCR.use_cassette("logstash-core") do
+      versions =  subject.supported_logstash(gemspec)
+      expect { versions.size }.not_to change { versions.uniq }
+    end
+  end
+end
diff --git a/tools/logstash-docgen/spec/logstash/docgen/task_runner_spec.rb b/tools/logstash-docgen/spec/logstash/docgen/task_runner_spec.rb
new file mode 100644
index 00000000000..c28d71c7a19
--- /dev/null
+++ b/tools/logstash-docgen/spec/logstash/docgen/task_runner_spec.rb
@@ -0,0 +1,116 @@
+# encoding: utf-8
+require "logstash/docgen/task_runner"
+
+describe LogStash::Docgen::TaskRunner::Status do
+  subject { LogStash::Docgen::TaskRunner::Status }
+
+  context "#success?" do
+    let(:name) { :making_stuff }
+    let(:error) { OpenStruct.new(:message => "Something bad, OOPS!") }
+
+    it "returns true when no errors was passed to the class" do
+      expect(subject.new(name).success?).to be_truthy
+    end
+
+    it "returns false when an errors was passed to the class" do
+      expect(subject.new(name, error).success?).to be_falsey
+    end
+
+    it "allows access to the name" do
+      expect(subject.new(name).name).to eq(name)
+    end
+
+    it "allows access to the error" do
+      expect(subject.new(name, error).error).to eq(error)
+    end
+  end
+end
+
+describe LogStash::Docgen::TaskRunner do
+  subject { LogStash::Docgen::TaskRunner.new }
+  let(:name) { :making_stuff }
+
+  context "an execution without errors" do
+    let(:job_with_no_errors) do
+      lambda do
+        1+1
+      end
+    end
+
+    it "outputs the name and the status of the task" do
+      output = capture do
+        subject.run(name) do
+          job_with_no_errors.call
+        end
+      end
+
+      expect(output).to match(/#{name} > \e\[32mSUCCESS\e\[0m/)
+    end
+
+    it "returns no failures" do
+      subject.run(name) do
+        job_with_no_errors.call
+      end
+
+      expect(subject.failures?).to be_falsey
+    end
+
+    it "doesn't output anything to standard output" do
+      subject.run(name) do
+        job_with_no_errors.call
+      end
+
+      output = capture do
+        subject.report_failures
+      end
+
+      expect(output).to match("")
+    end
+  end
+
+  context "an execution with errors" do
+    let(:job_with_with_errors) do
+      lambda do
+        1/0
+      end
+    end
+
+    it "outputs the name and the status of the task" do
+      output = capture do
+        subject.run(name) do
+          job_with_with_errors.call
+        end
+      end
+
+      expect(output).to match(/#{name} > \e\[31mFAIL\e\[0m/)
+    end
+
+    it "returns failures" do
+      subject.run(name) do
+        job_with_with_errors.call
+      end
+
+      expect(subject.failures?).to be_truthy
+    end
+
+    it "outputs errors to standard output" do
+      subject.run(name) do
+        job_with_no_errors.call
+      end
+
+      subject.run(:not_working_bob) do
+        job_with_no_errors.call
+      end
+
+      output = capture do
+        subject.report_failures
+      end
+
+      expect(output).to match(/FAILURE: #{name}/)
+      expect(output).to match(/Exception:/)
+
+
+      expect(output).to match(/FAILURE: not_working_bob/)
+    end
+  end
+end
diff --git a/tools/logstash-docgen/spec/logstash/docgen/util_spec.rb b/tools/logstash-docgen/spec/logstash/docgen/util_spec.rb
new file mode 100644
index 00000000000..b8164470f12
--- /dev/null
+++ b/tools/logstash-docgen/spec/logstash/docgen/util_spec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "logstash/docgen/util"
+require "spec_helper"
+
+describe LogStash::Docgen::Util do
+  subject { LogStash::Docgen::Util }
+
+  context "time_execution" do
+    it "prints the execution time to stdout" do
+
+      output = capture do
+        subject.time_execution do
+          sleep(0.1)
+        end
+      end
+
+      expect(output).to match(/Execution took: \d(\.\d+)?s/)
+    end
+
+    it "returns the value of the block" do
+      value = subject.time_execution do
+        1 + 2
+      end
+
+      expect(value).to eq(3)
+    end
+  end
+
+  it "returns a red string" do
+    expect(subject.red("Hello")).to eq("\e[31mHello\e[0m")
+  end
+
+  it "returns a green string" do
+    expect(subject.green("Hello")).to eq("\e[32mHello\e[0m")
+  end
+
+  it "returns a yellow string" do
+    expect(subject.yellow("Hello")).to eq("\e[33mHello\e[0m")
+  end
+end
diff --git a/tools/logstash-docgen/spec/spec_helper.rb b/tools/logstash-docgen/spec/spec_helper.rb
new file mode 100644
index 00000000000..1856c339ffe
--- /dev/null
+++ b/tools/logstash-docgen/spec/spec_helper.rb
@@ -0,0 +1,10 @@
+# encoding: utf-8
+#
+require "vcr"
+require "webmock"
+require_relative "support/helpers"
+
+VCR.configure do |config|
+  config.cassette_library_dir = ::File.join(::File.dirname(__FILE__), "fixtures", "vcr_cassettes")
+  config.hook_into :webmock
+end
diff --git a/tools/logstash-docgen/spec/support/helpers.rb b/tools/logstash-docgen/spec/support/helpers.rb
new file mode 100644
index 00000000000..30e2c33fc23
--- /dev/null
+++ b/tools/logstash-docgen/spec/support/helpers.rb
@@ -0,0 +1,14 @@
+# encoding: utf-8
+#
+# This make the test suite non thread safe.
+def capture(&block)
+  old_stdout = $stdout
+
+  begin
+    $stdout = StringIO.new
+    block.call
+    return $stdout.string
+  ensure
+    $stdout = old_stdout
+  end
+end
diff --git a/tools/logstash-docgen/templates/index-codecs.asciidoc.erb b/tools/logstash-docgen/templates/index-codecs.asciidoc.erb
new file mode 100644
index 00000000000..af0659be09b
--- /dev/null
+++ b/tools/logstash-docgen/templates/index-codecs.asciidoc.erb
@@ -0,0 +1,21 @@
+[[codec-plugins]]
+== Codec plugins
+
+A codec plugin changes the data representation of an event. Codecs are essentially stream filters that can operate as part
+of an input or output.
+
+The following codec plugins are available:
+
+|=======================================================================
+| Plugin | Description | Github repository
+<% plugins.each do |plugin| %>
+  | <<<%=plugin.full_name%>,<%=plugin.name%>>> | <%=plugin.description%> | <%=plugin.github_url%>[<%=plugin.full_name%>]
+<% end %>
+|=======================================================================
+
+<% plugins.each do |plugin| %>
+:edit_url: <%=plugin.edit_url%>
+include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
+<% end %>
+
+:edit_url:
diff --git a/tools/logstash-docgen/templates/index-filters.asciidoc.erb b/tools/logstash-docgen/templates/index-filters.asciidoc.erb
new file mode 100644
index 00000000000..47340c5c272
--- /dev/null
+++ b/tools/logstash-docgen/templates/index-filters.asciidoc.erb
@@ -0,0 +1,21 @@
+[[filter-plugins]]
+== Filter plugins
+
+A filter plugin performs intermediary processing on an event. Filters are often applied conditionally depending on the
+characteristics of the event.
+
+The following filter plugins are available:
+
+|=======================================================================
+| Plugin | Description | Github repository
+<% plugins.each do |plugin| %>
+  | <<<%=plugin.full_name%>,<%=plugin.name%>>> | <%=plugin.description%> | <%=plugin.github_url%>[<%=plugin.full_name%>]
+<% end %>
+|=======================================================================
+
+<% plugins.each do |plugin| %>
+:edit_url: <%=plugin.edit_url%>
+include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
+<% end %>
+
+:edit_url:
diff --git a/tools/logstash-docgen/templates/index-inputs.asciidoc.erb b/tools/logstash-docgen/templates/index-inputs.asciidoc.erb
new file mode 100644
index 00000000000..ecc477537d1
--- /dev/null
+++ b/tools/logstash-docgen/templates/index-inputs.asciidoc.erb
@@ -0,0 +1,20 @@
+[[input-plugins]]
+== Input plugins
+
+An input plugin enables a specific source of events to be read by Logstash.
+
+The following input plugins are available:
+
+|=======================================================================
+| Plugin | Description | Github repository
+<% plugins.each do |plugin| %>
+  | <<<%=plugin.full_name%>,<%=plugin.name%>>> | <%=plugin.description%> | <%=plugin.github_url%>[<%=plugin.full_name%>]
+<% end %>
+|=======================================================================
+
+<% plugins.each do |plugin| %>
+:edit_url: <%=plugin.edit_url%>
+include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
+<% end %>
+
+:edit_url:
diff --git a/tools/logstash-docgen/templates/index-outputs.asciidoc.erb b/tools/logstash-docgen/templates/index-outputs.asciidoc.erb
new file mode 100644
index 00000000000..6d6d241bab6
--- /dev/null
+++ b/tools/logstash-docgen/templates/index-outputs.asciidoc.erb
@@ -0,0 +1,20 @@
+[[output-plugins]]
+== Output plugins
+
+An output plugin sends event data to a particular destination. Outputs are the final stage in the event pipeline.
+
+The following output plugins are available:
+
+|=======================================================================
+| Plugin | Description | Github repository
+<% plugins.each do |plugin| %>
+  | <<<%=plugin.full_name%>,<%=plugin.name%>>> | <%=plugin.description%> | <%=plugin.github_url%>[<%=plugin.full_name%>]
+<% end %>
+|=======================================================================
+
+<% plugins.each do |plugin| %>
+:edit_url: <%=plugin.edit_url%>
+include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
+<% end %>
+
+:edit_url:
diff --git a/tools/logstash-docgen/templates/plugin-doc.asciidoc.erb b/tools/logstash-docgen/templates/plugin-doc.asciidoc.erb
new file mode 100644
index 00000000000..dc7de83bc5a
--- /dev/null
+++ b/tools/logstash-docgen/templates/plugin-doc.asciidoc.erb
@@ -0,0 +1,128 @@
+[[plugins-<%= section %>s-<%= name %>]]
+=== <%= name %>
+
+* Version: <%=version%>
+* Released on: <%=release_date%>
+* <%=changelog_url%>[Changelog]
+
+<% unless default_plugin? %>
+==== Installation
+
+For plugins not bundled by default, it is easy to install by running `bin/logstash-plugin install logstash-<%= section %>-<%= name %>`. See <<working-with-plugins>> for more details.
+<% end %>
+
+==== Getting Help
+
+For questions about the plugin, open a topic in the http://discuss.elastic.co[Discuss] forums. For bugs or feature requests, open an issue in https://github.com/elastic/logstash[Github].
+For the list of Elastic supported plugins, please consult the https://www.elastic.co/support/matrix#show_logstash_plugins[Elastic Support Matrix].
+
+==== Description
+
+<%= description %>
+
+&nbsp;
+
+==== Synopsis
+
+<% if sorted_attributes.count > 0 -%>
+This plugin supports the following configuration options:
+<% else -%>
+This plugin has no configuration options.
+<% end -%>
+
+<% if sorted_attributes.count > 0 -%>
+Required configuration options:
+<% else -%>
+Complete configuration example:
+<% end -%>
+
+[source,json]
+--------------------------
+<%= name %> {
+<% if sorted_attributes.count > 0 -%>
+<% sorted_attributes.each do |name, config|
+   next if config[:deprecated]
+   next if !config[:required]
+-%>
+<%= "  " if section == "codec" %>    <%= name %> => ...
+<% end -%>
+<%= "  " if section == "codec" %><% ; end -%>}
+--------------------------
+
+<% if sorted_attributes.count > 0 %>
+
+Available configuration options:
+
+[cols="<,<,<",options="header",]
+|=======================================================================
+|Setting |Input type|Required
+<% sorted_attributes.each do |name, config|
+   next if config[:obsolete]
+   next if config[:deprecated]
+   if config[:validate].is_a?(Array)
+     annotation = "|<<string,string>>, one of `#{config[:validate].inspect}`"
+   elsif config[:validate] == :path
+     annotation = "|a valid filesystem path"
+   else
+     annotation = "|<<#{config[:validate]},#{config[:validate]}>>"
+   end
+
+   if name.is_a?(Regexp)
+     name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+   end
+   if config[:required]
+     annotation += "|Yes"
+   else
+     annotation += "|No"
+   end
+-%>
+| <<plugins-<%= section %>s-<%=config_name%>-<%=name%>>> <%= annotation %>
+<% end -%>
+|=======================================================================
+<% end %>
+<% if sorted_attributes.count > 0 -%>
+
+==== Details
+
+&nbsp;
+
+<% sorted_attributes.each do |name, config| -%>
+<%
+     next if config[:obsolete]
+     if name.is_a?(Regexp)
+       name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
+       is_regexp = true
+     else
+       is_regexp = false
+     end
+-%>
+[[plugins-<%= section%>s-<%=config_name%>-<%= name%>]]
+===== `<%= name %>` <%= " (DEPRECATED)" if config[:deprecated] %>
+
+<% if config[:required] -%>
+  * This is a required setting.
+<% end -%>
+<% if config[:deprecated] -%>
+  * DEPRECATED WARNING: This configuration item is deprecated and may not be available in future versions.
+<% end -%>
+<% if is_regexp -%>
+  * The configuration attribute name here is anything that matches the above regular expression.
+<% end -%>
+<% if config[:validate].is_a?(Symbol) -%>
+  * Value type is <<<%= config[:validate] %>,<%= config[:validate] %>>>
+<% elsif config[:validate].nil? -%>
+  * Value type is <<string,string>>
+<% elsif config[:validate].is_a?(Array) -%>
+  * Value can be any of: `<%= config[:validate].join('`, `') %>`
+<% end -%>
+<% if config.include?(:default) -%>
+  * Default value is `<%= config[:default].inspect %>`
+<% else -%>
+  * There is no default value for this setting.
+<% end -%>
+
+<%= config[:description] %>
+
+<% end -%>
+
+<% end -%>
diff --git a/tools/logstash-docgen/templates/plugin-doc.css b/tools/logstash-docgen/templates/plugin-doc.css
new file mode 100644
index 00000000000..7bdb684994a
--- /dev/null
+++ b/tools/logstash-docgen/templates/plugin-doc.css
@@ -0,0 +1,595 @@
+#footer-wrapper {
+    clear: both;
+}
+
+#guide {
+    width:  95%;
+    margin: 40px auto 0;
+    max-width: 1200px;
+}
+
+#guide div.chapter,
+#guide div.part,
+#guide div.section {
+    margin-bottom: 2em;
+}
+
+#guide .article div.section,
+#guide .book div.part,
+#guide .book div.chapter,
+#guide .chapter div.section {
+    margin-top: 2em;
+}
+
+#guide hr {
+    margin: 1em 0;
+}
+
+#guide strong,
+#guide b {
+    font-weight: bold;
+}
+
+#guide > .article,
+#guide > .book,
+#guide > .preface,
+#guide > .breadcrumbs,
+#guide > .navheader,
+#guide > .navfooter,
+#guide > .chapter,
+#guide > .part,
+#guide > .section,
+#guide > .glossary,
+#guide > .appendix {
+    width: 67%;
+}
+
+#guide h1,
+#guide h2,
+#guide h3,
+#guide h4,
+#guide h5 {
+    margin: 0.8em 0 0.5em;
+    position: relative;
+    line-height: 1em;
+}
+
+#guide .titlepage h1,
+#guide .titlepage h2 {
+    font-size: 34px;
+    margin-top: 0;
+}
+
+/* Navheader */
+#guide .navheader .prev,
+#guide .navfooter .prev {
+    float: left;
+    width: 50%;
+}
+
+#guide .navheader .next,
+#guide .navfooter .next {
+    text-align: right;
+    width: 50%;
+    display: inline-block;
+}
+
+#guide .navheader {
+    display: inline-block;
+    border-bottom: 1px solid #ccc;
+    padding-bottom: 5px;
+    margin: 10px 0 20px;
+}
+
+#guide .navfooter {
+    display: inline-block;
+    border-top: 1px solid #ccc;
+    padding-top: 5px;
+    margin-bottom: 15px;
+}
+
+/* On this page */
+#this_page {
+    float: right;
+    clear: right;
+    width: 50%;
+    margin: 0 -50% 1em 0;
+    padding-left: 2.5em;
+}
+
+#this_page h2 {
+    font-size: 1em;
+    margin: 0 0 0.2em 0;
+}
+
+#this_page ul {
+    margin: 0;
+    font-size: 0.85em;
+}
+
+/* Right hand TOC */
+#guide div.toc {
+    float: right;
+    clear: right;
+    width: 47%;
+    margin: 0 -50% 2em 0;
+}
+
+#book_title {
+    color: #2b4590;
+}
+
+#book_title select {
+    background-color: #fcfcfc;
+    border: none;
+    margin-left: 1px;
+    color: #2b4590;
+}
+
+#guide ul.toc {
+    border: 1px solid #ddd;
+}
+
+#guide .toc ul {
+    margin: 0;
+    padding: 0;
+}
+
+#guide .toc li {
+    margin: 0;
+    padding: 0;
+    list-style: none;
+}
+
+#guide .toc span {
+    display: block;
+    padding: 0.1em 0;
+    font-size: 0.85em;
+}
+
+#guide .toc > li > span {
+    background-color: #efefef;
+    border-bottom: 1px solid #ddd;
+    padding: 0.3em 0 0.3em 20px;
+    font-size: 1em;
+}
+
+#guide .toc ul ul {
+    display: none;
+}
+
+#guide .toc .show > ul {
+    display: block;
+}
+
+#guide .toc .collapsible > span:hover {
+    background-color: #fafafa;
+    cursor: pointer;
+}
+
+#guide .toc .collapsible > span {
+    background-repeat: no-repeat;
+    background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPCAYAAAA71pVKAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH3wUQChsBZOOibwAAAHlJREFUKM9jYBh5gBGXhNGeDbb/mRgP/f//3/qCc8AxbGqYKLGZBV3AYP96AwYGBgaGfwwqEOMZVQ32r//GwMDAcMEx8AJezYz/mc4zMDAw/GeE8RkWIDmQEb9mRkZLBgYGhv///hswMDJM//+fIY2JifHy4AqwkQgAW7gloZBjerAAAAAASUVORK5CYII=);
+}
+
+#guide .toc .show > span {
+    background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAA8AAAAPCAYAAAA71pVKAAAABmJLR0QA/wD/AP+gvaeTAAAACXBIWXMAAA3XAAAN1wFCKJt4AAAAB3RJTUUH3wUQChsxQjqSwwAAADFJREFUKM9jYBgF9AOM2AQN9q83QBe74Bh4AV2MBauJ/5nOE2MRVs3/Gf8ZjkbKYAMA5XcICwYKcGoAAAAASUVORK5CYII=);
+}
+
+#guide .toc > .show > span {
+    background-color: white;
+    border-bottom: 1px solid white;
+}
+
+
+#guide .toc > .collapsible > span {
+    background-position: 0 8px;
+}
+
+#guide .toc > li > ul > li > span {
+    padding-left: 40px;
+    background-position: 20px 5px;
+}
+
+#guide .toc > li > ul > li > ul > li > span {
+    padding-left: 60px;
+    background-position: 40px 5px;
+}
+
+#guide .toc > li > ul > li > ul > li > ul > li >span {
+    padding-left: 80px;
+    background-position: 60px 5px;
+}
+
+#guide .toc > li > ul > li > ul > li > ul > li > ul > li >span {
+    padding-left: 80px;
+    background-image: none;
+    background-position: 80px 5px;
+}
+
+/* Home page TOC */
+#guide .article .toc,
+#guide .book .toc {
+    float: none;
+    clear: none;
+    width:  auto;
+    margin: 0 0 2em 0;
+    padding: 0;
+}
+
+/* Lists */
+#guide ul {
+    list-style: disc;
+    padding-left: 2em;
+}
+
+#guide ol {
+    list-style: decimal;
+    padding-left: 2em;
+}
+
+#guide li ol {
+    list-style: lower-alpha;
+}
+
+#guide li li ol {
+    list-style: lower-roman;
+}
+
+#guide li li li ol {
+    list-style: upper-alpha;
+}
+
+#guide ol[type=a] {
+    list-style: lower-alpha;
+}
+
+#guide ol[type="1"] {
+    list-style: decimal;
+}
+
+#guide li {
+    padding: 0.15em 1em 0.15em 0;
+}
+
+#guide ol p,
+#guide ul p,
+#guide table p {
+    margin-top: 0
+}
+
+#guide ol  ol,
+#guide ol  ul,
+#guide ul  ol,
+#guide ul  ul {
+    margin-bottom: 0;
+}
+
+#guide p.simpara+div.orderedlist,
+#guide p.simpara+div.itemizedlist {
+    margin-top: -0.9em;
+}
+
+#guide dt {
+    color: #2b4590;
+}
+
+#guide dd {
+    margin: 0 0 0.5em 2em;
+}
+
+/* Tables */
+#guide table{
+    margin-bottom:1em;
+    border: none;
+    width: 100%;
+}
+
+#guide table thead,#guide table tbody{
+    text-align:left;
+    vertical-align:top;
+}
+
+#guide table td:last-child{
+    padding-right:0;
+}
+
+#guide table th{
+    font-weight:700;
+    padding:.5em 0.2em;
+    border:none;
+    border-bottom:2px solid #e5eae4;
+}
+
+#guide table td{
+    vertical-align:top;
+    padding:.75em 0.2em 0;
+    border:none;
+    border-bottom:1px solid #e5eae4;
+}
+
+#guide table tr:last-child td {
+    border:none;
+}
+
+/* Code */
+#guide code {
+    background: #f8f8f8;
+    padding: 0 3px;
+    font-family: Consolas, Menlo, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console';
+    font-size: 0.9em;
+}
+#guide pre {
+    font-family: Consolas, Menlo, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console';
+    color #888;
+}
+
+#guide pre.prettyprint {
+    font-family: Consolas, Menlo, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console';
+    font-size: 0.9em;
+    margin: 0 0 15px 0;
+    padding: 8px 0 8px 18px;
+    border: none;
+    line-height: 1.5em;
+    overflow: auto;
+    white-space: pre;
+    background-color:#F0F0F0;
+    border-top-right-radius:5px;
+    border-bottom-right-radius:5px;
+    width: auto;
+    max-width: 1000px;
+}
+
+#guide .pre_wrapper {
+    overflow-x: auto;
+    position: relative;
+    width: 100%;
+    margin: 0 0 15px 0;
+    background-color:#F0F0F0;
+    border-left: 3px solid #31beb1;
+}
+
+#guide .pre_wrapper pre {
+    margin: 0;
+}
+
+#guide a code {
+    color: #31beb1;
+}
+
+/* Admonitions */
+#guide .admon {
+    background: #fbfbfb;
+    padding: 10px;
+    min-height: 80px;
+    margin: 15px 0;
+    border-radius: 5px;
+    position: relative
+}
+
+#guide .admon .icon {
+    width:  80px;
+    position: absolute;
+}
+
+#guide .admon .admon_content {
+    margin-left: 80px;
+}
+
+#guide .admon_content h4 {
+    margin: 9px 0;
+}
+
+#guide .example {
+    margin: 0;
+}
+
+#guide .example .title {
+    margin: 0 0 0 10px;
+}
+#guide .example-contents {
+    border: 1px solid #f0f0f0;
+    border-width: 1px;
+    border-radius: 5px;
+    background: #fdfdfd;
+    padding: 10px 10px 0;
+}
+
+#guide .sidebar {
+    border: 1px solid #31beb1;
+    border-radius: 10px;
+    background: #fbfbfb;
+    padding: 15px 20px 0;
+    margin: 10px 15px 20px;
+}
+
+#guide .sidebar .title {
+    margin: 0;
+    border-bottom: 1px solid #ddd;
+    margin-bottom: 10px;
+}
+
+#guide .experimental,
+#guide .coming,
+#guide .deprecated,
+#guide .added {
+    position: relative;
+    font-size: 16px;
+}
+
+#guide .experimental .exp_title,
+#guide .coming .version,
+#guide .deprecated .version,
+#guide .added .version {
+    font-family:Monaco, 'DejaVu Sans Mono', 'Bitstream Vera Sans Mono', 'Lucida Console', monospace!important;
+    font-size: 0.8em;
+    border-bottom: 1px dashed #31beb1;
+}
+
+#guide .deprecated .version {
+    text-decoration: line-through;
+}
+
+#guide .experimental .detail,
+#guide .coming .detail,
+#guide .deprecated .detail,
+#guide .added .detail {
+    min-width: 250px;
+    position: absolute;
+    padding: 15px;
+    border: 3px solid #e8e8e8;
+    background: white;
+    opacity: 0;
+    color: white;
+    left: -20000px;
+    bottom: 20px;
+    line-height: 1.2em;
+}
+
+#guide .experimental:hover ,
+#guide .coming:hover ,
+#guide .deprecated:hover ,
+#guide .added:hover {
+    color: black;
+    transition: all 0.3s ease;
+}
+
+#guide .experimental:hover .detail,
+#guide .coming:hover .detail,
+#guide .deprecated:hover .detail,
+#guide .added:hover .detail {
+    z-index: 10000;
+    left: -5em;
+    opacity: 1;
+    color: #555;
+    transition: color 0.5s ease;
+}
+
+#guide .toc .experimental,
+#guide .toc .coming,
+#guide .toc .added,
+#guide .toc .deprecated {
+    display: none;
+}
+
+#guide h1:hover a[id],
+#guide h2:hover a[id],
+#guide h3:hover a[id],
+#guide h4:hover a[id],
+#guide h5:hover a[id],
+#guide h6:hover a[id] {
+    display: block;
+    position: absolute;
+    bottom: 0;
+    margin: 0;
+    padding: 0;
+    left: -25px;
+    width: 100%;
+    height: 1em;
+    background-repeat: no-repeat;
+    background-position: 0% 50%;
+    background-image: url(data:image/png;base64,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);
+}
+
+#guide a.edit_me {
+    position: absolute;
+    font-size: 14px;
+    right: 0px;
+    top: 0;
+    z-index: 1000;
+    padding-top: 26px;
+    line-height: 1;
+    opacity: 0.3;
+    background-repeat: no-repeat;
+    background-image: url(data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABgAAAAYCAYAAADgdz34AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAAArgAAAK4B+ff3XQAAABl0RVh0U29mdHdhcmUAd3d3Lmlua3NjYXBlLm9yZ5vuPBoAAAKySURBVEiJrZVLbExRGMd/3713qkJZaBeIhIUIC2FBhOpjxHRaNkhTERZeC5EQ2pmSsBikMaYVj4h0Q4pERNKNMlMVnZZ4lIUuxYqFaLqRNNXHzJzPYm6FPmc6zuqenPP//c53v5tzRVXJZQSiZUcFqsUy58IV3T3j161c4HVt5YWIfFFhjVHrfSBW1vBfBeIxbYgGPR7HCzwBzgZjZc2hkPzhzloQjHq32NiHMSxLJJJdanETOKmwY2Tj1kU5CYLPy3eqmNcpNRfzLOMDWsUQU2VpamB0ZUNld/+sBfUdxUvU6F13WpxQu0KRh4i1Gxi8Wv1m6O/9WQlCIbFMyrkPFAKKUK/odUHjaKqwsTJ+YXwmK8HAptIzgDc9kyaUY8ACYEhsuifLZCyojZZvEiUEIPBOUBvY4MqOXdne9WWynJMJ/MyL7QtF9CHgIPxEpVnROy68JeLvfDBVNiNBMpVsFlgOgGqdwpV0IXzGyjs+XXZGQTBaekhEatKH5TYq+4AiYNiIVdPkax+cLj9tD+pj3lUqcsOd9qLah9tkhdqmipe9Mx1QprrsTkSr5syxfr1DWQcMChxVuA/YQGvEH98zE3zaCvIZCrtwRAmoctmFf3VSqSOZwKesINDu3YGatvQO7qEUALuAJFAS8cffZiqYUMGp9pLFqBm7Cj6L8smFI8j5bOCTChy19gMFwLCB8woNAKJ0RPzxcDZwGPeZ1j8tXW2J1Zmw1ScmmbDEuQXki9I3muccULL//f1TgbHVE67q/Hi1Iv5KxOOo8ghQY3Pg2rYXfdnCJwrEkbHnbwNFbxXpVvRgoy/eMRv4BIGo7g3EStYCrJjXv95Whhv9XS2zhbvM9Gs9/XjzXLsg7xvwXZHjozr3w43KZyO5wOGvJjvz8zcaUo8sM3QpUtXzI1fw2PgNtmkFbbGhF10AAAAASUVORK5CYII=);
+
+}
+
+#guide a.edit_me:hover {
+    opacity: 1;
+}
+
+
+/* JS API TOC */
+/* prevent the list from rendering at the top of the content */
+#js-api-method-index + .itemizedlist,
+#js-api-method-index-0-90 + .itemizedlist {
+    display: none;
+}
+.js-api-method-index h2 {
+    color: #454746;
+    font-family: "Gibson-Regular";
+    font-size: 1.17em;
+    margin: 1.5em 0 1em;
+}
+.js-api-method-index ul {
+    padding: 0 0 0 0;
+}
+
+.js-api-method-index ul li {
+    list-style: disc;
+    color: #74b73f;
+    padding: 0.05em 0;
+}
+
+.js-api-method-index a {
+    text-transform: none;
+}
+
+.js-client-docs a[href="#"] {
+    float: right;
+}
+
+/* Comments */
+p.remark {
+    color: red;
+    border: 3px red solid;
+    border-radius: 5px;
+    padding: 10px;
+    float: right;
+    max-width: 50%;
+    margin: -5px 0 10px 10px;
+    clear: right;
+}
+
+/* Images */
+#guide .mediaobject img {
+    max-width: 100%;
+}
+
+/* Sense widget */
+#guide a.sense_widget {
+    display: block;
+    margin: -22px 0 15px auto;
+    text-align: right;
+    border-radius: 0 0 3px 0;
+    border-top: 1px solid #e8e8e8;
+    background: #f8f8f8;
+    background: linear-gradient(#f8f8f8,#f0f0f0);
+    text-transform: uppercase;
+    font-family: helvetica, arial;
+    font-size: 13px;
+    font-weight: bold;
+    border-left: 3px solid #31beb1;
+    padding: 5px 15px 5px 0;
+}
+
+#guide a.sense_widget:hover {
+    background: linear-gradient(white,#f0f0f0);
+    color: #2b4590;
+}
+
+/* RTP container */
+#guide #rtpcontainer {
+    display: none;
+    position: static;
+    float: right;
+    width: 31%;
+    margin: 0 -0.2% 20px 0;
+    border-bottom: 1px solid #ccc
+}
+
+#guide ul.lists li {
+    background: none
+}
+
diff --git a/tools/paquet/.gitignore b/tools/paquet/.gitignore
new file mode 100644
index 00000000000..ebcca5e3bda
--- /dev/null
+++ b/tools/paquet/.gitignore
@@ -0,0 +1,6 @@
+/.bundle/
+/.yardoc
+/Gemfile.lock
+/spec/support/dependencies
+/spec/support/.bundle
+/spec/support/*.lock
diff --git a/tools/paquet/CHANGELOG.md b/tools/paquet/CHANGELOG.md
new file mode 100644
index 00000000000..4894ee05b7d
--- /dev/null
+++ b/tools/paquet/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.2.1
+ - Making sure the gems are downloaded in binary format, without it, gems downloaded on windows will be corrupted.
diff --git a/tools/paquet/Gemfile b/tools/paquet/Gemfile
new file mode 100644
index 00000000000..67dedf9c429
--- /dev/null
+++ b/tools/paquet/Gemfile
@@ -0,0 +1,4 @@
+source 'https://rubygems.org'
+
+# Specify your gem's dependencies in paquet.gemspec
+gemspec
diff --git a/tools/paquet/LICENSE b/tools/paquet/LICENSE
new file mode 100644
index 00000000000..43976b73b2b
--- /dev/null
+++ b/tools/paquet/LICENSE
@@ -0,0 +1,13 @@
+Copyright (c) 2012â€“2016 Elasticsearch <http://www.elastic.co>
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/tools/paquet/README.md b/tools/paquet/README.md
new file mode 100644
index 00000000000..ac61e149a0f
--- /dev/null
+++ b/tools/paquet/README.md
@@ -0,0 +1,70 @@
+# Paquet
+
+This gem allow a developer to create a uber gem, a uber gem is a gem that content the current gem and his dependencies and is distributed as a tarball.
+
+This tool allow to define what will be bundler and what should be ignored, it uses the dependencies defined in the gemspec and gemfile to know what to download.
+
+Note that by default no gems will be bundled.
+
+
+## Installation
+
+Add this line to your application's Gemfile:
+
+```ruby
+gem 'paquet'
+```
+
+And then execute:
+
+    $ bundle
+
+## Usage
+Define the dependencies in your Rakefile
+
+```ruby
+# encoding: utf-8
+require "paquet"
+
+TARGET_DIRECTORY = File.join(File.dirname(__FILE__), "dependencies")
+
+Paquet::Task.new(TARGET_DIRECTORY) do
+  pack "manticore"
+  pack "launchy"
+  pack "gemoji"
+  pack "logstash-output-elasticsearch"
+
+  # Everything not defined here will be assumed to be provided
+  # by the target installation
+  ignore "logstash-core-plugin-api"
+  ignore "logstash-core"
+end
+```
+
+And run
+
+```
+bundle exec rake paquet:vendor
+```
+
+The dependencies will be downloaded in your target directory.
+
+## Project Principles
+
+* Community: If a newbie has a bad time, it's a bug.
+* Software: Make it work, then make it right, then make it fast.
+* Technology: If it doesn't do a thing today, we can make it do it tomorrow.
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports,
+complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and
+maintainers or community members  saying "send patches or die" - you will not
+see that here.
+
+It is more important to me that you are able to contribute.
+
+For more information about contributing, see the
+[CONTRIBUTING](../CONTRIBUTING.md) file.
diff --git a/tools/paquet/Rakefile b/tools/paquet/Rakefile
new file mode 100644
index 00000000000..43022f711e2
--- /dev/null
+++ b/tools/paquet/Rakefile
@@ -0,0 +1,2 @@
+require "bundler/gem_tasks"
+task :default => :spec
diff --git a/tools/paquet/lib/paquet.rb b/tools/paquet/lib/paquet.rb
new file mode 100644
index 00000000000..0e19b6221eb
--- /dev/null
+++ b/tools/paquet/lib/paquet.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require "paquet/version"
+require "paquet/shell_ui"
+require "paquet/gem"
+require "paquet/dependency"
+require "paquet/rspec/tasks"
+
+module Paquet
+end
diff --git a/tools/paquet/lib/paquet/dependency.rb b/tools/paquet/lib/paquet/dependency.rb
new file mode 100644
index 00000000000..08fc739cc7a
--- /dev/null
+++ b/tools/paquet/lib/paquet/dependency.rb
@@ -0,0 +1,19 @@
+module Paquet
+  class Dependency
+    attr_reader :name, :version, :platform
+
+    def initialize(name, version, platform)
+      @name = name
+      @version = version
+      @platform = platform
+    end
+
+    def to_s
+      "#{name}-#{version}"
+    end
+
+    def ruby?
+      platform == "ruby"
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/gem.rb b/tools/paquet/lib/paquet/gem.rb
new file mode 100644
index 00000000000..88219651546
--- /dev/null
+++ b/tools/paquet/lib/paquet/gem.rb
@@ -0,0 +1,109 @@
+# encoding: utf-8
+require "paquet/dependency"
+require "paquet/shell_ui"
+require "paquet/utils"
+
+module Paquet
+  class Gem
+    RUBYGEMS_URI = "https://rubygems.org/downloads"
+
+    attr_reader :gems, :ignores
+
+    def initialize(target_path, cache = nil)
+      @target_path = target_path
+      @gems = []
+      @ignores = []
+      @cache = cache
+    end
+
+    def add(name)
+      @gems << name
+    end
+
+    def ignore(name)
+      @ignores << name
+    end
+
+    def pack
+      Paquet::ui.info("Cleaning existing target path: #{@target_path}")
+
+      FileUtils.rm_rf(@target_path)
+      FileUtils.mkdir_p(@target_path)
+
+      package_gems(collect_required_gems)
+    end
+
+    def package_gems(collect_required_gems)
+      gems_to_package = collect_required_gems
+        .collect { |gem| gem_full_name(gem) }
+        .uniq
+
+      if use_cache?
+        gems_to_package.each do |gem_name|
+          if gem_file = find_in_cache(gem_name)
+            destination = File.join(@target_path, File.basename(gem_file))
+            FileUtils.cp(gem_file, destination)
+            Paquet::ui.info("Vendoring: #{gem_name}, from cache: #{gem_file}")
+          else
+            download_gem(gem_name)
+          end
+        end
+      else
+        gems_to_package.each do |gem_name|
+          download_gem(gem_name)
+        end
+      end
+    end
+
+    def use_cache?
+      @cache
+    end
+
+    def find_in_cache(gem_name)
+      filename = File.join(@cache, gem_name)
+      File.exist?(filename) ? filename : nil
+    end
+
+    def size
+      @gems.size
+    end
+
+    def ignore?(name)
+      ignores.include?(name)
+    end
+
+    def collect_required_gems()
+      candidates = []
+      @gems.each do |name|
+        candidates += resolve_dependencies(name)
+      end
+      candidates.flatten
+    end
+
+    def resolve_dependencies(name)
+      return [] if ignore?(name)
+
+      spec = ::Gem::Specification.find_by_name(name)
+      current_dependency = Dependency.new(name, spec.version, spec.platform)
+      dependencies = spec.dependencies.select { |dep| dep.type == :runtime }
+
+      if dependencies.size == 0
+        [current_dependency]
+      else
+        [dependencies.collect { |spec| resolve_dependencies(spec.name) }, current_dependency].flatten.uniq { |s| s.name }
+      end
+    end
+
+    def gem_full_name(gem)
+      gem.ruby? ? "#{gem.name}-#{gem.version}.gem" : "#{gem.name}-#{gem.version}-#{gem.platform}.gem" 
+    end
+
+    def download_gem(gem_name)
+      source = "#{RUBYGEMS_URI}/#{gem_name}"
+      destination = File.join(@target_path, gem_name)
+
+      Paquet::ui.info("Vendoring: #{gem_name}, downloading: #{source}")
+      Paquet::Utils::download_file(source, destination)
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/rspec/tasks.rb b/tools/paquet/lib/paquet/rspec/tasks.rb
new file mode 100644
index 00000000000..de4aa9213a6
--- /dev/null
+++ b/tools/paquet/lib/paquet/rspec/tasks.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "bundler"
+require "rake"
+require "rake/tasklib"
+require "fileutils"
+require "net/http"
+require "paquet/gem"
+
+# This class add new rake methods to a an existing ruby gem,
+# these methods allow developers to create a Uber gem, a uber gem is
+# a tarball that contains the current gems and one or more of his dependencies.
+#
+# This Tool will take care of looking at the current dependency tree defined in the Gemspec and the gemfile
+# and will traverse all graph and download the gem file into a specified directory.
+#
+# By default, the tool won't fetch everything and the developer need to declare what gems he want to download.
+module Paquet
+  class Task < Rake::TaskLib
+    def initialize(target_path, cache_path = nil, &block)
+      @gem = Gem.new(target_path, cache_path)
+
+      instance_eval(&block)
+
+      namespace :paquet do
+        desc "Build a pack with #{@gem.size} gems: #{@gem.gems.join(",")}"
+        task :vendor do
+          @gem.pack
+        end
+      end
+    end
+
+    def pack(name)
+      @gem.add(name)
+    end
+
+    def ignore(name)
+      @gem.ignore(name)
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/shell_ui.rb b/tools/paquet/lib/paquet/shell_ui.rb
new file mode 100644
index 00000000000..331c37bef88
--- /dev/null
+++ b/tools/paquet/lib/paquet/shell_ui.rb
@@ -0,0 +1,37 @@
+# encoding: utf-8
+module Paquet
+  class SilentUI
+    class << self
+      def debug(message)
+      end
+      def info(message)
+      end
+    end
+  end
+
+  class ShellUi
+    def debug(message)
+      report_message(:debug, message) if debug?
+    end
+
+    def info(message)
+      report_message(:info, message)
+    end
+
+    def report_message(level, message)
+      puts "[#{level.upcase}]: #{message}"
+    end
+
+    def debug?
+      ENV["DEBUG"]
+    end
+  end
+
+  def self.ui
+    @logger ||= ShellUi.new
+  end
+
+  def self.ui=(new_output)
+    @logger = new_output
+  end
+end
diff --git a/tools/paquet/lib/paquet/utils.rb b/tools/paquet/lib/paquet/utils.rb
new file mode 100644
index 00000000000..b7800db2e0e
--- /dev/null
+++ b/tools/paquet/lib/paquet/utils.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+require "fileutils"
+require "uri"
+
+module Paquet
+  class Utils
+    HTTPS_SCHEME = "https"
+    REDIRECTION_LIMIT = 5
+
+    def self.download_file(source, destination, counter = REDIRECTION_LIMIT)
+      raise "Too many redirection" if counter == 0
+
+      begin
+        f = File.open(destination, "wb")
+
+        uri = URI.parse(source)
+
+        http = Net::HTTP.new(uri.host, uri.port, )
+        http.use_ssl = uri.scheme ==  HTTPS_SCHEME
+
+        response = http.get(uri.path)
+
+        case response
+        when Net::HTTPSuccess
+          f.write(response.body)
+        when Net::HTTPRedirection
+          counter -= 1
+          download_file(response['location'], destination, counter)
+        else
+          raise "Response not handled: #{response.class}, path: #{uri.path}"
+        end
+        f.path
+      rescue => e
+        FileUtils.rm_rf(f.path) rescue nil
+        raise e
+      ensure
+        f.close
+      end
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/version.rb b/tools/paquet/lib/paquet/version.rb
new file mode 100644
index 00000000000..720bad35e1e
--- /dev/null
+++ b/tools/paquet/lib/paquet/version.rb
@@ -0,0 +1,3 @@
+module Paquet
+  VERSION = "0.2.1"
+end
diff --git a/tools/paquet/paquet.gemspec b/tools/paquet/paquet.gemspec
new file mode 100644
index 00000000000..b195778d0ee
--- /dev/null
+++ b/tools/paquet/paquet.gemspec
@@ -0,0 +1,28 @@
+# coding: utf-8
+lib = File.expand_path('../lib', __FILE__)
+$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
+require 'paquet/version'
+
+Gem::Specification.new do |spec|
+  spec.name          = "paquet"
+  spec.version       = Paquet::VERSION
+  spec.authors       = ["Elastic"]
+  spec.email         = ["info@elastic.co"]
+  spec.license       = "Apache License (2.0)"
+
+  spec.summary       = %q{Rake helpers to create a uber gem}
+  spec.description   = %q{This gem add a few rake tasks to create a uber gems that will be shipped as a zip}
+  spec.homepage      = "https://github.com/elastic/logstash"
+
+
+  spec.files         = Dir.glob(File.join(File.dirname(__FILE__), "lib", "**", "*.rb"))
+
+  spec.bindir        = "exe"
+  spec.executables   = spec.files.grep(%r{^exe/}) { |f| File.basename(f) }
+  spec.require_paths = ["lib"]
+
+  spec.add_development_dependency "rspec"
+  spec.add_development_dependency "pry"
+  spec.add_development_dependency "webmock", "~> 2.2.0"
+  spec.add_development_dependency "stud"
+end
diff --git a/tools/paquet/spec/integration/paquet_spec.rb b/tools/paquet/spec/integration/paquet_spec.rb
new file mode 100644
index 00000000000..b5b52f1ecdb
--- /dev/null
+++ b/tools/paquet/spec/integration/paquet_spec.rb
@@ -0,0 +1,74 @@
+# encoding: utf-8
+require "bundler"
+require "fileutils"
+require "stud/temporary"
+
+describe "Pack the dependencies", :integration => true do
+  let(:path) { File.expand_path(File.join(File.dirname(__FILE__), "..", "support")) }
+  let(:vendor_path) { Stud::Temporary.pathname }
+  let(:dependencies_path) { File.join(path, "dependencies") }
+  let(:bundler_cmd) { "bundle install --path #{vendor_path}"}
+  let(:rake_cmd) { "bundler exec rake paquet:vendor" }
+  let(:bundler_config) { File.join(path, ".bundler") }
+  let(:cache_path) { File.join(path, "cache") }
+  let(:cache_flores_gem) { File.join(cache_path, "flores-0.0.6.gem")}
+  let(:dummy_checksum_content) { "hello world" }
+
+  before do
+    FileUtils.mkdir_p(cache_path)
+  end
+
+  context "with gems in cache" do
+    before do
+      File.open(cache_flores_gem, "w") { |f| f.write(dummy_checksum_content) }
+
+      FileUtils.rm_rf(bundler_config)
+      FileUtils.rm_rf(vendor_path)
+
+      Bundler.with_clean_env do
+        Dir.chdir(path) do
+          system(bundler_cmd)
+          system(rake_cmd)
+        end
+      end
+    end
+
+    after do
+      FileUtils.rm_rf(cache_flores_gem)
+    end
+
+    it "download the dependencies" do
+      downloaded_dependencies = Dir.glob(File.join(dependencies_path, "*.gem"))
+
+      expect(downloaded_dependencies.size).to eq(2)
+      expect(downloaded_dependencies).to include(/flores-0\.0\.6/,/stud/)
+      expect(downloaded_dependencies).not_to include(/logstash-devutils/)
+
+      expect(File.read(Dir.glob(File.join(dependencies_path, "flores*.gem")).first)).to eq(dummy_checksum_content)
+    end
+  end
+
+  context "without cached gems" do
+    before do
+      FileUtils.rm_rf(bundler_config)
+      FileUtils.rm_rf(vendor_path)
+
+      Bundler.with_clean_env do
+        Dir.chdir(path) do
+          system(bundler_cmd)
+          system(rake_cmd)
+        end
+      end
+    end
+
+    it "download the dependencies" do
+      downloaded_dependencies = Dir.glob(File.join(dependencies_path, "*.gem"))
+
+      expect(downloaded_dependencies.size).to eq(2)
+      expect(downloaded_dependencies).to include(/flores-0\.0\.6/,/stud/)
+      expect(downloaded_dependencies).not_to include(/logstash-devutils/)
+
+      expect(File.read(Dir.glob(File.join(dependencies_path, "flores*.gem")).first)).not_to eq(dummy_checksum_content)
+    end
+  end
+end
diff --git a/tools/paquet/spec/paquet/dependency_spec.rb b/tools/paquet/spec/paquet/dependency_spec.rb
new file mode 100644
index 00000000000..0ed02f9fe08
--- /dev/null
+++ b/tools/paquet/spec/paquet/dependency_spec.rb
@@ -0,0 +1,36 @@
+# encoding: utf-8
+require "paquet/dependency"
+
+describe Paquet::Dependency do
+  let(:name) { "mygem" }
+  let(:version) { "1.2.3" }
+  let(:platform) { "ruby" }
+
+  subject { described_class.new(name, version, platform) }
+
+  it "returns the name" do
+    expect(subject.name).to eq(name)
+  end
+
+  it "returns the version" do
+    expect(subject.version).to eq(version)
+  end
+
+  context "when the platform is mri" do
+    it "returns true" do
+      expect(subject.ruby?).to be_truthy
+    end
+  end
+
+  context "platform is jruby" do
+    let(:platform) { "java"}
+
+    it "returns false" do
+      expect(subject.ruby?).to be_falsey
+    end
+  end
+
+  it "return a meaningful string" do
+    expect(subject.to_s).to eq("#{name}-#{version}")
+  end
+end
diff --git a/tools/paquet/spec/paquet/gem_spec.rb b/tools/paquet/spec/paquet/gem_spec.rb
new file mode 100644
index 00000000000..c7c807f4acb
--- /dev/null
+++ b/tools/paquet/spec/paquet/gem_spec.rb
@@ -0,0 +1,67 @@
+# encoding: utf-8
+require "paquet/gem"
+require "stud/temporary"
+require "fileutils"
+
+describe Paquet::Gem do
+  let(:target_path) { Stud::Temporary.pathname }
+  let(:dummy_gem) { "dummy-gem" }
+
+  subject { described_class.new(target_path) }
+
+  it "adds gem to pack" do
+    subject.add(dummy_gem)
+    expect(subject.gems).to include(dummy_gem)
+  end
+
+  it "allows to ignore gems" do
+    subject.ignore(dummy_gem)
+    expect(subject.ignore?(dummy_gem))
+  end
+
+  it "keeps track of the number of gem to pack" do
+    expect { subject.add(dummy_gem) }.to change { subject.size }.by(1)
+  end
+
+  context "when not configuring cache" do
+    it "use_cache? returns false" do
+      expect(subject.use_cache?).to be_falsey
+    end
+  end
+
+  context "when configuring cache" do
+    let(:cache_path) do
+      p = Stud::Temporary.pathname
+      FileUtils.mkdir_p(p)
+      p
+    end
+
+    subject { described_class.new(target_path, cache_path) }
+
+    it "use_cache? returns true" do
+      expect(subject.use_cache?).to be_truthy
+    end
+
+    context "#find_in_cache" do
+      let(:gem_full_name) { "super-lib-0.1.0.gem" }
+
+      context "when the gem is in cache directory" do
+        let(:gem_file_path) { File.join(cache_path, gem_full_name) }
+
+        before do
+          FileUtils.touch(gem_file_path)
+        end
+
+        it "returns true" do
+          expect(subject.find_in_cache(gem_full_name)).to match(gem_file_path)
+        end
+      end
+
+      context "when the gem is not in the cache directory" do
+        it "returns false" do
+          expect(subject.find_in_cache(gem_full_name)).to be_falsey
+        end
+      end
+    end
+  end
+end
diff --git a/tools/paquet/spec/paquet/shell_ui_spec.rb b/tools/paquet/spec/paquet/shell_ui_spec.rb
new file mode 100644
index 00000000000..398274e9445
--- /dev/null
+++ b/tools/paquet/spec/paquet/shell_ui_spec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "paquet/shell_ui"
+
+describe Paquet::ShellUi do
+  let(:message) { "hello world" }
+
+  subject { described_class.new }
+
+  context "when debug is on" do
+    before :all do
+      @debug = ENV["debug"]
+      ENV["DEBUG"] = "1"
+    end
+
+    after :all do
+      ENV["DEBUG"] = @debug
+    end
+
+    it "show the debug statement" do
+      expect(subject).to receive(:puts).with("[DEBUG]: #{message}")
+      subject.debug(message)
+    end
+  end
+
+  context "not in debug" do
+    before :all do
+      @debug = ENV["debug"]
+      ENV["DEBUG"] = nil
+    end
+
+    after :all do
+      ENV["DEBUG"] = @debug
+    end
+
+    it "doesnt show the debug statement" do
+      expect(subject).not_to receive(:puts).with("[DEBUG]: #{message}")
+      subject.debug(message)
+    end
+  end
+end
diff --git a/tools/paquet/spec/paquet/utils_spec.rb b/tools/paquet/spec/paquet/utils_spec.rb
new file mode 100644
index 00000000000..d2c0f207408
--- /dev/null
+++ b/tools/paquet/spec/paquet/utils_spec.rb
@@ -0,0 +1,93 @@
+# encoding: utf-8
+require "paquet/utils"
+require "stud/temporary"
+require "spec_helper"
+
+describe Paquet::Utils do
+  subject { described_class }
+
+  let(:url) { "https://localhost:8898/my-file.txt"}
+  let(:destination) do
+    p = Stud::Temporary.pathname
+    FileUtils.mkdir_p(p)
+    File.join(p, "tmp-file")
+  end
+
+  let(:content) { "its halloween, halloween!" }
+
+  context "when the file exist" do
+    before do
+      stub_request(:get, url).to_return(
+        { :status => 200,
+          :body => content,
+          :headers => {}}
+      )
+    end
+
+    it "download the file to local temporary file" do
+      expect(File.read(subject.download_file(url, destination))).to match(content)
+    end
+
+    context "when an exception occur" do
+      let(:temporary_path) { Stud::Temporary.pathname }
+
+      before do
+        expect(URI).to receive(:parse).with(anything).and_raise("something went wrong")
+      end
+
+      it "deletes the temporary file" do
+        subject.download_file(url, destination) rescue nil
+        expect(File.exist?(destination)).to be_falsey
+      end
+    end
+  end
+
+  context "on redirection" do
+    let(:redirect_response) { instance_double("Net::HTTP::Response", :code => "302", :headers => { "location" => "https://localhost:8888/new_path" }) }
+    let(:response_ok) { instance_double("Net::HTTP::Response", :code => "200") }
+
+    context "less than the maximum of redirection" do
+      let(:redirect_url) { "https://localhost:8898/redirect/my-file.txt"}
+
+      before do
+        stub_request(:get, url).to_return(
+          { :status => 302, :headers => { "location" => redirect_url }}
+        )
+
+        stub_request(:get, url).to_return(
+          { :status => 200, :body => content }
+        )
+      end
+
+      it "follows the redirection" do
+        expect(File.read(subject.download_file(url, destination))).to match(content)
+      end
+    end
+
+    context "too many redirection" do
+      before do
+        stub_request(:get, url).to_return(
+          { :status => 302, :headers => { "location" => url }}
+        )
+      end
+
+      it "raises an exception" do
+        expect { subject.download_file(url, destination) }.to raise_error(/Too many redirection/)
+      end
+    end
+  end
+
+  [404, 400, 401, 500].each do |code|
+    context "When the server return #{code}" do
+      before do
+        stub_request(:get, url).to_return(
+          { :status => code }
+        )
+      end
+
+      it "raises an exception" do
+        expect { subject.download_file(url, destination) }.to raise_error(/Response not handled/)
+      end
+    end
+  end
+end
diff --git a/tools/paquet/spec/spec_helper.rb b/tools/paquet/spec/spec_helper.rb
new file mode 100644
index 00000000000..a2d0aa45d42
--- /dev/null
+++ b/tools/paquet/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "webmock/rspec"
diff --git a/tools/paquet/spec/support/Gemfile b/tools/paquet/spec/support/Gemfile
new file mode 100644
index 00000000000..884cbedd8cc
--- /dev/null
+++ b/tools/paquet/spec/support/Gemfile
@@ -0,0 +1,4 @@
+source "https://rubygems.org"
+gemspec
+gem "paquet", :path => "../../"
+
diff --git a/tools/paquet/spec/support/Rakefile b/tools/paquet/spec/support/Rakefile
new file mode 100644
index 00000000000..5e8040d8521
--- /dev/null
+++ b/tools/paquet/spec/support/Rakefile
@@ -0,0 +1,12 @@
+# encoding: utf-8
+require "paquet"
+
+TARGET_DIRECTORY = File.join(File.dirname(__FILE__), "dependencies")
+CACHE_PATH = File.join(File.dirname(__FILE__), "cache")
+
+Paquet::Task.new(TARGET_DIRECTORY, CACHE_PATH) do
+  pack "stud"
+  pack "flores"
+
+  ignore "logstash-devutils"
+end
diff --git a/tools/paquet/spec/support/paquet.gemspec b/tools/paquet/spec/support/paquet.gemspec
new file mode 100644
index 00000000000..668d109a74a
--- /dev/null
+++ b/tools/paquet/spec/support/paquet.gemspec
@@ -0,0 +1,17 @@
+# coding: utf-8
+
+Gem::Specification.new do |spec|
+  spec.name          = "paquet-test"
+  spec.version       = "0.0.0"
+  spec.authors       = ["Elastic"]
+  spec.email         = ["info@elastic.co"]
+  spec.license       = "Apache License (2.0)"
+
+  spec.summary       = %q{testing gem}
+  spec.description   = %q{testing gem}
+  spec.homepage      = "https://github.com/elastic/logstash"
+
+  spec.add_runtime_dependency "stud"
+  spec.add_runtime_dependency "flores", "0.0.6"
+  spec.add_runtime_dependency "logstash-devutils", "0.0.6"
+end
diff --git a/tools/release.sh b/tools/release.sh
deleted file mode 100644
index 435196a95cb..00000000000
--- a/tools/release.sh
+++ /dev/null
@@ -1,67 +0,0 @@
-#!/bin/bash
-
-logstash=$PWD
-contrib=$PWD/../logstash-contrib/
-
-workdir="$PWD/build/release/"
-mkdir -p $workdir
-
-# circuit breaker to fail if there's something silly wrong.
-if [ -z "$workdir" ] ; then
-  echo "workdir is empty?!"
-  exit 1
-fi
-
-if [ ! -d "$contrib" ] ; then
-  echo "Missing: $contrib"
-  echo "Maybe git clone it?"
-  exit 1
-fi
-
-set -e
-
-prepare() {
-  rsync -a --delete $logstash/{bin,docs,lib,spec,Makefile,gembag.rb,logstash.gemspec,tools,locales,patterns,LICENSE,README.md} $contrib/{lib,spec} $workdir
-  rm -f $logstash/.VERSION.mk
-  make -C $logstash .VERSION.mk
-  make -C $logstash tarball package
-  make -C $contrib tarball package
-  cp $logstash/.VERSION.mk $workdir
-  rm -f $workdir/build/pkg
-  rm -f $workdir/build/*.{zip,rpm,gz,deb} || true
-}
-
-docs() {
-  make -C $workdir build
-  (cd $contrib; find lib/logstash -type f -name '*.rb') > $workdir/build/contrib_plugins
-  make -C $workdir -j 4 docs
-}
-
-tests() {
-  make -C $logstash test QUIET=
-  make -C $logstash tarball test QUIET=
-}
-
-packages() {
-  for path in $logstash $contrib ; do
-    rm -f $path/build/*.tar.gz
-    rm -f $path/build/*.zip
-    echo "Building packages: $path"
-    make -C $path tarball
-    for dir in build pkg . ; do
-      [ ! -d "$path/$dir" ] && continue
-      (cd $path/$dir;
-        for i in *.gz *.rpm *.deb *.zip *.jar ; do
-          [ ! -f "$i" ] && continue
-          echo "Copying $path/$dir/$i"
-          cp $i $workdir/build
-        done
-      )
-    done
-  done
-}
-
-prepare
-tests
-docs
-packages
diff --git a/tools/upload.sh b/tools/upload.sh
deleted file mode 100644
index 72684486c8c..00000000000
--- a/tools/upload.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-
-basedir=$(dirname $0)/../
-bucket=download.elasticsearch.org
-
-s3cmd put -P $basedir/build/release/build/*.gz s3://${bucket}/logstash/logstash/
-s3cmd put -P $basedir/build/release/build/*.rpm s3://${bucket}/logstash/logstash/packages/centos/
-s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/debian
-s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/ubuntu
diff --git a/versions.yml b/versions.yml
new file mode 100644
index 00000000000..af8a2c8f622
--- /dev/null
+++ b/versions.yml
@@ -0,0 +1,4 @@
+---
+logstash: 5.4.3
+logstash-core: 5.4.3
+logstash-core-plugin-api: 2.1.12
