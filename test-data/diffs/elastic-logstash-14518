diff --git a/docs/static/monitoring/monitoring-apis.asciidoc b/docs/static/monitoring/monitoring-apis.asciidoc
index 97050e765b2..b7040fca7d8 100644
--- a/docs/static/monitoring/monitoring-apis.asciidoc
+++ b/docs/static/monitoring/monitoring-apis.asciidoc
@@ -315,6 +315,9 @@ Gets process stats, including stats about file descriptors, memory consumption,
 <<event-stats,`events`>>::
 Gets event-related statistics for the Logstash instance (regardless of how many
 pipelines were created and destroyed).
+<<flow-stats,`flow`>>::
+Gets flow-related statistics for the Logstash instance (regardless of how many
+pipelines were created and destroyed).
 <<pipeline-stats,`pipelines`>>::
 Gets runtime stats about each Logstash pipeline.
 <<reload-stats,`reloads`>>::
@@ -454,6 +457,85 @@ Example response:
   }
 --------------------------------------------------
 
+[discrete]
+[[flow-stats]]
+==== Flow stats
+
+The following request returns a JSON document containing flow-rates
+for the Logstash instance:
+
+[source,js]
+--------------------------------------------------
+curl -XGET 'localhost:9600/_node/stats/flow?pretty'
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "flow" : {
+    "input_throughput" : {
+      "current": 189.720,
+      "lifetime": 201.841
+    },
+    "filter_throughput" : {
+      "current": 187.810,
+      "lifetime": 201.799
+    },
+    "output_throughput" : {
+      "current": 191.087,
+      "lifetime": 201.761
+    },
+    "queue_backpressure" : {
+      "current": 0.277,
+      "lifetime": 0.031
+    },
+    "worker_concurrency" : {
+      "current": 1.973,
+      "lifetime": 1.721
+    }
+  }
+}
+--------------------------------------------------
+
+Flow rates provide visibility into how a Logstash instance or an individual pipeline is _currently_ performing relative to _itself_ over time.
+This allows us to attach _meaning_ to the cumulative-value metrics that are also presented by this API, and to determine whether an instance or pipeline is behaving better or worse than it has in the past.
+
+[%autowidth.stretch]
+|===
+|Flow Rate | Definition
+
+| `input_throughput` |
+This metric is expressed in events-per-second, and is the rate of events being pushed into the pipeline(s) queue(s) relative to wall-clock time (`events.in` / second).
+It includes events that are blocked by the queue and have not yet been accepted.
+
+| `filter_throughput` |
+This metric is expressed in events-per-second, and is the rate of events flowing through the filter phase of the pipeline(s) relative to wall-clock time (`events.filtered` / second).
+
+| `output_throughput` |
+This metric is expressed in events-per-second, and is the rate of events flowing through the output phase of the pipeline(s) relative to wall-clock time (`events.out` / second).
+
+| `worker_concurrency` |
+This is a unitless metric representing the cumulative time spent by all workers relative to wall-clock time (`duration_in_millis` / millisecond).
+
+A _pipeline_ is considered "saturated" when its `worker_concurrency` flow metric approaches its available `pipeline.workers`, because it indicates that all of its available workers are being kept busy.
+Tuning a saturated pipeline to have more workers can often work to increase that pipeline's throughput and decrease back-pressure to its queue, unless the pipeline is experiencing back-pressure from its outputs.
+
+A _process_ is also considered "saturated" when its top-level `worker_concurrency` flow metric approaches the _cumulative_ `pipeline.workers` across _all_ pipelines, and similarly can be addressed by tuning the <<pipeline-stats,individual pipelines>> that are saturated.
+
+| `queue_backpressure` |
+This is a unitless metric representing the cumulative time spent by all inputs blocked pushing events into their pipeline's queue, relative to wall-clock time (`queue_push_duration_in_millis` / millisecond).
+It is typically most useful when looking at the stats for an <<pipeline-stats,individual pipeline>>.
+
+While a "zero" value indicates no back-pressure to the queue, the magnitude of this metric is highly dependent on the _shape_ of the pipelines and their inputs.
+It cannot be used to compare one pipeline to another or even one process to _itself_ if the quantity or shape of its pipelines changes.
+A pipeline with only one single-threaded input may contribute up to 1.00, a pipeline whose inputs have hundreds of inbound connections may contribute much higher numbers to this combined value.
+
+Additionally, some amount of back-pressure is both _normal_ and _expected_ for pipelines that are _pulling_ data, as this back-pressure allows them to slow down and pull data at a rate its downstream pipeline can tolerate.
+
+|===
+
 [discrete]
 [[pipeline-stats]]
 ==== Pipeline stats
@@ -462,6 +544,7 @@ The following request returns a JSON document containing pipeline stats,
 including:
 
 * the number of events that were input, filtered, or output by each pipeline
+* the current and lifetime <<flow-stats,_flow_ rates>> for each pipeline
 * stats for each configured filter or output stage
 * info about config reload successes and failures
 (when <<reloading-config,config reload>> is enabled)
@@ -487,6 +570,28 @@ Example response:
         "out" : 216485,
         "queue_push_duration_in_millis" : 342466
       },
+      "flow" : {
+        "input_throughput" : {
+          "current": 189.720,
+          "lifetime": 201.841
+        },
+        "filter_throughput" : {
+          "current": 187.810,
+          "lifetime": 201.799
+        },
+        "output_throughput" : {
+          "current": 191.087,
+          "lifetime": 201.761
+        },
+        "queue_backpressure" : {
+          "current": 0.277,
+          "lifetime": 0.031
+        },
+        "worker_concurrency" : {
+          "current": 1.973,
+          "lifetime": 1.721
+        }
+      },
       "plugins" : {
         "inputs" : [ {
           "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-1",
@@ -546,6 +651,28 @@ Example response:
         "out" : 87247,
         "queue_push_duration_in_millis" : 1532
       },
+      "flow" : {
+        "input_throughput" : {
+          "current": 189.720,
+          "lifetime": 201.841
+        },
+        "filter_throughput" : {
+          "current": 187.810,
+          "lifetime": 201.799
+        },
+        "output_throughput" : {
+          "current": 191.087,
+          "lifetime": 201.761
+        },
+        "queue_backpressure" : {
+          "current": 0.871,
+          "lifetime": 0.031
+        },
+        "worker_concurrency" : {
+          "current": 4.71,
+          "lifetime": 1.201
+        }
+      },
       "plugins" : {
         "inputs" : [ {
           "id" : "d7ea8941c0fc48ac58f89c84a9da482107472b82-1",
@@ -601,6 +728,28 @@ Example response:
         "out" : 216485,
         "queue_push_duration_in_millis" : 342466
       },
+      "flow" : {
+        "input_throughput" : {
+          "current": 189.720,
+          "lifetime": 201.841
+        },
+        "filter_throughput" : {
+          "current": 187.810,
+          "lifetime": 201.799
+        },
+        "output_throughput" : {
+          "current": 191.087,
+          "lifetime": 201.761
+        },
+        "queue_backpressure" : {
+          "current": 0.277,
+          "lifetime": 0.031
+        },
+        "worker_concurrency" : {
+          "current": 1.973,
+          "lifetime": 1.721
+        }
+      },
       "plugins" : {
         "inputs" : [ {
           "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-1",
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 4080e9b0e6a..b6e72cf9e3c 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -92,6 +92,7 @@ def initialize(settings = LogStash::SETTINGS, source_loader = nil)
     @pipeline_reload_metric = metric.namespace([:stats, :pipelines])
     @instance_reload_metric = metric.namespace([:stats, :reloads])
     initialize_agent_metrics
+    initialize_flow_metrics
 
     initialize_geoip_database_metrics(metric)
     
@@ -534,6 +535,51 @@ def initialize_agent_metrics
     @instance_reload_metric.increment(:failures, 0)
   end
 
+  def initialize_flow_metrics
+    if collect_metrics? && metric.collector
+
+      java_import org.logstash.instrument.metrics.UptimeMetric
+      java_import org.logstash.instrument.metrics.UptimeMetric::ScaleUnits
+
+      uptime_metric = UptimeMetric.new
+      uptime_precise_millis = uptime_metric.with_units_precise(ScaleUnits::MILLISECONDS)
+      uptime_precise_seconds = uptime_metric.with_units_precise(ScaleUnits::SECONDS)
+
+      events_namespace = metric.namespace([:stats,:events])
+      flow_metrics = []
+      flow_metrics << create_flow_metric("input_throughput", get_counter(events_namespace, :in), uptime_precise_seconds)
+      flow_metrics << create_flow_metric("filter_throughput", get_counter(events_namespace, :out), uptime_precise_seconds)
+      flow_metrics << create_flow_metric("output_throughput", get_counter(events_namespace, :filtered), uptime_precise_seconds)
+      flow_metrics << create_flow_metric("queue_backpressure", get_counter(events_namespace, :queue_push_duration_in_millis), uptime_precise_millis)
+      flow_metrics << create_flow_metric("worker_concurrency", get_counter(events_namespace, :duration_in_millis), uptime_precise_millis)
+
+      registered, unregistered = flow_metrics.partition do |flow_metric|
+        @metric.collector.register?([:stats,:flow], flow_metric.name.to_sym, flow_metric)
+      end
+
+      unregistered.each do |unregistered_flow_metric|
+        logger.warn("Failed to register global flow metric #{unregistered_flow_metric.name}.")
+      end
+
+      @flow_metrics = registered.freeze
+    end
+  end
+
+  def get_counter(namespace, key)
+    org.logstash.instrument.metrics.counter.LongCounter.fromRubyBase(namespace, key)
+  end
+  private :get_counter
+
+  def create_flow_metric(name, numerator_metric, denominator_metric)
+    org.logstash.instrument.metrics.FlowMetric.new(name, numerator_metric, denominator_metric)
+  end
+  private :create_flow_metric
+
+  def capture_flow_metrics
+    @flow_metrics&.each(&:capture)
+  end
+  public :capture_flow_metrics
+
   def initialize_pipeline_metrics(action)
     @pipeline_reload_metric.namespace([action.pipeline_id, :reloads]).tap do |n|
       n.increment(:successes, 0)
diff --git a/logstash-core/lib/logstash/api/commands/stats.rb b/logstash-core/lib/logstash/api/commands/stats.rb
index 4c52857b3bd..4b6f1a74a69 100644
--- a/logstash-core/lib/logstash/api/commands/stats.rb
+++ b/logstash-core/lib/logstash/api/commands/stats.rb
@@ -81,6 +81,16 @@ def events
           {}
         end
 
+        def flow
+          extract_metrics(
+            [:stats,:flow],
+            :input_throughput, :filter_throughput, :output_throughput, :queue_backpressure, :worker_concurrency
+          )
+        rescue LogStash::Instrument::MetricStore::MetricNotFound
+          # if the stats/events metrics have not yet been populated, return an empty map
+          {}
+        end
+
         def pipeline(pipeline_id = nil, opts={})
           extended_stats = LogStash::Config::PipelinesInfo.format_pipelines_info(
             service.agent,
@@ -165,6 +175,7 @@ def plugin_stats(stats, plugin_type)
           def report(stats, extended_stats=nil, opts={})
             ret = {
               :events => stats[:events],
+              :flow => stats[:flow],
               :plugins => {
                 :inputs => plugin_stats(stats, :inputs),
                 :codecs => plugin_stats(stats, :codecs),
diff --git a/logstash-core/lib/logstash/api/modules/node_stats.rb b/logstash-core/lib/logstash/api/modules/node_stats.rb
index 5f1ffae2ba2..2bd017458fc 100644
--- a/logstash-core/lib/logstash/api/modules/node_stats.rb
+++ b/logstash-core/lib/logstash/api/modules/node_stats.rb
@@ -35,6 +35,7 @@ class NodeStats < ::LogStash::Api::Modules::Base
             :jvm => jvm_payload,
             :process => process_payload,
             :events => events_payload,
+            :flow => flow_payload,
             :pipelines => pipeline_payload,
             :reloads => reloads_payload,
             :os => os_payload,
@@ -61,6 +62,10 @@ def events_payload
           @stats.events
         end
 
+        def flow_payload
+          @stats.flow
+        end
+
         def jvm_payload
           @stats.jvm
         end
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
index 797dd471371..355a816e059 100644
--- a/logstash-core/lib/logstash/instrument/collector.rb
+++ b/logstash-core/lib/logstash/instrument/collector.rb
@@ -67,6 +67,28 @@ def get(namespaces_path, key, type)
       end
     end
 
+    ##
+    # Ensures that a metric on the provided `namespaces_path` with the provided `key`
+    # is registered, using the provided `metric_instance` IFF it is not already present.
+    #
+    # @param namespaces_path [Array<Symbol>]
+    # @param key [Symbol]
+    # @param metric_instance [Metric]
+    #
+    # @return [Boolean] true IFF the provided `metric_instance` was registered
+    def register?(namespaces_path, key, metric_instance)
+      registered = false
+
+      # Relies on MetricStore#fetch_or_store yielding the block
+      # EXACTLY ONCE to the winner in a race-condition.
+      @metric_store.fetch_or_store(namespaces_path, key) do
+        registered = true
+        metric_instance
+      end
+
+      registered
+    end
+
     # Snapshot the current Metric Store and return it immediately,
     # This is useful if you want to get access to the current metric store without
     # waiting for a periodic call.
diff --git a/logstash-core/lib/logstash/instrument/metric_store.rb b/logstash-core/lib/logstash/instrument/metric_store.rb
index eaac28a9c2b..1bb9c3e6ab5 100644
--- a/logstash-core/lib/logstash/instrument/metric_store.rb
+++ b/logstash-core/lib/logstash/instrument/metric_store.rb
@@ -51,11 +51,20 @@ def initialize
 
     # This method use the namespace and key to search the corresponding value of
     # the hash, if it doesn't exist it will create the appropriate namespaces
-    # path in the hash and return `new_value`
-    #
-    # @param [Array] The path where the values should be located
-    # @param [Symbol] The metric key
-    # @return [Object] Return the new_value of the retrieve object in the tree
+    # path in the hash and return `new_value`.
+    # @overload fetch_or_store(namespaces, key, default_value)
+    #   @param [Array<Symbol>] namespaces: The path where the values should be located
+    #   @param [Symbol] key: The metric key
+    #   @param [Metric] default_value: if no metric exists at this address, the
+    #                                  provided default_value will be stored
+    #   @return [Metric] the value as it exists in the tree after this operation
+    # @overload fetch_or_store(namespaces, key, &default_value_generator)
+    #   @param [Array<Symbol>] namespaces: The path where the values should be located
+    #   @param [Symbol] key: The metric key
+    #   @yield EXACTLY ONCE to the provided block IFF the metric does not exist
+    #   @yieldreturn [Metric] if no metric exists at this address, the result of yielding
+    #                         to the provided default_value_generator block will be stored.
+    #   @return [Metric] the value as it exists in the tree after this operation
     def fetch_or_store(namespaces, key, default_value = nil)
 
       # We first check in the `@fast_lookup` store to see if we have already see that metrics before,
@@ -63,20 +72,21 @@ def fetch_or_store(namespaces, key, default_value = nil)
       # data store (Which is a `o(n)` operation where `n` is the number of element in the namespace and
       # the value of the key). If the metric is already present in the `@fast_lookup`, then that value is sent
       # back directly to the caller.
-      #
-      # BUT. If the value is not present in the `@fast_lookup` the value will be inserted and we assume that we don't
-      # have it in the `@metric_store` for structured search so we add it there too.
-
-      value = @fast_lookup.get(namespaces.dup << key)
-      if value.nil?
-        value = block_given? ? yield(key) : default_value
-        @fast_lookup.put(namespaces.dup << key, value)
-        @structured_lookup_mutex.synchronize do
-            # If we cannot find the value this mean we need to save it in the store.
-          fetch_or_store_namespaces(namespaces).fetch_or_store(key, value)
+      fast_lookup_key = namespaces.dup << key
+      existing_value = @fast_lookup.get(fast_lookup_key)
+      return existing_value unless existing_value.nil?
+
+      # BUT. If the value was not present in the `@fast_lookup` we acquire the structured_lookup_lock
+      # before modifying _either_ the fast-lookup or the structured store.
+      @structured_lookup_mutex.synchronize do
+        # by using compute_if_absent, we ensure that we don't overwrite a value that was
+        # written by another thread that beat us to the @structured_lookup_mutex lock.
+        @fast_lookup.compute_if_absent(fast_lookup_key) do
+          generated_value = block_given? ? yield(key) : default_value
+          fetch_or_store_namespaces(namespaces).fetch_or_store(key, generated_value)
+          generated_value
         end
       end
-      return value;
     end
 
     # This method allow to retrieve values for a specific path,
@@ -298,36 +308,18 @@ def transform_to_hash(map, new_hash = Hash.new)
     # create it.
     #
     # @param [Array] The path where values should be located
-    # @raise [ConcurrentMapExpected] Raise if the retrieved object isn't a `Concurrent::Map`
+    # @raise [NamespacesExpectedError] Raise if the retrieved object isn't a `Concurrent::Map`
     # @return [Concurrent::Map] Map where the metrics should be saved
     def fetch_or_store_namespaces(namespaces_path)
-      path_map = fetch_or_store_namespace_recursively(@store, namespaces_path)
-
-      # This mean one of the namespace and key are colliding
-      # and we have to deal it upstream.
-      unless path_map.is_a?(Concurrent::Map)
-        raise NamespacesExpectedError, "Expecting a `Namespaces` but found class:  #{path_map.class.name} for namespaces_path: #{namespaces_path}"
-      end
-
-      return path_map
-    end
-
-    # Recursively fetch or create the namespace paths through the `MetricStove`
-    # This algorithm use an index to known which keys to search in the map.
-    # This doesn't cloning the array if we want to give a better feedback to the user
-    #
-    # @param [Concurrent::Map] Map to search for the key
-    # @param [Array] List of path to create
-    # @param [Integer] Which part from the list to create
-    #
-    def fetch_or_store_namespace_recursively(map, namespaces_path, idx = 0)
-      current = namespaces_path[idx]
+      namespaces_path.each_with_index.reduce(@store) do |memo, (current, index)|
+        node = memo.compute_if_absent(current) { Concurrent::Map.new }
 
-      # we are at the end of the namespace path, break out of the recursion
-      return map if current.nil?
+        unless node.kind_of?(Concurrent::Map)
+          raise NamespacesExpectedError, "Expecting a `Namespaces` but found class:  #{node.class.name} for namespaces_path: #{namespaces_path.first(index+1)}"
+        end
 
-      new_map = map.fetch_or_store(current) { Concurrent::Map.new }
-      return fetch_or_store_namespace_recursively(new_map, namespaces_path, idx + 1)
+        node
+      end
     end
 
     def delete_from_map(map, keys)
diff --git a/logstash-core/lib/logstash/instrument/metric_type.rb b/logstash-core/lib/logstash/instrument/metric_type.rb
index 395323658b8..5fc7b4ec35f 100644
--- a/logstash-core/lib/logstash/instrument/metric_type.rb
+++ b/logstash-core/lib/logstash/instrument/metric_type.rb
@@ -17,12 +17,14 @@
 
 require "logstash/instrument/metric_type/counter"
 require "logstash/instrument/metric_type/gauge"
+require "logstash/instrument/metric_type/uptime"
 
 module LogStash module Instrument
   module MetricType
     METRIC_TYPE_LIST = {
       :counter => LogStash::Instrument::MetricType::Counter,
-      :gauge => LogStash::Instrument::MetricType::Gauge
+      :gauge   => LogStash::Instrument::MetricType::Gauge,
+      :uptime  => LogStash::Instrument::MetricType::Uptime,
     }.freeze
 
     # Use the string to generate a concrete class for this metrics
diff --git a/logstash-core/lib/logstash/instrument/metric_type/uptime.rb b/logstash-core/lib/logstash/instrument/metric_type/uptime.rb
new file mode 100644
index 00000000000..0fe3803b9ac
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/metric_type/uptime.rb
@@ -0,0 +1,32 @@
+# Licensed to Elasticsearch B.V. under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Elasticsearch B.V. licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#  http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+java_import org.logstash.instrument.metrics.UptimeMetric
+
+module LogStash module Instrument module MetricType
+  class Uptime < UptimeMetric
+
+    def initialize(namespaces, key)
+      super(key.to_s)
+    end
+
+    def execute(action, value = nil)
+      fail("Unsupported operation `action` on Uptime Metric")
+    end
+
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/flow_rate.rb b/logstash-core/lib/logstash/instrument/periodic_poller/flow_rate.rb
new file mode 100644
index 00000000000..de867ade672
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/flow_rate.rb
@@ -0,0 +1,35 @@
+# Licensed to Elasticsearch B.V. under one or more contributor
+# license agreements. See the NOTICE file distributed with
+# this work for additional information regarding copyright
+# ownership. Elasticsearch B.V. licenses this file to you under
+# the Apache License, Version 2.0 (the "License"); you may
+# not use this file except in compliance with the License.
+# You may obtain a copy of the License at
+#
+#  http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing,
+# software distributed under the License is distributed on an
+# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+# KIND, either express or implied.  See the License for the
+# specific language governing permissions and limitations
+# under the License.
+
+require 'logstash/instrument/periodic_poller/base'
+
+module LogStash module Instrument module PeriodicPoller
+  class FlowRate < Base
+    def initialize(metric, agent, options = {})
+      super(metric, options)
+      @metric = metric
+      @agent = agent
+    end
+
+    def collect
+      @agent.capture_flow_metrics
+
+      pipelines = @agent.running_user_defined_pipelines
+      pipelines.values.compact.each(&:collect_flow_metrics)
+    end
+  end
+end end end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/instrument/periodic_pollers.rb b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
index 345dc7c3d6d..2fae2629f3a 100644
--- a/logstash-core/lib/logstash/instrument/periodic_pollers.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
@@ -19,6 +19,7 @@
 require "logstash/instrument/periodic_poller/os"
 require "logstash/instrument/periodic_poller/jvm"
 require "logstash/instrument/periodic_poller/pq"
+require "logstash/instrument/periodic_poller/flow_rate"
 
 module LogStash module Instrument
   # Each PeriodPoller manager his own thread to do the poller
@@ -32,7 +33,8 @@ def initialize(metric, queue_type, agent)
       @periodic_pollers = [PeriodicPoller::Os.new(metric),
                            PeriodicPoller::JVM.new(metric),
                            PeriodicPoller::PersistentQueue.new(metric, queue_type, agent),
-                           PeriodicPoller::DeadLetterQueue.new(metric, agent)]
+                           PeriodicPoller::DeadLetterQueue.new(metric, agent),
+                           PeriodicPoller::FlowRate.new(metric, agent)]
     end
 
     def start
diff --git a/logstash-core/lib/logstash/java_pipeline.rb b/logstash-core/lib/logstash/java_pipeline.rb
index 04063782faa..401daebf914 100644
--- a/logstash-core/lib/logstash/java_pipeline.rb
+++ b/logstash-core/lib/logstash/java_pipeline.rb
@@ -121,6 +121,7 @@ def start
     # this is useful in the context of pipeline reloading
     collect_stats
     collect_dlq_stats
+    initialize_flow_metrics
 
     @logger.debug("Starting pipeline", default_logging_keys)
 
@@ -533,6 +534,7 @@ def clear_pipeline_metrics
       # we want to keep other metrics like reload counts and error messages
       collector.clear("stats/pipelines/#{pipeline_id}/plugins")
       collector.clear("stats/pipelines/#{pipeline_id}/events")
+      collector.clear("stats/pipelines/#{pipeline_id}/flow")
     end
   end
 
diff --git a/logstash-core/spec/logstash/agent/metrics_spec.rb b/logstash-core/spec/logstash/agent/metrics_spec.rb
index 5e14b007d42..04ce55352f5 100644
--- a/logstash-core/spec/logstash/agent/metrics_spec.rb
+++ b/logstash-core/spec/logstash/agent/metrics_spec.rb
@@ -73,6 +73,15 @@ def mhash(*path_elements)
       expect(mval(:stats, :reloads, :successes)).to eq(0)
       expect(mval(:stats, :reloads, :failures)).to eq(0)
     end
+
+    it "makes the top-level flow metrics available" do
+      expect(mval(:stats, :flow, :input_throughput)).to be_a_kind_of(java.util.Map)
+      expect(mval(:stats, :flow, :output_throughput)).to be_a_kind_of(java.util.Map)
+      expect(mval(:stats, :flow, :filter_throughput)).to be_a_kind_of(java.util.Map)
+      expect(mval(:stats, :flow, :queue_backpressure)).to be_a_kind_of(java.util.Map)
+      expect(mval(:stats, :flow, :worker_concurrency)).to be_a_kind_of(java.util.Map)
+    end
+
   end
 
   context "when we try to start one pipeline" do
@@ -245,14 +254,16 @@ def mhash(*path_elements)
         # so we try a few times
         try(20) do
           expect { mhash(:stats, :pipelines, :main, :events) }.not_to raise_error , "Events pipeline stats should exist"
+          expect { mhash(:stats, :pipelines, :main, :flow) }.not_to raise_error , "Events pipeline stats should exist"
           expect { mhash(:stats, :pipelines, :main, :plugins) }.not_to raise_error, "Plugins pipeline stats should exist"
         end
 
         expect(subject.converge_state_and_update.success?).to be_truthy
 
         # We do have to retry here, since stopping a pipeline is a blocking operation
-        expect { mhash(:stats, :pipelines, :main, :plugins) }.to raise_error
-        expect { mhash(:stats, :pipelines, :main, :events) }.to raise_error
+        expect { mhash(:stats, :pipelines, :main, :plugins) }.to raise_error LogStash::Instrument::MetricStore::MetricNotFound
+        expect { mhash(:stats, :pipelines, :main, :flow) }.to raise_error LogStash::Instrument::MetricStore::MetricNotFound
+        expect { mhash(:stats, :pipelines, :main, :events) }.to raise_error LogStash::Instrument::MetricStore::MetricNotFound
       end
     end
   end
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index 0575bdfd46b..f5d6405ec8a 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -476,6 +476,28 @@
           expect(value).to be > initial_generator_threshold
         end
 
+        it "does not reset the global flow metrics" do
+          snapshot = subject.metric.collector.snapshot_metric
+          subject.capture_flow_metrics
+
+          flow_metrics = snapshot.metric_store.get_with_path("/stats/flow")[:stats][:flow]
+
+          input_throughput_current = flow_metrics[:input_throughput].value.get("current")
+          input_throughput_lifetime = flow_metrics[:input_throughput].value.get("lifetime")
+          filter_throughput_current = flow_metrics[:filter_throughput].value.get("current")
+          filter_throughput_lifetime = flow_metrics[:filter_throughput].value.get("lifetime")
+          worker_concurrency_current = flow_metrics[:worker_concurrency].value.get("current")
+          worker_concurrency_lifetime = flow_metrics[:worker_concurrency].value.get("lifetime")
+
+          # rates depend on [events/wall-clock time], the expectation is non-zero values
+          expect(input_throughput_current).to be > 0
+          expect(input_throughput_lifetime).to be > 0
+          expect(filter_throughput_current).to be > 0
+          expect(filter_throughput_lifetime).to be > 0
+          expect(worker_concurrency_current).to be > 0
+          expect(worker_concurrency_lifetime).to be > 0
+        end
+
         it "increases the successful reload count" do
           skip("This test fails randomly, tracked in https://github.com/elastic/logstash/issues/8005")
           snapshot = subject.metric.collector.snapshot_metric
diff --git a/logstash-core/spec/logstash/api/commands/stats_spec.rb b/logstash-core/spec/logstash/api/commands/stats_spec.rb
index a8877fdfc11..f7ecb0ebe7d 100644
--- a/logstash-core/spec/logstash/api/commands/stats_spec.rb
+++ b/logstash-core/spec/logstash/api/commands/stats_spec.rb
@@ -75,6 +75,19 @@
     end
   end
 
+  describe "#metric flows" do
+    let(:report_method) { :flow }
+
+    it "should validate flow metric keys are exist" do
+      expect(report.keys).to include(
+                               :input_throughput,
+                               :output_throughput,
+                               :filter_throughput,
+                               :queue_backpressure,
+                               :worker_concurrency)
+    end
+  end
+
   describe "#hot_threads" do
     let(:report_method) { :hot_threads }
 
@@ -144,6 +157,7 @@
       it "returns information on pipeline" do
         expect(report[:main].keys).to include(
           :events,
+          :flow,
           :plugins,
           :reloads,
           :queue,
@@ -158,6 +172,15 @@
           :queue_push_duration_in_millis
         )
       end
+      it "returns flow metric information" do
+        expect(report[:main][:flow].keys).to include(
+                                                 :output_throughput,
+                                                 :filter_throughput,
+                                                 :queue_backpressure,
+                                                 :worker_concurrency,
+                                                 :input_throughput
+                                               )
+      end
     end
     context "when using multiple pipelines" do
       before(:each) do
diff --git a/logstash-core/spec/logstash/api/modules/node_stats_spec.rb b/logstash-core/spec/logstash/api/modules/node_stats_spec.rb
index 247ef51360b..ef3c0e50328 100644
--- a/logstash-core/spec/logstash/api/modules/node_stats_spec.rb
+++ b/logstash-core/spec/logstash/api/modules/node_stats_spec.rb
@@ -87,7 +87,21 @@
         "percent"=>Numeric,
         # load_average is not supported on Windows, set it below
       }
-    },
+   },
+   "events" => {
+      "duration_in_millis" => Numeric,
+      "in" => Numeric,
+      "filtered" => Numeric,
+      "out" => Numeric,
+      "queue_push_duration_in_millis" => Numeric
+   },
+   "flow" => {
+      "output_throughput" => Hash,
+      "filter_throughput" => Hash,
+      "queue_backpressure" => Hash,
+      "worker_concurrency" => Hash,
+      "input_throughput" => Hash
+   },
    "pipelines" => {
      "main" => {
        "events" => {
@@ -97,6 +111,13 @@
          "out" => Numeric,
          "queue_push_duration_in_millis" => Numeric
        },
+       "flow" => {
+         "output_throughput" => Hash,
+         "filter_throughput" => Hash,
+         "queue_backpressure" => Hash,
+         "worker_concurrency" => Hash,
+         "input_throughput" => Hash
+       },
        "plugins" => {
           "inputs" => Array,
           "codecs" => Array,
diff --git a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
index ece0df7b7c2..5e68297d831 100644
--- a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
@@ -28,8 +28,8 @@
 import java.security.NoSuchAlgorithmException;
 import java.time.temporal.ChronoUnit;
 import java.time.temporal.TemporalUnit;
-import java.util.ArrayList;
 import java.time.Duration;
+import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.List;
 import java.util.UUID;
@@ -69,12 +69,19 @@
 import org.logstash.ext.JRubyWrappedWriteClientExt;
 import org.logstash.instrument.metrics.AbstractMetricExt;
 import org.logstash.instrument.metrics.AbstractNamespacedMetricExt;
-import org.logstash.instrument.metrics.MetricKeys;
+import org.logstash.instrument.metrics.FlowMetric;
+import org.logstash.instrument.metrics.Metric;
 import org.logstash.instrument.metrics.NullMetricExt;
+import org.logstash.instrument.metrics.UptimeMetric;
+import org.logstash.instrument.metrics.counter.LongCounter;
 import org.logstash.plugins.ConfigVariableExpander;
 import org.logstash.secret.store.SecretStore;
 import org.logstash.secret.store.SecretStoreExt;
 
+import static org.logstash.instrument.metrics.MetricKeys.*;
+import static org.logstash.instrument.metrics.UptimeMetric.ScaleUnits.MILLISECONDS;
+import static org.logstash.instrument.metrics.UptimeMetric.ScaleUnits.SECONDS;
+
 /**
  * JRuby extension to provide ancestor class for Ruby's Pipeline and JavaPipeline classes.
  * */
@@ -91,45 +98,8 @@ public class AbstractPipelineExt extends RubyBasicObject {
     private static final @SuppressWarnings("rawtypes") RubyArray DATA_NAMESPACE =
         RubyArray.newArray(RubyUtil.RUBY, RubyUtil.RUBY.newSymbol("data"));
 
-    private static final RubySymbol PAGE_CAPACITY_IN_BYTES =
-        RubyUtil.RUBY.newSymbol("page_capacity_in_bytes");
-
-    private static final RubySymbol MAX_QUEUE_SIZE_IN_BYTES =
-        RubyUtil.RUBY.newSymbol("max_queue_size_in_bytes");
-
-    private static final RubySymbol MAX_QUEUE_UNREAD_EVENTS =
-        RubyUtil.RUBY.newSymbol("max_unread_events");
-
-    private static final RubySymbol QUEUE_SIZE_IN_BYTES =
-        RubyUtil.RUBY.newSymbol("queue_size_in_bytes");
-
-    private static final RubySymbol FREE_SPACE_IN_BYTES =
-        RubyUtil.RUBY.newSymbol("free_space_in_bytes");
-
-    private static final RubySymbol STORAGE_TYPE = RubyUtil.RUBY.newSymbol("storage_type");
-
-    private static final RubySymbol PATH = RubyUtil.RUBY.newSymbol("path");
-
-    private static final RubySymbol TYPE_KEY = RubyUtil.RUBY.newSymbol("type");
-
-    private static final RubySymbol QUEUE_KEY = RubyUtil.RUBY.newSymbol("queue");
-
-    private static final RubySymbol DLQ_KEY = RubyUtil.RUBY.newSymbol("dlq");
-
-    private static final RubySymbol STORAGE_POLICY =
-            RubyUtil.RUBY.newSymbol("storage_policy");
-
-    private static final RubySymbol DROPPED_EVENTS =
-            RubyUtil.RUBY.newSymbol("dropped_events");
-
-    private static final RubySymbol EXPIRED_EVENTS =
-            RubyUtil.RUBY.newSymbol("expired_events");
-
-    private static final RubySymbol LAST_ERROR =
-            RubyUtil.RUBY.newSymbol("last_error");
-
     private static final @SuppressWarnings("rawtypes") RubyArray EVENTS_METRIC_NAMESPACE = RubyArray.newArray(
-        RubyUtil.RUBY, new IRubyObject[]{MetricKeys.STATS_KEY, MetricKeys.EVENTS_KEY}
+        RubyUtil.RUBY, new IRubyObject[]{STATS_KEY, EVENTS_KEY}
     );
 
     @SuppressWarnings("serial")
@@ -164,6 +134,8 @@ public class AbstractPipelineExt extends RubyBasicObject {
 
     private QueueReadClientBase filterQueueClient;
 
+    private ArrayList<FlowMetric> flowMetrics = new ArrayList<>();
+
     public AbstractPipelineExt(final Ruby runtime, final RubyClass metaClass) {
         super(runtime, metaClass);
     }
@@ -215,7 +187,7 @@ public final AbstractPipelineExt initialize(final ThreadContext context,
     }
 
     /**
-     * queue opening needs to happen out of the the initialize method because the
+     * queue opening needs to happen out of the initialize method because the
      * AbstractPipeline is used for pipeline config validation and the queue
      * should not be opened for this. This should be called only in the actual
      * Pipeline/JavaPipeline initialisation.
@@ -240,8 +212,10 @@ public final IRubyObject openQueue(final ThreadContext context) {
                 RubyArray.newArray(
                     context.runtime,
                     new IRubyObject[]{
-                        MetricKeys.STATS_KEY, MetricKeys.PIPELINES_KEY,
-                        pipelineId.convertToString().intern(), MetricKeys.EVENTS_KEY
+                            STATS_KEY,
+                            PIPELINES_KEY,
+                            pipelineId.convertToString().intern(),
+                            EVENTS_KEY
                     }
                 )
             )
@@ -369,29 +343,24 @@ public final PipelineReporterExt reporter() {
     @JRubyMethod(name = "collect_dlq_stats")
     public final IRubyObject collectDlqStats(final ThreadContext context) {
         if (dlqEnabled(context).isTrue()) {
-            getDlqMetric(context).gauge(
-                context, QUEUE_SIZE_IN_BYTES,
-                dlqWriter(context).callMethod(context, "get_current_queue_size")
-            );
-            getDlqMetric(context).gauge(
-                    context, STORAGE_POLICY,
-                    dlqWriter(context).callMethod(context, "get_storage_policy")
-            );
-            getDlqMetric(context).gauge(
-                    context, MAX_QUEUE_SIZE_IN_BYTES,
-                    getSetting(context, "dead_letter_queue.max_bytes").convertToInteger());
-            getDlqMetric(context).gauge(
-                    context, DROPPED_EVENTS,
-                    dlqWriter(context).callMethod(context, "get_dropped_events")
-            );
-            getDlqMetric(context).gauge(
-                    context, LAST_ERROR,
-                    dlqWriter(context).callMethod(context, "get_last_error")
-            );
-            getDlqMetric(context).gauge(
-                    context, EXPIRED_EVENTS,
-                    dlqWriter(context).callMethod(context, "get_expired_events")
-            );
+            getDlqMetric(context).gauge(context,
+                                        QUEUE_SIZE_IN_BYTES_KEY,
+                                        dlqWriter(context).callMethod(context, "get_current_queue_size"));
+            getDlqMetric(context).gauge(context,
+                                        STORAGE_POLICY_KEY,
+                                        dlqWriter(context).callMethod(context, "get_storage_policy"));
+            getDlqMetric(context).gauge(context,
+                                        MAX_QUEUE_SIZE_IN_BYTES_KEY,
+                                        getSetting(context, "dead_letter_queue.max_bytes").convertToInteger());
+            getDlqMetric(context).gauge(context,
+                                        DROPPED_EVENTS_KEY,
+                                        dlqWriter(context).callMethod(context, "get_dropped_events"));
+            getDlqMetric(context).gauge(context,
+                                        LAST_ERROR_KEY,
+                                        dlqWriter(context).callMethod(context, "get_last_error"));
+            getDlqMetric(context).gauge(context,
+                                        EXPIRED_EVENTS_KEY,
+                                        dlqWriter(context).callMethod(context, "get_expired_events"));
         }
         return context.nil;
     }
@@ -408,45 +377,140 @@ public final IRubyObject isConfiguredReloadable(final ThreadContext context) {
 
     @JRubyMethod(name = "collect_stats")
     public final IRubyObject collectStats(final ThreadContext context) throws IOException {
-        final AbstractNamespacedMetricExt pipelineMetric = metric.namespace(
-            context,
-            RubyArray.newArray(
-                context.runtime,
-                Arrays.asList(MetricKeys.STATS_KEY, MetricKeys.PIPELINES_KEY, pipelineId.asString().intern(), QUEUE_KEY)
-            )
-        );
+        final AbstractNamespacedMetricExt pipelineMetric =
+                metric.namespace(context, pipelineNamespacedPath(QUEUE_KEY));
+
         pipelineMetric.gauge(context, TYPE_KEY, getSetting(context, "queue.type"));
         if (queue instanceof JRubyWrappedAckedQueueExt) {
             final JRubyAckedQueueExt inner = ((JRubyWrappedAckedQueueExt) queue).rubyGetQueue();
             final RubyString dirPath = inner.ruby_dir_path(context);
             final AbstractNamespacedMetricExt capacityMetrics =
                 pipelineMetric.namespace(context, CAPACITY_NAMESPACE);
-            capacityMetrics.gauge(
-                context, PAGE_CAPACITY_IN_BYTES, inner.ruby_page_capacity(context)
-            );
-            capacityMetrics.gauge(
-                context, MAX_QUEUE_SIZE_IN_BYTES, inner.ruby_max_size_in_bytes(context)
-            );
-            capacityMetrics.gauge(
-                context, MAX_QUEUE_UNREAD_EVENTS, inner.ruby_max_unread_events(context)
-            );
-            capacityMetrics.gauge(
-                context, QUEUE_SIZE_IN_BYTES, inner.ruby_persisted_size_in_bytes(context)
-            );
+
+            capacityMetrics.gauge(context, PAGE_CAPACITY_IN_BYTES_KEY, inner.ruby_page_capacity(context));
+            capacityMetrics.gauge(context, MAX_QUEUE_SIZE_IN_BYTES_KEY, inner.ruby_max_size_in_bytes(context));
+            capacityMetrics.gauge(context, MAX_QUEUE_UNREAD_EVENTS_KEY, inner.ruby_max_unread_events(context));
+            capacityMetrics.gauge(context, QUEUE_SIZE_IN_BYTES_KEY, inner.ruby_persisted_size_in_bytes(context));
+
             final AbstractNamespacedMetricExt dataMetrics =
                 pipelineMetric.namespace(context, DATA_NAMESPACE);
             final FileStore fileStore = Files.getFileStore(Paths.get(dirPath.asJavaString()));
-            dataMetrics.gauge(
-                context, FREE_SPACE_IN_BYTES,
-                context.runtime.newFixnum(fileStore.getUnallocatedSpace())
-            );
-            dataMetrics.gauge(context, STORAGE_TYPE, context.runtime.newString(fileStore.type()));
-            dataMetrics.gauge(context, PATH, dirPath);
-            pipelineMetric.gauge(context, MetricKeys.EVENTS_KEY, inner.ruby_unread_count(context));
+            dataMetrics.gauge(context, FREE_SPACE_IN_BYTES_KEY, context.runtime.newFixnum(fileStore.getUnallocatedSpace()));
+            dataMetrics.gauge(context, STORAGE_TYPE_KEY, context.runtime.newString(fileStore.type()));
+            dataMetrics.gauge(context, PATH_KEY, dirPath);
+
+            pipelineMetric.gauge(context, EVENTS_KEY, inner.ruby_unread_count(context));
         }
         return context.nil;
     }
 
+    @SuppressWarnings("DuplicatedCode") // as much as this is sub-par, refactoring makes it harder to read.
+    @JRubyMethod(name = "initialize_flow_metrics")
+    public final IRubyObject initializeFlowMetrics(final ThreadContext context) {
+        if (metric.collector(context).isNil()) { return context.nil; }
+
+        final UptimeMetric uptimeMetric = initOrGetUptimeMetric(context, buildNamespace(), UPTIME_IN_MILLIS_KEY);
+        final Metric<Number> uptimeInPreciseMillis = uptimeMetric.withUnitsPrecise(MILLISECONDS);
+        final Metric<Number> uptimeInPreciseSeconds = uptimeMetric.withUnitsPrecise(SECONDS);
+
+        final RubySymbol[] flowNamespace = buildNamespace(FLOW_KEY);
+        final RubySymbol[] eventsNamespace = buildNamespace(EVENTS_KEY);
+
+        final LongCounter eventsInCounter = initOrGetCounterMetric(context, eventsNamespace, IN_KEY);
+        final FlowMetric inputThroughput = createFlowMetric(INPUT_THROUGHPUT_KEY, eventsInCounter, uptimeInPreciseSeconds);
+        this.flowMetrics.add(inputThroughput);
+        storeMetric(context, flowNamespace, inputThroughput);
+
+        final LongCounter eventsFilteredCounter = initOrGetCounterMetric(context, eventsNamespace, FILTERED_KEY);
+        final FlowMetric filterThroughput = createFlowMetric(FILTER_THROUGHPUT_KEY, eventsFilteredCounter, uptimeInPreciseSeconds);
+        this.flowMetrics.add(filterThroughput);
+        storeMetric(context, flowNamespace, filterThroughput);
+
+        final LongCounter eventsOutCounter = initOrGetCounterMetric(context, eventsNamespace, OUT_KEY);
+        final FlowMetric outputThroughput = createFlowMetric(OUTPUT_THROUGHPUT_KEY, eventsOutCounter, uptimeInPreciseSeconds);
+        this.flowMetrics.add(outputThroughput);
+        storeMetric(context, flowNamespace, outputThroughput);
+
+        final LongCounter queuePushWaitInMillis = initOrGetCounterMetric(context, eventsNamespace, PUSH_DURATION_KEY);
+        final FlowMetric backpressureFlow = createFlowMetric(QUEUE_BACKPRESSURE_KEY, queuePushWaitInMillis, uptimeInPreciseMillis);
+        this.flowMetrics.add(backpressureFlow);
+        storeMetric(context, flowNamespace, backpressureFlow);
+
+        final LongCounter durationInMillis = initOrGetCounterMetric(context, eventsNamespace, DURATION_IN_MILLIS_KEY);
+        final FlowMetric concurrencyFlow = createFlowMetric(WORKER_CONCURRENCY_KEY, durationInMillis, uptimeInPreciseMillis);
+        this.flowMetrics.add(concurrencyFlow);
+        storeMetric(context, flowNamespace, concurrencyFlow);
+
+        return context.nil;
+    }
+
+    @JRubyMethod(name = "collect_flow_metrics")
+    public final IRubyObject collectFlowMetrics(final ThreadContext context) {
+        this.flowMetrics.forEach(FlowMetric::capture);
+        return context.nil;
+    }
+
+    private static FlowMetric createFlowMetric(final RubySymbol name,
+                                               final Metric<? extends Number> numeratorMetric,
+                                               final Metric<? extends Number> denominatorMetric) {
+        return new FlowMetric(name.asJavaString(), numeratorMetric, denominatorMetric);
+    }
+
+    private LongCounter initOrGetCounterMetric(final ThreadContext context,
+                                               final RubySymbol[] subPipelineNamespacePath,
+                                               final RubySymbol metricName) {
+        final IRubyObject collector = this.metric.collector(context);
+        final IRubyObject fullNamespace = pipelineNamespacedPath(subPipelineNamespacePath);
+
+        final IRubyObject retrievedMetric = collector.callMethod(context, "get", new IRubyObject[]{fullNamespace, metricName, context.runtime.newSymbol("counter")});
+        return retrievedMetric.toJava(LongCounter.class);
+    }
+
+    private UptimeMetric initOrGetUptimeMetric(final ThreadContext context,
+                                               final RubySymbol[] subPipelineNamespacePath,
+                                               final RubySymbol uptimeMetricName) {
+        final IRubyObject collector = this.metric.collector(context);
+        final IRubyObject fullNamespace = pipelineNamespacedPath(subPipelineNamespacePath);
+
+        final IRubyObject retrievedMetric = collector.callMethod(context, "get", new IRubyObject[]{fullNamespace, uptimeMetricName, context.runtime.newSymbol("uptime")});
+        return retrievedMetric.toJava(UptimeMetric.class);
+    }
+
+    private <T> void storeMetric(final ThreadContext context,
+                                 final RubySymbol[] subPipelineNamespacePath,
+                                 final Metric<T> metric) {
+        final IRubyObject collector = this.metric.collector(context);
+        final IRubyObject fullNamespace = pipelineNamespacedPath(subPipelineNamespacePath);
+        final IRubyObject metricKey = context.runtime.newSymbol(metric.getName());
+
+        final IRubyObject wasRegistered = collector.callMethod(context, "register?", new IRubyObject[]{fullNamespace, metricKey, JavaUtil.convertJavaToUsableRubyObject(context.runtime, metric)});
+        if (!wasRegistered.toJava(Boolean.class)) {
+            LOGGER.warn(String.format("Metric registration error: `%s` could not be registered in namespace `%s`", metricKey, fullNamespace));
+        } else {
+            LOGGER.debug(String.format("Flow metric registered: `%s` in namespace `%s`", metricKey, fullNamespace));
+        }
+    }
+
+    private RubyArray<RubySymbol> pipelineNamespacedPath(final RubySymbol... subPipelineNamespacePath) {
+        final RubySymbol[] pipelineNamespacePath = new RubySymbol[] { STATS_KEY, PIPELINES_KEY, pipelineId.asString().intern() };
+        if (subPipelineNamespacePath.length == 0) {
+            return rubySymbolArray(pipelineNamespacePath);
+        }
+        final RubySymbol[] fullNamespacePath = Arrays.copyOf(pipelineNamespacePath, pipelineNamespacePath.length + subPipelineNamespacePath.length);
+        System.arraycopy(subPipelineNamespacePath, 0, fullNamespacePath, pipelineNamespacePath.length, subPipelineNamespacePath.length);
+
+        return rubySymbolArray(fullNamespacePath);
+    }
+
+    @SuppressWarnings("unchecked")
+    private RubyArray<RubySymbol> rubySymbolArray(final RubySymbol[] symbols) {
+        return getRuntime().newArray(symbols);
+    }
+
+    private RubySymbol[] buildNamespace(final RubySymbol... namespace) {
+        return namespace;
+    }
+
     @JRubyMethod(name = "input_queue_client")
     public final JRubyAbstractQueueWriteClientExt inputQueueClient() {
         return inputQueueClient;
@@ -521,15 +585,7 @@ protected SecretStore getSecretStore(final ThreadContext context) {
 
     private AbstractNamespacedMetricExt getDlqMetric(final ThreadContext context) {
         if (dlqMetric == null) {
-            dlqMetric = metric.namespace(
-                context, RubyArray.newArray(
-                    context.runtime,
-                    Arrays.asList(
-                        MetricKeys.STATS_KEY, MetricKeys.PIPELINES_KEY,
-                        pipelineId.asString().intern(), DLQ_KEY
-                    )
-                )
-            );
+            dlqMetric = metric.namespace(context, pipelineNamespacedPath(DLQ_KEY));
         }
         return dlqMetric;
     }
diff --git a/logstash-core/src/main/java/org/logstash/ext/JRubyWrappedWriteClientExt.java b/logstash-core/src/main/java/org/logstash/ext/JRubyWrappedWriteClientExt.java
index 0c850e1964c..329cea5de17 100644
--- a/logstash-core/src/main/java/org/logstash/ext/JRubyWrappedWriteClientExt.java
+++ b/logstash-core/src/main/java/org/logstash/ext/JRubyWrappedWriteClientExt.java
@@ -23,6 +23,7 @@
 import java.util.Collection;
 import java.util.Map;
 import java.util.concurrent.TimeUnit;
+
 import org.jruby.Ruby;
 import org.jruby.RubyArray;
 import org.jruby.RubyClass;
@@ -39,23 +40,25 @@
 import org.logstash.instrument.metrics.MetricKeys;
 import org.logstash.instrument.metrics.counter.LongCounter;
 
+import static org.logstash.instrument.metrics.MetricKeys.*;
+
 @JRubyClass(name = "WrappedWriteClient")
 public final class JRubyWrappedWriteClientExt extends RubyObject implements QueueWriter {
 
     private static final long serialVersionUID = 1L;
 
-    private static final RubySymbol PUSH_DURATION_KEY =
-        RubyUtil.RUBY.newSymbol("queue_push_duration_in_millis");
-
     private JRubyAbstractQueueWriteClientExt writeClient;
 
     private transient LongCounter eventsMetricsCounter;
+
     private transient LongCounter eventsMetricsTime;
 
     private transient LongCounter pipelineMetricsCounter;
+
     private transient LongCounter pipelineMetricsTime;
 
     private transient LongCounter pluginMetricsCounter;
+
     private transient LongCounter pluginMetricsTime;
 
     public JRubyWrappedWriteClientExt(final Ruby runtime, final RubyClass metaClass) {
@@ -69,28 +72,36 @@ public JRubyWrappedWriteClientExt initialize(final ThreadContext context,
             (AbstractMetricExt) args[2], args[3]);
     }
 
-    public JRubyWrappedWriteClientExt initialize(
-        final JRubyAbstractQueueWriteClientExt queueWriteClientExt, final String pipelineId,
-        final AbstractMetricExt metric, final IRubyObject pluginId) {
+    public JRubyWrappedWriteClientExt initialize(final JRubyAbstractQueueWriteClientExt queueWriteClientExt,
+                                                 final String pipelineId,
+                                                 final AbstractMetricExt metric,
+                                                 final IRubyObject pluginId) {
         this.writeClient = queueWriteClientExt;
+
+        final RubySymbol pipelineIdSym = getRuntime().newSymbol(pipelineId);
+        final RubySymbol pluginIdSym = pluginId.asString().intern();
+
         // Synchronize on the metric since setting up new fields on it is not threadsafe
         synchronized (metric) {
             final AbstractNamespacedMetricExt eventsMetrics =
-                getMetric(metric, "stats", "events");
+                getMetric(metric, STATS_KEY, EVENTS_KEY);
+
             eventsMetricsCounter = LongCounter.fromRubyBase(eventsMetrics, MetricKeys.IN_KEY);
-            eventsMetricsTime = LongCounter.fromRubyBase(eventsMetrics, PUSH_DURATION_KEY);
-            final AbstractNamespacedMetricExt pipelineMetrics =
-                getMetric(metric, "stats", "pipelines", pipelineId, "events");
-            pipelineMetricsCounter = LongCounter.fromRubyBase(pipelineMetrics, MetricKeys.IN_KEY);
-            pipelineMetricsTime = LongCounter.fromRubyBase(pipelineMetrics, PUSH_DURATION_KEY);
-            final AbstractNamespacedMetricExt pluginMetrics = getMetric(
-                metric, "stats", "pipelines", pipelineId, "plugins", "inputs",
-                pluginId.asJavaString(), "events"
-            );
+            eventsMetricsTime = LongCounter.fromRubyBase(eventsMetrics, MetricKeys.PUSH_DURATION_KEY);
+
+            final AbstractNamespacedMetricExt pipelineEventMetrics =
+                getMetric(metric, STATS_KEY, PIPELINES_KEY, pipelineIdSym, EVENTS_KEY);
+
+            pipelineMetricsCounter = LongCounter.fromRubyBase(pipelineEventMetrics, MetricKeys.IN_KEY);
+            pipelineMetricsTime = LongCounter.fromRubyBase(pipelineEventMetrics, MetricKeys.PUSH_DURATION_KEY);
+
+            final AbstractNamespacedMetricExt pluginMetrics =
+                    getMetric(metric, STATS_KEY, PIPELINES_KEY, pipelineIdSym, PLUGINS_KEY, INPUTS_KEY, pluginIdSym, EVENTS_KEY);
             pluginMetricsCounter =
                 LongCounter.fromRubyBase(pluginMetrics, MetricKeys.OUT_KEY);
-            pluginMetricsTime = LongCounter.fromRubyBase(pluginMetrics, PUSH_DURATION_KEY);
+            pluginMetricsTime = LongCounter.fromRubyBase(pluginMetrics, MetricKeys.PUSH_DURATION_KEY);
         }
+
         return this;
     }
 
@@ -144,17 +155,10 @@ private void incrementTimers(final long start) {
         pluginMetricsTime.increment(increment);
     }
 
-    private static AbstractNamespacedMetricExt getMetric(final AbstractMetricExt base,
-        final String... keys) {
-        return base.namespace(RubyUtil.RUBY.getCurrentContext(), toSymbolArray(keys));
-    }
 
-    private static IRubyObject toSymbolArray(final String... strings) {
-        final IRubyObject[] res = new IRubyObject[strings.length];
-        for (int i = 0; i < strings.length; ++i) {
-            res[i] = RubyUtil.RUBY.newSymbol(strings[i]);
-        }
-        return RubyUtil.RUBY.newArray(res);
+    private AbstractNamespacedMetricExt getMetric(final AbstractMetricExt base,
+                                                  final RubySymbol... keys) {
+        return base.namespace(getRuntime().getCurrentContext(), getRuntime().newArray(keys));
     }
 
     @Override
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/AbstractMetric.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/AbstractMetric.java
index 8a3fe881de0..997275b8e8f 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/AbstractMetric.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/AbstractMetric.java
@@ -23,6 +23,8 @@
 
 import com.fasterxml.jackson.annotation.JsonValue;
 
+import java.util.Objects;
+
 /**
  * Abstract implementation of a {@link Metric}. All metrics should subclass this.
  *
@@ -48,8 +50,7 @@ protected AbstractMetric(final String name) {
 
     @Override
     public String toString() {
-        return String.format("%s -  name: %s value:%s", this.getClass().getName(), this.name, getValue() == null ? "null" :
-                getValue().toString());
+        return String.format("%s -  name: %s value:%s", this.getClass().getName(), this.name, Objects.requireNonNullElse(getValue(),"null"));
     }
 
     @Override
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/FlowMetric.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/FlowMetric.java
new file mode 100644
index 00000000000..b7621a790b2
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/FlowMetric.java
@@ -0,0 +1,157 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.instrument.metrics;
+
+import java.math.BigDecimal;
+import java.math.RoundingMode;
+import java.time.Duration;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Objects;
+import java.util.Optional;
+import java.util.OptionalDouble;
+import java.util.concurrent.atomic.AtomicReference;
+import java.util.function.LongSupplier;
+import java.util.function.Supplier;
+
+public class FlowMetric extends AbstractMetric<Map<String,Double>> {
+
+    // metric sources
+    private final Metric<? extends Number> numeratorMetric;
+    private final Metric<? extends Number> denominatorMetric;
+
+    // useful capture nodes for calculation
+    private final Capture baseline;
+
+    private final AtomicReference<Capture> head;
+    private final AtomicReference<Capture> instant = new AtomicReference<>();
+
+    private final LongSupplier nanoTimeSupplier;
+
+    static final String LIFETIME_KEY = "lifetime";
+    static final String CURRENT_KEY = "current";
+
+    public FlowMetric(final String name,
+                      final Metric<? extends Number> numeratorMetric,
+                      final Metric<? extends Number> denominatorMetric) {
+        this(System::nanoTime, name, numeratorMetric, denominatorMetric);
+    }
+
+    FlowMetric(final LongSupplier nanoTimeSupplier,
+               final String name,
+               final Metric<? extends Number> numeratorMetric,
+               final Metric<? extends Number> denominatorMetric) {
+        super(name);
+        this.nanoTimeSupplier = nanoTimeSupplier;
+        this.numeratorMetric = numeratorMetric;
+        this.denominatorMetric = denominatorMetric;
+
+        this.baseline = doCapture();
+        this.head = new AtomicReference<>(this.baseline);
+    }
+
+    public void capture() {
+        final Capture newestHead = doCapture();
+        final Capture previousHead = head.getAndSet(newestHead);
+        instant.getAndAccumulate(previousHead, (current, given) -> {
+            // keep our current value if the given one is less than ~100ms older than our newestHead
+            // this is naive and when captures happen too frequently without relief can result in
+            // our "current" window growing indefinitely, but we are shipping with a 5s cadence
+            // and shouldn't hit this edge-case in practice.
+            return (newestHead.calculateCapturePeriod(given).toMillis() > 100) ? given : current;
+        });
+    }
+
+    /**
+     * @return a map containing all available finite rates (see {@link Capture#calculateRate(Capture)})
+     */
+    public Map<String, Double> getValue() {
+        final Capture headCapture = head.get();
+        if (Objects.isNull(headCapture)) {
+            return Map.of();
+        }
+
+        final Map<String, Double> rates = new HashMap<>();
+
+        headCapture.calculateRate(baseline).ifPresent((rate) -> rates.put(LIFETIME_KEY, rate));
+        headCapture.calculateRate(instant::get).ifPresent((rate) -> rates.put(CURRENT_KEY,  rate));
+
+        return Map.copyOf(rates);
+    }
+
+    Capture doCapture() {
+        return new Capture(numeratorMetric.getValue(), denominatorMetric.getValue(), nanoTimeSupplier.getAsLong());
+    }
+
+    @Override
+    public MetricType getType() {
+        return MetricType.FLOW_RATE;
+    }
+
+    private static class Capture {
+        private final Number numerator;
+        private final Number denominator;
+
+        private final long nanoTimestamp;
+
+        public Capture(final Number numerator, final Number denominator, final long nanoTimestamp) {
+            this.numerator = numerator;
+            this.denominator = denominator;
+            this.nanoTimestamp = nanoTimestamp;
+        }
+
+        /**
+         *
+         * @param baseline a non-null {@link Capture} from which to compare.
+         * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information
+         *         to calculate a finite rate of change of the numerator relative to the denominator.
+         */
+        OptionalDouble calculateRate(final Capture baseline) {
+            Objects.requireNonNull(baseline, "baseline");
+            if (baseline == this) { return OptionalDouble.empty(); }
+
+            final double deltaNumerator = this.numerator.doubleValue() - baseline.numerator.doubleValue();
+            final double deltaDenominator = this.denominator.doubleValue() - baseline.denominator.doubleValue();
+
+            // divide-by-zero safeguard
+            if (deltaDenominator == 0.0) { return OptionalDouble.empty(); }
+
+            // To prevent the appearance of false-precision, we round to 3 decimal places.
+            return OptionalDouble.of(BigDecimal.valueOf(deltaNumerator)
+                                               .divide(BigDecimal.valueOf(deltaDenominator), 3, RoundingMode.HALF_UP)
+                                               .doubleValue());
+        }
+
+        /**
+         * @param possibleBaseline a {@link Supplier<Capture>} that may return null
+         * @return an {@link OptionalDouble} that will be non-empty IFF we have sufficient information
+         *         to calculate a finite rate of change of the numerator relative to the denominator.
+         */
+        OptionalDouble calculateRate(final Supplier<Capture> possibleBaseline) {
+            return Optional.ofNullable(possibleBaseline.get())
+                           .map(this::calculateRate)
+                           .orElseGet(OptionalDouble::empty);
+        }
+
+        Duration calculateCapturePeriod(final Capture baseline) {
+            return Duration.ofNanos(Math.subtractExact(this.nanoTimestamp, baseline.nanoTimestamp));
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java
index eb7b6fcc1d2..21e8ca84885 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java
@@ -39,10 +39,60 @@ private MetricKeys() {
 
     public static final RubySymbol IN_KEY = RubyUtil.RUBY.newSymbol("in");
 
-    public static final RubySymbol DURATION_IN_MILLIS_KEY =
-        RubyUtil.RUBY.newSymbol("duration_in_millis");
+    public static final RubySymbol PLUGINS_KEY = RubyUtil.RUBY.newSymbol("plugins");
+
+    public static final RubySymbol INPUTS_KEY = RubyUtil.RUBY.newSymbol("inputs");
+
+    public static final RubySymbol DURATION_IN_MILLIS_KEY = RubyUtil.RUBY.newSymbol("duration_in_millis");
+
+    public static final RubySymbol PUSH_DURATION_KEY = RubyUtil.RUBY.newSymbol("queue_push_duration_in_millis");
+
+    public static final RubySymbol PAGE_CAPACITY_IN_BYTES_KEY = RubyUtil.RUBY.newSymbol("page_capacity_in_bytes");
+
+    public static final RubySymbol MAX_QUEUE_SIZE_IN_BYTES_KEY = RubyUtil.RUBY.newSymbol("max_queue_size_in_bytes");
+
+    public static final RubySymbol MAX_QUEUE_UNREAD_EVENTS_KEY = RubyUtil.RUBY.newSymbol("max_unread_events");
+
+    public static final RubySymbol QUEUE_SIZE_IN_BYTES_KEY = RubyUtil.RUBY.newSymbol("queue_size_in_bytes");
+
+    public static final RubySymbol FREE_SPACE_IN_BYTES_KEY = RubyUtil.RUBY.newSymbol("free_space_in_bytes");
+
+    public static final RubySymbol STORAGE_TYPE_KEY = RubyUtil.RUBY.newSymbol("storage_type");
+
+    public static final RubySymbol PATH_KEY = RubyUtil.RUBY.newSymbol("path");
+
+    public static final RubySymbol TYPE_KEY = RubyUtil.RUBY.newSymbol("type");
+
+    public static final RubySymbol QUEUE_KEY = RubyUtil.RUBY.newSymbol("queue");
+
+    public static final RubySymbol DLQ_KEY = RubyUtil.RUBY.newSymbol("dlq");
+
+    public static final RubySymbol STORAGE_POLICY_KEY = RubyUtil.RUBY.newSymbol("storage_policy");
+
+    public static final RubySymbol DROPPED_EVENTS_KEY = RubyUtil.RUBY.newSymbol("dropped_events");
+
+    public static final RubySymbol EXPIRED_EVENTS_KEY = RubyUtil.RUBY.newSymbol("expired_events");
+
+    public static final RubySymbol LAST_ERROR_KEY = RubyUtil.RUBY.newSymbol("last_error");
 
     public static final RubySymbol FILTERED_KEY = RubyUtil.RUBY.newSymbol("filtered");
 
     public static final RubySymbol STATS_KEY = RubyUtil.RUBY.newSymbol("stats");
+
+    // Flow metric keys
+    public static final RubySymbol FLOW_KEY = RubyUtil.RUBY.newSymbol("flow");
+
+    public static final RubySymbol INPUT_THROUGHPUT_KEY = RubyUtil.RUBY.newSymbol("input_throughput");
+
+    public static final RubySymbol OUTPUT_THROUGHPUT_KEY = RubyUtil.RUBY.newSymbol("output_throughput");
+
+    public static final RubySymbol FILTER_THROUGHPUT_KEY = RubyUtil.RUBY.newSymbol("filter_throughput");
+
+    public static final RubySymbol QUEUE_BACKPRESSURE_KEY = RubyUtil.RUBY.newSymbol("queue_backpressure");
+
+    public static final RubySymbol WORKER_CONCURRENCY_KEY = RubyUtil.RUBY.newSymbol("worker_concurrency");
+
+    public static final RubySymbol UPTIME_IN_SECONDS_KEY = RubyUtil.RUBY.newSymbol("uptime_in_seconds");
+
+    public static final RubySymbol UPTIME_IN_MILLIS_KEY = RubyUtil.RUBY.newSymbol("uptime_in_millis");
 }
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java
index b6fcf2c6ba7..ad4bd19e37c 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricType.java
@@ -32,6 +32,11 @@ public enum MetricType {
      * A counter backed by a {@link Long} type
      */
     COUNTER_LONG("counter/long"),
+
+    /**
+     * A counter backed by a {@link Number} type that includes decimal precision
+     */
+    COUNTER_DECIMAL("counter/decimal"),
     /**
      * A gauge backed by a {@link String} type
      */
@@ -55,7 +60,13 @@ public enum MetricType {
     /**
      * A gauge backed by a {@link org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp} type. Note - Java consumers should not use this, exist for legacy Ruby code.
      */
-    GAUGE_RUBYTIMESTAMP("gauge/rubytimestamp");
+    GAUGE_RUBYTIMESTAMP("gauge/rubytimestamp"),
+
+    /**
+     * A flow-rate {@link FlowMetric}, instantiated with one or more backing {@link Metric}{@code <Number>}.
+     */
+    FLOW_RATE("flow/rate"),
+    ;
 
     private final String type;
 
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/UptimeMetric.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/UptimeMetric.java
new file mode 100644
index 00000000000..92decd22387
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/UptimeMetric.java
@@ -0,0 +1,170 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.instrument.metrics;
+
+import java.math.BigDecimal;
+import java.util.Objects;
+import java.util.concurrent.TimeUnit;
+import java.util.function.LongSupplier;
+
+/**
+ * The {@link UptimeMetric} is an auto-advancing {@link Metric} whose value
+ * represents the amount of time since instantiation. It can be made to track the
+ * advancement of the clock in one of several {@link TimeUnit}s.
+ */
+public class UptimeMetric extends AbstractMetric<Long> {
+    private final LongSupplier nanoTimeSupplier;
+    private final long startNanos;
+
+    public enum ScaleUnits {
+        NANOSECONDS(0),
+        MICROSECONDS(3),
+        MILLISECONDS(6),
+        SECONDS(9),
+        ;
+
+        private final int nanoRelativeDecimalShift;
+
+        ScaleUnits(final int nanoRelativeDecimalShift) {
+            this.nanoRelativeDecimalShift = nanoRelativeDecimalShift;
+        }
+    }
+    private final TimeUnit timeUnit;
+
+    /**
+     * Constructs an {@link UptimeMetric} whose name is "uptime_in_millis" and whose units are milliseconds
+     */
+    public UptimeMetric() {
+        this(MetricKeys.UPTIME_IN_MILLIS_KEY.asJavaString());
+    }
+
+    /**
+     * Constructs an {@link UptimeMetric} with the provided name.
+     * @param name the name of the metric, which is used by our metric store, API retrieval, etc.
+     */
+    public UptimeMetric(final String name) {
+        this(name, System::nanoTime);
+    }
+
+    UptimeMetric(final String name, final LongSupplier nanoTimeSupplier) {
+        this(name, nanoTimeSupplier, nanoTimeSupplier.getAsLong(), TimeUnit.MILLISECONDS);
+    }
+
+    private UptimeMetric(final String name, final LongSupplier nanoTimeSupplier, final long startNanos, final TimeUnit timeUnit) {
+        super(Objects.requireNonNull(name, "name"));
+        this.nanoTimeSupplier = Objects.requireNonNull(nanoTimeSupplier, "nanoTimeSupplier");
+        this.timeUnit = Objects.requireNonNull(timeUnit, "timeUnit");
+        this.startNanos = Objects.requireNonNull(startNanos, "startNanos");
+    }
+
+    /**
+     * @return the number of {@link TimeUnit}s that have elapsed
+     *         since this {@link UptimeMetric} was instantiated
+     */
+    @Override
+    public Long getValue() {
+        return this.timeUnit.convert(getElapsedNanos(), TimeUnit.NANOSECONDS);
+    }
+
+    long getElapsedNanos() {
+        return this.nanoTimeSupplier.getAsLong() - this.startNanos;
+    }
+
+    /**
+     * @return the {@link MetricType}{@code .COUNTER_LONG} associated with
+     *         long-valued metrics that only-increment, for use in Monitoring data structuring.
+     */
+    @Override
+    public MetricType getType() {
+        return MetricType.COUNTER_LONG;
+    }
+
+    /**
+     * @return the {@link TimeUnit} associated with this {@link UptimeMetric}.
+     */
+    public TimeUnit getTimeUnit() {
+        return timeUnit;
+    }
+
+    /**
+     * Constructs a _copy_ of this {@link UptimeMetric} with a new name and timeUnit, but whose
+     * uptime is tracking from the same instant as this instance.
+     *
+     * @param name the new metric's name (typically includes units)
+     * @param timeUnit the new metric's units
+     * @return a _copy_ of this {@link UptimeMetric}.
+     */
+    public UptimeMetric withTimeUnit(final String name, final TimeUnit timeUnit) {
+        return new UptimeMetric(name, this.nanoTimeSupplier, this.startNanos, timeUnit);
+    }
+
+    /**
+     * Constructs a _view_ into this {@link UptimeMetric} whose value is a decimal number
+     * containing subunit precision.
+     *
+     * @param name the name of the metric
+     * @param scaleUnits the desired scale
+     * @return a {@link BigDecimal} representing the whole-and-fractional number
+     *         of {@link ScaleUnits} that have elapsed.
+     */
+    public ScaledView withUnitsPrecise(final String name, final ScaleUnits scaleUnits) {
+        return new ScaledView(name, this::getElapsedNanos, scaleUnits.nanoRelativeDecimalShift);
+    }
+
+    /**
+     * {@code name} defaults to something vaguely descriptive.
+     * Useful when the caller doesn't need the metric name.
+     *
+     * @see UptimeMetric#withUnitsPrecise(String, ScaleUnits)
+     */
+    public ScaledView withUnitsPrecise(final ScaleUnits scaleUnits) {
+        final String name = String.format("%s_scaled_to_%s", getName(), scaleUnits.name());
+        return this.withUnitsPrecise(name, scaleUnits);
+    }
+
+    static class ScaledView implements Metric<Number> {
+        private final String name;
+        private final int nanoRelativeDecimalShift;
+        private final LongSupplier elapsedNanosSupplier;
+
+        ScaledView(final String name,
+                   final LongSupplier elapsedNanosSupplier,
+                   final int nanoRelativeDecimalShift) {
+            this.name = name;
+            this.nanoRelativeDecimalShift = nanoRelativeDecimalShift;
+            this.elapsedNanosSupplier = elapsedNanosSupplier;
+        }
+
+        @Override
+        public String getName() {
+            return this.name;
+        }
+
+        @Override
+        public MetricType getType() {
+            return MetricType.COUNTER_DECIMAL;
+        }
+
+        @Override
+        public BigDecimal getValue() {
+            return BigDecimal.valueOf(elapsedNanosSupplier.getAsLong(), nanoRelativeDecimalShift);
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/plugins/factory/PluginMetricsFactoryExt.java b/logstash-core/src/main/java/org/logstash/plugins/factory/PluginMetricsFactoryExt.java
index 339ee7f3f50..bb6fd11a0f5 100644
--- a/logstash-core/src/main/java/org/logstash/plugins/factory/PluginMetricsFactoryExt.java
+++ b/logstash-core/src/main/java/org/logstash/plugins/factory/PluginMetricsFactoryExt.java
@@ -8,11 +8,12 @@
 import org.logstash.RubyUtil;
 import org.logstash.instrument.metrics.AbstractMetricExt;
 import org.logstash.instrument.metrics.AbstractNamespacedMetricExt;
-import org.logstash.instrument.metrics.MetricKeys;
 import org.logstash.instrument.metrics.NullMetricExt;
 
 import java.util.Arrays;
 
+import static org.logstash.instrument.metrics.MetricKeys.*;
+
 /**
  * JRuby extension to implement a factory class for Plugin's metrics
  * */
@@ -21,8 +22,6 @@ public final class PluginMetricsFactoryExt extends RubyBasicObject {
 
     private static final long serialVersionUID = 1L;
 
-    private static final RubySymbol PLUGINS = RubyUtil.RUBY.newSymbol("plugins");
-
     private RubySymbol pipelineId;
 
     private AbstractMetricExt metric;
@@ -48,11 +47,7 @@ AbstractNamespacedMetricExt getRoot(final ThreadContext context) {
             context,
             RubyArray.newArray(
                 context.runtime,
-                Arrays.asList(
-                    MetricKeys.STATS_KEY, MetricKeys.PIPELINES_KEY, pipelineId, PLUGINS
-                )
-            )
-        );
+                Arrays.asList(STATS_KEY, PIPELINES_KEY, pipelineId, PLUGINS_KEY)));
     }
 
     @JRubyMethod
diff --git a/logstash-core/src/test/java/org/logstash/config/ir/compiler/OutputDelegatorTest.java b/logstash-core/src/test/java/org/logstash/config/ir/compiler/OutputDelegatorTest.java
index a808fec3be8..d855251bf53 100644
--- a/logstash-core/src/test/java/org/logstash/config/ir/compiler/OutputDelegatorTest.java
+++ b/logstash-core/src/test/java/org/logstash/config/ir/compiler/OutputDelegatorTest.java
@@ -32,11 +32,13 @@
 import org.junit.Ignore;
 import org.junit.Test;
 import org.logstash.Event;
+import org.logstash.instrument.metrics.MetricKeys;
 
 import static org.assertj.core.api.Assertions.assertThat;
 import static org.junit.Assert.assertEquals;
 import static org.logstash.RubyUtil.RUBY;
 import static org.logstash.RubyUtil.RUBY_OUTPUT_DELEGATOR_CLASS;
+import static org.logstash.instrument.metrics.MetricKeys.EVENTS_KEY;
 
 @SuppressWarnings("rawtypes")
 @NotThreadSafe
@@ -202,7 +204,7 @@ private OutputDelegatorExt constructOutputDelegator() {
     }
 
     private RubyHash getMetricStore() {
-        return getMetricStore(new String[]{"output", "foo", "events"});
+        return getMetricStore(new String[]{"output", "foo", EVENTS_KEY.asJavaString()});
     }
 
     private long getMetricLongValue(String symbolName) {
diff --git a/logstash-core/src/test/java/org/logstash/instrument/metrics/FlowMetricTest.java b/logstash-core/src/test/java/org/logstash/instrument/metrics/FlowMetricTest.java
new file mode 100644
index 00000000000..8b3780f8fcb
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/instrument/metrics/FlowMetricTest.java
@@ -0,0 +1,63 @@
+package org.logstash.instrument.metrics;
+
+import org.junit.Test;
+import org.logstash.instrument.metrics.counter.LongCounter;
+
+import java.time.Duration;
+import java.time.Instant;
+import java.util.List;
+import java.util.Map;
+
+import static org.junit.Assert.*;
+import static org.logstash.instrument.metrics.FlowMetric.CURRENT_KEY;
+import static org.logstash.instrument.metrics.FlowMetric.LIFETIME_KEY;
+
+public class FlowMetricTest {
+    @Test
+    public void testBaselineFunctionality() {
+        final ManualAdvanceClock clock = new ManualAdvanceClock(Instant.now());
+        final LongCounter numeratorMetric = new LongCounter(MetricKeys.EVENTS_KEY.asJavaString());
+        final Metric<Number> denominatorMetric = new UptimeMetric("uptime", clock::nanoTime).withUnitsPrecise(UptimeMetric.ScaleUnits.SECONDS);
+        final FlowMetric instance = new FlowMetric(clock::nanoTime, "flow", numeratorMetric, denominatorMetric);
+
+        final Map<String, Double> ratesBeforeCaptures = instance.getValue();
+        assertTrue(ratesBeforeCaptures.isEmpty());
+
+        // 5 seconds pass, during which 1000 events are processed
+        clock.advance(Duration.ofSeconds(5));
+        numeratorMetric.increment(1000);
+        instance.capture();
+        final Map<String, Double> ratesAfterFirstCapture = instance.getValue();
+        assertFalse(ratesAfterFirstCapture.isEmpty());
+        assertEquals(Map.of(LIFETIME_KEY, 200.0, CURRENT_KEY, 200.0), ratesAfterFirstCapture);
+
+        // 5 more seconds pass, during which 2000 more events are processed
+        clock.advance(Duration.ofSeconds(5));
+        numeratorMetric.increment(2000);
+        instance.capture();
+        final Map<String, Double> ratesAfterSecondCapture = instance.getValue();
+        assertFalse(ratesAfterSecondCapture.isEmpty());
+        assertEquals(Map.of(LIFETIME_KEY, 300.0, CURRENT_KEY, 400.0), ratesAfterSecondCapture);
+
+        // 30 seconds pass, during which 11700 more events are seen by our numerator
+        for (Integer eventCount : List.of(1883, 2117, 1901, 2299, 1608, 1892)) {
+            clock.advance(Duration.ofSeconds(5));
+            numeratorMetric.increment(eventCount);
+            instance.capture();
+        }
+        final Map<String, Double> ratesAfterNthCapture = instance.getValue();
+        assertFalse(ratesAfterNthCapture.isEmpty());
+        assertEquals(Map.of(LIFETIME_KEY, 367.5, CURRENT_KEY, 378.4), ratesAfterNthCapture);
+
+        // less than half a second passes, during which 0 events are seen by our numerator.
+        // when our two most recent captures are very close together, we want to make sure that
+        // we continue to provide _meaningful_ metrics, namely that:
+        // (a) our CURRENT_KEY and LIFETIME_KEY account for newest capture, and
+        // (b) our CURRENT_KEY does not report _only_ the delta since the very-recent capture
+        clock.advance(Duration.ofMillis(10));
+        instance.capture();
+        final Map<String, Double> ratesAfterSmallAdvanceCapture = instance.getValue();
+        assertFalse(ratesAfterNthCapture.isEmpty());
+        assertEquals(Map.of(LIFETIME_KEY, 367.408, CURRENT_KEY, 377.645), ratesAfterSmallAdvanceCapture);
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/instrument/metrics/ManualAdvanceClock.java b/logstash-core/src/test/java/org/logstash/instrument/metrics/ManualAdvanceClock.java
new file mode 100644
index 00000000000..e1df9568be0
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/instrument/metrics/ManualAdvanceClock.java
@@ -0,0 +1,63 @@
+package org.logstash.instrument.metrics;
+
+import java.time.Clock;
+import java.time.Duration;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.time.temporal.ChronoUnit;
+import java.util.Objects;
+import java.util.concurrent.atomic.AtomicReference;
+
+class ManualAdvanceClock extends Clock {
+    private final ZoneId zoneId;
+    private final AtomicReference<Instant> currentInstant;
+    private final Instant zeroInstant;
+
+    public ManualAdvanceClock(final Instant currentInstant, final ZoneId zoneId) {
+        this(currentInstant, new AtomicReference<>(currentInstant), zoneId);
+    }
+
+    public ManualAdvanceClock(final Instant currentInstant) {
+        this(currentInstant, null);
+    }
+
+    private ManualAdvanceClock(final Instant zeroInstant, final AtomicReference<Instant> currentInstant, final ZoneId zoneId) {
+        this.zeroInstant = zeroInstant;
+        this.currentInstant = currentInstant;
+        this.zoneId = Objects.requireNonNullElseGet(zoneId, ZoneId::systemDefault);
+    }
+
+    @Override
+    public ZoneId getZone() {
+        return this.zoneId;
+    }
+
+    @Override
+    public Clock withZone(ZoneId zone) {
+        return new ManualAdvanceClock(this.zeroInstant, this.currentInstant, zone);
+    }
+
+    @Override
+    public Instant instant() {
+        return currentInstant.get();
+    }
+
+    /**
+     * @return an only-incrementing long value, meant as a drop-in replacement
+     *         for {@link System#nanoTime()}, using this {@link ManualAdvanceClock}
+     *         as the time source, and carrying the same constraints.
+     */
+    public long nanoTime() {
+        return zeroInstant.until(instant(), ChronoUnit.NANOS);
+    }
+
+    public void advance(final Duration duration) {
+        if (duration.isZero()) {
+            return;
+        }
+        if (duration.isNegative()) {
+            throw new IllegalArgumentException("duration must not be negative");
+        }
+        this.currentInstant.updateAndGet(previous -> previous.plus(duration));
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java b/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java
index bf7c2b17c07..d45dd1beffe 100644
--- a/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java
+++ b/logstash-core/src/test/java/org/logstash/instrument/metrics/MetricTypeTest.java
@@ -42,12 +42,14 @@ public class MetricTypeTest {
     public void ensurePassivity(){
         Map<MetricType, String> nameMap = new HashMap<>(EnumSet.allOf(MetricType.class).size());
         nameMap.put(MetricType.COUNTER_LONG, "counter/long");
+        nameMap.put(MetricType.COUNTER_DECIMAL, "counter/decimal");
         nameMap.put(MetricType.GAUGE_TEXT, "gauge/text");
         nameMap.put(MetricType.GAUGE_BOOLEAN, "gauge/boolean");
         nameMap.put(MetricType.GAUGE_NUMBER, "gauge/number");
         nameMap.put(MetricType.GAUGE_UNKNOWN, "gauge/unknown");
         nameMap.put(MetricType.GAUGE_RUBYHASH, "gauge/rubyhash");
         nameMap.put(MetricType.GAUGE_RUBYTIMESTAMP, "gauge/rubytimestamp");
+        nameMap.put(MetricType.FLOW_RATE, "flow/rate");
 
         //ensure we are testing all of the enumerations
         assertThat(EnumSet.allOf(MetricType.class).size()).isEqualTo(nameMap.size());
diff --git a/logstash-core/src/test/java/org/logstash/instrument/metrics/UptimeMetricTest.java b/logstash-core/src/test/java/org/logstash/instrument/metrics/UptimeMetricTest.java
new file mode 100644
index 00000000000..61f700f8385
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/instrument/metrics/UptimeMetricTest.java
@@ -0,0 +1,95 @@
+package org.logstash.instrument.metrics;
+
+import org.junit.Test;
+
+import java.math.BigDecimal;
+import java.time.Duration;
+import java.time.Instant;
+import java.util.concurrent.TimeUnit;
+
+import static org.junit.Assert.assertEquals;
+
+public class UptimeMetricTest {
+
+    @Test
+    public void testDefaultConstructor() {
+        final UptimeMetric defaultConstructorUptimeMetric = new UptimeMetric();
+        assertEquals(MetricKeys.UPTIME_IN_MILLIS_KEY.asJavaString(), defaultConstructorUptimeMetric.getName());
+        assertEquals(TimeUnit.MILLISECONDS, defaultConstructorUptimeMetric.getTimeUnit());
+    }
+
+    @Test
+    public void getNameExplicit() {
+        final String customName = "custom_uptime_name";
+        assertEquals(customName, new UptimeMetric(customName).getName());
+    }
+
+    @Test
+    public void getType() {
+        assertEquals(MetricType.COUNTER_LONG, new UptimeMetric().getType());
+    }
+
+    @Test
+    public void getValue() {
+        final ManualAdvanceClock clock = new ManualAdvanceClock(Instant.now());
+        final UptimeMetric uptimeMetric = new UptimeMetric("up", clock::nanoTime);
+        assertEquals(Long.valueOf(0L), uptimeMetric.getValue());
+
+        clock.advance(Duration.ofMillis(123));
+        assertEquals(Long.valueOf(123L), uptimeMetric.getValue());
+
+        clock.advance(Duration.ofMillis(456));
+        assertEquals(Long.valueOf(579L), uptimeMetric.getValue());
+
+        clock.advance(Duration.ofMinutes(15));
+        assertEquals(Long.valueOf(900579L), uptimeMetric.getValue());
+
+        clock.advance(Duration.ofHours(712));
+        assertEquals(Long.valueOf(2564100579L), uptimeMetric.getValue());
+    }
+
+    @Test
+    public void withTemporalUnit() {
+        final ManualAdvanceClock clock = new ManualAdvanceClock(Instant.now());
+        final UptimeMetric uptimeMetric = new UptimeMetric("up_millis", clock::nanoTime);
+        clock.advance(Duration.ofMillis(1_000_000_000));
+
+        // set-up: ensure advancing nanos reflects in our milli-based uptime
+        assertEquals(Long.valueOf(1_000_000_000), uptimeMetric.getValue());
+
+        final UptimeMetric secondsBasedCopy = uptimeMetric.withTimeUnit("up_seconds", TimeUnit.SECONDS);
+        assertEquals(Long.valueOf(1_000_000), secondsBasedCopy.getValue());
+
+        clock.advance(Duration.ofMillis(1_999));
+        assertEquals(Long.valueOf(1_000_001_999), uptimeMetric.getValue());
+        assertEquals(Long.valueOf(1_000_001), secondsBasedCopy.getValue());
+    }
+
+    @Test
+    public void withUnitsPrecise() {
+        final ManualAdvanceClock clock = new ManualAdvanceClock(Instant.now());
+        final UptimeMetric uptimeMetric = new UptimeMetric("up_millis", clock::nanoTime);
+        clock.advance(Duration.ofNanos(123_456_789_987L)); // 123.xx seconds
+
+        // set-up: ensure advancing nanos reflects in our milli-based uptime
+        assertEquals(Long.valueOf(123_456L), uptimeMetric.getValue());
+
+        final UptimeMetric.ScaledView secondsBasedView = uptimeMetric.withUnitsPrecise("up_seconds", UptimeMetric.ScaleUnits.SECONDS);
+        final UptimeMetric.ScaledView millisecondsBasedView = uptimeMetric.withUnitsPrecise("up_millis", UptimeMetric.ScaleUnits.MILLISECONDS);
+        final UptimeMetric.ScaledView microsecondsBasedView = uptimeMetric.withUnitsPrecise("up_micros", UptimeMetric.ScaleUnits.MICROSECONDS);
+        final UptimeMetric.ScaledView nanosecondsBasedView = uptimeMetric.withUnitsPrecise("up_nanos", UptimeMetric.ScaleUnits.NANOSECONDS);
+
+        assertEquals(new BigDecimal("123.456789987"), secondsBasedView.getValue());
+        assertEquals(new BigDecimal("123456.789987"), millisecondsBasedView.getValue());
+        assertEquals(new BigDecimal("123456789.987"), microsecondsBasedView.getValue());
+        assertEquals(new BigDecimal("123456789987"), nanosecondsBasedView.getValue());
+
+        clock.advance(Duration.ofMillis(1_999));
+        assertEquals(Long.valueOf(125_455L), uptimeMetric.getValue());
+        assertEquals(new BigDecimal("125.455789987"), secondsBasedView.getValue());
+        assertEquals(new BigDecimal("125455.789987"), millisecondsBasedView.getValue());
+        assertEquals(new BigDecimal("125455789.987"), microsecondsBasedView.getValue());
+        assertEquals(new BigDecimal("125455789987"), nanosecondsBasedView.getValue());
+    }
+
+}
\ No newline at end of file
diff --git a/qa/integration/specs/monitoring_api_spec.rb b/qa/integration/specs/monitoring_api_spec.rb
index c68c8ec3b85..386bfcfc37e 100644
--- a/qa/integration/specs/monitoring_api_spec.rb
+++ b/qa/integration/specs/monitoring_api_spec.rb
@@ -245,6 +245,38 @@
     logging_get_assert logstash_service, "INFO", "TRACE"
   end
 
+  it "should retrieve the pipeline flow statuses" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_with_stdin
+    logstash_service.wait_for_logstash
+    number_of_events.times {
+      logstash_service.write_to_stdin("Testing flow metrics")
+      sleep(1)
+    }
+
+    Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
+      # node_stats can fail if the stats subsystem isn't ready
+      result = logstash_service.monitoring_api.node_stats rescue nil
+      expect(result).not_to be_nil
+      # we use fetch here since we want failed fetches to raise an exception
+      # and trigger the retry block
+      expect(result).to include('pipelines' => hash_including('main' => hash_including('flow')))
+      flow_status = result.dig("pipelines", "main", "flow")
+      expect(flow_status).to_not be_nil
+      expect(flow_status).to include(
+        # due to three-decimal-place rounding, it is easy for our worker_concurrency and queue_backpressure
+        # to be zero, so we are just looking for these to be _populated_
+        'worker_concurrency' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+        'queue_backpressure' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+        # depending on flow capture interval, our current rate can easily be zero, but our lifetime rates
+        # should be non-zero so long as pipeline uptime is less than ~10 minutes.
+        'input_throughput'   => hash_including('current' => a_value >= 0, 'lifetime' => a_value >  0),
+        'filter_throughput'  => hash_including('current' => a_value >= 0, 'lifetime' => a_value >  0),
+        'output_throughput'  => hash_including('current' => a_value >= 0, 'lifetime' => a_value >  0)
+      )
+    end
+  end
+
   private
 
   def logging_get_assert(logstash_service, logstash_level, slowlog_level, skip: '')
diff --git a/qa/integration/specs/reload_config_spec.rb b/qa/integration/specs/reload_config_spec.rb
index f3f3cdf9a70..71c38e55ef9 100644
--- a/qa/integration/specs/reload_config_spec.rb
+++ b/qa/integration/specs/reload_config_spec.rb
@@ -25,6 +25,9 @@
 require "logstash/util"
 
 describe "Test Logstash service when config reload is enabled" do
+
+  define_negated_matcher :exclude, :include
+
   before(:all) {
     @fixture = Fixture.new(__FILE__)
   }
@@ -44,6 +47,8 @@
   let(:initial_config_file) { config_to_temp_file(@fixture.config("initial", { :port => initial_port, :file => output_file1 })) }
   let(:reload_config_file) { config_to_temp_file(@fixture.config("reload", { :port => reload_port, :file => output_file2 })) }
 
+  let(:max_retry) { 30 }
+
   it "can reload when changes are made to TCP port and grok pattern" do
     logstash_service = @fixture.get_service("logstash")
     logstash_service.spawn_logstash("-f", "#{initial_config_file}", "--config.reload.automatic", "true")
@@ -60,7 +65,13 @@
     result = logstash_service.monitoring_api.event_stats
     expect(result["in"]).to eq(1)
     expect(result["out"]).to eq(1)
-    
+
+    # make sure the pipeline flow has non-zero input throughput after receiving data
+    Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
+      pipeline_flow_stats = logstash_service.monitoring_api.pipeline_stats("main")["flow"]
+      expect(pipeline_flow_stats['input_throughput']).to include('lifetime' => (a_value >  0))
+    end
+
     # do a reload
     logstash_service.reload_config(initial_config_file, reload_config_file)
 
@@ -69,7 +80,20 @@
     
     # make sure old socket is closed
     expect(is_port_open?(initial_port)).to be false
-    
+
+    # check pipeline flow metrics. They should be both present and reset.
+    # since we have processed zero events since the reload, we expect their rates to be either unavailable or zero.
+    pipeline_flow_stats = logstash_service.monitoring_api.pipeline_stats("main")["flow"]
+    expect(pipeline_flow_stats).to_not be_nil
+    expect(pipeline_flow_stats).to include('input_throughput', 'queue_backpressure')
+    aggregate_failures do
+      expect(pipeline_flow_stats['input_throughput']).to exclude('lifetime').or(include('lifetime' => 0))
+      expect(pipeline_flow_stats['input_throughput']).to exclude('current').or(include('current' => 0))
+      expect(pipeline_flow_stats['queue_backpressure']).to exclude('lifetime').or(include('lifetime' => 0))
+      expect(pipeline_flow_stats['queue_backpressure']).to exclude('current').or(include('current' => 0))
+    end
+
+    # send data, and wait for at least some of the events to make it through the output.
     send_data(reload_port, sample_data)
     Stud.try(retry_attempts.times, RSpec::Expectations::ExpectationNotMetError) do
       expect(LogStash::Util.blank?(IO.read(output_file2))).to be false
@@ -84,7 +108,24 @@
     pipeline_event_stats = logstash_service.monitoring_api.pipeline_stats("main")["events"]
     expect(pipeline_event_stats["in"]).to eq(1)
     expect(pipeline_event_stats["out"]).to eq(1)
-    
+
+    # make sure the pipeline flow has non-zero input/output throughput after receiving data
+    Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
+      pipeline_flow_stats = logstash_service.monitoring_api.pipeline_stats("main")["flow"]
+      expect(pipeline_flow_stats).to_not be_nil
+      expect(pipeline_flow_stats).to include(
+        # due to three-decimal-place rounding, it is easy for our worker_concurrency and queue_backpressure
+        # to be zero, so we are just looking for these to be _populated_
+        'worker_concurrency' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+        'queue_backpressure' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+        # depending on flow capture interval, our current rate can easily be zero, but our lifetime rates
+        # should be non-zero so long as pipeline uptime is less than ~10 minutes.
+        'input_throughput'   => hash_including('current' => a_value >= 0, 'lifetime' => a_value >  0),
+        'filter_throughput'  => hash_including('current' => a_value >= 0, 'lifetime' => a_value >  0),
+        'output_throughput'  => hash_including('current' => a_value >= 0, 'lifetime' => a_value >  0)
+      )
+    end
+
     # check reload stats
     pipeline_reload_stats = logstash_service.monitoring_api.pipeline_stats("main")["reloads"]
     instance_reload_stats = logstash_service.monitoring_api.node_stats["reloads"]
diff --git a/tools/benchmark-cli/src/test/resources/org/logstash/benchmark/cli/metrics.json b/tools/benchmark-cli/src/test/resources/org/logstash/benchmark/cli/metrics.json
index 7d87bbab3a4..8d423fa1700 100644
--- a/tools/benchmark-cli/src/test/resources/org/logstash/benchmark/cli/metrics.json
+++ b/tools/benchmark-cli/src/test/resources/org/logstash/benchmark/cli/metrics.json
@@ -93,6 +93,28 @@
         "out" : 357125,
         "duration_in_millis" : 168492
       },
+      "flow": {
+        "output_throughput": {
+          "lifetime": 0.231,
+          "current": 4.0
+        },
+        "filter_throughput": {
+          "lifetime": 0.231,
+          "current": 4.0
+        },
+        "queue_backpressure": {
+          "lifetime": 0.0,
+          "current": 0.0
+        },
+        "worker_concurrency": {
+          "lifetime": 0.018,
+          "current": 0.288
+        },
+        "input_throughput": {
+          "lifetime": 0.25,
+          "current": 4.4
+        }
+      },
       "plugins" : {
         "inputs" : [ {
           "id" : "3ca119230f5eaf03a261b674ee2f2dfe1491894c1b2b8f21e1d9a02b656b36f1",
