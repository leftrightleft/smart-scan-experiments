diff --git a/CHANGELOG b/CHANGELOG
index f050f172abf..3cc75b01bf8 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -1,3 +1,33 @@
+1.4.4 (July 21, 2015)
+  # general
+  - Improved default security for SSL
+  - Update to Elasticsearch 1.7
+
+1.4.3 (June 2, 2015)
+  # general
+  - Updated to Elasticsearch 1.5.2, Kibana 3.1.2 and JRuby 1.7.17
+  - Bumped ruby-ftw version to 0.0.42 which properly handled closing of socket 
+    connections and prevents file descriptor leaks (#2000)
+
+  # output
+  - File: Sandbox output to protect against issues like creating new files
+    outside defined paths
+
+1.4.2 (June 24, 2014)
+  # general
+  - fixed path issues when invoking bin/logstash outside its home directory
+
+  # input
+  - bugfix: generator: fixed stdin option support
+  - bugfix: file: fixed debian 7 path issue
+
+  # codecs
+  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
+  - improvement: collectd: add support for NaN values
+
+  # outputs
+  - improvement: nagios_nsca: fix external command invocation to avoid shell escaping
+
 1.4.1 (May 6, 2014)
   # General
   - bumped Elasticsearch to 1.1.1 and Kibana to 3.0.1
diff --git a/Makefile b/Makefile
index f8b63897d66..053288fcbb1 100644
--- a/Makefile
+++ b/Makefile
@@ -2,15 +2,15 @@
 #   rsync
 #   wget or curl
 #
-JRUBY_VERSION=1.7.11
-ELASTICSEARCH_VERSION=1.1.1
+JRUBY_VERSION=1.7.17
+ELASTICSEARCH_VERSION=1.7.0
 
 WITH_JRUBY=java -jar $(shell pwd)/$(JRUBY) -S
 JRUBY=vendor/jar/jruby-complete-$(JRUBY_VERSION).jar
 JRUBY_URL=http://jruby.org.s3.amazonaws.com/downloads/$(JRUBY_VERSION)/jruby-complete-$(JRUBY_VERSION).jar
 JRUBY_CMD=bin/logstash env java -jar $(JRUBY)
 
-ELASTICSEARCH_URL=http://download.elasticsearch.org/elasticsearch/elasticsearch
+ELASTICSEARCH_URL=https://download.elastic.co/elasticsearch/elasticsearch
 ELASTICSEARCH=vendor/jar/elasticsearch-$(ELASTICSEARCH_VERSION)
 TYPESDB=vendor/collectd/types.db
 COLLECTD_VERSION=5.4.0
@@ -19,7 +19,7 @@ GEOIP=vendor/geoip/GeoLiteCity.dat
 GEOIP_URL=http://logstash.objects.dreamhost.com/maxmind/GeoLiteCity-2013-01-18.dat.gz
 GEOIP_ASN=vendor/geoip/GeoIPASNum.dat
 GEOIP_ASN_URL=http://logstash.objects.dreamhost.com/maxmind/GeoIPASNum-2014-02-12.dat.gz
-KIBANA_URL=https://download.elasticsearch.org/kibana/kibana/kibana-3.0.1.tar.gz
+KIBANA_URL=https://download.elastic.co/kibana/kibana/kibana-3.1.2.tar.gz
 PLUGIN_FILES=$(shell find lib -type f| egrep '^lib/logstash/(inputs|outputs|filters|codecs)/[^/]+$$' | egrep -v '/(base|threadable).rb$$|/inputs/ganglia/')
 QUIET=@
 ifeq (@,$(QUIET))
diff --git a/docs/tutorials/10-minute-walkthrough/hello-search.conf b/docs/tutorials/10-minute-walkthrough/hello-search.conf
index c99f014658a..638c39084d4 100644
--- a/docs/tutorials/10-minute-walkthrough/hello-search.conf
+++ b/docs/tutorials/10-minute-walkthrough/hello-search.conf
@@ -21,5 +21,7 @@ output {
     # This option below saves you from having to run a separate process just
     # for ElasticSearch, so you can get started quicker!
     embedded => true
+    # To avoid ES <-> Logstash Version Mismatch Error
+    protocol => http  
   }
 }
diff --git a/gembag.rb b/gembag.rb
index 9f03c8c9bfd..2962624404e 100644
--- a/gembag.rb
+++ b/gembag.rb
@@ -65,7 +65,7 @@ def default_lockfile
 # Try installing a few times in case we hit the "bad_record_mac" ssl error during installation.
 10.times do
   begin
-    Bundler::CLI.start(["install", "--gemfile=tools/Gemfile", "--path", target, "--clean"])
+    Bundler::CLI.start(["install", "--gemfile=tools/Gemfile", "--path", target, "--clean", "--without", "development"])
     break
   rescue Gem::RemoteFetcher::FetchError => e
     puts e.message
diff --git a/lib/logstash/codecs/collectd.rb b/lib/logstash/codecs/collectd.rb
index 2e7bf9afdf3..770dc58a295 100644
--- a/lib/logstash/codecs/collectd.rb
+++ b/lib/logstash/codecs/collectd.rb
@@ -2,6 +2,7 @@
 require "date"
 require "logstash/codecs/base"
 require "logstash/namespace"
+require "logstash/errors"
 require "tempfile"
 require "time"
 
@@ -39,6 +40,7 @@
 class ProtocolError < LogStash::Error; end
 class HeaderError < LogStash::Error; end
 class EncryptionError < LogStash::Error; end
+class NaNError < LogStash::Error; end
 
 class LogStash::Codecs::Collectd < LogStash::Codecs::Base
   config_name "collectd"
@@ -75,20 +77,21 @@ class LogStash::Codecs::Collectd < LogStash::Codecs::Base
 
   COLLECTD_TYPE_FIELDS = {
     'host' => true,
-    '@timestamp' => true, 
-    'plugin' => true, 
+    '@timestamp' => true,
+    'plugin' => true,
     'plugin_instance' => true,
+    'type_instance' => true,
   }
 
   INTERVAL_VALUES_FIELDS = {
-    "interval" => true, 
+    "interval" => true,
     "values" => true,
   }
 
   INTERVAL_BASE_FIELDS = {
     'host' => true,
     'collectd_type' => true,
-    'plugin' => true, 
+    'plugin' => true,
     'plugin_instance' => true,
     '@timestamp' => true,
     'type_instance' => true,
@@ -115,6 +118,20 @@ class LogStash::Codecs::Collectd < LogStash::Codecs::Base
   # collectd [Network plugin](https://collectd.org/wiki/index.php/Plugin:Network)
   config :security_level, :validate => [SECURITY_NONE, SECURITY_SIGN, SECURITY_ENCR],
     :default => "None"
+  
+  # What to do when a value in the event is NaN (Not a Number)
+  # - change_value (default): Change the NaN to the value of the nan_value option and add nan_tag as a tag
+  # - warn: Change the NaN to the value of the nan_value option, print a warning to the log and add nan_tag as a tag
+  # - drop: Drop the event containing the NaN (this only drops the single event, not the whole packet)
+  config :nan_handling, :validate => ['change_value','warn','drop'], :default => 'change_value'
+  
+  # Only relevant when nan_handeling is set to 'change_value'
+  # Change NaN to this configured value
+  config :nan_value, :validate => :number, :default => 0
+  
+  # The tag to add to the event if a NaN value was found
+  # Set this to an empty string ('') if you don't want to tag
+  config :nan_tag, :validate => :string, :default => '_collectdNaN'
 
   # Path to the authentication file. This file should have the same format as
   # the [AuthFile](http://collectd.org/documentation/manpages/collectd.conf.5.shtml#authfile_filename)
@@ -125,6 +142,7 @@ class LogStash::Codecs::Collectd < LogStash::Codecs::Base
   public
   def register
     @logger.info("Starting Collectd codec...")
+    init_lambdas!
     if @typesdb.nil?
       @typesdb = LogStash::Environment.vendor_path("collectd/types.db")
       if !File.exists?(@typesdb)
@@ -167,103 +185,121 @@ def get_types(paths)
     return types
   end # def get_types
 
-  # Lambdas for hash + closure methodology
-  # This replaces when statements for fixed values and is much faster
-  string_decoder = lambda { |body| body.pack("C*")[0..-2] }
-  numeric_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
-  counter_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("Q>")[0] }
-  gauge_decoder   = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
-  derive_decoder  = lambda { |body| body.slice!(0..7).pack("C*").unpack("q>")[0] }
-  # For Low-Resolution time
-  time_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2))).utc
-  end
-  # Hi-Resolution time
-  hirestime_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
-  end
-  # Hi resolution intervals
-  hiresinterval_decoder = lambda do |body|
-    byte1, byte2 = body.pack("C*").unpack("NN")
-    Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).to_i
-  end
-  # Values decoder
-  values_decoder = lambda do |body|
-    body.slice!(0..1)       # Prune the header
-    if body.length % 9 == 0 # Should be 9 fields
-      count = 0
-      retval = []
-      # Iterate through and take a slice each time
-      types = body.slice!(0..((body.length/9)-1))
-      while body.length > 0
-        # Use another hash + closure here...
-        retval << VALUES_DECODER[types[count]].call(body)
-        count += 1
+  def init_lambdas!
+    # Lambdas for hash + closure methodology
+    # This replaces when statements for fixed values and is much faster
+    string_decoder  = lambda { |body| body.pack("C*")[0..-2] }
+    numeric_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
+    counter_decoder = lambda { |body| body.slice!(0..7).pack("C*").unpack("Q>")[0] }
+    gauge_decoder   = lambda { |body| body.slice!(0..7).pack("C*").unpack("E")[0] }
+    derive_decoder  = lambda { |body| body.slice!(0..7).pack("C*").unpack("q>")[0] }
+    # For Low-Resolution time
+    time_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2))).utc
+    end
+    # Hi-Resolution time
+    hirestime_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).utc
+    end
+    # Hi resolution intervals
+    hiresinterval_decoder = lambda do |body|
+      byte1, byte2 = body.pack("C*").unpack("NN")
+      Time.at(( ((byte1 << 32) + byte2) * (2**-30) )).to_i
+    end
+    # Value type decoder
+    value_type_decoder = lambda do |body|
+      body.slice!(0..1)       # Prune the header
+      if body.length % 9 == 0 # Should be 9 fields
+        count = 0
+        retval = []
+        # Iterate through and take a slice each time
+        types = body.slice!(0..((body.length/9)-1))
+        while body.length > 0
+          # Use another hash + closure here...
+          v = @values_decoder[types[count]].call(body)
+          if types[count] == 1 && v.nan?
+            case @nan_handling
+            when 'drop'; drop = true
+            else
+              v = @nan_value
+              add_nan_tag = true
+              @nan_handling == 'warn' && @logger.warn("NaN replaced by #{@nan_value}")
+            end
+          end
+          retval << v
+          count += 1
+        end
+      else
+        @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
       end
-    else
-      @logger.error("Incorrect number of data fields for collectd record", :body => body.to_s)
+      return retval, drop, add_nan_tag
     end
-    return retval
-  end
-  # Signature
-  signature_decoder = lambda do |body|
-    if body.length < 32
-      @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
-    elsif body.length < 33
-      @logger.warning("Received signature without username")
-    else
+    # Signature
+    signature_decoder = lambda do |body|
+      if body.length < 32
+        @logger.warning("SHA256 signature too small (got #{body.length} bytes instead of 32)")
+      elsif body.length < 33
+        @logger.warning("Received signature without username")
+      else
+        retval = []
+        # Byte 32 till the end contains the username as chars (=unsigned ints)
+        retval << body[32..-1].pack('C*')
+        # Byte 0 till 31 contain the signature
+        retval << body[0..31].pack('C*')
+      end
+      return retval
+    end
+    # Encryption
+    encryption_decoder = lambda do |body|
       retval = []
-      # Byte 32 till the end contains the username as chars (=unsigned ints)
-      retval << body[32..-1].pack('C*')
-      # Byte 0 till 31 contain the signature
-      retval << body[0..31].pack('C*')
+      user_length = (body.slice!(0) << 8) + body.slice!(0)
+      retval << body.slice!(0..user_length-1).pack('C*') # Username
+      retval << body.slice!(0..15).pack('C*')            # IV
+      retval << body.pack('C*')
+      return retval
     end
-    return retval
-  end
-  # Encryption
-  encryption_decoder = lambda do |body|
-    retval = []
-    user_length = (body.slice!(0) << 8) + body.slice!(0)
-    retval << body.slice!(0..user_length-1).pack('C*') # Username
-    retval << body.slice!(0..15).pack('C*')            # IV
-    retval << body.pack('C*')
-    return retval
-  end
-  # Lambda Hashes
-  ID_DECODER = {
-    0 => string_decoder,
-    1 => time_decoder,
-    2 => string_decoder,
-    3 => string_decoder,
-    4 => string_decoder,
-    5 => string_decoder,
-    6 => values_decoder,
-    7 => numeric_decoder,
-    8 => hirestime_decoder,
-    9 => hiresinterval_decoder,
-    256 => string_decoder,
-    257 => numeric_decoder,
-    512 => signature_decoder,
-    528 => encryption_decoder
-  }
-  # TYPE VALUES:
-  # 0: COUNTER
-  # 1: GAUGE
-  # 2: DERIVE
-  # 3: ABSOLUTE
-  VALUES_DECODER = {
-    0 => counter_decoder,
-    1 => gauge_decoder,
-    2 => derive_decoder,
-    3 => counter_decoder
-  }
+    @id_decoder = {
+      0 => string_decoder,
+      1 => time_decoder,
+      2 => string_decoder,
+      3 => string_decoder,
+      4 => string_decoder,
+      5 => string_decoder,
+      6 => value_type_decoder,
+      7 => numeric_decoder,
+      8 => hirestime_decoder,
+      9 => hiresinterval_decoder,
+      256 => string_decoder,
+      257 => numeric_decoder,
+      512 => signature_decoder,
+      528 => encryption_decoder
+    }
+    # TYPE VALUES:
+    # 0: COUNTER
+    # 1: GAUGE
+    # 2: DERIVE
+    # 3: ABSOLUTE
+    @values_decoder = {
+      0 => counter_decoder,
+      1 => gauge_decoder,
+      2 => derive_decoder,
+      3 => counter_decoder
+    }
+  end # def init_lambdas!
 
   public
   def get_values(id, body)
+    drop = false
+    add_tag = false
+    if id == 6
+      retval, drop, add_nan_tag = @id_decoder[id].call(body)
     # Use hash + closure/lambda to speed operations
-    ID_DECODER[id].call(body)
+    else
+      retval = @id_decoder[id].call(body)
+    end
+    return retval, drop, add_nan_tag
   end
 
   private
@@ -369,7 +405,7 @@ def decode(payload)
         next
       end
 
-      values = get_values(typenum, body)
+      values, drop, add_nan_tag = get_values(typenum, body)
 
       case typenum
       when SIGNATURE_TYPE
@@ -424,9 +460,17 @@ def decode(payload)
           # This is better than looping over all keys every time.
           collectd.delete('type_instance') if collectd['type_instance'] == ""
           collectd.delete('plugin_instance') if collectd['plugin_instance'] == ""
+          if add_nan_tag
+            collectd['tags'] ||= []
+            collectd['tags'] << @nan_tag
+          end
           # This ugly little shallow-copy hack keeps the new event from getting munged by the cleanup
           # With pass-by-reference we get hosed (if we pass collectd, then clean it up rapidly, values can disappear)
-          yield LogStash::Event.new(collectd.dup)
+          if !drop # Drop the event if it's flagged true
+            yield LogStash::Event.new(collectd.dup)
+          else
+            raise(NaNError)
+          end
         end
         # Clean up the event
         collectd.each_key do |k|
@@ -434,8 +478,8 @@ def decode(payload)
         end
       end
     end # while payload.length > 0 do
-  rescue EncryptionError, ProtocolError, HeaderError
+  rescue EncryptionError, ProtocolError, HeaderError, NaNError
     # basically do nothing, we just want out
   end # def decode
 
-end # class LogStash::Codecs::Collectd
+end # class LogStash::Codecs::Collectd
\ No newline at end of file
diff --git a/lib/logstash/codecs/netflow.rb b/lib/logstash/codecs/netflow.rb
index 9e2d99de1c2..278cd214d89 100644
--- a/lib/logstash/codecs/netflow.rb
+++ b/lib/logstash/codecs/netflow.rb
@@ -45,7 +45,7 @@ def register
     @templates = Vash.new()
 
     # Path to default Netflow v9 field definitions
-    filename = File.join(File.dirname(__FILE__), "netflow/netflow.yaml")
+    filename = LogStash::Environment.plugin_path("codecs/netflow/netflow.yaml")
 
     begin
       @fields = YAML.load_file(filename)
@@ -162,7 +162,7 @@ def decode(payload, &block)
               # Purge any expired templates
               @templates.cleanup!
             end
-          end 
+          end
         when 256..65535
           # Data flowset
           #key = "#{flowset.source_id}|#{event["source"]}|#{record.flowset_id}"
@@ -180,7 +180,7 @@ def decode(payload, &block)
           # Template shouldn't be longer than the record and there should
           # be at most 3 padding bytes
           if template.num_bytes > length or ! (length % template.num_bytes).between?(0, 3)
-            @logger.warn("Template length doesn't fit cleanly into flowset", :template_id => record.flowset_id, :template_length => template.num_bytes, :record_length => length) 
+            @logger.warn("Template length doesn't fit cleanly into flowset", :template_id => record.flowset_id, :template_length => template.num_bytes, :record_length => length)
             next
           end
 
diff --git a/lib/logstash/environment.rb b/lib/logstash/environment.rb
index fd234644cf1..f8c12ef59be 100644
--- a/lib/logstash/environment.rb
+++ b/lib/logstash/environment.rb
@@ -31,5 +31,13 @@ def jruby?
     def vendor_path(path)
       return ::File.join(LOGSTASH_HOME, "vendor", path)
     end
+
+    def plugin_path(path)
+      return ::File.join(LOGSTASH_HOME, "lib/logstash", path)
+    end
+
+    def pattern_path(path)
+      return ::File.join(LOGSTASH_HOME, "patterns", path)
+    end
   end
 end
diff --git a/lib/logstash/filters/grok.rb b/lib/logstash/filters/grok.rb
index e992ccd5a2d..c438fa95ade 100644
--- a/lib/logstash/filters/grok.rb
+++ b/lib/logstash/filters/grok.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/environment"
 require "set"
 
 # Parse arbitrary text and structure it.
@@ -99,7 +100,7 @@
 #
 #     (?<queue_id>[0-9A-F]{10,11})
 #
-# Alternately, you can create a custom patterns file. 
+# Alternately, you can create a custom patterns file.
 #
 # * Create a directory called `patterns` with a file in it called `extra`
 #   (the file name doesn't matter, but name it meaningfully for yourself)
@@ -202,7 +203,7 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
   #
   #     filter {
   #       grok {
-  #         match => [ 
+  #         match => [
   #           "message",
   #           "%{SYSLOGBASE} %{DATA:message}"
   #         ]
@@ -216,7 +217,7 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
 
   # Detect if we are running from a jarfile, pick the right path.
   @@patterns_path ||= Set.new
-  @@patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
+  @@patterns_path += [LogStash::Environment.pattern_path("*")]
 
   public
   def initialize(params)
@@ -342,7 +343,7 @@ def compile_capture_handler(capture)
     syntax, semantic, coerce = capture.split(":")
 
     # each_capture do |fullname, value|
-    #   capture_handlers[fullname].call(value, event) 
+    #   capture_handlers[fullname].call(value, event)
     # end
 
     code = []
@@ -350,7 +351,7 @@ def compile_capture_handler(capture)
     code << "lambda do |value, event|"
     #code << "  p :value => value, :event => event"
     if semantic.nil?
-      if @named_captures_only 
+      if @named_captures_only
         # Abort early if we are only keeping named (semantic) captures
         # and this capture has no semantic name.
         code << "  return"
diff --git a/lib/logstash/filters/multiline.rb b/lib/logstash/filters/multiline.rb
index bcee5757aeb..239952b986a 100644
--- a/lib/logstash/filters/multiline.rb
+++ b/lib/logstash/filters/multiline.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/filters/base"
 require "logstash/namespace"
+require "logstash/environment"
 require "set"
 #
 # This filter will collapse multiline messages from a single source into one Logstash event.
@@ -102,7 +103,7 @@ class LogStash::Filters::Multiline < LogStash::Filters::Base
 
   # Detect if we are running from a jarfile, pick the right path.
   @@patterns_path = Set.new
-  @@patterns_path += ["#{File.dirname(__FILE__)}/../../../patterns/*"]
+  @@patterns_path += [LogStash::Environment.pattern_path("*")]
 
   public
   def initialize(config = {})
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 68afca36f30..fb404d50b4d 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -118,4 +118,20 @@ def decorate(event)
       event[field] = value
     end
   end
+
+  protected
+  def fix_streaming_codecs
+    require "logstash/codecs/plain"
+    require "logstash/codecs/line"
+    require "logstash/codecs/json"
+    require "logstash/codecs/json_lines"
+    case @codec
+      when LogStash::Codecs::Plain
+        @logger.info("Automatically switching from #{@codec.class.config_name} to line codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::Line.new
+      when LogStash::Codecs::JSON
+        @logger.info("Automatically switching from #{@codec.class.config_name} to json_lines codec", :plugin => self.class.config_name)
+        @codec = LogStash::Codecs::JSONLines.new
+    end
+  end
 end # class LogStash::Inputs::Base
diff --git a/lib/logstash/inputs/generator.rb b/lib/logstash/inputs/generator.rb
index 18813f2b3fb..45f50ed77fe 100644
--- a/lib/logstash/inputs/generator.rb
+++ b/lib/logstash/inputs/generator.rb
@@ -39,7 +39,7 @@ class LogStash::Inputs::Generator < LogStash::Inputs::Threadable
   #       }
   #     }
   #
-  # The above will emit "line 1" then "line 2" then "line", then "line 1", etc... 
+  # The above will emit "line 1" then "line 2" then "line", then "line 1", etc...
   config :lines, :validate => :array
 
   # Set how many messages should be generated.
@@ -51,7 +51,6 @@ class LogStash::Inputs::Generator < LogStash::Inputs::Threadable
   def register
     @host = Socket.gethostname
     @count = @count.first if @count.is_a?(Array)
-    @lines = [@message] if @lines.nil?
   end # def register
 
   def run(queue)
@@ -62,6 +61,7 @@ def run(queue)
       @message = $stdin.readline
       @logger.debug("Generator line read complete", :message => @message)
     end
+    @lines = [@message] if @lines.nil?
 
     while !finished? && (@count <= 0 || number < @count)
       @lines.each do |line|
diff --git a/lib/logstash/inputs/pipe.rb b/lib/logstash/inputs/pipe.rb
index 143b8ef1418..933c602102c 100644
--- a/lib/logstash/inputs/pipe.rb
+++ b/lib/logstash/inputs/pipe.rb
@@ -46,6 +46,8 @@ def run(queue)
             queue << event
           end
         end
+      rescue LogStash::ShutdownSignal => e
+        break
       rescue Exception => e
         @logger.error("Exception while running command", :e => e, :backtrace => e.backtrace)
       end
diff --git a/lib/logstash/inputs/stdin.rb b/lib/logstash/inputs/stdin.rb
index bc9756fffd7..d065e2b09ee 100644
--- a/lib/logstash/inputs/stdin.rb
+++ b/lib/logstash/inputs/stdin.rb
@@ -16,6 +16,7 @@ class LogStash::Inputs::Stdin < LogStash::Inputs::Base
   public
   def register
     @host = Socket.gethostname
+    fix_streaming_codecs
   end # def register
 
   def run(queue) 
diff --git a/lib/logstash/inputs/tcp.rb b/lib/logstash/inputs/tcp.rb
index 07cdf46738b..f328be16022 100644
--- a/lib/logstash/inputs/tcp.rb
+++ b/lib/logstash/inputs/tcp.rb
@@ -62,6 +62,13 @@ def register
     require "socket"
     require "timeout"
     require "openssl"
+
+    # monkey patch TCPSocket and SSLSocket to include socket peer
+    TCPSocket.module_eval{include ::LogStash::Util::SocketPeer}
+    OpenSSL::SSL::SSLSocket.module_eval{include ::LogStash::Util::SocketPeer}
+
+    fix_streaming_codecs
+
     if @ssl_enable
       @ssl_context = OpenSSL::SSL::SSLContext.new
       @ssl_context.cert = OpenSSL::X509::Certificate.new(File.read(@ssl_cert))
@@ -85,8 +92,7 @@ def register
       begin
         @server_socket = TCPServer.new(@host, @port)
       rescue Errno::EADDRINUSE
-        @logger.error("Could not start TCP server: Address in use",
-                      :host => @host, :port => @port)
+        @logger.error("Could not start TCP server: Address in use", :host => @host, :port => @port)
         raise
       end
       if @ssl_enable
@@ -99,8 +105,7 @@ def register
   def handle_socket(socket, client_address, output_queue, codec)
     while true
       buf = nil
-      # NOTE(petef): the timeout only hits after the line is read
-      # or socket dies
+      # NOTE(petef): the timeout only hits after the line is read or socket dies
       # TODO(sissel): Why do we have a timeout here? What's the point?
       if @data_timeout == -1
         buf = read(socket)
@@ -115,14 +120,16 @@ def handle_socket(socket, client_address, output_queue, codec)
         decorate(event)
         output_queue << event
       end
-    end # loop do
+    end # loop
   rescue EOFError
-    @logger.debug("Connection closed", :client => socket.peer)
+    @logger.debug? && @logger.debug("Connection closed", :client => socket.peer)
+  rescue Errno::ECONNRESET
+    @logger.debug? && @logger.debug("Connection reset by peer", :client => socket.peer)
   rescue => e
-    @logger.debug("An error occurred. Closing connection",
-                  :client => socket.peer, :exception => e, :backtrace => e.backtrace)
+    @logger.error("An error occurred. Closing connection", :client => socket.peer, :exception => e, :backtrace => e.backtrace)
   ensure
-    socket.close rescue IOError nil
+    socket.close rescue nil
+
     codec.respond_to?(:flush) && codec.flush do |event|
       event["host"] ||= client_address
       event["sslsubject"] ||= socket.peer_cert.subject if @ssl_enable && @ssl_verify
@@ -131,6 +138,20 @@ def handle_socket(socket, client_address, output_queue, codec)
     end
   end
 
+  private
+  def client_thread(output_queue, socket)
+    Thread.new(output_queue, socket) do |q, s|
+      begin
+        @logger.debug? && @logger.debug("Accepted connection", :client => s.peer, :server => "#{@host}:#{@port}")
+        handle_socket(s, s.peer, q, @codec.clone)
+      rescue Interrupted
+        s.close rescue nil
+      ensure
+        @client_threads_lock.synchronize{@client_threads.delete(Thread.current)}
+      end
+    end
+  end
+
   private
   def server?
     @mode == "server"
@@ -153,36 +174,29 @@ def run(output_queue)
   def run_server(output_queue)
     @thread = Thread.current
     @client_threads = []
-    loop do
-      # Start a new thread for each connection.
+    @client_threads_lock = Mutex.new
+
+    while true
       begin
-        @client_threads << Thread.start(@server_socket.accept) do |s|
-          # TODO(sissel): put this block in its own method.
-
-          # monkeypatch a 'peer' method onto the socket.
-          s.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
-          @logger.debug("Accepted connection", :client => s.peer,
-                        :server => "#{@host}:#{@port}")
-          begin
-            handle_socket(s, s.peer, output_queue, @codec.clone)
-          rescue Interrupted
-            s.close rescue nil
-          end
-        end # Thread.start
+        socket = @server_socket.accept
+        # start a new thread for each connection.
+        @client_threads_lock.synchronize{@client_threads << client_thread(output_queue, socket)}
       rescue OpenSSL::SSL::SSLError => ssle
         # NOTE(mrichar1): This doesn't return a useful error message for some reason
-        @logger.error("SSL Error", :exception => ssle,
-                      :backtrace => ssle.backtrace)
+        @logger.error("SSL Error", :exception => ssle, :backtrace => ssle.backtrace)
       rescue IOError, LogStash::ShutdownSignal
         if @interrupted
-          # Intended shutdown, get out of the loop
-          @server_socket.close
-          @client_threads.each do |thread|
-            thread.raise(LogStash::ShutdownSignal)
+          @server_socket.close rescue nil
+
+          threads = @client_threads_lock.synchronize{@client_threads.dup}
+          threads.each do |thread|
+            thread.raise(LogStash::ShutdownSignal) if thread.alive?
           end
+
+          # intended shutdown, get out of the loop
           break
         else
-          # Else it was a genuine IOError caused by something else, so propagate it up..
+          # it was a genuine IOError, propagate it up
           raise
         end
       end
@@ -193,7 +207,7 @@ def run_server(output_queue)
     @server_socket.close rescue nil
   end # def run_server
 
-  def run_client(output_queue) 
+  def run_client(output_queue)
     @thread = Thread.current
     while true
       client_socket = TCPSocket.new(@host, @port)
@@ -202,19 +216,17 @@ def run_client(output_queue)
         begin
           client_socket.connect
         rescue OpenSSL::SSL::SSLError => ssle
-          @logger.error("SSL Error", :exception => ssle,
-                        :backtrace => ssle.backtrace)
+          @logger.error("SSL Error", :exception => ssle, :backtrace => ssle.backtrace)
           # NOTE(mrichar1): Hack to prevent hammering peer
           sleep(5)
           next
         end
       end
-      client_socket.instance_eval { class << self; include ::LogStash::Util::SocketPeer end }
       @logger.debug("Opened connection", :client => "#{client_socket.peer}")
       handle_socket(client_socket, client_socket.peer, output_queue, @codec.clone)
     end # loop
   ensure
-    client_socket.close
+    client_socket.close rescue nil
   end # def run
 
   public
diff --git a/lib/logstash/outputs/elasticsearch.rb b/lib/logstash/outputs/elasticsearch.rb
index 6b92e346626..2e6458b094d 100644
--- a/lib/logstash/outputs/elasticsearch.rb
+++ b/lib/logstash/outputs/elasticsearch.rb
@@ -269,15 +269,9 @@ def register
   public
   def get_template
     if @template.nil?
-      if File.exists?("elasticsearch-template.json")
-        @template = "elasticsearch-template.json"
-      else
-        path = File.join(File.dirname(__FILE__), "elasticsearch/elasticsearch-template.json")
-        if File.exists?(path)
-          @template = path
-        else
-          raise "You must specify 'template => ...' in your elasticsearch_http output"
-        end
+      @template = LogStash::Environment.plugin_path("outputs/elasticsearch/elasticsearch-template.json")
+      if !File.exists?(@template)
+        raise "You must specify 'template => ...' in your elasticsearch output (I looked for '#{@template}')"
       end
     end
     template_json = IO.read(@template).gsub(/\n/,'')
diff --git a/lib/logstash/outputs/elasticsearch_http.rb b/lib/logstash/outputs/elasticsearch_http.rb
index 32a85d0f27f..496e108e809 100644
--- a/lib/logstash/outputs/elasticsearch_http.rb
+++ b/lib/logstash/outputs/elasticsearch_http.rb
@@ -173,17 +173,14 @@ def template_action(command)
   public
   def get_template_json
     if @template.nil?
-      if File.exists?("elasticsearch-template.json")
-        @template = "elasticsearch-template.json"
-      elsif File.exists?("lib/logstash/outputs/elasticsearch/elasticsearch-template.json")
-        @template = "lib/logstash/outputs/elasticsearch/elasticsearch-template.json"
-      else
-        raise "You must specify 'template => ...' in your elasticsearch_http output"
+      @template = LogStash::Environment.plugin_path("outputs/elasticsearch/elasticsearch-template.json")
+      if !File.exists?(@template)
+        raise "You must specify 'template => ...' in your elasticsearch_http output (I looked for '#{@template}')"
       end
     end
     @template_json = IO.read(@template).gsub(/\n/,'')
     @logger.info("Using mapping template", :template => @template_json)
-  end # def get_template
+  end # def get_template_json
 
   public
   def receive(event)
diff --git a/lib/logstash/outputs/file.rb b/lib/logstash/outputs/file.rb
index 4ca7b98ec50..bb39c0c58e5 100644
--- a/lib/logstash/outputs/file.rb
+++ b/lib/logstash/outputs/file.rb
@@ -1,22 +1,27 @@
 # encoding: utf-8
 require "logstash/namespace"
 require "logstash/outputs/base"
+require "logstash/errors"
 require "zlib"
 
 # This output will write events to files on disk. You can use fields
 # from the event as parts of the filename and/or path.
 class LogStash::Outputs::File < LogStash::Outputs::Base
+  FIELD_REF = /%\{[^}]+\}/
 
   config_name "file"
   milestone 2
 
-  # The path to the file to write. Event fields can be used here, 
-  # like "/var/log/logstash/%{host}/%{application}"
-  # One may also utilize the path option for date-based log 
+  # The path to the file to write. Event fields can be used here,
+  # like `/var/log/logstash/%{host}/%{application}`
+  # One may also utilize the path option for date-based log
   # rotation via the joda time format. This will use the event
   # timestamp.
-  # E.g.: path => "./test-%{+YYYY-MM-dd}.txt" to create 
-  # ./test-2013-05-29.txt 
+  # E.g.: `path => "./test-%{+YYYY-MM-dd}.txt"` to create
+  # `./test-2013-05-29.txt`
+  #
+  # If you use an absolute path you cannot start with a dynamic string.
+  # E.g: `/%{myfield}/`, `/test-%{myfield}/` are not valid paths
   config :path, :validate => :string, :required => true
 
   # The maximum size of file to write. When the file exceeds this
@@ -28,20 +33,24 @@ class LogStash::Outputs::File < LogStash::Outputs::Base
   config :max_size, :validate => :string
 
   # The format to use when writing events to the file. This value
-  # supports any string and can include %{name} and other dynamic
+  # supports any string and can include `%{name}` and other dynamic
   # strings.
   #
   # If this setting is omitted, the full json representation of the
   # event will be written as a single line.
   config :message_format, :validate => :string
 
-  # Flush interval (in seconds) for flushing writes to log files. 
+  # Flush interval (in seconds) for flushing writes to log files.
   # 0 will flush on every message.
   config :flush_interval, :validate => :number, :default => 2
 
   # Gzip the output stream before writing to disk.
   config :gzip, :validate => :boolean, :default => false
 
+  # If the generated path is invalid, the events will be saved
+  # into this file and inside the defined path.
+  config :filename_failure, :validate => :string, :default => '_filepath_failures'
+
   public
   def register
     require "fileutils" # For mkdir_p
@@ -49,35 +58,58 @@ def register
     workers_not_supported
 
     @files = {}
+    
+    @path = File.expand_path(path)
+
+    validate_path
+
+    if path_with_field_ref?
+      @file_root = extract_file_root
+      @failure_path = File.join(@file_root, @filename_failure)
+    end
+
     now = Time.now
     @last_flush_cycle = now
     @last_stale_cleanup_cycle = now
-    flush_interval = @flush_interval.to_i
+    @flush_interval = @flush_interval.to_i
     @stale_cleanup_interval = 10
   end # def register
 
+  private
+  def validate_path
+    if (root_directory =~ FIELD_REF) != nil
+      @logger.error("File: The starting part of the path should not be dynamic.", :path => @path)
+      raise LogStash::ConfigurationError.new("The starting part of the path should not be dynamic.")
+    end
+  end
+
+  private
+  def root_directory
+    parts = @path.split(File::SEPARATOR).select { |item| !item.empty?  }
+    if Gem.win_platform?
+      # First part is the drive letter
+      parts[1]
+    else
+      parts.first
+    end
+  end
+
   public
   def receive(event)
     return unless output?(event)
 
-    path = event.sprintf(@path)
-    fd = open(path)
-
-    # TODO(sissel): Check if we should rotate the file.
+    file_output_path = generate_filepath(event)
 
-    if @message_format
-      output = event.sprintf(@message_format)
-    else
-      output = event.to_json
+    if path_with_field_ref? && !inside_file_root?(file_output_path)
+      @logger.warn("File: the event tried to write outside the files root, writing the event to the failure file",  :event => event, :filename => @failure_path)
+      file_output_path = @failure_path
     end
 
-    fd.write(output)
-    fd.write("\n")
-
-    flush(fd)
-    close_stale_files
+    output = format_message(event)
+    write_event(file_output_path, output)
   end # def receive
 
+  public
   def teardown
     @logger.debug("Teardown: closing files")
     @files.each do |path, fd|
@@ -85,12 +117,57 @@ def teardown
         fd.close
         @logger.debug("Closed file #{path}", :fd => fd)
       rescue Exception => e
-        @logger.error("Excpetion while flushing and closing files.", :exception => e)
+        @logger.error("Exception while flushing and closing files.", :exception => e)
       end
     end
     finished
   end
 
+  private
+  def inside_file_root?(log_path)
+    target_file = File.expand_path(log_path)
+    return target_file.start_with?("#{@file_root.to_s}/")
+  end
+
+  private
+  def write_event(log_path, event)
+    @logger.debug("File, writing event to file.", :filename => log_path)
+    fd = open(log_path)
+
+    # TODO(sissel): Check if we should rotate the file.
+
+    fd.write(event)
+    fd.write("\n")
+
+    flush(fd)
+    close_stale_files
+  end
+
+  private
+  def generate_filepath(event)
+    event.sprintf(@path)
+  end
+
+  private
+  def path_with_field_ref?
+    path =~ FIELD_REF
+  end
+
+  private
+  def format_message(event)
+    if @message_format
+      event.sprintf(@message_format)
+    else
+      event.to_json
+    end
+  end
+
+  private
+  def extract_file_root
+    parts = File.expand_path(path).split(File::SEPARATOR)
+    parts.take_while { |part| part !~ FIELD_REF }.join(File::SEPARATOR)
+  end
+
   private
   def flush(fd)
     if flush_interval > 0
@@ -101,6 +178,7 @@ def flush(fd)
   end
 
   # every flush_interval seconds or so (triggered by events, but if there are no events there's no point flushing files anyway)
+  private
   def flush_pending_files
     return unless Time.now - @last_flush_cycle >= flush_interval
     @logger.debug("Starting flush cycle")
@@ -112,6 +190,7 @@ def flush_pending_files
   end
 
   # every 10 seconds or so (triggered by events, but if there are no events there's no point closing files anyway)
+  private
   def close_stale_files
     now = Time.now
     return unless now - @last_stale_cleanup_cycle >= @stale_cleanup_interval
@@ -128,6 +207,7 @@ def close_stale_files
     @last_stale_cleanup_cycle = now
   end
 
+  private
   def open(path)
     return @files[path] if @files.include?(path) and not @files[path].nil?
 
@@ -136,7 +216,7 @@ def open(path)
     dir = File.dirname(path)
     if !Dir.exists?(dir)
       @logger.info("Creating directory", :directory => dir)
-      FileUtils.mkdir_p(dir) 
+      FileUtils.mkdir_p(dir)
     end
 
     # work around a bug opening fifos (bug JRUBY-6280)
diff --git a/lib/logstash/outputs/nagios_nsca.rb b/lib/logstash/outputs/nagios_nsca.rb
index 3e721de953f..90f761e0fa5 100644
--- a/lib/logstash/outputs/nagios_nsca.rb
+++ b/lib/logstash/outputs/nagios_nsca.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/outputs/base"
 require "logstash/namespace"
+require "open3"
 
 # The nagios_nsca output is used for sending passive check results to Nagios
 # through the NSCA protocol.
@@ -105,19 +106,27 @@ def receive(event)
     # build the command
     # syntax: echo '<server>!<nagios_service>!<status>!<text>'  | \
     #           /usr/sbin/send_nsca -H <nagios_host> -d '!' -c <nsca_config>"
-    cmd = %(echo '#{nagios_host}~#{nagios_service}~#{status}~#{msg}' |)
-    cmd << %( #{@send_nsca_bin} -H #{@host} -p #{@port} -d '~')
-    cmd << %( -c #{@send_nsca_config}) if @send_nsca_config
-    cmd << %( 2>/dev/null >/dev/null)
-    @logger.debug("Running send_nsca command", "nagios_nsca_command" => cmd)
+
+    cmd = [@send_nsca_bin, "-H", @host, "-p", @port, "-d", "~"]
+    cmd = cmd + ["-c", @send_nsca_config]  if @send_nsca_config
+    message = "#{nagios_host}~#{nagios_service}~#{status}~#{msg}"
+
+    @logger.debug("Running send_nsca command", :nagios_nsca_command => cmd.join(" "), :message => message)
 
     begin
-      system cmd
+      Open3.popen3(*cmd) do |i, o, e|
+        i.puts(message)
+        i.close
+      end
     rescue => e
-      @logger.warn("Skipping nagios_nsca output; error calling send_nsca",
-                   "error" => $!, "nagios_nsca_command" => cmd,
-                   "missed_event" => event)
-      @logger.debug("Backtrace", e.backtrace)
+      @logger.warn(
+        "Skipping nagios_nsca output; error calling send_nsca",
+        :error => $!,
+        :nagios_nsca_command => cmd.join(" "),
+        :message => message,
+        :missed_event => event
+      )
+      @logger.debug("Backtrace", :backtrace => e.backtrace)
     end
   end # def receive
 end # class LogStash::Outputs::NagiosNsca
diff --git a/lib/logstash/patches.rb b/lib/logstash/patches.rb
new file mode 100644
index 00000000000..43bc0d1e165
--- /dev/null
+++ b/lib/logstash/patches.rb
@@ -0,0 +1 @@
+require "logstash/patches/stronger_openssl_defaults"
diff --git a/lib/logstash/patches/stronger_openssl_defaults.rb b/lib/logstash/patches/stronger_openssl_defaults.rb
new file mode 100644
index 00000000000..a1d8419d9f8
--- /dev/null
+++ b/lib/logstash/patches/stronger_openssl_defaults.rb
@@ -0,0 +1,67 @@
+require "openssl"
+
+# :nodoc:
+class OpenSSL::SSL::SSLContext
+  # Wrap SSLContext.new to a stronger default settings.
+  class << self
+    alias_method :orig_new, :new
+    def new(*args)
+      c = orig_new(*args)
+
+      # MRI nor JRuby seem to actually invoke `SSLContext#set_params` by
+      # default, which makes the default ciphers (and other settings) not
+      # actually defaults. Oops!
+      # To force this, and force our (hopefully more secure) defaults on
+      # all things using openssl in Ruby, we will invoke set_params
+      # on all new SSLContext objects.
+      c.set_params
+      c
+    end
+  end
+
+  # This cipher selection comes from https://wiki.mozilla.org/Security/Server_Side_TLS
+  MOZILLA_INTERMEDIATE_CIPHERS = "ECDHE-RSA-AES128-GCM-SHA256:ECDHE-ECDSA-AES128-GCM-SHA256:ECDHE-RSA-AES256-GCM-SHA384:ECDHE-ECDSA-AES256-GCM-SHA384:DHE-RSA-AES128-GCM-SHA256:DHE-DSS-AES128-GCM-SHA256:kEDH+AESGCM:ECDHE-RSA-AES128-SHA256:ECDHE-ECDSA-AES128-SHA256:ECDHE-RSA-AES128-SHA:ECDHE-ECDSA-AES128-SHA:ECDHE-RSA-AES256-SHA384:ECDHE-ECDSA-AES256-SHA384:ECDHE-RSA-AES256-SHA:ECDHE-ECDSA-AES256-SHA:DHE-RSA-AES128-SHA256:DHE-RSA-AES128-SHA:DHE-DSS-AES128-SHA256:DHE-RSA-AES256-SHA256:DHE-DSS-AES256-SHA:DHE-RSA-AES256-SHA:AES128-GCM-SHA256:AES256-GCM-SHA384:AES128-SHA256:AES256-SHA256:AES128-SHA:AES256-SHA:AES:CAMELLIA:DES-CBC3-SHA:!aNULL:!eNULL:!EXPORT:!DES:!RC4:!MD5:!PSK:!aECDH:!EDH-DSS-DES-CBC3-SHA:!EDH-RSA-DES-CBC3-SHA:!KRB5-DES-CBC3-SHA"
+
+  # Returns the value that should be used for the default SSLContext options
+  #
+  # This is a method instead of a constant because some constants (like
+  # OpenSSL::SSL::OP_NO_COMPRESSION) may not be available in all Ruby
+  # versions/platforms.
+  def self.__default_options
+    # ruby-core is refusing to patch ruby's default openssl settings to be more
+    # secure, so let's fix that here. The next few lines setting options and
+    # ciphers come from jmhodges' proposed patch
+    ssloptions = OpenSSL::SSL::OP_ALL
+ 
+    # TODO(sissel): JRuby doesn't have this. Maybe work on a fix?
+    if defined?(OpenSSL::SSL::OP_DONT_INSERT_EMPTY_FRAGMENTS)
+      ssloptions &= ~OpenSSL::SSL::OP_DONT_INSERT_EMPTY_FRAGMENTS
+    end
+
+    # TODO(sissel): JRuby doesn't have this. Maybe work on a fix?
+    if defined?(OpenSSL::SSL::OP_NO_COMPRESSION)
+      ssloptions |= OpenSSL::SSL::OP_NO_COMPRESSION
+    end
+
+    # Disable SSLv2 and SSLv3. They are insecure and highly discouraged.
+    ssloptions |= OpenSSL::SSL::OP_NO_SSLv2 if defined?(OpenSSL::SSL::OP_NO_SSLv2)
+    ssloptions |= OpenSSL::SSL::OP_NO_SSLv3 if defined?(OpenSSL::SSL::OP_NO_SSLv3)
+    ssloptions
+  end
+
+  # Overwriting the DEFAULT_PARAMS const idea from here: https://www.ruby-lang.org/en/news/2014/10/27/changing-default-settings-of-ext-openssl/
+  #
+  # This monkeypatch doesn't enforce a `VERIFY_MODE` on the SSLContext,
+  # SSLContext are both used for the client and the server implementation,
+  # If set the `verify_mode` to peer the server wont accept any connection,
+  # because it will try to verify the client certificate, this is a protocol
+  # details implemented at the plugin level.
+  #
+  # For more details see: https://github.com/elastic/logstash/issues/3657
+  remove_const(:DEFAULT_PARAMS) if const_defined?(:DEFAULT_PARAMS)
+  DEFAULT_PARAMS = {
+    :ssl_version => "SSLv23",
+    :ciphers => MOZILLA_INTERMEDIATE_CIPHERS,
+    :options => __default_options # Not a constant because it's computed at start-time.
+  }
+end
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index 4a396b0802c..4c207794327 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -41,6 +41,7 @@ module Cabin::Mixins::Logger
 end # PROFILE_BAD_LOG_CALLS
 
 require "logstash/monkeypatches-for-debugging"
+require "logstash/patches"
 require "logstash/namespace"
 require "logstash/program"
 require "i18n" # gem 'i18n'
diff --git a/lib/logstash/version.rb b/lib/logstash/version.rb
index 4094d0f02d5..e622838e813 100644
--- a/lib/logstash/version.rb
+++ b/lib/logstash/version.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 # The version of logstash.
-LOGSTASH_VERSION = "1.4.1"
+LOGSTASH_VERSION = "1.4.4"
 
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
diff --git a/logstash.gemspec b/logstash.gemspec
index 4917d83ed30..e70e96de2c1 100644
--- a/logstash.gemspec
+++ b/logstash.gemspec
@@ -25,7 +25,7 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "i18n", [">=0.6.6"]  #(MIT license)
 
   # Web dependencies
-  gem.add_runtime_dependency "ftw", ["~> 0.0.39"] #(Apache 2.0 license)
+  gem.add_runtime_dependency "ftw", ["~> 0.0.42"] #(Apache 2.0 license)
   gem.add_runtime_dependency "mime-types"         #(GPL 2.0)
   gem.add_runtime_dependency "rack"               # (MIT-style license)
   gem.add_runtime_dependency "sinatra"            # (MIT-style license)
@@ -49,7 +49,7 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "statsd-ruby", ["1.2.0"]           #(MIT license)
   gem.add_runtime_dependency "xml-simple"                       #(ruby license?)
   gem.add_runtime_dependency "xmpp4r", ["0.5"]                  #(ruby license)
-  gem.add_runtime_dependency "jls-lumberjack", [">=0.0.20"]     #(Apache 2.0 license)
+  gem.add_runtime_dependency "jls-lumberjack", [">=0.0.22"]     #(Apache 2.0 license)
   gem.add_runtime_dependency "geoip", [">= 1.3.2"]              #(GPL license)
   gem.add_runtime_dependency "beefcake", "0.3.7"                #(MIT license)
   gem.add_runtime_dependency "murmurhash3"                      #(MIT license)
@@ -58,7 +58,7 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "snmp"                             #(ruby license)
   gem.add_runtime_dependency "rbnacl"                           #(MIT license)
   gem.add_runtime_dependency "bindata", [">= 1.5.0"]            #(ruby license)
-  gem.add_runtime_dependency "twitter", "5.0.0.rc.1"            #(MIT license)
+  gem.add_runtime_dependency "twitter", "5.11.0"            #(MIT license)
   gem.add_runtime_dependency "edn"                              #(MIT license)
   gem.add_runtime_dependency "elasticsearch"                    #9Apache 2.0 license)
 
@@ -103,6 +103,7 @@ Gem::Specification.new do |gem|
   # Development Deps
   gem.add_development_dependency "coveralls"
   gem.add_development_dependency "kramdown"     # pure-ruby markdown parser (MIT license)
+  gem.add_runtime_dependency "flores", "~>0.0.6"
 
   # Jenkins Deps
   gem.add_runtime_dependency "ci_reporter"
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
index bb7555e1ab1..fddc14d5ed9 100755
--- a/pkg/logstash.sysv
+++ b/pkg/logstash.sysv
@@ -48,6 +48,7 @@ start() {
 
 
   JAVA_OPTS=${LS_JAVA_OPTS}
+  HOME=${LS_HOME}
   export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
 
   # set ulimit as (root, presumably) first, before we drop privileges
diff --git a/spec/codecs/collectd.rb b/spec/codecs/collectd.rb
index 0233e4427b1..fb638a95090 100644
--- a/spec/codecs/collectd.rb
+++ b/spec/codecs/collectd.rb
@@ -56,8 +56,99 @@
       # One of these will fail because I altered the payload from the normal packet
       insist { counter } == 27
     end # it "should drop a part with an header length"
+
+    # This payload contains a NaN value
+    it "should replace a NaN with a zero and add tag '_collectdNaN' by default" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 0   # Not a NaN
+          insist { event['tags'] } == ["_collectdNaN"]
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with a zero and add tag '_collectdNaN' by default"
   end # context "None"
 
+  context "Replace nan_value and nan_tag with non-default values" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_value" => 1,
+                                           "nan_tag" => "NaN_encountered"})
+    end
+    # This payload contains a NaN value
+    it "should replace a NaN with the specified value and tag 'NaN_encountered'" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 1   # Not a NaN
+          insist { event['tags'] } == ["NaN_encountered"]
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with the specified value and tag 'NaN_encountered'"
+  end # context "Replace nan_value and nan_tag with non-default values"
+
+  context "Warn on NaN event" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_handling" => "warn"})
+    end
+    # This payload contains a NaN value
+    it "should replace a NaN with a zero and receive a warning when 'nan_handling' set to warn" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.logger.should_receive(:warn).with("NaN replaced by 0")
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == 0   # Not a NaN
+        end
+        counter += 1
+      end
+      insist { counter } == 1
+    end # it "should replace a NaN with a zero and receive a warning when 'nan_handling' set to warn"
+  end # context "Warn on NaN event"
+
+  context "Drop NaN event" do
+    subject do
+      next LogStash::Codecs::Collectd.new({"nan_handling" => "drop"})
+    end
+    # This payload contains a NaN value
+    it "should drop an event with a NaN value when 'nan_handling' set to drop" do
+      payload = ["00000015746573742e6578616d706c652e636f6d000008000c14dc4c81831ef78b0009000c00000000400000000002000970696e67000004000970696e67000005001c70696e672d7461726765742e6578616d706c652e636f6d000006000f000101000000000000f87f"].pack('H*')
+      counter = 0
+      subject.decode(payload) do |event|
+        case counter
+        when 0
+          insist { event['host'] } == "test.example.com"
+          insist { event['plugin'] } == "ping"
+          insist { event['type_instance'] } == "ping-target.example.com"
+          insist { event['collectd_type'] } == "ping"
+          insist { event['value'] } == NaN   # NaN
+        end
+        counter += 1 # Because we're dropping this, it should not increment
+      end
+      insist { counter } == 0 # We expect no increment
+    end # it "should drop an event with a NaN value when 'nan_handling' set to drop"
+  end # context "Drop NaN event"
+
   # Create an authfile for the next tests
   authfile = Tempfile.new('logstash-collectd-authfile')
   File.open(authfile.path, "a") do |fd|
diff --git a/spec/inputs/generator.rb b/spec/inputs/generator.rb
index 45579f620d2..b21ffaeb77f 100644
--- a/spec/inputs/generator.rb
+++ b/spec/inputs/generator.rb
@@ -1,9 +1,9 @@
 require "test_utils"
 
-describe "inputs/generator", :performance => true do
+describe "inputs/generator" do
   extend LogStash::RSpec
 
-  describe "generate events" do
+  context "performance", :performance => true do
     event_count = 100000 + rand(50000)
 
     config <<-CONFIG
@@ -27,4 +27,60 @@
       pipeline.shutdown
     end # input
   end
+
+  context "generate configured message" do
+    config <<-CONFIG
+      input {
+        generator {
+          count => 2
+          message => "foo"
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      event = queue.pop
+      insist { event["sequence"] } == 0
+      insist { event["message"] } == "foo"
+
+      event = queue.pop
+      insist { event["sequence"] } == 1
+      insist { event["message"] } == "foo"
+
+      insist { queue.size } == 0
+      pipeline.shutdown
+    end # input
+
+    context "generate message from stdin" do
+      config <<-CONFIG
+        input {
+          generator {
+            count => 2
+            message => "stdin"
+          }
+        }
+      CONFIG
+
+      input do |pipeline, queue|
+        saved_stdin = $stdin
+        stdin_mock = StringIO.new
+        $stdin = stdin_mock
+        stdin_mock.should_receive(:readline).once.and_return("bar")
+
+        Thread.new { pipeline.run }
+        event = queue.pop
+        insist { event["sequence"] } == 0
+        insist { event["message"] } == "bar"
+
+        event = queue.pop
+        insist { event["sequence"] } == 1
+        insist { event["message"] } == "bar"
+
+        insist { queue.size } == 0
+        pipeline.shutdown
+        $stdin = saved_stdin
+      end # input
+    end
+  end
 end
diff --git a/spec/inputs/pipe.rb b/spec/inputs/pipe.rb
new file mode 100644
index 00000000000..067937b4a75
--- /dev/null
+++ b/spec/inputs/pipe.rb
@@ -0,0 +1,60 @@
+# encoding: utf-8
+require "test_utils"
+require "tempfile"
+
+describe "inputs/pipe" do
+  extend LogStash::RSpec
+
+  describe "echo" do
+    event_count = 1
+    tmp_file = Tempfile.new('logstash-spec-input-pipe')
+
+    config <<-CONFIG
+    input {
+      pipe {
+        command => "echo "
+      }
+    }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      events = event_count.times.collect { queue.pop }
+      event_count.times do |i|
+        insist { events[i]["message"] } == ""
+      end
+    end # input
+  end
+
+  describe "tail -f" do
+    event_count = 10
+    tmp_file = Tempfile.new('logstash-spec-input-pipe')
+
+    config <<-CONFIG
+    input {
+      pipe {
+        command => "tail -f #{tmp_file.path}"
+      }
+    }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      File.open(tmp_file, "a") do |fd|
+        event_count.times do |i|
+          # unicode smiley for testing unicode support!
+          fd.puts("#{i} ")
+        end
+      end
+      events = event_count.times.collect { queue.pop }
+      event_count.times do |i|
+        insist { events[i]["message"] } == "#{i} "
+      end
+    end # input
+  end
+
+end
diff --git a/spec/inputs/tcp.rb b/spec/inputs/tcp.rb
index e4ea8312aba..f6b17a0ad59 100644
--- a/spec/inputs/tcp.rb
+++ b/spec/inputs/tcp.rb
@@ -1,8 +1,9 @@
 # coding: utf-8
 require "test_utils"
 require "socket"
+require "timeout"
 
-describe "inputs/tcp", :socket => true do
+describe "inputs/tcp" do
   extend LogStash::RSpec
 
   describe "read plain with unicode" do
@@ -27,6 +28,9 @@
       end
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
       events = event_count.times.collect { queue.pop }
       event_count.times do |i|
         insist { events[i]["message"] } == "#{i} "
@@ -56,6 +60,9 @@
       socket.puts(text)
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
       event = queue.pop
       # Make sure the 0xA3 latin-1 code converts correctly to UTF-8.
       pending("charset conv broken") do
@@ -92,9 +99,12 @@
       socket.puts(data.to_json)
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
       event = queue.pop
       insist { event["hello"] } == data["hello"]
-      insist { event["foo"] } == data["foo"]
+      insist { event["foo"].to_a } == data["foo"] # to_a to cast Java ArrayList produced by JrJackson
       insist { event["baz"] } == data["baz"]
 
       # Make sure the tcp input, w/ json codec, uses the event's 'host' value,
@@ -126,6 +136,9 @@
       socket.puts(data.to_json)
       socket.close
 
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < 1}
+
       event = queue.pop
       insist { event["hello"] } == data["hello"]
       insist { event }.include?("host")
@@ -164,12 +177,86 @@
       (1..5).each do |idx|
         event = queue.pop
         insist { event["hello"] } == data["hello"]
-        insist { event["foo"] } == data["foo"]
+        insist { event["foo"].to_a } == data["foo"] # to_a to cast Java ArrayList produced by JrJackson
         insist { event["baz"] } == data["baz"]
         insist { event["idx"] } == idx
       end # do
     end # input
   end # describe
+
+  describe "one message per connection" do
+    event_count = 10
+    port = 5515
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      event_count.times do |i|
+        socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
+        socket.puts("#{i}")
+        socket.flush
+        socket.close
+      end
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
+      # since each message is sent on its own tcp connection & thread, exact receiving order cannot be garanteed
+      events = event_count.times.collect{queue.pop}.sort_by{|event| event["message"]}
+
+      event_count.times do |i|
+        insist { events[i]["message"] } == "#{i}"
+      end
+    end # input
+  end
+
+  describe "connection threads are cleaned up when connection is closed" do
+    event_count = 10
+    port = 5515
+    config <<-CONFIG
+      input {
+        tcp {
+          port => #{port}
+        }
+      }
+    CONFIG
+
+    input do |pipeline, queue|
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+
+      inputs = pipeline.instance_variable_get("@inputs")
+      insist { inputs.size } == 1
+
+      sockets = event_count.times.map do |i|
+        socket = Stud.try(5.times) { TCPSocket.new("127.0.0.1", port) }
+        socket.puts("#{i}")
+        socket.flush
+        socket
+      end
+
+      # wait till all events have been processed
+      Timeout.timeout(1) {sleep 0.1 while queue.size < event_count}
+
+      # we should have "event_count" pending threads since sockets were not closed yet
+      client_threads = inputs[0].instance_variable_get("@client_threads")
+      insist { client_threads.size } == event_count
+
+      # close all sockets and make sure there is not more pending threads
+      sockets.each{|socket| socket.close}
+      Timeout.timeout(1) {sleep 0.1 while client_threads.size > 0}
+      insist { client_threads.size } == 0 # this check is actually useless per previous line
+
+    end # input
+  end
 end
 
 
diff --git a/spec/logstash/patches_spec.rb b/spec/logstash/patches_spec.rb
new file mode 100644
index 00000000000..f22db6d07e5
--- /dev/null
+++ b/spec/logstash/patches_spec.rb
@@ -0,0 +1,90 @@
+# encoding: utf-8
+require "socket"
+require "logstash/patches"
+require "flores/pki"
+
+describe "OpenSSL defaults" do
+  subject { OpenSSL::SSL::SSLContext.new }
+
+  # OpenSSL::SSL::SSLContext#ciphers returns an array of 
+  # [ [ ciphername, version, bits, alg_bits ], [ ... ], ... ]
+ 
+  # List of cipher names
+  let(:ciphers) { subject.ciphers.map(&:first) }
+
+  # List of cipher encryption bit strength. 
+  let(:encryption_bits) { subject.ciphers.map { |_, _, _, a| a } }
+
+  it "should not include any export ciphers" do
+    # SSLContext#ciphers returns an array of [ciphername, tlsversion, key_bits, alg_bits]
+    # Let's just check the cipher names
+    expect(ciphers).not_to be_any { |name| name =~ /EXPORT/ || name =~ /^EXP/ }
+  end
+
+  it "should not include any weak ciphers (w/ less than 128 bits in encryption algorithm)" do
+    # SSLContext#ciphers returns an array of [ciphername, tlsversion, key_bits, alg_bits]
+    expect(encryption_bits).not_to be_any { |bits| bits < 128 }
+  end
+
+  it "should not include a default `verify_mode`" do
+    expect(OpenSSL::SSL::SSLContext::DEFAULT_PARAMS[:verify_mode]).to eq(nil)
+  end
+
+  context "SSLSocket" do
+    # Code taken from the flores library by @jordansissels,
+    # https://github.com/jordansissel/ruby-flores/blob/master/spec/flores/pki_integration_spec.rb
+    # since these helpers were created to fix this particular issue
+    let(:csr) { Flores::PKI::CertificateSigningRequest.new }
+    # Here, I use a 1024-bit key for faster tests. 
+    # Please do not use such small keys in production.
+    let(:key_bits) { 1024 }
+    let(:key) { OpenSSL::PKey::RSA.generate(key_bits, 65537) }
+    let(:certificate_duration) { Flores::Random.number(1..86400) }
+
+    context "with self-signed client/server certificate" do
+      let(:certificate_subject) { "CN=server.example.com" }
+      let(:certificate) { csr.create }
+
+      # Returns [socket, address, port]
+      let(:listener) { Flores::Random.tcp_listener }
+      let(:server) { listener[0] }
+      let(:server_address) { listener[1] }
+      let(:server_port) { listener[2] }
+
+      let(:server_context) { OpenSSL::SSL::SSLContext.new }
+      let(:client_context) { OpenSSL::SSL::SSLContext.new }
+
+      before do
+        csr.subject = certificate_subject
+        csr.public_key = key.public_key
+        csr.start_time = Time.now
+        csr.expire_time = csr.start_time + certificate_duration
+        csr.signing_key = key
+        csr.want_signature_ability = true
+
+        server_context.cert = certificate
+        server_context.key = key
+
+        client_store = OpenSSL::X509::Store.new
+        client_store.add_cert(certificate)
+        client_context.cert_store = client_store
+        client_context.verify_mode = OpenSSL::SSL::VERIFY_PEER
+
+        ssl_server = OpenSSL::SSL::SSLServer.new(server, server_context)
+        Thread.new do
+          begin
+            ssl_server.accept
+          rescue => e
+            puts "Server accept failed: #{e}"
+          end
+        end
+      end
+
+      it "should successfully connect as a client" do
+        socket = TCPSocket.new(server_address, server_port)
+        ssl_client = OpenSSL::SSL::SSLSocket.new(socket, client_context)
+        expect { ssl_client.connect }.not_to raise_error
+      end
+    end
+  end
+end
diff --git a/spec/outputs/file.rb b/spec/outputs/file.rb
index bdf6a769809..d7bfa299f46 100644
--- a/spec/outputs/file.rb
+++ b/spec/outputs/file.rb
@@ -1,6 +1,8 @@
+# encoding: utf-8
 require "test_utils"
 require "logstash/outputs/file"
 require "tempfile"
+require "stud/temporary"
 
 describe LogStash::Outputs::File do
   extend LogStash::RSpec
@@ -69,4 +71,174 @@
       insist {line_num} == event_count
     end # agent
   end
+
+  describe "#register" do
+    let(:path) { '/%{name}' }
+    let(:output) { LogStash::Outputs::File.new({ "path" => path }) }
+
+    it 'doesnt allow the path to start with a dynamic string' do
+      expect { output.register }.to raise_error(LogStash::ConfigurationError)
+      output.teardown
+    end
+
+    context 'doesnt allow the root directory to have some dynamic part' do
+      ['/a%{name}/',
+       '/a %{name}/',
+       '/a- %{name}/',
+       '/a- %{name}'].each do |test_path|
+         it "with path: #{test_path}" do
+           path = test_path
+           expect { output.register }.to raise_error(LogStash::ConfigurationError)
+           output.teardown
+         end
+       end
+    end
+
+    it 'allow to have dynamic part after the file root' do
+      path = '/tmp/%{name}'
+      output = LogStash::Outputs::File.new({ "path" => path })
+      expect { output.register }.not_to raise_error
+    end
+  end
+
+  describe "receiving events" do
+    context "when using an interpolated path" do
+      context "when trying to write outside the files root directory" do
+        let(:bad_event) do
+          event = LogStash::Event.new
+          event['error'] = '../uncool/directory'
+          event
+        end
+
+        it 'writes the bad event in the specified error file' do
+          Stud::Temporary.directory('filepath_error') do |path|
+            config = { 
+              "path" => "#{path}/%{error}",
+              "filename_failure" => "_error"
+            }
+
+            # Trying to write outside the file root
+            outside_path = "#{'../' * path.split(File::SEPARATOR).size}notcool"
+            bad_event["error"] = outside_path
+
+
+            output = LogStash::Outputs::File.new(config)
+            output.register
+            output.receive(bad_event)
+
+            error_file = File.join(path, config["filename_failure"])
+
+            expect(File.exist?(error_file)).to eq(true)
+            output.teardown
+          end
+        end
+
+        it 'doesnt decode relatives paths urlencoded' do
+          Stud::Temporary.directory('filepath_error') do |path|
+            encoded_once = "%2E%2E%2ftest"  # ../test
+            encoded_twice = "%252E%252E%252F%252E%252E%252Ftest" # ../../test
+
+            output = LogStash::Outputs::File.new({ "path" =>  "/#{path}/%{error}"})
+            output.register
+
+            bad_event['error'] = encoded_once
+            output.receive(bad_event)
+
+            bad_event['error'] = encoded_twice
+            output.receive(bad_event)
+
+            expect(Dir.glob(File.join(path, "*")).size).to eq(2)
+            output.teardown
+          end
+        end
+
+        it 'doesnt write outside the file if the path is double escaped' do
+          Stud::Temporary.directory('filepath_error') do |path|
+            output = LogStash::Outputs::File.new({ "path" =>  "/#{path}/%{error}"})
+            output.register
+
+            bad_event['error'] = '../..//test'
+            output.receive(bad_event)
+
+            expect(Dir.glob(File.join(path, "*")).size).to eq(1)
+            output.teardown
+          end
+        end
+      end
+
+      context 'when trying to write inside the file root directory' do
+        it 'write the event to the generated filename' do
+          good_event = LogStash::Event.new
+          good_event['error'] = '42.txt'
+
+          Stud::Temporary.directory do |path|
+            config = { "path" => "#{path}/%{error}" }
+            output = LogStash::Outputs::File.new(config)
+            output.register
+            output.receive(good_event)
+
+            good_file = File.join(path, good_event['error'])
+            expect(File.exist?(good_file)).to eq(true)
+            output.teardown
+          end
+        end
+
+        it 'write the events to a file when some part of a folder or file is dynamic' do
+          t = Time.now
+          good_event = LogStash::Event.new("@timestamp" => t)
+
+          Stud::Temporary.directory do |path|
+            dynamic_path = "#{path}/failed_syslog-%{+YYYY-MM-dd}"
+            expected_path = "#{path}/failed_syslog-#{t.strftime("%Y-%m-%d")}"
+
+            config = { "path" => dynamic_path }
+            output = LogStash::Outputs::File.new(config)
+            output.register
+            output.receive(good_event)
+
+            expect(File.exist?(expected_path)).to eq(true)
+            output.teardown
+          end
+        end
+
+        it 'write the events to the generated path containing multiples fieldref' do
+          t = Time.now
+          good_event = LogStash::Event.new("error" => 42,
+                                           "@timestamp" => t,
+                                           "level" => "critical",
+                                           "weird_path" => '/inside/../deep/nested')
+
+          Stud::Temporary.directory do |path|
+            dynamic_path = "#{path}/%{error}/%{level}/%{weird_path}/failed_syslog-%{+YYYY-MM-dd}"
+            expected_path = "#{path}/42/critical/deep/nested/failed_syslog-#{t.strftime("%Y-%m-%d")}"
+
+            config = { "path" => dynamic_path }
+
+            output = LogStash::Outputs::File.new(config)
+            output.register
+            output.receive(good_event)
+
+            expect(File.exist?(expected_path)).to eq(true)
+            output.teardown
+          end
+        end
+
+        it 'write the event to the generated filename with multiple deep' do
+          good_event = LogStash::Event.new
+          good_event['error'] = '/inside/errors/42.txt'
+
+          Stud::Temporary.directory do |path|
+            config = { "path" => "#{path}/%{error}" }
+            output = LogStash::Outputs::File.new(config)
+            output.register
+            output.receive(good_event)
+
+            good_file = File.join(path, good_event['error'])
+            expect(File.exist?(good_file)).to eq(true)
+            output.teardown
+          end
+        end
+      end
+    end
+  end
 end
diff --git a/tools/Gemfile.jruby-1.9.lock b/tools/Gemfile.jruby-1.9.lock
index dc11fd5429f..4391eb68793 100644
--- a/tools/Gemfile.jruby-1.9.lock
+++ b/tools/Gemfile.jruby-1.9.lock
@@ -12,12 +12,12 @@ GEM
       json (~> 1.4)
       nokogiri (>= 1.4.4)
       uuidtools (~> 2.1)
-    backports (3.6.0)
+    backports (3.6.4)
     beefcake (0.3.7)
     bindata (2.0.0)
     blankslate (2.1.2.4)
     bouncy-castle-java (1.5.0147)
-    buftok (0.1)
+    buftok (0.2.0)
     builder (3.2.2)
     cabin (0.6.1)
     ci_reporter (1.9.1)
@@ -43,39 +43,36 @@ GEM
     elasticsearch-transport (1.0.1)
       faraday
       multi_json
+    equalizer (0.0.11)
     extlib (0.9.16)
     faraday (0.9.0)
       multipart-post (>= 1.2, < 3)
-    ffi (1.9.3)
     ffi (1.9.3-java)
     ffi-rzmq (1.0.0)
       ffi
     filewatch (0.5.1)
-    ftw (0.0.39)
+    ftw (0.0.42)
       addressable
       backports (>= 2.6.2)
       cabin (> 0)
-      http_parser.rb (= 0.5.3)
+      http_parser.rb (~> 0.6)
     gelf (1.3.2)
       json
     gelfd (0.2.0)
     geoip (1.3.5)
     gmetric (0.1.3)
-    hitimes (1.2.1)
     hitimes (1.2.1-java)
-    http (0.5.0)
-      http_parser.rb
-    http_parser.rb (0.5.3)
-    http_parser.rb (0.5.3-java)
+    http (0.6.4)
+      http_parser.rb (~> 0.6.0)
+    http_parser.rb (0.6.0-java)
     i18n (0.6.9)
     insist (1.0.0)
     jls-grok (0.10.12)
       cabin (>= 0.6.0)
-    jls-lumberjack (0.0.20)
+    jls-lumberjack (0.0.22)
     jruby-httpclient (1.1.1-java)
     jruby-openssl (0.8.7)
       bouncy-castle-java (>= 1.5.0147)
-    json (1.8.1)
     json (1.8.1-java)
     kramdown (1.3.3)
     mail (2.5.3)
@@ -83,6 +80,8 @@ GEM
       mime-types (~> 1.16)
       treetop (~> 1.4.8)
     march_hare (2.1.2-java)
+    memoizable (0.4.1)
+      thread_safe (~> 0.2.0)
     metaclass (0.0.4)
     method_source (0.8.2)
     metriks (0.9.9.6)
@@ -98,6 +97,7 @@ GEM
     multi_json (1.8.4)
     multipart-post (2.0.0)
     murmurhash3 (0.1.4)
+    naught (1.0.0)
     nokogiri (1.6.1-java)
       mini_portile (~> 0.5.0)
     parslet (1.4.0)
@@ -162,12 +162,16 @@ GEM
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
-    twitter (5.0.0.rc.1)
-      buftok (~> 0.1.0)
-      faraday (>= 0.8, < 0.10)
-      http (>= 0.5.0.pre2, < 0.6)
-      http_parser.rb (~> 0.5.0)
+    twitter (5.11.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (~> 0.0.9)
+      faraday (~> 0.9.0)
+      http (~> 0.6.0)
+      http_parser.rb (~> 0.6.0)
       json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
       simple_oauth (~> 0.2.0)
     tzinfo (1.1.0)
       thread_safe (~> 0.1)
@@ -197,7 +201,7 @@ DEPENDENCIES
   ffi
   ffi-rzmq (= 1.0.0)
   filewatch (= 0.5.1)
-  ftw (~> 0.0.39)
+  ftw (~> 0.0.42)
   gelf (= 1.3.2)
   gelfd (= 0.2.0)
   geoip (>= 1.3.2)
@@ -205,7 +209,7 @@ DEPENDENCIES
   i18n (>= 0.6.6)
   insist (= 1.0.0)
   jls-grok (= 0.10.12)
-  jls-lumberjack (>= 0.0.20)
+  jls-lumberjack (>= 0.0.22)
   jruby-httpclient
   jruby-openssl (= 0.8.7)
   json
@@ -231,7 +235,7 @@ DEPENDENCIES
   spoon
   statsd-ruby (= 1.2.0)
   stud
-  twitter (= 5.0.0.rc.1)
+  twitter (= 5.11.0)
   user_agent_parser (>= 2.0.0)
   xml-simple
   xmpp4r (= 0.5)
