diff --git a/Gemfile b/Gemfile
index 1b6810f6280..31c65cfc052 100644
--- a/Gemfile
+++ b/Gemfile
@@ -8,7 +8,7 @@ gem "bunny" # for amqp support, MIT-style license
 gem "uuidtools" # for naming amqp queues, License ???
 gem "filewatch", "~> 0.3.0"  # for file tailing, BSD License
 gem "jls-grok", "0.9.0" # for grok filter, BSD License
-jruby? and gem "jruby-elasticsearch", "~> 0.0.10" # BSD License
+jruby? and gem "jruby-elasticsearch", "~> 0.0.11" # BSD License
 gem "stomp" # for stomp protocol, Apache 2.0 License
 gem "json" # Ruby license
 gem "awesome_print" # MIT License
diff --git a/Gemfile.lock b/Gemfile.lock
index 5d1feaa6ccf..f9d3e00a5e6 100644
--- a/Gemfile.lock
+++ b/Gemfile.lock
@@ -12,7 +12,7 @@ GEM
     gmetric (0.1.3)
     haml (3.1.3)
     jls-grok (0.9.0)
-    jruby-elasticsearch (0.0.10)
+    jruby-elasticsearch (0.0.11)
     jruby-openssl (0.7.4)
       bouncy-castle-java
     json (1.6.1-java)
@@ -46,7 +46,7 @@ DEPENDENCIES
   gmetric (~> 0.1.3)
   haml
   jls-grok (= 0.9.0)
-  jruby-elasticsearch (~> 0.0.10)
+  jruby-elasticsearch (~> 0.0.11)
   jruby-openssl
   json
   minitest
diff --git a/Makefile b/Makefile
index 79ca834c019..834aa5fc9b6 100644
--- a/Makefile
+++ b/Makefile
@@ -5,7 +5,7 @@
 #
 JRUBY_VERSION=1.6.4
 JRUBY_CMD=build/jruby/jruby-$(JRUBY_VERSION)/bin/jruby
-WITH_JRUBY=$(JRUBY_CMD) --1.9 -S
+WITH_JRUBY=bash $(JRUBY_CMD) --1.9 -S
 VERSION=$(shell ruby -r./lib/logstash/version -e 'puts LOGSTASH_VERSION')
 JRUBY_URL=http://repository.codehaus.org/org/jruby/jruby-complete/$(JRUBY_VERSION)
 JRUBY=vendor/jar/jruby-complete-$(JRUBY_VERSION).jar
@@ -163,7 +163,7 @@ build/logstash-$(VERSION)-monolithic.jar:
 
 .PHONY: test
 test:
-	$(QUIET)$(JRUBY_CMD) bin/logstash test
+	$(QUIET)bash $(JRUBY_CMD) bin/logstash test
 
 .PHONY: docs
 docs: docgen doccopy docindex
@@ -193,7 +193,7 @@ build/docs/%: docs/%
 
 build/docs/index.html: $(addprefix build/docs/,$(subst lib/logstash/,,$(subst .rb,.html,$(PLUGIN_FILES))))
 build/docs/index.html: docs/generate_index.rb
-	$(JRUBY_CMD) $< build/docs > $@
+	ruby $< build/docs > $@
 
 publish: | gem
 	$(QUIET)$(WITH_JRUBY) gem push logstash-$(VERSION).gem
diff --git a/lib/logstash/agent.rb b/lib/logstash/agent.rb
index 9b057b27d08..acbc70a2e2f 100644
--- a/lib/logstash/agent.rb
+++ b/lib/logstash/agent.rb
@@ -52,6 +52,7 @@ def initialize
     @filters = []
 
     @plugin_paths = []
+    @reloading = false
 
     # Add logstash's plugin path (plugin paths must contain inputs, outputs, filters)
     @plugin_paths << File.dirname(__FILE__)
@@ -85,7 +86,7 @@ def options(opts)
       @config_string = arg
     end # -e
 
-    opts.on("-d", "--daemonize", "Daemonize (default is run in foreground)") do 
+    opts.on("-d", "--daemonize", "Daemonize (default is run in foreground)") do
       @daemonize = true
     end
 
@@ -150,7 +151,7 @@ def parse_options(args)
     plugins = []
     args.each do |arg|
       # skip things that don't look like plugin flags
-      next unless arg =~ /^--[A-z0-9]+-/ 
+      next unless arg =~ /^--[A-z0-9]+-/
       name = arg.split("-")[2]  # pull the plugin name out
 
       # Try to load any plugin by that name
@@ -181,15 +182,15 @@ def parse_options(args)
         #@logger.fatal("Flag #{arg.inspect} requires plugin #{name}, but no plugin found.")
         #return false
       #end
-    end # @remaining_args.each 
-   
+    end # @remaining_args.each
+
     begin
       remainder = @opts.parse(args)
     rescue OptionParser::InvalidOption => e
       @logger.info e
       raise e
     end
- 
+
     return remainder
   end # def parse_options
 
@@ -237,19 +238,7 @@ def configure
     end
   end # def configure
 
-  public
-  def run(args, &block)
-    LogStash::Util::set_thread_name(self.class.name)
-    register_signal_handlers
-
-    remaining = parse_options(args)
-    if remaining == false
-      raise "Option parsing failed. See error log."
-    end
-
-    configure
-
-    # Load the config file
+  def read_config
     if @config_path
       # Support directory of config files.
       # https://logstash.jira.com/browse/LOGSTASH-106
@@ -272,22 +261,14 @@ def run(args, &block)
       config = LogStash::Config::File.new(nil, @config_string)
     end
     config.logger = @logger
-
-    @thread = Thread.new do
-      run_with_config(config, &block)
-    end
-
-    return remaining
-  end # def run
-
-  public
-  def wait
-    @thread.join
-    return 0
-  end # def wait
-
-  public
-  def run_with_config(config)
+    config
+  end
+
+  # Parses a config and returns [inputs, filters, outputs]
+  def parse_config(config)
+    inputs = []
+    filters = []
+    outputs = []
     config.parse do |plugin|
       # 'plugin' is a has containing:
       #   :type => the base class of the plugin (LogStash::Inputs::Base, etc)
@@ -303,91 +284,147 @@ def run_with_config(config)
 
       case type
         when "input"
-          @inputs << instance
+          inputs << instance
         when "filter"
-          @filters << instance
+          filters << instance
         when "output"
-          @outputs << instance
+          outputs << instance
         else
-          @logger.error("Unknown config type '#{type}'")
-          exit 1
+          msg = "Unknown config type '#{type}'"
+          @logger.error(msg)
+          raise msg
       end # case type
     end # config.parse
+    return inputs, filters, outputs
+  end
 
-    # If we are given a config string (run usually with 'agent -e "some config string"')
-    # then set up some defaults.
-    if @config_string
-      require "logstash/inputs/stdin"
-      require "logstash/outputs/stdout"
-
-      # set defaults if necessary
-      
-      # All filters default to 'stdin' type
-      @filters.each do |filter|
-        filter.type = "stdin" if filter.type.nil?
-      end
-      
-      # If no inputs are specified, use stdin by default.
-      @inputs = [LogStash::Inputs::Stdin.new("type" => [ "stdin" ])] if @inputs.length == 0
 
-      # If no outputs are specified, use stdout in debug mode.
-      @outputs = [LogStash::Outputs::Stdout.new("debug" => [ "true" ])] if @outputs.length == 0
+
+  public
+  def run(args, &block)
+    LogStash::Util::set_thread_name(self.class.name)
+    register_signal_handlers
+
+    remaining = parse_options(args)
+    if remaining == false
+      raise "Option parsing failed. See error log."
     end
 
-    if @inputs.length == 0 or @outputs.length == 0
-      raise "Must have both inputs and outputs configured."
+    configure
+
+    # Load the config file
+    config = read_config
+
+    @thread = Thread.new do
+      run_with_config(config, &block)
     end
 
-    # NOTE(petef) we should use a SizedQueue here (w/config params for size)
-    filter_queue = SizedQueue.new(10)
-    output_queue = LogStash::MultiQueue.new
+    return remaining
+  end # def run
 
-    @ready_queue = Queue.new
+  public
+  def wait
+    @thread.join
+    return 0
+  end # def wait
 
+  private
+  def start_input(input)
+    @logger.debug(["Starting input", input])
     # inputs should write directly to output queue if there are no filters.
-    input_target = @filters.length > 0 ? filter_queue : output_queue
-    # Start inputs
-    @inputs.each do |input|
-      @logger.debug(["Starting input", input])
-      @plugins[input] = Thread.new(input, input_target) do |*args|
-        run_input(*args)
+    input_target = @filters.length > 0 ? @filter_queue : @output_queue
+    @plugins[input] = Thread.new(input, input_target) do |*args|
+      run_input(*args)
+    end
+  end
+
+  private
+  def start_output(output)
+    @logger.debug(["Starting output", output])
+    queue = SizedQueue.new(10)
+    @output_queue.add_queue(queue)
+    @output_plugin_queues[output] = queue
+    @plugins[output] = Thread.new(output, queue) do |*args|
+      run_output(*args)
+    end
+  end
+
+
+  public
+  def run_with_config(config)
+    @plugins_mutex.synchronize do
+      @inputs, @filters, @outputs = parse_config(config)
+
+      # If we are given a config string (run usually with 'agent -e "some config string"')
+      # then set up some defaults.
+      if @config_string
+        require "logstash/inputs/stdin"
+        require "logstash/outputs/stdout"
+
+        # set defaults if necessary
+
+        # All filters default to 'stdin' type
+        @filters.each do |filter|
+          filter.type = "stdin" if filter.type.nil?
+        end
+
+        # If no inputs are specified, use stdin by default.
+        @inputs = [LogStash::Inputs::Stdin.new("type" => [ "stdin" ])] if @inputs.length == 0
+
+        # If no outputs are specified, use stdout in debug mode.
+        @outputs = [LogStash::Outputs::Stdout.new("debug" => [ "true" ])] if @outputs.length == 0
       end
-    end # @inputs.each
 
-    # Create N filter-worker threads
-    if @filters.length > 0
-      @filters.each do |filter|
-        filter.logger = @logger
-        filter.register
+      if @inputs.length == 0 or @outputs.length == 0
+        raise "Must have both inputs and outputs configured."
       end
-      2.times do |n|
-        # TODO(sissel): facter this out into a 'filterworker' that  accepts
-        # 'shutdown'
-        # Start a filter worker
-        filterworker = LogStash::FilterWorker.new(@filters, filter_queue,
-                                                  output_queue)
-        filterworker.logger = @logger
-        @plugins[filterworker] = \
-          Thread.new(filterworker, n, output_queue) do |*args|
+
+      # NOTE(petef) we should use a SizedQueue here (w/config params for size)
+      @filter_queue = SizedQueue.new(10)
+      @output_queue = LogStash::MultiQueue.new
+
+      @ready_queue = Queue.new
+
+      # Start inputs
+      @inputs.each do |input|
+        start_input(input)
+      end # @inputs.each
+
+      # Create N filter-worker threads
+      if @filters.length > 0
+        @filters.each do |filter|
+          filter.logger = @logger
+          filter.register
+        end
+        @filterworkers = {}
+        2.times do |n|
+          # TODO(sissel): facter this out into a 'filterworker' that  accepts
+          # 'shutdown'
+          # Start a filter worker
+          filterworker = LogStash::FilterWorker.new(@filters, @filter_queue,
+                                                    @output_queue)
+          filterworker.logger = @logger
+          thread = Thread.new(filterworker, n, @output_queue) do |*args|
             run_filter(*args)
           end
-      end # N.times
-    end # if @filters.length > 0
-
-    # Create output threads
-    @outputs.each do |output|
-      queue = SizedQueue.new(10)
-      output_queue.add_queue(queue)
-      @plugins[output] = Thread.new(output, queue) do |*args|
-        run_output(*args)
+          @plugins[filterworker] = thread
+          @filterworkers[filterworker] = thread
+        end # N.times
+      end # if @filters.length > 0
+
+      # Create output threads
+      @output_plugin_queues = {}
+      @outputs.each do |output|
+        start_output(output)
+      end # @outputs.each
+
+      # Wait for all inputs and outputs to be registered.
+      wait_count = outputs.size + inputs.size
+      while wait_count > 0 and @ready_queue.pop
+        wait_count -= 1
       end
-    end # @outputs.each
-
-    # Wait for all inputs and outputs to be registered.
-    wait_count = outputs.size + inputs.size
-    while wait_count > 0 and @ready_queue.pop 
-      wait_count -= 1
-    end
+      @logger.info("All plugins are started and registered.")
+    end # synchronize
 
     # yield to a block in case someone's waiting for us to be done setting up
     # like tests, etc.
@@ -407,26 +444,33 @@ def stop
   # Shutdown the agent.
   protected
   def shutdown
+    @logger.info("Starting shutdown sequence")
+    shutdown_plugins(@plugins)
+    # When we get here, all inputs have finished, all messages are done
+    @logger.info("Shutdown complete")
+    java.lang.System.exit(0)
+  end # def shutdown
+
+  def shutdown_plugins(plugins)
     return if @is_shutting_down
 
     @is_shutting_down = true
     Thread.new do
-      @logger.info("Starting shutdown sequence")
       LogStash::Util::set_thread_name("logstash shutdown process")
       # TODO(sissel): Make this a flag
       force_shutdown_time = Time.now + 10
 
       finished_queue = Queue.new
       # Tell everything to shutdown.
-      @logger.debug(@plugins.keys.collect(&:to_s))
-      @plugins.each do |plugin, thread|
-        @logger.debug("Telling to shutdown: #{plugin.to_s}")
-        plugin.shutdown(finished_queue)
+      @logger.debug(plugins.keys.collect(&:to_s))
+      plugins.each do |p, thread|
+        @logger.debug("Telling to shutdown: #{p.to_s}")
+        p.shutdown(finished_queue)
       end
 
       # Now wait until the queues we were given are empty.
       #@logger.debug(@plugins)
-      remaining = @plugins.select { |plugin, thread| plugin.running? }
+      remaining = plugins.select { |p, thread| p.running? }
       while remaining.size > 0
         if (Time.now > force_shutdown_time)
           @logger.warn("Time to quit, even if some plugins aren't finished yet.")
@@ -440,18 +484,114 @@ def shutdown
         if plugin.nil?
           sleep(1)
         else
-          remaining = @plugins.select { |plugin, thread| plugin.running? }
-          @logger.debug("#{plugin.to_s} finished, waiting on " \
+          remaining = plugins.select { |p, thread| plugin.running? }
+          @logger.debug("#{p.to_s} finished, waiting on " \
                         "#{remaining.size} plugins; " \
                         "#{remaining.map(&:first).join(", ")}")
         end
       end # while remaining.size > 0
+    end
+    @is_shutting_down = false
+  end
 
-      # When we get here, all inputs have finished, all messages are done
-      @logger.info("Shutdown complete")
-      exit(0)
+
+
+  # Reload configuration of filters, etc.
+  def reload
+    @plugins_mutex.synchronize do
+      begin
+        @reloading = true
+        # Reload the config file
+        begin
+          config = read_config
+          reloaded_inputs, reloaded_filters, reloaded_outputs = parse_config(config)
+        rescue Exception => e
+          @logger.error "Aborting reload due to bad configuration: #{e}"
+          return
+        end
+
+        new_inputs = reloaded_inputs - @inputs
+        new_filters = reloaded_filters - @filters
+        new_outputs = reloaded_outputs - @outputs
+
+        deleted_inputs = @inputs - reloaded_inputs
+        deleted_filters = @filters - reloaded_filters
+        deleted_outputs = @outputs - reloaded_outputs
+
+
+        # Handle shutdown of input and output plugins
+        obsolete_plugins = {}
+        [deleted_inputs].flatten.each do |p|
+          if @plugins.include? p
+            obsolete_plugins[p] = @plugins[p]
+            @plugins.delete(p)
+          else
+            @logger.warn("Couldn't find input plugin to stop: #{p}")
+          end
+        end
+
+        [deleted_outputs].flatten.each do |p|
+          if @plugins.include? p
+            obsolete_plugins[p] = @plugins[p]
+            @plugins.delete(p)
+            @output_queue.remove_queue(@output_plugin_queues[p])
+          else
+            @logger.warn("Couldn't find output plugin to stop: #{p}")
+          end
+        end
+
+        # Call reload on all existing plugins which are not being dropped
+        (@plugins.keys - obsolete_plugins.keys).each(&:reload)
+        (@filters - deleted_filters).each(&:reload)
+
+        # Also remove filters
+        deleted_filters.each {|f| obsolete_plugins[f] = nil}
+
+        if obsolete_plugins.size > 0
+          @logger.info("Stopping removed plugins:\n\t" + obsolete_plugins.keys.join("\n\t"))
+          shutdown_plugins(obsolete_plugins)
+        end
+        # require 'pry'; binding.pry()
+
+        # Start up filters
+        if new_filters.size > 0 || deleted_filters.size > 0
+          if new_filters.size > 0
+            @logger.info("Starting new filters: #{new_filters.join(', ')}")
+            new_filters.each do |f|
+              f.logger = @logger
+              f.register
+            end
+          end
+          @filters = reloaded_filters
+          @filterworkers.each_key do |filterworker|
+            filterworker.filters = @filters
+          end
+        end
+
+        if new_inputs.size > 0
+          @logger.info("Starting new inputs: #{new_inputs.join(', ')}")
+          new_inputs.each do |p|
+            start_input(p)
+          end
+        end
+        if new_outputs.size > 0
+          @logger.info("Starting new outputs: #{new_outputs.join(', ')}")
+          new_inputs.each do |p|
+            start_output(p)
+          end
+        end
+
+        # Wait for all inputs and outputs to be registered.
+        wait_count = new_outputs.size + new_inputs.size
+        while wait_count > 0 and @ready_queue.pop
+          wait_count -= 1
+        end
+      rescue Exception => e
+        @reloading = false
+        raise e
+      end
     end
-  end # def shutdown
+  end
 
   public
   def register_signal_handlers
@@ -476,15 +616,20 @@ def register_signal_handlers
       ##end
     #end # SIGUSR1
 
-    #Signal.trap("INT") do
-      #@logger.warn("SIGINT received, shutting down.")
-      #shutdown
-    #end
+    Signal.trap("INT") do
+      @logger.warn("SIGINT received, shutting down.")
+      shutdown
+    end
+
+    Signal.trap("HUP") do
+      @logger.warn("SIGHUP received, reloading.")
+      reload
+    end
 
-    #Signal.trap("TERM") do
-      #@logger.warn("SIGTERM received, shutting down.")
-      #shutdown
-    #end
+    Signal.trap("TERM") do
+      @logger.warn("SIGTERM received, shutting down.")
+      shutdown
+    end
   end # def register_signal_handlers
 
   private
@@ -492,7 +637,7 @@ def run_input(input, queue)
     LogStash::Util::set_thread_name("input|#{input.to_s}")
     input.logger = @logger
     input.register
-
+    @logger.info("Input #{input.to_s} registered")
     @ready_queue << input
     done = false
 
@@ -515,7 +660,7 @@ def run_input(input, queue)
     #@logger.info("Input #{input.to_s} shutting down")
 
     # If we get here, the plugin finished, check if we need to shutdown.
-    shutdown_if_none_running(LogStash::Inputs::Base, queue)
+    shutdown_if_none_running(LogStash::Inputs::Base, queue) unless @reloading
   end # def run_input
 
   # Run a filter thread
@@ -523,18 +668,18 @@ def run_input(input, queue)
   def run_filter(filterworker, index, output_queue)
     LogStash::Util::set_thread_name("filter|worker|#{index}")
     filterworker.run
-
     @logger.warn("Filter worker ##{index} shutting down")
 
     # If we get here, the plugin finished, check if we need to shutdown.
-    shutdown_if_none_running(LogStash::FilterWorker, output_queue)
+    shutdown_if_none_running(LogStash::FilterWorker, output_queue) unless @reloading
   end # def run_filter
 
   # TODO(sissel): Factor this into an 'outputworker'
   def run_output(output, queue)
     LogStash::Util::set_thread_name("output|#{output.to_s}")
-    output.register
     output.logger = @logger
+    output.register
+    @logger.info("Output #{output.to_s} registered")
     @ready_queue << output
 
     # TODO(sissel): We need a 'reset' or 'restart' method to call on errors
@@ -552,11 +697,11 @@ def run_output(output, queue)
       sleep(1)
       retry
     end # begin/rescue
- 
+
     @logger.warn("Output #{input.to_s} shutting down")
 
     # If we get here, the plugin finished, check if we need to shutdown.
-    shutdown_if_none_running(LogStash::Outputs::Base)
+    shutdown_if_none_running(LogStash::Outputs::Base) unless @reloading
   end # def run_output
 
   def shutdown_if_none_running(pluginclass, queue=nil)
@@ -573,7 +718,7 @@ def shutdown_if_none_running(pluginclass, queue=nil)
 
       if remaining == 0
         @logger.debug("All #{pluginclass} finished. Shutting down.")
-        
+
         # Send 'shutdown' to the filters.
         queue << LogStash::SHUTDOWN if !queue.nil?
         shutdown
diff --git a/lib/logstash/filters/base.rb b/lib/logstash/filters/base.rb
index 2104f0803c1..a131b9d7bd4 100644
--- a/lib/logstash/filters/base.rb
+++ b/lib/logstash/filters/base.rb
@@ -6,12 +6,10 @@
 class LogStash::Filters::Base < LogStash::Plugin
   include LogStash::Config::Mixin
 
-  attr_accessor :logger
-
   config_name "filter"
 
   # The type to act on. If a type is given, then this filter will only
-  # act on messages with the same type. See any input plugin's "type" 
+  # act on messages with the same type. See any input plugin's "type"
   # attribute for more.
   config :type, :validate => :string
 
@@ -44,7 +42,7 @@ class LogStash::Filters::Base < LogStash::Plugin
 
   public
   def initialize(params)
-    @logger = LogStash::Logger.new(STDOUT)
+    super
     config_init(params)
   end # def initialize
 
diff --git a/lib/logstash/filters/grok.rb b/lib/logstash/filters/grok.rb
index 8d81a2a15eb..ca102ef4540 100644
--- a/lib/logstash/filters/grok.rb
+++ b/lib/logstash/filters/grok.rb
@@ -8,11 +8,11 @@
 #
 # This filter requires you have libgrok installed.
 #
-# You can find libgrok here: 
+# You can find libgrok here:
 # <http://code.google.com/p/semicomplete/wiki/Grok>
 #
 # Compile/install notes can be found in the INSTALL file of the
-# grok tarball, or here: 
+# grok tarball, or here:
 # <https://github.com/jordansissel/grok/blob/master/INSTALL>
 #
 # Key dependencies:
@@ -43,7 +43,7 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
 
   # Any existing field name can be used as a config name here for matching
   # against.
-  #     
+  #
   #     # this config:
   #     foo => "some pattern"
   #
@@ -57,7 +57,7 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
   # patterns.
   #
   # Pattern files are plain text with format:
-  # 
+  #
   #     NAME PATTERN
   #
   # For example:
@@ -93,6 +93,13 @@ class LogStash::Filters::Grok < LogStash::Filters::Base
     @@patterns_path += val.split(":")
   end
 
+  public
+  def initialize(params)
+    super(params)
+    @match["@message"] ||= []
+    @match["@message"] += @pattern if @pattern # the config 'pattern' value (array)
+  end
+
   public
   def register
     gem "jls-grok", ">=0.4.3"
@@ -120,13 +127,10 @@ def register
     end
 
     @patterns = Hash.new { |h,k| h[k] = [] }
-    
-    @logger.info(:match => @match)
 
-    @match["@message"] ||= []
-    @match["@message"] += @pattern if @pattern # the config 'pattern' value (array)
+    @logger.info(:match => @match)
 
-    # TODO(sissel): Hash.merge  actually overrides, not merges arrays. 
+    # TODO(sissel): Hash.merge  actually overrides, not merges arrays.
     # Work around it by implementing our own?
     # TODO(sissel): Check if 'match' is empty?
     @match.merge(@config).each do |field, patterns|
@@ -137,7 +141,7 @@ def register
       patterns = [patterns] if patterns.is_a?(String)
 
       if !@patterns.include?(field)
-        @patterns[field] = Grok::Pile.new 
+        @patterns[field] = Grok::Pile.new
         add_patterns_from_files(@patternfiles, @patterns[field])
       end
       @logger.info(["Grok compile", { :field => field, :patterns => patterns }])
@@ -158,7 +162,7 @@ def filter(event)
       return
     end
 
-    if @type != event.type 
+    if @type != event.type
       @logger.debug("Skipping grok for event type=#{event.type} (wanted '#{@type}')")
       return
     end
@@ -189,9 +193,9 @@ def filter(event)
           # http://code.google.com/p/logstash/issues/detail?id=45
           # Permit typing of captures by giving an additional colon and a type,
           # like: %{FOO:name:int} for int coercion.
-          if type_coerce 
-            @logger.info("Match type coerce: #{type_coerce}") 
-            @logger.info("Patt: #{grok.pattern}") 
+          if type_coerce
+            @logger.info("Match type coerce: #{type_coerce}")
+            @logger.info("Patt: #{grok.pattern}")
           end
 
           case type_coerce
diff --git a/lib/logstash/filterworker.rb b/lib/logstash/filterworker.rb
index 4a60ea21edc..86a0578f0e4 100644
--- a/lib/logstash/filterworker.rb
+++ b/lib/logstash/filterworker.rb
@@ -6,11 +6,13 @@
 # TODO(sissel): Should this really be a 'plugin' ?
 class LogStash::FilterWorker < LogStash::Plugin
   attr_accessor :logger
+  attr_accessor :filters
 
   def initialize(filters, input_queue, output_queue)
     @filters = filters
     @input_queue = input_queue
     @output_queue = output_queue
+    @shutdown_requested = false
   end # def initialize
 
   def run
@@ -20,14 +22,19 @@ def run
       #filter.register
     #end
 
-    while event = @input_queue.pop
+    while !@shutdown_requested && event = @input_queue.pop
       if event == LogStash::SHUTDOWN
         finished
-        break
+        return
       end
 
       filter(event)
     end # while @input_queue.pop
+    finished
+  end
+
+  def teardown
+    @shutdown_requested = true
   end
 
   def filter(original_event)
@@ -61,6 +68,6 @@ def filter(original_event)
 
       @logger.debug(["Event finished filtering", { :event => event, :thread => Thread.current[:name] }])
       @output_queue.push(event) unless event.cancelled?
-    end # events.each 
+    end # events.each
   end # def filter
 end # class LogStash::FilterWorker
diff --git a/lib/logstash/inputs/amqp.rb b/lib/logstash/inputs/amqp.rb
index 52776e35e10..070e31dfed9 100644
--- a/lib/logstash/inputs/amqp.rb
+++ b/lib/logstash/inputs/amqp.rb
@@ -4,7 +4,7 @@
 # Pull events from an AMQP exchange.
 #
 # AMQP is a messaging system. It requires you to run an AMQP server or 'broker'
-# Examples of AMQP servers are [RabbitMQ](http://www.rabbitmq.com/) and 
+# Examples of AMQP servers are [RabbitMQ](http://www.rabbitmq.com/) and
 # [QPid](http://qpid.apache.org/)
 class LogStash::Inputs::Amqp < LogStash::Inputs::Base
   MQTYPES = [ "fanout", "direct", "topic" ]
@@ -31,7 +31,7 @@ class LogStash::Inputs::Amqp < LogStash::Inputs::Base
 
   # The name of the queue. If not set, defaults to the same name as the exchange.
   config :queue_name, :validate => :string
-  
+
   # The routing key to bind to
   config :key, :validate => :string
 
@@ -106,7 +106,7 @@ def run(queue)
       @queue = @bunny.queue(@queue_name, :durable => @queue_durable)
       exchange = @bunny.exchange(@name, :type => @exchange_type.to_sym, :durable => @durable)
       @queue.bind(exchange, :key => @key)
-      
+
       @queue.subscribe do |data|
         e = to_event(data[:payload], @amqpurl)
         if e
@@ -124,5 +124,6 @@ def run(queue)
 
   def teardown
     @bunny.close if @bunny
+    finished
   end # def teardown
 end # class LogStash::Inputs::Amqp
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 1d9dc0eaac9..b0c2b018645 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -7,8 +7,6 @@
 # This is the base class for logstash inputs.
 class LogStash::Inputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
-  attr_accessor :logger
-
   config_name "input"
 
   # Label this input with a type.
@@ -43,15 +41,16 @@ class LogStash::Inputs::Base < LogStash::Plugin
       #if v !~ re
         #return [false, "Tag '#{v}' does not match #{re}"]
       #end # check 'v'
-    #end # value.each 
+    #end # value.each
     #return true
   #end) # config :tag
 
+  attr_accessor :params
+
   public
   def initialize(params)
-    @logger = LogStash::Logger.new(STDOUT)
+    super
     config_init(params)
-
     @tags ||= []
   end # def initialize
 
diff --git a/lib/logstash/inputs/redis.rb b/lib/logstash/inputs/redis.rb
index 3a6d7643fe2..13082e77043 100644
--- a/lib/logstash/inputs/redis.rb
+++ b/lib/logstash/inputs/redis.rb
@@ -12,7 +12,7 @@ class LogStash::Inputs::Redis < LogStash::Inputs::Base
   # Name is used for logging in case there are multiple instances.
   # This feature has no real function and will be removed in future versions.
   config :name, :validate => :string, :default => "default", :deprecated => true
-  
+
   # The hostname of your redis server.
   config :host, :validate => :string, :default => "127.0.0.1"
 
@@ -36,7 +36,7 @@ class LogStash::Inputs::Redis < LogStash::Inputs::Base
   # TODO: change required to true
   config :key, :validate => :string, :required => false
 
-  # Either list or channel.  If redis_type is list, then we will BLPOP the 
+  # Either list or channel.  If redis_type is list, then we will BLPOP the
   # key.  If redis_type is channel, then we will SUBSCRIBE to the key.
   # If redis_type is pattern_channel, then we will PSUBSCRIBE to the key.
   # TODO: change required to true
@@ -72,7 +72,7 @@ def register
       )
     end
     # end TODO
-    
+
     @logger.info "Registering redis #{identity}"
   end # def register
 
@@ -105,7 +105,7 @@ def queue_event(msg, output_queue)
       @logger.debug(["Backtrace",  e.backtrace])
     end
   end
-  
+
   private
   def list_listener(redis, output_queue)
     response = redis.blpop @key, 0
@@ -184,5 +184,6 @@ def teardown
       @redis.quit
       @redis = nil
     end
+    finished
   end
 end # class LogStash::Inputs::Redis
diff --git a/lib/logstash/inputs/stdin.rb b/lib/logstash/inputs/stdin.rb
index 2b15d6067ca..cc2e71df912 100644
--- a/lib/logstash/inputs/stdin.rb
+++ b/lib/logstash/inputs/stdin.rb
@@ -27,5 +27,6 @@ def run(queue)
   public
   def teardown
     $stdin.close
+    finished
   end # def teardown
 end # class LogStash::Inputs::Stdin
diff --git a/lib/logstash/inputs/syslog.rb b/lib/logstash/inputs/syslog.rb
index d5b9e70658b..ada8869246c 100644
--- a/lib/logstash/inputs/syslog.rb
+++ b/lib/logstash/inputs/syslog.rb
@@ -26,6 +26,7 @@ class LogStash::Inputs::Syslog < LogStash::Inputs::Base
   public
   def initialize(params)
     super
+    @shutdown_requested = false
     BasicSocket.do_not_reverse_lookup = true
 
     # force "plain" format. others don't make sense here.
@@ -47,7 +48,7 @@ def register
 
     @grok_filter.register
     @date_filter.register
-    
+
     @tcp_clients = []
   end # def register
 
@@ -59,6 +60,7 @@ def run(output_queue)
       begin
         udp_listener(output_queue)
       rescue => e
+        break if @shutdown_requested
         @logger.warn("syslog udp listener died: #{$!}")
         @logger.debug(["Backtrace", e.backtrace])
         sleep(5)
@@ -72,6 +74,7 @@ def run(output_queue)
       begin
         tcp_listener(output_queue)
       rescue => e
+        break if @shutdown_requested
         @logger.warn("syslog tcp listener died: #{$!}")
         @logger.debug(["Backtrace", e.backtrace])
         sleep(5)
@@ -103,10 +106,7 @@ def udp_listener(output_queue)
       end
     end
   ensure
-    if @udp
-      @udp.close_read rescue nil
-      @udp.close_write rescue nil
-    end
+    close_udp
   end # def udp_listener
 
   private
@@ -124,7 +124,7 @@ def tcp_listener(output_queue)
         LogStash::Util::set_thread_name("input|syslog|tcp|#{ip}:#{port}}")
         if ip.include?(":") # ipv6
           source = "syslog://[#{ip}]/"
-        else 
+        else
           source = "syslog://#{ip}/"
         end
 
@@ -141,12 +141,35 @@ def tcp_listener(output_queue)
       end # Thread.new
     end # loop do
   ensure
+    close_tcp
+  end # def tcp_listener
+
+  public
+  def teardown
+    @shutdown_requested = true
+    close_udp
+    close_tcp
+    finished
+  end
+
+  private
+  def close_udp
+    if @udp
+      @udp.close_read rescue nil
+      @udp.close_write rescue nil
+    end
+    @udp = nil
+  end
+
+  private
+  def close_tcp
     # If we somehow have this left open, close it.
     @tcp_clients.each do |client|
       client.close rescue nil
     end
     @tcp.close if @tcp rescue nil
-  end # def tcp_listener
+    @tcp = nil
+  end
 
   # Following RFC3164 where sane, we'll try to parse a received message
   # as if you were relaying a syslog message to it.
diff --git a/lib/logstash/multiqueue.rb b/lib/logstash/multiqueue.rb
index 36753cd6a13..56a67e88fd4 100644
--- a/lib/logstash/multiqueue.rb
+++ b/lib/logstash/multiqueue.rb
@@ -27,6 +27,11 @@ def add_queue(queue)
     end
   end # def add_queue
 
+  public
+  def remove_queue(queue)
+    @queues.delete(queue)
+  end
+
   public
   def size
     return @queues.collect { |q| q.size }
diff --git a/lib/logstash/outputs/amqp.rb b/lib/logstash/outputs/amqp.rb
index dea7e706d04..e88bcf317b4 100644
--- a/lib/logstash/outputs/amqp.rb
+++ b/lib/logstash/outputs/amqp.rb
@@ -4,7 +4,7 @@
 # Push events to an AMQP exchange.
 #
 # AMQP is a messaging system. It requires you to run an AMQP server or 'broker'
-# Examples of AMQP servers are [RabbitMQ](http://www.rabbitmq.com/) and 
+# Examples of AMQP servers are [RabbitMQ](http://www.rabbitmq.com/) and
 # [QPid](http://qpid.apache.org/)
 class LogStash::Outputs::Amqp < LogStash::Outputs::Base
   MQTYPES = [ "fanout", "direct", "topic" ]
@@ -28,7 +28,7 @@ class LogStash::Outputs::Amqp < LogStash::Outputs::Base
 
   # The name of the exchange
   config :name, :validate => :string, :required => true
-  
+
   # Key to route to
   config :key, :validate => :string
 
@@ -125,10 +125,11 @@ def to_s
     return "amqp://#{@user}@#{@host}:#{@port}#{@vhost}/#{@exchange_type}/#{@name}"
   end
 
-  #public
-  #def teardown
-    #@bunny.close rescue nil
-    #@bunny = nil
-    #@target = nil
-  #end # def teardown
+  public
+  def teardown
+    @bunny.close rescue nil
+    @bunny = nil
+    @target = nil
+    finished
+  end # def teardown
 end # class LogStash::Outputs::Amqp
diff --git a/lib/logstash/outputs/base.rb b/lib/logstash/outputs/base.rb
index 3c7cccb7f0b..54b2628ad65 100644
--- a/lib/logstash/outputs/base.rb
+++ b/lib/logstash/outputs/base.rb
@@ -9,13 +9,11 @@
 class LogStash::Outputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
 
-  attr_accessor :logger
-
   config_name "output"
 
   public
   def initialize(params)
-    @logger = LogStash::Logger.new(STDOUT)
+    super
     config_init(params)
   end
 
diff --git a/lib/logstash/outputs/elasticsearch.rb b/lib/logstash/outputs/elasticsearch.rb
index ba980561b3c..f8845bb8511 100644
--- a/lib/logstash/outputs/elasticsearch.rb
+++ b/lib/logstash/outputs/elasticsearch.rb
@@ -1,3 +1,4 @@
+require "thread"
 require "logstash/namespace"
 require "logstash/outputs/base"
 
@@ -52,6 +53,11 @@ class LogStash::Outputs::ElasticSearch < LogStash::Outputs::Base
   # default.
   config :embedded_http_port, :validate => :string, :default => "9200-9300"
 
+  # The number of log events to store in memory before blocking
+  # Set to larger numbers for better performance. In the event of a crash,
+  # all buffered events will be lost.
+  config :buffer_size, :validate => :number, :default => 1
+
   # TODO(sissel): Config for river?
 
   public
@@ -83,7 +89,6 @@ def register
     @logger.info(:message => "New ElasticSearch output", :cluster => @cluster,
                  :host => @host, :port => @port, :embedded => @embedded)
     @pending = []
-    @callback = self.method(:receive_native)
     options = {
       :cluster => @cluster,
       :host => @host,
@@ -99,6 +104,20 @@ def register
     end
 
     @client = ElasticSearch::Client.new(options)
+
+    # Create a separate thread to do the flushing
+    @event_buffer = SizedQueue.new(@buffer_size) # events to send
+    @shutdown_requested = false
+    @elasticsearch_writer = Thread.new do
+      while !@shutdown_requested
+        begin
+          wait_and_flush_buffer
+        rescue Exception => e
+          @logger.error(["Failed to send events to ElasticSearch", e, e.backtrace])
+        end
+      end
+      wait_and_flush_buffer if @event_buffer.length > 0 # Final flush
+    end
   end # def register
 
   protected
@@ -112,110 +131,64 @@ def start_local_elasticsearch
     @embedded_elasticsearch.start
   end # def start_local_elasticsearch
 
-  # TODO(sissel): Needs migration to  jrubyland
   public
-  def ready(params)
-    case params["method"]
-    when "http"
-      @logger.debug "ElasticSearch using http with URL #{@url.to_s}"
-      #@http = EventMachine::HttpRequest.new(@url.to_s)
-      @callback = self.method(:receive_http)
-    when "river"
-      require "logstash/outputs/amqp"
-      params["port"] ||= 5672
-      auth = "#{params["user"] or "guest"}:#{params["pass"] or "guest"}"
-      mq_url = URI::parse("amqp://#{auth}@#{params["host"]}:#{params["port"]}/queue/#{params["queue"]}?durable=1")
-      @mq = LogStash::Outputs::Amqp.new(mq_url.to_s)
-      @mq.register
-      @callback = self.method(:receive_river)
-      em_url = URI.parse("http://#{@url.host}:#{@url.port}/_river/logstash#{@url.path.tr("/", "_")}/_meta")
-      unused, @es_index, @es_type = @url.path.split("/", 3)
-
-      river_config = {"type" => params["type"],
-                      params["type"] => {"host" => params["host"],
-                                         "user" => params["user"],
-                                         "port" => params["port"],
-                                         "pass" => params["pass"],
-                                         "vhost" => params["vhost"],
-                                         "queue" => params["queue"],
-                                         "exchange" => params["queue"],
-                                        },
-                     "index" => {"bulk_size" => 100,
-                                 "bulk_timeout" => "10ms",
-                                },
-                     }
-      @logger.debug(["ElasticSearch using river", river_config])
-      #http_setup = EventMachine::HttpRequest.new(em_url.to_s)
-      req = http_setup.put :body => river_config.to_json
-      req.errback do
-        @logger.warn "Error setting up river: #{req.response}"
-      end
-      @callback = self.method(:receive_river)
-    else raise "unknown elasticsearch method #{params["method"].inspect}"
-    end
+  def receive(event)
+    @event_buffer << event
+  end
 
-    #receive(LogStash::Event.new({
-      #"@source" => "@logstashinit",
-      #"@type" => "@none",
-      #"@message" => "Starting logstash output to elasticsearch",
-      #"@fields" => {
-        #"HOSTNAME" => Socket.gethostname
-      #},
-    #}))
+  # Make a blocking call to get at least one event, and then get the rest.
+  # Send them all to ElasticSearch
+  protected
+  def wait_and_flush_buffer
+    bulk_request = @client.bulk
+    num_events = add_index_request(bulk_request, @event_buffer.pop) # blocking
 
-    pending = @pending
-    @pending = []
-    @logger.info("Flushing #{pending.size} events")
-    pending.each do |event|
-      receive(event)
+    # Send up to the @buffer_size events at once
+    while @event_buffer.length > 0 && num_events < @buffer_size
+      num_events += add_index_request(bulk_request, @event_buffer.pop)
     end
-  end # def ready
 
-  public
-  def receive(event)
-    if @callback
-      @callback.call(event)
-    else
-      @pending << event
-    end
-  end # def receive
+    return if num_events <= 0
 
-  public
-  def receive_http(event, tries=5)
-    req = @http.post :body => event.to_json
-    req.errback do
-      @logger.warn("Request to index to #{@url.to_s} failed (will retry, #{tries} tries left). Event was #{event.to_s}")
-      EventMachine::add_timer(2) do
-        # TODO(sissel): Actually abort if we retry too many times.
-        receive_http(event, tries - 1)
-      end
+    bulk_request.on(:success) do |response|
+      @logger.debug("Index successful")
+    end.on(:failure) do |exception|
+      @logger.warn(["Failed to index an event", exception])
     end
-  end # def receive_http
 
-  public
-  def receive_native(event)
+    @logger.debug(["Sending bulk index request", {:num_events => num_events}])
+    bulk_request.execute! # synchronously
+  end
+
+  # Returns number of events added
+  protected
+  def add_index_request(bulk_request, event)
+    return 0 if event.nil?
+    @logger.debug(["Adding event to bulk index request", event.to_hash])
     index = event.sprintf(@index)
     type = event.sprintf(@type)
+
     # TODO(sissel): allow specifying the ID?
     # The document ID is how elasticsearch determines sharding hash, so it can
     # help performance if we allow folks to specify a specific ID.
-    req = @client.index(index, type, event.to_hash)
-    req.on(:success) do |response|
-      @logger.debug(["Successfully indexed", event.to_hash])
-    end.on(:failure) do |exception|
-      @logger.debug(["Failed to index an event", exception, event.to_hash])
-    end
-    req.execute
-  end # def receive_native
+    bulk_request.index(index, type, nil, event.to_hash)
+    return 1
+  end
 
   public
-  def receive_river(event)
-    # bulk format; see http://www.elasticsearch.com/docs/elasticsearch/river/rabbitmq/
-    index_message = {"index" => {"_index" => @es_index, "_type" => @es_type}}.to_json + "\n"
-    #index_message += {@es_type => event.to_hash}.to_json + "\n"
-    index_message += event.to_hash.to_json + "\n"
-    @mq.receive_raw(index_message)
-  end # def receive_river
+  def teardown
+    @shutdown_requested = true
+    @event_buffer << nil # Send a final event to wake up the thread
+    @logger.info("Waiting for elasticsearch writer to finish")
+
+    # There may be a race condition which will cause the thread to wait forever.
+    # Give it some time and then be aggressive.
+    unless @elasticsearch_writer.join(10)
+      @logger.info("Time's up; killing elasticsearch writer")
+      @elasticsearch_writer.kill
+    end
+    finished
+  end
 
   private
   def old_create_index
@@ -225,7 +198,7 @@ def old_create_index
     # Describe this index to elasticsearch
     indexmap = {
       # The name of the index
-      "settings" => { 
+      "settings" => {
         @url.path.split("/")[-1] => {
           "mappings" => {
             "@source" => { "type" => "string" },
@@ -237,7 +210,7 @@ def old_create_index
 
             # TODO(sissel): Hack for now until this bug is resolved:
             # https://github.com/elasticsearch/elasticsearch/issues/issue/604
-            "@fields" => { 
+            "@fields" => {
               "type" => "object",
               "properties" => {
                 "HOSTNAME" => { "type" => "string" },
diff --git a/lib/logstash/outputs/loggly.rb b/lib/logstash/outputs/loggly.rb
index 0d5046a03c9..d5167c2ae6e 100644
--- a/lib/logstash/outputs/loggly.rb
+++ b/lib/logstash/outputs/loggly.rb
@@ -25,6 +25,9 @@ class LogStash::Outputs::Loggly < LogStash::Outputs::Base
   #                                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
   #                                           \---------->   key   <-------------/
   #
+  # You can use %{foo} field lookups here if you need to pull the api key from
+  # the event. This is mainly aimed at multitenant hosting providers who want
+  # to offer shipping a customer's logs to that customer's loggly account.
   config :key, :validate => :string, :required => true
 
   public
@@ -40,8 +43,7 @@ def receive(event)
     end
 
     # Send the event over http.
-    #url = URI.parse("#{@url}inputs/#{@key}")
-    url = URI.parse("http://#{@host}/inputs/#{@key}")
+    url = URI.parse("http://#{@host}/inputs/#{event.sprintf(@key)}")
     @logger.info("Loggly URL: #{url}")
     request = Net::HTTP::Post.new(url.path)
     request.body = event.to_json
diff --git a/lib/logstash/plugin.rb b/lib/logstash/plugin.rb
index b928ba2520d..1af7051ad20 100644
--- a/lib/logstash/plugin.rb
+++ b/lib/logstash/plugin.rb
@@ -3,6 +3,25 @@
 require "logstash/config/mixin"
 
 class LogStash::Plugin
+  attr_accessor :params
+  attr_accessor :logger
+
+  public
+  def hash
+    params.hash ^
+    self.class.name.hash
+  end
+
+  public
+  def eql?(other)
+    self.class.name == other.class.name && @params == other.params
+  end
+
+  public
+  def initialize(params=nil)
+    @params = params
+    @logger = LogStash::Logger.new(STDOUT)
+  end
 
   # This method is called when someone or something wants this plugin to shut
   # down. When you successfully shutdown, you must call 'finished'
@@ -43,6 +62,13 @@ def finished
   public
   def teardown
     # nothing by default
+    finished
+  end
+
+  # This method is called when a SIGHUP triggers a reload operation
+  public
+  def reload
+    # Do nothing by default
   end
 
   public
@@ -60,4 +86,8 @@ def terminating?
     return @plugin_state == :terminating
   end # def terminating?
 
+  public
+  def to_s
+    return "#{self.class.name}: #{@params}"
+  end
 end # class LogStash::Plugin
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index 629ff0d75ca..d8a8168bf76 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -22,7 +22,7 @@ def main(args)
 
     if RUBY_VERSION != "1.9.2"
       $stderr.puts "Ruby 1.9.2 mode is required."
-      $stderr.puts "Options for fixin this: "
+      $stderr.puts "Options for fixing this: "
       $stderr.puts "  * If doing 'ruby bin/logstash ...' add --1.9 flag to 'ruby'"
       $stderr.puts "  * If doing 'java -jar ... ' add -Djruby.compat.version=RUBY1_9 to java flags"
       return 1
diff --git a/lib/logstash/util/require-helper.rb b/lib/logstash/util/require-helper.rb
index c41caa04da8..90e588b3a73 100644
--- a/lib/logstash/util/require-helper.rb
+++ b/lib/logstash/util/require-helper.rb
@@ -1,12 +1,12 @@
-require "logger"
 require "logstash/namespace"
+require "logstash/logging"
 
 module LogStash::Util::Require
   class << self
     attr_accessor :logger
 
     def require(lib, gemdep, message=nil)
-      @logger ||= Logger.new(STDERR)
+      @logger ||= LogStash::Logger.new(STDERR)
       begin
         require lib
       rescue LoadError => e
