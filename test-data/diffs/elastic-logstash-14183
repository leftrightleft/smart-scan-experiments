diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java
index a224c5f9308..858aef8201b 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java
@@ -53,11 +53,12 @@
 import java.nio.file.WatchEvent;
 import java.nio.file.WatchKey;
 import java.nio.file.WatchService;
+import java.time.Duration;
+import java.time.temporal.ChronoUnit;
 import java.util.Comparator;
 import java.util.NoSuchElementException;
 import java.util.Optional;
-import java.util.concurrent.ConcurrentSkipListSet;
-import java.util.concurrent.TimeUnit;
+import java.util.concurrent.*;
 import java.util.stream.Collectors;
 
 import static java.nio.file.StandardWatchEventKinds.ENTRY_CREATE;
@@ -66,13 +67,54 @@
 
 public final class DeadLetterQueueReader implements Closeable {
     private static final Logger logger = LogManager.getLogger(DeadLetterQueueReader.class);
+    private DeadLetterQueueSinceDB consumerPosition;
 
     private RecordIOReader currentReader;
     private final Path queuePath;
     private final ConcurrentSkipListSet<Path> segments;
     private final WatchService watchService;
+    private volatile int readEventsCounter = 0;
+    private final ScheduledExecutorService periodicFlusherPool = Executors.newSingleThreadScheduledExecutor(r -> customizeThreadName(r, "DLQ reader periodic flusher"));
+
+    private class PeriodicPositionFlushTask implements Runnable {
+
+        private int lastSeenReadEventsCounter = 0;
+
+        @Override
+        public void run() {
+            if (!cleanConsumed) {
+                return;
+            }
+            if (readEventsCounter == lastSeenReadEventsCounter) {
+                return;
+            }
+
+            logger.debug("Periodic flush of DLQ({}) tail position", queuePath);
+            consumerPosition.flush();
+            readEventsCounter = 0;
+            lastSeenReadEventsCounter = 0;
+        }
+    }
+
+    private static Thread customizeThreadName(Runnable r, String name) {
+        Thread thread = new Thread(r);
+        thread.setName(name);
+        return thread;
+    }
+
+    // config settings
+    private final boolean cleanConsumed;
+    private final int sinceDbFlushThreshold;
 
     public DeadLetterQueueReader(Path queuePath) throws IOException {
+        this(queuePath, false, 1000);
+    }
+
+    public DeadLetterQueueReader(Path queuePath, boolean cleanConsumed, int sinceDbFlushThreshold) throws IOException {
+        this(queuePath, cleanConsumed, sinceDbFlushThreshold, Duration.of(5, ChronoUnit.SECONDS));
+    }
+
+    public DeadLetterQueueReader(Path queuePath, boolean cleanConsumed, int sinceDbFlushThreshold, Duration flushInterval) throws IOException {
         this.queuePath = queuePath;
         this.watchService = FileSystems.getDefault().newWatchService();
         this.queuePath.register(watchService, ENTRY_CREATE, ENTRY_DELETE);
@@ -80,6 +122,13 @@ public DeadLetterQueueReader(Path queuePath) throws IOException {
                 Comparator.comparingInt(DeadLetterQueueUtils::extractSegmentId)
         );
         segments.addAll(getSegmentPaths(queuePath).collect(Collectors.toList()));
+        this.cleanConsumed = cleanConsumed;
+        this.sinceDbFlushThreshold = sinceDbFlushThreshold;
+        if (cleanConsumed) {
+            consumerPosition = DeadLetterQueueSinceDB.load(queuePath);
+            PeriodicPositionFlushTask flusherTask = new PeriodicPositionFlushTask();
+            periodicFlusherPool.scheduleWithFixedDelay(flusherTask, flushInterval.toMillis(), flushInterval.toMillis(), TimeUnit.MILLISECONDS);
+        }
     }
 
     public void seekToNextEvent(Timestamp timestamp) throws IOException {
@@ -172,11 +221,12 @@ public DLQEntry pollEntry(long timeout) throws IOException, InterruptedException
         return DLQEntry.deserialize(bytes);
     }
 
+    // package-private for test
     byte[] pollEntryBytes() throws IOException, InterruptedException {
         return pollEntryBytes(100);
     }
 
-    byte[] pollEntryBytes(long timeout) throws IOException, InterruptedException {
+    private byte[] pollEntryBytes(long timeout) throws IOException, InterruptedException {
         long timeoutRemaining = timeout;
         if (currentReader == null) {
             timeoutRemaining -= pollNewSegments(timeout);
@@ -209,6 +259,12 @@ byte[] pollEntryBytes(long timeout) throws IOException, InterruptedException {
                 pollNewSegments(timeoutRemaining);
             } else {
                 currentReader.close();
+                if (cleanConsumed) {
+                    consumerPosition.flush();
+                    Files.delete(currentReader.getPath()); // delete segment file only after current reader is closed
+                    // drop from segments list, it's the head of the list
+                    segments.remove(currentReader.getPath());
+                }
                 Optional<RecordIOReader> optReader = openNextExistingReader(currentReader.getPath());
                 if (!optReader.isPresent()) {
                     // segments were all already deleted files, do a poll
@@ -220,6 +276,16 @@ byte[] pollEntryBytes(long timeout) throws IOException, InterruptedException {
             }
         }
 
+        if (cleanConsumed) {
+            consumerPosition.updatePosition(this);
+            readEventsCounter++;
+
+            if (readEventsCounter > sinceDbFlushThreshold) {
+                consumerPosition.flush();
+                readEventsCounter = 0;
+            }
+        }
+
         return event;
     }
 
@@ -282,10 +348,23 @@ private Optional<RecordIOReader> openNextExistingReader(Path segmentPath) throws
         return Optional.empty();
     }
 
+    /**
+     * @deprecated
+     * This method could throw NullPointerException when there isn't the linkage with a segment file.
+     * Use {@link #getCurrentSegmentPath()}
+     * */
+    @Deprecated
     public Path getCurrentSegment() {
         return currentReader.getPath();
     }
 
+    public Optional<Path> getCurrentSegmentPath() {
+        if (currentReader == null) {
+            return Optional.empty();
+        }
+        return Optional.of(currentReader.getPath());
+    }
+
     public long getCurrentPosition() {
         return currentReader.getChannelPosition();
     }
@@ -296,5 +375,10 @@ public void close() throws IOException {
             currentReader.close();
         }
         this.watchService.close();
+        if (cleanConsumed) {
+            consumerPosition.updatePosition(this);
+            consumerPosition.flush();
+            periodicFlusherPool.shutdown();
+        }
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueSinceDB.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueSinceDB.java
new file mode 100644
index 00000000000..e6947e62a04
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueSinceDB.java
@@ -0,0 +1,135 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.common.io;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.Paths;
+import java.util.Comparator;
+import java.util.Optional;
+
+import static org.logstash.common.io.DeadLetterQueueWriter.getSegmentPaths;
+
+public class DeadLetterQueueSinceDB {
+
+    private static final Logger logger = LogManager.getLogger(DeadLetterQueueSinceDB.class);
+
+    private final static char VERSION = '1';
+    private Path sinceDb;
+    private Path currentSegment;
+    private long offset;
+
+    private DeadLetterQueueSinceDB(Path sinceDb, Path currentSegment, long offset) {
+        this.sinceDb = sinceDb;
+        this.currentSegment = currentSegment;
+        this.offset = offset;
+    }
+
+    public static DeadLetterQueueSinceDB load(Path queuePath) throws IOException {
+        if (queuePath == null) {
+            throw new IllegalArgumentException("queuePath can't be null");
+        }
+        Path sinceDbPath = queuePath.resolve("sincedb");
+        if (!Files.exists(sinceDbPath)) {
+            return startSegment(sinceDbPath, queuePath);
+        }
+
+        byte[] bytes = Files.readAllBytes(sinceDbPath);
+        if (bytes.length == 0) {
+            return startSegment(sinceDbPath, queuePath);
+        }
+
+        ByteBuffer buffer = ByteBuffer.wrap(bytes);
+        verifyVersion(buffer);
+        Path segmentPath = decodePath(buffer);
+        long offset = buffer.getLong();
+        return new DeadLetterQueueSinceDB(sinceDbPath, segmentPath, offset);
+    }
+
+    private static void verifyVersion(ByteBuffer buffer) {
+        char version = buffer.getChar();
+        if (VERSION != version) {
+            throw new RuntimeException("Sincedb version:" + version + " does not match: " + VERSION);
+        }
+    }
+
+    private static Path decodePath(ByteBuffer buffer) {
+        int segmentPathStringLength = buffer.getInt();
+        byte[] segmentPathBytes = new byte[segmentPathStringLength];
+        buffer.get(segmentPathBytes);
+        return Paths.get(new String(segmentPathBytes));
+    }
+
+    /**
+     * Return sincedb instance pointing to the start of the first segment, is present.
+     * If no segment files are present, the DLQ is empty and is going to be populated by next {@link #flush() flush}.
+     * */
+    private static DeadLetterQueueSinceDB startSegment(Path sinceDbPath, Path queuePath) throws IOException {
+        Path firstSegment = findFirstSegment(queuePath)
+                .orElse(null);
+        return new DeadLetterQueueSinceDB(sinceDbPath, firstSegment, 0L);
+    }
+
+    private static Optional<Path> findFirstSegment(Path queuePath) throws IOException {
+        return getSegmentPaths(queuePath)
+                .min(Comparator.comparingInt(DeadLetterQueueUtils::extractSegmentId));
+    }
+
+    public synchronized void flush() {
+        if (currentSegment == null) {
+            return;
+        }
+        logger.debug("Flushing DLQ last read position");
+        String path = currentSegment.toAbsolutePath().toString();
+        ByteBuffer buffer = ByteBuffer.allocate(path.length() + 1 + 64);
+        buffer.putChar(VERSION);
+        buffer.putInt(path.length());
+        buffer.put(path.getBytes());
+        buffer.putLong(offset);
+        try {
+            Files.write(sinceDb, buffer.array());
+        } catch (IOException e) {
+            logger.error("failed to write DLQ offset state to " + sinceDb, e);
+        }
+    }
+
+    private void updatePosition(Path segment, long offset) {
+        this.currentSegment = segment;
+        this.offset = offset;
+    }
+
+    public synchronized void updatePosition(DeadLetterQueueReader reader) {
+        reader.getCurrentSegmentPath()
+                .ifPresent(segment -> updatePosition(segment, reader.getCurrentPosition()));
+    }
+
+    public synchronized Path getCurrentSegment() {
+        return currentSegment;
+    }
+
+    public synchronized long getOffset() {
+        return offset;
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderCompletedSegmentRemovalTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderCompletedSegmentRemovalTest.java
new file mode 100644
index 00000000000..3996556f1ad
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderCompletedSegmentRemovalTest.java
@@ -0,0 +1,152 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.common.io;
+
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.charset.StandardCharsets;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.time.Duration;
+import java.util.Set;
+import java.util.stream.Collectors;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNotNull;
+import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
+import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
+
+public class DeadLetterQueueReaderCompletedSegmentRemovalTest {
+    private Path dir;
+
+    @Rule
+    public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    @Before
+    public void setUp() throws Exception {
+        dir = temporaryFolder.newFolder().toPath();
+    }
+
+    @Test
+    public void testReaderWithoutSinceDbAndNonReadOperationDoneThenCloseDoesntCreateAnySinceDBFile() throws IOException {
+        DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir, true, 10);
+        readManager.close();
+
+        Set<File> sincedbs = listSincedbFiles();
+        assertEquals("No sincedb file should be present", 0, sincedbs.size());
+    }
+
+    private Set<File> listSincedbFiles() throws IOException {
+        return Files.list(dir)
+                .filter(file -> file.getFileName().toString().equals("sincedb"))
+                .map(Path::toFile)
+                .collect(Collectors.toSet());
+    }
+
+    @Test
+    public void testReaderCreatesASinceDBFileOnCloseAfterReadSomeEvents() throws IOException, InterruptedException {
+        // write some data into a segment file
+        DeadLetterQueueTestUtils.writeSomeEventsInOneSegment(10, dir);
+
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir, true, 10)) {
+            byte[] rawStr = readManager.pollEntryBytes();
+            assertNotNull(rawStr);
+            assertEquals("0", new String(rawStr, StandardCharsets.UTF_8));
+        }
+
+        Set<File> sincedbs = listSincedbFiles();
+        assertEquals("Sincedb file must be present", 1, sincedbs.size());
+
+        DeadLetterQueueSinceDB sinceDB = DeadLetterQueueSinceDB.load(dir);
+        assertNotNull(sinceDB.getCurrentSegment());
+        assertEquals("0.log", sinceDB.getCurrentSegment().getFileName().toString());
+        assertEquals(VERSION_SIZE + RECORD_HEADER_SIZE + 1, sinceDB.getOffset());
+    }
+
+    @Test
+    public void testReaderFlushSinceDbEverytimePassesTheConfiguredThreshold() throws IOException, InterruptedException {
+        DeadLetterQueueTestUtils.writeSomeEventsInOneSegment(15, dir);
+
+        int thresholdLimit = 5;
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir, true, thresholdLimit)) {
+            // read 1 event more the sinceDb flush threshold
+            readEvents(readManager, thresholdLimit + 1);
+
+            // verify sinceDb file is created
+            DeadLetterQueueSinceDB sinceDB = DeadLetterQueueSinceDB.load(dir);
+            assertNotNull(sinceDB.getCurrentSegment());
+            assertEquals("0.log", sinceDB.getCurrentSegment().getFileName().toString());
+            assertEquals(VERSION_SIZE + (RECORD_HEADER_SIZE + 1) * (thresholdLimit + 1), sinceDB.getOffset());
+
+            for (int i = 0; i < thresholdLimit + 1; i++) {
+                readEvent(readManager, i + thresholdLimit + 1);
+            }
+
+            // verify sinceDb is updated
+            sinceDB = DeadLetterQueueSinceDB.load(dir);
+            assertNotNull(sinceDB.getCurrentSegment());
+            assertEquals("0.log", sinceDB.getCurrentSegment().getFileName().toString());
+            int doubleDigitsInts = 2; // 10 and 11 occupy 2 characters instead of one
+            assertEquals(VERSION_SIZE + (RECORD_HEADER_SIZE + 1) * 2 * (thresholdLimit + 1) + doubleDigitsInts, sinceDB.getOffset());
+        }
+    }
+
+    private void readEvents(DeadLetterQueueReader readManager, int numEvents) throws IOException, InterruptedException {
+        for (int i = 0; i < numEvents; i++) {
+            readEvent(readManager, i);
+        }
+    }
+
+    private void readEvent(DeadLetterQueueReader readManager, int i) throws IOException, InterruptedException {
+        byte[] payload = readManager.pollEntryBytes();
+        assertNotNull(payload);
+        assertEquals(Integer.toString(i), new String(payload, StandardCharsets.UTF_8));
+    }
+
+    @Test
+    public void testAutomaticFlusherUpdatesPosition() throws IOException, InterruptedException {
+        DeadLetterQueueTestUtils.writeSomeEventsInOneSegment(15, dir);
+
+        final Duration flushInterval = Duration.ofSeconds(1);
+
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir, true, 1_000, flushInterval)) {
+            // move forward the tail but keeping under the flush threshold
+            int readEvents = 2;
+            readEvents(readManager, readEvents);
+
+            assertEquals("No sincedb file should be present", 0, listSincedbFiles().size());
+
+            // sleep for a little bit more than a flush interval
+            Thread.sleep(flushInterval.plusMillis(500).toMillis());
+
+            assertEquals("SinceDB file must be created", 1, listSincedbFiles().size());
+            DeadLetterQueueSinceDB sinceDB = DeadLetterQueueSinceDB.load(dir);
+            assertNotNull(sinceDB.getCurrentSegment());
+            assertEquals("0.log", sinceDB.getCurrentSegment().getFileName().toString());
+            assertEquals(VERSION_SIZE + (RECORD_HEADER_SIZE + 1) * readEvents, sinceDB.getOffset());
+        }
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
index e03d579373f..302e1f352a0 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
@@ -68,18 +68,14 @@
 
 public class DeadLetterQueueReaderTest {
     public static final int INTERNAL_FRAG_PAYLOAD_SIZE = BLOCK_SIZE - RECORD_HEADER_SIZE - 5;
-    private Path dir;
-    private int defaultDlqSize = 100_000_000; // 100mb
-
+    private static final int DEFAULT_DLQ_SIZE = 100_000_000; // 100mb
     private static final int PAD_FOR_BLOCK_SIZE_EVENT = 32490;
 
+    private Path dir;
+
     @Rule
     public TemporaryFolder temporaryFolder = new TemporaryFolder();
 
-    private static String segmentFileName(int i) {
-        return String.format(DeadLetterQueueWriter.SEGMENT_FILE_PATTERN, i);
-    }
-
     @Before
     public void setUp() throws Exception {
         dir = temporaryFolder.newFolder().toPath();
@@ -90,7 +86,7 @@ public void testReadFromTwoSegments() throws Exception {
         RecordIOWriter writer = null;
 
         for (int i = 0; i < 5; i++) {
-            Path segmentPath = dir.resolve(segmentFileName(i));
+            Path segmentPath = dir.resolve(DeadLetterQueueTestUtils.segmentFileName(i));
             writer = new RecordIOWriter(segmentPath);
             for (int j = 0; j < 10; j++) {
                 writer.writeEvent((new StringElement("" + (i * 10 + j))).serialize());
@@ -123,7 +119,7 @@ public void testReadFromTwoSegments() throws Exception {
 
         writer.close();
 
-        Path segmentPath = dir.resolve(segmentFileName(5));
+        Path segmentPath = dir.resolve(DeadLetterQueueTestUtils.segmentFileName(5));
         writer = new RecordIOWriter(segmentPath);
 
         for (int j = 0; j < 10; j++) {
@@ -155,7 +151,7 @@ public void testRereadFinalBlock() throws Exception {
         event.setField("message", generateMessageContent(32495));
         long startTime = System.currentTimeMillis();
         int messageSize = 0;
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = 0; i < 2; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), constantSerializationLengthTimestamp(startTime++));
                 final int serializationLength = entry.serialize().length;
@@ -209,8 +205,8 @@ private void writeSegmentSizeEntries(int count) throws IOException {
         final Event event = createEventWithConstantSerializationOverhead();
         long startTime = System.currentTimeMillis();
         DLQEntry templateEntry = new DLQEntry(event, "1", "1", String.valueOf(0), constantSerializationLengthTimestamp(startTime));
-        int size = templateEntry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE + VERSION_SIZE;
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, size, defaultDlqSize, Duration.ofSeconds(1))) {
+        int size = templateEntry.serialize().length + RECORD_HEADER_SIZE + VERSION_SIZE;
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, size, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = 1; i <= count; i++) {
                 writeManager.writeEntry(new DLQEntry(event, "1", "1", String.valueOf(i), constantSerializationLengthTimestamp(startTime++)));
             }
@@ -242,10 +238,10 @@ public void testBlockBoundary() throws Exception {
         event.setField("T", new String(field));
         Timestamp timestamp = constantSerializationLengthTimestamp();
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = 0; i < 2; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", "", timestamp);
-                assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
+                assertThat(entry.serialize().length + RECORD_HEADER_SIZE, is(BLOCK_SIZE));
                 writeManager.writeEntry(entry);
             }
         }
@@ -265,13 +261,13 @@ public void testBlockBoundaryMultiple() throws Exception {
         event.setField("message", new String(field));
         long startTime = System.currentTimeMillis();
         int messageSize = 0;
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = 1; i <= 5; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", "", constantSerializationLengthTimestamp(startTime++));
                 messageSize += entry.serialize().length;
                 writeManager.writeEntry(entry);
                 if (i == 4){
-                    assertThat(messageSize + (RecordIOWriter.RECORD_HEADER_SIZE * 4), is(BLOCK_SIZE));
+                    assertThat(messageSize + (RECORD_HEADER_SIZE * 4), is(BLOCK_SIZE));
                 }
             }
         }
@@ -288,7 +284,7 @@ public void testFlushAfterWriterClose() throws Exception {
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT/8));
         Timestamp timestamp = new Timestamp();
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = 0; i < 6; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -309,7 +305,7 @@ public void testFlushAfterSegmentComplete() throws Exception {
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = new Timestamp();
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * EVENTS_BEFORE_FLUSH, defaultDlqSize, Duration.ofHours(1))) {
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * EVENTS_BEFORE_FLUSH, DEFAULT_DLQ_SIZE, Duration.ofHours(1))) {
             for (int i = 1; i < EVENTS_BEFORE_FLUSH; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -342,7 +338,7 @@ public void testMultiFlushAfterSegmentComplete() throws Exception {
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = new Timestamp();
 
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * eventsInSegment, defaultDlqSize, Duration.ofHours(1))) {
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * eventsInSegment, DEFAULT_DLQ_SIZE, Duration.ofHours(1))) {
             for (int i = 1; i < totalEventsToWrite; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -383,7 +379,7 @@ public void testFlushAfterDelay() throws Exception {
 
         System.out.println("events per block= " + eventsPerBlock);
 
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(2))) {
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, DEFAULT_DLQ_SIZE, Duration.ofSeconds(2))) {
             for (int i = 1; i < eventsToWrite; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", Integer.toString(i), timestamp);
                 writeManager.writeEntry(entry);
@@ -415,10 +411,10 @@ public void testBlockAndSegmentBoundary() throws Exception {
         event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = constantSerializationLengthTimestamp();
 
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))) {
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = 0; i < 2; i++) {
                 DLQEntry entry = new DLQEntry(event, "", "", "", timestamp);
-                assertThat(entry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE, is(BLOCK_SIZE));
+                assertThat(entry.serialize().length + RECORD_HEADER_SIZE, is(BLOCK_SIZE));
                 writeManager.writeEntry(entry);
             }
         }
@@ -436,7 +432,7 @@ public void testWriteReadRandomEventSize() throws Exception {
         int eventCount = 1024; // max = 1000 * 64kb = 64mb
         long startTime = System.currentTimeMillis();
 
-        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = 0; i < eventCount; i++) {
                 event.setField("message", generateMessageContent((int)(Math.random() * (maxEventSize))));
                 DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), new Timestamp(startTime++));
@@ -490,7 +486,7 @@ public void testSeekByTimestampMoveAfterDeletedSegment() throws IOException, Int
 
         try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
             // remove the first segment
-            Files.delete(dir.resolve(segmentFileName(1)));
+            Files.delete(dir.resolve(DeadLetterQueueTestUtils.segmentFileName(1)));
 
             //Exercise, seek in the middle of first segment
             final Timestamp seekTarget = new Timestamp(startTime + (eventsPerSegment / 2));
@@ -511,8 +507,8 @@ public void testSeekByTimestampWhenAllSegmentsAreDeleted() throws IOException, I
 
         try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
             // remove the first segment
-            Files.delete(dir.resolve(segmentFileName(1)));
-            Files.delete(dir.resolve(segmentFileName(2)));
+            Files.delete(dir.resolve(DeadLetterQueueTestUtils.segmentFileName(1)));
+            Files.delete(dir.resolve(DeadLetterQueueTestUtils.segmentFileName(2)));
 
             //Exercise, seek in the middle of first segment
             final Timestamp seekTarget = new Timestamp(startTime + (eventsPerSegment / 2));
@@ -537,7 +533,7 @@ public void testConcurrentWriteReadRandomEventSize() throws Exception {
             exec.submit(() -> {
                 final Event event = new Event();
                 long startTime = System.currentTimeMillis();
-                try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(10))) {
+                try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, DEFAULT_DLQ_SIZE, Duration.ofSeconds(10))) {
                     for (int i = 0; i < eventCount; i++) {
                         event.setField(
                                 "message",
@@ -665,12 +661,7 @@ public void testSeekToMiddleWhileTheLogIsRemoved() throws IOException, Interrupt
     @Test
     public void testStoreReaderPositionAndRestart() throws IOException, InterruptedException {
         // write some data into a segment file
-        Path segmentPath = dir.resolve(segmentFileName(0));
-        RecordIOWriter writer = new RecordIOWriter(segmentPath);
-        for (int j = 0; j < 10; j++) {
-            writer.writeEvent((new StringElement("" + j)).serialize());
-        }
-        writer.close();
+        DeadLetterQueueTestUtils.writeSomeEventsInOneSegment(10, dir);
 
         // read the first event and save read position
         Path currentSegment;
@@ -679,7 +670,7 @@ public void testStoreReaderPositionAndRestart() throws IOException, InterruptedE
             byte[] rawStr = reader.pollEntryBytes();
             assertNotNull(rawStr);
             assertEquals("0", new String(rawStr, StandardCharsets.UTF_8));
-            currentSegment = reader.getCurrentSegment();
+            currentSegment = reader.getCurrentSegmentPath().get();
             currentPosition = reader.getCurrentPosition();
         }
 
@@ -727,7 +718,7 @@ public void testStoreReaderPositionWithBlocksWithInternalFragmentation() throws
             byte[] rawStr = reader.pollEntryBytes();
             assertNotNull(rawStr);
             assertEquals(stringOf(INTERNAL_FRAG_PAYLOAD_SIZE, 'A'), new String(rawStr, StandardCharsets.UTF_8));
-            currentSegment = reader.getCurrentSegment();
+            currentSegment = reader.getCurrentSegmentPath().get();
             currentPosition = reader.getCurrentPosition();
         }
 
@@ -746,7 +737,7 @@ public void testStoreReaderPositionWithBlocksWithInternalFragmentationOnceMessag
         final int payloadSize = INTERNAL_FRAG_PAYLOAD_SIZE + BLOCK_SIZE;
         byte[] almostFullBlockPayload = new byte[payloadSize];
         Arrays.fill(almostFullBlockPayload, (byte) 'A');
-        Path segmentPath = dir.resolve(segmentFileName(0));
+        Path segmentPath = dir.resolve(DeadLetterQueueTestUtils.segmentFileName(0));
         RecordIOWriter writer = new RecordIOWriter(segmentPath);
         writer.writeEvent(almostFullBlockPayload);
 
@@ -764,7 +755,7 @@ public void testStoreReaderPositionWithBlocksWithInternalFragmentationOnceMessag
             byte[] rawStr = reader.pollEntryBytes();
             assertNotNull(rawStr);
             assertEquals(stringOf(payloadSize, 'A'), new String(rawStr, StandardCharsets.UTF_8));
-            currentSegment = reader.getCurrentSegment();
+            currentSegment = reader.getCurrentSegmentPath().get();
             currentPosition = reader.getCurrentPosition();
         }
 
@@ -781,7 +772,7 @@ public void testStoreReaderPositionWithBlocksWithInternalFragmentationOnceMessag
     private void writeSegmentWithFirstBlockContainingInternalFragmentation() throws IOException {
         byte[] almostFullBlockPayload = new byte[INTERNAL_FRAG_PAYLOAD_SIZE];
         Arrays.fill(almostFullBlockPayload, (byte) 'A');
-        Path segmentPath = dir.resolve(segmentFileName(0));
+        Path segmentPath = dir.resolve(DeadLetterQueueTestUtils.segmentFileName(0));
         RecordIOWriter writer = new RecordIOWriter(segmentPath);
         writer.writeEvent(almostFullBlockPayload);
 
@@ -878,7 +869,7 @@ private void seekReadAndVerify(final Timestamp seekTarget, final String expected
     }
 
     private void writeEntries(final Event event, int offset, final int numberOfEvents, long startTime) throws IOException {
-        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             for (int i = offset; i <= offset + numberOfEvents; i++) {
                 DLQEntry entry = new DLQEntry(event, "foo", "bar", String.valueOf(i), new Timestamp(startTime++));
                 writeManager.writeEntry(entry);
@@ -899,7 +890,7 @@ private int prepareFilledSegmentFiles(int segments, long start) throws IOExcepti
 
         final int maxSegmentSize = 10 * MB;
         final int loopPerSegment = (int) Math.floor((maxSegmentSize - 1.0) / BLOCK_SIZE);
-        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, maxSegmentSize, defaultDlqSize, Duration.ofSeconds(1))) {
+        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, maxSegmentSize, DEFAULT_DLQ_SIZE, Duration.ofSeconds(1))) {
             final int loops = loopPerSegment * segments;
             for (int i = 0; i < loops; i++) {
                 entry = new DLQEntry(event, "", "", String.format("%05d", i), constantSerializationLengthTimestamp(start++));
@@ -926,4 +917,5 @@ public void testRestartFromCommitPointRealData() throws IOException, Interrupted
             }
         }
     }
+
 }
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
index 6d898909894..5828a101f80 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
@@ -18,10 +18,28 @@
  */
 package org.logstash.common.io;
 
+import org.logstash.ackedqueue.StringElement;
+
+import java.io.IOException;
+import java.nio.file.Path;
+
 import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 
 public class DeadLetterQueueTestUtils {
     public static final int MB = 1024 * 1024;
     public static final int GB = 1024 * 1024 * 1024;
     public static final int FULL_SEGMENT_FILE_SIZE = 319 * BLOCK_SIZE + 1; // 319 records that fills completely a block plus the 1 byte header of the segment file
+
+    static void writeSomeEventsInOneSegment(int eventCount, Path outputDir) throws IOException {
+        Path segmentPath = outputDir.resolve(segmentFileName(0));
+        RecordIOWriter writer = new RecordIOWriter(segmentPath);
+        for (int j = 0; j < eventCount; j++) {
+            writer.writeEvent((new StringElement("" + j)).serialize());
+        }
+        writer.close();
+    }
+
+    static String segmentFileName(int i) {
+        return String.format(DeadLetterQueueWriter.SEGMENT_FILE_PATTERN, i);
+    }
 }
