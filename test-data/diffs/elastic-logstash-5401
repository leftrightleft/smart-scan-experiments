diff --git a/.gitignore b/.gitignore
index 9360eb39bb9..d6cbc6a259f 100644
--- a/.gitignore
+++ b/.gitignore
@@ -28,3 +28,8 @@ rspec.xml
 .vendor
 integration_run
 .mvn/
+qa/.vm_ssh_config
+qa/.vagrant
+qa/.rspec
+qa/acceptance/.vagrant
+qa/Gemfile.lock
diff --git a/ci/ci_acceptance.sh b/ci/ci_acceptance.sh
new file mode 100755
index 00000000000..5665fe5cc4a
--- /dev/null
+++ b/ci/ci_acceptance.sh
@@ -0,0 +1,49 @@
+#!/usr/bin/env bash
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+SELECTED_TEST_SUITE=$1
+
+if [[ $SELECTED_TEST_SUITE == $"redhat" ]]; then
+  echo "Generating the RPM, make sure you start with a clean environment before generating other packages."
+  rake artifact:rpm
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["redhat"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:redhat
+  bundle exec rake qa:vm:halt["redhat"]
+elif [[ $SELECTED_TEST_SUITE == $"debian" ]]; then
+  echo "Generating the DEB, make sure you start with a clean environment before generating other packages."
+  rake artifact:deb
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["debian"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:debian
+  bundle exec rake qa:vm:halt["debian"]
+elif [[ $SELECTED_TEST_SUITE == $"all" ]]; then
+  echo "Building Logstash artifacts"
+  rake artifact:all
+
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:all
+  bundle exec rake qa:vm:halt
+  cd ..
+fi
diff --git a/docs/static/advanced-pipeline.asciidoc b/docs/static/advanced-pipeline.asciidoc
index 836b9377cb4..f38bc3aa1c4 100644
--- a/docs/static/advanced-pipeline.asciidoc
+++ b/docs/static/advanced-pipeline.asciidoc
@@ -1,17 +1,12 @@
 [[advanced-pipeline]]
-=== Setting Up an Advanced Logstash Pipeline
+=== Parsing Logs with Logstash
 
-A Logstash pipeline in most use cases has one or more input, filter, and output plugins. The scenarios in this section
-build Logstash configuration files to specify these plugins and discuss what each plugin is doing.
+In <<first-event>>, you created a basic Logstash pipeline to test your Logstash setup. In the real world, a Logstash
+pipeline is a bit more complex: it typically has one or more input, filter, and output plugins.  
 
-The Logstash configuration file defines your _Logstash pipeline_. When you start a Logstash instance, use the
-`-f <path/to/file>` option to specify the configuration file that defines that instance’s pipeline.
-
-A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
-plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
-the data to a destination.
-
-image::static/images/basic_logstash_pipeline.png[]
+In this section, you create a Logstash pipeline that takes Apache web logs as input, parses those
+logs to create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Rather than
+defining the pipeline configuration at the command line, you'll define the pipeline in a config file. 
 
 The following text represents the skeleton of a configuration pipeline:
 
@@ -30,34 +25,30 @@ output {
 }
 --------------------------------------------------------------------------------
 
-This skeleton is non-functional, because the input and output sections don’t have any valid options defined. The
-examples in this tutorial build configuration files to address specific use cases.
-
-Paste the skeleton into a file named `first-pipeline.conf` in your home Logstash directory.
-
-[[parsing-into-es]]
-==== Parsing Apache Logs into Elasticsearch
-
-This example creates a Logstash pipeline that takes Apache web logs as input, parses those logs to create specific,
-named fields from the logs, and writes the parsed data to an Elasticsearch cluster.
+This skeleton is non-functional, because the input and output sections don’t have any valid options defined. 
 
-You can download the sample data set used in this example
-https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here]. Unpack this file.
+To get started, copy and paste the skeleton configuration pipeline into a file named `first-pipeline.conf` in your home
+Logstash directory. Then go https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here] to
+download the sample data set used in this example. Unpack the file.
 
 [float]
 [[configuring-file-input]]
 ==== Configuring Logstash for File Input
 
-To start your Logstash pipeline, configure the Logstash instance to read from a file using the
-{logstash}plugins-inputs-file.html[file] input plugin.
+NOTE: This example uses the file input plugin for convenience. To tail files in the real world, you'll use
+Filebeat to ship log events to Logstash. You learn how to <<configuring-lsf,configure the Filebeat input plugin>> later
+when you build a more sophisticated pipeline.
 
-Edit the `first-pipeline.conf` file to add the following text:
+To begin your Logstash pipeline, configure the Logstash instance to read from a file by using the
+{logstash}plugins-inputs-file.html[`file`] input plugin.
+
+Edit the `first-pipeline.conf` file and replace the entire `input` section with the following text:
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
     file {
-        path => "/path/to/logstash-tutorial.log"
+        path => "/path/to/file/*.log"
         start_position => beginning <1>
         ignore_older => 0 <2>
     }
@@ -67,21 +58,23 @@ input {
 <1> The default behavior of the file input plugin is to monitor a file for new information, in a manner similar to the
 UNIX `tail -f` command. To change this default behavior and process the entire file, we need to specify the position
 where Logstash starts processing the file.
-<2> The default behavior of the file input plugin is to ignore files whose last modification is greater than 86400s. To change this default behavior and process the tutorial file (which date can be much older than a day), we need to specify to not ignore old files.
+<2> The default behavior of the file input plugin is to ignore files whose last modification is greater than 86400s. To change this default behavior and process the tutorial file (which is probably much older than a day), we need to configure Logstash so that it does not ignore old files.
 
-Replace `/path/to/` with the actual path to the location of `logstash-tutorial.log` in your file system.
+Replace `/path/to/file` with the absolute path to the location of `logstash-tutorial.log` in your file system.
 
 [float]
 [[configuring-grok-filter]]
-===== Parsing Web Logs with the Grok Filter Plugin
+==== Parsing Web Logs with the Grok Filter Plugin
 
 The {logstash}plugins-filters-grok.html[`grok`] filter plugin is one of several plugins that are available by default in
 Logstash. For details on how to manage Logstash plugins, see the <<working-with-plugins,reference documentation>> for
 the plugin manager.
 
-Because the `grok` filter plugin looks for patterns in the incoming log data, configuration requires you to make
-decisions about how to identify the patterns that are of interest to your use case. A representative line from the web
-server log sample looks like this:
+The `grok` filter plugin enables you to parse the unstructured log data into something structured and queryable.
+
+Because the `grok` filter plugin looks for patterns in the incoming log data, configuring the plugin requires you to
+make decisions about how to identify the patterns that are of interest to your use case. A representative line from the
+web server log sample looks like this:
 
 [source,shell]
 --------------------------------------------------------------------------------
@@ -90,8 +83,7 @@ HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-
 Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
 --------------------------------------------------------------------------------
 
-The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. In this tutorial, use
-the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
+The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. To parse the data, you can use the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
 
 [horizontal]
 *Information*:: *Field Name*
@@ -107,7 +99,7 @@ Bytes served:: `bytes`
 Referrer URL:: `referrer`
 User agent:: `agent`
 
-Edit the `first-pipeline.conf` file to add the following text:
+Edit the `first-pipeline.conf` file and replace the entire `filter` section with the following text:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -117,15 +109,8 @@ filter {
     }
 }
 --------------------------------------------------------------------------------
-And change output to see a result :
-[source,json]
---------------------------------------------------------------------------------
-output {
-  stdout { codec => rubydebug }
-}
---------------------------------------------------------------------------------
 
-After processing, the sample line has the following JSON representation:
+After processing the log file with the grok pattern, the sample line will have the following JSON representation:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -145,60 +130,76 @@ After processing, the sample line has the following JSON representation:
 --------------------------------------------------------------------------------
 
 [float]
-[[indexing-parsed-data-into-elasticsearch]]
-===== Indexing Parsed Data into Elasticsearch
+[[configuring-geoip-plugin]]
+==== Enhancing Your Data with the Geoip Filter Plugin
 
-Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
-Elasticsearch cluster. Edit the `first-pipeline.conf` file to add the following text after the `input` section:
+In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing
+data. As an example, the {logstash}plugins-filters-geoip.html[`geoip`] plugin looks up IP addresses, derives geographic
+location information from the addresses, and adds that location information to the logs.
+
+Configure your Logstash instance to use the `geoip` filter plugin by adding the following lines to the `filter` section
+of the `first-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
-output {
-    elasticsearch {
+    geoip {
+        source => "clientip"
     }
-}
 --------------------------------------------------------------------------------
 
-With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes Logstash
-and Elasticsearch to be running on the same instance. You can specify a remote Elasticsearch instance using `hosts`
-configuration like `hosts => "es-machine:9092"`.
+The `geoip` plugin configuration requires you to specify the name of the source field that contains the IP address to look up. In this example, the `clientip` field contains the IP address.
 
-[float]
-[[configuring-geoip-plugin]]
-===== Enhancing Your Data with the Geoip Filter Plugin
+Since filters are evaluated in sequence, make sure that the `geoip` section is after the `grok` section of 
+the configuration file and that both the `grok` and `geoip` sections are nested within the `filter` section 
+like this:
 
-In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing
-data. As an example, the {logstash}plugins-filters-geoip.html[`geoip`] plugin looks up IP addresses, derives geographic
-location information from the addresses, and adds that location information to the logs.
+[source,json]
+--------------------------------------------------------------------------------
+ filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+    geoip {
+        source => "clientip"
+    }
+--------------------------------------------------------------------------------
 
-Configure your Logstash instance to use the `geoip` filter plugin by adding the following lines to the `filter` section
-of the `first-pipeline.conf` file:
+
+[float]
+[[indexing-parsed-data-into-elasticsearch]]
+==== Indexing Your Data into Elasticsearch
+
+Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
+Elasticsearch cluster. Edit the `first-pipeline.conf` file and replace the entire `output` section with the following
+text:
 
 [source,json]
 --------------------------------------------------------------------------------
-geoip {
-    source => "clientip"
+output {
+    elasticsearch {
+        hosts => [ "localhost:9200" ]
+    }
 }
 --------------------------------------------------------------------------------
 
-The `geoip` plugin configuration requires data that is already defined as separate fields. Make sure that the `geoip`
-section is after the `grok` section of the configuration file.
-
-Specify the name of the field that contains the IP address to look up. In this tutorial, the field name is `clientip`.
+With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes that
+Logstash and Elasticsearch are running on the same instance. You can specify a remote Elasticsearch instance by using
+the `hosts` configuration to specify something like `hosts => "es-machine:9092"`.
 
 [float]
 [[testing-initial-pipeline]]
 ===== Testing Your Initial Pipeline
 
 At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
-like this:
+something like this:
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
     file {
-        path => "/Users/palecur/logstash-1.5.2/logstash-tutorial-dataset"
+        path => "/Users/myusername/tutorialdata/*.log"
         start_position => beginning
+        ignore_older => 0 
     }
 }
 filter {
@@ -210,8 +211,9 @@ filter {
     }
 }
 output {
-    elasticsearch {}
-    stdout {}
+    elasticsearch {
+        hosts => "localhost:9200"
+    }
 }
 --------------------------------------------------------------------------------
 
@@ -219,11 +221,11 @@ To verify your configuration, run the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf --configtest
+bin/logstash -f first-pipeline.conf --config.test_and_exit
 --------------------------------------------------------------------------------
 
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
+The `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file
+passes the configuration test, start Logstash with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
@@ -234,212 +236,228 @@ Try a test query to Elasticsearch based on the fields created by the `grok` filt
 
 [source,shell]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=response=200'
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=response=200'
 --------------------------------------------------------------------------------
 
 Replace $DATE with the current date, in YYYY.MM.DD format.
 
-Since our sample has just one 200 HTTP response, we get one hit back:
+We get multiple hits back. For example:
 
 [source,json]
 --------------------------------------------------------------------------------
-{"took":2,
-"timed_out":false,
-"_shards":{"total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.5351382,
-  "hits":[{"_index":"logstash-2015.07.30",
-    "_type":"logs",
-    "_id":"AU7gqOky1um3U6ZomFaF",
-    "_score":1.5351382,
-    "_source":{"message":"83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
-      "@version":"1",
-      "@timestamp":"2015-07-30T20:30:41.265Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"83.149.9.216",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:13:45 +0000",
-      "verb":"GET",
-      "request":"/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"52878",
-      "referrer":"\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
-      "agent":"\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\""
+{
+  "took" : 4,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 98,
+    "max_score" : 4.833623,
+    "hits" : [ {
+      "_index" : "logstash-2016.05.27",
+      "_type" : "logs",
+      "_id" : "AVT0nBiGe_tzyi1erg7-",
+      "_score" : 4.833623,
+      "_source" : {
+        "request" : "/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
+        "agent" : "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+        "geoip" : {
+          "timezone" : "Europe/Moscow",
+          "ip" : "83.149.9.216",
+          "latitude" : 55.7522,
+          "continent_code" : "EU",
+          "city_name" : "Moscow",
+          "country_code2" : "RU",
+          "country_name" : "Russia",
+          "dma_code" : null,
+          "country_code3" : "RU",
+          "region_name" : "Moscow",
+          "location" : [ 37.6156, 55.7522 ],
+          "postal_code" : "101194",
+          "longitude" : 37.6156,
+          "region_code" : "MOW"
+        },
+        "auth" : "-",
+        "ident" : "-",
+        "verb" : "GET",
+        "message" : "83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+        "referrer" : "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
+        "@timestamp" : "2016-05-27T23:45:50.828Z",
+        "response" : "200",
+        "bytes" : "52878",
+        "clientip" : "83.149.9.216",
+        "@version" : "1",
+        "host" : "myexamplehost",
+        "httpversion" : "1.1",
+        "timestamp" : "04/Jan/2015:05:13:45 +0000"
       }
-    }]
-  }
-}
+    }, 
+    ...
 --------------------------------------------------------------------------------
 
 Try another search for the geographic information derived from the IP address:
 
 [source,shell]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=geoip.city_name=Buffalo'
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=geoip.city_name=Buffalo'
 --------------------------------------------------------------------------------
 
 Replace $DATE with the current date, in YYYY.MM.DD format.
 
-Only one of the log entries comes from Buffalo, so the query produces a single response:
+A few log entries come from Buffalo, so the query produces the following response:
 
 [source,json]
 --------------------------------------------------------------------------------
-{"took":3,
-"timed_out":false,
-"_shards":{
-  "total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.03399,
-  "hits":[{"_index":"logstash-2015.07.31",
-    "_type":"logs",
-    "_id":"AU7mK3CVSiMeBsJ0b_EP",
-    "_score":1.03399,
-    "_source":{
-      "message":"108.174.55.234 - - [04/Jan/2015:05:27:45 +0000] \"GET /?flav=rss20 HTTP/1.1\" 200 29941 \"-\" \"-\"",
-      "@version":"1",
-      "@timestamp":"2015-07-31T22:11:22.347Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"108.174.55.234",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:27:45 +0000",
-      "verb":"GET",
-      "request":"/?flav=rss20",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"29941",
-      "referrer":"\"-\"",
-      "agent":"\"-\"",
-      "geoip":{
-        "ip":"108.174.55.234",
-        "country_code2":"US",
-        "country_code3":"USA",
-        "country_name":"United States",
-        "continent_code":"NA",
-        "region_name":"NY",
-        "city_name":"Buffalo",
-        "postal_code":"14221",
-        "latitude":42.9864,
-        "longitude":-78.7279,
-        "dma_code":514,
-        "area_code":716,
-        "timezone":"America/New_York",
-        "real_region_name":"New York",
-        "location":[-78.7279,42.9864]
+{
+  "took" : 2,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 3,
+    "max_score" : 1.0520113,
+    "hits" : [ {
+      "_index" : "logstash-2016.05.27",
+      "_type" : "logs",
+      "_id" : "AVT0nBiHe_tzyi1erg9T",
+      "_score" : 1.0520113,
+      "_source" : {
+        "request" : "/blog/geekery/solving-good-or-bad-problems.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29",
+        "agent" : "\"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\"",
+        "geoip" : {
+          "timezone" : "America/New_York",
+          "ip" : "198.46.149.143",
+          "latitude" : 42.9864,
+          "continent_code" : "NA",
+          "city_name" : "Buffalo",
+          "country_code2" : "US",
+          "country_name" : "United States",
+          "dma_code" : 514,
+          "country_code3" : "US",
+          "region_name" : "New York",
+          "location" : [ -78.7279, 42.9864 ],
+          "postal_code" : "14221",
+          "longitude" : -78.7279,
+          "region_code" : "NY"
+        },
+        "auth" : "-",
+        "ident" : "-",
+        "verb" : "GET",
+        "message" : "198.46.149.143 - - [04/Jan/2015:05:29:13 +0000] \"GET /blog/geekery/solving-good-or-bad-problems.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1\" 200 10756 \"-\" \"Tiny Tiny RSS/1.11 (http://tt-rss.org/)\"",
+        "referrer" : "\"-\"",
+        "@timestamp" : "2016-05-27T23:45:50.836Z",
+        "response" : "200",
+        "bytes" : "10756",
+        "clientip" : "198.46.149.143",
+        "@version" : "1",
+        "host" : "myexamplehost",
+        "httpversion" : "1.1",
+        "timestamp" : "04/Jan/2015:05:29:13 +0000"
       }
-    }
-  }]
- }
-}
+    }, 
+    ...
 --------------------------------------------------------------------------------
 
 [[multiple-input-output-plugins]]
-==== Multiple Input and Output Plugins
+=== Stitching Together Multiple Input and Output Plugins
 
 The information you need to manage often comes from several disparate sources, and use cases can require multiple
 destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these
 requirements.
 
-This example creates a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
+In this section, you create a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
 sends the information to an Elasticsearch cluster as well as writing the information directly to a file.
 
 [float]
 [[twitter-configuration]]
-==== Reading from a Twitter feed
+==== Reading from a Twitter Feed
 
-To add a Twitter feed, you need several pieces of information:
+To add a Twitter feed, you use the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin. To
+configure the plugin, you need several pieces of information:
 
-* A _consumer_ key, which uniquely identifies your Twitter app, which is Logstash in this case.
+* A _consumer_ key, which uniquely identifies your Twitter app.
 * A _consumer secret_, which serves as the password for your Twitter app.
-* One or more _keywords_ to search in the incoming feed.
+* One or more _keywords_ to search in the incoming feed. The example shows using "cloud" as a keyword, but you can use whatever you want.
 * An _oauth token_, which identifies the Twitter account using this app.
 * An _oauth token secret_, which serves as the password of the Twitter account.
 
-Visit https://dev.twitter.com/apps to set up a Twitter account and generate your consumer key and secret, as well as
-your OAuth token and secret.
+Visit https://dev.twitter.com/apps[https://dev.twitter.com/apps] to set up a Twitter account and generate your consumer
+key and secret, as well as your access token and secret. See the docs for the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin if you're not sure how to generate these keys. 
+
+Like you did earlier when you worked on <<advanced-pipeline>>, create a config file (called `second-pipeline.conf`) that
+contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make
+sure you pass in the correct config file name when you run Logstash. 
 
-Use this information to add the following lines to the `input` section of the `first-pipeline.conf` file:
+Add the following lines to the `input` section of the `second-pipeline.conf` file, substituting your values for the 
+placeholder values shown here:
 
 [source,json]
 --------------------------------------------------------------------------------
-twitter {
-    consumer_key =>
-    consumer_secret =>
-    keywords =>
-    oauth_token =>
-    oauth_token_secret =>
-}
+    twitter {
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
+    }
 --------------------------------------------------------------------------------
 
 [float]
 [[configuring-lsf]]
 ==== The Filebeat Client
 
-The https://github.com/elastic/beats/tree/master/filebeat[filebeat] client is a lightweight, resource-friendly tool that
-collects logs from files on the server and forwards these logs to your Logstash instance for processing. The
-Filebeat client uses the secure Beats protocol to communicate with your Logstash instance. The
-lumberjack protocol is designed for reliability and low latency. Filebeat uses the computing resources of
-the machine hosting the source data, and the {logstash}plugins-inputs-beats.html[Beats input] plugin minimizes the
+The https://github.com/elastic/beats/tree/master/filebeat[Filebeat] client is a lightweight, resource-friendly tool that
+collects logs from files on the server and forwards these logs to your Logstash instance for processing. Filebeat is 
+designed for reliability and low latency. Filebeat uses the computing resources of the machine hosting the source data,
+and the {logstash}plugins-inputs-beats.html[`Beats input`] plugin minimizes the
 resource demands on the Logstash instance.
 
 NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your
 Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
 same machine.
 
-Default Logstash configuration includes the {logstash}plugins-inputs-beats.html[Beats input plugin], which is
-designed to be resource-friendly. To install Filebeat on your data source machine, download the
-appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page].
+The default Logstash configuration includes the {logstash}plugins-inputs-beats.html[`Beats input`] plugin. To install
+Filebeat on your data source machine, download the appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page].
 
-Create a configuration file for Filebeat similar to the following example:
+After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
+directory, and replace the contents with the following lines. Make sure `paths` points to your syslog: 
 
 [source,shell]
 --------------------------------------------------------------------------------
-filebeat:
-  prospectors:
-    -
-      paths:
-        - "/path/to/sample-log" <1>
-      fields:
-        type: syslog
-output:
-  logstash:
-    hosts: ["localhost:5043"]
-  tls:
-    certificate: /path/to/ssl-certificate.crt <2>
-    certificate_key: /path/to/ssl-certificate.key
-    certificate_authorities: /path/to/ssl-certificate.crt
-    timeout: 15
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/*.log <1>
+  fields:
+    type: syslog <2>
+output.logstash:
+  hosts: ["localhost:5043"]
 --------------------------------------------------------------------------------
 
-<1> Path to the file or files that Filebeat processes.
-<2> Path to the SSL certificate for the Logstash instance.
+<1> Absolute path to the file or files that Filebeat processes.
+<2> Adds a field called `type` with the value `syslog` to the event.
 
-Save this configuration file as `filebeat.yml`.
+Save your changes. 
+
+To keep the example configuration simple, you won't specify TLS/SSL settings as you would in a real world
+scenario.
 
 Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the `input` section
-of the `first-pipeline.conf` file:
+of the `second-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
 beats {
     port => "5043"
-    ssl => true
-    ssl_certificate => "/path/to/ssl-cert" <1>
-    ssl_key => "/path/to/ssl-key" <2>
 }
 --------------------------------------------------------------------------------
 
-<1> Path to the SSL certificate that the Logstash instance uses to authenticate itself to Filebeat.
-<2> Path to the key for the SSL certificate.
-
 [float]
 [[logstash-file-output]]
 ==== Writing Logstash Data to a File
@@ -448,7 +466,7 @@ You can configure your Logstash pipeline to write data directly to a file with t
 {logstash}plugins-outputs-file.html[`file`] output plugin.
 
 Configure your Logstash instance to use the `file` output plugin by adding the following lines to the `output` section
-of the `first-pipeline.conf` file:
+of the `second-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -464,7 +482,7 @@ file {
 Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as
 providing redundant points of entry into the cluster when a particular node is unavailable.
 
-To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the output section of the `first-pipeline.conf` file to read:
+To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the `output` section of the `second-pipeline.conf` file to read:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -483,23 +501,20 @@ default port for Elasticsearch is `9200` and can be omitted in the configuration
 [[testing-second-pipeline]]
 ===== Testing the Pipeline
 
-At this point, your `first-pipeline.conf` file looks like this:
+At this point, your `second-pipeline.conf` file looks like this: 
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
     twitter {
-        consumer_key =>
-        consumer_secret =>
-        keywords =>
-        oauth_token =>
-        oauth_token_secret =>
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
     }
     beats {
         port => "5043"
-        ssl => true
-        ssl_certificate => "/path/to/ssl-cert"
-        ssl_key => "/path/to/ssl-key"
     }
 }
 output {
@@ -507,7 +522,7 @@ output {
         hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
     }
     file {
-        path => /path/to/target/file
+        path => "/path/to/target/file"
     }
 }
 --------------------------------------------------------------------------------
@@ -529,91 +544,41 @@ To verify your configuration, run the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf --configtest
+bin/logstash -f second-pipeline.conf --config.test_and_exit
 --------------------------------------------------------------------------------
 
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
+The `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file
+passes the configuration test, start Logstash with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf
+bin/logstash -f second-pipeline.conf
 --------------------------------------------------------------------------------
 
 Use the `grep` utility to search in the target file to verify that information is present:
 
 [source,shell]
 --------------------------------------------------------------------------------
-grep Mozilla /path/to/target/file
+grep syslog /path/to/target/file
 --------------------------------------------------------------------------------
 
 Run an Elasticsearch query to find the same information in the Elasticsearch cluster:
 
 [source,shell]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-2015.07.30/_search?q=agent=Mozilla'
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=fields.type:syslog'
 --------------------------------------------------------------------------------
 
-[[stalled-shutdown]]
-=== Stalled Shutdown Detection
-
-Shutting down a running Logstash instance involves the following steps:
-
-* Stop all input, filter and output plugins
-* Process all in-flight events
-* Terminate the Logstash process
-
-The following conditions affect the shutdown process:
-
-* An input plugin receiving data at a slow pace.
-* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy
-query.
-* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+Replace $DATE with the current date, in YYYY.MM.DD format.
 
-These situations make the duration and success of the shutdown process unpredictable.
+To see data from the Twitter feed, try this query:
 
-Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
-This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
-worker threads.
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'http://localhost:9200/logstash-$DATE/_search?pretty&q=client:iphone'
+--------------------------------------------------------------------------------
 
-To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--allow-unsafe-shutdown` flag when
-you start Logstash.
+Again, remember to replace $DATE with the current date, in YYYY.MM.DD format. 
 
-[[shutdown-stall-example]]
-==== Stall Detection Example
 
-In this example, slow filter execution prevents the pipeline from clean shutdown. By starting Logstash with the
-`--allow-unsafe-shutdown` flag, quitting with *Ctrl+C* results in an eventual shutdown that loses 20 events.
 
-========
-[source,shell]
-% bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } \
-                     output { stdout { codec => dots } }' -w 1 --allow-unsafe-shutdown
-Default settings used: Filter workers: 1
-Logstash startup completed
-^CSIGINT received. Shutting down the pipeline. {:level=>:warn}
-Received shutdown signal, but pipeline is still waiting for in-flight events
-to be processed. Sending another ^C will force quit Logstash, but this may cause
-data loss. {:level=>:warn}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
-The shutdown process appears to be stalled due to busy or blocked plugins. Check
-    the logs for more information.
-{:level=>:error}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
-Forcefully quitting logstash.. {:level=>:fatal}
-========
-
-When `--allow-unsafe-shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index 2b432131c76..fc722534fdd 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -2,6 +2,23 @@
 == Breaking changes
 
 **Breaking changes in 5.0**
+
+Application Settings: Introduced a new way to configure application settings for Logstash through a settings.yml file. This file 
+is typically located in `LS_HOME/config`, or `/etc/logstash` when installed via packages. Logstash will not be able 
+to start without this file, so please make sure to pass in `--path.settings` if you are starting Logstash manually 
+after installing it via a package (RPM, DEB).
+
+Release Packages: When Logstash is installed via DEB, RPM packages, it uses `/usr/share/logstash` and `/var/lib/logstash` to install binaries and config files 
+respectively. Previously it used to install in `/opt` directory. This change was done to make the user experience 
+consistent with other Elastic products. Full directory layout is described https://www.elastic.co/guide/en/logstash/5.0/dir-layout.html[here].
+
+Command Line Interface: Most of the long form https://www.elastic.co/guide/en/logstash/5.0/command-line-flags.html[options] have been renamed 
+to adhere to the yml dot notation to be used in the settings file. Short form options have not been changed.
+
+Plugin Developers: The Event class has a https://github.com/elastic/logstash/issues/5141[new API] to access its data. You will no longer be able to directly use 
+the Event class through the ruby hash paradigm. All the plugins packaged with Logstash has been updated 
+to use the new API and their versions bumped to the next major.
+
 The command `bin/plugin` has been renamed to `bin/logstash-plugin`. `bin/plugin <plugin>`` which is the current 
 way of install packs/plugins is problematic because it pollutes the global namespace if it is put in the path. 
 This command can now install both plugins and "Packs" - a single zip that contains 0 or 1 plugin for each system 
diff --git a/docs/static/configuration.asciidoc b/docs/static/configuration.asciidoc
index b74750d4b99..90e51154b42 100644
--- a/docs/static/configuration.asciidoc
+++ b/docs/static/configuration.asciidoc
@@ -203,6 +203,21 @@ Example:
   my_password => "password"
 ----------------------------------
 
+[[uri]]
+[float]
+==== URI
+
+A URI can be anything from a full URL like 'http://elastic.co/' to a simple identifier
+like 'foobar'. If the URI contains a password such as 'http://user:pass@example.net' the password
+portion of the URI will not be logged or printed.
+
+Example:
+[source,js]
+----------------------------------
+  my_uri => "http://foo:bar@example.net"
+----------------------------------
+
+
 [[path]]
 [float]
 ==== Path
@@ -450,7 +465,7 @@ doesn't exist versus a field that's simply false. The expression `if [foo]` retu
 
 * `[foo]` doesn't exist in the event,
 * `[foo]` exists in the event, but is false, or
-* `[foo]` exists in the event, but is nil
+* `[foo]` exists in the event, but is null
 
 For more complex examples, see <<using-conditionals, Using Conditionals>>.
 
diff --git a/docs/static/contributing-to-logstash.asciidoc b/docs/static/contributing-to-logstash.asciidoc
index 238b26fa4fe..05bdeade5e7 100644
--- a/docs/static/contributing-to-logstash.asciidoc
+++ b/docs/static/contributing-to-logstash.asciidoc
@@ -14,7 +14,7 @@ Since plugins can now be developed and deployed independently of the Logstash
 core, there are documents which guide you through the process of coding and
 deploying your own plugins:
 
-
+* <<plugin-generator,Generating a New Plugin>>
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html[How to write a Logstash input plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_codec_plugin.html[How to write a Logstash codec plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_filter_plugin.html[How to write a Logstash filter plugin]
diff --git a/docs/static/getting-started-with-logstash.asciidoc b/docs/static/getting-started-with-logstash.asciidoc
index 231512b96f4..95479e260f1 100644
--- a/docs/static/getting-started-with-logstash.asciidoc
+++ b/docs/static/getting-started-with-logstash.asciidoc
@@ -2,15 +2,15 @@
 == Getting Started with Logstash
 
 This section guides you through the process of installing Logstash and verifying that everything is running properly.
-Later sections deal with increasingly complex configurations to address selected use cases. This section includes the
-following topics:
+After learning how to stash your first event, you go on to create a more advanced pipeline that takes Apache web logs as
+input, parses the logs, and writes the parsed data to an Elasticsearch cluster. Then you learn how to stitch together multiple input and output plugins to unify data from a variety of disparate sources.
+
+This section includes the following topics:
 
 * <<installing-logstash>>
 * <<first-event>>
 * <<advanced-pipeline>>
-* <<stalled-shutdown>>
-* <<pipeline>>
-
+* <<multiple-input-output-plugins>>
 
 [[installing-logstash]]
 === Installing Logstash
@@ -129,7 +129,17 @@ yum install logstash
 --------------------------------------------------
 
 [[first-event]]
-=== Stashing Your First Event: Basic Logstash Example
+=== Stashing Your First Event
+
+First, let's test your Logstash installation by running the most basic _Logstash pipeline_.
+
+A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
+plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
+the data to a destination.
+
+//TODO: REPLACE WITH NEW IMAGE
+
+image::static/images/basic_logstash_pipeline.png[]
 
 To test your Logstash installation, run the most basic Logstash pipeline:
 
@@ -141,10 +151,10 @@ bin/logstash -e 'input { stdin { } } output { stdout {} }'
 
 The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the
 command line lets you quickly test configurations without having to edit a file between iterations.
-This pipeline takes input from the standard input, `stdin`, and moves that input to the standard output, `stdout`, in a
-structured format.
+The pipeline in the example takes input from the standard input, `stdin`, and moves that input to the standard output,
+`stdout`, in a structured format.
 
-Once "Logstash startup completed" is displayed, type hello world at the command prompt to see Logstash respond:
+After starting Logstash, wait until you see "Pipeline main started" and then enter `hello world` at the command prompt:
 
 [source,shell]
 hello world
@@ -153,5 +163,5 @@ hello world
 Logstash adds timestamp and IP address information to the message. Exit Logstash by issuing a *CTRL-D* command in the
 shell where Logstash is running.
 
-The <<advanced-pipeline,Advanced Tutorial>> expands the capabilities of your Logstash instance to cover broader
-use cases.
+Congratulations! You've created and run a basic Logstash pipeline. Next, you learn how to create a more realistic pipeline.
+
diff --git a/docs/static/images/basic_logstash_pipeline.png b/docs/static/images/basic_logstash_pipeline.png
index d1b31401a49..61341fc68ac 100644
Binary files a/docs/static/images/basic_logstash_pipeline.png and b/docs/static/images/basic_logstash_pipeline.png differ
diff --git a/docs/static/include/pluginbody.asciidoc b/docs/static/include/pluginbody.asciidoc
index a51c4f74aa9..bb44db58c3b 100644
--- a/docs/static/include/pluginbody.asciidoc
+++ b/docs/static/include/pluginbody.asciidoc
@@ -883,7 +883,7 @@ time.
 
 **Version messaging from Logstash**
 
-If you start Logstash with the `--verbose` flag, you will see messages like
+If you start Logstash with the `--log.level verbose` flag, you will see messages like
 these to indicate the relative maturity indicated by the plugin version number:
 
 ** **0.1.x**
diff --git a/docs/static/introduction.asciidoc b/docs/static/introduction.asciidoc
index 9b69959d6a6..597e5b41c58 100644
--- a/docs/static/introduction.asciidoc
+++ b/docs/static/introduction.asciidoc
@@ -27,7 +27,7 @@ Collect more, so you can know more. Logstash welcomes data of all shapes and siz
 Where it all started.
 
 * Handle all types of logging data
-** Easily ingest a multitude of web logs like <<parsing-into-es,Apache>>, and application
+** Easily ingest a multitude of web logs like <<advanced-pipeline,Apache>>, and application
 logs like <<plugins-inputs-log4j,log4j>> for Java
 ** Capture many other log formats like <<plugins-inputs-syslog,syslog>>,
 <<plugins-inputs-eventlog,Windows event logs>>, networking and firewall logs, and more
diff --git a/docs/static/life-of-an-event.asciidoc b/docs/static/life-of-an-event.asciidoc
index b85549ecc36..d1431d116b6 100644
--- a/docs/static/life-of-an-event.asciidoc
+++ b/docs/static/life-of-an-event.asciidoc
@@ -1,5 +1,5 @@
 [[pipeline]]
-=== Logstash Processing Pipeline
+== How Logstash Works
 
 The Logstash event processing pipeline has three stages: inputs -> filters ->
 outputs. Inputs generate events, filters modify them, and outputs ship them
@@ -8,7 +8,7 @@ the data as it enters or exits the pipeline without having to use a separate
 filter.
 
 [float]
-==== Inputs
+=== Inputs
 You use inputs to get data into Logstash. Some of the more commonly-used inputs
 are:
 
@@ -25,7 +25,7 @@ For more information about the available inputs, see
 <<input-plugins,Input Plugins>>.
 
 [float]
-==== Filters
+=== Filters
 Filters are intermediary processing devices in the Logstash pipeline. You can
 combine filters with conditionals to perform an action on an event if it meets
 certain criteria. Some useful filters include:
@@ -45,7 +45,7 @@ For more information about the available filters, see
 <<filter-plugins,Filter Plugins>>.
 
 [float]
-==== Outputs
+=== Outputs
 Outputs are the final phase of the Logstash pipeline. An event can pass through
 multiple outputs, but once all output processing is complete, the event has
 finished its execution. Some commonly used outputs include:
@@ -55,7 +55,7 @@ your data in an efficient, convenient, and easily queryable format...
 Elasticsearch is the way to go. Period. Yes, we're biased :)
 * *file*: write event data to a file on disk.
 * *graphite*: send event data to graphite, a popular open source tool for
-storing and graphing metrics. http://graphite.wikidot.com/
+storing and graphing metrics. http://graphite.readthedocs.io/en/latest/
 * *statsd*: send event data to statsd, a service that "listens for statistics,
 like counters and timers, sent over UDP and sends aggregates to one or more
 pluggable backend services". If you're already using statsd, this could be
@@ -65,7 +65,7 @@ For more information about the available outputs, see
 <<output-plugins,Output Plugins>>.
 
 [float]
-==== Codecs
+=== Codecs
 Codecs are basically stream filters that can operate as part of an input or
 output. Codecs enable you to easily separate the transport of your messages from
 the serialization process. Popular codecs include `json`, `msgpack`, and `plain`
@@ -78,16 +78,16 @@ stacktrace messages into a single event.
 For more information about the available codecs, see
 <<codec-plugins,Codec Plugins>>.
 
-[float]
+[[fault-tolerance]]
 === Fault Tolerance
 
 Logstash keeps all events in main memory during processing. Logstash responds to a SIGTERM by attempting to halt inputs and waiting for pending events to finish processing before shutting down. When the pipeline cannot be flushed due to a stuck output or filter, Logstash waits indefinitely. For example, when a pipeline sends output to a database that is unreachable by the Logstash instance, the instance waits indefinitely after receiving a SIGTERM.
 
-To enable Logstash to detect these situations and terminate with a stalled pipeline, use the `--allow-unsafe-shutdown` flag.
+To enable Logstash to detect these situations and terminate with a stalled pipeline, use the `--pipeline.unsafe_shutdown` flag.
 
 WARNING: Unsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason result in data loss. Shut down Logstash safely whenever possible.
 
-[float]
+[[execution-model]]
 ==== Execution Model
 
 The Logstash pipeline coordinates the execution of inputs, filters, and outputs. The following schematic sketches the data flow of a pipeline:
@@ -156,3 +156,4 @@ Examining the in-depth GC statistics with a tool similar to the excellent https:
 
 NOTE: As long as the GC pattern is acceptable, heap sizes that occasionally increase to the maximum are acceptable. Such heap size spikes happen in response to a burst of large events passing through the pipeline. In general practice, maintain a gap between the used amount of heap memory and the maximum.
 This document is not a comprehensive guide to JVM GC tuning. Read the official http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html[Oracle guide] for more information on the topic. We also recommend reading http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance].
+
diff --git a/docs/static/monitoring-apis.asciidoc b/docs/static/monitoring-apis.asciidoc
index 6b694ee6eb3..7e5717e04ac 100644
--- a/docs/static/monitoring-apis.asciidoc
+++ b/docs/static/monitoring-apis.asciidoc
@@ -99,11 +99,13 @@ Example response:
 --------------------------------------------------
 
 [[stats-info-api]]
-=== Stats Info API
+=== Node Stats API
+
+coming[5.0.0-beta3,Replaces the Stats Info API]
 
 experimental[]
 
-The stats info API retrieves runtime stats about Logstash. 
+The node stats API retrieves runtime stats about Logstash. 
 
 // COMMENTED OUT until Logstash supports multiple pipelines: To retrieve all stats for the Logstash instance, use the `_node/stats` endpoint:
 
@@ -127,9 +129,11 @@ By default, all stats are returned. You can limit this by combining any of the f
 
 [horizontal]
 `events`::
-	Gets event information since startup. 
+Gets event information since startup. 
 `jvm`::
-	Gets JVM stats, including stats about garbage collection. 
+Gets JVM stats, including stats about threads. coming[5.0.0-alpha3,Adds thread count]
+`process`::
+Gets process stats, including stats about file descriptors, memory consumption, and CPU usage. coming[5.0.0-alpha3]   
 
 For example, the following request returns a JSON document that shows the number of events
 that were input, filtered, and output by Logstash since startup:
@@ -144,12 +148,11 @@ Example response:
 [source,js]
 --------------------------------------------------
 {
-    "events": {
-        "in": 91,
-        "filtered": 91,
-        "out": 91
-    }
-}
+  "events" : {
+    "in" : 59685,
+    "filtered" : 59685,
+    "out" : 59685
+  }
 --------------------------------------------------
 
 The following request returns a JSON document containing JVM stats:
@@ -163,42 +166,45 @@ Example response:
 
 [source,js]
 --------------------------------------------------
-"jvm":{  
-   "timestamp":1453233447702,
-   "uptime_in_millis":211125811,
-   "mem":{  
-      "heap_used_in_bytes":58442576,
-      "heap_used_percent":5,
-      "heap_committed_in_bytes":259522560,
-      "heap_max_in_bytes":1037959168,
-      "non_heap_used_in_bytes":56332256,
-      "non_heap_committed_in_bytes":57475072,
-      "pools":{  
-         "young":{  
-            "used_in_bytes":41672000,
-            "max_in_bytes":286326784,
-            "peak_used_in_bytes":71630848,
-            "peak_max_in_bytes":286326784
-         },
-         "survivor":{  
-            "used_in_bytes":260552,
-            "max_in_bytes":35782656,
-            "peak_used_in_bytes":8912896,
-            "peak_max_in_bytes":35782656
-         },
-         "old":{  
-            "used_in_bytes":16510024,
-            "max_in_bytes":715849728,
-            "peak_used_in_bytes":16510024,
-            "peak_max_in_bytes":715849728
-         }
-      }
-   }
+{
+  "jvm" : {
+    "threads" : {
+      "count" : 32,
+      "peak_count" : 34
+    }
+  }
+--------------------------------------------------
+
+The following request returns a JSON document containing process stats: 
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/process
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "process" : {
+    "peak_open_file_descriptors" : 64,
+    "max_file_descriptors" : 10240,
+    "open_file_descriptors" : 64,
+    "mem" : {
+      "total_virtual_in_bytes" : 5278068736
+    },
+    "cpu" : {
+      "total_in_millis" : 103290097000,
+      "percent" : 0
+    }
+  }
 --------------------------------------------------
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
 Logstash monitoring APIs.
 
+
 [[hot-threads-api]]
 === Hot Threads API
 
diff --git a/docs/static/plugin-generator.asciidoc b/docs/static/plugin-generator.asciidoc
new file mode 100644
index 00000000000..0563b466b88
--- /dev/null
+++ b/docs/static/plugin-generator.asciidoc
@@ -0,0 +1,19 @@
+[[plugin-generator]]
+== Generating Plugins
+
+You can now create your own Logstash plugin in seconds! The generate subcommand of `bin/logstash-plugin` creates the foundation 
+for a new Logstash plugin with templatized files. It creates the right directory structure, gemspec files and dependencies so you 
+can start adding custom code process data with Logstash.
+
+**Example Usage**
+
+[source,sh]
+--------------------------------------------
+bin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins
+-------------------------------------------
+
+* `--type`: Type of plugin - input, filter, output and codec
+* `--name`: Name for the new plugin
+* `--path`: Directory path where the new plugin structure will be created. If not specified, it will be '
+created in the current directory.
+
diff --git a/docs/static/plugin-manager.asciidoc b/docs/static/plugin-manager.asciidoc
index 33aee50894b..294aedcb334 100644
--- a/docs/static/plugin-manager.asciidoc
+++ b/docs/static/plugin-manager.asciidoc
@@ -57,14 +57,14 @@ bin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem
 
 [[installing-local-plugins-path]]
 [float]
-==== Advanced: Using `--pluginpath`
+==== Advanced: Using `--path.plugins`
 
-Using the `--pluginpath` flag, you can load a plugin source code located on your file system. Typically this is used by
+Using the Logstash `--path.plugins` flag, you can load a plugin source code located on your file system. Typically this is used by
 developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
 
 [source,shell]
 ----------------------------------
-bin/logstash --pluginpath /opt/shared/lib/logstash/input/my-custom-plugin-code.rb
+bin/logstash --path.plugins /opt/shared/lib/logstash/input/my-custom-plugin-code.rb
 ----------------------------------
 
 [[updating-plugins]]
@@ -111,6 +111,8 @@ bin/logstash-plugin install logstash-output-kafka
 
 Once set, plugin commands install, update can be used through this proxy.
 
+include::plugin-generator.asciidoc[]
+
 include::offline-plugins.asciidoc[]
 
 include::private-gem-repo.asciidoc[]
diff --git a/docs/static/reloading-config.asciidoc b/docs/static/reloading-config.asciidoc
index 22f3118aac6..22cc383d7dd 100644
--- a/docs/static/reloading-config.asciidoc
+++ b/docs/static/reloading-config.asciidoc
@@ -4,19 +4,19 @@
 Starting with Logstash 2.3, you can set Logstash to detect and reload configuration
 changes automatically.
 
-To enable automatic config reloading, start Logstash with the `--auto-reload` (or `-r`)
+To enable automatic config reloading, start Logstash with the `--config.reload.automatic` (or `-r`)
 command-line option specified. For example:
 
 [source,shell]
 ----------------------------------
-bin/logstash –f apache.config --auto-reload
+bin/logstash –f apache.config --config.reload.automatic
 ----------------------------------
 
-NOTE: The `--auto-reload` option is not available when you specify the `-e` flag to pass
+NOTE: The `--config.reload.automatic` option is not available when you specify the `-e` flag to pass
 in  configuration settings from the command-line.
 
 By default, Logstash checks for configuration changes every 3 seconds. To change this interval,
-use the `--reload-interval <seconds>` option,  where `seconds` specifies how often Logstash
+use the `----config.reload.interval <seconds>` option,  where `seconds` specifies how often Logstash
 checks the config files for changes. 
 
 If Logstash is already running without auto-reload enabled, you can force Logstash to
diff --git a/docs/static/stalled-shutdown.asciidoc b/docs/static/stalled-shutdown.asciidoc
new file mode 100644
index 00000000000..14fde1ee3de
--- /dev/null
+++ b/docs/static/stalled-shutdown.asciidoc
@@ -0,0 +1,63 @@
+[[stalled-shutdown]]
+=== Stalled Shutdown Detection
+
+When you attempt to shut down a running Logstash instance, Logstash performs several steps before it can safely shut down. It must:
+
+* Stop all input, filter and output plugins
+* Process all in-flight events
+* Terminate the Logstash process
+
+The following conditions affect the shutdown process:
+
+* An input plugin receiving data at a slow pace.
+* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy
+query.
+* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+
+These situations make the duration and success of the shutdown process unpredictable.
+
+Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
+This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
+worker threads.
+
+To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--pipeline.unsafe_shutdown` flag when
+you start Logstash.
+
+[[shutdown-stall-example]]
+==== Stall Detection Example
+
+In this example, slow filter execution prevents the pipeline from clean shutdown. By starting Logstash with the
+`--pipeline.unsafe_shutdown` flag, quitting with *Ctrl+C* results in an eventual shutdown that loses 20 events.
+
+========
+[source,shell]
+% bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } \
+                     output { stdout { codec => dots } }' -w 1 --pipeline.unsafe_shutdown
+Default settings used: Filter workers: 1
+Logstash startup completed
+^CSIGINT received. Shutting down the pipeline. {:level=>:warn}
+Received shutdown signal, but pipeline is still waiting for in-flight events
+to be processed. Sending another ^C will force quit Logstash, but this may cause
+data loss. {:level=>:warn}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+The shutdown process appears to be stalled due to busy or blocked plugins. Check
+    the logs for more information.
+{:level=>:error}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+Forcefully quitting logstash.. {:level=>:fatal}
+========
+
+When `--pipeline.unsafe_shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
\ No newline at end of file
diff --git a/docs/static/upgrading.asciidoc b/docs/static/upgrading.asciidoc
index d30e2797ce7..f466673f39b 100644
--- a/docs/static/upgrading.asciidoc
+++ b/docs/static/upgrading.asciidoc
@@ -17,7 +17,7 @@ This procedure uses <<package-repositories,package managers>> to upgrade Logstas
 2. Using the directions in the _Package Repositories_ section, update your repository links to point to the 2.0 repositories
 instead of the previous version.
 3. Run the `apt-get upgrade logstash` or `yum update logstash` command as appropriate for your operating system.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
 some Logstash plugins have changed in the 2.0 release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
@@ -28,7 +28,7 @@ This procedure downloads the relevant Logstash binaries directly from Elastic.
 1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
 2. Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
 3. Unpack the installation file into your Logstash directory.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
 some Logstash plugins have changed in the 2.0 release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
@@ -91,8 +91,8 @@ The default batch size of the pipeline is 125 events per worker. This will by de
 used for the elasticsearch output. The Elasticsearch output's `flush_size` now acts only as a maximum bulk
 size (still defaulting to 500). For example, if your pipeline batch size is 3000 events, Elasticsearch
 Output will send 500 events at a time, in 6 separate bulk requests. In other words, for Elasticsearch output,
-bulk request size is chunked based on `flush_size` and `--pipeline-batch-size`. If `flush_size` is set greater
-than `--pipeline-batch-size`, it is ignored and `--pipeline-batch-size` will be used.
+bulk request size is chunked based on `flush_size` and `--pipeline.batch.size`. If `flush_size` is set greater
+than `--pipeline.batch.size`, it is ignored and `--pipeline.batch.size` will be used.
 
 The default number of output workers in Logstash 2.2 is now equal to the number of pipeline workers (`-w`)
 unless overridden in the Logstash config file. This can be problematic for some users as the
diff --git a/lib/pluginmanager/generate.rb b/lib/pluginmanager/generate.rb
index ccd0009ce79..6717682e021 100644
--- a/lib/pluginmanager/generate.rb
+++ b/lib/pluginmanager/generate.rb
@@ -8,17 +8,14 @@
 
 class LogStash::PluginManager::Generate < LogStash::PluginManager::Command
 
-  TYPES = [ "input", "filter", "output" ]
-
-  option "--type", "TYPE", "Type of the plugin {input, filter, output}s" do |arg|
-    raise(ArgumentError, "should be one of: input, output or filter") unless TYPES.include?(arg)
-    arg
-  end
+  TYPES = [ "input", "filter", "output", "codec" ]
 
+  option "--type", "TYPE", "Type of the plugin {input, filter, codec, output}s", :required => true
   option "--name", "PLUGIN", "Name of the new plugin", :required => true
   option "--path", "PATH", "Location where the plugin skeleton will be created", :default => Dir.pwd
 
   def execute
+    validate_params
     source = File.join(File.dirname(__FILE__), "templates", "#{type}-plugin")
     @target_path = File.join(path, full_plugin_name)
     FileUtils.mkdir(@target_path)
@@ -35,6 +32,10 @@ def execute
 
   private
 
+  def validate_params
+    raise(ArgumentError, "should be one of: input, filter, codec or output") unless TYPES.include?(type)
+  end
+
   def create_scaffold(source, target)
     transform_r(source, target)
   end
diff --git a/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..654a05b6614
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-codec-<%= plugin_name %>
+Example codec plugin. This should help bootstrap your effort to write your own codec plugin!
diff --git a/lib/pluginmanager/templates/codec-plugin/Gemfile b/lib/pluginmanager/templates/codec-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/codec-plugin/LICENSE b/lib/pluginmanager/templates/codec-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/codec-plugin/README.md b/lib/pluginmanager/templates/codec-plugin/README.md
new file mode 100644
index 00000000000..a75e88df936
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-codec-awesome", :path => "/your/local/logstash-codec-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'codec {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-codec-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-codec-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/codec-plugin/Rakefile b/lib/pluginmanager/templates/codec-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
new file mode 100644
index 00000000000..b1a618562d1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
@@ -0,0 +1,44 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/namespace"
+
+# This <%= @plugin_name %> codec will append a string to the message field
+# of an event, either in the decoding or encoding methods
+#
+# This is only intended to be used as an example.
+#
+# input {
+#   stdin { codec => <%= @plugin_name %> }
+# }
+#
+# or
+#
+# output {
+#   stdout { codec => <%= @plugin_name %> }
+# }
+#
+class LogStash::Codecs::<%= classify(plugin_name) %> < LogStash::Codecs::Base
+
+  # The codec name
+  config_name "<%= plugin_name %>"
+
+  # Append a string to the message
+  config :append, :validate => :string, :default => ', Hello World!'
+
+  def register
+    @lines = LogStash::Codecs::Line.new
+    @lines.charset = "UTF-8"
+  end # def register
+
+  def decode(data)
+    @lines.decode(data) do |line|
+      replace = { "message" => line.get("message").to_s + @append }
+      yield LogStash::Event.new(replace)
+    end
+  end # def decode
+
+  def encode(event)
+    @on_event.call(event, event.get("message").to_s + @append + NL)
+  end # def encode
+
+end # class LogStash::Codecs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
new file mode 100644
index 00000000000..91e1b0600f1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-codec-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "codec" }
+
+  # Gem dependencies
+  s.add_runtime_dependency 'logstash-core-plugin-api', "~> <%= min_version %>"
+  s.add_runtime_dependency 'logstash-codec-line'
+  s.add_development_dependency 'logstash-devutils'
+end
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
new file mode 100644
index 00000000000..48cca741ab2
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
@@ -0,0 +1,3 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require "logstash/codecs/<%= plugin_name %>"
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
new file mode 100644
index 00000000000..dc64aba12c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
diff --git a/logstash-core-event-java/build.gradle b/logstash-core-event-java/build.gradle
index b2a4a55ec43..053b44bab84 100644
--- a/logstash-core-event-java/build.gradle
+++ b/logstash-core-event-java/build.gradle
@@ -3,9 +3,13 @@ buildscript {
         mavenLocal()
         mavenCentral()
         jcenter()
+        maven {
+            url "https://plugins.gradle.org/m2/"
+        }
     }
     dependencies {
         classpath 'net.saliman:gradle-cobertura-plugin:2.2.8'
+        classpath "gradle.plugin.me.champeau.gradle:jmh-gradle-plugin:0.3.0"
     }
 }
 
@@ -22,12 +26,15 @@ gradle.projectsEvaluated {
     }
 }
 
+
+
 apply plugin: 'java'
 apply plugin: 'idea'
+apply plugin: "me.champeau.gradle.jmh"
 
 group = 'org.logstash'
 
-project.sourceCompatibility = 1.7
+project.sourceCompatibility = 1.8
 
 task sourcesJar(type: Jar, dependsOn: classes) {
     from sourceSets.main.allSource
@@ -92,11 +99,17 @@ idea {
 }
 
 dependencies {
-    compile 'com.fasterxml.jackson.core:jackson-core:2.7.1'
-    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.1-1'
+    compile 'com.fasterxml.jackson.core:jackson-core:2.7.3'
+    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.3'
+    compile 'com.esotericsoftware:kryo:3.0.3'
+
     provided 'org.jruby:jruby-core:1.7.22'
     testCompile 'junit:junit:4.12'
     testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
+
+    runtime 'com.fasterxml.jackson.core:jackson-databind:2.7.4'
+    runtime 'org.jruby:jruby-complete:1.7.22'
+    jmh compile('org.jruby:jruby-complete:1.7.22')
 }
 
 // See http://www.gradle.org/docs/current/userguide/gradle_wrapper.html
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
index 25611753f15..9ab00671c54 100644
--- a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
+++ b/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
@@ -1,6 +1,6 @@
-#Fri Jan 22 14:29:02 EST 2016
+#Mon May 16 10:40:54 BST 2016
 distributionBase=GRADLE_USER_HOME
 distributionPath=wrapper/dists
 zipStoreBase=GRADLE_USER_HOME
 zipStorePath=wrapper/dists
-distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-bin.zip
+distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-all.zip
diff --git a/logstash-core-event-java/src/jmh/java/com/logstash/EventSerializeBenchmark.java b/logstash-core-event-java/src/jmh/java/com/logstash/EventSerializeBenchmark.java
new file mode 100644
index 00000000000..86b818305a3
--- /dev/null
+++ b/logstash-core-event-java/src/jmh/java/com/logstash/EventSerializeBenchmark.java
@@ -0,0 +1,226 @@
+package com.logstash;
+
+import org.openjdk.jmh.annotations.*;
+import org.openjdk.jmh.infra.Blackhole;
+import org.openjdk.jmh.profile.GCProfiler;
+import org.openjdk.jmh.runner.Runner;
+import org.openjdk.jmh.runner.RunnerException;
+import org.openjdk.jmh.runner.options.Options;
+import org.openjdk.jmh.runner.options.OptionsBuilder;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.security.SecureRandom;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Random;
+import java.util.concurrent.TimeUnit;
+
+@BenchmarkMode(Mode.AverageTime)
+@OutputTimeUnit(TimeUnit.MICROSECONDS)
+@Warmup(iterations = 20, time = 2, timeUnit = TimeUnit.SECONDS)
+@Measurement(iterations = 16, time = 250, timeUnit = TimeUnit.MILLISECONDS)
+@Fork(2)
+public class EventSerializeBenchmark {
+    public static void main(String[] args) throws RunnerException {
+        Options opt = new OptionsBuilder()
+                .include(EventSerializeBenchmark.class.getSimpleName())
+                .threads(4)
+                .addProfiler( GCProfiler.class )
+                .build();
+
+        new Runner(opt).run();
+    }
+
+    @Benchmark
+    public void dUniqueByteSerialize(Blackhole bh) {
+        bh.consume(UniqueThreadState.event().byteSerialize());
+    }
+
+    @Benchmark
+    public void dUniqueJSONSerialize(Blackhole bh) {
+        try {
+            bh.consume(UniqueThreadState.event().toJson());
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    @Benchmark
+    public void cTypicalByteSerialize(Blackhole bh) {
+        bh.consume(TypicalThreadState.event().byteSerialize());
+    }
+
+    @Benchmark
+    public void cTypicalJSONSerialize(Blackhole bh) {
+        try {
+            bh.consume(TypicalThreadState.event().toJson());
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    @Benchmark
+    public void bMediumByteSerialize(Blackhole bh) {
+        bh.consume(MediumThreadState.event().byteSerialize());
+    }
+
+    @Benchmark
+    public void bMediumJSONSerialize(Blackhole bh) {
+        try {
+            bh.consume(MediumThreadState.event().toJson());
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    @Benchmark
+    public void aaLargeByteSerialize(Blackhole bh) {
+        bh.consume(LargeThreadState.event().byteSerialize());
+    }
+
+    @Benchmark
+    public void aaLargeJSONSerialize(Blackhole bh) {
+        try {
+            bh.consume(LargeThreadState.event().toJson());
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    @Benchmark
+    public void aExtraLargeByteSerialize(Blackhole bh) {
+        bh.consume(ExtraLargeThreadState.event().byteSerialize());
+    }
+
+    @Benchmark
+    public void aExtraLargeJSONSerialize(Blackhole bh) {
+        try {
+            bh.consume(ExtraLargeThreadState.event().toJson());
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    @Benchmark
+    public void eCannedByteSerialize(Blackhole bh) {
+        Event e = CannedThreadState.event();
+        bh.consume(e.byteSerialize());
+    }
+
+    @Benchmark
+    public void eCannedJSONSerialize(Blackhole bh) {
+        try {
+            bh.consume(CannedThreadState.event().toJson());
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
+    public final static class Randomizer {
+        private static final SecureRandom random = new SecureRandom();
+        private static final Random rng = new Random();
+
+        public static String nextString() {
+            return nextBigInteger().toString(32);
+        }
+
+        public static String nextBigString() {
+
+            return nextString() +
+                    nextString() +
+                    nextString() +
+                    nextString() +
+                    nextString() +
+                    nextString();
+        }
+
+        public static BigInteger nextBigInteger() {
+            return new BigInteger(130, random);
+        }
+
+        public static BigDecimal nextBigDecimal() {
+            return BigDecimal.valueOf(nextDouble() / 3.0);
+        }
+
+        public static double nextDouble() {
+            return nextBigInteger().doubleValue();
+        }
+
+        public static long nextLong() {
+            return nextBigInteger().longValue();
+        }
+
+        public static int nextInt() {
+            return nextBigInteger().intValue();
+        }
+
+        public static long nextEpoch() {
+            int ago = rng.nextInt(7 * 24 * 60 * 60 * 1000);
+            return System.currentTimeMillis() - ago;
+        }
+    }
+
+    @State(Scope.Thread)
+    public static class CannedThreadState extends ThreadStateBase {
+        public static Event cached_event;
+
+        public static Event event() {
+            if (cached_event == null) {
+                Map meta = new HashMap<String, Object>();
+                meta.put(EventSerializeBenchmark.Randomizer.nextString(), EventSerializeBenchmark.Randomizer.nextString());
+                cached_event = new Event(buildRandomMap(meta));
+                decorateEventRandomly(cached_event);
+            }
+            return cached_event;
+        }
+    }
+
+    @State(Scope.Thread)
+    public static class TypicalThreadState extends ThreadStateBase {
+        public static Event event() {
+            Event event = new Event(buildTypicalMap(new HashMap<>()));
+            decorateEventTypically(event);
+            return event;
+        }
+    }
+
+    @State(Scope.Thread)
+    public static class MediumThreadState extends ThreadStateBase {
+        public static Event event() {
+            Event event = new Event(buildMediumMap(null));
+            decorateEventTypically(event);
+            return event;
+        }
+    }
+
+    @State(Scope.Thread)
+    public static class LargeThreadState extends ThreadStateBase {
+        public static Event event() {
+            Event event = new Event(buildLargeMap(null));
+            decorateEventTypically(event);
+            return event;
+        }
+    }
+
+    @State(Scope.Thread)
+    public static class ExtraLargeThreadState extends ThreadStateBase {
+        public static Event event() {
+            Event event = new Event(buildExtraLargeMap(null));
+            decorateEventTypically(event);
+            return event;
+        }
+    }
+
+    @State(Scope.Thread)
+    public static class UniqueThreadState extends ThreadStateBase {
+        public static Event event() {
+            Map meta = new HashMap<String, Object>();
+            meta.put(EventSerializeBenchmark.Randomizer.nextString(), EventSerializeBenchmark.Randomizer.nextString());
+            Event event = new Event(buildRandomMap(meta));
+            decorateEventRandomly(event);
+            return event;
+        }
+    }
+}
diff --git a/logstash-core-event-java/src/jmh/java/com/logstash/ThreadStateBase.java b/logstash-core-event-java/src/jmh/java/com/logstash/ThreadStateBase.java
new file mode 100644
index 00000000000..7827981ad55
--- /dev/null
+++ b/logstash-core-event-java/src/jmh/java/com/logstash/ThreadStateBase.java
@@ -0,0 +1,106 @@
+package com.logstash;
+
+import java.math.BigDecimal;
+import java.util.HashMap;
+import java.util.Map;
+
+public abstract class ThreadStateBase {
+    private static long execution_count = Long.valueOf(123456);
+    private static BigDecimal customer_charge = BigDecimal.valueOf(12.6667);
+    private static String customer_price_plan = "3245v6ih5v62h5v6hv667bv858vvu5683lhu358";
+
+    public static Event event(){
+        return new Event(new HashMap<String, Object>());
+    }
+
+    protected static void decorateEventRandomly(Event event) {
+        Timestamp t = new Timestamp(EventSerializeBenchmark.Randomizer.nextEpoch());
+        event.setField(ref(), EventSerializeBenchmark.Randomizer.nextLong());
+        event.setField(ref(), EventSerializeBenchmark.Randomizer.nextBigDecimal());
+        event.setField(ref(), EventSerializeBenchmark.Randomizer.nextBigInteger());
+        event.setField(mref(), EventSerializeBenchmark.Randomizer.nextString());
+        event.setField(Event.TIMESTAMP, t);
+        event.cancel();
+    }
+
+    protected static Map buildRandomMap(Map<String, Object> meta) {
+        Map data = new HashMap();
+        data.put(EventSerializeBenchmark.Randomizer.nextString(), EventSerializeBenchmark.Randomizer.nextInt());
+        data.put(EventSerializeBenchmark.Randomizer.nextString(), EventSerializeBenchmark.Randomizer.nextString());
+        data.put(EventSerializeBenchmark.Randomizer.nextString(), EventSerializeBenchmark.Randomizer.nextDouble());
+        data.put(Event.METADATA, meta);
+        return data;
+    }
+
+    protected static void decorateEventTypically(Event event) {
+        Timestamp t = new Timestamp(EventSerializeBenchmark.Randomizer.nextEpoch());
+        event.setField("execution_count", execution_count);
+        event.setField("customer_charge", customer_charge);
+        event.setField("customer_id", EventSerializeBenchmark.Randomizer.nextBigInteger());
+        event.setField("customer_price_plan", customer_price_plan);
+        event.setField(Event.TIMESTAMP, t);
+        event.cancel();
+    }
+
+    protected static Map buildTypicalMap(Map<String, Object> meta) {
+        Map data = buildSizedMap(8);
+        data.put("port", 6667);
+        data.put("execution_time", 42.24);
+        data.put("execution_unit", "ms");
+        if (meta != null) data.put(Event.METADATA, meta);
+        return data;
+    }
+    protected static Map buildMediumMap(Map<String, Object> meta) {
+        Map data = buildSizedMap(80);
+        data.put("port", 9900);
+        data.put("execution_time", 678.24);
+        data.put("execution_unit", "ms");
+        if (meta != null) data.put(Event.METADATA, meta);
+        return data;
+    }
+
+    protected static Map buildLargeMap(Map<String, Object> meta) {
+        Map data = buildSizedMap(800);
+        data.put("port", 9678);
+        data.put("execution_time", 705.24);
+        data.put("execution_unit", "ms");
+        if (meta != null) data.put(Event.METADATA, meta);
+        return data;
+    }
+
+    protected static Map buildExtraLargeMap(Map<String, Object> meta) {
+        Map data = buildSizedMap(8000);
+        data.put("port", 9909);
+        data.put("execution_time", 0.705);
+        data.put("execution_unit", "s");
+        if (meta != null) data.put(Event.METADATA, meta);
+        return data;
+    }
+
+    protected static Map buildSizedMap(int size) {
+        Map data = new HashMap();
+        if (size == 1) {
+            data.put("message", EventSerializeBenchmark.Randomizer.nextBigString());
+        } else {
+            data.put("message", largeString(size));
+        }
+        return data;
+    }
+
+    private static String ref() {
+        return "[" + EventSerializeBenchmark.Randomizer.nextString() + "]";
+    }
+
+    private static String mref() {
+        return "[@metadata][" + EventSerializeBenchmark.Randomizer.nextString() + "]";
+    }
+
+    private static String largeString(int count){
+        StringBuilder sb = new StringBuilder(26 * count);
+        String repeat = EventSerializeBenchmark.Randomizer.nextString();
+        for (int i = 0; i < count; i++) {
+            sb.append(repeat);
+        }
+        return sb.toString();
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Event.java b/logstash-core-event-java/src/main/java/com/logstash/Event.java
index d8979b41003..0133baad1c4 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Event.java
+++ b/logstash-core-event-java/src/main/java/com/logstash/Event.java
@@ -1,7 +1,10 @@
 package com.logstash;
 
+import com.esotericsoftware.kryo.Kryo;
 import com.fasterxml.jackson.databind.ObjectMapper;
 import com.logstash.ext.JrubyTimestampExtLibrary;
+import com.logstash.kyro.KryoInputOutput;
+import com.logstash.kyro.KryoInstances;
 import org.joda.time.DateTime;
 import org.jruby.RubySymbol;
 
@@ -132,7 +135,8 @@ public Object getField(String reference) {
     public void setField(String reference, Object value) {
         if (reference.equals(TIMESTAMP)) {
             // TODO(talevy): check type of timestamp
-            this.accessors.set(reference, value);
+            // can we use initTimestamp to force type?
+            this.accessors.set(reference, initTimestamp(value));
         } else if (reference.equals(METADATA_BRACKETS) || reference.equals(METADATA)) {
             this.metadata = (HashMap<String, Object>) value;
             this.metadata_accessors = new Accessors(this.metadata);
@@ -192,6 +196,49 @@ public Map toMap() {
         return Cloner.deep(this.data);
     }
 
+    public static Event byteDeserialize(byte[] bytes) {
+        Kryo kryo = null;
+        try {
+            kryo = KryoInstances.get();
+            KryoInputOutput kio = (KryoInputOutput) kryo.getContext().get("kio");
+            kio.getInput().setBuffer(bytes);
+            Map map = (Map) kryo.readClassAndObject(kio.getInput());
+            Object cancelled = map.remove("cancelled");
+            Event event = new Event(map);
+            if (Boolean.TRUE.equals(cancelled)) {
+                event.cancel();
+            }
+            return event;
+        }
+        finally {
+            if (kryo != null)
+                KryoInstances.release(kryo);
+        }
+    }
+
+    public byte[] byteSerialize() {
+        Map<String, Object> map = getData();
+        if (!map.containsKey(METADATA)) {
+            map.put(METADATA, this.metadata);
+        }
+        if (!map.containsKey(TIMESTAMP)) {
+            map.put(TIMESTAMP, this.timestamp);
+        }
+        map.put("cancelled", this.cancelled);
+        Kryo kryo = null;
+        try {
+            kryo = KryoInstances.get();
+            KryoInputOutput kio = (KryoInputOutput) kryo.getContext().get("kio");
+            kio.getOutput().clear();
+            kryo.writeClassAndObject(kio.getOutput(), map);
+            return kio.getOutput().toBytes();
+        }
+        finally {
+            if (kryo != null)
+                KryoInstances.release(kryo);
+        }
+    }
+
     public Event overwrite(Event e) {
         this.data = e.getData();
         this.accessors = e.getAccessors();
@@ -205,7 +252,6 @@ public Event overwrite(Event e) {
         return this;
     }
 
-
     public Event append(Event e) {
         Util.mapMerge(this.data, e.data);
 
@@ -293,4 +339,6 @@ public void tag(String tag) {
     public static void setLogger(Logger logger) {
         Event.logger = logger;
     }
+
+
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/SerializeProfileRunner.java b/logstash-core-event-java/src/main/java/com/logstash/SerializeProfileRunner.java
new file mode 100644
index 00000000000..fb5bf1244b4
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/SerializeProfileRunner.java
@@ -0,0 +1,121 @@
+package com.logstash;
+
+import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.util.Map;
+import java.util.HashMap;
+import java.security.SecureRandom;
+import java.util.Random;
+
+public class SerializeProfileRunner {
+    public static class ProExec {
+
+        public Event cached_event;
+
+        public Event eventBuilder() {
+            String str1 = Randomizer.nextString();
+            String str2 = Randomizer.nextString();
+            long num = Randomizer.nextLong();
+            BigDecimal bd = Randomizer.nextBigDecimal();
+            BigInteger bi = Randomizer.nextBigInteger();
+            Timestamp t = new Timestamp(Randomizer.nextEpoch());
+
+            Map data = new HashMap();
+            data.put(Randomizer.nextString(), Randomizer.nextInt());
+            data.put(Randomizer.nextString(), Randomizer.nextString());
+            data.put(Randomizer.nextString(), Randomizer.nextDouble());
+
+            Map meta = new HashMap();
+            meta.put(Randomizer.nextString(), str1);
+
+            data.put(Event.METADATA, meta);
+            Event event = new Event(data);
+
+            event.setField(ref(), num);
+            event.setField(ref(), bd);
+            event.setField(ref(), bi);
+            event.setField(mref(), str2);
+            event.setField(Event.TIMESTAMP, t);
+            event.cancel();
+            return event;
+        }
+
+        public byte[] measureByteSerialize() {
+//            return event().byteSerialize();
+            return cached_event.byteSerialize();
+        }
+
+        public String measureJSONSerialize() {
+            String result;
+            try {
+                result = cached_event.toJson();
+            } catch (IOException e) {
+                System.err.println(e.getMessage());
+                result = e.getMessage();
+            }
+            return result;
+        }
+
+        public void doSetup() {
+            cached_event = eventBuilder();
+            System.out.println("Do Setup");
+        }
+
+        public void doTearDown() {
+            System.out.println("Do TearDown");
+        }
+
+        private String ref() {
+            return "[" + Randomizer.nextString() + "]";
+        }
+
+        private String mref() {
+            return "[@metadata][" + Randomizer.nextString() + "]";
+        }
+    }
+
+    public final static class Randomizer {
+        private static SecureRandom random = new SecureRandom();
+        private static final Random rng = new Random();
+
+        public static String nextString() {
+            return nextBigInteger().toString(32);
+        }
+        public static BigInteger nextBigInteger() {
+            return new BigInteger(130, random);
+        }
+        public static BigDecimal nextBigDecimal() {
+            return BigDecimal.valueOf(nextDouble() / 3.0);
+        }
+
+        public static double nextDouble() {
+            return nextBigInteger().doubleValue();
+        }
+
+        public static long nextLong() {
+            return nextBigInteger().longValue();
+        }
+
+        public static int nextInt() {
+            return nextBigInteger().intValue();
+        }
+        public static long nextEpoch() {
+            int ago = rng.nextInt(7 * 24 * 60 * 60 * 1000);
+            return System.currentTimeMillis() - ago;
+        }
+    }
+
+    public static void main(String[] args) {
+//        int max = 2;
+        int max = 200000000;
+        ProExec pe = new ProExec();
+        pe.doSetup();
+
+        for (int i = 0; i < max; i++) {
+//            System.out.println(pe.measureJSONSerialize());
+//            pe.measureJSONSerialize();
+            pe.measureByteSerialize();
+        }
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java b/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
index 434dc93a13c..c619cf9cb5f 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
+++ b/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
@@ -1,6 +1,7 @@
 package com.logstash;
 
 import com.fasterxml.jackson.databind.annotation.JsonSerialize;
+import com.logstash.json.TimestampSerializer;
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 import org.joda.time.LocalDateTime;
@@ -74,6 +75,11 @@ public long usec() {
         // note that getMillis() return millis since epoch
         return (new Duration(JAN_1_1970.toDateTime(DateTimeZone.UTC), this.time).getMillis() % 1000) * 1000;
     }
+    public long msec() {
+        // used for serialization to bytes
+        // note that getMillis() return millis since epoch
+        return new Duration(JAN_1_1970.toDateTime(DateTimeZone.UTC), this.time).getMillis();
+    }
 
     @Override
     public Timestamp clone() throws CloneNotSupportedException {
diff --git a/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java b/logstash-core-event-java/src/main/java/com/logstash/json/TimestampSerializer.java
similarity index 88%
rename from logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
rename to logstash-core-event-java/src/main/java/com/logstash/json/TimestampSerializer.java
index c90afdd9227..0f95a405835 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
+++ b/logstash-core-event-java/src/main/java/com/logstash/json/TimestampSerializer.java
@@ -1,8 +1,9 @@
-package com.logstash;
+package com.logstash.json;
 
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.databind.JsonSerializer;
 import com.fasterxml.jackson.databind.SerializerProvider;
+import com.logstash.Timestamp;
 
 import java.io.IOException;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/kyro/KryoInputOutput.java b/logstash-core-event-java/src/main/java/com/logstash/kyro/KryoInputOutput.java
new file mode 100644
index 00000000000..60cfde51168
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/kyro/KryoInputOutput.java
@@ -0,0 +1,22 @@
+package com.logstash.kyro;
+
+import com.esotericsoftware.kryo.io.Input;
+import com.esotericsoftware.kryo.io.Output;
+
+public class KryoInputOutput {
+    private Input input;
+    private Output output;
+
+    public KryoInputOutput(Input input, Output output) {
+        this.input = input;
+        this.output = output;
+    }
+
+    public Input getInput() {
+        return input;
+    }
+
+    public Output getOutput() {
+        return output;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/kyro/KryoInstances.java b/logstash-core-event-java/src/main/java/com/logstash/kyro/KryoInstances.java
new file mode 100644
index 00000000000..267fd228af7
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/kyro/KryoInstances.java
@@ -0,0 +1,37 @@
+package com.logstash.kyro;
+
+import com.esotericsoftware.kryo.Kryo;
+import com.esotericsoftware.kryo.io.ByteBufferInput;
+import com.esotericsoftware.kryo.io.ByteBufferOutput;
+import com.esotericsoftware.kryo.pool.KryoFactory;
+import com.esotericsoftware.kryo.pool.KryoPool;
+import com.esotericsoftware.kryo.serializers.MapSerializer;
+import com.logstash.Timestamp;
+
+import java.util.HashMap;
+
+public class KryoInstances {
+
+    private static KryoFactory factory = () -> {
+        Kryo kryo = new Kryo();
+        // add customisation here
+        kryo.register(Timestamp.class, new TimestampSerializer(), 201);
+        MapSerializer serializer = new MapSerializer();
+        serializer.setKeyClass(String.class, kryo.getSerializer(String.class));
+        kryo.register(HashMap.class, serializer, 202);
+        KryoInputOutput kio = new KryoInputOutput(new ByteBufferInput(), new ByteBufferOutput(512, -1));
+        kryo.getContext().put("kio", kio);
+        return kryo;
+    };
+
+    private static KryoPool pool = new KryoPool.Builder(factory).softReferences().build();
+
+    public static Kryo get() {
+        return pool.borrow();
+    }
+
+    public static void release(Kryo instance) {
+        pool.release(instance);
+    }
+}
+
diff --git a/logstash-core-event-java/src/main/java/com/logstash/kyro/TimestampSerializer.java b/logstash-core-event-java/src/main/java/com/logstash/kyro/TimestampSerializer.java
new file mode 100644
index 00000000000..2699a667f22
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/com/logstash/kyro/TimestampSerializer.java
@@ -0,0 +1,18 @@
+package com.logstash.kyro;
+
+import com.esotericsoftware.kryo.Kryo;
+import com.esotericsoftware.kryo.Serializer;
+import com.esotericsoftware.kryo.io.Input;
+import com.esotericsoftware.kryo.io.Output;
+import com.logstash.Timestamp;
+
+public class TimestampSerializer extends Serializer<Timestamp> {
+
+    public void write (Kryo kryo, Output output, Timestamp t) {
+        output.writeLong(t.msec());
+    }
+
+    public Timestamp read (Kryo kryo, Input input, Class<Timestamp> type) {
+        return new Timestamp(input.readLong());
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/EventTest.java b/logstash-core-event-java/src/test/java/com/logstash/EventTest.java
index 61bb4bb7a58..f88e914b763 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/EventTest.java
+++ b/logstash-core-event-java/src/test/java/com/logstash/EventTest.java
@@ -3,6 +3,8 @@
 import org.junit.Test;
 
 import java.io.IOException;
+import java.math.BigDecimal;
+import java.math.BigInteger;
 import java.util.*;
 
 import static org.junit.Assert.*;
@@ -196,4 +198,48 @@ public void testFromJsonWithInvalidJsonArray2() throws Exception {
     public void testFromJsonWithPartialInvalidJsonArray() throws Exception {
         Event.fromJson("[{\"foo\":\"bar\"}, 1]");
     }
+
+    @Test
+    public void testByteSerializeRoundTrip() throws Exception {
+        String pangramDe = "Victor jagt zwölf Boxkämpfer quer über den großen Sylter Deich";
+        String pangramEn = "Jived fox nymph grabs quick waltz";
+        long num = 87654321;
+        BigDecimal bd = BigDecimal.valueOf(123456789.99);
+        BigInteger bi = BigInteger.valueOf(num);
+        Timestamp t = new Timestamp("2014-09-23T08:00:00.000Z");
+
+        Map data = new HashMap();
+        data.put("a", 1);
+        data.put("b", "bar");
+        data.put("c", 1.0);
+
+        Map meta = new HashMap();
+        meta.put("g", pangramDe);
+
+        data.put(Event.METADATA, meta);
+        Event e = new Event(data);
+
+        e.setField("[d]", num);
+        e.setField("[e]", bd);
+        e.setField("[f]", bi);
+        e.setField("[@metadata][h]", pangramEn);
+        e.setField(Event.TIMESTAMP, t);
+        e.cancel();
+
+        byte[] oneWay = e.byteSerialize();
+        Event returnTrip = Event.byteDeserialize(oneWay);
+
+        assertEquals(272, oneWay.length);
+        assertNotEquals(t, returnTrip.getTimestamp());
+        assertEquals(t.toIso8601(), returnTrip.getTimestamp().toIso8601());
+        assertEquals(1, returnTrip.getField("[a]"));
+        assertEquals("bar", returnTrip.getField("[b]"));
+        assertEquals(1.0, returnTrip.getField("[c]"));
+        assertEquals(num, returnTrip.getField("[d]"));
+        assertEquals(bd, returnTrip.getField("[e]"));
+        assertEquals(bi, returnTrip.getField("[f]"));
+        assertEquals(pangramDe, returnTrip.getField("[@metadata][g]"));
+        assertEquals(pangramEn, returnTrip.getField("[@metadata][h]"));
+        assertTrue(returnTrip.isCancelled());
+    }
 }
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 0e356ec76b3..8bb3649e9ca 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -38,6 +38,7 @@ def initialize(settings = LogStash::SETTINGS)
     @node_name = setting("node.name")
     @http_host = setting("http.host")
     @http_port = setting("http.port")
+    @http_environment = setting("http.environment")
 
     @config_loader = LogStash::Config::Loader.new(@logger)
     @reload_interval = setting("config.reload.interval")
@@ -129,7 +130,7 @@ def running_pipelines?
 
   private
   def start_webserver
-    options = {:http_host => @http_host, :http_port => @http_port }
+    options = {:http_host => @http_host, :http_port => @http_port, :http_environment => @http_environment }
     @webserver = LogStash::WebServer.new(@logger, options)
     Thread.new(@webserver) do |webserver|
       LogStash::Util.set_thread_name("Api Webserver")
diff --git a/logstash-core/lib/logstash/api/modules/base.rb b/logstash-core/lib/logstash/api/modules/base.rb
index 4f9855f6e50..7a750d02f32 100644
--- a/logstash-core/lib/logstash/api/modules/base.rb
+++ b/logstash-core/lib/logstash/api/modules/base.rb
@@ -7,15 +7,16 @@ module Modules
       class Base < ::Sinatra::Base
         helpers AppHelpers
 
+        # These options never change
+        # Sinatra isn't good at letting you change internal settings at runtime
+        # which is a requirement. We always propagate errors up and catch them
+        # in a custom rack handler in the RackApp class
         set :environment, :production
+        set :raise_errors, true
+        set :show_exceptions, false
 
         attr_reader :factory
 
-        if settings.environment != :production
-          set :raise_errors, true
-          set :show_exceptions, :after_handler
-        end
-
         include LogStash::Util::Loggable
 
         helpers AppHelpers
@@ -31,12 +32,6 @@ def initialize(app=nil)
           text = as == :string ? "" : {}
           respond_with(text, :as => as)
         end
-
-        error do
-          e = env['sinatra.error']
-          logger.error(e.message, :url => request.url, :ip => request.ip, :params => request.params, :class => e.class.name, :backtrace => e.backtrace)
-        end
-
       end
     end
   end
diff --git a/logstash-core/lib/logstash/api/modules/node.rb b/logstash-core/lib/logstash/api/modules/node.rb
index 931b2152c38..38ae44f7b7c 100644
--- a/logstash-core/lib/logstash/api/modules/node.rb
+++ b/logstash-core/lib/logstash/api/modules/node.rb
@@ -1,4 +1,6 @@
 # encoding: utf-8
+require "logstash/api/modules/base"
+
 module LogStash
   module Api
     module Modules
diff --git a/logstash-core/lib/logstash/api/rack_app.rb b/logstash-core/lib/logstash/api/rack_app.rb
index 5624d1c3516..861e26d0697 100644
--- a/logstash-core/lib/logstash/api/rack_app.rb
+++ b/logstash-core/lib/logstash/api/rack_app.rb
@@ -1,3 +1,5 @@
+require "sinatra"
+require "rack"
 require "logstash/api/modules/base"
 require "logstash/api/modules/node"
 require "logstash/api/modules/node_stats"
@@ -8,9 +10,83 @@
 module LogStash
   module Api
     module RackApp
-      def self.app
+      # Cabin is not compatible with CommonLogger, and this gives us more control anyway
+      METADATA_FIELDS = [:request_method, :path_info, :query_string, :http_version, :http_accept].freeze
+      def self.log_metadata(status, env)
+        METADATA_FIELDS.reduce({:status => status}) do |acc, field|
+          acc[field] = env[field.to_s.upcase]
+          acc
+        end        
+      end
+      
+      class ApiLogger
+        LOG_MESSAGE = "API HTTP Request".freeze
+        
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          res = @app.call(env)
+          status, headers, body = res
+          
+          if fatal_error?(status)
+            @logger.warn? && @logger.warn(LOG_MESSAGE, RackApp.log_metadata(status, env))                      
+          else          
+            @logger.info? && @logger.info(LOG_MESSAGE, RackApp.log_metadata(status, env))                      
+          end
+
+          res          
+        end
+
+        def fatal_error?(status)
+          status >= 500 && status < 600
+        end
+      end
+      
+      class ApiErrorHandler
+        LOG_MESSAGE = "Internal API server error".freeze
+        
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          @app.call(env)
+        rescue => e
+          body = RackApp.log_metadata(500, env).
+                   merge({
+                           :error => "Unexpected Internal Error",
+                           :class => e.class.name,
+                           :message => e.message,
+                           :backtrace => e.backtrace
+                         })
+
+          @logger.error(LOG_MESSAGE, body)
+          
+          [500,
+           {'Content-Type' => 'application/json'},
+           [LogStash::Json.dump(body)]
+          ]
+        end
+      end
+      
+      def self.app(logger, environment)
         namespaces = rack_namespaces 
         Rack::Builder.new do
+          # Custom logger object. Rack CommonLogger does not work with cabin
+          use ApiLogger, logger
+          
+          # In test env we want errors to propogate up the chain
+          # so we get easy to understand test failures.
+          # In production / dev we don't want a bad API endpoint
+          # to crash the process
+          if environment != "test"
+            use ApiErrorHandler, logger
+          end
+          
           run LogStash::Api::Modules::Root
           namespaces.each_pair do |namespace, app|
             map(namespace) do
diff --git a/logstash-core/lib/logstash/api/service.rb b/logstash-core/lib/logstash/api/service.rb
index 799e802b6f9..3eaeb2535ef 100644
--- a/logstash-core/lib/logstash/api/service.rb
+++ b/logstash-core/lib/logstash/api/service.rb
@@ -26,7 +26,7 @@ def agent
       end
 
       def started?
-        !@snapshot.nil? && has_counters?
+        !@snapshot.nil? && has_counters?        
       end
 
       def update(snapshot)
diff --git a/logstash-core/lib/logstash/config/config_ast.rb b/logstash-core/lib/logstash/config/config_ast.rb
index b235dded4ed..6963fc7d31e 100644
--- a/logstash-core/lib/logstash/config/config_ast.rb
+++ b/logstash-core/lib/logstash/config/config_ast.rb
@@ -76,6 +76,14 @@ def self.defered_conditionals_index=(val)
     @defered_conditionals_index = val
   end
 
+  def self.plugin_instance_index
+    @plugin_instance_index
+  end
+
+  def self.plugin_instance_index=(val)
+    @plugin_instance_index = val
+  end
+
   class Node < Treetop::Runtime::SyntaxNode
     def text_value_for_comments
       text_value.gsub(/[\r\n]/, " ")
@@ -86,6 +94,7 @@ class Config < Node
     def compile
       LogStash::Config::AST.defered_conditionals = []
       LogStash::Config::AST.defered_conditionals_index = 0
+      LogStash::Config::AST.plugin_instance_index = 0
       code = []
 
       code << <<-CODE
@@ -140,7 +149,6 @@ class PluginSection < Node
     # like @filter_<name>_1
     def initialize(*args)
       super(*args)
-      @i = 0
     end
 
     # Generate ruby code to initialize all the plugins.
@@ -196,9 +204,9 @@ def generate_variables
 
       plugins.each do |plugin|
         # Unique number for every plugin.
-        @i += 1
+        LogStash::Config::AST.plugin_instance_index += 1
         # store things as ivars, like @filter_grok_3
-        var = :"#{plugin.plugin_type}_#{plugin.plugin_name}_#{@i}"
+        var = :"#{plugin.plugin_type}_#{plugin.plugin_name}_#{LogStash::Config::AST.plugin_instance_index}"
         # puts("var=#{var.inspect}")
         @variables[plugin] = var
       end
diff --git a/logstash-core/lib/logstash/config/mixin.rb b/logstash-core/lib/logstash/config/mixin.rb
index d5a1637f354..6929b6eedc9 100644
--- a/logstash-core/lib/logstash/config/mixin.rb
+++ b/logstash-core/lib/logstash/config/mixin.rb
@@ -4,6 +4,7 @@
 require "logstash/plugins/registry"
 require "logstash/logging"
 require "logstash/util/password"
+require "logstash/util/safe_uri"
 require "logstash/version"
 require "logstash/environment"
 require "logstash/util/plugin_version"
@@ -513,6 +514,12 @@ def validate_value(value, validator)
             end
 
             result = value.first.is_a?(::LogStash::Util::Password) ? value.first : ::LogStash::Util::Password.new(value.first)
+          when :uri
+            if value.size > 1
+              return false, "Expected uri (one value), got #{value.size} values?"
+            end
+            
+            result = value.first.is_a?(::LogStash::Util::SafeURI) ? value.first : ::LogStash::Util::SafeURI.new(value.first)
           when :path
             if value.size > 1 # Only 1 value wanted
               return false, "Expected path (one value), got #{value.size} values?"
@@ -551,6 +558,10 @@ def secure_params!(params)
         if @config[key][:validate] == :password && !value.is_a?(::LogStash::Util::Password)
           params[key] = ::LogStash::Util::Password.new(value)
         end
+
+        if @config[key][:validate] == :uri && !value.is_a?(::LogStash::Util::SafeURI)
+          params[key] = ::LogStash::Util::SafeURI.new(value)
+        end
       end
     end
 
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 6cfea6868be..9f1777498fd 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -29,6 +29,7 @@ module LogStash
             Setting::String.new("log.format", "plain", true, ["json", "plain"]),
             Setting::String.new("http.host", "127.0.0.1"),
               Setting::Port.new("http.port", 9600),
+            Setting::String.new("http.environment", "production"),
   ].each {|setting| SETTINGS.register(setting) }
 
   module Environment
diff --git a/logstash-core/lib/logstash/util/safe_uri.rb b/logstash-core/lib/logstash/util/safe_uri.rb
new file mode 100644
index 00000000000..9d24386e63b
--- /dev/null
+++ b/logstash-core/lib/logstash/util/safe_uri.rb
@@ -0,0 +1,44 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+
+# This class exists to quietly wrap a password string so that, when printed or
+# logged, you don't accidentally print the password itself.
+class LogStash::Util::SafeURI
+  PASS_PLACEHOLDER = "xxxxxx".freeze
+  
+  extend Forwardable
+  
+  def_delegators :@uri, :coerce, :query=, :route_from, :port=, :default_port, :select, :normalize!, :absolute?, :registry=, :path, :password, :hostname, :merge, :normalize, :host, :component_ary, :userinfo=, :query, :set_opaque, :+, :merge!, :-, :password=, :parser, :port, :set_host, :set_path, :opaque=, :scheme, :fragment=, :set_query, :set_fragment, :userinfo, :hostname=, :set_port, :path=, :registry, :opaque, :route_to, :set_password, :hierarchical?, :set_user, :set_registry, :set_userinfo, :fragment, :component, :user=, :set_scheme, :absolute, :host=, :relative?, :scheme=, :user
+  
+  attr_reader :uri
+  
+  public
+  def initialize(arg)    
+    @uri = case arg
+           when String
+             URI.parse(arg)
+           when URI
+             arg
+           else
+             raise ArgumentError, "Expected a string or URI, got a #{arg.class} creating a URL"
+           end
+  end
+
+  def to_s
+    sanitized.to_s
+  end
+
+  def inspect
+    sanitized.to_s
+  end
+
+  def sanitized
+    return uri unless uri.password # nothing to sanitize here!
+    
+    safe = uri.clone
+    safe.password = PASS_PLACEHOLDER
+    safe
+  end
+end
+
diff --git a/logstash-core/lib/logstash/webserver.rb b/logstash-core/lib/logstash/webserver.rb
index 9fbd9718b32..6cf8366e3ba 100644
--- a/logstash-core/lib/logstash/webserver.rb
+++ b/logstash-core/lib/logstash/webserver.rb
@@ -1,25 +1,25 @@
 # encoding: utf-8
 require "puma"
 require "puma/server"
-require "sinatra"
-require "rack"
 require "logstash/api/rack_app"
 
 module LogStash 
   class WebServer
     extend Forwardable
 
-    attr_reader :logger, :status, :config, :options, :cli_options, :runner, :binder, :events, :http_host, :http_port
+    attr_reader :logger, :status, :config, :options, :cli_options, :runner, :binder, :events, :http_host, :http_port, :http_environment
 
     def_delegator :@runner, :stats
 
     DEFAULT_HOST = "127.0.0.1".freeze
     DEFAULT_PORT = 9600.freeze
+    DEFAULT_ENVIRONMENT = 'production'.freeze
 
     def initialize(logger, options={})
       @logger      = logger
       @http_host    = options[:http_host] || DEFAULT_HOST
       @http_port    = options[:http_port] || DEFAULT_PORT
+      @http_environment = options[:http_environment] || DEFAULT_ENVIRONMENT
       @options     = {}
       @cli_options = options.merge({ :rackup => ::File.join(::File.dirname(__FILE__), "api", "init.ru"),
                                      :binds => ["tcp://#{http_host}:#{http_port}"],
@@ -37,7 +37,8 @@ def run
 
       stop # Just in case
 
-      @server = ::Puma::Server.new(LogStash::Api::RackApp.app)
+      app = LogStash::Api::RackApp.app(logger, http_environment)
+      @server = ::Puma::Server.new(app)
       @server.add_tcp_listener(http_host, http_port)
 
       @server.run.join
diff --git a/logstash-core/spec/api/lib/rack_app_spec.rb b/logstash-core/spec/api/lib/rack_app_spec.rb
new file mode 100644
index 00000000000..0546df9fbf6
--- /dev/null
+++ b/logstash-core/spec/api/lib/rack_app_spec.rb
@@ -0,0 +1,88 @@
+require "logstash/api/rack_app"
+require "rack/test"
+
+describe LogStash::Api::RackApp do
+  include Rack::Test::Methods
+
+  class DummyApp
+    class RaisedError < StandardError; end
+    
+    def call(env)
+      case env["PATH_INFO"]
+      when "/good-page"
+        [200, {}, ["200 OK"]]
+      when "/service-unavailable"
+        [503, {}, ["503 service unavailable"]]
+      when "/raise-error"
+        raise RaisedError, "Error raised"
+      else
+        [404, {}, ["404 Page not found"]]
+      end
+    end
+  end
+
+  let(:logger) { Cabin::Channel.get }
+
+  describe LogStash::Api::RackApp::ApiErrorHandler do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+
+    it "should let good requests through as normal" do
+      get "/good-page"
+      expect(last_response).to be_ok
+    end
+
+    it "should let through 5xx codes" do
+      get "/service-unavailable"
+      expect(last_response.status).to eql(503)
+    end
+
+    describe "raised exceptions" do
+      before do
+        allow(logger).to receive(:error).with(any_args)
+        get "/raise-error"
+      end
+      
+      it "should return a 500 error" do
+        expect(last_response.status).to eql(500)
+      end
+
+      it "should return valid JSON" do
+        expect { LogStash::Json.load(last_response.body) }.not_to raise_error
+      end
+
+      it "should log the error" do
+        expect(logger).to have_received(:error).with(LogStash::Api::RackApp::ApiErrorHandler::LOG_MESSAGE, anything).once
+      end
+    end
+  end
+
+  describe LogStash::Api::RackApp::ApiLogger do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+    
+    it "should log good requests as info" do
+      expect(logger).to receive(:info).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/good-page"
+    end
+
+    it "should log 5xx requests as warnings" do
+      expect(logger).to receive(:warn).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/service-unavailable"
+    end
+  end
+end
diff --git a/logstash-core/spec/api/spec_helper.rb b/logstash-core/spec/api/spec_helper.rb
index befead55d1e..041311b9314 100644
--- a/logstash-core/spec/api/spec_helper.rb
+++ b/logstash-core/spec/api/spec_helper.rb
@@ -46,6 +46,7 @@ def initialize
       "log.level" => "debug",
       "node.name" => "test_agent",
       "http.port" => rand(9600..9700),
+      "http.environment" => "test",      
       "config.string" => @config_str,
       "pipeline.batch.size" => 1,
       "pipeline.workers" => 1
@@ -60,7 +61,8 @@ def start
     @runner = Thread.new(agent) do |_agent|
       _agent.execute
     end
-    wait_until_snapshot_received
+
+    wait_until_ready
   end
 
   def stop
@@ -71,8 +73,9 @@ def stop
 
   private
 
-  def wait_until_snapshot_received
-    while !LogStash::Api::Service.instance.started? do
+  def wait_until_ready
+    # Wait until the service and pipeline have started
+    while !(LogStash::Api::Service.instance.started? && agent.pipelines["main"].running?) do
       sleep 0.5
     end
   end
diff --git a/logstash-core/spec/logstash/config/config_ast_spec.rb b/logstash-core/spec/logstash/config/config_ast_spec.rb
index fcf989fcd0c..657b00523c4 100644
--- a/logstash-core/spec/logstash/config/config_ast_spec.rb
+++ b/logstash-core/spec/logstash/config/config_ast_spec.rb
@@ -144,6 +144,49 @@
     end
   end
 
+  context "when using two plugin sections of the same type" do
+    let(:pipeline_klass) do
+      Class.new do
+        def initialize(config)
+          grammar = LogStashConfigParser.new
+          @config = grammar.parse(config)
+          @code = @config.compile
+          eval(@code)
+        end
+        def plugin(*args);end
+      end
+    end
+    context "(filters)" do
+      let(:config_string) {
+        "input { generator { } }
+         filter { filter1 { } }
+         filter { filter1 { } }
+         output { output1 { } }"
+      }
+
+
+      it "should create a pipeline with both sections" do
+        generated_objects = pipeline_klass.new(config_string).instance_variable_get("@generated_objects")
+        filters = generated_objects.keys.map(&:to_s).select {|obj_name| obj_name.match(/^filter.+?_\d+$/) }
+        expect(filters.size).to eq(2)
+      end
+    end
+
+    context "(filters)" do
+      let(:config_string) {
+        "input { generator { } }
+         output { output1 { } }
+         output { output1 { } }"
+      }
+
+
+      it "should create a pipeline with both sections" do
+        generated_objects = pipeline_klass.new(config_string).instance_variable_get("@generated_objects")
+        outputs = generated_objects.keys.map(&:to_s).select {|obj_name| obj_name.match(/^output.+?_\d+$/) }
+        expect(outputs.size).to eq(2)
+      end
+    end
+  end
   context "when creating two instances of the same configuration" do
 
     let(:config_string) {
diff --git a/logstash-core/spec/logstash/config/mixin_spec.rb b/logstash-core/spec/logstash/config/mixin_spec.rb
index ca20d3649eb..5c18b6e88d1 100644
--- a/logstash-core/spec/logstash/config/mixin_spec.rb
+++ b/logstash-core/spec/logstash/config/mixin_spec.rb
@@ -102,6 +102,76 @@
     end
   end
 
+  context "when validating :uri" do
+    let(:klass) do
+      Class.new(LogStash::Filters::Base)  do
+        config_name "fakeuri"
+        config :uri, :validate => :uri
+      end
+    end
+
+    shared_examples("safe URI") do            
+      subject { klass.new("uri" => uri_str) }
+
+      it "should be a SafeURI object" do
+        expect(subject.uri).to(be_a(LogStash::Util::SafeURI))
+      end
+
+      it "should make password values hidden with #to_s" do
+        expect(subject.uri.to_s).to eql(uri_hidden)
+      end
+
+      it "should make password values hidden with #inspect" do
+        expect(subject.uri.inspect).to eql(uri_hidden)
+      end
+
+      it "should correctly copy URI types" do
+        clone = subject.class.new(subject.params)
+        expect(clone.uri.to_s).to eql(uri_hidden)
+      end
+
+      it "should make the real URI object availale under #uri" do
+        expect(subject.uri.uri).to be_a(::URI)
+      end
+
+      it "should obfuscate original_params" do
+        expect(subject.original_params['uri']).to(be_a(LogStash::Util::SafeURI))
+      end
+
+      context "attributes" do
+        [:scheme, :user, :password, :hostname, :path].each do |attr|
+          it "should make #{attr} available" do
+            expect(subject.uri.send(attr)).to eql(self.send(attr))
+          end
+        end
+      end
+    end
+
+    context "with a username / password" do
+      let(:scheme) { "myscheme" }
+      let(:user) { "myuser" }
+      let(:password) { "fancypants" }
+      let(:hostname) { "myhostname" }
+      let(:path) { "/my/path" }
+      let(:uri_str) { "#{scheme}://#{user}:#{password}@#{hostname}#{path}" }
+      let(:uri_hidden) { "#{scheme}://#{user}:#{LogStash::Util::SafeURI::PASS_PLACEHOLDER}@#{hostname}#{path}" }
+
+      include_examples("safe URI")
+    end
+
+    context "without a username / password" do
+      let(:scheme) { "myscheme" }
+      let(:user) { nil }
+      let(:password) { nil }
+      let(:hostname) { "myhostname" }
+      let(:path) { "/my/path" }
+      let(:uri_str) { "#{scheme}://#{hostname}#{path}" }
+      let(:uri_hidden) { "#{scheme}://#{hostname}#{path}" }
+
+      include_examples("safe URI")
+    end
+  end
+
   describe "obsolete settings" do
     let(:plugin_class) do
       Class.new(LogStash::Inputs::Base) do
diff --git a/qa/Gemfile b/qa/Gemfile
new file mode 100644
index 00000000000..1919a50f272
--- /dev/null
+++ b/qa/Gemfile
@@ -0,0 +1,5 @@
+source "https://rubygems.org"
+gem "runner-tool", :git => "https://github.com/purbon/runner-tool.git"
+gem "rspec", "~> 3.1.0"
+gem "rake"
+gem "pry", :group => :test
diff --git a/qa/README.md b/qa/README.md
new file mode 100644
index 00000000000..3d4b152d4ee
--- /dev/null
+++ b/qa/README.md
@@ -0,0 +1,198 @@
+## Acceptance test Framework
+
+Welcome to the acceptance test framework for logstash, in this small
+README we're going to describe it's features and the necessary steps you will need to
+follow to setup your environment.
+
+### Setup your environment
+
+In summary this test framework is composed of:
+
+* A collection of rspec helpers and matchers that make creating tests
+  easy.
+* This rspecs helpers execute commands over SSH to a set of machines.
+* The tests are run, for now, as vagrant (virtualbox provided) machines.
+
+As of this, you need to have installed:
+
+* The latest version vagrant (=> 1.8.1)
+* Virtualbox as VM provider (=> 5.0)
+
+Is important to notice that the first time you set everything up, or when a
+new VM is added, there is the need to download the box (this will
+take a while depending on your internet speed).
+
+### Running Tests
+
+It is possible to run the full suite of the acceptance test with the codebase by 
+running the command `ci/ci_acceptance.sh`, this command will generate the artifacts, bootstrap
+the VM and run the tests.
+
+
+This test are based on a collection of Vagrant defined VM's where the
+different test are going to be executed, so first setup necessary is to
+have vagrant properly available, see https://www.vagrantup.com/ for
+details on how to install it.
+
+_Inside the `qa` directory_
+
+First of all execute the command `bundle` this will pull the necessary
+dependencies in your environment, after this is done, this is the collection of task available for you:
+
+```
+skywalker% rake -T
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+rake qa:vm:halt[platform]           # Halt all VM's involved in the acceptance test round
+rake qa:vm:setup[platform]          # Bootstrap all the VM's used for this tests
+rake qa:vm:ssh_config               # Generate a valid ssh-config
+```
+
+Important to be aware that using any of this commands:
+
+```
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+```
+
+will bootstrap all selected machines. If you're willing to run on single
+platform you should use
+
+```
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+```
+
+this will not do any bootstrap, so you are required to previously
+boostrap the VM yourself by doing `vagrant up`. This is like this
+because this command is only here for developers, not for automated
+CI's.
+
+
+### How to run tests
+
+In this framework we're using ssh to connect to a collection of Vagrant
+machines, so first and most important is to generate a valid ssh config
+file, this could be done running `rake qa:vm:ssh_config`. When this task
+is finished a file named `.vm_ssh_config` will be generated with all the
+necessary information to connect with the different machines.
+
+Now is time to run your test and to do that we have different options:
+
+* rake qa:acceptance:all              # Run all acceptance
+* rake qa:acceptance:debian           # Run acceptance test in debian machines
+* rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+* rake qa:acceptance:suse             # Run acceptance test in suse machines
+* rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+
+Generally speaking this are complex tests so they take a long time to
+finish completly, if you look for faster feedback see at the end of this
+README how to run fewer tests.
+
+## Architecture of the Framework
+
+If you wanna know more about how this framework works, here is your
+section of information.
+
+### Directory structure
+
+* ```acceptance/``` here it goes all the specs definitions.
+* ```config```  inside you can find all config files, for now only the
+  platform definition.
+* ```rspec``` here stay all framework parts necessary to get the test
+  running, you will find the commands, the rspec matchers and a
+collection of useful helpers for your test.
+* ```sys``` a collection of bash scripts used to bootstrap the machines.
+* ```vagrant``` classes and modules used to help us running vagrant.
+
+### The platform configuration file
+
+Located inside the config directory there is the platforms.json which is used to define the different platforms we test with.
+Important bits here are:
+
+* `latest` key defines the latest published version of LS release which is used to test the package upgrade scenario.
+* inside the `platforms` key you will find the list of current available
+  OS we tests with, this include the box name, their type and if they
+have to go under specific bootstrap scripts (see ```specific: true ```
+in the platform definition).
+
+This file is the one that you will use to know about differnt OS's
+testes, add new ones, etc..
+
+### I want to add a test, whad should I do?
+
+To add a test you basically should start by the acceptance directory,
+here you will find an already created tests, most important locations
+here are:
+
+* ```lib``` here is where the tests are living. If a test is not going
+  to be reused it should be created here.
+* ```shared_examples``` inside that directory should be living all tests
+  that could be reused in different scenarios, like you can see the CLI
+ones.
+
+but we want to write tests, here is an example of how do they look like,
+including the different moving parts we encounter in the framework.
+
+
+```
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    ##
+    # ServiceTester::Artifact is the component used to interact with the
+    # destination machineri and the one that keep the necessary logic
+    # for it.
+    ##
+
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+
+    ## your test code goes here.
+  end
+```
+
+this is important because as you know we test with different machines,
+so the build out artifact will be the component necessary to run the
+actions with the destination machine.
+
+but this is the main parts, to run your test you need the framework
+located inside the ```rspec``` directory. Here you will find a
+collection of commands, properly organized per operating system, that
+will let you operate and get your tests done. But don't freak out, we
+got all logic necessary to select the right one for your test.
+
+You'll probably find enough supporting classes for different platforms, but if not, feel free to add it.
+
+FYI, this is how a command looks like:
+
+```
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+  end
+  ```
+this is how we run operations and wrap them as ruby code.
+
+### Running a test (detailed level)
+
+There is also the possibility to run your tests with more granularity by
+using the `rspec` command, this will let you for example run a single
+tests, a collection of them using filtering, etc.
+
+Check https://relishapp.com/rspec/rspec-core/v/3-4/docs/command-line for more details, but here is a quick cheat sheet to run them:
+
+# Run the examples that get "is installed" in their description
+
+*  bundle exec rspec acceptance/spec -e "is installed" 
+
+# Run the example desfined at line 11
+
+*  bundle exec rspec acceptance/spec/lib/artifact_operation_spec.rb:11
diff --git a/qa/Rakefile b/qa/Rakefile
new file mode 100644
index 00000000000..ffcde89f07a
--- /dev/null
+++ b/qa/Rakefile
@@ -0,0 +1,68 @@
+require "rspec"
+require "rspec/core/runner"
+require "rspec/core/rake_task"
+require_relative "vagrant/helpers"
+require_relative "platform_config"
+
+platforms = PlatformConfig.new
+
+task :spec    => 'spec:all'
+task :default => :spec
+
+namespace :qa do
+
+  namespace :vm do
+
+    desc "Generate a valid ssh-config"
+    task :ssh_config do
+      require "json"
+      raw_ssh_config    = LogStash::VagrantHelpers.fetch_config.stdout.split("\n");
+      parsed_ssh_config = LogStash::VagrantHelpers.parse(raw_ssh_config)
+      File.write(".vm_ssh_config", parsed_ssh_config.to_json)
+    end
+
+    desc "Bootstrap all the VM's used for this tests"
+    task :setup, :platform do |t, args|
+      config   = PlatformConfig.new
+      machines = config.select_names_for(args[:platform])
+
+      message  = "bootstraping all VM's defined in acceptance/Vagrantfile"
+      message  = "#{message} for #{args[:platform]}: #{machines}" if !args[:platform].nil?
+
+      LogStash::VagrantHelpers.destroy(machines)
+      LogStash::VagrantHelpers.bootstrap(machines)
+    end
+
+    desc "Halt all VM's involved in the acceptance test round"
+    task :halt, :platform do |t, args|
+      config   = PlatformConfig.new
+      machines = config.select_names_for(args[:platform])
+      message = "halting all VM's defined inside Vagrantfile"
+      message  = "#{message} for #{args[:platform]}: #{machines}" if !args[:platform].nil?
+      puts message
+
+      LogStash::VagrantHelpers.halt(machines)
+    end
+  end
+
+  namespace :acceptance do
+    desc "Run all acceptance"
+    task :all do
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/*_spec.rb"]]))
+    end
+
+    platforms.types.each do |type|
+      desc "Run acceptance test in #{type} machines"
+      task type do
+        ENV['LS_TEST_PLATFORM']=type
+        exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/*_spec.rb"]]))
+      end
+    end
+
+    desc "Run one single machine acceptance test"
+    task :single, :machine do |t, args|
+      ENV['LS_VAGRANT_HOST']  = args[:machine]
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/**/*_spec.rb"]]))
+    end
+  end
+end
diff --git a/qa/Vagrantfile b/qa/Vagrantfile
new file mode 100644
index 00000000000..b7da73064f6
--- /dev/null
+++ b/qa/Vagrantfile
@@ -0,0 +1,28 @@
+# -*- mode: ruby -*-
+# vi: set ft=ruby :
+require_relative "./platform_config.rb"
+
+Vagrant.configure(2) do |config|
+  platforms = PlatformConfig.new
+
+  platforms.each do |platform|
+    config.vm.define platform.name do |machine|
+      machine.vm.box = platform.box
+      machine.vm.provider "virtualbox" do |v|
+        v.memory = 2096
+        v.cpus = 4
+      end
+      machine.vm.synced_folder "../build", "/logstash-build", create: true
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.privileged
+        sh.privileged = true
+      end
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.non_privileged
+        sh.privileged = false
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/config_helper.rb b/qa/acceptance/spec/config_helper.rb
new file mode 100644
index 00000000000..3d9730c2d2d
--- /dev/null
+++ b/qa/acceptance/spec/config_helper.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "json"
+
+module SpecsHelper
+
+  def self.configure(vagrant_boxes)
+    setup_config = JSON.parse(File.read(File.join(File.dirname(__FILE__), "..", "..", ".vm_ssh_config")))
+    boxes        = vagrant_boxes.inject({}) do |acc, v|
+      acc[v.name] = v.type
+      acc
+    end
+    ServiceTester.configure do |config|
+      config.servers = []
+      config.lookup  = {}
+      setup_config.each do |host_info|
+        next unless boxes.keys.include?(host_info["host"])
+        url = "#{host_info["hostname"]}:#{host_info["port"]}"
+        config.servers << url
+        config.lookup[url] = {"host" => host_info["host"], "type" => boxes[host_info["host"]] }
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/lib/artifact_operation_spec.rb b/qa/acceptance/spec/lib/artifact_operation_spec.rb
new file mode 100644
index 00000000000..fdebf23de31
--- /dev/null
+++ b/qa/acceptance/spec/lib/artifact_operation_spec.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require_relative '../shared_examples/installed'
+require_relative '../shared_examples/running'
+require_relative '../shared_examples/updated'
+
+# This tests verify that the generated artifacts could be used properly in a relase, implements https://github.com/elastic/logstash/issues/5070
+describe "artifacts operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "installable", logstash
+    it_behaves_like "updated", logstash
+  end
+end
diff --git a/qa/acceptance/spec/lib/cli_operation_spec.rb b/qa/acceptance/spec/lib/cli_operation_spec.rb
new file mode 100644
index 00000000000..2a0fbaa2176
--- /dev/null
+++ b/qa/acceptance/spec/lib/cli_operation_spec.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require_relative "../spec_helper"
+require_relative "../shared_examples/cli/logstash/version"
+require_relative "../shared_examples/cli/logstash-plugin/install"
+require_relative "../shared_examples/cli/logstash-plugin/list"
+require_relative "../shared_examples/cli/logstash-plugin/uninstall"
+require_relative "../shared_examples/cli/logstash-plugin/update"
+
+# This is the collection of test for the CLI interface, this include the plugin manager behaviour, 
+# it also include the checks for other CLI options.
+describe "CLI operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "logstash version", logstash
+    it_behaves_like "logstash install", logstash
+    it_behaves_like "logstash list", logstash
+    it_behaves_like "logstash uninstall", logstash
+    it_behaves_like "logstash update", logstash
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
new file mode 100644
index 00000000000..13cb52036a2
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash install" do |logstash|
+  before(:each) do
+    logstash.install(LOGSTASH_VERSION)
+  end
+
+  after(:each) do
+    logstash.uninstall
+  end
+
+  describe "on #{logstash.hostname}" do
+    context "with a direct internet connection" do
+      context "when the plugin exist" do
+        context "from a local `.GEM` file" do
+          let(:gem_name) { "logstash-filter-qatest-0.1.1.gem" }
+          let(:gem_path_on_vagrant) { "/tmp/#{gem_name}" }
+          before(:each) do
+            logstash.download("https://rubygems.org/gems/#{gem_name}", gem_path_on_vagrant)
+          end
+
+          after(:each) { logstash.delete_file(gem_path_on_vagrant) }
+
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install #{gem_path_on_vagrant}")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-dns")
+          end
+        end
+
+        context "when fetching a gem from rubygems" do
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest")
+          end
+
+          it "allow to install a specific version" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install --version 0.1.0 logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest", "0.1.0")
+          end
+        end
+      end
+
+      context "when the plugin doesnt exist" do
+        it "fails to install and report an error" do
+          command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify logstash-output-impossible-plugin")
+          expect(command.stderr).to match(/Plugin not found, aborting/)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
new file mode 100644
index 00000000000..64e6cd1be1a
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash list" do |logstash|
+  describe "logstash-plugin list on #{logstash.hostname}" do
+    before(:all) do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after(:all) do
+      logstash.uninstall
+    end
+
+    context "without a specific plugin" do
+      it "display a list of plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "display a list of installed plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --installed")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "list the plugins with their versions" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose")
+        result.stdout.split("\n").each do |plugin|
+          expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+\)/)
+        end
+      end
+    end
+
+    context "with a specific plugin" do
+      let(:plugin_name) { "logstash-input-stdin" }
+      it "list the plugin and display the plugin name" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name}$/)
+      end
+
+      it "list the plugin with his version" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
new file mode 100644
index 00000000000..d12bbb954c0
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash uninstall" do |logstash|
+  describe "logstash uninstall on #{logstash.hostname}" do
+    before :each do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    context "when the plugin isn't installed" do
+      it "fails to uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stderr).to match(/ERROR: Uninstall Aborted, message: This plugin has not been previously installed, aborting/)
+      end
+    end
+
+    # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
+    xcontext "when the plugin is installed" do
+      it "succesfully uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+        expect(logstash).to have_installed?("logstash-filter-qatest")
+
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stdout).to match(/^Uninstalling logstash-filter-qatest/)
+        expect(logstash).not_to have_installed?("logstash-filter-qatest")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
new file mode 100644
index 00000000000..3aaaa30523a
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash update" do |logstash|
+  describe "logstash update on #{logstash.hostname}" do
+    before :each do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    let(:plugin_name) { "logstash-filter-qatest" }
+    let(:previous_version) { "0.1.0" }
+
+    before do
+      logstash.run_command_in_path("bin/logstash-plugin install --version #{previous_version} #{plugin_name}")
+      # Logstash wont update when we have a pinned versionin the gemfile so we remove them
+      logstash.replace_in_gemfile(',\s"0.1.0"', "")
+      expect(logstash).to have_installed?(plugin_name, previous_version)
+    end
+
+    context "update a specific plugin" do
+      it "has executed succesfully" do
+        cmd = logstash.run_command_in_path("bin/logstash-plugin update #{plugin_name}")
+        expect(cmd.stdout).to match(/Updating #{plugin_name}/)
+        expect(logstash).not_to have_installed?(plugin_name, previous_version)
+      end
+    end
+
+    context "update all the plugins" do
+      it "has executed succesfully" do
+        logstash.run_command_in_path("bin/logstash-plugin update")
+        expect(logstash).to have_installed?(plugin_name, "0.1.1")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash/version.rb b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
new file mode 100644
index 00000000000..97a2027064b
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash version" do |logstash|
+  describe "logstash --version" do
+    before :all do
+      logstash.install(LOGSTASH_VERSION)
+    end
+
+    after :all do
+      logstash.uninstall
+    end
+
+    context "on #{logstash.hostname}" do
+      it "returns the right logstash version" do
+        result = logstash.run_command_in_path("bin/logstash --path.settings=/etc/logstash --version")
+        expect(result).to run_successfully_and_output(/#{LOGSTASH_VERSION}/)
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/installed.rb b/qa/acceptance/spec/shared_examples/installed.rb
new file mode 100644
index 00000000000..94089232174
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/installed.rb
@@ -0,0 +1,25 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if a package is possible to be installed without errors.
+RSpec.shared_examples "installable" do |logstash|
+
+  before(:each) do
+    logstash.install(LOGSTASH_VERSION)
+  end
+
+  it "is installed on #{logstash.hostname}" do
+    expect(logstash).to be_installed
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+  it "is removable on #{logstash.hostname}" do
+    logstash.uninstall
+    expect(logstash).to be_removed
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/running.rb b/qa/acceptance/spec/shared_examples/running.rb
new file mode 100644
index 00000000000..4c051dc8f29
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/running.rb
@@ -0,0 +1,17 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# Test if an installed package can actually be started and runs OK.
+RSpec.shared_examples "runnable" do |logstash|
+
+  before(:each) do
+    logstash.install(LOGSTASH_VERSION)
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+end
diff --git a/qa/acceptance/spec/shared_examples/updated.rb b/qa/acceptance/spec/shared_examples/updated.rb
new file mode 100644
index 00000000000..689a9d190b2
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/updated.rb
@@ -0,0 +1,25 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if the current package could used to update from the latest version released.
+RSpec.shared_examples "updated" do |logstash|
+
+  before(:all) { logstash.uninstall }
+  after(:all)  do
+    logstash.stop_service # make sure the service is stopped
+    logstash.uninstall #remove the package to keep uniform state
+  end
+
+  before(:each) do
+    logstash.install(LOGSTASH_LATEST_VERSION, "./") # make sure latest version is installed
+  end
+
+  it "can be updated an run on #{logstash.hostname}" do
+    # Performing the update
+    logstash.install(LOGSTASH_VERSION)
+    expect(logstash).to be_installed
+    # starts the service to be sure it runs after the upgrade
+    logstash.start_service
+    expect(logstash).to be_running
+  end
+end
diff --git a/qa/acceptance/spec/spec_helper.rb b/qa/acceptance/spec/spec_helper.rb
new file mode 100644
index 00000000000..8cceefd5c13
--- /dev/null
+++ b/qa/acceptance/spec/spec_helper.rb
@@ -0,0 +1,32 @@
+# encoding: utf-8
+require 'runner-tool'
+require_relative '../../rspec/helpers'
+require_relative '../../rspec/matchers'
+require_relative 'config_helper'
+require_relative "../../platform_config"
+
+ROOT = File.expand_path(File.join(File.dirname(__FILE__), '..', '..', '..'))
+$LOAD_PATH.unshift File.join(ROOT, 'logstash-core/lib')
+
+RunnerTool.configure
+
+RSpec.configure do |c|
+  c.include ServiceTester
+end
+
+platform = ENV['LS_TEST_PLATFORM'] || 'all'
+
+config                  = PlatformConfig.new
+LOGSTASH_LATEST_VERSION = config.latest
+
+default_vagrant_boxes = ( platform == 'all' ? config.platforms : config.filter_type(platform) )
+
+selected_boxes = if ENV.include?('LS_VAGRANT_HOST') then
+                   config.platforms.select { |p| p.name  == ENV['LS_VAGRANT_HOST'] }
+                 else
+                   default_vagrant_boxes
+                 end
+
+SpecsHelper.configure(selected_boxes)
+
+puts "[Acceptance specs] running on #{ServiceTester.configuration.hosts}" if !selected_boxes.empty?
diff --git a/qa/config/platforms.json b/qa/config/platforms.json
new file mode 100644
index 00000000000..09a7d6ce481
--- /dev/null
+++ b/qa/config/platforms.json
@@ -0,0 +1,18 @@
+{ 
+  "latest": "5.0.0-alpha2",
+  "platforms" : {
+    "ubuntu-1204": { "box": "elastic/ubuntu-12.04-x86_64", "type": "debian" },
+    "ubuntu-1404": { "box": "elastic/ubuntu-14.04-x86_64", "type": "debian", "specific": true },
+    "ubuntu-1604": { "box": "elastic/ubuntu-16.04-x86_64", "type": "debian" },
+    "centos-6": { "box": "elastic/centos-6-x86_64", "type": "redhat" },
+    "centos-7": { "box": "elastic/centos-7-x86_64", "type": "redhat" },
+    "oel-6": { "box": "elastic/oraclelinux-6-x86_64", "type": "redhat" },
+    "oel-7": { "box": "elastic/oraclelinux-7-x86_64", "type": "redhat" },
+    "fedora-22": { "box": "elastic/fedora-22-x86_64", "type": "redhat" },
+    "fedora-23": { "box": "elastic/fedora-23-x86_64", "type": "redhat" },
+    "debian-8": { "box": "elastic/debian-8-x86_64", "type": "debian", "specific":  true },
+    "opensuse-13": { "box": "elastic/opensuse-13-x86_64", "type": "suse" },
+    "sles-11": { "box": "elastic/sles-11-x86_64", "type": "suse", "specific": true },
+    "sles-12": { "box": "elastic/sles-12-x86_64", "type": "suse", "specific": true }
+  }
+}
diff --git a/qa/platform_config.rb b/qa/platform_config.rb
new file mode 100644
index 00000000000..42e4b197404
--- /dev/null
+++ b/qa/platform_config.rb
@@ -0,0 +1,77 @@
+# encoding: utf-8
+require "json"
+require "ostruct"
+
+# This is a wrapper to encapsulate the logic behind the different platforms we test with, 
+# this is done here in order to simplify the necessary configuration for bootstrap and interactions
+# necessary later on in the tests phases.
+#
+class PlatformConfig
+
+  # Abstract the idea of a platform, aka an OS
+  class Platform
+
+    attr_reader :name, :box, :type, :bootstrap
+
+    def initialize(name, data)
+      @name = name
+      @box  = data["box"]
+      @type = data["type"]
+      configure_bootstrap_scripts(data)
+    end
+
+    private
+
+    def configure_bootstrap_scripts(data)
+      @bootstrap = OpenStruct.new(:privileged     => "sys/#{type}/bootstrap.sh",
+                                  :non_privileged => "sys/#{type}/user_bootstrap.sh")
+      ##
+      # for now the only specific boostrap scripts are ones need
+      # with privileged access level, whenever others are also
+      # required we can update this section as well with the same pattern.
+      ##
+      @bootstrap.privileged = "sys/#{type}/#{name}/bootstrap.sh" if data["specific"]
+    end
+  end
+
+  DEFAULT_CONFIG_LOCATION = File.join(File.dirname(__FILE__), "config", "platforms.json").freeze
+
+  attr_reader :platforms, :latest
+
+  def initialize(config_path = DEFAULT_CONFIG_LOCATION)
+    @config_path = config_path
+    @platforms = []
+
+    data = JSON.parse(File.read(@config_path))
+    data["platforms"].each do |k, v|
+      @platforms << Platform.new(k, v)
+    end
+    @platforms.sort! { |a, b| a.name <=> b.name }
+    @latest = data["latest"]
+  end
+
+  def find!(platform_name)
+    result = @platforms.find { |platform| platform.name == platform_name }.first
+    if result.nil?
+      raise "Cannot find platform named: #{platform_name} in @config_path"
+    else
+      return result
+    end
+  end
+
+  def each(&block)
+    @platforms.each(&block)
+  end
+
+  def filter_type(type_name)
+    @platforms.select { |platform| platform.type == type_name }
+  end
+
+  def select_names_for(platform=nil)
+    !platform.nil? ? filter_type(platform).map{ |p| p.name } : ""
+  end
+
+  def types
+    @platforms.collect(&:type).uniq.sort
+  end
+end
diff --git a/qa/rspec/commands.rb b/qa/rspec/commands.rb
new file mode 100644
index 00000000000..c4e739198e8
--- /dev/null
+++ b/qa/rspec/commands.rb
@@ -0,0 +1,132 @@
+# encoding: utf-8
+require_relative "./commands/debian"
+require_relative "./commands/ubuntu"
+require_relative "./commands/redhat"
+require_relative "./commands/suse"
+require_relative "./commands/centos/centos-6"
+require_relative "./commands/oel/oel-6"
+require_relative "./commands/ubuntu/ubuntu-1604"
+require_relative "./commands/suse/sles-11"
+
+require "forwardable"
+
+module ServiceTester
+
+  # An artifact is the component being tested, it's able to interact with
+  # a destination machine by holding a client and is basically provides all 
+  # necessary abstractions to make the test simple.
+  class Artifact
+
+    extend Forwardable
+    def_delegators :@client, :installed?, :removed?, :running?
+
+    attr_reader :host, :client
+
+    def initialize(host, options={})
+      @host    = host
+      @options = options
+      @client  = CommandsFactory.fetch(options["type"], options["host"])
+    end
+
+    def hostname
+      @options["host"]
+    end
+
+    def name
+      "logstash"
+    end
+
+    def hosts
+      [@host]
+    end
+
+    def snapshot
+      client.snapshot(@options["host"])
+    end
+
+    def restore
+      client.restore(@options["host"])
+    end
+
+    def start_service
+      client.start_service(name, host)
+    end
+
+    def stop_service
+      client.stop_service(name, host)
+    end
+
+    def install(version, base=ServiceTester::Base::LOCATION)
+      package = client.package_for(version, base)
+      client.install(package, host)
+    end
+
+    def uninstall
+      client.uninstall(name, host)
+    end
+
+    def run_command_in_path(cmd)
+      client.run_command_in_path(cmd, host)
+    end
+
+    def run_command(cmd)
+      client.run_command(cmd, host)
+    end
+
+    def plugin_installed?(name, version = nil)
+      client.plugin_installed?(host, name, version)
+    end
+
+    def download(from, to)
+      client.download(from, to , host)
+    end
+    
+    def replace_in_gemfile(pattern, replace)
+      client.replace_in_gemfile(pattern, replace, host)
+    end
+
+    def delete_file(path)
+      client.delete_file(path, host)
+    end
+
+    def to_s
+      "Artifact #{name}@#{host}"
+    end
+  end
+
+  # Factory of commands used to select the right clients for a given type of OS and host name,
+  # this give you as much granularity as required.
+  class CommandsFactory
+
+    def self.fetch(type, host)
+      case type
+      when "debian"
+        if host.start_with?("ubuntu")
+          if host == "ubuntu-1604"
+            return Ubuntu1604Commands.new
+          else
+            return UbuntuCommands.new
+          end
+        else
+          return DebianCommands.new
+        end
+      when "suse"
+        if host == "sles-11"
+          return Sles11Commands.new
+        else
+          return SuseCommands.new
+        end
+      when "redhat"
+        if host == "centos-6"
+          return Centos6Commands.new
+        elsif host == "oel-6"
+          return Oel6Commands.new
+        else
+          return RedhatCommands.new
+        end
+      else
+        return
+      end
+    end
+  end
+end
diff --git a/qa/rspec/commands/base.rb b/qa/rspec/commands/base.rb
new file mode 100644
index 00000000000..61112f12536
--- /dev/null
+++ b/qa/rspec/commands/base.rb
@@ -0,0 +1,68 @@
+# encoding: utf-8
+require_relative "../../vagrant/helpers"
+require_relative "system_helpers"
+
+module ServiceTester
+
+  class Base
+    LOCATION="/logstash-build".freeze
+    LOGSTASH_PATH="/usr/share/logstash/".freeze
+
+    def snapshot(host)
+      LogStash::VagrantHelpers.save_snapshot(host)
+    end
+
+    def restore(host)
+      LogStash::VagrantHelpers.restore_snapshot(host)
+    end
+
+    def start_service(service, host=nil)
+      service_manager(service, "start", host)
+    end
+
+    def stop_service(service, host=nil)
+      service_manager(service, "stop", host)
+    end
+
+    def run_command(cmd, host)
+      hosts = (host.nil? ? servers : Array(host))
+
+      response = nil
+      at(hosts, {in: :serial}) do |_host|
+        response = sudo_exec!(cmd)
+      end
+      response
+    end
+
+    def replace_in_gemfile(pattern, replace, host)
+      gemfile = File.join(LOGSTASH_PATH, "Gemfile")
+      cmd = "sed -i.sedbak 's/#{pattern}/#{replace}/' #{gemfile}"
+      run_command(cmd, host)
+    end
+
+    def run_command_in_path(cmd, host)
+      run_command("#{File.join(LOGSTASH_PATH, cmd)}", host)
+    end
+
+    def plugin_installed?(host, plugin_name, version = nil)
+      if version.nil?
+        cmd = run_command_in_path("bin/logstash-plugin list", host)
+        search_token = plugin_name
+      else
+        cmd = run_command_in_path("bin/logstash-plugin list --verbose", host)
+        search_token ="#{plugin_name} (#{version})"
+      end
+
+      plugins_list = cmd.stdout.split("\n")
+      plugins_list.include?(search_token)
+    end
+
+    def download(from, to, host)
+      run_command("wget #{from} -O #{to}", host)
+    end
+
+    def delete_file(path, host)
+      run_command("rm -rf #{path}", host)
+    end
+  end
+end
diff --git a/qa/rspec/commands/centos/centos-6.rb b/qa/rspec/commands/centos/centos-6.rb
new file mode 100644
index 00000000000..371590490e6
--- /dev/null
+++ b/qa/rspec/commands/centos/centos-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Centos6Commands < RedhatCommands
+      include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/debian.rb b/qa/rspec/commands/debian.rb
new file mode 100644
index 00000000000..710ffc48d57
--- /dev/null
+++ b/qa/rspec/commands/debian.rb
@@ -0,0 +1,50 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class DebianCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+    end
+
+    def package_for(version, base=ServiceTester::Base::LOCATION)
+      File.join(base, "logstash-#{version}.deb")
+    end
+
+    def install(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("dpkg -i  #{package}")
+      end
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("dpkg -r #{package}")
+        sudo_exec!("dpkg --purge #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s #{package}")
+        stdout = cmd.stderr
+      end
+      (
+        stdout.match(/^Package `#{package}' is not installed and no info is available.$/) ||
+        stdout.match(/^dpkg-query: package '#{package}' is not installed and no information is available$/)
+      )
+    end
+  end
+end
diff --git a/qa/rspec/commands/oel/oel-6.rb b/qa/rspec/commands/oel/oel-6.rb
new file mode 100644
index 00000000000..ed92a8ce11d
--- /dev/null
+++ b/qa/rspec/commands/oel/oel-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Oel6Commands < RedhatCommands
+    include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/redhat.rb b/qa/rspec/commands/redhat.rb
new file mode 100644
index 00000000000..8f0cf753c23
--- /dev/null
+++ b/qa/rspec/commands/redhat.rb
@@ -0,0 +1,49 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class RedhatCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("yum list installed  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Installed Packages$/)
+      stdout.match(/^logstash.noarch/)
+    end
+
+    def package_for(version, base=ServiceTester::Base::LOCATION)
+      File.join(base, "logstash-#{version}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = {}
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("yum install -y  #{package}")
+        errors[_host] = cmd.stderr unless cmd.stderr.empty?
+      end
+      errors
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("yum remove -y #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("yum list installed #{package}")
+        stdout = cmd.stderr
+      end
+      stdout.match(/^Error: No matching Packages to list$/)
+    end
+  end
+end
diff --git a/qa/rspec/commands/suse.rb b/qa/rspec/commands/suse.rb
new file mode 100644
index 00000000000..2a214a53b7e
--- /dev/null
+++ b/qa/rspec/commands/suse.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class SuseCommands < Base
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^i | logstash | An extensible logging pipeline | package$/)
+    end
+
+    def package_for(version, base=ServiceTester::Base::LOCATION)
+      File.join(base, "logstash-#{version}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = {}
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive install  #{package}")
+        errors[_host] = cmd.stderr unless cmd.stderr.empty?
+      end
+      errors
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive remove #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd    = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/No packages found/)
+    end
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} started.$/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/suse/sles-11.rb b/qa/rspec/commands/suse/sles-11.rb
new file mode 100644
index 00000000000..80dd94dd719
--- /dev/null
+++ b/qa/rspec/commands/suse/sles-11.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../suse"
+
+module ServiceTester
+  class Sles11Commands < SuseCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("/etc/init.d/#{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} is running$/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("/etc/init.d/#{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/system_helpers.rb b/qa/rspec/commands/system_helpers.rb
new file mode 100644
index 00000000000..8cb8922946b
--- /dev/null
+++ b/qa/rspec/commands/system_helpers.rb
@@ -0,0 +1,42 @@
+require_relative "base"
+
+module ServiceTester
+  module SystemD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      (
+        stdout.match(/Active: active \(running\)/) &&
+        stdout.match(/#{package}.service - #{package}/)
+      )
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+  end
+
+  module InitD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("initctl status #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} start\/running/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("initctl #{action} #{service}")
+      end
+    end 
+  end
+end
diff --git a/qa/rspec/commands/ubuntu.rb b/qa/rspec/commands/ubuntu.rb
new file mode 100644
index 00000000000..1d1ae75f96d
--- /dev/null
+++ b/qa/rspec/commands/ubuntu.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require_relative "debian"
+
+module ServiceTester
+  class UbuntuCommands < DebianCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^#{package} start\/running/)
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/ubuntu/ubuntu-1604.rb b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
new file mode 100644
index 00000000000..ae26bc09f28
--- /dev/null
+++ b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../ubuntu"
+
+module ServiceTester
+  class Ubuntu1604Commands < UbuntuCommands
+      include ::ServiceTester::SystemD
+  end
+end
diff --git a/qa/rspec/helpers.rb b/qa/rspec/helpers.rb
new file mode 100644
index 00000000000..a939fa7dfca
--- /dev/null
+++ b/qa/rspec/helpers.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require_relative "commands"
+
+module ServiceTester
+
+  class Configuration
+    attr_accessor :servers, :lookup
+    def initialize
+      @servers  = []
+      @lookup   = {}
+    end
+
+    def hosts
+      lookup.values.map { |val| val["host"] }
+    end
+  end
+
+  class << self
+    attr_accessor :configuration
+  end
+
+  def self.configure
+    self.configuration ||= Configuration.new
+    yield(configuration) if block_given?
+  end
+
+  def servers
+    ServiceTester.configuration.servers
+  end
+
+  def select_client
+    CommandsFactory.fetch(current_example.metadata[:platform])
+  end
+
+  def current_example
+    RSpec.respond_to?(:current_example) ? RSpec.current_example : self.example
+  end
+end
diff --git a/qa/rspec/matchers.rb b/qa/rspec/matchers.rb
new file mode 100644
index 00000000000..4da583262f2
--- /dev/null
+++ b/qa/rspec/matchers.rb
@@ -0,0 +1,4 @@
+# encoding: utf-8
+require_relative "./matchers/be_installed"
+require_relative "./matchers/be_running"
+require_relative "./matchers/cli_matchers"
diff --git a/qa/rspec/matchers/be_installed.rb b/qa/rspec/matchers/be_installed.rb
new file mode 100644
index 00000000000..4de70ae21ed
--- /dev/null
+++ b/qa/rspec/matchers/be_installed.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_installed do
+  match do |subject|
+    subject.installed?(subject.hosts, subject.name)
+  end
+end
+
+RSpec::Matchers.define :be_removed do
+  match do |subject|
+    subject.removed?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/be_running.rb b/qa/rspec/matchers/be_running.rb
new file mode 100644
index 00000000000..dc687e1b11d
--- /dev/null
+++ b/qa/rspec/matchers/be_running.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_running do
+  match do |subject|
+    subject.running?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/cli_matchers.rb b/qa/rspec/matchers/cli_matchers.rb
new file mode 100644
index 00000000000..e31aa050af3
--- /dev/null
+++ b/qa/rspec/matchers/cli_matchers.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+RSpec::Matchers.define :be_successful do
+  match do |actual|
+    actual.exit_status == 0
+  end
+end
+
+RSpec::Matchers.define :fail_and_output do |expected_output|
+  match do |actual|
+    actual.exit_status == 1 && actual.stderr =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :run_successfully_and_output do |expected_output|
+  match do |actual|
+    (actual.exit_status == 0 || actual.exit_status.nil?) && actual.stdout =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :have_installed? do |name,*args|
+  match do |actual|
+    version = args.first
+    actual.plugin_installed?(name, version)
+  end
+end
+
+RSpec::Matchers.define :install_successfully do
+  match do |cmd|
+    expect(cmd).to run_successfully_and_output(/Installation successful/)
+  end
+end
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
new file mode 100644
index 00000000000..1e9fe168abe
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
@@ -0,0 +1,25 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.1'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
+
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
new file mode 100644
index 00000000000..82e3be79baf
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.0'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
diff --git a/qa/sys/debian/bootstrap.sh b/qa/sys/debian/bootstrap.sh
new file mode 100644
index 00000000000..29c2c840346
--- /dev/null
+++ b/qa/sys/debian/bootstrap.sh
@@ -0,0 +1,7 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
diff --git a/qa/sys/debian/debian-8/bootstrap.sh b/qa/sys/debian/debian-8/bootstrap.sh
new file mode 100644
index 00000000000..d1a23d54430
--- /dev/null
+++ b/qa/sys/debian/debian-8/bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+echo "deb http://http.debian.net/debian jessie-backports main" >> /etc/apt/sources.list
+apt-get update
+apt-get install -y openjdk-8-jdk
diff --git a/qa/sys/debian/ubuntu-1404/bootstrap.sh b/qa/sys/debian/ubuntu-1404/bootstrap.sh
new file mode 100644
index 00000000000..728b0c3d13f
--- /dev/null
+++ b/qa/sys/debian/ubuntu-1404/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
+update-ca-certificates -f
diff --git a/qa/sys/debian/user_bootstrap.sh b/qa/sys/debian/user_bootstrap.sh
new file mode 100644
index 00000000000..8d9fcb70c1d
--- /dev/null
+++ b/qa/sys/debian/user_bootstrap.sh
@@ -0,0 +1,6 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}_all.deb"
+wget -q https://download.elastic.co/logstash/logstash/packages/debian/$LOGSTASH_FILENAME
+mv $LOGSTASH_FILENAME "logstash-${VERSION}.deb" # necessary patch until new version with the standard name format are released
diff --git a/qa/sys/redhat/bootstrap.sh b/qa/sys/redhat/bootstrap.sh
new file mode 100644
index 00000000000..5976f47722f
--- /dev/null
+++ b/qa/sys/redhat/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+yum update
+yum install -y java-1.8.0-openjdk-devel.x86_64
diff --git a/qa/sys/redhat/user_bootstrap.sh b/qa/sys/redhat/user_bootstrap.sh
new file mode 100644
index 00000000000..db964babc63
--- /dev/null
+++ b/qa/sys/redhat/user_bootstrap.sh
@@ -0,0 +1,6 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.noarch.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
+mv $LOGSTASH_FILENAME "logstash-${VERSION}.rpm" # necessary patch until new version with the standard name format are released
diff --git a/qa/sys/suse/bootstrap.sh b/qa/sys/suse/bootstrap.sh
new file mode 100644
index 00000000000..4dba83eb9ea
--- /dev/null
+++ b/qa/sys/suse/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+zypper --non-interactive list-updates
+zypper --non-interactive --no-gpg-checks --quiet install --no-recommends java-1_8_0-openjdk-devel
diff --git a/qa/sys/suse/sles-11/bootstrap.sh b/qa/sys/suse/sles-11/bootstrap.sh
new file mode 100644
index 00000000000..654be5d7ec0
--- /dev/null
+++ b/qa/sys/suse/sles-11/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+ln -s /usr/sbin/update-alternatives /usr/sbin/alternatives
+curl -L 'https://edelivery.oracle.com/otn-pub/java/jdk/8u77-b03/jdk-8u77-linux-x64.rpm' -H 'Accept-Encoding: gzip, deflate, sdch' -H 'Accept-Language: en-US,en;q=0.8' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0' -H 'Cookie: oraclelicense=accept-securebackup-cookie;' -H 'Connection: keep-alive' --compressed -o oracle_jdk_1.8.rpm
+zypper -q -n --non-interactive install oracle_jdk_1.8.rpm
diff --git a/qa/sys/suse/sles-12/bootstrap.sh b/qa/sys/suse/sles-12/bootstrap.sh
new file mode 100644
index 00000000000..3ed7a04ed15
--- /dev/null
+++ b/qa/sys/suse/sles-12/bootstrap.sh
@@ -0,0 +1,9 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+zypper addrepo http://download.opensuse.org/repositories/Java:Factory/SLE_12/Java:Factory.repo || true
+zypper --no-gpg-checks --non-interactive refresh
+zypper --non-interactive list-updates
+zypper --non-interactive --no-gpg-checks --quiet install --no-recommends java-1_8_0-openjdk-devel
diff --git a/qa/sys/suse/user_bootstrap.sh b/qa/sys/suse/user_bootstrap.sh
new file mode 100644
index 00000000000..77653c4e980
--- /dev/null
+++ b/qa/sys/suse/user_bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.noarch.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
+mv $LOGSTASH_FILENAME "logstash-${VERSION}.rpm" # necessary patch until new version with the standard name format are released
diff --git a/qa/vagrant/command.rb b/qa/vagrant/command.rb
new file mode 100644
index 00000000000..740514df8e7
--- /dev/null
+++ b/qa/vagrant/command.rb
@@ -0,0 +1,45 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+
+module LogStash
+  class CommandExecutor
+    class CommandError < StandardError; end
+
+    class CommandResponse
+      attr_reader :stdin, :stdout, :stderr, :exitstatus
+
+      def initialize(stdin, stdout, stderr, exitstatus)
+        @stdin = stdin
+        @stdout = stdout
+        @stderr = stderr
+        @exitstatus = exitstatus
+      end
+
+      def success?
+        exitstatus == 0
+      end
+    end
+
+    def self.run(cmd)
+      # This block is require to be able to launch a ruby subprocess
+      # that use bundler.
+      Bundler.with_clean_env do
+        Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr|
+          CommandResponse.new(stdin, stdout.read.chomp, stderr.read.chomp, wait_thr.value.exitstatus)
+        end
+      end
+    end
+
+    # This method will raise an exception if the `CMD`
+    # was not run successfully and will display the content of STDERR
+    def self.run!(cmd)
+      response = run(cmd)
+
+      unless response.success?
+        raise CommandError, "CMD: #{cmd} STDERR: #{response.stderr}"
+      end
+      response
+    end
+  end
+end
diff --git a/qa/vagrant/helpers.rb b/qa/vagrant/helpers.rb
new file mode 100644
index 00000000000..1b2a63929a7
--- /dev/null
+++ b/qa/vagrant/helpers.rb
@@ -0,0 +1,52 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+require_relative "command"
+
+module LogStash
+  class VagrantHelpers
+
+    def self.halt(machines="")
+      CommandExecutor.run!("vagrant halt #{machines.join(' ')}")
+    end
+
+    def self.destroy(machines="")
+      CommandExecutor.run!("vagrant destroy --force #{machines.join(' ')}") 
+    end
+
+    def self.bootstrap(machines="")
+      CommandExecutor.run!("vagrant up #{machines.join(' ')}")
+    end
+
+    def self.save_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot save #{machine} #{machine}-snapshot")
+    end
+
+    def self.restore_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot restore #{machine} #{machine}-snapshot")
+    end
+
+    def self.fetch_config
+      machines = CommandExecutor.run!("vagrant status").stdout.split("\n").select { |l| l.include?("running") }.map { |r| r.split(' ')[0]}
+      CommandExecutor.run!("vagrant ssh-config #{machines.join(' ')}")
+    end
+
+    def self.parse(lines)
+      hosts, host = [], {}
+      lines.each do |line|
+        if line.match(/Host\s(.*)$/)
+          host = { :host => line.gsub("Host","").strip }
+        elsif line.match(/HostName\s(.*)$/)
+          host[:hostname] = line.gsub("HostName","").strip
+        elsif line.match(/Port\s(.*)$/)
+          host[:port]     = line.gsub("Port","").strip
+        elsif line.empty?
+          hosts << host
+          host = {}
+        end
+      end
+      hosts << host
+      hosts
+    end
+  end
+end
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index 1655302e4ac..581d8c798cb 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -55,6 +55,20 @@ namespace "artifact" do
     end.flatten.uniq
   end
 
+  task "all" => ["prepare"] do
+    Rake::Task["artifact:deb"].invoke
+    Rake::Task["artifact:rpm"].invoke
+    Rake::Task["artifact:zip"].invoke
+    Rake::Task["artifact:tar"].invoke
+  end
+
+  task "all-all-plugins" => ["prepare-all"] do
+    Rake::Task["artifact:deb"].invoke
+    Rake::Task["artifact:rpm"].invoke
+    Rake::Task["artifact:zip"].invoke
+    Rake::Task["artifact:tar"].invoke
+  end
+
   # We create an empty bundle config file
   # This will allow the deb and rpm to create a file
   # with the correct user group and permission.
@@ -122,6 +136,7 @@ namespace "artifact" do
   end
 
   task "prepare" => ["bootstrap", "plugin:install-default", "install-logstash-core", "install-logstash-core-event", "install-logstash-core-plugin-api", "clean-bundle-config"]
+
   task "prepare-all" => ["bootstrap", "plugin:install-all", "install-logstash-core", "install-logstash-core-event", "install-logstash-core-plugin-api", "clean-bundle-config"]
 
   desc "Build a tar.gz of default logstash plugins with all dependencies"
