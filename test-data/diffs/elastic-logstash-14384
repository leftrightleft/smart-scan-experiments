diff --git a/docs/include/attributes-ls.asciidoc b/docs/include/attributes-ls.asciidoc
index 714982cadef..5856b1ae997 100644
--- a/docs/include/attributes-ls.asciidoc
+++ b/docs/include/attributes-ls.asciidoc
@@ -8,3 +8,6 @@ with a corresponding change to the VPR settings in
 logstash-docs/docs/versioned-plugins/include/attributes-ls-vpr.asciidoc
 /////
 
+:lsyml: Available in`logstash.yml`.
+:plyml: Available in`pipelines.yml`.
+:bothyml: Available in `logstash.yml` and `pipelines.yml`.
diff --git a/docs/static/dead-letter-queues.asciidoc b/docs/static/dead-letter-queues.asciidoc
index daf23f9960e..72d4e321710 100644
--- a/docs/static/dead-letter-queues.asciidoc
+++ b/docs/static/dead-letter-queues.asciidoc
@@ -11,6 +11,11 @@ You can <<es-proc-dlq,process events from the DLQ>> with the <<plugins-inputs-de
 Processing events does not delete items from the queue, and the DLQ sometimes needs attention.
 See <<dlq-size>> and <<dlq-clear>> for more info. 
 
+[[dlq-settings]]
+==== DLQ settings
+
+include::settings/settings-dlq.asciidoc[]
+
 [[dead-letter-how]]
 ==== How the dead letter queue works
 
diff --git a/docs/static/persistent-queues.asciidoc b/docs/static/persistent-queues.asciidoc
index 255b43f0093..1bfe0994cc8 100644
--- a/docs/static/persistent-queues.asciidoc
+++ b/docs/static/persistent-queues.asciidoc
@@ -41,40 +41,11 @@ When you set values for capacity and sizing settings, remember that the value yo
 
 TIP: If you want to define values for a specific pipeline, use <<multiple-pipelines,`pipelines.yml`>>.
 
-`queue.type`:: Specify `persisted` to enable persistent queues. By default, persistent queues are disabled (default: `queue.type: memory`).
-`path.queue`:: The directory path where the data files will be stored. By default, the files are stored in `path.data/queue`.
-`queue.page_capacity`:: The queue data consists of append-only files called "pages." This value sets the maximum size of a queue page in bytes. 
-The default size of 64mb is a good value for most users, and changing this value is unlikely to have performance benefits. 
-If you change the page capacity of an existing queue, the new size applies only to the new page.
-`queue.drain`:: Specify `true` if you want Logstash to wait until the persistent queue is drained before shutting down. The amount of time it takes to drain the queue depends on the number of events that have accumulated in the queue. Therefore, you should avoid using this setting unless the queue, even when full, is relatively small and can be drained quickly. 
-`queue.max_events`:: The maximum number of events not yet read by the pipeline worker. The default is 0 (unlimited).
-We use this setting for internal testing. 
-Users generally shouldn't be changing this value.
-`queue.max_bytes`:: The total capacity of _each queue_ in number of bytes. 
-Unless overridden in `pipelines.yml` or central management, each persistent
-queue will be sized at the value of `queue.max_bytes` specified in
-`logstash.yml`. 
-The default is 1024mb (1gb).
-+
-NOTE: Be sure that your disk has sufficient capacity to handle the cumulative total of `queue.max_bytes` across all persistent queues.
-The total of `queue.max_bytes` for _all_ queues should be
-lower than the capacity of your disk.
-+
-TIP: If you are using persistent queues to protect against data loss, but don't
-require much buffering, you can set `queue.max_bytes` to a smaller value.
-A smaller value produces smaller queues and improves queue performance. 
-
-`queue.checkpoint.acks`:: Sets the number of acked events before forcing a checkpoint. 
-Default is `1024`. Set to `0` for unlimited.
-`queue.checkpoint.writes`:: Sets the maximum number of written events before a forced checkpoint. 
-Default is `1024`. Set to `0` for unlimited.
-+
-To avoid losing data in the persistent queue, you can set `queue.checkpoint.writes: 1` to force a checkpoint after each event is
-written. Keep in mind that disk writes have a resource cost. Setting this value
-to `1` ensures maximum durability, but can severely impact performance.
-See <<durability-persistent-queues>> to better understand the trade-offs.
-`queue.checkpoint.interval`:: Sets the interval in milliseconds when a checkpoint is forced on the head page.
-Default is `1000`. Set to `0` to eliminate periodic checkpoints.
+
+[[pq-settings]]
+==== PQ settings
+include::settings/settings-pq.asciidoc[]
+
 
 [[pq-config-notes]]
 ==== Configuration notes
diff --git a/docs/static/setting-up-logstash.asciidoc b/docs/static/setting-up-logstash.asciidoc
index b7c2864d44c..c09d8835d90 100644
--- a/docs/static/setting-up-logstash.asciidoc
+++ b/docs/static/setting-up-logstash.asciidoc
@@ -8,6 +8,7 @@ This section includes additional information on how to set up and run Logstash,
 * <<dir-layout>>
 * <<config-setting-files>>
 * <<logstash-settings-file>>
+* <<logstash-yml-settings>>
 * <<keystore>>
 * <<running-logstash-command-line>>
 * <<running-logstash>>
diff --git a/docs/static/settings-file.asciidoc b/docs/static/settings-file.asciidoc
index b4770e92771..c45de4d5211 100644
--- a/docs/static/settings-file.asciidoc
+++ b/docs/static/settings-file.asciidoc
@@ -63,287 +63,5 @@ modules:
 
 IMPORTANT: If the <<command-line-flags,command-line flag>> `--modules` is used, any modules defined in the `logstash.yml` file will be ignored.
 
-The `logstash.yml` file includes the following settings. 
 
-[options="header"]
-|=======================================================================
-| Setting | Description | Default value
-
-| `node.name`
-| A descriptive name for the node.
-| Machine's hostname
-
-| `path.data`
-| The directory that Logstash and its plugins use for any persistent needs.
-|`LOGSTASH_HOME/data`
-
-| `pipeline.id`
-| The ID of the pipeline.
-| `main`
-
-| `pipeline.workers` 
-| The number of workers that will, in parallel, execute the filter and output
-stages of the pipeline. This setting uses the
-https://docs.oracle.com/javase/7/docs/api/java/lang/Runtime.html#availableProcessors()[`java.lang.Runtime.getRuntime.availableProcessors`]
-value as a default if not overridden by `pipeline.workers` in `pipelines.yml` or
-`pipeline.workers` from `logstash.yml`.  If you have modified this setting and
-see that events are backing up, or that the CPU is not saturated, consider
-increasing this number to better utilize machine processing power. 
-| Number of the host's CPU cores
-
-| `pipeline.batch.size`
-| The maximum number of events an individual worker thread will collect from inputs
-  before attempting to execute its filters and outputs.
-  Larger batch sizes are generally more efficient, but come at the cost of increased memory
-  overhead. You may need to increase JVM heap space in the `jvm.options` config file.
-  See <<config-setting-files>> for more info.
-| `125`
-
-| `pipeline.batch.delay`
-| When creating pipeline event batches, how long in milliseconds to wait for
-  each event before dispatching an undersized batch to pipeline workers.
-| `50`
-
-| `pipeline.unsafe_shutdown`
-| When set to `true`, forces Logstash to exit during shutdown even if there are still inflight events
-  in memory. By default, Logstash will refuse to quit until all received events
-  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
-| `false`
-
-| `pipeline.plugin_classloaders`
-| (Beta) Load Java plugins in independent classloaders to isolate their dependencies.
-| `false`
-
-| `pipeline.ordered`
-a|
-Set the pipeline event ordering. Valid options are:
-
-* `auto`. Automatically enables ordering if the `pipeline.workers` setting is `1`, and disables otherwise.
-* `true`. Enforces ordering on the pipeline and prevents Logstash from starting
-if there are multiple workers.
-* `false`. Disables the processing required to preserve order. Ordering will not be
-guaranteed, but you save the processing cost of preserving order.
-
-| `auto`
-
-| `pipeline.ecs_compatibility`
-a|
-Sets the pipeline's default value for `ecs_compatibility`, a setting that is available to plugins that implement an ECS compatibility mode for use with the Elastic Common Schema.
-Possible values are:
-
-* `disabled`
-* `v1`
-* `v8`
-
-This option allows the <<ecs-ls,early opt-in (or preemptive opt-out) of ECS compatibility>> modes in plugins,
-which is scheduled to be on-by-default in a future major release of {ls}.
-
-Values other than `disabled` are currently considered BETA, and may produce unintended consequences when upgrading {ls}.
-
-| `disabled`
-
-| `path.config`
-| The path to the Logstash config for the main pipeline. If you specify a directory or wildcard,
-  config files are read from the directory in alphabetical order.
-| Platform-specific. See <<dir-layout>>.
-
-| `config.string`
-| A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as
-  the config file.
-| _N/A_
-
-| `config.test_and_exit`
-| When set to `true`, checks that the configuration is valid and then exits. Note that grok patterns are not checked for
-  correctness with this setting. Logstash can read multiple config files from a directory. If you combine this
-  setting with `log.level: debug`, Logstash will log the combined config file, annotating
-  each config block with the source file it came from.
-| `false`
-
-| `config.reload.automatic`
-| When set to `true`, periodically checks if the configuration has changed and reloads the configuration whenever it is changed.
-  This can also be triggered manually through the SIGHUP signal.
-| `false`
-
-| `config.reload.interval`
-| How often in seconds Logstash checks the config files for changes. Note that the unit qualifier (`s`) is required.
-| `3s`
-
-| `config.debug`
-| When set to `true`, shows the fully compiled configuration as a debug log message. You must also set `log.level: debug`.
-  WARNING: The log message will include any 'password' options passed to plugin configs as plaintext, and may result
-  in plaintext passwords appearing in your logs!
-| `false`
-
-| `config.support_escapes`
-| When set to `true`, quoted strings will process the following escape sequences: `\n` becomes a literal newline (ASCII 10). `\r` becomes a literal carriage return (ASCII 13). `\t` becomes a literal tab (ASCII 9). `\\` becomes a literal backslash `\`. `\"` becomes a literal double quotation mark. `\'` becomes a literal quotation mark.
-| `false`
-
-| `config.field_reference.escape_style`
-a| Provides a way to reference fields that contain <<formal-grammar-escape-sequences,field reference special characters>> `[` and `]`.
-
-NOTE: This feature is in *technical preview* and may change in the future.
-
-Current options are:
-
-* `percent`: URI-style `%`{plus}`HH` hexadecimal encoding of UTF-8 bytes (`[` -> `%5B`; `]` -> `%5D`)
-* `ampersand`: HTML-style `&#`{plus}`DD`{plus}`;` encoding of decimal Unicode code-points (`[` -> `&#91;`; `]` -> `&#93;`)
-* `none`: field names containing special characters _cannot_ be referenced.
-
-| `none`
-
-| `modules`
-| When configured, `modules` must be in the nested YAML structure described above this table.
-| _N/A_
-
-| `queue.type`
-| The internal queuing model to use for event buffering. Specify `memory` for legacy in-memory based queuing, or `persisted` for disk-based ACKed queueing (<<persistent-queues,persistent queues>>).
-| `memory`
-
-| `path.queue`
-| The directory path where the data files will be stored when persistent queues are enabled (`queue.type: persisted`).
-| `path.data/queue`
-
-| `queue.page_capacity`
-| The size of the page data files used when persistent queues are enabled (`queue.type: persisted`). The queue data consists of append-only data files separated into pages.
-| 64mb
-
-| `queue.max_events`
-| The maximum number of unread events in the queue when persistent queues are enabled (`queue.type: persisted`).
-| 0 (unlimited)
-
-| `queue.max_bytes`
-| The total capacity of the queue (`queue.type: persisted`) in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both `queue.max_events` and `queue.max_bytes` are specified, Logstash uses whichever criteria is reached first.
-| 1024mb (1g)
-
-| `queue.checkpoint.acks`
-| The maximum number of ACKed events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.acks: 0` to set this value to unlimited.
-|1024
-
-| `queue.checkpoint.writes`
-| The maximum number of written events before forcing a checkpoint when persistent queues are enabled (`queue.type: persisted`). Specify `queue.checkpoint.writes: 0` to set this value to unlimited.
-| 1024
-
-| `queue.checkpoint.retry`
-| When enabled, Logstash will retry four times per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on Windows platform, filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. (`queue.type: persisted`)
-| `true`
-
-| `queue.drain`
-| When enabled, Logstash waits until the persistent queue (`queue.type: persisted`) is drained before shutting down.
-| `false`
-
-| `dead_letter_queue.enable`
-| Flag to instruct Logstash to enable the DLQ feature supported by plugins.
-| `false`
-
-| `dead_letter_queue.max_bytes`
-| The maximum size of each dead letter queue. Entries will be dropped if they
-  would increase the size of the dead letter queue beyond this setting.
-| `1024mb`
-
-| `dead_letter_queue.storage_policy`
-| Defines the action to take when the dead_letter_queue.max_bytes setting is reached: `drop_newer` stops accepting new values that would push the file size over the limit, and `drop_older` removes the oldest events to make space for new ones.
-| `drop_newer`
-
-| `path.dead_letter_queue`
-| The directory path where the data files will be stored for the dead-letter queue.
-| `path.data/dead_letter_queue`
-
-| `api.enabled`
-|  The HTTP API is enabled by default. It can be disabled, but features that rely on it will not work as intended.
-| `true`
-
-| `api.environment`
-| The API returns the provided string as a part of its response. Setting your environment may help to disambiguate between similarly-named nodes in production vs test environments.
-| `production`
-
-| `api.http.host`
-| The bind address for the HTTP API endpoint.
-  By default, the {ls} HTTP API binds only to the local loopback interface.
-  When configured securely (`api.ssl.enabled: true` and `api.auth.type: basic`), the HTTP API binds to _all_ available interfaces.
-| `"127.0.0.1"`
-
-| `api.http.port`
-| The bind port for the HTTP API endpoint.
-| `9600-9700`
-
-| `api.ssl.enabled`
-| Set to `true` to enable SSL on the HTTP API.
-  Doing so requires both `api.ssl.keystore.path` and `api.ssl.keystore.password` to be set.
-| `false`
-
-| `api.ssl.keystore.path`
-| The path to a valid JKS or PKCS12 keystore for use in securing the {ls} API.
-  The keystore must be password-protected, and must contain a single certificate chain and a private key.
-  This setting is ignored unless `api.ssl.enabled` is set to `true`.
-| _N/A_
-
-| `api.ssl.keystore.password`
-| The password to the keystore provided with `api.ssl.keystore.path`.
-  This setting is ignored unless `api.ssl.enabled` is set to `true`.
-| _N/A_
-
-| `api.auth.type`
-| Set to `basic` to require HTTP Basic auth on the API using the credentials supplied with `api.auth.basic.username` and `api.auth.basic.password`.
-| `none`
-
-| `api.auth.basic.username`
-| The username to require for HTTP Basic auth
-  Ignored unless `api.auth.type` is set to `basic`.
-| _N/A_
-
-| `api.auth.basic.password`
-a| The password to require for HTTP Basic auth. Ignored unless `api.auth.type` is set to `basic`.
-It should meet default password policy which requires non-empty minimum 8 char string that includes a digit, upper case letter and lower case letter.
-The default password policy can be customized by following options:
-
-* Set `api.auth.basic.password_policy.include.digit` `REQUIRED` (default) to accept only passwords that include at least one digit or `OPTIONAL` to exclude from requirement.
-* Set `api.auth.basic.password_policy.include.upper` `REQUIRED` (default) to accept only passwords that include at least one upper case letter or `OPTIONAL` to exclude from requirement.
-* Set `api.auth.basic.password_policy.include.lower` `REQUIRED` (default) to accept only passwords that include at least one lower case letter or `OPTIONAL` to exclude from requirement.
-* Set `api.auth.basic.password_policy.include.symbol` `REQUIRED` to accept only passwords that include at least one special character or `OPTIONAL` (default) to exclude from requirement.
-* Set `api.auth.basic.password_policy.length.minimum` to a value from 9 to 1024 if you want to require more than the 8 character default setting for passwords.
-
-| _N/A_
-
-| `api.auth.basic.password_policy.mode`
-| Raises either `WARN` or `ERROR` message when password requirements are not met.
-Ignored unless `api.auth.type` is set to `basic`.
-| `WARN`
-
-| `log.level`
-a|
-The log level. Valid options are:
-
-* `fatal`
-* `error`
-* `warn`
-* `info`
-* `debug`
-* `trace`
-
-| `info`
-
-| `log.format`
-| The log format. Set to `json` to log in JSON format, or `plain` to use `Object#.inspect`.
-| `plain`
-
-| `path.logs`
-| The directory where Logstash will write its log to.
-| `LOGSTASH_HOME/logs`
-
-| `pipeline.separate_logs`
-|  This a boolean setting to enable separation of logs per pipeline in different log files. If enabled Logstash will create a different log file for each pipeline,
-using the pipeline.id as name of the file. The destination directory is taken from the `path.log`s setting. When there are many pipelines configured in Logstash,
-separating each log lines per pipeline could be helpful in case you need to troubleshoot what’s happening in a single pipeline, without interference of the other ones.
-| `false`
-
-| `path.plugins`
-| Where to find custom plugins. You can specify this setting multiple times to include
-  multiple paths. Plugins are expected to be in a specific directory hierarchy:
-  `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`,
-  and `NAME` is the name of the plugin.
-| Platform-specific. See <<dir-layout>>.
-
-| `allow_superuser`
-| Setting to `true` to allow or `false` to block running Logstash as a superuser.
-| `true`
-|=======================================================================
+include::./settings/settings.asciidoc[]
\ No newline at end of file
diff --git a/docs/static/settings/configuration-management-settings.asciidoc b/docs/static/settings/configuration-management-settings.asciidoc
index 45142a94c6a..779c24385a8 100644
--- a/docs/static/settings/configuration-management-settings.asciidoc
+++ b/docs/static/settings/configuration-management-settings.asciidoc
@@ -1,14 +1,11 @@
-[role="xpack"]
 [[configuration-management-settings]]
-==== Configuration Management Settings in Logstash
+==== Configuration management settings in Logstash
 ++++
-<titleabbrev>Configuration Management Settings</titleabbrev>
+<titleabbrev>Configuration management settings</titleabbrev>
 ++++
 
-You can set the following `xpack.management` settings in `logstash.yml` to
-enable
+Use the `xpack.management` settings in `logstash.yml` to enable
 <<logstash-centralized-pipeline-management,centralized pipeline management>>.
-For more information about configuring Logstash, see <<logstash-settings-file>>.
 
 The following example shows basic settings that assume {es} and {kib} are
 installed on the localhost with basic AUTH enabled, but no SSL. If you're using
@@ -24,103 +21,4 @@ xpack.management.logstash.poll_interval: 5s
 xpack.management.pipeline.id: ["apache", "cloudwatch_logs"]
 -----
 
-
-`xpack.management.enabled`::
-
-Set to `true` to enable {xpack} centralized configuration management for
-Logstash.
-
-`xpack.management.logstash.poll_interval`::
-
-How often the Logstash instance polls for pipeline changes from Elasticsearch.
-The default is 5s.
-
-`xpack.management.pipeline.id`::
-
-Specify a comma-separated list of pipeline IDs to register for centralized
-pipeline management. After changing this setting, you need to restart Logstash
-to pick up changes.
-Pipeline IDs support `*` as a <<wildcard-in-pipeline-id, wildcard>> for matching multiple IDs
-
-`xpack.management.elasticsearch.hosts`::
-
-The {es} instance that will store the Logstash pipeline configurations and
-metadata. This might be the same {es} instance specified in the `outputs`
-section in your Logstash configuration, or a different one. Defaults to
-`http://localhost:9200`.
-
-`xpack.management.elasticsearch.username` and `xpack.management.elasticsearch.password`::
-
-If your {es} cluster is protected with basic authentication, these settings
-provide the username and password that the Logstash instance uses to
-authenticate for accessing the configuration data. The username you specify here
-should have the built-in `logstash_admin` role and the customized `logstash_writer` role, which provides access to system
-indices for managing configurations. Starting with Elasticsearch version 7.10.0, the
-`logstash_admin` role inherits the `manage_logstash_pipelines` cluster privilege for centralized pipeline management.
-If a user has created their own roles and granted them access to the .logstash index, those roles will continue to work in 7.x but will need to be updated for 8.0.
-
-`xpack.management.elasticsearch.proxy`::
-
-Optional setting that allows you to specify a proxy URL if Logstash needs to use a proxy
-to reach your Elasticsearch cluster.
-
-`xpack.management.elasticsearch.ssl.ca_trusted_fingerprint`::
-
-Optional setting that enables you to specify the hex-encoded SHA-256 fingerprint of the
-certificate authority for your {es} instance.
-[NOTE]
-=====
-A self-secured Elasticsearch cluster will provide the fingerprint of its CA to the console during setup.
-
-You can also get the SHA256 fingerprint of an Elasticsearch's CA using the `openssl` command-line utility on the Elasticsearch host:
-
-[source,shell]
---------------------------------------------------
-openssl x509 -fingerprint -sha256 -in $ES_HOME/config/certs/http_ca.crt
---------------------------------------------------
-=====
-
-`xpack.management.elasticsearch.ssl.certificate_authority`::
-
-Optional setting that enables you to specify a path to the `.pem` file for the
-certificate authority for your {es} instance.
-
-`xpack.management.elasticsearch.ssl.truststore.path`::
-
-Optional setting that provides the path to the Java keystore (JKS) to validate
-the server’s certificate.
-
-`xpack.management.elasticsearch.ssl.truststore.password`::
-
-Optional setting that provides the password to the truststore.
-
-`xpack.management.elasticsearch.ssl.keystore.path`::
-
-Optional setting that provides the path to the Java keystore (JKS) to validate
-the client’s certificate.
-
-`xpack.management.elasticsearch.ssl.keystore.password`::
-
-Optional setting that provides the password to the keystore.
-
-`xpack.management.elasticsearch.cloud_id`::
-
-If you're using {es} in {ecloud}, you should specify the identifier here.
-This setting is an alternative to `xpack.management.elasticsearch.hosts`.
-If `cloud_id` is configured, `xpack.management.elasticsearch.hosts` should not be used.
-This {es} instance will store the Logstash pipeline configurations and metadata.
-
-`xpack.management.elasticsearch.cloud_auth`::
-
-If you're using {es} in {ecloud}, you can set your auth credentials here.
-This setting is an alternative to both `xpack.management.elasticsearch.username`
-and `xpack.management.elasticsearch.password`. If `cloud_auth` is configured,
-those settings should not be used.
-The credentials you specify here should be for a user with the `logstash_admin` role, which
-provides access to system indices for managing configurations.
-
-`xpack.management.elasticsearch.api_key`::
-
-Authenticate using an Elasticsearch API key. Note that this option also requires using SSL.
-The API key Format is `id:api_key` where `id` and `api_key` are as returned by the Elasticsearch
-https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html[Create API key API].
+include::settings-management.asciidoc[]
diff --git a/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc b/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc
index 0304f7ba674..89929c9ceb9 100644
--- a/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc
+++ b/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc
@@ -1,4 +1,3 @@
-[role="xpack"]
 [[wildcard-in-pipeline-id]]
 ==== Wildcard support in pipeline ID
 ++++
diff --git a/docs/static/settings/settings-api.asciidoc b/docs/static/settings/settings-api.asciidoc
new file mode 100644
index 00000000000..8c92e55d7c4
--- /dev/null
+++ b/docs/static/settings/settings-api.asciidoc
@@ -0,0 +1,53 @@
+api.enabled::
+The HTTP API is enabled by default. It can be disabled, but features that rely on it will not work as intended. Default is `true`.
+
+api.environment:: 
+The API returns the provided string as a part of its response. Setting your environment may help to disambiguate between similarly-named nodes in production vs test environments. 
+Default is `production`.
+
+api.http.host::
+The bind address for the HTTP API endpoint. 
+By default, the {ls} HTTP API binds only to the local loopback interface.
+When configured securely (`api.ssl.enabled: true` and `api.auth.type: basic`), the HTTP API binds to _all_ available interfaces. 
+Default is `"127.0.0.1"`.
+
+api.http.port::
+The bind port for the HTTP API endpoint. 
+Default is `9600-9700`.
+
+api.ssl.enabled::
+Set to `true` to enable SSL on the HTTP API. 
+Doing so requires both `api.ssl.keystore.path` and `api.ssl.keystore.password` to be set. +
+Default is `false.`
+
+api.ssl.keystore.path::
+The path to a valid JKS or PKCS12 keystore for use in securing the {ls} API. 
+The keystore must be password-protected, and must contain a single certificate chain and a private key.  This setting is ignored unless `api.ssl.enabled` is set to `true`.
+
+api.ssl.keystore.password::
+The password to the keystore provided with `api.ssl.keystore.path`. 
+This setting is ignored unless `api.ssl.enabled` is set to `true`.
+
+api.auth.type::
+Set to `basic` to require HTTP Basic auth on the API using the credentials supplied with `api.auth.basic.username` and `api.auth.basic.password`. 
+Default is `none`.
+
+api.auth.basic.username::
+The username to require for HTTP Basic auth. 
+Ignored unless `api.auth.type` is set to `basic`.
+
+api.auth.basic.password::
+The password to require for HTTP Basic auth. Ignored unless `api.auth.type` is set to `basic`. 
+It should meet default password policy which requires non-empty minimum 8 char string that includes a digit, upper case letter and lower case letter. 
+You can customize the default password policy with these options:
+
+* Set `api.auth.basic.password_policy.include.digit` `REQUIRED` (default) to accept only passwords that include at least one digit or `OPTIONAL` to exclude from requirement.
+* Set `api.auth.basic.password_policy.include.upper` `REQUIRED` (default) to accept only passwords that include at least one upper case letter or `OPTIONAL` to exclude from requirement.
+* Set `api.auth.basic.password_policy.include.lower` `REQUIRED` (default) to accept only passwords that include at least one lower case letter or `OPTIONAL` to exclude from requirement.
+* Set `api.auth.basic.password_policy.include.symbol` `REQUIRED` to accept only passwords that include at least one special character or `OPTIONAL` (default) to exclude from requirement.
+* Set `api.auth.basic.password_policy.length.minimum` to a value from 9 to 1024 if you want to require more than the 8 character default setting for passwords.
+
+api.auth.basic.password_policy.mode:: 
+Raises either `WARN` or `ERROR` message when password requirements are not met. 
+Ignored unless `api.auth.type` is set to `basic`. Default is  `WARN`.
+
diff --git a/docs/static/settings/settings-debug.asciidoc b/docs/static/settings/settings-debug.asciidoc
new file mode 100644
index 00000000000..abc5615cf7c
--- /dev/null
+++ b/docs/static/settings/settings-debug.asciidoc
@@ -0,0 +1,10 @@
+`log-level`::
+Options for log.level are `fatal`, `error`, `warn`, `info`, `debug`, `trace`. Default is  `info`.
+
+`pathlogs`::
+The directory where Logstash writes its logs.
+Default is `LOGSTASH_HOME/logs`.
+
+`logformat`::
+The format to use for outputting logs. Set to `json` to log in JSON format, or `plain` to use `Object#.inspect`. +
+Default is `plain`.
diff --git a/docs/static/settings/settings-dlq.asciidoc b/docs/static/settings/settings-dlq.asciidoc
new file mode 100644
index 00000000000..ad40a7a80e9
--- /dev/null
+++ b/docs/static/settings/settings-dlq.asciidoc
@@ -0,0 +1,30 @@
+These settings apply when `dead_letter_queue.enable: true`. 
+
+`dead_letter_queue.enable`::
+Flag to turn on dead letter queue (DLQ) supported by plugins. Default is `false`. 
+
+`dead_letter_queue.max_bytes`::
+The maximum size of each dead letter queue (DLQ). 
+Entries will be dropped if they would increase the size of the dead letter queue beyond this value. Default is `1024mb`.
+
+`dead_letter_queue.flush_interval`::
+The interval in milliseconds a dead letter queue file will be written if no additional DLQ events have been created. 
+A low value means that more, smaller queue files may be written. 
+A larger value introduces more latency between items being "written" to the dead letter queue and available to be read by the dead_letter_queue input.
+Default is 5000. 
+
+`dead_letter_queue.storage_policy`::
+Defines the action to take when the dead_letter_queue.max_bytes is reached.
+Options are "drop_newer" (default) or "drop_older". 
+With drop_newer, messages that were inserted most recently are dropped, logging an error line. With drop_older setting, the oldest messages are dropped as new ones are inserted. 
+Default value is "drop_newer". 
+
+`path.dead_letter_queue`::
+Sets the directory path where the data files will be stored. 
+Default is `path.data/dead_letter_queue`.
+
+`dead_letter_queue.retain.age`::
+Sets the retention period for events in the DLQ. 
+Use this setting to have {ls} remove events that are older than a value you define.
+Available time units are `d`, `h`, `m`, `s` respectively for days, hours, minutes and seconds.
+
diff --git a/docs/static/settings/settings-general.asciidoc b/docs/static/settings/settings-general.asciidoc
new file mode 100644
index 00000000000..a88585d0152
--- /dev/null
+++ b/docs/static/settings/settings-general.asciidoc
@@ -0,0 +1,17 @@
+`allow_superuser`::
+Allow or block running Logstash as superuser. Default: `true`.
+
+`node.name`::
+A descriptive name for the node. Default is machine's hostname.
+
+`path.data`::
+The directory that Logstash and its plugins use for any persistent needs, such as persistent queue (PQ) data or dead letter queue (DLQ) data. Default is `LOGSTASH_HOME/data`.
+
+`path.plugins`::
+File location for custom plugins. 
+You can specify this setting multiple times to include multiple paths. 
+Plugins are expected to be in a specific directory hierarchy: `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`, and `NAME` is the name of the plugin. 
+This setting is platform-specific. See <<dir-layout>> for more info.
+
+`modules`::
+When configured, modules must be in the nested YAML structure.
diff --git a/docs/static/settings/settings-management.asciidoc b/docs/static/settings/settings-management.asciidoc
new file mode 100644
index 00000000000..4dd369b65a8
--- /dev/null
+++ b/docs/static/settings/settings-management.asciidoc
@@ -0,0 +1,70 @@
+
+`xpack.management.enabled`::
+Set to `true` to enable {xpack} centralized configuration management for
+Logstash.
+
+`xpack.management.logstash.poll_interval`::
+How often the Logstash instance polls for pipeline changes from Elasticsearch.
+The default is 5s.
+
+`xpack.management.pipeline.id`::
+Specify a comma-separated list of pipeline IDs to register for centralized
+pipeline management. After changing this setting, you need to restart Logstash
+to pick up changes.
+Pipeline IDs support `*` as a <<wildcard-in-pipeline-id, wildcard>> for matching multiple IDs
+
+`xpack.management.elasticsearch.hosts`::
+The {es} instance that will store the Logstash pipeline configurations and
+metadata. This might be the same {es} instance specified in the `outputs`
+section in your Logstash configuration, or a different one. Defaults to
+`http://localhost:9200`.
+
+`xpack.management.elasticsearch.username` and `xpack.management.elasticsearch.password`::
+If your {es} cluster is protected with basic authentication, these settings
+provide the username and password that the Logstash instance uses to
+authenticate for accessing the configuration data. The username you specify here
+should have the built-in `logstash_admin` role and the customized `logstash_writer` role, which provides access to system
+indices for managing configurations. Starting with Elasticsearch version 7.10.0, the
+`logstash_admin` role inherits the `manage_logstash_pipelines` cluster privilege for centralized pipeline management.
+If a user has created their own roles and granted them access to the .logstash index, those roles will continue to work in 7.x but will need to be updated for 8.0.
+
+`xpack.management.elasticsearch.proxy`::
+Optional setting that allows you to specify a proxy URL if Logstash needs to use a proxy
+to reach your Elasticsearch cluster.
+
+`xpack.management.elasticsearch.ssl.certificate_authority`::
+Optional setting that enables you to specify a path to the `.pem` file for the
+certificate authority for your {es} instance.
+
+`xpack.management.elasticsearch.ssl.truststore.path`::
+Optional setting that provides the path to the Java keystore (JKS) to validate
+the server’s certificate.
+
+`xpack.management.elasticsearch.ssl.truststore.password`::
+Optional setting that provides the password to the truststore.
+
+`xpack.management.elasticsearch.ssl.keystore.path`::
+Optional setting that provides the path to the Java keystore (JKS) to validate
+the client’s certificate.
+
+`xpack.management.elasticsearch.ssl.keystore.password`::
+Optional setting that provides the password to the keystore.
+
+`xpack.management.elasticsearch.cloud_id`::
+If you're using {es} in {ecloud}, you should specify the identifier here.
+This setting is an alternative to `xpack.management.elasticsearch.hosts`.
+If `cloud_id` is configured, `xpack.management.elasticsearch.hosts` should not be used.
+This {es} instance will store the Logstash pipeline configurations and metadata.
+
+`xpack.management.elasticsearch.cloud_auth`::
+If you're using {es} in {ecloud}, you can set your auth credentials here.
+This setting is an alternative to both `xpack.management.elasticsearch.username`
+and `xpack.management.elasticsearch.password`. If `cloud_auth` is configured,
+those settings should not be used.
+The credentials you specify here should be for a user with the `logstash_admin` role, which
+provides access to system indices for managing configurations.
+
+`xpack.management.elasticsearch.api_key`::
+Authenticate using an Elasticsearch API key. Note that this option also requires using SSL.
+The API key Format is `id:api_key` where `id` and `api_key` are as returned by the Elasticsearch
+https://www.elastic.co/guide/en/elasticsearch/reference/current/security-api-create-api-key.html[Create API key API].
diff --git a/docs/static/settings/settings-pipeline-config.asciidoc b/docs/static/settings/settings-pipeline-config.asciidoc
new file mode 100644
index 00000000000..e4decdb9f72
--- /dev/null
+++ b/docs/static/settings/settings-pipeline-config.asciidoc
@@ -0,0 +1,46 @@
+`path.config`::
+The path to the Logstash pipeline configuration for the main pipeline. 
+If you specify a directory or wildcard, config files are read from the directory in alphabetical order. 
+This setting is platform-specific. See <<dir-layout>> for more info.
+ 
+`config.string`::
+A string that contains the pipeline configuration for the main pipeline. Use the same syntax as the config file.
+
+`config.test_and_exit`::
+When set to `true`, at startup checks to ensure that the configuration is valid and then exits. Logstash can read multiple config files from a directory. 
+Note that grok patterns are not checked for correctness. 
+If you combine this setting with `log.level: debug`, Logstash logs the combined config file, annotating each config block with the source file it came from. 
+Default is `false`. 
+
+`config.reload.automatic`::
+Periodically checks to see if the pipeline configuration has changed and reloads the pipeline after changes. 
+This can also be triggered manually through the SIGHUP signal. Default is `false`. 
+
+`config.reload.interval`::
+How often to check if the pipeline configuration has changed (in seconds). 
+Note that the unit value (s) is required. Values without a qualifier (such as 60) are treated as nanoseconds. Default is 3s. 
+IMPORTANT: Setting the interval this way is not recommended and might change in later versions. 
+
+`config.debug`::
+When set to `true`, shows the fully compiled configuration as a debug log message. 
+You must also set log.level: debug. Default is `false`. 
+WARNING: The log message will include any password options passed to plugin configs as plaintext, and may result in plaintext passwords appearing in your logs.
+
+`config.support_escapes`::
+When enabled, process escaped characters such as \n and \" in strings in the pipeline configuration files. Default is `false`. 
+When set to `true`, quoted strings process these escape sequences: 
+`\n` becomes a literal newline (ASCII 10).
+`\r` becomes a literal carriage return (ASCII 13).
+`\t` becomes a literal tab (ASCII 9).
+`\\` becomes a literal backslash (`\`).  
+`\"` becomes a literal double quotation mark.
+`\'` becomes a literal single quotation mark.
+
+`config.field_reference.escape_style`::
+[Technical preview]. Provides a way to reference fields that contain <<formal-grammar-escape-sequences,field reference special characters>> `[` and `]`. 
+Current options are:   
+* `percent`: URI-style `%`+`HH` hexadecimal encoding of UTF-8 bytes (`[` -> `%5B`; `]` -> `%5D`). 
+* `ampersand`: HTML-style `&#`+`DD`+`;` encoding of decimal Unicode code-points (`[` -> `&#91;`; `]` -> `&#93;`). 
+* `none`: field names containing special characters _cannot_ be referenced. 
+
+NOTE: This feature is in *technical preview* and may change in the future.
diff --git a/docs/static/settings/settings-pipeline.asciidoc b/docs/static/settings/settings-pipeline.asciidoc
new file mode 100644
index 00000000000..703984d7b5d
--- /dev/null
+++ b/docs/static/settings/settings-pipeline.asciidoc
@@ -0,0 +1,43 @@
+`pipeline.id`::
+The ID of the pipeline. Default: `main`.
+
+`pipeline.workers`::
+Set the number of workers that will, in parallel, execute the filters+outputs stage of the pipeline. Defaults to the number of the host's CPU cores. 
+If you have modified this setting and see that events are backing up, or that the CPU is not saturated, consider increasing this number to better utilize machine processing power. 
+
+`pipeline.batch.size`::
+The maximum number of events an individual worker thread should collect from inputs before attempting to execute its filters and outputs. 
+Larger batch sizes are generally more efficient, but come at the cost of increased memory overhead. 
+You may need to increase JVM heap space in the `jvm.options` config file. 
+See <<config-setting-files>> for more info. Default is 125.
+
+`pipeline.batch.delay`::
+How long to wait in milliseconds while polling for the next event before dispatching an undersized batch to filters+outputs. Default is 50. 
+
+`pipeline.unsafe_shutdown`::
+When set to `true`, forces Logstash to exit during shutdown, even if there are still inflight events in memory. 
+By default, Logstash refuses to quit until all received events have been pushed to the outputs. 
+Default is `false`.
+WARNING: Enabling this setting can lead to data loss during shutdown. 
+
+`pipeline.plugin_classloaders`::
+(Beta) Load Java plugins in independent classloaders to isolate their dependencies. Default is `false`. 
+
+`pipeline.ordered`::
+Set the pipeline event ordering. Options are "auto" (default), "true", "false". 
+Setting this value to "auto" automatically enables ordering if the 'pipeline.workers' setting is also set to '1', and disables otherwise. 
+Setting this value to "true" enforces ordering on the pipeline and prevent logstash from starting if there are multiple workers. 
+Setting this value to "false" disables any extra processing necessary for preserving ordering.
+
+`pipeline.separate_logs`::
+Flag to output log lines of each pipeline to a separate log file. If enabled, Logstash creates a different log file for each pipeline, using the pipeline.id as name of the file. 
+When you have many Logstash pipelines, separating each log by pipeline could be helpful when you need to troubleshoot a single pipeline.
+Default is `false`.
+
+`pipeline.ecs_compatibility`::
+Sets the pipeline's default value for `ecs_compatibility`, a setting that is available to plugins that implement an ECS Compatibility mode for use with the Elastic Common Schema. 
+Possible values are `disabled`, `v1`, `v8` (default). 
+Pipelines defined before Logstash 8 operate without ECS in mind. 
+To ensure a migrated pipeline continues to operate as it did before your upgrade, opt-OUT of ECS for the individual pipeline in its `pipelines.yml` definition. 
+Setting this value sets the default for _all_ pipelines, including new ones.
+
diff --git a/docs/static/settings/settings-pq.asciidoc b/docs/static/settings/settings-pq.asciidoc
new file mode 100644
index 00000000000..92f82f3f0cd
--- /dev/null
+++ b/docs/static/settings/settings-pq.asciidoc
@@ -0,0 +1,42 @@
+NOTE: These settings apply only if `queue.type: persisted`.
+ 
+`path.queue`::
+The directory path where the data files will be stored when persistent queues are enabled (`queue.type: persisted`). Default: `path.data/queue`. 
+
+`queue.page_capacity`::
+The queue data consists of append-only data files separated into "pages. 
+This option sets the maximum size of the page data files in bytes (`queue.type: persisted`). 
+The default size of 64mb is a good value for most users, and changing this value is unlikely to have performance benefits. 
+If you change the page capacity of an existing queue, the new size applies only to the new page. Default is `64mb`.
+
+`queue.drain`::
+When set to `true`, Logstash waits until the persistent queue is drained before shutting down. 
+The amount of time it takes to drain the queue depends on the number of events that have accumulated in the queue. 
+Tip: Avoid using this setting unless the queue, even when full, is relatively small and can be drained quickly. Default is `false`.
+
+`queue.max_events`::
+The maximum number of events not yet read by the pipeline worker (`queue.type: persisted`). The default is `0` (unlimited). 
+We use this setting for internal testing. 
+Users generally shouldn't change this value.
+
+`queue.max_bytes`::
+The total capacity of each queue in number of bytes (`queue.type: persisted`). Unless overridden in `pipelines.yml` or central management, each persistent queue will be sized at the value of `queue.max_bytes` specified in `logstash.yml`. 
+Be sure that your disk has sufficient capacity to handle the cumulative total of `queue.max_bytes` across all persistent queues. The total of `queue.max_bytes` for ALL queues should be lower than the capacity of your disk. 
+If both `queue.max_events` and `queue.max_bytes` are specified, Logstash uses whichever criteria is reached first. Default: 1024mb (1gb).
+
+`queue.checkpoint.acks`::
+Sets the maximum number of acked events before forcing a checkpoint (`queue.type: persisted`). Default is 1024. Specify `queue.checkpoint.acks: 0` to set this value to unlimited.
+    
+`queue.checkpoint.writes`::
+Sets the maximum number of written events before a forced checkpoint (`queue.type: persisted`). Default is 1024. Specify `queue.checkpoint.writes: 0` to set this value to unlimited. 
+To avoid losing data in the persistent queue, you can set `queue.checkpoint.writes: 1` to force a checkpoint after each event is written. +
+ Keep in mind that disk writes have a resource cost. Setting this value to 1 ensures maximum durability, but can severely impact performance. See <<durability-persistent-queues>> to better understand the trade-offs.
+
+`queue.checkpoint.retry`::
+When enabled, Logstash will retry four times per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. 
+This is a workaround for failed checkpoint writes that have been seen only on Windows platform, filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. (queue.type: persisted)
+Default is `true`.
+
+`queue.checkpoint.interval`::
+Sets the interval in milliseconds when a checkpoint is forced on the head page (`queue.type: persisted`). Default is 1000. Set to 0 to eliminate periodic checkpoints. 
+
diff --git a/docs/static/settings/settings.asciidoc b/docs/static/settings/settings.asciidoc
new file mode 100644
index 00000000000..db9a24907aa
--- /dev/null
+++ b/docs/static/settings/settings.asciidoc
@@ -0,0 +1,53 @@
+[[logstash-yml-settings]]
+=== Logstash settings
+
+The `logstash.yml` file, located in the Logstash `/config/` directory includes these settings. 
+
+
+[[settings-general]]
+==== Set up and general settings
+
+include::settings-general.asciidoc[]
+
+[[settings-queue]]
+==== Queue settings
+
+`queue.type`:: The internal queuing model to use for event buffering.  
+Specify `persisted` to enable <<persistent-queues,persistent queues>> for disk-based ACKed queueing, or `memory` for legacy <<memory-queue,in-memory based queueing>>. 
+Default is `memory`.
+
+
+[[settings-pq]]
+===== Persisted queue (PQ) settings
+
+include::settings-pq.asciidoc[]
+
+
+[[settings-dlq]]
+===== Dead letter queue (DLQ) settings
+
+include::settings-dlq.asciidoc[]
+
+
+[[settings-pipeline]]
+==== Pipeline settings
+
+include::settings-pipeline.asciidoc[]
+
+
+[[settings-pipeline-config]]
+==== Pipeline configuration settings
+
+include::settings-pipeline-config.asciidoc[]
+
+
+[[settings-api]]
+===== API settings
+
+include::settings-api.asciidoc[]
+
+
+[[settings-debug]]
+===== Debug settings
+
+include::settings-debug.asciidoc[]
