diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index 07920e78861..756eb1fb0ac 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -3,36 +3,64 @@
 
 This section discusses the changes that you need to be aware of when migrating your application to Logstash {version}.
 
-**Application Settings:** Introduced a new way to configure application settings for Logstash through a settings.yml file. This file
+[float]
+=== Changes in Logstash Core
+
+* **Application Settings:** Introduced a new way to configure application settings for Logstash through a settings.yml file. This file
 is typically located in `LS_HOME/config`, or `/etc/logstash` when installed via packages. Logstash will not be able
 to start without this file, so please make sure to pass in `--path.settings` if you are starting Logstash manually
 after installing it via a package (RPM, DEB).
 
-**Release Packages:** When Logstash is installed via DEB, RPM packages, it uses `/usr/share/logstash` and `/var/lib/logstash` to install binaries and config files
-respectively. Previously it used to install in `/opt` directory. This change was done to make the user experience
+* **Release Packages:** When Logstash is installed via DEB, RPM packages, it uses `/usr/share/logstash` and `/var/lib/logstash` to install binaries. 
+Previously it used to install in `/opt/logstash` directory. This change was done to make the user experience
 consistent with other Elastic products. Full directory layout is described in <<dir-layout>>.
 
-**Command Line Interface:** Most of the long form <<command-line-flags,options>> have been renamed
+* **Default Logging Level:** Changed the default log severity level to INFO instead of WARN to match Elasticsearch. Existing logs
+(in core and plugins) were too noisy at INFO level, so we had to audit log messages and switch some of them to DEBUG
+level.
+
+* **Command Line Interface:** Most of the long form <<command-line-flags,options>> have been renamed
 to adhere to the yml dot notation to be used in the settings file. Short form options have not been changed.
 
-**Plugin Manager Renamed:** `bin/plugin` has been renamed to `bin/logstash-plugin`. This change was to mainly prevent `PATH` being polluted when
+* **Plugin Manager Renamed:** `bin/plugin` has been renamed to `bin/logstash-plugin`. This change was to mainly prevent `PATH` being polluted when
 other components of the Elastic stack are installed on the same instance. Also, this provides a foundation
 for future change which will allow Elastic Stack packs to be installed via this script.
 
-**Kafka Input/Output Configuration Changes:** This release added support for the new 0.9 consumer/producer API which supports security features introduced by Kafka.
+[float]
+=== Breaking Changes in Plugins
+
+* **Elasticsearch Output Index Template:** The index template for 5.0 has been changed to reflect {ref}breaking_50_mapping_changes.html[Elasticsearch's mapping changes]. Most
+importantly, the subfield for string multi-fields has changed from `.raw` to `.keyword` to match Elasticsearch's default
+behavior. The impact of this change to various user groups is detailed below:
+
+** New Logstash 5.0 and Elasticsearch 5.0 users - subfields use `.keyword` from the outset. In Kibana, you can use
+`field.keyword` to perform aggregations.
+** Existing users with custom templates - most of you won't be impacted because you use a custom template.
+** Existing users with default template - Logstash does not force you to upgrade templates if one already exists. If you
+intend to move to the new template and want to use `.keyword`, you'll have to reindex existing data. Elasticsearch's
+ {ref}docs-reindex.html[reindexing API] can help move your data from using `.raw` subfields to `.keyword`.
+
+* **Kafka Input/Output Configuration Changes:** This release added support for the new 0.10 consumer/producer API which supports security features introduced by Kafka.
 A few Configuration options were renamed to make it consistent with Kafka consumer and producer settings.
 Also, this plugin version will not work with Kafka 0.8 broker.
 
 Please see the following specific plugin documentation for new configuration options:
 
-* <<plugins-inputs-kafka,Kafka Input>>
+* <<plugins-inputs-kafka, Kafka Input>>
 * <<plugins-outputs-kafka, Kafka Output>>
 
-**Ruby Filter and Custom Plugin Developers:** With the migration to the Java Event (https://github.com/elastic/logstash/issues/4191[Issue 4191]), we have changed
-how you can access internal data. The Event object no longer returns a reference to the data. Instead, it returns a
-copy. This might change how you do manipulation of your data, especially when working with nested hashes.
-When working with nested hashes, it’s recommended that you use the `fieldref` syntax instead of using multiple brackets.
-Also note that we have introduced new Getter/Setter APIs for accessing information in the Event object.
+* **File Input:** SinceDB file is now saved in `<path.data>/plugins/inputs/file` location, not user's home. If you have manually specified `sincedb_path` 
+configuration, this change will not affect you. If you are moving from 2.x to 5.x, and would like to use the existing SinceDB file, it 
+has to be copied over to `path.data` manually to use the save state.
+
+[float]
+=== Ruby Filter and Custom Plugin Developers
+
+With the migration to the new <<event-api>>, we have changed how you can access internal data compared to previous release. 
+The Event object no longer returns a reference to the data. Instead, it returns a copy. This might change how you do manipulation of 
+your data, especially when working with nested hashes. When working with nested hashes, it’s recommended that you 
+use the `fieldref` syntax instead of using multiple brackets. Also note that we have introduced new Getter/Setter APIs
+for accessing information in the Event object. Refer <<event-api>> for details.
 
 **Examples:**
 
@@ -40,17 +68,21 @@ Also note that we have introduced new Getter/Setter APIs for accessing informati
 ----------------------------------
 filter {
   ruby {
-    codec => "event.set('uuid', event.get('uuid').gsub(/b/, ''))" # instead of using event['uuid'].gsub!(/b/, '')
+    codec => "event.set('[product][price]', 10)"
   }
 }
 ----------------------------------
 
+Instead of:
+
 [source, js]
 ----------------------------------
 filter {
   ruby {
-    codec => "event.set('[product][price]', 10)" # instead of using event['product']['price'] = 10
+    codec => "event['product']['price'] = 10"
   }
 }
 ----------------------------------
 
+The above syntax is not supported, and will produce an error at run-time.
+
diff --git a/docs/static/event-api.asciidoc b/docs/static/event-api.asciidoc
new file mode 100644
index 00000000000..f55841a1c8f
--- /dev/null
+++ b/docs/static/event-api.asciidoc
@@ -0,0 +1,120 @@
+[[event-api]]
+== Event API
+
+This section is targeted for plugin developers and users of Logstash's Ruby filter. Below we document recent 
+changes (starting with version 5.0) in the way users have been accessing Logstash's event based data in 
+custom plugins and in the Ruby filter. Note that <<event-dependent-configuration>> 
+data flow in Logstash's config files -- using <<logstash-config-field-references>> -- is 
+not affected by this change, and will continue to use existing syntax.
+
+[float]
+=== Event Object
+
+Event is the main object that encapsulates data flow internally in Logstash and provides an API for the plugin 
+developers to interact with the event's content. Typically, this API is used in plugins and in a Ruby filter to 
+retrieve data and use it for transformations. Event object contains the original data sent to Logstash and any additional 
+fields created during Logstash's filter stages.
+
+In 5.0, we've re-implemented the Event class and its supporting classes in pure Java. Since Event is a critical component 
+in data processing,  a rewrite in Java improves performance and provides efficient serialization when storing data on disk. For the most part, this change aims at keeping backward compatibility and is transparent to the users. To this extent we've updated and published most of the plugins in Logstash's ecosystem to adhere to the new API changes. However, if you are maintaining a custom plugin, or have a Ruby filter, this change will affect you. The aim of this guide is to describe the new API and provide examples to migrate to the new changes.
+
+[float]
+==== Event API
+
+Prior to version 5.0, developers could access and manipulate event data by directly using Ruby hash syntax. For 
+example, `event[field] = foo`. While this is powerful, our goal is to abstract the internal implementation details 
+and provide well-defined getter and setter APIs.
+
+**Get API**
+
+The getter is a read-only access of field-based data in an Event.
+
+**Syntax:** `event.get(field)`
+
+**Returns:** Value for this field or nil if the field does not exist. Returned values could be a string, 
+numeric or timestamp scalar value.
+
+`field` is a structured field sent to Logstash or created after the transformation process. `field` can also 
+be a nested field reference such as `[field][bar]`.
+
+Examples:
+
+[source,ruby]
+--------------------------------------------------
+event.get("foo" ) # => "baz"
+event.get("[foo]") # => "zab"
+event.get("[foo][bar]") # => 1
+event.get("[foo][bar]") # => 1.0
+event.get("[foo][bar]") # =>  [1, 2, 3]
+event.get("[foo][bar]") # => {"a" => 1, "b" => 2}
+event.get("[foo][bar]") # =>  {"a" => 1, "b" => 2, "c" => [1, 2]}
+--------------------------------------------------
+
+Accessing @metdata
+
+[source,ruby]
+--------------------------------------------------
+event.get("[@metadata][foo]") # => "baz"
+--------------------------------------------------
+
+**Set API**
+
+This API can be used to mutate data in an Event. 
+
+**Syntax:** `event.set(field, value)`
+
+**Returns:**  The current Event  after the mutation, which can be used for chainable calls.
+
+Examples:
+
+[source,ruby]
+--------------------------------------------------
+event.set("foo", "baz")
+event.set("[foo]", "zab")
+event.set("[foo][bar]", 1)
+event.set("[foo][bar]", 1.0)
+event.set("[foo][bar]", [1, 2, 3])
+event.set("[foo][bar]", {"a" => 1, "b" => 2})
+event.set("[foo][bar]", {"a" => 1, "b" => 2, "c" => [1, 2]})
+event.set("[@metadata][foo]", "baz")
+--------------------------------------------------
+
+Mutating a collection after setting it in the Event has an undefined behaviour and is not allowed.
+
+[source,ruby]
+--------------------------------------------------
+h = {"a" => 1, "b" => 2, "c" => [1, 2]}
+event.set("[foo][bar]", h)
+
+h["c"] = [3, 4]
+event.get("[foo][bar][c]") # => undefined
+
+Suggested way of mutating collections:
+
+h = {"a" => 1, "b" => 2, "c" => [1, 2]}
+event.set("[foo][bar]", h)
+
+h["c"] = [3, 4]
+event.set("[foo][bar]", h)
+
+# Alternatively,
+event.set("[foo][bar][c]", [3, 4]) 
+--------------------------------------------------
+
+[float]
+=== Ruby Filter
+
+The <<plugins-filters-ruby,Ruby Filter>> can be used to execute any ruby code and manipulate event data using the 
+API described above. For example, using the new API:
+
+[source,ruby]
+--------------------------------------------------
+filter {
+  ruby {
+    code => 'event.set("lowercase_field", event.get("message").downcase)'
+  }  
+}    
+--------------------------------------------------
+
+This filter will lowercase the `message` field, and set it to a new field called `lowercase_field`
+
