diff --git a/tools/ingest-converter/build.gradle b/tools/ingest-converter/build.gradle
index b30a0e7c51b..1e96991161f 100644
--- a/tools/ingest-converter/build.gradle
+++ b/tools/ingest-converter/build.gradle
@@ -43,6 +43,7 @@ buildscript {
 
 dependencies {
   implementation 'net.sf.jopt-simple:jopt-simple:4.6'
+  implementation 'com.fasterxml.jackson.core:jackson-databind:2.11.3'
   testImplementation "junit:junit:4.12"
   testImplementation 'commons-io:commons-io:2.5'
 }
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestAppend.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestAppend.java
new file mode 100644
index 00000000000..eee5d8dfbdd
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestAppend.java
@@ -0,0 +1,72 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestAppend {
+
+    /**
+     * Converts Ingest Append JSON to LS mutate filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestAppend::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("mutate", appendHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String appendHash(Map<String, Map> processor) {
+        Map append_json = processor.get("append");
+        Object value = append_json.get("value");
+        Object value_contents;
+        if (value instanceof List) {
+            value_contents = IngestConverter.createArray((List) value);
+        } else {
+            value_contents = IngestConverter.quoteString((String) value);
+        }
+        Object mutate_contents = IngestConverter.createField(
+                IngestConverter.quoteString(IngestConverter.dotsToSquareBrackets((String) append_json.get("field"))),
+                (String) value_contents);
+        return IngestConverter.createField("add_field", IngestConverter.wrapInCurly((String) mutate_contents));
+    }
+
+    public static boolean has_append(Map<String, Object> processor) {
+        return processor.containsKey("append");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestConvert.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestConvert.java
new file mode 100644
index 00000000000..6c1128264ac
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestConvert.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestConvert {
+
+    /**
+     * Converts Ingest Convert JSON to LS Date filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestConvert::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("mutate", convertHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String convertHash(Map<String, Map> processor) {
+        Map convert_json = processor.get("convert");
+
+        Object mutate_contents = IngestConverter.createField(
+                IngestConverter.quoteString(IngestConverter.dotsToSquareBrackets((String) convert_json.get("field"))),
+                IngestConverter.quoteString((String) convert_json.get("type")));
+        return IngestConverter.createField("convert", IngestConverter.wrapInCurly((String) mutate_contents));
+    }
+
+    public static boolean has_convert(Map<String, Object> processor) {
+        return processor.containsKey("convert");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestConverter.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestConverter.java
new file mode 100644
index 00000000000..16bc91e3c3f
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestConverter.java
@@ -0,0 +1,231 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+package org.logstash.ingest;
+
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.List;
+import java.util.Map;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+import java.util.stream.Collectors;
+
+public class IngestConverter {
+
+    /**
+     * Translates the JSON naming pattern (`name.qualifier.sub`) into the LS pattern
+     * [name][qualifier][sub] for all applicable tokens in the given string.
+     * This function correctly identifies and omits renaming of string literals.
+     * @param content to replace naming pattern in
+     * @returns {string} with Json naming translated into grok naming
+     */
+    public static String dotsToSquareBrackets(String content) {
+        final Pattern pattern = Pattern.compile("\\(\\?:%\\{.*\\|-\\)");
+        final Matcher matcher = pattern.matcher(content);
+        List<String> tokens = new ArrayList<>();
+        String right = content;
+        while (matcher.find()) {
+            final int start = matcher.start();
+            final int end = matcher.end();
+            final String matchContent = content.substring(start, end);
+            right = content.substring(end);
+            tokens.add(tokenDotsToSquareBrackets(content.substring(0, start)));
+            tokens.add(matchContent);
+        }
+        tokens.add(tokenDotsToSquareBrackets(right));
+        return String.join("", tokens);
+    }
+
+    private static String tokenDotsToSquareBrackets(String content) {
+        //Break out if this is not a naming pattern we convert
+        final String adjusted;
+        if (Pattern.compile("([\\w_]+\\.)+[\\w_]+").matcher(content).find()) {
+            adjusted = content.replaceAll("(\\w*)\\.(\\w*)", "$1][$2")
+                    .replaceAll("\\[(\\w+)(}|$)", "[$1]$2")
+                    .replaceAll("\\{(\\w+):(\\w+)]", "{$1:[$2]")
+                    .replaceAll("^(\\w+)]\\[", "[$1][");
+        } else {
+            adjusted = content;
+        }
+        return adjusted;
+    }
+
+    public static String quoteString(String content) {
+        return "\"" + content.replace("\"", "\\\"") + "\"";
+    }
+
+    public static String wrapInCurly(String content) {
+        return "{\n" + content + "\n}";
+    }
+
+    public static String createField(String fieldName, String content) {
+        return fieldName + " => " + content;
+    }
+
+    public static String createHash(String fieldName, String content) {
+        return fieldName + " " + wrapInCurly(content);
+    }
+
+    /**
+     * All hash fields in LS start on a new line.
+     * @param fields Array of Strings of Serialized Hash Fields
+     * @returns {string} Joined Serialization of Hash Fields
+     */
+    public static String joinHashFields(String... fields) {
+        return String.join("\n", fields);
+    }
+
+    /**
+     * Fixes indentation in LS string.
+     * @param content LS string to fix indentation in, that has no indentation intentionally with
+     * all lines starting on a token without preceding spaces.
+     * @return LS string indented by 3 spaces per level
+     */
+    public static String fixIndent(String content) {
+        final String[] lines = content.split("\n");
+        int count = 0;
+        for (int i = 0; i < lines.length; i++) {
+            if (Pattern.compile("(\\{|\\[)$").matcher(lines[i]).find()) {
+                lines[i] = indent(lines[i], count);
+                ++count;
+            } else if (Pattern.compile("(\\}|\\])$").matcher(lines[i]).find()) {
+                --count;
+                lines[i] = indent(lines[i], count);
+                // Only indent line if previous line ended on relevant control char.
+            } else if (i > 0 && Pattern.compile("(=>\\s+\".+\"|,|\\{|\\}|\\[|\\])$").matcher(lines[i - 1]).find()) {
+                lines[i] = indent(lines[i], count);
+            }
+        }
+
+        return String.join("\n", lines);
+    }
+
+    private static String indent(String content, int shifts) {
+        StringBuilder spacing = new StringBuilder();
+        for (int i = 0; i < shifts * 3; i++) {
+            spacing.append(" ");
+        }
+        return spacing.append(content).toString();
+    }
+
+    /**
+     * Converts Ingest/JSON style pattern array to LS pattern array, performing necessary variable
+     * name and quote escaping adjustments.
+     * @param patterns Pattern Array in JSON formatting
+     * @return Pattern array in LS formatting
+     */
+    public static String createPatternArray(String... patterns) {
+        final String body = Arrays.stream(patterns)
+                .map(IngestConverter::dotsToSquareBrackets)
+                .map(IngestConverter::quoteString)
+                .collect(Collectors.joining(",\n"));
+        return "[\n" + body + "\n]";
+    }
+
+    public static String createArray(List<String> ingestArray) {
+        final String body = ingestArray.stream()
+                .map(IngestConverter::quoteString)
+                .collect(Collectors.joining(",\n"));
+        return "[\n" + body + "\n]";
+    }
+
+
+    /**
+     * Converts Ingest/JSON style pattern array to LS pattern array or string if the given array
+     * contains a single element only, performing necessary variable name and quote escaping
+     * adjustments.
+     * @param patterns Pattern Array in JSON formatting
+     * @return Pattern array or string in LS formatting
+     */
+    public static String createPatternArrayOrField(String... patterns) {
+        return patterns.length == 1
+                ? quoteString(dotsToSquareBrackets(patterns[0]))
+                : createPatternArray(patterns);
+    }
+
+    public static String filterHash(String contents) {
+        return fixIndent(createHash("filter", contents));
+    }
+
+    public static String filtersToFile(String... filters) {
+        return String.join("\n\n", filters) + "\n";
+    }
+
+    /**
+     * Does it have an on_failure field?
+     * @param processor Json
+     * @param name Name of the processor
+     * @return true if has on failure
+     */
+    @SuppressWarnings("rawtypes")
+    public static boolean hasOnFailure(Map<String, Map> processor, String name) {
+        final List onFailure = (List) processor.get(name).get("on_failure");
+        return onFailure != null && !onFailure.isEmpty();
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static List<Map<String, Map>> getOnFailure(Map<String, Map> processor, String name) {
+        return (List<Map<String, Map>>) processor.get(name).get("on_failure");
+    }
+
+    /**
+     * Creates an if clause with the tag name
+     * @param tag String tag name to find in [tags] field
+     * @param onFailurePipeline The on failure pipeline converted to LS to tack on in the conditional
+     * @return a string representing a conditional logic
+     */
+    public static String createTagConditional(String tag, String onFailurePipeline) {
+        return "if " + quoteString(tag) + " in [tags] {\n" +
+                onFailurePipeline + "\n" +
+                "}";
+    }
+
+    public static String getElasticsearchOutput() {
+        return fixIndent("output {\n" +
+                "elasticsearch {\n" +
+                "hosts => \"localhost\"\n" +
+                "}\n" +
+                "}");
+    }
+
+    public static String getStdinInput() {
+        return fixIndent("input {\n" +
+                "stdin {\n" +
+                "}\n" +
+                "}");
+    }
+
+    public static String getStdoutOutput() {
+        return fixIndent("output {\n" +
+                "stdout {\n" +
+                "codec => \"rubydebug\"\n" +
+                "}\n" +
+                "}");
+    }
+
+    public static String appendIoPlugins(List<String> filtersPipeline, boolean appendStdio) {
+        // TODO create unique list to join all
+        String filtersPipelineStr = String.join("\n", filtersPipeline);
+        if (appendStdio) {
+            return String.join("\n", IngestConverter.getStdinInput(), filtersPipelineStr, IngestConverter.getStdoutOutput());
+        } else {
+            return String.join("\n", filtersPipelineStr, IngestConverter.getElasticsearchOutput());
+        }
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestDate.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestDate.java
new file mode 100644
index 00000000000..6aea685e815
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestDate.java
@@ -0,0 +1,97 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestDate {
+
+    /**
+     * Converts Ingest Date JSON to LS Date filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestDate::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("date", dateHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String dateHash(Map<String, Map> processor) {
+        Map date_json = processor.get("date");
+        List<String> formats = (List<String>) date_json.get("formats");
+
+        final String firstElem = IngestConverter.dotsToSquareBrackets((String) date_json.get("field"));
+        List<String> match_contents = new ArrayList<>();
+        match_contents.add(firstElem);
+        for (String f : formats) {
+            match_contents.add(f);
+        }
+        String date_contents = IngestConverter.createField(
+                "match",
+                IngestConverter.createPatternArray(match_contents.toArray(new String[0])));
+        if (JsUtil.isNotEmpty((String) date_json.get("target_field"))) {
+            String target = IngestConverter.createField(
+                    "target",
+                    IngestConverter.quoteString(
+                            IngestConverter.dotsToSquareBrackets((String) date_json.get("target_field"))
+                    )
+            );
+            date_contents = IngestConverter.joinHashFields(date_contents, target);
+        }
+        if (JsUtil.isNotEmpty((String) date_json.get("timezone"))) {
+            String timezone = IngestConverter.createField(
+                    "timezone",
+                    IngestConverter.quoteString((String) date_json.get("timezone"))
+            );
+            date_contents = IngestConverter.joinHashFields(date_contents, timezone);
+        }
+        if (JsUtil.isNotEmpty((String) date_json.get("locale"))) {
+            String locale = IngestConverter.createField(
+                    "locale",
+                    IngestConverter.quoteString((String) date_json.get("locale"))
+            );
+            date_contents = IngestConverter.joinHashFields(date_contents, locale);
+        }
+        return date_contents;
+    }
+
+    public static boolean has_date(Map<String, Object> processor) {
+        return processor.containsKey("date");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGeoIp.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGeoIp.java
new file mode 100644
index 00000000000..3394ba52d39
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGeoIp.java
@@ -0,0 +1,85 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestGeoIp {
+
+
+    /**
+     * Converts Ingest Date JSON to LS Date filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestGeoIp::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("geoip", geoIpHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String geoIpHash(Map<String, Map> processor) {
+        Map geoip_data = processor.get("geoip");
+        final String sourceField = IngestConverter.createField(
+                "source",
+                IngestConverter.quoteString(
+                        IngestConverter.dotsToSquareBrackets((String) geoip_data.get("field"))
+                )
+        );
+
+        final String targetField = IngestConverter.createField(
+                "target",
+                IngestConverter.quoteString(
+                        IngestConverter.dotsToSquareBrackets((String) geoip_data.get("target_field"))
+                )
+        );
+
+        if (geoip_data.containsKey("properties")) {
+            String fields = IngestConverter.createField(
+                    "fields",
+                    IngestConverter.createPatternArray(((List<String>) geoip_data.get("properties")).toArray(new String[0])
+                    ));
+            return IngestConverter.joinHashFields(sourceField, targetField, fields);
+        } else {
+            return IngestConverter.joinHashFields(sourceField, targetField);
+        }
+    }
+
+    public static boolean has_geoip(Map<String, Object> processor) {
+        return processor.containsKey("geoip");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGrok.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGrok.java
new file mode 100644
index 00000000000..9c8c8067b69
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGrok.java
@@ -0,0 +1,99 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestGrok {
+
+    /**
+     * Converts Ingest JSON to LS Grok.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestGrok::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("grok", grokHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String grokHash(Map<String, Map> processor) {
+        Map grok_data = processor.get("grok");
+        String grok_contents = createHashField("match",
+                IngestConverter.createField(
+                        IngestConverter.quoteString((String) grok_data.get("field")),
+                        IngestConverter.createPatternArrayOrField(((List<String>) grok_data.get("patterns")).toArray(new String[0]))
+                ));
+        if (grok_data.containsKey("pattern_definitions")) {
+            grok_contents = IngestConverter.joinHashFields(
+                    grok_contents,
+                    createPatternDefinitionHash((Map<String, String>) grok_data.get("pattern_definitions"))
+            );
+        }
+        return grok_contents;
+    }
+
+    private static String createHashField(String name, String content) {
+        return IngestConverter.createField(name, IngestConverter.wrapInCurly(content));
+    }
+
+    private static String createPatternDefinitionHash(Map<String, String> definitions) {
+        List<String> content = new ArrayList<>();
+        for(Map.Entry<String, String> entry : definitions.entrySet()) {
+            content.add(IngestConverter.createField(
+                    IngestConverter.quoteString(entry.getKey()),
+                    IngestConverter.quoteString(entry.getValue())));
+        }
+
+        final String patternDefs = content.stream().map(IngestConverter::dotsToSquareBrackets)
+                .collect(Collectors.joining("\n"));
+
+        return createHashField(
+                "pattern_definitions",
+                patternDefs
+        );
+    }
+
+    public static boolean has_grok(Map<String, Object> processor) {
+        return processor.containsKey(get_name());
+    }
+
+    public static String get_name() {
+        return "grok";
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGsub.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGsub.java
new file mode 100644
index 00000000000..1ff3e3b51f3
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestGsub.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestGsub {
+
+    /**
+     * Converts Ingest JSON to LS Grok.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestGsub::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("mutate", gsubHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String gsubHash(Map<String, Map> processor) {
+        Map gsub_data = processor.get("gsub");
+        final String body = String.join(", ",
+                IngestConverter.quoteString(IngestConverter.dotsToSquareBrackets((String) gsub_data.get("field"))),
+                IngestConverter.quoteString((String) gsub_data.get("pattern")),
+                IngestConverter.quoteString((String) gsub_data.get("replacement")));
+
+        return IngestConverter.createField("gsub", "[\n" + body + "\n]");
+    }
+
+    public static boolean has_gsub(Map<String, Object> processor) {
+        return processor.containsKey("gsub");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestJson.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestJson.java
new file mode 100644
index 00000000000..77a9b37df62
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestJson.java
@@ -0,0 +1,79 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestJson {
+
+    /**
+     * Converts Ingest json processor to LS json filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestJson::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("json", jsonHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String jsonHash(Map<String, Map> processor) {
+        Map json_data = processor.get("json");
+
+        List<String> parts = new ArrayList();
+        parts.add(IngestConverter.createField("source",
+                IngestConverter.quoteString(
+                        IngestConverter.dotsToSquareBrackets((String) json_data.get("field"))
+                )
+        ));
+
+        if (json_data.containsKey("target_field")) {
+            parts.add(IngestConverter.createField(
+                    "target",
+                    IngestConverter.quoteString(
+                            IngestConverter.dotsToSquareBrackets((String) json_data.get("target_field"))
+                    )
+            ));
+        }
+        return IngestConverter.joinHashFields(parts.toArray(new String[0]));
+    }
+
+    public static boolean has_json(Map<String, Object> processor) {
+        return processor.containsKey("json");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestLowercase.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestLowercase.java
new file mode 100644
index 00000000000..09b3c703a3c
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestLowercase.java
@@ -0,0 +1,67 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestLowercase {
+
+    /**
+     * Converts Ingest Lowercase JSON to LS mutate filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestLowercase::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("mutate", lowercaseHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String lowercaseHash(Map<String, Map> processor) {
+        Map lowercase_data = processor.get("lowercase");
+        return IngestConverter.createField(
+                "lowercase",
+                IngestConverter.quoteString(
+                        IngestConverter.dotsToSquareBrackets((String) lowercase_data.get("field"))
+                )
+        );
+    }
+
+    public static boolean has_lowercase(Map<String, Object> processor) {
+        return processor.containsKey("lowercase");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestPipeline.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestPipeline.java
new file mode 100644
index 00000000000..1084f680c8c
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestPipeline.java
@@ -0,0 +1,136 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.ArrayList;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestPipeline {
+
+    /**
+     * Converts Ingest JSON to LS.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestPipeline::mapProcessor).collect(Collectors.toList());
+
+        String logstash_pipeline = IngestConverter.filterHash(
+                IngestConverter.joinHashFields(filters_pipeline.toArray(new String[0])));
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(Collections.singletonList(logstash_pipeline), appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        List<String> filter_blocks = new ArrayList<>();
+        if (IngestGrok.has_grok(processor)) {
+            filter_blocks.add(IngestConverter.createHash(IngestGrok.get_name(), IngestGrok.grokHash(processor)));
+
+            if (IngestConverter.hasOnFailure(processor, IngestGrok.get_name())) {
+                filter_blocks.add(
+                        handle_on_failure_pipeline(
+                                IngestConverter.getOnFailure(processor, IngestGrok.get_name()),
+                                "_grokparsefailure"
+                        )
+                );
+            }
+        }
+        boolean processed = false;
+        if (IngestDate.has_date(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("date", IngestDate.dateHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestGeoIp.has_geoip(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("geoip", IngestGeoIp.geoIpHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestConvert.has_convert(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("mutate", IngestConvert.convertHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestGsub.has_gsub(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("mutate", IngestGsub.gsubHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestAppend.has_append(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("mutate", IngestAppend.appendHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestJson.has_json(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("json", IngestJson.jsonHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestRename.has_rename(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("mutate", IngestRename.renameHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestLowercase.has_lowercase(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("mutate", IngestLowercase.lowercaseHash(processor))
+            );
+            processed = true;
+        }
+        if (IngestSet.has_set(processor)) {
+            filter_blocks.add(
+                    IngestConverter.createHash("mutate", IngestSet.setHash(processor))
+            );
+            processed = true;
+        }
+        if (!processed) {
+            System.out.println("WARN Found unrecognized processor named: " + processor.keySet().iterator().next());
+        }
+        return IngestConverter.joinHashFields(filter_blocks.toArray(new String[0]));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String  handle_on_failure_pipeline(List<Map> on_failure_json, String tag_name) {
+        final List<String> mapped = on_failure_json.stream().map(IngestPipeline::mapProcessor).collect(Collectors.toList());
+        return IngestConverter.createTagConditional(tag_name,
+                IngestConverter.joinHashFields(mapped.toArray(new String[0]))
+        );
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestRename.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestRename.java
new file mode 100644
index 00000000000..a21c81f53a8
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestRename.java
@@ -0,0 +1,66 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestRename {
+
+    /**
+     * Converts Ingest Rename JSON to LS mutate filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestRename::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("mutate", renameHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String renameHash(Map<String, Map> processor) {
+        Map rename_json = processor.get("rename");
+        final String mutateContents = IngestConverter.createField(
+                IngestConverter.quoteString(IngestConverter.dotsToSquareBrackets((String) rename_json.get("field"))),
+                IngestConverter.quoteString(IngestConverter.dotsToSquareBrackets((String) rename_json.get("target_field")))
+        );
+        return IngestConverter.createField("rename", IngestConverter.wrapInCurly(mutateContents));
+    }
+
+    public static boolean has_rename(Map<String, Object> processor) {
+        return processor.containsKey("rename");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestSet.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestSet.java
new file mode 100644
index 00000000000..044d787ed87
--- /dev/null
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/IngestSet.java
@@ -0,0 +1,80 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.ingest;
+
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.core.type.TypeReference;
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+import java.util.stream.Collectors;
+
+public class IngestSet {
+
+    /**
+     * Converts Ingest Set JSON to LS mutate filter.
+     */
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    public static String toLogstash(String json, boolean appendStdio) throws JsonProcessingException {
+        ObjectMapper mapper = new ObjectMapper();
+        TypeReference<HashMap<String, Object>> typeRef = new TypeReference<HashMap<String, Object>>() {};
+        final HashMap<String, Object> jsonDefinition = mapper.readValue(json, typeRef);
+        final List<Map> processors = (List<Map>) jsonDefinition.get("processors");
+        List<String> filters_pipeline = processors.stream().map(IngestSet::mapProcessor).collect(Collectors.toList());
+
+        return IngestConverter.filtersToFile(
+                IngestConverter.appendIoPlugins(filters_pipeline, appendStdio));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    private static String mapProcessor(Map processor) {
+        return IngestConverter.filterHash(IngestConverter.createHash("mutate", setHash(processor)));
+    }
+
+    @SuppressWarnings({"rawtypes", "unchecked"})
+    static String setHash(Map<String, Map> processor) {
+        Map set_json = processor.get("set");
+        final Object value = set_json.get("value");
+        final Object value_contents;
+        if (value instanceof String) {
+            value_contents = IngestConverter.quoteString((String) value);
+        } else {
+            value_contents = value;
+        }
+        if (set_json.containsKey("if") && set_json.get("if") != null) {
+            String painless_condition = (String) set_json.get("if");
+            if (!painless_condition.isEmpty()) {
+                System.out.println("WARN Found in 'set' processor an 'if' painless condition not translated: " + painless_condition);
+            }
+
+        }
+
+        String mutate_contents = IngestConverter.createField(
+                IngestConverter.quoteString(IngestConverter.dotsToSquareBrackets((String) set_json.get("field"))),
+                value_contents.toString());
+        return IngestConverter.createField("add_field", IngestConverter.wrapInCurly(mutate_contents));
+    }
+
+    public static boolean has_set(Map<String, Object> processor) {
+        return processor.containsKey("set");
+    }
+}
diff --git a/tools/ingest-converter/src/main/java/org/logstash/ingest/JsUtil.java b/tools/ingest-converter/src/main/java/org/logstash/ingest/JsUtil.java
index fd71da88aec..806c2f36f27 100644
--- a/tools/ingest-converter/src/main/java/org/logstash/ingest/JsUtil.java
+++ b/tools/ingest-converter/src/main/java/org/logstash/ingest/JsUtil.java
@@ -98,13 +98,79 @@ public static void convert(final String[] args, final String jsFunc)
                 parser.printHelpOn(System.out);
                 throw ex;
             }
-            final ScriptEngine engine = JsUtil.engine();
-            Files.write(
-                Paths.get(options.valueOf(output)),
-                ((String) ((Invocable) engine).invokeFunction(
-                    jsFunc, input(options.valueOf(input)), options.has(appendStdio)
-                )).getBytes(StandardCharsets.UTF_8)
-            );
+            switch (jsFunc) {
+                case "ingest_append_to_logstash":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestAppend.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_convert_to_logstash":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestConvert.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_to_logstash_date":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestDate.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_to_logstash_geoip":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestGeoIp.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_to_logstash_grok":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestGrok.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_to_logstash_gsub":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestGsub.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_json_to_logstash":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestJson.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_lowercase_to_logstash":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestLowercase.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_rename_to_logstash":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestRename.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_set_to_logstash":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestSet.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+                case "ingest_pipeline_to_logstash":
+                    Files.write(
+                            Paths.get(options.valueOf(output)),
+                            IngestPipeline.toLogstash(input(options.valueOf(input)), options.has(appendStdio)).getBytes(StandardCharsets.UTF_8)
+                    );
+                    break;
+
+                default: {
+                    throw new IllegalArgumentException("Can't recognize " + jsFunc + " processor");
+                }
+            }
+
         } catch (final IOException ex) {
             throw new IllegalStateException(ex);
         }
@@ -132,4 +198,13 @@ private static void add(final ScriptEngine engine, final String file)
             engine.eval(reader);
         }
     }
+
+    /***
+     * Not empty check with nullability
+     * @param s string to check
+     * @return true iff s in not null and not empty
+     */
+    static boolean isNotEmpty(String s) {
+        return s != null && !s.isEmpty();
+    }
 }
diff --git a/tools/ingest-converter/src/main/resources/ingest-append.js b/tools/ingest-converter/src/main/resources/ingest-append.js
deleted file mode 100644
index 191d7fdd8a2..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-append.js
+++ /dev/null
@@ -1,40 +0,0 @@
-var IngestAppend = {
-    has_append: function (processor) {
-        return !!processor["append"];
-    },
-    append_hash: function (processor) {
-        var append_json = processor["append"];
-        var value_contents;
-        var value = append_json["value"];
-        if (Array.isArray(value)) {
-            value_contents = IngestConverter.create_array(value);
-        } else {
-            value_contents = IngestConverter.quote_string(value);
-        }
-        var mutate_contents = IngestConverter.create_field(
-            IngestConverter.quote_string(IngestConverter.dots_to_square_brackets(append_json["field"])),
-            value_contents);
-        return IngestConverter.create_field("add_field", IngestConverter.wrap_in_curly(mutate_contents));
-    }
-};
-
-/**
- * Converts Ingest Append JSON to LS mutate filter.
- */
-function ingest_append_to_logstash(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash(
-                "mutate", IngestAppend.append_hash(processor)
-            )
-        );
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-convert.js b/tools/ingest-converter/src/main/resources/ingest-convert.js
deleted file mode 100644
index 977a4804939..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-convert.js
+++ /dev/null
@@ -1,34 +0,0 @@
-var IngestConvert = {
-    has_convert: function (processor) {
-        return !!processor["convert"];
-    },
-    convert_hash: function (processor) {
-        var convert_json = processor["convert"];
-        var mutate_contents = IngestConverter.create_field(
-            IngestConverter.quote_string(IngestConverter.dots_to_square_brackets(convert_json["field"])),
-            IngestConverter.quote_string(convert_json["type"])
-        );
-        return IngestConverter.create_field("convert", IngestConverter.wrap_in_curly(mutate_contents));
-    }
-};
-
-/**
- * Converts Ingest Convert JSON to LS Date filter.
- */
-function ingest_convert_to_logstash(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash(
-                "mutate", IngestConvert.convert_hash(processor)
-            )
-        );
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-date.js b/tools/ingest-converter/src/main/resources/ingest-date.js
deleted file mode 100644
index b007ae918f6..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-date.js
+++ /dev/null
@@ -1,60 +0,0 @@
-var IngestDate = {
-    has_date: function (processor) {
-        return !!processor["date"];
-    },
-    date_hash: function (processor) {
-        var date_json = processor["date"];
-        var formats = date_json["formats"];
-        var match_contents = [IngestConverter.dots_to_square_brackets(date_json["field"])];
-        for (var f in formats) {
-            match_contents.push(formats[f]);
-        }
-        var date_contents = IngestConverter.create_field(
-            "match",
-            IngestConverter.create_pattern_array(match_contents)
-        );
-        if (date_json["target_field"]) {
-            var target = IngestConverter.create_field(
-                "target",
-                IngestConverter.quote_string(
-                    IngestConverter.dots_to_square_brackets(date_json["target_field"])
-                )
-            );
-            date_contents = IngestConverter.join_hash_fields([date_contents, target]);
-        }
-        if (date_json["timezone"]) {
-            var timezone = IngestConverter.create_field(
-                "timezone",
-                IngestConverter.quote_string(date_json["timezone"])
-            );
-            date_contents = IngestConverter.join_hash_fields([date_contents, timezone]);
-        }
-        if (date_json["locale"]) {
-            var locale = IngestConverter.create_field(
-                "locale", IngestConverter.quote_string(date_json["locale"]));
-            date_contents = IngestConverter.join_hash_fields([date_contents, locale]);
-        }
-        return date_contents;
-    }
-};
-
-/**
- * Converts Ingest Date JSON to LS Date filter.
- */
-function ingest_to_logstash_date(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash(
-                "date", IngestDate.date_hash(processor)
-            )
-        );
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-geoip.js b/tools/ingest-converter/src/main/resources/ingest-geoip.js
deleted file mode 100644
index fcaa816409d..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-geoip.js
+++ /dev/null
@@ -1,50 +0,0 @@
-var IngestGeoIp = {
-    has_geoip: function (processor) {
-        return !!processor["geoip"];
-    },
-    geoip_hash: function (processor) {
-        var geoip_data = processor["geoip"];
-        var parts = [
-            IngestConverter.create_field(
-                "source",
-                IngestConverter.quote_string(
-                    IngestConverter.dots_to_square_brackets(geoip_data["field"])
-                )
-            ),
-            IngestConverter.create_field(
-                "target",
-                IngestConverter.quote_string(
-                    IngestConverter.dots_to_square_brackets(geoip_data["target_field"])
-                )
-            )
-        ];
-        if (geoip_data["properties"]) {
-            parts.push(
-                IngestConverter.create_field(
-                    "fields",
-                    IngestConverter.create_pattern_array(geoip_data["properties"])
-                )
-            );
-        }
-        return IngestConverter.join_hash_fields(parts);
-    }
-};
-
-/**
- * Converts Ingest JSON to LS Grok.
- */
-function ingest_to_logstash_geoip(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash("geoip", IngestGeoIp.geoip_hash(processor))
-        )
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-grok.js b/tools/ingest-converter/src/main/resources/ingest-grok.js
deleted file mode 100644
index 94e37903b45..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-grok.js
+++ /dev/null
@@ -1,68 +0,0 @@
-var IngestGrok = {
-    has_grok: function (processor) {
-        return !!processor[this.get_name()];
-    },
-    get_name: function () {
-        return "grok";
-    },
-    grok_hash: function (processor) {
-
-        function create_hash_field(name, content) {
-            return IngestConverter.create_field(
-                name, IngestConverter.wrap_in_curly(content)
-            );
-        }
-
-        function create_pattern_definition_hash(definitions) {
-            var content = [];
-            for (var key in definitions) {
-                if (definitions.hasOwnProperty(key)) {
-                    content.push(
-                        IngestConverter.create_field(
-                            IngestConverter.quote_string(key),
-                            IngestConverter.quote_string(definitions[key]))
-                    );
-                }
-            }
-            return create_hash_field(
-                "pattern_definitions", 
-                content.map(IngestConverter.dots_to_square_brackets).join("\n")
-            );
-        }
-
-        var grok_data = processor["grok"];
-        var grok_contents = create_hash_field(
-            "match",
-            IngestConverter.create_field(
-                IngestConverter.quote_string(grok_data["field"]),
-                IngestConverter.create_pattern_array_or_field(grok_data["patterns"])
-            )
-        );
-        if (grok_data["pattern_definitions"]) {
-            grok_contents = IngestConverter.join_hash_fields([
-                grok_contents,
-                create_pattern_definition_hash(grok_data["pattern_definitions"])
-            ])
-        }
-        return grok_contents;
-    }
-};
-
-/**
- * Converts Ingest JSON to LS Grok.
- */
-function ingest_to_logstash_grok(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash("grok", IngestGrok.grok_hash(processor))
-        )
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-gsub.js b/tools/ingest-converter/src/main/resources/ingest-gsub.js
deleted file mode 100644
index 488649869d1..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-gsub.js
+++ /dev/null
@@ -1,33 +0,0 @@
-var IngestGsub = {
-    has_gsub: function (processor) {
-        return !!processor["gsub"];
-    },
-    gsub_hash: function (processor) {
-        var gsub_data = processor["gsub"];
-        return IngestConverter.create_field(
-            "gsub",
-            "[\n" + [IngestConverter.dots_to_square_brackets(gsub_data["field"]),
-                gsub_data["pattern"], gsub_data["replacement"]].map(IngestConverter.quote_string)
-                .join(", ") + "\n]"
-        );
-    }
-};
-
-/**
- * Converts Ingest JSON to LS Grok.
- */
-function ingest_to_logstash_gsub(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash("mutate", IngestGsub.gsub_hash(processor))
-        )
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-json.js b/tools/ingest-converter/src/main/resources/ingest-json.js
deleted file mode 100644
index ad2a71bc853..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-json.js
+++ /dev/null
@@ -1,48 +0,0 @@
-var IngestJson = {
-    has_json: function (processor) {
-        return !!processor["json"];
-    },
-    json_hash: function (processor) {
-        var json_data = processor["json"];
-        var parts = [
-            IngestConverter.create_field(
-                "source",
-                IngestConverter.quote_string(
-                    IngestConverter.dots_to_square_brackets(json_data["field"])
-                )
-            )
-        ];
-
-        if (json_data["target_field"]) {
-            parts.push(
-                IngestConverter.create_field(
-                    "target",
-                    IngestConverter.quote_string(
-                        IngestConverter.dots_to_square_brackets(json_data["target_field"])
-                    )
-                )
-            );
-        }
-
-        return IngestConverter.join_hash_fields(parts);
-    }
-};
-
-/**
- * Converts Ingest json processor to LS json filter.
- */
-function ingest_json_to_logstash(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash("json", IngestJson.json_hash(processor))
-        )
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-lowercase.js b/tools/ingest-converter/src/main/resources/ingest-lowercase.js
deleted file mode 100644
index aae5a908736..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-lowercase.js
+++ /dev/null
@@ -1,34 +0,0 @@
-var IngestLowercase = {
-    has_lowercase: function (processor) {
-        return !!processor["lowercase"];
-    },
-    lowercase_hash: function (processor) {
-        return IngestConverter.create_field(
-            "lowercase", 
-            IngestConverter.quote_string(
-                IngestConverter.dots_to_square_brackets(processor["lowercase"]["field"])
-            )
-        );
-    }
-};
-
-/**
- * Converts Ingest Lowercase JSON to LS mutate filter.
- */
-function ingest_lowercase_to_logstash(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash(
-                "mutate", IngestLowercase.lowercase_hash(processor)
-            )
-        );
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-pipeline.js b/tools/ingest-converter/src/main/resources/ingest-pipeline.js
deleted file mode 100644
index 1d2c0f08892..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-pipeline.js
+++ /dev/null
@@ -1,97 +0,0 @@
-/**
- * Converts Ingest JSON to LS Grok.
- */
-function ingest_pipeline_to_logstash(json, append_stdio) {
-
-    function handle_on_failure_pipeline(on_failure_json, tag_name) {
-
-        return IngestConverter.create_tag_conditional(tag_name,
-            IngestConverter.join_hash_fields(on_failure_json.map(map_processor))
-        );
-    }
-
-    function map_processor(processor) {
-
-        var filter_blocks = [];
-        if (IngestGrok.has_grok(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash(IngestGrok.get_name(), IngestGrok.grok_hash(processor))
-            );
-            if (IngestConverter.has_on_failure(processor, IngestGrok.get_name())) {
-                filter_blocks.push(
-                    handle_on_failure_pipeline(
-                        IngestConverter.get_on_failure(processor, IngestGrok.get_name()),
-                        "_grokparsefailure"
-                    )
-                );
-            }
-        }
-        var processed = false;
-        if (IngestDate.has_date(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("date", IngestDate.date_hash(processor))
-            )
-            processed = true;
-        }
-        if (IngestGeoIp.has_geoip(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("geoip", IngestGeoIp.geoip_hash(processor))
-            )
-            processed = true;
-        }
-        if (IngestConvert.has_convert(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("mutate", IngestConvert.convert_hash(processor))
-            );
-            processed = true;
-        }
-        if (IngestGsub.has_gsub(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("mutate", IngestGsub.gsub_hash(processor))
-            );
-            processed = true;
-        }
-        if (IngestAppend.has_append(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("mutate", IngestAppend.append_hash(processor))
-            );
-            processed = true;
-        }
-        if (IngestJson.has_json(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("json", IngestJson.json_hash(processor))
-            );
-            processed = true;
-        }
-        if (IngestRename.has_rename(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("mutate", IngestRename.rename_hash(processor))
-            );
-            processed = true;
-        }
-        if (IngestLowercase.has_lowercase(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("mutate", IngestLowercase.lowercase_hash(processor))
-            );
-            processed = true;
-        }
-        if (IngestSet.has_set(processor)) {
-            filter_blocks.push(
-                IngestConverter.create_hash("mutate", IngestSet.set_hash(processor))
-            );
-            processed = true;
-        }
-        if (!processed) {
-            print("WARN Found unrecognized processor named: " + Object.keys(processor)[0]);
-        }
-        return IngestConverter.join_hash_fields(filter_blocks);
-    }
-
-    var logstash_pipeline = IngestConverter.filter_hash(
-        IngestConverter.join_hash_fields(JSON.parse(json)["processors"].map(map_processor))
-    );
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(logstash_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-rename.js b/tools/ingest-converter/src/main/resources/ingest-rename.js
deleted file mode 100644
index e861dbed5ca..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-rename.js
+++ /dev/null
@@ -1,34 +0,0 @@
-var IngestRename = {
-    has_rename: function (processor) {
-        return !!processor["rename"];
-    },
-    rename_hash: function (processor) {
-        var rename_json = processor["rename"];
-        var mutate_contents = IngestConverter.create_field(
-            IngestConverter.quote_string(IngestConverter.dots_to_square_brackets(rename_json["field"])),
-            IngestConverter.quote_string(IngestConverter.dots_to_square_brackets(rename_json["target_field"]))
-        );
-        return IngestConverter.create_field("rename", IngestConverter.wrap_in_curly(mutate_contents));
-    }
-};
-
-/**
- * Converts Ingest Rename JSON to LS mutate filter.
- */
-function ingest_rename_to_logstash(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash(
-                "mutate", IngestRename.rename_hash(processor)
-            )
-        );
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-set.js b/tools/ingest-converter/src/main/resources/ingest-set.js
deleted file mode 100644
index 122dc44a5fb..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-set.js
+++ /dev/null
@@ -1,44 +0,0 @@
-var IngestSet = {
-    has_set: function (processor) {
-        return !!processor["set"];
-    },
-    set_hash: function (processor) {
-        var set_json = processor["set"];
-        var value_contents;
-        var value = set_json["value"];
-        if (typeof value === 'string' || value instanceof String) {
-            value_contents = IngestConverter.quote_string(value);
-        } else {
-            value_contents = value;
-        }
-        var painless_condition = set_json["if"];
-        if (!!painless_condition) {
-          print("WARN Found in 'set' processor an 'if' painless condition not translated: " + painless_condition);
-        }
-        var mutate_contents = IngestConverter.create_field(
-            IngestConverter.quote_string(IngestConverter.dots_to_square_brackets(set_json["field"])),
-            value_contents);
-        return IngestConverter.create_field("add_field", IngestConverter.wrap_in_curly(mutate_contents));
-    }
-};
-
-/**
- * Converts Ingest Set JSON to LS mutate filter.
- */
-function ingest_set_to_logstash(json, append_stdio) {
-
-    function map_processor(processor) {
-
-        return IngestConverter.filter_hash(
-            IngestConverter.create_hash(
-                "mutate", IngestSet.set_hash(processor)
-            )
-        );
-    }
-
-    var filters_pipeline = JSON.parse(json)["processors"].map(map_processor);
-    return IngestConverter.filters_to_file([
-        IngestConverter.append_io_plugins(filters_pipeline, append_stdio)
-        ]
-    );
-}
diff --git a/tools/ingest-converter/src/main/resources/ingest-shared.js b/tools/ingest-converter/src/main/resources/ingest-shared.js
deleted file mode 100644
index 4f36ccafc64..00000000000
--- a/tools/ingest-converter/src/main/resources/ingest-shared.js
+++ /dev/null
@@ -1,184 +0,0 @@
-var IngestConverter = {
-    /**
-     * Translates the JSON naming pattern (`name.qualifier.sub`) into the LS pattern
-     * [name][qualifier][sub] for all applicable tokens in the given string.
-     * This function correctly identifies and omits renaming of string literals.
-     * @param string to replace naming pattern in
-     * @returns {string} with Json naming translated into grok naming
-     */
-    dots_to_square_brackets: function (string) {
-
-        function token_dots_to_square_brackets(string) {
-            var adjusted;
-            //Break out if this is not a naming pattern we convert
-            if (string.match(/([\w_]+\.)+[\w_]+/)) {
-                adjusted = string.replace(/(\w*)\.(\w*)/g, "$1][$2")
-                    .replace(/\[(\w+)(}|$)/g, "[$1]$2")
-                    .replace(/{(\w+):(\w+)]/g, "{$1:[$2]")
-                    .replace(/^(\w+)]\[/g, "[$1][");
-            } else {
-                adjusted = string;
-            }
-            return adjusted;
-        }
-
-        var literals = string.match(/\(\?:%{.*\|-\)/);
-        var i;
-        var tokens = [];
-        // Copy String before Manipulation
-        var right = string;
-        if (literals) {
-            for (i = 0; i < literals.length; ++i) {
-                var parts = right.split(literals[i], 2);
-                right = parts[1];
-                tokens.push(token_dots_to_square_brackets(parts[0]));
-                tokens.push(literals[i]);
-            }
-        }
-        tokens.push(token_dots_to_square_brackets(right));
-        return tokens.join("");
-    }, quote_string: function (string) {
-        return "\"" + string.replace(/"/g, "\\\"") + "\"";
-    }, wrap_in_curly: function (string) {
-        return "{\n" + string + "\n}";
-    }, create_field: function (name, content) {
-        return name + " => " + content;
-    }, create_hash: function (name, content) {
-        return name + " " + this.wrap_in_curly(content);
-    },
-
-    /**
-     * All hash fields in LS start on a new line.
-     * @param fields Array of Strings of Serialized Hash Fields
-     * @returns {string} Joined Serialization of Hash Fields
-     */
-    join_hash_fields: function (fields) {
-        return fields.join("\n");
-    },
-
-    /**
-     * Fixes indentation in LS string.
-     * @param string LS string to fix indentation in, that has no indentation intentionally with
-     * all lines starting on a token without preceding spaces.
-     * @returns {string} LS string indented by 3 spaces per level
-     */
-    fix_indent: function (string) {
-
-        function indent(string, shifts) {
-            return new Array(shifts * 3 + 1).join(" ") + string;
-        }
-
-        var lines = string.split("\n");
-        var count = 0;
-        var i;
-        for (i = 0; i < lines.length; ++i) {
-            if (lines[i].match(/(\{|\[)$/)) {
-                lines[i] = indent(lines[i], count);
-                ++count;
-            } else if (lines[i].match(/(\}|\])$/)) {
-                --count;
-                lines[i] = indent(lines[i], count);
-                // Only indent line if previous line ended on relevant control char.
-            } else if (i > 0 && lines[i - 1].match(/(=>\s+".+"|,|\{|\}|\[|\])$/)) {
-                lines[i] = indent(lines[i], count);
-            }
-        }
-        return lines.join("\n");
-    },
-
-    /**
-     * Converts Ingest/JSON style pattern array to LS pattern array, performing necessary variable
-     * name and quote escaping adjustments.
-     * @param patterns Pattern Array in JSON formatting
-     * @returns {string} Pattern array in LS formatting
-     */
-    create_pattern_array: function (patterns) {
-        return "[\n"
-            + patterns.map(this.dots_to_square_brackets).map(this.quote_string).join(",\n")
-            + "\n]";
-    },
-
-    create_array: function (ingest_array) {
-        return "[\n"
-            + ingest_array.map(this.quote_string).join(",\n")
-            + "\n]";
-    },
-
-    /**
-     * Converts Ingest/JSON style pattern array to LS pattern array or string if the given array
-     * contains a single element only, performing necessary variable name and quote escaping
-     * adjustments.
-     * @param patterns Pattern Array in JSON formatting
-     * @returns {string} Pattern array or string in LS formatting
-     */
-    create_pattern_array_or_field: function (patterns) {
-        return patterns.length === 1
-            ? this.quote_string(this.dots_to_square_brackets(patterns[0]))
-            : this.create_pattern_array(patterns);
-    },
-
-    filter_hash: function(contents) {
-        return this.fix_indent(this.create_hash("filter", contents))
-    },
-
-    filters_to_file: function(filters) {
-        return filters.join("\n\n") + "\n";
-    },
-
-    /**
-     * Does it have an on_failure field?
-     * @param processor Json
-     * @param name Name of the processor
-     * @returns {boolean} True if has on failure
-     */
-    has_on_failure: function (processor, name) {
-        return !!processor[name]["on_failure"];
-    },
-
-    get_on_failure: function (processor, name) {
-        return processor[name]["on_failure"];
-    },
-
-    /**
-     * Creates an if clause with the tag name
-     * @param tag String tag name to find in [tags] field
-     * @param on_failure_pipeline The on failure pipeline converted to LS to tack on in the conditional
-     * @returns {string} a string representing a conditional logic
-     */
-    create_tag_conditional: function (tag, on_failure_pipeline) {
-        return "if " + this.quote_string(tag) + " in [tags] {\n" +
-                on_failure_pipeline + "\n" +
-                "}";
-    },
-
-    get_elasticsearch_output: function () {
-        return this.fix_indent("output {\n" +
-            "elasticsearch {\n" +
-            "hosts => \"localhost\"\n" +
-            "}\n" +
-            "}");
-    },
-
-    get_stdin_input: function () {
-        return this.fix_indent("input {\n" +
-            "stdin {\n" +
-            "}\n" +
-            "}");
-    },
-
-    get_stdout_output: function () {
-        return this.fix_indent("output {\n" +
-            "stdout {\n" +
-            "codec => \"rubydebug\"\n" +
-            "}\n" +
-            "}");
-    },
-
-    append_io_plugins: function(filters_pipeline, append_stdio) {
-        if (append_stdio === true) {
-            return [IngestConverter.get_stdin_input(), filters_pipeline, IngestConverter.get_stdout_output()].join("\n");
-        } else {
-            return [filters_pipeline, IngestConverter.get_elasticsearch_output()].join("\n");
-        }
-    }
-};
