diff --git a/logstash-core/build.gradle b/logstash-core/build.gradle
index 6b921972dd9..640e794dafc 100644
--- a/logstash-core/build.gradle
+++ b/logstash-core/build.gradle
@@ -122,6 +122,9 @@ dependencies {
     compile "org.jruby:jruby-complete:${jrubyVersion}"
     compile 'com.google.googlejavaformat:google-java-format:1.5'
     testCompile 'org.apache.logging.log4j:log4j-core:2.9.1:tests'
+    compile 'org.elasticsearch.client:elasticsearch-rest-client-sniffer:6.1.3'
+    testCompile 'org.apache.logging.log4j:log4j-core:2.6.2:tests'
+    testCompile 'org.apache.logging.log4j:log4j-api:2.6.2:tests'
     testCompile 'junit:junit:4.12'
     testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
     testCompile 'org.elasticsearch:securemock:1.2'
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 840aec14050..e3023a29600 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -74,12 +74,19 @@ def initialize(settings = LogStash::SETTINGS, source_loader = nil)
 
     @dispatcher = LogStash::EventDispatcher.new(self)
     LogStash::PLUGIN_REGISTRY.hooks.register_emitter(self.class, dispatcher)
-    dispatcher.fire(:after_initialize)
+       dispatcher.fire(:after_initialize)
 
     @running = Concurrent::AtomicBoolean.new(false)
   end
 
   def execute
+    # Load default plugins
+    require 'logstash/inputs/elastiqueue'
+    require 'logstash/outputs/elastiqueue'
+    LogStash::PLUGIN_REGISTRY.add(:input, "elastiqueue", ::LogStash::Inputs::Elastiqueue )
+    LogStash::PLUGIN_REGISTRY.add(:output, "elastiqueue", ::LogStash::Outputs::Elastiqueue )
+
+
     @thread = Thread.current # this var is implicitly used by Stud.stop?
     logger.debug("Starting agent")
 
diff --git a/logstash-core/lib/logstash/inputs/elastiqueue.rb b/logstash-core/lib/logstash/inputs/elastiqueue.rb
new file mode 100644
index 00000000000..e890a4bc8ec
--- /dev/null
+++ b/logstash-core/lib/logstash/inputs/elastiqueue.rb
@@ -0,0 +1,35 @@
+class LogStash::Inputs::Elastiqueue < LogStash::Inputs::Base
+  config_name "elastiqueue"
+
+  config :hosts, :validate => :uri, :list => true
+  config :topic, :validate => :string
+  config :partitions, :validate => :number
+  config :consumer_group, :validate => :string
+  config :consumer_name, :validate => :string
+  config :user, :validate => :string
+  config :password, :validate => :password
+
+  def register
+    plain_password = password ? password.value : nil
+    @elastiqueue = org.logstash.elastiqueue.Elastiqueue.make(user, plain_password, hosts.map(&:uri).map(&:to_s).to_a)
+    @topic = @elastiqueue.topic(topic, partitions)
+    @consumer = @topic.makeConsumer(consumer_group, consumer_name)
+    @events_processed = java.util.concurrent.atomic.LongAdder.new()
+  end
+
+  def run(queue)
+    @consumer.ruby_consume_partitions do |events|
+      @events_processed.add(events.size)
+      queue.push_batch(events)
+    end
+    last_events_size = 0
+    while !stop?
+      sleep 1
+    end
+  end
+
+  def close
+    @consumer.close()
+    @elastiqueue.close()
+  end
+end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/outputs/elastiqueue.rb b/logstash-core/lib/logstash/outputs/elastiqueue.rb
new file mode 100644
index 00000000000..ba275f7ee89
--- /dev/null
+++ b/logstash-core/lib/logstash/outputs/elastiqueue.rb
@@ -0,0 +1,26 @@
+class LogStash::Outputs::Elastiqueue < LogStash::Outputs::Base
+  config_name "elastiqueue"
+  concurrency :shared
+
+  config :hosts, :validate => :uri, :list => true
+  config :topic, :validate => :string
+  config :partitions, :validate => :number
+  config :user, :validate => :string
+  config :password, :validate => :password
+
+  def register
+    plain_password = password ? password.value : nil
+    @elastiqueue = org.logstash.elastiqueue.Elastiqueue.make(user, plain_password, hosts.map(&:uri).map(&:to_s).to_a)
+    @topic = @elastiqueue.topic(topic, partitions)
+    @producer = @topic.makeProducer("A producer")
+  end
+
+  def multi_receive(events)
+    @producer.rubyWrite(events)
+  end
+
+  def close
+    @producer.close
+    @elastiqueue.close
+  end
+end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/plugins/registry.rb b/logstash-core/lib/logstash/plugins/registry.rb
index c784442594b..e93bb3d9180 100644
--- a/logstash-core/lib/logstash/plugins/registry.rb
+++ b/logstash-core/lib/logstash/plugins/registry.rb
@@ -261,4 +261,5 @@ def key_for(type, plugin_name)
   end end
 
   PLUGIN_REGISTRY = Plugins::Registry.new
+
 end
diff --git a/logstash-core/src/main/java/org/logstash/Event.java b/logstash-core/src/main/java/org/logstash/Event.java
index 1cda5eee31f..1f0ae34c979 100644
--- a/logstash-core/src/main/java/org/logstash/Event.java
+++ b/logstash-core/src/main/java/org/logstash/Event.java
@@ -1,14 +1,13 @@
 package org.logstash;
 
 import com.fasterxml.jackson.core.JsonProcessingException;
+
+import java.io.ByteArrayInputStream;
 import java.io.IOException;
+import java.io.InputStream;
 import java.io.Serializable;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Date;
-import java.util.HashMap;
-import java.util.List;
-import java.util.Map;
+import java.util.*;
+
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
 import org.joda.time.DateTime;
@@ -196,6 +195,33 @@ private static Event fromSerializableMap(final byte[] source) throws IOException
         return new Event(dataMap);
     }
 
+    private static Event[] fromSerializableArrayOfMaps(final InputStream source) throws IOException {
+        final Map<String, Map<String, Object>>[] representation = CBOR_MAPPER.readValue(source, Map[].class);
+
+        if (representation == null) {
+            throw new IOException("incompatible from binary object type only HashMap is supported");
+        }
+
+        Event[] events = new Event[representation.length];
+
+        int i = 0;
+        for (Map<String, Map<String, Object>> item : representation) {
+            final Map<String, Object> dataMap = item.get(DATA_MAP_KEY);
+            if (dataMap == null) {
+                throw new IOException("The deserialized Map must contain the \"DATA\" key");
+            }
+            final Map<String, Object> metaMap = item.get(META_MAP_KEY);
+            if (metaMap == null) {
+                throw new IOException("The deserialized Map must contain the \"META\" key");
+            }
+            dataMap.put(METADATA, metaMap);
+            events[i] = new Event(dataMap);
+            i++;
+        }
+
+        return events;
+    }
+
     public String toJson() throws JsonProcessingException {
         return JSON_MAPPER.writeValueAsString(this.data);
     }
@@ -389,10 +415,36 @@ public byte[] serialize() throws JsonProcessingException {
         return CBOR_MAPPER.writeValueAsBytes(map);
     }
 
+    public static byte[] serializeMany(Event... events) throws JsonProcessingException {
+        Map[] mappedList = new Map[events.length];
+        int i = 0;
+        for (Event e : events) {
+            final Map<String, Map<String, Object>> map = new HashMap<>(2, 1.0F);
+            map.put(DATA_MAP_KEY, e.data);
+            map.put(META_MAP_KEY, e.metadata);
+            mappedList[i] = map;
+            i++;
+        }
+        return CBOR_MAPPER.writeValueAsBytes(mappedList);
+    }
+
     public static Event deserialize(byte[] data) throws IOException {
         if (data == null || data.length == 0) {
             return new Event();
         }
         return fromSerializableMap(data);
     }
+
+    public static Event[] deserializeMany(byte[] data) throws IOException {
+        if (data == null || data.length ==0) {
+            return new Event[0];
+        }
+
+        ByteArrayInputStream bais = new ByteArrayInputStream(data);
+        return deserializeMany(bais);
+    }
+
+    public static Event[] deserializeMany(InputStream data) throws IOException {
+        return fromSerializableArrayOfMaps(data);
+    }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ObjectMappers.java b/logstash-core/src/main/java/org/logstash/ObjectMappers.java
index b3ff39aab26..4335834444a 100644
--- a/logstash-core/src/main/java/org/logstash/ObjectMappers.java
+++ b/logstash-core/src/main/java/org/logstash/ObjectMappers.java
@@ -18,6 +18,7 @@
 import java.io.IOException;
 import java.math.BigDecimal;
 import java.math.BigInteger;
+import java.util.ArrayList;
 import java.util.HashMap;
 import org.jruby.RubyBignum;
 import org.jruby.RubyBoolean;
@@ -62,6 +63,9 @@ public final class ObjectMappers {
     public static final JavaType EVENT_MAP_TYPE =
         CBOR_MAPPER.getTypeFactory().constructMapType(HashMap.class, String.class, Object.class);
 
+    public static final JavaType EVENT_LIST_TYPE =
+        CBOR_MAPPER.getTypeFactory().constructArrayType(ArrayList.class);
+
     private ObjectMappers() {
     }
 
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/Consumer.java b/logstash-core/src/main/java/org/logstash/elastiqueue/Consumer.java
new file mode 100644
index 00000000000..b0b2176cb0f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/Consumer.java
@@ -0,0 +1,340 @@
+package org.logstash.elastiqueue;
+
+import com.fasterxml.jackson.core.JsonFactory;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.http.client.entity.EntityBuilder;
+import org.elasticsearch.client.Response;
+import org.jruby.RubyArray;
+import org.jruby.RubyObject;
+import org.logstash.Event;
+import org.logstash.RubyUtil;
+import org.logstash.ext.JrubyEventExtLibrary;
+
+import java.io.*;
+import java.util.*;
+import java.util.concurrent.*;
+import java.util.function.Function;
+import java.util.stream.Stream;
+import java.util.zip.GZIPInputStream;
+
+public class Consumer implements AutoCloseable {
+    private final Elastiqueue elastiqueue;
+    private final Topic topic;
+    private final String consumerId;
+    private final int prefetchAmount;
+    private final Thread prefetchThread;
+    private final ArrayBlockingQueue<Object> prefetchWakeup;
+    private final ConsumerGroup consumerGroup;
+    private final List<Thread> consumerThreads;
+    private volatile long offset;
+    private Map<Partition,Long> partitionOffsets = new ConcurrentHashMap<>();
+    private Map<String, Partition> partitionsByIndexName = new ConcurrentHashMap<>();
+    private Map<Partition,Long> partitionLastPrefetchOffsets = new ConcurrentHashMap<>();
+    private Map<Partition, BlockingQueue<CompressedEventsWithSeq>> topicPrefetch;
+    private volatile boolean running = true;
+
+    Consumer(Elastiqueue elastiqueue, Topic topic, String consumerGroupName, String name) throws IOException {
+        this.elastiqueue = elastiqueue;
+        this.topic = topic;
+        this.consumerId = name;
+        this.offset = 0;
+        this.prefetchAmount = 10000;
+        this.consumerGroup = new ConsumerGroup(topic, consumerGroupName);
+        this.topicPrefetch = new HashMap<>();
+
+        topic.getPartitions().forEach(p -> {
+            topicPrefetch.put(p, new ArrayBlockingQueue<>(prefetchAmount / topic.getNumPartitions()));
+            partitionOffsets.put(p, (long) 0);
+            partitionLastPrefetchOffsets.put(p, (long) -1);
+            partitionsByIndexName.put(p.getIndexName(), p);
+        });
+
+        this.prefetchWakeup = new ArrayBlockingQueue<>(1);
+
+        this.prefetchThread = new Thread(new Runnable() {
+            @Override
+            public void run() {
+                while(running) {
+                    fillPrefetch();
+                    try {
+                        prefetchWakeup.poll(100, TimeUnit.MILLISECONDS);
+                    } catch(InterruptedException ex) {
+                        // Shouldn't happen
+                        ex.printStackTrace();
+                    }
+                }
+            }
+        }, "Consumer Prefetcher");
+
+        this.consumerThreads = new ArrayList<Thread>(topic.getNumPartitions());
+        prefetchThread.start();
+    }
+
+    public void consumePartitions(java.util.function.Consumer<EventsWithSeq> func) {
+        for (Partition p : this.partitionsByIndexName.values()) {
+            Thread t = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    BlockingQueue<CompressedEventsWithSeq> prefetchQueue = topicPrefetch.get(p);
+                    while (running) {
+                        try {
+                            CompressedEventsWithSeq polled = prefetchQueue.poll(100, TimeUnit.MILLISECONDS);
+                            if (polled != null) {
+                                EventsWithSeq deserialized = polled.deserialize();
+                                //System.out.println("Consumed" + deserialized.getLastSeq());
+                                func.accept(deserialized);
+                                deserialized.setOffset();
+                            }
+                        } catch (InterruptedException | IOException e) {
+                            throw new RuntimeException(e);
+                        }
+                    }
+                }
+            }, "Partition Consumer " + this.topic.getName() + "/" + this.getConsumerGroup().getName() + "/" + p.getNumber());
+            this.consumerThreads.add(t);
+            t.start();
+        }
+    }
+
+    public void rubyConsumePartitions(Function<RubyArray, RubyObject> func) {
+        consumePartitions(eventsWithSeq -> func.apply(eventsWithSeq.getRubyEvents()));
+    }
+
+    public ConsumerGroup getConsumerGroup() {
+        return consumerGroup;
+    }
+
+    @Override
+    public void close() throws Exception {
+        this.running = false;
+        this.consumerGroup.close();
+        this.prefetchThread.join();
+        for (Thread t : this.consumerThreads) {
+            t.join();
+        }
+    }
+
+    private class DocumentUrl {
+        private final String indexName;
+        private final String id;
+
+        public DocumentUrl(String indexName, String id) {
+            this.indexName = indexName;
+            this.id = id;
+        }
+
+        public String getIndexName() {
+            return indexName;
+        }
+
+        public String getType() {
+            return "doc";
+        }
+
+        public String getId() {
+            return id;
+        }
+
+        public String toString() {
+            return "DocUrl<" + getIndexName() + "/" + getType() + "/" + getId() + ">";
+        }
+    }
+
+    private void fillPrefetch() {
+        Map<DocumentUrl, Partition> docsToPartitions = new HashMap<>(prefetchAmount *topic.getNumPartitions());
+
+        Collection<Partition> partitions = topic.getPartitions();
+        List<Partition> shuffled = new ArrayList<>(partitions);
+        Collections.shuffle(shuffled);
+        for (Partition partition : shuffled) {
+            BlockingQueue<CompressedEventsWithSeq> partitionPrefetch = topicPrefetch.get(partition);
+            if (!consumerGroup.isPartitionLocallyActive(partition)) {
+                continue;
+            }
+
+            int neededPrefetches = prefetchAmount - partitionPrefetch.size(); // .size is slow for juc, we should count ourselves
+            List<DocumentUrl> docUrls = new ArrayList<>();
+            for (long i = 0; i < neededPrefetches; i++) {
+                Long lastPrefetch = consumerGroup.getPrefetchOffsetFor(partition);
+                //System.out.println("LASTPREFETCH" + lastPrefetch);
+                long nextPrefetch = lastPrefetch + i + 1;
+                DocumentUrl docUrl = new DocumentUrl(partition.getIndexName(), Long.toString(nextPrefetch));
+                docUrls.add(docUrl);
+            }
+
+            if (docUrls.isEmpty()) {
+                continue;
+            }
+
+            prefetchDocumentUrls(docUrls);
+        }
+    }
+
+    static ConcurrentHashMap<String, Long> seenSeqs = new ConcurrentHashMap<>();
+
+    public void prefetchDocumentUrls(List<DocumentUrl> docUrls) {
+        try(ByteArrayOutputStream baos = new ByteArrayOutputStream()) {
+            JsonGenerator jg = new JsonFactory().createGenerator(baos);
+            jg.writeStartObject();
+            jg.writeFieldName("docs");
+            jg.writeStartArray();
+            for (DocumentUrl docUrl : docUrls) {
+                //System.out.println(docUrl);
+                jg.writeStartObject();
+                jg.writeStringField("_index", docUrl.getIndexName());
+                jg.writeStringField("_type", docUrl.getType());
+                jg.writeStringField("_id", docUrl.getId());
+                jg.writeEndObject();
+            }
+            jg.writeEndArray();
+            jg.close();
+
+            Map<Partition,List<CompressedEventsWithSeq>> resultsByPartition = new HashMap<>();
+            while (true) {
+                Response resp = elastiqueue.simpleRequest(
+                        "get",
+                        "/_mget",
+                        EntityBuilder.create().setBinary(baos.toByteArray()).setContentEncoding("UTF-8").build()
+                );
+                int responseCode = resp.getStatusLine().getStatusCode();
+                if (responseCode == 200) {
+                    //System.out.println("GOT 200");
+                    InputStream is = resp.getEntity().getContent();
+                    ObjectMapper om = new ObjectMapper();
+                    HashMap<String,Object> parsed = om.readValue(is, HashMap.class);
+                    List<Map<String, Object>> docs = (List) parsed.get("docs");
+                    Stream<Map<String, Object>> foundDocs = docs.stream().
+                            filter(d -> d.containsKey("_source"));
+                    // We can't just assume that we can advance the reader because there could be a gap
+                    // in the prefetch. For this stage we can ignore this, but this isn't prod quality
+                    // we need to check we prefetched without gaps before advancing an offset
+                    Base64.Decoder decoder = Base64.getDecoder();
+                    //Ordered so we update the seqs monotonically later
+                    //System.out.println("FOUND DOCS" + docs.stream().filter(d -> d.containsKey("_source")).count());
+                    foundDocs.forEach(doc -> {
+                        Event[] events;
+                        String index = (String) doc.get("_index");
+                        Partition partition = partitionsByIndexName.get(index);
+                        Long seq = Long.valueOf((String) doc.get("_id"));
+                        Map<String,Object> source = (Map<String, Object>) doc.get("_source");
+                        String eventsEncoded = (String) source.get("events");
+                        byte[] compressedEvents = decoder.decode(eventsEncoded);
+
+                        resultsByPartition.computeIfAbsent(partition, k -> new LinkedList<>());
+                        //System.out.println("ADD DOC" + seq);
+                        resultsByPartition.get(partition).add(new CompressedEventsWithSeq(partition, seq, compressedEvents));
+                    });
+
+                    for (Map.Entry<Partition, List<CompressedEventsWithSeq>> entry : resultsByPartition.entrySet()) {
+                        Partition partition = entry.getKey();
+                        List<CompressedEventsWithSeq> cewsList = entry.getValue();
+                        cewsList.sort(Comparator.comparingLong(o -> o.seq));
+                        BlockingQueue<CompressedEventsWithSeq> partitionPrefetch = topicPrefetch.get(partition);
+                        long fetchLastSeq = -1L;
+                        for (CompressedEventsWithSeq cews : cewsList) {
+                            if ((fetchLastSeq == -1L) || (cews.seq == (fetchLastSeq + 1))) {
+                                fetchLastSeq = cews.seq;
+                                partitionPrefetch.put(cews);
+                                //System.out.println("PREFETCH" + partition + cews.seq);
+                                consumerGroup.setPrefetchOffsetFor(partition, cews.seq);
+                            } else {
+                                System.err.println("OUT OF ORDER PREFETCH");
+                                // We are out of order, read from an out of sync replica? We'll refetch
+                                break;
+                            }
+                        }
+                    }
+                    break;
+                } else if (responseCode != 429) {
+                    break;
+                }
+            }
+        } catch (IOException e) {
+            throw new RuntimeException(e);
+        } catch (InterruptedException e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+
+    private static byte[] gzipUncompress(byte[] compressedData) {
+        byte[] result = new byte[]{};
+        try (ByteArrayInputStream bis = new ByteArrayInputStream(compressedData);
+             ByteArrayOutputStream bos = new ByteArrayOutputStream();
+             GZIPInputStream gzipIS = new GZIPInputStream(bis)) {
+            byte[] buffer = new byte[1024];
+            int len;
+            while ((len = gzipIS.read(buffer)) != -1) {
+                bos.write(buffer, 0, len);
+            }
+            result = bos.toByteArray();
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+        return result;
+    }
+
+    public class EventsWithSeq {
+        private final Partition partition;
+        private final long lastSeq;
+        private final Event[] events;
+
+        EventsWithSeq(Partition partition, long lastSeq, Event[] events) {
+            this.partition = partition;
+            this.lastSeq = lastSeq;
+            this.events = events;
+        }
+
+        public long getLastSeq() {
+            return lastSeq;
+        }
+
+        public Event[] getEvents() {
+            return events;
+        }
+
+        public RubyArray getRubyEvents() {
+            RubyArray arr = RubyArray.newArray(RubyUtil.RUBY, events.length);
+            for (Event e : events) {
+                JrubyEventExtLibrary.RubyEvent re = JrubyEventExtLibrary.RubyEvent.newRubyEvent(RubyUtil.RUBY, e);
+                arr.add(re);
+            }
+            return arr;
+        }
+
+        public void setOffset() {
+            consumerGroup.setOffset(partition, lastSeq);
+        }
+
+        public Partition getPartition() {
+            return partition;
+        }
+    }
+
+    private class CompressedEventsWithSeq {
+        private final byte[] compressedEvents;
+        private final long seq;
+        private final Partition partition;
+
+        CompressedEventsWithSeq(Partition partition, long seq, byte[] compressedEvents) {
+            this.partition = partition;
+            this.seq = seq;
+            this.compressedEvents = compressedEvents;
+        }
+
+        public EventsWithSeq deserialize() throws IOException {
+            try (ByteArrayInputStream bis = new ByteArrayInputStream(compressedEvents);
+                 ByteArrayOutputStream baos = new ByteArrayOutputStream();
+                 GZIPInputStream gzipIS = new GZIPInputStream(bis)) {
+                byte[] buffer = new byte[1024];
+                int len;
+                while ((len = gzipIS.read(buffer)) != -1) {
+                    baos.write(buffer, 0, len);
+                }
+                bis.close();
+                return new EventsWithSeq(partition, seq, Event.deserializeMany(baos.toByteArray()));
+            }
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/ConsumerGroup.java b/logstash-core/src/main/java/org/logstash/elastiqueue/ConsumerGroup.java
new file mode 100644
index 00000000000..7647bf6280c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/ConsumerGroup.java
@@ -0,0 +1,454 @@
+package org.logstash.elastiqueue;
+
+import com.fasterxml.jackson.annotation.JsonGetter;
+import com.fasterxml.jackson.annotation.JsonIgnore;
+import com.fasterxml.jackson.annotation.JsonProperty;
+import com.fasterxml.jackson.core.JsonFactory;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.elasticsearch.client.Response;
+import org.elasticsearch.client.ResponseException;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.UUID;
+import java.util.concurrent.ConcurrentHashMap;
+
+public class ConsumerGroup implements AutoCloseable {
+    private static ObjectMapper objectMapper = new ObjectMapper();
+
+    private final long instantiatedAt;
+    private final Thread partitionStateKeeper;
+    private Topic topic;
+    private final Elastiqueue elastiqueue;
+    private final String name;
+    private final Map<Partition, ConsumerGroupPartitionState> partitionStates;
+    private final Map<Partition, Long> partitionStatesInternalClock = new ConcurrentHashMap<>();
+    private final String consumerUUID;
+    private volatile boolean shutdown = false;
+
+    ConsumerGroup(Topic topic, String name) throws IOException {
+        this.topic = topic;
+        this.elastiqueue = topic.getElastiqueue();
+        this.name = name;
+        this.instantiatedAt = System.currentTimeMillis();
+        this.consumerUUID = UUID.randomUUID().toString();
+        ensureDocument();
+
+        this.partitionStates = new HashMap<>();
+        for (Partition partition : topic.getPartitions()) {
+            ConsumerGroupPartitionState partitionState = new ConsumerGroupPartitionState(this, partition);
+            partitionStates.put(partition, partitionState);
+            partitionStatesInternalClock.put(partition, partitionState.getClock());
+        }
+
+        this.partitionStateKeeper = new Thread(() -> {
+            while (!shutdown) {
+                try {
+                    Thread.sleep(1000);
+
+                    for (Map.Entry<Partition, ConsumerGroupPartitionState> entry : partitionStates.entrySet()) {
+                        Partition partition = entry.getKey();
+                        ConsumerGroupPartitionState partitionState = entry.getValue();
+
+                        partitionState.refresh();
+
+                        Long internalClock = partitionStatesInternalClock.get(partition);
+                        internalClock += 1;
+
+                        // If already owned and in use
+                        if (partitionState.getConsumerUUID().equals(this.getConsumerUUID())) {
+                            if (partitionState.getTakeoverClock() != null && partitionState.getTakeoverClock() > 0 && partitionState.getTakeoverClock() < internalClock) {
+                                System.out.println("Relinquishing control!" + partitionState);
+                                partitionState.relinquishControl();
+                            } else {
+                                partitionState.updateRemote();
+                            }
+                        } else {
+                            Long partClock = partitionState.getClock();
+                            System.out.println("Internal clock compare" +  partClock + " | " + internalClock);
+                            if (partClock == null || internalClock > (partClock + 10L)) {
+                                System.out.println("Taking over!" + partitionState);
+                                partitionState.executeTakeover();
+                            }
+                        }
+
+                        partitionStatesInternalClock.put(partition, internalClock);
+                    }
+                } catch (InterruptedException e) {
+                    e.printStackTrace();
+                } catch (IOException e) {
+                    e.printStackTrace();
+                }
+            }
+        }, "Partition State Keeper " + this);
+        this.partitionStateKeeper.start();
+    }
+
+    public String toString() {
+        return "ConsumerGroup(" + this.getTopicName() + this.getName() + ")";
+    }
+
+    @JsonGetter("topic")
+    public String getTopicName() {
+        return this.topic.getName();
+    }
+
+    @JsonGetter("name")
+    public String getName() {
+        return this.name;
+    }
+
+    private void ensureDocument() throws IOException {
+        //String body = objectMapper.writeValueAsString(newGroup);
+        ConsumerGroup.ConsumerGroupDoc wrapped = new ConsumerGroup.ConsumerGroupDoc(this);
+        String body = objectMapper.writeValueAsString(wrapped);
+        // what if the create fails?
+        try {
+            Response createResp = elastiqueue.simpleRequest(
+                    "post",
+                    "/" + url() + "/_create",
+                    body
+            );
+        } catch (ResponseException re) {
+            // Not an error!
+            if (re.getResponse().getStatusLine().getStatusCode() == 409) return;
+            //throw new IOException(re);
+        }
+    }
+
+    private String url() {
+        return topic.getMetadataIndexName() + "/doc/consumer_group_" + this.name;
+    }
+
+    @JsonGetter("created_at")
+    public long instantiatedAt() {
+        return instantiatedAt;
+    }
+
+    public Topic getTopic() {
+        return topic;
+    }
+
+    public String getConsumerUUID() {
+        return consumerUUID;
+    }
+
+    @Override
+    public void close() throws Exception {
+        this.shutdown = true;
+        this.partitionStateKeeper.join();
+    }
+
+    public void setOffset(Partition partition, long lastSeq) {
+        this.partitionStates.get(partition).setOffset(lastSeq);
+    }
+
+    public boolean isPartitionLocallyActive(Partition partition) {
+        return this.partitionStates.get(partition).isLocallyActive();
+    }
+
+    public Long getPrefetchOffsetFor(Partition partition) {
+        return this.partitionStates.get(partition).getPrefetchOffset();
+    }
+
+    public void setPrefetchOffsetFor(Partition partition, Long offset) {
+        this.partitionStates.get(partition).setPrefetchOffset(offset);
+    }
+
+    static class ConsumerGroupPartitionState {
+        @JsonIgnore
+        private final ConsumerGroup consumerGroup;
+        private final Partition partition;
+        private final String url;
+        private String consumerUUID;
+        @JsonIgnore
+        private String consumerName;
+        private volatile Long offset = -1L;
+        @JsonIgnore
+        private volatile Long prefetchOffset;
+        private Long clock = 0L;
+        private Long consumerLastUpdate = System.currentTimeMillis();
+        private Long takeoverClock = null;
+        private String takeoverBy;
+        private Elastiqueue elastiqueue;
+
+        public String getConsumerName() {
+            return consumerName;
+        }
+
+        public void setConsumerName(String consumerName) {
+            this.consumerName = consumerName;
+        }
+
+        ConsumerGroupPartitionState(ConsumerGroup group, Partition partition) throws IOException {
+            this.consumerGroup = group;
+            this.partition = partition;
+            this.consumerName = consumerGroup.getName();
+            this.consumerUUID = consumerGroup.getConsumerUUID();
+            this.clock = 0L;
+            this.url = consumerGroup.getTopic().getMetadataIndexName() +
+                    "/doc/consumer_group_partition_" +
+                    getPartitionNumber();
+
+            ensureDocumentExists();
+            refresh();
+            stateTakeoverIntent();
+        }
+
+        public String toString() {
+            String base =  "PartitionState(" + this.getPartitionNumber() + "@" + " " +
+                    this.getOffset() + ") " +
+                    this.consumerName + "<" + this.consumerUUID + "> ";
+
+            if (this.takeoverBy != null) {
+                return base + " TAKEOVER " +
+                this.takeoverBy + "<" + this.takeoverClock + "> ";
+            } else {
+                return base;
+            }
+        }
+
+        public void refresh() throws IOException {
+            Response resp = elastiqueue.simpleRequest("GET", "/" + url + "/_source");
+            Map<String, Map<String, Object>> deserialized = objectMapper.readValue(resp.getEntity().getContent(), HashMap.class);
+            Map<String, Object> fields = deserialized.get("consumer_group_partition");
+            this.consumerName = (String) fields.get("consumer_name");
+            this.consumerUUID = (String) fields.get("consumer_uuid");
+
+            // The authoritative offset is local in this case, ot remote
+            if (!this.isLocallyActive()) {
+                this.offset = fields.get("offset") != null ? ((Number) fields.get("offset")).longValue() : null;
+                this.prefetchOffset = null;
+                System.out.println("Not locally active, syncing offset" + this);
+            } else {
+                if (this.prefetchOffset == null) {
+                    this.prefetchOffset = this.offset;
+                }
+            }
+
+            this.clock = fields.get("clock") != null ? ((Number) fields.get("clock")).longValue() : null;
+            this.consumerLastUpdate = fields.get("consumer_last_update") != null ? ((Number) fields.get("consumer_last_update")).longValue() : null;
+            this.takeoverClock = fields.get("takeover_clock") != null ? ((Number) fields.get("takeover_clock")).longValue() : null;
+            this.takeoverBy = (String) fields.get("takeover_uuid");
+        }
+
+        public void executeScript(String script) throws IOException {
+            executeScript(script, null);
+        }
+
+        public void executeScript(String script, Map<String, Object> argParams) throws IOException {
+            Map<String, Object> params = new HashMap<>();
+            params.put("consumer_uuid", this.consumerGroup.getConsumerUUID());
+            params.put("consumer_name", this.consumerGroup.getConsumerUUID());
+
+            if (argParams != null) {
+                params.putAll(argParams);
+            }
+
+            JsonFactory jsonFactory = new JsonFactory();
+            byte[] requestSource;
+            try (
+                ByteArrayOutputStream baos = new ByteArrayOutputStream();
+                JsonGenerator jsonGenerator = jsonFactory.createGenerator(baos);
+            ) {
+                jsonGenerator.writeStartObject();
+
+                // Open script
+                jsonGenerator.writeObjectFieldStart("script");
+
+                jsonGenerator.writeStringField("source", script);
+                jsonGenerator.writeStringField("lang", "painless");
+                jsonGenerator.writeFieldName("params");
+                objectMapper.writeValue(jsonGenerator, params);
+
+                // Close script
+                jsonGenerator.writeEndObject();
+
+                // Close outer object
+                jsonGenerator.writeEndObject();
+
+                jsonGenerator.close();
+                requestSource = baos.toByteArray();
+            }
+            String s = new String(requestSource);
+            //System.out.println("Execute Script " + s);
+            elastiqueue.simpleRequest("POST", "/" + url + "/_update", requestSource);
+        }
+
+        public void updateRemote() throws IOException {
+            String script = "if (ctx._source.consumer_group_partition.consumer_uuid == params.consumer_uuid) { " +
+                        "  ctx._source.consumer_group_partition.clock += 1; " +
+                        "  ctx._source.consumer_group_partition.offset = params.offset; " +
+                        "}";
+
+            executeScript(script, Collections.singletonMap("offset", consumerGroup.partitionStates.get(partition).getOffset()));
+        }
+
+        public void stateTakeoverIntent() throws IOException {
+           String script = "if (ctx._source.consumer_group_partition.consumer_uuid != params.consumer_uuid) { " +
+                        "  ctx._source.consumer_group_partition.takeover_uuid = params.consumer_uuid; " +
+                        "  ctx._source.consumer_group_partition.takeover_name = params.consumer_name; " +
+                        "  ctx._source.consumer_group_partition.takeover_clock = ctx._source.consumer_group_partition.clock + 10L; " +
+                        "}";
+
+           executeScript(script);
+        }
+
+        public void executeTakeover() throws IOException {
+            String script = "if (ctx._source.consumer_group_partition.consumer_uuid != params.consumer_uuid) {" +
+                                // If this consumer is registered to takeover and the clock has counted up the original should be dead
+                                "if ( ctx._source.consumer_group_partition.takeover_uuid == params.consumer_uuid) {" +
+                                "  ctx._source.consumer_group_partition.consumer_uuid = params.consumer_uuid; " +
+                                "  ctx._source.consumer_group_partition.consumer_name = params.consumer_name; " +
+                                "  ctx._source.consumer_group_partition.takeover_uuid = null; " +
+                                "  ctx._source.consumer_group_partition.takeover_name = null; " +
+                                "  ctx._source.consumer_group_partition.takeover_clock = null; " +
+                                // stake takeover intent if the consumer staging registered to execute the attempt has failed
+                                "} else {" +
+                                "  ctx._source.consumer_group_partition.takeover_uuid = params.consumer_uuid; " +
+                                "  ctx._source.consumer_group_partition.takeover_name = params.consumer_name; " +
+                                "  ctx._source.consumer_group_partition.takeover_clock = ctx._source.consumer_group_partition.clock + 10L; " +
+                                "}" +
+                            "}";
+            executeScript(script);
+        }
+
+        public void relinquishControl() throws IOException {
+            String script = "if (ctx._source.consumer_group_partition.takeover_uuid != null) {" +
+                                "  ctx._source.consumer_group_partition.consumer_uuid = ctx._source.consumer_group_partition.takeover_uuid; " +
+                                "  ctx._source.consumer_group_partition.consumer_name = ctx._source.consumer_group_partition.takeover_name; " +
+                                "  ctx._source.consumer_group_partition.takeover_uuid = null; " +
+                                "  ctx._source.consumer_group_partition.takeover_name = null; " +
+                            "}";
+            executeScript(script);
+        }
+
+        private void ensureDocumentExists() throws IOException {
+            ConsumerGroupPartitionStateDoc wrapped = new ConsumerGroupPartitionStateDoc(this);
+            this.elastiqueue = consumerGroup.getTopic().getElastiqueue();
+            String json = objectMapper.writeValueAsString(wrapped);
+            try {
+                elastiqueue.simpleRequest("POST", "/" + url + "/_create", json);
+            } catch (ResponseException e) {
+                if (e.getResponse().getStatusLine().getStatusCode() == 409) return;
+                throw e;
+            }
+
+        }
+
+
+        @JsonGetter("topic")
+        public String getTopicName() {
+            return consumerGroup.getTopicName();
+        }
+
+        @JsonGetter("partition")
+        public int getPartitionNumber() {
+            return partition.getNumber();
+        }
+
+        @JsonGetter("consumer_group")
+        public String getConsumerGroupName() {
+            return consumerGroup.getName();
+        }
+
+        @JsonProperty("consumer_uuid")
+        public String getConsumerUUID() {
+            return this.consumerUUID;
+        }
+
+        @JsonProperty("consumer_uuid")
+        public void setConsumerUUID(String consumerUUID) {
+            this.consumerUUID = consumerUUID;
+        }
+
+        public ConsumerGroup getConsumerGroup() {
+            return consumerGroup;
+        }
+
+        @JsonProperty("offset")
+        public Long getOffset() {
+            return offset;
+        }
+
+        @JsonProperty("offset")
+        public void setOffset(long consumerOffset) {
+            this.offset = consumerOffset;
+        }
+
+        @JsonProperty("clock")
+        public Long getClock() {
+            return clock;
+        }
+
+        @JsonProperty("clock")
+        public void setClock(long clock) {
+            this.clock = clock;
+        }
+
+        @JsonProperty("consumer_last_update")
+        public Long getConsumerLastUpdate() {
+            return consumerLastUpdate;
+        }
+
+        @JsonProperty("consumer_last_update")
+        public void setConsumerLastUpdate(long consumerLastUpdate) {
+            this.consumerLastUpdate = consumerLastUpdate;
+        }
+
+        @JsonProperty("takeover_clock")
+        public Long getTakeoverClock() {
+            return takeoverClock;
+        }
+
+        @JsonProperty("takeover_clock")
+        public void setTakeoverClock(long takeoverClock) {
+            this.takeoverClock = takeoverClock;
+        }
+
+        @JsonProperty("takeover_uuid")
+        public String getTakeoverBy() {
+            return takeoverBy;
+        }
+
+        @JsonProperty("takeover_uuid")
+        public void setTakeoverBy(String takeoverBy) {
+            this.takeoverBy = takeoverBy;
+        }
+
+        public boolean isLocallyActive() {
+            return this.consumerUUID.equals(consumerGroup.consumerUUID);
+        }
+
+        public Long getPrefetchOffset() {
+            return prefetchOffset;
+        }
+
+        public void setPrefetchOffset(Long prefetchOffset) {
+            this.prefetchOffset = prefetchOffset;
+        }
+
+
+        public static class ConsumerGroupPartitionStateDoc {
+            @JsonProperty("consumer_group_partition")
+            public ConsumerGroupPartitionState consumerGroupPartitionState;
+
+            ConsumerGroupPartitionStateDoc(ConsumerGroupPartitionState consumerGroupPartitionState) {
+                this.consumerGroupPartitionState = consumerGroupPartitionState;
+            }
+        }
+    }
+
+    public static class ConsumerGroupDoc {
+        @JsonProperty("consumer_group")
+        public ConsumerGroup consumerGroup;
+
+        ConsumerGroupDoc(ConsumerGroup consumerGroup) {
+            this.consumerGroup = consumerGroup;
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/ConsumerGroupStateManager.java b/logstash-core/src/main/java/org/logstash/elastiqueue/ConsumerGroupStateManager.java
new file mode 100644
index 00000000000..ca2a9c70dcc
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/ConsumerGroupStateManager.java
@@ -0,0 +1,4 @@
+package org.logstash.elastiqueue;
+
+public class ConsumerGroupStateManager {
+}
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/Elastiqueue.java b/logstash-core/src/main/java/org/logstash/elastiqueue/Elastiqueue.java
new file mode 100644
index 00000000000..0288a98d4da
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/Elastiqueue.java
@@ -0,0 +1,130 @@
+package org.logstash.elastiqueue;
+
+import org.apache.http.Header;
+import org.apache.http.HttpEntity;
+import org.apache.http.HttpHost;
+import org.apache.http.auth.AuthScope;
+import org.apache.http.auth.UsernamePasswordCredentials;
+import org.apache.http.client.CredentialsProvider;
+import org.apache.http.entity.StringEntity;
+import org.apache.http.impl.client.BasicCredentialsProvider;
+import org.apache.http.impl.nio.client.HttpAsyncClientBuilder;
+import org.apache.http.message.BasicHeader;
+import org.elasticsearch.client.Response;
+import org.elasticsearch.client.RestClient;
+import org.elasticsearch.client.RestClientBuilder;
+import org.jruby.RubyArray;
+import org.jruby.RubyString;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.io.InputStream;
+import java.io.UnsupportedEncodingException;
+import java.net.URL;
+import java.util.*;
+import java.util.concurrent.BlockingQueue;
+import java.util.stream.Collectors;
+import java.util.stream.Stream;
+
+public class Elastiqueue implements Closeable {
+    private final RestClient client;
+    private static final Header defaultHeaders[] = {
+            new BasicHeader("Content-Type", "application/json"),
+    };
+
+    public static Elastiqueue make(String username, String password, RubyArray hosts) throws IOException {
+        String[] javaHosts = new String[hosts.size()];
+        for (int i = 0; i < hosts.size(); i++) {
+            javaHosts[i] = hosts.get(i).toString();
+        }
+        return make(username, password, javaHosts);
+    }
+
+    public static Elastiqueue make(String... hostStrings) throws IOException {
+        return make(null, null, hostStrings);
+    }
+
+    public static Elastiqueue make(String username, String password, String... hostStrings) throws IOException {
+        CredentialsProvider credentialsProvider = null;
+        if (username != null && password != null) {
+            credentialsProvider = new BasicCredentialsProvider();
+            credentialsProvider.setCredentials(AuthScope.ANY, new UsernamePasswordCredentials(username, password));
+        }
+
+        HttpHost[] hosts = new HttpHost[hostStrings.length];
+        for (int i = 0; i < hostStrings.length; i++) {
+            hosts[i] = HttpHost.create(hostStrings[i]);
+        }
+        return new Elastiqueue(credentialsProvider, hosts);
+    }
+
+    public Elastiqueue(CredentialsProvider credentialsProvider, HttpHost... hosts) throws IOException {
+        client = RestClient.builder(hosts)
+                .setHttpClientConfigCallback(httpClientBuilder -> {
+                    if (credentialsProvider != null) {
+                        httpClientBuilder.setDefaultCredentialsProvider(credentialsProvider);
+                    }
+                    return httpClientBuilder;
+                })
+                .build();
+        setup();
+    }
+     public Topic topic(String name, int numPartitions) {
+        return new Topic(this, name, numPartitions);
+     }
+
+    private void setup() throws IOException {
+        putTemplate("elastiqueue_segments");
+        putTemplate("elastiqueue_meta");
+    }
+
+    private void putTemplate(String id) throws IOException {
+        simpleRequest(
+                "put",
+                "/_template/" + id,
+                new StringEntity(template(id))
+        );
+
+    }
+
+    Response simpleRequest(String method, String endpoint, HttpEntity body) throws IOException {
+        return client.performRequest(method, endpoint, Collections.emptyMap(), body, defaultHeaders);
+    }
+
+    Response simpleRequest(String method, String endpoint, String body) throws IOException {
+        StringEntity bodyEntity = new StringEntity(body);
+        return simpleRequest(method, endpoint, bodyEntity);
+    }
+
+    Response simpleRequest(String method, String endpoint) throws IOException {
+        return client.performRequest(method, endpoint, Collections.emptyMap(), new StringEntity(""), defaultHeaders);
+    }
+
+    Response simpleRequest(String method, String endpoint, byte[] body) throws IOException {
+        return simpleRequest(method, endpoint, new String(body, "UTF-8"));
+    }
+
+    private String template(String id) throws IOException {
+        String templateFilename = id + "_template.json";
+        URL resource = getClass().getClassLoader().getResource(templateFilename);
+
+        if (resource == null) {
+            throw new IOException("Could not find template resource " + templateFilename);
+        }
+
+        try (InputStream is = resource.openStream()) {
+            return new Scanner(is, "UTF-8").useDelimiter("\\A").next();
+        }
+    }
+
+    public RestClient getClient() {
+        return client;
+    }
+
+    @Override
+    public void close() throws IOException {
+        client.close();
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/Partition.java b/logstash-core/src/main/java/org/logstash/elastiqueue/Partition.java
new file mode 100644
index 00000000000..e21fffa6cc5
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/Partition.java
@@ -0,0 +1,50 @@
+package org.logstash.elastiqueue;
+
+import java.io.Closeable;
+import java.io.IOException;
+import java.util.concurrent.locks.ReentrantLock;
+
+public class Partition implements Closeable {
+    private final Elastiqueue elastiqueue;
+    private final Topic topic;
+    private final int number;
+    private final String indexName;
+    private volatile long seq = -1;
+    private static String PARTITION_PREFIX = "elastiqueue-partition-";
+
+    public Partition(Elastiqueue elastiqueue, Topic topic, int partitionId) {
+        this.elastiqueue = elastiqueue;
+        this.topic = topic;
+        this.number = partitionId;
+        this.indexName = PARTITION_PREFIX + topic.getName() + "-" + Integer.toString(partitionId);
+    }
+
+    @Override
+    public void close() throws IOException {
+        try {
+            this.topic.returnPartitionToWritePool(this);
+        } catch (InterruptedException e) {
+            throw new IOException("Could not return partition to write pool", e);
+        }
+    }
+
+    public String getIndexName() {
+        return indexName;
+    }
+
+    public long getSeq() {
+        return seq;
+    }
+
+    public void setSeq(long num) {
+        seq = num;
+    }
+
+    public String toString() {
+        return "Partition<" + this.topic.getName() + "/" + this.number + ">";
+    }
+
+    public int getNumber() {
+        return number;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/Producer.java b/logstash-core/src/main/java/org/logstash/elastiqueue/Producer.java
new file mode 100644
index 00000000000..8ace19ee432
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/Producer.java
@@ -0,0 +1,110 @@
+package org.logstash.elastiqueue;
+
+import com.fasterxml.jackson.core.JsonEncoding;
+import com.fasterxml.jackson.core.JsonFactory;
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.core.JsonProcessingException;
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.http.HttpEntity;
+import org.apache.http.client.entity.EntityBuilder;
+import org.apache.http.entity.BasicHttpEntity;
+import org.elasticsearch.client.Response;
+import org.jruby.RubyArray;
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+
+import java.io.ByteArrayOutputStream;
+import java.io.IOException;
+import java.time.Instant;
+import java.time.ZoneOffset;
+import java.time.format.DateTimeFormatter;
+import java.util.*;
+import java.util.concurrent.ConcurrentHashMap;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.zip.GZIPOutputStream;
+
+public class Producer implements AutoCloseable {
+    private final Elastiqueue elastiqueue;
+    private final Topic topic;
+    private final String producerId;
+    private final static DateTimeFormatter dateFormatter = DateTimeFormatter.ofPattern("yyyy-MM-dd'T'HH:mmX")
+            .withZone(ZoneOffset.UTC);
+    private final AtomicLong writeOps;
+
+    public Producer(Elastiqueue elastiqueue, Topic topic, String producerId) {
+        this.elastiqueue = elastiqueue;
+        this.topic = topic;
+        this.producerId = producerId;
+        this.writeOps = new AtomicLong();
+    }
+
+    public void rubyWrite(RubyArray events) throws IOException, InterruptedException {
+        Event[] javaEvents = new Event[events.size()];
+        int size = events.size();
+        for (int i = 0; i < size; i++) {
+            JrubyEventExtLibrary.RubyEvent rubyEvent = (JrubyEventExtLibrary.RubyEvent) events.get(i);
+            javaEvents[i] = rubyEvent.getEvent();
+        }
+        write(javaEvents);
+    }
+
+    public long write(Event... events) throws IOException, InterruptedException {
+        try (Partition partition = topic.getWritablePartition();
+             ByteArrayOutputStream baos = new ByteArrayOutputStream();
+             JsonGenerator jf = new JsonFactory().createGenerator(baos, JsonEncoding.UTF8)) {
+
+            jf.writeStartObject();
+
+            jf.writeStringField("@timestamp", dateFormatter.format(Instant.now()));
+            final long seq = partition.getSeq()+1;
+            jf.writeNumberField("seq", seq);
+            jf.writeNumberField("event_count", events.length);
+
+            jf.writeBinaryField("events", gzipCompress(Event.serializeMany(events)));
+
+            jf.writeEndObject();
+
+            jf.close();
+
+            while (true) {
+                HttpEntity body = EntityBuilder.create().setBinary(baos.toByteArray()).build();
+                String docUrl = String.format("/%s/doc/%d", partition.getIndexName(), seq);
+
+                //System.out.println("DOC URL " + docUrl);
+                Response response = elastiqueue.simpleRequest("put", docUrl, body);
+                //System.out.println(writeOps.incrementAndGet());
+                int code = response.getStatusLine().getStatusCode();
+                //System.out.println("DOC RESP" + response.getStatusLine().getStatusCode());
+                if (code != 201 && code != 200) {
+                    System.out.println("CODE" + code);
+                }
+                if (code != 429) {
+                    break;
+                }
+                Thread.sleep(100);
+            }
+
+            // Only set once we're done writing do we increment
+            partition.setSeq(seq);
+
+            return seq;
+        }
+    }
+
+    private static byte[] gzipCompress(byte[] uncompressedData) {
+        byte[] result = new byte[]{};
+        try (ByteArrayOutputStream bos = new ByteArrayOutputStream(uncompressedData.length);
+             GZIPOutputStream gzipOS = new GZIPOutputStream(bos)) {
+            gzipOS.write(uncompressedData);
+            gzipOS.close();
+            result = bos.toByteArray();
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+        return result;
+    }
+
+    @Override
+    public void close() throws Exception {
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/Topic.java b/logstash-core/src/main/java/org/logstash/elastiqueue/Topic.java
new file mode 100644
index 00000000000..40155032da3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/Topic.java
@@ -0,0 +1,71 @@
+package org.logstash.elastiqueue;
+
+import java.io.IOException;
+import java.util.*;
+import java.util.concurrent.ArrayBlockingQueue;
+import java.util.concurrent.BlockingQueue;
+
+public class Topic {
+    private final Elastiqueue elastiqueue;
+    private final String name;
+    private final List<Partition> partitions = new ArrayList<>();
+    private final BlockingQueue<Partition> writablePartitions;
+    private final Object metadataIndexName;
+    private int numPartitions;
+
+    public Topic(final Elastiqueue elastiqueue, String name, int numPartitions) {
+        this.elastiqueue = elastiqueue;
+        this.name = name;
+        this.numPartitions = numPartitions;
+        this.metadataIndexName = "elastiqueue-metadata-" + getName();
+        writablePartitions = new ArrayBlockingQueue<>(this.numPartitions);
+        for (int i = 0; i < numPartitions; i++) {
+            Partition p = new Partition(elastiqueue, this, i);
+            partitions.add(p);
+            try {
+                writablePartitions.put(p);
+            } catch (InterruptedException e) {
+                // Should never happen
+                throw new RuntimeException(e);
+            }
+        }
+    }
+    
+    
+
+    public Producer makeProducer(String producerId) {
+        return new Producer(elastiqueue, this, producerId);
+    }
+
+    public Collection<Partition> getPartitions() {
+        return partitions;
+    }
+
+    public Partition getWritablePartition() throws InterruptedException {
+        return writablePartitions.take();
+    }
+
+    public void returnPartitionToWritePool(Partition partition) throws InterruptedException {
+        writablePartitions.put(partition);
+    }
+
+    public String getName() {
+        return name;
+    }
+
+    public int getNumPartitions() {
+        return numPartitions;
+    }
+
+    public Consumer makeConsumer(String consumerGroupName, String consumerId) throws IOException {
+        return new Consumer(elastiqueue, this, consumerGroupName, consumerId);
+    }
+
+    public Elastiqueue getElastiqueue() {
+        return elastiqueue;
+    }
+
+    public Object getMetadataIndexName() {
+        return metadataIndexName;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/elastiqueue/Util.java b/logstash-core/src/main/java/org/logstash/elastiqueue/Util.java
new file mode 100644
index 00000000000..10ad1772711
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/elastiqueue/Util.java
@@ -0,0 +1,12 @@
+package org.logstash.elastiqueue;
+
+import java.io.InputStream;
+import java.util.Scanner;
+
+public class Util {
+    public static String inputStringToStream(InputStream stream) {
+        try (Scanner s = new Scanner(stream)) {
+            return s.useDelimiter("\\A").hasNext() ? s.next() : null;
+        }
+    }
+}
diff --git a/logstash-core/src/main/resources/elastiqueue_meta_template.json b/logstash-core/src/main/resources/elastiqueue_meta_template.json
new file mode 100644
index 00000000000..80a7c8ef392
--- /dev/null
+++ b/logstash-core/src/main/resources/elastiqueue_meta_template.json
@@ -0,0 +1,47 @@
+{
+  "index_patterns": "elastiqueue-metadata-*",
+  "settings": {
+    "index": {
+      "number_of_shards": 1
+    }
+  },
+  "mappings": {
+    "doc": {
+      "properties": {
+        "@timestamp": {
+          "type": "date"
+        },
+        "type": {"type": "keyword"},
+        "topic": {
+          "properties": {
+            "name": {"type": "keyword"},
+            "partitions": {"type": "integer"},
+            "created_at": {"type": "date"}
+          }
+        },
+        "consumer_group": {
+          "properties": {
+            "name": {"type": "keyword"},
+            "created_at": {"type": "date"},
+            "topic": {"type": "keyword"}
+          }
+        },
+        "consumer_group_partition": {
+          "properties": {
+            "topic": {"type": "keyword"},
+            "consumer_group": {"type": "keyword"},
+            "partition": {"type": "integer"},
+            "consumer_uuid": {"type": "keyword"},
+            "consumer_name": {"type": "keyword"},
+            "offset": {"type": "long"},
+            "clock": {"type": "long"},
+            "consumer_last_update": {"type": "date"},
+            "takeover_clock": {"type": "long"},
+            "takeover_uuid": {"type": "keyword"},
+            "takeover_name": {"type": "keyword"}
+          }
+        }
+      }
+    }
+  }
+}
diff --git a/logstash-core/src/main/resources/elastiqueue_segments_template.json b/logstash-core/src/main/resources/elastiqueue_segments_template.json
new file mode 100644
index 00000000000..3ffa7dc59f3
--- /dev/null
+++ b/logstash-core/src/main/resources/elastiqueue_segments_template.json
@@ -0,0 +1,28 @@
+{
+  "index_patterns": "elastiqueue-partition-*",
+  "settings": {
+    "index": {
+      "sort.field": "seq",
+      "sort.order": "asc",
+      "number_of_shards": 1,
+      "refresh_interval": "5s"
+
+    }
+  },
+  "mappings": {
+    "doc": {
+      "properties": {
+        "@timestamp": {
+          "type": "date"
+        },
+        "seq": {
+          "type": "long"
+        },
+        "event_count": {"type": "integer"},
+        "events": {
+            "type": "binary"
+        }
+      }
+    }
+  }
+}
diff --git a/logstash-core/src/test/java/org/logstash/EventTest.java b/logstash-core/src/test/java/org/logstash/EventTest.java
index e8a0e43f8ad..210d0d63c22 100644
--- a/logstash-core/src/test/java/org/logstash/EventTest.java
+++ b/logstash-core/src/test/java/org/logstash/EventTest.java
@@ -10,6 +10,7 @@
 import java.util.List;
 import java.util.Map;
 
+import com.fasterxml.jackson.core.JsonProcessingException;
 import org.jruby.RubySymbol;
 import org.jruby.RubyTime;
 import org.jruby.java.proxies.ConcreteJavaProxy;
@@ -17,6 +18,7 @@
 import org.logstash.ext.JrubyTimestampExtLibrary;
 
 import static net.javacrumbs.jsonunit.JsonAssert.assertJsonEquals;
+import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.Assert.assertEquals;
@@ -25,6 +27,22 @@
 
 public final class EventTest {
 
+    @Test
+    public void serializeManyTest() throws IOException {
+        Event e1 = new Event();
+        e1.setField("Foo", "bar");
+        e1.setField("[@metadata][qux]", "qat");
+        Event e2 = new Event();
+        e2.setField("Baz", "Bot");
+        e2.setField("[@metadata][blot]", "bling");
+        Event[] events = {e1, e2};
+        byte[] serialized = Event.serializeMany(events);
+        Event[] deserialized = Event.deserializeMany(serialized);
+        assertThat(events.length, is(deserialized.length));
+        assertThat(e1, equalTo(deserialized[0]));
+        assertThat(e2, equalTo(deserialized[1]));
+    }
+
     @Test
     public void queueableInterfaceRoundTrip() throws Exception {
         Event e = new Event();
diff --git a/logstash-core/src/test/java/org/logstash/elastiqueue/ConsumerGroupTest.java b/logstash-core/src/test/java/org/logstash/elastiqueue/ConsumerGroupTest.java
new file mode 100644
index 00000000000..67f82ce9694
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/elastiqueue/ConsumerGroupTest.java
@@ -0,0 +1,17 @@
+package org.logstash.elastiqueue;
+
+import org.apache.http.HttpHost;
+import org.junit.Test;
+
+import java.io.IOException;
+
+import static org.junit.Assert.*;
+
+public class ConsumerGroupTest {
+    @Test
+    void testMacro() throws IOException {
+        Elastiqueue elastiqueue = new Elastiqueue(HttpHost.create("http://localhost:9200"));
+        Topic topic = elastiqueue.topic("test", 10);
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/elastiqueue/ElastiqueueTest.java b/logstash-core/src/test/java/org/logstash/elastiqueue/ElastiqueueTest.java
new file mode 100644
index 00000000000..b9b6f70e134
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/elastiqueue/ElastiqueueTest.java
@@ -0,0 +1,136 @@
+package org.logstash.elastiqueue;
+
+import org.apache.http.HttpHost;
+import org.junit.Test;
+import org.logstash.Event;
+import org.logstash.Timestamp;
+
+import java.io.IOException;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.atomic.AtomicInteger;
+import java.util.concurrent.atomic.AtomicLong;
+import java.util.concurrent.atomic.LongAdder;
+
+import static org.junit.Assert.*;
+
+public class ElastiqueueTest {
+    static String local = "http://localhost:9200";
+    static String lan = "http://192.168.1.80:9200";
+    public static final HttpHost localhost = HttpHost.create(local);
+    public static final AtomicInteger eventsCounter = new AtomicInteger();
+
+    public List<Event> makeEvents(int batchSize) {
+        List<Event> events = new ArrayList<>();
+        for (int i = 0; i<batchSize; i++) {
+            Event e = new Event();
+            e.setField("Foo", "bar");
+            e.setField("Another T", Timestamp.now());
+            e.setField("Sequence", eventsCounter.incrementAndGet());
+            events.add(e);
+        }
+        return events;
+    }
+
+    @Test
+    public void testSetup() throws Exception {
+        int parallelism = 10;
+
+        Elastiqueue eq = new Elastiqueue(localhost);
+        Topic topic = eq.topic("test", parallelism);
+        Producer producer = topic.makeProducer("testProducer");
+
+        System.out.println("Start");
+        long startedAt = System.nanoTime();
+        List<Thread> threads = new ArrayList<>();
+        LongAdder eventsWritten = new LongAdder();
+        LongAdder batchesWritten = new LongAdder();
+        LongAdder eventsRead = new LongAdder();
+        LongAdder batchesRead = new LongAdder();
+
+        int numProducers = parallelism;
+        int batchesPerProducer = 1000;
+        int batchSize = 1000;
+        int totalBatches = numProducers * batchesPerProducer;
+        AtomicLong batchesLeft = new AtomicLong(totalBatches);
+
+        final boolean doProduce = true;
+
+        for (int i =0; i<numProducers; i++) {
+            Thread t = new Thread(new Runnable() {
+                @Override
+                public void run() {
+                    long lastSeq = 0;
+                    int i = 0;
+                    while (doProduce && batchesLeft.getAndDecrement() > 0)  {
+                        i++;
+                        try {
+                            List<Event> events = makeEvents(batchSize);
+                            eventsWritten.add(events.size());
+                            batchesWritten.increment();
+                            lastSeq = producer.write(events.toArray(new Event[events.size()]));
+                        } catch (IOException e) {
+                            e.printStackTrace();
+                        } catch (RuntimeException e) {
+                            if (e.getCause() instanceof java.util.concurrent.TimeoutException) {
+                                System.out.println("Timeout encountered");
+                                continue;
+                            } else {
+                              e.printStackTrace();
+                            }
+                        } catch (InterruptedException e) {
+                            e.printStackTrace();
+                        }
+                    }
+                    System.out.println("Last Seq " + lastSeq + " TIMES " + i);
+                }
+            }, "Producer "+i);
+            threads.add(t);
+            t.start();
+        }
+
+        Consumer consumer = topic.makeConsumer("testGroup", "testConsId");
+        Thread reporter = new Thread(new Runnable() {
+            @Override
+            public void run() {
+                while (true) {
+                    try {
+                        Thread.sleep(1000);
+                        long endedAt = System.nanoTime();
+                                    float runTimeMillis = TimeUnit.MILLISECONDS.convert(endedAt-startedAt, TimeUnit.NANOSECONDS);
+                                    float eps = eventsRead.floatValue() / (runTimeMillis / 1000.0f);
+                                    System.out.println("\nWRITTEN " + eventsWritten.longValue() + "/" + batchesWritten + " | READ " + eventsRead.longValue() + " | EPS " + eps);
+                    } catch (InterruptedException e) {
+                        e.printStackTrace();
+                    }
+                }
+            }
+        }, "Reporter");
+        reporter.start();
+
+        consumer.consumePartitions(eventsWithSeq -> {
+            eventsRead.add(eventsWithSeq.getEvents().length);
+            batchesRead.increment();
+            //System.out.println("SET OFFSET " + results.getLastSeq());
+            eventsWithSeq.setOffset();
+        });
+
+        for (Thread t : threads) {
+            t.join();
+        }
+
+        consumer.close();
+
+        long endedAt = System.nanoTime();
+
+        float runTimeMillis = TimeUnit.MILLISECONDS.convert(endedAt-startedAt, TimeUnit.NANOSECONDS);
+        float eps = eventsRead.floatValue() / (runTimeMillis / 1000.0f);
+        System.out.println("\nWRITTEN " + eventsWritten.longValue() + " | READ " + eventsRead.longValue() + " | EPS " + eps);
+
+        //List<Event> res = consumer.poll();
+        //System.out.println(res);
+        assertEquals(eventsWritten.longValue(), eventsRead.longValue());
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ext/JrubyMemoryReadClientExtTest.java b/logstash-core/src/test/java/org/logstash/ext/JrubyMemoryReadClientExtTest.java
index b66279525c3..2a3027f4ea6 100644
--- a/logstash-core/src/test/java/org/logstash/ext/JrubyMemoryReadClientExtTest.java
+++ b/logstash-core/src/test/java/org/logstash/ext/JrubyMemoryReadClientExtTest.java
@@ -18,7 +18,6 @@
  * Tests for {@link JrubyMemoryReadClientExt}.
  */
 public final class JrubyMemoryReadClientExtTest {
-
     @Test
     public void testInflightBatchesTracking() throws InterruptedException, IOException {
         final BlockingQueue<JrubyEventExtLibrary.RubyEvent> queue =
