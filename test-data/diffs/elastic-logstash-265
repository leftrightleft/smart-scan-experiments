diff --git a/.tailor b/.tailor
new file mode 100644
index 00000000000..5e883dba31d
--- /dev/null
+++ b/.tailor
@@ -0,0 +1,8 @@
+Tailor.config do |config|
+  config.file_set '*.rb' do |style|
+    style.indentation_spaces 2, :level => :off
+    style.max_line_length 80, :level => :off
+    style.allow_trailing_line_spaces true, :level => :off
+    style.spaces_after_comma false, :level => :off
+  end
+end
diff --git a/CHANGELOG b/CHANGELOG
index 7b0a505c42e..dd2e3f82c06 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -16,6 +16,7 @@
    encoded log files. This should help resolve the many UTF-8 bugs that were
    reported recently.
  - bugfix: zeromq: 'topology' is now a required setting
+ - lumberjack: jls-lumberjack gem updated to 0.0.7
 
  ## filters
  - improvement: grok: now accepts (?<foo>...) named captures. This lets you
@@ -37,6 +38,7 @@
    field as a mongodb date instead of a string. (#224, patch by Kevin Amorin)
  - improvement: gelf: Allow full_message gelf property to be overridden (#245, 
    patch by SÃ©bastien Masset)
+ - lumberjack: jls-lumberjack gem updated to 0.0.6
 
 1.1.5 (November 10, 2012)
  ## Overview of this release:
diff --git a/README.md b/README.md
index fef0af2c575..1fe94e77cbf 100755
--- a/README.md
+++ b/README.md
@@ -61,7 +61,6 @@ That said, some basic guidelines, which you are free to ignore :)
 * If you want to send patches, best way is to fork this repo and send me a pull
   request. If you don't know git, I also accept diff(1) formatted patches -
   whatever is most comfortable for you. 
-    * _DO NOT MODIFY `Gemfile.lock` IN YOUR PULL REQUESTS_. THIS WILL CAUSE MERGE FAILURES.
 * Want to lurk about and see what others are doing? IRC (#logstash on
   irc.freenode.org) is a good place for this as is the 
   [mailing list](http://groups.google.com/group/logstash-users)
diff --git a/docs/flags.md b/docs/flags.md
index 23828369840..ee14036c162 100644
--- a/docs/flags.md
+++ b/docs/flags.md
@@ -9,7 +9,7 @@ layout: content_right
 The logstash agent has the following flags (also try using the '--help' flag)
 
 <dl>
-<dt> --config CONFIGFILE </dt>
+<dt> -f, --config CONFIGFILE </dt>
 <dd> Load the logstash config from a specific file, directory, or a wildcard. If given a directory or wildcard, config files will be read in order lexigraphically.  </dd>
 <dt> --log FILE </dt>
 <dd> Log to a given path. Default is to log to stdout </dd>
diff --git a/lib/logstash/config/grammar.rl b/lib/logstash/config/grammar.rl
index 14c036f00f8..5b05a0e2ccf 100644
--- a/lib/logstash/config/grammar.rl
+++ b/lib/logstash/config/grammar.rl
@@ -106,6 +106,9 @@ require "logstash/namespace"
   regexp_literal = ( "/" ( ( (any - [\\'\n]) | "\\" any )* ) "/" )  ;
 
   array = ( "[" ws ( string | numeric ) ws ("," ws (string | numeric ) ws)* "]" ) >array_init %array_push;
+  # TODO(sissel): Implement hash syntax { key => value, ... }
+  # TODO(sissel): hashes should support arrays as values.
+
   parameter_value = ( numeric | string | array );
   parameter = ( string ws "=>" ws parameter_value ) %parameter ;
   parameters = ( parameter ( ws parameter )** ) >parameter_init ;
diff --git a/lib/logstash/config/mixin.rb b/lib/logstash/config/mixin.rb
index 2233506e177..cf8359c66ea 100644
--- a/lib/logstash/config/mixin.rb
+++ b/lib/logstash/config/mixin.rb
@@ -157,7 +157,7 @@ def inherited(subclass)
     end # def inherited
 
     def validate(params)
-      @plugin_name = [ancestors[1].config_name, config_name].join("/")
+      @plugin_name = [superclass.config_name, config_name].join("/")
       @logger = LogStash::Logger.new(STDOUT)
       is_valid = true
 
diff --git a/lib/logstash/filters/date.rb b/lib/logstash/filters/date.rb
index 0f59667c755..b6333230121 100644
--- a/lib/logstash/filters/date.rb
+++ b/lib/logstash/filters/date.rb
@@ -56,6 +56,10 @@ class LogStash::Filters::Date < LogStash::Filters::Base
   # [dateformats]: http://download.oracle.com/javase/1.4.2/docs/api/java/text/SimpleDateFormat.html
   config /[A-Za-z0-9_-]+/, :validate => :array
 
+  # An array with field name first, and format patterns following, [ field, formats... ]
+  # Using this more than once will have unpredictable results, so only use it once per date filter.
+  config :match, :validate => :array, :default => []
+
   # LOGSTASH-34
   DATEPATTERNS = %w{ y d H m s S } 
 
@@ -103,14 +107,21 @@ def parseLocale(localeString)
   def register
     require "java"
     # TODO(sissel): Need a way of capturing regexp configs better.
-    locale = parseLocale(@config["locale"][0]) if @config["locale"] != nil and @config["locale"][0] != nil 
+    locale = parseLocale(@config["locale"][0]) if @config["locale"] != nil and @config["locale"][0] != nil
+    missing = []
     @config.each do |field, value|
       next if (RESERVED + ["locale"]).include?(field)
+      next if (RESERVED + ["match"]).include?(field)
 
       # values here are an array of format strings for the given field.
-      missing = []
-      value.each do |format|
-        case format
+      setupMatcher(field, locale, missing, value) # value.each
+    end # @config.each
+    setupMatcher(@config["match"].shift, locale, missing, @config["match"] )
+  end
+
+  def setupMatcher(field, locale, missing, value)
+    value.each do |format|
+      case format
         when "ISO8601"
           joda_parser = org.joda.time.format.ISODateTimeFormat.dateTimeParser.withOffsetParsed
           parser = lambda { |date| joda_parser.parseDateTime(date) }
@@ -122,7 +133,7 @@ def register
           parser = lambda { |date| org.joda.time.Instant.new((date[1..15].hex * 1000 - 10000)+(date[16..23].hex/1000000)).toDateTime }
         else
           joda_parser = org.joda.time.format.DateTimeFormat.forPattern(format).withOffsetParsed
-          if(locale != nil) 
+          if (locale != nil)
             joda_parser = joda_parser.withLocale(locale)
           end
           parser = lambda { |date| joda_parser.parseDateTime(date) }
@@ -134,17 +145,18 @@ def register
           # are not specified so we can inject them later. (jordansissel)
           # LOGSTASH-34
           missing = DATEPATTERNS.reject { |p| format.include?(p) }
-        end
+      end
 
-        @logger.debug("Adding type with date config", :type => @type,
-                      :field => field, :format => format)
-        @parsers[field] << {
+      @logger.debug("Adding type with date config", :type => @type,
+                    :field => field, :format => format)
+      @parsers[field] << {
           :parser => parser,
           :missing => missing
-        }
-      end # value.each
-    end # @config.each
-  end # def register
+      }
+    end
+  end
+
+  # def register
 
   public
   def filter(event)
diff --git a/lib/logstash/filters/grok.rb b/lib/logstash/filters/grok.rb
index d8482da150e..04995b74aa0 100644
--- a/lib/logstash/filters/grok.rb
+++ b/lib/logstash/filters/grok.rb
@@ -168,7 +168,7 @@ def filter(event)
         next
       end
 
-      @logger.debug("Trying pattern", :pile => pile, :field => field )
+      @logger.debug("Trying pattern", :pile => pile, :field => field)
       (event[field].is_a?(Array) ? event[field] : [event[field]]).each do |fieldvalue|
         begin
           # Coerce all field values to string. This turns arrays, hashes, numbers, etc
diff --git a/lib/logstash/filters/metrics.rb b/lib/logstash/filters/metrics.rb
new file mode 100644
index 00000000000..7623eb1a362
--- /dev/null
+++ b/lib/logstash/filters/metrics.rb
@@ -0,0 +1,56 @@
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# TODO(sissel): Fill in
+class LogStash::Filters::Metrics < LogStash::Filters::Base
+  config_name "metrics"
+  plugin_status "experimental"
+
+  # syntax: meter => [ "name of metric", "name of metric" ]
+  config :meter, :validate => :array, :default => []
+
+  # syntax: timer => [ "name of metric", "%{time_value}" ]
+  config :timer, :validate => :hash, :default => {}
+
+  def register
+    require "metriks"
+
+    @metric_meters = Hash.new { |h,k| h[k] = Metriks.meter(k) }
+    @metric_timers = Hash.new { |h,k| h[k] = Metriks.timer(k) }
+  end # def register
+
+  def filter(event)
+    return unless filter?(event)
+
+    @meter.each do |m|
+      @metric_meters[event.sprintf(m)].mark
+    end
+
+    @timer.each do |name, value|
+      @metric_timers[event.sprintf(name)].update(event.sprintf(value).to_f)
+    end
+  end # def filter
+
+  def flush
+    event = LogStash::Event.new
+    @metric_meters.each do |name, metric|
+      event["#{name}.count"] = metric.count
+      event["#{name}.rate_1m"] = metric.one_minute_rate
+      event["#{name}.rate_5m"] = metric.five_minute_rate
+      event["#{name}.rate_15m"] = metric.fifteen_minute_rate
+    end
+
+    @metric_timers.each do |name, metric|
+      event["#{name}.count"] = metric.count
+      event["#{name}.rate_1m"] = metric.one_mintute_rate
+      event["#{name}.rate_5m"] = metric.five_minute_rate
+      event["#{name}.rate_15m"] = metric.fifteen_minute_rate
+      event["#{name}.min"] = metric.min
+      event["#{name}.max"] = metric.max
+      event["#{name}.stddev"] = metric.stddev
+    end
+
+    filter_matched(event)
+    return [event]
+  end
+end # class LogStash::Filter::KV
diff --git a/lib/logstash/filters/mutate.rb b/lib/logstash/filters/mutate.rb
index 8f8ac4c97e3..33ff575bef9 100644
--- a/lib/logstash/filters/mutate.rb
+++ b/lib/logstash/filters/mutate.rb
@@ -117,7 +117,7 @@ def register
     @gsub_parsed = []
     @gsub.nil? or @gsub.each_slice(3) do |field, needle, replacement|
       if [field, needle, replacement].any? {|n| n.nil?}
-        @logger.error("Invalid gsub configuration. gsub has to define 3 elements per config entry", :file => file, :needle => needle, :replacement => replacement)
+        @logger.error("Invalid gsub configuration. gsub has to define 3 elements per config entry", :field => field, :needle => needle, :replacement => replacement)
         raise "Bad configuration, aborting."
       end
       @gsub_parsed << {
diff --git a/lib/logstash/filterworker.rb b/lib/logstash/filterworker.rb
index 2db7b1653f0..ed9b00ca7f8 100644
--- a/lib/logstash/filterworker.rb
+++ b/lib/logstash/filterworker.rb
@@ -2,9 +2,11 @@
 require "logstash/logging"
 require "logstash/plugin"
 require "logstash/config/mixin"
+require "stud/interval"
 
 # TODO(sissel): Should this really be a 'plugin' ?
 class LogStash::FilterWorker < LogStash::Plugin
+  include Stud
   attr_accessor :logger
   attr_accessor :filters
 
@@ -19,11 +21,12 @@ def initialize(filters, input_queue, output_queue)
   end # def initialize
 
   def run
-    # for each thread.
-    #@filters.each do |filter|
-      #filter.logger = @logger
-      #filter.register
-    #end
+    # TODO(sissel): Run a flusher thread for each plugin requesting flushes
+    # > It seems reasonable that you could want a multiline filter to flush
+    #   after 5 seconds, but want a metrics filter to flush every 10 or 60.
+
+    # Set up the periodic flusher thread.
+    @flusher = Thread.new { interval(5) { flusher } }
 
     while !@shutdown_requested && event = @input_queue.pop
       if event == LogStash::SHUTDOWN
@@ -35,7 +38,30 @@ def run
       filter(event)
     end # while @input_queue.pop
     finished
-  end
+  end # def run
+
+  def flusher
+    events = []
+    @filters.each do |filter|
+
+      # Filter any events generated so far in this flush.
+      events.each do |event|
+        # TODO(sissel): watchdog on flush filtration?
+        filter.filter(event) unless event.cancelled?
+      end
+
+      # TODO(sissel): watchdog on flushes?
+      if filter.respond_to?(:flush)
+        flushed = filter.flush 
+        events += flushed if !flushed.nil? && flushed.any?
+      end
+    end
+
+    events.each do |event|
+      @logger.debug("Pushing flushed events", :event => event)
+      @output_queue.push(event) unless event.cancelled?
+    end
+  end # def flusher
 
   def teardown
     @shutdown_requested = true
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 2e4d7cc0e71..9ab98b50005 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -91,7 +91,9 @@ def to_event(raw, source)
       event.message = raw
     when "json"
       begin
-        fields = JSON.parse(raw)
+        # JSON must be valid UTF-8, and many inputs come from ruby IO
+        # instances, which almost all default to ASCII-8BIT. Force UTF-8
+        fields = JSON.parse(raw.force_encoding("UTF-8"))
         fields.each { |k, v| event[k] = v }
         if @message_format
           event.message = event.sprintf(@message_format)
@@ -108,7 +110,9 @@ def to_event(raw, source)
       end
     when "json_event"
       begin
-        event = LogStash::Event.from_json(raw)
+        # JSON must be valid UTF-8, and many inputs come from ruby IO
+        # instances, which almost all default to ASCII-8BIT. Force UTF-8
+        event = LogStash::Event.from_json(raw.force_encoding("UTF-8"))
         event.tags += @tags
         if @message_format
           event.message ||= event.sprintf(@message_format)
diff --git a/lib/logstash/outputs/cloudwatch.rb b/lib/logstash/outputs/cloudwatch.rb
new file mode 100644
index 00000000000..8d42d962467
--- /dev/null
+++ b/lib/logstash/outputs/cloudwatch.rb
@@ -0,0 +1,255 @@
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+require "thread"
+require "rufus/scheduler"
+require "aws"
+
+# This output lets you aggregate and send metric data to AWS CloudWatch
+#
+# Configuration is done partly in this output and partly using fields added
+# to your events by other input & filter plugins.
+#
+# Events which do not have a "CW_metric" field will be ignored, so to send
+# events to CloudWatch you must at least add the "CW_metric" field to the
+# desired events (using grep for example)
+#
+# Other fields which can be added to events to modify the behavior of this
+# plugin are, "CW_namespace", "CW_unit", "CW_value", and the pair of
+# "CW_dimensionName" & "CW_dimensionValue".  All of these field names are
+# configurable in this output.  See below for details.
+#
+# You can read more about AWS CloudWatch here: http://aws.amazon.com/cloudwatch/
+class LogStash::Outputs::CloudWatch < LogStash::Outputs::Base
+  config_name "cloudwatch"
+  plugin_status "experimental"
+
+  # The AWS Region to send logs to.
+  config :region, :validate => :string, :default => "us-east-1"
+
+  # The AWS Access Key ID
+  config :access_key, :validate => :string, :required => true
+
+  # The AWS Secret Access Key
+  config :secret_key, :validate => :string, :required => true
+
+  # How often to send data to CloudWatch
+  # This does not affect the event timestamps, events will always have their
+  # actual timestamp (to-the-minute) sent to CloudWatch.
+  #
+  # Increasing this may reduce the number of CloudWatch API calls, which would
+  # reduce costs in heavy usage.
+  #
+  # See here for allowed values: https://github.com/jmettraux/rufus-scheduler#the-time-strings-understood-by-rufus-scheduler
+  config :timeframe, :validate => :string, :default => "1m"
+
+  # The default namespace to use for events which do not have a "CW_namespace" field
+  config :namespace, :validate => :string, :default => "Logstash"
+
+  # The name of the field used to set the metric name on an event
+  config :field_metric, :validate => :string, :default => "CW_metric"
+
+  # The name of the field used to set a different namespace per event
+  config :field_namespace, :validate => :string, :default => "CW_namespace"
+
+  # The name of the field used to set the units on an event metric
+  config :field_unit, :validate => :string, :default => "CW_unit"
+
+  # The name of the field used to set the value (float) on an event metric
+  config :field_value, :validate => :string, :default => "CW_value"
+
+  # The name of the field used to set the dimension name on an event metric
+  config :field_dimensionname, :validate => :string, :default => "CW_dimensionName"
+
+  # The name of the field used to set the dimension value on an event metric
+  config :field_dimensionvalue, :validate => :string, :default => "CW_dimensionValue"
+
+  # aggregate_key members
+  DIM_NAME = "dimensionName"
+  DIM_VALUE = "dimensionValue"
+  TIMESTAMP = "timestamp"
+  METRIC = "metric"
+  COUNT = "count"
+  UNIT = "unit"
+  SUM = "sum"
+  MIN = "min"
+  MAX = "max"
+
+  # Units
+  COUNT_UNIT = "Count"
+  NONE = "None"
+
+  public
+  def register
+    AWS.config(
+        :access_key_id => @access_key,
+        :secret_access_key => @secret_key,
+        :cloud_watch_endpoint => "monitoring.#{@region}.amazonaws.com"
+    )
+    @cw = AWS::CloudWatch.new
+
+    @valid_units = ["Seconds", "Microseconds", "Milliseconds", "Bytes", "Kilobytes", "Megabytes", "Gigabytes", "Terabytes", "Bits", "Kilobits", "Megabits", "Gigabits", "Terabits", "Percent", COUNT_UNIT, "Bytes/Second", "Kilobytes/Second", "Megabytes/Second", "Gigabytes/Second", "Terabytes/Second", "Bits/Second", "Kilobits/Second", "Megabits/Second", "Gigabits/Second", "Terabits/Second", "Count/Second", NONE]
+
+    @event_queue = Queue.new
+    @scheduler = Rufus::Scheduler.start_new
+    @job = @scheduler.every @timeframe do
+      @logger.info("Scheduler Activated")
+      send(aggregate({}))
+    end
+  end
+
+  public
+  def receive(event)
+    return unless output?(event)
+
+    if event == LogStash::SHUTDOWN
+      job.trigger()
+      job.unschedule()
+      @logger.info("CloudWatch aggregator thread shutdown.")
+      finished
+      return
+    end
+
+    return unless event.fields.member?(@field_metric)
+
+    @logger.info("Queueing event", :event => event)
+    @event_queue << event
+  end # def receive
+
+  private
+  def send(aggregates)
+    aggregates.each { |namespace, data|
+      @logger.info("Namespace, data: ", :namespace => namespace, :data => data)
+      metric_data = []
+      data.each { |aggregate_key, stats|
+        new_data = {
+            :metric_name => aggregate_key[METRIC],
+            :timestamp => aggregate_key[TIMESTAMP],
+            :unit => aggregate_key[UNIT],
+            :statistic_values => {
+                :sample_count => stats[COUNT],
+                :sum => stats[SUM],
+                :minimum => stats[MIN],
+                :maximum => stats[MAX],
+            }
+        }
+        if (aggregate_key[DIM_NAME] != nil && aggregate_key[DIM_VALUE] != nil)
+          new_data[:dimensions] = [{
+                                       :name => aggregate_key[DIM_NAME],
+                                       :value => aggregate_key[DIM_VALUE]
+                                   }]
+        end
+        metric_data << new_data
+      } # data.each
+
+      begin
+        response = @cw.put_metric_data(
+            :namespace => namespace,
+            :metric_data => metric_data
+        )
+        @logger.info("Sent data to AWS CloudWatch OK")
+      rescue Exception => e
+        @logger.warn("Failed to send to AWS CloudWatch", :exception => e, :namespace => namespace, :metric_data => metric_data)
+        break
+      end
+    } # aggregates.each
+    return aggregates
+  end
+
+  # def send
+
+  private
+  def aggregate(aggregates)
+
+    @logger.info("QUEUE SIZE ", :queuesize => @event_queue.size)
+    until @event_queue.empty? do
+      begin
+        count(aggregates, @event_queue.pop(true))
+      rescue Exception => e
+        @logger.warn("Exception!  Breaking count loop", :exception => e)
+        break
+      end
+    end
+    return aggregates
+  end
+
+  private
+  def count(aggregates, event)
+
+    # If the event doesnt declare a namespace, use the default
+    ns = field(event, @field_namespace)
+    namespace = (!ns) ? @namespace : ns
+
+    unit = field(event, @field_unit)
+    value = field(event, @field_value)
+
+    # If neither Units nor Value is set, then we simply count the event
+    if (!unit && !value)
+      unit = COUNT
+      value = "1"
+    end
+
+    # If Units is still not set (or is invalid), then we know Value must BE set, so set Units to "None"
+    # And warn about misconfiguration
+    if (!unit || !@valid_units.include?(unit))
+      unit = NONE
+      @logger.warn("Possible config error: CloudWatch Value found with invalid or missing Units")
+    end
+
+
+    if (!aggregates[namespace])
+      aggregates[namespace] = {}
+      @logger.info("INITIALIZING NAMESPACE DATA")
+    end
+
+    aggregate_key = {
+        METRIC => field(event, @field_metric),
+        DIM_NAME => field(event, @field_dimensionname),
+        DIM_VALUE => field(event, @field_dimensionvalue),
+        UNIT => unit,
+        TIMESTAMP => normalizeTimestamp(event.timestamp)
+    }
+
+    if (!aggregates[namespace][aggregate_key])
+      aggregates[namespace][aggregate_key] = {}
+    end
+
+    # We may get to this point with valid Units but missing value.  Send zeros.
+    val = (!value) ? 0.0 : value.to_f
+
+    if (!aggregates[namespace][aggregate_key][MAX] || val > aggregates[namespace][aggregate_key][MAX])
+      aggregates[namespace][aggregate_key][MAX] = val
+    end
+
+    if (!aggregates[namespace][aggregate_key][MIN] || val < aggregates[namespace][aggregate_key][MIN])
+      aggregates[namespace][aggregate_key][MIN] = val
+    end
+
+    if (!aggregates[namespace][aggregate_key][COUNT])
+      aggregates[namespace][aggregate_key][COUNT] = 1
+    else
+      aggregates[namespace][aggregate_key][COUNT] += 1
+    end
+
+    if (!aggregates[namespace][aggregate_key][SUM])
+      aggregates[namespace][aggregate_key][SUM] = val
+    else
+      aggregates[namespace][aggregate_key][SUM] += val
+    end
+  end
+
+  # Zeros out the seconds in a ISO8601 timestamp like event.timestamp
+  public
+  def normalizeTimestamp(time)
+    tz = (time[-1, 1] == "Z") ? "Z" : time[-5, 5]
+    totheminute = time[0..16]
+    normal = totheminute + "00.000" + tz
+    return normal
+  end
+
+  private
+  def field(event, fieldname)
+    return event.fields.member?(fieldname) ? event.fields[fieldname][0] : nil
+  end
+
+end # class LogStash::Outputs::CloudWatch
diff --git a/lib/logstash/outputs/http.rb b/lib/logstash/outputs/http.rb
index 6ea3d13bf26..94efd02dcfb 100644
--- a/lib/logstash/outputs/http.rb
+++ b/lib/logstash/outputs/http.rb
@@ -27,31 +27,40 @@ class LogStash::Outputs::Http < LogStash::Outputs::Base
   config :headers, :validate => :hash
 
   # Content type
-  config :content_type, :validate => :string, :default => "application/json"
-
-  # Mapping
-  # Normally Logstash will send the `json_event`
-  # as is
-  # If you provide a Logstash hash here,
-  # it will be mapped into a JSON structure
-  # e.g.
-  # `mapping => ["foo", "%{@source_host}", "bar", "%{@type}"]
-  # with generate a json like so:
-  # `{"foo":"localhost.domain.com","bar":"stdin-type"}`
+  #
+  # If not specified, this defaults to the following:
+  #
+  # * if format is "json", "application/json"
+  # * if format is "form", "application/x-www-form-urlencoded"
+  config :content_type, :validate => :string
+
+  # This lets you choose the structure and parts of the event that are sent.
+  #
+  #
+  # For example:
+  #
+  #    mapping => ["foo", "%{@source_host}", "bar", "%{@type}"]
   config :mapping, :validate => :hash
 
+  # Set the format of the http body.
+  #
+  # If form, then the body will be the mapping (or whole event) converted
+  # into a query parameter string (foo=bar&baz=fizz...)
+  #
+  # Otherwise, the event is sent as json.
+  config :format, :validate => ["json", "form"], :default => "json"
+
   public
   def register
-    require "net/https"
+    require "ftw"
     require "uri"
-    @uri = URI.parse(@url)
-    @client = Net::HTTP.new(@uri.host, @uri.port)
-    if @uri.scheme == "https"
-      @client.use_ssl = true
-      if @verify_ssl == true
-        @client.verify_mode = OpenSSL::SLL::VERIFY_PEER
-      else
-        @client.verify_mode = OpenSSL::SSL::VERIFY_NONE
+    @agent = FTW::Agent.new
+    # TODO(sissel): SSL verify mode?
+
+    if @content_type.nil?
+      case @format
+        when "form" ; @content_type = "application/x-www-form-urlencoded"
+        when "json" ; @content_type = "application/json"
       end
     end
   end # def register
@@ -61,35 +70,51 @@ def receive(event)
     return unless output?(event)
 
     if @mapping
-      @evt = Hash.new
+      evt = Hash.new
       @mapping.each do |k,v|
-        @evt[k] = event.sprintf(v)
+        evt[k] = event.sprintf(v)
       end
     else
-      @evt = event
+      evt = event.to_hash
     end
 
     case @http_method
     when "put"
-      @request = Net::HTTP::Put.new(@uri.path)
+      request = @agent.put(event.sprintf(@url))
     when "post"
-      @request = Net::HTTP::Post.new(@uri.path)
+      request = @agent.post(event.sprintf(@url))
     else
       @logger.error("Unknown verb:", :verb => @http_method)
     end
     
     if @headers
       @headers.each do |k,v|
-        @request.add_field(k, event.sprintf(v))
+        request.headers[k] = event.sprintf(v)
       end
     end
-    @request.add_field("Content-Type", @content_type)
+
+    request["Content-Type"] = @content_type
 
     begin
-      @request.body = @evt.to_json
-      response = @client.request(@request)
+      if @format == "json"
+        request.body = evt.to_json
+      else
+        request.body = encode(evt)
+      end
+      puts request
+      puts 
+      puts request.body
+      response = @agent.execute(request)
+      puts response
+      response.read_body { |c| puts c }
     rescue Exception => e
-      @logger.warn("Unhandled exception", :request => @request, :response => @response)
+      @logger.warn("Unhandled exception", :request => request, :response => response, :exception => e, :stacktrace => e.backtrace)
     end
   end # def receive
+
+  def encode(hash)
+    return hash.collect do |key, value|
+      CGI.escape(key) + "=" + CGI.escape(value)
+    end.join("&")
+  end # def encode
 end
diff --git a/lib/logstash/outputs/lumberjack.rb b/lib/logstash/outputs/lumberjack.rb
index bacf5d1b8d3..f43edbfb687 100644
--- a/lib/logstash/outputs/lumberjack.rb
+++ b/lib/logstash/outputs/lumberjack.rb
@@ -25,7 +25,14 @@ def register
   def receive(event)
     return unless output?(event)
     begin
-      @client.write("line" => event.message, "host" => event.source_host, "file" => event.source_path)
+      @client.write(
+        {
+          "line" => event.message, 
+          "host" => event.source_host, 
+          "file" => event.source_path,
+          "type" => event.type
+        }.merge(event.fields)
+      )
     rescue Exception => e
       @logger.log("Client write error", :e => e, :backtrace => e.backtrace)
       connect
@@ -42,9 +49,9 @@ def connect
         :ssl_certificate => @ssl_certificate, :window_size => @window_size)
     rescue Exception => e
       @logger.error("All hosts unavailable, sleeping", :hosts => @hosts, :e => e, 
-        :backtrace => e.backtrace, :host => @client.host)
+        :backtrace => e.backtrace)
       sleep(10)
       retry
     end
   end
-end
\ No newline at end of file
+end
diff --git a/lib/logstash/outputs/stdout.rb b/lib/logstash/outputs/stdout.rb
index a2174e72ab6..dc17b14f342 100644
--- a/lib/logstash/outputs/stdout.rb
+++ b/lib/logstash/outputs/stdout.rb
@@ -16,6 +16,9 @@ class LogStash::Outputs::Stdout < LogStash::Outputs::Base
   # Debug output format: ruby (default), json
   config :debug_format, :default => "ruby", :validate => ["ruby", "json", "dots"]
 
+  # The message to emit to stdout.
+  config :message, :validate => :string, :default => "%{@timestamp} %{@source}: %{@message}"
+
   public
   def register
     @print_method = method(:ap) rescue method(:p)
@@ -58,7 +61,7 @@ def register
           finished
           return
         end
-        puts event.to_s
+        puts event.sprintf(@message)
       end
     end
   end
diff --git a/logstash.gemspec b/logstash.gemspec
index 11a7de3f372..172b4697112 100644
--- a/logstash.gemspec
+++ b/logstash.gemspec
@@ -45,6 +45,7 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "jls-grok", ["0.10.10"]
   gem.add_runtime_dependency "mail"
   gem.add_runtime_dependency "mongo"
+  gem.add_runtime_dependency "metriks"
   gem.add_runtime_dependency "onstomp"
   gem.add_runtime_dependency "redis"
   gem.add_runtime_dependency "riak-client", ["1.0.3"]
@@ -53,9 +54,10 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "uuidtools" # For generating amqp queue names
   gem.add_runtime_dependency "xml-simple"
   gem.add_runtime_dependency "xmpp4r", ["0.5"]
-  gem.add_runtime_dependency "jls-lumberjack", ["0.0.4"]
+  gem.add_runtime_dependency "jls-lumberjack", ["0.0.7"]
   gem.add_runtime_dependency "geoip", [">= 1.1.0"]
   gem.add_runtime_dependency "beefcake", "0.3.7"
+  gem.add_runtime_dependency "rufus-scheduler"
 
   if RUBY_PLATFORM == 'java'
     gem.platform = RUBY_PLATFORM
diff --git a/patterns/grok-patterns b/patterns/grok-patterns
index 98c78aeb2fb..4f9d4229e60 100755
--- a/patterns/grok-patterns
+++ b/patterns/grok-patterns
@@ -94,4 +94,4 @@ SYSLOGBASE %{SYSLOGTIMESTAMP:timestamp} (?:%{SYSLOGFACILITY} )?%{SYSLOGHOST:logs
 COMBINEDAPACHELOG %{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] "(?:%{WORD:verb} %{NOTSPACE:request}(?: HTTP/%{NUMBER:httpversion})?|-)" %{NUMBER:response} (?:%{NUMBER:bytes}|-) %{QS:referrer} %{QS:agent}
 
 # Log Levels
-LOGLEVEL ([D|d]ebug|DEBUG|[N|n]otice|NOTICE|[I|i]nfo|INFO|[W|w]arn?(?:ing)?|WARN?(?:ING)?|[E|e]rr?(?:or)?|ERR?(?:OR)?|[C|c]rit?(?:ical)?|CRIT?(?:ICAL)?|[F|f]atal|FATAL|[S|s]evere|SEVERE)
+LOGLEVEL ([T|t]race|TRACE|[D|d]ebug|DEBUG|[N|n]otice|NOTICE|[I|i]nfo|INFO|[W|w]arn?(?:ing)?|WARN?(?:ING)?|[E|e]rr?(?:or)?|ERR?(?:OR)?|[C|c]rit?(?:ical)?|CRIT?(?:ICAL)?|[F|f]atal|FATAL|[S|s]evere|SEVERE)
diff --git a/spec/filters/date.rb b/spec/filters/date.rb
index 76ce8d39187..c2381a1d3b3 100644
--- a/spec/filters/date.rb
+++ b/spec/filters/date.rb
@@ -160,4 +160,22 @@
       # nothing to do, if this crashes it's an error..
     end
   end
+
+  describe "accept match config option with hash value like grep (LOGSTASH-735)" do
+    config <<-CONFIG
+      filter {
+        date {
+          match => [ "mydate", "ISO8601" ]
+        }
+      }
+    CONFIG
+
+    time = "2001-09-09T01:46:40.000Z"
+
+    sample({"@fields" => {"mydate" => time}}) do
+      insist { subject["mydate"] } == time
+      insist { subject.timestamp } == time
+      insist { subject["@timestamp"] } == time
+    end
+  end
 end
