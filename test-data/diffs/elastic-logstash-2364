diff --git a/docs/asciidoc/static/configuration.asciidoc b/docs/asciidoc/static/configuration.asciidoc
index a9a99d1b793..0c628bf375f 100644
--- a/docs/asciidoc/static/configuration.asciidoc
+++ b/docs/asciidoc/static/configuration.asciidoc
@@ -1,8 +1,30 @@
-== Logstash Configuration Language
+[[configuration]]
+== Configuring Logstash
 
-Logstash config files enable you to specify which plugins you want to use and settings for each plugin.
+To configure Logstash, you create a config file that specifies which plugins you want to use and settings for each plugin.
 You can reference event fields in a configuration and use conditionals to process events when they meet certain
-criteria.
+criteria. When you run logstash, you use the `-f` to specify your config file.
+
+Let's step through creating a simple config file and using it to run Logstash. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
+
+[source,ruby]
+----------------------------------
+input { stdin { } }
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+Then, run logstash and specify the configuration file with the `-f` flag.
+
+[source,ruby]
+----------------------------------
+bin/logstash -f logstash-simple.conf
+----------------------------------
+
+Et voilà! Logstash reads  the specified configuration file and outputs to both Elasticsearch and stdout. Before we
+move on to some <<config-examples,more complex examples>>, let's take a closer look at what's in a config file.
 
 [float]
 === Structure of a Config File
@@ -288,6 +310,7 @@ output {
 ----------------------------------
 
 [float]
+[[conditionals]]
 === Conditionals
 
 Sometimes you only want to filter or output an event under
@@ -341,32 +364,6 @@ filter {
 }
 ----------------------------------
 
-How about a more complex example?
-
-* alert nagios of any apache events with status 5xx
-* record any 4xx status to elasticsearch
-* record all status code hits via statsd
-
-To tell nagios about any http event that has a 5xx status code, you
-first need to check the value of the `type` field. If it's apache, then you can 
-check to see if the `status` field contains a 5xx error. If it is, send it to nagios. If it isn't 
-a 5xx error, check to see if the `status` field contains a 4xx error. If so, send it to Elasticsearch. 
-Finally, send all apache status codes to statsd no matter what the `status` field contains:
-
-[source,js]
-----------------------------------
-output {
-  if [type] == "apache" {
-    if [status] =~ /^5\d\d/ {
-      nagios { ...  }
-    } else if [status] =~ /^4\d\d/ {
-      elasticsearch { ... }
-    }
-    statsd { increment => "apache.%{status}" }
-  }
-}
-----------------------------------
-
 You can specify multiple expressions in a single condition:
 
 [source,js]
@@ -381,7 +378,7 @@ output {
 }
 ----------------------------------
 
-Here are some examples for testing with the `in` conditional:
+The `in` conditional enables you to compare against the value of a field:
 
 [source,js]
 ----------------------------------
@@ -407,7 +404,9 @@ filter {
 }
 ----------------------------------
 
-Or, to test if grok was successful:
+You use the `not in` conditional the same way. For example,
+you could use `not in` to only route events to elasticsearch
+when `grok` is successful:
 
 [source,js]
 ----------------------------------
@@ -417,3 +416,290 @@ output {
   }
 }
 ----------------------------------
+
+For more complex examples, see <<using-conditionals, Using Conditionals>>.
+
+
+[[config-examples]]
+=== Logstash Configuration Examples
+The following examples illustrate how you can configure Logstash to filter events, process Apache logs and syslog messages, and use conditionals to control what events are processed by a filter or output.
+
+[float]
+[[filter-example]]
+==== Configuring Filters
+Filters are an in-line processing mechanism that provide the flexibility to slice and dice your data to fit your needs. Let's take a look at some filters in action. The following configuration file sets up the `grok` and `date` filters.
+
+[source,ruby]
+----------------------------------
+input { stdin { } }
+
+filter {
+  grok {
+    match => { "message" => "%{COMBINEDAPACHELOG}" }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+Run Logstash with this configuration:
+
+[source,ruby]
+----------------------------------
+bin/logstash -f logstash-filter.conf
+----------------------------------
+
+Now, paste the following line into your terminal so it will be processed by the stdin input:
+[source,ruby]
+----------------------------------
+127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
+----------------------------------
+
+You should see something returned to stdout that looks like this:
+
+[source,ruby]
+----------------------------------
+{
+        "message" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
+     "@timestamp" => "2013-12-11T08:01:45.000Z",
+       "@version" => "1",
+           "host" => "cadenza",
+       "clientip" => "127.0.0.1",
+          "ident" => "-",
+           "auth" => "-",
+      "timestamp" => "11/Dec/2013:00:01:45 -0800",
+           "verb" => "GET",
+        "request" => "/xampp/status.php",
+    "httpversion" => "1.1",
+       "response" => "200",
+          "bytes" => "3891",
+       "referrer" => "\"http://cadenza/xampp/navi.php\"",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
+}
+----------------------------------
+
+As you can see, Logstash (with help from the `grok` filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This is extremely useful once you start querying and analyzing our log data. For example, you'll be able to easily run reports on HTTP response codes, IP addresses, referrers, and so on. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you need to parse a common log format, someone has already done the work for you. For more information, see the list of https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns[Logstash grok patterns] on GitHub.
+
+The other filter used in this example is the `date` filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the `@timestamp` field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs. It gives you the ability to tell Logstash "use this value as the timestamp for this event".
+
+[float]
+==== Processing Apache Logs
+Let's do something that's actually *useful*: process apache2 access log files! We are going to read the input from a file on the localhost, and use a <<conditionals,conditional>> to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you can change the log's file path to suit your needs):
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/tmp/access_log"
+    start_position => "beginning"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { "type" => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+  }
+  date {
+    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+  }
+}
+
+output {
+  elasticsearch {
+    host => localhost
+  }
+  stdout { codec => rubydebug }
+}
+
+----------------------------------
+
+Then, create the input file you configured above (in this example, "/tmp/access_log") with the following log entries (or use some from your own webserver):
+
+[source,js]
+----------------------------------
+71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
+134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
+98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
+----------------------------------
+
+Now, run Logstash with the -f flag to pass in the configuration file:
+
+[source,js]
+----------------------------------
+bin/logstash -f logstash-apache.conf
+----------------------------------
+
+Now you should see your apache log data in Elasticsearch! Logstash opened and read the specified input file, processing each event it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events, and stored in Elasticsearch. As an added bonus, they are stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
+
+In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching `*log`), by changing one line in the above configuration:
+
+[source,js]
+----------------------------------
+input {
+  file {
+    path => "/tmp/*_log"
+...
+----------------------------------
+
+When you restart Logstash, it will process both the error and access logs. However, if you inspect your data (using elasticsearch-kopf, perhaps), you'll see that the access_log is broken up into discrete fields, but the error_log isn't. That's because we used a `grok` filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
+
+Note that Logstash did not reprocess the events that were already seen in the access_log file. When reading from a file, Logstash saves its position and only processes new lines as they are added. Neat!
+
+[float]
+[[using-conditionals]]
+==== Using Conditionals
+You use conditionals to control what events are processed by a filter or output. For example, you could label each event according to which file it appeared in (access_log, error_log, and other random files that end with "log").
+
+[source,ruby]
+----------------------------------
+input {
+  file {
+    path => "/tmp/*_log"
+  }
+}
+
+filter {
+  if [path] =~ "access" {
+    mutate { replace => { type => "apache_access" } }
+    grok {
+      match => { "message" => "%{COMBINEDAPACHELOG}" }
+    }
+    date {
+      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
+    }
+  } else if [path] =~ "error" {
+    mutate { replace => { type => "apache_error" } }
+  } else {
+    mutate { replace => { type => "random_logs" } }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+This example labels all events using the `type` field, but doesn't actually parse the `error` or `random` files. There are so many types of error logs that how they should be labeled really depends on what logs you're working with.
+
+Similarly, you can use conditionals to direct events to particular outputs. For example, you could:
+
+* alert nagios of any apache events with status 5xx
+* record any 4xx status to elasticsearch
+* record all status code hits via statsd
+
+To tell nagios about any http event that has a 5xx status code, you
+first need to check the value of the `type` field. If it's apache, then you can 
+check to see if the `status` field contains a 5xx error. If it is, send it to nagios. If it isn't 
+a 5xx error, check to see if the `status` field contains a 4xx error. If so, send it to Elasticsearch. 
+Finally, send all apache status codes to statsd no matter what the `status` field contains:
+
+[source,js]
+----------------------------------
+output {
+  if [type] == "apache" {
+    if [status] =~ /^5\d\d/ {
+      nagios { ...  }
+    } else if [status] =~ /^4\d\d/ {
+      elasticsearch { ... }
+    }
+    statsd { increment => "apache.%{status}" }
+  }
+}
+----------------------------------
+
+[float]
+==== Processing Syslog Messages
+Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line so you can get a feel for what happens.
+
+First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
+
+[source,ruby]
+----------------------------------
+input {
+  tcp {
+    port => 5000
+    type => syslog
+  }
+  udp {
+    port => 5000
+    type => syslog
+  }
+}
+
+filter {
+  if [type] == "syslog" {
+    grok {
+      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
+      add_field => [ "received_at", "%{@timestamp}" ]
+      add_field => [ "received_from", "%{host}" ]
+    }
+    syslog_pri { }
+    date {
+      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
+    }
+  }
+}
+
+output {
+  elasticsearch { host => localhost }
+  stdout { codec => rubydebug }
+}
+----------------------------------
+
+Run Logstash with this new configuration:
+
+[source,ruby]
+----------------------------------
+bin/logstash -f logstash-syslog.conf
+----------------------------------
+
+Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. For this example, we'll just telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). Open another shell window to interact with the Logstash syslog input and enter the following command:
+
+[source,ruby]
+----------------------------------
+telnet localhost 5000
+----------------------------------
+
+Copy and paste the following lines as samples. (Feel free to try some of your own, but keep in mind they might not parse if the `grok` filter is not correct for your data).
+
+[source,ruby]
+----------------------------------
+Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
+Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
+Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)
+Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
+----------------------------------
+
+Now you should see the output of Logstash in your original shell as it processes and parses messages!
+
+[source,ruby]
+----------------------------------
+{
+                 "message" => "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+              "@timestamp" => "2013-12-23T22:30:01.000Z",
+                "@version" => "1",
+                    "type" => "syslog",
+                    "host" => "0:0:0:0:0:0:0:1:52617",
+        "syslog_timestamp" => "Dec 23 14:30:01",
+         "syslog_hostname" => "louis",
+          "syslog_program" => "CRON",
+              "syslog_pid" => "619",
+          "syslog_message" => "(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
+             "received_at" => "2013-12-23 22:49:22 UTC",
+           "received_from" => "0:0:0:0:0:0:0:1:52617",
+    "syslog_severity_code" => 5,
+    "syslog_facility_code" => 1,
+         "syslog_facility" => "user-level",
+         "syslog_severity" => "notice"
+}
+----------------------------------
diff --git a/docs/asciidoc/static/getting-started-with-logstash.asciidoc b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
index 68828990e2e..a7fd2b62eb7 100644
--- a/docs/asciidoc/static/getting-started-with-logstash.asciidoc
+++ b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
@@ -164,335 +164,5 @@ You might have noticed that Logstash is smart enough to create a new index in El
 
 [float]
 === Moving On
-Before we talk about more advanced configurations, let's take a quick look at some of the core features of Logstash and how they interact with the Logstash engine.
-[float]
-==== The Life of an Event
-
-Inputs, Outputs, Codecs and Filters are at the heart of the Logstash configuration. By creating an event processing pipeline, Logstash can extract the relevant data from your logs and make it available to Elasticsearch so you can efficiently query your data. To get you thinking about the various options available in Logstash, let's discuss some of the more common configurations currently in use. For more information, see <<pipeline, the life of an event>>.
-
-[float]
-===== Inputs
-Inputs are the mechanism for passing log data to Logstash. Some of the more commonly-used inputs are:
-
-* *file*: reads from a file on the filesystem, much like the UNIX command `tail -0a`
-* *syslog*: listens on the well-known port 514 for syslog messages and parses according to the RFC3164 format
-* *redis*: reads from a redis server, using both redis channels and redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
-* *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
-
-For more information about the available inputs, see <<input-plugins,Input Plugins>>.
-
-[float]
-===== Filters
-Filters are used as intermediary processing devices in the Logstash pipeline. They are often combined with conditionals to perform an action on an event if it matches particular criteria. Some useful filters:
-
-* *grok*: parse and structure arbitrary text. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns built-in to Logstash, it's more than likely you'll find one that meets your needs!
-* *mutate*: perform general transformations on event fields. You can rename, remove, replace, and modify fields in your events.
-* *drop*: drop an event completely, for example, 'debug' events.
-* *clone*: make a copy of an event, possibly adding or removing fields.
-* *geoip*: add information about geographical location of IP addresses (also displays amazing charts in Kibana!)
-
-For more information about the available filters, see <<filter-plugins,Filter Plugins>>.
-
-[float]
-===== Outputs
-Outputs are the final phase of the Logstash pipeline. An event can pass through multiple outputs, but once all output processing is complete, the event has finished its execution. Some commonly used outputs include:
-
-* *elasticsearch*: send event data to Elasticsearch. If you're planning to save your data in an efficient, convenient, and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
-* *file*: write event data to a file on disk.
-* *graphite*: send event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
-* *statsd*: send event data to statsd, a service that "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
-
-For more information about the available outputs, see <<output-plugins,Output Plugins>>.
-
-[float]
-===== Codecs
-Codecs are basically stream filters that can operate as part of an input or output. Codecs allow you to easily separate the transport of your messages from the serialization process. Popular codecs include `json`, `msgpack`, and `plain` (text).
-
-* *json*: encode or decode data in the JSON format.
-* *multiline*: merge multiple-line text events such as java exception and stacktrace messages into a single event.  
-
-For more information about the available codecs, see <<codec-plugins,Codec Plugins>>.
-
-[float]
-=== More fun with Logstash
-[float]
-==== Persistent Configuration files
-
-While specifying configurations on the command line is convenient for experimenting with different configurations, for more advanced setups with more complex configurations you'll want to use a configuration file. 
-
-Let's create a simple configuration file and use it to run Logstash. Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
-
-[source,ruby]
-----------------------------------
-input { stdin { } }
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----------------------------------
-
-Then, run logstash and specify the configuration file with the -f flag.
-
-[source,ruby]
-----------------------------------
-bin/logstash -f logstash-simple.conf
-----------------------------------
-
-Et voilà! Logstash reads  the specified configuration file and outputs to both Elasticsearch and stdout. This is a very simple case, of course, so let's move on to some more complex examples.
-
-[float]
-==== Filters
-Filters are an in-line processing mechanism that provide the flexibility to slice and dice your data to fit your needs. Let's take a look at some filters in action. The following configuration file sets up the *grok* and *date* filters.
-
-[source,ruby]
-----------------------------------
-input { stdin { } }
-
-filter {
-  grok {
-    match => { "message" => "%{COMBINEDAPACHELOG}" }
-  }
-  date {
-    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
-  }
-}
-
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----------------------------------
-
-Run Logstash with this configuration:
-
-[source,ruby]
-----------------------------------
-bin/logstash -f logstash-filter.conf
-----------------------------------
-
-Now, paste the following line into your terminal so it will be processed by the stdin input:
-[source,ruby]
-----------------------------------
-127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] "GET /xampp/status.php HTTP/1.1" 200 3891 "http://cadenza/xampp/navi.php" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0"
-----------------------------------
-
-You should see something returned to stdout that looks like this:
-
-[source,ruby]
-----------------------------------
-{
-        "message" => "127.0.0.1 - - [11/Dec/2013:00:01:45 -0800] \"GET /xampp/status.php HTTP/1.1\" 200 3891 \"http://cadenza/xampp/navi.php\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\"",
-     "@timestamp" => "2013-12-11T08:01:45.000Z",
-       "@version" => "1",
-           "host" => "cadenza",
-       "clientip" => "127.0.0.1",
-          "ident" => "-",
-           "auth" => "-",
-      "timestamp" => "11/Dec/2013:00:01:45 -0800",
-           "verb" => "GET",
-        "request" => "/xampp/status.php",
-    "httpversion" => "1.1",
-       "response" => "200",
-          "bytes" => "3891",
-       "referrer" => "\"http://cadenza/xampp/navi.php\"",
-          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10.9; rv:25.0) Gecko/20100101 Firefox/25.0\""
-}
-----------------------------------
-
-As you can see, Logstash (with help from the *grok* filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This is extremely useful once you start querying and analyzing our log data. For example, you'll be able to easily run reports on HTTP response codes, IP addresses, referrers, and so on. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you need to parse a common log format, someone has already done the work for you. For more information, see the list of https://github.com/logstash-plugins/logstash-patterns-core/tree/master/patterns[Logstash grok patterns] on GitHub.
-
-The other filter used in this example is the *date* filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the @timestamp field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs. It gives you the ability to tell Logstash "use this value as the timestamp for this event".
-
-[float]
-=== Useful Examples
-
-[float]
-==== Apache logs (from files)
-Now, let's configure something actually *useful*... apache2 access log files! We are going to read the input from a file on the localhost, and use a *conditional* to process the event according to our needs. First, create a file called something like 'logstash-apache.conf' with the following contents (you can change the log's file path to suit your needs):
-
-[source,js]
-----------------------------------
-input {
-  file {
-    path => "/tmp/access_log"
-    start_position => "beginning"
-  }
-}
-
-filter {
-  if [path] =~ "access" {
-    mutate { replace => { "type" => "apache_access" } }
-    grok {
-      match => { "message" => "%{COMBINEDAPACHELOG}" }
-    }
-  }
-  date {
-    match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
-  }
-}
-
-output {
-  elasticsearch {
-    host => localhost
-  }
-  stdout { codec => rubydebug }
-}
-
-----------------------------------
-
-Then, create the input file you configured above (in this example, "/tmp/access_log") with the following log entries (or use some from your own webserver):
-
-[source,js]
-----------------------------------
-71.141.244.242 - kurt [18/May/2011:01:48:10 -0700] "GET /admin HTTP/1.1" 301 566 "-" "Mozilla/5.0 (Windows; U; Windows NT 5.1; en-US; rv:1.9.2.3) Gecko/20100401 Firefox/3.6.3"
-134.39.72.245 - - [18/May/2011:12:40:18 -0700] "GET /favicon.ico HTTP/1.1" 200 1189 "-" "Mozilla/4.0 (compatible; MSIE 8.0; Windows NT 5.1; Trident/4.0; .NET CLR 2.0.50727; .NET CLR 3.0.4506.2152; .NET CLR 3.5.30729; InfoPath.2; .NET4.0C; .NET4.0E)"
-98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
-----------------------------------
-
-Now, run Logstash with the -f flag to pass in the configuration file:
-
-[source,js]
-----------------------------------
-bin/logstash -f logstash-apache.conf
-----------------------------------
-
-Now you should see your apache log data in Elasticsearch! Logstash opened and read the specified input file, processing each event it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events, and stored in Elasticsearch. As an added bonus, they are stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
-
-In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching '*log'), by changing one line in the above configuration:
-
-[source,js]
-----------------------------------
-input {
-  file {
-    path => "/tmp/*_log"
-...
-----------------------------------
-
-When you restart Logstash, it will process both the error and access logs. However, if you inspect your data (using elasticsearch-kopf, perhaps), you'll see that the access_log is broken up into discrete fields, but the error_log isn't. That's because we used a "grok" filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
-
-Note that Logstash did not reprocess the events that were already seen in the access_log file. When reading from a file, Logstash saves its position and only processes new lines as they are added. Neat!
-
-[float]
-==== Conditionals
-In the previous example, we introduced the concept of a *conditional*. As in many other programming languages, you can use 'if', 'else if' and 'else' statements to control what events are processed. For example, you could label each event according to which file it appeared in (access_log, error_log, and other random files that end with "log").
-
-[source,ruby]
-----------------------------------
-input {
-  file {
-    path => "/tmp/*_log"
-  }
-}
-
-filter {
-  if [path] =~ "access" {
-    mutate { replace => { type => "apache_access" } }
-    grok {
-      match => { "message" => "%{COMBINEDAPACHELOG}" }
-    }
-    date {
-      match => [ "timestamp" , "dd/MMM/yyyy:HH:mm:ss Z" ]
-    }
-  } else if [path] =~ "error" {
-    mutate { replace => { type => "apache_error" } }
-  } else {
-    mutate { replace => { type => "random_logs" } }
-  }
-}
-
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----------------------------------
-
-This example labels all events using the "type" field, but doesn't actually parse the "error" or "random" files. There are so many types of error logs that how they should be labeled really depends on what logs you're working with.
-
-[float]
-==== Syslog
-Let's move on to another incredibly useful example: *syslog*. Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line so you can get a feel for what happens.
-
-First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
-
-[source,ruby]
-----------------------------------
-input {
-  tcp {
-    port => 5000
-    type => syslog
-  }
-  udp {
-    port => 5000
-    type => syslog
-  }
-}
-
-filter {
-  if [type] == "syslog" {
-    grok {
-      match => { "message" => "%{SYSLOGTIMESTAMP:syslog_timestamp} %{SYSLOGHOST:syslog_hostname} %{DATA:syslog_program}(?:\[%{POSINT:syslog_pid}\])?: %{GREEDYDATA:syslog_message}" }
-      add_field => [ "received_at", "%{@timestamp}" ]
-      add_field => [ "received_from", "%{host}" ]
-    }
-    syslog_pri { }
-    date {
-      match => [ "syslog_timestamp", "MMM  d HH:mm:ss", "MMM dd HH:mm:ss" ]
-    }
-  }
-}
-
-output {
-  elasticsearch { host => localhost }
-  stdout { codec => rubydebug }
-}
-----------------------------------
-
-Run Logstash with this new configuration:
-
-[source,ruby]
-----------------------------------
-bin/logstash -f logstash-syslog.conf
-----------------------------------
-
-Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. For this example, we'll just telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). Open another shell window to interact with the Logstash syslog input and enter the following command:
-
-[source,ruby]
-----------------------------------
-telnet localhost 5000
-----------------------------------
-
-Copy and paste the following lines as samples. (Feel free to try some of your own, but keep in mind they might not parse if the grok filter is not correct for your data).
-
-[source,ruby]
-----------------------------------
-Dec 23 12:11:43 louis postfix/smtpd[31499]: connect from unknown[95.75.93.154]
-Dec 23 14:42:56 louis named[16000]: client 199.48.164.7#64817: query (cache) 'amsterdamboothuren.com/MX/IN' denied
-Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)
-Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
-----------------------------------
-
-Now you should see the output of Logstash in your original shell as it processes and parses messages!
-
-[source,ruby]
-----------------------------------
-{
-                 "message" => "Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
-              "@timestamp" => "2013-12-23T22:30:01.000Z",
-                "@version" => "1",
-                    "type" => "syslog",
-                    "host" => "0:0:0:0:0:0:0:1:52617",
-        "syslog_timestamp" => "Dec 23 14:30:01",
-         "syslog_hostname" => "louis",
-          "syslog_program" => "CRON",
-              "syslog_pid" => "619",
-          "syslog_message" => "(www-data) CMD (php /usr/share/cacti/site/poller.php >/dev/null 2>/var/log/cacti/poller-error.log)",
-             "received_at" => "2013-12-23 22:49:22 UTC",
-           "received_from" => "0:0:0:0:0:0:0:1:52617",
-    "syslog_severity_code" => 5,
-    "syslog_facility_code" => 1,
-         "syslog_facility" => "user-level",
-         "syslog_severity" => "notice"
-}
-----------------------------------
-
-Congratulations! You're well on your way to being a real Logstash power user. You should be comfortable configuring, running and sending events to Logstash, but there's much more to explore.
+Configuring inputs and outputs from the command line is convenient for getting started and doing quick testing. To move beyond
+these simple examples, however, you need to know a bit more about the Logstash event processing pipeline and how to specify pipeline options in a config file. To learn about the event processing pipeline, see <<pipeline,Logstash Processing Pipeline>>. To see how to configure more complex pipelines using config files, see <<configuration, Configuring Logstash>>.
diff --git a/docs/asciidoc/static/life-of-an-event.asciidoc b/docs/asciidoc/static/life-of-an-event.asciidoc
index ca417b01da1..a5d683b3bb6 100644
--- a/docs/asciidoc/static/life-of-an-event.asciidoc
+++ b/docs/asciidoc/static/life-of-an-event.asciidoc
@@ -1,15 +1,60 @@
 [[pipeline]]
-== the life of an event
+== Logstash Processing Pipeline
 
-The Logstash agent is an event processing pipeline that has three stages: inputs -> filters -> outputs. Inputs generate events, filters modify them, and outputs ship them elsewhere.
+The Logstash event processing pipeline has three stages: inputs -> filters -> outputs. Inputs generate events, filters modify them, and outputs ship them elsewhere. Inputs and outputs support codecs that enable you to encode or decode the data as it enters or exits the pipeline without having to use a separate filter.
 
-Events are passed from stage to stage using internal queues implemented with a Ruby `SizedQueue`. A `SizedQueue` has a maximum number of items it can contain.  When the queue is at maximum capacity, all writes to the queue are blocked.
+[float]
+==== Inputs
+You use inputs to get data into Logstash. Some of the more commonly-used inputs are:
 
-Logstash sets the size of each queue to 20. This means a maximum of 20 events can be pending for the next stage, which helps prevent data loss and keeps Logstash from acting as a data storage system. These internal queues are not intended for storing messages long-term.
+* *file*: reads from a file on the filesystem, much like the UNIX command `tail -0a`
+* *syslog*: listens on the well-known port 514 for syslog messages and parses according to the RFC3164 format
+* *redis*: reads from a redis server, using both redis channels and redis lists. Redis is often used as a "broker" in a centralized Logstash installation, which queues Logstash events from remote Logstash "shippers".
+* *lumberjack*: processes events sent in the lumberjack protocol. Now called https://github.com/elasticsearch/logstash-forwarder[logstash-forwarder].
+
+For more information about the available inputs, see <<input-plugins,Input Plugins>>.
+
+[float]
+==== Filters
+Filters are intermediary processing devices in the Logstash pipeline. You can combine filters with conditionals to perform an action on an event if it meets certain criteria. Some useful filters include:
+
+* *grok*: parse and structure arbitrary text. Grok is currently the best way in Logstash to parse unstructured log data into something structured and queryable. With 120 patterns built-in to Logstash, it's more than likely you'll find one that meets your needs!
+* *mutate*: perform general transformations on event fields. You can rename, remove, replace, and modify fields in your events.
+* *drop*: drop an event completely, for example, 'debug' events.
+* *clone*: make a copy of an event, possibly adding or removing fields.
+* *geoip*: add information about geographical location of IP addresses (also displays amazing charts in Kibana!)
+
+For more information about the available filters, see <<filter-plugins,Filter Plugins>>.
+
+[float]
+==== Outputs
+Outputs are the final phase of the Logstash pipeline. An event can pass through multiple outputs, but once all output processing is complete, the event has finished its execution. Some commonly used outputs include:
+
+* *elasticsearch*: send event data to Elasticsearch. If you're planning to save your data in an efficient, convenient, and easily queryable format... Elasticsearch is the way to go. Period. Yes, we're biased :)
+* *file*: write event data to a file on disk.
+* *graphite*: send event data to graphite, a popular open source tool for storing and graphing metrics. http://graphite.wikidot.com/
+* *statsd*: send event data to statsd, a service that "listens for statistics, like counters and timers, sent over UDP and sends aggregates to one or more pluggable backend services". If you're already using statsd, this could be useful for you!
+
+For more information about the available outputs, see <<output-plugins,Output Plugins>>.
+
+[float]
+==== Codecs
+Codecs are basically stream filters that can operate as part of an input or output. Codecs enable you to easily separate the transport of your messages from the serialization process. Popular codecs include `json`, `msgpack`, and `plain` (text).
+
+* *json*: encode or decode data in the JSON format.
+* *multiline*: merge multiple-line text events such as java exception and stacktrace messages into a single event.  
+
+For more information about the available codecs, see <<codec-plugins,Codec Plugins>>.
 
 [float]
 === Fault Tolerance
 
+Events are passed from stage to stage using internal queues implemented with a Ruby `SizedQueue`. A `SizedQueue` has a maximum number of items it can contain.  When the queue is at maximum capacity, all writes to the queue are blocked.
+
+Logstash sets the size of each queue to 20. This means a maximum of 20 events can be pending for the next stage, which helps prevent data loss and keeps Logstash from acting as a data storage system. These internal queues are not intended for storing messages long-term.
+
+The small queue sizes mean that Logstash simply blocks and stalls safely when there's a heavy load or temporary pipeline problems. The alternatives would be to either have an unlimited queue or drop messages when there's a problem. An unlimited queue can grow unbounded and eventually exceed memory, causing a crash that loses all of the queued messages. In most cases, dropping messages outright is equally undesirable.
+
 An output can fail or have problems due to downstream issues, such as a full disk, permissions problems, temporary network failures, or service outages. Most outputs keep retrying to ship events affected by the failure.
 
 If an output is failing, the output thread waits until the output is able to successfully send the message. The output stops reading from the output queue, which means the queue can fill up with events. 
@@ -48,9 +93,7 @@ The output worker model is currently a single thread. Outputs receive events in
 Outputs might decide to temporarily buffer events before publishing them. One example of this is the `elasticsearch` output, which buffers events and flushes them all at once using a separate thread. This mechanism (buffering many events and writing in a separate thread) can improve performance because it prevents the Logstash pipeline from being stalled waiting for a response from elasticsearch.
 
 [float]
-=== Consequences and Expectations
-
-The small queue sizes mean that Logstash simply blocks and stalls safely when there's a heavy load or temporary pipeline problems. The alternatives would be to either have an unlimited queue or drop messages when there's a problem. An unlimited queue can grow unbounded and eventually exceed memory, causing a crash that loses all of the queued messages. In most cases, dropping messages outright is undesirable.
+=== Resource Usage
 
 Logstash typically has at least 3 threads (2 if you have no filters). One input thread, one filter worker thread, and one output thread. If you see Logstash using multiple CPUs, this is likely why. If you want to know more about what each thread is doing, you should read this article: http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance]. Threads in Java have names and you can use `jstack` and `top` to figure out who is using what resources. 
 
