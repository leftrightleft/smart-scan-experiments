diff --git a/docs/static/monitoring/monitoring-apis.asciidoc b/docs/static/monitoring/monitoring-apis.asciidoc
index 63c2961f442..42736baff61 100644
--- a/docs/static/monitoring/monitoring-apis.asciidoc
+++ b/docs/static/monitoring/monitoring-apis.asciidoc
@@ -632,6 +632,15 @@ Example response:
             "out" : 216485,
             "queue_push_duration_in_millis" : 342466
           },
+          "flow": {
+            "throughput": {
+              "current": 1.223,
+              "last_1_minute": 0.2532,
+              "last_5_minute": 0.3264,
+              "last_15_minute": 0.4721,
+              "lifetime": 0.06579
+            }
+          },
           "name" : "beats"
         } ],
         "filters" : [ {
@@ -645,6 +654,22 @@ Example response:
           "patterns_per_field" : {
             "message" : 1
           },
+          "flow": {
+            "worker_utilization": {
+              "current": 8.368,
+              "last_1_minute": 3.224,
+              "last_5_minutes": 3.091,
+              "last_15_minutes": 3.539,
+              "lifetime": 6.706
+            },
+            "worker_millis_per_event": {
+              "current": 0.1,
+              "last_1_minute": 0.03333,
+              "last_5_minutes": 0.03115,
+              "last_15_minutes": 0.03552,
+              "lifetime": 0.06773
+            }
+          },
           "name" : "grok"
         }, {
           "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-3",
@@ -653,6 +678,22 @@ Example response:
             "in" : 216485,
             "out" : 216485
           },
+          "flow": {
+            "worker_utilization": {
+              "current": 8.368,
+              "last_1_minute": 3.224,
+              "last_5_minutes": 3.091,
+              "last_15_minutes": 3.539,
+              "lifetime": 6.706
+            },
+            "worker_millis_per_event": {
+              "current": 0.1,
+              "last_1_minute": 0.03333,
+              "last_5_minutes": 0.03115,
+              "last_15_minutes": 0.03552,
+              "lifetime": 0.06773
+            }
+          },
           "name" : "geoip"
         } ],
         "outputs" : [ {
@@ -662,6 +703,22 @@ Example response:
             "in" : 216485,
             "out" : 216485
           },
+          "flow": {
+            "worker_utilization": {
+              "current": 8.368,
+              "last_1_minute": 3.224,
+              "last_5_minutes": 3.091,
+              "last_15_minutes": 3.539,
+              "lifetime": 6.706
+            },
+            "worker_millis_per_event": {
+              "current": 0.1,
+              "last_1_minute": 0.03333,
+              "last_5_minutes": 0.03115,
+              "last_15_minutes": 0.03552,
+              "lifetime": 0.06773
+            }
+          },
           "name" : "elasticsearch"
         } ]
       },
@@ -713,6 +770,15 @@ Example response:
             "out" : 87247,
             "queue_push_duration_in_millis" : 1532
           },
+          "flow": {
+            "throughput": {
+              "current": 1.223,
+              "last_1_minute": 0.2532,
+              "last_5_minute": 0.3264,
+              "last_15_minute": 0.4721,
+              "lifetime": 0.06579
+            }
+          },
           "name" : "twitter"
         } ],
         "filters" : [ ],
@@ -723,6 +789,22 @@ Example response:
             "in" : 87247,
             "out" : 87247
           },
+          "flow": {
+            "worker_utilization": {
+              "current": 8.368,
+              "last_1_minute": 3.224,
+              "last_5_minutes": 3.091,
+              "last_15_minutes": 3.539,
+              "lifetime": 6.706
+            },
+            "worker_millis_per_event": {
+              "current": 0.1,
+              "last_1_minute": 0.03333,
+              "last_5_minutes": 0.03115,
+              "last_15_minutes": 0.03552,
+              "lifetime": 0.06773
+            }
+          },
           "name" : "elasticsearch"
         } ]
       },
@@ -798,6 +880,15 @@ Example response:
             "out" : 216485,
             "queue_push_duration_in_millis" : 342466
           },
+          "flow": {
+            "throughput": {
+              "current": 1.223,
+              "last_1_minute": 0.2532,
+              "last_5_minute": 0.3264,
+              "last_15_minute": 0.4721,
+              "lifetime": 0.06579
+            }
+          },
           "name" : "beats"
         } ],
         "filters" : [ {
@@ -811,6 +902,22 @@ Example response:
           "patterns_per_field" : {
             "message" : 1
           },
+          "flow": {
+            "worker_utilization": {
+              "current": 8.368,
+              "last_1_minute": 3.224,
+              "last_5_minutes": 3.091,
+              "last_15_minutes": 3.539,
+              "lifetime": 6.706
+            },
+            "worker_millis_per_event": {
+              "current": 0.1,
+              "last_1_minute": 0.03333,
+              "last_5_minutes": 0.03115,
+              "last_15_minutes": 0.03552,
+              "lifetime": 0.06773
+            }
+          },
           "name" : "grok"
         }, {
           "id" : "35131f351e2dc5ed13ee04265a8a5a1f95292165-3",
@@ -828,6 +935,22 @@ Example response:
             "in" : 216485,
             "out" : 216485
           },
+          "flow": {
+            "worker_utilization": {
+              "current": 8.368,
+              "last_1_minute": 3.224,
+              "last_5_minutes": 3.091,
+              "last_15_minutes": 3.539,
+              "lifetime": 6.706
+            },
+            "worker_millis_per_event": {
+              "current": 0.1,
+              "last_1_minute": 0.03333,
+              "last_5_minutes": 0.03115,
+              "last_15_minutes": 0.03552,
+              "lifetime": 0.06773
+            }
+          },
           "name" : "elasticsearch"
         } ]
       },
@@ -885,6 +1008,29 @@ NOTE: The size of a PQ on disk includes both unacknowledged events and previousl
       This means it grows gradually as individual events are added, but shrinks in large chunks each time a whole page of processed events is reclaimed (read more: <<garbage-collection, PQ disk garbage collection>>).
 |===
 
+[discrete]
+[[plugin-flow-rates]]
+===== Plugin flow rates
+
+Several additional plugin-level flow rates are available, and can be helpful for identifying problems with individual plugins:
+
+[%autowidth.stretch]
+|===
+| Plugin Types | Flow Rate | Definition
+
+| Inputs | `throughput` | This metric is expressed in events-per-second, and is the rate of events this input plugin is pushing into the pipeline's queue relative to wall-clock time (`events.in` / `second`).
+It includes events that are blocked by the queue and have not yet been accepted.
+
+| Filters, Outputs | `worker_utilization` |
+This is a unitless metric that indicates the percentage of available worker time being used by this individual plugin (`duration` / (`uptime` * `pipeline.workers`).
+It is useful for identifying which plugins in a pipeline are using the available worker resources.
+
+| Filters, Outputs | `worker_millis_per_event` |
+This metric is expressed in worker-millis-spent-per-event (`duration_in_millis` / `events.in`) with higher scores indicating more resources spent per event.
+It is especially useful for identifying issues with plugins that operate on a small subset of events.
+An `"Infinity"` value for a given flow window indicates that worker millis have been spent without any events completing processing, and can indicate a plugin that is stuck.
+
+|===
 [discrete]
 [[reload-stats]]
 ==== Reload stats
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/QueueFactoryExt.java b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueFactoryExt.java
index 457d19ae4db..b5d1288b89f 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/QueueFactoryExt.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueFactoryExt.java
@@ -33,6 +33,7 @@
 import org.jruby.runtime.builtin.IRubyObject;
 import org.logstash.RubyUtil;
 import org.logstash.ackedqueue.ext.JRubyWrappedAckedQueueExt;
+import org.logstash.common.SettingKeyDefinitions;
 import org.logstash.execution.AbstractWrappedQueueExt;
 import org.logstash.ext.JrubyWrappedSynchronousQueueExt;
 
@@ -55,7 +56,7 @@ public final class QueueFactoryExt extends RubyBasicObject {
     /**
      * A contextual name to expose the queue type.
      */
-    public static String CONTEXT_NAME = "queue.type";
+    public static String QUEUE_TYPE_CONTEXT_NAME = "queue.type";
 
     private static final long serialVersionUID = 1L;
 
@@ -66,11 +67,11 @@ public QueueFactoryExt(final Ruby runtime, final RubyClass metaClass) {
     @JRubyMethod(meta = true)
     public static AbstractWrappedQueueExt create(final ThreadContext context, final IRubyObject recv,
         final IRubyObject settings) throws IOException {
-        final String type = getSetting(context, settings, CONTEXT_NAME).asJavaString();
+        final String type = getSetting(context, settings, QUEUE_TYPE_CONTEXT_NAME).asJavaString();
         if (PERSISTED_TYPE.equals(type)) {
             final Path queuePath = Paths.get(
-                getSetting(context, settings, "path.queue").asJavaString(),
-                getSetting(context, settings, "pipeline.id").asJavaString()
+                getSetting(context, settings, SettingKeyDefinitions.PATH_QUEUE).asJavaString(),
+                getSetting(context, settings, SettingKeyDefinitions.PIPELINE_ID).asJavaString()
             );
 
             // Files.createDirectories raises a FileAlreadyExistsException
@@ -83,13 +84,13 @@ public static AbstractWrappedQueueExt create(final ThreadContext context, final
                 .initialize(
                     context, new IRubyObject[]{
                         context.runtime.newString(queuePath.toString()),
-                        getSetting(context, settings, "queue.page_capacity"),
-                        getSetting(context, settings, "queue.max_events"),
-                        getSetting(context, settings, "queue.checkpoint.writes"),
-                        getSetting(context, settings, "queue.checkpoint.acks"),
-                        getSetting(context, settings, "queue.checkpoint.interval"),
-                        getSetting(context, settings, "queue.checkpoint.retry"),
-                        getSetting(context, settings, "queue.max_bytes")
+                        getSetting(context, settings, SettingKeyDefinitions.QUEUE_PAGE_CAPACITY),
+                        getSetting(context, settings, SettingKeyDefinitions.QUEUE_MAX_EVENTS),
+                        getSetting(context, settings, SettingKeyDefinitions.QUEUE_CHECKPOINT_WRITES),
+                        getSetting(context, settings, SettingKeyDefinitions.QUEUE_CHECKPOINT_ACKS),
+                        getSetting(context, settings, SettingKeyDefinitions.QUEUE_CHECKPOINT_INTERVAL),
+                        getSetting(context, settings, SettingKeyDefinitions.QUEUE_CHECKPOINT_RETRY),
+                        getSetting(context, settings, SettingKeyDefinitions.QUEUE_MAX_BYTES)
                     }
                 );
         } else if (MEMORY_TYPE.equals(type)) {
@@ -97,9 +98,9 @@ public static AbstractWrappedQueueExt create(final ThreadContext context, final
                 context.runtime, RubyUtil.WRAPPED_SYNCHRONOUS_QUEUE_CLASS
             ).initialize(
                 context, context.runtime.newFixnum(
-                    getSetting(context, settings, "pipeline.batch.size")
+                    getSetting(context, settings, SettingKeyDefinitions.PIPELINE_BATCH_SIZE)
                         .convertToInteger().getIntValue()
-                        * getSetting(context, settings, "pipeline.workers")
+                        * getSetting(context, settings, SettingKeyDefinitions.PIPELINE_WORKERS)
                         .convertToInteger().getIntValue()
                 )
             );
diff --git a/logstash-core/src/main/java/org/logstash/common/SettingKeyDefinitions.java b/logstash-core/src/main/java/org/logstash/common/SettingKeyDefinitions.java
new file mode 100644
index 00000000000..97f00d358e7
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/SettingKeyDefinitions.java
@@ -0,0 +1,48 @@
+/*
+ * Licensed to Elasticsearch B.V. under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch B.V. licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *	http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.common;
+
+/**
+ * A class to contain Logstash setting definitions broadly used in the Java scope.
+ */
+public class SettingKeyDefinitions {
+
+    public static final String PIPELINE_ID = "pipeline.id";
+
+    public static final String PIPELINE_WORKERS = "pipeline.workers";
+
+    public static final String PIPELINE_BATCH_SIZE = "pipeline.batch.size";
+
+    public static final String PATH_QUEUE = "path.queue";
+
+    public static final String QUEUE_PAGE_CAPACITY = "queue.page_capacity";
+
+    public static final String QUEUE_MAX_EVENTS = "queue.max_events";
+
+    public static final String QUEUE_CHECKPOINT_WRITES = "queue.checkpoint.writes";
+
+    public static final String QUEUE_CHECKPOINT_ACKS = "queue.checkpoint.acks";
+
+    public static final String QUEUE_CHECKPOINT_INTERVAL = "queue.checkpoint.interval";
+
+    public static final String QUEUE_CHECKPOINT_RETRY = "queue.checkpoint.retry";
+
+    public static final String QUEUE_MAX_BYTES = "queue.max_bytes";
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/JavaCodecDelegator.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/JavaCodecDelegator.java
index ab338b937cb..7440633269e 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/compiler/JavaCodecDelegator.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/JavaCodecDelegator.java
@@ -38,10 +38,6 @@
 
 public class JavaCodecDelegator implements Codec {
 
-    public static final String ENCODE_KEY = "encode";
-    public static final String DECODE_KEY = "decode";
-    public static final String IN_KEY = "writes_in";
-
     private final Codec codec;
 
     protected final CounterMetric encodeMetricIn;
@@ -54,7 +50,6 @@ public class JavaCodecDelegator implements Codec {
 
     protected final TimerMetric decodeMetricTime;
 
-
     public JavaCodecDelegator(final Context context, final Codec codec) {
         this.codec = codec;
 
@@ -63,12 +58,12 @@ public JavaCodecDelegator(final Context context, final Codec codec) {
         synchronized(metric.root()) {
             metric.gauge(MetricKeys.NAME_KEY.asJavaString(), codec.getName());
 
-            final NamespacedMetric encodeMetric = metric.namespace(ENCODE_KEY);
-            encodeMetricIn = encodeMetric.counter(IN_KEY);
+            final NamespacedMetric encodeMetric = metric.namespace(MetricKeys.ENCODE_KEY.asJavaString());
+            encodeMetricIn = encodeMetric.counter(MetricKeys.WRITES_IN_KEY.asJavaString());
             encodeMetricTime = encodeMetric.timer(MetricKeys.DURATION_IN_MILLIS_KEY.asJavaString());
 
-            final NamespacedMetric decodeMetric = metric.namespace(DECODE_KEY);
-            decodeMetricIn = decodeMetric.counter(IN_KEY);
+            final NamespacedMetric decodeMetric = metric.namespace(MetricKeys.DECODE_KEY.asJavaString());
+            decodeMetricIn = decodeMetric.counter(MetricKeys.WRITES_IN_KEY.asJavaString());
             decodeMetricOut = decodeMetric.counter(MetricKeys.OUT_KEY.asJavaString());
             decodeMetricTime = decodeMetric.timer(MetricKeys.DURATION_IN_MILLIS_KEY.asJavaString());
         }
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/OutputDelegatorExt.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/OutputDelegatorExt.java
index 7d5d03a31d2..bea5d5bab5c 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/compiler/OutputDelegatorExt.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/OutputDelegatorExt.java
@@ -38,8 +38,7 @@
 import static org.logstash.RubyUtil.RUBY;
 
 @JRubyClass(name = "OutputDelegator")
-public final class
-OutputDelegatorExt extends AbstractOutputDelegatorExt {
+public final class OutputDelegatorExt extends AbstractOutputDelegatorExt {
 
     private static final long serialVersionUID = 1L;
 
@@ -49,17 +48,27 @@
 
     @JRubyMethod(required = 5)
     public OutputDelegatorExt initialize(final ThreadContext context, final IRubyObject[] arguments) {
+        ExecutionContextExt executionContext = (ExecutionContextExt) arguments[2];
+        RubyClass klass = (RubyClass) arguments[0];
+        AbstractMetricExt metric = (AbstractMetricExt) arguments[1];
+        OutputStrategyExt.OutputStrategyRegistryExt strategyRegistry = (OutputStrategyExt.OutputStrategyRegistryExt) arguments[3];
+        RubyHash args = (RubyHash) arguments[4];
         return initialize(
-            context, (RubyHash) arguments[4], (RubyClass) arguments[0], (AbstractMetricExt) arguments[1],
-            (ExecutionContextExt) arguments[2],
-            (OutputStrategyExt.OutputStrategyRegistryExt) arguments[3]
+            context,
+            args,
+            klass,
+            metric,
+            executionContext,
+            strategyRegistry
         );
     }
 
-    public OutputDelegatorExt initialize(final ThreadContext context, final RubyHash args,
-        final RubyClass outputClass, final AbstractMetricExt metric,
-        final ExecutionContextExt executionContext,
-        final OutputStrategyExt.OutputStrategyRegistryExt strategyRegistry) {
+    public OutputDelegatorExt initialize(final ThreadContext context,
+                                         final RubyHash args,
+                                         final RubyClass outputClass,
+                                         final AbstractMetricExt metric,
+                                         final ExecutionContextExt executionContext,
+                                         final OutputStrategyExt.OutputStrategyRegistryExt strategyRegistry) {
         this.outputClass = outputClass;
         initMetrics(
             args.op_aref(context, RubyString.newString(context.runtime, "id")).asJavaString(),
diff --git a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
index 151e24d0794..ae1c2174e9c 100644
--- a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
@@ -65,6 +65,7 @@
 import org.logstash.common.DeadLetterQueueFactory;
 import org.logstash.common.EnvironmentVariableProvider;
 import org.logstash.common.IncompleteSourceWithMetadataException;
+import org.logstash.common.SettingKeyDefinitions;
 import org.logstash.common.SourceWithMetadata;
 import org.logstash.common.io.DeadLetterQueueWriter;
 import org.logstash.common.io.QueueStorageType;
@@ -73,6 +74,8 @@
 import org.logstash.config.ir.InvalidIRException;
 import org.logstash.config.ir.PipelineConfig;
 import org.logstash.config.ir.PipelineIR;
+import org.logstash.config.ir.compiler.AbstractFilterDelegatorExt;
+import org.logstash.config.ir.compiler.AbstractOutputDelegatorExt;
 import org.logstash.execution.queue.QueueWriter;
 import org.logstash.ext.JRubyAbstractQueueWriteClientExt;
 import org.logstash.ext.JRubyWrappedWriteClientExt;
@@ -82,6 +85,7 @@
 import org.logstash.instrument.metrics.Metric;
 import org.logstash.instrument.metrics.MetricType;
 import org.logstash.instrument.metrics.NullMetricExt;
+import org.logstash.instrument.metrics.UpScaledMetric;
 import org.logstash.instrument.metrics.timer.TimerMetric;
 import org.logstash.instrument.metrics.UptimeMetric;
 import org.logstash.instrument.metrics.counter.LongCounter;
@@ -210,7 +214,7 @@ private AbstractPipelineExt initialize(final ThreadContext context,
             )
         );
         settings = pipelineSettings.callMethod(context, "settings");
-        final IRubyObject id = getSetting(context, "pipeline.id");
+        final IRubyObject id = getSetting(context, SettingKeyDefinitions.PIPELINE_ID);
         if (id.isNil()) {
             pipelineId = id();
         } else {
@@ -532,23 +536,8 @@ public final IRubyObject initializeFlowMetrics(final ThreadContext context) {
         this.flowMetrics.add(concurrencyFlow);
         storeMetric(context, flowNamespace, concurrencyFlow);
 
-        // collect the queue_persisted_growth_events & queue_persisted_growth_bytes metrics if only persisted queue is enabled.
-        if (getSetting(context, QueueFactoryExt.CONTEXT_NAME).asJavaString()
-                .equals(QueueFactoryExt.PERSISTED_TYPE)) {
-
-            final RubySymbol[] queueNamespace = buildNamespace(QUEUE_KEY);
-            final RubySymbol[] queueCapacityNamespace = buildNamespace(QUEUE_KEY, CAPACITY_KEY);
-
-            final Supplier<NumberGauge> eventsGaugeMetricSupplier = () -> initOrGetNumberGaugeMetric(context, queueNamespace, EVENTS_KEY).orElse(null);
-            final FlowMetric growthEventsFlow = createFlowMetric(QUEUE_PERSISTED_GROWTH_EVENTS_KEY, eventsGaugeMetricSupplier, () -> uptimeInPreciseSeconds);
-            this.flowMetrics.add(growthEventsFlow);
-            storeMetric(context, flowNamespace, growthEventsFlow);
-
-            final Supplier<NumberGauge> queueSizeInBytesMetricSupplier = () -> initOrGetNumberGaugeMetric(context, queueCapacityNamespace, QUEUE_SIZE_IN_BYTES_KEY).orElse(null);
-            final FlowMetric growthBytesFlow = createFlowMetric(QUEUE_PERSISTED_GROWTH_BYTES_KEY, queueSizeInBytesMetricSupplier, () -> uptimeInPreciseSeconds);
-            this.flowMetrics.add(growthBytesFlow);
-            storeMetric(context, flowNamespace, growthBytesFlow);
-        }
+        initializePqFlowMetrics(context, flowNamespace, uptimeMetric);
+        initializePluginFlowMetrics(context, uptimeMetric);
         return context.nil;
     }
 
@@ -563,6 +552,7 @@ private static FlowMetric createFlowMetric(final RubySymbol name,
                                                final Metric<? extends Number> denominatorMetric) {
         return FlowMetric.create(name.asJavaString(), numeratorMetric, denominatorMetric);
     }
+
     private static FlowMetric createFlowMetric(final RubySymbol name,
                                                final Supplier<? extends Metric<? extends Number>> numeratorMetricSupplier,
                                                final Supplier<? extends Metric<? extends Number>> denominatorMetricSupplier) {
@@ -614,6 +604,75 @@ private UptimeMetric initOrGetUptimeMetric(final ThreadContext context,
         return retrievedMetric.toJava(UptimeMetric.class);
     }
 
+    private void initializePqFlowMetrics(final ThreadContext context, final RubySymbol[] flowNamespace, final UptimeMetric uptime) {
+        final Metric<Number> uptimeInPreciseSeconds = uptime.withUnitsPrecise(SECONDS);
+        final IRubyObject queueContext = getSetting(context, QueueFactoryExt.QUEUE_TYPE_CONTEXT_NAME);
+        if (!queueContext.isNil() && queueContext.asJavaString().equals(QueueFactoryExt.PERSISTED_TYPE)) {
+
+            final RubySymbol[] queueNamespace = buildNamespace(QUEUE_KEY);
+            final RubySymbol[] queueCapacityNamespace = buildNamespace(QUEUE_KEY, CAPACITY_KEY);
+
+            final Supplier<NumberGauge> eventsGaugeMetricSupplier = () -> initOrGetNumberGaugeMetric(context, queueNamespace, EVENTS_KEY).orElse(null);
+            final FlowMetric growthEventsFlow = createFlowMetric(QUEUE_PERSISTED_GROWTH_EVENTS_KEY, eventsGaugeMetricSupplier, () -> uptimeInPreciseSeconds);
+            this.flowMetrics.add(growthEventsFlow);
+            storeMetric(context, flowNamespace, growthEventsFlow);
+
+            final Supplier<NumberGauge> queueSizeInBytesMetricSupplier = () -> initOrGetNumberGaugeMetric(context, queueCapacityNamespace, QUEUE_SIZE_IN_BYTES_KEY).orElse(null);
+            final FlowMetric growthBytesFlow = createFlowMetric(QUEUE_PERSISTED_GROWTH_BYTES_KEY, queueSizeInBytesMetricSupplier, () -> uptimeInPreciseSeconds);
+            this.flowMetrics.add(growthBytesFlow);
+            storeMetric(context, flowNamespace, growthBytesFlow);
+        }
+    }
+
+    private void initializePluginFlowMetrics(final ThreadContext context, final UptimeMetric uptime) {
+        for (IRubyObject rubyObject: lirExecution.inputs()) {
+            IRubyObject rubyfiedId = rubyObject.callMethod(context, "id");
+            String id = rubyfiedId.toString();
+            initializePluginThroughputFlowMetric(context, uptime, id);
+        }
+
+        final int workerCount = getSetting(context, SettingKeyDefinitions.PIPELINE_WORKERS).convertToInteger().getIntValue();
+
+        for (AbstractFilterDelegatorExt delegator: lirExecution.filters()) {
+            initializePluginWorkerFlowMetrics(context, workerCount, uptime, FILTERS_KEY, delegator.getId().asJavaString());
+        }
+
+        for (AbstractOutputDelegatorExt delegator: lirExecution.outputs()) {
+            initializePluginWorkerFlowMetrics(context, workerCount, uptime, OUTPUTS_KEY, delegator.getId().asJavaString());
+        }
+    }
+
+    private void initializePluginThroughputFlowMetric(final ThreadContext context, final UptimeMetric uptime, final String id) {
+        final Metric<Number> uptimeInPreciseSeconds = uptime.withUnitsPrecise(SECONDS);
+        final RubySymbol[] eventsNamespace = buildNamespace(PLUGINS_KEY, INPUTS_KEY, RubyUtil.RUBY.newSymbol(id), EVENTS_KEY);
+        final LongCounter eventsOut = initOrGetCounterMetric(context, eventsNamespace, OUT_KEY);
+
+        final FlowMetric throughputFlow = createFlowMetric(PLUGIN_THROUGHPUT_KEY, eventsOut, uptimeInPreciseSeconds);
+        this.flowMetrics.add(throughputFlow);
+
+        final RubySymbol[] flowNamespace = buildNamespace(PLUGINS_KEY, INPUTS_KEY, RubyUtil.RUBY.newSymbol(id), FLOW_KEY);
+        storeMetric(context, flowNamespace, throughputFlow);
+    }
+
+    private void initializePluginWorkerFlowMetrics(final ThreadContext context, final int workerCount, final UptimeMetric uptime, final RubySymbol key, final String id) {
+        final Metric<Number> uptimeInPreciseMillis = uptime.withUnitsPrecise(MILLISECONDS);
+
+        final RubySymbol[] eventsNamespace = buildNamespace(PLUGINS_KEY, key, RubyUtil.RUBY.newSymbol(id), EVENTS_KEY);
+        final TimerMetric durationInMillis = initOrGetTimerMetric(context, eventsNamespace, DURATION_IN_MILLIS_KEY);
+        final LongCounter counterEvents = initOrGetCounterMetric(context, eventsNamespace, IN_KEY);
+        final FlowMetric workerCostPerEvent = createFlowMetric(WORKER_MILLIS_PER_EVENT_KEY, durationInMillis, counterEvents);
+        this.flowMetrics.add(workerCostPerEvent);
+
+        final UpScaledMetric percentScaledDurationInMillis = new UpScaledMetric(durationInMillis, 100);
+        final UpScaledMetric availableWorkerTimeInMillis = new UpScaledMetric(uptimeInPreciseMillis, workerCount);
+        final FlowMetric workerUtilization = createFlowMetric(WORKER_UTILIZATION_KEY, percentScaledDurationInMillis, availableWorkerTimeInMillis);
+        this.flowMetrics.add(workerUtilization);
+
+        final RubySymbol[] flowNamespace = buildNamespace(PLUGINS_KEY, key, RubyUtil.RUBY.newSymbol(id), FLOW_KEY);
+        storeMetric(context, flowNamespace, workerCostPerEvent);
+        storeMetric(context, flowNamespace, workerUtilization);
+    }
+
     private <T> void storeMetric(final ThreadContext context,
                                  final RubySymbol[] subPipelineNamespacePath,
                                  final Metric<T> metric) {
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java
index 89a04945e5c..2730ab83749 100644
--- a/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/MetricKeys.java
@@ -43,6 +43,10 @@ private MetricKeys() {
 
     public static final RubySymbol INPUTS_KEY = RubyUtil.RUBY.newSymbol("inputs");
 
+    public static final RubySymbol FILTERS_KEY = RubyUtil.RUBY.newSymbol("filters");
+
+    public static final RubySymbol OUTPUTS_KEY = RubyUtil.RUBY.newSymbol("outputs");
+
     public static final RubySymbol DURATION_IN_MILLIS_KEY = RubyUtil.RUBY.newSymbol("duration_in_millis");
 
     public static final RubySymbol PUSH_DURATION_KEY = RubyUtil.RUBY.newSymbol("queue_push_duration_in_millis");
@@ -94,9 +98,22 @@ private MetricKeys() {
 
     public static final RubySymbol WORKER_CONCURRENCY_KEY = RubyUtil.RUBY.newSymbol("worker_concurrency");
 
+    public static final RubySymbol WORKER_MILLIS_PER_EVENT_KEY = RubyUtil.RUBY.newSymbol("worker_millis_per_event");
+
+    public static final RubySymbol WORKER_UTILIZATION_KEY = RubyUtil.RUBY.newSymbol("worker_utilization");
+
     public static final RubySymbol UPTIME_IN_MILLIS_KEY = RubyUtil.RUBY.newSymbol("uptime_in_millis");
 
     public static final RubySymbol QUEUE_PERSISTED_GROWTH_EVENTS_KEY = RubyUtil.RUBY.newSymbol("queue_persisted_growth_events");
 
     public static final RubySymbol QUEUE_PERSISTED_GROWTH_BYTES_KEY = RubyUtil.RUBY.newSymbol("queue_persisted_growth_bytes");
+
+    public static final RubySymbol PLUGIN_THROUGHPUT_KEY = RubyUtil.RUBY.newSymbol("throughput");
+
+    public static final RubySymbol ENCODE_KEY = RubyUtil.RUBY.newSymbol("encode");
+
+    public static final RubySymbol DECODE_KEY = RubyUtil.RUBY.newSymbol("decode");
+
+    public static final RubySymbol WRITES_IN_KEY = RubyUtil.RUBY.newSymbol("writes_in");
+
 }
diff --git a/logstash-core/src/main/java/org/logstash/instrument/metrics/UpScaledMetric.java b/logstash-core/src/main/java/org/logstash/instrument/metrics/UpScaledMetric.java
new file mode 100644
index 00000000000..b16d9c06db3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/instrument/metrics/UpScaledMetric.java
@@ -0,0 +1,51 @@
+package org.logstash.instrument.metrics;
+
+import java.math.BigDecimal;
+
+import static org.logstash.instrument.metrics.MetricType.GAUGE_NUMBER;
+
+/**
+ * A scaled metric to apply given scale at metric get value.
+ */
+public class UpScaledMetric extends AbstractMetric<BigDecimal> implements Metric<BigDecimal> {
+
+    private final Metric<? extends Number> metric;
+
+    private final BigDecimal scaleFactor;
+
+    public UpScaledMetric(final String name,
+                          final Metric<? extends Number> metric,
+                          final int scaleFactor) {
+        super(name);
+        this.metric = metric;
+        this.scaleFactor = BigDecimal.valueOf(scaleFactor);
+    }
+
+    public UpScaledMetric(final Metric<? extends Number> metric,
+                          final int scaleFactor) {
+        this(String.format("%s_x%s", metric.getName(), scaleFactor), metric, scaleFactor);
+    }
+
+    @Override
+    public MetricType getType() {
+        return GAUGE_NUMBER;
+    }
+
+    @Override
+    public BigDecimal getValue() {
+        final Number unscaledValue = this.metric.getValue();
+        final BigDecimal scaledValue = convertToBigDecimal(unscaledValue).multiply(scaleFactor);
+        return scaledValue;
+    }
+
+    private static BigDecimal convertToBigDecimal(Number number) {
+        if (number instanceof BigDecimal) { return (BigDecimal) number; }
+        if (number instanceof Integer
+                || number instanceof Long
+                || number instanceof Short
+                || number instanceof Byte) {
+            return BigDecimal.valueOf(number.longValue());
+        }
+        return BigDecimal.valueOf(number.doubleValue());
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/plugins/NamespacedMetricImpl.java b/logstash-core/src/main/java/org/logstash/plugins/NamespacedMetricImpl.java
index 56137837ff0..88af5bac9fb 100644
--- a/logstash-core/src/main/java/org/logstash/plugins/NamespacedMetricImpl.java
+++ b/logstash-core/src/main/java/org/logstash/plugins/NamespacedMetricImpl.java
@@ -42,7 +42,9 @@
  * metrics and other namespaces to it.
  */
 public class NamespacedMetricImpl implements NamespacedMetric {
+
     private final ThreadContext threadContext;
+
     private final AbstractNamespacedMetricExt metrics;
 
     public NamespacedMetricImpl(final ThreadContext threadContext, final AbstractNamespacedMetricExt metrics) {
diff --git a/logstash-core/src/main/java/org/logstash/plugins/factory/CodecPluginCreator.java b/logstash-core/src/main/java/org/logstash/plugins/factory/CodecPluginCreator.java
index 66f12a83c9c..e62c142967c 100644
--- a/logstash-core/src/main/java/org/logstash/plugins/factory/CodecPluginCreator.java
+++ b/logstash-core/src/main/java/org/logstash/plugins/factory/CodecPluginCreator.java
@@ -11,7 +11,7 @@
 
 import java.util.Map;
 
-class CodecPluginCreator extends  AbstractPluginCreator<Codec> {
+class CodecPluginCreator extends AbstractPluginCreator<Codec> {
 
     @Override
     public IRubyObject createDelegator(String name, Map<String, Object> pluginArgs, String id,
diff --git a/logstash-core/src/test/java/org/logstash/instrument/metrics/UpScaledMetricTest.java b/logstash-core/src/test/java/org/logstash/instrument/metrics/UpScaledMetricTest.java
new file mode 100644
index 00000000000..5938c5e0908
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/instrument/metrics/UpScaledMetricTest.java
@@ -0,0 +1,35 @@
+package org.logstash.instrument.metrics;
+
+import org.junit.Assert;
+import org.junit.Before;
+import org.junit.Test;
+
+import java.math.BigDecimal;
+import java.time.Duration;
+import java.time.Instant;
+
+public class UpScaledMetricTest {
+
+    Metric<Number> uptimeMetric;
+
+    ManualAdvanceClock clock;
+
+    @Before
+    public void setUp() {
+        clock = new ManualAdvanceClock(Instant.now());
+        uptimeMetric = new UptimeMetric("uptime", clock::nanoTime).withUnitsPrecise(UptimeMetric.ScaleUnits.SECONDS);
+    }
+
+    @Test
+    public void testGetType() {
+        UpScaledMetric upScaledMetric = new UpScaledMetric("up_scaled_metric", uptimeMetric, 2);
+        Assert.assertEquals(upScaledMetric.getType(), MetricType.GAUGE_NUMBER);
+    }
+
+    @Test
+    public void testGetValue() {
+        UpScaledMetric upScaledMetric = new UpScaledMetric("up_scaled_metric", uptimeMetric, 10);
+        clock.advance(Duration.ofSeconds(10));
+        Assert.assertTrue(BigDecimal.valueOf(100.0d).compareTo(upScaledMetric.getValue()) == 0);
+    }
+}
\ No newline at end of file
diff --git a/qa/integration/specs/monitoring_api_spec.rb b/qa/integration/specs/monitoring_api_spec.rb
index fa7515bce24..de22f2c030d 100644
--- a/qa/integration/specs/monitoring_api_spec.rb
+++ b/qa/integration/specs/monitoring_api_spec.rb
@@ -37,6 +37,7 @@
   
   let(:number_of_events) { 5 }
   let(:max_retry) { 120 }
+  let(:plugins_config) { "input { stdin {} } filter { mutate { add_tag => 'integration test adding tag' } } output { stdout {} }" }
 
   it "can retrieve event stats" do
     logstash_service = @fixture.get_service("logstash")
@@ -286,6 +287,45 @@
     end
   end
 
+  it "should retrieve plugin level flow metrics" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_with_stdin(plugins_config)
+    logstash_service.wait_for_logstash
+    number_of_events.times {
+      logstash_service.write_to_stdin("Testing plugin-level flow metrics")
+      sleep(1)
+    }
+
+    Stud.try(max_retry.times, [StandardError, RSpec::Expectations::ExpectationNotMetError]) do
+      # node_stats can fail if the stats subsystem isn't ready
+      result = logstash_service.monitoring_api.node_stats rescue nil
+      # if the result is nil, we probably aren't ready yet
+      # our assertion failure will cause Stud to retry
+      expect(result).not_to be_nil
+
+      expect(result).to include('pipelines' => hash_including('main' => hash_including('plugins' => hash_including('inputs', 'filters', 'outputs'))))
+
+      input_plugins = result.dig("pipelines", "main", "plugins", "inputs")
+      filter_plugins = result.dig("pipelines", "main", "plugins", "filters")
+      output_plugins = result.dig("pipelines", "main", "plugins", "outputs")
+      expect(input_plugins[0]).to_not be_nil
+
+      input_plugin_flow_status = input_plugins[0].dig("flow")
+      filter_plugin_flow_status = filter_plugins[0].dig("flow")
+      output_plugin_flow_status = output_plugins[0].dig("flow")
+
+      expect(input_plugin_flow_status).to include('throughput' => hash_including('current' => a_value >= 0, 'lifetime' => a_value > 0))
+      expect(filter_plugin_flow_status).to include(
+                                             'worker_utilization' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+                                             'worker_millis_per_event' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+                                           )
+      expect(output_plugin_flow_status).to include(
+                                             'worker_utilization' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+                                             'worker_millis_per_event' => hash_including('current' => a_value >= 0, 'lifetime' => a_value >= 0),
+                                           )
+    end
+  end
+
   private
 
   def logging_get_assert(logstash_service, logstash_level, slowlog_level, skip: '')
