diff --git a/docs/asciidoc/static/advanced-pipeline.asciidoc b/docs/asciidoc/static/advanced-pipeline.asciidoc
index 10e06aabaf4..5481eb4eb3e 100644
--- a/docs/asciidoc/static/advanced-pipeline.asciidoc
+++ b/docs/asciidoc/static/advanced-pipeline.asciidoc
@@ -39,7 +39,8 @@ Paste the skeleton into a file named `first-pipeline.conf` in your home Logstash
 This example creates a Logstash pipeline that takes Apache web logs as input, parses those logs to create specific, 
 named fields from the logs, and writes the parsed data to an Elasticsearch cluster.
 
-// You can download the sample data set used in this example http://tbd.co/groksample.log[here]. Unpack this file.
+You can download the sample data set used in this example 
+https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here]. Unpack this file.
 
 [float]
 [[configuring-file-input]]
@@ -53,7 +54,7 @@ Edit the `first-pipeline.conf` file to add the following text:
 [source,json]
 input {
     file {
-        path => "/path/to/groksample.log"
+        path => "/path/to/logstash-tutorial.log"
         start_position => beginning <1>
     }
 }
@@ -62,7 +63,7 @@ input {
 UNIX `tail -f` command. To change this default behavior and process the entire file, we need to specify the position 
 where Logstash starts processing the file.
 
-Replace `/path/to/` with the actual path to the location of `groksample.log` in your file system.
+Replace `/path/to/` with the actual path to the location of `logstash-tutorial.log` in your file system.
 
 [float]
 [[configuring-grok-filter]]
