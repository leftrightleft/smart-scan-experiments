diff --git a/CHANGELOG b/CHANGELOG
index 7aba7ea0309..62a11ed86e6 100644
--- a/CHANGELOG
+++ b/CHANGELOG
@@ -1,1572 +1 @@
-1.5.0-rc3
-  # general
-  - Added back the ability to install plugin gems built locally on top of Logstash. This will 
-    help plugin developers iterate and test locally without having to publish plugins (#2779)
-  - Fixed performance regressions from 1.4.2 especially for configurations which have 
-    conditionals in filter and output. Throughput numbers are either inline with 1.4.2
-    or improved for certain configurations (#2870)
-  - Fixed issue in core which was causing Logstash to not shutdown properly (#2796)    
-  - Added ability to add extra JVM options while running LS. You can use the LS_JAVA_OPTS 
-    environment variable to add to the default JVM options set out of the box. You could also
-    completely overwrite all the default options if you wish by setting JAVA_OPTS before
-    starting Logstash (#2942)
-  - Fixed a regression from 1.4.2 where removing a tag in filter fails if the input event is
-    JSON formatted (#2261)
-  - Fixed issue where setting workers > 1 would trigger messages like
-    "You are using a deprecated config setting ..." (#2865)
-  - Deprecated elasticsearch_http output plugin: All functionality is ported to
-    logstash-output-elasticsearch plugin using http protocol (#1757). If you try to use
-    the elasticsearch_http plugin, it will log a deprecated notice now.
-
-  # input
-  - File: When shutting down Logstash with file input, it would log a "permissions denied"
-    message. We fixed the underlying sinceDB issue while writing to a directory with no
-    permissions (#2964, #2935, #2882, file-input#16)
-
-  # filter
-  - Multiline: Fixed an issue where Logstash would crash while processing JSON formatted
-    events on Java 8 (#10)
-  - Mutate: Fixed issue where you can safely delete/rename fields which can have nil
-    values (#2977)
-
-  # output
-  - Deprecate the usage of index_type configuration. Added document_type to be consistent
-    with document_id (#102)
-  - Added warning when used with config embedded => true. Starting an embedded Elasticsearch
-    node is only recommended while prototyping. This should never be used in 
-    production setting (#99)
-
-1.5.0.rc2
-  # general
-  - Fixed an issue with packaging our release artifacts. Release artifacts were missing jar files
-    which caused Logstash to not start with Elasticsearch output while using the node or transport
-    client. (#2780)
-
-  # input
-  - Lumberjack: Fixed Logstash crashing because it was using old jls-lumberjack version (#7)
-
-1.5.0.rc1
-  # general
-  - You can now use LS_HOME/patterns directory to add generic patterns for those that may not be
-    associated with a particular plugin. Patterns in this dir will be loaded by default (#2225)
-  - We now check if the config file is correctly encoded. Otherwise we show a verbose error message
-    to convert the failing config file(s) to UTF-8 (#LOGSTASH-1103)
-  - Fixed bug in pipeline to gracefully teardown output workers when num workers > 1 (#2180)
-  - Fixed nologin path in release debian packages (#2283)
-  - Resolved issue where Logstash was crashing for users still using exclude_tags in their output
-    configuration (#2323)
-  - Updated and releases a new version of the logstash-event gem with latest changes in 1.5.0 RC1
-  - Windows: Significantly improved the initial user experience with Windows platform (#2504, #1426). 
-    Fixed many issues related to File input. Added support for using the plugin 
-    framework (installing, upgrading, removing)
-  - With the release of Kibana 4, we have removed the `bin/logstash web` command and any reference to
-    Kibana from Logstash (#2661)
-
-  # input
-  - Added new CouchDB input plugin to fetch data from CouchDB. Using the _changes API, data can be kept
-    in sync with any output like Elasticsearch by using this input
-  - File: Fixed a number of issues on Windows platform. These include:
-    - Resolving file locking issues which was causing log files to not rotate (#1557, #1389)
-    - Added support for using SinceDB to record multiple files' last read information (#1902)
-    - Fixed encoding issues which applies to many inputs (#2507)
-    - Resolved Logstash skipping lines when moving between files which are being followed (#1902)
-  - Kafka: Added support for whitelisting and blacklisting topics in the input. 
-  - EventLog: For Windows, this input gracefully shutsdown if there is a timeout while receiving events
-    This also prevents Logstash from being stuck (#1672)
-  - Heartbeat: We created a new input plugin for generating heartbeat messages at periodic intervals. 
-    Use this to monitor Logstash -- you can measure the latency of the pipeline using these heartbeat events,
-    and also check for availability
-  - S3: 
-    - Added IAM roles support so you can securely read and write events from S3 without providing your
-      AWS credentials (#1575). 
-    - Added support for using temporary credentials obtained from AWS STS (#1946)
-  - Lumberjack: Resolved issue where unrelated events were getting merged into a single event while using
-    this input with with the multiline codec (#2016)
-
-  # filter
-  - Mutate: 
-    - Resolved issue where convert option was creating an extra field in the event (#2268)
-    - Fixed issue where mutate with non-existent field was throwing an error (#2379)
-  - Multiline: Handled cases where we unintentionally deduplicated lines, such
-    as repeated lines in xml messages (#3) 
-
-  # output
-  - Elasticsearch: 
-    - Added support to be more resilient to transient errors in Elasticsearch. Previously, partial
-      failures from the bulk indexing functionality were not handled properly. With this fix, we added the ability
-      to capture failed requests from Elasticsearch and retry them. Error codes like 429 (too many requests) will
-      now be retried by default for 3 times. The number of retries and the interval between consecutive retries 
-      can be configured (#1631)
-    - Logstash does not create a "message.raw" by default whic is usually not_analyzed; this
-      helps save disk space (#11)
-    - Added sniffing config to be able to list machines in the cluster while using the transport client (#22) 
-  - S3: 
-    - Fixed a critical problem in the S3 Output plugin when using the size_file option. This could cause
-      data loss and data corruption of old logs ()
-    - Added IAM roles support so you can securely read and write events from S3 without providing your AWS
-      credentials (#1575)
-    - Added support for using temporary credentials obtained from AWS STS (#1946)
-    - Fixed a bug when the tags were not set in the plain text format (#1626)
-
-  # codec
-  - Added new Elasticsearch bulk codec which can be used to read data formatted in the Elasticsearch 
-    Bulk API (multiline json) format. For example, this codec can be used in combination with RabbitMQ input
-    to mirror the functionality of the RabbitMQ Elasticsearch river
-  - Cloudfront: Added support for handling Amazon CloudFront events
-  - Avro: We added a new codec for data serialization (#1566) 
-
-1.5.0.beta1
-  # general
-  - Performance improvements: Logstash 1.5.0 is much faster -- we have improved the throughput 
-    of grok filter in some cases by 100%. In our benchmark testing, using only grok filter and
-    ingesting apache logs, throughput increased from 34K eps to 50K eps. 
-    JSON serialization/deserialization are now implemented using JrJackson library which 
-    improved performance significantly. Ingesting JSON events 1.3KB in size measured a throughput
-    increase from 16Keps to 30K eps. With events 45KB in size, throughput increased from 
-    850 eps to 3.5K eps
-  - Allow spaces in field references like [hello world] (#1513)
-  - Add Plugin manager functionality to Logstash which allows to install, delete and 
-    update Logstash plugins
-  - Remove ability to run multiple subcommands from bin/logstash like 
-    bin/logstash agent -f something.conf -- web (#1747)
-  - Fixed Logstash crashing on converting from ASCII to UTF-8. This was caused by charset
-    conversion issues in input codec (LOGSTASH-1789)
-  - Allow storing 'metadata' to an event which is not sent/encoded on output. This eliminates
-    the need for intermediate fields for example, while using date filter. (#1834)
-  - Accept file and http uri in -f command line option for specifying config files (#1873)
-  - Filters that generated events (multiline, clone, split, metrics) now propagate those events 
-    correctly to future conditionals (#1431)
-  - Bump Kibana version to 3.1.2
-  - Fixed file descriptor leaks when using HTTP. The fix prevents Logstash from stalling, and
-    in some cases crashing from out-of-memory errors (#1604, LOGSTASH-892)
-
-  # input
-  - Lumberjack: fixed Logstash crashes with Java Out Of Memory because of TCP 
-    thread leaks (#LOGSTASH-2168)
-  - TCP: fixed connection threads leak (#1509)
-  - Stdin: prevent overwrite of host field if already present in Event (#1668)
-  - S3: AWS credentials can be specfied through environment variables (#1619)
-  - Kafka: merge @joekiller's plugin to Logstash to get events from Kafka (#1472)
-  - RabbitMQ: fixed march_hare client uses incorrect connection url (LOGSTASH-2276)
-  - RabbitMQ: use Bunny 1.5.0+ (#1894)
-  - Twitter: added improvements, robustness, fixes. full_tweet option now works, we handle 
-    Twitter rate limiting errors (#1471)
-  - TCP: fixed input host field also contains source port (LOGSTASH-1849)
-  - Syslog: if input does not match syslog format, add tag _grokparsefailure_sysloginputplugin
-    which can be used to debug (#1593)
-
-  # filter
-  - Mutate: gsub evaluates variables like %{format} in the replacement text (#1529)
-  - Grok: "break_on_match => false" option now works correctly (#1547)
-  - Grok: allow user@hostname in commonapache log pattern (#1500 #1736)
-  - Grok: use optimized ruby-grok library which improves throughput in some cases by 50% (#1657)
-  - Date: fixed match defaults to 1970-01-01 when none of the formats matches and 
-    UNIX format is present in the list (#1236, LOGSTASH-1597)
-  - Mutate: fixed confusing error message for invalid type conversion (#1656, LOGSTASH-2003)
-  - Date: support parsing almost-ISO8601 patterns like 2001-11-06 20:45:45.123-0000 (without a T)
-    which does not match %{TIMESTAMP_ISO8601}
-  - KV: allows dynamic include/exclude keys. For example, if an event has a key field and the user 
-    wants to parse out a value using the kv filter, the user should be able to 
-    include_keys: [ "%{key}" ]
-  - DNS: fixed add_tag adds tags even if filter was unsuccessful (#1785)
-  - XML: fixed UndefinedConversionError with UTF-8 encoding (LOGSTASH-2246)
-  - Mutate: fixed nested field notation for convert option like 
-    'convert => [ "[a][0]", "float" ]' (#1401)
-
-  # output
-  - RabbitMQ: fixed crash while running Logstash for longer periods, typically when there's no
-    traffic on the logstash<->rabbitmq socket (LOGSTASH-1886)
-  - Statsd: fixed issue of converting very small float numbers to scientific notation 
-    like 9.3e-05 (#1670)
-  - Fixed undefined method error when conditional on an output (#LOGSTASH-2288)
-  - Elasticsearch output: Added support for multiple hosts in configuration and enhanced stability
-  - Elasticsearch output: Logstash will not create a message.raw field by default now. Message field 
-    is not_analyzed by Elasticsearch and adding a multi-field was essentially doubling the disk space
-    required, with no benefit
-  - Elasticsearch output: We have improved the security of the Elasticsearch output, input, and filter by
-    adding authentication and transport encryption support. In http protocol you can configure SSL/TLS to 
-    enable encryption and HTTP basic authentication to provide a username and password while making
-    requests (#1453)
-  - Kafka: merge @joekiller's plugin into Logstash to produce events to Kafka (#1472)
-  - File: Added enhancements and validations for destination path. Absolute path cannot start with a dynamic
-    string like /%{myfield}/, /test-%{myfield}/
-
-1.4.2 (June 24, 2014)
-  # general
-  - fixed path issues when invoking bin/logstash outside its home directory
-
-  # input
-  - bugfix: generator: fixed stdin option support
-  - bugfix: file: fixed debian 7 path issue
-
-  # codecs
-  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
-  - improvement: collectd: add support for NaN values
-
-  # outputs
-  - improvement: nagios_nsca: fix external command invocation to avoid shell escaping
-
-1.4.1 (May 6, 2014)
-  # General
-  - bumped Elasticsearch to 1.1.1 and Kibana to 3.0.1
-  - improved specs & testing (Colin Surprenant), packaging (Richard Pijnenburg) & doc (James Turnbull)
-  - better $JAVA_HOME handling (Marc Chadwick)
-  - fixed bin/plugin target dir for when installing out from form logstash home (lr1980)
-  - fixed Accessors reset bug in Event#overwrite that was causing the infamous
-    "undefined method `tv_sec'" bug with the multiline filter (Colin Surprenant)
-  - fixed agent stalling when also using web option (Colin Surprenant)
-  - fixed accessing array-indexed event fields (Jonathan Van Eenwyk)
-  - new sysv init style scripts based on pleaserun (Richard Pijnenburg)
-  - better handling of invalid command line parameters (LOGSTASH-2024, Colin Surprenant)
-  - fixed running from a path containing spaces (LOGSTASH-1983, Colin Surprenant)
-
-  # inputs
-  - improvement: rabbitmq: upgraded Bunny gem to 1.1.8, fixes a threading leak and improves
-    latency (Michael Klishin)
-  - improvement: twitter: added "full_tweet" option (Jordan Sissel)
-  - improvement: generator: fixed the example doc (LOGSTASH-2093, Jason Kendall)
-  - improvement: imap: option to disable certificate validation (Sverre Bakke)
-
-  # codecs
-  - new: collectd: better performance & error handling than collectd input (Aaron Mildenstein)
-  - improvement: graphite: removed unused charset option (Colin Surprenant)
-  - improvement: json_spooler: is now deprecated (Colin Surprenant)
-  - improvement: proper charset support in all codecs (Colin Surprenant)
-
-  # filters
-  - bugfix: date: on_success actions only when date parsing actually succeed (Philippe Weber)
-  - bugfix: multiline: "undefined method `tv_sec'" fix (Colin Surprenant)
-  - bugfix: multiline: fix for "undefined method `[]' for nil:NilClass" (#1258, Colin Surprenant)
-  - improvement: date: fix specs for non "en" locale (Olivier Le Moal)
-  - improvement: grok: better pattern for RFC-5424 syslog format (Guillaume Espanel)
-  - improvement: grok: refactored the LOGLEVEL pattern (Lorenzo González)
-  - improvement: grok: fix example doc (LOGSTASH-2093, Jason Kendall)
-  - improvement: metrics: document .pXX metric (Juarez Bochi)
-
-  # outputs
-  - improvement: rabbitmq: upgraded Bunny gem to 1.1.8, fixes a threading leak and improves
-    latency (Michael Klishin)
-  - improvement: elasticsearch: start embedded server before creating a client to fix discovery
-    problems "waited for 30s ..." (Jordan Sissel)
-  - improvement: elasticsearch: have embedded ES use "bind_host" option for "network.host"
-    ES config (Jordan Sissel)
-
-1.4.0 (March 20, 2014)
-  # General
-  - We've included some upgrade-specific release notes with more details about
-    the tarball changes and contrib packaging here:
-    http://logstash.net/docs/1.4.0/release-notes
-  - Ships with Kibana 3.0.0
-  - Much faster field reference implementation (Colin Surprenant)
-  - Fix a bug in character encoding which would cause inputs using non-UTF-8
-    codecs to accidentally skip re-encoding the text to UTF-8. This should
-    solve a great number of UTF-8-related bugs. (Colin Surprenant)
-  - Fixes missing gem  for logstash web which was broken in 1.4.0 beta1
-    (LOGSTASH-1918, Jordan Sissel)
-  - Fix 'help' output being emitted twice when --help is invoked.
-    (LOGSTASH-1952, #1168)
-  - Logstash now supports deletes! See outputs section below.
-  - Update template to fit ES 1.0 API changes (untergeek)
-  - Lots of Makefile, gem and build improvements courtesy of untergeek, Faye
-    Salwin, mrsolo, ronnocol, electrical, et al
-  - Add `env` command so you can run arbitrary commands with the logstash
-    environment setup (jordansissel)
-  - Bug fixes (lots).  Did I mention bug fixes? (Thanks, community!)
-  - Elasticsearch 1.0 libraries are now included. See the Elasticsearch
-    release notes for details: http://www.elasticsearch.org/downloads/1-0-0/
-  - Kibana 3 milestone 5 is included as the 'web' process.
-  - An empty --pluginpath directory is now accepted (#917, Richard Pijnenburg)
-  - Piles of documentation improvements! A brand new introductory tutorial is
-    included, and many of the popular plugins have had their docs greatly
-    improved. This effort was lead by Kurt Hurtado with assists by James
-    Turnbull, Aaron Mildenstein, Brad Fritz, and others.
-  - Testing was another focus of this release. We added many more tests
-    to help us prevent regressions and verify expected behavior. Helping with
-    this effort was Richard Pijnenburg, Jordan Sissel, and others.
-  - The 'debug' setting was removed from most plugins. Prior to this,
-    most plugins advertised the availability of this setting but actually
-    did not use it (#996, Jordan Sissel).
-  - bugfix: --pluginpath now lets you load codecs. (#1077, Sergey Zhemzhitsky)
-
-  # inputs
-  - bugfix: collectd: Improve handling of 'NaN' values (#1015, Pieter Lexis)
-  - bugfix: snmptrap: Fixes exception when not specifying yamlmibdir (#950, Andres Koetsier)
-  - improvement: Add Multi-threaded workers and queues to UDP input (johnarnold + untergeek)
-  - improvement: log4j: port now defaults to 4560, the default log4j
-    SocketAppender port. (#757, davux)
-  - bugfix: rabbitmq: auto_delete and exclusive now default to 'false'.
-    The previous version's defaults caused data loss on logstash restarts.
-    Further, these settings are recommended by the RabbitMQ folks. (#864,
-    Michael Klishin)
-    This change breaks past default behavior, so just be aware. (Michael
-    Klishin)
-  - bugfix: collectd: fix some type calculation bugs (#905, Pieter Lexis)
-  - improvement: collectd: Now supports decryption and signature verification
-    (#905, Pieter Lexis)
-  - improvement: wmi: now supports remote hosts (#918, Richard Pijnenburg)
-  - bugfix: elasticsearch: Long scrollids now work correctly (#935, Jonathan
-    Van Eenwyk)
-  - bugfix: tcp: the 'host' field is correctly set now if you are using the
-    json codec and include a 'host' field in your events (#937, Jordan Sissel)
-  - bugfix: file: the 'host' field is correctly set now if you are using the
-    json codec and include a 'host' field in your events (#949, Piotr
-    Popieluch)
-  - bugfix: udp: the 'host' field is correctly set now if you are using the
-    json codec and include a 'host' field in your events (#965, Devin
-    Christensen)
-  - bugfix: syslog: fix regression (#986, Joshua Bussdieker)
-
-  # codecs
-  - improvement: netflow: You can now specify your own netflow field
-    definitions using the 'definitions' setting. See the netflow codec
-    docs for examples on how to do this. (#808, Matt Dainty)
-
-  # filters
-  - bugfix: clone: Correctly clone events with numeric field values.
-    (LOGSTASH-1225, #1158, Darren Holloway)
-  - bugfix: zeromq: Add `timeout` and `retries` settings for retrying on
-    request failures. Also adds `add_tag_on_timeout` so you can act on retry
-    failures. (logstash-contrib#23, Michael Hart)
-  - new: fingerprint: Checksum, anonymize, generate UUIDs, etc! A generalized
-    solution to replace the following filters: uuid, checksum, and anonymize.
-    (#907, Richard Pijnenburg)
-  - new: throttle: Allows you to tag or add fields to events that occur with a
-    given frequency. One use case is to have logstash email you only once if an
-    event occurs at least 3 times in 60 seconds. (#940, Mike Pilone) -
-  - improvement: translate: A new 'refresh_interval' setting lets you tell
-    logstash to periodically try reloading the 'dictionary_path' file
-    without requiring a restart. (#975, Kurt Hurtado)
-  - improvement: geoip: Now safe to use with multiple filter workers and
-    (#990, #997, LOGSTASH-1842; Avleen Vig, Jordan Sissel)
-  - improvement: metrics: Now safe to use with multiple filter workers (#993,
-    Bernd Ahlers)
-  - bugfix: date: Fix regression that caused times to be local time instead of
-    the intended timezone of UTC. (#1010, Jordan Sissel)
-  - bugfix: geoip: Fix encoding of fields created by geoip lookups
-    (LOGSTASH-1354, LOGSTASH-1372, LOGSTASH-1853, #1054, #1058; Jordan Sissel,
-    Nick Ethier)
-
-  # outputs
-  - bugfix: elasticsearch: flush any buffered events on logstash shutdown
-    (#1175)
-  - feature: riemann: Automatically map event fields to rieman event fields
-    (logstash-contrib#15, Byron Pezan)
-  - bugfix: lumberjack: fix off-by-one errors causing writes to another
-    logstash agent to block indefinitely
-  - bugfix: elasticsearch: Fix NameError Socket crash on startup
-    (LOGSTASH-1974, #1167)
-  - improvement: Added `action` awesomeness to elasticsearch output (#1105, jordansissel)
-  - improvement: Implement `protocol => http` in elasticsearch output (#1105, jordansissel)
-  - bugfix: fix broken pipe output to allow EBADF instead of EPIPE,
-    allowing pipe command to be restarted (#974, Paweł Puterla)
-  - improvement: Adding dns resolution to lumberjack output (#1048, Nathan Burns )
-  - improvement: added pre- and post-messages to the IRC output (#1111, Lance O'Connor)
-  - bugfix: pipe: fix handling of command failures (#1023, #1034, LOGSTASH-1860; ronnocol, Jordan Sissel)
-  - improvement: lumberjack: now supports codecs (#1048, LOGSTASH-1680; Nathan Burns)
-
-1.3.3 (January 17, 2014)
-  # general
-  - bugfix: Fix SSL cert load problem on plugins using aws-sdk: S3, SNS, etc.
-    (LOGSTASH-1778, LOGSTASH-1787, LOGSTASH-1784, #924; Adam Peck)
-  - bugfix: Fix library load problems for aws-sdk (LOGSTASH-1718, #923; Jordan
-    Sissel)
-  - bugfix: Fix regression introduced in 1.3.2 while trying to improve time
-    parsing performance. (LOGSTASH-1732, LOGSTASH-1738, #913; Jordan Sissel)
-  - bugfix: rabbitmq: honour the passive option when creating queues.
-    (LOGSTASH-1461, Tim Potter)
-
-  # codecs
-  - bugfix: json_lines, json: Fix bug causing invalid json to be incorrectly
-    handled with respect to encoding (#920, LOGSTASH-1595; Jordan Sissel)
-
-1.3.2 (December 23, 2013)
-  # upgrade notes
-  - Users of logstash 1.3.0 or 1.3.1 should set 'template_overwrite => true' in
-    your elasticsearch (or elasticsearch_http) outputs before upgrading to this
-    version to ensure you receive the fixed index template.
-
-  # general
-  - web: don't crash if an invalid http request was sent
-    (#878, LOGSTASH-704; Jordan Sissel)
-  - Ships with Elasticsearch 0.90.9
-  - logstash will now try to make sure the @timestamp field is of the
-    correct format.
-  - Fix a bug in 1.3.1/1.3.0's elasticsearch index template causing phrase
-    searching to not work. Added tests to ensure search behavior works as
-    expected with this template. (Aaron Mildenstein, Jordan Sissel)
-  - Update README.md to be consistent with Makefile use of JRuby 1.7.8
-  - Time parsing in things like the json codec (and other similar parts of
-    logstash) are *much* faster now. This fixes a speed regression that was
-    introduced in logstash 1.2.0.
-
-  # filters
-  - improvement: date: roughly 20% faster (Jordan Sissel)
-
-  # outputs
-  - new: csv: write csv format to files output. (Matt Gray)
-    (This output will become a codec usable with file output in the next
-     major version!)
-
-1.3.1 (December 11, 2013)
-  # general
-  - Fix path to the built-in elasticsearch index template
-
-1.3.0 (December 11, 2013)
-  # general
-  - oops: The --help flag now reports help again, instead of barfing an "I need
-    help" exception (LOGSTASH-1436, LOGSTASH-1392; Jordan Sissel)
-  - Resolved encoding errors caused by environmental configurations, such as
-    'InvalidByteSequenceError ... on US-ASCII' (LOGSTASH-1595, #842;
-    Jordan Sissel)
-  - Fix bug causing "no such file to load -- base64" (LOGSTASH-1310,
-    LOGSTASH-1519, LOGSTASH-1325, LOGSTASH-1522, #834; Jordan Sissel)
-  - Elasticsearch version 0.90.7
-  - Bug fixes galore!
-
-  ## inputs
-  - new: collectd: receive metrics from collectd's network protocol
-    (#785, Aaron Mildenstein)
-  - bugfix: gelf: handle chunked gelf message properly (#718, Thomas De Smedt)
-  - bugfix: s3: fix bug in region endpoint setting (#740, Andrea Ascari)
-  - bugfix: pipe: restart the command when it finishes (#754, Jonathan Van
-    Eenwyk)
-  - bugfix: redis: if redis fails, reconnect. (#767, LOGSTASH-1475; Jordan Sissel)
-  - feature: imap: add 'content_type' setting for multipart messages and
-    choosing the part that becomes the event message. (#784, Brad Fritz)
-  - bugfix: zeromq: don't override the 'host' field if the event already
-    has one. (Jordan Sissel)
-  - bugfix: ganglia: fix regressions; plugin should work again (LOGSTASH-1655,
-    #818; Jordan Sissel)
-  - bugfix: Fix missing library in sqs input (#775, LOGSTASH-1294; Toby
-    Collier)
-
-  ## filters
-  - new: unique: removes duplicate values from a given field in an event.
-    (#676, Adam Tucker)
-  - new: elapsed: time duration between two tagged events. (#713, Andrea Forni)
-  - new: i18n: currently supports 'transliterate' which does best-effort
-    conversion of text to "plain" letters. Like 'ó' to 'o'.  (#671,
-    Juarez Bochi)
-  - bugfix: restore filter flushing thread (LOGSTASH-1284, #689; Bernd Ahlers)
-  - new: elasticsearch: query elasticsearch and update your event based on the
-    results. (#707, Jonathan Van Eenwyk)
-  - new: sumnumbers: finds all numbers in a message and sums them (#752, Avleen
-    Vig)
-  - feature: geoip: new field 'location' is GeoJSON derived from the lon/lat
-    coordinates for use with elasticsearch, kibana, and anything else that
-    understands GeoJSON (#763, Aaron Mildenstein)
-  - new: punct: Removes all text except punctuation and stores it in another
-    field. Useful for as a means for fingerprinting events. (#813, Guixing Bai)
-  - feature: metrics: Make percentiles configurable. Also make rates (1, 5,
-    15-minute) optional. (#817, Juarez Bochi)
-
-  ## codecs
-  - new: compressed_spooler: batches events and sends/receives them in
-    compressed form. Useful over high latency links or with transports
-    with higher-than-desired transmission costs. (Avleen Vig)
-  - new: fluent: receive data serialized using the Fluent::Logger for easier
-    migration away from fluentd or for folks who simply like the logger
-    library (#759, Jordan Sissel)
-  - new: edn: encode and decode the EDN serialization format. Commonly used
-    in Clojure. For more details, see: https://github.com/edn-format/edn
-    (#778, Lee Hinman)
-  - bugfix: oldlogstashjson: Fix encoding to work correctly. (#788, #795;
-    Brad Fritz)
-  - bugfix: oldlogstashjson: Fallback to plain text on invalid JSON
-    (LOGSTASH-1534, #850; Jordan Sissel)
-
-  ## outputs
-  - feature: elasticsearch and elasticsearch_http now will apply a default
-    index mapping template (included) which has the settings recommended by
-    Elasticsearch for Logstash specifically.
-    Configuration options allow disabling this feature and providing a path
-    to your own template. (#826, #839; Aaron Mildenstein)
-  - feature: elasticsearch_http: optional 'user' and 'password' settings to
-    make use of http authentication (LOGSTASH-902, #684; Ian Neubert)
-  - new: google_bigquery: upload logs to bigquery for analysis later (Rodrigo
-    De Castro)
-  - bugfix: datadog_metrics: fix validation bug (#789, Ian Paredes)
-  - feature: elasticsearch: new 'transport' setting letting you tell logstash
-    to act as a cluster node (default, prior behavior) or as a 'transport
-    client'. With the new 'transport' mode, your firewall rules may be simpler
-    (unicast, one direction) and transport clients do not show up in your
-    cluster node list. (LOGSTASH-102, #841; Jordan Sissel)
-  - feature: elasticsearch: new 'bind_port setting for 'node' protocol which
-    lets you chose the local port to bind on (#841, Jordan Sissel)
-  - bugfix: Fix missing library in sqs input (#775, LOGSTASH-1294; Toby
-    Collier)
-
-1.2.2 (October 22, 2013)
-  # general
-  - new 'worker' setting for outputs. This helps improve throughput on
-    request-oriented outputs such as redis, rabbitmq, elasticsearch,
-    elasticsearch_http, etc. Workers run in separate threads each handling
-    events as they come in. This allows you to linearly scale up outputs across
-    cores or as blocking-io permits.
-  - grok performance is up 600%
-  - lots of bug fixes
-  - bugfixes to conditionals (#682, Matt Dainty)
-  - rabbitmq now replaces the old deprecated amqp plugins. amqp plugins are
-    removed.
-  - inputs will now do their best to handle text which is encoded differently
-    than the charset you have specified (LOGSTASH-1443, Jordan Sissel)
-
-  ## inputs
-  - bugfix: udp: respects teardown requests via SIGINT, etc (LOGSTASH-1290,
-    Jordan Sissel)
-  - bugfix: rabbitmq: disable automatic connection recovery (LOGSTASH-1350,
-    #641, #642; Michael Klishin)
-  - bugfix: twitter: works again (#640, Bernd Ahlers)
-  - compatibility: Restored the old 'format' setting behavior. It is still
-    deprecated, but was accidentally removed in 1.2.0. It will be removed
-    later, but is restored as part of our backwards-compat promise (Jordan
-    Sissel)
-  - bugfix: s3: fix LOGSTASH-1321 and LOGSTASH-1319 (Richard Pijnenburg)
-  - bugfix: log4j: fix typo (Jordan Sissel)
-  - bugfix: rabbitmq: disable automatic connection recover because logstash
-    will handle it (LOGSTASH-1350, Michael Klishin)
-  - bugfix: heroku: works again (LOGSTASH-1347, #643; Bernd Ahlers)
-  - bugfix: tcp: improve detection of closed connections to reduce lost events
-    (Jordan Sissel)
-  - bugfix: elasticsearch: now works correctly (#670, Richard Pijnenburg)
-  - improvement: elasticsearch: make size and scroll time configurable (#670,
-    Richard Pijnenburg)
-  - improvement: elasticsearch: tunable search type (#670, Richard Pijnenburg)
-  - compatibility: restore 'format' setting which was accidentally removed in
-    1.2.0. This feature is still deprecated, but it has been restored
-    temporarily as part of our backwards compatibility promise. (#706, Jordan
-    Sissel)
-  - bugfix: syslog: fix socket leakage (#704, Bernd Ahlers)
-  - improvement: all aws-related plugins: Add proxy_uri setting (#714, Malthe
-    Borch)
-  - bugfix: unix: fix variable name crash (#720, Nikolay Bryskin)
-
-  ## codecs
-  - new: graphite: parse graphite formated events (Nick Ethier)
-  - new: json_lines: parse streams that are lines of json objects (#731, Nick
-    Ethier)
-  - bugfix: multiline: time is now correctly in UTC. (Jordan Sissel)
-  - bugfix: oldlogstashjson: improved conversion of old logstash json to the
-    new schema (#654, Jordan Sissel)
-  - bugfix: oldlogstashjson: fix typo breaking encoding (#665, Tom Howe)
-  - bugfix: json: now assumes json delimited by newline character
-    (LOGSTASH-1332, #710; Nick Ethier)
-  - improvements: netflow: new target and versions settings (#686, Matt Dainty)
-
-  ## filters
-  - performance: grok: 6.3x performance improvement (#681, Jordan Sissel)
-  - bugfix: geoip: empty values (nil, empty string) are not put into the event
-    anymore. (Jordan Sissel)
-  - bugfix: geoip: allow using Maxmind's ASN database (LOGSTASH-1394, #694;
-    Bernd Ahlers)
-  - improvement: kv: target will now overwrite any existing fields, including
-    the source (Jordan Sissel).
-  - improvement: Kv: 'prefix' setting now respects sprintf (LOGSTASH-913,
-    #647; Richard Pijnenburg)
-  - checksum: sha128 was not a valid digest, removed from list
-  - feature: metrics: added clear_interval and flush_interval parameters for
-    setting flush rates and when to clear metrics (#545)
-  - new: collate: group events by time and/or count into a single event. (#609,
-    Neway Liu)
-  - feature: date: now supports a 'target' field for writing the timestamp into
-    a field other than @timestamp. (#625, Jonathan Van Eenwyk)
-  - bugfix: riemann: event tagging works again (#631, Marc Fournier)
-  - improvement: grok: IPV6 pattern (#623, Matt Dainty)
-  - improvement: metrics: add clear_interval and flush_interval settings (#545,
-    Juarez Bochi)
-  - improvement: useragent: include operating system details (#656, Philip
-    Kubat)
-  - improvement: csv: new quote_char setting (#725, Alex Markham)
-
-  ## outputs
-  - feature: all outputs have a 'worker' setting  now that allows you to
-    perform more work at the same time. This is useful for plugins like
-    elasticsearch_http, redis, etc, which can bottleneck on waiting for
-    requests to complete but would otherwise be happy processing more
-    simultaneous requests. (#708, Jordan Sissel)
-  - bugfix: elasticsearch: requests are now synchronous. This avoid overloading
-    the client and server with unlimited in-flight requests. (#688, Jordan
-    Sissel)
-  - bugfix: elasticsearch_http: fix bug when sending multibyte utf-8 events
-    (LOGSTASH-1328, #678, #679, #695; Steve Merrill, Christian Winther,
-    NickEthier, Jordan Sissel)
-  - performance: elasticsearch_http: http client library uses TCP_NODELAY now
-    which dramatically improves performance. (#696, Jordan Sissel)
-  - feature: elasticsearch_http now supports a 'replication' setting to
-    allow you to choose how you wait for the response. THe default is 'sync'
-    which waits for all replica shards to be written. If you set it to 'async'
-    then all index requests will respond once only the primary shards have been
-    written and the replica shards will be written later. This can improve
-    throughput. (#700, Nick Ethier, Jordan Sissel)
-  - bugfix: elasticsearch: the default port range is now 9300-9305; the older
-    range up to 9400 was unnecessary and could cause problems for the
-    elasticsearch cluster in some cases.
-  - improvement: aws-based outputs (e.g. cloudwatch) now support proxy uri.
-  - bugfix: rabbitmq: disable automatic connection recovery (LOGSTASH-1350)
-    (#642)
-  - bugfix: riemann: fixed tagging of riemann events (#631)
-  - bugfix: s3: fix LOGSTASH-1321 and LOGSTASH-1319 (#636, #645; Richard
-    Pijnenburg)
-  - bugfix: mongodb: Fix mongodb auth (LOGSTASH-1371, #659; bitsofinfo)
-  - bugfix: datadog: Fix time conversion (LOGSTASH-1427, #690; Bernd Ahlers)
-  - bugfix: statsd: Permit plain floating point values correctly in the
-    config. Example: sample_rate => 0.5 (LOGSTASH-1441, #705; Jordan Sissel)
-  - bugfix: syslog: Fix timestamp date formation. 'timestamp' setting is now
-    deprecated and the format of the time depends on your rfc selection.
-    (LOGSTASH-1423, #692, #739; Jordan Sissel, Bernd Ahlers)
-
-  ## patterns
-  - improvement: added IPV6 suppot to IP pattern (#623)
-
-1.2.1 (September 7, 2013)
-  # general
-  - This is primarily a bugfix/stability release based on feedback from 1.2.0
-  - web: kibana's default dashboard now works with the new logstash 1.2 schema.
-  - docs: updated the tutorials to work in logstash 1.2.x
-  - agent: Restored the --configtest flag (unintentionally removed from 1.2.0)
-  - deprecation: Using deprecated plugin settings can now advise you on a
-    corrective path to take. One example is the 'type' setting on filters and
-    outputs will now advise you to use conditionals and give an example.
-  - conditionals: The "not in" operator is now supported.
-
-  ## inputs
-  - bugfix: pipe: reopen the pipe and retry on any error. (#619, Jonathan Van
-    Eenwyk)
-  - bugfix: syslog: 'message' field no longer appears as an array.
-  - bugfix: rabbitmq: can now bind the queue to the exchange (#624, #628,
-    LOGSTASH-1300, patches by Jonathan Tron and Jonathan Van Eenwyk)
-
-  ## codecs
-  - compatibility: json: if data given is not valid as json will now be used as
-    the "message" of an event . This restores the older behavior when using
-    1.1.13's "format => json" feature on inputs. (LOGSTASH-1299)
-  - new: netflow: process netflow data (#580, patches by Nikolay Bryskin and
-    Matt Dainty)
-
-  ## filters
-  - bugfix: multiline: the multiline filter returns! It was unintentionally
-    removed from the previous (1.2.0) release.
-  - bugfix: json_encode: fix a syntax error in the code. (LOGSTASH-1296)
-  - feature: kv: now captures duplicate field names as a list, so 'foo=bar
-    foo=baz' becomes the field 'foo' with value ['bar', 'baz'] (an array).
-    (#622, patch by Matt Dainty)
-
-  ## outputs
-  - new: google_cloud_storage: archive logs to Google Cloud Storage (#572,
-    Rodrigo De Castro)
-  - bugfix: fixed bug with 'tags' and 'exclude_tags' on outputs that would
-    crash if the event had no tags. (LOGSTASH-1286)
-
-1.2.0 (September 3, 2013)
-  # general
-  - The logstash json schema has changed. (LOGSTASH-675)
-    For prior logstash users, you will be impacted one of several ways:
-    * You should check your elasticsearch templates and update them accordingly.
-    * If you want to reindex old data from elasticsearch with the new schema,
-      you should be able to do this with the elasticsearch input. Just make
-      sure you set 'codec => oldlogstashjson' in your elasticsearch input.
-  - The old logstash web ui has been replaced by Kibana 3. Kibana is a far
-    superior search and analytics interface.
-  - New feature: conditionals! You can now make "if this, then ..." decisions
-    in your filters or outputs. See the docs here:
-    http://logstash.net/docs/latest/configuration#conditionals
-  - A new syntax exists for referencing fields (LOGSTASH-1153). This replaces
-    the prior and undocumented syntax for field access (was 'foo.bar' and is
-    now '[foo][bar]'). Learn more about this here:
-    http://logstash.net/docs/latest/configuration#fieldreferences
-  - A saner hash syntax in the logstash config is now supported. It uses the
-    perl/ruby hash-rocket syntax: { "key" => "value", ... } (LOGSTASH-728)
-  - ElasticSearch version 0.90.3 is included. (#486, Gang Chen)
-  - The elasticsearch plugin now uses the bulk index api which should result
-    in lower cpu usage as well as higher performance than the previous
-    logstash version.
-  - Many deprecated features have been removed. If your config caused
-    deprecation warnings on startup in logstash v1.1.13, there is a good
-    chance that these deprecated settings are now absent.
-  - 'type' is no longer a required setting on inputs.
-  - New plugin type: codec. Used to implement decoding of events for inputs and
-    encoding of events for outputs. Codecs allow us to separate transport (like
-    tcp, redis, rabbitmq) from serialization (gzip text, json, msgpack, etc).
-  - Improved error messages that try to be helpful. If you see bad or confusing
-    error messages, it is a bug, so let us know! (Patch by Nick Ethier)
-  - The old 'plugin status' concept has been replaced by 'milestones'
-    (LOGSTASH-1137)
-  - SIGHUP should cause logstash to reopen it's logfile if you are using the
-    --log flag
-
-  ## inputs
-  - new: s3: reads files from s3 (#537, patch by Mathieu Guillaume)
-  - feature: imap: now marks emails as read (#542, Raffael Schmid)
-  - feature: imap: lets you delete read email (#591, Jonathan Van Eenwyk)
-  - feature: rabbitmq: now well-supported again (patches by Michael Klishin)
-  - bugfix: gelf: work around gelf parser errors (#476, patch by Chris McCoy)
-  - broken: the twitter input is disabled because the twitter stream v1 api is
-    no longer supported and I couldn't find a replacement library that works
-    under JRuby.
-  - new: sqlite input (#484, patch by Evan Livingston)
-  - improvement: snmptrap: new 'yamlmibdir' setting for specifying an external
-    source for MIB definitions. (#477, patch by Dick Davies)
-  - improvement: stomp: vhost support (#490, patch by Matt Dainty)
-  - new: unix: unix socket input (#496, patch by Nikolay Bryskin)
-  - new: wmi: for querying wmi (windows). (#497, patch by Philip Seidel)
-  - improvement: sqs: new id_field and md5_field settings (LOGSTASH-1118, Louis
-    Zuckerman)
-
-  ## filters
-  - feature: grok: 'singles' now defaults to true.
-  - bugfix: grep: allow repeating a field in the hash config (LOGSTASH-919)
-  - feature: specify timezone in date filter (#470, patch by Philippe Weber)
-  - feature: grok setting 'overwrite' now lets you overwrite fields instead
-    of appending to them.
-  - feature: the useragent filter now defaults to writing results to the top
-    level of the event instead of "ua"
-  - feature: grok now defaults 'singles' to true, meaning captured fields are
-    stored as single values in most cases instead of the old behavior of being
-    captured as an array of values.
-  - new: json_encoder filter (#554, patch by Ralph Meijer)
-  - new: cipher: gives you many options for encrypting fields (#493, patch by
-    saez0pub)
-  - feature: kv: new settings include_fields and exclude_fields. (patch by
-    Piavlo)
-  - feature: geoip: new 'target' setting for where to write geoip results.
-    (#491, patch by Richard Pijnenburg)
-  - feature: dns: now accepts custom nameservers to query (#495, patch by
-    Nikolay Bryskin)
-  - feature: dns: now accepts a timeout setting (#507, patch by Jay Luker)
-  - bugfix: ruby: multiple ruby filter instances now work (#501, patch by
-    Nikolay Bryskin)
-  - feature: uuid: new filter to add a uuid to each event (#531, Tomas Doran)
-  - feature: useragent: added 'prefix' setting to prefix field names created
-    by this filter. (#524, patch by Jay Luker)
-  - bugfix: mutate: strip works now (#590, Jonathan Van Eenwyk)
-  - new: extractnumbers: extract all numbers from a message (#579, patch by
-    Pablo Barrera)
-
-  ## outputs
-  - new: jira: create jira tickets from an event (#536, patch by Martin Cleaver)
-  - feature: rabbitmq: now well-supported again (patches by Michael Klishin)
-  - improvement: stomp: vhost support (Patch by Matt Dainty)
-  - feature: elasticsearch: now uses the bulk index api and supports
-    a tunable bulk flushing size.
-  - feature: elasticsearch_http: will now flush when idle instead of always
-    waiting for a full buffer. This helps in slow-sender situations such
-    as testing by hand.
-  - feature: irc: add messages_per_second tunable (LOGSTASH-962)
-  - bugfix: email: restored initial really useful documentation
-  - improvement: emails: allow @message, @source, @... in match (LOGSTASH-826,
-    LOGSTASH-823)
-  - feature: email: can now set Reply-To (#540, Tim Meighen)
-  - feature: mongodb: replica sets are supported (#389, patch by Mathias Gug)
-  - new: s3: New plugin to write to amazon S3 (#439, patch by Mattia Peterle)
-  - feature: statsd: now supports 'set' metrics (#513, patch by David Warden)
-  - feature: sqs: now supports batching (#522, patch by AaronTheApe)
-  - feature: ganglia: add slope and group settings (#583, patch by divanikus)
-
-1.1.13 (May 28, 2013)
-  ## general
-  - fixed bug in static file serving for logstash web (LOGSTASH-1067)
-
-  ## outputs
-  - feature: irc: add messages_per_second tunable (LOGSTASH-962)
-
-1.1.12 (May 7, 2013)
-  ## filters
-  - bugfix: useragent filter now works correctly with the built-in regexes.yaml
-  - bugfix: mail output with smtp now works again
-
-1.1.11 (May 7, 2013)
-  ## general
-  - This release is primarily a bugfix release for bugs introduced by the
-    previous release.
-  - Support for Rubinius and MRI exists once again.
-
-  ## inputs
-  - bugfix: lumberjack now respects field data again (lumberjack --field foo=bar)
-  - bugfix: rabbitmq was broken by the previous release (LOGSTASH-1003,
-    LOGSTASH-1038; Patch by Jason Koppe)
-  - bugfix: relp: allow multiple client socket connections to RELP input
-    (LOGSTASH-707, LOGSTASH-736, LOGSTASH-921)
-
-  ## filters
-  - bugfix: geoip was broken by the previous release (LOGSTASH-1013)
-  - feature: sleep now accepts an 'every' setting which causes it to
-    sleep every N events. Example; sleep every 10 events: every => 10.
-  - feature: grok now permits dashes and dots in captures, such as
-    %{WORD:foo-bar}.
-  - bugfix: useragent filter now ships with a default regexes.yaml file
-    that is used by default unless you otherwise specify (LOGSTASH-1051)
-  - bugfix: add_field now correctly sets top-level fields like @message
-  - bugfix: mutate 'replace' now sets a field regardless of whether or not
-    it exists.
-  - feature: new mutate 'update' setting to change a field's value but
-    only if that field exists already.
-
-  ## outputs
-  - feature: irc output now supports 'secure' setting to use ssl (LOGSTASH-139)
-  - feature: nagios_nsca has new setting 'message_format'
-  - bugfix: fix graphite plugin broken in 1.1.10 (LOGSTASH-968)
-  - bugfix: elasticsearch_http was broken in 1.1.10 (LOGSTASH-1004)
-  - bugfix: rabbitmq was broken by the previous release (LOGSTASH-1003,
-    LOGSTASH-1038; Patch by Jason Koppe)
-  - feature: hipchat 'notify' setting now called 'trigger_notify' (#467, patch
-    by Richard Pijnenburg)
-
-1.1.10 (April 16, 2013)
-  ## general
-  - On linux, all threads will set their process names so you can identify
-    threads in tools like top(1).
-  - Java 5 is no longer supported (You must use Java 6 or newer).
-  - Windows line terminators (CRLF) are now accepted in config files.
-  - All AWS-related plugins now have the same configuration options:
-    region, access_key_id, secret_access_key, use_ssl, and
-    aws_credentials_file. Affected plugins: cloudwatch output,
-    sns output, sqs output, sqs input. (LOGSTASH-805)
-  - Lots of documentation fixes (James Turnbull, et al)
-  - The amqp plugins are now named 'rabbitmq' because it *only* works
-    with rabbitmq. The old 'amqp' name should still work, but it will
-    be removed soon while 'rabbitmq' will stay. (Patches by Michael Zaccari)
-  - New flag '--configtest' to test config and exit. (Patch by Darren Patterson)
-  - Improved error feedback logstash gives to you as a user.
-
-  ## inputs
-  - new: elasticsearch: this input allows you to stream search results from
-    elasticsearch; it uses the Scroll API.
-  - new: websocket. Currently supports acting as a websocket client.
-  - new: snmptrap, to receive SNMP traps (patch by Paul Czar)
-  - new: varnishlog input to read from the Varnish Cache server's shared memory
-    log (LOGSTASH-978, #422; Louis Zuckerman)
-  - new: graphite input. Supports the plain text carbon tcp protocol.
-  - new: imap input. Read mail!
-  - feature: twitter: supports http proxying now (#276, patch by Richard
-    Pijnenburg)
-  - feature: loggly: supports http proxying now (#276, patch by Richard
-    Pijnenburg)
-  - feature: tcp: ssl now supported! (#318, patch by Matthew Richardson)
-  - feature: redis: now supports 'batch_count' option for doing bulk fetches
-    from redis lists. Requires Redis 2.6.0 or higher. (#320, patch by Piavlo)
-  - feature: irc: will use ssl if you set 'secure' (#393, patch by Tomas Doran)
-  - bugfix: log4j: respect add_fields (LOGSTASH-904, #358)
-  - bugfix: eventlog: input should now work
-  - bugfix: irc: passwords now work (#412, Nick Ethier)
-
-  ## filters
-  - new: useragent: parses user agent strings in to structured data based on
-    BrowserScope data (#347, patch by Dan Everton)
-  - new: sleep: sleeps a given amount of time before passing the event.
-    Useful for rate limiting or replay simulation.
-  - new: ruby: experimental ruby plugin that lets you call custom ruby code
-    on every event.
-  - new: translate: for mapping values (#335, patch by Paul Czar)
-  - new: clone: creates a copy of the event.
-  - feature: grok: Adds tag_on_failure setting so you can prevent grok from
-    tagging events on failure. (#328, patch by Neil Prosser)
-  - deprecated: grok: deprecated the --grok-patterns-path flag (LOGSTASH-803)
-  - feature: date: nested field access is allowed now
-  - feature: csv, xml, kv, json, geoip: new common settings!
-    (LOGSTASH-756, #310, #311, #312, #383, #396; patches by Richard Pijnenburg)
-      source - what field the text comes from
-      target - where to store the parse result.
-  - feature: csv: new setting: columns - labels for each column parsed.
-  - bugfix: geoip: The built-in geoip database should work now (#326, patch
-    by Vincent Batts)
-  - bugfix: kv filter now respects add_tag, etc (LOGSTASH-935)
-
-  ## outputs
-  - new: hipchat output (#428, Cameron Stokes)
-  - bugfix: mongo would fail to load bson_java support (LOGSTASH-849)
-  - bugfix: tags support to gelf output. Returns tags as _tags field
-    (LOGSTASH-880, patch by James Turnbull)
-  - bugfix: elasticsearch: Fix a race condition. (#340, patch by Raymond Feng)
-  - improvement: http: now supports a custom 'message' format for building your
-    own http bodies from an event. (#319, patch by Christian S)
-  - bugfix: Fix opentsdb output (LOGSTASH-689, #317; patch by Emmet Murphy)
-  - improvement: http output now supports a custom message format with
-    the 'message' setting (Patch by Christian Schröder)
-  - graphite output now lets you ship the whole (or part) of an event's fields
-    to graphite as metric updates. (#350, patch by Piavlo)
-  - email output now correctly defaults to not using authentication
-    (LOGSTASH-559, #365; patch by Stian Mathiassen)
-  - bugfix: file output now works correctly on fifos
-  - bugfix: irc passwords now work (#412, Nick Ethier)
-  - improvement: redis output now supports congestion detection. If
-    it appears nothing is consuming from redis, the output will stall
-    until that problem is resolved. This helps prevent a dead reader
-    from letting redis fill up memory. (Piavlo)
-  - feature: boundary: New 'auto' setting. (#413, Alden Jole)
-
-1.1.9 (January 10, 2013)
-  ## inputs
-  - bugfix: all inputs: fix bug where some @source values were not valid urls
-
-  ## filters
-  - bugfix: mutate: skip missing fields in 'convert' (#244, patch by Ralph Meijer)
-
-  ## outputs
-  - improvement: gelf: new tunable 'ignore_metadata' flag to set which fields
-    to ignore if ship_metadata is set. (#244, patch by Ralph Meijer)
-  - improvement: gelf: make short_message's field name tunable (#244, patch by
-    Ralph Meijer)
-
-1.1.8 (January 10, 2013)
-  ## general
-  - patched another work around for JRUBY-6970 (LOGSTASH-801)
-
-  ## inputs
-  - bugfix: tcp: 'Address in use' errors now report the host/port involved.
-    (LOGSTASH-831)
-  - bugfix: zeromq: fix bug where an invalid url could be given as a source
-    (LOGSTASH-821, #306)
-
-  ## outputs
-  - bugfix: elasticsearch_river: it now resolves evaluates %{} variables in
-    index and index_type settings. (LOGSTASH-819)
-
-1.1.7 (January 3, 2013)
- ## inputs
- - fix bug where @source_host was set to 'false' in many cases.
-
- ## outputs
- - improvement: redis: shuffle_hosts is now enabled by default
-
-1.1.6 (January 2, 2013)
- ## Overview of this release:
- - new inputs: drupal_dblog.
- - new filters: anonymize, metrics.
- - new outputs: syslog, cloudwatch.
- - new 'charset' setting for all inputs. This should resolve all known encoding
-   problems. The default charset is UTF-8.
- - grok now captures (?<somename>...) regexp into 'somename' field
- - Elasticsearch 0.20.2 is included. This means you are required to upgrade
-   your elasticsearch cluster to 0.20.2. If you wish to continue using an old
-   version of elasticsearch, you should use the elasticsearch_http plugin
-   instead of the elasticsearch one.
-
- ## general
- - fixed internal dependency versioning on 'addressable' gem (LOGSTASH-694)
- - fixed another case of 'watchdog timeout' (LOGSTASH-701)
- - plugin flags are now deprecated. The grok filter (--grok-pattern-path) was
-   the only plugin to make use of this.
- - the grok filter has improved documentation
- - lots of documentation fixes (James Turnbull, Louis Zuckerman)
- - lots of testing improvements (Philippe Weber, Laust Rud Jacobsen)
- - all 'name' settings have been deprecated in favor of more descriptive
-   settings (LOGSTASH-755)
- - JRuby upgraded to 1.7.1
- - removed use of bundler
- - Fixed timestamp parsing in MRI (patch by Rene Lengwinat)
-
- ## inputs
- - All inputs now have a 'charset' setting to help you inform logstash of the
-   text encoding of the input. This is useful if you have Shift_JIS or CP1251
-   encoded log files. This should help resolve the many UTF-8 bugs that were
-   reported recently. The default charset is UTF-8.
- - new: drupal_dblog: read events from a DBLog-enabled Drupal. (#251, Patch by
-   theduke)
- - bugfix: zeromq: 'topology' is now a required setting
- - bugfix: lumberjack: client connection closing is now handled properly.
-   (Patch by Nick Ethier)
- - misc: lumberjack: jls-lumberjack gem updated to 0.0.7
- - bugfix: stomp: fix startup problems causing early termination (#226
- - bugfix: tcp: the 'source host' for events is now the client ip:port that
-   sent it, instead of the listen address that received it. (LOGSTASH-796)
- - improvement: tcp: the default data_timeout is now -1 (never timeout).
-   This change was made because read timeouts were causing data loss, and
-   logstash should avoid losing events by default.
- - improvement: amqp: the 'name' setting is now called 'queue' (#274)
- - improvement: eventlog: the 'name' setting is now called 'logfile' (#274)
- - bugfix: log4j: fix stacktrace reading (#253, patch by Alex Arutyunyants)
-
- ## filters
- - new: anonymize: supports many hash mechanisms (murmur3, sha1, md5, etc) as
-   well as IP address anonymization (#280, #261; patches by Richard Pijnenburg
-   and Avishai Ish-Shalom)
- - new: metrics: allows you to aggregate metrics from events and emit them
-   periodically. Think of this like 'statsd' but implemented as a logstash
-   filter instead of an external service.
- - feature: date: now accepts 'match' as a setting. Use of this is preferable
-   to the old syntax. Where you previously had 'date { somefield =>
-   "somepattern" }' you should now do: 'date { match => [ "somefield",
-   "somepattern" ] }'. (#248, LOGSTASH-734, Patch by Louis Zuckerman)
- - feature: grok: now accepts (?<foo>...) named captures. This lets you
-   compose a pattern in the grok config without needing to define it in a
-   patterns file. Example: (?<hostport>%{HOST}:%{POSINT}) to capture 'hostport'
- - improvement: grok: allow '$' in JAVACLASS pattern (#241, patch by Corry
-   Haines)
- - improvement: grok: can now match against number types. Example, if you're
-   sending a json format event with { "status": 403 } you can now grok that
-   field.  The number is represented as a string "403" before pattern matching.
- - bugfix: date: Fix a bug that would crash the pipeline if no date pattern
-   matched. (LOGSTASH-705)
- - feature: kv: Adds field_split, value_split, prefix, and container
-   settings. (#225, patch by Alex Wheeler)
- - bugfix: mutate: rename on a nonexistant field now does nothing as expected.
-   (LOGSTASH-757)
- - bugfix: grok: don't tag an event with _grokparsefailure if it's already so
-   (#248, patch by Greg Brockman)
- - feature: mutate: new settings - split, join, strip. "split" splits a field
-   into an array. "join" merges an array into a string. "strip" strips leading and
-   trailing whitespace. (Patch by Avishai Ish-Shalom)
-
- ## outputs
- - new: syslog output supporting both RFC3164 and RFC5424 (#180, patch by
-   Rui Alves)
- - new: cloudwatch output to emit metrics and other events to Amazon CloudWatch.
-   (LOGSTASH-461, patch by Louis Zuckerman)
- - feature: stdout: added 'message' setting for configuring the output message
-   format. The default is same behavior as before this feature.
- - feature: http: added 'format' option to select 'json' or form-encoded
-   request body to send with each request.
- - feature: http: added 'content_Type' option set the Content-Type header.
-   This defaults to "application/json" if the 'format' is 'json'. Will default
-   to 'application/x-www-form-urlencoded' if the 'format' is 'form'
- - bugfix: zeromq: 'topology' is now a required setting
- - feature: mongodb: new setting 'isodate' that, when true, stores the
-   @timestamp field as a mongodb date instead of a string. (#224, patch by
-   Kevin Amorin)
- - improvement: gelf: Allow full_message gelf property to be overridden (#245,
-   patch by Sébastien Masset)
- - misc: lumberjack: jls-lumberjack gem updated to 0.0.6
- - feature: nagios: New 'nagios_level' setting to let you change the level
-   of the passive check result sent to nagios. (#298, Patch by James Turnbull)
- - feature: elasticsearch, elasticsearch_http, elasticsearch_river: new setting
-   'document_id' for explicitly setting the document id in each write to
-   elasticsearch. This is useful for overwriting existing documents.
- - improvement: elasticsearch_river: 'name' is now 'queue' (#274)
- - improvement: amqp: 'name' is now 'exchange' (#274)
- - bugfix: the websocket output works again (supports RFC6455)
-
-1.1.5 (November 10, 2012)
- ## Overview of this release:
- * New inputs: zenoss, gemfire
- * New outputs: lumberjack, gemfire
- * Many UTF-8 crashing bugs were resolved
-
- ## general
- - new runner command 'rspec' - lets you run rspec tests from the jar
-   This means you should now be able to write external tests that execute your
-   logstash configs and verify functionality.
- - "file not found" errors related to paths that had "jar:" prefixes should
-   now work. (Fixes LOGSTASH-649, LOGSTASH-642, LOGSTASH-655)
- - several plugins received UTF-8-related fixes (file, lumberjack, etc)
-   File bugs if you see any UTF-8 related crashes.
- - 'json_event' format inputs will now respect 'tags' (#239, patch by
-   Tim Laszlo)
- - logstash no longer uses nor recommends bundler (see 'gembag.rb'). The
-   Gemfile will be purged in the near future.
- - amqp plugins are now marked 'unsupported' as there is no active maintainer
-   nor is there source of active support in the community. If you're interested
-   in maintainership, please email the mailling list or contact Jordan!
-
- ## inputs
- - irc: now stores irc nick
- - new: zenoss (#232, patch by Chet Luther)
- - new: gemfire (#235, patch by Andrea Campi)
- - bugfix: udp: skip close() call if we're already closed (#238, patch by kcrayon)
-
- ## filters
- - bugfix: fix for zeromq filter initializer (#237, patch by Tom Howe)
-
- ## outputs
- - new: lumberjack output (patch by Nick Ethier)
- - new: gemfire output (#234, patch by Andrea Campi)
- - improved: nagios_ncsa (patch by Tomas Doran)
- - improved: elasticsearch: permit setting 'host' even if embedded. Also set the
-   host default to 'localhost' when using embedded. These fixes should help resolve
-   issues new users have when their distros surprisingly block multicast by
-   default.
- - improved: elasticsearch: failed index attempts will be retried
- - improved: irc: new 'password' setting (#283, patch by theduke)
-
-1.1.4 (October 28, 2012)
- ## Overview of this release:
- - bug fixes mostly
-
- ## filters
- - date: Fix crashing on date filter failures. Wrote test to cover this case.
-   (LOGSTASH-641)
- - grok: Improve QUOTEDSTRING pattern to avoid some more 'watchdog timeout' problems
-
- ## outputs
- - nagios_nsca: Allow check status to be set from the event (#228, patch by
-   Tomas Doran)
- - elasticsearch_http: Fix OpenSSL::X509::StoreError (LOGSTASH-642)
-
-1.1.3 (October 22, 2012)
- - rebuilt 1.1.2 for java 5 and 6
-
-1.1.2 (October 22, 2012)
- ## Overview of this release:
-  * New input plugins: lumberjack, sqs, relp
-  * New output plugins: exec, sqs
-  * New filter plugins: kv, geoip, urldecode, alter
-  * file input supports backfill via 'start_position'
-  * filter watchdog timer set to 10 seconds (was 2 seconds)
-
- ## general
- - Stopped using 'Gemfile' for dependencies, the logstash.gemspec has returned.
-   (Patch by Grant Rogers)
- - New 'logstash-event.gemspec' for generating logstash events in your own
-   ruby programs (Patch by Garry Shutler)
- - Wildcard config files are now sorted properly (agent -f
-   /etc/logstash/*.conf)
- - The old '-vvv' setting ruby's internal $DEBUG is now gone. It was causing
-   too much confusion for users due to noise.
- - Improved 'logstash event' creation speed by 3.5x
- - Now uses JRuby 1.7.0
- - Now ships with Elasticsearch 0.19.10
-
- ## inputs
- - bugfix: redis: [LOGSTASH-526] fix bug with password passing
- - new: lumberjack: for use with the lumberjack log shipper
-   (https://github.com/jordansissel/lumberjack)
- - new: sqs: Amazon SQS input (Patch by Sean Laurent, #211)
- - new: relp: RELP (rsyslog) plugin (Patch by Mike Worth, #177)
- - file input: sincedb path is now automatically generated if not specified.
-   This helps work around a problem where two file inputs don't specify a
-   sincedb_path would clobber eachother (LOGSTASH-554)
- - file input: no longer crashes if HOME is not set in env (LOGSTASH-458)
- - log4j input: now supports MDC 'event properties' which are stored as fields
-   in the logstash event. (#216, #179. Patches by Charles Robertson and Jurjan
-   Woltman)
- - pipe input: should work now.
-
- ## filters
- - new: kv: useful for parsing log formats taht use 'foo=bar baz=fizz' and
-   similar key-value-like things.
- - new: urldecode: a filter for urldecoding fields in your event. (Patch by
-   Joey Imbasciano, LOGSTASH-612)
- - new: geoip: query a local geoip database for location information (Patch by
-   Avishai Ish-Shalom, #208)
- - improvement: zeromq: an empty reply is now considered as a 'cancel this
-   event' operation (LOGSTASH-574)
- - bugfix: mutate: fix bug in uppercase and lowercase feature that would
-   prevent it from actually doing the uppercasing/lowercasing.
- - improvement: mutate: do the 'remove' action last (LOGSTASH-543)
- - feature: grok: new 'singles' config option which, when true, stores
-   single-value fields simply as a single value rather than as an array, like
-   [value]. (LOGSTASH-185)
- - grok patterns: the URIPARAM pattern now includes pipe '|' as a valid
-   character. (Patch by Chris Mague)
- - grok patterns: improve haproxy log patterns (Patch by Kevin Nuckolls)
- - grok patterns: include 'FATAL' as a valid LOGLEVEL match
-   (patch by Corry Haines)
- - grok patterns: 'ZONE' is no longer captured by name in the HTTPDATE pattern
- - new: alter: adds some conditional field modification as well as a
-   'coalesce' feature which sets the value of a field to the first non-null
-   value given in a list. (Patch by Francesco Salbaroli)
- - improvement: date: add TAI64N support
- - improvement: date: subsecond precision on UNIX timestamps is retained on
-   conversion (#213, Patch by Ralph Meijer)
- - improvement: date: Add locale setting; useful for day/month name parsing.
-   (#100, Patch by Christian Schröder)
-
- ## outputs
- - new: exec: run arbitrary commands based on an event.
- - new: sqs: Amazon SQS output (Patch by Sean Laurent, #211)
- - bugfix: redis: [LOGSTASH-526] fix bug with password passing
- - improvement: redis: [LOGSTASH-573] retry on failure even in batch-mode. This
-   also fixes a prior bug where an exception in batch mode would cause logstash
-   to crash. (Patch by Alex Dean)
- - improvement: riemann: metric and ttl values in riemann_event now support
-   sprintf %{foo} values. (pull #174)
- - improvement: stdout: new 'dots' debug_format value emits one dot per event
-   useful for tracking event rates.
- - gelf output: correct severity level mappings (patch by Jason Koppe)
- - xmpp output: users and rooms are separate config settings now (patch by
-   Parker DeBardelaben)
- - improvement: redis: 'host' setting now accepts a list of hosts for failover
-   of writes should the current host go down. (#222, patch by Corry Haines)
-
-1.1.1 (July 14, 2012)
- ## Overview of this release:
-  * New input plugins: generator, heroku, pipe, ganglia, irc
-  * New output plugins: juggernaut, metricscatcher, nagios_ncsa, pipe,
-                        opentsdb, pagerduty, irc
-  * New filter plugins: zeromq, environment, xml, csv, syslog_pri
-  * Fixes for gelf output
-  * Support for more than 1 filter worker (agent argument "-w")
-
- ## IMPORTANT CHANGES FOR UPGRADES FROM 1.1.0
-  - zeromq input and output rewritten
-      The previous zeromq support was an MVP. It has now been rewritten into
-      something more flexible. The configuration options have changed entirely.
-      While this is still listed as `experimental`, we don't predict any more
-      configuration syntax changes. The next release will bump this to beta.
-  - unix_timestamp
-      Previously, several plugins did not work as expected on MRI due to the
-      usage of the JRuby-only Jodatime library. We now have a contributed fix
-      for a slower parser on MRI/CRuby!
-  - elasticsearch version is now 0.19.8
-      This means your elasticsearch cluster must be running 0.19.x for
-      compatibility reasons.
-  - grok pattern %{POSINT} used to match '0' -- now it does not. If you want
-    to match non-negative integers, there is now a %{NONNEGINT} pattern.
-  - bug in file input fixed that led to an extra leading slash in @source_path.
-    Previously, file input would have @source = 'file://host//var/log/foo' and
-    @source_path = '//var/log/foo'; now @source = 'file://host/var/log/foo'
-    and @source_path = '/var/log/foo'. [LOGSTASH-501]
-  - file input now rejects relative paths. [LOGSTASH-503]
-  - event sprintf can now look inside structured field data. %{foo.bar} will
-    look in the event field "foo" (if it is a hash) for "bar".  To preserve
-    compatibility, we first look for a top-level key that matches exactly
-    (so %{foo.bar} will first look for a field named "foo.bar", then look for
-    "bar" under "foo").
-
-  ## general
-  - NOTE: gemspec removed; deploying logstash as a gem hasn't been supported
-    for a while.
-  - feature: logstash sub-commands "irb" and "pry" for an interactive debug
-    console, useful to debug jruby when running from the monolithic jar
-  - misc: newer cabin gem for logging
-  - misc: initial support for reporting internal metrics (currently outputs
-    to INFO log; eventually will be an internal event type)
-  - misc: added a "thread watchdog" to detect hanging filter workers, and
-    crash logstash w/an informational message
-  - misc: jar is built with jruby 1.6.7.2
-  - misc: better shutdown behavior when there are no inputs/plugins running
-  - feature: logstash web now uses relative URLs; useful if you want to
-    reverseproxy with a path other than "/"
-
-  ## inputs
-  - bugfix: stdin: exit plugin gracefully on EOF
-  - feature: [LOGSTASH-410] - inputs can now be duplicated with the
-    'threads' parameter (where supported)
-  - bugfix: [LOGSTASH-490] - include cacert.pem in jar for twitter input
-  - feature: [LOGSTASH-139] - support for IRC
-
-  ## filters
-  - feature: all filters support 'remove_tag' (remove tags on success)
-  - feature: all filters support 'exclude_tags' (inverse of 'tags')
-  - bugfix: [LOGSTASH-300] - bump grok pattern replace limit to 1000,
-    fixes "deep recursion pattern compilation" problems
-  - bugfix: [LOGSTASH-375] - fix bug in grep: don't drop when field is nil
-    and negate is true
-  - bugfix: [LOGSTASH-386] - fix some grok patterns for haproxy
-  - bugfix: [LOGSTASH-446] - fix grok %{QUOTEDSTRING} pattern, should fix
-    some grok filter hangs
-  - bugfix: some enhancements to grok pattern %{COMBINEDAPACHELOG}
-  - bugfix: grok: %{URIPATH} and %{URIPARAM} enhancements
-  - feature: grok: add %{UUID} pattern
-  - bugfix: grok: better error message when expanding unknown %{pattern}
-  - feature: mutate: now supports a 'gsub' operation for applying a regexp
-    substitution on event fields
-
-  ## outputs
-  - bugfix: [LOGSTASH-351] - fix file input on windows
-  - feature: [LOGSTASH-356] - make file output flush intervals configurable
-  - feature: [LOGSTASH-392] - add 'field' attribute to restrict which fields
-    get sent to an output
-  - feature: [LOGSTASH-374] - add gzip support to file output
-  - bugfix: elastic search river now respects exchange_type and queue_name
-  - bugfix: ganglia plugin now respects metric_type
-  - bugfix: GELF output facility fixes; now defaults to 'logstash-gelf'
-  - feature: [LOGSTASH-139] - support for IRC
-  - bugfix: es_river: check river status after creation to verify status
-  - feature: es: allow setting node_name
-  - feature: redis: output batching for list mode
-
-1.1.0.1 (January 30, 2012)
-  ## Overview of this release:
-    * date filter bugfix: [LOGSTASH-438] - update joda-time to properly
-      handle leap days
-
-1.1.0 (January 30, 2012)
-  ## Overview of this release:
-    * New input plugins: zeromq, gelf
-    * New filter plugins: mutate, dns, json
-    * New output plugins: zeromq, file
-    * The logstash agent now runs also in MRI 1.9.2 and above
-
-    This is a large release due to the longevity of the 1.1.0 betas.
-    We don't like long releases and will try to avoid this in the future.
-
-  ## IMPORTANT CHANGES FOR UPGRADES FROM 1.0.x
-    - grok filter: named_captures_only now defaults to true
-        This means simple patterns %{NUMBER} without any other name will
-        now not be included in the field set. You can revert to the old
-        behavior by setting 'named_captures_only => false' in your grok
-        filter config.
-    - grok filter: now uses Ruby's regular expression engine
-        The previous engine was PCRE. It is now Oniguruma (Ruby). Their
-        syntaxes are quite similar, but it is something to be aware of.
-    - elasticsearch library upgraded to 0.18.7
-        This means you will need to upgrade your elasticsearch servers,
-        if any, to the this version: 0.18.7
-    - AMQP parameters and usage have changed for the better. You might
-      find that your old (1.0.x) AMQP logstash configs do not work.
-      If so, please consult the documentation for that plugin to find
-      the new names of the parameters.
-
-  ## general
-  - feature: [LOGSTASH-158] - MRI-1.9 compatible (except for some
-    plugins/functions which will throw a compatibility exception) This means
-    you can use most of the logstash agent under standard ruby.
-  - feature: [LOGSTASH-118] - logstash version output (--version or -V for
-    agent)
-  - feature: all plugins now have a 'plugin status' indicating the expectation
-    of stability, successful deployment, and rate of code change. If you
-    use an unstable plugin, you will now see a warning message on startup.
-  - bugfix: AMQP overhaul (input & output), please see docs for updated
-    config parameters.
-  - bugfix: [LOGSTASH-162,177,196] make sure plugin-contained global actions
-    happen serially across all plugins (with a mutex)
-  - bugfix: [LOGSTASH-286] - logstash agent should not truncate logfile on
-    startup
-  - misc: [LOGSTASH-160] - now use gnu make instead of rake.
-  - misc: now using cabin library for all internal logging
-  - test: use minitest
-  - upgrade: now using jruby in 1.9 mode
-
-  ## inputs
-  - feature: zeromq input. Requires you have libzmq installed on your system.
-  - feature, bugfix: [LOGSTASH-40,65,234,296]: much smarter file watching for
-    file inputs. now supports globs, keeps state between runs, can handle
-    truncate, log rotation, etc. no more inotify is required, either (file
-    input now works on all platforms)
-  - feature: [LOGSTASH-172,201] - syslog input accepts ISO8601 timestamps
-  - feature: [LOGSTASH-159] - TCP input lets you configure what identifies
-    an input stream to the multiline filter (unique per host, or connection)
-  - feature: [LOGSTASH-168] - add new GELF input plugin
-  - bugfix: [LOGSTASH-8,233] - fix stomp input
-  - bugfix: [LOGSTASH-136,142] - file input should behave better with log rotations
-  - bugfix: [LOGSTASH-249] - Input syslog force facility type to be an integer
-  - bugfix: [LOGSTASH-317] - fix file input not to crash when a file
-    is unreadable
-
-  ## filters
-  - feature: [LOGSTASH-66,150]: libgrok re-written in pure ruby (no more
-    FFI / external libgrok.so dependency!)
-  - feature: [LOGSTASH-292,316] - Filters should run on all events if no condition
-    is applied (type, etc).
-  - feature: [LOGSTASH-292,316] - Filters can now act on specific tags (or sets
-    of tags).
-  - bugfix: [LOGSTASH-285] - for grok, add 'keep_empty_captures' setting to
-    allow dropping of empty captures. This is true by default.
-  - feature: [LOGSTASH-219] - support parsing unix epoch times
-  - feature: [LOGSTASH-207] - new filter to parse a field as json merging it
-    into the event.
-  - feature: [LOGSTASH-267,254] - add DNS filter for doing forward or
-    reverse DNS on an event field
-  - feature: [LOGSTASH-57] - add mutate filter to help with manipulating
-    event field content and type
-
-  ## outputs
-  - feature: zeromq output. Requires you have libzmq installed on your system.
-  - feature: new file output plugin
-  - bugfix: [LOGSTASH-307] embedded elasticsearch now acts as a full ES server;
-    previously embedded was only accessible from within the logstash process.
-  - bugfix: [LOGSTASH-302] - logstash's log level (-v, -vv flags) now control
-    the log output from the elasticsearch client via log4j.
-  - bugfix: many gelf output enhancements and bugfixes
-  - feature: [LOGSTASH-281] - add https support to loggly output
-  - bugfix: [LOGSTASH-167] - limit number of in-flight requests to the
-    elasticsearch node to avoid creating too many threads (one thread per
-    pending write request)
-  - bugfix: [LOGSTASH-181] - output/statsd: set sender properly
-  - bugfix: [LOGSTASH-173] - GELF output can throw an exception during gelf notify
-  - bugfix: [LOGSTASH-182] - grep filter should act on all events if no type is
-    specified.
-  - bugfix: [LOGSTASH-309] - file output can now write to named pipes (fifo)
-
-
-1.0.17 (Aug 12, 2011)
-  - Bugs fixed
-    - [LOGSTASH-147] - grok filter incorrectly adding fields when a match failed
-    - [LOGSTASH-151] - Fix bug in routing keys on AMQP
-    - [LOGSTASH-156] - amqp issue with 1.0.16?
-
-  - Improvement
-    - [LOGSTASH-148] - AMQP input should allow queue name to be specified separately from exchange name
-    - [LOGSTASH-157] - Plugin doc generator should make regexp config names more readable
-
-  - New Feature
-    - [LOGSTASH-153] - syslog input: make timestamp an optional field
-    - [LOGSTASH-154] - Make error reporting show up in the web UI
-
-1.0.16 (Aug 18, 2011)
-  - Fix elasticsearch client problem with 1.0.15 - jruby-elasticsearch gem
-    version required is now 0.0.10 (to work with elasticsearch 0.17.6)
-
-1.0.15 (Aug 18, 2011)
-  - IMPORTANT: Upgraded to ElasticSearch 0.17.6 - this brings a number of bug
-    fixes including an OOM error caused during high index rates in some
-    conditions.
-    NOTE: You *must* use same main version of elasticsearch as logstash does,
-    so if you are still using elasticsearch server 0.16.x - you need to upgrade
-    your server before the elasticsearch output will work. If you are using
-    the 'embedded' elasticsearch feature of logstash, you do not need to make
-    any changes.
-  - feature: tcp input and output plugins can now operate in either client
-    (connect) or server (listen) modes.
-  - feature: new output plugin "statsd" which lets you increment or record
-    timings from your logs to a statsd agent
-  - feature: new redis 'pattern_channel' input support for PSUBSCRIBE
-  - feature: new output plugin "graphite" for taking metrics from events and
-    shipping them off to your graphite/carbon server.
-  - feature: new output plugin "ganglia" for shipping metrics to ganglia
-    gmond server.
-  - feature: new output plugin "xmpp" for shipping events over jabber/xmpp
-  - feature: new input plugin "xmpp" for receiving events over jabber/xmpp
-  - feature: amqp input now supports routing keys.
-    https://logstash.jira.com/browse/LOGSTASH-122
-  - feature: amqp output now supports setting routing key dynamically.
-    https://logstash.jira.com/browse/LOGSTASH-122
-  - feature: amqp input/output both now support SSL.
-    https://logstash.jira.com/browse/LOGSTASH-131
-  - feature: new input plugin "exec" for taking events from executed commands
-    like shell scripts or other tools.
-  - feature: new filter plugin "split" for splitting one event into multiple.
-    It was written primarily for the new "exec" input to allow you to split
-    the output of a single command run by line into multiple events.
-  - misc: upgraded jar releases to use JRuby 1.6.3
-  - bugfix: syslog input shouldn't crash anymore on weird network behaviors
-    like portscanning, etc.
-    https://logstash.jira.com/browse/LOGSTASH-130
-
-1.0.14 (Jul 1, 2011)
-  - feature: new output plugin "loggly" which lets you ship logs to loggly.com
-  - feature: new output plugin "zabbix" - similar to the nagios output, but
-    works with the Zabbix monitoring system. Contributed by Johan at
-    Mach Technology.
-  - feature: New agent '-e' flag which lets you specify a config in a string.
-    If you specify no 'input' plugins, default is stdin { type => stdin }
-    If you specify no 'output' plugins, default is stdout { debug => true }
-    This is intended to be used for hacking with or debugging filters, but
-    you can specify an entire config here if you choose.
-  - feature: Agent '-f' flag now supports directories and globs. If you specify
-    a directory, all files in that directory will be loaded as a single config.
-    If you specify a glob, all files matching that glob will be loaded as a
-    single config.
-  - feature: gelf output now allows you to override the 'sender'. This defaults
-    to the source host originating the event, but can be set to anything now.
-    It supports dynamic values, so you can use fields from your event as the
-    sender. Contributed by John Vincent
-    Issue: https://github.com/logstash/logstash/pull/30
-  - feature: added new feature to libgrok that allows you to define patterns
-    in-line, like "%{FOO=\d+}" defines 'FOO' match \d+ and captures as such.
-    To use this new feature, you must upgrade libgrok to at least 1.20110630
-    Issue: https://logstash.jira.com/browse/LOGSTASH-94
-  - feature: grok filter now supports 'break_on_match' defaulting to true
-    (this was the original behavior). If you set it to false, it will attempt
-    to match all patterns and create new fields as normal. If left default
-    (true), it will break after the first successful match.
-  - feature: grok filter now supports parsing any field. You can do either of
-    these: grok { match => [ "fieldname", "pattern" ] }
-    or this: grok { fieldname => "pattern" }
-    The older 'pattern' attribute still means the same thing, and is equivalent
-    to this: grok { match => [ "@message", "pattern" ] }
-    Issue: https://logstash.jira.com/browse/LOGSTASH-101
-  - feature: elasticsearch - when embedded is true, you can now set the
-    'embedded_http_port' to configure which port the embedded elasticsearch
-    server listens on. This is only valid for the embedded elasticsearch
-    configuration. https://logstash.jira.com/browse/LOGSTASH-117
-  - bugfix: amqp input now reconnects properly when the amqp broker restarts.
-  - bugfix: Fix bug in gelf output when a fields were not arrays but numbers.
-    Issue: https://logstash.jira.com/browse/LOGSTASH-113
-  - bugfix: Fix a bug in syslog udp input due to misfeatures in Ruby's URI
-    class. https://logstash.jira.com/browse/LOGSTASH-115
-  - misc: jquery and jquery ui now ship with logstash; previously they were
-    loaded externally
-  - testing: fixed some bugs in the elasticsearch test itself, all green now.
-  - testing: fixed logstash-test to now run properly
-
-1.0.12 (Jun 9, 2011)
-  - misc: clean up some excess debugging output
-  - feature: for tcp input, allow 'data_timeout => -1' to mean "never time out"
-
-1.0.11 (Jun 9, 2011)
-  - deprecated: The redis 'name' and 'queue' options for both input and output
-    are now deprecated. They will be removed in a future version.
-  - feature: The redis input and output now supports both lists and channels.
-  - feature: Refactor runner to allow you to run multiple things in a single
-    process.  You can end each instance with '--' flag. For example, to run one
-    agent and one web instance:
-      % java -jar logstash-blah.jar agent -f myconfig -- web
-  - feature: Add 'embedded' option to the elasticsearch output:
-      elasticsearch { embedded => true }
-    Default is false. If true, logstash will run an elasticsearch server
-    in the same process as logstash. This is really useful if you are just
-    starting out or only need one one elasticsearch server.
-  - feature: Added a logstash web backend feature for elasticsearch that tells
-    logstash to use the 'local' (in process) elasticsearch:
-      --backend elasticsearch:///?local
-  - feature: Added 'named_captures_only' option to grok filter. This will have
-    logstash only keep the captures you give names to - for example %{NUMBER}
-    won't be kept, but %{NUMBER:bytes} will be.
-  - feature: Add 'bind_host' option to elasticsearch output. This lets you choose the
-    address ElasticSearch client uses to bind to - useful if you have a
-    multihomed server.
-  - feature: The mongodb output now supports authentication
-  - bugfix: Fix bug in GELF output that caused the gelf short_message to be set as an
-    array if it came from a grok value. The short_message field should only
-    now be a string properly.
-  - bugfix: Fix bug in grep filter that would drop/cancel events if you had
-    more than one event type flowing through filters and didn't have a grep
-    filter defined for each type.
-  - misc: Updated gem dependencies (tests still pass)
-  - misc: With the above two points, you can now run a single logstash process
-    that includes elasticsearch server, logstash agent, and logstash web.
-
-1.0.10 (May 23, 2011)
-  - Fix tcp input bug (LOGSTASH-88) that would drop connections.
-  - Grok patterns_dir (filter config) and --grok-patterns-dir (cmdline opt)
-    are now working.
-  - GELF output now properly sends extra fields from the log event (prefixed
-    with a "_") and sets timestamp to seconds-since-epoch (millisecond
-    precision and time zone information is lost, but this is the format GELF
-    asks for).
-  - Inputs support specifying the format of input data (see "format" and
-    "message_format" input config parameters).
-  - Grok filter no longer incorrectly tags _grokparsefailure when more than
-    one grok filter is enabled (for multiple types) or when an event has
-    no grok configuration for it's type.
-  - Fix bug where an invalid HTTP Referer: would break grok parsing of the
-    log line (used to expect %{URI}). Since Referer: is not sanitized in
-    the HTTP layer, we cannot assume it will be a well formed %{URI}.
-
-1.0.9 (May 18, 2011)
-  - Fix crash bug caused by refactoring that left 'break' calls in code
-    that no longer used loops.
-
-1.0.8 (May 17, 2011)
-  - Remove beanstalk support because the library (beanstalk-client) is GPL3. I
-    am not a lawyer, but I'm not waiting around to have someone complain about
-    license incompatibilities.
-  - fix bug in jar build
-
-1.0.7 (May 16, 2011)
-  - logstash 'web' now allows you to specify the elasticsearch clustername;
-    --backend elasticsearch://[host[:port]]/[clustername]
-  - GELF output now supports dynamic strings for level and facility
-    https://logstash.jira.com/browse/LOGSTASH-83
-  - 'amqp' output supports persistent messages over AMQP, now. Tunable.
-    https://logstash.jira.com/browse/LOGSTASH-81
-  - Redis input and output are now supported. (Contributed by dokipen)
-  - Add shutdown processing. Shutdown starts when all inputs finish (like
-    stdin) The sequence progresses using the same pipeline as the
-    inputs/filters/outputs, so all in-flight events should finish getting
-    processed before the final shutdown event makes it's way to the outputs.
-  - Add retries to unhandled input exceptions (LOGSTASH-84)
-
-1.0.6 (May 11, 2011)
-  * Remove 'sigar' from monolithic jar packaging. This removes a boatload of
-    unnecessary warning messages on startup whenever you use elasticsearch
-    output or logstash-web.
-    Issue: https://logstash.jira.com/browse/LOGSTASH-79
-
-1.0.5 (May 10, 2011)
-  * fix queues when durable is set to true
-
-1.0.4 (May 9, 2011)
-  * Fix bugs in syslog input
-
-1.0.2 (May 8, 2011)
-  * Fix default-value handling for configs when the validation type is
-    'password'
-
-1.0.1 (May 7, 2011)
-  * Fix password auth for amqp and stomp (Reported by Luke Macken)
-  * Fix default elasticsearch target for logstash-web (Reported by Donald Gordon)
-
-1.0.0 (May 6, 2011)
-  * First major release.
+## Note: this changelog has been moved to markdown format and is available at https://github.com/elastic/logstash/blob/master/CHANGELOG.md
\ No newline at end of file
diff --git a/CHANGELOG.md b/CHANGELOG.md
new file mode 100644
index 00000000000..e2fe3af0806
--- /dev/null
+++ b/CHANGELOG.md
@@ -0,0 +1,1618 @@
+## 1.5.2 (July 1, 2015)
+### general
+  - Plugin manager: Added validation and warning when updating plugins between major versions (#3383).
+  - Performance improvements: String interpolation is widely used in LS to create keys combining dynamic
+    values from extracted fields. Added a caching mechanism where we compile this template on first use
+    and reuse them subsequently, giving us a good performance gain in configs that do a lot of date 
+    processing, sprintf, and use field reference syntax (#3425).
+  - Added warning when LS is running on a JVM version which has known issues/bugs (#2547).  
+  - Updated AWS based plugins to v2 of AWS ruby SDK. This involves an update to s3-input, s3-output,
+    sqs-input, sns-output.
+
+### input
+  - Lumberjack: This input was not handling backpressure properly from downstream plugins and
+    would continue to accept data, eventually running out of memory. We added a circuit breaker to stop
+    accepting new connections when we detect this situation. Please note that `max_clients` setting 
+    intoduced in v0.1.9 has been deprecated. This setting temporarily solved the problem by configuring
+    an upper limit to the number of LSF connections (#12).
+  - Http: Added new input to receive data via http(s).
+  - File: Fixed a critical bug where new files being added to a dir being watched would crash LS.
+    This issue also happens when using a wildcard to watch files matching a pattern (#3473).
+
+### output
+  - SNS: Provided support to use codecs for formatting output (#3).
+  - Elasticsearch: Added path setting for `http` protocol. When ES is running behind a proxy, you can use
+    the path option to specify the exact location of the ES end point (#168).
+
+## 1.5.1 (June 16, 2015)
+### general
+  - Fixed an issue which caused Logstash to hang when used with single worker (`-w 1`) configuration. 
+    This issue was caused by a deadlock in the internal queue when the filter worker was trying to
+    exclusively remove elements which conflicted with the periodic flushing in filters (#3361).
+  - Fixed performance regression when using field reference syntax in config like `[tweet][username]`. 
+    This fix increases throughput in certain configs by 30% (#3238)
+  - Windows: Added support to launch Logstash from a path with spaces (#2904)
+  - Update to jruby-1.7.20 which brings in numerous fixes. This will also make file input work
+    properly on FreeBSD.
+  - Fixed regression in 1.5.0 where conditionals spread over multiple lines in a config was not
+    working properly (#2850)
+  - Fixed a permission issue in rpm and debian repos. When Logstash was installed using these 
+    repos, only the logstash user was able to run commands like `bin/logstash version` (#3249)
+
+### filter
+  - GeoIP: Logstash no longer crashes when IPv6 addresses are used in lookup (#8)
+
+### output
+  - Elasticsearch: 
+    - Added an option to disable SSL certificate verification (#160)
+    - Bulk requests were timing out because of aggressive timeout setting in the HTTP client.
+      Restored this to 1.4.2 behavior where there are no timeouts by default. As a follow up
+      to this, we will be exposing an option to control timeouts in the HTTP client (#103)
+  - JIRA: 
+    - Newly created issues now have description set (#3)
+    - Summary field now expands variables in events
+    - API authentication method changed from cookie to basic
+
+## 1.4.3 (June 2, 2015)
+### general
+  - Updated to Elasticsearch 1.5.2, Kibana 3.1.2 and JRuby 1.7.17
+
+### output
+  - File: Sandbox output to protect against issues like creating new files
+    outside defined paths
+
+## 1.5.0 (May 14, 2015)
+### general
+  - Performance improvements: Logstash 1.5.0 is much faster -- we have improved the throughput 
+    of grok filter in some cases by 100%. In our benchmark testing, using only grok filter and
+    ingesting apache logs, throughput increased from 34K eps to 50K eps. 
+    JSON serialization/deserialization are now implemented using JrJackson library which 
+    improved performance significantly. Ingesting JSON events 1.3KB in size measured a throughput
+    increase from 16Keps to 30K eps. With events 45KB in size, throughput increased from 
+    850 eps to 3.5K eps
+  - Fixed performance regressions from 1.4.2 especially for configurations which have 
+    conditionals in filter and output. Throughput numbers are either inline with 1.4.2
+    or improved for certain configurations (#2870)  
+  - Add Plugin manager functionality to Logstash which allows to install, delete and 
+    update Logstash plugins. Plugins are separated from core and published to RubyGems.org
+  - Added the ability to install plugin gems built locally on top of Logstash. This will 
+    help plugin developers iterate and test locally without having to publish plugins (#2779)  
+  - With the release of Kibana 4, we have removed the `bin/logstash web` command and any reference 
+    to Kibana from Logstash (#2661)
+  - Windows: Significantly improved the initial user experience with Windows platform (#2504, #1426). 
+    Fixed many issues related to File input. Added support for using the plugin 
+    framework (installing, upgrading, removing)  
+  - Deprecated elasticsearch_http output plugin: All functionality is ported to
+    logstash-output-elasticsearch plugin using http protocol (#1757). If you try to use
+    the elasticsearch_http plugin, it will log a deprecated notice now.   
+  - Fixed issue in core which was causing Logstash to not shutdown properly (#2796)    
+  - Added ability to add extra JVM options while running LS. You can use the LS_JAVA_OPTS 
+    environment variable to add to the default JVM options set out of the box. You could also
+    completely overwrite all the default options if you wish by setting JAVA_OPTS before
+    starting Logstash (#2942)
+  - Fixed a regression from 1.4.2 where removing a tag in filter fails if the input event is
+    JSON formatted (#2261)
+  - Fixed issue where setting workers > 1 would trigger messages like
+    "You are using a deprecated config setting ..." (#2865) 
+  - Remove ability to run multiple subcommands from bin/logstash like 
+    bin/logstash agent -f something.conf -- web (#1747)  
+  - Fixed Logstash crashing on converting from ASCII to UTF-8. This was caused by charset
+    conversion issues in input codec (LOGSTASH-1789)
+  - Allow storing 'metadata' to an event which is not sent/encoded on output. This eliminates
+    the need for intermediate fields for example, while using date filter. (#1834)
+  - Accept file and http uri in -f command line option for specifying config files (#1873)
+  - Filters that generated events (multiline, clone, split, metrics) now propagate those events 
+    correctly to future conditionals (#1431)
+  - Fixed file descriptor leaks when using HTTP. The fix prevents Logstash from stalling, and
+    in some cases crashing from out-of-memory errors (#1604, LOGSTASH-892)
+  - You can now use LS_HOME/patterns directory to add generic patterns for those that may not be
+    associated with a particular plugin. Patterns in this dir will be loaded by default (#2225)
+  - We now check if the config file is correctly encoded. Otherwise we show a verbose error message
+    to convert the failing config file(s) to UTF-8 (#LOGSTASH-1103)
+  - Fixed bug in pipeline to gracefully teardown output workers when num workers > 1 (#2180)
+  - Fixed nologin path in release debian packages (#2283)
+  - Resolved issue where Logstash was crashing for users still using exclude_tags in their output
+    configuration (#2323)
+  - Allow spaces in field references like [hello world] (#1513)    
+
+### input
+  - Lumberjack: 
+    - Fixed Logstash crashes with Java Out Of Memory because of TCP thread leaks (#LOGSTASH-2168)
+    - Created a temporary fix to handle out of memory and eventual Logstash crash resulting from
+      pipeline backpressure. With this fix, you can create an upper limit on the number of 
+      Lumberjack connections after which no new connections will be accepted. This is defaulted
+      to 1000 connections, but can be changed using the config (#3003)
+    - Resolved issue where unrelated events were getting merged into a single event while using 
+      this input with with the multiline codec (#2016)
+    - Fixed Logstash crashing because it was using old jls-lumberjack version (#7)  
+  - TCP: 
+    - Fixed connection threads leak (#1509)
+    - Fixed input host field also contains source port (LOGSTASH-1849)
+  - Stdin: prevent overwrite of host field if already present in Event (#1668)
+  - Kafka: 
+    - Merged @joekiller's plugin to Logstash to get events from Kafka (#1472)
+    - Added support for whitelisting and blacklisting topics in the input.
+  - S3: 
+    - Added IAM roles support so you can securely read and write events from S3 without providing your
+      AWS credentials (#1575). 
+    - Added support for using temporary credentials obtained from AWS STS (#1946)
+    - AWS credentials can be specfied through environment variables (#1619)  
+  - RabbitMQ: 
+    - Fixed march_hare client uses incorrect connection url (LOGSTASH-2276)
+    - Use Bunny 1.5.0+ (#1894)
+  - Twitter: added improvements, robustness, fixes. full_tweet option now works, we handle 
+    Twitter rate limiting errors (#1471)
+  - Syslog: if input does not match syslog format, add tag _grokparsefailure_sysloginputplugin
+    which can be used to debug (#1593)
+  - File: When shutting down Logstash with file input, it would log a "permissions denied"
+    message. We fixed the underlying sinceDB issue while writing to a directory with no
+    permissions (#2964, #2935, #2882, file-input#16)
+  - File: Fixed a number of issues on Windows platform. These include:
+    - Resolving file locking issues which was causing log files to not rotate (#1557, #1389)
+    - Added support for using SinceDB to record multiple files' last read information (#1902)
+    - Fixed encoding issues which applies to many inputs (#2507)
+    - Resolved Logstash skipping lines when moving between files which are being followed (#1902)
+  - CouchDB: Added new input plugin to fetch data from CouchDB. Using the _changes API, data can be kept
+    in sync with any output like Elasticsearch by using this input  
+  - EventLog: For Windows, this input gracefully shutsdown if there is a timeout while receiving events
+    This also prevents Logstash from being stuck (#1672)
+  - Heartbeat: We created a new input plugin for generating heartbeat messages at periodic intervals. 
+    Use this to monitor Logstash -- you can measure the latency of the pipeline using these heartbeat 
+    events, and also check for availability
+
+### filter
+  - Multiline: 
+    - Fixed an issue where Logstash would crash while processing JSON formatted events on
+      Java 8 (#10)
+    - Handled cases where we unintentionally deduplicated lines, such as repeated lines in
+      xml messages (#3) 
+  - Grok: 
+    - "break_on_match => false" option now works correctly (#1547)
+    - allow user@hostname in commonapache log pattern (#1500 #1736)
+    - use optimized ruby-grok library which improves throughput in some cases by 50% (#1657)
+  - Date: 
+    - Fixed match defaults to 1970-01-01 when none of the formats matches and UNIX format is present
+      in the list (#1236, LOGSTASH-1597)
+    - support parsing almost-ISO8601 patterns like 2001-11-06 20:45:45.123-0000 (without a T)
+      which does not match %{TIMESTAMP_ISO8601}
+  - KV: allows dynamic include/exclude keys. For example, if an event has a key field and the user 
+    wants to parse out a value using the kv filter, the user should be able to 
+    include_keys: [ "%{key}" ]
+  - DNS: fixed add_tag adds tags even if filter was unsuccessful (#1785)
+  - XML: fixed UndefinedConversionError with UTF-8 encoding (LOGSTASH-2246)
+  - Mutate: 
+    - Fixed nested field notation for convert option like 'convert => [ "[a][0]", "float" ]' (#1401)
+    - Fixed issue where you can safely delete/rename fields which can have nil values (#2977)  
+    - gsub evaluates variables like %{format} in the replacement text (#1529)
+    - fixed confusing error message for invalid type conversion (#1656, LOGSTASH-2003)
+    - Resolved issue where convert option was creating an extra field in the event (#2268)
+    - Fixed issue where mutate with non-existent field was throwing an error (#2379)
+
+### output
+  - Elasticsearch:
+    - We have improved the security of the Elasticsearch output, input, and filter by adding
+      authentication and transport encryption support. In http protocol you can configure SSL/TLS to
+      enable encryption and HTTP basic authentication to provide a username and password while making
+      requests (#1453)
+    - Added support to be more resilient to transient errors in Elasticsearch. Previously, partial
+      failures from the bulk indexing functionality were not handled properly. With this fix, we added
+      the ability to capture failed requests from Elasticsearch and retry them. Error codes like 
+      429 (too many requests) will now be retried by default for 3 times. The number of retries and the
+      interval between consecutive retries can be configured (#1631)
+    - Logstash does not create a "message.raw" by default whic is usually not_analyzed; this
+      helps save disk space (#11)
+    - Added sniffing config to be able to list machines in the cluster while using the transport client (#22) 
+    - Deprecate the usage of index_type configuration. Added document_type to be consistent
+      with document_id (#102)
+    - Added warning when used with config embedded => true. Starting an embedded Elasticsearch
+      node is only recommended while prototyping. This should never be used in 
+      production setting (#99)
+    - Added support for multiple hosts in configuration and enhanced stability
+    - Logstash will not create a message.raw field by default now. Message field is not_analyzed
+      by Elasticsearch and adding a multi-field was essentially doubling the disk space required,
+      with no benefit
+
+  - S3: 
+    - Fixed a critical problem in the S3 Output plugin when using the size_file option. This could cause
+      data loss and data corruption of old logs ()
+    - Added IAM roles support so you can securely read and write events from S3 without providing your AWS
+      credentials (#1575)
+    - Added support for using temporary credentials obtained from AWS STS (#1946)
+    - Fixed a bug when the tags were not set in the plain text format (#1626)
+
+  - Kafka: merge @joekiller's plugin into Logstash to produce events to Kafka (#1472)
+  - File: Added enhancements and validations for destination path. Absolute path cannot start with a
+    dynamic string like /%{myfield}/, /test-%{myfield}/
+  - RabbitMQ: fixed crash while running Logstash for longer periods, typically when there's no
+    traffic on the logstash<->rabbitmq socket (LOGSTASH-1886)
+  - Statsd: fixed issue of converting very small float numbers to scientific notation 
+    like 9.3e-05 (#1670)
+  - Fixed undefined method error when conditional on an output (#LOGSTASH-2288)
+  
+### codec
+  - Netflow: Fixed a JSON serialization issue while using this codec (#2945)
+  - Added new Elasticsearch bulk codec which can be used to read data formatted in the Elasticsearch 
+    Bulk API (multiline json) format. For example, this codec can be used in combination with RabbitMQ 
+    input to mirror the functionality of the RabbitMQ Elasticsearch river
+  - Cloudfront: Added support for handling Amazon CloudFront events
+  - Avro: We added a new codec for data serialization (#1566)
+
+## 1.4.2 (June 24, 2014)
+### general
+  - fixed path issues when invoking bin/logstash outside its home directory
+
+### input
+  - bugfix: generator: fixed stdin option support
+  - bugfix: file: fixed debian 7 path issue
+
+### codecs
+  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
+  - improvement: collectd: add support for NaN values
+
+### outputs
+  - improvement: nagios_nsca: fix external command invocation to avoid shell escaping
+
+## 1.4.1 (May 6, 2014)
+### General
+  - bumped Elasticsearch to 1.1.1 and Kibana to 3.0.1
+  - improved specs & testing (Colin Surprenant), packaging (Richard Pijnenburg) & doc (James Turnbull)
+  - better $JAVA_HOME handling (Marc Chadwick)
+  - fixed bin/plugin target dir for when installing out from form logstash home (lr1980)
+  - fixed Accessors reset bug in Event#overwrite that was causing the infamous
+    "undefined method `tv_sec'" bug with the multiline filter (Colin Surprenant)
+  - fixed agent stalling when also using web option (Colin Surprenant)
+  - fixed accessing array-indexed event fields (Jonathan Van Eenwyk)
+  - new sysv init style scripts based on pleaserun (Richard Pijnenburg)
+  - better handling of invalid command line parameters (LOGSTASH-2024, Colin Surprenant)
+  - fixed running from a path containing spaces (LOGSTASH-1983, Colin Surprenant)
+
+### inputs
+  - improvement: rabbitmq: upgraded Bunny gem to 1.1.8, fixes a threading leak and improves
+    latency (Michael Klishin)
+  - improvement: twitter: added "full_tweet" option (Jordan Sissel)
+  - improvement: generator: fixed the example doc (LOGSTASH-2093, Jason Kendall)
+  - improvement: imap: option to disable certificate validation (Sverre Bakke)
+
+### codecs
+  - new: collectd: better performance & error handling than collectd input (Aaron Mildenstein)
+  - improvement: graphite: removed unused charset option (Colin Surprenant)
+  - improvement: json_spooler: is now deprecated (Colin Surprenant)
+  - improvement: proper charset support in all codecs (Colin Surprenant)
+
+### filters
+  - bugfix: date: on_success actions only when date parsing actually succeed (Philippe Weber)
+  - bugfix: multiline: "undefined method `tv_sec'" fix (Colin Surprenant)
+  - bugfix: multiline: fix for "undefined method `[]' for nil:NilClass" (#1258, Colin Surprenant)
+  - improvement: date: fix specs for non "en" locale (Olivier Le Moal)
+  - improvement: grok: better pattern for RFC-5424 syslog format (Guillaume Espanel)
+  - improvement: grok: refactored the LOGLEVEL pattern (Lorenzo González)
+  - improvement: grok: fix example doc (LOGSTASH-2093, Jason Kendall)
+  - improvement: metrics: document .pXX metric (Juarez Bochi)
+
+### outputs
+  - improvement: rabbitmq: upgraded Bunny gem to 1.1.8, fixes a threading leak and improves
+    latency (Michael Klishin)
+  - improvement: elasticsearch: start embedded server before creating a client to fix discovery
+    problems "waited for 30s ..." (Jordan Sissel)
+  - improvement: elasticsearch: have embedded ES use "bind_host" option for "network.host"
+    ES config (Jordan Sissel)
+
+## 1.4.0 (March 20, 2014)
+### General
+  - We've included some upgrade-specific release notes with more details about
+    the tarball changes and contrib packaging here:
+    http://logstash.net/docs/1.4.0/release-notes
+  - Ships with Kibana 3.0.0
+  - Much faster field reference implementation (Colin Surprenant)
+  - Fix a bug in character encoding which would cause inputs using non-UTF-8
+    codecs to accidentally skip re-encoding the text to UTF-8. This should
+    solve a great number of UTF-8-related bugs. (Colin Surprenant)
+  - Fixes missing gem  for logstash web which was broken in 1.4.0 beta1
+    (LOGSTASH-1918, Jordan Sissel)
+  - Fix 'help' output being emitted twice when --help is invoked.
+    (LOGSTASH-1952, #1168)
+  - Logstash now supports deletes! See outputs section below.
+  - Update template to fit ES 1.0 API changes (untergeek)
+  - Lots of Makefile, gem and build improvements courtesy of untergeek, Faye
+    Salwin, mrsolo, ronnocol, electrical, et al
+  - Add `env` command so you can run arbitrary commands with the logstash
+    environment setup (jordansissel)
+  - Bug fixes (lots).  Did I mention bug fixes? (Thanks, community!)
+  - Elasticsearch 1.0 libraries are now included. See the Elasticsearch
+    release notes for details: http://www.elasticsearch.org/downloads/1-0-0/
+  - Kibana 3 milestone 5 is included as the 'web' process.
+  - An empty --pluginpath directory is now accepted (#917, Richard Pijnenburg)
+  - Piles of documentation improvements! A brand new introductory tutorial is
+    included, and many of the popular plugins have had their docs greatly
+    improved. This effort was lead by Kurt Hurtado with assists by James
+    Turnbull, Aaron Mildenstein, Brad Fritz, and others.
+  - Testing was another focus of this release. We added many more tests
+    to help us prevent regressions and verify expected behavior. Helping with
+    this effort was Richard Pijnenburg, Jordan Sissel, and others.
+  - The 'debug' setting was removed from most plugins. Prior to this,
+    most plugins advertised the availability of this setting but actually
+    did not use it (#996, Jordan Sissel).
+  - bugfix: --pluginpath now lets you load codecs. (#1077, Sergey Zhemzhitsky)
+
+### inputs
+  - bugfix: collectd: Improve handling of 'NaN' values (#1015, Pieter Lexis)
+  - bugfix: snmptrap: Fixes exception when not specifying yamlmibdir (#950, Andres Koetsier)
+  - improvement: Add Multi-threaded workers and queues to UDP input (johnarnold + untergeek)
+  - improvement: log4j: port now defaults to 4560, the default log4j
+    SocketAppender port. (#757, davux)
+  - bugfix: rabbitmq: auto_delete and exclusive now default to 'false'.
+    The previous version's defaults caused data loss on logstash restarts.
+    Further, these settings are recommended by the RabbitMQ folks. (#864,
+    Michael Klishin)
+    This change breaks past default behavior, so just be aware. (Michael
+    Klishin)
+  - bugfix: collectd: fix some type calculation bugs (#905, Pieter Lexis)
+  - improvement: collectd: Now supports decryption and signature verification
+    (#905, Pieter Lexis)
+  - improvement: wmi: now supports remote hosts (#918, Richard Pijnenburg)
+  - bugfix: elasticsearch: Long scrollids now work correctly (#935, Jonathan
+    Van Eenwyk)
+  - bugfix: tcp: the 'host' field is correctly set now if you are using the
+    json codec and include a 'host' field in your events (#937, Jordan Sissel)
+  - bugfix: file: the 'host' field is correctly set now if you are using the
+    json codec and include a 'host' field in your events (#949, Piotr
+    Popieluch)
+  - bugfix: udp: the 'host' field is correctly set now if you are using the
+    json codec and include a 'host' field in your events (#965, Devin
+    Christensen)
+  - bugfix: syslog: fix regression (#986, Joshua Bussdieker)
+
+### codecs
+  - improvement: netflow: You can now specify your own netflow field
+    definitions using the 'definitions' setting. See the netflow codec
+    docs for examples on how to do this. (#808, Matt Dainty)
+
+### filters
+  - bugfix: clone: Correctly clone events with numeric field values.
+    (LOGSTASH-1225, #1158, Darren Holloway)
+  - bugfix: zeromq: Add `timeout` and `retries` settings for retrying on
+    request failures. Also adds `add_tag_on_timeout` so you can act on retry
+    failures. (logstash-contrib#23, Michael Hart)
+  - new: fingerprint: Checksum, anonymize, generate UUIDs, etc! A generalized
+    solution to replace the following filters: uuid, checksum, and anonymize.
+    (#907, Richard Pijnenburg)
+  - new: throttle: Allows you to tag or add fields to events that occur with a
+    given frequency. One use case is to have logstash email you only once if an
+    event occurs at least 3 times in 60 seconds. (#940, Mike Pilone) -
+  - improvement: translate: A new 'refresh_interval' setting lets you tell
+    logstash to periodically try reloading the 'dictionary_path' file
+    without requiring a restart. (#975, Kurt Hurtado)
+  - improvement: geoip: Now safe to use with multiple filter workers and
+    (#990, #997, LOGSTASH-1842; Avleen Vig, Jordan Sissel)
+  - improvement: metrics: Now safe to use with multiple filter workers (#993,
+    Bernd Ahlers)
+  - bugfix: date: Fix regression that caused times to be local time instead of
+    the intended timezone of UTC. (#1010, Jordan Sissel)
+  - bugfix: geoip: Fix encoding of fields created by geoip lookups
+    (LOGSTASH-1354, LOGSTASH-1372, LOGSTASH-1853, #1054, #1058; Jordan Sissel,
+    Nick Ethier)
+
+### outputs
+  - bugfix: elasticsearch: flush any buffered events on logstash shutdown
+    (#1175)
+  - feature: riemann: Automatically map event fields to rieman event fields
+    (logstash-contrib#15, Byron Pezan)
+  - bugfix: lumberjack: fix off-by-one errors causing writes to another
+    logstash agent to block indefinitely
+  - bugfix: elasticsearch: Fix NameError Socket crash on startup
+    (LOGSTASH-1974, #1167)
+  - improvement: Added `action` awesomeness to elasticsearch output (#1105, jordansissel)
+  - improvement: Implement `protocol => http` in elasticsearch output (#1105, jordansissel)
+  - bugfix: fix broken pipe output to allow EBADF instead of EPIPE,
+    allowing pipe command to be restarted (#974, Paweł Puterla)
+  - improvement: Adding dns resolution to lumberjack output (#1048, Nathan Burns )
+  - improvement: added pre- and post-messages to the IRC output (#1111, Lance O'Connor)
+  - bugfix: pipe: fix handling of command failures (#1023, #1034, LOGSTASH-1860; ronnocol, Jordan Sissel)
+  - improvement: lumberjack: now supports codecs (#1048, LOGSTASH-1680; Nathan Burns)
+
+## 1.3.3 (January 17, 2014)
+### general
+  - bugfix: Fix SSL cert load problem on plugins using aws-sdk: S3, SNS, etc.
+    (LOGSTASH-1778, LOGSTASH-1787, LOGSTASH-1784, #924; Adam Peck)
+  - bugfix: Fix library load problems for aws-sdk (LOGSTASH-1718, #923; Jordan
+    Sissel)
+  - bugfix: Fix regression introduced in 1.3.2 while trying to improve time
+    parsing performance. (LOGSTASH-1732, LOGSTASH-1738, #913; Jordan Sissel)
+  - bugfix: rabbitmq: honour the passive option when creating queues.
+    (LOGSTASH-1461, Tim Potter)
+
+### codecs
+  - bugfix: json_lines, json: Fix bug causing invalid json to be incorrectly
+    handled with respect to encoding (#920, LOGSTASH-1595; Jordan Sissel)
+
+## 1.3.2 (December 23, 2013)
+### upgrade notes
+  - Users of logstash 1.3.0 or 1.3.1 should set 'template_overwrite => true' in
+    your elasticsearch (or elasticsearch_http) outputs before upgrading to this
+    version to ensure you receive the fixed index template.
+
+### general
+  - web: don't crash if an invalid http request was sent
+    (#878, LOGSTASH-704; Jordan Sissel)
+  - Ships with Elasticsearch 0.90.9
+  - logstash will now try to make sure the @timestamp field is of the
+    correct format.
+  - Fix a bug in 1.3.1/1.3.0's elasticsearch index template causing phrase
+    searching to not work. Added tests to ensure search behavior works as
+    expected with this template. (Aaron Mildenstein, Jordan Sissel)
+  - Update README.md to be consistent with Makefile use of JRuby 1.7.8
+  - Time parsing in things like the json codec (and other similar parts of
+    logstash) are *much* faster now. This fixes a speed regression that was
+    introduced in logstash 1.2.0.
+
+### filters
+  - improvement: date: roughly 20% faster (Jordan Sissel)
+
+### outputs
+  - new: csv: write csv format to files output. (Matt Gray)
+    (This output will become a codec usable with file output in the next
+     major version!)
+
+## 1.3.1 (December 11, 2013)
+### general
+  - Fix path to the built-in elasticsearch index template
+
+## 1.3.0 (December 11, 2013)
+### general
+  - oops: The --help flag now reports help again, instead of barfing an "I need
+    help" exception (LOGSTASH-1436, LOGSTASH-1392; Jordan Sissel)
+  - Resolved encoding errors caused by environmental configurations, such as
+    'InvalidByteSequenceError ... on US-ASCII' (LOGSTASH-1595, #842;
+    Jordan Sissel)
+  - Fix bug causing "no such file to load -- base64" (LOGSTASH-1310,
+    LOGSTASH-1519, LOGSTASH-1325, LOGSTASH-1522, #834; Jordan Sissel)
+  - Elasticsearch version 0.90.7
+  - Bug fixes galore!
+
+### inputs
+  - new: collectd: receive metrics from collectd's network protocol
+    (#785, Aaron Mildenstein)
+  - bugfix: gelf: handle chunked gelf message properly (#718, Thomas De Smedt)
+  - bugfix: s3: fix bug in region endpoint setting (#740, Andrea Ascari)
+  - bugfix: pipe: restart the command when it finishes (#754, Jonathan Van
+    Eenwyk)
+  - bugfix: redis: if redis fails, reconnect. (#767, LOGSTASH-1475; Jordan Sissel)
+  - feature: imap: add 'content_type' setting for multipart messages and
+    choosing the part that becomes the event message. (#784, Brad Fritz)
+  - bugfix: zeromq: don't override the 'host' field if the event already
+    has one. (Jordan Sissel)
+  - bugfix: ganglia: fix regressions; plugin should work again (LOGSTASH-1655,
+    #818; Jordan Sissel)
+  - bugfix: Fix missing library in sqs input (#775, LOGSTASH-1294; Toby
+    Collier)
+
+### filters
+  - new: unique: removes duplicate values from a given field in an event.
+    (#676, Adam Tucker)
+  - new: elapsed: time duration between two tagged events. (#713, Andrea Forni)
+  - new: i18n: currently supports 'transliterate' which does best-effort
+    conversion of text to "plain" letters. Like 'ó' to 'o'.  (#671,
+    Juarez Bochi)
+  - bugfix: restore filter flushing thread (LOGSTASH-1284, #689; Bernd Ahlers)
+  - new: elasticsearch: query elasticsearch and update your event based on the
+    results. (#707, Jonathan Van Eenwyk)
+  - new: sumnumbers: finds all numbers in a message and sums them (#752, Avleen
+    Vig)
+  - feature: geoip: new field 'location' is GeoJSON derived from the lon/lat
+    coordinates for use with elasticsearch, kibana, and anything else that
+    understands GeoJSON (#763, Aaron Mildenstein)
+  - new: punct: Removes all text except punctuation and stores it in another
+    field. Useful for as a means for fingerprinting events. (#813, Guixing Bai)
+  - feature: metrics: Make percentiles configurable. Also make rates (1, 5,
+    15-minute) optional. (#817, Juarez Bochi)
+
+### codecs
+  - new: compressed_spooler: batches events and sends/receives them in
+    compressed form. Useful over high latency links or with transports
+    with higher-than-desired transmission costs. (Avleen Vig)
+  - new: fluent: receive data serialized using the Fluent::Logger for easier
+    migration away from fluentd or for folks who simply like the logger
+    library (#759, Jordan Sissel)
+  - new: edn: encode and decode the EDN serialization format. Commonly used
+    in Clojure. For more details, see: https://github.com/edn-format/edn
+    (#778, Lee Hinman)
+  - bugfix: oldlogstashjson: Fix encoding to work correctly. (#788, #795;
+    Brad Fritz)
+  - bugfix: oldlogstashjson: Fallback to plain text on invalid JSON
+    (LOGSTASH-1534, #850; Jordan Sissel)
+
+### outputs
+  - feature: elasticsearch and elasticsearch_http now will apply a default
+    index mapping template (included) which has the settings recommended by
+    Elasticsearch for Logstash specifically.
+    Configuration options allow disabling this feature and providing a path
+    to your own template. (#826, #839; Aaron Mildenstein)
+  - feature: elasticsearch_http: optional 'user' and 'password' settings to
+    make use of http authentication (LOGSTASH-902, #684; Ian Neubert)
+  - new: google_bigquery: upload logs to bigquery for analysis later (Rodrigo
+    De Castro)
+  - bugfix: datadog_metrics: fix validation bug (#789, Ian Paredes)
+  - feature: elasticsearch: new 'transport' setting letting you tell logstash
+    to act as a cluster node (default, prior behavior) or as a 'transport
+    client'. With the new 'transport' mode, your firewall rules may be simpler
+    (unicast, one direction) and transport clients do not show up in your
+    cluster node list. (LOGSTASH-102, #841; Jordan Sissel)
+  - feature: elasticsearch: new 'bind_port setting for 'node' protocol which
+    lets you chose the local port to bind on (#841, Jordan Sissel)
+  - bugfix: Fix missing library in sqs input (#775, LOGSTASH-1294; Toby
+    Collier)
+
+## 1.2.2 (October 22, 2013)
+### general
+  - new 'worker' setting for outputs. This helps improve throughput on
+    request-oriented outputs such as redis, rabbitmq, elasticsearch,
+    elasticsearch_http, etc. Workers run in separate threads each handling
+    events as they come in. This allows you to linearly scale up outputs across
+    cores or as blocking-io permits.
+  - grok performance is up 600%
+  - lots of bug fixes
+  - bugfixes to conditionals (#682, Matt Dainty)
+  - rabbitmq now replaces the old deprecated amqp plugins. amqp plugins are
+    removed.
+  - inputs will now do their best to handle text which is encoded differently
+    than the charset you have specified (LOGSTASH-1443, Jordan Sissel)
+
+### inputs
+  - bugfix: udp: respects teardown requests via SIGINT, etc (LOGSTASH-1290,
+    Jordan Sissel)
+  - bugfix: rabbitmq: disable automatic connection recovery (LOGSTASH-1350,
+    #641, #642; Michael Klishin)
+  - bugfix: twitter: works again (#640, Bernd Ahlers)
+  - compatibility: Restored the old 'format' setting behavior. It is still
+    deprecated, but was accidentally removed in 1.2.0. It will be removed
+    later, but is restored as part of our backwards-compat promise (Jordan
+    Sissel)
+  - bugfix: s3: fix LOGSTASH-1321 and LOGSTASH-1319 (Richard Pijnenburg)
+  - bugfix: log4j: fix typo (Jordan Sissel)
+  - bugfix: rabbitmq: disable automatic connection recover because logstash
+    will handle it (LOGSTASH-1350, Michael Klishin)
+  - bugfix: heroku: works again (LOGSTASH-1347, #643; Bernd Ahlers)
+  - bugfix: tcp: improve detection of closed connections to reduce lost events
+    (Jordan Sissel)
+  - bugfix: elasticsearch: now works correctly (#670, Richard Pijnenburg)
+  - improvement: elasticsearch: make size and scroll time configurable (#670,
+    Richard Pijnenburg)
+  - improvement: elasticsearch: tunable search type (#670, Richard Pijnenburg)
+  - compatibility: restore 'format' setting which was accidentally removed in
+    1.2.0. This feature is still deprecated, but it has been restored
+    temporarily as part of our backwards compatibility promise. (#706, Jordan
+    Sissel)
+  - bugfix: syslog: fix socket leakage (#704, Bernd Ahlers)
+  - improvement: all aws-related plugins: Add proxy_uri setting (#714, Malthe
+    Borch)
+  - bugfix: unix: fix variable name crash (#720, Nikolay Bryskin)
+
+### codecs
+  - new: graphite: parse graphite formated events (Nick Ethier)
+  - new: json_lines: parse streams that are lines of json objects (#731, Nick
+    Ethier)
+  - bugfix: multiline: time is now correctly in UTC. (Jordan Sissel)
+  - bugfix: oldlogstashjson: improved conversion of old logstash json to the
+    new schema (#654, Jordan Sissel)
+  - bugfix: oldlogstashjson: fix typo breaking encoding (#665, Tom Howe)
+  - bugfix: json: now assumes json delimited by newline character
+    (LOGSTASH-1332, #710; Nick Ethier)
+  - improvements: netflow: new target and versions settings (#686, Matt Dainty)
+
+### filters
+  - performance: grok: 6.3x performance improvement (#681, Jordan Sissel)
+  - bugfix: geoip: empty values (nil, empty string) are not put into the event
+    anymore. (Jordan Sissel)
+  - bugfix: geoip: allow using Maxmind's ASN database (LOGSTASH-1394, #694;
+    Bernd Ahlers)
+  - improvement: kv: target will now overwrite any existing fields, including
+    the source (Jordan Sissel).
+  - improvement: Kv: 'prefix' setting now respects sprintf (LOGSTASH-913,
+    #647; Richard Pijnenburg)
+  - checksum: sha128 was not a valid digest, removed from list
+  - feature: metrics: added clear_interval and flush_interval parameters for
+    setting flush rates and when to clear metrics (#545)
+  - new: collate: group events by time and/or count into a single event. (#609,
+    Neway Liu)
+  - feature: date: now supports a 'target' field for writing the timestamp into
+    a field other than @timestamp. (#625, Jonathan Van Eenwyk)
+  - bugfix: riemann: event tagging works again (#631, Marc Fournier)
+  - improvement: grok: IPV6 pattern (#623, Matt Dainty)
+  - improvement: metrics: add clear_interval and flush_interval settings (#545,
+    Juarez Bochi)
+  - improvement: useragent: include operating system details (#656, Philip
+    Kubat)
+  - improvement: csv: new quote_char setting (#725, Alex Markham)
+
+### outputs
+  - feature: all outputs have a 'worker' setting  now that allows you to
+    perform more work at the same time. This is useful for plugins like
+    elasticsearch_http, redis, etc, which can bottleneck on waiting for
+    requests to complete but would otherwise be happy processing more
+    simultaneous requests. (#708, Jordan Sissel)
+  - bugfix: elasticsearch: requests are now synchronous. This avoid overloading
+    the client and server with unlimited in-flight requests. (#688, Jordan
+    Sissel)
+  - bugfix: elasticsearch_http: fix bug when sending multibyte utf-8 events
+    (LOGSTASH-1328, #678, #679, #695; Steve Merrill, Christian Winther,
+    NickEthier, Jordan Sissel)
+  - performance: elasticsearch_http: http client library uses TCP_NODELAY now
+    which dramatically improves performance. (#696, Jordan Sissel)
+  - feature: elasticsearch_http now supports a 'replication' setting to
+    allow you to choose how you wait for the response. THe default is 'sync'
+    which waits for all replica shards to be written. If you set it to 'async'
+    then all index requests will respond once only the primary shards have been
+    written and the replica shards will be written later. This can improve
+    throughput. (#700, Nick Ethier, Jordan Sissel)
+  - bugfix: elasticsearch: the default port range is now 9300-9305; the older
+    range up to 9400 was unnecessary and could cause problems for the
+    elasticsearch cluster in some cases.
+  - improvement: aws-based outputs (e.g. cloudwatch) now support proxy uri.
+  - bugfix: rabbitmq: disable automatic connection recovery (LOGSTASH-1350)
+    (#642)
+  - bugfix: riemann: fixed tagging of riemann events (#631)
+  - bugfix: s3: fix LOGSTASH-1321 and LOGSTASH-1319 (#636, #645; Richard
+    Pijnenburg)
+  - bugfix: mongodb: Fix mongodb auth (LOGSTASH-1371, #659; bitsofinfo)
+  - bugfix: datadog: Fix time conversion (LOGSTASH-1427, #690; Bernd Ahlers)
+  - bugfix: statsd: Permit plain floating point values correctly in the
+    config. Example: sample_rate => 0.5 (LOGSTASH-1441, #705; Jordan Sissel)
+  - bugfix: syslog: Fix timestamp date formation. 'timestamp' setting is now
+    deprecated and the format of the time depends on your rfc selection.
+    (LOGSTASH-1423, #692, #739; Jordan Sissel, Bernd Ahlers)
+
+### patterns
+  - improvement: added IPV6 suppot to IP pattern (#623)
+
+## 1.2.1 (September 7, 2013)
+### general
+  - This is primarily a bugfix/stability release based on feedback from 1.2.0
+  - web: kibana's default dashboard now works with the new logstash 1.2 schema.
+  - docs: updated the tutorials to work in logstash 1.2.x
+  - agent: Restored the --configtest flag (unintentionally removed from 1.2.0)
+  - deprecation: Using deprecated plugin settings can now advise you on a
+    corrective path to take. One example is the 'type' setting on filters and
+    outputs will now advise you to use conditionals and give an example.
+  - conditionals: The "not in" operator is now supported.
+
+### inputs
+  - bugfix: pipe: reopen the pipe and retry on any error. (#619, Jonathan Van
+    Eenwyk)
+  - bugfix: syslog: 'message' field no longer appears as an array.
+  - bugfix: rabbitmq: can now bind the queue to the exchange (#624, #628,
+    LOGSTASH-1300, patches by Jonathan Tron and Jonathan Van Eenwyk)
+
+### codecs
+  - compatibility: json: if data given is not valid as json will now be used as
+    the "message" of an event . This restores the older behavior when using
+    1.1.13's "format => json" feature on inputs. (LOGSTASH-1299)
+  - new: netflow: process netflow data (#580, patches by Nikolay Bryskin and
+    Matt Dainty)
+
+### filters
+  - bugfix: multiline: the multiline filter returns! It was unintentionally
+    removed from the previous (1.2.0) release.
+  - bugfix: json_encode: fix a syntax error in the code. (LOGSTASH-1296)
+  - feature: kv: now captures duplicate field names as a list, so 'foo=bar
+    foo=baz' becomes the field 'foo' with value ['bar', 'baz'] (an array).
+    (#622, patch by Matt Dainty)
+
+### outputs
+  - new: google_cloud_storage: archive logs to Google Cloud Storage (#572,
+    Rodrigo De Castro)
+  - bugfix: fixed bug with 'tags' and 'exclude_tags' on outputs that would
+    crash if the event had no tags. (LOGSTASH-1286)
+
+## 1.2.0 (September 3, 2013)
+### general
+  - The logstash json schema has changed. (LOGSTASH-675)
+    For prior logstash users, you will be impacted one of several ways:
+    * You should check your elasticsearch templates and update them accordingly.
+    * If you want to reindex old data from elasticsearch with the new schema,
+      you should be able to do this with the elasticsearch input. Just make
+      sure you set 'codec => oldlogstashjson' in your elasticsearch input.
+  - The old logstash web ui has been replaced by Kibana 3. Kibana is a far
+    superior search and analytics interface.
+  - New feature: conditionals! You can now make "if this, then ..." decisions
+    in your filters or outputs. See the docs here:
+    http://logstash.net/docs/latest/configuration#conditionals
+  - A new syntax exists for referencing fields (LOGSTASH-1153). This replaces
+    the prior and undocumented syntax for field access (was 'foo.bar' and is
+    now '[foo][bar]'). Learn more about this here:
+    http://logstash.net/docs/latest/configuration#fieldreferences
+  - A saner hash syntax in the logstash config is now supported. It uses the
+    perl/ruby hash-rocket syntax: { "key" => "value", ... } (LOGSTASH-728)
+  - ElasticSearch version 0.90.3 is included. (#486, Gang Chen)
+  - The elasticsearch plugin now uses the bulk index api which should result
+    in lower cpu usage as well as higher performance than the previous
+    logstash version.
+  - Many deprecated features have been removed. If your config caused
+    deprecation warnings on startup in logstash v1.1.13, there is a good
+    chance that these deprecated settings are now absent.
+  - 'type' is no longer a required setting on inputs.
+  - New plugin type: codec. Used to implement decoding of events for inputs and
+    encoding of events for outputs. Codecs allow us to separate transport (like
+    tcp, redis, rabbitmq) from serialization (gzip text, json, msgpack, etc).
+  - Improved error messages that try to be helpful. If you see bad or confusing
+    error messages, it is a bug, so let us know! (Patch by Nick Ethier)
+  - The old 'plugin status' concept has been replaced by 'milestones'
+    (LOGSTASH-1137)
+  - SIGHUP should cause logstash to reopen it's logfile if you are using the
+    --log flag
+
+### inputs
+  - new: s3: reads files from s3 (#537, patch by Mathieu Guillaume)
+  - feature: imap: now marks emails as read (#542, Raffael Schmid)
+  - feature: imap: lets you delete read email (#591, Jonathan Van Eenwyk)
+  - feature: rabbitmq: now well-supported again (patches by Michael Klishin)
+  - bugfix: gelf: work around gelf parser errors (#476, patch by Chris McCoy)
+  - broken: the twitter input is disabled because the twitter stream v1 api is
+    no longer supported and I couldn't find a replacement library that works
+    under JRuby.
+  - new: sqlite input (#484, patch by Evan Livingston)
+  - improvement: snmptrap: new 'yamlmibdir' setting for specifying an external
+    source for MIB definitions. (#477, patch by Dick Davies)
+  - improvement: stomp: vhost support (#490, patch by Matt Dainty)
+  - new: unix: unix socket input (#496, patch by Nikolay Bryskin)
+  - new: wmi: for querying wmi (windows). (#497, patch by Philip Seidel)
+  - improvement: sqs: new id_field and md5_field settings (LOGSTASH-1118, Louis
+    Zuckerman)
+
+### filters
+  - feature: grok: 'singles' now defaults to true.
+  - bugfix: grep: allow repeating a field in the hash config (LOGSTASH-919)
+  - feature: specify timezone in date filter (#470, patch by Philippe Weber)
+  - feature: grok setting 'overwrite' now lets you overwrite fields instead
+    of appending to them.
+  - feature: the useragent filter now defaults to writing results to the top
+    level of the event instead of "ua"
+  - feature: grok now defaults 'singles' to true, meaning captured fields are
+    stored as single values in most cases instead of the old behavior of being
+    captured as an array of values.
+  - new: json_encoder filter (#554, patch by Ralph Meijer)
+  - new: cipher: gives you many options for encrypting fields (#493, patch by
+    saez0pub)
+  - feature: kv: new settings include_fields and exclude_fields. (patch by
+    Piavlo)
+  - feature: geoip: new 'target' setting for where to write geoip results.
+    (#491, patch by Richard Pijnenburg)
+  - feature: dns: now accepts custom nameservers to query (#495, patch by
+    Nikolay Bryskin)
+  - feature: dns: now accepts a timeout setting (#507, patch by Jay Luker)
+  - bugfix: ruby: multiple ruby filter instances now work (#501, patch by
+    Nikolay Bryskin)
+  - feature: uuid: new filter to add a uuid to each event (#531, Tomas Doran)
+  - feature: useragent: added 'prefix' setting to prefix field names created
+    by this filter. (#524, patch by Jay Luker)
+  - bugfix: mutate: strip works now (#590, Jonathan Van Eenwyk)
+  - new: extractnumbers: extract all numbers from a message (#579, patch by
+    Pablo Barrera)
+
+### outputs
+  - new: jira: create jira tickets from an event (#536, patch by Martin Cleaver)
+  - feature: rabbitmq: now well-supported again (patches by Michael Klishin)
+  - improvement: stomp: vhost support (Patch by Matt Dainty)
+  - feature: elasticsearch: now uses the bulk index api and supports
+    a tunable bulk flushing size.
+  - feature: elasticsearch_http: will now flush when idle instead of always
+    waiting for a full buffer. This helps in slow-sender situations such
+    as testing by hand.
+  - feature: irc: add messages_per_second tunable (LOGSTASH-962)
+  - bugfix: email: restored initial really useful documentation
+  - improvement: emails: allow @message, @source, @... in match (LOGSTASH-826,
+    LOGSTASH-823)
+  - feature: email: can now set Reply-To (#540, Tim Meighen)
+  - feature: mongodb: replica sets are supported (#389, patch by Mathias Gug)
+  - new: s3: New plugin to write to amazon S3 (#439, patch by Mattia Peterle)
+  - feature: statsd: now supports 'set' metrics (#513, patch by David Warden)
+  - feature: sqs: now supports batching (#522, patch by AaronTheApe)
+  - feature: ganglia: add slope and group settings (#583, patch by divanikus)
+
+## 1.1.13 (May 28, 2013)
+### general
+  - fixed bug in static file serving for logstash web (LOGSTASH-1067)
+
+### outputs
+  - feature: irc: add messages_per_second tunable (LOGSTASH-962)
+
+## 1.1.12 (May 7, 2013)
+### filters
+  - bugfix: useragent filter now works correctly with the built-in regexes.yaml
+  - bugfix: mail output with smtp now works again
+
+## 1.1.11 (May 7, 2013)
+### general
+  - This release is primarily a bugfix release for bugs introduced by the
+    previous release.
+  - Support for Rubinius and MRI exists once again.
+
+### inputs
+  - bugfix: lumberjack now respects field data again (lumberjack --field foo=bar)
+  - bugfix: rabbitmq was broken by the previous release (LOGSTASH-1003,
+    LOGSTASH-1038; Patch by Jason Koppe)
+  - bugfix: relp: allow multiple client socket connections to RELP input
+    (LOGSTASH-707, LOGSTASH-736, LOGSTASH-921)
+
+### filters
+  - bugfix: geoip was broken by the previous release (LOGSTASH-1013)
+  - feature: sleep now accepts an 'every' setting which causes it to
+    sleep every N events. Example; sleep every 10 events: every => 10.
+  - feature: grok now permits dashes and dots in captures, such as
+    %{WORD:foo-bar}.
+  - bugfix: useragent filter now ships with a default regexes.yaml file
+    that is used by default unless you otherwise specify (LOGSTASH-1051)
+  - bugfix: add_field now correctly sets top-level fields like @message
+  - bugfix: mutate 'replace' now sets a field regardless of whether or not
+    it exists.
+  - feature: new mutate 'update' setting to change a field's value but
+    only if that field exists already.
+
+### outputs
+  - feature: irc output now supports 'secure' setting to use ssl (LOGSTASH-139)
+  - feature: nagios_nsca has new setting 'message_format'
+  - bugfix: fix graphite plugin broken in 1.1.10 (LOGSTASH-968)
+  - bugfix: elasticsearch_http was broken in 1.1.10 (LOGSTASH-1004)
+  - bugfix: rabbitmq was broken by the previous release (LOGSTASH-1003,
+    LOGSTASH-1038; Patch by Jason Koppe)
+  - feature: hipchat 'notify' setting now called 'trigger_notify' (#467, patch
+    by Richard Pijnenburg)
+
+## 1.1.10 (April 16, 2013)
+### general
+  - On linux, all threads will set their process names so you can identify
+    threads in tools like top(1).
+  - Java 5 is no longer supported (You must use Java 6 or newer).
+  - Windows line terminators (CRLF) are now accepted in config files.
+  - All AWS-related plugins now have the same configuration options:
+    region, access_key_id, secret_access_key, use_ssl, and
+    aws_credentials_file. Affected plugins: cloudwatch output,
+    sns output, sqs output, sqs input. (LOGSTASH-805)
+  - Lots of documentation fixes (James Turnbull, et al)
+  - The amqp plugins are now named 'rabbitmq' because it *only* works
+    with rabbitmq. The old 'amqp' name should still work, but it will
+    be removed soon while 'rabbitmq' will stay. (Patches by Michael Zaccari)
+  - New flag '--configtest' to test config and exit. (Patch by Darren Patterson)
+  - Improved error feedback logstash gives to you as a user.
+
+### inputs
+  - new: elasticsearch: this input allows you to stream search results from
+    elasticsearch; it uses the Scroll API.
+  - new: websocket. Currently supports acting as a websocket client.
+  - new: snmptrap, to receive SNMP traps (patch by Paul Czar)
+  - new: varnishlog input to read from the Varnish Cache server's shared memory
+    log (LOGSTASH-978, #422; Louis Zuckerman)
+  - new: graphite input. Supports the plain text carbon tcp protocol.
+  - new: imap input. Read mail!
+  - feature: twitter: supports http proxying now (#276, patch by Richard
+    Pijnenburg)
+  - feature: loggly: supports http proxying now (#276, patch by Richard
+    Pijnenburg)
+  - feature: tcp: ssl now supported! (#318, patch by Matthew Richardson)
+  - feature: redis: now supports 'batch_count' option for doing bulk fetches
+    from redis lists. Requires Redis 2.6.0 or higher. (#320, patch by Piavlo)
+  - feature: irc: will use ssl if you set 'secure' (#393, patch by Tomas Doran)
+  - bugfix: log4j: respect add_fields (LOGSTASH-904, #358)
+  - bugfix: eventlog: input should now work
+  - bugfix: irc: passwords now work (#412, Nick Ethier)
+
+### filters
+  - new: useragent: parses user agent strings in to structured data based on
+    BrowserScope data (#347, patch by Dan Everton)
+  - new: sleep: sleeps a given amount of time before passing the event.
+    Useful for rate limiting or replay simulation.
+  - new: ruby: experimental ruby plugin that lets you call custom ruby code
+    on every event.
+  - new: translate: for mapping values (#335, patch by Paul Czar)
+  - new: clone: creates a copy of the event.
+  - feature: grok: Adds tag_on_failure setting so you can prevent grok from
+    tagging events on failure. (#328, patch by Neil Prosser)
+  - deprecated: grok: deprecated the --grok-patterns-path flag (LOGSTASH-803)
+  - feature: date: nested field access is allowed now
+  - feature: csv, xml, kv, json, geoip: new common settings!
+    (LOGSTASH-756, #310, #311, #312, #383, #396; patches by Richard Pijnenburg)
+      source - what field the text comes from
+      target - where to store the parse result.
+  - feature: csv: new setting: columns - labels for each column parsed.
+  - bugfix: geoip: The built-in geoip database should work now (#326, patch
+    by Vincent Batts)
+  - bugfix: kv filter now respects add_tag, etc (LOGSTASH-935)
+
+### outputs
+  - new: hipchat output (#428, Cameron Stokes)
+  - bugfix: mongo would fail to load bson_java support (LOGSTASH-849)
+  - bugfix: tags support to gelf output. Returns tags as _tags field
+    (LOGSTASH-880, patch by James Turnbull)
+  - bugfix: elasticsearch: Fix a race condition. (#340, patch by Raymond Feng)
+  - improvement: http: now supports a custom 'message' format for building your
+    own http bodies from an event. (#319, patch by Christian S)
+  - bugfix: Fix opentsdb output (LOGSTASH-689, #317; patch by Emmet Murphy)
+  - improvement: http output now supports a custom message format with
+    the 'message' setting (Patch by Christian Schröder)
+  - graphite output now lets you ship the whole (or part) of an event's fields
+    to graphite as metric updates. (#350, patch by Piavlo)
+  - email output now correctly defaults to not using authentication
+    (LOGSTASH-559, #365; patch by Stian Mathiassen)
+  - bugfix: file output now works correctly on fifos
+  - bugfix: irc passwords now work (#412, Nick Ethier)
+  - improvement: redis output now supports congestion detection. If
+    it appears nothing is consuming from redis, the output will stall
+    until that problem is resolved. This helps prevent a dead reader
+    from letting redis fill up memory. (Piavlo)
+  - feature: boundary: New 'auto' setting. (#413, Alden Jole)
+
+## 1.1.9 (January 10, 2013)
+### inputs
+  - bugfix: all inputs: fix bug where some @source values were not valid urls
+
+### filters
+  - bugfix: mutate: skip missing fields in 'convert' (#244, patch by Ralph Meijer)
+
+### outputs
+  - improvement: gelf: new tunable 'ignore_metadata' flag to set which fields
+    to ignore if ship_metadata is set. (#244, patch by Ralph Meijer)
+  - improvement: gelf: make short_message's field name tunable (#244, patch by
+    Ralph Meijer)
+
+## 1.1.8 (January 10, 2013)
+### general
+  - patched another work around for JRUBY-6970 (LOGSTASH-801)
+
+### inputs
+  - bugfix: tcp: 'Address in use' errors now report the host/port involved.
+    (LOGSTASH-831)
+  - bugfix: zeromq: fix bug where an invalid url could be given as a source
+    (LOGSTASH-821, #306)
+
+### outputs
+  - bugfix: elasticsearch_river: it now resolves evaluates %{} variables in
+    index and index_type settings. (LOGSTASH-819)
+
+## 1.1.7 (January 3, 2013)
+### inputs
+ - fix bug where @source_host was set to 'false' in many cases.
+
+### outputs
+ - improvement: redis: shuffle_hosts is now enabled by default
+
+## 1.1.6 (January 2, 2013)
+### Overview of this release:
+ - new inputs: drupal_dblog.
+ - new filters: anonymize, metrics.
+ - new outputs: syslog, cloudwatch.
+ - new 'charset' setting for all inputs. This should resolve all known encoding
+   problems. The default charset is UTF-8.
+ - grok now captures (?<somename>...) regexp into 'somename' field
+ - Elasticsearch 0.20.2 is included. This means you are required to upgrade
+   your elasticsearch cluster to 0.20.2. If you wish to continue using an old
+   version of elasticsearch, you should use the elasticsearch_http plugin
+   instead of the elasticsearch one.
+
+ ### general
+ - fixed internal dependency versioning on 'addressable' gem (LOGSTASH-694)
+ - fixed another case of 'watchdog timeout' (LOGSTASH-701)
+ - plugin flags are now deprecated. The grok filter (--grok-pattern-path) was
+   the only plugin to make use of this.
+ - the grok filter has improved documentation
+ - lots of documentation fixes (James Turnbull, Louis Zuckerman)
+ - lots of testing improvements (Philippe Weber, Laust Rud Jacobsen)
+ - all 'name' settings have been deprecated in favor of more descriptive
+   settings (LOGSTASH-755)
+ - JRuby upgraded to 1.7.1
+ - removed use of bundler
+ - Fixed timestamp parsing in MRI (patch by Rene Lengwinat)
+
+ ### inputs
+ - All inputs now have a 'charset' setting to help you inform logstash of the
+   text encoding of the input. This is useful if you have Shift_JIS or CP1251
+   encoded log files. This should help resolve the many UTF-8 bugs that were
+   reported recently. The default charset is UTF-8.
+ - new: drupal_dblog: read events from a DBLog-enabled Drupal. (#251, Patch by
+   theduke)
+ - bugfix: zeromq: 'topology' is now a required setting
+ - bugfix: lumberjack: client connection closing is now handled properly.
+   (Patch by Nick Ethier)
+ - misc: lumberjack: jls-lumberjack gem updated to 0.0.7
+ - bugfix: stomp: fix startup problems causing early termination (#226
+ - bugfix: tcp: the 'source host' for events is now the client ip:port that
+   sent it, instead of the listen address that received it. (LOGSTASH-796)
+ - improvement: tcp: the default data_timeout is now -1 (never timeout).
+   This change was made because read timeouts were causing data loss, and
+   logstash should avoid losing events by default.
+ - improvement: amqp: the 'name' setting is now called 'queue' (#274)
+ - improvement: eventlog: the 'name' setting is now called 'logfile' (#274)
+ - bugfix: log4j: fix stacktrace reading (#253, patch by Alex Arutyunyants)
+
+ ### filters
+ - new: anonymize: supports many hash mechanisms (murmur3, sha1, md5, etc) as
+   well as IP address anonymization (#280, #261; patches by Richard Pijnenburg
+   and Avishai Ish-Shalom)
+ - new: metrics: allows you to aggregate metrics from events and emit them
+   periodically. Think of this like 'statsd' but implemented as a logstash
+   filter instead of an external service.
+ - feature: date: now accepts 'match' as a setting. Use of this is preferable
+   to the old syntax. Where you previously had 'date { somefield =>
+   "somepattern" }' you should now do: 'date { match => [ "somefield",
+   "somepattern" ] }'. (#248, LOGSTASH-734, Patch by Louis Zuckerman)
+ - feature: grok: now accepts (?<foo>...) named captures. This lets you
+   compose a pattern in the grok config without needing to define it in a
+   patterns file. Example: (?<hostport>%{HOST}:%{POSINT}) to capture 'hostport'
+ - improvement: grok: allow '$' in JAVACLASS pattern (#241, patch by Corry
+   Haines)
+ - improvement: grok: can now match against number types. Example, if you're
+   sending a json format event with { "status": 403 } you can now grok that
+   field.  The number is represented as a string "403" before pattern matching.
+ - bugfix: date: Fix a bug that would crash the pipeline if no date pattern
+   matched. (LOGSTASH-705)
+ - feature: kv: Adds field_split, value_split, prefix, and container
+   settings. (#225, patch by Alex Wheeler)
+ - bugfix: mutate: rename on a nonexistant field now does nothing as expected.
+   (LOGSTASH-757)
+ - bugfix: grok: don't tag an event with _grokparsefailure if it's already so
+   (#248, patch by Greg Brockman)
+ - feature: mutate: new settings - split, join, strip. "split" splits a field
+   into an array. "join" merges an array into a string. "strip" strips leading and
+   trailing whitespace. (Patch by Avishai Ish-Shalom)
+
+### outputs
+ - new: syslog output supporting both RFC3164 and RFC5424 (#180, patch by
+   Rui Alves)
+ - new: cloudwatch output to emit metrics and other events to Amazon CloudWatch.
+   (LOGSTASH-461, patch by Louis Zuckerman)
+ - feature: stdout: added 'message' setting for configuring the output message
+   format. The default is same behavior as before this feature.
+ - feature: http: added 'format' option to select 'json' or form-encoded
+   request body to send with each request.
+ - feature: http: added 'content_Type' option set the Content-Type header.
+   This defaults to "application/json" if the 'format' is 'json'. Will default
+   to 'application/x-www-form-urlencoded' if the 'format' is 'form'
+ - bugfix: zeromq: 'topology' is now a required setting
+ - feature: mongodb: new setting 'isodate' that, when true, stores the
+   @timestamp field as a mongodb date instead of a string. (#224, patch by
+   Kevin Amorin)
+ - improvement: gelf: Allow full_message gelf property to be overridden (#245,
+   patch by Sébastien Masset)
+ - misc: lumberjack: jls-lumberjack gem updated to 0.0.6
+ - feature: nagios: New 'nagios_level' setting to let you change the level
+   of the passive check result sent to nagios. (#298, Patch by James Turnbull)
+ - feature: elasticsearch, elasticsearch_http, elasticsearch_river: new setting
+   'document_id' for explicitly setting the document id in each write to
+   elasticsearch. This is useful for overwriting existing documents.
+ - improvement: elasticsearch_river: 'name' is now 'queue' (#274)
+ - improvement: amqp: 'name' is now 'exchange' (#274)
+ - bugfix: the websocket output works again (supports RFC6455)
+
+## 1.1.5 (November 10, 2012)
+### Overview of this release:
+ * New inputs: zenoss, gemfire
+ * New outputs: lumberjack, gemfire
+ * Many UTF-8 crashing bugs were resolved
+
+### general
+ - new runner command 'rspec' - lets you run rspec tests from the jar
+   This means you should now be able to write external tests that execute your
+   logstash configs and verify functionality.
+ - "file not found" errors related to paths that had "jar:" prefixes should
+   now work. (Fixes LOGSTASH-649, LOGSTASH-642, LOGSTASH-655)
+ - several plugins received UTF-8-related fixes (file, lumberjack, etc)
+   File bugs if you see any UTF-8 related crashes.
+ - 'json_event' format inputs will now respect 'tags' (#239, patch by
+   Tim Laszlo)
+ - logstash no longer uses nor recommends bundler (see 'gembag.rb'). The
+   Gemfile will be purged in the near future.
+ - amqp plugins are now marked 'unsupported' as there is no active maintainer
+   nor is there source of active support in the community. If you're interested
+   in maintainership, please email the mailling list or contact Jordan!
+
+### inputs
+ - irc: now stores irc nick
+ - new: zenoss (#232, patch by Chet Luther)
+ - new: gemfire (#235, patch by Andrea Campi)
+ - bugfix: udp: skip close() call if we're already closed (#238, patch by kcrayon)
+
+### filters
+ - bugfix: fix for zeromq filter initializer (#237, patch by Tom Howe)
+
+### outputs
+ - new: lumberjack output (patch by Nick Ethier)
+ - new: gemfire output (#234, patch by Andrea Campi)
+ - improved: nagios_ncsa (patch by Tomas Doran)
+ - improved: elasticsearch: permit setting 'host' even if embedded. Also set the
+   host default to 'localhost' when using embedded. These fixes should help resolve
+   issues new users have when their distros surprisingly block multicast by
+   default.
+ - improved: elasticsearch: failed index attempts will be retried
+ - improved: irc: new 'password' setting (#283, patch by theduke)
+
+## 1.1.4 (October 28, 2012)
+### Overview of this release:
+ - bug fixes mostly
+
+### filters
+ - date: Fix crashing on date filter failures. Wrote test to cover this case.
+   (LOGSTASH-641)
+ - grok: Improve QUOTEDSTRING pattern to avoid some more 'watchdog timeout' problems
+
+### outputs
+ - nagios_nsca: Allow check status to be set from the event (#228, patch by
+   Tomas Doran)
+ - elasticsearch_http: Fix OpenSSL::X509::StoreError (LOGSTASH-642)
+
+## 1.1.3 (October 22, 2012)
+ - rebuilt 1.1.2 for java 5 and 6
+
+## 1.1.2 (October 22, 2012)
+### Overview of this release:
+  * New input plugins: lumberjack, sqs, relp
+  * New output plugins: exec, sqs
+  * New filter plugins: kv, geoip, urldecode, alter
+  * file input supports backfill via 'start_position'
+  * filter watchdog timer set to 10 seconds (was 2 seconds)
+
+### general
+ - Stopped using 'Gemfile' for dependencies, the logstash.gemspec has returned.
+   (Patch by Grant Rogers)
+ - New 'logstash-event.gemspec' for generating logstash events in your own
+   ruby programs (Patch by Garry Shutler)
+ - Wildcard config files are now sorted properly (agent -f
+   /etc/logstash/*.conf)
+ - The old '-vvv' setting ruby's internal $DEBUG is now gone. It was causing
+   too much confusion for users due to noise.
+ - Improved 'logstash event' creation speed by 3.5x
+ - Now uses JRuby 1.7.0
+ - Now ships with Elasticsearch 0.19.10
+
+### inputs
+ - bugfix: redis: [LOGSTASH-526] fix bug with password passing
+ - new: lumberjack: for use with the lumberjack log shipper
+   (https://github.com/jordansissel/lumberjack)
+ - new: sqs: Amazon SQS input (Patch by Sean Laurent, #211)
+ - new: relp: RELP (rsyslog) plugin (Patch by Mike Worth, #177)
+ - file input: sincedb path is now automatically generated if not specified.
+   This helps work around a problem where two file inputs don't specify a
+   sincedb_path would clobber eachother (LOGSTASH-554)
+ - file input: no longer crashes if HOME is not set in env (LOGSTASH-458)
+ - log4j input: now supports MDC 'event properties' which are stored as fields
+   in the logstash event. (#216, #179. Patches by Charles Robertson and Jurjan
+   Woltman)
+ - pipe input: should work now.
+
+### filters
+ - new: kv: useful for parsing log formats taht use 'foo=bar baz=fizz' and
+   similar key-value-like things.
+ - new: urldecode: a filter for urldecoding fields in your event. (Patch by
+   Joey Imbasciano, LOGSTASH-612)
+ - new: geoip: query a local geoip database for location information (Patch by
+   Avishai Ish-Shalom, #208)
+ - improvement: zeromq: an empty reply is now considered as a 'cancel this
+   event' operation (LOGSTASH-574)
+ - bugfix: mutate: fix bug in uppercase and lowercase feature that would
+   prevent it from actually doing the uppercasing/lowercasing.
+ - improvement: mutate: do the 'remove' action last (LOGSTASH-543)
+ - feature: grok: new 'singles' config option which, when true, stores
+   single-value fields simply as a single value rather than as an array, like
+   [value]. (LOGSTASH-185)
+ - grok patterns: the URIPARAM pattern now includes pipe '|' as a valid
+   character. (Patch by Chris Mague)
+ - grok patterns: improve haproxy log patterns (Patch by Kevin Nuckolls)
+ - grok patterns: include 'FATAL' as a valid LOGLEVEL match
+   (patch by Corry Haines)
+ - grok patterns: 'ZONE' is no longer captured by name in the HTTPDATE pattern
+ - new: alter: adds some conditional field modification as well as a
+   'coalesce' feature which sets the value of a field to the first non-null
+   value given in a list. (Patch by Francesco Salbaroli)
+ - improvement: date: add TAI64N support
+ - improvement: date: subsecond precision on UNIX timestamps is retained on
+   conversion (#213, Patch by Ralph Meijer)
+ - improvement: date: Add locale setting; useful for day/month name parsing.
+   (#100, Patch by Christian Schröder)
+
+### outputs
+ - new: exec: run arbitrary commands based on an event.
+ - new: sqs: Amazon SQS output (Patch by Sean Laurent, #211)
+ - bugfix: redis: [LOGSTASH-526] fix bug with password passing
+ - improvement: redis: [LOGSTASH-573] retry on failure even in batch-mode. This
+   also fixes a prior bug where an exception in batch mode would cause logstash
+   to crash. (Patch by Alex Dean)
+ - improvement: riemann: metric and ttl values in riemann_event now support
+   sprintf %{foo} values. (pull #174)
+ - improvement: stdout: new 'dots' debug_format value emits one dot per event
+   useful for tracking event rates.
+ - gelf output: correct severity level mappings (patch by Jason Koppe)
+ - xmpp output: users and rooms are separate config settings now (patch by
+   Parker DeBardelaben)
+ - improvement: redis: 'host' setting now accepts a list of hosts for failover
+   of writes should the current host go down. (#222, patch by Corry Haines)
+
+##1.1.1 (July 14, 2012)
+### Overview of this release:
+  * New input plugins: generator, heroku, pipe, ganglia, irc
+  * New output plugins: juggernaut, metricscatcher, nagios_ncsa, pipe,
+                        opentsdb, pagerduty, irc
+  * New filter plugins: zeromq, environment, xml, csv, syslog_pri
+  * Fixes for gelf output
+  * Support for more than 1 filter worker (agent argument "-w")
+
+### IMPORTANT CHANGES FOR UPGRADES FROM 1.1.0
+  - zeromq input and output rewritten
+      The previous zeromq support was an MVP. It has now been rewritten into
+      something more flexible. The configuration options have changed entirely.
+      While this is still listed as `experimental`, we don't predict any more
+      configuration syntax changes. The next release will bump this to beta.
+  - unix_timestamp
+      Previously, several plugins did not work as expected on MRI due to the
+      usage of the JRuby-only Jodatime library. We now have a contributed fix
+      for a slower parser on MRI/CRuby!
+  - elasticsearch version is now 0.19.8
+      This means your elasticsearch cluster must be running 0.19.x for
+      compatibility reasons.
+  - grok pattern %{POSINT} used to match '0' -- now it does not. If you want
+    to match non-negative integers, there is now a %{NONNEGINT} pattern.
+  - bug in file input fixed that led to an extra leading slash in @source_path.
+    Previously, file input would have @source = 'file://host//var/log/foo' and
+    @source_path = '//var/log/foo'; now @source = 'file://host/var/log/foo'
+    and @source_path = '/var/log/foo'. [LOGSTASH-501]
+  - file input now rejects relative paths. [LOGSTASH-503]
+  - event sprintf can now look inside structured field data. %{foo.bar} will
+    look in the event field "foo" (if it is a hash) for "bar".  To preserve
+    compatibility, we first look for a top-level key that matches exactly
+    (so %{foo.bar} will first look for a field named "foo.bar", then look for
+    "bar" under "foo").
+
+### general
+  - NOTE: gemspec removed; deploying logstash as a gem hasn't been supported
+    for a while.
+  - feature: logstash sub-commands "irb" and "pry" for an interactive debug
+    console, useful to debug jruby when running from the monolithic jar
+  - misc: newer cabin gem for logging
+  - misc: initial support for reporting internal metrics (currently outputs
+    to INFO log; eventually will be an internal event type)
+  - misc: added a "thread watchdog" to detect hanging filter workers, and
+    crash logstash w/an informational message
+  - misc: jar is built with jruby 1.6.7.2
+  - misc: better shutdown behavior when there are no inputs/plugins running
+  - feature: logstash web now uses relative URLs; useful if you want to
+    reverseproxy with a path other than "/"
+
+### inputs
+  - bugfix: stdin: exit plugin gracefully on EOF
+  - feature: [LOGSTASH-410] - inputs can now be duplicated with the
+    'threads' parameter (where supported)
+  - bugfix: [LOGSTASH-490] - include cacert.pem in jar for twitter input
+  - feature: [LOGSTASH-139] - support for IRC
+
+### filters
+  - feature: all filters support 'remove_tag' (remove tags on success)
+  - feature: all filters support 'exclude_tags' (inverse of 'tags')
+  - bugfix: [LOGSTASH-300] - bump grok pattern replace limit to 1000,
+    fixes "deep recursion pattern compilation" problems
+  - bugfix: [LOGSTASH-375] - fix bug in grep: don't drop when field is nil
+    and negate is true
+  - bugfix: [LOGSTASH-386] - fix some grok patterns for haproxy
+  - bugfix: [LOGSTASH-446] - fix grok %{QUOTEDSTRING} pattern, should fix
+    some grok filter hangs
+  - bugfix: some enhancements to grok pattern %{COMBINEDAPACHELOG}
+  - bugfix: grok: %{URIPATH} and %{URIPARAM} enhancements
+  - feature: grok: add %{UUID} pattern
+  - bugfix: grok: better error message when expanding unknown %{pattern}
+  - feature: mutate: now supports a 'gsub' operation for applying a regexp
+    substitution on event fields
+
+### outputs
+  - bugfix: [LOGSTASH-351] - fix file input on windows
+  - feature: [LOGSTASH-356] - make file output flush intervals configurable
+  - feature: [LOGSTASH-392] - add 'field' attribute to restrict which fields
+    get sent to an output
+  - feature: [LOGSTASH-374] - add gzip support to file output
+  - bugfix: elastic search river now respects exchange_type and queue_name
+  - bugfix: ganglia plugin now respects metric_type
+  - bugfix: GELF output facility fixes; now defaults to 'logstash-gelf'
+  - feature: [LOGSTASH-139] - support for IRC
+  - bugfix: es_river: check river status after creation to verify status
+  - feature: es: allow setting node_name
+  - feature: redis: output batching for list mode
+
+## 1.1.0.1 (January 30, 2012)
+### Overview of this release:
+    * date filter bugfix: [LOGSTASH-438] - update joda-time to properly
+      handle leap days
+
+### 1.1.0 (January 30, 2012)
+  ## Overview of this release:
+    * New input plugins: zeromq, gelf
+    * New filter plugins: mutate, dns, json
+    * New output plugins: zeromq, file
+    * The logstash agent now runs also in MRI 1.9.2 and above
+
+    This is a large release due to the longevity of the 1.1.0 betas.
+    We don't like long releases and will try to avoid this in the future.
+
+### IMPORTANT CHANGES FOR UPGRADES FROM 1.0.x
+    - grok filter: named_captures_only now defaults to true
+        This means simple patterns %{NUMBER} without any other name will
+        now not be included in the field set. You can revert to the old
+        behavior by setting 'named_captures_only => false' in your grok
+        filter config.
+    - grok filter: now uses Ruby's regular expression engine
+        The previous engine was PCRE. It is now Oniguruma (Ruby). Their
+        syntaxes are quite similar, but it is something to be aware of.
+    - elasticsearch library upgraded to 0.18.7
+        This means you will need to upgrade your elasticsearch servers,
+        if any, to the this version: 0.18.7
+    - AMQP parameters and usage have changed for the better. You might
+      find that your old (1.0.x) AMQP logstash configs do not work.
+      If so, please consult the documentation for that plugin to find
+      the new names of the parameters.
+
+### general
+  - feature: [LOGSTASH-158] - MRI-1.9 compatible (except for some
+    plugins/functions which will throw a compatibility exception) This means
+    you can use most of the logstash agent under standard ruby.
+  - feature: [LOGSTASH-118] - logstash version output (--version or -V for
+    agent)
+  - feature: all plugins now have a 'plugin status' indicating the expectation
+    of stability, successful deployment, and rate of code change. If you
+    use an unstable plugin, you will now see a warning message on startup.
+  - bugfix: AMQP overhaul (input & output), please see docs for updated
+    config parameters.
+  - bugfix: [LOGSTASH-162,177,196] make sure plugin-contained global actions
+    happen serially across all plugins (with a mutex)
+  - bugfix: [LOGSTASH-286] - logstash agent should not truncate logfile on
+    startup
+  - misc: [LOGSTASH-160] - now use gnu make instead of rake.
+  - misc: now using cabin library for all internal logging
+  - test: use minitest
+  - upgrade: now using jruby in 1.9 mode
+
+### inputs
+  - feature: zeromq input. Requires you have libzmq installed on your system.
+  - feature, bugfix: [LOGSTASH-40,65,234,296]: much smarter file watching for
+    file inputs. now supports globs, keeps state between runs, can handle
+    truncate, log rotation, etc. no more inotify is required, either (file
+    input now works on all platforms)
+  - feature: [LOGSTASH-172,201] - syslog input accepts ISO8601 timestamps
+  - feature: [LOGSTASH-159] - TCP input lets you configure what identifies
+    an input stream to the multiline filter (unique per host, or connection)
+  - feature: [LOGSTASH-168] - add new GELF input plugin
+  - bugfix: [LOGSTASH-8,233] - fix stomp input
+  - bugfix: [LOGSTASH-136,142] - file input should behave better with log rotations
+  - bugfix: [LOGSTASH-249] - Input syslog force facility type to be an integer
+  - bugfix: [LOGSTASH-317] - fix file input not to crash when a file
+    is unreadable
+
+### filters
+  - feature: [LOGSTASH-66,150]: libgrok re-written in pure ruby (no more
+    FFI / external libgrok.so dependency!)
+  - feature: [LOGSTASH-292,316] - Filters should run on all events if no condition
+    is applied (type, etc).
+  - feature: [LOGSTASH-292,316] - Filters can now act on specific tags (or sets
+    of tags).
+  - bugfix: [LOGSTASH-285] - for grok, add 'keep_empty_captures' setting to
+    allow dropping of empty captures. This is true by default.
+  - feature: [LOGSTASH-219] - support parsing unix epoch times
+  - feature: [LOGSTASH-207] - new filter to parse a field as json merging it
+    into the event.
+  - feature: [LOGSTASH-267,254] - add DNS filter for doing forward or
+    reverse DNS on an event field
+  - feature: [LOGSTASH-57] - add mutate filter to help with manipulating
+    event field content and type
+
+### outputs
+  - feature: zeromq output. Requires you have libzmq installed on your system.
+  - feature: new file output plugin
+  - bugfix: [LOGSTASH-307] embedded elasticsearch now acts as a full ES server;
+    previously embedded was only accessible from within the logstash process.
+  - bugfix: [LOGSTASH-302] - logstash's log level (-v, -vv flags) now control
+    the log output from the elasticsearch client via log4j.
+  - bugfix: many gelf output enhancements and bugfixes
+  - feature: [LOGSTASH-281] - add https support to loggly output
+  - bugfix: [LOGSTASH-167] - limit number of in-flight requests to the
+    elasticsearch node to avoid creating too many threads (one thread per
+    pending write request)
+  - bugfix: [LOGSTASH-181] - output/statsd: set sender properly
+  - bugfix: [LOGSTASH-173] - GELF output can throw an exception during gelf notify
+  - bugfix: [LOGSTASH-182] - grep filter should act on all events if no type is
+    specified.
+  - bugfix: [LOGSTASH-309] - file output can now write to named pipes (fifo)
+
+
+## 1.0.17 (Aug 12, 2011)
+  - Bugs fixed
+    - [LOGSTASH-147] - grok filter incorrectly adding fields when a match failed
+    - [LOGSTASH-151] - Fix bug in routing keys on AMQP
+    - [LOGSTASH-156] - amqp issue with 1.0.16?
+
+  - Improvement
+    - [LOGSTASH-148] - AMQP input should allow queue name to be specified separately from exchange name
+    - [LOGSTASH-157] - Plugin doc generator should make regexp config names more readable
+
+  - New Feature
+    - [LOGSTASH-153] - syslog input: make timestamp an optional field
+    - [LOGSTASH-154] - Make error reporting show up in the web UI
+
+## 1.0.16 (Aug 18, 2011)
+  - Fix elasticsearch client problem with 1.0.15 - jruby-elasticsearch gem
+    version required is now 0.0.10 (to work with elasticsearch 0.17.6)
+
+## 1.0.15 (Aug 18, 2011)
+  - IMPORTANT: Upgraded to ElasticSearch 0.17.6 - this brings a number of bug
+    fixes including an OOM error caused during high index rates in some
+    conditions.
+    NOTE: You *must* use same main version of elasticsearch as logstash does,
+    so if you are still using elasticsearch server 0.16.x - you need to upgrade
+    your server before the elasticsearch output will work. If you are using
+    the 'embedded' elasticsearch feature of logstash, you do not need to make
+    any changes.
+  - feature: tcp input and output plugins can now operate in either client
+    (connect) or server (listen) modes.
+  - feature: new output plugin "statsd" which lets you increment or record
+    timings from your logs to a statsd agent
+  - feature: new redis 'pattern_channel' input support for PSUBSCRIBE
+  - feature: new output plugin "graphite" for taking metrics from events and
+    shipping them off to your graphite/carbon server.
+  - feature: new output plugin "ganglia" for shipping metrics to ganglia
+    gmond server.
+  - feature: new output plugin "xmpp" for shipping events over jabber/xmpp
+  - feature: new input plugin "xmpp" for receiving events over jabber/xmpp
+  - feature: amqp input now supports routing keys.
+    https://logstash.jira.com/browse/LOGSTASH-122
+  - feature: amqp output now supports setting routing key dynamically.
+    https://logstash.jira.com/browse/LOGSTASH-122
+  - feature: amqp input/output both now support SSL.
+    https://logstash.jira.com/browse/LOGSTASH-131
+  - feature: new input plugin "exec" for taking events from executed commands
+    like shell scripts or other tools.
+  - feature: new filter plugin "split" for splitting one event into multiple.
+    It was written primarily for the new "exec" input to allow you to split
+    the output of a single command run by line into multiple events.
+  - misc: upgraded jar releases to use JRuby 1.6.3
+  - bugfix: syslog input shouldn't crash anymore on weird network behaviors
+    like portscanning, etc.
+    https://logstash.jira.com/browse/LOGSTASH-130
+
+## 1.0.14 (Jul 1, 2011)
+  - feature: new output plugin "loggly" which lets you ship logs to loggly.com
+  - feature: new output plugin "zabbix" - similar to the nagios output, but
+    works with the Zabbix monitoring system. Contributed by Johan at
+    Mach Technology.
+  - feature: New agent '-e' flag which lets you specify a config in a string.
+    If you specify no 'input' plugins, default is stdin { type => stdin }
+    If you specify no 'output' plugins, default is stdout { debug => true }
+    This is intended to be used for hacking with or debugging filters, but
+    you can specify an entire config here if you choose.
+  - feature: Agent '-f' flag now supports directories and globs. If you specify
+    a directory, all files in that directory will be loaded as a single config.
+    If you specify a glob, all files matching that glob will be loaded as a
+    single config.
+  - feature: gelf output now allows you to override the 'sender'. This defaults
+    to the source host originating the event, but can be set to anything now.
+    It supports dynamic values, so you can use fields from your event as the
+    sender. Contributed by John Vincent
+    Issue: https://github.com/logstash/logstash/pull/30
+  - feature: added new feature to libgrok that allows you to define patterns
+    in-line, like "%{FOO=\d+}" defines 'FOO' match \d+ and captures as such.
+    To use this new feature, you must upgrade libgrok to at least 1.20110630
+    Issue: https://logstash.jira.com/browse/LOGSTASH-94
+  - feature: grok filter now supports 'break_on_match' defaulting to true
+    (this was the original behavior). If you set it to false, it will attempt
+    to match all patterns and create new fields as normal. If left default
+    (true), it will break after the first successful match.
+  - feature: grok filter now supports parsing any field. You can do either of
+    these: grok { match => [ "fieldname", "pattern" ] }
+    or this: grok { fieldname => "pattern" }
+    The older 'pattern' attribute still means the same thing, and is equivalent
+    to this: grok { match => [ "@message", "pattern" ] }
+    Issue: https://logstash.jira.com/browse/LOGSTASH-101
+  - feature: elasticsearch - when embedded is true, you can now set the
+    'embedded_http_port' to configure which port the embedded elasticsearch
+    server listens on. This is only valid for the embedded elasticsearch
+    configuration. https://logstash.jira.com/browse/LOGSTASH-117
+  - bugfix: amqp input now reconnects properly when the amqp broker restarts.
+  - bugfix: Fix bug in gelf output when a fields were not arrays but numbers.
+    Issue: https://logstash.jira.com/browse/LOGSTASH-113
+  - bugfix: Fix a bug in syslog udp input due to misfeatures in Ruby's URI
+    class. https://logstash.jira.com/browse/LOGSTASH-115
+  - misc: jquery and jquery ui now ship with logstash; previously they were
+    loaded externally
+  - testing: fixed some bugs in the elasticsearch test itself, all green now.
+  - testing: fixed logstash-test to now run properly
+
+## 1.0.12 (Jun 9, 2011)
+  - misc: clean up some excess debugging output
+  - feature: for tcp input, allow 'data_timeout => -1' to mean "never time out"
+
+## 1.0.11 (Jun 9, 2011)
+  - deprecated: The redis 'name' and 'queue' options for both input and output
+    are now deprecated. They will be removed in a future version.
+  - feature: The redis input and output now supports both lists and channels.
+  - feature: Refactor runner to allow you to run multiple things in a single
+    process.  You can end each instance with '--' flag. For example, to run one
+    agent and one web instance:
+      % java -jar logstash-blah.jar agent -f myconfig -- web
+  - feature: Add 'embedded' option to the elasticsearch output:
+      elasticsearch { embedded => true }
+    Default is false. If true, logstash will run an elasticsearch server
+    in the same process as logstash. This is really useful if you are just
+    starting out or only need one one elasticsearch server.
+  - feature: Added a logstash web backend feature for elasticsearch that tells
+    logstash to use the 'local' (in process) elasticsearch:
+      --backend elasticsearch:///?local
+  - feature: Added 'named_captures_only' option to grok filter. This will have
+    logstash only keep the captures you give names to - for example %{NUMBER}
+    won't be kept, but %{NUMBER:bytes} will be.
+  - feature: Add 'bind_host' option to elasticsearch output. This lets you choose the
+    address ElasticSearch client uses to bind to - useful if you have a
+    multihomed server.
+  - feature: The mongodb output now supports authentication
+  - bugfix: Fix bug in GELF output that caused the gelf short_message to be set as an
+    array if it came from a grok value. The short_message field should only
+    now be a string properly.
+  - bugfix: Fix bug in grep filter that would drop/cancel events if you had
+    more than one event type flowing through filters and didn't have a grep
+    filter defined for each type.
+  - misc: Updated gem dependencies (tests still pass)
+  - misc: With the above two points, you can now run a single logstash process
+    that includes elasticsearch server, logstash agent, and logstash web.
+
+## 1.0.10 (May 23, 2011)
+  - Fix tcp input bug (LOGSTASH-88) that would drop connections.
+  - Grok patterns_dir (filter config) and --grok-patterns-dir (cmdline opt)
+    are now working.
+  - GELF output now properly sends extra fields from the log event (prefixed
+    with a "_") and sets timestamp to seconds-since-epoch (millisecond
+    precision and time zone information is lost, but this is the format GELF
+    asks for).
+  - Inputs support specifying the format of input data (see "format" and
+    "message_format" input config parameters).
+  - Grok filter no longer incorrectly tags _grokparsefailure when more than
+    one grok filter is enabled (for multiple types) or when an event has
+    no grok configuration for it's type.
+  - Fix bug where an invalid HTTP Referer: would break grok parsing of the
+    log line (used to expect %{URI}). Since Referer: is not sanitized in
+    the HTTP layer, we cannot assume it will be a well formed %{URI}.
+
+## 1.0.9 (May 18, 2011)
+  - Fix crash bug caused by refactoring that left 'break' calls in code
+    that no longer used loops.
+
+## 1.0.8 (May 17, 2011)
+  - Remove beanstalk support because the library (beanstalk-client) is GPL3. I
+    am not a lawyer, but I'm not waiting around to have someone complain about
+    license incompatibilities.
+  - fix bug in jar build
+
+## 1.0.7 (May 16, 2011)
+  - logstash 'web' now allows you to specify the elasticsearch clustername;
+    --backend elasticsearch://[host[:port]]/[clustername]
+  - GELF output now supports dynamic strings for level and facility
+    https://logstash.jira.com/browse/LOGSTASH-83
+  - 'amqp' output supports persistent messages over AMQP, now. Tunable.
+    https://logstash.jira.com/browse/LOGSTASH-81
+  - Redis input and output are now supported. (Contributed by dokipen)
+  - Add shutdown processing. Shutdown starts when all inputs finish (like
+    stdin) The sequence progresses using the same pipeline as the
+    inputs/filters/outputs, so all in-flight events should finish getting
+    processed before the final shutdown event makes it's way to the outputs.
+  - Add retries to unhandled input exceptions (LOGSTASH-84)
+
+## 1.0.6 (May 11, 2011)
+  * Remove 'sigar' from monolithic jar packaging. This removes a boatload of
+    unnecessary warning messages on startup whenever you use elasticsearch
+    output or logstash-web.
+    Issue: https://logstash.jira.com/browse/LOGSTASH-79
+
+## 1.0.5 (May 10, 2011)
+  * fix queues when durable is set to true
+
+## 1.0.4 (May 9, 2011)
+  * Fix bugs in syslog input
+
+## 1.0.2 (May 8, 2011)
+  * Fix default-value handling for configs when the validation type is
+    'password'
+
+## 1.0.1 (May 7, 2011)
+  * Fix password auth for amqp and stomp (Reported by Luke Macken)
+  * Fix default elasticsearch target for logstash-web (Reported by Donald Gordon)
+
+## 1.0.0 (May 6, 2011)
+  * First major release.
diff --git a/Gemfile b/Gemfile
index 55a261ee2f9..b77f7f7e690 100644
--- a/Gemfile
+++ b/Gemfile
@@ -2,9 +2,120 @@
 # If you modify this file manually all comments and formatting will be lost.
 
 source "https://rubygems.org"
-gem "logstash-core", "2.0.0.dev", :path => "."
+gem "logstash-core", "1.5.2.2"
 gem "file-dependencies", "0.1.6"
-gem "ci_reporter", "1.9.3", :group => :development
+gem "ci_reporter_rspec", "1.0.0", :group => :development
 gem "simplecov", :group => :development
 gem "coveralls", :group => :development
-gem "rspec", "~> 2.14.0", :group => :development
+gem "rspec", "~> 3.1.0", :group => :development
+gem "logstash-devutils", "~> 0", :group => :development
+gem "benchmark-ips", :group => :development
+gem "octokit", "3.8.0", :group => :build
+gem "stud", "~> 0.0.19", :group => :build
+gem "fpm", "~> 1.3.3", :group => :build
+gem "rubyzip", "~> 1.1.7", :group => :build
+gem "gems", "~> 0.8.3", :group => :build
+gem "logstash-input-heartbeat"
+gem "logstash-output-zeromq"
+gem "logstash-codec-collectd"
+gem "logstash-output-xmpp"
+gem "logstash-codec-dots"
+gem "logstash-codec-edn"
+gem "logstash-codec-edn_lines"
+gem "logstash-codec-fluent"
+gem "logstash-codec-es_bulk"
+gem "logstash-codec-graphite"
+gem "logstash-codec-json"
+gem "logstash-codec-json_lines"
+gem "logstash-codec-line"
+gem "logstash-codec-msgpack"
+gem "logstash-codec-multiline"
+gem "logstash-codec-netflow"
+gem "logstash-codec-oldlogstashjson"
+gem "logstash-codec-plain"
+gem "logstash-codec-rubydebug"
+gem "logstash-filter-anonymize"
+gem "logstash-filter-checksum"
+gem "logstash-filter-clone"
+gem "logstash-filter-csv"
+gem "logstash-filter-date"
+gem "logstash-filter-dns"
+gem "logstash-filter-drop"
+gem "logstash-filter-fingerprint"
+gem "logstash-filter-geoip"
+gem "logstash-filter-grok"
+gem "logstash-filter-json"
+gem "logstash-filter-kv"
+gem "logstash-filter-metrics"
+gem "logstash-filter-multiline"
+gem "logstash-filter-mutate"
+gem "logstash-filter-ruby"
+gem "logstash-filter-sleep"
+gem "logstash-filter-split"
+gem "logstash-filter-syslog_pri"
+gem "logstash-filter-throttle"
+gem "logstash-filter-urldecode"
+gem "logstash-filter-useragent"
+gem "logstash-filter-uuid"
+gem "logstash-filter-xml"
+gem "logstash-input-couchdb_changes"
+gem "logstash-input-elasticsearch"
+gem "logstash-input-eventlog"
+gem "logstash-input-exec"
+gem "logstash-input-file"
+gem "logstash-input-ganglia"
+gem "logstash-input-gelf"
+gem "logstash-input-generator"
+gem "logstash-input-graphite"
+gem "logstash-input-http"
+gem "logstash-input-imap"
+gem "logstash-input-irc"
+gem "logstash-input-log4j"
+gem "logstash-input-lumberjack"
+gem "logstash-input-pipe"
+gem "logstash-input-rabbitmq"
+gem "logstash-input-redis"
+gem "logstash-input-s3"
+gem "logstash-input-snmptrap"
+gem "logstash-input-sqs"
+gem "logstash-input-stdin"
+gem "logstash-input-syslog"
+gem "logstash-input-tcp"
+gem "logstash-input-twitter"
+gem "logstash-input-udp"
+gem "logstash-input-unix"
+gem "logstash-input-xmpp"
+gem "logstash-input-zeromq"
+gem "logstash-input-kafka"
+gem "logstash-output-cloudwatch"
+gem "logstash-output-csv"
+gem "logstash-output-elasticsearch"
+gem "logstash-output-elasticsearch_http"
+gem "logstash-output-email"
+gem "logstash-output-exec"
+gem "logstash-output-file"
+gem "logstash-output-ganglia"
+gem "logstash-output-gelf"
+gem "logstash-output-graphite"
+gem "logstash-output-hipchat"
+gem "logstash-output-http"
+gem "logstash-output-irc"
+gem "logstash-output-juggernaut"
+gem "logstash-output-lumberjack"
+gem "logstash-output-nagios"
+gem "logstash-output-nagios_nsca"
+gem "logstash-output-null"
+gem "logstash-output-opentsdb"
+gem "logstash-output-pagerduty"
+gem "logstash-output-pipe"
+gem "logstash-output-rabbitmq"
+gem "logstash-output-redis"
+gem "logstash-output-s3"
+gem "logstash-output-sns"
+gem "logstash-output-sqs"
+gem "logstash-output-statsd"
+gem "logstash-output-stdout"
+gem "logstash-output-tcp"
+gem "logstash-output-udp"
+gem "logstash-output-kafka"
+gem "flores", "~> 0.0.4", :group => :development
diff --git a/Gemfile.jruby-1.9.lock b/Gemfile.jruby-1.9.lock
index e6b2c9a1c53..1685bc137df 100644
--- a/Gemfile.jruby-1.9.lock
+++ b/Gemfile.jruby-1.9.lock
@@ -1,27 +1,40 @@
-PATH
-  remote: .
-  specs:
-    logstash-core (2.0.0.dev-java)
-      cabin (~> 0.7.0)
-      clamp (~> 0.6.5)
-      filesize (= 0.0.4)
-      i18n (= 0.6.9)
-      jrjackson (~> 0.2.8)
-      minitar (~> 0.5.4)
-      pry (~> 0.10.1)
-      stud (~> 0.0.19)
-      treetop (< 1.5.0)
-
 GEM
   remote: https://rubygems.org/
   specs:
+    addressable (2.3.8)
+    arr-pm (0.0.10)
+      cabin (> 0)
+    atomic (1.1.99-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.6.1)
+    aws-sdk (2.1.2)
+      aws-sdk-resources (= 2.1.2)
+    aws-sdk-core (2.1.2)
+      jmespath (~> 1.0)
+    aws-sdk-resources (2.1.2)
+      aws-sdk-core (= 2.1.2)
+    aws-sdk-v1 (1.64.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+    backports (3.6.4)
+    benchmark-ips (2.2.0)
+    bindata (2.1.0)
+    buftok (0.2.0)
     builder (3.2.2)
     cabin (0.7.1)
-    ci_reporter (1.9.3)
+    childprocess (0.5.6)
+      ffi (~> 1.0, >= 1.0.11)
+    ci_reporter (2.0.0)
       builder (>= 2.1.2)
+    ci_reporter_rspec (1.0.0)
+      ci_reporter (~> 2.0)
+      rspec (>= 2.14, < 4)
+    cinch (2.2.5)
     clamp (0.6.5)
     coderay (1.1.0)
-    coveralls (0.8.1)
+    concurrent-ruby (0.8.0-java)
+    coveralls (0.8.2)
       json (~> 1.8)
       rest-client (>= 1.6.8, < 2)
       simplecov (~> 0.10.0)
@@ -31,62 +44,654 @@ GEM
     docile (1.1.5)
     domain_name (0.5.24)
       unf (>= 0.0.5, < 1.0.0)
-    ffi (1.9.8-java)
+    edn (1.0.7)
+    elasticsearch (1.0.12)
+      elasticsearch-api (= 1.0.12)
+      elasticsearch-transport (= 1.0.12)
+    elasticsearch-api (1.0.12)
+      multi_json
+    elasticsearch-transport (1.0.12)
+      faraday
+      multi_json
+    equalizer (0.0.11)
+    faraday (0.9.1)
+      multipart-post (>= 1.2, < 3)
+    ffi (1.9.9-java)
+    ffi-rzmq (2.0.4)
+      ffi-rzmq-core (>= 1.0.1)
+    ffi-rzmq-core (1.0.3)
+      ffi (~> 1.9)
     file-dependencies (0.1.6)
       minitar
     filesize (0.0.4)
+    filewatch (0.6.4)
+    flores (0.0.4)
+    fpm (1.3.3)
+      arr-pm (~> 0.0.9)
+      backports (>= 2.6.2)
+      cabin (>= 0.6.0)
+      childprocess
+      clamp (~> 0.6)
+      ffi
+      json (>= 1.7.7)
+    ftw (0.0.44)
+      addressable
+      backports (>= 2.6.2)
+      cabin (> 0)
+      http_parser.rb (~> 0.6)
+    gelf (1.3.2)
+      json
+    gelfd (0.2.0)
+    gem_publisher (1.5.0)
+    gems (0.8.3)
+    geoip (1.6.1)
+    gmetric (0.1.3)
+    hitimes (1.2.2-java)
+    http (0.6.4)
+      http_parser.rb (~> 0.6.0)
     http-cookie (1.0.2)
       domain_name (~> 0.5)
+    http_parser.rb (0.6.0-java)
     i18n (0.6.9)
-    jrjackson (0.2.8)
-    json (1.8.2-java)
+    insist (1.0.0)
+    jar-dependencies (0.1.15)
+    jls-grok (0.11.2)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.22)
+    jmespath (1.0.2)
+      multi_json (~> 1.0)
+    jrjackson (0.2.9)
+    jruby-kafka (1.4.0-java)
+      jar-dependencies (~> 0)
+      ruby-maven (~> 3.1)
+    jruby-win32ole (0.8.5)
+    json (1.8.3-java)
+    kramdown (1.7.0)
+    logstash-codec-collectd (1.0.1)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-dots (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-edn (1.0.0)
+      edn
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-edn_lines (1.0.0)
+      edn
+      logstash-codec-line
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-es_bulk (1.0.0)
+      logstash-codec-line
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-fluent (1.0.0-java)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      msgpack-jruby
+    logstash-codec-graphite (1.0.0)
+      logstash-codec-line
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-json (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-json_lines (1.0.0)
+      logstash-codec-line
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-line (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-msgpack (1.0.0-java)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      msgpack-jruby
+    logstash-codec-multiline (1.0.0)
+      jls-grok (~> 0.11.1)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-patterns-core
+    logstash-codec-netflow (1.0.0)
+      bindata (>= 1.5.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-oldlogstashjson (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-plain (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-codec-rubydebug (1.0.0)
+      awesome_print
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-core (1.5.2.2-java)
+      cabin (~> 0.7.0)
+      clamp (~> 0.6.5)
+      filesize (= 0.0.4)
+      gems (~> 0.8.3)
+      i18n (= 0.6.9)
+      jrjackson (~> 0.2.9)
+      minitar (~> 0.5.4)
+      pry (~> 0.10.1)
+      stud (~> 0.0.19)
+      thread_safe (~> 0.3.5)
+      treetop (< 1.5.0)
+    logstash-devutils (0.0.15-java)
+      gem_publisher
+      insist (= 1.0.0)
+      kramdown
+      minitar
+      rake
+      rspec (~> 3.1.0)
+      stud (>= 0.0.20)
+    logstash-filter-anonymize (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      murmurhash3
+    logstash-filter-checksum (1.0.1)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-clone (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-csv (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-date (1.0.0)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-input-generator
+      logstash-output-null
+    logstash-filter-dns (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-drop (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-fingerprint (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      murmurhash3
+    logstash-filter-geoip (1.0.1)
+      geoip (>= 1.3.2)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-grok (1.0.0)
+      jls-grok (~> 0.11.1)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-patterns-core
+    logstash-filter-json (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-kv (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-metrics (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      metriks
+      thread_safe
+    logstash-filter-multiline (1.0.0)
+      jls-grok (~> 0.11.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-filter-mutate
+      logstash-patterns-core
+    logstash-filter-mutate (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-filter-grok
+      logstash-patterns-core
+    logstash-filter-ruby (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-filter-date
+    logstash-filter-sleep (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-split (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-syslog_pri (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-throttle (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-urldecode (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-useragent (1.0.1)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      user_agent_parser (>= 2.0.0)
+    logstash-filter-uuid (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-filter-xml (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      nokogiri
+      xml-simple
+    logstash-input-couchdb_changes (1.0.0)
+      json
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-elasticsearch (1.0.0)
+      elasticsearch (~> 1.0, >= 1.0.6)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-eventlog (1.0.0-java)
+      jruby-win32ole
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-exec (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-file (1.0.0)
+      addressable
+      filewatch (~> 0.6, >= 0.6.4)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-ganglia (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-gelf (1.0.0)
+      gelf (= 1.3.2)
+      gelfd (= 0.2.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-generator (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-graphite (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-input-tcp
+    logstash-input-heartbeat (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+      stud
+    logstash-input-http (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+      puma
+      stud
+    logstash-input-imap (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+      mail
+      stud
+    logstash-input-irc (1.0.0)
+      cinch
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-kafka (1.0.0)
+      jruby-kafka (>= 1.2.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-log4j (1.0.0-java)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-lumberjack (1.0.2)
+      concurrent-ruby
+      jls-lumberjack (>= 0.0.20)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-pipe (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-rabbitmq (1.0.0-java)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+      march_hare (~> 2.5.1)
+    logstash-input-redis (1.0.0)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+      redis
+    logstash-input-s3 (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-input-snmptrap (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      snmp
+    logstash-input-sqs (1.0.0)
+      aws-sdk
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-stdin (1.0.0)
+      concurrent-ruby
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-syslog (1.0.0)
+      concurrent-ruby
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-filter-date
+      logstash-filter-grok
+      thread_safe
+    logstash-input-tcp (1.0.0)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-twitter (1.0.1)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      twitter (= 5.12.0)
+    logstash-input-udp (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-unix (1.0.0)
+      logstash-codec-line
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-input-xmpp (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+      xmpp4r (= 0.5)
+    logstash-input-zeromq (1.0.0)
+      ffi-rzmq (~> 2.0.4)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-mixin-aws (1.0.0)
+      aws-sdk (~> 2.1.0)
+      aws-sdk-v1 (>= 1.61.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-cloudwatch (1.0.0)
+      aws-sdk
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+      rufus-scheduler (~> 3.0.9)
+    logstash-output-csv (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-filter-json
+      logstash-output-file
+    logstash-output-elasticsearch (1.0.1-java)
+      cabin (~> 0.6)
+      concurrent-ruby
+      elasticsearch (~> 1.0, >= 1.0.10)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      manticore (~> 0.3)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-elasticsearch_http (1.0.0)
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-output-elasticsearch
+    logstash-output-email (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      mail (~> 2.6.0, >= 2.6.3)
+    logstash-output-exec (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-file (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-input-generator
+    logstash-output-ganglia (1.0.0)
+      gmetric (= 0.1.3)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-gelf (1.0.0)
+      gelf (= 1.3.2)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-graphite (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-hipchat (1.0.0)
+      ftw (~> 0.0.40)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-http (1.0.0)
+      ftw (~> 0.0.40)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-irc (1.0.0)
+      cinch
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-juggernaut (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      redis
+    logstash-output-kafka (1.0.0)
+      jruby-kafka (>= 1.1.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-lumberjack (1.0.0)
+      jls-lumberjack (>= 0.0.20)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-nagios (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-nagios_nsca (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-null (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-opentsdb (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-pagerduty (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-pipe (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-rabbitmq (1.0.0-java)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      march_hare (~> 2.5.1)
+    logstash-output-redis (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      redis
+      stud
+    logstash-output-s3 (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-output-sns (2.0.1)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-sqs (1.0.0)
+      aws-sdk
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-mixin-aws
+      stud
+    logstash-output-statsd (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      logstash-input-generator
+      statsd-ruby (= 1.2.0)
+    logstash-output-stdout (1.0.0)
+      logstash-codec-line
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-tcp (1.0.0)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+      stud
+    logstash-output-udp (1.0.0)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-output-xmpp (1.0.0)
+      logstash-core (>= 1.4.0, < 2.0.0)
+      xmpp4r (= 0.5)
+    logstash-output-zeromq (1.0.0)
+      ffi-rzmq (~> 2.0.4)
+      logstash-codec-json
+      logstash-core (>= 1.4.0, < 2.0.0)
+    logstash-patterns-core (0.1.10)
+      logstash-core (>= 1.4.0, < 2.0.0)
+    mail (2.6.3)
+      mime-types (>= 1.16, < 3)
+    manticore (0.4.1-java)
+      addressable (~> 2.3)
+    march_hare (2.5.1-java)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
     method_source (0.8.2)
-    mime-types (2.5)
+    metriks (0.9.9.7)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.6.1)
     minitar (0.5.4)
+    msgpack-jruby (1.4.1-java)
+    multi_json (1.11.1)
+    multipart-post (2.0.0)
+    murmurhash3 (0.1.6-java)
+    naught (1.0.0)
     netrc (0.10.3)
+    nokogiri (1.6.6.2-java)
+    octokit (3.8.0)
+      sawyer (~> 0.6.0, >= 0.5.3)
     polyglot (0.3.5)
     pry (0.10.1-java)
       coderay (~> 1.1.0)
       method_source (~> 0.8.1)
       slop (~> 3.4)
       spoon (~> 0.0)
+    puma (2.11.3-java)
+      rack (>= 1.1, < 2.0)
+    rack (1.6.4)
+    rake (10.4.2)
+    redis (3.2.1)
     rest-client (1.8.0)
       http-cookie (>= 1.0.2, < 2.0)
       mime-types (>= 1.16, < 3.0)
       netrc (~> 0.7)
-    rspec (2.14.1)
-      rspec-core (~> 2.14.0)
-      rspec-expectations (~> 2.14.0)
-      rspec-mocks (~> 2.14.0)
-    rspec-core (2.14.8)
-    rspec-expectations (2.14.5)
-      diff-lcs (>= 1.1.3, < 2.0)
-    rspec-mocks (2.14.6)
+    rspec (3.1.0)
+      rspec-core (~> 3.1.0)
+      rspec-expectations (~> 3.1.0)
+      rspec-mocks (~> 3.1.0)
+    rspec-core (3.1.7)
+      rspec-support (~> 3.1.0)
+    rspec-expectations (3.1.2)
+      diff-lcs (>= 1.2.0, < 2.0)
+      rspec-support (~> 3.1.0)
+    rspec-mocks (3.1.3)
+      rspec-support (~> 3.1.0)
+    rspec-support (3.1.2)
+    ruby-maven (3.3.3)
+      ruby-maven-libs (~> 3.3.1)
+    ruby-maven-libs (3.3.3)
+    rubyzip (1.1.7)
+    rufus-scheduler (3.0.9)
+      tzinfo
+    sawyer (0.6.0)
+      addressable (~> 2.3.5)
+      faraday (~> 0.8, < 0.10)
+    simple_oauth (0.3.1)
     simplecov (0.10.0)
       docile (~> 1.1.0)
       json (~> 1.8)
       simplecov-html (~> 0.10.0)
     simplecov-html (0.10.0)
     slop (3.6.0)
+    snmp (1.2.0)
     spoon (0.0.4)
       ffi
-    stud (0.0.19)
-    term-ansicolor (1.3.0)
+    statsd-ruby (1.2.0)
+    stud (0.0.20)
+    term-ansicolor (1.3.2)
       tins (~> 1.0)
     thor (0.19.1)
-    tins (1.5.1)
+    thread_safe (0.3.5-java)
+    tins (1.5.4)
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
+    twitter (5.12.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (~> 0.0.9)
+      faraday (~> 0.9.0)
+      http (~> 0.6.0)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.2)
+      thread_safe (~> 0.1)
     unf (0.1.4-java)
+    user_agent_parser (2.2.0)
+    xml-simple (1.1.5)
+    xmpp4r (0.5)
 
 PLATFORMS
   java
 
 DEPENDENCIES
-  ci_reporter (= 1.9.3)
+  benchmark-ips
+  ci_reporter_rspec (= 1.0.0)
   coveralls
   file-dependencies (= 0.1.6)
-  logstash-core (= 2.0.0.dev)!
-  rspec (~> 2.14.0)
+  flores (~> 0.0.4)
+  fpm (~> 1.3.3)
+  gems (~> 0.8.3)
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-es_bulk
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-oldlogstashjson
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-core (= 1.5.2.2)
+  logstash-devutils (~> 0)
+  logstash-filter-anonymize
+  logstash-filter-checksum
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-multiline
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-uuid
+  logstash-filter-xml
+  logstash-input-couchdb_changes
+  logstash-input-elasticsearch
+  logstash-input-eventlog
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-heartbeat
+  logstash-input-http
+  logstash-input-imap
+  logstash-input-irc
+  logstash-input-kafka
+  logstash-input-log4j
+  logstash-input-lumberjack
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-input-xmpp
+  logstash-input-zeromq
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-elasticsearch_http
+  logstash-output-email
+  logstash-output-exec
+  logstash-output-file
+  logstash-output-ganglia
+  logstash-output-gelf
+  logstash-output-graphite
+  logstash-output-hipchat
+  logstash-output-http
+  logstash-output-irc
+  logstash-output-juggernaut
+  logstash-output-kafka
+  logstash-output-lumberjack
+  logstash-output-nagios
+  logstash-output-nagios_nsca
+  logstash-output-null
+  logstash-output-opentsdb
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-statsd
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-xmpp
+  logstash-output-zeromq
+  octokit (= 3.8.0)
+  rspec (~> 3.1.0)
+  rubyzip (~> 1.1.7)
   simplecov
+  stud (~> 0.0.19)
diff --git a/NOTICE.TXT b/NOTICE.TXT
new file mode 100644
index 00000000000..0b8a9475f05
--- /dev/null
+++ b/NOTICE.TXT
@@ -0,0 +1,5 @@
+Elasticsearch
+Copyright 2012-2015 Elasticsearch
+
+This product includes software developed by The Apache Software
+Foundation (http://www.apache.org/).
\ No newline at end of file
diff --git a/benchmark/event_sprintf.rb b/benchmark/event_sprintf.rb
new file mode 100644
index 00000000000..718d1fd6149
--- /dev/null
+++ b/benchmark/event_sprintf.rb
@@ -0,0 +1,50 @@
+require "benchmark/ips"
+require "lib/logstash/event"
+
+options = { :time => 10, :warmup => 10 }
+puts "Same Event instance"
+Benchmark.ips do |x|
+  x.config(options)
+  event = LogStash::Event.new("foo" => "bar",
+                    "foobar" => "morebar")
+
+  x.report("Complex cached: Event#sprintf") { event.sprintf("/first/%{foo}/%{foobar}/%{+YYY-mm-dd}") }
+  x.report("Date only cached: Event#sprintf") { event.sprintf("%{+YYY-mm-dd}") }
+  x.report("string only cached: Event#sprintf") { event.sprintf("bleh") }
+  x.report("key only cached: Event#sprintf") { event.sprintf("%{foo}") }
+  x.compare!
+end
+
+puts "New Event on each iteration"
+Benchmark.ips do |x|
+  x.config(options)
+
+
+  x.report("Complex cached: Event#sprintf") do
+    event = LogStash::Event.new("foo" => "bar",
+                                "foobar" => "morebar")
+    event.sprintf("/first/%{foo}/%{foobar}/%{+YYY-mm-dd}")
+  end
+
+
+  x.report("Date only cached: Event#sprintf") do
+    event = LogStash::Event.new("foo" => "bar",
+                                "foobar" => "morebar")
+
+    event.sprintf("%{+YYY-mm-dd}")
+  end
+
+  x.report("string only cached: Event#sprintf") do
+    event = LogStash::Event.new("foo" => "bar",
+                                "foobar" => "morebar")
+    event.sprintf("bleh")
+  end
+
+  x.report("key only cached: Event#sprintf") do
+    event = LogStash::Event.new("foo" => "bar",
+                                "foobar" => "morebar")
+    event.sprintf("%{foo}")
+  end
+
+  x.compare!
+end
diff --git a/bin/logstash b/bin/logstash
index ec9657bfaf7..02e4446009c 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -18,6 +18,7 @@
 #   USE_DRIP=1 to force use drip
 #   DEBUG=1 to output debugging information
 
+unset CDPATH
 . "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
 setup
 
diff --git a/bin/logstash.bat b/bin/logstash.bat
index bc9b672e927..4d8a8db646e 100644
--- a/bin/logstash.bat
+++ b/bin/logstash.bat
@@ -3,7 +3,7 @@
 SETLOCAL
 
 set SCRIPT_DIR=%~dp0
-CALL %SCRIPT_DIR%\setup.bat
+CALL "%SCRIPT_DIR%\setup.bat"
 
 :EXEC
 REM is the first argument a flag? If so, assume 'agent'
diff --git a/bin/plugin b/bin/plugin
index 5971f8c0bbe..39b19b8df30 100755
--- a/bin/plugin
+++ b/bin/plugin
@@ -1,5 +1,6 @@
 #!/bin/sh
 
+unset CDPATH
 . "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
 setup
 
diff --git a/bin/plugin.bat b/bin/plugin.bat
index e51beecca65..4248264869f 100644
--- a/bin/plugin.bat
+++ b/bin/plugin.bat
@@ -3,7 +3,7 @@
 SETLOCAL
 
 set SCRIPT_DIR=%~dp0
-CALL %SCRIPT_DIR%\setup.bat
+CALL "%SCRIPT_DIR%\setup.bat"
 
 :EXEC
 if "%VENDORED_JRUBY%" == "" (
diff --git a/bin/rspec b/bin/rspec
index bae3c0d43f8..b05b117c7b9 100755
--- a/bin/rspec
+++ b/bin/rspec
@@ -1,5 +1,6 @@
 #!/bin/sh
 
+unset CDPATH
 . "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
 setup
 
diff --git a/bin/rspec.bat b/bin/rspec.bat
new file mode 100644
index 00000000000..52ce84dca74
--- /dev/null
+++ b/bin/rspec.bat
@@ -0,0 +1,15 @@
+@echo off
+
+SETLOCAL
+
+set SCRIPT_DIR=%~dp0
+CALL "%SCRIPT_DIR%\setup.bat"
+
+:EXEC
+if "%VENDORED_JRUBY%" == "" (
+  %RUBYCMD% "%LS_HOME%\lib\bootstrap\rspec.rb" %*
+) else (
+  %JRUBY_BIN% %jruby_opts% "%LS_HOME%\lib\bootstrap\rspec.rb" %*
+)
+
+ENDLOCAL
diff --git a/bin/setup.bat b/bin/setup.bat
index bf2cfac23e3..28826f0de04 100644
--- a/bin/setup.bat
+++ b/bin/setup.bat
@@ -52,7 +52,7 @@ REM Causes the JVM to dump its heap on OutOfMemory.
 set JAVA_OPTS=%JAVA_OPTS% -XX:+HeapDumpOnOutOfMemoryError
 REM The path to the heap dump location, note directory must exists and have enough
 REM space for a full heap dump.
-REM JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath=$LS_HOME/logs/heapdump.hprof
+REM JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="$LS_HOME/logs/heapdump.hprof"
 
 REM setup_vendored_jruby()
 set JRUBY_BIN="%LS_HOME%\vendor\jruby\bin\jruby"
diff --git a/ci/ci_setup.sh b/ci/ci_setup.sh
index e4dfe6899ed..fea695cb2c5 100755
--- a/ci/ci_setup.sh
+++ b/ci/ci_setup.sh
@@ -18,4 +18,5 @@ rake bootstrap # Bootstrap your logstash instance
 
 # Set up some general options for the rspec runner
 echo "--order rand" > .rspec
-echo "--format CI::Reporter::RSpec" >> .rspec
+echo "--format progress" >> .rspec
+echo "--format CI::Reporter::RSpecFormatter" >> .rspec
diff --git a/docs/asciidoc/static/command-line-flags.asciidoc b/docs/asciidoc/static/command-line-flags.asciidoc
index c3d6793860d..66784934233 100644
--- a/docs/asciidoc/static/command-line-flags.asciidoc
+++ b/docs/asciidoc/static/command-line-flags.asciidoc
@@ -1,12 +1,12 @@
 == Command-line flags
 
 [float]
-=== Agent
+=== Pipeline
 
-The Logstash agent has the following flags. (You can use the '--help' flag to
+The Logstash pipeline has the following flags. (You can use the `--help` flag to
 display this information.)
 
-[source,js]
+[source,shell]
 ----------------------------------
 -f, --config CONFIGFILE
  Load the Logstash config from a specific file, directory, or a wildcard. If
@@ -36,19 +36,33 @@ display this information.)
 -v
  *DEPRECATED: see --verbose/debug* Increase verbosity. There are multiple levels
  of verbosity available with '-vv' currently being the highest
-
---pluginpath PLUGIN_PATH
- A colon-delimited path to find other Logstash plugins in
 ----------------------------------
 
 [float]
-=== Web
+=== Plugins
 
-[source,js]
-----------------------------------
--a, --address ADDRESS
- Address on which to start webserver. Default is 0.0.0.0.
+`$LS_HOME/bin/plugin` script will be used for all plugin lifecycle interaction
 
--p, --port PORT
- Port on which to start webserver. Default is 9292.
-----------------------------------
+### Installing a plugin
+`bin/plugin install logstash-output-kafka`
+
+Alternatively, using a file location:
+`bin/plugin install /path/to/logstash-output-kafka-1.0.0.gem`
+
+### Removing a plugin
+
+`bin/plugin uninstall logstash-output-kafka`
+
+### Updating one or all plugins
+
+`bin/plugin update` will update all installed plugins
+
+`bin/plugin update logstash-output-kafka` will update only this plugin
+
+### Listing plugins
+
+`bin/plugin list`
+
+`bin/plugin list <namefragment>` Will list all plugins containing a `namefragment`
+
+`bin/plugin list --group output` Will list plugins for a particular group
diff --git a/docs/asciidoc/static/configuration.asciidoc b/docs/asciidoc/static/configuration.asciidoc
index 43a86ff2f33..7a3e1f64c8b 100644
--- a/docs/asciidoc/static/configuration.asciidoc
+++ b/docs/asciidoc/static/configuration.asciidoc
@@ -315,7 +315,7 @@ output {
 }
 ----------------------------------
 
-You can also format times using in this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[time format].
+You can also format times using this sprintf format. Instead of specifying a field name, use the `+FORMAT` syntax where `FORMAT` is a http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[time format].
 
 For example, if you want to use the file output to write to logs based on the
 hour and the 'type' field:
diff --git a/docs/asciidoc/static/deploying.asciidoc b/docs/asciidoc/static/deploying.asciidoc
new file mode 100644
index 00000000000..54f829b4fdb
--- /dev/null
+++ b/docs/asciidoc/static/deploying.asciidoc
@@ -0,0 +1,152 @@
+[[deploying-and-scaling]]
+== Deploying and Scaling Logstash
+
+As your use case for Logstash evolves, the preferred architecture at a given scale will change. This section discusses 
+a range of Logstash architectures in increasing order of complexity, starting from a minimal installation and adding 
+elements to the system. The example deployments in this section write to an Elasticsearch cluster, but Logstash can 
+write to a large variety of {logstash}output-plugins.html[endpoints].
+
+[float]
+[[deploying-minimal-install]]
+=== The Minimal Installation
+
+The minimal Logstash installation has one Logstash instance and one Elasticsearch instance. These instances are 
+directly connected. Logstash uses an {logstash}input-plugins.html[_input plugin_] to ingest data and an 
+Elasticsearch {logstash}output-plugins.html[_output plugin_] to index the data in Elasticsearch, following the Logstash 
+{logstash}pipeline.html[_processing pipeline_]. A Logstash instance has a fixed pipeline constructed at startup, 
+based on the instance’s configuration file. You must specify an input plugin. Output defaults to `stdout`, and the 
+filtering section of the pipeline, which is discussed in the next section, is optional.
+
+image::static/images/deploy_1.png[]
+
+[float]
+[[deploying-filter-threads]]
+=== Using Filters
+
+Log data is typically unstructured, often contains extraneous information that isn’t relevant to your use case, and 
+sometimes is missing relevant information that can be derived from the log contents. You can use a 
+{logstash}filter-plugins.html[filter plugin] to parse the log into fields, remove unnecessary information, and derive 
+additional information from the existing fields. For example, filters can derive geolocation information from an IP 
+address and add that information to the logs, or parse and structure arbitrary text with the 
+{logstash}plugins-filters-grok.html[grok] filter.
+
+Adding a filter plugin can significantly affect performance, depending on the amount of computation the filter plugin 
+performs, as well as on the volume of the logs being processed. The `grok` filter’s regular expression computation is 
+particularly resource-intensive. One way to address this increased demand for computing resources is to use 
+parallel processing on multicore machines. Use the `-w` switch to set the number of execution threads for Logstash 
+filtering tasks. For example the `bin/logstash -w 8` command uses eight different threads for filter processing.
+
+image::static/images/deploy_2.png[]
+
+[float]
+[[deploying-logstash-forwarder]]
+=== Using Logstash Forwarder
+
+The https://github.com/elastic/logstash-forwarder[Logstash Forwarder] is a lightweight, resource-friendly tool written 
+in Go that collects logs from files on the server and forwards these logs to other machines for processing. The 
+Logstash Forwarder uses a secure protocol called Lumberjack to communicate with a centralized Logstash instance. 
+Configure the Logstash instances that receive Lumberjack data to use the 
+{logstash}plugins-inputs-lumberjack.html[Lumberjack input plugin].
+
+The Logstash Forwarder uses the computing resources of the machine hosting the source data, and the Lumberjack input 
+plugin minimizes the resource demands on the Logstash instance, making this architecture attractive for use cases with 
+resource constraints.
+
+image::static/images/deploy_3.png[]
+
+[float]
+[[deploying-larger-cluster]]
+=== Scaling to a Larger Elasticsearch Cluster
+
+Typically, Logstash does not communicate with a single Elasticsearch node, but with a cluster that comprises several 
+nodes. Logstash can use any of the protocols that Elasticsearch supports to move data into the cluster: 
+{guide}_transport_client_versus_node_client.html[HTTP, transport, or node]. You can configure Logstash for each of 
+these communication modes by changing the value of the 
+{logstash}plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-protocol[`protocol`] setting.
+
+You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the 
+indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR 
+files and has no performance disadvantages compared to the transport or node protocols. You can secure communications 
+that use the HTTP REST APIs with the {shield}[Shield] plugin, which supports SSL and HTTP basic authentication.
+
+When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically 
+load-balance indexing requests across a 
+{logstash}plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-host[specified set of hosts] in the 
+Elasticsearch cluster. Specifying multiple Elasticsearch nodes also provides high availability for the Elasticsearch 
+cluster by routing traffic to active Elasticsearch nodes.
+
+You can also use the Elasticsearch Java APIs to serialize the data into a binary representation, using 
+the transport protocol. The transport protocol can sniff the endpoint of the request and select an 
+arbitrary client or data node in the Elasticsearch cluster. 
+
+Using the HTTP or transport protocols keep your Logstash instances separate from the Elasticsearch cluster. The node 
+protocol, by contrast, has the machine running the Logstash instance join the Elasticsearch cluster, running an 
+Elasticsearch instance. The data that needs indexing propagates from this node to the rest of the cluster. Since the 
+machine is part of the cluster, the cluster topology is available, making the node protocol a good fit for use cases 
+that use a relatively small number of persistent connections.
+
+You can also use a third-party hardware or software load balancer to handle connections between Logstash and 
+external applications.
+
+NOTE: Make sure that your Logstash configuration does not connect directly to Elasticsearch dedicated
+{ref}modules-node.html[master nodes], which perform dedicated cluster management. Connect Logstash to client or data 
+nodes to protect the stability of your Elasticsearch cluster.
+
+image::static/images/deploy_4.png[]
+
+[float]
+[[deploying-message-queueing]]
+=== Managing Throughput Spikes with Message Queueing
+
+When the data coming into a Logstash pipeline exceeds the Elasticsearch cluster's ability to ingest the data, you can 
+use a message queue as a buffer. By default, Logstash throttles incoming events when 
+indexer consumption rates fall below incoming data rates. Since this throttling can lead to events being buffered at 
+the data source, preventing backpressure with message queues becomes an important part of managing your deployment.
+
+Adding a message queue to your Logstash deployment also provides a level of protection from data loss. When a Logstash 
+instance that has consumed data from the message queue fails, the data can be replayed from the message queue to an 
+active Logstash instance.
+
+Several third-party message queues exist, such as Redis, Kafka, or RabbitMQ. Logstash provides input and output plugins 
+to integrate with several of these third-party message queues. When your Logstash deployment has a message queue 
+configured, Logstash functionally exists in two phases: shipping instances, which handles data ingestion and storage in 
+the message queue, and indexing instances, which retrieve the data from the message queue, apply any configured 
+filtering, and write the filtered data to an Elasticsearch index.
+
+image::static/images/deploy_5.png[]
+
+[float]
+[[deploying-logstash-ha]]
+=== Multiple Connections for Logstash High Availability
+
+To make your Logstash deployment more resilient to individual instance failures, you can set up a load balancer between 
+your data source machines and the Logstash cluster. The load balancer handles the individual connections to the 
+Logstash instances to ensure continuity of data ingestion and processing even when an individual instance is unavailable.
+
+image::static/images/deploy_6.png[]
+
+The architecture in the previous diagram is unable to process input from a specific type, such as an RSS feed or a 
+file, if the Logstash instance dedicated to that input type becomes unavailable. For more robust input processing, 
+configure each Logstash instance for multiple inputs, as in the following diagram:
+
+image::static/images/deploy_7.png[]
+
+This architecture parallelizes the Logstash workload based on the inputs you configure. With more inputs, you can add 
+more Logstash instances to scale horizontally. Separate parallel pipelines also increases the reliability of your stack 
+by eliminating single points of failure.
+
+[float]
+[[deploying-scaling]]
+=== Scaling Logstash
+
+A mature Logstash deployment typically has the following pipeline:
+
+* The _input_ tier consumes data from the source, and consists of Logstash instances with the proper input plugins.
+* The _message queue_ serves as a buffer to hold ingested data and serve as failover protection.
+* The _filter_ tier applies parsing and other processing to the data consumed from the message queue.
+* The _indexing_ tier moves the processed data into Elasticsearch.
+
+Any of these layers can be scaled by adding computing resources. Examine the performance of these components regularly 
+as your use case evolves and add resources as needed. When Logstash routinely throttles incoming events, consider 
+adding storage for your message queue. Alternately, increase the Elasticsearch cluster's rate of data consumption by 
+adding more Logstash indexing instances.
diff --git a/docs/asciidoc/static/getting-started-with-logstash.asciidoc b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
index 19e539a526e..ff3ec338827 100644
--- a/docs/asciidoc/static/getting-started-with-logstash.asciidoc
+++ b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
@@ -104,7 +104,7 @@ stored data when you query it.
 Now, you're probably saying, "that's all fine and dandy, but typing all my logs
 into Logstash isn't really an option, and merely seeing them spit to STDOUT
 isn't very useful." Good point. First, let's set up Elasticsearch to store the
-messages we send into Logstash. If you don't have Elasticearch already
+messages we send into Logstash. If you don't have Elasticsearch already
 installed, you can
 http://www.elastic.co/download/[download the RPM or DEB package], or install
 manually by downloading the current release tarball, by issuing the following
@@ -182,23 +182,6 @@ This should return something like the following:
 
 Congratulations! You've successfully stashed logs in Elasticsearch via Logstash.
 
-[float]
-==== Elasticsearch Plugins (an aside)
-Another very useful tool for querying your Logstash data (and Elasticsearch in
-general) is the Elasticearch-kopf plugin. (For more information about
-Elasticsearch plugins, see
-http://www.elastic.co/guide/en/elasticsearch/reference/current/modules-plugins.html[Elasticsearch plugins].)
-To install elasticsearch-kopf,  issue the following command from your
-Elasticsearch directory (the same one from which you started Elasticsearch):
-
-[source,js]
-----------------------------------
-bin/plugin -install lmenezes/elasticsearch-kopf
-----------------------------------
-Now you can go to
-http://localhost:9200/_plugin/kopf/[http://localhost:9200/_plugin/kopf/]
-to browse your Elasticsearch data, settings, and mappings!
-
 [float]
 ==== Multiple Outputs
 
diff --git a/docs/asciidoc/static/images/deploy_1.png b/docs/asciidoc/static/images/deploy_1.png
new file mode 100644
index 00000000000..9b0485bf877
Binary files /dev/null and b/docs/asciidoc/static/images/deploy_1.png differ
diff --git a/docs/asciidoc/static/images/deploy_2.png b/docs/asciidoc/static/images/deploy_2.png
new file mode 100644
index 00000000000..e3753f48fac
Binary files /dev/null and b/docs/asciidoc/static/images/deploy_2.png differ
diff --git a/docs/asciidoc/static/images/deploy_3.png b/docs/asciidoc/static/images/deploy_3.png
new file mode 100644
index 00000000000..cda4337fa9d
Binary files /dev/null and b/docs/asciidoc/static/images/deploy_3.png differ
diff --git a/docs/asciidoc/static/images/deploy_4.png b/docs/asciidoc/static/images/deploy_4.png
new file mode 100644
index 00000000000..b3134459467
Binary files /dev/null and b/docs/asciidoc/static/images/deploy_4.png differ
diff --git a/docs/asciidoc/static/images/deploy_5.png b/docs/asciidoc/static/images/deploy_5.png
new file mode 100644
index 00000000000..02bf2b76b07
Binary files /dev/null and b/docs/asciidoc/static/images/deploy_5.png differ
diff --git a/docs/asciidoc/static/images/deploy_6.png b/docs/asciidoc/static/images/deploy_6.png
new file mode 100644
index 00000000000..7bbb78dd646
Binary files /dev/null and b/docs/asciidoc/static/images/deploy_6.png differ
diff --git a/docs/asciidoc/static/images/deploy_7.png b/docs/asciidoc/static/images/deploy_7.png
new file mode 100644
index 00000000000..837d4002269
Binary files /dev/null and b/docs/asciidoc/static/images/deploy_7.png differ
diff --git a/docs/asciidoc/static/include/pluginbody.asciidoc b/docs/asciidoc/static/include/pluginbody.asciidoc
index 5f8769ffa8e..4a09f40cfb4 100644
--- a/docs/asciidoc/static/include/pluginbody.asciidoc
+++ b/docs/asciidoc/static/include/pluginbody.asciidoc
@@ -372,7 +372,7 @@ match => {
 
 
 TIP: For more asciidoc formatting tips, see the excellent reference at
-https://github.com/elasticsearch/docs#asciidoc-guide
+https://github.com/elastic/docs#asciidoc-guide
 
 [float]
 === `class` Declaration
@@ -841,7 +841,7 @@ Gem::Specification.new do |s|
   s.metadata = { "logstash_plugin" => "true", "logstash_group" => "{plugintype}" }
 
   # Gem dependencies
-  s.add_runtime_dependency 'logstash', '>= 1.4.0', '< 2.0.0'
+  s.add_runtime_dependency 'logstash-core', '>= 1.4.0', '< 2.0.0'
   s.add_development_dependency 'logstash-devutils'
 end
 ----------------------------------
@@ -899,7 +899,7 @@ Logstash plugins:
 [subs="attributes"]
 ----------------------------------
   # Gem dependencies
-  s.add_runtime_dependency 'logstash', '>= 1.4.0', '< 2.0.0'
+  s.add_runtime_dependency 'logstash-core', '>= 1.4.0', '< 2.0.0'
   s.add_development_dependency 'logstash-devutils'
 ----------------------------------
 This gemspec has a runtime dependency on the core Logstash gem and requires that
diff --git a/docs/asciidoc/static/repositories.asciidoc b/docs/asciidoc/static/repositories.asciidoc
index b19242a4083..a36860f8210 100644
--- a/docs/asciidoc/static/repositories.asciidoc
+++ b/docs/asciidoc/static/repositories.asciidoc
@@ -1,5 +1,3 @@
-:branch: 1.4
-
 [[package-repositories]]
 == Package Repositories
 
@@ -7,13 +5,13 @@ We also have repositories available for APT and YUM based distributions. Note
 that we only provide binary packages, but no source packages, as the packages
 are created as part of the Logstash build.
 
-We have split the major versions in separate urls to avoid accidental upgrades
-across major version. For all 1.4.x releases use 1.4 as version number, for
-1.3.x use 1.3, etc.
+We have split the Logstash package repositories by version into separate urls 
+to avoid accidental upgrades across major or minor versions. For all 1.5.x 
+releases use 1.5 as version number, for 1.4.x use 1.4, etc.
 
 We use the PGP key
 http://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
-Elasticsearch Signing Key, with fingerprint
+Elastic's Signing Key, with fingerprint
 
     4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
 
diff --git a/docs/asciidoc/static/roadmap/index.asciidoc b/docs/asciidoc/static/roadmap/index.asciidoc
index c94cba77b92..465fd23656f 100644
--- a/docs/asciidoc/static/roadmap/index.asciidoc
+++ b/docs/asciidoc/static/roadmap/index.asciidoc
@@ -17,63 +17,6 @@ adjustments to our timelines based on community feedback. For the latest release
 status information, please search for the {LABELS}roadmap[roadmap] tag in
 GitHub.
 
-== Logstash 1.5-GA status
-
-We recently released
-http://www.elasticsearch.org/blog/logstash-1-5-0-beta1-released/[Logstash 1.5 beta1],
-http://www.elasticsearch.org/blog/announcing-logstash-1-5-0-release-candidate/[Logstash 1.5 RC1],
-and http://www.elasticsearch.org/blog/logstash-1-5-0-rc2-released/[Logstash 1.5 RC2]!
-The main themes of this release are improved plugin management, increased
-performance, and Apache Kafka integration (see more details in the Logstash 1.5
-beta1 announcement). We are currently working to incorporate community feedback
-and to release Logstash 1.5 GA. You can track our progress on GitHub by looking
-at issues with the milestone
-https://github.com/elastic/logstash/issues?q=is%3Aopen+is%3Aissue+milestone%3Av1.5.0[v1.5.0].
-
-== Plugin Framework
-[float]
-=== status: ongoing; v1.5
-
-Logstash has a rich collection of 165+ plugins, which are developed by
-Elasticsearch and contributed by the community. Previously, most commonly-used
-plugins were bundled with Logstash to make the getting started experience
-easier. However, there was no way to update plugins outside of the Logstash
-release cycle. In Logstash 1.5, we created a powerful plugin framework based on
-https://rubygems.org/[RubyGems.org] to facilitate per-plugin installation and
-updates. We will continue to distribute commonly-used plugins with Logstash, but
-now users will be able to install new plugins and receive plugin updates at any
-time. Read more about these changes in the
-http://www.elastic.co/blog/plugin-ecosystem-changes/[Logstash Plugin Ecosystem Changes]
-announcement.
-
-== Windows Support
-[float]
-=== status: ongoing; v1.5, v2.x
-
-Leading up to the 1.5 release, we greatly improved automated Windows testing of
-Logstash. As a result of this testing, we identified and
-https://github.com/elastic/logstash/issues?q=is%3Aissue+label%3Awindows+is%3Aclosed[resolved]
-a number of critical issues affecting the Windows platform, pertaining to
-initial setup, upgrade, and file input plugin. You can follow the outstanding
-issues we are still working on using the GitHub
-https://github.com/elastic/logstash/issues?q=is%3Aissue+label%3Awindows+is%3Aopen[windows]
-label.
-
-
-== Performance
-[float]
-=== status: ongoing; v1.5, v2.x
-
-In the 1.5 release, we significantly improved the performance of the Grok
-filter, which is used to parse text via regular expressions. Based on our
-internal benchmarks, parsing common log formats, such as Apache logs, was 2x
-faster in Logstash 1.5 compared to previous versions. We also sped up JSON
-serialization and deserialization. In future releases of Logstash, we plan to
-incorporate additional JRuby optimizations to make the code even more efficient.
-We also plan to seek community feedback in terms of prioritizing other aspects
-of performance, such as startup time, resource utilization, and pipeline
-latency.
-
 == Resiliency
 [float]
 === status: ongoing; v2.x
@@ -144,6 +87,8 @@ that makes administration of Logstash more efficient and less error-prone. You
 can follow this effort on GitHub by searching for issues that have the
 {LABELS}manageability[manageability] tag.
 
+*Better Defaults.*  Today, some Logstash defaults are geared toward the development experience, rather than production environments. We plan to audit and re-evaluate a number of defaults to alleviate the burden of tuning Logstash performance in production ({ISSUES}1512[#1512]). In addition, we are undertaking additional benchmarking to evaluate the performance of node, transport, and HTTP protocols in the Elasticsearch output to provide additional confirmation for our proposal to switch the default from node to HTTP (https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/150[#150]).
+
 *Logstash Monitoring API ({ISSUES}2611[#2611]).* Today, most Logstash monitoring
 functions are accomplished by tailing logs or outputting debug messages. As a
 result, it is hard to monitor the Logstash health and track success or failure
@@ -202,6 +147,49 @@ Logstash Forwarder. We recently delivered
 http://www.elasticsearch.org/blog/logstash-forwarder-0-4-0-released/[Logstash Forwarder 0.4.0],
 which addressed many existing issues our users have been reporting.
 
+== Performance
+[float]
+=== status: ongoing; v1.5, v2.x
+
+In the 1.5 release, we significantly improved the performance of the Grok
+filter, which is used to parse text via regular expressions. Based on our
+internal benchmarks, parsing common log formats, such as Apache logs, was 2x
+faster in Logstash 1.5 compared to previous versions. We also sped up JSON
+serialization and deserialization. In future releases of Logstash, we plan to
+incorporate additional JRuby optimizations to make the code even more efficient.
+We also plan to seek community feedback in terms of prioritizing other aspects
+of performance, such as startup time, resource utilization, and pipeline
+latency. You can follow our benchmarking and performance improvements in this issue ({ISSUES}3499[#3499]).
+
+== Windows Support
+[float]
+=== status: ongoing; v1.5, v2.x
+
+Leading up to the 1.5 release, we greatly improved automated Windows testing of
+Logstash. As a result of this testing, we identified and
+https://github.com/elastic/logstash/issues?q=is%3Aissue+label%3Awindows+is%3Aclosed[resolved]
+a number of critical issues affecting the Windows platform, pertaining to
+initial setup, upgrade, and file input plugin. You can follow the outstanding
+issues we are still working on using the GitHub
+https://github.com/elastic/logstash/issues?q=is%3Aissue+label%3Awindows+is%3Aopen[windows]
+label.
+
+== Plugin Framework
+[float]
+=== status: completed; v1.5
+
+Logstash has a rich collection of 165+ plugins, which are developed by
+Elasticsearch and contributed by the community. Previously, most commonly-used
+plugins were bundled with Logstash to make the getting started experience
+easier. However, there was no way to update plugins outside of the Logstash
+release cycle. In Logstash 1.5, we created a powerful plugin framework based on
+https://rubygems.org/[RubyGems.org] to facilitate per-plugin installation and
+updates. We will continue to distribute commonly-used plugins with Logstash, but
+now users will be able to install new plugins and receive plugin updates at any
+time. Read more about these changes in the
+http://www.elastic.co/blog/plugin-ecosystem-changes/[Logstash Plugin Ecosystem Changes]
+announcement.
+
 == New Plugins
 [float]
 === status: ongoing
@@ -212,8 +200,7 @@ include https://github.com/logstash-plugins?query=kafka[Kafka],
 https://github.com/logstash-plugins?query=couchdb[CouchDB], and
 https://github.com/logstash-plugins/logstash-input-rss[RSS], just to name a few.
 In Logstash 1.5, we made it easier than ever to add and maintain plugins by
-putting each plugin into its own repository (read more about that in
-http://www.elasticsearch.org/blog/plugin-ecosystem-changes/[Logstash Plugin Ecosystem Changes]).
+putting each plugin into its own repository (see "Plugin Framework" section).
 We also greatly improved the S3, Twitter, RabbitMQ plugins. To follow requests
 for new Logstash plugins or contribute to the discussion, look for issues that
-have the {LABELS}new-plugin[new-plugin] tag in Github.
+have the {LABELS}new-plugin[new-plugin] tag in Github.
\ No newline at end of file
diff --git a/docs/asciidocgen.rb b/docs/asciidocgen.rb
index 0467eb7fc70..d12c8672463 100644
--- a/docs/asciidocgen.rb
+++ b/docs/asciidocgen.rb
@@ -20,7 +20,7 @@ class LogStashConfigAsciiDocGenerator
   def initialize
     @rules = {
       COMMENT_RE => lambda { |m| add_comment(m[1]) },
-      /^ *class.*< *::LogStash::(Outputs|Filters|Inputs|Codecs)::(Base|Threadable)/ => \
+      /^ *class.*< *(::)?LogStash::(Outputs|Filters|Inputs|Codecs)::(Base|Threadable)/ => \
         lambda { |m| set_class_description },
       /^ *config +[^=].*/ => lambda { |m| add_config(m[0]) },
       /^ *milestone .*/ => lambda { |m| set_milestone(m[0]) },
@@ -140,7 +140,6 @@ def generate(file, settings)
 
     # local scoping for the monkeypatch belowg
     attributes = @attributes
-
     # Monkeypatch the 'config' method to capture
     # Note, this monkeypatch requires us do the config processing
     # one at a time.
diff --git a/docs/repositories.md b/docs/repositories.md
index 320059e5e9b..21075147db7 100644
--- a/docs/repositories.md
+++ b/docs/repositories.md
@@ -12,24 +12,24 @@ Our public signing key can be found on the [Elasticsearch packages apt GPG signi
 
 Add the key:
 
-     wget -O - https://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
+    wget -O - https://packages.elasticsearch.org/GPG-KEY-elasticsearch | apt-key add -
 
 Add the repo to /etc/apt/sources.list
 
-     deb http://packages.elasticsearch.org/logstash/1.4/debian stable main
+    deb http://packages.elasticsearch.org/logstash/1.4/debian stable main
 
 
 ## YUM based distributions
 
 Add the key:
 
-     rpm --import https://packages.elasticsearch.org/GPG-KEY-elasticsearch
+    rpm --import https://packages.elasticsearch.org/GPG-KEY-elasticsearch
 
 Add the repo to /etc/yum.repos.d/ directory
 
-     [logstash-1.4]
-     name=logstash repository for 1.4.x packages
-     baseurl=https://packages.elasticsearch.org/logstash/1.4/centos
-     gpgcheck=1
-     gpgkey=https://packages.elasticsearch.org/GPG-KEY-elasticsearch
-     enabled=1
+    [logstash-1.4]
+    name=logstash repository for 1.4.x packages
+    baseurl=https://packages.elasticsearch.org/logstash/1.4/centos
+    gpgcheck=1
+    gpgkey=https://packages.elasticsearch.org/GPG-KEY-elasticsearch
+    enabled=1
diff --git a/docs/tutorials/10-minute-walkthrough/apache-parse.conf b/docs/tutorials/10-minute-walkthrough/apache-parse.conf
index 17adeaf06a0..dc0653cfa76 100644
--- a/docs/tutorials/10-minute-walkthrough/apache-parse.conf
+++ b/docs/tutorials/10-minute-walkthrough/apache-parse.conf
@@ -28,6 +28,6 @@ filter {
 output {
   # Use stdout in debug mode again to see what logstash makes of the event.
   stdout {
-    debug => true
+    codec => rubydebug
   }
 }
diff --git a/dripmain.rb b/dripmain.rb
index 92946007605..0324150871a 100644
--- a/dripmain.rb
+++ b/dripmain.rb
@@ -2,7 +2,7 @@
 # that we can do to speedup future startup using drip.
 
 require_relative "lib/bootstrap/environment"
-LogStash::Bundler.setup!
+LogStash::Bundler.setup!({:without => [:build]})
 
 # typical required gems and libs
 require "logstash/environment"
diff --git a/lib/bootstrap/bundler.rb b/lib/bootstrap/bundler.rb
index 163b3eca04e..be8a9622a85 100644
--- a/lib/bootstrap/bundler.rb
+++ b/lib/bootstrap/bundler.rb
@@ -11,6 +11,21 @@ def default_lockfile
           Pathname.new("#{default_gemfile}.#{ruby}.lock")
         end
       end
+
+      # Patch to prevent Bundler to save a .bundle/config file in the root 
+      # of the application
+      ::Bundler::Settings.module_exec do
+        def set_key(key, value, hash, file)
+          key = key_for(key)
+
+          unless hash[key] == value
+            hash[key] = value
+            hash.delete(key) if value.nil?
+          end
+
+          value
+        end
+      end
     end
 
     def setup!(options = {})
@@ -61,7 +76,6 @@ def invoke!(options = {})
 
       require "bundler"
       require "bundler/cli"
-      # require "logstash/patches/bundler"
       LogStash::Bundler.patch!
 
       # force Rubygems sources to our Gemfile sources
diff --git a/lib/bootstrap/environment.rb b/lib/bootstrap/environment.rb
index c6644b09e47..7348eeced32 100644
--- a/lib/bootstrap/environment.rb
+++ b/lib/bootstrap/environment.rb
@@ -43,7 +43,7 @@ def logstash_gem_home
 # defined and exposing the LogStash::Runner#main instance method which will be called with the current ARGV
 # currently lib/logstash/runner.rb and lib/pluginmanager/main.rb are called using this.
 if $0 == __FILE__
-  LogStash::Bundler.setup!
+  LogStash::Bundler.setup!({:without => [:build, :development]})
   require ARGV.shift
   LogStash::Runner.new.main(ARGV)
 end
\ No newline at end of file
diff --git a/lib/bootstrap/rspec.rb b/lib/bootstrap/rspec.rb
index e70779176f4..2f477003dfb 100755
--- a/lib/bootstrap/rspec.rb
+++ b/lib/bootstrap/rspec.rb
@@ -1,5 +1,5 @@
 require_relative "environment"
-LogStash::Bundler.setup!({:without => []})
+LogStash::Bundler.setup!({:without => [:build]})
 require "logstash/environment"
 
 $LOAD_PATH.unshift(File.join(LogStash::Environment::LOGSTASH_CORE, "spec"))
diff --git a/lib/logstash/agent.rb b/lib/logstash/agent.rb
index afe8bfaaef7..a4ef68710fc 100644
--- a/lib/logstash/agent.rb
+++ b/lib/logstash/agent.rb
@@ -113,8 +113,16 @@ def execute
 
     # Make SIGINT shutdown the pipeline.
     sigint_id = Stud::trap("INT") do
-      @logger.warn(I18n.t("logstash.agent.sigint"))
-      pipeline.shutdown
+
+      if @interrupted_once
+        @logger.fatal(I18n.t("logstash.agent.forced_sigint"))
+        exit
+      else
+        @logger.warn(I18n.t("logstash.agent.sigint"))
+        Thread.new(@logger) {|logger| sleep 5; logger.warn(I18n.t("logstash.agent.slow_shutdown")) }
+        @interrupted_once = true
+        pipeline.shutdown
+      end
     end
 
     # Make SIGTERM shutdown the pipeline.
@@ -164,14 +172,8 @@ def show_version
 
     if [:info, :debug].include?(verbosity?) || debug? || verbose?
       show_version_ruby
-
-      if RUBY_PLATFORM == "java"
-        show_version_java
-      end
-
-      if [:debug].include?(verbosity?) || debug?
-        show_gems
-      end
+      show_version_java if LogStash::Environment.jruby?
+      show_gems if [:debug].include?(verbosity?) || debug?
     end
   end # def show_version
 
diff --git a/lib/logstash/config/config_ast.rb b/lib/logstash/config/config_ast.rb
index 35ba4d440ad..ace7322fedb 100644
--- a/lib/logstash/config/config_ast.rb
+++ b/lib/logstash/config/config_ast.rb
@@ -76,7 +76,11 @@ def self.defered_conditionals_index=(val)
     @defered_conditionals_index = val
   end
 
-  class Node < Treetop::Runtime::SyntaxNode; end
+  class Node < Treetop::Runtime::SyntaxNode
+    def text_value_for_comments
+      text_value.gsub(/[\r\n]/, " ")
+    end
+  end
 
   class Config < Node
     def compile
@@ -412,14 +416,14 @@ class BranchEntry < Node; end
   class If < BranchEntry
     def compile
       children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
-      return "if #{condition.compile} # if #{condition.text_value}\n" \
+      return "if #{condition.compile} # if #{condition.text_value_for_comments}\n" \
         << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
     end
   end
   class Elsif < BranchEntry
     def compile
       children = recursive_inject { |e| e.is_a?(Branch) || e.is_a?(Plugin) }
-      return "elsif #{condition.compile} # else if #{condition.text_value}\n" \
+      return "elsif #{condition.compile} # else if #{condition.text_value_for_comments}\n" \
         << children.collect(&:compile).map { |s| s.split("\n", -1).map { |l| "  " + l }.join("\n") }.join("") << "\n"
     end
   end
diff --git a/lib/logstash/environment.rb b/lib/logstash/environment.rb
index 512c096a643..244671533ec 100644
--- a/lib/logstash/environment.rb
+++ b/lib/logstash/environment.rb
@@ -6,16 +6,13 @@ module Environment
     extend self
 
     # rehydrate the bootstrap environment if the startup was not done by executing bootstrap.rb
-    unless LogStash::Environment.const_defined?("LOGSTASH_HOME")
-      abort("ERROR: missing LOGSTASH_HOME environment variable") if ENV["LOGSTASH_HOME"].to_s.empty?
+    # and we are in the context of the logstash package
+    if !LogStash::Environment.const_defined?("LOGSTASH_HOME") &&  !ENV["LOGSTASH_HOME"].to_s.empty?
       $LOAD_PATH << ::File.join(ENV["LOGSTASH_HOME"], "lib")
       require "bootstrap/environment"
     end
 
     LOGSTASH_CORE = ::File.expand_path(::File.join(::File.dirname(__FILE__), "..", ".."))
-    BUNDLE_CONFIG_PATH = ::File.join(LOGSTASH_HOME, ".bundle", "config")
-    BOOTSTRAP_GEM_PATH = ::File.join(LOGSTASH_HOME, 'build', 'bootstrap')
-
     LOGSTASH_ENV = (ENV["LS_ENV"] || 'production').to_s.freeze
 
     def env
@@ -80,7 +77,7 @@ def jruby?
     end
 
     def windows?
-      Gem.win_platform?
+      ::Gem.win_platform?
     end
 
     def vendor_path(path)
diff --git a/lib/logstash/event.rb b/lib/logstash/event.rb
index 2b55fbe1377..c00d5531305 100644
--- a/lib/logstash/event.rb
+++ b/lib/logstash/event.rb
@@ -3,10 +3,10 @@
 require "date"
 require "cabin"
 require "logstash/namespace"
-require "logstash/util/fieldreference"
 require "logstash/util/accessors"
 require "logstash/timestamp"
 require "logstash/json"
+require "logstash/string_interpolation"
 
 # transcient pipeline events for normal in-flow signaling as opposed to
 # flow altering exceptions. for now having base classes is adequate and
@@ -141,6 +141,7 @@ def []=(fieldref, value)
       @metadata_accessors.set(fieldref[METADATA_BRACKETS.length .. -1], value)
     elsif fieldref == METADATA
       @metadata = value
+      @metadata_accessors = LogStash::Util::Accessors.new(@metadata)
     else
       @accessors.set(fieldref, value)
     end
@@ -216,57 +217,10 @@ def remove(fieldref)
   #
   # If a %{name} value is an array, then we will join by ','
   # If a %{name} value does not exist, then no substitution occurs.
-  #
-  # TODO(sissel): It is not clear what the value of a field that
-  # is an array (or hash?) should be. Join by comma? Something else?
   public
   def sprintf(format)
-    if format.is_a?(Float) and
-        (format < MIN_FLOAT_BEFORE_SCI_NOT or format >= MAX_FLOAT_BEFORE_SCI_NOT) then
-      format = ("%.15f" % format).sub(/0*$/,"")
-    else
-      format = format.to_s
-    end
-    if format.index("%").nil?
-      return format
-    end
-
-    return format.gsub(/%\{[^}]+\}/) do |tok|
-      # Take the inside of the %{ ... }
-      key = tok[2 ... -1]
-
-      if key[0] == "+" && !@data.has_key?(TIMESTAMP)
-        raise LogStash::Error, "Unable to format \"#{key}\" in string \"#{format}\", #{TIMESTAMP} field not found"
-      end
-
-      if key == "+%s"
-        # Got %{+%s}, support for unix epoch time
-        next @data[TIMESTAMP].to_i
-      elsif key[0,1] == "+"
-        t = @data[TIMESTAMP]
-        formatter = org.joda.time.format.DateTimeFormat.forPattern(key[1 .. -1])\
-          .withZone(org.joda.time.DateTimeZone::UTC)
-        #next org.joda.time.Instant.new(t.tv_sec * 1000 + t.tv_usec / 1000).toDateTime.toString(formatter)
-        # Invoke a specific Instant constructor to avoid this warning in JRuby
-        #  > ambiguous Java methods found, using org.joda.time.Instant(long)
-        org.joda.time.Instant.java_class.constructor(Java::long).new_instance(
-          t.tv_sec * 1000 + t.tv_usec / 1000
-        ).to_java.toDateTime.toString(formatter)
-      else
-        value = self[key]
-        case value
-          when nil
-            tok # leave the %{foo} if this field does not exist in this event.
-          when Array
-            value.join(",") # Join by ',' if value is an array
-          when Hash
-            LogStash::Json.dump(value) # Convert hashes to json
-          else
-            value # otherwise return the value
-        end # case value
-      end # 'key' checking
-    end # format.gsub...
-  end # def sprintf
+    LogStash::StringInterpolation.evaluate(self, format)
+  end
 
   def tag(value)
     # Generalize this method for more usability
diff --git a/lib/logstash/filters/base.rb b/lib/logstash/filters/base.rb
index 131661cfb54..61bf7887554 100644
--- a/lib/logstash/filters/base.rb
+++ b/lib/logstash/filters/base.rb
@@ -4,6 +4,7 @@
 require "logstash/logging"
 require "logstash/plugin"
 require "logstash/config/mixin"
+require "logstash/util/decorators"
 
 class LogStash::Filters::Base < LogStash::Plugin
   include LogStash::Config::Mixin
@@ -179,21 +180,7 @@ def threadsafe?
   # matches the filter's conditions (right type, etc)
   protected
   def filter_matched(event)
-    @add_field.each do |field, value|
-      field = event.sprintf(field)
-      value = [value] if !value.is_a?(Array)
-      value.each do |v|
-        v = event.sprintf(v)
-        if event.include?(field)
-          event[field] = [event[field]] if !event[field].is_a?(Array)
-          event[field] << v
-        else
-          event[field] = v
-        end
-        @logger.debug? and @logger.debug("filters/#{self.class.name}: adding value to field",
-                                       :field => field, :value => value)
-      end
-    end
+    LogStash::Util::Decorators.add_fields(@add_field,event,"filters/#{self.class.name}")
 
     @remove_field.each do |field|
       field = event.sprintf(field)
@@ -202,12 +189,7 @@ def filter_matched(event)
       event.remove(field)
     end
 
-    @add_tag.each do |tag|
-      tag = event.sprintf(tag)
-      @logger.debug? and @logger.debug("filters/#{self.class.name}: adding tag",
-                                       :tag => tag)
-      (event["tags"] ||= []) << tag
-    end
+    LogStash::Util::Decorators.add_tags(@add_tag,event,"filters/#{self.class.name}")
 
     @remove_tag.each do |tag|
       break if event["tags"].nil?
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 2874b3d3886..f28d04e0e98 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -5,6 +5,7 @@
 require "logstash/logging"
 require "logstash/config/mixin"
 require "logstash/codecs/base"
+require "logstash/util/decorators"
 
 # This is the base class for Logstash inputs.
 class LogStash::Inputs::Base < LogStash::Plugin
@@ -40,7 +41,7 @@ class LogStash::Inputs::Base < LogStash::Plugin
   # or in another character set other than `UTF-8`.
   #
   # This only affects `plain` format logs since json is `UTF-8` already.
-  config :charset, :validate => ::Encoding.name_list, :deprecated => true
+  config :charset, :deprecated => "Use the codec setting instead. For example: input { %PLUGIN% { codec => plain { charset => \"UTF-8\" } }"
 
   # If format is `json`, an event `sprintf` string to build what
   # the display `@message` should be given (defaults to the raw JSON).
@@ -101,7 +102,7 @@ def tag(newtag)
 
   protected
   def to_event(raw, source)
-    raise LogStash::ThisMethodWasRemoved("LogStash::Inputs::Base#to_event - you should use codecs now instead of to_event. Not sure what this means? Get help on logstash-users@googlegroups.com!")
+    raise LogStash::ThisMethodWasRemoved("LogStash::Inputs::Base#to_event - you should use codecs now instead of to_event. Not sure what this means? Get help on https://discuss.elastic.co/c/logstash")
   end # def to_event
 
   protected
@@ -109,14 +110,8 @@ def decorate(event)
     # Only set 'type' if not already set. This is backwards-compatible behavior
     event["type"] = @type if @type && !event.include?("type")
 
-    if @tags.any?
-      event["tags"] ||= []
-      event["tags"] += @tags
-    end
-
-    @add_field.each do |field, value|
-      event[field] = value
-    end
+    LogStash::Util::Decorators.add_fields(@add_field,event,"inputs/#{self.class.name}")
+    LogStash::Util::Decorators.add_tags(@tags,event,"inputs/#{self.class.name}")
   end
 
   protected
diff --git a/lib/logstash/java_integration.rb b/lib/logstash/java_integration.rb
index cacbf171284..8d37d95755e 100644
--- a/lib/logstash/java_integration.rb
+++ b/lib/logstash/java_integration.rb
@@ -92,4 +92,8 @@ def |(other)
     duped.addAll(other)
     duped
   end
+
+  def inspect
+    "<#{self.class.name}:#{self.hashCode} #{self.to_a(&:inspect)}>"
+  end
 end
\ No newline at end of file
diff --git a/lib/logstash/patches/bugfix_jruby_2558.rb b/lib/logstash/patches/bugfix_jruby_2558.rb
index 579b7614b4b..34f5d521a36 100644
--- a/lib/logstash/patches/bugfix_jruby_2558.rb
+++ b/lib/logstash/patches/bugfix_jruby_2558.rb
@@ -3,8 +3,8 @@
 if LogStash::Environment.windows? && LogStash::Environment.jruby?
   require "socket"
   module JRubyBug2558SocketPeerAddrBugFix
-    def peeraddr
-      orig_peeraddr.map do |v|
+    def peeraddr(*args)
+      orig_peeraddr(*args).map do |v|
         case v
         when String
           v.force_encoding(Encoding::UTF_8)
diff --git a/lib/logstash/patches/bundler.rb b/lib/logstash/patches/bundler.rb
index 182222e4532..bcfe02074a8 100644
--- a/lib/logstash/patches/bundler.rb
+++ b/lib/logstash/patches/bundler.rb
@@ -9,6 +9,21 @@ def default_lockfile
     end
   end
 
+  # Patch to prevent Bundler to save a .bundle/config file in the root 
+  # of the application
+  class Settings
+    def set_key(key, value, hash, file)
+      key = key_for(key)
+
+      unless hash[key] == value
+        hash[key] = value
+        hash.delete(key) if value.nil?
+      end
+
+      value
+    end
+  end
+
   # Add the Bundler.reset! method which has been added in master but is not in 1.7.9.
   class << self
     unless self.method_defined?("reset!")
diff --git a/lib/logstash/pipeline.rb b/lib/logstash/pipeline.rb
index 3c0457669bc..b6b1a30f4d3 100644
--- a/lib/logstash/pipeline.rb
+++ b/lib/logstash/pipeline.rb
@@ -115,7 +115,7 @@ def wait_inputs
   end
 
   def shutdown_filters
-    @flusher_lock.synchronize { @flusher_thread.kill }
+    @flusher_thread.kill
     @input_to_filter.push(LogStash::SHUTDOWN)
   end
 
@@ -156,8 +156,7 @@ def start_filters
       Thread.new { filterworker }
     end
 
-    @flusher_lock = Mutex.new
-    @flusher_thread = Thread.new { Stud.interval(5) { @flusher_lock.synchronize { @input_to_filter.push(LogStash::FLUSH) } } }
+    @flusher_thread = Thread.new { Stud.interval(5) { @input_to_filter.push(LogStash::FLUSH) } }
   end
 
   def start_outputs
@@ -220,7 +219,7 @@ def filterworker
         when LogStash::FlushEvent
           # handle filter flushing here so that non threadsafe filters (thus only running one filterworker)
           # don't have to deal with thread safety implementing the flush method
-          @flusher_lock.synchronize { flush_filters_to_output! }
+          flush_filters_to_output!
         when LogStash::ShutdownEvent
           # pass it down to any other filterworker and stop this worker
           @input_to_filter.push(event)
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index 340deb32d7a..9f425a6f8bb 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -16,6 +16,7 @@ class LogStash::Runner
 
   def main(args)
     require "logstash/util"
+    require "logstash/util/java_version"
     require "stud/trap"
     require "stud/task"
     @startup_interruption_trap = Stud::trap("INT") { puts "Interrupted"; exit 0 }
@@ -27,6 +28,9 @@ def main(args)
       return 1
     end
 
+    # Print a warning to STDERR for bad java versions
+    LogStash::Util::JavaVersion.warn_on_bad_java_version
+
     Stud::untrap("INT", @startup_interruption_trap)
 
     task = run(args)
diff --git a/lib/logstash/string_interpolation.rb b/lib/logstash/string_interpolation.rb
new file mode 100644
index 00000000000..93a8c0be27d
--- /dev/null
+++ b/lib/logstash/string_interpolation.rb
@@ -0,0 +1,139 @@
+require "thread_safe"
+require "forwardable"
+
+module LogStash
+  module StringInterpolation
+    extend self 
+
+    # Floats outside of these upper and lower bounds are forcibly converted
+    # to scientific notation by Float#to_s
+    MIN_FLOAT_BEFORE_SCI_NOT = 0.0001
+    MAX_FLOAT_BEFORE_SCI_NOT = 1000000000000000.0
+
+    CACHE = ThreadSafe::Cache.new
+    TEMPLATE_TAG_REGEXP = /%\{[^}]+\}/
+
+    def evaluate(event, template)
+      if template.is_a?(Float) && (template < MIN_FLOAT_BEFORE_SCI_NOT || template >= MAX_FLOAT_BEFORE_SCI_NOT)
+        return ("%.15f" % template).sub(/0*$/,"")
+      end
+      
+      template = template.to_s
+      
+      return template if not_cachable?(template)
+
+      compiled = CACHE.get_or_default(template, nil) || CACHE.put(template, compile_template(template))
+      compiled.evaluate(event)
+    end
+
+    private
+    def not_cachable?(template)
+      template.index("%").nil?
+    end
+
+    def compile_template(template)
+      nodes = Template.new
+
+      position = 0
+      matches = template.to_enum(:scan, TEMPLATE_TAG_REGEXP).map { |m| $~ }
+
+      matches.each do |match|
+        tag = match[0][2..-2]
+        start = match.offset(0).first
+        nodes << StaticNode.new(template[position..(start-1)]) if start > 0
+        nodes << identify(tag)
+        position = match.offset(0).last
+      end
+
+      if position < template.size - 1
+        nodes << StaticNode.new(template[position..-1])
+      end
+
+      optimize(nodes)
+    end
+
+    def optimize(nodes)
+      nodes.size == 1 ?  nodes.first : nodes
+    end
+
+    def identify(tag)
+      if tag == "+%s"
+        EpocNode.new
+      elsif tag[0, 1] == "+"
+        DateNode.new(tag[1..-1])
+      else
+        KeyNode.new(tag)
+      end
+    end
+  end
+
+  class Template
+    extend Forwardable
+    def_delegators :@nodes, :<<, :push, :size, :first
+
+    def initialize
+      @nodes = []
+    end
+
+    def evaluate(event)
+      @nodes.collect { |node| node.evaluate(event) }.join
+    end
+  end
+
+  class EpocNode
+    def evaluate(event)
+      t = event.timestamp
+      raise LogStash::Error, "Unable to format in string \"#{@format}\", #{LogStash::Event::TIMESTAMP} field not found" unless t
+      t.to_i.to_s
+    end
+  end
+
+  class StaticNode
+    def initialize(content)
+      @content = content
+    end
+
+    def evaluate(event)
+      @content
+    end
+  end
+
+  class KeyNode
+    def initialize(key)
+      @key = key
+    end
+
+    def evaluate(event)
+      value = event[@key]
+
+      case value
+      when nil
+        "%{#{@key}}"
+      when Array
+        value.join(",")
+      when Hash
+        LogStash::Json.dump(value)
+      else
+        value
+      end
+    end
+  end
+
+  class DateNode
+    def initialize(format)
+      @format = format
+      @formatter = org.joda.time.format.DateTimeFormat.forPattern(@format)
+          .withZone(org.joda.time.DateTimeZone::UTC)
+    end
+
+    def evaluate(event)
+      t = event.timestamp
+
+      raise LogStash::Error, "Unable to format in string \"#{@format}\", #{LogStash::Event::TIMESTAMP} field not found" unless t
+
+      org.joda.time.Instant.java_class.constructor(Java::long).new_instance(
+        t.tv_sec * 1000 + t.tv_usec / 1000
+      ).to_java.toDateTime.toString(@formatter)
+    end
+  end
+end
diff --git a/lib/logstash/util/accessors.rb b/lib/logstash/util/accessors.rb
index 648c26be319..d169be240c9 100644
--- a/lib/logstash/util/accessors.rb
+++ b/lib/logstash/util/accessors.rb
@@ -2,90 +2,120 @@
 
 require "logstash/namespace"
 require "logstash/util"
+require "thread_safe"
 
 module LogStash::Util
 
-  # PathCache is a singleton which globally caches a parsed fields path for the path to the
-  # container hash and key in that hash.
+  # PathCache is a singleton which globally caches the relation between a field reference and its
+  # decomposition into a [key, path array] tuple. For example the field reference [foo][bar][baz]
+  # is decomposed into ["baz", ["foo", "bar"]].
   module PathCache
     extend self
 
-    def get(accessor)
-      @cache ||= {}
-      @cache[accessor] ||= parse(accessor)
+    # requiring libraries and defining constants is thread safe in JRuby so
+    # PathCache::CACHE will be corretly initialized, once, when accessors.rb
+    # will be first required
+    CACHE = ThreadSafe::Cache.new
+
+    def get(field_reference)
+      # the "get_or_default(x, nil) || put(x, parse(x))" is ~2x faster than "get || put" because the get call is
+      # proxied through the JRuby JavaProxy op_aref method. the correct idiom here would be to use
+      # "compute_if_absent(x){parse(x)}" but because of the closure creation, it is ~1.5x slower than
+      # "get_or_default || put".
+      # this "get_or_default || put" is obviously non-atomic which is not really important here
+      # since all threads will set the same value and this cache will stabilize very quickly after the first
+      # few events.
+      CACHE.get_or_default(field_reference, nil) || CACHE.put(field_reference, parse(field_reference))
     end
 
-    def parse(accessor)
-      path = accessor.split(/[\[\]]/).select{|s| !s.empty?}
+    def parse(field_reference)
+      path = field_reference.split(/[\[\]]/).select{|s| !s.empty?}
       [path.pop, path]
     end
   end
 
-  # Accessors uses a lookup table to speedup access of an accessor field of the type
+  # Accessors uses a lookup table to speedup access of a field reference of the form
   # "[hello][world]" to the underlying store hash into {"hello" => {"world" => "foo"}}
   class Accessors
 
+    # @param store [Hash] the backing data store field refereces point to
     def initialize(store)
       @store = store
+
+      # @lut is a lookup table between a field reference and a [target, key] tuple
+      # where target is the containing Hash or Array for key in @store.
+      # this allows us to directly access the containing object for key instead of
+      # walking the field reference path into the inner @store objects
       @lut = {}
     end
 
-    def get(accessor)
-      target, key = lookup(accessor)
-      unless target.nil?
-        target.is_a?(Array) ? target[key.to_i] : target[key]
-      end
+    # @param field_reference [String] the field reference
+    # @return [Object] the value in @store for this field reference
+    def get(field_reference)
+      target, key = lookup(field_reference)
+      return nil unless target
+      target.is_a?(Array) ? target[key.to_i] : target[key]
     end
 
-    def set(accessor, value)
-      target, key = store_and_lookup(accessor)
+    # @param field_reference [String] the field reference
+    # @param value [Object] the value to set in @store for this field reference
+    # @return [Object] the value set
+    def set(field_reference, value)
+      target, key = lookup_or_create(field_reference)
       target[target.is_a?(Array) ? key.to_i : key] = value
     end
 
-    def strict_set(accessor, value)
-      set(accessor, LogStash::Event.validate_value(value))
-    end
-
-    def del(accessor)
-      target, key = lookup(accessor)
-      unless target.nil?
-        target.is_a?(Array) ? target.delete_at(key.to_i) : target.delete(key)
-      end
+    # @param field_reference [String] the field reference to remove
+    # @return [Object] the removed value in @store for this field reference
+    def del(field_reference)
+      target, key = lookup(field_reference)
+      return nil unless target
+      target.is_a?(Array) ? target.delete_at(key.to_i) : target.delete(key)
     end
 
-    def include?(accessor)
-      target, key = lookup_path(accessor)
+    # @param field_reference [String] the field reference to test for inclusion in the store
+    # @return [Boolean] true if the store contains a value for this field reference
+    def include?(field_reference)
+      target, key = lookup(field_reference)
       return false unless target
+
       target.is_a?(Array) ? !target[key.to_i].nil? : target.include?(key)
     end
 
     private
 
-    def lookup(accessor)
-      target, key = lookup_path(accessor)
-      if target.nil?
-        [target, key]
-      else
-        @lut[accessor] = [target, key]
-      end
+    # retrieve the [target, key] tuple associated with this field reference
+    # @param field_reference [String] the field referece
+    # @return [[Object, String]] the  [target, key] tuple associated with this field reference
+    def lookup(field_reference)
+      @lut[field_reference] ||= find_target(field_reference)
     end
 
-    def store_and_lookup(accessor)
-      @lut[accessor] ||= store_path(accessor)
+    # retrieve the [target, key] tuple associated with this field reference and create inner
+    # container objects if they do not exists
+    # @param field_reference [String] the field referece
+    # @return [[Object, String]] the  [target, key] tuple associated with this field reference
+    def lookup_or_create(field_reference)
+      @lut[field_reference] ||= find_or_create_target(field_reference)
     end
 
-    def lookup_path(accessor)
-      key, path = PathCache.get(accessor)
+    # find the target container object in store for this field reference
+    # @param field_reference [String] the field referece
+    # @return [Object] the target container object in store associated with this field reference
+    def find_target(field_reference)
+      key, path = PathCache.get(field_reference)
       target = path.inject(@store) do |r, k|
-        if r.nil?
-          return nil
-        end
+        return nil unless r
         r[r.is_a?(Array) ? k.to_i : k]
       end
-      [target, key]
+      target ? [target, key] : nil
     end
 
-    def store_path(accessor)
+    # find the target container object in store for this field reference and create inner
+    # container objects if they do not exists
+    # @param field_reference [String] the field referece
+    # @return [Object] the target container object in store associated with this field reference
+    def find_or_create_target(accessor)
       key, path = PathCache.get(accessor)
       target = path.inject(@store) {|r, k| r[r.is_a?(Array) ? k.to_i : k] ||= {}}
       [target, key]
diff --git a/lib/logstash/util/decorators.rb b/lib/logstash/util/decorators.rb
new file mode 100644
index 00000000000..3be970d2bf3
--- /dev/null
+++ b/lib/logstash/util/decorators.rb
@@ -0,0 +1,46 @@
+# encoding: utf-8
+
+require "logstash/namespace"
+require "logstash/util"
+
+module LogStash::Util
+
+  # Decorators provides common manipulation on the event data.
+  module Decorators
+    extend self
+    
+    @logger = Cabin::Channel.get(LogStash)
+
+    # fields is a hash of field => value
+    # where both `field` and `value` can use sprintf syntax.
+    def add_fields(fields,event, pluginname)
+      fields.each do |field, value|
+        field = event.sprintf(field)
+        value = Array(value)
+        value.each do |v|
+          v = event.sprintf(v)
+          if event.include?(field)
+            event[field] = Array(event[field])
+            event[field] << v
+          else
+            event[field] = v
+          end
+          @logger.debug? and @logger.debug("#{pluginname}: adding value to field",
+                                         :field => field, :value => value)
+        end
+      end
+    end
+
+    # tags is an array of string. sprintf syntax can be used.
+    def add_tags(tags, event, pluginname)
+      tags.each do |tag|
+        tag = event.sprintf(tag)
+        @logger.debug? and @logger.debug("#{pluginname}: adding tag",
+                                       :tag => tag)
+        (event["tags"] ||= []) << tag
+      end
+    end
+
+  end # module LogStash::Util::Decorators
+
+end # module LogStash::Util
diff --git a/lib/logstash/util/fieldreference.rb b/lib/logstash/util/fieldreference.rb
deleted file mode 100644
index 0683310533c..00000000000
--- a/lib/logstash/util/fieldreference.rb
+++ /dev/null
@@ -1,68 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/util"
-
-module LogStash::Util::FieldReference
-
-  def compile(accessor)
-    if accessor[0,1] != '['
-      return <<-"CODE"
-        lambda do |store, &block|
-          return block.nil? ? store[#{accessor.inspect}] : block.call(store, #{accessor.inspect})
-        end
-      CODE
-    end
-
-    code = "lambda do |store, &block|\n"
-    selectors = accessor.scan(/(?<=\[).+?(?=\])/)
-    selectors.each_with_index do |tok, i|
-      last = (i == selectors.count() - 1)
-      code << "   # [#{tok}]#{ last ? " (last selector)" : "" }\n"
-
-      if last
-        code << <<-"CODE"
-          return block.call(store, #{tok.inspect}) unless block.nil?
-        CODE
-      end
-
-      code << <<-"CODE"
-        store = store.is_a?(Array) ? store[#{tok.to_i}] : store[#{tok.inspect}]
-        return store if store.nil?
-      CODE
-
-    end
-    code << "return store\nend"
-    #puts code
-    return code
-  end # def compile
-
-  def exec(accessor, store, &block)
-    @__fieldeval_cache ||= {}
-    @__fieldeval_cache[accessor] ||= eval(compile(accessor))
-    return @__fieldeval_cache[accessor].call(store, &block)
-  end
-
-  def set(accessor, value, store)
-    # The assignment can fail if the given field reference (accessor) does not exist
-    # In this case, we'll want to set the value manually.
-    if exec(accessor, store) { |hash, key| hash[key] = value }.nil?
-      return (store[accessor] = value) if accessor[0,1] != "["
-
-      # No existing element was found, so let's set one.
-      *parents, key = accessor.scan(/(?<=\[)[^\]]+(?=\])/)
-      parents.each do |p|
-        if store.include?(p)
-          store = store[p]
-        else
-          store[p] = {}
-          store = store[p]
-        end
-      end
-      store[key] = value
-    end
-
-    return value
-  end
-
-  extend self
-end # module LogStash::Util::FieldReference
diff --git a/lib/logstash/util/java_version.rb b/lib/logstash/util/java_version.rb
new file mode 100644
index 00000000000..092de78a9a7
--- /dev/null
+++ b/lib/logstash/util/java_version.rb
@@ -0,0 +1,61 @@
+require 'cabin'
+
+module LogStash::Util::JavaVersion
+  def self.logger
+    @logger ||= Cabin::Channel.get(LogStash)
+  end
+
+  # Print a warning if we're on a bad version of java
+  def self.warn_on_bad_java_version
+    if self.bad_java_version?(self.version)
+      msg = "!!! Please upgrade your java version, the current version '#{self.version}' may cause problems. We recommend a minimum version of 1.7.0_51"
+      STDERR.puts(msg)
+      logger.warn(msg)
+    end
+  end
+
+  # Return the current java version string. Returns nil if this is a non-java platform (e.g. MRI).
+  def self.version
+    return nil unless LogStash::Environment.jruby?
+    java.lang.System.getProperty("java.runtime.version")
+  end
+
+  # Takes a string of a java version ex: "1.8.0_24-beta"
+  # and returns a parsed map of the components.
+  # nil inputs will be returned as nil.
+  def self.parse_java_version(version_string)
+    return nil if version_string.nil?
+
+    # Crazy java versioning rules @ http://www.oracle.com/technetwork/java/javase/versioning-naming-139433.html
+    # The regex below parses this all correctly http://rubular.com/r/sInQc3Nc7f
+
+    match = version_string.match(/\A(\d+)\.(\d+)\.(\d+)(_(\d+))?(-(.+))?\Z/)
+    major, minor, patch, ufull, update, bfull, build = match.captures
+
+    {
+      :full => version_string,
+      :major => major.to_i,
+      :minor => minor.to_i,
+      :patch => patch.to_i,
+      :update => update.to_i, # this is always coerced to an int (a nil will be zero) to make comparisons easier
+      :build => build # not an integer, could be b06 for instance!,
+    }
+  end
+
+  # Determine if the given java version string is a bad version of java
+  # If it is, return true, if it isn't return false.
+  # Accepts nil, returning nil.
+  def self.bad_java_version?(version_string)
+    return nil if version_string.nil?
+
+    parsed = parse_java_version(version_string)
+
+    if parsed[:major] == 1 && parsed[:minor] == 7 && parsed[:patch] == 0 && parsed[:update] < 51
+      true
+    elsif parsed[:major] == 1 && parsed[:minor] < 7
+      true
+    else
+      false
+    end
+  end
+end
\ No newline at end of file
diff --git a/lib/logstash/util/plugin_version.rb b/lib/logstash/util/plugin_version.rb
index ed6cb154304..1266646e684 100644
--- a/lib/logstash/util/plugin_version.rb
+++ b/lib/logstash/util/plugin_version.rb
@@ -22,8 +22,14 @@ def initialize(*options)
 
     def self.find_version!(name)
       begin
-        specification = Gem::Specification.find_by_name(name)
-        new(specification.version)
+        spec = Gem::Specification.find_by_name(name)
+        if spec.nil?
+          # Checking for nil? is a workaround for situations where find_by_name
+          # is not able to find the real spec, as for example with pre releases
+          # of plugins
+          spec = Gem::Specification.find_all_by_name(name).first
+        end
+        new(spec.version)
       rescue Gem::LoadError
         # Rescuing the LoadError and raise a Logstash specific error.
         # Likely we can't find the gem in the current GEM_PATH
@@ -39,5 +45,11 @@ def self.find_plugin_version!(type, name)
     def <=>(other)
       version <=> other.version
     end
+
+    private
+
+    def self.build_from_spec(spec)
+      new(spec.version)
+    end
   end
 end
diff --git a/lib/logstash/util/unicode_trimmer.rb b/lib/logstash/util/unicode_trimmer.rb
new file mode 100644
index 00000000000..13f3fa36e59
--- /dev/null
+++ b/lib/logstash/util/unicode_trimmer.rb
@@ -0,0 +1,80 @@
+module UnicodeTrimmer
+  # The largest possible unicode chars are 4 bytes
+  # http://stackoverflow.com/questions/9533258/what-is-the-maximum-number-of-bytes-for-a-utf-8-encoded-character
+  # http://tools.ietf.org/html/rfc3629
+  MAX_CHAR_BYTES = 4
+
+  # Takes a unicode string and makes sure it fits in a max of `desired_bytes`
+  # This aims to be somewhat efficient about this for the average case and get as close to
+  # O(1) as possible. Given certain distributions of multi-byte characters it'll be slower
+  # It tries to find the point the truncation *should* happen based on the average byte size.
+  # If that snips it in the wrong place it'll try to add or remove chars to get it to the right
+  # spot and preserve as much data as possible.
+  public
+  def self.trim_bytes(orig_str, desired_bytes)
+    return orig_str if orig_str.bytesize <= desired_bytes
+
+    pre_shortened = pre_shorten(orig_str, desired_bytes)
+
+    case pre_shortened.bytesize <=> desired_bytes
+    when 0
+      pre_shortened
+    when 1
+      shrink_bytes(pre_shortened, orig_str, desired_bytes)
+    when -1
+      grow_bytes(pre_shortened, orig_str, desired_bytes)
+    end
+  end
+
+  private
+  # Try to cut the string at the right place based on the avg. byte size
+  def self.pre_shorten(orig_str, desired_bytes)
+    # Compute the average size to get an idea of where should chop
+    orig_len = orig_str.length
+    orig_bs = orig_str.bytesize
+    avg_size = (orig_bs.to_f / orig_len.to_f)
+
+    # Try to do an initial shortening based on the average char size
+    # The goal here is to get us somewhere above or below the boundary quickly
+    orig_extra_bytes = orig_bs - desired_bytes
+    pre_shorten_by = (orig_extra_bytes  / avg_size).to_i
+    orig_str.slice(0, orig_len - pre_shorten_by)
+  end
+
+  private
+  def self.grow_bytes(pre_shortened, orig_str, desired_bytes)
+    res_str = pre_shortened.clone()
+
+    loop do
+      bs = res_str.bytesize
+      deficit = desired_bytes - bs
+      lengthen_by = deficit / MAX_CHAR_BYTES
+      lengthen_by = 1 if lengthen_by < 1
+      append = orig_str.slice(res_str.length, lengthen_by)
+
+      break if (bs + append.bytesize) > desired_bytes
+
+      res_str << append
+    end
+
+    res_str
+  end
+
+  private
+  def self.shrink_bytes(pre_shortened, orig_str, desired_bytes)
+    res_str = pre_shortened.clone()
+
+    loop do
+      bs = res_str.bytesize
+      break if bs <= desired_bytes
+
+      extra = bs - desired_bytes
+      shorten_by = extra / MAX_CHAR_BYTES
+      shorten_by = 1 if shorten_by < 1
+
+      res_str.slice!(res_str.length - shorten_by)
+    end
+
+    res_str
+  end
+end
diff --git a/lib/logstash/version.rb b/lib/logstash/version.rb
index 36f2bad27fe..206275745dd 100644
--- a/lib/logstash/version.rb
+++ b/lib/logstash/version.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 # The version of logstash.
-LOGSTASH_VERSION = "2.0.0.dev"
+LOGSTASH_VERSION = "1.5.2"
 
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
diff --git a/lib/pluginmanager/install.rb b/lib/pluginmanager/install.rb
index d1e51bb08c2..2596d356dbe 100644
--- a/lib/pluginmanager/install.rb
+++ b/lib/pluginmanager/install.rb
@@ -1,3 +1,4 @@
+require "pluginmanager/command"
 require "jar-dependencies"
 require "jar_install_post_install_hook"
 require "file-dependencies/gem"
diff --git a/lib/pluginmanager/list.rb b/lib/pluginmanager/list.rb
index b60b7b35e3f..7fd32cbe3db 100644
--- a/lib/pluginmanager/list.rb
+++ b/lib/pluginmanager/list.rb
@@ -1,4 +1,5 @@
 require 'rubygems/spec_fetcher'
+require "pluginmanager/command"
 
 class LogStash::PluginManager::List < LogStash::PluginManager::Command
 
@@ -12,7 +13,7 @@ class LogStash::PluginManager::List < LogStash::PluginManager::Command
   end
 
   def execute
-    LogStash::Bundler.setup!
+    LogStash::Bundler.setup!({:without => [:build, :development]})
 
     signal_error("No plugins found") if filtered_specs.empty?
 
diff --git a/lib/pluginmanager/main.rb b/lib/pluginmanager/main.rb
index b88ee1e9a07..923d5225d00 100644
--- a/lib/pluginmanager/main.rb
+++ b/lib/pluginmanager/main.rb
@@ -11,7 +11,6 @@ module PluginManager
 end
 
 require "clamp"
-require "pluginmanager/command"
 require "pluginmanager/util"
 require "pluginmanager/gemfile"
 require "pluginmanager/install"
@@ -39,4 +38,4 @@ class Main < Clamp::Command
     $stderr.puts(e.message)
     exit(1)
   end
-end
\ No newline at end of file
+end
diff --git a/lib/pluginmanager/uninstall.rb b/lib/pluginmanager/uninstall.rb
index 277745fd199..acc77b3fbc8 100644
--- a/lib/pluginmanager/uninstall.rb
+++ b/lib/pluginmanager/uninstall.rb
@@ -1,9 +1,19 @@
+require "pluginmanager/command"
+
 class LogStash::PluginManager::Uninstall < LogStash::PluginManager::Command
+
   parameter "PLUGIN", "plugin name"
 
   def execute
     signal_error("File #{LogStash::Environment::GEMFILE_PATH} does not exist or is not writable, aborting") unless File.writable?(LogStash::Environment::GEMFILE_PATH)
 
+    ##
+    # Need to setup the bundler status to enable uninstall of plugins
+    # installed as local_gems, otherwise gem:specification is not
+    # finding the plugins
+    ##
+    LogStash::Bundler.setup!({:without => [:build, :development]})
+
     # make sure this is an installed plugin and present in Gemfile.
     # it is not possible to uninstall a dependency not listed in the Gemfile, for example a dependent codec
     signal_error("This plugin has not been previously installed, aborting") unless LogStash::PluginManager.installed_plugin?(plugin, gemfile)
diff --git a/lib/pluginmanager/update.rb b/lib/pluginmanager/update.rb
index b9276a996ba..59ea4e936ef 100644
--- a/lib/pluginmanager/update.rb
+++ b/lib/pluginmanager/update.rb
@@ -1,32 +1,38 @@
+require "pluginmanager/command"
 require "jar-dependencies"
 require "jar_install_post_install_hook"
 require "file-dependencies/gem"
 
 class LogStash::PluginManager::Update < LogStash::PluginManager::Command
+  REJECTED_OPTIONS = [:path, :git, :github]
+
   parameter "[PLUGIN] ...", "Plugin name(s) to upgrade to latest version", :attribute_name => :plugins_arg
 
   def execute
     local_gems = gemfile.locally_installed_gems
 
-    if update_all? && !local_gems.empty?
-      error_plugin_that_use_path!(local_gems)
-    else
-      plugins_with_path = plugins_arg & local_gems
-      error_plugin_that_use_path!(plugins_with_path) if plugins_with_path.size > 0
+    if local_gems.size > 0
+      if update_all?
+        plugins_with_path = local_gems.map(&:name)
+      else
+        plugins_with_path = plugins_arg & local_gems.map(&:name)
+      end
+
+      warn_local_gems(plugins_with_path)
     end
 
     update_gems!
   end
 
   private
-  def error_plugin_that_use_path!(plugins)
-    signal_error("Update is not supported for manually defined plugins or local .gem plugin installations: #{plugins.collect(&:name).join(",")}")
-  end
-
   def update_all?
     plugins_arg.size == 0
   end
 
+  def warn_local_gems(plugins_with_path)
+    puts("Update is not supported for manually defined plugins or local .gem plugin installations, skipping: #{plugins_with_path.join(", ")}")
+  end
+
   def update_gems!
     # If any error is raise inside the block the Gemfile will restore a backup of the Gemfile
     previous_gem_specs_map = find_latest_gem_specs
@@ -34,14 +40,16 @@ def update_gems!
     # remove any version constrain from the Gemfile so the plugin(s) can be updated to latest version
     # calling update without requiremend will remove any previous requirements
     plugins = plugins_to_update(previous_gem_specs_map)
-    plugins
-      .select { |plugin| gemfile.find(plugin) }
-      .each { |plugin| gemfile.update(plugin) }
+    filtered_plugins = plugins.map { |plugin| gemfile.find(plugin) }
+      .compact
+      .reject { |plugin| REJECTED_OPTIONS.any? { |key| plugin.options.has_key?(key) } }
+      .select { |plugin| validate_major_version(plugin.name) }
+      .each   { |plugin| gemfile.update(plugin.name) }
 
     # force a disk sync before running bundler
     gemfile.save
 
-    puts("Updating " + plugins.join(", "))
+    puts("Updating #{filtered_plugins.collect(&:name).join(", ")}") unless filtered_plugins.empty?
 
     # any errors will be logged to $stderr by invoke!
     # Bundler cannot update and clean gems in one operation so we have to call the CLI twice.
@@ -56,12 +64,29 @@ def update_gems!
     display_bundler_output(output)
   end
 
+  # validate if there is any major version update so then we can ask the user if he is
+  # sure to update or not.
+  def validate_major_version(plugin)
+    require "gems"
+    latest_version  = Gems.versions(plugin)[0]['number'].split(".")
+    current_version = Gem::Specification.find_by_name(plugin).version.version.split(".")
+    if (latest_version[0].to_i > current_version[0].to_i)
+      ## warn if users want to continue
+      puts("You are updating #{plugin} to a new version #{latest_version.join('.')}, which may not be compatible with #{current_version.join('.')}. are you sure you want to proceed (Y/N)?")
+      return ( "y" == STDIN.gets.strip.downcase ? true : false)
+    end
+    true
+  end
+
   # create list of plugins to update
   def plugins_to_update(previous_gem_specs_map)
     if update_all?
       previous_gem_specs_map.values.map{|spec| spec.name}
     else
-      not_installed = plugins_arg.select{|plugin| !previous_gem_specs_map.has_key?(plugin.downcase)}
+      # If the plugins isn't available in the gemspec or in 
+      # the gemfile defined with a local path, we assume the plugins is not
+      # installed.
+      not_installed = plugins_arg.select{|plugin| !previous_gem_specs_map.has_key?(plugin.downcase) && !gemfile.find(plugin) }
       signal_error("Plugin #{not_installed.join(', ')} is not installed so it cannot be updated, aborting") unless not_installed.empty?
       plugins_arg
     end
diff --git a/lib/pluginmanager/util.rb b/lib/pluginmanager/util.rb
index 0902e3c91de..e16f16b6cb5 100644
--- a/lib/pluginmanager/util.rb
+++ b/lib/pluginmanager/util.rb
@@ -17,7 +17,10 @@ def self.logstash_plugin?(plugin, version = nil)
       end
     else
       dep = Gem::Dependency.new(plugin, version || Gem::Requirement.default)
-      specs, error = Gem::SpecFetcher.fetcher.spec_for_dependency(dep)
+      specs, errors = Gem::SpecFetcher.fetcher.spec_for_dependency(dep)
+
+      # dump errors
+      errors.each { |error| $stderr.puts(error.wordy) }
 
       # depending on version requirements, multiple specs can be returned in which case
       # we will grab the one with the highest version number
@@ -27,7 +30,7 @@ def self.logstash_plugin?(plugin, version = nil)
         end
         return valid
       else
-        $stderr.puts("Plugin #{plugin}" + (version ? " version #{version}" : "") + " does not exist")
+        $stderr.puts("Plugin #{plugin}" + (version ? " version #{version}" : "") + " does not exist") if errors.empty?
         return false
       end
     end
diff --git a/locales/en.yml b/locales/en.yml
index 154af8b0a50..bbfb1236bac 100644
--- a/locales/en.yml
+++ b/locales/en.yml
@@ -66,6 +66,12 @@ en:
         SIGINT received. Shutting down the pipeline.
       sigterm: >-
         SIGTERM received. Shutting down the pipeline.
+      slow_shutdown: |-
+        Received shutdown signal, but pipeline is still waiting for in-flight events
+        to be processed. Sending another ^C will force quit Logstash, but this may cause
+        data loss.
+      forced_sigint: >-
+        SIGINT received. Terminating immediately..
       configtest-flag-information: |-
         You may be interested in the '--configtest' flag which you can
         use to validate logstash's configuration before you choose
diff --git a/logstash-core.gemspec b/logstash-core.gemspec
index 03c12988ead..c0e820b34ab 100644
--- a/logstash-core.gemspec
+++ b/logstash-core.gemspec
@@ -15,13 +15,14 @@ Gem::Specification.new do |gem|
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core"
   gem.require_paths = ["lib"]
-  gem.version       = LOGSTASH_VERSION
+  gem.version       = LOGSTASH_VERSION.gsub(/-/, '.')
 
   gem.add_runtime_dependency "cabin", "~> 0.7.0" #(Apache 2.0 license)
-  gem.add_runtime_dependency "pry", "~> 0.10.1" #(Ruby license)
+  gem.add_runtime_dependency "pry", "~> 0.10.1"  #(Ruby license)
   gem.add_runtime_dependency "stud", "~> 0.0.19" #(Apache 2.0 license)
   gem.add_runtime_dependency "clamp", "~> 0.6.5" #(MIT license) for command line args/flags
   gem.add_runtime_dependency "filesize", "0.0.4" #(MIT license) for :bytes config validator
+  gem.add_runtime_dependency "gems", "~> 0.8.3"  #(MIT license)
 
   # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
   # investigated what the cause might be. -Jordan
@@ -32,10 +33,11 @@ Gem::Specification.new do |gem|
 
   # filetools and rakelib
   gem.add_runtime_dependency "minitar", "~> 0.5.4"
+  gem.add_runtime_dependency "thread_safe", "~> 0.3.5" #(Apache 2.0 license)
 
   if RUBY_PLATFORM == 'java'
     gem.platform = RUBY_PLATFORM
-    gem.add_runtime_dependency "jrjackson", "~> 0.2.8" #(Apache 2.0 license)
+    gem.add_runtime_dependency "jrjackson", "~> 0.2.9" #(Apache 2.0 license)
   else
     gem.add_runtime_dependency "oj" #(MIT-style license)
   end
@@ -48,7 +50,4 @@ Gem::Specification.new do |gem|
     # https://github.com/rubinius/rubinius/issues/2632#issuecomment-26954565
     gem.add_runtime_dependency "racc"
   end
-
-  gem.add_development_dependency "rspec", "~> 2.14" #(MIT license)
-  gem.add_development_dependency "logstash-devutils", "~> 0"
 end
diff --git a/logstash-event.gemspec b/logstash-event.gemspec
index 39f3af777df..ea6cce87e1a 100644
--- a/logstash-event.gemspec
+++ b/logstash-event.gemspec
@@ -19,7 +19,6 @@ Gem::Specification.new do |gem|
     lib/logstash/version.rb
     lib/logstash/util.rb
     lib/logstash/util/accessors.rb
-    lib/logstash/util/fieldreference.rb
     LICENSE
   }
 
diff --git a/pkg/build.sh b/pkg/build.sh
deleted file mode 100755
index 8ad876b1f0e..00000000000
--- a/pkg/build.sh
+++ /dev/null
@@ -1,121 +0,0 @@
-#!/bin/bash
-# We only need to build two packages now, rpm and deb.  Leaving the os/version stuff in case things change.
-
-[ ! -f ../.VERSION.mk ] && make -C .. .VERSION.mk
-
-. ../.VERSION.mk
-
-DEB_REVISION="${REVISION}"
-RPM_REVISION="${REVISION}"
-
-URL="http://logstash.net"
-DESCRIPTION="An extensible logging pipeline"
-
-if [ "$#" -ne 2 ] ; then
-  echo "Usage: $0 <os> <release>"
-  echo
-  echo "Example: $0 ubuntu 12.10"
-  exit 1
-fi
-
-os=$1
-release=$2
-
-echo "Building package for $os $release"
-
-destdir=build/$(echo "$os" | tr ' ' '_')
-prefix=/opt/logstash
-
-if [ "$destdir/$prefix" != "/" -a -d "$destdir/$prefix" ] ; then
-  rm -rf "$destdir/$prefix"
-fi
-
-mkdir -p $destdir/$prefix
-
-# Deploy the tarball to /opt/logstash
-tar="$(dirname $0)/../build/logstash-$VERSION.tar.gz"
-if [ ! -f "$tar" ] ; then
-echo "Unable to find $tar"
-exit 1
-fi
-
-tar -C $destdir/$prefix --strip-components 1 -zxpf $tar
-
-case $os@$release in
- centos@*|fedora@*|el6@*|sl6@*)
-    mkdir -p $destdir/etc/logrotate.d
-    mkdir -p $destdir/etc/sysconfig
-    mkdir -p $destdir/etc/init.d
-    mkdir -p $destdir/etc/logstash/conf.d
-    mkdir -p $destdir/opt/logstash/tmp
-    mkdir -p $destdir/var/lib/logstash
-    mkdir -p $destdir/var/run/logstash
-    mkdir -p $destdir/var/log/logstash
-    chmod 0755 $destdir/opt/logstash/bin/logstash
-    install -m644 logrotate.conf $destdir/etc/logrotate.d/logstash
-    install -m644 logstash.default $destdir/etc/sysconfig/logstash
-    install -m755 logstash.sysv $destdir/etc/init.d/logstash
-    ;;
-  ubuntu@*|debian@*)
-    mkdir -p $destdir/etc/logstash/conf.d
-    mkdir -p $destdir/etc/logrotate.d
-    mkdir -p $destdir/etc/init
-    mkdir -p $destdir/etc/init.d
-    mkdir -p $destdir/var/lib/logstash
-    mkdir -p $destdir/var/log/logstash
-    mkdir -p $destdir/etc/default
-    touch $destdir/etc/default/logstash
-    install -m644 logrotate.conf $destdir/etc/logrotate.d/logstash
-    install -m644 logstash.default $destdir/etc/default/logstash
-    install -m755 logstash.upstart.ubuntu $destdir/etc/init/logstash.conf
-    install -m755 logstash.sysv $destdir/etc/init.d/logstash
-    ;;
-  *)
-    echo "Unknown OS: $os $release"
-    exit 1
-    ;;
-esac
-
-description="logstash is a system for managing and processing events and logs"
-case $os in
-  centos|fedora|redhat|sl)
-    fpm -s dir -t rpm -n logstash -v "$RELEASE" \
-      -a noarch --iteration "1_${RPM_REVISION}" \
-      --url "$URL" \
-      --description "$DESCRIPTION" \
-      -d "jre >= 1.6.0" \
-      --vendor "Elasticsearch" \
-      --license "ASL 2.0" \
-      --rpm-use-file-permissions \
-      --rpm-user root --rpm-group root \
-      --before-install centos/before-install.sh \
-      --before-remove centos/before-remove.sh \
-      --after-install centos/after-install.sh \
-      --config-files etc/sysconfig/logstash \
-      --config-files etc/logrotate.d/logstash \
-      -f -C $destdir .
-    ;;
-  ubuntu|debian)
-    if ! echo $RELEASE | grep -q '\.(dev\|rc.*)'; then
-      # This is a dev or RC version... So change the upstream version
-      # example: 1.2.2.dev => 1.2.2~dev
-      # This ensures a clean upgrade path.
-      RELEASE="$(echo $RELEASE | sed 's/\.\(dev\|rc.*\)/~\1/')"
-    fi
-
-    fpm -s dir -t deb -n logstash -v "$RELEASE" \
-      -a all --iteration "1-${DEB_REVISION}" \
-      --url "$URL" \
-      --description "$DESCRIPTION" \
-      --vendor "Elasticsearch" \
-      --license "Apache 2.0" \
-      -d "java7-runtime-headless | java6-runtime-headless | j2re1.7" \
-      --deb-user root --deb-group root \
-      --before-install $os/before-install.sh \
-      --before-remove $os/before-remove.sh \
-      --after-install $os/after-install.sh \
-      --config-files /etc/default/logstash \
-      --config-files /etc/logrotate.d/logstash \
-      -f -C $destdir .
-    ;;
-esac
diff --git a/pkg/debian/after-install.sh b/pkg/debian/after-install.sh
index 5e0fc08f830..69c3bbe7f40 100644
--- a/pkg/debian/after-install.sh
+++ b/pkg/debian/after-install.sh
@@ -3,3 +3,4 @@
 chown -R logstash:logstash /opt/logstash
 chown logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
+chmod 755 /etc/logstash
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
index ac4ed6d1998..d7ad053334d 100755
--- a/pkg/logstash.sysv
+++ b/pkg/logstash.sysv
@@ -50,11 +50,20 @@ start() {
   HOME=${LS_HOME}
   export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
 
+  # chown doesn't grab the suplimental groups when setting the user:group - so we have to do it for it.
+  # Boy, I hope we're root here. 
+  SGROUPS=$(id -Gn "$LS_USER" | tr " " "," | sed 's/,$//'; echo '')
+
+  if [ ! -z $SGROUPS ]
+  then
+	EXTRA_GROUPS="--groups $SGROUPS"
+  fi
+
   # set ulimit as (root, presumably) first, before we drop privileges
   ulimit -n ${LS_OPEN_FILES}
 
   # Run the program!
-  nice -n ${LS_NICE} chroot --userspec $LS_USER:$LS_GROUP / sh -c "
+  nice -n ${LS_NICE} chroot --userspec $LS_USER:$LS_GROUP $EXTRA_GROUPS / sh -c "
     cd $LS_HOME
     ulimit -n ${LS_OPEN_FILES}
     exec \"$program\" $args
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index 0f27581951b..e2637f09995 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -1,9 +1,12 @@
+require "logstash/version"
+
 namespace "artifact" do
 
   def package_files
     [
       "LICENSE",
-      "CHANGELOG",
+      "CHANGELOG.md",
+      "NOTICE.TXT",
       "CONTRIBUTORS",
       "bin/**/*",
       "lib/bootstrap/**/*",
@@ -51,7 +54,20 @@ namespace "artifact" do
     File.open(".bundle/config", "w") { }
   end
 
-  task "prepare" => ["bootstrap", "plugin:install-default", "plugin:install-local-logstash-core-gem", "clean-bundle-config"]
+  # locate the "gem "logstash-core" ..." line in Gemfile, and if the :path => "." option if specified
+  # build and install the local logstash-core gem otherwise just do nothing, bundler will deal with it.
+  task "install-logstash-core" do
+    lines = File.readlines("Gemfile")
+    matches = lines.select{|line| line[/^gem\s+["']logstash-core["']/i]}
+    abort("ERROR: Gemfile format error, need a single logstash-core gem specification") if matches.size != 1
+    if matches.first =~ /:path\s*=>\s*["']\.["']/
+      Rake::Task["plugin:install-local-logstash-core-gem"].invoke
+    else
+      puts("[artifact:install-logstash-core] using logstash-core from Rubygems")
+    end
+  end
+
+  task "prepare" => ["bootstrap", "plugin:install-default", "install-logstash-core", "clean-bundle-config"]
 
   desc "Build a tar.gz of logstash with all dependencies"
   task "tar" => ["prepare"] do
@@ -91,7 +107,6 @@ namespace "artifact" do
   end
 
   task "zip" => ["prepare"] do
-    Rake::Task["dependency:rubyzip"].invoke
     require 'zip'
     zippath = "build/logstash-#{LOGSTASH_VERSION}.zip"
     puts("[artifact:zip] building #{zippath}")
@@ -106,8 +121,6 @@ namespace "artifact" do
   end
 
   def package(platform, version)
-    Rake::Task["dependency:fpm"].invoke
-    Rake::Task["dependency:stud"].invoke
     require "stud/temporary"
     require "fpm/errors" # TODO(sissel): fix this in fpm
     require "fpm/package/dir"
@@ -197,6 +210,13 @@ namespace "artifact" do
     out.vendor = "Elasticsearch"
     out.dependencies << "logrotate"
 
+    # Because we made a mistake in naming the RC version numbers, both rpm and deb view
+    # "1.5.0.rc1" higher than "1.5.0". Setting the epoch to 1 ensures that we get a kind
+    # of clean slate as to how we compare package versions. The default epoch is 0, and
+    # epoch is sorted first, so a version 1:1.5.0 will have greater priority
+    # than 1.5.0.rc4
+    out.epoch = 1
+
     # We don't specify a dependency on Java because:
     # - On Red Hat, Oracle and Red Hat both label their java packages in
     #   incompatible ways. Further, there is no way to guarantee a qualified
diff --git a/rakelib/benchmark.rake b/rakelib/benchmark.rake
new file mode 100644
index 00000000000..148922f6531
--- /dev/null
+++ b/rakelib/benchmark.rake
@@ -0,0 +1,8 @@
+namespace :benchmark do
+  desc "Run benchmark code in benchmark/*.rb"
+  task :run => ["test:setup"] do
+    path = File.join(LogStash::Environment::LOGSTASH_HOME, "benchmark", "*.rb")
+    Dir.glob(path).each { |f| require f }
+  end
+end
+task :benchmark => "benchmark:run"
diff --git a/rakelib/default_plugins.rb b/rakelib/default_plugins.rb
index ba57271ca8a..988cc9bd791 100644
--- a/rakelib/default_plugins.rb
+++ b/rakelib/default_plugins.rb
@@ -55,6 +55,7 @@ module RakeLib
       logstash-input-gelf
       logstash-input-generator
       logstash-input-graphite
+      logstash-input-http
       logstash-input-imap
       logstash-input-irc
       logstash-input-log4j
@@ -134,7 +135,13 @@ module RakeLib
       /^logstash-output-logentries$/,
       /^logstash-input-jdbc$/,
       /^logstash-output-newrelic$/,
-      /^logstash-output-slack$/
+      /^logstash-output-slack$/,
+      /^logstash-input-neo4j$/,
+      /^logstash-output-neo4j$/,
+      /^logstash-input-perfmon$/,
+      /^logstash-output-webhdfs$/,
+      /^logstash-input-rackspace$/,
+      /^logstash-output-rackspace$/
     ])
 
 
@@ -150,7 +157,7 @@ def self.fetch_all_plugins
 
     def self.is_released?(plugin)
       require 'gems'
-      !Gems.search(plugin).empty?
+      Gems.info(plugin) != "This rubygem could not be found."
     end
   end
 end
diff --git a/rakelib/docs.rake b/rakelib/docs.rake
index 5139fe4bc3b..9ba82c334f5 100644
--- a/rakelib/docs.rake
+++ b/rakelib/docs.rake
@@ -1,7 +1,6 @@
 namespace "docs" do
 
   task "generate" do
-    Rake::Task['dependency:octokit'].invoke
     Rake::Task['plugin:install-all'].invoke
     Rake::Task['docs:generate-docs'].invoke
     Rake::Task['docs:generate-index'].invoke
@@ -9,9 +8,9 @@ namespace "docs" do
 
   task "generate-docs" do
     require "bootstrap/environment"
-
-    list = Dir.glob("#{LogStash::Environment.logstash_gem_home}/gems/logstash-*/lib/logstash/{input,output,filter,codec}s/*.rb").join(" ")
-    cmd = "bin/logstash docgen -o asciidoc_generated #{list}"
+    pattern = "#{LogStash::Environment.logstash_gem_home}/gems/logstash-*/lib/logstash/{input,output,filter,codec}s/*.rb"
+    list    = Dir.glob(pattern).join(" ")
+    cmd     = "bin/logstash docgen -o asciidoc_generated #{list}"
     system(cmd)
   end
 
diff --git a/rakelib/plugin.rake b/rakelib/plugin.rake
index 48708667287..9c2065c1f56 100644
--- a/rakelib/plugin.rake
+++ b/rakelib/plugin.rake
@@ -50,7 +50,7 @@ namespace "plugin" do
     task.reenable # Allow this task to be run again
   end
 
-  task "install-all" => [ "dependency:octokit", "dependency:gems" ] do
+  task "install-all" do
     puts("[plugin:install-all] Installing all plugins from https://github.com/logstash-plugins")
     install_plugins("--no-verify", *LogStash::RakeLib.fetch_all_plugins)
 
@@ -61,6 +61,8 @@ namespace "plugin" do
     Dir["logstash-core*.gem"].each do |gem|
       rm(gem)
     end
+
+    task.reenable # Allow this task to be run again
   end
 
   task "build-logstash-core-gem" => [ "clean-logstash-core-gem" ] do
diff --git a/rakelib/test.rake b/rakelib/test.rake
index 8b307dd96d2..1e5a3408233 100644
--- a/rakelib/test.rake
+++ b/rakelib/test.rake
@@ -4,12 +4,19 @@
 require "pluginmanager/util"
 
 namespace "test" do
+
   task "setup" do
+    # Need to be run here as because if run aftewarse (after the bundler.setup task) then the report got wrong
+    # numbers and misses files. There is an issue with our setup! method as this does not happen with the regular
+    # bundler.setup used in regular bundler flows.
+    Rake::Task["test:setup-simplecov"].invoke if ENV['COVERAGE']
+
     require "bootstrap/environment"
-    LogStash::Bundler.setup!({:without => []})
+    LogStash::Bundler.setup!({:without => [:build]})
 
     require "rspec/core/runner"
     require "rspec"
+    require 'ci/reporter/rake/rspec_loader'
   end
 
   desc "run core specs"
@@ -44,6 +51,30 @@ namespace "test" do
   task "install-vendor-plugins" => ["bootstrap", "plugin:install-vendor", "plugin:install-development-dependencies"]
 
   task "install-jar-dependencies-plugins" => ["bootstrap", "plugin:install-jar-dependencies", "plugin:install-development-dependencies"]
+
+  # Setup simplecov to group files per functional modules, like this is easier to spot places with small coverage
+  task "setup-simplecov" do
+    require "simplecov"
+    SimpleCov.start do
+      # Skip non core related directories and files.
+      ["vendor/", "spec/", "bootstrap/rspec", "Gemfile", "gemspec"].each do |pattern|
+        add_filter pattern
+      end
+
+      add_group "bootstrap", "bootstrap/" # This module is used during bootstraping of LS
+      add_group "plugin manager", "pluginmanager/" # Code related to the plugin manager
+      add_group "core" do |src_file| # The LS core codebase
+        /logstash\/\w+.rb/.match(src_file.filename)
+      end
+      add_group "core-util", "logstash/util" # Set of LS utils module
+      add_group "core-config", "logstash/config" # LS Configuration modules
+      add_group "core-patches", "logstash/patches" # Patches used to overcome known issues in dependencies.
+      # LS Core plugins code base.
+      add_group "core-plugins", [ "logstash/codecs", "logstash/filters", "logstash/outputs", "logstash/inputs" ]
+    end
+    task.reenable
+  end
+
 end
 
 task "test" => [ "test:core" ]
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
index f026b2dae56..d92644506ff 100644
--- a/rakelib/vendor.rake
+++ b/rakelib/vendor.rake
@@ -1,6 +1,6 @@
 namespace "vendor" do
   VERSIONS = {
-    "jruby" => { "version" => "1.7.19", "sha1" => "a3296d1ae9b9aa78825b8d65a0d2498b449eaa3d" },
+    "jruby" => { "version" => "1.7.20", "sha1" => "3c11f01d38b9297cef2c281342f8bb799772e481" },
   }
 
   def vendor(*args)
@@ -42,7 +42,7 @@ namespace "vendor" do
             next if stat.size == entry_size && (stat.mode & 0777) == entry_mode
           end
         end
-        puts "Extracting #{entry.full_name} from #{tarball} #{entry_mode.to_s(8)}"
+        puts "Extracting #{entry.full_name} from #{tarball} #{entry_mode.to_s(8)}" if ENV['DEBUG']
         File.open(path, "wb") do |fd|
           # eof? check lets us skip empty files. Necessary because the API provided by
           # Archive::Tar::Minitar::Reader::EntryStream only mostly acts like an
@@ -53,7 +53,6 @@ namespace "vendor" do
             chunk = entry.read(16384)
             fd.write(chunk)
           end
-            #IO.copy_stream(entry, fd)
         end
         File.chmod(entry_mode, path)
       end
@@ -66,16 +65,12 @@ namespace "vendor" do
     info = VERSIONS[name]
     version = info["version"]
 
-    discard_patterns = Regexp.union([ /^samples/,
-                                      /@LongLink/,
-                                      /lib\/ruby\/1.8/,
-                                      /lib\/ruby\/2.0/,
-                                      /lib\/ruby\/shared\/rdoc/,
-
-                                      # Don't provide jar_installer.rb from jruby's release
-                                      # We'll provide a newer version with some bugfixes.
-                                      # See the 'vendor:jruby-patch' task for this.
-                                      /lib\/ruby\/shared\/jar_installer\.rb$/,
+    discard_patterns = Regexp.union([
+      /^samples/,
+      /@LongLink/,
+      /lib\/ruby\/1.8/,
+      /lib\/ruby\/2.0/,
+      /lib\/ruby\/shared\/rdoc/,
     ])
 
     url = "http://jruby.org.s3.amazonaws.com/downloads/#{version}/jruby-bin-#{version}.tar.gz"
@@ -93,15 +88,8 @@ namespace "vendor" do
       next if out =~ discard_patterns
       vendor(name, out)
     end # untar
-    Rake::Task["vendor:jruby-patch"].invoke
   end # jruby
 
-  task "jruby-patch" do |task, args|
-    # Patch JRuby's old jar-dependencies thing. This fixes bugs on windows
-    patched_jar_installer = File.join(File.dirname(__FILE__), "..", "tools", "patches", "jar_installer.rb")
-    patch_target = File.join(File.dirname(__FILE__), "..", "vendor", "jruby", "lib", "ruby", "shared", "jar_installer.rb")
-    FileUtils.cp(patched_jar_installer, patch_target)
-  end
   task "all" => "jruby"
 
   task "system_gem", :jruby_bin, :name, :version do |task, args|
@@ -128,7 +116,6 @@ namespace "vendor" do
     require "bootstrap/environment"
 
     Rake::Task["dependency:rbx-stdlib"] if LogStash::Environment.ruby_engine == "rbx"
-    Rake::Task["dependency:stud"].invoke
     Rake::Task["dependency:bundler"].invoke
 
     puts("Invoking bundler install...")
diff --git a/rakelib/z_rubycheck.rake b/rakelib/z_rubycheck.rake
index 367369ffe3c..ed22ed016c7 100644
--- a/rakelib/z_rubycheck.rake
+++ b/rakelib/z_rubycheck.rake
@@ -1,6 +1,6 @@
 if ENV['USE_RUBY'] != '1'
   if RUBY_ENGINE != "jruby" or Gem.ruby !~ /vendor\/jruby\/bin\/jruby/
-    puts "Restarting myself under Vendored JRuby (currently #{RUBY_ENGINE} #{RUBY_VERSION})"  if $DEBUG
+    puts "Restarting myself under Vendored JRuby (currently #{RUBY_ENGINE} #{RUBY_VERSION})" if ENV['DEBUG']
 
     if ["mingw32", "mswin32"].include?(RbConfig::CONFIG["host_os"])
       # Use our own SSL certs when on Windows
diff --git a/spec/core/conditionals_spec.rb b/spec/core/conditionals_spec.rb
index cd57977d5f0..7dc9f7af46b 100644
--- a/spec/core/conditionals_spec.rb
+++ b/spec/core/conditionals_spec.rb
@@ -2,7 +2,7 @@
 
 module ConditionalFanciness
   def description
-    return example.metadata[:example_group][:description_args][0]
+    return self.metadata[:description]
   end
 
   def conditional(expression, &block)
diff --git a/spec/core/config_spec.rb b/spec/core/config_spec.rb
index d290c287ebc..917e0575916 100644
--- a/spec/core/config_spec.rb
+++ b/spec/core/config_spec.rb
@@ -48,6 +48,44 @@
   end
 
   context "#compile" do
+    context "if with multiline conditionals" do
+      let(:config) { <<-CONFIG }
+        filter {
+          if [something]
+             or [anotherthing]
+             or [onemorething] {
+          }
+        }
+      CONFIG
+      subject { LogStashConfigParser.new }
+         
+      it "should compile successfully" do
+        result = subject.parse(config)
+        expect(result).not_to(be_nil)
+        expect { eval(result.compile) }.not_to(raise_error)
+      end
+    end
+
+    context "elsif with multiline conditionals" do
+      let(:config) { <<-CONFIG }
+        filter {
+          if [notathing] {
+          } else if [something]
+                or [anotherthing]
+                or [onemorething] {
+          }
+        }
+      CONFIG
+      subject { LogStashConfigParser.new }
+         
+      it "should compile successfully" do
+        result = subject.parse(config)
+        expect(result).not_to(be_nil)
+        expect { eval(result.compile) }.not_to(raise_error)
+      end
+    end
+
+
     context "invalid configuration" do
       it "rejects duplicate hash key" do
         parser = LogStashConfigParser.new
diff --git a/spec/core/event_spec.rb b/spec/core/event_spec.rb
index 9c4a73f5fb4..e6bae63fc5f 100644
--- a/spec/core/event_spec.rb
+++ b/spec/core/event_spec.rb
@@ -38,6 +38,11 @@
         expect(subject["[foo][bar]"] = "zab").to eq("zab")
         expect(subject["[foo][bar]"]).to eq("zab")
       end
+
+      it "allow to set the @metadata key to a hash" do
+        subject["@metadata"] = { "action" => "index" }
+        expect(subject["[@metadata][action]"]).to eq("index")
+      end
     end
 
     context "#sprintf" do
@@ -58,6 +63,10 @@
         expect(subject.sprintf("%{+HH}")).to eq("00")
       end
 
+      it "should support mixed string" do
+        expect(subject.sprintf("foo %{+YYYY-MM-dd} %{type}")).to eq("foo 2013-01-01 sprintf")
+      end
+
       it "should raise error with %{+format} syntax when @timestamp field is missing", :if => RUBY_ENGINE == "jruby" do
         str = "logstash-%{+YYYY}"
         subj = subject.clone
@@ -86,6 +95,10 @@
       it "should allow to use nested hash from the metadata field" do
         expect(subject.sprintf("%{[@metadata][have-to-go][deeper]}")).to eq("inception")
       end
+
+      it "should return a json string if the key is a hash" do
+        expect(subject.sprintf("%{[j][k3]}")).to eq("{\"4\":\"m\"}")
+      end
     end
 
     context "#[]" do
@@ -118,32 +131,32 @@
 
     context "#include?" do
       it "should include existing fields" do
-        expect(subject.include?("c")).to be_true
-        expect(subject.include?("[c][d]")).to be_true
-        expect(subject.include?("[j][k4][0][nested]")).to be_true
+        expect(subject.include?("c")).to eq(true)
+        expect(subject.include?("[c][d]")).to eq(true)
+        expect(subject.include?("[j][k4][0][nested]")).to eq(true)
       end
 
       it "should include field with nil value" do
-        expect(subject.include?("nilfield")).to be_true
+        expect(subject.include?("nilfield")).to eq(true)
       end
 
       it "should include @metadata field" do
-        expect(subject.include?("@metadata")).to be_true
+        expect(subject.include?("@metadata")).to eq(true)
       end
 
       it "should include field within @metadata" do
-        expect(subject.include?("[@metadata][fancy]")).to be_true
+        expect(subject.include?("[@metadata][fancy]")).to eq(true)
       end
 
       it "should not include non-existing fields" do
-        expect(subject.include?("doesnotexist")).to be_false
-        expect(subject.include?("[j][doesnotexist]")).to be_false
-        expect(subject.include?("[tag][0][hello][yes]")).to be_false
+        expect(subject.include?("doesnotexist")).to eq(false)
+        expect(subject.include?("[j][doesnotexist]")).to eq(false)
+        expect(subject.include?("[tag][0][hello][yes]")).to eq(false)
       end
 
       it "should include within arrays" do
-        expect(subject.include?("[tags][0]")).to be_true
-        expect(subject.include?("[tags][1]")).to be_false
+        expect(subject.include?("[tags][0]")).to eq(true)
+        expect(subject.include?("[tags][1]")).to eq(false)
       end
     end
 
@@ -415,14 +428,14 @@
         end
       end
     end
+  end
 
-    context "signal events" do
-      it "should define the shutdown event" do
-        # the SHUTDOWN and FLUSH constants are part of the plugin API contract
-        # if they are changed, all plugins must be updated
-        expect(LogStash::SHUTDOWN).to be_a(LogStash::ShutdownEvent)
-        expect(LogStash::FLUSH).to be_a(LogStash::FlushEvent)
-      end
+  context "signal events" do
+    it "should define the shutdown event" do
+      # the SHUTDOWN and FLUSH constants are part of the plugin API contract
+      # if they are changed, all plugins must be updated
+      expect(LogStash::SHUTDOWN).to be_a(LogStash::ShutdownEvent)
+      expect(LogStash::FLUSH).to be_a(LogStash::FlushEvent)
     end
   end
 
diff --git a/spec/core/pipeline_spec.rb b/spec/core/pipeline_spec.rb
index 36cd876ebf0..c1f4c99d9a8 100644
--- a/spec/core/pipeline_spec.rb
+++ b/spec/core/pipeline_spec.rb
@@ -58,16 +58,13 @@ class TestPipeline < LogStash::Pipeline
 
 describe LogStash::Pipeline do
 
-  context "teardown" do
-
-    before(:each) do
-      LogStash::Plugin.stub(:lookup)
-        .with("input", "dummyinput").and_return(DummyInput)
-      LogStash::Plugin.stub(:lookup)
-        .with("codec", "plain").and_return(DummyCodec)
-      LogStash::Plugin.stub(:lookup)
-        .with("output", "dummyoutput").and_return(DummyOutput)
-    end
+context "teardown" do
+
+  before(:each) do
+    allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+    allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+  end
 
     let(:test_config_without_output_workers) {
       <<-eos
diff --git a/spec/coverage_helper.rb b/spec/coverage_helper.rb
new file mode 100644
index 00000000000..82710a67d70
--- /dev/null
+++ b/spec/coverage_helper.rb
@@ -0,0 +1,23 @@
+# Useful module to help loading all logstash content when
+# running coverage analysis
+module CoverageHelper
+
+  ##
+  # Skip list used to avoid loading certain patterns within
+  # the logstash directories, this patterns are excluded becuause
+  # of potential problems or because they are going to be loaded
+  # in another way.
+  ##
+  SKIP_LIST = Regexp.union([
+    /^lib\/bootstrap\/rspec.rb$/,
+    /^lib\/logstash\/util\/prctl.rb$/
+  ])
+
+  def self.eager_load
+    Dir.glob("lib/**/*.rb") do |file|
+      next if file =~ SKIP_LIST
+      require file
+    end
+  end
+
+end
diff --git a/spec/inputs/base_spec.rb b/spec/inputs/base_spec.rb
index 5fe8b362373..d87f07b49f6 100644
--- a/spec/inputs/base_spec.rb
+++ b/spec/inputs/base_spec.rb
@@ -1,6 +1,67 @@
 # encoding: utf-8
 require "spec_helper"
 
+# use a dummy NOOP input to test Inputs::Base
+class LogStash::Inputs::NOOP < LogStash::Inputs::Base
+  config_name "noop"
+  milestone 2
+
+  def register; end
+
+end
+
+describe "LogStash::Inputs::Base#decorate" do
+  it "should add tag" do
+    input = LogStash::Inputs::NOOP.new("tags" => "value")
+    evt = LogStash::Event.new({"type" => "noop"})
+    input.instance_eval {decorate(evt)}
+    expect(evt["tags"]).to eq(["value"])
+  end
+
+  it "should add multiple tag" do
+    input = LogStash::Inputs::NOOP.new("tags" => ["value1","value2"])
+    evt = LogStash::Event.new({"type" => "noop"})
+    input.instance_eval {decorate(evt)}
+    expect(evt["tags"]).to eq(["value1","value2"])
+  end
+
+  it "should allow duplicates  tag" do
+    input = LogStash::Inputs::NOOP.new("tags" => ["value","value"])
+    evt = LogStash::Event.new({"type" => "noop"})
+    input.instance_eval {decorate(evt)}
+    expect(evt["tags"]).to eq(["value","value"])
+  end
+
+  it "should add tag with sprintf" do
+    input = LogStash::Inputs::NOOP.new("tags" => "%{type}")
+    evt = LogStash::Event.new({"type" => "noop"})
+    input.instance_eval {decorate(evt)}
+    expect(evt["tags"]).to eq(["noop"])
+  end
+
+  it "should add single field" do
+    input = LogStash::Inputs::NOOP.new("add_field" => {"field" => "value"})
+    evt = LogStash::Event.new({"type" => "noop"})
+    input.instance_eval {decorate(evt)}
+    expect(evt["field"]).to eq("value")
+  end
+
+  it "should add single field with sprintf" do
+    input = LogStash::Inputs::NOOP.new("add_field" => {"%{type}" => "%{type}"})
+    evt = LogStash::Event.new({"type" => "noop"})
+    input.instance_eval {decorate(evt)}
+    expect(evt["noop"]).to eq("noop")
+  end
+
+  it "should add multiple field" do
+    input = LogStash::Inputs::NOOP.new("add_field" => {"field" => ["value1", "value2"], "field2" => "value"})
+    evt = LogStash::Event.new({"type" => "noop"})
+    input.instance_eval {decorate(evt)}
+    expect(evt["field"]).to eq(["value1","value2"])
+    expect(evt["field2"]).to eq("value")
+  end
+end
+
 describe "LogStash::Inputs::Base#fix_streaming_codecs" do
   it "should carry the charset setting along when switching" do
     require "logstash/inputs/tcp"
diff --git a/spec/lib/logstash/bundler_spec.rb b/spec/lib/logstash/bundler_spec.rb
index e821a668bd1..df6b4d169a5 100644
--- a/spec/lib/logstash/bundler_spec.rb
+++ b/spec/lib/logstash/bundler_spec.rb
@@ -30,6 +30,7 @@
     original_stderr = $stderr
 
     subject { LogStash::Bundler.invoke!(options) }
+
     # by default we want to fail fast on the test
     let(:options) { { :install => true, :max_tries => 0, :without => [:development]} }
     let(:bundler_args) { LogStash::Bundler.bundler_arguments(options) }
diff --git a/spec/lib/logstash/java_integration_spec.rb b/spec/lib/logstash/java_integration_spec.rb
index 0d4219a8e4c..a86ce6b382d 100644
--- a/spec/lib/logstash/java_integration_spec.rb
+++ b/spec/lib/logstash/java_integration_spec.rb
@@ -13,7 +13,7 @@
     context "Java::JavaUtil::ArrayList" do
 
       it "should report to be a Ruby Array" do
-        expect(Java::JavaUtil::ArrayList.new.is_a?(Array)).to be_true
+        expect(Java::JavaUtil::ArrayList.new.is_a?(Array)).to eq(true)
       end
 
       it "should be class equivalent to Ruby Array" do
@@ -26,13 +26,13 @@
           end
         end.not_to raise_error
 
-        expect(Array === Java::JavaUtil::ArrayList.new).to be_true
+        expect(Array === Java::JavaUtil::ArrayList.new).to eq(true)
       end
     end
 
     context "Java::JavaUtil::LinkedHashMap" do
       it "should report to be a Ruby Hash" do
-        expect(Java::JavaUtil::LinkedHashMap.new.is_a?(Hash)).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new.is_a?(Hash)).to eq(true)
       end
 
       it "should be class equivalent to Ruby Hash" do
@@ -45,7 +45,7 @@
           end
         end.not_to raise_error
 
-        expect(Hash === Java::JavaUtil::LinkedHashMap.new).to be_true
+        expect(Hash === Java::JavaUtil::LinkedHashMap.new).to eq(true)
       end
     end
   end
@@ -82,6 +82,23 @@
   context "Java::JavaUtil::Collection" do
     subject{Java::JavaUtil::ArrayList.new(initial_array)}
 
+    context "when inspecting" do
+      let(:items) { [:a, {:b => :c}] }
+      subject { java.util.ArrayList.new(items) }
+
+      it "should include the contents of the Collection" do
+        expect(subject.inspect).to include(items.inspect)
+      end
+
+      it "should include the class name" do
+        expect(subject.inspect).to include("ArrayList")
+      end
+
+      it "should include the hash code of the collection" do
+        expect(subject.inspect).to include(subject.hashCode.to_s)
+      end
+    end
+
     context "when deleting a unique instance" do
       let(:initial_array) {["foo", "bar"]}
 
@@ -200,57 +217,57 @@
     context "Java Map interface should report key with nil value as included" do
 
       it "should support include? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).include?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).include?("foo")).to eq(true)
       end
 
       it "should support has_key? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).has_key?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).has_key?("foo")).to eq(true)
       end
 
       it "should support member? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).member?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).member?("foo")).to eq(true)
       end
 
       it "should support key? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).key?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => nil}).key?("foo")).to eq(true)
       end
     end
 
     context "Java Map interface should report key with a value as included" do
 
       it "should support include? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).include?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).include?("foo")).to eq(true)
       end
 
       it "should support has_key? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).has_key?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).has_key?("foo")).to eq(true)
       end
 
       it "should support member? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).member?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).member?("foo")).to eq(true)
       end
 
       it "should support key? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).key?("foo")).to be_true
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).key?("foo")).to eq(true)
       end
     end
 
     context "Java Map interface should report non existing key as not included" do
 
       it "should support include? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).include?("bar")).to be_false
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1})).not_to include("bar")
       end
 
       it "should support has_key? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).has_key?("bar")).to be_false
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).has_key?("bar")).to eq(false)
       end
 
       it "should support member? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).member?("bar")).to be_false
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).member?("bar")).to eq(false)
       end
 
       it "should support key? method" do
-        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).key?("bar")).to be_false
+        expect(Java::JavaUtil::LinkedHashMap.new({"foo" => 1}).key?("bar")).to eq(false)
       end
     end
   end
diff --git a/spec/license_spec.rb b/spec/license_spec.rb
new file mode 100644
index 00000000000..e7a692f495e
--- /dev/null
+++ b/spec/license_spec.rb
@@ -0,0 +1,52 @@
+require 'spec_helper'
+require 'rakelib/default_plugins'
+
+describe "Project licenses" do
+
+  let(:expected_licenses) {
+    ##
+    # Expected licenses are Apache License 2.0, BSD license, MIT license and the ruby one,
+    # this not exclude that this list change in the feature.
+    ##
+    Regexp.union([ /mit/,
+                   /apache*/,
+                   /bsd/,
+                   /ruby/,
+                   /lgpl/])
+  }
+
+  shared_examples "runtime license test" do
+
+    subject(:gem_name) do |example|
+      example.metadata[:example_group][:parent_example_group][:description]
+    end
+
+    let(:spec) { Gem::Specification.find_all_by_name(gem_name)[0] }
+
+    it "have an expected license" do
+      spec.licenses.each do |license|
+        expect(license.downcase).to match(expected_licenses)
+      end
+    end
+
+    it "has runtime dependencies with expected licenses" do
+      spec.runtime_dependencies.map { |dep| dep.to_spec }.each do |runtime_spec|
+        next unless runtime_spec
+        runtime_spec.licenses.each do |license|
+          expect(license.downcase).to match(expected_licenses)
+        end
+      end
+    end
+  end
+
+  describe "logstash-core" do
+    it_behaves_like "runtime license test"
+  end
+
+  installed_plugins.each do |plugin|
+    describe plugin do
+      it_behaves_like "runtime license test"
+    end
+  end
+
+end
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index 6bf5945f26f..d52ff8be413 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -1 +1,10 @@
+require_relative 'coverage_helper'
+# In order to archive an expected coverage analysis we need to eager load
+# all logstash code base, otherwise it will not get a good analysis.
+CoverageHelper.eager_load if ENV['COVERAGE']
+
 require "logstash/devutils/rspec/spec_helper"
+
+def installed_plugins
+  Gem::Specification.find_all.select { |spec| spec.metadata["logstash_plugin"] }.map { |plugin| plugin.name }
+end
diff --git a/spec/util/accessors_spec.rb b/spec/util/accessors_spec.rb
index 0b1a15605bc..af719a32999 100644
--- a/spec/util/accessors_spec.rb
+++ b/spec/util/accessors_spec.rb
@@ -121,14 +121,6 @@
       expect(data).to eq({ "hello" => "foo" })
     end
 
-    it "should strict_set shallow string value" do
-      str = "[hello]"
-      data = {}
-      accessors = LogStash::Util::Accessors.new(data)
-      expect(accessors.strict_set(str, "foo")).to eq("foo")
-      expect(data).to eq({ "hello" => "foo"})
-    end
-
     it "should set deep string value" do
       str = "[hello][world]"
       data = {}
@@ -145,14 +137,6 @@
       expect(data).to eq({ "hello" => { "world" => ["foo", "bar"] } })
     end
 
-    it "should strict_set deep array value" do
-      str = "[hello][world]"
-      data = {}
-      accessors = LogStash::Util::Accessors.new(data)
-      expect(accessors.strict_set(str, ["foo", "bar"]) ).to eq(["foo", "bar"])
-      expect(data).to eq({ "hello" => { "world" => ["foo", "bar"] } })
-    end
-
     it "should set element within array value" do
       str = "[hello][0]"
       data = {"hello" => ["foo", "bar"]}
@@ -181,35 +165,6 @@
       accessors = LogStash::Util::Accessors.new(data)
       expect(accessors.del(str)).to eq(4)
       expect(data).to eq({ "geocoords" => [2] })
-    end  end
-
-  context "using invalid encoding" do
-    it "strinct_set should raise on non UTF-8 string encoding" do
-      str = "[hello]"
-      data = {}
-      accessors = LogStash::Util::Accessors.new(data)
-      expect { accessors.strict_set(str, "foo".encode("US-ASCII")) }.to raise_error
-    end
-
-    it "strinct_set should raise on non UTF-8 string encoding in array" do
-      str = "[hello]"
-      data = {}
-      accessors = LogStash::Util::Accessors.new(data)
-      expect { accessors.strict_set(str, ["foo", "bar".encode("US-ASCII")]) }.to raise_error
-    end
-
-    it "strinct_set should raise on invalid UTF-8 string encoding" do
-      str = "[hello]"
-      data = {}
-      accessors = LogStash::Util::Accessors.new(data)
-      expect { accessors.strict_set(str, "foo \xED\xB9\x81\xC3") }.to raise_error
-    end
-
-    it "strinct_set should raise on invalid UTF-8 string encoding in array" do
-      str = "[hello]"
-      data = {}
-      accessors = LogStash::Util::Accessors.new(data)
-      expect { accessors.strict_set(str, ["foo", "bar \xED\xB9\x81\xC3"]) }.to raise_error
     end
   end
 end
diff --git a/spec/util/fieldeval_spec.rb b/spec/util/fieldeval_spec.rb
deleted file mode 100644
index e2a70d14d5a..00000000000
--- a/spec/util/fieldeval_spec.rb
+++ /dev/null
@@ -1,96 +0,0 @@
-require "spec_helper"
-require "logstash/util/fieldreference"
-
-describe LogStash::Util::FieldReference, :if => true do
-
-  context "using simple accessor" do
-
-    it "should retrieve value" do
-      str = "hello"
-      m = eval(subject.compile(str))
-      data = { "hello" => "world" }
-      expect(m.call(data)).to eq(data[str])
-    end
-
-    it "should handle delete in block" do
-      str = "simple"
-      m = eval(subject.compile(str))
-      data = { "simple" => "things" }
-      m.call(data) { |obj, key| obj.delete(key) }
-      expect(data).to be_empty
-    end
-
-    it "should handle assignment in block" do
-      str = "simple"
-      m = eval(subject.compile(str))
-      data = {}
-      expect(m.call(data) { |obj, key| obj[key] = "things" }).to eq("things")
-      expect(data).to eq({ "simple" => "things" })
-    end
-
-    it "should handle assignment using set" do
-      str = "simple"
-      data = {}
-      expect(subject.set(str, "things", data)).to eq("things")
-      expect(data).to eq({ "simple" => "things" })
-    end
-  end
-
-  context "using accessor path" do
-
-    it "should retrieve shallow value" do
-      str = "[hello]"
-      m = eval(subject.compile(str))
-      data = { "hello" =>  "world" }
-      expect(m.call(data)).to eq("world")
-    end
-
-    it "should retrieve deep value" do
-      str = "[hello][world]"
-      m = eval(subject.compile(str))
-      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
-      expect(m.call(data)).to eq(data["hello"]["world"])
-    end
-
-    it "should handle delete in block" do
-      str = "[hello][world]"
-      m = eval(subject.compile(str))
-      data = { "hello" => { "world" => "foo", "bar" => "baz" } }
-      m.call(data) { |obj, key| obj.delete(key) }
-
-      # Make sure the "world" key is removed.
-      expect(data["hello"]).to eq({ "bar" => "baz" })
-    end
-
-    it "should not handle assignment in block" do
-      str = "[hello][world]"
-      m = eval(subject.compile(str))
-      data = {}
-      expect(m.call(data) { |obj, key| obj[key] = "things" }).to be_nil
-      expect(data).to be_empty
-    end
-
-    it "should set shallow value" do
-      str = "[hello]"
-      data = {}
-      expect(subject.set(str, "foo", data)).to eq("foo")
-      expect(data).to eq({ "hello" => "foo" })
-    end
-
-    it "should set deep value" do
-      str = "[hello][world]"
-      data = {}
-      expect(subject.set(str, "foo", data)).to eq("foo")
-      expect(data).to eq({ "hello" => { "world" => "foo" } })
-    end
-
-    it "should retrieve array item" do
-      data = { "hello" => { "world" => ["a", "b"], "bar" => "baz" } }
-      m = eval(subject.compile("[hello][world][0]"))
-      expect(m.call(data)).to eq(data["hello"]["world"][0])
-
-      m = eval(subject.compile("[hello][world][1]"))
-      expect(m.call(data)).to eq(data["hello"]["world"][1])
-    end
-  end
-end
diff --git a/spec/util/java_version_spec.rb b/spec/util/java_version_spec.rb
new file mode 100644
index 00000000000..35a409644b3
--- /dev/null
+++ b/spec/util/java_version_spec.rb
@@ -0,0 +1,66 @@
+require 'spec_helper'
+require 'logstash/util/java_version'
+
+describe "LogStash::Util::JavaVersion" do
+  subject(:mod) { LogStash::Util::JavaVersion }
+
+  it "should get the current java version if we're on Java" do
+    if LogStash::Environment.jruby?
+      expect(LogStash::Util::JavaVersion.version).to be_a(String)
+    end
+  end
+
+  it "should mark a bad beta version as bad" do
+    expect(mod.bad_java_version?("1.7.0_45-beta")).to be_truthy
+  end
+
+  it "should mark a bad standard version as bad" do
+    expect(mod.bad_java_version?("1.6.0")).to be_truthy
+  end
+
+  it "should mark a good standard java version as good" do
+    expect(mod.bad_java_version?("1.7.0_51")).to be_falsey
+  end
+
+  it "should mark a good beta version as good" do
+    expect(mod.bad_java_version?("1.8.0-beta")).to be_falsey
+  end
+
+  describe "parsing java versions" do
+    it "should return nil on a nil version" do
+      expect(mod.parse_java_version(nil)).to be_nil
+    end
+
+    shared_examples("version parsing") do |desc, string, major, minor, patch, update, build|
+      context("#{desc} with version #{string}") do
+        subject(:parsed) { LogStash::Util::JavaVersion.parse_java_version(string) }
+
+        it "should have the correct major version" do
+          expect(parsed[:major]).to eql(major)
+        end
+
+        it "should have the correct minor version" do
+          expect(parsed[:minor]).to eql(minor)
+        end
+
+        it "should have the correct patch version" do
+          expect(parsed[:patch]).to eql(patch)
+        end
+
+        it "should have the correct update version" do
+          expect(parsed[:update]).to eql(update)
+        end
+
+        it "should have the correct build string" do
+          expect(parsed[:build]).to eql(build)
+        end
+      end
+    end
+
+    include_examples("version parsing", "a plain version", "1.3.0", 1, 3, 0, 0, nil)
+    include_examples("version parsing", "an update", "1.4.0_03", 1, 4, 0, 3, nil)
+    include_examples("version parsing", "a build", "1.4.0-beta", 1, 4, 0, 0,"beta")
+    include_examples("version parsing", "an update+build", "1.4.0_03-beta", 1, 4, 0, 3, "beta")
+  end
+
+end
\ No newline at end of file
diff --git a/spec/util/json_spec.rb b/spec/util/json_spec.rb
index 9d5c44bce09..f0304f219c8 100644
--- a/spec/util/json_spec.rb
+++ b/spec/util/json_spec.rb
@@ -4,7 +4,7 @@
 require "logstash/environment"
 require "logstash/util"
 
-describe LogStash::Json do
+describe "LogStash::Json" do
 
   let(:hash)   {{"a" => 1}}
   let(:json_hash)   {"{\"a\":1}"}
@@ -33,27 +33,26 @@
   if LogStash::Environment.jruby?
 
     ### JRuby specific
-
+    # Former expectation in this code were removed because of https://github.com/rspec/rspec-mocks/issues/964
+    # as soon as is fix we can re introduce them if decired, however for now the completeness of the test
+    # is also not affected as the conversion would not work if the expectation where not meet.
+    ###
     context "jruby deserialize" do
       it "should respond to load and deserialize object" do
-        expect(JrJackson::Raw).to receive(:parse_raw).with(json_hash).and_call_original
         expect(LogStash::Json.load(json_hash)).to eql(hash)
       end
     end
 
     context "jruby serialize" do
       it "should respond to dump and serialize object" do
-        expect(JrJackson::Json).to receive(:dump).with(string).and_call_original
         expect(LogStash::Json.dump(string)).to eql(json_string)
       end
 
       it "should call JrJackson::Raw.generate for Hash" do
-        expect(JrJackson::Raw).to receive(:generate).with(hash).and_call_original
         expect(LogStash::Json.dump(hash)).to eql(json_hash)
       end
 
       it "should call JrJackson::Raw.generate for Array" do
-        expect(JrJackson::Raw).to receive(:generate).with(array).and_call_original
         expect(LogStash::Json.dump(array)).to eql(json_array)
       end
 
diff --git a/spec/util/plugin_version_spec.rb b/spec/util/plugin_version_spec.rb
index 62ad8955a28..21c35f87dff 100644
--- a/spec/util/plugin_version_spec.rb
+++ b/spec/util/plugin_version_spec.rb
@@ -1,18 +1,33 @@
 require "spec_helper"
 require "logstash/util/plugin_version"
 
-describe LogStash::Util::PluginVersion do
+describe "LogStash::Util::PluginVersion" do
+
   subject { LogStash::Util::PluginVersion }
 
   context "#find_version!" do
+
+    let(:gem)     { "bundler" }
+
     it 'raises an PluginNoVersionError if we cant find the plugin in the gem path' do
       dummy_name ='this-character-doesnt-exist-in-the-marvel-universe'
       expect { subject.find_version!(dummy_name) }.to raise_error(LogStash::PluginNoVersionError)
     end
 
     it 'returns the version of the gem' do
-      expect { subject.find_version!('bundler') }.not_to raise_error
+      expect { subject.find_version!(gem) }.not_to raise_error
     end
+
+    context "with a pre release gem" do
+
+      it 'return the version of the gem' do
+        # Gem::Specification.find_by_name return nil if the gem is not activated, as for
+        # example the pre release ones.
+        expect(Gem::Specification).to receive(:find_by_name).and_return(nil)
+        expect { subject.find_version!(gem) }.not_to raise_error
+      end
+    end
+
   end
 
   context "#new" do
diff --git a/spec/util/unicode_trimmer_spec.rb b/spec/util/unicode_trimmer_spec.rb
new file mode 100644
index 00000000000..796a21d50a3
--- /dev/null
+++ b/spec/util/unicode_trimmer_spec.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/util/unicode_trimmer"
+require "flores/rspec"
+require "flores/random"
+
+RSpec.configure do |config|
+  Flores::RSpec.configure(config)
+end
+
+describe "truncating unicode strings correctly" do
+  context "with extra bytes before the snip" do
+    let(:ustr) { "Testing «ταБЬℓσ»: 1<2 & 4+1>3, now 20% off!" }
+
+    it "should truncate to exact byte boundaries when possible" do
+      expect(UnicodeTrimmer.trim_bytes(ustr, 21).bytesize).to eql(21)
+    end
+
+    it "should truncate below the bytesize when splitting a byte" do
+      expect(UnicodeTrimmer.trim_bytes(ustr, 20).bytesize).to eql(18)
+    end
+
+    it "should not truncate the string when the bytesize is already OK" do
+      expect(UnicodeTrimmer.trim_bytes(ustr, ustr.bytesize)).to eql(ustr)
+    end
+  end
+
+  context "with extra bytes after the snip" do
+    let(:ustr) { ": 1<2 & 4+1>3, now 20% off! testing «ταБЬℓσ»" }
+
+    it "should truncate to exact byte boundaries when possible" do
+      expect(UnicodeTrimmer.trim_bytes(ustr, 21).bytesize).to eql(21)
+    end
+
+    it "should truncate below the bytesize when splitting a byte" do
+      expect(UnicodeTrimmer.trim_bytes(ustr, 52).bytesize).to eql(51)
+    end
+
+    it "should not truncate the string when the bytesize is already OK" do
+      expect(UnicodeTrimmer.trim_bytes(ustr, ustr.bytesize)).to eql(ustr)
+    end
+  end
+
+  context "randomized testing" do
+    let(:text) { Flores::Random.text(1..1000) }
+    let(:size) { Flores::Random.integer(1..text.bytesize) }
+    let(:expected_range) { (size - 4)..size }
+
+    stress_it "should be near the boundary of requested size" do
+      expect(expected_range).to include(UnicodeTrimmer.trim_bytes(text, size).bytesize)
+    end
+  end
+end
diff --git a/tools/patches/jar_installer.rb b/tools/patches/jar_installer.rb
deleted file mode 100644
index 56d00c7c82c..00000000000
--- a/tools/patches/jar_installer.rb
+++ /dev/null
@@ -1,279 +0,0 @@
-require 'jar_dependencies'
-require 'uri'
-module Jars
-  class JarInstaller
-
-    class Dependency
-
-      attr_reader :path, :file, :gav, :scope, :type, :coord
-
-      def self.new( line )
-        if line.match /:jar:|:pom:/
-          super
-        end
-      end
-
-      def setup_type( line )
-        if line.match /:pom:/
-          @type = :pom
-        elsif line.match /:jar:/
-          @type = :jar
-        end
-      end
-      private :setup_type
-
-      def setup_scope( line )
-        @scope =
-          case line
-          when /:provided:/
-            :provided
-          when /:test:/
-            :test
-          else
-            :runtime
-          end
-      end
-      private :setup_scope
-
-      def initialize( line )
-        setup_type( line )
-
-        line.sub!( /^\s+/, '' )
-        @coord = line.sub( /:[^:]+:([A-Z]:\\)?[^:]+$/, '' )
-        first, second = @coord.split( /:#{type}:/ )
-        group_id, artifact_id = first.split( /:/ )
-        parts = group_id.split( '.' )
-        parts << artifact_id
-        parts << second.split( /:/ )[ -1 ]
-        parts << File.basename( line.sub /.:/, '' )
-        @path = File.join( parts ).strip
-
-        setup_scope( line )
-
-        reg = /:jar:|:pom:|:test:|:compile:|:runtime:|:provided:|:system:/
-        @file = line.slice(@coord.length, line.length).sub(reg, '').strip
-        @system = nil != (line =~ /:system:/)
-        @gav = @coord.sub(reg, ':')
-      end
-
-      def system?
-        @system
-      end
-    end
-
-    def self.install_jars( write_require_file = false )
-      new.install_jars( write_require_file )
-    end
-
-    def self.vendor_jars( write_require_file = false )
-      new.vendor_jars( write_require_file )
-    end
-
-    def self.load_from_maven( file )
-      result = []
-      File.read( file ).each_line do |line|
-        dep = Dependency.new( line )
-        result << dep if dep
-      end
-      result
-    end
-
-    def self.write_require_file( require_filename )
-      FileUtils.mkdir_p( File.dirname( require_filename ) )
-      comment = '# this is a generated file, to avoid over-writing it just delete this comment'
-      if ! File.exists?( require_filename ) || File.read( require_filename ).match( comment )
-        f = File.open( require_filename, 'w' )
-        f.puts comment
-        f.puts "require 'jar_dependencies'"
-        f.puts
-        f
-      end
-    end
-
-    def self.vendor_file( dir, dep )
-      vendored = File.join( dir, dep.path )
-      FileUtils.mkdir_p( File.dirname( vendored ) )
-      FileUtils.cp( dep.file, vendored ) unless dep.system?
-    end
-
-    def self.write_dep( file, dir, dep, vendor )
-      return if dep.type != :jar || dep.scope != :runtime
-      if dep.system?
-        file.puts( "require( '#{dep.file}' )" ) if file
-      elsif dep.scope == :runtime
-        vendor_file( dir, dep ) if vendor
-        file.puts( "require_jar( '#{dep.gav.gsub( /:/, "', '" )}' )" ) if file
-      end
-    end
-
-    def self.install_deps( deps, dir, require_filename, vendor )
-      f = write_require_file( require_filename ) if require_filename
-      deps.each do |dep|
-        write_dep( f, dir, dep, vendor )
-      end
-      yield f if block_given? and f
-    ensure
-      f.close if f
-    end
-
-    def find_spec( allow_no_file )
-      specs = Dir[ '*.gemspec' ]
-      case specs.size
-      when 0
-        raise 'no gemspec found' unless allow_no_file
-      when 1
-        specs.first
-      else
-        raise 'more then one gemspec found. please specify a specfile' unless allow_no_file
-      end
-    end
-    private :find_spec
-
-    def initialize( spec = nil )
-      setup( spec )
-    end
-
-    def setup( spec = nil, allow_no_file = false )
-      spec ||= find_spec( allow_no_file )
-
-      case spec
-      when String
-        @specfile = File.expand_path( spec )
-        @basedir = File.dirname( @specfile )
-        spec =  eval( File.read( spec ) )
-      when Gem::Specification
-        if File.exists?( spec.spec_file )
-          @basedir = spec.gem_dir
-          @specfile = spec.spec_file
-        else
-          # this happens with bundle and local gems
-          # there the spec_file is "not installed" but inside
-          # the gem_dir directory
-          Dir.chdir( spec.gem_dir ) do
-            setup( nil, true )
-          end
-        end
-      when NilClass
-      else
-        raise 'spec must be either String or Gem::Specification'
-      end
-
-      @spec = spec
-    rescue
-      # for all those strange gemspec we skip looking for jar-dependencies
-    end
-
-    def ruby_maven_install_options=( options )
-      @options = options.dup
-      @options.delete( :ignore_dependencies )
-    end
-
-    def vendor_jars( write_require_file = true )
-      return unless has_jars?
-      # do not vendor only if set explicitly via ENV/system-properties
-      do_install( Jars.to_prop( Jars::VENDOR ) != 'false', write_require_file )
-    end
-
-    def install_jars( write_require_file = true )
-      return unless has_jars?
-      do_install( false, write_require_file )
-    end
-
-    private
-
-    def has_jars?
-      # first look if there are any requirements in the spec
-      # and then if gem depends on jar-dependencies
-      # only then install the jars declared in the requirements
-      ! @spec.requirements.empty? && @spec.dependencies.detect { |d| d.name == 'jar-dependencies' && d.type == :runtime }
-    end
-
-    def do_install( vendor, write_require_file )
-      vendor_dir = File.join( @basedir, @spec.require_path )
-      jars_file = File.join( vendor_dir, "#{@spec.name}_jars.rb" )
-
-      # write out new jars_file it write_require_file is true or
-      # check timestamps:
-      # do not generate file if specfile is older then the generated file
-      if ! write_require_file &&
-          File.exists?( jars_file ) &&
-          File.mtime( @specfile ) < File.mtime( jars_file )
-        # leave jars_file as is
-        jars_file = nil
-      end
-      self.class.install_deps( install_dependencies, vendor_dir,
-                               jars_file, vendor )
-    end
-
-    def setup_arguments( deps )
-      args = [ 'dependency:list', "-DoutputFile=#{deps}", '-DoutputAbsoluteArtifactFilename=true', '-DincludeTypes=jar', '-DoutputScope=true', '-f', @specfile ]
-
-      if Jars.debug?
-        args << '-X'
-      elsif not Jars.verbose?
-        args << '--quiet'
-      end
-
-      if Jars.maven_user_settings.nil? && (proxy = Gem.configuration[ :proxy ]).is_a?( String )
-        uri = URI.parse( proxy )
-        args << "-DproxySet=true"
-        args << "-DproxyHost=#{uri.host}"
-        args << "-DproxyPort=#{uri.port}"
-      end
-
-      if defined? JRUBY_VERSION
-        args << "-Dmaven.repo.local=#{java.io.File.new( Jars.home ).absolute_path}"
-      else
-        args << "-Dmaven.repo.local=#{File.expand_path( Jars.home )}"
-      end
-
-      args
-    end
-
-    def lazy_load_maven
-      require 'maven/ruby/maven'
-    rescue LoadError
-      install_ruby_maven
-      require 'maven/ruby/maven'
-    end
-
-    def install_ruby_maven
-      require 'rubygems/dependency_installer'
-      jars = Gem.loaded_specs[ 'jar-dependencies' ]
-      dep = jars.dependencies.detect { |d| d.name == 'ruby-maven' }
-      req = dep.nil? ? Gem::Requirement.create( '>0' ) : dep.requirement
-      inst = Gem::DependencyInstaller.new( @options || {} )
-      inst.install 'ruby-maven', req
-    rescue => e
-      warn e.backtrace.join( "\n" ) if Jars.verbose?
-      raise "there was an error installing 'ruby-maven'. please install it manually: #{e.inspect}"
-    end
-
-    def monkey_patch_gem_dependencies
-      # monkey patch to NOT include gem dependencies
-      require 'maven/tools/gemspec_dependencies'
-      eval <<EOF
-      class ::Maven::Tools::GemspecDependencies
-        def runtime; []; end
-        def development; []; end
-      end
-EOF
-    end
-
-    def install_dependencies
-      lazy_load_maven
-
-      monkey_patch_gem_dependencies
-
-      deps = File.join( @basedir, 'deps.lst' )
-
-      maven = Maven::Ruby::Maven.new
-      maven.verbose = Jars.verbose?
-      maven.exec( *setup_arguments( deps ) )
-
-      self.class.load_from_maven( deps )
-    ensure
-      FileUtils.rm_f( deps ) if deps
-    end
-  end
-end
