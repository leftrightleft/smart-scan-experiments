diff --git a/.gitignore b/.gitignore
index ced4dece32f..1dc5063f91f 100644
--- a/.gitignore
+++ b/.gitignore
@@ -26,3 +26,4 @@ spec/reports
 rspec.xml
 .install-done
 .vendor
+integration_run
diff --git a/CHANGELOG.md b/CHANGELOG.md
index cc5037655d9..f509d5a443e 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,138 @@
+## 2.1.2 (Jan 19, 2016)
+### general
+ - Fixed an issue with configtest switch in sysv init script ([#4321](https://github.com/elastic/logstash/pull/4321))
+ - Update jruby-openssl library to 0.9.13
+
+### input
+ - File:
+   - Added config option `close_older` which closes any files that remain unmodified for longer
+     than the specified timespan in seconds ([#44](https://github.com/logstash-plugins/logstash-input-file/issues/81))
+   - Added config option `ignore_older` which monitors if a file that was last modified before
+     the specified timespan in seconds, then its contents are ignored.([#44](https://github.com/logstash-plugins/logstash-input-file/issues/81)) ([#44](https://github.com/logstash-plugins/logstash-input-file/issues/81))
+ - Beats:
+   - Refactored beats input to fix thread synchronization issues under high data volume.([#14](https://github.com/logstash-plugins/logstash-input-beats/issues/14))
+ - Kafka:
+   - Fixed a CPU load issue when no new messages were available in Kafka broker ([#59](https://github.com/logstash-plugins/logstash-input-kafka/issues/59))
+ - http:
+   - Support compressed and gzip POST requests ([#33](https://github.com/logstash-plugins/logstash-input-http/issues/33))  
+
+### filter
+ - Date:
+   - The `timezone` setting now supports sprintf format ([#23](https://github.com/logstash-plugins/logstash-filter-date/issues/23))
+   - New year rollover should be handled better now when a year is not present in the time format.
+     If local time is December, and event time is January, the year will be set to next year. Similar
+     for if local time is January and Event time is December, the year will be set to the previous year.
+     This should help keep times correct in the upcoming year rollover ([#33](https://github.com/logstash-plugins/logstash-filter-date/issues/33))
+
+### output
+ - Elasticsearch:
+   - Added scripted update support ([#235](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/235))
+   - Changed retry behavior: too busy and service unavailable errors from ES are retried infinitely.
+     Never retry conflict errors ([#321](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/321))
+ - File:
+   - Added config setting to set dir and file permmission ([#18](https://github.com/logstash-plugins/logstash-output-file/issues/18))
+
+### codec
+  - Multiline: Added `auto_flush` config option, with no default. If not set, no auto_flush is done.
+    This feature flushes events buffered as part of a multiline event when used with
+    file input, for example ([#18](https://github.com/logstash-plugins/logstash-codec-multiline/pull/18)).
+
+## 2.1.1 (Dec 8, 2015)
+### general
+ - This release bundles a new version of JRuby - [1.7.23](http://jruby.org/2015/11/24/jruby-1-7-23.html), which fixes a memory leak issue reported on Windows when using the file input ([#3754](https://github.com/elastic/logstash/issues/3754)).
+ - Fixed Logstash fails to start if the parent directory contains a space ([#4283](https://github.com/elastic/logstash/issues/4283)).
+ - Allow Logstash to be launched from a symlink ([#4291](https://github.com/elastic/logstash/issues/4291)).
+
+### input
+  - File:
+    - Properly release file handles for older files which allows users to keep old files in the watched directory without having to delete/rename them ([#31](https://github.com/logstash-plugins/logstash-input-file/issues/31)).
+    - Clean up resource usage when the pipeline restarts an instance of the file input plugin ([#77](https://github.com/logstash-plugins/logstash-input-file/issues/77)).
+  - Twitter: Added proxy support ([#7](https://github.com/logstash-plugins/logstash-input-twitter/issues/7)).  
+
+### output
+  - Elasticsearch:
+    - Added option to set `_parent` in order to properly handle parent-child relationships ([#297](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/297)).
+    - Fixed a defect which caused Logstash to exhaust file handles when sniffing was used in http protocol ([#306](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/306)).   
+
+## 2.1.0 (Nov 24, 2015)
+### general
+ - Added ability to install and upgrade Logstash plugins without requiring internet connectivity (#2376).
+ - Support alternate or private Ruby gems server to install and update plugins (#3576).
+ - Added ability to reliably shutdown Logstash when there is a stall in event processing. This option
+   can be enabled by passing `--allow-unsafe-shutdown` flag while starting Logstash. Please be aware that
+   any in-flight events will be lost when shutdown happens (#3451)
+ - Fixed a memory leak which could be triggered when events having a date were serialized to string (#4222).
+ - Added JDBC input to default package.
+ - Adding --debug to --configtest now shows the configuration in blocks annotated by source config file. Very
+   useful when using multiple config files in a directory. (#3243)
+ - Reset default worker threads to 1 when using non thread-safe filters like multiline (#4130).
+ - Fixed file permissions for the logrotate configuration file.
+ - Changed the default heap size from 500MB to 1GB (#3861)
+ - Fixed config check option when starting Logstash through init scripts (#3645)
+
+### input
+  Twitter
+    - Added an option to fetch data from the sample Twitter streaming endpoint (#21).
+    - Added hashtags, symbols and user_mentions as data for the non extended tweet event (#22).
+    - Added an option to filter per location and language (#20).
+    - Added an option to stream data from a list of users (#11).
+  Beats
+    - Properly handle multiline events from multiple sources, originating from Filebeat (#10).
+
+  File
+    - Properly handle multiline events from multiple sources (#44).
+
+  Eventlog
+    - Change the underlying library to capture Event Logs from Windows more reliably (#11).
+
+### output
+  Elasticsearch
+    - Improved the default template to use doc_values wherever possible.
+    - Improved the default template to disable fielddata on analyzed string fields.
+    - Added New setting: timeout. This lets you control the behavior of a slow/stuck request to Elasticsearch
+      that could be, for example, caused by network, firewall, or load balancer issues (#260).
+
+## 2.0.0-beta2 (October 14, 2015)
+### general
+ - Better shutdown handling in Logstash core and its plugins. Previously, the shutdown
+   handling was through an injected exception which made it non-deterministic. The change
+   introduces cleaner APIs in the core to signal a shutdown event which can be used by
+   the plugins (#3210)
+ - Upgrade to JrJackson version 0.3.5 which fixes numerous bugs and also provides performance
+   boost for JSON handling (#3702)
+ - Better defaults: Out of the box, the default value of the filter_workers (-w) setting will be set
+   to half of the CPU cores of the machine. Increasing the workers provides parallelism in filter
+   execution which is crucial when doing heavier processing like complex grok patterns or the useragent
+   filter. You can still override the default by passing `-w` flag when starting Logstash (#3870)
+ - Improved default security configuration for SSL/TLS. Default is now TLS1.2 (#3955)
+ - Added obsolete setting which will cause a configuration error if a config marked obsolete
+   is used. The purpose of :obsolete is to help inform users when a setting has been completely removed.
+   The lifecycle of a plugin setting is now 4 phases: available, deprecated, obsolete, deleted. (#3977)
+
+### input
+ - Obsolete config settings (which were already deprecated): `debug`, `format`, `charset`, `message_format`.
+   Logstash will not start if you use these settings.
+
+### output
+ - Obsolete config settings (which were already deprecated): `type`, `tags`, `exclude_tags`.
+   Logstash will not start if you use these settings.
+
+### filter
+ - Obsolete config settings (which were already deprecated): `type`, `tags`, `exclude_tags`.
+   Logstash will not start if you use these settings.
+
+## 2.0.0-beta1 (September 15, 2015)
+### output
+  - Elasticsearch:
+    - Changed the default from node to http protocol.
+    - Backward incompatible config options. Renamed host to hosts
+    - Separate plugins for Java clients: transport and node options are not packaged by default but
+      can be installed using the logstash-output-elasticsearch_java plugin.
+    - Java client defaults to transport protocol  
+  - Kafka:
+    - Update to new 0.8.2 Java producer API with new producer configuration
+    - Backward incompatible config settings introduced to match Kafka options
+
 ## 1.5.4 (August 20, 2015)
 ### general
   - Reverted a change in our stronger ssl patch that prevented logstash-forwarder clients
@@ -13,7 +148,7 @@
 
 ### output
   - Lumberjack:
-    - For SSL certificate verification, The client now enforces the `VERIFY_PEER` mode when 
+    - For SSL certificate verification, The client now enforces the `VERIFY_PEER` mode when
        connecting to the server. ([#4](https://github.com/elastic/ruby-lumberjack/issues/4))
     - Added better handling of congestion scenario on the output by using a buffered send of events ([#7](https://github.com/logstash-plugins/logstash-output-lumberjack/pull/7))
   - Elasticsearch: Added the ability to update existing ES documents and support of upsert  -- if document doesn't exists, create it.([#116](https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/116))
@@ -28,13 +163,13 @@
   - Improved default security configuration for SSL (#3579).
   - For debian and rpm packages added ability to force stop Logstash. This can be enabled by setting
     the environment variable `KILL_ON_STOP_TIMEOUT=1` before stopping. If the Logstash process
-    has not stopped within a reasonable time, this will force it to shutdown. 
+    has not stopped within a reasonable time, this will force it to shutdown.
     **Note**: Please be aware that you could lose inflight messages if you force stop
     Logstash (#3578).
   - Added a periodic report of inflight events during shutdown. This provides feedback to users
     about events being processed while shutdown is being handled (#3484).
   - Added ability to install and use pre-released plugins (beta and RC versions)
-  - Fixed a permission issue in the init script for Debian and RPM packages. While running as 
+  - Fixed a permission issue in the init script for Debian and RPM packages. While running as
     logstash user it was not possible to access files owned by supplemental groups (#1449).
 
 ### codec
@@ -42,7 +177,7 @@
     individual events (#12).
 
 ### output
-  - Elasticsearch: 
+  - Elasticsearch:
     - Added support for sending http indexing requests through a forwarding proxy (#199).
     - Added support for using PKI/client certificates for authenticating requests to a secure
       Elasticsearch cluster (#170).
@@ -60,7 +195,7 @@
   - Plugin manager: Added validation and warning when updating plugins between major versions (#3383).
   - Performance improvements: String interpolation is widely used in LS to create keys combining dynamic
     values from extracted fields. Added a caching mechanism where we compile this template on first use
-    and reuse them subsequently, giving us a good performance gain in configs that do a lot of date 
+    and reuse them subsequently, giving us a good performance gain in configs that do a lot of date
     processing, sprintf, and use field reference syntax (#3425).
   - Added warning when LS is running on a JVM version which has known issues/bugs (#2547).  
   - Updated AWS based plugins to v2 of AWS ruby SDK. This involves an update to s3-input, s3-output,
@@ -69,7 +204,7 @@
 ### input
   - Lumberjack: This input was not handling backpressure properly from downstream plugins and
     would continue to accept data, eventually running out of memory. We added a circuit breaker to stop
-    accepting new connections when we detect this situation. Please note that `max_clients` setting 
+    accepting new connections when we detect this situation. Please note that `max_clients` setting
     intoduced in v0.1.9 has been deprecated. This setting temporarily solved the problem by configuring
     an upper limit to the number of LSF connections (#12).
   - Http: Added new input to receive data via http(s).
@@ -83,29 +218,29 @@
 
 ## 1.5.1 (June 16, 2015)
 ### general
-  - Fixed an issue which caused Logstash to hang when used with single worker (`-w 1`) configuration. 
+  - Fixed an issue which caused Logstash to hang when used with single worker (`-w 1`) configuration.
     This issue was caused by a deadlock in the internal queue when the filter worker was trying to
     exclusively remove elements which conflicted with the periodic flushing in filters (#3361).
-  - Fixed performance regression when using field reference syntax in config like `[tweet][username]`. 
+  - Fixed performance regression when using field reference syntax in config like `[tweet][username]`.
     This fix increases throughput in certain configs by 30% (#3238)
   - Windows: Added support to launch Logstash from a path with spaces (#2904)
   - Update to jruby-1.7.20 which brings in numerous fixes. This will also make file input work
     properly on FreeBSD.
   - Fixed regression in 1.5.0 where conditionals spread over multiple lines in a config was not
     working properly (#2850)
-  - Fixed a permission issue in rpm and debian repos. When Logstash was installed using these 
+  - Fixed a permission issue in rpm and debian repos. When Logstash was installed using these
     repos, only the logstash user was able to run commands like `bin/logstash version` (#3249)
 
 ### filter
   - GeoIP: Logstash no longer crashes when IPv6 addresses are used in lookup (#8)
 
 ### output
-  - Elasticsearch: 
+  - Elasticsearch:
     - Added an option to disable SSL certificate verification (#160)
     - Bulk requests were timing out because of aggressive timeout setting in the HTTP client.
       Restored this to 1.4.2 behavior where there are no timeouts by default. As a follow up
       to this, we will be exposing an option to control timeouts in the HTTP client (#103)
-  - JIRA: 
+  - JIRA:
     - Newly created issues now have description set (#3)
     - Summary field now expands variables in events
     - API authentication method changed from cookie to basic
@@ -120,45 +255,45 @@
 
 ## 1.5.0 (May 14, 2015)
 ### general
-  - Performance improvements: Logstash 1.5.0 is much faster -- we have improved the throughput 
+  - Performance improvements: Logstash 1.5.0 is much faster -- we have improved the throughput
     of grok filter in some cases by 100%. In our benchmark testing, using only grok filter and
-    ingesting apache logs, throughput increased from 34K eps to 50K eps. 
-    JSON serialization/deserialization are now implemented using JrJackson library which 
+    ingesting apache logs, throughput increased from 34K eps to 50K eps.
+    JSON serialization/deserialization are now implemented using JrJackson library which
     improved performance significantly. Ingesting JSON events 1.3KB in size measured a throughput
-    increase from 16Keps to 30K eps. With events 45KB in size, throughput increased from 
+    increase from 16Keps to 30K eps. With events 45KB in size, throughput increased from
     850 eps to 3.5K eps
-  - Fixed performance regressions from 1.4.2 especially for configurations which have 
+  - Fixed performance regressions from 1.4.2 especially for configurations which have
     conditionals in filter and output. Throughput numbers are either inline with 1.4.2
     or improved for certain configurations (#2870)  
-  - Add Plugin manager functionality to Logstash which allows to install, delete and 
+  - Add Plugin manager functionality to Logstash which allows to install, delete and
     update Logstash plugins. Plugins are separated from core and published to RubyGems.org
-  - Added the ability to install plugin gems built locally on top of Logstash. This will 
+  - Added the ability to install plugin gems built locally on top of Logstash. This will
     help plugin developers iterate and test locally without having to publish plugins (#2779)  
-  - With the release of Kibana 4, we have removed the `bin/logstash web` command and any reference 
+  - With the release of Kibana 4, we have removed the `bin/logstash web` command and any reference
     to Kibana from Logstash (#2661)
-  - Windows: Significantly improved the initial user experience with Windows platform (#2504, #1426). 
-    Fixed many issues related to File input. Added support for using the plugin 
+  - Windows: Significantly improved the initial user experience with Windows platform (#2504, #1426).
+    Fixed many issues related to File input. Added support for using the plugin
     framework (installing, upgrading, removing)  
   - Deprecated elasticsearch_http output plugin: All functionality is ported to
     logstash-output-elasticsearch plugin using http protocol (#1757). If you try to use
     the elasticsearch_http plugin, it will log a deprecated notice now.   
   - Fixed issue in core which was causing Logstash to not shutdown properly (#2796)    
-  - Added ability to add extra JVM options while running LS. You can use the LS_JAVA_OPTS 
+  - Added ability to add extra JVM options while running LS. You can use the LS_JAVA_OPTS
     environment variable to add to the default JVM options set out of the box. You could also
     completely overwrite all the default options if you wish by setting JAVA_OPTS before
     starting Logstash (#2942)
   - Fixed a regression from 1.4.2 where removing a tag in filter fails if the input event is
     JSON formatted (#2261)
   - Fixed issue where setting workers > 1 would trigger messages like
-    "You are using a deprecated config setting ..." (#2865) 
-  - Remove ability to run multiple subcommands from bin/logstash like 
+    "You are using a deprecated config setting ..." (#2865)
+  - Remove ability to run multiple subcommands from bin/logstash like
     bin/logstash agent -f something.conf -- web (#1747)  
   - Fixed Logstash crashing on converting from ASCII to UTF-8. This was caused by charset
     conversion issues in input codec (LOGSTASH-1789)
   - Allow storing 'metadata' to an event which is not sent/encoded on output. This eliminates
     the need for intermediate fields for example, while using date filter. (#1834)
   - Accept file and http uri in -f command line option for specifying config files (#1873)
-  - Filters that generated events (multiline, clone, split, metrics) now propagate those events 
+  - Filters that generated events (multiline, clone, split, metrics) now propagate those events
     correctly to future conditionals (#1431)
   - Fixed file descriptor leaks when using HTTP. The fix prevents Logstash from stalling, and
     in some cases crashing from out-of-memory errors (#1604, LOGSTASH-892)
@@ -173,31 +308,31 @@
   - Allow spaces in field references like [hello world] (#1513)    
 
 ### input
-  - Lumberjack: 
+  - Lumberjack:
     - Fixed Logstash crashes with Java Out Of Memory because of TCP thread leaks (#LOGSTASH-2168)
     - Created a temporary fix to handle out of memory and eventual Logstash crash resulting from
-      pipeline backpressure. With this fix, you can create an upper limit on the number of 
+      pipeline backpressure. With this fix, you can create an upper limit on the number of
       Lumberjack connections after which no new connections will be accepted. This is defaulted
       to 1000 connections, but can be changed using the config (#3003)
-    - Resolved issue where unrelated events were getting merged into a single event while using 
+    - Resolved issue where unrelated events were getting merged into a single event while using
       this input with with the multiline codec (#2016)
     - Fixed Logstash crashing because it was using old jls-lumberjack version (#7)  
-  - TCP: 
+  - TCP:
     - Fixed connection threads leak (#1509)
     - Fixed input host field also contains source port (LOGSTASH-1849)
   - Stdin: prevent overwrite of host field if already present in Event (#1668)
-  - Kafka: 
+  - Kafka:
     - Merged @joekiller's plugin to Logstash to get events from Kafka (#1472)
     - Added support for whitelisting and blacklisting topics in the input.
-  - S3: 
+  - S3:
     - Added IAM roles support so you can securely read and write events from S3 without providing your
-      AWS credentials (#1575). 
+      AWS credentials (#1575).
     - Added support for using temporary credentials obtained from AWS STS (#1946)
     - AWS credentials can be specfied through environment variables (#1619)  
-  - RabbitMQ: 
+  - RabbitMQ:
     - Fixed march_hare client uses incorrect connection url (LOGSTASH-2276)
     - Use Bunny 1.5.0+ (#1894)
-  - Twitter: added improvements, robustness, fixes. full_tweet option now works, we handle 
+  - Twitter: added improvements, robustness, fixes. full_tweet option now works, we handle
     Twitter rate limiting errors (#1471)
   - Syslog: if input does not match syslog format, add tag _grokparsefailure_sysloginputplugin
     which can be used to debug (#1593)
@@ -213,31 +348,31 @@
     in sync with any output like Elasticsearch by using this input  
   - EventLog: For Windows, this input gracefully shutsdown if there is a timeout while receiving events
     This also prevents Logstash from being stuck (#1672)
-  - Heartbeat: We created a new input plugin for generating heartbeat messages at periodic intervals. 
-    Use this to monitor Logstash -- you can measure the latency of the pipeline using these heartbeat 
+  - Heartbeat: We created a new input plugin for generating heartbeat messages at periodic intervals.
+    Use this to monitor Logstash -- you can measure the latency of the pipeline using these heartbeat
     events, and also check for availability
 
 ### filter
-  - Multiline: 
+  - Multiline:
     - Fixed an issue where Logstash would crash while processing JSON formatted events on
       Java 8 (#10)
     - Handled cases where we unintentionally deduplicated lines, such as repeated lines in
-      xml messages (#3) 
-  - Grok: 
+      xml messages (#3)
+  - Grok:
     - "break_on_match => false" option now works correctly (#1547)
     - allow user@hostname in commonapache log pattern (#1500 #1736)
     - use optimized ruby-grok library which improves throughput in some cases by 50% (#1657)
-  - Date: 
+  - Date:
     - Fixed match defaults to 1970-01-01 when none of the formats matches and UNIX format is present
       in the list (#1236, LOGSTASH-1597)
     - support parsing almost-ISO8601 patterns like 2001-11-06 20:45:45.123-0000 (without a T)
       which does not match %{TIMESTAMP_ISO8601}
-  - KV: allows dynamic include/exclude keys. For example, if an event has a key field and the user 
-    wants to parse out a value using the kv filter, the user should be able to 
+  - KV: allows dynamic include/exclude keys. For example, if an event has a key field and the user
+    wants to parse out a value using the kv filter, the user should be able to
     include_keys: [ "%{key}" ]
   - DNS: fixed add_tag adds tags even if filter was unsuccessful (#1785)
   - XML: fixed UndefinedConversionError with UTF-8 encoding (LOGSTASH-2246)
-  - Mutate: 
+  - Mutate:
     - Fixed nested field notation for convert option like 'convert => [ "[a][0]", "float" ]' (#1401)
     - Fixed issue where you can safely delete/rename fields which can have nil values (#2977)  
     - gsub evaluates variables like %{format} in the replacement text (#1529)
@@ -253,23 +388,23 @@
       requests (#1453)
     - Added support to be more resilient to transient errors in Elasticsearch. Previously, partial
       failures from the bulk indexing functionality were not handled properly. With this fix, we added
-      the ability to capture failed requests from Elasticsearch and retry them. Error codes like 
+      the ability to capture failed requests from Elasticsearch and retry them. Error codes like
       429 (too many requests) will now be retried by default for 3 times. The number of retries and the
       interval between consecutive retries can be configured (#1631)
     - Logstash does not create a "message.raw" by default whic is usually not_analyzed; this
       helps save disk space (#11)
-    - Added sniffing config to be able to list machines in the cluster while using the transport client (#22) 
+    - Added sniffing config to be able to list machines in the cluster while using the transport client (#22)
     - Deprecate the usage of index_type configuration. Added document_type to be consistent
       with document_id (#102)
     - Added warning when used with config embedded => true. Starting an embedded Elasticsearch
-      node is only recommended while prototyping. This should never be used in 
+      node is only recommended while prototyping. This should never be used in
       production setting (#99)
     - Added support for multiple hosts in configuration and enhanced stability
     - Logstash will not create a message.raw field by default now. Message field is not_analyzed
       by Elasticsearch and adding a multi-field was essentially doubling the disk space required,
       with no benefit
 
-  - S3: 
+  - S3:
     - Fixed a critical problem in the S3 Output plugin when using the size_file option. This could cause
       data loss and data corruption of old logs ()
     - Added IAM roles support so you can securely read and write events from S3 without providing your AWS
@@ -282,14 +417,14 @@
     dynamic string like /%{myfield}/, /test-%{myfield}/
   - RabbitMQ: fixed crash while running Logstash for longer periods, typically when there's no
     traffic on the logstash<->rabbitmq socket (LOGSTASH-1886)
-  - Statsd: fixed issue of converting very small float numbers to scientific notation 
+  - Statsd: fixed issue of converting very small float numbers to scientific notation
     like 9.3e-05 (#1670)
   - Fixed undefined method error when conditional on an output (#LOGSTASH-2288)
-  
+
 ### codec
   - Netflow: Fixed a JSON serialization issue while using this codec (#2945)
-  - Added new Elasticsearch bulk codec which can be used to read data formatted in the Elasticsearch 
-    Bulk API (multiline json) format. For example, this codec can be used in combination with RabbitMQ 
+  - Added new Elasticsearch bulk codec which can be used to read data formatted in the Elasticsearch
+    Bulk API (multiline json) format. For example, this codec can be used in combination with RabbitMQ
     input to mirror the functionality of the RabbitMQ Elasticsearch river
   - Cloudfront: Added support for handling Amazon CloudFront events
   - Avro: We added a new codec for data serialization (#1566)
diff --git a/Gemfile b/Gemfile
index 9e0a9318bf2..9b888845c02 100644
--- a/Gemfile
+++ b/Gemfile
@@ -2,11 +2,12 @@
 # If you modify this file manually all comments and formatting will be lost.
 
 source "https://rubygems.org"
-gem "logstash-core", "2.0.0.dev", :path => "."
+gem "logstash-core", "2.1.2.snapshot1"
 gem "file-dependencies", "0.1.6"
 gem "ci_reporter_rspec", "1.0.0", :group => :development
 gem "simplecov", :group => :development
 gem "coveralls", :group => :development
+gem "tins", "1.6", :group => :development
 gem "rspec", "~> 3.1.0", :group => :development
 gem "logstash-devutils", "~> 0", :group => :development
 gem "benchmark-ips", :group => :development
@@ -16,3 +17,107 @@ gem "fpm", "~> 1.3.3", :group => :build
 gem "rubyzip", "~> 1.1.7", :group => :build
 gem "gems", "~> 0.8.3", :group => :build
 gem "flores", "~> 0.0.6", :group => :development
+gem "logstash-input-heartbeat"
+gem "logstash-output-zeromq"
+gem "logstash-codec-collectd"
+gem "logstash-output-xmpp"
+gem "logstash-codec-dots"
+gem "logstash-codec-edn"
+gem "logstash-codec-edn_lines"
+gem "logstash-codec-fluent"
+gem "logstash-codec-es_bulk"
+gem "logstash-codec-graphite"
+gem "logstash-codec-json"
+gem "logstash-codec-json_lines"
+gem "logstash-codec-line"
+gem "logstash-codec-msgpack"
+gem "logstash-codec-multiline"
+gem "logstash-codec-netflow"
+gem "logstash-codec-oldlogstashjson"
+gem "logstash-codec-plain"
+gem "logstash-codec-rubydebug"
+gem "logstash-filter-anonymize"
+gem "logstash-filter-checksum"
+gem "logstash-filter-clone"
+gem "logstash-filter-csv"
+gem "logstash-filter-date"
+gem "logstash-filter-dns"
+gem "logstash-filter-drop"
+gem "logstash-filter-fingerprint"
+gem "logstash-filter-geoip"
+gem "logstash-filter-grok"
+gem "logstash-filter-json"
+gem "logstash-filter-kv"
+gem "logstash-filter-metrics"
+gem "logstash-filter-multiline"
+gem "logstash-filter-mutate"
+gem "logstash-filter-ruby"
+gem "logstash-filter-sleep"
+gem "logstash-filter-split"
+gem "logstash-filter-syslog_pri"
+gem "logstash-filter-throttle"
+gem "logstash-filter-urldecode"
+gem "logstash-filter-useragent"
+gem "logstash-filter-uuid"
+gem "logstash-filter-xml"
+gem "logstash-input-beats"
+gem "logstash-input-couchdb_changes"
+gem "logstash-input-elasticsearch"
+gem "logstash-input-eventlog"
+gem "logstash-input-exec"
+gem "logstash-input-file"
+gem "logstash-input-ganglia"
+gem "logstash-input-gelf"
+gem "logstash-input-generator"
+gem "logstash-input-graphite"
+gem "logstash-input-http"
+gem "logstash-input-imap"
+gem "logstash-input-irc"
+gem "logstash-input-log4j"
+gem "logstash-input-lumberjack"
+gem "logstash-input-pipe"
+gem "logstash-input-rabbitmq"
+gem "logstash-input-redis"
+gem "logstash-input-s3"
+gem "logstash-input-snmptrap"
+gem "logstash-input-sqs"
+gem "logstash-input-stdin"
+gem "logstash-input-syslog"
+gem "logstash-input-tcp"
+gem "logstash-input-twitter"
+gem "logstash-input-udp"
+gem "logstash-input-unix"
+gem "logstash-input-xmpp"
+gem "logstash-input-zeromq"
+gem "logstash-input-kafka"
+gem "logstash-output-cloudwatch"
+gem "logstash-output-csv"
+gem "logstash-output-elasticsearch"
+gem "logstash-output-email"
+gem "logstash-output-exec"
+gem "logstash-output-file"
+gem "logstash-output-ganglia"
+gem "logstash-output-gelf"
+gem "logstash-output-graphite"
+gem "logstash-output-hipchat"
+gem "logstash-output-http"
+gem "logstash-output-irc"
+gem "logstash-output-juggernaut"
+gem "logstash-output-lumberjack"
+gem "logstash-output-nagios"
+gem "logstash-output-nagios_nsca"
+gem "logstash-output-null"
+gem "logstash-output-opentsdb"
+gem "logstash-output-pagerduty"
+gem "logstash-output-pipe"
+gem "logstash-output-rabbitmq"
+gem "logstash-output-redis"
+gem "logstash-output-s3"
+gem "logstash-output-sns"
+gem "logstash-output-sqs"
+gem "logstash-output-statsd"
+gem "logstash-output-stdout"
+gem "logstash-output-tcp"
+gem "logstash-output-udp"
+gem "logstash-output-kafka"
+gem "logstash-input-jdbc"
diff --git a/Gemfile.jruby-1.9.lock b/Gemfile.jruby-1.9.lock
index 34c6e5625fa..9d7f2e55f64 100644
--- a/Gemfile.jruby-1.9.lock
+++ b/Gemfile.jruby-1.9.lock
@@ -1,54 +1,71 @@
-PATH
-  remote: .
-  specs:
-    logstash-core (2.0.0.dev-java)
-      cabin (~> 0.7.0)
-      clamp (~> 0.6.5)
-      filesize (= 0.0.4)
-      gems (~> 0.8.3)
-      i18n (= 0.6.9)
-      jrjackson (~> 0.2.9)
-      minitar (~> 0.5.4)
-      pry (~> 0.10.1)
-      stud (~> 0.0.19)
-      thread_safe (~> 0.3.5)
-      treetop (< 1.5.0)
-
 GEM
   remote: https://rubygems.org/
   specs:
     addressable (2.3.8)
     arr-pm (0.0.10)
       cabin (> 0)
-    backports (3.6.4)
-    benchmark-ips (2.2.0)
+    atomic (1.1.99-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.6.1)
+    aws-sdk (2.1.36)
+      aws-sdk-resources (= 2.1.36)
+    aws-sdk-core (2.1.36)
+      jmespath (~> 1.0)
+    aws-sdk-resources (2.1.36)
+      aws-sdk-core (= 2.1.36)
+    aws-sdk-v1 (1.66.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+    backports (3.6.7)
+    benchmark-ips (2.3.0)
+    bindata (2.1.0)
+    buftok (0.2.0)
     builder (3.2.2)
-    cabin (0.7.1)
-    childprocess (0.5.6)
+    cabin (0.7.2)
+    childprocess (0.5.9)
       ffi (~> 1.0, >= 1.0.11)
     ci_reporter (2.0.0)
       builder (>= 2.1.2)
     ci_reporter_rspec (1.0.0)
       ci_reporter (~> 2.0)
       rspec (>= 2.14, < 4)
+    cinch (2.3.1)
     clamp (0.6.5)
     coderay (1.1.0)
-    coveralls (0.8.1)
+    concurrent-ruby (0.9.2-java)
+    coveralls (0.8.10)
       json (~> 1.8)
       rest-client (>= 1.6.8, < 2)
-      simplecov (~> 0.10.0)
+      simplecov (~> 0.11.0)
       term-ansicolor (~> 1.3)
       thor (~> 0.19.1)
+      tins (~> 1.6.0)
     diff-lcs (1.2.5)
     docile (1.1.5)
-    domain_name (0.5.24)
+    domain_name (0.5.25)
       unf (>= 0.0.5, < 1.0.0)
-    faraday (0.9.1)
+    edn (1.1.0)
+    elasticsearch (1.0.15)
+      elasticsearch-api (= 1.0.15)
+      elasticsearch-transport (= 1.0.15)
+    elasticsearch-api (1.0.15)
+      multi_json
+    elasticsearch-transport (1.0.15)
+      faraday
+      multi_json
+    equalizer (0.0.10)
+    faraday (0.9.2)
       multipart-post (>= 1.2, < 3)
-    ffi (1.9.8-java)
+    ffi (1.9.10-java)
+    ffi-rzmq (2.0.4)
+      ffi-rzmq-core (>= 1.0.1)
+    ffi-rzmq-core (1.0.4)
+      ffi (~> 1.9)
     file-dependencies (0.1.6)
       minitar
     filesize (0.0.4)
+    filewatch (0.7.1)
     flores (0.0.6)
     fpm (1.3.3)
       arr-pm (~> 0.0.9)
@@ -58,34 +75,497 @@ GEM
       clamp (~> 0.6)
       ffi
       json (>= 1.7.7)
+    gelf (1.3.2)
+      json
+    gelfd (0.2.0)
     gem_publisher (1.5.0)
     gems (0.8.3)
+    geoip (1.6.1)
+    gmetric (0.1.3)
+    hipchat (1.5.2)
+      httparty
+      mimemagic
+    hitimes (1.2.3-java)
+    http (0.9.8)
+      addressable (~> 2.3)
+      http-cookie (~> 1.0)
+      http-form_data (~> 1.0.1)
+      http_parser.rb (~> 0.6.0)
     http-cookie (1.0.2)
       domain_name (~> 0.5)
+    http-form_data (1.0.1)
+    http_parser.rb (0.6.0-java)
+    httparty (0.13.7)
+      json (~> 1.8)
+      multi_xml (>= 0.5.2)
     i18n (0.6.9)
     insist (1.0.0)
-    jrjackson (0.2.9)
-    json (1.8.2-java)
-    logstash-devutils (0.0.14-java)
+    jar-dependencies (0.3.1)
+    jls-grok (0.11.2)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.26)
+      concurrent-ruby
+    jmespath (1.1.3)
+    jrjackson (0.3.8)
+    jruby-kafka (1.5.0-java)
+      jar-dependencies (~> 0)
+      ruby-maven (~> 3.3.8)
+    jruby-openssl (0.9.13-java)
+    json (1.8.3-java)
+    kramdown (1.9.0)
+    logstash-codec-collectd (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-dots (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-edn (2.0.2)
+      edn
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-edn_lines (2.0.2)
+      edn
+      logstash-codec-line
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-es_bulk (2.0.2)
+      logstash-codec-line
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-fluent (2.0.2-java)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      msgpack-jruby
+    logstash-codec-graphite (2.0.2)
+      logstash-codec-line
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-json (2.0.4)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-json_lines (2.0.3)
+      logstash-codec-line
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-line (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-msgpack (2.0.2-java)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      msgpack-jruby
+    logstash-codec-multiline (2.0.6)
+      jls-grok (~> 0.11.1)
+      logstash-core (>= 2.0.0, < 3.0.0)
+      logstash-patterns-core
+    logstash-codec-netflow (2.0.3)
+      bindata (>= 1.5.0)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-oldlogstashjson (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-plain (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-codec-rubydebug (2.0.5)
+      awesome_print
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-core (2.1.2.snapshot1-java)
+      cabin (~> 0.7.0)
+      clamp (~> 0.6.5)
+      concurrent-ruby (= 0.9.2)
+      filesize (= 0.0.4)
+      gems (~> 0.8.3)
+      i18n (= 0.6.9)
+      jrjackson (~> 0.3.7)
+      jruby-openssl (= 0.9.13)
+      minitar (~> 0.5.4)
+      pry (~> 0.10.1)
+      rubyzip (~> 1.1.7)
+      stud (~> 0.0.19)
+      thread_safe (~> 0.3.5)
+      treetop (< 1.5.0)
+    logstash-devutils (0.0.18-java)
       gem_publisher
       insist (= 1.0.0)
+      kramdown
       minitar
       rake
       rspec (~> 3.1.0)
+      rspec-wait
+      stud (>= 0.0.20)
+    logstash-filter-anonymize (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      murmurhash3
+    logstash-filter-checksum (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-clone (2.0.4)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-csv (2.1.1)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-date (2.1.1)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-input-generator
+      logstash-output-null
+    logstash-filter-dns (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-drop (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-fingerprint (2.0.3)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      murmurhash3
+    logstash-filter-geoip (2.0.5)
+      geoip (>= 1.3.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      lru_redux (~> 1.1.0)
+    logstash-filter-grok (2.0.3)
+      jls-grok (~> 0.11.1)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-patterns-core
+    logstash-filter-json (2.0.3)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-kv (2.0.3)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-metrics (3.0.0)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      metriks
+      thread_safe
+    logstash-filter-multiline (2.0.3)
+      jls-grok (~> 0.11.0)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-patterns-core
+    logstash-filter-mutate (2.0.3)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-filter-grok
+      logstash-patterns-core
+    logstash-filter-ruby (2.0.3)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-filter-date
+    logstash-filter-sleep (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-split (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-syslog_pri (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-throttle (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-urldecode (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-useragent (2.0.4)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      lru_redux (~> 1.1.0)
+      user_agent_parser (>= 2.0.0)
+    logstash-filter-uuid (2.0.3)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-filter-xml (2.1.1)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      nokogiri
+      xml-simple
+    logstash-input-beats (2.1.2)
+      concurrent-ruby (~> 0.9.2)
+      logstash-codec-multiline (~> 2.0.5)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0, < 3.0.0)
+      thread_safe (~> 0.3.5)
+    logstash-input-couchdb_changes (2.0.2)
+      json
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (>= 0.0.22)
+    logstash-input-elasticsearch (2.0.3)
+      elasticsearch (~> 1.0, >= 1.0.6)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-input-eventlog (3.0.1)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (~> 0.0.22)
+      win32-eventlog (~> 0.6.5)
+    logstash-input-exec (2.0.4)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (~> 0.0.22)
+    logstash-input-file (2.1.3)
+      addressable
+      filewatch (~> 0.7, >= 0.7.1)
+      logstash-codec-multiline (~> 2.0.5)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0, < 3.0.0)
+    logstash-input-ganglia (2.0.4)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (~> 0.0.22)
+    logstash-input-gelf (2.0.2)
+      gelfd (= 0.2.0)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (~> 0.0.22)
+    logstash-input-generator (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-input-graphite (2.0.5)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-input-tcp
+    logstash-input-heartbeat (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud
+    logstash-input-http (2.1.0)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      puma (~> 2.11.3)
+      stud
+    logstash-input-imap (2.0.3)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      mail
+      stud (~> 0.0.22)
+    logstash-input-irc (2.0.3)
+      cinch
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (~> 0.0.22)
+    logstash-input-jdbc (2.1.1)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      rufus-scheduler
+      sequel
+      tzinfo
+      tzinfo-data
+    logstash-input-kafka (2.0.3)
+      jruby-kafka (>= 1.2.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (>= 0.0.22, < 0.1.0)
+    logstash-input-log4j (2.0.5-java)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-input-lumberjack (2.0.5)
+      concurrent-ruby
+      jls-lumberjack (~> 0.0.26)
+      logstash-codec-multiline (~> 2.0.4)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-input-pipe (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (~> 0.0.22)
+    logstash-input-rabbitmq (3.1.2)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-mixin-rabbitmq_connection (>= 2.3.0, < 3.0.0)
+    logstash-input-redis (2.0.2)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      redis
+    logstash-input-s3 (2.0.4)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-input-snmptrap (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      snmp
+    logstash-input-sqs (2.0.3)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-input-stdin (2.0.2)
+      concurrent-ruby
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-input-syslog (2.0.2)
+      concurrent-ruby
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-filter-date
+      logstash-filter-grok
+      stud (>= 0.0.22, < 0.1.0)
+      thread_safe
+    logstash-input-tcp (3.0.1)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-input-twitter (2.2.0)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (>= 0.0.22, < 0.1)
+      twitter (= 5.15.0)
+    logstash-input-udp (2.0.3)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud (~> 0.0.22)
+    logstash-input-unix (2.0.4)
+      logstash-codec-line
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-input-xmpp (2.0.3)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      xmpp4r (= 0.5)
+    logstash-input-zeromq (2.0.2)
+      ffi-rzmq (~> 2.0.4)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-mixin-aws (2.0.2)
+      aws-sdk (~> 2.1.0)
+      aws-sdk-v1 (>= 1.61.0)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-mixin-http_client (2.2.0)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      manticore (>= 0.5.2, < 1.0.0)
+    logstash-mixin-rabbitmq_connection (2.3.0-java)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      march_hare (~> 2.15.0)
+      stud (~> 0.0.22)
+    logstash-output-cloudwatch (2.0.2)
+      aws-sdk
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-mixin-aws
+      rufus-scheduler (~> 3.0.9)
+    logstash-output-csv (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-filter-json
+      logstash-output-file
+    logstash-output-elasticsearch (2.4.0-java)
+      cabin (~> 0.6)
+      concurrent-ruby
+      elasticsearch (~> 1.0, >= 1.0.13)
+      logstash-core (>= 2.0.0, < 3.0.0)
+      manticore (>= 0.5.2, < 1.0.0)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-email (3.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      mail (~> 2.6.0, >= 2.6.3)
+    logstash-output-exec (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-file (2.2.3)
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-ganglia (2.0.2)
+      gmetric (= 0.1.3)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-gelf (2.0.3)
+      gelf (= 1.3.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-graphite (2.0.3)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-hipchat (3.0.2)
+      hipchat
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-http (2.1.0)
+      logstash-core (>= 2.0.0, < 3.0.0)
+      logstash-mixin-http_client (>= 2.2.0, < 3.0.0)
+    logstash-output-irc (2.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-juggernaut (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      redis
+    logstash-output-kafka (2.0.1)
+      jruby-kafka (>= 1.4.0, < 2.0.0)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-lumberjack (2.0.4)
+      jls-lumberjack (>= 0.0.26)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud
+    logstash-output-nagios (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-nagios_nsca (2.0.3)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-null (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-opentsdb (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-pagerduty (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-pipe (2.0.2)
+      logstash-codec-plain
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-rabbitmq (3.0.7-java)
+      logstash-core (>= 2.0.0, < 3.0.0)
+      logstash-mixin-rabbitmq_connection (>= 2.3.0, < 3.0.0)
+    logstash-output-redis (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      redis
+      stud
+    logstash-output-s3 (2.0.4)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-mixin-aws
+      stud (~> 0.0.22)
+    logstash-output-sns (3.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-sqs (2.0.2)
+      aws-sdk
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-mixin-aws
+      stud
+    logstash-output-statsd (2.0.5)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      logstash-input-generator
+      statsd-ruby (= 1.2.0)
+    logstash-output-stdout (2.0.4)
+      logstash-codec-line
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-tcp (2.0.2)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      stud
+    logstash-output-udp (2.0.2)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-output-xmpp (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+      xmpp4r (= 0.5)
+    logstash-output-zeromq (2.0.2)
+      ffi-rzmq (~> 2.0.4)
+      logstash-codec-json
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    logstash-patterns-core (2.0.2)
+      logstash-core (>= 2.0.0.beta2, < 3.0.0)
+    lru_redux (1.1.0)
+    mail (2.6.3)
+      mime-types (>= 1.16, < 3)
+    manticore (0.5.2-java)
+      jar-dependencies
+    march_hare (2.15.0-java)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
     method_source (0.8.2)
-    mime-types (2.5)
+    metriks (0.9.9.7)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.99)
+    mimemagic (0.3.1)
     minitar (0.5.4)
+    msgpack-jruby (1.4.1-java)
+    multi_json (1.11.2)
+    multi_xml (0.5.5)
     multipart-post (2.0.0)
-    netrc (0.10.3)
+    murmurhash3 (0.1.6-java)
+    naught (1.1.0)
+    netrc (0.11.0)
+    nokogiri (1.6.7.1-java)
     octokit (3.8.0)
       sawyer (~> 0.6.0, >= 0.5.3)
     polyglot (0.3.5)
-    pry (0.10.1-java)
+    pry (0.10.3-java)
       coderay (~> 1.1.0)
       method_source (~> 0.8.1)
       slop (~> 3.4)
       spoon (~> 0.0)
-    rake (10.4.2)
+    puma (2.11.3-java)
+      rack (>= 1.1, < 2.0)
+    rack (1.6.4)
+    rake (10.5.0)
+    redis (3.2.2)
     rest-client (1.8.0)
       http-cookie (>= 1.0.2, < 2.0)
       mime-types (>= 1.16, < 3.0)
@@ -102,28 +582,59 @@ GEM
     rspec-mocks (3.1.3)
       rspec-support (~> 3.1.0)
     rspec-support (3.1.2)
+    rspec-wait (0.0.8)
+      rspec (>= 2.11, < 3.5)
+    ruby-maven (3.3.8)
+      ruby-maven-libs (~> 3.3.1)
+    ruby-maven-libs (3.3.3)
     rubyzip (1.1.7)
+    rufus-scheduler (3.0.9)
+      tzinfo
     sawyer (0.6.0)
       addressable (~> 2.3.5)
       faraday (~> 0.8, < 0.10)
-    simplecov (0.10.0)
+    sequel (4.30.0)
+    simple_oauth (0.3.1)
+    simplecov (0.11.1)
       docile (~> 1.1.0)
       json (~> 1.8)
       simplecov-html (~> 0.10.0)
     simplecov-html (0.10.0)
     slop (3.6.0)
+    snmp (1.2.0)
     spoon (0.0.4)
       ffi
-    stud (0.0.21)
-    term-ansicolor (1.3.0)
+    statsd-ruby (1.2.0)
+    stud (0.0.22)
+    term-ansicolor (1.3.2)
       tins (~> 1.0)
     thor (0.19.1)
     thread_safe (0.3.5-java)
-    tins (1.5.1)
+    tins (1.6.0)
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
+    twitter (5.15.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (= 0.0.10)
+      faraday (~> 0.9.0)
+      http (>= 0.4, < 0.10)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.2)
+      thread_safe (~> 0.1)
+    tzinfo-data (1.2015.7)
+      tzinfo (>= 1.0.0)
     unf (0.1.4-java)
+    user_agent_parser (2.3.0)
+    win32-eventlog (0.6.5)
+      ffi
+    xml-simple (1.1.5)
+    xmpp4r (0.5)
 
 PLATFORMS
   java
@@ -136,10 +647,115 @@ DEPENDENCIES
   flores (~> 0.0.6)
   fpm (~> 1.3.3)
   gems (~> 0.8.3)
-  logstash-core (= 2.0.0.dev)!
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-es_bulk
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-oldlogstashjson
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-core (= 2.1.2.snapshot1)
   logstash-devutils (~> 0)
+  logstash-filter-anonymize
+  logstash-filter-checksum
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-multiline
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-uuid
+  logstash-filter-xml
+  logstash-input-beats
+  logstash-input-couchdb_changes
+  logstash-input-elasticsearch
+  logstash-input-eventlog
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-heartbeat
+  logstash-input-http
+  logstash-input-imap
+  logstash-input-irc
+  logstash-input-jdbc
+  logstash-input-kafka
+  logstash-input-log4j
+  logstash-input-lumberjack
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-input-xmpp
+  logstash-input-zeromq
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-email
+  logstash-output-exec
+  logstash-output-file
+  logstash-output-ganglia
+  logstash-output-gelf
+  logstash-output-graphite
+  logstash-output-hipchat
+  logstash-output-http
+  logstash-output-irc
+  logstash-output-juggernaut
+  logstash-output-kafka
+  logstash-output-lumberjack
+  logstash-output-nagios
+  logstash-output-nagios_nsca
+  logstash-output-null
+  logstash-output-opentsdb
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-statsd
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-xmpp
+  logstash-output-zeromq
   octokit (= 3.8.0)
   rspec (~> 3.1.0)
   rubyzip (~> 1.1.7)
   simplecov
   stud (~> 0.0.21)
+  tins (= 1.6)
diff --git a/LICENSE b/LICENSE
index 8026afdc77f..43976b73b2b 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,4 +1,4 @@
-Copyright (c) 20122015 Elasticsearch <http://www.elastic.co>
+Copyright (c) 20122016 Elasticsearch <http://www.elastic.co>
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
diff --git a/README.md b/README.md
index d6e6891a001..1faf0f4748c 100644
--- a/README.md
+++ b/README.md
@@ -1,9 +1,16 @@
 # Logstash [![Code Climate](https://codeclimate.com/github/elasticsearch/logstash/badges/gpa.svg)](https://codeclimate.com/github/elasticsearch/logstash) [![Coverage Status](https://coveralls.io/repos/elasticsearch/logstash/badge.svg?branch=origin%2Fmaster)](https://coveralls.io/r/elasticsearch/logstash?branch=origin%2Fmaster)
 
+### Build status
+
+| Branch | master | 2.x | 2.1
+|---|---|---|---|
+| core | [![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_2x/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_2x/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_21/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_21/) |
+| integration | [![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_2x/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_2x/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_21/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_21/) |
+
 Logstash is a tool for managing events and logs. You can use it to collect
 logs, parse them, and store them for later use (like, for searching).  If you
-store them in [Elasticsearch](http://www.elastic.co/guide/en/elasticsearch/reference/current/index.html),
-you can view and analyze them with [Kibana](http://www.elastic.co/guide/en/kibana/current/index.html).
+store them in [Elasticsearch](https://www.elastic.co/guide/en/elasticsearch/reference/current/index.html),
+you can view and analyze them with [Kibana](https://www.elastic.co/guide/en/kibana/current/index.html).
 
 It is fully free and fully open source. The license is Apache 2.0, meaning you
 are pretty much free to use it however you want in whatever way.
@@ -18,7 +25,7 @@ repositories under the [logstash-plugins](https://github.com/logstash-plugins) g
 gets published to RubyGems.org. Logstash has added plugin infrastructure to easily maintain the lifecyle of the plugin.
 For more details and rationale behind these changes, see our [blogpost](https://www.elastic.co/blog/plugin-ecosystem-changes/).
 
-[Elasticsearch logstash-contrib repo](https://github.com/elasticsearch/logstash-contrib) is deprecated. We
+[Elasticsearch logstash-contrib repo](https://github.com/elastic/logstash-contrib) is deprecated. We
 have moved all of the plugins that existed there into their own repositories. We are migrating all of the pull requests
 and issues from logstash-contrib to the new repositories.
 
@@ -26,9 +33,6 @@ For more info on developing and testing these plugins, please see the [README](h
 
 ### Plugin Issues and Pull Requests
 
-We are migrating all of the existing pull requests to their respective repositories. Rest assured, we will maintain
-all of the git history for these requests.
-
 **Please open new issues and pull requests for plugins under its own repository**
 
 For example, if you have to report an issue/enhancement for the Elasticsearch output, please do so [here](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues).
@@ -39,7 +43,7 @@ Logstash core will continue to exist under this repository and all related issue
 
 - [#logstash on freenode IRC](https://webchat.freenode.net/?channels=logstash)
 - [logstash-users on Google Groups](https://groups.google.com/d/forum/logstash-users)
-- [Logstash Documentation](http://www.elastic.co/guide/en/logstash/current/index.html)
+- [Logstash Documentation](https://www.elastic.co/guide/en/logstash/current/index.html)
 - [Logstash Product Information](https://www.elastic.co/products/logstash)
 - [Elastic Support](https://www.elastic.co/subscriptions)
 
@@ -66,7 +70,7 @@ To verify your environment, run `bin/logstash version` which should look like th
 
 ## Testing
 
-For tesing you can use the *test* `rake` tasks and the `bin/rspec` command, see instructions below. Note that the `bin/logstash rspec` command has been replaced by `bin/rspec`.
+For testing you can use the *test* `rake` tasks and the `bin/rspec` command, see instructions below. Note that the `bin/logstash rspec` command has been replaced by `bin/rspec`.
 
 ### Core tests
 
@@ -115,7 +119,7 @@ The documentation for developing plugins can be found in the plugins README, see
 
 ## Drip Launcher
 
-[Drip](https://github.com/ninjudd/drip) is a tool which help solve the slow JVM startup problem. The drip script is intended to be a drop-in replacement for the java command. We recommend using drip during development, in particular for running tests. Using drip, the first invokation of a command will not be faster but the subsequent commands will be swift.
+[Drip](https://github.com/ninjudd/drip) is a tool that solves the slow JVM startup problem. The drip script is intended to be a drop-in replacement for the java command. We recommend using drip during development, in particular for running tests. Using drip, the first invocation of a command will not be faster but the subsequent commands will be swift.
 
 To tell logstash to use drip, either set the `USE_DRIP=1` environment variable or set `` JAVACMD=`which drip` ``.
 
@@ -126,7 +130,7 @@ Examples:
 
 **Caveats**
 
-Drip does not work with STDIN. You cannot use drip for running configs which uses the stdin plugin.
+Drip does not work with STDIN. You cannot use drip for running configs which use the stdin plugin.
 
 
 ## Building
diff --git a/bin/bundle b/bin/bundle
index 605370c3261..420b7dbddce 100755
--- a/bin/bundle
+++ b/bin/bundle
@@ -9,14 +9,19 @@
 Signal.trap("INT") { exit 1 }
 
 require_relative "../lib/bootstrap/environment"
-Gem.clear_paths
-Gem.paths = ENV['GEM_HOME'] = ENV['GEM_PATH'] = LogStash::Environment.logstash_gem_home
+::Gem.clear_paths
+::Gem.paths = ENV['GEM_HOME'] = ENV['GEM_PATH'] = LogStash::Environment.logstash_gem_home
+
+ENV["BUNDLE_GEMFILE"] = LogStash::Environment::GEMFILE_PATH
 
 require "bundler"
 require "bundler/cli"
 require "bundler/friendly_errors"
 LogStash::Bundler.patch!
 
-Bundler.with_friendly_errors do
-  Bundler::CLI.start(ARGV, :debug => true)
+::Bundler.settings[:path] = LogStash::Environment::BUNDLE_DIR
+::Bundler.settings[:gemfile] = LogStash::Environment::GEMFILE_PATH
+
+::Bundler.with_friendly_errors do
+  ::Bundler::CLI.start(ARGV, :debug => true)
 end
diff --git a/bin/logstash b/bin/logstash
index 02e4446009c..2c39fb88011 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -19,7 +19,31 @@
 #   DEBUG=1 to output debugging information
 
 unset CDPATH
-. "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+# This unwieldy bit of scripting is to try to catch instances where Logstash
+# was launched from a symlink, rather than a full path to the Logstash binary
+if [ -L $0 ]; then
+  # Launched from a symlink
+  # --Test for the readlink binary
+  RL=$(which readlink)
+  if [ $? -eq 0 ]; then
+    # readlink exists
+    SOURCEPATH=$($RL $0)
+  else
+    # readlink not found, attempt to parse the output of stat
+    SOURCEPATH=$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\//' -e 's/\//')
+    if [ $? -ne 0 ]; then
+      # Failed to execute or parse stat
+      echo "Failed to find source library at path $(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+      echo "You may need to launch Logstash with a full path instead of a symlink."
+      exit 1
+    fi
+  fi
+else
+  # Not a symlink
+  SOURCEPATH=$0
+fi
+
+. "$(cd `dirname $SOURCEPATH`/..; pwd)/bin/logstash.lib.sh"
 setup
 
 case $1 in
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index f3704876d72..5c9a22dfc4f 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -1,9 +1,33 @@
 unset CDPATH
-LOGSTASH_HOME=$(cd `dirname $0`/..; pwd)
+# This unwieldy bit of scripting is to try to catch instances where Logstash
+# was launched from a symlink, rather than a full path to the Logstash binary
+if [ -L $0 ]; then
+  # Launched from a symlink
+  # --Test for the readlink binary
+  RL=$(which readlink)
+  if [ $? -eq 0 ]; then
+    # readlink exists
+    SOURCEPATH=$($RL $0)
+  else
+    # readlink not found, attempt to parse the output of stat
+    SOURCEPATH=$(stat -c %N $0 | awk '{print $3}' | sed -e 's/\//' -e 's/\//')
+    if [ $? -ne 0 ]; then
+      # Failed to execute or parse stat
+      echo "Failed to set LOGSTASH_HOME from $(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+      echo "You may need to launch Logstash with a full path instead of a symlink."
+      exit 1
+    fi
+  fi
+else
+  # Not a symlink
+  SOURCEPATH=$0
+fi
+
+LOGSTASH_HOME=$(cd `dirname $SOURCEPATH`/..; pwd)
 export LOGSTASH_HOME
 
 # Defaults you can override with environment variables
-LS_HEAP_SIZE="${LS_HEAP_SIZE:=500m}"
+LS_HEAP_SIZE="${LS_HEAP_SIZE:=1g}"
 
 setup_java() {
   if [ -z "$JAVACMD" ] ; then
@@ -35,6 +59,11 @@ setup_java() {
 
     JAVA_OPTS="$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75"
     JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly"
+    # Causes the JVM to dump its heap on OutOfMemory.
+    JAVA_OPTS="$JAVA_OPTS -XX:+HeapDumpOnOutOfMemoryError"
+    # The path to the heap dump location
+    # This variable needs to be isolated since it may contain spaces
+    HEAP_DUMP_PATH="-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof"
   fi
 
   if [ "$LS_JAVA_OPTS" ] ; then
@@ -147,8 +176,8 @@ ruby_exec() {
     # $VENDORED_JRUBY is non-empty so use the vendored JRuby
 
     if [ "$DEBUG" ] ; then
-      echo "DEBUG: exec ${JRUBY_BIN} $(jruby_opts) $@"
+      echo "DEBUG: exec ${JRUBY_BIN} $(jruby_opts) "-J$HEAP_DUMP_PATH" $@"
     fi
-    exec "${JRUBY_BIN}" $(jruby_opts) "$@"
+    exec "${JRUBY_BIN}" $(jruby_opts) "-J$HEAP_DUMP_PATH" "$@"
   fi
 }
diff --git a/bin/setup.bat b/bin/setup.bat
index 28826f0de04..4ad640ac7fa 100644
--- a/bin/setup.bat
+++ b/bin/setup.bat
@@ -18,15 +18,11 @@ REM setup_java()
 if not defined JAVA_HOME goto missing_java_home
 REM ***** JAVA options *****
 
-if "%LS_MIN_MEM%" == "" (
-set LS_MIN_MEM=256m
+if "%LS_HEAP_SIZE%" == "" (
+set LS_HEAP_SIZE=1g
 )
 
-if "%LS_MAX_MEM%" == "" (
-set LS_MAX_MEM=1g
-)
-
-set JAVA_OPTS=%JAVA_OPTS% -Xms%LS_MIN_MEM% -Xmx%LS_MAX_MEM%
+set JAVA_OPTS=%JAVA_OPTS% -Xmx%LS_HEAP_SIZE%
 
 REM Enable aggressive optimizations in the JVM
 REM    - Disabled by default as it might cause the JVM to crash
@@ -52,7 +48,7 @@ REM Causes the JVM to dump its heap on OutOfMemory.
 set JAVA_OPTS=%JAVA_OPTS% -XX:+HeapDumpOnOutOfMemoryError
 REM The path to the heap dump location, note directory must exists and have enough
 REM space for a full heap dump.
-REM JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="$LS_HOME/logs/heapdump.hprof"
+set JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="$LS_HOME/heapdump.hprof"
 
 REM setup_vendored_jruby()
 set JRUBY_BIN="%LS_HOME%\vendor\jruby\bin\jruby"
diff --git a/ci/ci_integration.sh b/ci/ci_integration.sh
new file mode 100755
index 00000000000..139408fefc9
--- /dev/null
+++ b/ci/ci_integration.sh
@@ -0,0 +1,3 @@
+#!/bin/sh
+rake test:install-default
+rake test:integration
diff --git a/docs/asciidoc/static/advanced-pipeline.asciidoc b/docs/asciidoc/static/advanced-pipeline.asciidoc
index 5481eb4eb3e..2b1a85c69da 100644
--- a/docs/asciidoc/static/advanced-pipeline.asciidoc
+++ b/docs/asciidoc/static/advanced-pipeline.asciidoc
@@ -16,6 +16,7 @@ image::static/images/basic_logstash_pipeline.png[]
 The following text represents the skeleton of a configuration pipeline:
 
 [source,shell]
+--------------------------------------------------------------------------------
 # The # character at the beginning of a line indicates a comment. Use
 # comments to describe your configuration.
 input {
@@ -27,6 +28,7 @@ input {
 # }
 output {
 }
+--------------------------------------------------------------------------------
 
 This skeleton is non-functional, because the input and output sections dont have any valid options defined. The 
 examples in this tutorial build configuration files to address specific use cases.
@@ -44,7 +46,7 @@ https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.
 
 [float]
 [[configuring-file-input]]
-===== Configuring Logstash for File Input
+==== Configuring Logstash for File Input
 
 To start your Logstash pipeline, configure the Logstash instance to read from a file using the 
 {logstash}plugins-inputs-file.html[file] input plugin.
@@ -52,12 +54,14 @@ To start your Logstash pipeline, configure the Logstash instance to read from a
 Edit the `first-pipeline.conf` file to add the following text:
 
 [source,json]
+--------------------------------------------------------------------------------
 input {
     file {
         path => "/path/to/logstash-tutorial.log"
         start_position => beginning <1>
     }
 }
+--------------------------------------------------------------------------------
 
 <1> The default behavior of the file input plugin is to monitor a file for new information, in a manner similar to the 
 UNIX `tail -f` command. To change this default behavior and process the entire file, we need to specify the position 
@@ -78,9 +82,11 @@ decisions about how to identify the patterns that are of interest to your use ca
 server log sample looks like this:
 
 [source,shell]
+--------------------------------------------------------------------------------
 83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png 
 HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel 
 Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+--------------------------------------------------------------------------------
 
 The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. In this tutorial, use 
 the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
@@ -102,15 +108,18 @@ User agent:: `agent`
 Edit the `first-pipeline.conf` file to add the following text:
 
 [source,json]
+--------------------------------------------------------------------------------
 filter {
     grok {
         match => { "message" => "%{COMBINEDAPACHELOG}"}
     }
 }
+--------------------------------------------------------------------------------
 
 After processing, the sample line has the following JSON representation:
 
 [source,json]
+--------------------------------------------------------------------------------
 {
 "clientip" : "83.149.9.216",
 "ident" : ,
@@ -124,6 +133,7 @@ After processing, the sample line has the following JSON representation:
 "referrer" : "http://semicomplete.com/presentations/logstash-monitorama-2013/",
 "agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
 }
+--------------------------------------------------------------------------------
 
 [float]
 [[indexing-parsed-data-into-elasticsearch]]
@@ -133,16 +143,16 @@ Now that the web logs are broken down into specific fields, the Logstash pipelin
 Elasticsearch cluster. Edit the `first-pipeline.conf` file to add the following text after the `input` section:
 
 [source,json]
+--------------------------------------------------------------------------------
 output {
     elasticsearch {
-        protocol => "http"
     }
 }
+--------------------------------------------------------------------------------
 
-With this configuration, Logstash uses multicast discovery to connect to Elasticsearch. 
-
-NOTE: Multicast discovery is acceptable for development work, but unsuited for production environments. For the 
-purposes of this example, however, the default behavior is sufficient.
+With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes Logstash
+and Elasticsearch to be running on the same instance. You can specify a remote Elasticsearch instance using `hosts`
+configuration like `hosts => "es-machine:9092"`. 
 
 [float]
 [[configuring-geoip-plugin]]
@@ -156,9 +166,11 @@ Configure your Logstash instance to use the `geoip` filter plugin by adding the
 of the `first-pipeline.conf` file:
 
 [source,json]
+--------------------------------------------------------------------------------
 geoip {
     source => "clientip"
 }
+--------------------------------------------------------------------------------
 
 The `geoip` plugin configuration requires data that is already defined as separate fields. Make sure that the `geoip` 
 section is after the `grok` section of the configuration file.
@@ -173,6 +185,7 @@ At this point, your `first-pipeline.conf` file has input, filter, and output sec
 like this:
 
 [source,json]
+--------------------------------------------------------------------------------
 input {
     file {
         path => "/Users/palecur/logstash-1.5.2/logstash-tutorial-dataset"
@@ -188,33 +201,39 @@ filter {
     }
 }
 output {
-    elasticsearch {
-        protocol => "http"
-    }
+    elasticsearch {}
     stdout {}
 }
+--------------------------------------------------------------------------------
 
 To verify your configuration, run the following command:
 
 [source,shell]
+--------------------------------------------------------------------------------
 bin/logstash -f first-pipeline.conf --configtest
+--------------------------------------------------------------------------------
 
 The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
 the configuration test, start Logstash with the following command:
 
 [source,shell]
+--------------------------------------------------------------------------------
 bin/logstash -f first-pipeline.conf
+--------------------------------------------------------------------------------
 
 Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin:
 
 [source,shell]
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=response=401'
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?q=response=200'
+--------------------------------------------------------------------------------
 
 Replace $DATE with the current date, in YYYY.MM.DD format.
 
-Since our sample has just one 401 HTTP response, we get one hit back:
+Since our sample has just one 200 HTTP response, we get one hit back:
 
 [source,json]
+--------------------------------------------------------------------------------
 {"took":2,
 "timed_out":false,
 "_shards":{"total":5,
@@ -246,17 +265,21 @@ Since our sample has just one 401 HTTP response, we get one hit back:
     }]
   }
 }
+--------------------------------------------------------------------------------
 
 Try another search for the geographic information derived from the IP address:
 
 [source,shell]
+--------------------------------------------------------------------------------
 curl -XGET 'localhost:9200/logstash-$DATE/_search?q=geoip.city_name=Buffalo'
+--------------------------------------------------------------------------------
 
 Replace $DATE with the current date, in YYYY.MM.DD format.
 
 Only one of the log entries comes from Buffalo, so the query produces a single response:
 
 [source,json]
+--------------------------------------------------------------------------------
 {"took":3,
 "timed_out":false,
 "_shards":{
@@ -307,6 +330,7 @@ Only one of the log entries comes from Buffalo, so the query produces a single r
   }]
  }
 }
+--------------------------------------------------------------------------------
 
 [[multiple-input-output-plugins]]
 ==== Multiple Input and Output Plugins
@@ -315,12 +339,12 @@ The information you need to manage often comes from several disparate sources, a
 destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these 
 requirements.
 
-This example creates a Logstash pipeline that takes input from a Twitter feed and the Logstash Forwarder client, then 
+This example creates a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then 
 sends the information to an Elasticsearch cluster as well as writing the information directly to a file.
 
 [float]
 [[twitter-configuration]]
-===== Reading from a Twitter feed
+==== Reading from a Twitter feed
 
 To add a Twitter feed, you need several pieces of information:
 
@@ -336,6 +360,7 @@ your OAuth token and secret.
 Use this information to add the following lines to the `input` section of the `first-pipeline.conf` file:
 
 [source,json]
+--------------------------------------------------------------------------------
 twitter {
     consumer_key =>
     consumer_secret =>
@@ -343,68 +368,73 @@ twitter {
     oauth_token =>
     oauth_token_secret => 
 }
+--------------------------------------------------------------------------------
 
 [float]
 [[configuring-lsf]]
-===== The Logstash Forwarder
+==== The Filebeat Client
 
-The https://github.com/elastic/logstash-forwarder[Logstash Forwarder] is a lightweight, resource-friendly tool that 
+The https://github.com/elastic/filebeat[filebeat] client is a lightweight, resource-friendly tool that 
 collects logs from files on the server and forwards these logs to your Logstash instance for processing. The 
-Logstash Forwarder uses a secure protocol called _lumberjack_ to communicate with your Logstash instance. The 
-lumberjack protocol is designed for reliability and low latency. The Logstash Forwarder uses the computing resources of 
-the machine hosting the source data, and the Lumberjack input plugin minimizes the resource demands on the Logstash 
-instance.
+Filebeat client uses the Beats protocol to communicate with your Logstash instance. The 
+Beats protocol is designed for reliability and low latency. Filebeat uses the computing resources of 
+the machine hosting the source data, and the {logstash}plugins-inputs-beats.html[Beats input] plugin minimizes the 
+resource demands on the Logstash instance.
 
-NOTE: In a typical use case, the Logstash Forwarder client runs on a separate machine from the machine running your 
-Logstash instance. For the purposes of this tutorial, both Logstash and the Logstash Forwarder will be running on the
+NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your 
+Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
 same machine.
 
-Default Logstash configuration includes the {logstash}plugins-inputs-lumberjack.html[Lumberjack input plugin], which is 
-designed to be resource-friendly. To install the Logstash Forwarder on your data source machine, install the 
-appropriate package from the main Logstash https://www.elastic.co/downloads/logstash[product page].
+Default Logstash configuration includes the {logstash}plugins-inputs-beats.html[Beats input plugin], which is 
+designed to be resource-friendly. To install Filebeat on your data source machine, download the 
+appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page].
 
-Create a configuration file for the Logstash Forwarder similar to the following example:
+Create a configuration file for Filebeat similar to the following example:
 
-[source,json]
+[source,shell]
 --------------------------------------------------------------------------------
-{
-    "network": {
-        "servers": [ "localhost:5043" ],
-        "ssl ca": "/path/to/localhost.crt", <1>
-        "timeout": 15
-    },
-    "files": [
-        {
-            "paths": [
-                "/path/to/sample-log" <2>
-            ],
-            "fields": { "type": "apache" }
-        }
-    ]
-}
+filebeat:
+  prospectors:
+    -
+      paths:
+        - "/path/to/sample-log" <2>
+      fields:
+        type: syslog
+output:
+  elasticsearch:
+    enabled: true
+    hosts: ["http://localhost:5043"]
+  tls:
+    certificate: /path/to/ssl-certificate.crt <2>
+    certificate_key: /path/to/ssl-certificate.key
+    certificate_authorities: /path/to/ssl-certificate.crt
+    timeout: 15
+
+<1> Path to the file or files that Filebeat processes.
+<2> Path to the SSL certificate for the Logstash instance.
 --------------------------------------------------------------------------------
 
-<1> Path to the SSL certificate for the Logstash instance.
-<2> Path to the file or files that the Logstash Forwarder processes.
-
-Save this configuration file as `logstash-forwarder.conf`. 
+Save this configuration file as `filebeat.yml`. 
 
-Configure your Logstash instance to use the Lumberjack input plugin by adding the following lines to the `input` section 
+Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the `input` section 
 of the `first-pipeline.conf` file:
 
 [source,json]
-lumberjack {
+--------------------------------------------------------------------------------
+beats {
     port => "5043"
+    ssl => true
     ssl_certificate => "/path/to/ssl-cert" <1>
     ssl_key => "/path/to/ssl-key" <2>
 }
+--------------------------------------------------------------------------------
 
-<1> Path to the SSL certificate that the Logstash instance uses to authenticate itself to Logstash Forwarder.
+<1> Path to the SSL certificate that the Logstash instance uses to authenticate itself to Filebeat.
 <2> Path to the key for the SSL certificate.
 
 [float]
 [[logstash-file-output]]
-===== Writing Logstash Data to a File
+==== Writing Logstash Data to a File
 
 You can configure your Logstash pipeline to write data directly to a file with the 
 {logstash}plugins-outputs-file.html[`file`] output plugin.
@@ -413,13 +443,15 @@ Configure your Logstash instance to use the `file` output plugin by adding the f
 of the `first-pipeline.conf` file:
 
 [source,json]
+--------------------------------------------------------------------------------
 file {
     path => /path/to/target/file
 }
+--------------------------------------------------------------------------------
 
 [float]
 [[multiple-es-nodes]]
-===== Writing to multiple Elasticsearch nodes
+==== Writing to multiple Elasticsearch nodes
 
 Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as 
 providing redundant points of entry into the cluster when a particular node is unavailable.
@@ -430,14 +462,14 @@ To configure your Logstash instance to write to multiple Elasticsearch nodes, ed
 --------------------------------------------------------------------------------
 output {
     elasticsearch {
-        protocol => "http"
-        host => ["IP Address 1", "IP Address 2", "IP Address 3"]
+        hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
     }
 }
 --------------------------------------------------------------------------------
 
-Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the `host` 
-parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses.
+Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the `hosts` 
+parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
+default port for Elasticsearch is `9200` and can be omitted in the configuration above.
 
 [float]
 [[testing-second-pipeline]]
@@ -455,16 +487,16 @@ input {
         oauth_token =>
         oauth_token_secret =>
     }
-    lumberjack {
+    beats {
         port => "5043"
+        ssl => true
         ssl_certificate => "/path/to/ssl-cert"
         ssl_key => "/path/to/ssl-key"
     }
 }
 output {
     elasticsearch {
-        protocol => "http"
-        host => ["IP Address 1", "IP Address 2", "IP Address 3"]
+        hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
     }
     file {
         path => /path/to/target/file
@@ -472,34 +504,108 @@ output {
 }
 --------------------------------------------------------------------------------
 
-Logstash is consuming data from the Twitter feed you configured, receiving data from the Logstash Forwarder, and 
+Logstash is consuming data from the Twitter feed you configured, receiving data from Filebeat, and 
 indexing this information to three nodes in an Elasticsearch cluster as well as writing to a file.
 
-At the data source machine, run the Logstash Forwarder with the following command:
+At the data source machine, run Filebeat with the following command:
 
 [source,shell]
-logstash-forwarder -config logstash-forwarder.conf
+--------------------------------------------------------------------------------
+sudo ./filebeat -e -c filebeat.yml -d "publish"
+--------------------------------------------------------------------------------
 
-Logstash Forwarder will attempt to connect on port 5403. Until Logstash starts with an active Lumberjack plugin, there 
+Filebeat will attempt to connect on port 5403. Until Logstash starts with an active Beats plugin, there 
 wont be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
 
 To verify your configuration, run the following command:
 
 [source,shell]
+--------------------------------------------------------------------------------
 bin/logstash -f first-pipeline.conf --configtest
+--------------------------------------------------------------------------------
 
 The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
 the configuration test, start Logstash with the following command:
 
 [source,shell]
+--------------------------------------------------------------------------------
 bin/logstash -f first-pipeline.conf
+--------------------------------------------------------------------------------
 
 Use the `grep` utility to search in the target file to verify that information is present:
 
 [source,shell]
+--------------------------------------------------------------------------------
 grep Mozilla /path/to/target/file
+--------------------------------------------------------------------------------
 
 Run an Elasticsearch query to find the same information in the Elasticsearch cluster:
 
 [source,shell]
+--------------------------------------------------------------------------------
 curl -XGET 'localhost:9200/logstash-2015.07.30/_search?q=agent=Mozilla'
+--------------------------------------------------------------------------------
+
+[[stalled-shutdown]]
+=== Stalled Shutdown Detection
+
+Shutting down a running Logstash instance involves the following steps:
+
+* Stop all input, filter and output plugins
+* Process all in-flight events
+* Terminate the Logstash process
+
+The following conditions affect the shutdown process:
+
+* An input plugin receiving data at a slow pace.
+* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy 
+query.
+* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+
+These situations make the duration and success of the shutdown process unpredictable.
+
+Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
+This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy 
+worker threads.
+
+To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--allow-unsafe-shutdown` flag when 
+you start Logstash.
+
+[[shutdown-stall-example]]
+==== Stall Detection Example
+
+In this example, slow filter execution prevents the pipeline from clean shutdown. By starting Logstash with the
+`--allow-unsafe-shutdown` flag, quitting with *Ctrl+C* results in an eventual shutdown that loses 20 events.
+
+========
+[source,shell]
+% bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } \
+                     output { stdout { codec => dots } }' -w 1 --allow-unsafe-shutdown
+Default settings used: Filter workers: 1
+Logstash startup completed
+^CSIGINT received. Shutting down the pipeline. {:level=>:warn}
+Received shutdown signal, but pipeline is still waiting for in-flight events
+to be processed. Sending another ^C will force quit Logstash, but this may cause
+data loss. {:level=>:warn}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20}, 
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15, 
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+The shutdown process appears to be stalled due to busy or blocked plugins. Check 
+    the logs for more information. 
+{:level=>:error}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20}, 
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15, 
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+ {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20}, 
+ "STALLING_THREADS"=>
+ {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15, 
+ "name"=>"|filterworker.0", "current_call"=>"
+ (ruby filter code):1:in `sleep'"}]}}
+Forcefully quitting logstash.. {:level=>:fatal}
+========
+
+When `--allow-unsafe-shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
diff --git a/docs/asciidoc/static/breaking-changes.asciidoc b/docs/asciidoc/static/breaking-changes.asciidoc
new file mode 100644
index 00000000000..b664be903e3
--- /dev/null
+++ b/docs/asciidoc/static/breaking-changes.asciidoc
@@ -0,0 +1,63 @@
+[[breaking-changes]]
+== Breaking changes
+
+Version 2.0 of Logstash has some changes that are incompatible with previous versions of Logstash. This section discusses 
+what you need to be aware of when migrating to this version.
+
+[float]
+== Elasticsearch Output Default
+
+Starting with the 2.0 release of Logstash, the default Logstash output for Elasticsearch is HTTP. To use the `node` or
+`transport` protocols, download the https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-elasticsearch_java.html[Elasticsearch Java plugin]. The 
+Logstash HTTP output to Elasticsearch now supports sniffing.
+
+NOTE: The `elasticsearch_java` plugin has two versions specific to the version of the underlying Elasticsearch cluster. 
+Be sure to specify the correct value for the `--version` option during installation:
+* For Elasticsearch versions before 2.0, use the command 
+`bin/plugin install --version 1.5.x logstash-output-elasticsearch_java`
+* For Elasticsearch versions 2.0 and after, use the command 
+`bin/plugin install --version 2.0.0 logstash-output-elasticsearch_java`
+
+[float]
+=== Configuration Changes
+
+The Elasticsearch output plugin configuration has the following changes:
+
+* The `host` configuration option is now `hosts`, allowing you to specify multiple hosts and associated ports in the 
+`myhost:9200` format
+* New options: `bind_host`, `bind_port`, `cluster`, `embedded`, `embedded_http_port`, `port`, `sniffing_delay`
+* The `max_inflight_requests` option, which was deprecated in the 1.5 release, is now removed
+* Since the `hosts` option allows specification of ports for the hosts, the redundant `port` option is now removed
+* The `node_name` and `protocol` options have been moved to the `elasticsearch_java` plugin
+
+The following deprecated configuration settings are removed in this release:
+
+* input plugin configuration settings: `debug`, `format`, `charset`, `message_format`
+* output plugin configuration settings: `type`, `tags`, `exclude_tags`.
+* filter plugin configuration settings: `type`, `tags`, `exclude_tags`.
+
+Configuration files with these settings present are invalid and prevent Logstash from starting.
+
+[float]
+=== Kafka Output Configuration Changes
+
+The 2.0 release of Logstash includes a new version of the Kafka output plugin with significant configuration changes.
+Please compare the documentation pages for the 
+https://www.elastic.co/guide/en/logstash/1.5/plugins-outputs-kafka.html[Logstash 1.5] and
+https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-kafka.html[Logstash 2.0] versions of the Kafka output plugin 
+and update your configuration files accordingly.
+
+[float]
+=== Metrics Filter Changes
+Prior implementations of the metrics filter plugin used dotted field names. Elasticsearch does not allow field names to 
+have dots, beginning with version 2.0, so a change was made to use sub-fields instead of dots in this plugin. Please note 
+that these changes make version 3.0.0 of the metrics filter plugin incompatible with previous releases.
+
+
+[float]
+=== Filter Worker Default Change
+
+Starting with the 2.0 release of Logstash, the default value of the `filter_workers` configuration option for filter 
+plugins is half of the available CPU cores, instead of 1. This change increases parallelism in filter execution for 
+resource-intensive filtering operations. You can continue to use the `-w` flag to manually set the value for this option, 
+as in previous releases.
diff --git a/docs/asciidoc/static/command-line-flags.asciidoc b/docs/asciidoc/static/command-line-flags.asciidoc
index 104647076f6..c91f2db94c2 100644
--- a/docs/asciidoc/static/command-line-flags.asciidoc
+++ b/docs/asciidoc/static/command-line-flags.asciidoc
@@ -16,12 +16,7 @@ Logstash has the following flags. You can use the `--help` flag to display this
  is specified, 'stdout { codec => rubydebug }}' is default.
 
 -w, --filterworkers COUNT
- Sets the number of filter workers to run (default: 1)
-
---watchdog-timeout TIMEOUT
- Set watchdog timeout value in seconds. Default is 10. This timeout is used to detect
- stuck filters; stuck filters usually symptoms of bugs. When a filter takes longer than 
- TIMEOUT seconds, it will cause Logstash to abort.
+ Sets the number of filter workers to run (default: half the number of cores)
 
 -l, --log FILE
  Log to a given path. Default is to log to stdout
@@ -43,7 +38,10 @@ Logstash has the following flags. You can use the `--help` flag to display this
 
 -t, --configtest
   Checks configuration and then exit. Note that grok patterns are not checked for 
-  correctness with this flag
+  correctness with this flag. 
+  Logstash can read multiple config files from a directory. If you combine this 
+  flag with `--debug`, Logstash will log the combined config file, annotating the
+  individual config blocks with the source file it came from.
 
 -h, --help
   Print help  
diff --git a/docs/asciidoc/static/configuration.asciidoc b/docs/asciidoc/static/configuration.asciidoc
index 27a715c124f..adcd454e700 100644
--- a/docs/asciidoc/static/configuration.asciidoc
+++ b/docs/asciidoc/static/configuration.asciidoc
@@ -11,7 +11,7 @@ Let's step through creating a simple config file and using it to run Logstash. C
 ----------------------------------
 input { stdin { } }
 output {
-  elasticsearch { host => localhost }
+  elasticsearch { hosts => ["localhost:9200"] }
   stdout { codec => rubydebug }
 }
 ----------------------------------
@@ -150,6 +150,8 @@ Input codecs provide a convenient way to decode your data before it enters the i
 Output codecs provide a convenient way to encode your data before it leaves the output.
 Using an input or output codec eliminates the need for a separate filter in your Logstash pipeline.
 
+A list of available codecs can be found at the <<codec-plugins,Codec Plugins>> page.
+
 Example:
 
 [source,js]
@@ -590,7 +592,7 @@ output {
   elasticsearch {
     action => "%{[@metadata][action]}"
     document_id => "%{[@metadata][_id]}"
-    host => "example.com"
+    hosts => ["example.com"]
     index => "index_name"
     protocol => "http"
   }
@@ -620,7 +622,7 @@ filter {
 }
 
 output {
-  elasticsearch { host => localhost }
+  elasticsearch { hosts => ["localhost:9200"] }
   stdout { codec => rubydebug }
 }
 ----------------------------------
@@ -692,7 +694,7 @@ filter {
 
 output {
   elasticsearch {
-    host => localhost
+    hosts => ["localhost:9200"]
   }
   stdout { codec => rubydebug }
 }
@@ -761,7 +763,7 @@ filter {
 }
 
 output {
-  elasticsearch { host => localhost }
+  elasticsearch { hosts => ["localhost:9200"] }
   stdout { codec => rubydebug }
 }
 ----------------------------------
@@ -828,7 +830,7 @@ filter {
 }
 
 output {
-  elasticsearch { host => localhost }
+  elasticsearch { hosts => ["localhost:9200"] }
   stdout { codec => rubydebug }
 }
 ----------------------------------
diff --git a/docs/asciidoc/static/contributing-to-logstash.asciidoc b/docs/asciidoc/static/contributing-to-logstash.asciidoc
index 8f4cc11ae46..b3ee89cefd4 100644
--- a/docs/asciidoc/static/contributing-to-logstash.asciidoc
+++ b/docs/asciidoc/static/contributing-to-logstash.asciidoc
@@ -19,6 +19,23 @@ deploying your own plugins:
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_filter_plugin.html[How to write a Logstash filter plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_output_plugin.html[How to write a Logstash output plugin]
 
+[float]
+==== Plugin API Changes added[2.0]
+
+The 2.0 release of Logstash changes how input plugins shut down to increase shutdown reliability. There are three methods
+for plugin shutdown: `stop`, `stop?`, and `close`.
+
+* Call the `stop` method from outside the plugin thread. This method signals the plugin to stop.
+* The `stop?` method returns `true` when the `stop` method has already been called for that plugin.
+* The `close` method performs final bookkeeping and cleanup after the plugin's `run` method and the plugin's thread both 
+exit. The `close` method is a a new name for the method known as `teardown` in previous versions of Logstash.
+
+The `shutdown`, `finished`, `finished?`, `running?`, and `terminating?` methods are redundant and no longer present in the 
+Plugin Base class.
+
+Sample code for the new plugin shutdown APIs is https://github.com/logstash-plugins/logstash-input-example/blob/master/lib/logstash/inputs/example.rb[available].
+
+
 [float]
 === Extending Logstash core
 
diff --git a/docs/asciidoc/static/deploying.asciidoc b/docs/asciidoc/static/deploying.asciidoc
index 56ac9c9db74..d33ea69a65e 100644
--- a/docs/asciidoc/static/deploying.asciidoc
+++ b/docs/asciidoc/static/deploying.asciidoc
@@ -39,18 +39,17 @@ filtering tasks. For example the `bin/logstash -w 8` command uses eight differen
 image::static/images/deploy_2.png[]
 
 [float]
-[[deploying-logstash-forwarder]]
-==== Using Logstash Forwarder
+[[deploying-filebeat]]
+==== Using Filebeat
 
-The https://github.com/elastic/logstash-forwarder[Logstash Forwarder] is a lightweight, resource-friendly tool written 
-in Go that collects logs from files on the server and forwards these logs to other machines for processing. The 
-Logstash Forwarder uses a secure protocol called Lumberjack to communicate with a centralized Logstash instance. 
-Configure the Logstash instances that receive Lumberjack data to use the 
-{logstash}plugins-inputs-lumberjack.html[Lumberjack input plugin].
+https://www.elastic.co/guide/en/beats/filebeat/current/index.html[Filebeat] is a lightweight, resource-friendly tool
+written in Go that collects logs from files on the server and forwards these logs to other machines for processing. 
+Filebeat uses the https://www.elastic.co/guide/en/beats/libbeat/current/index.html[Beats] protocol to communicate with a 
+centralized Logstash instance. Configure the Logstash instances that receive Beats data to use the 
+{logstash}plugins-inputs-beats.html[Beats input plugin].
 
-The Logstash Forwarder uses the computing resources of the machine hosting the source data, and the Lumberjack input 
-plugin minimizes the resource demands on the Logstash instance, making this architecture attractive for use cases with 
-resource constraints.
+Filebeat uses the computing resources of the machine hosting the source data, and the Beats input plugin minimizes the
+resource demands on the Logstash instance, making this architecture attractive for use cases with resource constraints.
 
 image::static/images/deploy_3.png[]
 
@@ -59,10 +58,7 @@ image::static/images/deploy_3.png[]
 ==== Scaling to a Larger Elasticsearch Cluster
 
 Typically, Logstash does not communicate with a single Elasticsearch node, but with a cluster that comprises several 
-nodes. Logstash can use any of the protocols that Elasticsearch supports to move data into the cluster: 
-{guide}_transport_client_versus_node_client.html[HTTP, transport, or node]. You can configure Logstash for each of 
-these communication modes by changing the value of the 
-{logstash}plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-protocol[`protocol`] setting.
+nodes. By default, Logstash uses the HTTP protocol to move data into the cluster.
 
 You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the 
 indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR 
@@ -71,9 +67,7 @@ that use the HTTP REST APIs with the {shield}[Shield] plugin, which supports SSL
 
 When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically 
 load-balance indexing requests across a 
-{logstash}plugins-outputs-elasticsearch.html#plugins-outputs-elasticsearch-host[specified set of hosts] in the 
-Elasticsearch cluster. Specifying multiple Elasticsearch nodes also provides high availability for the Elasticsearch 
-cluster by routing traffic to active Elasticsearch nodes.
+specified set of hosts in the Elasticsearch cluster. Specifying multiple Elasticsearch nodes also provides high availability for the Elasticsearch cluster by routing traffic to active Elasticsearch nodes.
 
 You can also use the Elasticsearch Java APIs to serialize the data into a binary representation, using 
 the transport protocol. The transport protocol can sniff the endpoint of the request and select an 
diff --git a/docs/asciidoc/static/getting-started-with-logstash.asciidoc b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
index 487090ba47c..354200b548d 100644
--- a/docs/asciidoc/static/getting-started-with-logstash.asciidoc
+++ b/docs/asciidoc/static/getting-started-with-logstash.asciidoc
@@ -38,7 +38,7 @@ install Logstash.
 To test your Logstash installation, run the most basic Logstash pipeline:
 
 [source,shell]
-cd logstash-1.5.2
+cd logstash-{logstash_version}
 bin/logstash -e 'input { stdin { } } output { stdout {} }'
 
 The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the 
diff --git a/docs/asciidoc/static/howtos-and-tutorials.asciidoc b/docs/asciidoc/static/howtos-and-tutorials.asciidoc
deleted file mode 100644
index fe98a8e0cb9..00000000000
--- a/docs/asciidoc/static/howtos-and-tutorials.asciidoc
+++ /dev/null
@@ -1,16 +0,0 @@
-[[howtos-and-tutorials]]
-== Logstash HOWTOs and Tutorials
-Pretty self-explanatory, really
-
-=== Downloads and Releases
-* http://elasticsearch.org/#[Getting Started with Logstash]
-* http://elasticsearch.org/#[Configuration file overview]
-* http://elasticsearch.org/#[Command-line flags]
-* http://elasticsearch.org/#[The life of an event in Logstash]
-* http://elasticsearch.org/#[Using conditional logic]
-* http://elasticsearch.org/#[Glossary]
-* http://elasticsearch.org/#[referring to fields `[like][this]`]
-* http://elasticsearch.org/#[using the `%{fieldname}` syntax]
-* http://elasticsearch.org/#[Metrics from Logs]
-* http://elasticsearch.org/#[Using RabbitMQ]
-* http://elasticsearch.org/#[Contributing to Logstash]
diff --git a/docs/asciidoc/static/include/pluginbody.asciidoc b/docs/asciidoc/static/include/pluginbody.asciidoc
index 8d77b9aeed9..9b654da21bb 100644
--- a/docs/asciidoc/static/include/pluginbody.asciidoc
+++ b/docs/asciidoc/static/include/pluginbody.asciidoc
@@ -439,7 +439,7 @@ endif::encode_method[]
 ==== Configuration Parameters
 [source,ruby]
 ----------------------------------
-  config :variable_name, :validate => :variable_type, :default => "Default value", :required => boolean, :deprecated => boolean
+  config :variable_name, :validate => :variable_type, :default => "Default value", :required => boolean, :deprecated => boolean, :obsolete => string
 ----------------------------------
 The configuration, or `config` section allows you to define as many (or as few)
 parameters as are needed to enable Logstash to process events.
@@ -457,6 +457,7 @@ will become a valid boolean in the config.  This coercion works for the
 * `:required` - whether or not this parameter is mandatory (a Boolean `true` or
 `false`)
 * `:deprecated` - informational (also a Boolean `true` or `false`)
+* `:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.
 
 ==== Plugin Methods
 
@@ -725,24 +726,24 @@ endif::receive_method[]
 
 // Teardown is now in the base class... can be pruned?
 // /////////////////////////////////////////////////////////////////////////////
-// If teardown_method is defined (should only be for input or output plugin page)
+// If close_method is defined (should only be for input or output plugin page)
 // /////////////////////////////////////////////////////////////////////////////
-// ifdef::teardown_method[]
+// ifdef::close_method[]
 // [float]
-// ==== `teardown` Method
+// ==== `close` Method
 // [source,ruby]
 // [subs="attributes"]
 // ----------------------------------
 // public
-// def teardown
+// def close
 //   @udp.close if @udp && !@udp.closed?
 // end
 // ----------------------------------
-// The `teardown` method is not present in all input or output plugins.  It is
+// The `close` method is not present in all input or output plugins.  It is
 // called when a shutdown happens to ensure that sockets, files, connections and
 // threads are all closed down properly.  If your plugin uses these connections,
-// you should include a teardown method.
-// endif::teardown_method[]
+// you should include a close method.
+// endif::close_method[]
 
 ==== Building the Plugin
 
diff --git a/docs/asciidoc/static/introduction.asciidoc b/docs/asciidoc/static/introduction.asciidoc
index ec9af52cd15..c00119237b8 100644
--- a/docs/asciidoc/static/introduction.asciidoc
+++ b/docs/asciidoc/static/introduction.asciidoc
@@ -1,15 +1,3 @@
-[[introduction]]
-== Logstash Introduction
-
-Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically 
-unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all 
-your data for diverse advanced downstream analytics and visualization use cases.
-
-While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any 
-type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many 
-native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater 
-volume and variety of data.
-
 [float]
 [[power-of-logstash]]
 == The Power of Logstash
@@ -43,8 +31,7 @@ Where it all started.
 logs like <<plugins-inputs-log4j,log4j>> for Java
 ** Capture many other log formats like <<plugins-inputs-syslog,syslog>>, 
 <<plugins-inputs-eventlog,Windows event logs>>, networking and firewall logs, and more
-* Enjoy complementary secure log forwarding capabilities with https://github.com/elastic/logstash-forwarder[Logstash 
-Forwarder]
+* Enjoy complementary secure log forwarding capabilities with https://github.com/elastic/filebeat[Filebeat]
 * Collect metrics from <<plugins-inputs-ganglia,Ganglia>>, <<plugins-codecs-collectd,collectd>>, 
 <<plugins-codecs-netflow,NetFlow>>, <<plugins-inputs-jmx,JMX>>, and many other infrastructure 
 and application platforms over <<plugins-inputs-tcp,TCP>> and <<plugins-inputs-udp,UDP>>
diff --git a/docs/asciidoc/static/life-of-an-event.asciidoc b/docs/asciidoc/static/life-of-an-event.asciidoc
index 569bd545f7c..41cadb8ac6e 100644
--- a/docs/asciidoc/static/life-of-an-event.asciidoc
+++ b/docs/asciidoc/static/life-of-an-event.asciidoc
@@ -19,8 +19,7 @@ according to the RFC3164 format
 * *redis*: reads from a redis server, using both redis channels and redis lists.
 Redis is often used as a "broker" in a centralized Logstash installation, which
 queues Logstash events from remote Logstash "shippers".
-* *lumberjack*: processes events sent in the lumberjack protocol. Now called
-https://github.com/elastic/logstash-forwarder[logstash-forwarder].
+* *beats*: processes events sent by https://www.elastic.co/downloads/beats/filebeat[Filebeat].
 
 For more information about the available inputs, see
 <<input-plugins,Input Plugins>>.
diff --git a/docs/asciidoc/static/maintainer-guide.asciidoc b/docs/asciidoc/static/maintainer-guide.asciidoc
new file mode 100644
index 00000000000..8f9872aedfc
--- /dev/null
+++ b/docs/asciidoc/static/maintainer-guide.asciidoc
@@ -0,0 +1,168 @@
+[[community-maintainer]]
+== Logstash Plugins Community Maintainer Guide
+
+This document, to be read by new Maintainers, should explain their responsibilities.  It was inspired by the 
+http://rfc.zeromq.org/spec:22[C4] document from the ZeroMQ project.  This document is subject to change and suggestions 
+through Pull Requests and issues are strongly encouraged.
+
+[float]
+== Contribution Guidelines
+
+For general guidance around contributing to Logstash Plugins, see the 
+https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html[_Contributing to Logstash_] section.
+
+[float]
+== Document Goals
+
+To help make the Logstash plugins community  participation easy with positive feedback.
+
+To increase diversity.
+
+To reduce code review, merge and release dependencies on the core team by providing support and tools to the Community and 
+Maintainers.
+
+To support the natural life cycle of a plugin.
+
+To codify the roles and responsibilities of: Maintainers and Contributors with specific focus on patch testing, code 
+review, merging and release.
+
+[float]
+== Development Workflow
+
+All Issues and Pull Requests must be tracked using the Github issue tracker.
+
+The plugin uses the http://www.apache.org/licenses/LICENSE-2.0[Apache 2.0 license]. Maintainers should check whether a 
+patch introduces code which has an incompatible license. Patch ownership and copyright is defined in the Elastic 
+https://www.elastic.co/contributor-agreement[Contributor License Agreement] (CLA).
+
+[float]
+=== Terminology
+
+A "Contributor" is a role a person assumes when providing a patch. Contributors will not have commit access to the 
+repository. They need to sign the Elastic https://www.elastic.co/contributor-agreement[Contributor License Agreement] 
+before a patch can be reviewed. Contributors can add themselves to the plugin Contributor list.
+
+A "Maintainer" is a role a person assumes when maintaining a plugin and keeping it healthy, including triaging issues, and 
+reviewing and merging patches.
+
+[float]
+=== Patch Requirements
+
+A patch is a minimal and accurate answer to exactly one identified and agreed upon problem. It must conform to the code 
+style guidelines and must include RSpec tests that verify the fitness of the solution.
+
+A patch will be automatically tested by a CI system that will report on the Pull Request status.
+
+A patch CLA will be automatically verified and reported on the Pull Request status.
+
+A patch commit message has a single short (less than 50 character) first line summarizing the change, a blank second line, 
+and any additional lines as necessary for change explanation and rationale.
+
+A patch is mergeable when it satisfies the above requirements and has been reviewed positively by at least one other 
+person.
+
+[float]
+=== Development Process
+
+A user will log an issue on the issue tracker describing the problem they face or observe with as much detail as possible.
+
+To work on an issue, a Contributor forks the plugin repository and then works on their forked repository and submits a 
+patch by creating a pull request back to the plugin.
+
+Maintainers must not merge patches where the author has not signed the CLA. 
+
+Before a patch can be accepted it should be reviewed. Maintainers should merge accepted patches without delay.
+
+Maintainers should not merge their own patches except in exceptional cases, such as non-responsiveness from other 
+Maintainers or core team for an extended period (more than 2 weeks).
+
+Reviewers comments should not be based on personal preferences.
+
+The Maintainers should label Issues and Pull Requests.
+
+Maintainers should involve the core team if help is needed to reach consensus.
+
+Review non-source changes such as documentation in the same way as source code changes.
+
+[float]
+=== Branch Management
+
+The plugin has a master branch that always holds the latest in-progress version and should always build.  Topic branches 
+should kept to the minimum.
+
+[float]
+== Versioning Plugins
+
+Logstash core and its plugins have separate product development lifecycles. Hence the versioning and release strategy for 
+the core and plugins do not have to be aligned. In fact, this was one of our goals during the great separation of plugins 
+work in Logstash 1.5. 
+
+At times, there will be changes in core API in Logstash, which will require mass update of plugins to reflect the changes 
+in core. However, this does not happen frequently. 
+
+For plugins, we would like to adhere to a versioning and release strategy that can better inform our users, about any 
+breaking changes to the Logstash configuration formats and functionality.
+
+Plugin releases follows a three-placed numbering scheme X.Y.Z. where X denotes a major release version which may break 
+compatibility with existing configuration or functionality. Y denotes releases which includes features which are backward 
+compatible. Z denotes releases which includes bug fixes and patches. 
+
+[float]
+=== Changing the version
+
+Version can be changed in the Gemspec, which needs to be associated with a changelog entry. Following this, we can publish 
+the gem to RubyGem.org manually. At this point only the core developers can publish a gem.
+
+[float]
+=== Labeling
+
+Labeling is a critical aspect of maintaining plugins. All issues in GitHub should be labeled correctly so it can: 
+
+* Provide good feedback to users/developers 
+* Help prioritize changes 
+* Be used in release notes
+
+Most labels are self explanatory, but heres a quick recap of few important labels:
+
+* `bug`: Labels an issue as an unintentional defect
+* `needs details`: If a the issue reporter has incomplete details, please ask them for more info and label as needs 
+details.
+* `missing cla`: Contributor License Agreement is missing and patch cannot be accepted without it
+* `adopt me`: Ask for help from the community to take over this issue
+* `enhancement`: New feature, not a bug fix
+* `needs tests`: Patch has no tests, and cannot be accepted without unit/integration tests
+* `docs`: Documentation related issue/PR
+
+[float]
+== Logging
+
+Although its important not to bog down performance with excessive logging, debug level logs can be immensely helpful when 
+diagnosing and troubleshooting issues with Logstash.  Please remember to liberally add debug logs wherever it makes sense 
+as users will be forever gracious.
+
+[source,shell]
+@logger.debug("Logstash loves debug logs!", :actions => actions)
+
+[float]
+== Contributor License Agreement (CLA) Guidance
+
+[qanda]
+Why is a https://www.elastic.co/contributor-agreement[CLA] required?::
+     We ask this of all Contributors in order to assure our users of the origin and continuing existence of the code. We 
+     are not asking Contributors to assign copyright to us, but to give us the right to distribute a Contributors code 
+     without restriction.
+
+Please make sure the CLA is signed by every Contributor prior to reviewing PRs and commits.::
+     Contributors only need to sign the CLA once and should sign with the same email as used in Github. If a Contributor
+     signs the CLA after a PR is submitted, they can refresh the automated CLA checker by pushing another 
+     comment on the PR after 5 minutes of signing.
+
+[float]
+== Community Administration
+
+The core team is there to support the plugin Maintainers and overall ecosystem.
+
+Maintainers should propose Contributors to become a Maintainer.
+
+Contributors and Maintainers should follow the Elastic Community https://www.elastic.co/community/codeofconduct[Code of 
+Conduct].  The core team should block or ban bad actors.
diff --git a/docs/asciidoc/static/managing-multiline-events.asciidoc b/docs/asciidoc/static/managing-multiline-events.asciidoc
new file mode 100644
index 00000000000..2e1cd694e70
--- /dev/null
+++ b/docs/asciidoc/static/managing-multiline-events.asciidoc
@@ -0,0 +1,112 @@
+[[multiline]]
+=== Managing Multiline Events
+
+Several use cases generate events that span multiple lines of text. In order to correctly handle these multline events, 
+Logstash needs to know how to tell which lines are part of a single event.
+
+Multiline event processing is complex and relies on proper event ordering. The best way to guarantee ordered log 
+processing is to implement the processing as early in the pipeline as possible. The preferred tool in the Logstash 
+pipeline is the {logstash}plugins-codecs-multiline.html[multiline codec], which merges lines from a single input using 
+a simple set of rules.
+
+
+The most important aspects of configuring either multiline plugin are the following:
+
+* The `pattern` option specifies a regular expression. Lines that match the specified regular expression are considered 
+either continuations of a previous line or the start of a new multiline event. You can use 
+{logstash}plugins-filters-grok.html[grok] regular expression templates with this configuration option.
+* The `what` option takes two values: `previous` or `next`. The `previous` value specifies that lines that match the 
+value in the `pattern` option are part of the previous line. The `next` value specifies that lines that match the value 
+in the `pattern` option are part of the following line.* The `negate` option applies the multiline codec to lines that 
+_do not_ match the regular expression specified in the `pattern` option.
+
+See the full documentation for the {logstash}plugins-codecs-multiline.html[multiline codec] or the 
+{logstash}plugins-filters-multiline.html[multiline filter] plugin for more information on configuration options.
+
+NOTE: For more complex needs, the {logstash}plugins-filters-multiline.html[multiline filter] performs a similar task at 
+the filter stage of processing, where the Logstash instance aggregates multiple inputs.
+The multiline filter plugin is not thread-safe. Avoid using multiple filter workers with the multiline filter. You can 
+track the progress of upgrades to the functionality of the multiline codec at 
+https://github.com/logstash-plugins/logstash-codec-multiline/issues/10[this Github issue].
+
+==== Examples of Multiline Plugin Configuration
+
+The examples in this section cover the following use cases:
+
+* Combining a Java stack trace into a single event
+* Combining C-style line continuations into a single event
+* Combining multiple lines from time-stamped events
+
+===== Java Stack Traces
+
+Java stack traces consist of multiple lines, with each line after the initial line beginning with whitespace, as in 
+this example:
+
+[source,java]
+Exception in thread "main" java.lang.NullPointerException
+        at com.example.myproject.Book.getTitle(Book.java:16)
+        at com.example.myproject.Author.getBookTitles(Author.java:25)
+        at com.example.myproject.Bootstrap.main(Bootstrap.java:14)
+
+To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:
+
+[source,json]
+input {
+  stdin {
+    codec => multiline {
+      pattern => "^\s"
+      what => "previous"
+    }
+  }
+}
+
+This configuration merges any line that begins with whitespace up to the previous line.
+
+===== Line Continuations
+
+Several programming languages use the `\` character at the end of a line to denote that the line continues, as in this 
+example:
+
+[source,c]
+printf ("%10.10ld  \t %10.10ld \t %s\
+  %f", w, x, y, z );
+
+To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:
+
+[source,json]
+input {
+  stdin {
+    codec => multiline {
+      pattern => "\\$"
+      what => "next"
+    }
+  }
+}
+
+This configuration merges any line that ends with the `\` character with the following line.
+
+===== Timestamps
+
+Activity logs from services such as Elasticsearch typically begin with a timestamp, followed by information on the 
+specific activity, as in this example:
+
+[source,shell]
+[2015-08-24 11:49:14,389][INFO ][env                      ] [Letha] using [1] data paths, mounts [[/ 
+(/dev/disk1)]], net usable_space [34.5gb], net total_space [118.9gb], types [hfs]
+
+To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:
+
+[source,json]
+input {
+  file {
+    path => "/var/log/someapp.log"
+    codec => multiline {
+      pattern => "^%{TIMESTAMP_ISO8601} "
+      negate => true
+      what => previous
+    }
+  }
+}
+
+This configuration uses the `negate` option to specify that any line that does not begin with a timestamp belongs to 
+the previous line.
diff --git a/docs/asciidoc/static/offline-plugins.asciidoc b/docs/asciidoc/static/offline-plugins.asciidoc
new file mode 100644
index 00000000000..87ea494e4c5
--- /dev/null
+++ b/docs/asciidoc/static/offline-plugins.asciidoc
@@ -0,0 +1,62 @@
+[[offline-plugins]]
+=== Offline Plugin Management
+
+The Logstash <<working-with-plugins,plugin manager>> was introduced in the 1.5 release. This section discusses setting up
+local repositories of plugins for use on systems without access to the Internet.
+
+The procedures in this section require a staging machine running Logstash that has access to a public or private Rubygems 
+server. This staging machine downloads and packages the files used for offline installation.
+
+See the <<private-rubygem,Private Gem Repositories>> section for information on setting up your own private 
+Rubygems server.
+
+Users who can work with a larger Logstash artifact size can use the *Logstash (All Plugins)* download link from the
+https://www.elastic.co/downloads/logstash[Logstash product page] to download Logstash bundled with the latest version of
+all available plugins. You can distribute this bundle to all nodes without further plugin staging.
+
+[float]
+=== Building the Offline Package
+
+Working with offline plugins requires you to create an _offline package_, which is a compressed file that contains all of 
+the plugins your offline Logstash installation requires, along with the dependencies for those plugins.
+
+. Create the offline package with the `bin/plugin pack` subcommand.
++
+When you run the `bin/plugin pack` subcommand, Logstash creates a compressed bundle that contains all of the currently
+installed plugins and the dependencies for those plugins. By default, the compressed bundle is a GZipped TAR file when you 
+run the `bin/plugin pack` subcommand on a UNIX machine. By default, when you run the `bin/plugin pack` subcommand on a 
+Windows machine, the compressed bundle is a ZIP file. See <<managing-packs,Managing Plugin Packs>> for details on changing 
+these default behaviors.
++
+NOTE: Downloading all dependencies for the specified plugins may take some time, depending on the plugins listed.
+
+. Move the compressed bundle to the offline machines that are the source for offline plugin installation, then use the
+`bin/plugin unpack` subcommand to make the packaged plugins available.
+
+[float]
+=== Install or Update a local plugin
+
+To install or update a local plugin, use the `--local` option with the install and update commands, as in the following 
+examples:
+
+.Installing a local plugin
+============
+`bin/plugin install --local logstash-input-jdbc`
+============
+
+.Updating a local plugin
+============
+`bin/plugin update --local logstash-input-jdbc`
+============
+
+[float]
+[[managing-packs]]
+=== Managing Plugin Packs
+
+The `pack` and `unpack` subcommands for `bin/plugin` take the following options:
+
+[horizontal]
+`--tgz`:: Generate the offline package as a GZipped TAR file. The default behavior on UNIX systems.
+`--zip`:: Generate the offline package as a ZIP file. The default behavior on Windows systems.
+`[packname] --override`:: Generates a new offline package that overwrites an existing offline with the specified name.
+`[packname] --[no-]clean`: Deletes offline packages matching the specified name.
\ No newline at end of file
diff --git a/docs/asciidoc/static/plugin-manager.asciidoc b/docs/asciidoc/static/plugin-manager.asciidoc
index f15ea99dbd0..2f531083008 100644
--- a/docs/asciidoc/static/plugin-manager.asciidoc
+++ b/docs/asciidoc/static/plugin-manager.asciidoc
@@ -1,15 +1,17 @@
 [[working-with-plugins]]
 == Working with plugins
 
-Logstash has a rich collection of input, filter, codec and output plugins. Plugins are available as self-contained packages called gems and hosted on RubyGems.org. The plugin manager accesed via `bin/plugin` script is used to manage the lifecycle of plugins in your Logstash deployment. You can install, uninstall and upgrade plugins using these Command Line Interface (CLI) described below.
-
-NOTE: Some sections here are for advanced users
+Logstash has a rich collection of input, filter, codec and output plugins. Plugins are available as self-contained 
+packages called gems and hosted on RubyGems.org. The plugin manager accesed via `bin/plugin` script is used to manage the 
+lifecycle of plugins in your Logstash deployment. You can install, uninstall and upgrade plugins using these Command Line 
+Interface (CLI) described below.
 
 [float]
 [[listing-plugins]]
 === Listing plugins
 
-Logstash release packages bundle common plugins so you can use them out of the box. To list the plugins currently available in your deployment:
+Logstash release packages bundle common plugins so you can use them out of the box. To list the plugins currently 
+available in your deployment:
 
 [source,shell]
 ----------------------------------
@@ -30,7 +32,9 @@ bin/plugin list --group output <4>
 [[installing-plugins]]
 === Adding plugins to your deployment
 
-The most common situation when dealing with plugin installation is when you have access to internet. Using this method, you will be able to retrieve plugins hosted on the public repository (RubyGems.org) and install on top of your Logstash installation.
+The most common situation when dealing with plugin installation is when you have access to internet. Using this method, 
+you will be able to retrieve plugins hosted on the public repository (RubyGems.org) and install on top of your Logstash 
+installation.
 
 [source,shell]
 ----------------------------------
@@ -43,7 +47,8 @@ Once the plugin is successfully installed, you can start using it in your config
 [float]
 ==== Advanced: Adding a locally built plugin
 
-In some cases, you want to install plugins which have not yet been released and not hosted on RubyGems.org. Logstash provides you the option to install a locally built plugin which is packaged as a ruby gem. Using a file location:
+In some cases, you want to install plugins which have not yet been released and not hosted on RubyGems.org. Logstash 
+provides you the option to install a locally built plugin which is packaged as a ruby gem. Using a file location:
 
 [source,shell]
 ----------------------------------
@@ -54,7 +59,8 @@ bin/plugin install /path/to/logstash-output-kafka-1.0.0.gem
 [float]
 ==== Advanced: Using `--pluginpath`
 
-Using the `--pluginpath` flag, you can load a plugin source code located on your file system. Typically this is used by developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
+Using the `--pluginpath` flag, you can load a plugin source code located on your file system. Typically this is used by 
+developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
 
 [source,shell]
 ----------------------------------
@@ -65,7 +71,8 @@ bin/logstash --pluginpath /opt/shared/lib/logstash/input/my-custom-plugin-code.r
 [float]
 === Updating plugins
 
-Plugins have their own release cycle and are often released independent of Logstashs core release cycle. Using the update sub-command you can get the latest or update to a particular version of the plugin.
+Plugins have their own release cycle and are often released independent of Logstashs core release cycle. Using the update 
+subcommand you can get the latest or update to a particular version of the plugin.
 
 [source,shell]
 ----------------------------------
@@ -91,7 +98,9 @@ bin/plugin uninstall logstash-output-kafka
 [float]
 === Proxy Support
 
-The previous sections relied on Logstash being able to communicate with RubyGems.org. In certain environments, Forwarding Proxy is used to handle HTTP requests. Logstash Plugins can be installed and updated through a Proxy by setting the `HTTP_PROXY` environment variable:
+The previous sections relied on Logstash being able to communicate with RubyGems.org. In certain environments, Forwarding 
+Proxy is used to handle HTTP requests. Logstash Plugins can be installed and updated through a Proxy by setting the 
+`HTTP_PROXY` environment variable:
 
 [source,shell]
 ----------------------------------
@@ -101,3 +110,7 @@ bin/plugin install logstash-output-kafka
 ----------------------------------
 
 Once set, plugin commands install, update can be used through this proxy.
+
+include::offline-plugins.asciidoc[]
+
+include::private-gem-repo.asciidoc[]
\ No newline at end of file
diff --git a/docs/asciidoc/static/private-gem-repo.asciidoc b/docs/asciidoc/static/private-gem-repo.asciidoc
new file mode 100644
index 00000000000..dd96f63a60d
--- /dev/null
+++ b/docs/asciidoc/static/private-gem-repo.asciidoc
@@ -0,0 +1,53 @@
+[[private-rubygem]]
+=== Private Gem Repositories
+
+The Logstash plugin manager connects to a Ruby gems repository to install and update Logstash plugins. By default, this
+repository is http://rubygems.org.
+
+Some use cases are unable to use the default repository, as in the following examples:
+
+* A firewall blocks access to the default repository.
+* You are developing your own plugins locally.
+* Airgap requirements on the local system.
+
+When you use a custom gem repository, be sure to make plugin dependencies available.
+
+Several open source projects enable you to run your own plugin server, among them:
+
+* https://github.com/geminabox/geminabox[Geminabox]
+* https://github.com/PierreRambaud/gemirro[Gemirro]
+* https://gemfury.com/[Gemfury]
+* http://www.jfrog.com/open-source/[Artifactory]
+
+==== Editing the Gemfile
+
+The gemfile is a configuration file that specifies information required for plugin management. Each gem file has a
+`source` line that specifies a location for plugin content.
+
+By default, the gemfile's `source` line reads:
+
+[source,shell]
+----------
+# This is a Logstash generated Gemfile.
+# If you modify this file manually all comments and formatting will be lost.
+
+source "https://rubygems.org"
+----------
+
+To change the source, edit the `source` line to contain your preferred source, as in the following example:
+
+[source,shell]
+----------
+# This is a Logstash generated Gemfile.
+# If you modify this file manually all comments and formatting will be lost.
+
+source "https://my.private.repository"
+----------
+
+After saving the new version of the gemfile, use <<working-with-plugins,plugin management commands>> normally.
+
+The following links contain further material on setting up some commonly used repositories:
+
+* https://github.com/geminabox/geminabox/blob/master/README.markdown[Geminabox]
+* https://www.jfrog.com/confluence/display/RTF/RubyGems+Repositories[Artifactory]
+* Running a http://guides.rubygems.org/run-your-own-gem-server/[rubygems mirror]
diff --git a/docs/asciidoc/static/releasenotes.asciidoc b/docs/asciidoc/static/releasenotes.asciidoc
new file mode 100644
index 00000000000..9685faa1fde
--- /dev/null
+++ b/docs/asciidoc/static/releasenotes.asciidoc
@@ -0,0 +1,61 @@
+[[releasenotes]]
+== Logstash 2.1 Release Notes
+
+[float]
+== General
+
+* {lsissue}2376[Issue 2376]: Added ability to install and upgrade Logstash plugins without requiring internet 
+connectivity. 
+* {lsissue}3576[Issue 3576]: Support alternate or private Ruby gems server to install and update plugins.
+* {lsissue}3451[Issue 3451]: Added ability to reliably shutdown Logstash when there is a stall in event processing. This 
+option can be enabled by passing `--allow-unsafe-shutdown` flag while starting Logstash. Please be aware that any in-
+flight events will be lost when shutdown happens.
+* {lsissue}4222[Issue 4222]: Fixed a memory leak which could be triggered when events having a date were serialized to 
+string.
+* Added JDBC input to default package.
+* {lsissue}3243[Issue 3243]: Adding `--debug` to `--configtest` now shows the configuration in blocks annotated by source 
+config file. Very useful when using multiple config files in a directory.
+* {lsissue}4130[Issue 4130]: Reset default worker threads to 1 when using non thread-safe filters like multiline.
+* Fixed file permissions for the `logrotate` configuration file.
+* {lsissue}3861[Issue 3861]: Changed the default heap size from 500MB to 1GB.
+* {lsissue}3645[Issue 3645]: Fixed config check option when starting Logstash through init scripts.
+
+[float]
+== Input Plugins
+
+[float]
+=== Twitter
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/21[Issue 21]: Added an option to fetch data from the 
+sample Twitter streaming endpoint.
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/22[Issue 22]: Added hashtags, symbols and 
+user_mentions as data for the non extended tweet event.
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/20[Issue 20]: Added an option to filter per location 
+and language.
+* https://github.com/logstash-plugins/logstash-input-twitter/issues/11[Issue 11]: Added an option to stream data from a 
+list of users.
+
+[float]
+=== Beats
+* https://github.com/logstash-plugins/logstash-input-beats/issues/10[Issue 10]: Properly handle multiline events from 
+multiple sources, originating from Filebeat.
+
+[float]
+=== File
+* https://github.com/logstash-plugins/logstash-input-file/issues/44[Issue 44]: Properly handle multiline events from 
+multiple sources.
+
+[float]
+=== Eventlog
+* https://github.com/logstash-plugins/logstash-input-eventlog/issues/11[Issue 11]: Change the underlying library to 
+capture Event Logs from Windows more reliably. 
+
+[float]
+== Output
+
+[float]
+=== Elasticsearch
+* Improved the default template to use doc_values wherever possible.
+* Improved the default template to disable fielddata on analyzed string fields.
+* https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/260[Issue 260]: Added New setting: timeout. 
+This lets you control the behavior of a slow/stuck request to Elasticsearch that could be, for example, caused by network, 
+firewall, or load balancer issues.
diff --git a/docs/asciidoc/static/repositories.asciidoc b/docs/asciidoc/static/repositories.asciidoc
index a36860f8210..7a394885dac 100644
--- a/docs/asciidoc/static/repositories.asciidoc
+++ b/docs/asciidoc/static/repositories.asciidoc
@@ -24,14 +24,14 @@ Download and install the Public Signing Key:
 
 [source,sh]
 --------------------------------------------------
-wget -qO - https://packages.elasticsearch.org/GPG-KEY-elasticsearch | sudo apt-key add -
+wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
 --------------------------------------------------
 
 Add the repository definition to your `/etc/apt/sources.list` file:
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-echo "deb http://packages.elasticsearch.org/logstash/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list
+echo "deb http://packages.elastic.co/logstash/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list
 --------------------------------------------------
 
 [WARNING]
@@ -62,7 +62,7 @@ Download and install the public signing key:
 
 [source,sh]
 --------------------------------------------------
-rpm --import https://packages.elasticsearch.org/GPG-KEY-elasticsearch
+rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
 --------------------------------------------------
 
 Add the following in your `/etc/yum.repos.d/` directory
@@ -72,9 +72,9 @@ in a file with a `.repo` suffix, for example `logstash.repo`
 --------------------------------------------------
 [logstash-{branch}]
 name=Logstash repository for {branch}.x packages
-baseurl=http://packages.elasticsearch.org/logstash/{branch}/centos
+baseurl=http://packages.elastic.co/logstash/{branch}/centos
 gpgcheck=1
-gpgkey=http://packages.elasticsearch.org/GPG-KEY-elasticsearch
+gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
 enabled=1
 --------------------------------------------------
 
diff --git a/docs/asciidoc/static/roadmap/index.asciidoc b/docs/asciidoc/static/roadmap/index.asciidoc
index 465fd23656f..b52e271675a 100644
--- a/docs/asciidoc/static/roadmap/index.asciidoc
+++ b/docs/asciidoc/static/roadmap/index.asciidoc
@@ -72,8 +72,7 @@ https://github.com/elastic/logstash/labels/resiliency[resiliency] tag.
 
 *Known unknowns.* If we dont know its happening, its hard for us to fix it!
 Please report your issues in GitHub, under the
-https://github.com/elastic/logstash/issues[Logstash],
-https://github.com/elastic/logstash-forwarder/issues[Logstash Forwarder], or
+https://github.com/elastic/logstash/issues[Logstash] or
 individual https://github.com/logstash-plugins/[Logstash plugin] repositories.
 
 == Manageability
@@ -125,28 +124,6 @@ distributing the load between instances based on the latest cluster state. This
 is a complex use case that will require input from the community on current
 approaches to implementing HA and load balancing of Logstash instances.
 
-== Logstash Forwarder
-[float]
-=== status: ongoing; v2.x
-
-Logstash Forwarder uses a different code base from Logstash, and as a result it
-has been a challenge for us to keep feature parity between the two projects. We
-are experimenting with unifying the two code bases to improve ongoing
-maintenance of the Logstash Forwarder. Currently, Logstash Forwarder is written
-in Go and Logstash is written in Ruby and runs on JRuby. We are investigating
-the feasibility of replacing Logstash Forwarder with Logstash Ruby code executed
-on Matz Ruby Interpreter (http://en.wikipedia.org/wiki/Ruby_MRI[MRI]). Important
-criteria for success in this POC is to keep Logstash Forwarder lightweight and
-still distribute it as a binary so it doesnt introduce language dependencies on
-the servers where it is deployed. You can follow this effort on GitHub through
-the Logstash Forwarder
-https://github.com/elastic/logstash-forwarder/issues[issues list].
-
-While we are working on these enhancements, we are committed to maintaining the
-Logstash Forwarder. We recently delivered
-http://www.elasticsearch.org/blog/logstash-forwarder-0-4-0-released/[Logstash Forwarder 0.4.0],
-which addressed many existing issues our users have been reporting.
-
 == Performance
 [float]
 === status: ongoing; v1.5, v2.x
@@ -203,4 +180,4 @@ In Logstash 1.5, we made it easier than ever to add and maintain plugins by
 putting each plugin into its own repository (see "Plugin Framework" section).
 We also greatly improved the S3, Twitter, RabbitMQ plugins. To follow requests
 for new Logstash plugins or contribute to the discussion, look for issues that
-have the {LABELS}new-plugin[new-plugin] tag in Github.
\ No newline at end of file
+have the {LABELS}new-plugin[new-plugin] tag in Github.
diff --git a/docs/asciidoc/static/upgrading.asciidoc b/docs/asciidoc/static/upgrading.asciidoc
new file mode 100644
index 00000000000..3043c67d5b0
--- /dev/null
+++ b/docs/asciidoc/static/upgrading.asciidoc
@@ -0,0 +1,80 @@
+[[upgrading-logstash]]
+== Upgrading Logstash
+
+[IMPORTANT]
+===========================================
+Before upgrading Logstash:
+
+* Consult the <<breaking-changes,breaking changes>> docs.
+* Test upgrades in a development environment before upgrading your production cluster.
+===========================================
+
+=== Upgrading Using Package Managers
+
+This procedure uses <<package-repositories,package managers>> to upgrade Logstash.
+
+1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
+2. Using the directions in the _Package Repositories_ section, update your repository links to point to the 2.0 repositories
+instead of the previous version.
+3. Run the `apt-get update logstash` or `yum update logstash` command as appropriate for your operating system.
+4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
+some Logstash plugins have changed in the 2.0 release.
+5. Restart your Logstash pipeline after updating your configuration file.
+
+=== Upgrading Using a Direct Download
+
+This procedure downloads the relevant Logstash binaries directly from Elastic.
+
+1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
+2. Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
+3. Unpack the installation file into your Logstash directory.
+4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
+some Logstash plugins have changed in the 2.0 release.
+5. Restart your Logstash pipeline after updating your configuration file.
+
+=== Upgrading Logstash and Elasticsearch to 2.0
+
+If you are using Elasticsearch as an output, and wish to upgrade to Elasticsearch 2.0, please be
+aware of https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html[breaking changes]
+before you upgrade. In addition, the following steps needs to be performed after upgrading to Elasticsearch 2.0:
+
+**Mapping changes:** Users may have custom template changes, so by default a Logstash upgrade will
+leave the template as is. Even if you don't have a custom template, Logstash will not overwrite an existing
+template by default.
+
+There is one known issue (removal of https://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-object-type.html#_path_3[path]) with using GeoIP filter that needs a manual update to the template.
+
+Note: If you have custom template changes, please make sure to save it and merge any changes. You can
+get the existing template by running:
+
+[source,shell]
+curl -XGET localhost:9200/_template/logstash
+
+
+Add the following option to your Logstash config:
+
+[source,json]
+output {
+	elasticsearch {
+		template_overwrite => true
+	}
+}
+
+Restart Logstash.
+
+**Dots in fields:** Elasticsearch 2.0 does not allow field names to contain the `.` character.
+Further details about this change https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking_20_mapping_changes.html#_field_names_may_not_contain_dots[here]. Some plugins already have been updated to compensate
+for this breaking change, including logstash-filter-metrics and logstash-filter-elapsed.
+These plugin updates are available for Logstash 2.0. To upgrade to the latest version of these
+plugins, the command is:
+
+[source,shell]
+bin/plugin update <plugin_name>
+
+**Multiline Filter:** If you are using the Multiline Filter in your configuration and upgrade to Logstash 2.0,
+you will get an error. Make sure to explicitly set the number of filter workers (`-w`) to `1`. You can set the number
+of workers by passing a command line flag such as:
+
+[source,shell]
+bin/logstash `-w 1`
+
diff --git a/docs/extending/index.md b/docs/extending/index.md
index 014f9700337..4a4ab66d877 100644
--- a/docs/extending/index.md
+++ b/docs/extending/index.md
@@ -46,8 +46,8 @@ so:
 
 The name of the option is specified, here `:host` and then the
 attributes of the option. They can include `:validate`, `:default`,
-`:required` (a Boolean `true` or `false`), and `:deprecated` (also a
-Boolean).  
+`:required` (a Boolean `true` or `false`), `:deprecated` (also a
+Boolean), and `:obsolete` (a String value).  
  
 ## Inputs
 
diff --git a/docs/flags.md b/docs/flags.md
index 508c18ddea5..e7777f372fe 100644
--- a/docs/flags.md
+++ b/docs/flags.md
@@ -20,8 +20,6 @@ default. If no output is specified, 'stdout { debug => true }}' is
 default. </dd>
 <dt> -w, --filterworkers COUNT </dt>
 <dd> Run COUNT filter workers (default: 1) </dd>
-<dt> --watchdog-timeout TIMEOUT </dt>
-<dd> Set watchdog timeout value in seconds. Default is 10.</dd>
 <dt> -l, --log FILE </dt>
 <dd> Log to a given path. Default is to log to stdout </dd>
 <dt> --verbose </dt>
diff --git a/docs/plugin-doc.asciidoc.erb b/docs/plugin-doc.asciidoc.erb
index c8bb1d23c29..e00319d537a 100644
--- a/docs/plugin-doc.asciidoc.erb
+++ b/docs/plugin-doc.asciidoc.erb
@@ -28,6 +28,7 @@ This plugin has no configuration options.
 
 <% sorted_attributes.each do |name, config| -%>
 <%
+     next if config[:obsolete]
      if name.is_a?(Regexp)
        name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
        is_regexp = true
diff --git a/docs/plugin-synopsis.asciidoc.erb b/docs/plugin-synopsis.asciidoc.erb
index 7cae738e4a4..6b1d047c4d6 100644
--- a/docs/plugin-synopsis.asciidoc.erb
+++ b/docs/plugin-synopsis.asciidoc.erb
@@ -27,6 +27,7 @@ Available configuration options:
 |=======================================================================
 |Setting |Input type|Required|Default value
 <% sorted_attributes.each do |name, config|
+   next if config[:obsolete]
    next if config[:deprecated]
    if config[:validate].is_a?(Array)
      annotation = "|<<string,string>>, one of `#{config[:validate].inspect}`"
diff --git a/integration/logstash_config/file_input_to_file_output_spec.rb b/integration/logstash_config/file_input_to_file_output_spec.rb
new file mode 100644
index 00000000000..f857465b8ee
--- /dev/null
+++ b/integration/logstash_config/file_input_to_file_output_spec.rb
@@ -0,0 +1,41 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+require "stud/temporary"
+
+describe "File input to File output" do
+  let(:number_of_events) { IO.readlines(sample_log).size }
+  let(:sample_log) { File.expand_path(File.join(File.dirname(__FILE__), "..", "support", "sample.log")) }
+  let(:output_file) { Stud::Temporary.file.path }
+  let(:config) { 
+<<EOS
+    input {
+       file {
+         path => \"#{sample_log}\"
+         stat_interval => 0
+         start_position => \"beginning\"
+         sincedb_path => \"/dev/null\"
+       }
+      }
+    output {
+      file {
+        path => \"#{output_file}\"
+      }
+    }
+EOS
+  }
+
+  before :all do
+    command("bin/plugin install logstash-input-file logstash-output-file")
+  end
+
+  it "writes events to file" do
+    cmd = "bin/logstash -e '#{config}'"
+    launch_logstash(cmd)
+
+    expect(File.exist?(output_file)).to eq(true)
+
+    # on shutdown the events arent flushed to disk correctly
+    # Known issue https://github.com/logstash-plugins/logstash-output-file/issues/12
+    expect(IO.readlines(output_file).size).to be_between(number_of_events - 10, number_of_events).inclusive
+  end
+end
diff --git a/integration/plugin_manager/logstash_spec.rb b/integration/plugin_manager/logstash_spec.rb
new file mode 100644
index 00000000000..8c2f4c97d73
--- /dev/null
+++ b/integration/plugin_manager/logstash_spec.rb
@@ -0,0 +1,11 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+require_relative "../../lib/logstash/version"
+
+describe "bin/logstash" do
+  it "returns the logstash version" do
+    result = command("bin/logstash version")
+    expect(result.exit_status).to eq(0)
+    expect(result.stdout).to match(/^logstash\s#{LOGSTASH_VERSION}/)
+  end
+end
diff --git a/integration/plugin_manager/plugin_install_spec.rb b/integration/plugin_manager/plugin_install_spec.rb
new file mode 100644
index 00000000000..313fd1b1f73
--- /dev/null
+++ b/integration/plugin_manager/plugin_install_spec.rb
@@ -0,0 +1,41 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+require "fileutils"
+
+context "bin/plugin install" do
+  context "with a local gem" do
+    let(:gem_name) { "logstash-input-wmi" }
+    let(:local_gem) { gem_fetch(gem_name) }
+
+    it "install the gem succesfully" do
+      result = command("bin/plugin install --no-verify #{local_gem}")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout).to match(/^Installing\s#{gem_name}\nInstallation\ssuccessful$/)
+    end
+  end
+
+  context "when the plugin exist" do
+    let(:plugin_name) { "logstash-input-drupal_dblog" }
+
+    it "sucessfully install" do
+      result = command("bin/plugin install #{plugin_name}")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout).to match(/^Validating\s#{plugin_name}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
+    end
+
+    it "allow to install a specific version" do
+      version = "2.0.2"
+      result = command("bin/plugin install --version 2.0.2 #{plugin_name}")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout).to match(/^Validating\s#{plugin_name}-#{version}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
+    end
+  end
+
+  context "when the plugin doesn't exist" do
+    it "fails to install" do
+      result = command("bin/plugin install --no-verify logstash-output-impossible-plugin")
+      expect(result.exit_status).to eq(1)
+      expect(result.stderr).to match(/Installation Aborted, message: Could not find gem/)
+    end
+  end
+end
diff --git a/integration/plugin_manager/plugin_list_spec.rb b/integration/plugin_manager/plugin_list_spec.rb
new file mode 100644
index 00000000000..a8a2b19e453
--- /dev/null
+++ b/integration/plugin_manager/plugin_list_spec.rb
@@ -0,0 +1,41 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+
+describe "bin/plugin list" do
+  context "without a specific plugin" do
+    it "display a list of plugins" do
+      result = command("bin/plugin list")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout.split("\n").size).to be > 1
+    end
+
+    it "display a list of installed plugins" do
+      result = command("bin/plugin list --installed")
+      expect(result.exit_status).to eq(0)
+      expect(result.stdout.split("\n").size).to be > 1
+    end
+
+    it "list the plugins with their versions" do
+      result = command("bin/plugin list --verbose")
+      result.stdout.split("\n").each do |plugin|
+        expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+\)/)
+      end
+      expect(result.exit_status).to eq(0)
+    end
+  end
+
+  context "with a specific plugin" do
+    let(:plugin_name) { "logstash-input-stdin" }
+    it "list the plugin and display the plugin name" do
+      result = command("bin/plugin list #{plugin_name}")
+      expect(result.stdout).to match(/^#{plugin_name}$/)
+      expect(result.exit_status).to eq(0)
+    end
+
+    it "list the plugin with his version" do
+      result = command("bin/plugin list --verbose #{plugin_name}")
+      expect(result.stdout).to match(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
+      expect(result.exit_status).to eq(0)
+    end
+  end
+end
diff --git a/integration/plugin_manager/plugin_uninstall_spec.rb b/integration/plugin_manager/plugin_uninstall_spec.rb
new file mode 100644
index 00000000000..87f2fd747e2
--- /dev/null
+++ b/integration/plugin_manager/plugin_uninstall_spec.rb
@@ -0,0 +1,24 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+
+describe "bin/plugin uninstall" do
+  context "when the plugin isn't installed" do
+    it "fails to uninstall it" do
+      result = command("bin/plugin uninstall logstash-filter-cidr")
+      expect(result.stderr).to match(/ERROR: Uninstall Aborted, message: This plugin has not been previously installed, aborting/)
+      expect(result.exit_status).to eq(1)
+    end
+  end
+
+  context "when the plugin is installed" do
+      it "succesfully uninstall it" do
+      # make sure we have the plugin installed.
+      command("bin/plugin install logstash-filter-ruby")
+
+      result = command("bin/plugin uninstall logstash-filter-ruby")
+
+      expect(result.stdout).to match(/^Uninstalling logstash-filter-ruby/)
+      expect(result.exit_status).to eq(0)
+    end
+  end
+end
diff --git a/integration/plugin_manager/plugin_update_spec.rb b/integration/plugin_manager/plugin_update_spec.rb
new file mode 100644
index 00000000000..d8b291739cc
--- /dev/null
+++ b/integration/plugin_manager/plugin_update_spec.rb
@@ -0,0 +1,32 @@
+# Encoding: utf-8
+require_relative "../spec_helper"
+
+describe "update" do
+  let(:plugin_name) { "logstash-input-stdin" }
+  let(:previous_version) { "2.0.1" }
+
+  before do
+    command("bin/plugin install --version #{previous_version} #{plugin_name}")
+    cmd = command("bin/plugin list --verbose #{plugin_name}")
+    expect(cmd.stdout).to match(/#{plugin_name} \(#{previous_version}\)/)
+  end
+
+  context "update a specific plugin" do
+    subject { command("bin/plugin update #{plugin_name}") }
+
+    it "has executed succesfully" do
+      expect(subject.exit_status).to eq(0)
+      expect(subject.stdout).to match(/Updating #{plugin_name}/)
+    end
+  end
+
+  context "update all the plugins" do
+    subject { command("bin/plugin update") }
+
+    it "has executed succesfully" do
+      expect(subject.exit_status).to eq(0)
+      cmd = command("bin/plugin list --verbose #{plugin_name}").stdout
+      expect(cmd).to match(/logstash-input-stdin \(#{LogStashTestHelpers.latest_version(plugin_name)}\)/)
+    end
+  end
+end
diff --git a/integration/spec_helper.rb b/integration/spec_helper.rb
new file mode 100644
index 00000000000..0076b8b2cdf
--- /dev/null
+++ b/integration/spec_helper.rb
@@ -0,0 +1,37 @@
+# encoding: utf-8
+require_relative "support/integration_test_helpers"
+require_relative "../lib/logstash/environment"
+require "fileutils"
+
+if LogStash::Environment.windows?
+  puts "[integration] Theses integration test are specifically made to be run on under linux/unix"
+  puts "[integration] Please see our windows version of the tests https://github.com/elastic/logstash/tree/master/test/windows"
+end
+
+# Configure the test environment
+source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
+integration_path = File.join(source, "integration_run")
+
+puts "[integration_spec] configure environment"
+
+if Dir.exists?(integration_path)
+  # We copy the current logstash into a temporary directory
+  # since the tests are a bit destructive
+  FileUtils.mkdir_p(integration_path)
+  rsync_cmd = "rsync -a --delete --exclude 'rspec' --exclude '#{File.basename(integration_path)}' --exclude 'integration_spec' --exclude '.git' #{source} #{integration_path}"
+
+  puts "[integration_spec] Rsync source code into: #{integration_path}"
+  system(rsync_cmd)
+  puts "[integration_spec] Finish rsync"
+
+  LOGSTASH_TEST_PATH = File.join(integration_path, "logstash")
+else
+  LOGSTASH_TEST_PATH = File.expand_path(File.join(File.dirname(__FILE__), ".."))
+end
+
+puts "[integration_spec] Running the test in #{LOGSTASH_TEST_PATH}"
+puts "[integration_spec] Running specs"
+
+RSpec.configure do |config|
+  config.order = "random"
+end
diff --git a/integration/support/integration_test_helpers.rb b/integration/support/integration_test_helpers.rb
new file mode 100644
index 00000000000..aad90f8f07a
--- /dev/null
+++ b/integration/support/integration_test_helpers.rb
@@ -0,0 +1,89 @@
+# encoding: utf-8
+require "json"
+require "open3"
+require "open-uri"
+require "stud/temporary"
+require "fileutils"
+require "bundler"
+require "gems"
+
+class CommandResponse
+  attr_reader :stdin, :stdout, :stderr, :exit_status
+
+  def initialize(cmd, stdin, stdout, stderr, exit_status)
+    @stdin = stdin
+    @stdout = stdout
+    @stderr = stderr
+    @exit_status = exit_status
+    @cmd = cmd
+  end
+
+  def to_debug
+    "DEBUG: stdout: #{stdout}, stderr: #{stderr}, exit_status: #{exit_status}"
+  end
+
+  def to_s
+    @cmd
+  end
+end
+
+def command(cmd, path = nil)
+  # http://bundler.io/v1.3/man/bundle-exec.1.html
+  # see shelling out.
+  #
+  # Since most of the integration test are environment destructive
+  # its better to run them in a cloned directory.
+  path = LOGSTASH_TEST_PATH if path == nil
+
+  Bundler.with_clean_env do
+    Dir.chdir(path) do
+      Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr|
+          CommandResponse.new(cmd,
+            stdin,
+            stdout.read.chomp,
+            stderr.read.chomp,
+            wait_thr.value.exitstatus)
+      end
+    end
+  end
+end
+
+def gem_fetch(name)
+  tmp = Stud::Temporary.directory
+  FileUtils.mkdir_p(tmp)
+
+  c = command("gem fetch #{name}", tmp)
+
+  if c.exit_status == 1
+    raise RuntimeError, "Can't fetch gem #{name}"
+  end
+
+  return Dir.glob(File.join(tmp, "#{name}*.gem")).first
+end
+
+# This is a bit hacky since JRuby doesn't support fork,
+# we use popen4 which return the pid of the process and make sure we kill it
+# after letting it run for a few seconds.
+def launch_logstash(cmd, path = nil)
+  path = LOGSTASH_TEST_PATH if path == nil
+  pid = 0
+
+  Thread.new do
+    Bundler.with_clean_env do
+      Dir.chdir(path) do
+        pid, input, output, error = IO.popen4(cmd) #jruby only
+      end
+    end
+  end
+  sleep(30)
+  begin
+    Process.kill("INT", pid)
+  rescue
+  end
+end
+
+module LogStashTestHelpers
+  def self.latest_version(name)
+    Gems.versions(name).first["number"] 
+  end
+end
diff --git a/integration/support/sample.log b/integration/support/sample.log
new file mode 100644
index 00000000000..8f304b59c45
--- /dev/null
+++ b/integration/support/sample.log
@@ -0,0 +1,50 @@
+83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1" 200 171717 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/highlight/highlight.js HTTP/1.1" 200 26185 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/zoom-js/zoom.js HTTP/1.1" 200 7697 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/plugin/notes/notes.js HTTP/1.1" 200 2892 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/sad-medic.png HTTP/1.1" 200 430406 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Bold.ttf HTTP/1.1" 200 38720 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Regular.ttf HTTP/1.1" 200 41820 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1" 200 52878 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard.png HTTP/1.1" 200 321631 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/Dreamhost_logo.svg HTTP/1.1" 200 2126 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard2.png HTTP/1.1" 200 394967 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/apache-icon.gif HTTP/1.1" 200 8095 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/nagios-sms5.png HTTP/1.1" 200 78075 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/redis.png HTTP/1.1" 200 25230 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/elasticsearch.png HTTP/1.1" 200 8026 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/logstashbook.png HTTP/1.1" 200 54662 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/github-contributions.png HTTP/1.1" 200 34245 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/css/print/paper.css HTTP/1.1" 200 4254 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/1983_delorean_dmc-12-pic-38289.jpeg HTTP/1.1" 200 220562 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/simple-inputs-filters-outputs.jpg HTTP/1.1" 200 1168622 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/tiered-outputs-to-inputs.jpg HTTP/1.1" 200 1079983 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+83.149.9.216 - - [26/Aug/2014:21:13:53 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+24.236.252.67 - - [26/Aug/2014:21:14:10 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:26.0) Gecko/20100101 Firefox/26.0"
+93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1" 200 18848 "http://www.google.ro/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCwQFjAB&url=http%3A%2F%2Fwww.semicomplete.com%2Farticles%2Fdynamic-dns-with-dhcp%2F&ei=W88AU4n9HOq60QXbv4GwBg&usg=AFQjCNEF1X4Rs52UYQyLiySTQxa97ozM4g&bvm=bv.61535280,d.d2k" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
+66.249.73.135 - - [26/Aug/2014:21:15:03 +0000] "GET /blog/tags/ipv6 HTTP/1.1" 200 12251 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+50.16.19.13 - - [26/Aug/2014:21:15:15 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "http://www.semicomplete.com/blog/tags/puppet?flav=rss20" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
+66.249.73.185 - - [26/Aug/2014:21:15:23 +0000] "GET / HTTP/1.1" 200 37932 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+110.136.166.128 - - [26/Aug/2014:21:16:11 +0000] "GET /projects/xdotool/ HTTP/1.1" 200 12292 "http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&sqi=2&ved=0CFYQFjAE&url=http%3A%2F%2Fwww.semicomplete.com%2Fprojects%2Fxdotool%2F&ei=6cwAU_bRHo6urAeI0YD4Ag&usg=AFQjCNE3V_aCf3-gfNcbS924S6jZ6FqffA&bvm=bv.61535280,d.bmk" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+46.105.14.53 - - [26/Aug/2014:21:16:17 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+123.125.71.35 - - [26/Aug/2014:21:16:31 +0000] "GET /blog/tags/release HTTP/1.1" 200 40693 "-" "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
+110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
+50.150.204.184 - - [26/Aug/2014:21:17:06 +0000] "GET /images/googledotcom.png HTTP/1.1" 200 65748 "http://www.google.com/search?q=https//:google.com&source=lnms&tbm=isch&sa=X&ei=4-r8UvDrKZOgkQe7x4CICw&ved=0CAkQ_AUoAA&biw=320&bih=441" "Mozilla/5.0 (Linux; U; Android 4.0.4; en-us; LG-MS770 Build/IMM76I) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30"
+207.241.237.225 - - [26/Aug/2014:21:17:35 +0000] "GET /blog/tags/examples HTTP/1.0" 200 9208 "http://www.semicomplete.com/blog/tags/C" "Mozilla/5.0 (compatible; archive.org_bot +http://www.archive.org/details/archive.org_bot)"
+200.49.190.101 - - [26/Aug/2014:21:17:39 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
+200.49.190.100 - - [26/Aug/2014:21:17:37 +0000] "GET /blog/tags/web HTTP/1.1" 200 44019 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
+200.49.190.101 - - [26/Aug/2014:21:17:41 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
+200.49.190.101 - - [26/Aug/2014:21:17:48 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
+66.249.73.185 - - [26/Aug/2014:21:18:48 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+66.249.73.135 - - [26/Aug/2014:21:18:55 +0000] "GET /blog/tags/munin HTTP/1.1" 200 9746 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
+66.249.73.135 - - [26/Aug/2014:21:19:16 +0000] "GET /blog/tags/firefox?flav=rss20 HTTP/1.1" 200 16021 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
diff --git a/lib/bootstrap/bundler.rb b/lib/bootstrap/bundler.rb
index 23944d347fe..2948fe8aa29 100644
--- a/lib/bootstrap/bundler.rb
+++ b/lib/bootstrap/bundler.rb
@@ -27,6 +27,16 @@ def set_key(key, value, hash, file)
           value
         end
       end
+
+      # This patch makes rubygems fetch directly from the remote servers
+      # the dependencies he need and might not have downloaded in a local
+      # repository. This basically enabled the offline feature to work as
+      # we remove the gems from the vendor directory before packacing.
+      ::Bundler::Source::Rubygems.module_exec do
+        def cached_gem(spec)
+          cached_built_in_gem(spec)
+        end
+      end
     end
 
     def setup!(options = {})
@@ -56,11 +66,19 @@ def setup!(options = {})
 
     # execute bundle install and capture any $stdout output. any raised exception in the process will be trapped
     # and returned. logs errors to $stdout.
-    # @param options [Hash] invoke options with default values, :max_tries => 10, :clean => false, :install => false, :update => false
-    # @param   options[:update] must be either false or a String or an Array of String
+    # @param [Hash] options invoke options with default values, :max_tries => 10, :clean => false, :install => false, :update => false
+    # @option options [Boolean] :max_tries The number of times bundler is going to try the installation before failing (default: 10)
+    # @option options [Boolean] :clean It cleans the unused gems (default: false)
+    # @option options [Boolean] :install Run the installation of a set of gems defined in a Gemfile (default: false)
+    # @option options [Boolean, String, Array] :update Update the current environment, must be either false or a String or an Array of String (default: false)
+    # @option options [Boolean] :local Do not attempt to fetch gems remotely and use the gem cache instead (default: false)
+    # @option options [Boolean] :package Locks and then caches all dependencies to be reused later on (default: false)
+    # @option options [Boolean] :all It packages dependencies defined with :git or :path (default: false)
+    # @option options [Array] :without  Exclude gems that are part of the specified named group (default: [:development])
     # @return [String, Exception] the installation captured output and any raised exception or nil if none
     def invoke!(options = {})
-      options = {:max_tries => 10, :clean => false, :install => false, :update => false, :without => [:development]}.merge(options)
+      options = {:max_tries => 10, :clean => false, :install => false, :update => false, :local => false,
+                 :all => false, :package => false, :without => [:development]}.merge(options)
       options[:without] = Array(options[:without])
       options[:update] = Array(options[:update]) if options[:update]
 
@@ -80,14 +98,13 @@ def invoke!(options = {})
       LogStash::Bundler.patch!
 
       # force Rubygems sources to our Gemfile sources
-      ::Gem.sources = options[:rubygems_source] if options[:rubygems_source]
+      ::Gem.sources = ::Gem::SourceList.from(options[:rubygems_source]) if options[:rubygems_source]
 
       ::Bundler.settings[:path] = LogStash::Environment::BUNDLE_DIR
       ::Bundler.settings[:gemfile] = LogStash::Environment::GEMFILE_PATH
       ::Bundler.settings[:without] = options[:without].join(":")
 
       try = 0
-
       # capture_stdout also traps any raised exception and pass them back as the function return [output, exception]
       output, exception = capture_stdout do
         loop do
@@ -130,11 +147,19 @@ def bundler_arguments(options = {})
       if options[:install]
         arguments << "install"
         arguments << "--clean" if options[:clean]
+        if options[:local]
+          arguments << "--local"
+          arguments << "--no-prune" # From bundler docs: Don't remove stale gems from the cache.
+        end
       elsif options[:update]
         arguments << "update"
         arguments << options[:update]
+        arguments << "--local" if options[:local]
       elsif options[:clean]
         arguments << "clean"
+      elsif options[:package]
+        arguments << "package"
+        arguments << "--all" if options[:all]
       end
 
       arguments.flatten
diff --git a/lib/bootstrap/environment.rb b/lib/bootstrap/environment.rb
index 9f3e59f5b08..beaab548ca5 100644
--- a/lib/bootstrap/environment.rb
+++ b/lib/bootstrap/environment.rb
@@ -16,6 +16,7 @@ module Environment
     BUNDLE_DIR = ::File.join(LOGSTASH_HOME, "vendor", "bundle")
     GEMFILE_PATH = ::File.join(LOGSTASH_HOME, "Gemfile")
     LOCAL_GEM_PATH = ::File.join(LOGSTASH_HOME, 'vendor', 'local_gems')
+    CACHE_PATH = File.join(LOGSTASH_HOME, "vendor", "cache")
 
     # @return [String] the ruby version string bundler uses to craft its gem path
     def gem_ruby_version
@@ -32,6 +33,14 @@ def ruby_engine
       RUBY_ENGINE
     end
 
+    def windows?
+      ::Gem.win_platform?
+    end
+
+    def jruby?
+      @jruby ||= !!(RUBY_PLATFORM == "java")
+    end
+
     def logstash_gem_home
       ::File.join(BUNDLE_DIR, ruby_engine, gem_ruby_version)
     end
diff --git a/lib/bootstrap/util/compress.rb b/lib/bootstrap/util/compress.rb
new file mode 100644
index 00000000000..79bd38461b4
--- /dev/null
+++ b/lib/bootstrap/util/compress.rb
@@ -0,0 +1,122 @@
+# encoding: utf-8
+require "zip"
+require "rubygems/package"
+require "fileutils"
+require "zlib"
+require "stud/temporary"
+
+module LogStash
+
+  class CompressError < StandardError; end
+
+  module Util
+    module Zip
+
+      extend self
+
+      # Extract a zip file into a destination directory.
+      # @param source [String] The location of the file to extract
+      # @param target [String] Where you do want the file to be extracted
+      # @raise [IOError] If the target directory already exist
+      def extract(source, target)
+        raise CompressError.new("Directory #{target} exist") if ::File.exist?(target)
+        ::Zip::File.open(source) do |zip_file|
+          zip_file.each do |file|
+            path = ::File.join(target, file.name)
+            FileUtils.mkdir_p(::File.dirname(path))
+            zip_file.extract(file, path)
+          end
+        end
+      end
+
+      # Compress a directory into a zip file
+      # @param dir [String] The directory to be compressed
+      # @param target [String] Destination to save the generated file
+      # @raise [IOError] If the target file already exist
+      def compress(dir, target)
+        raise CompressError.new("File #{target} exist") if ::File.exist?(target)
+        ::Zip::File.open(target, ::Zip::File::CREATE) do |zipfile|
+          Dir.glob("#{dir}/**/*").each do |file|
+            path_in_zip = file.gsub("#{dir}/","")
+            zipfile.add(path_in_zip, file)
+          end
+        end
+      end
+    end
+
+    module Tar
+
+      extend self
+
+      # Extract a tar.gz file into a destination directory.
+      # @param source [String] The location of the file to extract
+      # @param target [String] Where you do want the file to be extracted
+      # @raise [IOError] If the target directory already exist
+      def extract(file, target)
+        raise CompressError.new("Directory #{target} exist") if ::File.exist?(target)
+
+        FileUtils.mkdir(target)
+        Zlib::GzipReader.open(file) do |gzip_file|
+          ::Gem::Package::TarReader.new(gzip_file) do |tar_file|
+            tar_file.each do |entry|
+              target_path = ::File.join(target, entry.full_name)
+
+              if entry.directory?
+                FileUtils.mkdir_p(target_path)
+              else # is a file to be extracted
+                ::File.open(target_path, "wb") { |f| f.write(entry.read) }
+              end
+            end
+          end
+        end
+      end
+
+      # Compress a directory into a tar.gz file
+      # @param dir [String] The directory to be compressed
+      # @param target [String] Destination to save the generated file
+      # @raise [IOError] If the target file already exist
+      def compress(dir, target)
+        raise CompressError.new("File #{target} exist") if ::File.exist?(target)
+
+        Stud::Temporary.file do |tar_file|
+          ::Gem::Package::TarWriter.new(tar_file) do |tar|
+            Dir.glob("#{dir}/**/*").each do |file|
+              name = file.gsub("#{dir}/","")
+              stats = ::File.stat(file)
+              mode  = stats.mode
+
+              if ::File.directory?(file)
+                tar.mkdir(name, mode)
+              else # is a file to be added
+                tar.add_file(name,mode) do |out|
+                  File.open(file, "rb") do |fd|
+                    chunk = nil
+                    size = 0
+                    size += out.write(chunk) while chunk = fd.read(16384)
+                    if stats.size != size
+                      raise "Failure to write the entire file (#{path}) to the tarball. Expected to write #{stats.size} bytes; actually write #{size}"
+                    end
+                  end
+                end
+              end
+            end
+          end
+
+          tar_file.rewind
+          gzip(target, tar_file)
+        end
+      end
+
+      # Compress a file using gzip
+      # @param path [String] The location to be compressed
+      # @param target_file [String] Destination of the generated file
+      def gzip(path, target_file)
+        ::File.open(path, "wb") do |file|
+          gzip_file = ::Zlib::GzipWriter.new(file)
+          gzip_file.write(target_file.read)
+          gzip_file.close
+        end
+      end
+    end
+  end
+end
diff --git a/lib/logstash/agent.rb b/lib/logstash/agent.rb
index 2fc743fe359..31b46d3a21e 100644
--- a/lib/logstash/agent.rb
+++ b/lib/logstash/agent.rb
@@ -2,6 +2,7 @@
 require "clamp" # gem 'clamp'
 require "logstash/environment"
 require "logstash/errors"
+require "logstash/config/cpu_core_strategy"
 require "uri"
 require "net/http"
 LogStash::Environment.load_locale!
@@ -21,11 +22,8 @@ class LogStash::Agent < Clamp::Command
 
   option ["-w", "--filterworkers"], "COUNT",
     I18n.t("logstash.agent.flag.filterworkers"),
-    :attribute_name => :filter_workers, :default => 1, &:to_i
-
-  option "--watchdog-timeout", "SECONDS",
-    I18n.t("logstash.agent.flag.watchdog-timeout"),
-    :default => 10, &:to_f
+    :attribute_name => :filter_workers,
+    :default => 0, &:to_i
 
   option ["-l", "--log"], "FILE",
     I18n.t("logstash.agent.flag.log"),
@@ -52,6 +50,11 @@ class LogStash::Agent < Clamp::Command
     I18n.t("logstash.agent.flag.configtest"),
     :attribute_name => :config_test
 
+  option "--[no-]allow-unsafe-shutdown", :flag,
+    I18n.t("logstash.agent.flag.unsafe_shutdown"),
+    :attribute_name => :unsafe_shutdown,
+    :default => false
+
   # Emit a warning message.
   def warn(message)
     # For now, all warnings are fatal.
@@ -77,6 +80,9 @@ def execute
     require "logstash/plugin"
     @logger = Cabin::Channel.get(LogStash)
 
+    LogStash::ShutdownController.unsafe_shutdown = unsafe_shutdown?
+    LogStash::ShutdownController.logger = @logger
+
     if version?
       show_version
       return 0
@@ -130,14 +136,14 @@ def execute
         @logger.warn(I18n.t("logstash.agent.sigint"))
         Thread.new(@logger) {|logger| sleep 5; logger.warn(I18n.t("logstash.agent.slow_shutdown")) }
         @interrupted_once = true
-        pipeline.shutdown
+        shutdown(pipeline)
       end
     end
 
     # Make SIGTERM shutdown the pipeline.
     sigterm_id = Stud::trap("TERM") do
       @logger.warn(I18n.t("logstash.agent.sigterm"))
-      pipeline.shutdown
+      shutdown(pipeline)
     end
 
     Stud::trap("HUP") do
@@ -145,7 +151,7 @@ def execute
       configure_logging(log_file)
     end
 
-    pipeline.configure("filter-workers", filter_workers)
+    pipeline.configure("filter-workers", filter_workers) if filter_workers > 0
 
     # Stop now if we are only asking for a config test.
     if config_test?
@@ -176,6 +182,12 @@ def execute
     Stud::untrap("TERM", sigterm_id) unless sigterm_id.nil?
   end # def execute
 
+  def shutdown(pipeline)
+    pipeline.shutdown do
+      ::LogStash::ShutdownController.start(pipeline)
+    end
+  end
+
   def show_version
     show_version_logstash
 
@@ -307,19 +319,27 @@ def local_config(path)
     Dir.glob(path).sort.each do |file|
       next unless File.file?(file)
       if file.match(/~$/)
-        @logger.debug("NOT reading config file because it is a temp file", :file => file)
+        @logger.debug("NOT reading config file because it is a temp file", :config_file => file)
         next
       end
-      @logger.debug("Reading config file", :file => file)
+      @logger.debug("Reading config file", :config_file => file)
       cfg = File.read(file)
       if !cfg.ascii_only? && !cfg.valid_encoding?
         encoding_issue_files << file
       end
       config << cfg + "\n"
+      if config_test?
+        @logger.debug? && @logger.debug("\nThe following is the content of a file", :config_file => file.to_s)
+        @logger.debug? && @logger.debug("\n" + cfg + "\n\n")
+      end
     end
     if (encoding_issue_files.any?)
       fail("The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}")
     end
+    if config_test?
+      @logger.debug? && @logger.debug("\nThe following is the merged configuration")
+      @logger.debug? && @logger.debug("\n" + config + "\n\n")
+    end
     return config
   end # def load_config
 
diff --git a/lib/logstash/codecs/base.rb b/lib/logstash/codecs/base.rb
index 662f054dfde..25fad9da702 100644
--- a/lib/logstash/codecs/base.rb
+++ b/lib/logstash/codecs/base.rb
@@ -28,7 +28,7 @@ def encode(event)
   end # def encode
 
   public 
-  def teardown; end;
+  def close; end;
 
   # @param block [Proc(event, data)] the callback proc passing the original event and the encoded event
   public
diff --git a/lib/logstash/config/Makefile b/lib/logstash/config/Makefile
deleted file mode 100644
index ffb1755fe0b..00000000000
--- a/lib/logstash/config/Makefile
+++ /dev/null
@@ -1,4 +0,0 @@
-
-#ragel -R grammar.rl
-grammar.rb: grammar.treetop
-	tt grammar.treetop
diff --git a/lib/logstash/config/cpu_core_strategy.rb b/lib/logstash/config/cpu_core_strategy.rb
new file mode 100644
index 00000000000..f6c09f097b3
--- /dev/null
+++ b/lib/logstash/config/cpu_core_strategy.rb
@@ -0,0 +1,32 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/config/defaults"
+
+module LogStash module Config module CpuCoreStrategy
+
+  extend self
+
+  def maximum
+    LogStash::Config::Defaults.cpu_cores
+  end
+
+  def fifty_percent
+    [1, (maximum * 0.5)].max.floor
+  end
+
+  def seventy_five_percent
+    [1, (maximum * 0.75)].max.floor
+  end
+
+  def twenty_five_percent
+    [1, (maximum * 0.25)].max.floor
+  end
+
+  def max_minus_one
+    [1, (maximum - 1)].max.floor
+  end
+
+  def max_minus_two
+    [1, (maximum - 2)].max.floor
+  end
+end end end
diff --git a/lib/logstash/config/defaults.rb b/lib/logstash/config/defaults.rb
new file mode 100644
index 00000000000..ac3466f771d
--- /dev/null
+++ b/lib/logstash/config/defaults.rb
@@ -0,0 +1,12 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "concurrent"
+
+module LogStash module Config module Defaults
+
+  extend self
+
+  def cpu_cores
+    Concurrent.processor_count
+  end
+end end end
diff --git a/lib/logstash/config/mixin.rb b/lib/logstash/config/mixin.rb
index 12df2e232e8..cbfdcf62331 100644
--- a/lib/logstash/config/mixin.rb
+++ b/lib/logstash/config/mixin.rb
@@ -35,12 +35,6 @@ module LogStash::Config::Mixin
   attr_accessor :config
   attr_accessor :original_params
 
-  CONFIGSORT = {
-    Symbol => 0,
-    String => 0,
-    Regexp => 100,
-  }
-
   PLUGIN_VERSION_1_0_0 = LogStash::Util::PluginVersion.new(1, 0, 0)
   PLUGIN_VERSION_0_9_0 = LogStash::Util::PluginVersion.new(0, 9, 0)
 
@@ -76,6 +70,13 @@ def config_init(params)
                      "about this, please visit the #logstash channel " +
                      "on freenode irc.", :name => name, :plugin => self)
       end
+      if opts && opts[:obsolete]
+        extra = opts[:obsolete].is_a?(String) ? opts[:obsolete] : ""
+        extra.gsub!("%PLUGIN%", self.class.config_name)
+        raise LogStash::ConfigurationError,
+          I18n.t("logstash.agent.configuration.obsolete", :name => name,
+                 :plugin => self.class.config_name, :extra => extra)
+      end
     end
 
     # Set defaults from 'config :foo, :default => somevalue'
@@ -103,6 +104,16 @@ def config_init(params)
         I18n.t("logstash.agent.configuration.invalid_plugin_settings")
     end
 
+    # We remove any config options marked as obsolete,
+    # no code should be associated to them and their values should not bleed
+    # to the plugin context.
+    #
+    # This need to be done after fetching the options from the parents classed
+    params.reject! do |name, value|
+      opts = self.class.get_config[name]
+      opts.include?(:obsolete)
+    end
+
     # set instance variables like '@foo'  for each config value given.
     params.each do |key, value|
       next if key[0, 1] == "@"
diff --git a/lib/logstash/config/test.conf b/lib/logstash/config/test.conf
deleted file mode 100644
index af69223e761..00000000000
--- a/lib/logstash/config/test.conf
+++ /dev/null
@@ -1,18 +0,0 @@
-input {
-  rabbitmq {
-    port => 12345 
-    tag => [ a, b, c ]
-  }
-
-  stomp {
-    port => 12345
-    tag => [ stomp ]
-  }
-}
-
-filter {
-  date { 
-    hello => world
-    hello => "Hello"
-  }
-}
diff --git a/lib/logstash/event.rb b/lib/logstash/event.rb
index c00d5531305..70eed147392 100644
--- a/lib/logstash/event.rb
+++ b/lib/logstash/event.rb
@@ -106,7 +106,7 @@ def clone
 
   public
   def to_s
-    self.sprintf("#{timestamp.to_iso8601} %{host} %{message}")
+    "#{timestamp.to_iso8601} #{self.sprintf("%{host} %{message}")}"
   end # def to_s
 
   public
diff --git a/lib/logstash/filters/base.rb b/lib/logstash/filters/base.rb
index 61bf7887554..cf5a3ddd946 100644
--- a/lib/logstash/filters/base.rb
+++ b/lib/logstash/filters/base.rb
@@ -11,22 +11,11 @@ class LogStash::Filters::Base < LogStash::Plugin
 
   config_name "filter"
 
-  # Note that all of the specified routing options (`type`,`tags`,`exclude_tags`,`include_fields`,
-  # `exclude_fields`) must be met in order for the event to be handled by the filter.
+  config :type, :validate => :string, :default => "", :obsolete => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
 
-  # The type to act on. If a type is given, then this filter will only
-  # act on messages with the same type. See any input plugin's `type`
-  # attribute for more.
-  # Optional.
-  config :type, :validate => :string, :default => "", :deprecated => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
-
-  # Only handle events with all of these tags.
-  # Optional.
-  config :tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if \"sometag\" in [tags] { %PLUGIN% { ... } }`"
+  config :tags, :validate => :array, :default => [], :obsolete => "You can achieve similar behavior with the new conditionals, like: `if \"sometag\" in [tags] { %PLUGIN% { ... } }`"
 
-  # Only handle events without any of these tags.
-  # Optional.
-  config :exclude_tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if !(\"sometag\" in [tags]) { %PLUGIN% { ... } }`"
+  config :exclude_tags, :validate => :array, :default => [], :obsolete => "You can achieve similar behavior with the new conditionals, like: `if (\"sometag\" not in [tags]) { %PLUGIN% { ... } }`"
 
   # If this filter is successful, add arbitrary tags to the event.
   # Tags can be dynamic and include parts of the event using the `%{field}`
@@ -128,8 +117,6 @@ class LogStash::Filters::Base < LogStash::Plugin
   # Optional.
   config :periodic_flush, :validate => :boolean, :default => false
 
-  RESERVED = ["type", "tags", "exclude_tags", "include_fields", "exclude_fields", "add_tag", "remove_tag", "add_field", "remove_field", "include_any", "exclude_any"]
-
   public
   def initialize(params)
     super
@@ -156,6 +143,7 @@ def filter(event)
   # @return [Array<LogStash::Event] filtered events and any new events generated by the filter
   public
   def multi_filter(events)
+    LogStash::Util.set_thread_plugin(self)
     result = []
     events.each do |event|
       unless event.cancelled?
@@ -202,40 +190,12 @@ def filter_matched(event)
 
   protected
   def filter?(event)
-    if !@type.empty?
-      if event["type"] != @type
-        @logger.debug? and @logger.debug("filters/#{self.class.name}: Skipping event because type doesn't match",
-                                         :type=> @type, :event => event)
-        return false
-      end
-    end
-
-    if !@tags.empty?
-      # this filter has only works on events with certain tags,
-      # and this event has no tags.
-      return false if !event["tags"]
-
-      # Is @tags a subset of the event's tags? If not, skip it.
-      if (event["tags"] & @tags).size != @tags.size
-        @logger.debug? and @logger.debug("filters/#{self.class.name}: Skipping event because tags don't match",
-                                         :tags => tags, :event => event)
-        return false
-      end
-    end
-
-    if !@exclude_tags.empty? && event["tags"]
-      if (diff_tags = (event["tags"] & @exclude_tags)).size != 0
-        @logger.debug("filters/#{self.class.name}: Skipping event because tags contains excluded tags:",
-                      :diff_tags => diff_tags, :exclude_tags => @exclude_tags, :event => event)
-        return false
-      end
-    end
-
-    return true
-  end
+    # TODO: noop for now, remove this once we delete this call from all plugins
+    true
+  end # def filter?
 
   public
-  def teardown
+  def close
     # Nothing to do by default.
   end
 end # class LogStash::Filters::Base
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index f28d04e0e98..f72dfd743ac 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -26,31 +26,16 @@ class LogStash::Inputs::Base < LogStash::Plugin
   # when sent to another Logstash server.
   config :type, :validate => :string
 
-  config :debug, :validate => :boolean, :default => false, :deprecated => "This setting no longer has any effect. In past releases, it existed, but almost no plugin made use of it."
+  config :debug, :validate => :boolean, :default => false, :obsolete => "This setting no longer has any effect. In past releases, it existed, but almost no plugin made use of it."
 
-  # The format of input data (plain, json, json_event)
-  config :format, :validate => ["plain", "json", "json_event", "msgpack_event"], :deprecated => "You should use the newer 'codec' setting instead."
+  config :format, :validate => ["plain", "json", "json_event", "msgpack_event"], :obsolete => "You should use the newer 'codec' setting instead."
 
-  # The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.
-  config :codec, :validate => :codec, :default => "plain"
+  config :charset, :obsolete => "Use the codec setting instead. For example: input { %PLUGIN% { codec => plain { charset => \"UTF-8\" } }"
 
-  # The character encoding used in this input. Examples include `UTF-8`
-  # and `cp1252`
-  #
-  # This setting is useful if your log files are in `Latin-1` (aka `cp1252`)
-  # or in another character set other than `UTF-8`.
-  #
-  # This only affects `plain` format logs since json is `UTF-8` already.
-  config :charset, :deprecated => "Use the codec setting instead. For example: input { %PLUGIN% { codec => plain { charset => \"UTF-8\" } }"
+  config :message_format, :validate => :string, :obsolete => "Setting is no longer valid."
 
-  # If format is `json`, an event `sprintf` string to build what
-  # the display `@message` should be given (defaults to the raw JSON).
-  # `sprintf` format strings look like `%{fieldname}`
-  #
-  # If format is `json_event`, ALL fields except for `@type`
-  # are expected to be present. Not receiving all fields
-  # will cause unexpected results.
-  config :message_format, :validate => :string, :deprecated => true
+  # The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.
+  config :codec, :validate => :codec, :default => "plain"
 
   # Add any number of arbitrary tags to your event.
   #
@@ -67,27 +52,9 @@ class LogStash::Inputs::Base < LogStash::Plugin
   def initialize(params={})
     super
     @threadable = false
+    @stop_called = Concurrent::AtomicBoolean.new(false)
     config_init(params)
     @tags ||= []
-
-    if @charset && @codec.class.get_config.include?("charset")
-      # charset is deprecated on inputs, but provide backwards compatibility
-      # by copying the charset setting into the codec.
-
-      @logger.info("Copying input's charset setting into codec", :input => self, :codec => @codec)
-      charset = @charset
-      @codec.instance_eval { @charset = charset }
-    end
-
-    # Backwards compat for the 'format' setting
-    case @format
-      when "plain"; # do nothing
-      when "json"
-        @codec = LogStash::Plugin.lookup("codec", "json").new
-      when "json_event"
-        @codec = LogStash::Plugin.lookup("codec", "oldlogstashjson").new
-    end
-
   end # def initialize
 
   public
@@ -100,10 +67,27 @@ def tag(newtag)
     @tags << newtag
   end # def tag
 
-  protected
-  def to_event(raw, source)
-    raise LogStash::ThisMethodWasRemoved("LogStash::Inputs::Base#to_event - you should use codecs now instead of to_event. Not sure what this means? Get help on https://discuss.elastic.co/c/logstash")
-  end # def to_event
+  public
+  # override stop if you need to do more than do_stop to
+  # enforce the input plugin to return from `run`.
+  # e.g. a tcp plugin might need to close the tcp socket
+  # so blocking read operation aborts
+  def stop
+    # override if necessary
+  end
+
+  public
+  def do_stop
+    @logger.debug("stopping", :plugin => self)
+    @stop_called.make_true
+    stop
+  end
+
+  # stop? should never be overriden
+  public
+  def stop?
+    @stop_called.value
+  end
 
   protected
   def decorate(event)
diff --git a/lib/logstash/java_integration.rb b/lib/logstash/java_integration.rb
index 27ef229e07d..670ceaae650 100644
--- a/lib/logstash/java_integration.rb
+++ b/lib/logstash/java_integration.rb
@@ -78,6 +78,22 @@ def delete(o)
     self.removeAll([o]) ? o : block_given? ? yield : nil
   end
 
+  def compact
+    duped = Java::JavaUtil::ArrayList.new(self)
+    duped.compact!
+    duped
+  end
+
+  def compact!
+    size_before = self.size
+    self.removeAll(java::util::Collections.singleton(nil))
+    if size_before == self.size
+      nil
+    else
+      self
+    end
+  end
+
   # support the Ruby intersection method on Java Collection
   def &(other)
     # transform self into a LinkedHashSet to remove duplicates and preserve order as defined by the Ruby Array intersection contract
diff --git a/lib/logstash/json.rb b/lib/logstash/json.rb
index 5079de759a0..adbabff18c5 100644
--- a/lib/logstash/json.rb
+++ b/lib/logstash/json.rb
@@ -32,15 +32,23 @@ def mri_dump(o)
     ### JRuby
 
     def jruby_load(data, options = {})
-      options[:symbolize_keys] ? JrJackson::Raw.parse_sym(data) : JrJackson::Raw.parse_raw(data)
+      # TODO [guyboertje] remove these comments in 5.0
+      # options[:symbolize_keys] ? JrJackson::Raw.parse_sym(data) : JrJackson::Raw.parse_raw(data)
+
+      JrJackson::Ruby.parse(data, options)
+
     rescue JrJackson::ParseError => e
       raise LogStash::Json::ParserError.new(e.message)
     end
 
     def jruby_dump(o)
+      # TODO [guyboertje] remove these comments in 5.0
       # test for enumerable here to work around an omission in JrJackson::Json.dump to
       # also look for Java::JavaUtil::ArrayList, see TODO submit issue
-      o.is_a?(Enumerable) ? JrJackson::Raw.generate(o) : JrJackson::Json.dump(o)
+      # o.is_a?(Enumerable) ? JrJackson::Raw.generate(o) : JrJackson::Json.dump(o)
+
+      JrJackson::Base.generate(o, {})
+
     rescue => e
       raise LogStash::Json::GeneratorError.new(e.message)
     end
diff --git a/lib/logstash/multiqueue.rb b/lib/logstash/multiqueue.rb
deleted file mode 100644
index 237105662f8..00000000000
--- a/lib/logstash/multiqueue.rb
+++ /dev/null
@@ -1,53 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "cabin"
-
-class LogStash::MultiQueue
-  attr_accessor :logger
-
-  public
-  def initialize(*queues)
-    @logger = Cabin::Channel.get(LogStash)
-    @mutex = Mutex.new
-    @queues = queues
-  end # def initialize
-
-  public
-  def logger=(_logger)
-    @logger = _logger
-
-    # Set the logger for all known queues, too.
-    @queues.each do |q|
-      q.logger = _logger
-    end
-  end # def logger=
-
-  # Push an object to all queues.
-  public
-  def push(object)
-    @queues.each { |q| q.push(object) }
-  end # def push
-  alias :<< :push
-
-  alias_method :<<, :push
-
-  # Add a new Queue to this queue.
-  public
-  def add_queue(queue)
-    @mutex.synchronize do
-      @queues << queue
-    end
-  end # def add_queue
-
-  public
-  def remove_queue(queue)
-    @mutex.synchronize do
-      @queues.delete(queue)
-    end
-  end # def remove_queue
-
-  public
-  def size
-    return @queues.collect { |q| q.size }
-  end # def size
-end # class LogStash::MultiQueue
diff --git a/lib/logstash/outputs/base.rb b/lib/logstash/outputs/base.rb
index 831f5fb806d..29bf7f7e191 100644
--- a/lib/logstash/outputs/base.rb
+++ b/lib/logstash/outputs/base.rb
@@ -1,30 +1,20 @@
 # encoding: utf-8
-require "cgi"
 require "logstash/event"
 require "logstash/logging"
 require "logstash/plugin"
 require "logstash/namespace"
 require "logstash/config/mixin"
-require "uri"
 
 class LogStash::Outputs::Base < LogStash::Plugin
   include LogStash::Config::Mixin
 
   config_name "output"
 
-  # The type to act on. If a type is given, then this output will only
-  # act on messages with the same type. See any input plugin's `type`
-  # attribute for more.
-  # Optional.
-  config :type, :validate => :string, :default => "", :deprecated => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
+  config :type, :validate => :string, :default => "", :obsolete => "You can achieve this same behavior with the new conditionals, like: `if [type] == \"sometype\" { %PLUGIN% { ... } }`."
 
-  # Only handle events with all of these tags.
-  # Optional.
-  config :tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if \"sometag\" in [tags] { %PLUGIN% { ... } }`"
+  config :tags, :validate => :array, :default => [], :obsolete => "You can achieve similar behavior with the new conditionals, like: `if \"sometag\" in [tags] { %PLUGIN% { ... } }`"
 
-  # Only handle events without any of these tags.
-  # Optional.
-  config :exclude_tags, :validate => :array, :default => [], :deprecated => "You can achieve similar behavior with the new conditionals, like: `if !(\"sometag\" in [tags]) { %PLUGIN% { ... } }`"
+  config :exclude_tags, :validate => :array, :default => [], :obsolete => "You can achieve similar behavior with the new conditionals, like: `if (\"sometag\" not in [tags]) { %PLUGIN% { ... } }`"
 
   # The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.
   config :codec, :validate => :codec, :default => "plain"
@@ -33,7 +23,7 @@ class LogStash::Outputs::Base < LogStash::Plugin
   # Note that this setting may not be useful for all outputs.
   config :workers, :validate => :number, :default => 1
 
-  attr_reader :worker_plugins, :worker_queue
+  attr_reader :worker_plugins, :worker_queue, :worker_threads
 
   public
   def workers_not_supported(message=nil)
@@ -66,13 +56,15 @@ def receive(event)
   def worker_setup
     if @workers == 1
       @worker_plugins = [self]
+      @worker_threads = []
     else
       define_singleton_method(:handle, method(:handle_worker))
       @worker_queue = SizedQueue.new(20)
       @worker_plugins = @workers.times.map { self.class.new(@original_params.merge("workers" => 1)) }
-      @worker_plugins.map.with_index do |plugin, i|
+      @worker_threads = @worker_plugins.map.with_index do |plugin, i|
         Thread.new(original_params, @worker_queue) do |params, queue|
-          LogStash::Util::set_thread_name(">#{self.class.config_name}.#{i}")
+          LogStash::Util.set_thread_name(">#{self.class.config_name}.#{i}")
+          LogStash::Util.set_thread_plugin(self)
           plugin.register
           while true
             event = queue.pop
@@ -85,40 +77,18 @@ def worker_setup
 
   public
   def handle(event)
+    LogStash::Util.set_thread_plugin(self)
     receive(event)
   end # def handle
 
   def handle_worker(event)
+    LogStash::Util.set_thread_plugin(self)
     @worker_queue.push(event)
   end
 
   private
   def output?(event)
-    if !@type.empty?
-      if event["type"] != @type
-        @logger.debug? and @logger.debug("outputs/#{self.class.name}: Dropping event because type doesn't match",
-                                         :type => @type, :event => event)
-        return false
-      end
-    end
-
-    if !@tags.empty?
-      return false if !event["tags"]
-      if (event["tags"] & @tags).size != @tags.size
-        @logger.debug? and @logger.debug("outputs/#{self.class.name}: Dropping event because tags don't match",
-                                         :tags => @tags, :event => event)
-        return false
-      end
-    end
-
-    if !@exclude_tags.empty? && event["tags"]
-      if (diff_tags = (event["tags"] & @exclude_tags)).size != 0
-        @logger.debug? and @logger.debug("outputs/#{self.class.name}: Dropping event because tags contains excluded tags",
-                                         :diff_tags => diff_tags, :exclude_tags => @exclude_tags, :event => event)
-        return false
-      end
-    end
-
-    return true
-  end
+    # TODO: noop for now, remove this once we delete this call from all plugins
+    true
+  end # def output?
 end # class LogStash::Outputs::Base
diff --git a/lib/logstash/patches/stronger_openssl_defaults.rb b/lib/logstash/patches/stronger_openssl_defaults.rb
index f1239b1d516..ccd43ac1f86 100644
--- a/lib/logstash/patches/stronger_openssl_defaults.rb
+++ b/lib/logstash/patches/stronger_openssl_defaults.rb
@@ -61,7 +61,7 @@ def self.__default_options
   # For more details see: https://github.com/elastic/logstash/issues/3657
   remove_const(:DEFAULT_PARAMS) if const_defined?(:DEFAULT_PARAMS)
   DEFAULT_PARAMS = {
-    :ssl_version => "SSLv23",
+    :ssl_version => "TLS",
     :ciphers => MOZILLA_INTERMEDIATE_CIPHERS,
     :options => __default_options # Not a constant because it's computed at start-time.
   }
diff --git a/lib/logstash/pipeline.rb b/lib/logstash/pipeline.rb
index 0089e3f1688..1a74d15456e 100644
--- a/lib/logstash/pipeline.rb
+++ b/lib/logstash/pipeline.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
-require "thread" #
+require "thread"
 require "stud/interval"
+require "concurrent"
 require "logstash/namespace"
 require "logstash/errors"
 require "logstash/event"
@@ -8,18 +9,26 @@
 require "logstash/filters/base"
 require "logstash/inputs/base"
 require "logstash/outputs/base"
-require "logstash/util/reporter"
+require "logstash/config/cpu_core_strategy"
+require "logstash/util/defaults_printer"
+require "logstash/shutdown_controller"
 
-class LogStash::Pipeline
+module LogStash; class Pipeline
+  attr_reader :inputs, :filters, :outputs, :input_to_filter, :filter_to_output
 
   def initialize(configstr)
     @logger = Cabin::Channel.get(LogStash)
+
+    @inputs = nil
+    @filters = nil
+    @outputs = nil
+
     grammar = LogStashConfigParser.new
     @config = grammar.parse(configstr)
+
     if @config.nil?
       raise LogStash::ConfigurationError, grammar.failure_reason
     end
-
     # This will compile the config to ruby and evaluate the resulting code.
     # The code will initialize all the plugins and define the
     # filter and output methods.
@@ -34,40 +43,50 @@ def initialize(configstr)
     end
 
     @input_to_filter = SizedQueue.new(20)
+    # if no filters, pipe inputs directly to outputs
+    @filter_to_output = filters? ? SizedQueue.new(20) : @input_to_filter
 
-    # If no filters, pipe inputs directly to outputs
-    if !filters?
-      @filter_to_output = @input_to_filter
-    else
-      @filter_to_output = SizedQueue.new(20)
-    end
     @settings = {
-      "filter-workers" => 1,
+      "default-filter-workers" => LogStash::Config::CpuCoreStrategy.fifty_percent
     }
 
-    @run_mutex = Mutex.new
-    @ready = false
-    @started = false
+    # @ready requires thread safety since it is typically polled from outside the pipeline thread
+    @ready = Concurrent::AtomicBoolean.new(false)
     @input_threads = []
+    @filter_threads = []
   end # def initialize
 
   def ready?
-    @run_mutex.synchronize{@ready}
+    @ready.value
   end
 
-  def started?
-    @run_mutex.synchronize{@started}
+  def configure(setting, value)
+    @settings[setting] = value
   end
 
-  def configure(setting, value)
-    if setting == "filter-workers"
-      # Abort if we have any filters that aren't threadsafe
-      if value > 1 && @filters.any? { |f| !f.threadsafe? }
-        plugins = @filters.select { |f| !f.threadsafe? }.collect { |f| f.class.config_name }
-        raise LogStash::ConfigurationError, "Cannot use more than 1 filter worker because the following plugins don't work with more than one worker: #{plugins.join(", ")}"
+  def safe_filter_worker_count
+    default = @settings["default-filter-workers"]
+    thread_count = @settings["filter-workers"] #override from args "-w 8" or config
+    safe_filters, unsafe_filters = @filters.partition(&:threadsafe?)
+    if unsafe_filters.any?
+      plugins = unsafe_filters.collect { |f| f.class.config_name }
+      case thread_count
+      when nil
+        # user did not specify a worker thread count
+        # warn if the default is multiple
+        @logger.warn("Defaulting filter worker threads to 1 because there are some filters that might not work with multiple worker threads",
+          :count_was => default, :filters => plugins) if default > 1
+        1 # can't allow the default value to propagate if there are unsafe filters
+      when 0, 1
+        1
+      else
+        @logger.warn("Warning: Manual override - there are filters that might not work with multiple worker threads",
+          :worker_threads => thread_count, :filters => plugins)
+        thread_count # allow user to force this even if there are unsafe filters
       end
+    else
+      thread_count || default
     end
-    @settings[setting] = value
   end
 
   def filters?
@@ -75,14 +94,17 @@ def filters?
   end
 
   def run
-    @run_mutex.synchronize{@started = true}
-
-    # synchronize @input_threads between run and shutdown
-    @run_mutex.synchronize{start_inputs}
-    start_filters if filters?
-    start_outputs
+    @logger.terminal(LogStash::Util::DefaultsPrinter.print(@settings))
 
-    @run_mutex.synchronize{@ready = true}
+    begin
+      start_inputs
+      start_filters if filters?
+      start_outputs
+    ensure
+      # it is important to garantee @ready to be true after the startup sequence has been completed
+      # to potentially unblock the shutdown method which may be waiting on @ready to proceed
+      @ready.make_true
+    end
 
     @logger.info("Pipeline started")
     @logger.terminal("Logstash startup completed")
@@ -107,12 +129,6 @@ def run
 
   def wait_inputs
     @input_threads.each(&:join)
-  rescue Interrupt
-    # rbx does weird things during do SIGINT that I haven't debugged
-    # so we catch Interrupt here and signal a shutdown. For some reason the
-    # signal handler isn't invoked it seems? I dunno, haven't looked much into
-    # it.
-    shutdown
   end
 
   def shutdown_filters
@@ -121,7 +137,7 @@ def shutdown_filters
   end
 
   def wait_filters
-    @filter_threads.each(&:join) if @filter_threads
+    @filter_threads.each(&:join)
   end
 
   def shutdown_outputs
@@ -153,10 +169,22 @@ def start_inputs
 
   def start_filters
     @filters.each(&:register)
-    @filter_threads = @settings["filter-workers"].times.collect do
-      Thread.new { filterworker }
+    # dynamically get thread count based on filter threadsafety
+    # moved this test to here to allow for future config reloading
+    to_start = safe_filter_worker_count
+    @filter_threads = to_start.times.collect do |i|
+      Thread.new do
+        LogStash::Util.set_thread_name("|filterworker.#{i}")
+        filterworker
+      end
+    end
+    actually_started = @filter_threads.select(&:alive?).size
+    msg = "Worker threads expected: #{to_start}, worker threads started: #{actually_started}"
+    if actually_started < to_start
+      @logger.warn(msg)
+    else
+      @logger.info(msg)
     end
-
     @flusher_thread = Thread.new { Stud.interval(5) { @input_to_filter.push(LogStash::FLUSH) } }
   end
 
@@ -172,12 +200,20 @@ def start_input(plugin)
   end
 
   def inputworker(plugin)
-    LogStash::Util::set_thread_name("<#{plugin.class.config_name}")
+    LogStash::Util.set_thread_name("<#{plugin.class.config_name}")
+    LogStash::Util.set_thread_plugin(plugin)
     begin
       plugin.run(@input_to_filter)
-    rescue LogStash::ShutdownSignal
-      # ignore and quit
     rescue => e
+      # if plugin is stopping, ignore uncatched exceptions and exit worker
+      if plugin.stop?
+        @logger.debug("Input plugin raised exception during shutdown, ignoring it.",
+                      :plugin => plugin.class.config_name, :exception => e,
+                      :backtrace => e.backtrace)
+        return
+      end
+
+      # otherwise, report error and restart
       if @logger.debug?
         @logger.error(I18n.t("logstash.pipeline.worker-error-debug",
                              :plugin => plugin.inspect, :error => e.to_s,
@@ -187,28 +223,17 @@ def inputworker(plugin)
         @logger.error(I18n.t("logstash.pipeline.worker-error",
                              :plugin => plugin.inspect, :error => e))
       end
-      puts e.backtrace if @logger.debug?
-      # input teardown must be synchronized since is can be called concurrently by
-      # the input worker thread and from the pipeline thread shutdown method.
-      # this means that input teardown methods must support multiple calls.
-      @run_mutex.synchronize{plugin.teardown}
-      sleep 1
-      retry
-    end
-  ensure
-    begin
-      # input teardown must be synchronized since is can be called concurrently by
-      # the input worker thread and from the pipeline thread shutdown method.
-      # this means that input teardown methods must support multiple calls.
-      @run_mutex.synchronize{plugin.teardown}
-    rescue LogStash::ShutdownSignal
-      # teardown could receive the ShutdownSignal, retry it
+
+      # Assuming the failure that caused this exception is transient,
+      # let's sleep for a bit and execute #run again
+      sleep(1)
       retry
+    ensure
+      plugin.do_close
     end
   end # def inputworker
 
   def filterworker
-    LogStash::Util::set_thread_name("|worker")
     begin
       while true
         event = @input_to_filter.pop
@@ -227,65 +252,51 @@ def filterworker
           break
         end
       end
-    rescue => e
-      @logger.error("Exception in filterworker", "exception" => e, "backtrace" => e.backtrace)
+    rescue Exception => e
+      # Plugins authors should manage their own exceptions in the plugin code
+      # but if an exception is raised up to the worker thread they are considered
+      # fatal and logstash will not recover from this situation.
+      #
+      # Users need to check their configuration or see if there is a bug in the
+      # plugin.
+      @logger.error("Exception in filterworker, the pipeline stopped processing new events, please check your filter configuration and restart Logstash.",
+                    "exception" => e, "backtrace" => e.backtrace)
+      raise
+    ensure
+      @filters.each(&:do_close)
     end
-
-    @filters.each(&:teardown)
   end # def filterworker
 
   def outputworker
-    LogStash::Util::set_thread_name(">output")
+    LogStash::Util.set_thread_name(">output")
     @outputs.each(&:worker_setup)
 
     while true
       event = @filter_to_output.pop
       break if event == LogStash::SHUTDOWN
       output_func(event)
-    end # while true
-
+      LogStash::Util.set_thread_plugin(nil)
+    end
+  ensure
     @outputs.each do |output|
-      output.worker_plugins.each(&:teardown)
+      output.worker_plugins.each(&:do_close)
     end
   end # def outputworker
 
-  # Shutdown this pipeline.
-  #
-  # This method is intended to be called from another thread
-  def shutdown
-    InflightEventsReporter.logger = @logger
-    InflightEventsReporter.start(@input_to_filter, @filter_to_output, @outputs)
-    @input_threads.each do |thread|
-      # Interrupt all inputs
-      @logger.info("Sending shutdown signal to input thread", :thread => thread)
-
-      # synchronize both ShutdownSignal and teardown below. by synchronizing both
-      # we will avoid potentially sending a shutdown signal when the inputworker is
-      # executing the teardown method.
-      @run_mutex.synchronize do
-        thread.raise(LogStash::ShutdownSignal)
-        begin
-          thread.wakeup # in case it's in blocked IO or sleeping
-        rescue ThreadError
-        end
-      end
-    end
+  # initiate the pipeline shutdown sequence
+  # this method is intended to be called from outside the pipeline thread
+  # @param before_stop [Proc] code block called before performing stop operation on input plugins
+  def shutdown(&before_stop)
+    # shutdown can only start once the pipeline has completed its startup.
+    # avoid potential race conditoon between the startup sequence and this
+    # shutdown method which can be called from another thread at any time
+    sleep(0.1) while !ready?
 
-    # sometimes an input is stuck in a blocking I/O so we need to tell it to teardown directly
-    @inputs.each do |input|
-      begin
-        # input teardown must be synchronized since is can be called concurrently by
-        # the input worker thread and from the pipeline thread shutdown method.
-        # this means that input teardown methods must support multiple calls.
-        @run_mutex.synchronize{input.teardown}
-      rescue LogStash::ShutdownSignal
-        # teardown could receive the ShutdownSignal, retry it
-        retry
-      end
-    end
+    # TODO: should we also check against calling shutdown multiple times concurently?
+
+    before_stop.call if block_given?
 
-    # No need to send the ShutdownEvent to the filters/outputs nor to wait for
-    # the inputs to finish, because in the #run method we wait for that anyway.
+    @inputs.each(&:do_stop)
   end # def shutdown
 
   def plugin(plugin_type, name, *args)
@@ -324,4 +335,49 @@ def flush_filters_to_output!(options = {})
     end
   end # flush_filters_to_output!
 
-end # class Pipeline
+  def inflight_count
+    data = {}
+    total = 0
+
+    input_to_filter = @input_to_filter.size
+    total += input_to_filter
+    filter_to_output = @filter_to_output.size
+    total += filter_to_output
+
+    data["input_to_filter"] = input_to_filter if input_to_filter > 0
+    data["filter_to_output"] = filter_to_output if filter_to_output > 0
+
+    output_worker_queues = []
+    @outputs.each do |output|
+      next unless output.worker_queue && output.worker_queue.size > 0
+      plugin_info = output.debug_info
+      size = output.worker_queue.size
+      total += size
+      plugin_info << size
+      output_worker_queues << plugin_info
+    end
+    data["output_worker_queues"] = output_worker_queues unless output_worker_queues.empty?
+    data["total"] = total
+    data
+  end
+
+  def stalling_threads
+    plugin_threads
+     .reject {|t| t["blocked_on"] } # known begnin blocking statuses
+     .each {|t| t.delete("backtrace") }
+     .each {|t| t.delete("blocked_on") }
+     .each {|t| t.delete("status") }
+  end
+
+  def plugin_threads
+    input_threads = @input_threads.select {|t| t.alive? }.map {|t| thread_info(t) }
+    filter_threads = @filter_threads.select {|t| t.alive? }.map {|t| thread_info(t) }
+    output_threads = @output_threads.select {|t| t.alive? }.map {|t| thread_info(t) }
+    output_worker_threads = @outputs.flat_map {|output| output.worker_threads }.map {|t| thread_info(t) }
+    input_threads + filter_threads + output_threads + output_worker_threads
+  end
+
+  def thread_info(thread)
+    LogStash::Util.thread_info(thread)
+  end
+end; end
diff --git a/lib/logstash/plugin.rb b/lib/logstash/plugin.rb
index 76d3eeb43fb..bfab9a58d28 100644
--- a/lib/logstash/plugin.rb
+++ b/lib/logstash/plugin.rb
@@ -3,6 +3,7 @@
 require "logstash/logging"
 require "logstash/config/mixin"
 require "cabin"
+require "concurrent"
 
 class LogStash::Plugin
   attr_accessor :params
@@ -27,88 +28,25 @@ def initialize(params=nil)
     @logger = Cabin::Channel.get(LogStash)
   end
 
-  # This method is called when someone or something wants this plugin to shut
-  # down. When you successfully shutdown, you must call 'finished'
-  # You must also call 'super' in any subclasses.
+  # close is called during shutdown, after the plugin worker
+  # main task terminates
   public
-  def shutdown(queue)
-    # By default, shutdown is assumed a no-op for all plugins.
-    # If you need to take special efforts to shutdown (like waiting for
-    # an operation to complete, etc)
-    teardown
-    @logger.info("Received shutdown signal", :plugin => self)
-
-    @shutdown_queue = queue
-    if @plugin_state == :finished
-      finished
-    else
-      @plugin_state = :terminating
-    end
-  end # def shutdown
-
-  # You should call this method when you (the plugin) are done with work
-  # forever.
-  public
-  def finished
-    # TODO(sissel): I'm not sure what I had planned for this shutdown_queue
-    # thing
-    if @shutdown_queue
-      @logger.info("Sending shutdown event to agent queue", :plugin => self)
-      @shutdown_queue << self
-    end
-
-    if @plugin_state != :finished
-      @logger.info("Plugin is finished", :plugin => self)
-      @plugin_state = :finished
-    end
-  end # def finished
-
-  # Subclasses should implement this teardown method if you need to perform any
-  # special tasks during shutdown (like flushing, etc.)
-  public
-  def teardown
-    # nothing by default
-    finished
+  def do_close
+    @logger.debug("closing", :plugin => self)
+    close
   end
 
-  # This method is called when a SIGHUP triggers a reload operation
+  # Subclasses should implement this close method if you need to perform any
+  # special tasks during shutdown (like flushing, etc.)
   public
-  def reload
-    # Do nothing by default
+  def close
+    # ..
   end
 
-  public
-  def finished?
-    return @plugin_state == :finished
-  end # def finished?
-
-  public
-  def running?
-    return @plugin_state != :finished
-  end # def finished?
-
-  public
-  def terminating?
-    return @plugin_state == :terminating
-  end # def terminating?
-
-  public
   def to_s
     return "#{self.class.name}: #{@params}"
   end
 
-  protected
-  def update_watchdog(state)
-    Thread.current[:watchdog] = Time.now
-    Thread.current[:watchdog_state] = state
-  end
-
-  protected
-  def clear_watchdog
-    Thread.current[:watchdog] = nil
-    Thread.current[:watchdog_state] = nil
-  end
-
   public
   def inspect
     if !@params.nil?
@@ -121,6 +59,11 @@ def inspect
     end
   end
 
+  public
+  def debug_info
+    [self.class.to_s, original_params]
+  end
+
   # Look up a plugin by type and name.
   public
   def self.lookup(type, name)
diff --git a/lib/logstash/shutdown_controller.rb b/lib/logstash/shutdown_controller.rb
new file mode 100644
index 00000000000..6941753bbc8
--- /dev/null
+++ b/lib/logstash/shutdown_controller.rb
@@ -0,0 +1,127 @@
+# encoding: utf-8
+
+module LogStash
+  class ShutdownController
+
+    CHECK_EVERY = 1 # second
+    REPORT_EVERY = 5 # checks
+    ABORT_AFTER = 3 # stalled reports
+
+    attr_reader :cycle_period, :report_every, :abort_threshold
+
+    def initialize(pipeline, cycle_period=CHECK_EVERY, report_every=REPORT_EVERY, abort_threshold=ABORT_AFTER)
+      @pipeline = pipeline
+      @cycle_period = cycle_period
+      @report_every = report_every
+      @abort_threshold = abort_threshold
+      @reports = []
+    end
+
+    def self.unsafe_shutdown=(boolean)
+      @unsafe_shutdown = boolean
+    end
+
+    def self.unsafe_shutdown?
+      @unsafe_shutdown
+    end
+
+    def self.logger=(logger)
+      @logger = logger
+    end
+
+    def self.logger
+      @logger ||= Cabin::Channel.get(LogStash)
+    end
+
+    def self.start(pipeline, cycle_period=CHECK_EVERY, report_every=REPORT_EVERY, abort_threshold=ABORT_AFTER)
+      controller = self.new(pipeline, cycle_period, report_every, abort_threshold)
+      Thread.new(controller) { |controller| controller.start }
+    end
+
+    def logger
+      self.class.logger
+    end
+
+    def start
+      sleep(@cycle_period)
+      cycle_number = 0
+      stalled_count = 0
+      Stud.interval(@cycle_period) do
+        @reports << Report.from_pipeline(@pipeline)
+        @reports.delete_at(0) if @reports.size > @report_every # expire old report
+        if cycle_number == (@report_every - 1) # it's report time!
+          logger.warn(@reports.last.to_hash)
+
+          if shutdown_stalled?
+            logger.error("The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.") if stalled_count == 0
+            stalled_count += 1
+
+            if self.class.unsafe_shutdown? && @abort_threshold == stalled_count
+              logger.fatal("Forcefully quitting logstash..")
+              force_exit()
+              break
+            end
+          else
+            stalled_count = 0
+          end
+        end
+        cycle_number = (cycle_number + 1) % @report_every
+      end
+    end
+
+    # A pipeline shutdown is stalled if
+    # * at least REPORT_EVERY reports have been created
+    # * the inflight event count is in monotonically increasing
+    # * there are worker threads running which aren't blocked on SizedQueue pop/push
+    # * the stalled thread list is constant in the previous REPORT_EVERY reports
+    def shutdown_stalled?
+      return false unless @reports.size == @report_every #
+      # is stalled if inflight count is either constant or increasing
+      stalled_event_count = @reports.each_cons(2).all? do |prev_report, next_report|
+        prev_report.inflight_count["total"] <= next_report.inflight_count["total"]
+      end
+      if stalled_event_count
+        @reports.each_cons(2).all? do |prev_report, next_report|
+          prev_report.stalling_threads == next_report.stalling_threads
+        end
+      else
+        false
+      end
+    end
+
+    def force_exit
+      exit(-1)
+    end
+  end
+
+  class Report
+
+    attr_reader :inflight_count, :stalling_threads
+
+    def self.from_pipeline(pipeline)
+      new(pipeline.inflight_count, pipeline.stalling_threads)
+    end
+
+    def initialize(inflight_count, stalling_threads)
+      @inflight_count = inflight_count
+      @stalling_threads = format_threads_by_plugin(stalling_threads)
+    end
+
+    def to_hash
+      {
+        "INFLIGHT_EVENT_COUNT" => @inflight_count,
+        "STALLING_THREADS" => @stalling_threads
+      }
+    end
+
+    def format_threads_by_plugin(stalling_threads)
+      stalled_plugins = {}
+      stalling_threads.each do |thr|
+        key = (thr.delete("plugin") || "other")
+        stalled_plugins[key] ||= []
+        stalled_plugins[key] << thr
+      end
+      stalled_plugins
+    end
+  end
+end
diff --git a/lib/logstash/string_interpolation.rb b/lib/logstash/string_interpolation.rb
index 04bb55edc49..fc357f67515 100644
--- a/lib/logstash/string_interpolation.rb
+++ b/lib/logstash/string_interpolation.rb
@@ -46,7 +46,7 @@ def compile_template(template)
         position = match.offset(0).last
       end
 
-      if position < template.size - 1
+      if position < template.size
         nodes << StaticNode.new(template[position..-1])
       end
 
diff --git a/lib/logstash/threadwatchdog.rb b/lib/logstash/threadwatchdog.rb
deleted file mode 100644
index ab41d3a49a0..00000000000
--- a/lib/logstash/threadwatchdog.rb
+++ /dev/null
@@ -1,37 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/logging"
-
-class LogStash::ThreadWatchdog
-  attr_accessor :logger
-  attr_accessor :threads
-
-  class TimeoutError < StandardError; end
-
-  public
-  def initialize(threads, watchdog_timeout=10)
-    @threads = threads
-    @watchdog_timeout = watchdog_timeout
-  end # def initialize
-
-  public
-  def watch
-    while sleep(1)
-      cutoff = Time.now - @watchdog_timeout
-      @threads.each do |t|
-        watchdog = t[:watchdog]
-        if watchdog and watchdog <= cutoff
-          age = Time.now - watchdog
-          @logger.fatal("thread watchdog timeout",
-                        :thread => t,
-                        :backtrace => t.backtrace,
-                        :thread_watchdog => watchdog,
-                        :age => age,
-                        :cutoff => @watchdog_timeout,
-                        :state => t[:watchdog_state])
-          raise TimeoutError, "watchdog timeout"
-        end
-      end
-    end
-  end # def watch
-end # class LogStash::ThreadWatchdog
diff --git a/lib/logstash/util.rb b/lib/logstash/util.rb
index 2034803f43c..d3c5fe6ff41 100644
--- a/lib/logstash/util.rb
+++ b/lib/logstash/util.rb
@@ -24,6 +24,41 @@ def self.set_thread_name(name)
     end
   end # def set_thread_name
 
+  def self.set_thread_plugin(plugin)
+    Thread.current[:plugin] = plugin
+  end
+
+  def self.get_thread_id(thread)
+    if RUBY_ENGINE == "jruby"
+      JRuby.reference(thread).native_thread.id
+    else
+      raise Exception.new("Native thread IDs aren't supported outside of JRuby")
+    end
+  end
+
+  def self.thread_info(thread)
+    backtrace = thread.backtrace.map do |line|
+      line.gsub(LogStash::Environment::LOGSTASH_HOME, "[...]")
+    end
+
+    blocked_on = case backtrace.first
+                 when /in `push'/ then "blocked_on_push"
+                 when /(?:pipeline|base).*pop/ then "waiting_for_events"
+                 else nil
+                 end
+
+    {
+      "thread_id" => get_thread_id(thread),
+      "name" => thread[:name],
+      "plugin" => (thread[:plugin] ? thread[:plugin].debug_info : nil),
+      "backtrace" => backtrace,
+      "blocked_on" => blocked_on,
+      "status" => thread.status,
+      "current_call" => backtrace.first
+    }
+  end
+
+
   # Merge hash 'src' into 'dst' nondestructively
   #
   # Duplicate keys will become array values
diff --git a/lib/logstash/util/defaults_printer.rb b/lib/logstash/util/defaults_printer.rb
new file mode 100644
index 00000000000..6dd850e1d50
--- /dev/null
+++ b/lib/logstash/util/defaults_printer.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+require "logstash/util/worker_threads_default_printer"
+
+
+# This class exists to format the settings for defaults used
+module LogStash module Util class DefaultsPrinter
+  def self.print(settings)
+    new(settings).print
+  end
+
+  def initialize(settings)
+    @settings = settings
+    @printers = [workers]
+  end
+
+  def print
+    collector = []
+    @printers.each do |printer|
+      printer.visit(collector)
+    end
+    "Settings: " + collector.join(', ')
+  end
+
+  private
+
+  def workers
+    WorkerThreadsDefaultPrinter.new(@settings)
+  end
+end end end
diff --git a/lib/logstash/util/reporter.rb b/lib/logstash/util/reporter.rb
deleted file mode 100644
index 4d983a25e3e..00000000000
--- a/lib/logstash/util/reporter.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-# encoding: utf-8
-class InflightEventsReporter
-  def self.logger=(logger)
-    @logger = logger
-  end
-
-  def self.start(input_to_filter, filter_to_output, outputs)
-    Thread.new do
-      loop do
-        sleep 5
-        report(input_to_filter, filter_to_output, outputs)
-      end
-    end
-  end
-
-  def self.report(input_to_filter, filter_to_output, outputs)
-    report = {
-      "input_to_filter" => input_to_filter.size,
-      "filter_to_output" => filter_to_output.size,
-      "outputs" => []
-    }
-    outputs.each do |output|
-      next unless output.worker_queue && output.worker_queue.size > 0
-      report["outputs"] << [output.inspect, output.worker_queue.size]
-    end
-    @logger.warn ["INFLIGHT_EVENTS_REPORT", Time.now.iso8601, report]
-  end
-end
diff --git a/lib/logstash/util/require-helper.rb b/lib/logstash/util/require-helper.rb
deleted file mode 100644
index 6e9fde0d86a..00000000000
--- a/lib/logstash/util/require-helper.rb
+++ /dev/null
@@ -1,18 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/logging"
-
-module LogStash::Util::Require
-  class << self
-    attr_accessor :logger
-
-    def require(lib, gemdep, message=nil)
-      @logger ||= LogStash::Logger.new(STDERR)
-      begin
-        require lib
-      rescue LoadError => e
-        @logger.error("Failed loading '#{lib}'")
-      end
-    end # def require
-  end # class << self
-end # def LogStash::Util::Require
diff --git a/lib/logstash/util/worker_threads_default_printer.rb b/lib/logstash/util/worker_threads_default_printer.rb
new file mode 100644
index 00000000000..82e88196e96
--- /dev/null
+++ b/lib/logstash/util/worker_threads_default_printer.rb
@@ -0,0 +1,29 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+
+# This class exists to format the settings for default worker threads
+module LogStash module Util class WorkerThreadsDefaultPrinter
+
+  def initialize(settings)
+    @setting = settings.fetch('filter-workers', 0)
+    @default = settings.fetch('default-filter-workers', 0)
+  end
+
+  def visit(collector)
+    visit_setting(collector)
+    visit_default(collector)
+  end
+
+  def visit_setting(collector)
+    return if @setting == 0
+    collector.push("User set filter workers: #{@setting}")
+  end
+
+  def visit_default(collector)
+    return if @default == 0
+    collector.push "Default filter workers: #{@default}"
+  end
+
+end end end
+
diff --git a/lib/logstash/version.rb b/lib/logstash/version.rb
index 36f2bad27fe..551e01e4938 100644
--- a/lib/logstash/version.rb
+++ b/lib/logstash/version.rb
@@ -1,6 +1,6 @@
 # encoding: utf-8
 # The version of logstash.
-LOGSTASH_VERSION = "2.0.0.dev"
+LOGSTASH_VERSION = "2.1.2.snapshot1"
 
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
diff --git a/lib/pluginmanager/install.rb b/lib/pluginmanager/install.rb
index bbc486ab236..27c865ca68e 100644
--- a/lib/pluginmanager/install.rb
+++ b/lib/pluginmanager/install.rb
@@ -10,6 +10,7 @@ class LogStash::PluginManager::Install < LogStash::PluginManager::Command
   option "--version", "VERSION", "version of the plugin to install"
   option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
   option "--development", :flag, "install all development dependencies of currently installed plugins", :default => false
+  option "--local", :flag, "force local-only plugin installation. see bin/plugin package|unpack", :default => false
 
   # the install logic below support installing multiple plugins with each a version specification
   # but the argument parsing does not support it for now so currently if specifying --version only
@@ -23,7 +24,7 @@ def execute
       gems = plugins_development_gems
     else
       gems = plugins_gems
-      verify_remote!(gems) if verify?
+      verify_remote!(gems) if !local? && verify?
     end
 
     install_gems_list!(gems)
@@ -45,12 +46,20 @@ def validate_cli_options!
   # Check if the specified gems contains
   # the logstash `metadata`
   def verify_remote!(gems)
+    options = { :rubygems_source => gemfile.gemset.sources }
     gems.each do |plugin, version|
       puts("Validating #{[plugin, version].compact.join("-")}")
-      signal_error("Installation aborted, verification failed for #{plugin} #{version}") unless LogStash::PluginManager.logstash_plugin?(plugin, version)
+      next if validate_plugin(plugin, version, options)
+      signal_error("Installation aborted, verification failed for #{plugin} #{version}")
     end
   end
 
+  def validate_plugin(plugin, version, options)
+    LogStash::PluginManager.logstash_plugin?(plugin, version, options)
+  rescue SocketError
+    false
+  end
+
   def plugins_development_gems
     # Get currently defined gems and their dev dependencies
     specs = []
@@ -89,6 +98,7 @@ def install_gems_list!(install_list)
     bundler_options = {:install => true}
     bundler_options[:without] = [] if development?
     bundler_options[:rubygems_source] = gemfile.gemset.sources
+    bundler_options[:local] = true if local?
 
     output = LogStash::Bundler.invoke!(bundler_options)
 
diff --git a/lib/pluginmanager/main.rb b/lib/pluginmanager/main.rb
index c2f15775932..e14a131c84e 100644
--- a/lib/pluginmanager/main.rb
+++ b/lib/pluginmanager/main.rb
@@ -18,6 +18,8 @@ module PluginManager
 require "pluginmanager/uninstall"
 require "pluginmanager/list"
 require "pluginmanager/update"
+require "pluginmanager/pack"
+require "pluginmanager/unpack"
 
 module LogStash
   module PluginManager
@@ -26,7 +28,9 @@ class Error < StandardError; end
     class Main < Clamp::Command
       subcommand "install", "Install a plugin", LogStash::PluginManager::Install
       subcommand "uninstall", "Uninstall a plugin", LogStash::PluginManager::Uninstall
-      subcommand "update", "Install a plugin", LogStash::PluginManager::Update
+      subcommand "update", "Update a plugin", LogStash::PluginManager::Update
+      subcommand "pack", "Package currently installed plugins", LogStash::PluginManager::Pack
+      subcommand "unpack", "Unpack packaged plugins", LogStash::PluginManager::Unpack
       subcommand "list", "List all installed plugins", LogStash::PluginManager::List
     end
   end
diff --git a/lib/pluginmanager/pack.rb b/lib/pluginmanager/pack.rb
new file mode 100644
index 00000000000..18b46e18511
--- /dev/null
+++ b/lib/pluginmanager/pack.rb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+require_relative "pack_command"
+
+class LogStash::PluginManager::Pack < LogStash::PluginManager::PackCommand
+  option "--tgz", :flag, "compress package as a tar.gz file", :default => !LogStash::Environment.windows?
+  option "--zip", :flag, "compress package as a zip file", :default => LogStash::Environment.windows?
+  option "--[no-]clean", :flag, "clean up the generated dump of plugins", :default => true
+  option "--overwrite", :flag, "Overwrite a previously generated package file", :default => false
+
+  def execute
+    puts("Packaging plugins for offline usage")
+
+    validate_target_file
+    LogStash::Bundler.invoke!({:package => true, :all => true})
+    archive_manager.compress(LogStash::Environment::CACHE_PATH, target_file)
+    FileUtils.rm_rf(LogStash::Environment::CACHE_PATH) if clean?
+
+    puts("Generated at #{target_file}")
+  end
+
+  private
+
+  def delete_target_file?
+    return true if overwrite?
+    puts("File #{target_file} exist, do you want to overwrite it? (Y/N)")
+    ( "y" == STDIN.gets.strip.downcase ? true : false)
+  end
+
+  def validate_target_file
+    if File.exist?(target_file)
+      if  delete_target_file?
+        File.delete(target_file)
+      else
+        signal_error("Package creation cancelled, a previously generated package exist at location: #{target_file}, move this file to safe place and run the command again")
+      end
+    end
+  end
+
+  def target_file
+    target_file = File.join(LogStash::Environment::LOGSTASH_HOME, "plugins_package")
+    "#{target_file}#{file_extension}"
+  end
+end
diff --git a/lib/pluginmanager/pack_command.rb b/lib/pluginmanager/pack_command.rb
new file mode 100644
index 00000000000..2409b212f97
--- /dev/null
+++ b/lib/pluginmanager/pack_command.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+require "bootstrap/util/compress"
+require "fileutils"
+
+class LogStash::PluginManager::PackCommand < LogStash::PluginManager::Command
+  def archive_manager
+    zip? ? LogStash::Util::Zip : LogStash::Util::Tar
+  end
+
+  def file_extension
+    zip? ? ".zip" : ".tar.gz"
+  end
+end
diff --git a/lib/pluginmanager/unpack.rb b/lib/pluginmanager/unpack.rb
new file mode 100644
index 00000000000..4e7da6fb94e
--- /dev/null
+++ b/lib/pluginmanager/unpack.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "pack_command"
+
+class LogStash::PluginManager::Unpack < LogStash::PluginManager::PackCommand
+  option "--tgz", :flag, "unpack a packaged tar.gz file", :default => !LogStash::Environment.windows?
+  option "--zip", :flag, "unpack a packaged  zip file", :default => LogStash::Environment.windows?
+
+  parameter "file", "the package file name", :attribute_name => :package_file, :required => true
+
+  def execute
+    puts("Unpacking #{package_file}")
+
+    FileUtils.rm_rf(LogStash::Environment::CACHE_PATH)
+    validate_cache_location
+    archive_manager.extract(package_file, LogStash::Environment::CACHE_PATH)
+    puts("Unpacked at #{LogStash::Environment::CACHE_PATH}")
+    puts("The unpacked plugins can now be installed in local-only mode using bin/plugin install --local [plugin name]")
+  end
+
+  private
+
+  def validate_cache_location
+    cache_location = LogStash::Environment::CACHE_PATH
+    if File.exist?(cache_location)
+      puts("Directory #{cache_location} is going to be overwritten, do you want to continue? (Y/N)")
+      override = ( "y" == STDIN.gets.strip.downcase ? true : false)
+      if override
+        FileUtils.rm_rf(cache_location)
+      else
+        puts("Unpack cancelled: file #{cache_location} already exists, please delete or move it")
+        exit
+      end
+    end
+  end
+end
diff --git a/lib/pluginmanager/update.rb b/lib/pluginmanager/update.rb
index 0d067991677..64c9767ea57 100644
--- a/lib/pluginmanager/update.rb
+++ b/lib/pluginmanager/update.rb
@@ -8,6 +8,8 @@ class LogStash::PluginManager::Update < LogStash::PluginManager::Command
   REJECTED_OPTIONS = [:path, :git, :github]
 
   parameter "[PLUGIN] ...", "Plugin name(s) to upgrade to latest version", :attribute_name => :plugins_arg
+  option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
+  option "--local", :flag, "force local-only plugin update. see bin/plugin package|unpack", :default => false
 
   def execute
     local_gems = gemfile.locally_installed_gems
@@ -21,7 +23,6 @@ def execute
 
       warn_local_gems(plugins_with_path)
     end
-
     update_gems!
   end
 
@@ -41,10 +42,12 @@ def update_gems!
     # remove any version constrain from the Gemfile so the plugin(s) can be updated to latest version
     # calling update without requiremend will remove any previous requirements
     plugins = plugins_to_update(previous_gem_specs_map)
+    # Skipping the major version validation when using a local cache as we can have situations
+    # without internet connection.
     filtered_plugins = plugins.map { |plugin| gemfile.find(plugin) }
       .compact
       .reject { |plugin| REJECTED_OPTIONS.any? { |key| plugin.options.has_key?(key) } }
-      .select { |plugin| validate_major_version(plugin.name) }
+      .select { |plugin| local? || (verify? ? validates_version(plugin.name) : true) }
       .each   { |plugin| gemfile.update(plugin.name) }
 
     # force a disk sync before running bundler
@@ -54,9 +57,10 @@ def update_gems!
 
     # any errors will be logged to $stderr by invoke!
     # Bundler cannot update and clean gems in one operation so we have to call the CLI twice.
-    output = LogStash::Bundler.invoke!(:update => plugins)
+    options = {:update => plugins, :rubygems_source => gemfile.gemset.sources}
+    options[:local] = true if local?
+    output = LogStash::Bundler.invoke!(options)
     output = LogStash::Bundler.invoke!(:clean => true)
-
     display_updated_plugins(previous_gem_specs_map)
   rescue => exception
     gemfile.restore!
@@ -67,16 +71,8 @@ def update_gems!
 
   # validate if there is any major version update so then we can ask the user if he is
   # sure to update or not.
-  def validate_major_version(plugin)
-    require "gems"
-    latest_version  = Gems.versions(plugin)[0]['number'].split(".")
-    current_version = Gem::Specification.find_by_name(plugin).version.version.split(".")
-    if (latest_version[0].to_i > current_version[0].to_i)
-      ## warn if users want to continue
-      puts("You are updating #{plugin} to a new version #{latest_version.join('.')}, which may not be compatible with #{current_version.join('.')}. are you sure you want to proceed (Y/N)?")
-      return ( "y" == STDIN.gets.strip.downcase ? true : false)
-    end
-    true
+  def validates_version(plugin)
+    LogStash::PluginManager.update_to_major_version?(plugin)
   end
 
   # create list of plugins to update
@@ -115,7 +111,7 @@ def display_updated_plugins(previous_gem_specs_map)
   # retrieve only the latest spec for all locally installed plugins
   # @return [Hash] result hash {plugin_name.downcase => plugin_spec}
   def find_latest_gem_specs
-    LogStash::PluginManager.find_plugins_gem_specs.inject({}) do |result, spec|
+    LogStash::PluginManager.all_installed_plugins_gem_specs(gemfile).inject({}) do |result, spec|
       previous = result[spec.name.downcase]
       result[spec.name.downcase] = previous ? [previous, spec].max_by{|s| s.version} : spec
       result
diff --git a/lib/pluginmanager/util.rb b/lib/pluginmanager/util.rb
index f5d08dae6bd..149ff6256d4 100644
--- a/lib/pluginmanager/util.rb
+++ b/lib/pluginmanager/util.rb
@@ -2,12 +2,18 @@
 require "rubygems/package"
 
 module LogStash::PluginManager
+
+  class ValidationError < StandardError; end
+
   # check for valid logstash plugin gem name & version or .gem file, logs errors to $stdout
   # uses Rubygems API and will remotely validated agains the current Gem.sources
   # @param plugin [String] plugin name or .gem file path
   # @param version [String] gem version requirement string
+  # @param [Hash] options the options used to setup external components
+  # @option options [Array<String>] :rubygems_source Gem sources to lookup for the verification
   # @return [Boolean] true if valid logstash plugin gem name & version or a .gem file
-  def self.logstash_plugin?(plugin, version = nil)
+  def self.logstash_plugin?(plugin, version = nil, options={})
+
     if plugin_file?(plugin)
       begin
         return logstash_plugin_gem_spec?(plugin_file_spec(plugin))
@@ -18,6 +24,7 @@ def self.logstash_plugin?(plugin, version = nil)
       end
     else
       dep = Gem::Dependency.new(plugin, version || Gem::Requirement.default)
+      Gem.sources = Gem::SourceList.from(options[:rubygems_source]) if options[:rubygems_source]
       specs, errors = Gem::SpecFetcher.fetcher.spec_for_dependency(dep)
 
       # dump errors
@@ -37,6 +44,35 @@ def self.logstash_plugin?(plugin, version = nil)
     end
   end
 
+  # Fetch latest version information as in rubygems
+  # @param [String] The plugin name
+  # @param [Hash] Set of available options when fetching the information
+  # @option options [Boolean] :pre Include pre release versions in the search (default: false)
+  # @return [Hash] The plugin version information as returned by rubygems
+  def self.fetch_latest_version_info(plugin, options={})
+    require "gems"
+    exclude_prereleases =  options.fetch(:pre, false)
+    versions = Gems.versions(plugin)
+    raise ValidationError.new("Something went wrong with the validation. You can skip the validation with the --no-verify option") if !versions.is_a?(Array) || versions.empty?
+    versions = versions.select { |version| !version["prerelease"] } if !exclude_prereleases
+    versions.first
+  end
+
+  # Let's you decide to update to the last version of a plugin if this is a major version
+  # @param [String] A plugin name
+  # @return [Boolean] True in case the update is moving forward, false otherwise
+  def self.update_to_major_version?(plugin_name)
+    plugin_version  = fetch_latest_version_info(plugin_name)
+    latest_version  = plugin_version['number'].split(".")
+    current_version = Gem::Specification.find_by_name(plugin_name).version.version.split(".")
+    if (latest_version[0].to_i > current_version[0].to_i)
+      ## warn if users want to continue
+      puts("You are updating #{plugin_name} to a new version #{latest_version.join('.')}, which may not be compatible with #{current_version.join('.')}. are you sure you want to proceed (Y/N)?")
+      return ( "y" == STDIN.gets.strip.downcase ? true : false)
+    end
+    true
+  end
+
   # @param spec [Gem::Specification] plugin gem specification
   # @return [Boolean] true if this spec is for an installable logstash plugin
   def self.logstash_plugin_gem_spec?(spec)
diff --git a/locales/en.yml b/locales/en.yml
index 129d459dff3..3626144521a 100644
--- a/locales/en.yml
+++ b/locales/en.yml
@@ -77,6 +77,10 @@ en:
         use to validate logstash's configuration before you choose
         to restart a running system.
       configuration:
+        obsolete: >-
+          The setting `%{name}` in plugin `%{plugin}` is obsolete and is no
+          longer available. %{extra} If you have any questions about this, you
+          are invited to visit https://discuss.elastic.co/c/logstash and ask.
         file-not-found: |-
           No config files found: %{path}
           Can you make sure this path is a logstash config file?
@@ -153,12 +157,6 @@ en:
           Check configuration for valid syntax and then exit.
         filterworkers: |+
           Sets the number of filter workers to run.
-        watchdog-timeout: |+
-          Set the filter watchdog timeout (in seconds).
-          This timeout is used to detect stuck filters;
-          stuck filters usually symptoms of bugs.
-          When a filter takes longer than TIMEOUT
-          seconds, it will cause logstash to abort.
         log: |+
           Write logstash internal logs to the given
           file. Without this flag, logstash will emit
@@ -189,3 +187,8 @@ en:
         debug: |+
           Most verbose logging. This causes 'debug'
           level logs to be emitted.
+        unsafe_shutdown: |+
+          Force logstash to exit during shutdown even
+          if there are still inflight events in memory.
+          By default, logstash will refuse to quit until all
+          received events have been pushed to the outputs.
diff --git a/logstash-core.gemspec b/logstash-core.gemspec
index 2df64952a60..774d79424ad 100644
--- a/logstash-core.gemspec
+++ b/logstash-core.gemspec
@@ -15,7 +15,7 @@ Gem::Specification.new do |gem|
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core"
   gem.require_paths = ["lib"]
-  gem.version       = LOGSTASH_VERSION
+  gem.version       = LOGSTASH_VERSION.gsub(/-/, '.')
 
   gem.add_runtime_dependency "cabin", "~> 0.7.0" #(Apache 2.0 license)
   gem.add_runtime_dependency "pry", "~> 0.10.1"  #(Ruby license)
@@ -23,6 +23,8 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "clamp", "~> 0.6.5" #(MIT license) for command line args/flags
   gem.add_runtime_dependency "filesize", "0.0.4" #(MIT license) for :bytes config validator
   gem.add_runtime_dependency "gems", "~> 0.8.3"  #(MIT license)
+  gem.add_runtime_dependency "concurrent-ruby", "0.9.2"
+  gem.add_runtime_dependency "jruby-openssl", "0.9.13" # Required to support TLSv1.2
 
   # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
   # investigated what the cause might be. -Jordan
@@ -33,11 +35,12 @@ Gem::Specification.new do |gem|
 
   # filetools and rakelib
   gem.add_runtime_dependency "minitar", "~> 0.5.4"
+  gem.add_runtime_dependency "rubyzip", "~> 1.1.7"
   gem.add_runtime_dependency "thread_safe", "~> 0.3.5" #(Apache 2.0 license)
 
   if RUBY_PLATFORM == 'java'
     gem.platform = RUBY_PLATFORM
-    gem.add_runtime_dependency "jrjackson", "~> 0.2.9" #(Apache 2.0 license)
+    gem.add_runtime_dependency "jrjackson", "~> 0.3.7" #(Apache 2.0 license)
   else
     gem.add_runtime_dependency "oj" #(MIT-style license)
   end
diff --git a/pkg/centos/after-install.sh b/pkg/centos/after-install.sh
index 0e0be57a6d3..224904e4b32 100644
--- a/pkg/centos/after-install.sh
+++ b/pkg/centos/after-install.sh
@@ -3,3 +3,4 @@
 chown -R logstash:logstash /opt/logstash
 chown logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
+chmod 0644 /etc/logrotate.d/logstash
diff --git a/pkg/debian/after-install.sh b/pkg/debian/after-install.sh
index 69c3bbe7f40..18b4ea8c32c 100644
--- a/pkg/debian/after-install.sh
+++ b/pkg/debian/after-install.sh
@@ -4,3 +4,4 @@ chown -R logstash:logstash /opt/logstash
 chown logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
 chmod 755 /etc/logstash
+chmod 0644 /etc/logrotate.d/logstash
diff --git a/pkg/logrotate.conf b/pkg/logrotate.conf
index 69977aeecc8..0182d214474 100644
--- a/pkg/logrotate.conf
+++ b/pkg/logrotate.conf
@@ -1,4 +1,4 @@
-/var/log/logstash/*.log {
+/var/log/logstash/*.log /var/log/logstash/*.err /var/log/logstash/*.stdout {
         daily
         rotate 7
         copytruncate
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
index 4d17b27b1e7..b4adaff6ab5 100755
--- a/pkg/logstash.sysv
+++ b/pkg/logstash.sysv
@@ -44,6 +44,11 @@ LS_OPTS=""
 program=/opt/logstash/bin/logstash
 args="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
 
+quiet() {
+  "$@" > /dev/null 2>&1
+  return $?
+}
+
 start() {
 
   LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
@@ -129,6 +134,23 @@ force_stop() {
   fi
 }
 
+configtest() {
+  # Check if a config file exists
+  if [ ! "$(ls -A ${LS_CONF_DIR}/* 2> /dev/null)" ]; then
+    echo "There aren't any configuration files in ${LS_CONF_DIR}"
+    return 1
+  fi
+
+  JAVA_OPTS=${LS_JAVA_OPTS}
+  HOME=${LS_HOME}
+  export PATH HOME JAVA_OPTS LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING
+
+  test_args="-f ${LS_CONF_DIR} --configtest ${LS_OPTS}"
+  $program ${test_args}
+  [ $? -eq 0 ] && return 0
+  # Program not configured
+  return 6
+}
 
 case "$1" in
   start)
@@ -156,10 +178,20 @@ case "$1" in
     ;;
   restart)
 
+    quiet configtest
+    RET=$?
+    if [ ${RET} -ne 0 ]; then
+      echo "Configuration error. Not restarting. Re-run with configtest parameter for details"
+      exit ${RET}
+    fi
     stop && start
     ;;
+  configtest)
+    configtest
+    exit $?
+    ;;
   *)
-    echo "Usage: $SCRIPTNAME {start|stop|force-stop|status|restart}" >&2
+    echo "Usage: $SCRIPTNAME {start|stop|force-stop|status|restart|configtest}" >&2
     exit 3
   ;;
 esac
diff --git a/pkg/ubuntu/after-install.sh b/pkg/ubuntu/after-install.sh
index 5e0fc08f830..bcecadf8af7 100644
--- a/pkg/ubuntu/after-install.sh
+++ b/pkg/ubuntu/after-install.sh
@@ -3,3 +3,4 @@
 chown -R logstash:logstash /opt/logstash
 chown logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
+chmod 0644 /etc/logrotate.d/logstash
diff --git a/rakelib/default_plugins.rb b/rakelib/default_plugins.rb
index 988cc9bd791..f67b4a08609 100644
--- a/rakelib/default_plugins.rb
+++ b/rakelib/default_plugins.rb
@@ -58,6 +58,7 @@ module RakeLib
       logstash-input-http
       logstash-input-imap
       logstash-input-irc
+      logstash-input-jdbc
       logstash-input-log4j
       logstash-input-lumberjack
       logstash-input-pipe
@@ -75,10 +76,10 @@ module RakeLib
       logstash-input-xmpp
       logstash-input-zeromq
       logstash-input-kafka
+      logstash-input-beats
       logstash-output-cloudwatch
       logstash-output-csv
       logstash-output-elasticsearch
-      logstash-output-elasticsearch_http
       logstash-output-email
       logstash-output-exec
       logstash-output-file
@@ -141,7 +142,8 @@ module RakeLib
       /^logstash-input-perfmon$/,
       /^logstash-output-webhdfs$/,
       /^logstash-input-rackspace$/,
-      /^logstash-output-rackspace$/
+      /^logstash-output-rackspace$/,
+      /^logstash-input-dynamodb$/
     ])
 
 
diff --git a/rakelib/package.rake b/rakelib/package.rake
new file mode 100644
index 00000000000..96d06559844
--- /dev/null
+++ b/rakelib/package.rake
@@ -0,0 +1,13 @@
+namespace "package" do
+
+  task "bundle" do
+    system("bin/plugin", "package")
+    raise(RuntimeError, $!.to_s) unless $?.success?
+  end
+
+  desc "Build a package with the default plugins, including dependencies, to be installed offline"
+  task "plugins-default" => ["test:install-default", "bundle"]
+
+  desc "Build a package with all the plugins, including dependencies, to be installed offline"
+  task "plugins-all" => ["test:install-all", "bundle"]
+end
diff --git a/rakelib/test.rake b/rakelib/test.rake
index 1e5a3408233..7ac22c304af 100644
--- a/rakelib/test.rake
+++ b/rakelib/test.rake
@@ -26,17 +26,29 @@ namespace "test" do
 
   desc "run core specs in fail-fast mode"
   task "core-fail-fast" => ["setup"] do
-    exit(Spec::Core::Runner.run(["--fail-fast", Rake::FileList["spec/**/*_spec.rb"]]))
+    exit(RSpec::Core::Runner.run(["--fail-fast", Rake::FileList["spec/**/*_spec.rb"]]))
+  end
+
+  desc "run core specs on a single file"
+  task "core-single-file", [:specfile] => ["setup"] do |t,args|
+    exit(RSpec::Core::Runner.run([Rake::FileList[args.specfile]]))
   end
 
   desc "run all installed plugins specs"
   task "plugins" => ["setup"] do
+    plugins_to_exclude = ENV.fetch("EXCLUDE_PLUGIN", "").split(",")
     # grab all spec files using the live plugins gem specs. this allows correclty also running the specs
     # of a local plugin dir added using the Gemfile :path option. before this, any local plugin spec would
     # not be run because they were not under the vendor/bundle/jruby/1.9/gems path
     test_files = LogStash::PluginManager.find_plugins_gem_specs.map do |spec|
-      Rake::FileList[File.join(spec.gem_dir, "spec/{input,filter,codec,output}s/*_spec.rb")]
-    end.flatten
+      if plugins_to_exclude.size > 0
+        if !plugins_to_exclude.include?(Pathname.new(spec.gem_dir).basename.to_s)
+          Rake::FileList[File.join(spec.gem_dir, "spec/{input,filter,codec,output}s/*_spec.rb")]
+        end
+      else
+        Rake::FileList[File.join(spec.gem_dir, "spec/{input,filter,codec,output}s/*_spec.rb")]
+      end
+    end.flatten.compact
 
     # "--format=documentation"
     exit(RSpec::Core::Runner.run(["--order", "rand", test_files]))
@@ -75,6 +87,28 @@ namespace "test" do
     task.reenable
   end
 
+  task "integration" => ["setup"] do
+    require "fileutils" 
+
+    source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
+    integration_path = File.join(source, "integration_run")
+    FileUtils.rm_rf(integration_path)
+
+    exit(RSpec::Core::Runner.run([Rake::FileList["integration/**/*_spec.rb"]]))
+  end
+
+  namespace "integration" do
+    task "local" => ["setup"] do
+      require "fileutils"
+
+      source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
+      integration_path = File.join(source, "integration_run")
+      FileUtils.mkdir_p(integration_path)
+
+      puts "[integration_spec] configuring local environment for running test in #{integration_path}, if you want to change this behavior delete the directory."
+      exit(RSpec::Core::Runner.run([Rake::FileList["integration/**/*_spec.rb"]]))
+    end
+  end
 end
 
 task "test" => [ "test:core" ]
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
index d92644506ff..c304685e62c 100644
--- a/rakelib/vendor.rake
+++ b/rakelib/vendor.rake
@@ -1,6 +1,6 @@
 namespace "vendor" do
   VERSIONS = {
-    "jruby" => { "version" => "1.7.20", "sha1" => "3c11f01d38b9297cef2c281342f8bb799772e481" },
+    "jruby" => { "version" => "1.7.23", "sha1" => "2b5e796feeed2bcfab02f8bf2ff3d77ca318e310" },
   }
 
   def vendor(*args)
diff --git a/spec/core/config_cpu_core_strategy_spec.rb b/spec/core/config_cpu_core_strategy_spec.rb
new file mode 100644
index 00000000000..c9b69fd2657
--- /dev/null
+++ b/spec/core/config_cpu_core_strategy_spec.rb
@@ -0,0 +1,123 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/config/cpu_core_strategy"
+
+describe LogStash::Config::CpuCoreStrategy do
+
+  before do
+    allow(LogStash::Config::Defaults).to receive(:cpu_cores).and_return(cores)
+  end
+
+  context 'when the machine has 6 cores' do
+    let(:cores) { 6 }
+
+    it ".maximum should return 6" do
+      expect(described_class.maximum).to eq(6)
+    end
+
+    it ".fifty_percent should return 3" do
+      expect(described_class.fifty_percent).to eq(3)
+    end
+
+    it ".seventy_five_percent should return 4" do
+      expect(described_class.seventy_five_percent).to eq(4)
+    end
+
+    it ".twenty_five_percent should return 1" do
+      expect(described_class.twenty_five_percent).to eq(1)
+    end
+
+    it ".max_minus_one should return 5" do
+      expect(described_class.max_minus_one).to eq(5)
+    end
+
+    it ".max_minus_two should return 4" do
+      expect(described_class.max_minus_two).to eq(4)
+    end
+  end
+
+  context 'when the machine has 4 cores' do
+    let(:cores) { 4 }
+
+    it ".maximum should return 4" do
+      expect(described_class.maximum).to eq(4)
+    end
+
+    it ".fifty_percent should return 2" do
+      expect(described_class.fifty_percent).to eq(2)
+    end
+
+    it ".seventy_five_percent should return 3" do
+      expect(described_class.seventy_five_percent).to eq(3)
+    end
+
+    it ".twenty_five_percent should return 1" do
+      expect(described_class.twenty_five_percent).to eq(1)
+    end
+
+    it ".max_minus_one should return 3" do
+      expect(described_class.max_minus_one).to eq(3)
+    end
+
+    it ".max_minus_two should return 2" do
+      expect(described_class.max_minus_two).to eq(2)
+    end
+  end
+
+  context 'when the machine has 2 cores' do
+    let(:cores) { 2 }
+
+    it ".maximum should return 2" do
+      expect(described_class.maximum).to eq(2)
+    end
+
+    it ".fifty_percent should return 1" do
+      expect(described_class.fifty_percent).to eq(1)
+    end
+
+    it ".seventy_five_percent should return 1" do
+      expect(described_class.seventy_five_percent).to eq(1)
+    end
+
+    it ".twenty_five_percent should return 1" do
+      expect(described_class.twenty_five_percent).to eq(1)
+    end
+
+    it ".max_minus_one should return 1" do
+      expect(described_class.max_minus_one).to eq(1)
+    end
+
+    it ".max_minus_two should return 1" do
+      expect(described_class.max_minus_two).to eq(1)
+    end
+  end
+
+  context 'when the machine has 1 core' do
+    let(:cores) { 1 }
+
+    it ".maximum should return 1" do
+      expect(described_class.maximum).to eq(1)
+    end
+
+    it ".fifty_percent should return 1" do
+      expect(described_class.fifty_percent).to eq(1)
+    end
+
+    it ".seventy_five_percent should return 1" do
+      expect(described_class.seventy_five_percent).to eq(1)
+    end
+
+    it ".twenty_five_percent should return 1" do
+      expect(described_class.twenty_five_percent).to eq(1)
+    end
+
+    it ".max_minus_one should return 1" do
+      expect(described_class.max_minus_one).to eq(1)
+    end
+
+    it ".max_minus_two should return 1" do
+      expect(described_class.max_minus_two).to eq(1)
+    end
+  end
+
+end
diff --git a/spec/core/config_defaults_spec.rb b/spec/core/config_defaults_spec.rb
new file mode 100644
index 00000000000..6fb363f48dd
--- /dev/null
+++ b/spec/core/config_defaults_spec.rb
@@ -0,0 +1,10 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/config/defaults"
+
+describe LogStash::Config::Defaults do
+  it ".cpu_cores should return a positive integer" do
+    expect(described_class.cpu_cores.nil?).to be false
+    expect(described_class.cpu_cores.zero?).to be false
+  end
+end
diff --git a/spec/core/config_mixin_spec.rb b/spec/core/config_mixin_spec.rb
index 128f6287b5f..7c73b805d63 100644
--- a/spec/core/config_mixin_spec.rb
+++ b/spec/core/config_mixin_spec.rb
@@ -97,4 +97,58 @@
       expect(clone.password.value).to(be == secret)
     end
   end
+
+  describe "obsolete settings" do
+    let(:plugin_class) do
+      Class.new(LogStash::Inputs::Base) do
+        include LogStash::Config::Mixin
+        config_name "example"
+        config :foo, :validate => :string, :obsolete => "This feature was removed."
+      end
+    end
+
+    context "when using an obsolete setting" do
+      it "should cause a configuration error" do
+        expect {
+          plugin_class.new("foo" => "hello")
+        }.to raise_error(LogStash::ConfigurationError)
+      end
+    end
+
+    context "when using an obsolete settings from the parent class" do
+      it "should cause a configuration error" do
+        expect {
+          plugin_class.new("debug" => true)
+        }.to raise_error(LogStash::ConfigurationError)
+      end
+    end
+
+    context "when not using an obsolete setting" do
+      it "should not cause a configuration error" do
+        expect {
+          plugin_class.new({})
+        }.not_to raise_error
+      end
+    end
+  end
+
+  context "#params" do
+    let(:plugin_class) do
+      Class.new(LogStash::Filters::Base)  do
+        config_name "fake"
+        config :password, :validate => :password
+        config :bad, :validate => :string, :default => "my default", :obsolete => "not here"
+      end
+    end
+
+    subject { plugin_class.new({ "password" => "secret" }) }
+
+    it "should not return the obsolete options" do
+      expect(subject.params).not_to include("bad")
+    end
+
+    it "should include any other params" do
+      expect(subject.params).to include("password")
+    end
+  end
 end
diff --git a/spec/core/event_spec.rb b/spec/core/event_spec.rb
index 798cab1bb04..bf7471e6109 100644
--- a/spec/core/event_spec.rb
+++ b/spec/core/event_spec.rb
@@ -50,6 +50,10 @@
         expect(subject.sprintf("%{+%s}")).to eq("1356998400")
       end
 
+      it "should work if there is no fieldref in the string" do
+        expect(subject.sprintf("bonjour")).to eq("bonjour")
+      end
+
       it "should raise error when formatting %{+%s} when @timestamp field is missing" do
         str = "hello-%{+%s}"
         subj = subject.clone
@@ -100,6 +104,10 @@
         expect(subject.sprintf("%{[j][k3]}")).to eq("{\"4\":\"m\"}")
       end
 
+      it "should not strip last character" do
+        expect(subject.sprintf("%{type}%{message}|")).to eq("sprintfhello world|")
+      end
+
       context "#encoding" do
         it "should return known patterns as UTF-8" do
           expect(subject.sprintf("%{message}").encoding).to eq(Encoding::UTF_8)
@@ -488,4 +496,23 @@
       subject{LogStash::Event.new(LogStash::Json.load(LogStash::Json.dump(event_hash)))}
     end
   end
+
+
+  describe "#to_s" do
+    let(:timestamp) { LogStash::Timestamp.new }
+    let(:event1) { LogStash::Event.new({ "@timestamp" => timestamp, "host" => "foo", "message" => "bar"}) }
+    let(:event2) { LogStash::Event.new({ "host" => "bar", "message" => "foo"}) }
+
+    it "should cache only one template" do
+      LogStash::StringInterpolation::CACHE.clear
+      expect {
+        event1.to_s
+        event2.to_s
+      }.to change { LogStash::StringInterpolation::CACHE.size }.by(1)
+    end
+
+    it "return the string containing the timestamp, the host and the message" do
+      expect(event1.to_s).to eq("#{timestamp.to_iso8601} #{event1["host"]} #{event1["message"]}")
+    end
+  end
 end
diff --git a/spec/core/pipeline_spec.rb b/spec/core/pipeline_spec.rb
index 89ad07ef4bd..35f7acf0592 100644
--- a/spec/core/pipeline_spec.rb
+++ b/spec/core/pipeline_spec.rb
@@ -11,7 +11,7 @@ def register
   def run(queue)
   end
 
-  def teardown
+  def close
   end
 end
 
@@ -27,7 +27,7 @@ def encode(event)
     event
   end
 
-  def teardown
+  def close
   end
 end
 
@@ -35,11 +35,11 @@ class DummyOutput < LogStash::Outputs::Base
   config_name "dummyoutput"
   milestone 2
 
-  attr_reader :num_teardowns
+  attr_reader :num_closes
 
   def initialize(params={})
     super
-    @num_teardowns = 0
+    @num_closes = 0
   end
 
   def register
@@ -48,25 +48,132 @@ def register
   def receive(event)
   end
 
-  def teardown
-    @num_teardowns += 1
+  def close
+    @num_closes += 1
   end
 end
 
+class DummyFilter < LogStash::Filters::Base
+  config_name "dummyfilter"
+  milestone 2
+
+  def register() end
+
+  def filter(event) end
+
+  def threadsafe?() false; end
+
+  def close() end
+end
+
+class DummySafeFilter < LogStash::Filters::Base
+  config_name "dummysafefilter"
+  milestone 2
+
+  def register() end
+
+  def filter(event) end
+
+  def threadsafe?() true; end
+
+  def close() end
+end
+
 class TestPipeline < LogStash::Pipeline
-  attr_reader :outputs
+  attr_reader :outputs, :filter_threads, :settings, :logger
 end
 
 describe LogStash::Pipeline do
+  let(:worker_thread_count)     { 8 }
+  let(:safe_thread_count)       { 1 }
+  let(:override_thread_count)   { 42 }
+
+  describe "defaulting the filter workers based on thread safety" do
+    before(:each) do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummysafefilter").and_return(DummySafeFilter)
+      allow(LogStash::Config::CpuCoreStrategy).to receive(:fifty_percent).and_return(worker_thread_count)
+    end
 
-context "teardown" do
+    context "when there are some not threadsafe filters" do
+      let(:test_config_with_filters) {
+        <<-eos
+        input {
+          dummyinput {}
+        }
 
-  before(:each) do
-    allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
-    allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+        filter {
+          dummyfilter {}
+        }
+
+        output {
+          dummyoutput {}
+        }
+        eos
+      }
+
+      context "when there is no command line -w N set" do
+        it "starts one filter thread" do
+          msg = "Defaulting filter worker threads to 1 because there are some" +
+                " filters that might not work with multiple worker threads"
+          pipeline = TestPipeline.new(test_config_with_filters)
+          expect(pipeline.logger).to receive(:warn).with(msg,
+            {:count_was=>worker_thread_count, :filters=>["dummyfilter"]})
+          pipeline.run
+          expect(pipeline.filter_threads.size).to eq(safe_thread_count)
+        end
+      end
+
+      context "when there is command line -w N set" do
+        it "starts multiple filter thread" do
+          msg = "Warning: Manual override - there are filters that might" +
+                " not work with multiple worker threads"
+          pipeline = TestPipeline.new(test_config_with_filters)
+          expect(pipeline.logger).to receive(:warn).with(msg,
+            {:worker_threads=> override_thread_count, :filters=>["dummyfilter"]})
+          pipeline.configure("filter-workers", override_thread_count)
+          pipeline.run
+          expect(pipeline.filter_threads.size).to eq(override_thread_count)
+        end
+      end
+    end
+
+    context "when there are threadsafe filters only" do
+      let(:test_config_with_filters) {
+        <<-eos
+        input {
+          dummyinput {}
+        }
+
+        filter {
+          dummysafefilter {}
+        }
+
+        output {
+          dummyoutput {}
+        }
+        eos
+      }
+
+      it "starts multiple filter threads" do
+        pipeline = TestPipeline.new(test_config_with_filters)
+        pipeline.run
+        expect(pipeline.filter_threads.size).to eq(worker_thread_count)
+      end
+    end
   end
 
+  context "close" do
+    before(:each) do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    end
+
+
     let(:test_config_without_output_workers) {
       <<-eos
       input {
@@ -93,24 +200,24 @@ class TestPipeline < LogStash::Pipeline
       eos
     }
 
-    context "output teardown" do
-      it "should call teardown of output without output-workers" do
+    context "output close" do
+      it "should call close of output without output-workers" do
         pipeline = TestPipeline.new(test_config_without_output_workers)
         pipeline.run
 
         expect(pipeline.outputs.size ).to eq(1)
         expect(pipeline.outputs.first.worker_plugins.size ).to eq(1)
-        expect(pipeline.outputs.first.worker_plugins.first.num_teardowns ).to eq(1)
+        expect(pipeline.outputs.first.worker_plugins.first.num_closes ).to eq(1)
       end
 
-      it "should call output teardown correctly with output workers" do
+      it "should call output close correctly with output workers" do
         pipeline = TestPipeline.new(test_config_with_output_workers)
         pipeline.run
 
         expect(pipeline.outputs.size ).to eq(1)
-        expect(pipeline.outputs.first.num_teardowns).to eq(0)
+        expect(pipeline.outputs.first.num_closes).to eq(0)
         pipeline.outputs.first.worker_plugins.each do |plugin|
-          expect(plugin.num_teardowns ).to eq(1)
+          expect(plugin.num_closes ).to eq(1)
         end
       end
     end
@@ -191,6 +298,28 @@ class TestPipeline < LogStash::Pipeline
         expect(subject[2]["foo"]).to eq("bar")
       end
     end
+  end
+
+  describe "stalling_threads" do
+    before(:each) do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    end
+
+    context "when the pipeline doesn't have filters" do
+      let(:pipeline_with_no_filters) do
+        <<-eos
+        input { dummyinput {} }
+        output { dummyoutput {} }
+        eos
+      end
 
+      it "doesn't raise an error" do
+        pipeline = TestPipeline.new(pipeline_with_no_filters)
+        pipeline.run
+        expect { pipeline.stalling_threads }.to_not raise_error
+      end
+    end
   end
 end
diff --git a/spec/core/shutdown_controller_spec.rb b/spec/core/shutdown_controller_spec.rb
new file mode 100644
index 00000000000..5f755f290a8
--- /dev/null
+++ b/spec/core/shutdown_controller_spec.rb
@@ -0,0 +1,107 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/shutdown_controller"
+
+describe LogStash::ShutdownController do
+
+  let(:check_every) { 0.01 }
+  let(:check_threshold) { 100 }
+  subject { LogStash::ShutdownController.new(pipeline, check_every) }
+  let(:pipeline) { double("pipeline") }
+  report_count = 0
+
+  before :each do
+    allow(LogStash::Report).to receive(:from_pipeline).and_wrap_original do |m, *args|
+      report_count += 1
+      m.call(*args)
+    end
+  end
+
+  after :each do
+    report_count = 0
+  end
+
+  context "when pipeline is stalled" do
+    let(:increasing_count) { (1..5000).to_a.map {|i| { "total" => i } } }
+    before :each do
+      allow(pipeline).to receive(:inflight_count).and_return(*increasing_count)
+      allow(pipeline).to receive(:stalling_threads) { { } }
+    end
+
+    describe ".unsafe_shutdown = true" do
+      let(:abort_threshold) { subject.abort_threshold }
+      let(:report_every) { subject.report_every }
+
+      before :each do
+        subject.class.unsafe_shutdown = true
+      end
+
+      it "should force the shutdown" do
+        expect(subject).to receive(:force_exit).once
+        subject.start
+      end
+
+      it "should do exactly \"abort_threshold\" stall checks" do
+        allow(subject).to receive(:force_exit)
+        expect(subject).to receive(:shutdown_stalled?).exactly(abort_threshold).times.and_call_original
+        subject.start
+      end
+
+      it "should do exactly \"abort_threshold\"*\"report_every\" stall checks" do
+        allow(subject).to receive(:force_exit)
+        expect(LogStash::Report).to receive(:from_pipeline).exactly(abort_threshold*report_every).times.and_call_original
+        subject.start
+      end
+    end
+
+    describe ".unsafe_shutdown = false" do
+
+      before :each do
+        subject.class.unsafe_shutdown = false
+      end
+
+      it "shouldn't force the shutdown" do
+        expect(subject).to_not receive(:force_exit)
+        thread = Thread.new(subject) {|subject| subject.start }
+        sleep 0.1 until report_count > check_threshold
+        thread.kill
+      end
+    end
+  end
+
+  context "when pipeline is not stalled" do
+    let(:decreasing_count) { (1..5000).to_a.reverse.map {|i| { "total" => i } } }
+    before :each do
+      allow(pipeline).to receive(:inflight_count).and_return(*decreasing_count)
+      allow(pipeline).to receive(:stalling_threads) { { } }
+    end
+
+    describe ".unsafe_shutdown = true" do
+
+      before :each do
+        subject.class.unsafe_shutdown = true
+      end
+
+      it "should force the shutdown" do
+        expect(subject).to_not receive(:force_exit)
+        thread = Thread.new(subject) {|subject| subject.start }
+        sleep 0.1 until report_count > check_threshold
+        thread.kill
+      end
+    end
+
+    describe ".unsafe_shutdown = false" do
+
+      before :each do
+        subject.class.unsafe_shutdown = false
+      end
+
+      it "shouldn't force the shutdown" do
+        expect(subject).to_not receive(:force_exit)
+        thread = Thread.new(subject) {|subject| subject.start }
+        sleep 0.1 until report_count > check_threshold
+        thread.kill
+      end
+    end
+  end
+end
diff --git a/spec/filters/base_spec.rb b/spec/filters/base_spec.rb
index 321b72965d1..177c44dcb8c 100644
--- a/spec/filters/base_spec.rb
+++ b/spec/filters/base_spec.rb
@@ -24,7 +24,7 @@ def filter(event)
   end
 
   it "should provide class public API" do
-    [:register, :filter, :multi_filter, :execute, :threadsafe?, :filter_matched, :filter?, :teardown].each do |method|
+    [:register, :filter, :multi_filter, :execute, :threadsafe?, :filter_matched, :filter?, :close].each do |method|
       expect(subject).to respond_to(method)
     end
   end
@@ -70,7 +70,6 @@ def filter(event)
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
         add_tag => ["test"]
       }
     }
@@ -79,25 +78,19 @@ def filter(event)
     sample("type" => "noop") do
       insist { subject["tags"] } == ["test"]
     end
-
-    sample("type" => "not_noop") do
-      insist { subject["tags"] }.nil?
-    end
   end
 
   describe "tags parsing with one tag" do
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
-        tags => ["t1"]
         add_tag => ["test"]
       }
     }
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] }.nil?
+      insist { subject["tags"] } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
@@ -109,19 +102,17 @@ def filter(event)
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
-        tags => ["t1", "t2"]
         add_tag => ["test"]
       }
     }
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] }.nil?
+      insist { subject["tags"] } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1"]) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject["tags"] } == ["t1", "test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
@@ -133,62 +124,10 @@ def filter(event)
     end
   end
 
-  describe "exclude_tags with 1 tag" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1"]
-        add_tag => ["test"]
-        exclude_tags => ["t2"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop") do
-      insist { subject["tags"] }.nil?
-    end
-
-    sample("type" => "noop", "tags" => ["t1"]) do
-      insist { subject["tags"] } == ["t1", "test"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2"]
-    end
-  end
-
-  describe "exclude_tags with >1 tags" do
-    config <<-CONFIG
-    filter {
-      noop {
-        type => "noop"
-        tags => ["t1"]
-        add_tag => ["test"]
-        exclude_tags => ["t2", "t3"]
-      }
-    }
-    CONFIG
-
-    sample("type" => "noop", "tags" => ["t1", "t2", "t4"]) do
-      insist { subject["tags"] } == ["t1", "t2", "t4"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t3", "t4"]) do
-      insist { subject["tags"] } == ["t1", "t3", "t4"]
-    end
-
-    sample("type" => "noop", "tags" => ["t1", "t4", "t5"]) do
-      insist { subject["tags"] } == ["t1", "t4", "t5", "test"]
-    end
-  end
-
   describe "remove_tag" do
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
-        tags => ["t1"]
         remove_tag => ["t2", "t3"]
       }
     }
@@ -223,8 +162,6 @@ def filter(event)
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
-        tags => ["t1"]
         remove_tag => ["%{blackhole}"]
       }
     }
@@ -245,7 +182,6 @@ def filter(event)
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
         remove_field => ["t2", "t3"]
       }
     }
@@ -271,7 +207,6 @@ def filter(event)
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
         remove_field => ["[t1][t2]"]
       }
     }
@@ -288,7 +223,6 @@ def filter(event)
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
         remove_field => ["[t1][0]"]
       }
     }
@@ -304,7 +238,6 @@ def filter(event)
     config <<-CONFIG
     filter {
       noop {
-        type => "noop"
         remove_field => ["%{blackhole}"]
       }
     }
diff --git a/spec/lib/logstash/java_integration_spec.rb b/spec/lib/logstash/java_integration_spec.rb
index a86ce6b382d..e4b36a2bd68 100644
--- a/spec/lib/logstash/java_integration_spec.rb
+++ b/spec/lib/logstash/java_integration_spec.rb
@@ -211,6 +211,36 @@
         end
       end
     end
+
+    context "when compacting" do
+      context "#compact with nils" do
+        let(:initial_array) { [1,2,3,nil,nil,6] }
+        it "should remove nil values from a copy" do
+          expect(subject.compact).to eq([1,2,3,6])
+          expect(subject).to eq([1,2,3,nil,nil,6])
+        end
+      end
+
+      context "#compact! with nils" do
+        let(:initial_array) { [1,2,3,nil,nil,6] }
+        it "should remove nil values" do
+          expect(subject.compact!).to eq([1,2,3,6])
+          expect(subject).to eq([1,2,3,6])
+        end
+
+        it "should return the original" do
+          expect(subject.compact!.object_id).to eq(subject.object_id)
+        end
+      end
+
+      context "#compact! without nils" do
+        let(:initial_array) { [1,2,3,6] }
+        it "should return nil" do
+          expect(subject.compact!).to be nil
+          expect(subject).to eq([1,2,3,6])
+        end
+      end
+    end
   end
 
   context "Enumerable implementation" do
diff --git a/spec/license_spec.rb b/spec/license_spec.rb
index 6e8da1a7693..f37f29d0431 100644
--- a/spec/license_spec.rb
+++ b/spec/license_spec.rb
@@ -12,10 +12,23 @@
     Regexp.union([ /mit/,
                    /apache*/,
                    /bsd/,
+                   /artistic 2.*/,
                    /ruby/,
                    /lgpl/])
   }
 
+  ##
+  # This licenses are skipped from the license test of many reasons, check
+  # the exact dependency for detailed information.
+  ##
+  let(:skipped_dependencies) do
+    [
+      # Skipped because of already included and bundled within JRuby so checking here is redundant.
+      # Need to take action about jruby licenses to enable again or keep skeeping.
+      "jruby-openssl"
+    ]
+  end
+
   shared_examples "runtime license test" do
 
     subject(:gem_name) do |example|
@@ -33,6 +46,7 @@
     it "has runtime dependencies with expected licenses" do
       spec.runtime_dependencies.map { |dep| dep.to_spec }.each do |runtime_spec|
         next unless runtime_spec
+        next if skipped_dependencies.include?(runtime_spec.name)
         runtime_spec.licenses.each do |license|
           expect(license.downcase).to match(expected_licenses)
         end
diff --git a/spec/outputs/base_spec.rb b/spec/outputs/base_spec.rb
index 2702d9603ce..841ba424df9 100644
--- a/spec/outputs/base_spec.rb
+++ b/spec/outputs/base_spec.rb
@@ -24,24 +24,3 @@ def receive(event)
     output.worker_setup
   end
 end
-
-describe "LogStash::Outputs::Base#output?" do
-  it "should filter by type" do
-    output = LogStash::Outputs::NOOP.new("type" => "noop")
-    expect(output.receive(LogStash::Event.new({"type" => "noop"}))).to eq(true)
-    expect(output.receive(LogStash::Event.new({"type" => "not_noop"}))).to eq(false)
-  end
-  
-  it "should filter by tags" do
-    output = LogStash::Outputs::NOOP.new("tags" => ["value", "value2"])
-    expect(output.receive(LogStash::Event.new({"tags" => ["value","value2"]}))).to eq(true)
-    expect(output.receive(LogStash::Event.new({"tags" => ["notvalue"]}))).to eq(false)
-    expect(output.receive(LogStash::Event.new({"tags" => ["value"]}))).to eq(false)
-  end
-
-  it "should exclude by tags" do
-    output = LogStash::Outputs::NOOP.new("exclude_tags" => ["value"])
-    expect(output.receive(LogStash::Event.new({"tags" => ["value"]}))).to eq(false)
-    expect(output.receive(LogStash::Event.new({"tags" => ["notvalue"]}))).to eq(true)
-  end
-end
diff --git a/spec/plugin_manager/install_spec.rb b/spec/plugin_manager/install_spec.rb
new file mode 100644
index 00000000000..40eb3dfe408
--- /dev/null
+++ b/spec/plugin_manager/install_spec.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require 'spec_helper'
+require 'pluginmanager/main'
+
+describe LogStash::PluginManager::Install do
+  let(:cmd) { LogStash::PluginManager::Install.new("install") }
+
+  before(:each) do
+    expect(cmd).to receive(:validate_cli_options!).and_return(nil)
+  end
+
+  context "when validating plugins" do
+    let(:sources) { ["https://rubygems.org", "http://localhost:9292"] }
+
+    before(:each) do
+      expect(cmd).to receive(:plugins_gems).and_return([["dummy", nil]])
+      expect(cmd).to receive(:install_gems_list!).and_return(nil)
+      expect(cmd).to receive(:remove_unused_locally_installed_gems!).and_return(nil)
+      cmd.verify = true
+    end
+
+    it "should load all the sources defined in the Gemfile" do
+      expect(cmd.gemfile.gemset).to receive(:sources).and_return(sources)
+      expect(LogStash::PluginManager).to receive(:logstash_plugin?).with("dummy", nil, {:rubygems_source => sources}).and_return(true)
+      cmd.execute
+    end
+  end
+end
diff --git a/spec/plugin_manager/update_spec.rb b/spec/plugin_manager/update_spec.rb
new file mode 100644
index 00000000000..5498f9dea0c
--- /dev/null
+++ b/spec/plugin_manager/update_spec.rb
@@ -0,0 +1,39 @@
+# encoding: utf-8
+require 'spec_helper'
+require 'pluginmanager/main'
+
+describe LogStash::PluginManager::Update do
+  let(:cmd)     { LogStash::PluginManager::Update.new("update") }
+  let(:sources) { cmd.gemfile.gemset.sources }
+
+  before(:each) do
+    expect(cmd).to receive(:find_latest_gem_specs).and_return({})
+    allow(cmd).to receive(:warn_local_gems).and_return(nil)
+    expect(cmd).to receive(:display_updated_plugins).and_return(nil)
+    expect_any_instance_of(LogStash::Bundler).to receive(:invoke!).with(:clean => true)
+  end
+
+  it "pass all gem sources to the bundle update command" do
+    sources = cmd.gemfile.gemset.sources
+    expect_any_instance_of(LogStash::Bundler).to receive(:invoke!).with(:update => [], :rubygems_source => sources)
+    cmd.execute
+  end
+
+  context "when skipping validation" do
+    let(:cmd)    { LogStash::PluginManager::Update.new("update") }
+    let(:plugin) { OpenStruct.new(:name => "dummy", :options => {} ) }
+
+    before(:each) do
+      expect(cmd.gemfile).to receive(:find).with(plugin).and_return(plugin)
+      expect(cmd.gemfile).to receive(:save).and_return(nil)
+      expect(cmd).to receive(:plugins_to_update).and_return([plugin])
+      expect_any_instance_of(LogStash::Bundler).to receive(:invoke!).with(:update => [plugin], :rubygems_source => sources).and_return(nil)
+    end
+
+    it "skips version verification when ask for it" do
+      cmd.verify = false
+      expect(cmd).to_not receive(:validates_version)
+      cmd.execute
+    end
+  end
+end
diff --git a/spec/plugin_manager/util_spec.rb b/spec/plugin_manager/util_spec.rb
new file mode 100644
index 00000000000..10824e56adc
--- /dev/null
+++ b/spec/plugin_manager/util_spec.rb
@@ -0,0 +1,71 @@
+#encoding: utf-8
+require 'spec_helper'
+require 'pluginmanager/util'
+require 'gems'
+
+describe LogStash::PluginManager do
+
+  describe "fetching plugin information" do
+    let(:plugin_name) { "logstash-output-elasticsearch" }
+
+    let(:version_data) do
+      [ { "authors"=>"Elastic", "built_at"=>"2015-08-11T00:00:00.000Z", "description"=>"Output events to elasticsearch",
+          "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"2.0.0.pre",
+          "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>true,
+          "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"},
+      { "authors"=>"Elastic", "built_at"=>"2015-08-10T00:00:00.000Z", "description"=>"Output events to elasticsearch",
+        "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"1.0.7",
+        "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>false,
+        "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"},
+      { "authors"=>"Elastic", "built_at"=>"2015-08-09T00:00:00.000Z", "description"=>"Output events to elasticsearch",
+        "downloads_count"=>1638, "metadata"=>{"logstash_group"=>"output", "logstash_plugin"=>"true"}, "number"=>"1.0.4",
+        "summary"=>"Logstash Output to Elasticsearch", "platform"=>"java", "ruby_version"=>">= 0", "prerelease"=>false,
+        "licenses"=>["apache-2.0"], "requirements"=>[], "sha"=>"194b27099c13605a882a3669e2363fdecccaab1de48dd44b0cda648dd5516799"} ]
+    end
+
+    before(:each) do
+      allow(Gems).to receive(:versions).with(plugin_name).and_return(version_data)
+    end
+
+    context "fetch plugin info" do
+      it "should search for the last version infomation non prerelease" do
+        version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name)
+        expect(version_info["number"]).to eq("1.0.7")
+      end
+
+
+      it "should search for the last version infomation with prerelease" do
+        version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name, :pre => true)
+        expect(version_info["number"]).to eq("2.0.0.pre")
+      end
+    end
+  end
+
+  describe "a logstash_plugin validation" do
+    let(:plugin)  { "foo" }
+    let(:version) { "9.0.0.0" }
+
+    let(:sources) { ["http://source.01", "http://source.02"] }
+    let(:options) { {:rubygems_source => sources} }
+
+    let(:gemset)  { double("gemset") }
+    let(:gemfile) { double("gemfile") }
+    let(:dep)     { double("dep") }
+    let(:fetcher) { double("fetcher") }
+
+    before(:each) do
+      allow(gemfile).to  receive(:gemset).and_return(gemset)
+      allow(gemset).to   receive(:sources).and_return(sources)
+      expect(fetcher).to receive(:spec_for_dependency).and_return([[],[]])
+    end
+
+    it "should load all available sources" do
+      expect(subject).to receive(:plugin_file?).and_return(false)
+      expect(Gem::Dependency).to receive(:new).and_return(dep)
+      expect(Gem::SpecFetcher).to receive(:fetcher).and_return(fetcher)
+
+      subject.logstash_plugin?(plugin, version, options)
+      expect(Gem.sources.map { |source| source }).to eq(sources)
+    end
+  end
+end
diff --git a/spec/util/buftok_spec.rb b/spec/util/buftok_spec.rb
new file mode 100644
index 00000000000..8c6a06628d3
--- /dev/null
+++ b/spec/util/buftok_spec.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/util/buftok"
+
+describe  FileWatch::BufferedTokenizer  do
+
+  subject { FileWatch::BufferedTokenizer.new }
+
+  it "should tokenize a single token" do
+    expect(subject.extract("foo\n")).to eq(["foo"])
+  end
+
+  it "should merge multiple token" do
+    expect(subject.extract("foo")).to eq([])
+    expect(subject.extract("bar\n")).to eq(["foobar"])
+  end
+
+  it "should tokenize multiple token" do
+    expect(subject.extract("foo\nbar\n")).to eq(["foo", "bar"])
+  end
+
+  it "should ignore empty payload" do
+    expect(subject.extract("")).to eq([])
+    expect(subject.extract("foo\nbar")).to eq(["foo"])
+  end
+
+  it "should tokenize empty payload with newline" do
+    expect(subject.extract("\n")).to eq([""])
+    expect(subject.extract("\n\n\n")).to eq(["", "", ""])
+  end
+end
diff --git a/spec/util/compress_spec.rb b/spec/util/compress_spec.rb
new file mode 100644
index 00000000000..47bab9e995a
--- /dev/null
+++ b/spec/util/compress_spec.rb
@@ -0,0 +1,121 @@
+# encoding: utf-8
+require "spec_helper"
+require 'ostruct'
+require "bootstrap/util/compress"
+
+describe LogStash::Util::Zip do
+
+  subject { Class.new { extend LogStash::Util::Zip } }
+
+  context "#extraction" do
+
+    let(:source) { File.join(File.expand_path("."), "source_file.zip") }
+    let(:target) { File.expand_path("target_dir") }
+
+    it "raise an exception if the target dir exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.extract(source, target) }.to raise_error
+    end
+
+    let(:zip_file) do
+      [ "foo", "bar", "zoo" ].inject([]) do |acc, name|
+        acc << OpenStruct.new(:name => name)
+        acc
+      end
+    end
+
+    it "extract the list of entries from a zip file" do
+      allow(Zip::File).to receive(:open).with(source).and_yield(zip_file)
+      expect(FileUtils).to receive(:mkdir_p).exactly(3).times
+      expect(zip_file).to receive(:extract).exactly(3).times
+      subject.extract(source, target)
+    end
+  end
+
+  context "#compression" do
+
+    let(:target) { File.join(File.expand_path("."), "target_file.zip") }
+    let(:source) { File.expand_path("source_dir") }
+
+    it "raise an exception if the target file exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.compress(source, target) }.to raise_error
+    end
+
+    let(:dir_files) do
+      [ "foo", "bar", "zoo" ]
+    end
+
+    let(:zip_file) { Class.new }
+
+    it "add a dir to a zip file" do
+      allow(Zip::File).to receive(:open).with(target, ::Zip::File::CREATE).and_yield(zip_file)
+      allow(Dir).to receive(:glob).and_return(dir_files)
+      expect(zip_file).to receive(:add).exactly(3).times
+      subject.compress(source, target)
+    end
+  end
+end
+
+describe LogStash::Util::Tar do
+
+  subject { Class.new { extend LogStash::Util::Tar } }
+
+  context "#extraction" do
+
+    let(:source) { File.join(File.expand_path("."), "source_file.tar.gz") }
+    let(:target) { File.expand_path("target_dir") }
+
+    it "raise an exception if the target dir exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.extract(source, target) }.to raise_error
+    end
+
+    let(:gzip_file) { Class.new }
+
+    let(:tar_file) do
+      [ "foo", "bar", "zoo" ].inject([]) do |acc, name|
+        acc << OpenStruct.new(:full_name => name)
+        acc
+      end
+    end
+
+    it "extract the list of entries from a tar.gz file" do
+      allow(Zlib::GzipReader).to receive(:open).with(source).and_yield(gzip_file)
+      allow(Gem::Package::TarReader).to receive(:new).with(gzip_file).and_yield(tar_file)
+
+      expect(FileUtils).to receive(:mkdir).with(target)
+      expect(File).to receive(:open).exactly(3).times
+      subject.extract(source, target)
+    end
+  end
+
+  context "#compression" do
+
+    let(:target) { File.join(File.expand_path("."), "target_file.tar.gz") }
+    let(:source) { File.expand_path("source_dir") }
+
+    it "raise an exception if the target file exist" do
+      allow(File).to receive(:exist?).with(target).and_return(true)
+      expect { subject.compress(source, target) }.to raise_error
+    end
+
+    let(:dir_files) do
+      [ "foo", "bar", "zoo" ]
+    end
+
+    let(:tar_file) { Class.new }
+    let(:tar)      { Class.new }
+
+    it "add a dir to a tgz file" do
+      allow(Stud::Temporary).to receive(:file).and_yield(tar_file)
+      allow(Gem::Package::TarWriter).to receive(:new).with(tar_file).and_yield(tar)
+      allow(Dir).to receive(:glob).and_return(dir_files)
+      expect(File).to receive(:stat).exactly(3).times.and_return(OpenStruct.new(:mode => "rw"))
+      expect(tar).to receive(:add_file).exactly(3).times
+      expect(tar_file).to receive(:rewind)
+      expect(subject).to receive(:gzip).with(target, tar_file)
+      subject.compress(source, target)
+    end
+  end
+end
diff --git a/spec/util/defaults_printer_spec.rb b/spec/util/defaults_printer_spec.rb
new file mode 100644
index 00000000000..ed47cf7ca50
--- /dev/null
+++ b/spec/util/defaults_printer_spec.rb
@@ -0,0 +1,50 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/util/defaults_printer"
+
+describe LogStash::Util::DefaultsPrinter do
+  shared_examples "a defaults printer" do
+    it 'the .print method returns a defaults description' do
+      expect(actual_block.call).to eq(expected)
+    end
+  end
+
+  let(:workers)  { 1 }
+  let(:expected) { "Settings: User set filter workers: #{workers}" }
+  let(:settings) { {} }
+
+  describe 'class methods API' do
+    let(:actual_block) do
+      -> {described_class.print(settings)}
+    end
+
+    context 'when the settings hash is empty' do
+      let(:expected) { "Settings: " }
+      it_behaves_like "a defaults printer"
+    end
+
+    context 'when the settings hash has content' do
+      let(:workers) { 42 }
+      let(:settings) { {'filter-workers' => workers} }
+      it_behaves_like "a defaults printer"
+    end
+  end
+
+  describe 'instance method API' do
+    let(:actual_block) do
+      -> {described_class.new(settings).print}
+    end
+
+    context 'when the settings hash is empty' do
+      let(:expected) { "Settings: " }
+      it_behaves_like "a defaults printer"
+    end
+
+    context 'when the settings hash has content' do
+      let(:workers) { 13 }
+      let(:settings) { {'filter-workers' => workers} }
+
+      it_behaves_like "a defaults printer"
+    end
+  end
+end
diff --git a/spec/util/retryable_spec.rb b/spec/util/retryable_spec.rb
new file mode 100644
index 00000000000..518b64400ea
--- /dev/null
+++ b/spec/util/retryable_spec.rb
@@ -0,0 +1,139 @@
+require "logstash/util/retryable"
+
+describe LogStash::Retryable do
+  class C
+    include LogStash::Retryable
+  end
+
+  class E < StandardError; end;
+  class F < StandardError; end;
+
+  subject {C.new}
+
+  context "with default fixed 1 second retry sleep" do
+
+    it "should execute once" do
+      expect(subject).to receive(:sleep).never
+      expect(subject.retryable(:rescue => nil){|i| expect(i).to eq(0); "foo"}).to eq("foo")
+    end
+
+    it "should not retry on non rescued exceptions" do
+      i = 0
+      expect(subject).to receive(:sleep).never
+      expect{subject.retryable(:rescue => E){i += 1; raise F}}.to raise_error(F)
+      expect(i).to eq(1)
+    end
+
+    it "should execute once and retry once by default" do
+      i = 0
+      expect(subject).to receive(:sleep).once.with(1)
+      expect{subject.retryable{i += 1; raise E}}.to raise_error(E)
+      expect(i).to eq(2)
+    end
+
+    it "should retry on rescued exceptions" do
+      i = 0
+      expect(subject).to receive(:sleep).once.with(1)
+      expect{subject.retryable(:rescue => E){i += 1; raise E}}.to raise_error(E)
+      expect(i).to eq(2)
+    end
+
+    it "should retry indefinitely" do
+      i = 0
+      expect(subject).to receive(:sleep).exactly(50).times.with(1)
+      expect{subject.retryable(:tries => 0, :rescue => E){i += 1; raise i <= 50 ? E : F}}.to raise_error(F)
+    end
+
+    it "should execute once and retry once by default and execute on_retry callback" do
+      i = 0
+      callback_values = []
+
+      callback = lambda do |retry_count, e|
+        callback_values << [retry_count, e]
+      end
+
+      expect(subject).to receive(:sleep).once.with(1)
+
+      expect do
+        subject.retryable(:on_retry => callback){i += 1; raise E}
+      end.to raise_error
+
+      expect(i).to eq(2)
+
+      expect(callback_values.size).to eq(1)
+      expect(callback_values[0][0]).to eq(1)
+      expect(callback_values[0][1]).to be_a(E)
+    end
+
+    it "should execute once and retry n times" do
+      i = 0
+      n = 3
+      expect(subject).to receive(:sleep).exactly(n).times.with(1)
+      expect{subject.retryable(:tries => n){i += 1; raise E}}.to raise_error(E)
+      expect(i).to eq(n + 1)
+    end
+
+    it "should execute once and retry n times and execute on_retry callback" do
+      i = 0
+      n = 3
+      callback_values = []
+
+      callback = lambda do |retry_count, e|
+        callback_values << [retry_count, e]
+      end
+
+      expect(subject).to receive(:sleep).exactly(n).times.with(1)
+
+      expect do
+        subject.retryable(:tries => n, :on_retry => callback){i += 1; raise E}
+      end.to raise_error
+
+      expect(i).to eq(n + 1)
+
+      expect(callback_values.size).to eq(n)
+      n.times.each do |j|
+        expect(callback_values[j].first).to eq(j + 1)
+        expect(callback_values[j].last).to be_a(E)
+      end
+    end
+  end
+
+  context "with exponential backoff" do
+
+    it "should execute once and retry once with base sleep by default" do
+      expect(subject).to receive(:sleep).once.with(2)
+      expect do
+        subject.retryable(:base_sleep => 2, :max_sleep => 10){raise E}
+      end.to raise_error(E)
+    end
+
+    it "should execute once and retry n times with exponential backoff sleep" do
+      n = 3
+      s = 0.5
+
+      n.times.each do |i|
+        expect(subject).to receive(:sleep).once.with(s * (2 ** i)).ordered
+      end
+      expect do
+        subject.retryable(:tries => n, :base_sleep => s, :max_sleep => 100){raise E}
+      end.to raise_error(E)
+    end
+
+    it "should execute once and retry n times with exponential backoff sleep capping at max_sleep" do
+      n = 20
+      base_sleep = 0.1
+      max_sleep = 1
+
+      expect(subject).to receive(:sleep).once.with(0.1).ordered
+      expect(subject).to receive(:sleep).once.with(0.2).ordered
+      expect(subject).to receive(:sleep).once.with(0.4).ordered
+      expect(subject).to receive(:sleep).once.with(0.8).ordered
+      (n - 4).times.each do |i|
+        expect(subject).to receive(:sleep).once.with(1).ordered
+      end
+      expect do
+        subject.retryable(:tries => n, :base_sleep => base_sleep, :max_sleep => max_sleep){raise E}
+      end.to raise_error(E)
+    end
+  end
+end
\ No newline at end of file
diff --git a/spec/util/worker_threads_default_printer_spec.rb b/spec/util/worker_threads_default_printer_spec.rb
new file mode 100644
index 00000000000..c2f5391cf38
--- /dev/null
+++ b/spec/util/worker_threads_default_printer_spec.rb
@@ -0,0 +1,45 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/util/worker_threads_default_printer"
+
+describe LogStash::Util::WorkerThreadsDefaultPrinter do
+  let(:settings)  { {} }
+  let(:collector) { [] }
+
+  subject { described_class.new(settings) }
+
+  before { subject.visit(collector) }
+
+  describe "the #visit method" do
+    context 'when the settings hash is empty' do
+      it 'adds nothing to the collector' do
+        subject.visit(collector)
+        expect(collector).to eq([])
+      end
+    end
+
+    context 'when the settings hash has both user and default content' do
+      let(:settings) { {'filter-workers' => 42, 'default-filter-workers' => 5} }
+
+      it 'adds two strings' do
+        expect(collector).to eq(["User set filter workers: 42", "Default filter workers: 5"])
+      end
+    end
+
+    context 'when the settings hash has only user content' do
+      let(:settings) { {'filter-workers' => 42} }
+
+      it 'adds a string with user set filter workers' do
+        expect(collector.first).to eq("User set filter workers: 42")
+      end
+    end
+
+    context 'when the settings hash has only default content' do
+      let(:settings) { {'default-filter-workers' => 5} }
+
+      it 'adds a string with default filter workers' do
+        expect(collector.first).to eq("Default filter workers: 5")
+      end
+    end
+  end
+end
diff --git a/test/windows/acceptance/logstash_release_acceptance.ps1 b/test/windows/acceptance/logstash_release_acceptance.ps1
new file mode 100644
index 00000000000..da812277543
--- /dev/null
+++ b/test/windows/acceptance/logstash_release_acceptance.ps1
@@ -0,0 +1,71 @@
+# Created By: Gabriel Moskovicz
+#
+# To be run on Jenkins
+#
+# Requirements to run the test:
+#
+# - Powershell 4
+# - Windows 7 or newer
+# - Java 7 or newer
+
+$LS_CONFIG="test.conf"
+$LS_BRANCH=$env:LS_BRANCH
+$Logstash_path = "C:\logstash"
+$Logstash_Snapshot_Directory = "$Logstash_path\logstash-latest-SNAPSHOT.zip"
+$Logstash_URL = "https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash/$LS_BRANCH/nightly/JDK7/logstash-latest-SNAPSHOT.zip"
+
+If (Test-Path $Logstash_path){
+	ri -Recurse -Force $Logstash_path
+}
+
+md -Path $Logstash_path
+(New-Object System.Net.WebClient).DownloadFile($Logstash_URL, $Logstash_Snapshot_Directory)
+
+#Unzip file
+$Destination = "$Logstash_path\logstash_" + $LS_BRANCH
+Add-Type -assembly "system.io.compression.filesystem"
+[io.compression.zipfile]::ExtractToDirectory($Logstash_Snapshot_Directory, $Destination)
+
+#Remove old files
+ri $Logstash_Snapshot_Directory
+
+#Move folder
+cd $Destination
+mv log* logstash
+cd logstash
+
+#Create Configuration
+ni $LS_CONFIG -it file
+sc -Path $LS_CONFIG -Encoding ascii -Value "input {
+	tcp {
+			port => "+ (Get-Random -minimum 2000 -maximum 3000) +"
+		}
+	}
+
+output {
+	stdout { }
+}"
+
+#Start Process
+$app = start .\bin\logstash.bat -ArgumentList "-f $LS_CONFIG" -PassThru -NoNewWindow
+sleep 30
+
+$RUNNING_TEST = $app.Id
+
+$PORT_TEST = netstat -na | select-string 2000
+
+If ($RUNNING_TEST -le 0){
+  echo "Logstash not running"
+  exit 1
+}
+
+echo "Logstash running"
+
+echo "Port: $PORT_TEST"
+
+If ($PORT_TEST.length -le  0){
+  echo "Port test failed"
+  exit 1
+}
+
+taskkill /PID $app.Id /F /T
\ No newline at end of file
diff --git a/test/windows/acceptance/logstash_release_default_plugins.ps1 b/test/windows/acceptance/logstash_release_default_plugins.ps1
new file mode 100644
index 00000000000..f4cf177e718
--- /dev/null
+++ b/test/windows/acceptance/logstash_release_default_plugins.ps1
@@ -0,0 +1,28 @@
+# Created By: Gabriel Moskovicz
+#
+# To be run on Jenkins
+#
+# Requirements to run the test:
+#
+# - Powershell 4
+# - Windows 7 or newer
+# - Java 7 or newer
+# - Ruby 7 or newer
+
+$ruby = $env:RUBY_HOME  + "\jruby.exe"
+
+sleep 30
+
+cd rakelib
+
+$install_default = start $ruby -ArgumentList "-S rake test:install-default" -Passthru -NoNewWindow -Wait
+
+If ($install_default.exitCode -gt 0){
+     exit 1
+}
+
+$plugins = start $ruby -ArgumentList "-S rake test:plugins" -Passthru -NoNewWindow -Wait
+
+If ($plugins.exitCode -gt 0){
+     exit 1
+}
\ No newline at end of file
diff --git a/test/windows/event_log/logstash_event_log_plugin_integration.ps1 b/test/windows/event_log/logstash_event_log_plugin_integration.ps1
new file mode 100644
index 00000000000..a267c05e72d
--- /dev/null
+++ b/test/windows/event_log/logstash_event_log_plugin_integration.ps1
@@ -0,0 +1,137 @@
+# Created By: Gabriel Moskovicz
+#
+# This is a script to test integration between logstash and elasticsearch.
+# It uses a simple json filter to parse the content of a simple text file an then
+# verifying if the message has been found in elasticsearch
+#
+# Requirements to run the test:
+#
+# - Powershell 4
+# - Windows 7 or newer
+# - Java 7 or newer
+
+Add-Type -assembly "system.io.compression.filesystem"
+
+
+$Main_path = "C:\integration_test"
+If (Test-Path $Main_path){
+	ri -Recurse -Force $Main_path
+}
+$Download_path  = "$Main_path\download"
+md -Path $Download_path
+
+## Logstash variables
+
+$LS_CONFIG="test.conf"
+$LS_BRANCH=$env:LS_BRANCH
+$Logstash_path = "$Main_path\logstash"
+$Logstash_zip_file = "$Download_path\logstash.zip"
+$Logstash_URL = "https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash/$LS_BRANCH/nightly/JDK7/logstash-latest-SNAPSHOT.zip"
+
+## ----------------------------------------
+
+## Elasticsearch variables
+
+$ES_Version = $env:ES_VERSION
+$ES_path = "$Main_path\elasticsearch"
+$ES_zip_file = "$Main_path\download\elasticsearch.zip"
+$ES_URL = "https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-$ES_Version.zip"
+
+## ----------------------------------------
+
+## Download and unzip Logstash
+
+md -Path $Logstash_path
+(New-Object System.Net.WebClient).DownloadFile($Logstash_URL, $Logstash_zip_file)
+[System.IO.Compression.ZipFile]::ExtractToDirectory($Logstash_zip_file, $Download_path)
+ri $Logstash_zip_file
+mv "$Download_path\log*\*" $Logstash_path
+
+## --------------------------------
+
+
+## Download and unzip Elasticsearch
+
+md -Path $ES_path
+(New-Object System.Net.WebClient).DownloadFile($ES_URL, $ES_zip_file)
+[System.IO.Compression.ZipFile]::ExtractToDirectory($ES_zip_file, $Download_path)
+ri $ES_zip_file
+mv "$Download_path\elastic*\*" "$ES_path"
+
+## --------------------------------
+
+
+# START ELASTICSEARCH
+
+echo "Starting Elasticsearch"
+$elasticsearchApp = start "$ES_path\bin\elasticsearch" -PassThru
+echo "Elasticsearch running"
+sleep 30
+
+# -------------------------------------------
+
+
+# Create logstash Configuration
+
+ni "$Logstash_path\$LS_CONFIG" -it file
+sc -Path "$Logstash_path\$LS_CONFIG" -Encoding ascii -Value "input {
+    eventlog {
+        logfile  => 'Application'
+    }
+}
+
+filter {
+    mutate {
+        replace => { 'type' => '%{SourceName}' }
+        remove_field => [ 'Type', 'Message' ]
+    }
+}
+
+output {
+	elasticsearch {
+        protocol => http
+        index => 'windows_eventlog_test_index'
+    }
+    stdout { codec => rubydebug }
+}"
+
+# -------------------------------------------
+
+
+# START LOGSTASH
+
+echo "Starting Logstash"
+$logstashApp = start "$Logstash_path\bin\logstash" -ArgumentList "-f $Logstash_path\$LS_CONFIG" -PassThru
+echo "Logstash running"
+sleep 30
+
+# -------------------------------------------
+
+New-EventLog -LogName Application -Source ElasticsearchSource
+Write-EventLog -LogName Application -Source ElasticsearchSource -EntryType Information -EventId 1 -Message "Example log Entry"
+
+sleep 15
+
+$searchresponse = curl "http://localhost:9200/windows_eventlog_test_index/ElasticsearchSource/_search" -UseBasicParsing
+$json_response = ConvertFrom-Json $searchresponse.Content
+$hit_source = $json_response.hits.hits[0]._source
+
+If (!($hit_source.SourceName -eq "ElasticsearchSource")){
+    echo "ERROR: Message was not indexed. Wrong Source Name. Test unsuccessful. Expected 'ElasticsearchSource' Received " + $hit_source.SourceName
+    exit 1
+}
+
+If (!($hit_source.EventCode -eq 1)){
+    echo "ERROR: Wrong expected value: EventCode. Test unsuccessful. Expected 1 Received " + $hit_source.EventCode
+    exit 1
+}
+
+If (!($hit_source.message -eq "Example log Entry")){
+    echo "ERROR: Wrong expected value: Message. Text Test unsuccessful. Expected 'Example log Entry' Received " + $hit_source.message
+    exit 1
+}
+
+echo "Test Succeeded"
+
+taskkill /PID $logstashApp.Id
+taskkill /PID $elasticsearchApp.Id /T
\ No newline at end of file
diff --git a/test/windows/integration/logstash_simple_integration.ps1 b/test/windows/integration/logstash_simple_integration.ps1
new file mode 100644
index 00000000000..73fe2fcff7e
--- /dev/null
+++ b/test/windows/integration/logstash_simple_integration.ps1
@@ -0,0 +1,145 @@
+# Created By: Gabriel Moskovicz
+#
+# This is a script to test integration between logstash and elasticsearch.
+# It uses a simple json filter to parse the content of a simple text file an then
+# verifying if the message has been found in elasticsearch
+#
+# Requirements to run the test:
+#
+# - Powershell 4
+# - Windows 7 or newer
+# - Java 7 or newer
+
+Add-Type -assembly "system.io.compression.filesystem"
+
+
+$Main_path = "C:\integration_test"
+If (Test-Path $Main_path){
+	ri -Recurse -Force $Main_path
+}
+$Download_path  = "$Main_path\download"
+md -Path $Download_path
+
+## Logstash variables
+
+$LS_CONFIG="test.conf"
+$LS_BRANCH=$env:LS_BRANCH
+$Logstash_path = "$Main_path\logstash"
+$Logstash_zip_file = "$Download_path\logstash.zip"
+$Logstas_URL = "https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash/$LS_BRANCH/nightly/JDK7/logstash-latest-SNAPSHOT.zip"
+
+## ----------------------------------------
+
+## Elasticsearch variables
+
+$ES_Version =$env:ES_VERSION
+$ES_path = "$Main_path\elasticsearch"
+$ES_zip_file = "$Main_path\download\elasticsearch.zip"
+$ES_URL = "https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch-$ES_Version.zip"
+
+## ----------------------------------------
+
+## Download and unzip Logstash
+
+md -Path $Logstash_path
+(New-Object System.Net.WebClient).DownloadFile($Logstas_URL, $Logstash_zip_file)
+[System.IO.Compression.ZipFile]::ExtractToDirectory($Logstash_zip_file, $Download_path)
+ri $Logstash_zip_file
+mv "$Download_path\log*\*" $Logstash_path
+
+## --------------------------------
+
+
+## Download and unzip Elasticsearch
+
+md -Path $ES_path
+(New-Object System.Net.WebClient).DownloadFile($ES_URL, $ES_zip_file)
+[System.IO.Compression.ZipFile]::ExtractToDirectory($ES_zip_file, $Download_path)
+ri $ES_zip_file
+mv "$Download_path\elastic*\*" "$ES_path"
+
+## --------------------------------
+
+
+# START ELASTICSEARCH
+
+echo "Starting Elasticsearch"
+$elasticsearchApp = start "$ES_path\bin\elasticsearch" -PassThru
+echo "Elasticsearch running"
+sleep 30
+
+# -------------------------------------------
+
+
+# Create logstash Configuration and Files
+
+ni "$Logstash_path\logs.txt" -it file
+sc -Path "$Logstash_path\logs.txt" -Encoding ascii -Value "{ ""ismessage"": true, ""day"": 2, ""text"": ""test message"" }"
+
+ni "$Logstash_path\$LS_CONFIG" -it file
+$logstash_config = "input {
+	file {
+			path => ['$Logstash_path\logs.txt']
+            start_position => 'beginning'
+		}
+}
+
+filter {
+    json {
+        source => 'message'
+    }
+}
+
+output {
+	elasticsearch { "
+
+if ( [convert]::ToDouble($LS_BRANCH) -lt 2 ) {
+    $logstash_config = $logstash_config + "
+        protocol => http"
+}
+
+$logstash_config = $logstash_config + "
+        index => 'windows_test_index'
+    }
+    stdout { codec => rubydebug }
+}"
+
+
+sc -Path "$Logstash_path\$LS_CONFIG" -Encoding ascii -Value $logstash_config
+
+# -------------------------------------------
+
+
+# START LOGSTASH
+
+echo "Starting Logstash"
+$logstashApp = start "$Logstash_path\bin\logstash" -ArgumentList "-f $Logstash_path\$LS_CONFIG" -PassThru
+echo "Logstash running"
+sleep 30
+
+# -------------------------------------------
+
+
+$searchresponse = curl "http://localhost:9200/windows_test_index/_search" -UseBasicParsing
+$json_response = ConvertFrom-Json $searchresponse.Content
+$hit_source = $json_response.hits.hits[0]._source
+
+If (!$hit_source.ismessage){
+    echo "ERROR: Message was not indexed. Test unsuccessful. Expected true, got false".
+    exit 1
+}
+
+If (!($hit_source.day -eq 2)){
+    echo "ERROR: Wrong expected value. Test unsuccessful. Expected 2, got " + $hit_source.day
+    exit 1
+}
+
+If (!($hit_source.text -eq "test message")){
+    echo "ERROR: Wrong expected value. Test unsuccessful. Expected 'test message', got " + $hit_source.text
+    exit 1
+}
+
+echo "Test Succeeded"
+
+taskkill /PID $logstashApp.Id
+taskkill /PID $elasticsearchApp.Id /T
\ No newline at end of file
diff --git a/tools/Gemfile.beaker b/tools/Gemfile.beaker
index e0f894e80a3..97a67a20ade 100644
--- a/tools/Gemfile.beaker
+++ b/tools/Gemfile.beaker
@@ -1,10 +1,11 @@
 source 'https://rubygems.org'
 
-gem 'beaker', :git => 'git@github.com:electrical/beaker.git', :branch => 'hiera_config'
-gem 'beaker-rspec', :git => 'git@github.com:puppetlabs/beaker-rspec.git', :branch => 'master'
+gem 'beaker', '2.27.0'
+gem 'beaker-rspec'
 gem 'pry'
 gem 'docker-api', '~> 1.0'
 gem 'rubysl-securerandom'
 gem 'rspec_junit_formatter'
 gem 'rspec', '~> 3.1'
 gem 'rake'
+gem 'fog-google', '~> 0.0.9'
