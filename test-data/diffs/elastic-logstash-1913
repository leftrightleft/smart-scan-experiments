diff --git a/CONTRIBUTORS b/CONTRIBUTORS
index e14761e28fe..c045c8f1c25 100644
--- a/CONTRIBUTORS
+++ b/CONTRIBUTORS
@@ -74,6 +74,7 @@ Contributors:
 * Andrea Forni (andreaforni)
 * Leandro Moreira (leandromoreira)
 * Hao Chen (haoch)
+* Ryan O'Keeffe (danielredoak)
 
 Note: If you've sent me patches, bug reports, or otherwise contributed to
 logstash, and you aren't on the list above and want to be, please let me know
diff --git a/lib/logstash/codecs/cloudfront.rb b/lib/logstash/codecs/cloudfront.rb
new file mode 100644
index 00000000000..e363f3fd42a
--- /dev/null
+++ b/lib/logstash/codecs/cloudfront.rb
@@ -0,0 +1,85 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/codecs/plain"
+require "logstash/json"
+
+# This codec will read cloudfront encoded content
+class LogStash::Codecs::Cloudfront < LogStash::Codecs::Base
+  config_name "cloudfront"
+
+  milestone 3
+
+  # The character encoding used in this codec. Examples include "UTF-8" and
+  # "CP1252"
+  #
+  # JSON requires valid UTF-8 strings, but in some cases, software that
+  # emits JSON does so in another encoding (nxlog, for example). In
+  # weird cases like this, you can set the charset setting to the
+  # actual encoding of the text and logstash will convert it for you.
+  #
+  # For nxlog users, you'll want to set this to "CP1252"
+  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
+
+  public
+  def initialize(params={})
+    super(params)
+    @converter = LogStash::Util::Charset.new(@charset)
+    @converter.logger = @logger
+  end
+
+  public
+  def decode(data)
+    begin
+      @gzip = Zlib::GzipReader.new(data)
+
+      metadata = extract_metadata(@gzip)
+
+      @logger.debug("Cloudfront: Extracting metadata", :metadata => metadata)
+
+      @gzip.each_line do |line|
+        yield create_event(line, metadata)
+      end
+
+    rescue Zlib::Error, Zlib::GzipFile::Error=> e
+      file = data.is_a?(String) ? data : data.class
+
+      @logger.error("Cloudfront codec: We cannot uncompress the gzip file", :filename => file)
+      raise e
+    end
+  end # def decode
+
+  public
+  def create_event(line, metadata)
+    event = LogStash::Event.new("message" => @converter.convert(line))
+    event["cloudfront_version"] = metadata["cloudfront_version"]
+    event["cloudfront_fields"] = metadata["cloudfront_fields"]
+    event
+  end
+
+
+  def extract_metadata(io)
+    version = extract_version(io.gets)
+    fields = extract_fields(io.gets)
+
+    return {
+      "cloudfront_version" => version,
+      "cloudfront_fields" => fields,
+    }
+  end
+
+
+  def extract_version(line)
+    if /^#Version: .+/.match(line)
+      junk, version = line.strip().split(/#Version: (.+)/)
+      version unless version.nil?
+    end
+  end
+
+
+  def extract_fields(line)
+    if /^#Fields: .+/.match(line)
+      junk, format = line.strip().split(/#Fields: (.+)/)
+      format unless format.nil?
+    end
+  end
+end # class LogStash::Codecs::Cloudfront
diff --git a/lib/logstash/codecs/gzip_lines.rb b/lib/logstash/codecs/gzip_lines.rb
new file mode 100644
index 00000000000..e889da9ada0
--- /dev/null
+++ b/lib/logstash/codecs/gzip_lines.rb
@@ -0,0 +1,45 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/codecs/plain"
+require "logstash/json"
+
+# This codec will read gzip encoded content
+class LogStash::Codecs::GzipLines < LogStash::Codecs::Base
+  config_name "gzip_lines"
+
+  milestone 3
+
+  # The character encoding used in this codec. Examples include "UTF-8" and
+  # "CP1252"
+  #
+  # JSON requires valid UTF-8 strings, but in some cases, software that
+  # emits JSON does so in another encoding (nxlog, for example). In
+  # weird cases like this, you can set the charset setting to the
+  # actual encoding of the text and logstash will convert it for you.
+  #
+  # For nxlog users, you'll want to set this to "CP1252"
+  config :charset, :validate => ::Encoding.name_list, :default => "UTF-8"
+
+  public
+  def initialize(params={})
+    super(params)
+    @converter = LogStash::Util::Charset.new(@charset)
+    @converter.logger = @logger
+  end
+
+  public
+  def decode(data)
+    @decoder = Zlib::GzipReader.new(data)
+
+    begin
+      @decoder.each_line do |line|
+        yield LogStash::Event.new("message" => @converter.convert(line))
+      end
+    rescue Zlib::Error, Zlib::GzipFile::Error=> e
+      file = data.is_a?(String) ? data : data.class
+
+      @logger.error("Gzip codec: We cannot uncompress the gzip file", :filename => file)
+      raise e
+    end
+  end # def decode
+end # class LogStash::Codecs::GzipLines
diff --git a/lib/logstash/codecs/s3_plain.rb b/lib/logstash/codecs/s3_plain.rb
new file mode 100644
index 00000000000..96f2db4be8e
--- /dev/null
+++ b/lib/logstash/codecs/s3_plain.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/util/charset"
+
+# The "s3_plain" codec is used for backward compatibility with previous version of the S3 Output
+#
+class LogStash::Codecs::S3Plain < LogStash::Codecs::Base
+  config_name "s3_plain"
+  milestone 3
+
+  public
+  def decode(data)
+    raise RuntimeError.new("This codec is only used for backward compatibility with the previous S3 output.")
+  end # def decode
+
+  public
+  def encode(event)
+    if event.is_a?(LogStash::Event)
+
+      message = "Date: #{event[LogStash::Event::TIMESTAMP]}\n"
+      message << "Source: #{event["source"]}\n"
+      message << "Tags: #{Array(event["tags"]).join(', ')}\n"
+      message << "Fields: #{event.to_hash.inspect}\n"
+      message << "Message: #{event["message"]}"
+
+      @on_event.call(message)
+    else
+      @on_event.call(event.to_s)
+    end
+  end # def encode
+end # class LogStash::Codecs::S3Plain
diff --git a/lib/logstash/inputs/base.rb b/lib/logstash/inputs/base.rb
index 22bda363343..5c1b5756165 100644
--- a/lib/logstash/inputs/base.rb
+++ b/lib/logstash/inputs/base.rb
@@ -20,7 +20,7 @@ class LogStash::Inputs::Base < LogStash::Plugin
   #
   # If you try to set a type on an event that already has one (for
   # example when you send an event from a shipper to an indexer) then
-  # a new input will not override the existing type. A type set at 
+  # a new input will not override the existing type. A type set at
   # the shipper stays with that event for its life even
   # when sent to another Logstash server.
   config :type, :validate => :string
@@ -100,7 +100,7 @@ def tag(newtag)
   end # def tag
 
   protected
-  def to_event(raw, source) 
+  def to_event(raw, source)
     raise LogStash::ThisMethodWasRemoved("LogStash::Inputs::Base#to_event - you should use codecs now instead of to_event. Not sure what this means? Get help on logstash-users@googlegroups.com!")
   end # def to_event
 
diff --git a/lib/logstash/inputs/s3.rb b/lib/logstash/inputs/s3.rb
index 403aaebd349..ab9fb6e9206 100644
--- a/lib/logstash/inputs/s3.rb
+++ b/lib/logstash/inputs/s3.rb
@@ -1,54 +1,56 @@
 # encoding: utf-8
 require "logstash/inputs/base"
 require "logstash/namespace"
+require "logstash/plugin_mixins/aws_config"
 
 require "time"
 require "tmpdir"
+require "stud/interval"
+require "stud/temporary"
 
 # Stream events from files from a S3 bucket.
 #
 # Each line from each file generates an event.
 # Files ending in '.gz' are handled as gzip'ed files.
 class LogStash::Inputs::S3 < LogStash::Inputs::Base
+  include LogStash::PluginMixins::AwsConfig
+
   config_name "s3"
   milestone 1
 
-  # TODO(sissel): refactor to use 'line' codec (requires removing both gzip
-  # support and readline usage). Support gzip through a gzip codec! ;)
-  default :codec, "plain"
+  default :codec, "line"
 
-  # The credentials of the AWS account used to access the bucket.
+  # DEPRECATED: The credentials of the AWS account used to access the bucket.
   # Credentials can be specified:
   # - As an ["id","secret"] array
   # - As a path to a file containing AWS_ACCESS_KEY_ID=... and AWS_SECRET_ACCESS_KEY=...
   # - In the environment, if not set (using variables AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY)
-  config :credentials, :validate => :array, :default => []
+  config :credentials, :validate => :array, :default => [], :deprecated => "This only exists to be backwards compatible. This plugin now uses the AwsConfig from PluginMixins"
 
   # The name of the S3 bucket.
   config :bucket, :validate => :string, :required => true
 
-  # The AWS region for your bucket.
-  config :region, :validate => ["us-east-1", "us-west-1", "us-west-2",
-                                "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"],
-                                :deprecated => "'region' has been deprecated in favor of 'region_endpoint'"
-
   # The AWS region for your bucket.
   config :region_endpoint, :validate => ["us-east-1", "us-west-1", "us-west-2",
                                 "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
+                                "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1", :deprecated => "This only exists to be backwards compatible. This plugin now uses the AwsConfig from PluginMixins"
 
-  # If specified, the prefix the filenames in the bucket must match (not a regexp)
+  # If specified, the prefix of filenames in the bucket must match (not a regexp)
   config :prefix, :validate => :string, :default => nil
 
   # Where to write the since database (keeps track of the date
   # the last handled file was added to S3). The default will write
   # sincedb files to some path matching "$HOME/.sincedb*"
-  config :sincedb_path, :validate => :string, :default => nil
+  config :sincedb_path, :validate => :string, :default => ENV['HOME']
 
   # Name of a S3 bucket to backup processed files to.
   config :backup_to_bucket, :validate => :string, :default => nil
 
+  # Append a prefix to the key (full path including file name in s3) after processing.
+  # If backing up to another (or the same) bucket, this effectively lets you
+  # choose a new 'folder' to place the files in
+  config :backup_add_prefix, :validate => :string, :default => nil
+
   # Path of a local directory to backup processed files to.
   config :backup_to_dir, :validate => :string, :default => nil
 
@@ -59,19 +61,57 @@ class LogStash::Inputs::S3 < LogStash::Inputs::Base
   # Value is in seconds.
   config :interval, :validate => :number, :default => 60
 
+  # Ruby style regexp of keys to exclude from the bucket
+  config :exclude_pattern, :validate => :string, :default => nil
+
+
+  config :since_db_backend, :validate => ["local", "s3"]
+
   public
   def register
     require "digest/md5"
     require "aws-sdk"
 
-    @region_endpoint = @region if @region && !@region.empty?
+    @region = get_region
 
-    @logger.info("Registering s3 input", :bucket => @bucket, :region_endpoint => @region_endpoint)
+    @logger.info("Registering s3 input", :bucket => @bucket, :region => @region)
+
+    if @sincedb_path.nil?
+      @logger.error("S3 input: Configuration error, no HOME or sincedb_path set")
+      raise ConfigurationError.new('No HOME or sincedb_path set')
+    else
+      sincedb_file = File.join(ENV["HOME"], ".sincedb_" + Digest::MD5.hexdigest("#{@bucket}+#{@prefix}"))
+      @sincedb = SinceDB::File.new(sincedb_file)
+    end
+
+    s3 = get_s3object
+
+    @s3bucket = s3.buckets[@bucket]
 
-    if @credentials.length == 0
-      @access_key_id = ENV['AWS_ACCESS_KEY_ID']
-      @secret_access_key = ENV['AWS_SECRET_ACCESS_KEY']
-    elsif @credentials.length == 1
+    unless @backup_to_bucket.nil?
+      @backup_bucket = s3.buckets[@backup_to_bucket]
+      unless @backup_bucket.exists?
+        s3.buckets.create(@backup_to_bucket)
+      end
+    end
+
+    unless @backup_to_dir.nil?
+      Dir.mkdir(@backup_to_dir, 0700) unless File.exists?(@backup_to_dir)
+    end
+  end # def register
+
+  def get_region
+    # TODO: (ph) Deprecated, it will be removed
+    if @region_endpoint && !@region_endpoint.empty? && !@region
+      @region_endpoint
+    else
+      @region
+    end
+  end
+
+  def get_s3object
+    # TODO: (ph) Deprecated, it will be removed
+    if @credentials.length == 1
       File.open(@credentials[0]) { |f| f.each do |line|
         unless (/^\#/.match(line))
           if(/\s*=\s*/.match(line))
@@ -90,189 +130,165 @@ def register
     elsif @credentials.length == 2
       @access_key_id = @credentials[0]
       @secret_access_key = @credentials[1]
-    else
-      raise ArgumentError.new('Credentials must be of the form "/path/to/file" or ["id", "secret"]')
-    end
-
-    if @access_key_id.nil? or @secret_access_key.nil?
-      raise ArgumentError.new('Missing AWS credentials')
-    end
-
-    if @bucket.nil?
-      raise ArgumentError.new('Missing AWS bucket')
-    end
-
-    if @sincedb_path.nil?
-      if ENV['HOME'].nil?
-        raise ArgumentError.new('No HOME or sincedb_path set')
-      end
-      @sincedb_path = File.join(ENV["HOME"], ".sincedb_" + Digest::MD5.hexdigest("#{@bucket}+#{@prefix}"))
-    end
-
-    s3 = AWS::S3.new(
-      :access_key_id => @access_key_id,
-      :secret_access_key => @secret_access_key,
-      :region => @region_endpoint
-    )
-
-    @s3bucket = s3.buckets[@bucket]
-
-    unless @backup_to_bucket.nil?
-      @backup_bucket = s3.buckets[@backup_to_bucket]
-      unless @backup_bucket.exists?
-        s3.buckets.create(@backup_to_bucket)
-      end
     end
 
-    unless @backup_to_dir.nil?
-      Dir.mkdir(@backup_to_dir, 0700) unless File.exists?(@backup_to_dir)
+    if @credentials
+      s3 = AWS::S3.new(
+        :access_key_id => @access_key_id,
+        :secret_access_key => @secret_access_key,
+        :region => @region
+      )
+    else
+      s3 = AWS::S3.new(aws_options_hash)
     end
+  end
 
-  end # def register
+  public
+  def aws_service_endpoint(region)
+    return { :s3_endpoint => region }
+  end
 
   public
   def run(queue)
-    loop do
-      process_new(queue)
-      sleep(@interval)
+    Stud.interval(@interval) do
+      process_files(queue)
     end
-    finished
   end # def run
 
   private
-  def process_new(queue, since=nil)
-
-    if since.nil?
-        since = sincedb_read()
-    end
-
-    objects = list_new(since)
-    objects.each do |k|
-      @logger.debug("S3 input processing", :bucket => @bucket, :key => k)
-      lastmod = @s3bucket.objects[k].last_modified
-      process_log(queue, k)
-      sincedb_write(lastmod)
-    end
+  def process_files(queue, since=nil)
+    objects = list_new_files
+    objects.each do |key|
+      @logger.debug("S3 input processing", :bucket => @bucket, :key => key)
 
-  end # def process_new
+      lastmod = @s3bucket.objects[key].last_modified
 
-  private
-  def list_new(since=nil)
+      process_log(queue, key)
 
-    if since.nil?
-      since = Time.new(0)
+      @sincedb.write(lastmod)
     end
+  end # def process_files
 
+  public
+  def list_new_files
     objects = {}
+
     @s3bucket.objects.with_prefix(@prefix).each do |log|
-      if log.last_modified > since
-        objects[log.key] = log.last_modified
+      @logger.debug("S3 input: Found key", :key => log.key)
+
+      unless ignore_filename?(log.key)
+
+        if @sincedb.newer?(log.last_modified)
+          objects[log.key] = log.last_modified
+          @logger.debug("S3 input: Adding to objects[]", :key => log.key)
+        end
       end
     end
-
     return sorted_objects = objects.keys.sort {|a,b| objects[a] <=> objects[b]}
+  end # def fetch_new_files
 
-  end # def list_new
+  private
+  def ignore_filename?(filename)
+    if (@backup_add_prefix && @backup_to_bucket == @bucket && filename =~ /^#{backup_add_prefix}/)
+      return true
+    elsif @exclude_pattern.nil?
+      return false
+    elsif filename =~ Regexp.new(@exclude_pattern)
+      return true
+    else
+      return false
+    end
+  end
 
   private
   def process_log(queue, key)
-
     object = @s3bucket.objects[key]
-    tmp = Dir.mktmpdir("logstash-")
-    begin
-      filename = File.join(tmp, File.basename(key))
-      File.open(filename, 'wb') do |s3file|
-        object.read do |chunk|
-          s3file.write(chunk)
-        end
-      end
-      process_local_log(queue, filename)
-      unless @backup_to_bucket.nil?
-        backup_object = @backup_bucket.objects[key]
-        backup_object.write(Pathname.new(filename))
-      end
-      unless @backup_to_dir.nil?
-        FileUtils.cp(filename, @backup_to_dir)
-      end
-      if @delete
-        object.delete()
-      end
-    end
-    FileUtils.remove_entry_secure(tmp, force=true)
 
-  end # def process_log
+    tmp = Stud::Temporary.directory("logstash-")
 
-  private
-  def process_local_log(queue, filename)
+    filename = File.join(tmp, File.basename(key))
 
-    metadata = {
-      :version => nil,
-      :format => nil,
-    }
-    File.open(filename) do |file|
-      if filename.end_with?('.gz')
-        gz = Zlib::GzipReader.new(file)
-        gz.each_line do |line|
-          metadata = process_line(queue, metadata, line)
-        end
-      else
-        file.each do |line|
-          metadata = process_line(queue, metadata, line)
-        end
-      end
-    end
+    download_remote_file(object, filename)
 
-  end # def process_local_log
+    process_local_log(queue, filename)
 
-  private
-  def process_line(queue, metadata, line)
+    backup_to_bucket(object, key)
+    backup_to_dir(filename)
 
-    if /#Version: .+/.match(line)
-      junk, version = line.strip().split(/#Version: (.+)/)
-      unless version.nil?
-        metadata[:version] = version
-      end
-    elsif /#Fields: .+/.match(line)
-      junk, format = line.strip().split(/#Fields: (.+)/)
-      unless format.nil?
-        metadata[:format] = format
-      end
-    else
-      @codec.decode(line) do |event|
-        decorate(event)
-        unless metadata[:version].nil?
-          event["cloudfront_version"] = metadata[:version]
-        end
-        unless metadata[:format].nil?
-          event["cloudfront_fields"] = metadata[:format]
-        end
-        queue << event
+    delete_file_from_bucket()
+  end
+
+  def download_remote_file(remote_object, local_filename)
+    @logger.debug("S3 input: Download remove file", :remote_key => remote_object.key, :local_filename => local_filename)
+    File.open(local_filename, 'wb') do |s3file|
+      remote_object.read do |chunk|
+        s3file.write(chunk)
       end
     end
-    return metadata
+  end
 
-  end # def process_line
+  def delete_file_from_bucket
+    if @delete and @backup_to_bucket.nil?
+      object.delete()
+    end
+  end
 
-  private
-  def sincedb_read()
+  public
+  def backup_to_bucket(object, key)
+    unless @backup_to_bucket.nil?
+      backup_key = "#{@backup_add_prefix}#{key}"
+      if @delete
+        object.move_to(backup_key, :bucket => @backup_bucket)
+      else
+        object.copy_to(backup_key, :bucket => @backup_bucket)
+      end
+    end
+  end
 
-    if File.exists?(@sincedb_path)
-      since = Time.parse(File.read(@sincedb_path).chomp.strip)
-    else
-      since = Time.new(0)
+  public
+  def backup_to_dir(filename)
+    unless @backup_to_dir.nil?
+      FileUtils.cp(filename, @backup_to_dir)
     end
-    return since
+  end
 
-  end # def sincedb_read
+  def delete_file_from_bucket
+    if @delete and @backup_to_bucket.nil?
+      object.delete()
+    end
+  end
 
   private
-  def sincedb_write(since=nil)
-
-    if since.nil?
-      since = Time.now()
+  def process_local_log(queue, filename)
+    @codec.decode(File.open(filename, 'rb')) do |event|
+      decorate(event)
+      queue << event
     end
-    File.open(@sincedb_path, 'w') { |file| file.write(since.to_s) }
+  end # def process_local_log
+
+  module SinceDB
+    class File
+      def initialize(file)
+        @sincedb_path = file
+      end
 
-  end # def sincedb_write
+      def newer?(date)
+        date > read
+      end
 
+      def read
+        if ::File.exists?(@sincedb_path)
+          since = Time.parse(::File.read(@sincedb_path).chomp.strip)
+        else
+          since = Time.new(0)
+        end
+        return since
+      end
+
+      def write(since = nil)
+        since = Time.now() if since.nil?
+        ::File.open(@sincedb_path, 'w') { |file| file.write(since.to_s) }
+      end
+    end
+  end
 end # class LogStash::Inputs::S3
diff --git a/lib/logstash/outputs/file.rb b/lib/logstash/outputs/file.rb
index 4ca7b98ec50..5171c66ee6a 100644
--- a/lib/logstash/outputs/file.rb
+++ b/lib/logstash/outputs/file.rb
@@ -10,13 +10,13 @@ class LogStash::Outputs::File < LogStash::Outputs::Base
   config_name "file"
   milestone 2
 
-  # The path to the file to write. Event fields can be used here, 
+  # The path to the file to write. Event fields can be used here,
   # like "/var/log/logstash/%{host}/%{application}"
-  # One may also utilize the path option for date-based log 
+  # One may also utilize the path option for date-based log
   # rotation via the joda time format. This will use the event
   # timestamp.
-  # E.g.: path => "./test-%{+YYYY-MM-dd}.txt" to create 
-  # ./test-2013-05-29.txt 
+  # E.g.: path => "./test-%{+YYYY-MM-dd}.txt" to create
+  # ./test-2013-05-29.txt
   config :path, :validate => :string, :required => true
 
   # The maximum size of file to write. When the file exceeds this
@@ -35,7 +35,7 @@ class LogStash::Outputs::File < LogStash::Outputs::Base
   # event will be written as a single line.
   config :message_format, :validate => :string
 
-  # Flush interval (in seconds) for flushing writes to log files. 
+  # Flush interval (in seconds) for flushing writes to log files.
   # 0 will flush on every message.
   config :flush_interval, :validate => :number, :default => 2
 
@@ -136,7 +136,7 @@ def open(path)
     dir = File.dirname(path)
     if !Dir.exists?(dir)
       @logger.info("Creating directory", :directory => dir)
-      FileUtils.mkdir_p(dir) 
+      FileUtils.mkdir_p(dir)
     end
 
     # work around a bug opening fifos (bug JRUBY-6280)
diff --git a/lib/logstash/outputs/s3.rb b/lib/logstash/outputs/s3.rb
index c493d29b526..4885b4878ab 100644
--- a/lib/logstash/outputs/s3.rb
+++ b/lib/logstash/outputs/s3.rb
@@ -1,357 +1,395 @@
 # encoding: utf-8
 require "logstash/outputs/base"
 require "logstash/namespace"
+require "logstash/plugin_mixins/aws_config"
+
+require "stud/temporary"
+
 require "socket" # for Socket.gethostname
+require "thread"
+require "tmpdir"
+require "fileutils"
 
-# TODO integrate aws_config in the future
-#require "logstash/plugin_mixins/aws_config"
 
 # INFORMATION:
 
 # This plugin was created for store the logstash's events into Amazon Simple Storage Service (Amazon S3).
 # For use it you needs authentications and an s3 bucket.
 # Be careful to have the permission to write file on S3's bucket and run logstash with super user for establish connection.
-
+#
 # S3 plugin allows you to do something complex, let's explain:)
-
+#
 # S3 outputs create temporary files into "/opt/logstash/S3_temp/". If you want, you can change the path at the start of register method.
 # This files have a special name, for example:
-
+#
 # ls.s3.ip-10-228-27-95.2013-04-18T10.00.tag_hello.part0.txt
-
+#
 # ls.s3 : indicate logstash plugin s3
-
+#
 # "ip-10-228-27-95" : indicate you ip machine, if you have more logstash and writing on the same bucket for example.
 # "2013-04-18T10.00" : represents the time whenever you specify time_file.
 # "tag_hello" : this indicate the event's tag, you can collect events with the same tag.
 # "part0" : this means if you indicate size_file then it will generate more parts if you file.size > size_file.
 #           When a file is full it will pushed on bucket and will be deleted in temporary directory.
 #           If a file is empty is not pushed, but deleted.
-
+#
 # This plugin have a system to restore the previous temporary files if something crash.
-
+#
 ##[Note] :
-
+#
 ## If you specify size_file and time_file then it will create file for each tag (if specified), when time_file or
 ## their size > size_file, it will be triggered then they will be pushed on s3's bucket and will delete from local disk.
-
 ## If you don't specify size_file, but time_file then it will create only one file for each tag (if specified).
 ## When time_file it will be triggered then the files will be pushed on s3's bucket and delete from local disk.
-
+#
 ## If you don't specify time_file, but size_file  then it will create files for each tag (if specified),
 ## that will be triggered when their size > size_file, then they will be pushed on s3's bucket and will delete from local disk.
-
+#
 ## If you don't specific size_file and time_file you have a curios mode. It will create only one file for each tag (if specified).
 ## Then the file will be rest on temporary directory and don't will be pushed on bucket until we will restart logstash.
-
-# INFORMATION ABOUT CLASS:
-
-# I tried to comment the class at best i could do.
-# I think there are much thing to improve, but if you want some points to develop here a list:
-
-# TODO Integrate aws_config in the future
-# TODO Find a method to push them all files when logtstash close the session.
-# TODO Integrate @field on the path file
-# TODO Permanent connection or on demand? For now on demand, but isn't a good implementation.
-#      Use a while or a thread to try the connection before break a time_out and signal an error.
-# TODO If you have bugs report or helpful advice contact me, but remember that this code is much mine as much as yours,
-#      try to work on it if you want :)
-
-
-# USAGE:
-
+#
+#
+# #### Usage:
 # This is an example of logstash config:
-
-# output {
-#    s3{
-#      access_key_id => "crazy_key"             (required)
-#      secret_access_key => "monkey_access_key" (required)
-#      endpoint_region => "eu-west-1"           (required)
-#      bucket => "boss_please_open_your_bucket" (required)
-#      size_file => 2048                        (optional)
-#      time_file => 5                           (optional)
-#      format => "plain"                        (optional)
-#      canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
+#
+#    output {
+#       s3{
+#         access_key_id => "crazy_key"             (required)
+#         secret_access_key => "monkey_access_key" (required)
+#         endpoint_region => "eu-west-1"           (required)
+#         bucket => "boss_please_open_your_bucket" (required)
+#         size_file => 2048                        (optional)
+#         time_file => 5                           (optional)
+#         canned_acl => "private"                  (optional. Options are "private", "public_read", "public_read_write", "authenticated_read". Defaults to "private" )
+#       }
 #    }
-# }
-
-# We analize this:
-
-# access_key_id => "crazy_key"
-# Amazon will give you the key for use their service if you buy it or try it. (not very much open source anyway)
-
-# secret_access_key => "monkey_access_key"
-# Amazon will give you the secret_access_key for use their service if you buy it or try it . (not very much open source anyway).
+#
+class LogStash::Outputs::S3 < LogStash::Outputs::Base
+  include LogStash::PluginMixins::AwsConfig
 
-# endpoint_region => "eu-west-1"
-# When you make a contract with Amazon, you should know where the services you use.
+  TEMPFILE_EXTENSION = "txt"
+  S3_INVALID_CHARACTERS = /[\^`><]/
 
-# bucket => "boss_please_open_your_bucket"
-# Be careful you have the permission to write on bucket and know the name.
+  config_name "s3"
+  milestone 1
 
-# size_file => 2048
-# Means the size, in KB, of files who can store on temporary directory before you will be pushed on bucket.
-# Is useful if you have a little server with poor space on disk and you don't want blow up the server with unnecessary temporary log files.
+  # S3 bucket
+  config :bucket, :validate => :string
 
-# time_file => 5
-# Means, in minutes, the time  before the files will be pushed on bucket. Is useful if you want to push the files every specific time.
+  # AWS endpoint_region
+  config :endpoint_region, :validate => ["us-east-1", "us-west-1", "us-west-2",
+                                         "eu-west-1", "ap-southeast-1", "ap-southeast-2",
+                                        "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1", :deprecated => 'Deprecated, use region instead.'
 
-# format => "plain"
-# Means the format of events you want to store in the files
+  # Set the size of file in bytes, this means that files on bucket when have dimension > file_size, they are stored in two or more file.
+  # If you have tags then it will generate a specific size file for every tags
+  ##NOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket.
+  config :size_file, :validate => :number, :default => 0
 
-# canned_acl => "private"
-# The S3 canned ACL to use when putting the file. Defaults to "private".
+  # Set the time, in minutes, to close the current sub_time_section of bucket.
+  # If you define file_size you have a number of files in consideration of the section and the current tag.
+  # 0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,
+  # for now the only thing this plugin can do is to put the file when logstash restart.
+  config :time_file, :validate => :number, :default => 0
 
-# LET'S ROCK AND ROLL ON THE CODE!
+  ## IMPORTANT: if you use multiple instance of s3, you should specify on one of them the "restore=> true" and on the others "restore => false".
+  ## This is hack for not destroy the new files after restoring the initial files.
+  ## If you do not specify "restore => true" when logstash crashes or is restarted, the files are not sent into the bucket,
+  ## for example if you have single Instance.
+  config :restore, :validate => :boolean, :default => false
 
-class LogStash::Outputs::S3 < LogStash::Outputs::Base
- #TODO integrate aws_config in the future
- #  include LogStash::PluginMixins::AwsConfig
+  # The S3 canned ACL to use when putting the file. Defaults to "private".
+  config :canned_acl, :validate => ["private", "public_read", "public_read_write", "authenticated_read"],
+         :default => "private"
 
- config_name "s3"
- milestone 1
+  # Set the directory where logstash will store the tmp files before sending it to S3
+  # default to the current OS temporary directory in linux /tmp/logstash
+  config :temporary_directory, :validate => :string, :default => File.join(Dir.tmpdir(), "logstash")
 
- # Aws access_key.
- config :access_key_id, :validate => :string
+  # Specify a prefix to the uploaded filename, this can simulate directories on S3
+  config :prefix, :validate => :string, :default => ''
 
- # Aws secret_access_key
- config :secret_access_key, :validate => :string
+  # Specify how many workers to use to upload the files to S3
+  config :upload_workers_count, :validate => :number, :default => 1
 
- # S3 bucket
- config :bucket, :validate => :string
 
- # Aws endpoint_region
- config :endpoint_region, :validate => ["us-east-1", "us-west-1", "us-west-2",
-                                        "eu-west-1", "ap-southeast-1", "ap-southeast-2",
-                                        "ap-northeast-1", "sa-east-1", "us-gov-west-1"], :default => "us-east-1"
+  # Exposed attributes for testing purpose.
+  attr_accessor :tempfile
+  attr_reader :page_counter
 
- # Set the size of file in KB, this means that files on bucket when have dimension > file_size, they are stored in two or more file.
- # If you have tags then it will generate a specific size file for every tags
- ##NOTE: define size of file is the better thing, because generate a local temporary file on disk and then put it in bucket.
- config :size_file, :validate => :number, :default => 0
+  def aws_s3_config
+    @logger.info("Registering s3 output", :bucket => @bucket, :endpoint_region => @region)
+    @s3 = AWS::S3.new(aws_options_hash)
+  end
 
- # Set the time, in minutes, to close the current sub_time_section of bucket.
- # If you define file_size you have a number of files in consideration of the section and the current tag.
- # 0 stay all time on listerner, beware if you specific 0 and size_file 0, because you will not put the file on bucket,
- # for now the only thing this plugin can do is to put the file when logstash restart.
- config :time_file, :validate => :number, :default => 0
+  def aws_service_endpoint(region)
+    # Make the deprecated endpoint_region work
+    # TODO: (ph) Remove this after deprecation.
+    if @endpoint_region
+      region_to_use = @endpoint_region
+    else
+      region_to_use = region
+    end
 
- # The event format you want to store in files. Defaults to plain text.
- config :format, :validate => [ "json", "plain", "nil" ], :default => "plain"
+    return {
+      :s3_endpoint => region_to_use == 'us-east-1' ? 's3.amazonaws.com' : "s3-#{region_to_use}.amazonaws.com"
+    }
+  end
 
- ## IMPORTANT: if you use multiple instance of s3, you should specify on one of them the "restore=> true" and on the others "restore => false".
- ## This is hack for not destroy the new files after restoring the initial files.
- ## If you do not specify "restore => true" when logstash crashes or is restarted, the files are not sent into the bucket,
- ## for example if you have single Instance.
- config :restore, :validate => :boolean, :default => false
+  public
+  def write_on_bucket(file)
+    # find and use the bucket
+    bucket = @s3.buckets[@bucket]
 
- # Aws canned ACL
- config :canned_acl, :validate => ["private", "public_read", "public_read_write", "authenticated_read"],
-        :default => "private"
+    remote_filename = "#{@prefix}#{File.basename(file)}"
 
- # Method to set up the aws configuration and establish connection
- def aws_s3_config
+    @logger.debug("S3: ready to write file in bucket", :remote_filename => remote_filename, :bucket => @bucket)
 
-  @endpoint_region == 'us-east-1' ? @endpoint_region = 's3.amazonaws.com' : @endpoint_region = 's3-'+@endpoint_region+'.amazonaws.com'
+    begin
+      # prepare for write the file
+      object = bucket.objects[remote_filename]
+      object.write(:file => file, :acl => @canned_acl)
+    rescue AWS::Errors::Base => e
+      @logger.error("S3: AWS error", :error => e)
+      raise LogStash::ConfigurationError, "AWS Configuration Error"
+    end
 
-  @logger.info("Registering s3 output", :bucket => @bucket, :endpoint_region => @endpoint_region)
+    @logger.debug("S3: has written remote file in bucket with canned ACL", :remote_filename => remote_filename, :bucket  => @bucket, :canned_acl => @canned_acl)
+  end
 
-  AWS.config(
-    :access_key_id => @access_key_id,
-    :secret_access_key => @secret_access_key,
-    :s3_endpoint => @endpoint_region
-  )
-  @s3 = AWS::S3.new
+  # This method is used for create new empty temporary files for use. Flag is needed for indicate new subsection time_file.
+  public
+  def create_temporary_file
+    filename = get_temporary_filename(@page_counter)
 
- end
+    @logger.debug("S3: Creating a new temporary file", :filename => filename)
 
- # This method is used to manage sleep and awaken thread.
- def time_alert(interval)
+    @file_rotation_lock.synchronize do
+      unless @tempfile.nil?
+        @tempfile.close
+      end
 
-   Thread.new do
-    loop do
-      start_time = Time.now
-      yield
-      elapsed = Time.now - start_time
-      sleep([interval - elapsed, 0].max)
+      @tempfile = File.open(filename, "a")
     end
-   end
-
- end
-
- # this method is used for write files on bucket. It accept the file and the name of file.
- def write_on_bucket (file_data, file_basename)
-
-  # if you lose connection with s3, bad control implementation.
-  if ( @s3 == nil)
-    aws_s3_config
   end
 
-  # find and use the bucket
-  bucket = @s3.buckets[@bucket]
+  public
+  def register
+    require "aws-sdk"
+    # required if using ruby version < 2.0
+    # http://ruby.awsblog.com/post/Tx16QY1CI5GVBFT/Threading-with-the-AWS-SDK-for-Ruby
+    AWS.eager_autoload!(AWS::S3)
 
-  @logger.debug "S3: ready to write "+file_basename+" in bucket "+@bucket+", Fire in the hole!"
+    workers_not_supported
 
-  # prepare for write the file
-  object = bucket.objects[file_basename]
-  object.write(:file => file_data, :acl => @canned_acl)
+    @s3 = aws_s3_config
+    @upload_queue = Queue.new
+    @file_rotation_lock = Mutex.new
 
-  @logger.debug "S3: has written "+file_basename+" in bucket "+@bucket + " with canned ACL \"" + @canned_acl + "\""
-
- end
-
- # this method is used for create new path for name the file
- def getFinalPath
-
-   @pass_time = Time.now
-   return @temp_directory+"ls.s3."+Socket.gethostname+"."+(@pass_time).strftime("%Y-%m-%dT%H.%M")
+    if @prefix && @prefix =~ S3_INVALID_CHARACTERS
+      @logger.error("S3: prefix contains invalid characters", :prefix => @prefix, :contains => S3_INVALID_CHARACTERS)
+      raise LogStash::ConfigurationError, "S3: prefix contains invalid characters"
+    end
 
- end
+    if !Dir.exist?(@temporary_directory)
+      FileUtils.mkdir_p(@temporary_directory)
+    end
 
- # This method is used for restore the previous crash of logstash or to prepare the files to send in bucket.
- # Take two parameter: flag and name. Flag indicate if you want to restore or not, name is the name of file
- def upFile(flag, name)
+    test_s3_write()
 
-   Dir[@temp_directory+name].each do |file|
-     name_file = File.basename(file)
+    restore_from_crashes() if @restore == true
+    reset_page_counter()
+    create_temporary_file()
+    configure_periodic_uploader() if time_file != 0
+    configure_upload_workers()
 
-     if (flag == true)
-      @logger.warn "S3: have found temporary file: "+name_file+", something has crashed before... Prepare for upload in bucket!"
-     end
+    @codec.on_event do |event|
+      handle_event(event)
+    end
+  end
 
-     if (!File.zero?(file))
-       write_on_bucket(file, name_file)
+  public
+  def configure_upload_workers
+    @logger.debug("S3: Configure upload workers")
 
-       if (flag == true)
-          @logger.debug "S3: file: "+name_file+" restored on bucket "+@bucket
-       else
-          @logger.debug "S3: file: "+name_file+" was put on bucket "+@bucket
-       end
-     end
+    @upload_workers = @upload_workers_count.times.map do |worker_id|
+      Thread.new do
+        LogStash::Util::set_thread_name("<S3 upload worker #{worker_id}")
 
-     File.delete (file)
+        while true do
+          @logger.debug("S3: upload worker is waiting for a new file to upload.", :worker_id => worker_id)
 
-   end
- end
+          upload_worker
+        end
+      end
+    end
+  end
 
- # This method is used for create new empty temporary files for use. Flag is needed for indicate new subsection time_file.
- def newFile (flag)
+  private
+  def upload_worker
+    file = @upload_queue.deq
+
+    case file
+      when LogStash::ShutdownEvent
+        @logger.debug("S3: upload worker is shutting down gracefuly")
+        @upload_queue.enq(LogStash::ShutdownEvent)
+        break
+      else
+        @logger.debug("S3: upload working is uploading a new file", :filename => File.basename(file))
+        move_file_to_bucket(file)
+    end
+  end
 
-   if (flag == true)
-     @current_final_path = getFinalPath
-     @sizeCounter = 0
-   end
+  public
+  def next_page
+    @page_counter += 1
+  end
 
-   if (@tags.size != 0)
-     @tempFile = File.new(@current_final_path+".tag_"+@tag_path+"part"+@sizeCounter.to_s+".txt", "w")
-   else
-     @tempFile = File.new(@current_final_path+".part"+@sizeCounter.to_s+".txt", "w")
-   end
+  def reset_page_counter
+    @page_counter = 0
+  end
 
- end
+  # Use the same method that Amazon use to check
+  # permission on the user bucket by creating a small file
+  public
+  def test_s3_write
+    @logger.debug("S3: Creating a test file on S3")
 
- public
- def register
-   require "aws-sdk"
-   @temp_directory = "/opt/logstash/S3_temp/"
+    test_filename = File.join(@temporary_directory, "logstash-programmatic-access-test-object")
 
-   if (@tags.size != 0)
-       @tag_path = ""
-       for i in (0..@tags.size-1)
-          @tag_path += @tags[i].to_s+"."
-       end
-   end
+    File.open(test_filename, 'a') do |file|
+      file.write('test')
+    end
 
-   if !(File.directory? @temp_directory)
-    @logger.debug "S3: Directory "+@temp_directory+" doesn't exist, let's make it!"
-    Dir.mkdir(@temp_directory)
-   else
-    @logger.debug "S3: Directory "+@temp_directory+" exist, nothing to do"
-   end
+    begin
+      write_on_bucket(test_filename)
+    ensure
+      File.delete(test_filename)
+    end
+  end
 
-   if (@restore == true )
-     @logger.debug "S3: is attempting to verify previous crashes..."
+  public
+  def restore_from_crashes
+    @logger.debug("S3: is attempting to verify previous crashes...")
 
-     upFile(true, "*.txt")
-   end
+    Dir[File.join(@temporary_directory, "*.#{TEMPFILE_EXTENSION}")].each do |file|
+      name_file = File.basename(file)
+      @logger.warn("S3: have found temporary file the upload process crashed, uploading file to S3.", :filename => name_file)
+      move_file_to_bucket_async(file)
+    end
+  end
 
-   newFile(true)
+  public
+  def move_file_to_bucket(file)
+    if !File.zero?(file)
+      write_on_bucket(file)
+      @logger.debug("S3: file was put on the upload thread", :filename => File.basename(file), :bucket => @bucket)
+    end
 
-   if (time_file != 0)
-      first_time = true
-      @thread = time_alert(@time_file*60) do
-       if (first_time == false)
-         @logger.debug "S3: time_file triggered,  let's bucket the file if dosen't empty  and create new file "
-         upFile(false, File.basename(@tempFile))
-         newFile(true)
-       else
-         first_time = false
-       end
-     end
-   end
+    begin
+      File.delete(file)
+    rescue Errno::ENOENT
+      # Something else deleted the file, logging but not raising the issue
+      @logger.warn("S3: Cannot delete the temporary file since it doesn't exist on disk", :filename => File.basename(file))
+    rescue Errno::EACCES
+      @logger.error("S3: Logstash doesnt have the permission to delete the file in the temporary directory.", :filename => File.basename, :temporary_directory => @temporary_directory)
+    end
+  end
 
- end
-
- public
- def receive(event)
-  return unless output?(event)
-
-  # Prepare format of Events
-  if (@format == "plain")
-     message = self.class.format_message(event)
-  elsif (@format == "json")
-     message = event.to_json
-  else
-     message = event.to_s
+  def move_file_to_bucket_async(file)
+    @logger.debug("S3: Sending the file to the upload queue.", :filename => File.basename(file))
+    @upload_queue.enq(file)
   end
 
-  if(time_file !=0)
-     @logger.debug "S3: trigger files after "+((@pass_time+60*time_file)-Time.now).to_s
+  public
+  def configure_periodic_uploader
+    @periodic_upload_thread = Thread.new do
+      LogStash::Util::set_thread_name("<S3 periodic uploader")
+
+      first_interval = true
+      Stud.interval(@time_file * 60) do
+        if first_interval == false
+          @logger.debug("S3: time_file triggered, bucketing the file")
+
+          move_file_to_bucket_async(@tempfile.path)
+          create_temporary_file
+        else
+          first_interval = false
+        end
+      end
+    end
   end
 
-  # if specific the size
-  if(size_file !=0)
+  public
+  def get_temporary_filename(page_counter = 0)
+    current_time = Time.now
+    filename = "ls.s3.#{Socket.gethostname}.#{current_time.strftime("%Y-%m-%dT%H.%M")}"
 
-    if (@tempFile.size < @size_file )
+    if @tags.size > 0
+      return File.join(@temporary_directory, "#{filename}.tag_#{@tags.join('.')}.part#{page_counter}.#{TEMPFILE_EXTENSION}")
+    else
+      return File.join(@temporary_directory, "#{filename}.part#{page_counter}.#{TEMPFILE_EXTENSION}")
+    end
+  end
 
-       @logger.debug "S3: File have size: "+@tempFile.size.to_s+" and size_file is: "+ @size_file.to_s
-       @logger.debug "S3: put event into: "+File.basename(@tempFile)
+  public
+  def receive(event)
+    return unless output?(event)
+    @codec.encode(event)
+  end
 
-       # Put the event in the file, now!
-       File.open(@tempFile, 'a') do |file|
-         file.puts message
-         file.write "\n"
-       end
+  def handle_event(event)
+    if write_events_to_multiple_files?
+      if rotate_events_log?
+        @logger.debug("S3: tempfile is too large, let's bucket it and create new file", :tempfile => File.basename(@tempfile))
 
-     else
+        move_file_to_bucket_async(@tempfile.path)
+        next_page()
+      else
+        @logger.debug("S3: tempfile file size report.", :tempfile_size => @tempfile.size, :size_file => @size_file)
+      end
 
-       @logger.debug "S3: file: "+File.basename(@tempFile)+" is too large, let's bucket it and create new file"
-       upFile(false, File.basename(@tempFile))
-       @sizeCounter += 1
-       newFile(false)
+      write_to_tempfile(event)
+    else
+      write_to_tempfile(event)
+    end
+  end
 
-     end
+  public
+  def rotate_events_log?
+    @tempfile.size > @size_file
+  end
 
-  # else we put all in one file
-  else
+  public
+  def write_events_to_multiple_files?
+    @size_file > 0
+  end
 
-    @logger.debug "S3: put event into "+File.basename(@tempFile)
-    File.open(@tempFile, 'a') do |file|
-      file.puts message
-      file.write "\n"
+  public
+  def write_to_tempfile(event)
+    begin
+      @logger.debug("S3: put event into tempfile ", :tempfile => File.basename(@tempfile))
+
+      @file_rotation_lock.synchronize do
+        @tempfile.syswrite(event)
+      end
+    rescue Errno::ENOSPC
+      @logger.error("S3: No space left in temporary directory", :temporary_directory => @temporary_directory)
+      teardown()
     end
   end
 
- end
+  public
+  def shutdown_upload_workers
+    @upload_queue << LogStash::ShutdownEvent
+  end
 
- def self.format_message(event)
-    message = "Date: #{event[LogStash::Event::TIMESTAMP]}\n"
-    message << "Source: #{event["source"]}\n"
-    message << "Tags: #{event["tags"].join(', ')}\n"
-    message << "Fields: #{event.to_hash.inspect}\n"
-    message << "Message: #{event["message"]}"
- end
+  def teardown
+    # TODO: implement stop! in the Stud gem to gracefull stop the interval loop
+    # Could also add a skip_first_interval options.
+    shutdown_upload_workers
 
+    @tempfile.close
+    finished
+  end
 end
-
-# Enjoy it, by Bistic:)
diff --git a/spec/codecs/cloudfront_spec.rb b/spec/codecs/cloudfront_spec.rb
new file mode 100644
index 00000000000..60f281aa55c
--- /dev/null
+++ b/spec/codecs/cloudfront_spec.rb
@@ -0,0 +1,80 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/codecs/cloudfront"
+require "logstash/errors"
+require "stringio"
+require "zlib"
+
+describe LogStash::Codecs::Cloudfront do
+  let!(:uncompressed_cloudfront_log) do
+    # Using format from
+    # http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html
+    str = StringIO.new
+
+    str << "#Version: 1.0\n"
+    str << "#Fields: date time x-edge-location c-ip x-event sc-bytes x-cf-status x-cf-client-id cs-uri-stem cs-uri-query c-referrer x-page-url​  c-user-agent x-sname x-sname-query x-file-ext x-sid\n"
+    str << "2010-03-12   23:51:20   SEA4   192.0.2.147   connect   2014   OK   bfd8a98bee0840d9b871b7f6ade9908f   rtmp://shqshne4jdp4b6.cloudfront.net/cfx/st​  key=value   http://player.longtailvideo.com/player.swf   http://www.longtailvideo.com/support/jw-player-setup-wizard?example=204   LNX%2010,0,32,18   -   -   -   -\n"
+    str << "2010-03-12   23:51:21   SEA4   192.0.2.222   play   3914   OK   bfd8a98bee0840d9b871b7f6ade9908f   rtmp://shqshne4jdp4b6.cloudfront.net/cfx/st​  key=value   http://player.longtailvideo.com/player.swf   http://www.longtailvideo.com/support/jw-player-setup-wizard?example=204   LNX%2010,0,32,18   myvideo   p=2&q=4   flv   1\n"
+
+    str.rewind
+    str
+  end
+
+  describe "#decode" do
+    it "should create events from a gzip file" do
+      events = []
+
+      subject.decode(compress_with_gzip(uncompressed_cloudfront_log)) do |event|
+        events << event
+      end
+
+      expect(events.size).to eq(2)
+    end
+
+    it 'should extract the metadata of the file' do
+      events = []
+
+      subject.decode(compress_with_gzip(uncompressed_cloudfront_log)) do |event|
+        events << event
+      end
+
+      expect(events.first["cloudfront_version"]).to eq("1.0")
+      expect(events.first["cloudfront_fields"]).to eq("date time x-edge-location c-ip x-event sc-bytes x-cf-status x-cf-client-id cs-uri-stem cs-uri-query c-referrer x-page-url​  c-user-agent x-sname x-sname-query x-file-ext x-sid")
+    end
+  end
+
+  describe "#extract_version" do
+    it "returns the version from a matched string" do
+      line = "#Version: 1.0"
+
+      expect(subject.extract_version(line)).to eq("1.0")
+    end
+
+    it "doesn't return anything if version isnt matched" do
+      line = "Bleh my string"
+      expect(subject.extract_version(line)).to eq(nil)
+    end
+
+    it "doesn't match if #Version is not at the beginning of the string" do
+      line = "2010-03-12   23:53:44   SEA4   192.0.2.4   stop   323914   OK   bfd8a98bee0840d9b871b7f6ade9908f #Version: 1.0 Bleh blah"
+      expect(subject.extract_version(line)).to eq(nil)
+    end
+  end
+
+  describe "#extract_fields" do
+    it "return a string with all the fields" do
+      line = "#Fields: date time x-edge-location c-ip x-event sc-bytes x-cf-status x-cf-client-id cs-uri-stem cs-uri-query c-referrer x-page-url​  c-user-agent x-sname x-sname-query x-file-ext x-sid"
+      expect(subject.extract_fields(line)).to eq("date time x-edge-location c-ip x-event sc-bytes x-cf-status x-cf-client-id cs-uri-stem cs-uri-query c-referrer x-page-url​  c-user-agent x-sname x-sname-query x-file-ext x-sid")
+    end
+
+    it "doesn't return anything if we can the fields list" do
+      line = "Bleh my string"
+      expect(subject.extract_fields(line)).to eq(nil)
+    end
+
+    it "doesnt match if #Fields: is not at the beginning of the string" do
+      line = "2010-03-12   23:53:44   SEA4   192.0.2.4   stop   323914   OK   bfd8a98bee0840d9b871b7f6ade9908f #Fields: 1.0 Bleh blah"
+      expect(subject.extract_fields(line)).to eq(nil)
+    end
+  end
+end
diff --git a/spec/codecs/gzip_lines.rb b/spec/codecs/gzip_lines.rb
new file mode 100644
index 00000000000..2c115158161
--- /dev/null
+++ b/spec/codecs/gzip_lines.rb
@@ -0,0 +1,32 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/codecs/gzip_lines"
+require "logstash/errors"
+require "stringio"
+
+
+describe LogStash::Codecs::GzipLines do
+  let!(:uncompressed_log) do
+    # Using format from
+    # http://docs.aws.amazon.com/AmazonCloudFront/latest/DeveloperGuide/AccessLogs.html
+    str = StringIO.new
+
+    str << "2010-03-12   23:51:20   SEA4   192.0.2.147   connect   2014   OK   bfd8a98bee0840d9b871b7f6ade9908f   rtmp://shqshne4jdp4b6.cloudfront.net/cfx/st​  key=value   http://player.longtailvideo.com/player.swf   http://www.longtailvideo.com/support/jw-player-setup-wizard?example=204   LNX%2010,0,32,18   -   -   -   -\n"
+    str << "2010-03-12   23:51:21   SEA4   192.0.2.222   play   3914   OK   bfd8a98bee0840d9b871b7f6ade9908f   rtmp://shqshne4jdp4b6.cloudfront.net/cfx/st​  key=value   http://player.longtailvideo.com/player.swf   http://www.longtailvideo.com/support/jw-player-setup-wizard?example=204   LNX%2010,0,32,18   myvideo   p=2&q=4   flv   1\n"
+
+    str.rewind
+    str
+  end
+
+  describe "#decode" do
+    it "should create events from a gzip file" do
+      events = []
+
+      subject.decode(compress_with_gzip(uncompressed_log)) do |event|
+        events << event
+      end
+
+      expect(events.size).to eq(2)
+    end
+  end
+end
diff --git a/spec/codecs/s3_plain_spec.rb b/spec/codecs/s3_plain_spec.rb
new file mode 100644
index 00000000000..ebde8e030d3
--- /dev/null
+++ b/spec/codecs/s3_plain_spec.rb
@@ -0,0 +1,37 @@
+require "spec_helper"
+require "logstash/event"
+require "logstash/codecs/s3_plain"
+
+describe LogStash::Codecs::S3Plain do
+  subject { LogStash::Codecs::S3Plain.new }
+
+  describe "#encode" do
+    it 'should accept a nil list for the tags' do
+      subject.on_event do |data|
+        data.should match(/\nTags:\s\n/)
+      end
+
+      subject.encode(LogStash::Event.new)
+    end
+
+    it 'should accept a list of tags' do
+      event = LogStash::Event.new({"tags" => ["elasticsearch", "logstash", "kibana"] })
+
+      subject.on_event do |data|
+        data.should match(/\nTags:\selasticsearch,\slogstash,\skibana\n/)
+      end
+
+      subject.encode(event)
+    end
+
+    it "return to_s if its not LogStash::Event" do
+      event = {"test" => "A-B-C" }
+
+      subject.on_event do |data|
+        data.should == event.to_s
+      end
+
+      subject.encode(event)
+    end
+  end
+end
diff --git a/spec/inputs/s3_spec.rb b/spec/inputs/s3_spec.rb
new file mode 100644
index 00000000000..10800c0d805
--- /dev/null
+++ b/spec/inputs/s3_spec.rb
@@ -0,0 +1,146 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/inputs/s3"
+require "logstash/errors"
+
+require "aws-sdk"
+require "stud/temporary"
+
+describe LogStash::Inputs::S3 do
+  before { AWS.stub! }
+  let(:day) { 3600 * 24 }
+  let(:settings) {
+    {
+      "access_key_id" => "1234",
+      "secret_access_key" => "secret",
+      "bucket" => "logstash-test"
+    }
+  }
+
+  describe "#list_new_files" do
+    let(:present_object) { double(:key => 'this-should-be-present', :last_modified => Time.now) }
+    let(:objects_list) {
+      [
+        double(:key => 'exclude-this-file-1', :last_modified => Time.now - 2 * day),
+        double(:key => 'exclude/logstash', :last_modified => Time.now - 2 * day),
+        present_object
+      ]
+    }
+
+    it 'should allow user to exclude files from the s3 bucket' do
+      allow_any_instance_of(AWS::S3::ObjectCollection).to receive(:with_prefix).with(nil) { objects_list }
+
+      config = LogStash::Inputs::S3.new(settings.merge({ "exclude_pattern" => "^exclude" }))
+      config.register
+      expect(config.list_new_files).to eq([present_object.key])
+    end
+
+    it 'should support not providing a exclude pattern' do
+      allow_any_instance_of(AWS::S3::ObjectCollection).to receive(:with_prefix).with(nil) { objects_list }
+
+      config = LogStash::Inputs::S3.new(settings)
+      config.register
+      expect(config.list_new_files).to eq(objects_list.map(&:key))
+    end
+
+    context "If the bucket is the same as the backup bucket" do
+      it 'should ignore files from the bucket if they match the backup prefix' do
+        objects_list = [
+          double(:key => 'mybackup-log-1', :last_modified => Time.now),
+          present_object
+        ]
+
+        allow_any_instance_of(AWS::S3::ObjectCollection).to receive(:with_prefix).with(nil) { objects_list }
+
+        config = LogStash::Inputs::S3.new(settings.merge({ 'backup_add_prefix' => 'mybackup',
+                                                           'backup_to_bucket' => settings['bucket']}))
+        config.register
+        expect(config.list_new_files).to eq([present_object.key])
+      end
+    end
+
+    it 'should ignore files older than X' do
+      allow_any_instance_of(AWS::S3::ObjectCollection).to receive(:with_prefix).with(nil) { objects_list }
+      config = LogStash::Inputs::S3.new(settings.merge({ 'backup_add_prefix' => 'exclude-this-file'}))
+
+      expect_any_instance_of(LogStash::Inputs::S3::SinceDB::File).to receive(:read).exactly(objects_list.size) { Time.now - day }
+      config.register
+
+      expect(config.list_new_files).to eq([present_object.key])
+    end
+
+    it 'should sort return object sorted by last_modification date with older first' do
+      objects = [
+        double(:key => 'YESTERDAY', :last_modified => Time.now - day),
+        double(:key => 'TODAY', :last_modified => Time.now),
+        double(:key => 'TWO_DAYS_AGO', :last_modified => Time.now - 2 * day)
+      ]
+
+      allow_any_instance_of(AWS::S3::ObjectCollection).to receive(:with_prefix).with(nil) { objects }
+
+
+      config = LogStash::Inputs::S3.new(settings)
+      config.register
+      expect(config.list_new_files).to eq(['TWO_DAYS_AGO', 'YESTERDAY', 'TODAY'])
+    end
+
+    describe "when doing backup on the s3" do
+      it 'should copy to another s3 bucket when keeping the original file' do
+        config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_bucket" => "mybackup"}))
+        config.register
+
+        s3object = double()
+        expect(s3object).to receive(:copy_to).with('test-file', :bucket => an_instance_of(AWS::S3::Bucket))
+
+        config.backup_to_bucket(s3object, 'test-file')
+      end
+
+      it 'should move to another s3 bucket when deleting the original file' do
+        config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_bucket" => "mybackup", "delete" => true }))
+        config.register
+
+        s3object = double()
+        expect(s3object).to receive(:move_to).with('test-file', :bucket => an_instance_of(AWS::S3::Bucket))
+
+        config.backup_to_bucket(s3object, 'test-file')
+      end
+
+      it 'should add the specified prefix to the backup file' do
+        config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_bucket" => "mybackup",
+                                                           "backup_add_prefix" => 'backup-' }))
+        config.register
+
+        s3object = double()
+        expect(s3object).to receive(:copy_to).with('backup-test-file', :bucket => an_instance_of(AWS::S3::Bucket))
+
+        config.backup_to_bucket(s3object, 'test-file')
+      end
+    end
+
+    it 'should support doing local backup of files' do
+      backup_dir = Dir.mktmpdir
+
+      Stud::Temporary.directory do |backup_dir|
+        Stud::Temporary.file do |source_file|
+          backup_file = File.join(backup_dir.to_s, Pathname.new(source_file.path).basename.to_s)
+
+          config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_dir" => backup_dir }))
+
+          config.backup_to_dir(source_file)
+
+          File.exists?(backup_file).should be_true
+        end
+      end
+    end
+
+    it 'should accepts a list of credentials for the aws-sdk, this is deprecated' do
+      old_credentials_settings = {
+        "credentials" => ['1234', 'secret'],
+        "bucket" => "logstash-test"
+      }
+
+      config = LogStash::Inputs::S3.new(settings.merge({ "backup_to_dir" => "/tmp/mybackup" }))
+      config.register
+    end
+  end
+end
diff --git a/spec/logstash_helpers.rb b/spec/logstash_helpers.rb
index 0439661e059..cad5344e87f 100644
--- a/spec/logstash_helpers.rb
+++ b/spec/logstash_helpers.rb
@@ -75,3 +75,14 @@ def agent(&block)
 
 end # module LogStash
 
+def compress_with_gzip(io)
+  compressed = StringIO.new('', 'r+b')
+
+  gzip = Zlib::GzipWriter.new(compressed)
+  gzip.write(io.read)
+  gzip.finish
+
+  compressed.rewind
+
+  compressed
+end # def compress_with_gzip
diff --git a/spec/outputs/s3_spec.rb b/spec/outputs/s3_spec.rb
new file mode 100644
index 00000000000..916ff331551
--- /dev/null
+++ b/spec/outputs/s3_spec.rb
@@ -0,0 +1,249 @@
+require "spec_helper"
+require "logstash/outputs/s3"
+require 'socket'
+require "aws-sdk"
+require "fileutils"
+require "stud/temporary"
+
+describe LogStash::Outputs::S3 do
+  before do
+    # We stub all the calls from S3, for more information see:
+    # http://ruby.awsblog.com/post/Tx2SU6TYJWQQLC3/Stubbing-AWS-Responses
+    AWS.stub!
+  end
+
+  let(:minimal_settings)  {  { "access_key_id" => "1234",
+                               "secret_access_key" => "secret",
+                               "bucket" => "my-bucket" } }
+
+  describe "configuration" do
+    it "should support the deprecated endpoint_region as a configuration option" do
+      config = { "endpoint_region" => "sa-east-1" }
+      s3 = LogStash::Outputs::S3.new(config)
+      expect(s3.aws_options_hash[:s3_endpoint]).to eq("s3-sa-east-1.amazonaws.com")
+    end
+
+    it "should use the depracated option before failling back to the region" do
+      config = { "region" => "us-east-1", "endpoint_region" => "sa-east-1" }
+      s3 = LogStash::Outputs::S3.new(config)
+      expect(s3.aws_options_hash[:s3_endpoint]).to eq("s3-sa-east-1.amazonaws.com")
+    end
+  end
+
+  describe "#register" do
+    it "should create the tmp directory if it doesn't exist" do
+      temporary_directory = Stud::Temporary.pathname("temporary_directory")
+
+      config = {
+        "access_key_id" => "1234",
+        "secret_access_key" => "secret",
+        "bucket" => "logstash",
+        "size_file" => 10,
+        "temporary_directory" => temporary_directory
+      }
+
+      s3 = LogStash::Outputs::S3.new(config)
+      allow(s3).to receive(:test_s3_write)
+      s3.register
+
+      expect(Dir.exist?(temporary_directory)).to eq(true)
+      FileUtils.rm_r(temporary_directory)
+    end
+
+    it "should raise a ConfigurationError if the prefix contains one or more '\^`><' characters" do
+      config = {
+        "prefix" => "`no\><^"
+      }
+
+      s3 = LogStash::Outputs::S3.new(config)
+
+      expect {
+        s3.register
+      }.to raise_error(LogStash::ConfigurationError)
+    end
+  end
+
+  describe "#generate_temporary_filename" do
+    before do
+      Socket.stub(:gethostname) { "logstash.local" }
+      Time.stub(:now) { Time.new('2015-10-09-09:00') }
+    end
+
+    it "should add tags to the filename if present" do
+      config = minimal_settings.merge({ "tags" => ["elasticsearch", "logstash", "kibana"], "temporary_directory" => "/tmp/logstash"})
+      s3 = LogStash::Outputs::S3.new(config)
+      expect(s3.get_temporary_filename).to eq("/tmp/logstash/ls.s3.logstash.local.2015-01-01T00.00.tag_elasticsearch.logstash.kibana.part0.txt")
+    end
+
+    it "should not add the tags to the filename" do
+      config = minimal_settings.merge({ "tags" => [], "temporary_directory" => "/tmp/logstash" })
+      s3 = LogStash::Outputs::S3.new(config)
+      expect(s3.get_temporary_filename(3)).to eq("/tmp/logstash/ls.s3.logstash.local.2015-01-01T00.00.part3.txt")
+    end
+
+    it "should default to the os temporary directory" do
+      config = minimal_settings.merge({ "tags" => [] })
+      s3 = LogStash::Outputs::S3.new(config)
+      expect(s3.get_temporary_filename(2)).to eq(File.join(Dir.tmpdir, "logstash", "ls.s3.logstash.local.2015-01-01T00.00.part2.txt"))
+    end
+
+    it "normalized the temp directory to include the trailing slash if missing" do
+      s3 = LogStash::Outputs::S3.new(minimal_settings.merge({ "temporary_directory" => "/tmp/logstash" }))
+      expect(s3.get_temporary_filename).to eq("/tmp/logstash/ls.s3.logstash.local.2015-01-01T00.00.part0.txt")
+    end
+  end
+
+  describe "#write_on_bucket" do
+    after(:all) do
+      File.unlink(fake_data.path)
+    end
+
+    let!(:fake_data) { Stud::Temporary.file }
+
+    let(:fake_bucket) do
+      s3 = double('S3Object')
+      s3.stub(:write)
+      s3
+    end
+
+    it "should prefix the file on the bucket if a prefix is specified" do
+      prefix = "my-prefix"
+
+      config = minimal_settings.merge({
+        "prefix" => prefix,
+        "bucket" => "my-bucket"
+      })
+
+      expect_any_instance_of(AWS::S3::ObjectCollection).to receive(:[]).with("#{prefix}#{File.basename(fake_data)}") { fake_bucket }
+
+      s3 = LogStash::Outputs::S3.new(config)
+      allow(s3).to receive(:test_s3_write)
+      s3.register
+      s3.write_on_bucket(fake_data)
+    end
+
+    it 'should use the same local filename if no prefix is specified' do
+      config = minimal_settings.merge({
+        "bucket" => "my-bucket"
+      })
+
+      expect_any_instance_of(AWS::S3::ObjectCollection).to receive(:[]).with(File.basename(fake_data)) { fake_bucket }
+
+      s3 = LogStash::Outputs::S3.new(minimal_settings)
+      allow(s3).to receive(:test_s3_write)
+      s3.register
+      s3.write_on_bucket(fake_data)
+    end
+  end
+
+  describe "#write_events_to_multiple_files?" do
+    it 'returns true if the size_file is != 0 ' do
+      s3 = LogStash::Outputs::S3.new(minimal_settings.merge({ "size_file" => 200 }))
+      expect(s3.write_events_to_multiple_files?).to eq(true)
+    end
+
+    it 'returns false if size_file is zero or not set' do
+      s3 = LogStash::Outputs::S3.new(minimal_settings)
+      expect(s3.write_events_to_multiple_files?).to eq(false)
+    end
+  end
+
+  describe "#write_to_tempfile" do
+    it "should append the event to a file" do
+      Stud::Temporary.file("logstash", "a+") do |tmp|
+        s3 = LogStash::Outputs::S3.new(minimal_settings)
+        allow(s3).to receive(:test_s3_write)
+        s3.register
+        s3.tempfile = tmp
+        s3.write_to_tempfile("test-write")
+        tmp.rewind
+        expect(tmp.read).to eq("test-write")
+      end
+    end
+  end
+
+  describe "#rotate_events_log" do
+    it "returns true if the tempfile is over the file_size limit" do
+      Stud::Temporary.file do |tmp|
+        tmp.stub(:size) { 2024001 }
+
+        s3 = LogStash::Outputs::S3.new(minimal_settings.merge({ "size_file" => 1024 }))
+        s3.tempfile = tmp
+        expect(s3.rotate_events_log?).to be(true)
+      end
+    end
+
+    it "returns false if the tempfile is under the file_size limit" do
+      Stud::Temporary.file do |tmp|
+        tmp.stub(:size) { 100 }
+
+        s3 = LogStash::Outputs::S3.new(minimal_settings.merge({ "size_file" => 1024 }))
+        s3.tempfile = tmp
+        expect(s3.rotate_events_log?).to eq(false)
+      end
+    end
+  end
+
+  describe "#move_file_to_bucket" do
+    let!(:s3) { LogStash::Outputs::S3.new(minimal_settings) }
+
+    before do
+      # Assume the AWS test credentials pass.
+      allow(s3).to receive(:test_s3_write)
+      s3.register
+    end
+
+    it "should always delete the source file" do
+      tmp = Stud::Temporary.file
+
+      allow(File).to receive(:zero?).and_return(true)
+      expect(File).to receive(:delete).with(tmp)
+
+      s3.move_file_to_bucket(tmp)
+    end
+
+    it 'should not upload the file if the size of the file is zero' do
+      temp_file = Stud::Temporary.file
+      allow(temp_file).to receive(:zero?).and_return(true)
+
+      expect(s3).not_to receive(:write_on_bucket)
+      s3.move_file_to_bucket(temp_file)
+    end
+
+    it "should upload the file if the size > 0" do
+      tmp = Stud::Temporary.file
+
+      allow(File).to receive(:zero?).and_return(false)
+      expect(s3).to receive(:write_on_bucket)
+
+      s3.move_file_to_bucket(tmp)
+    end
+  end
+
+  describe "#restore_from_crashes" do
+    it "read the temp directory and upload the matching file to s3" do
+      s3 = LogStash::Outputs::S3.new(minimal_settings.merge({ "temporary_directory" => "/tmp/logstash/" }))
+
+      expect(Dir).to receive(:[]).with("/tmp/logstash/*.txt").and_return(["/tmp/logstash/01.txt"])
+      expect(s3).to receive(:move_file_to_bucket_async).with("/tmp/logstash/01.txt")
+
+
+      s3.restore_from_crashes
+    end
+  end
+
+  describe "#receive" do
+    it "should send the event through the codecs" do
+      data = {"foo" => "bar", "baz" => {"bah" => ["a","b","c"]}, "@timestamp" => "2014-05-30T02:52:17.929Z"}
+      event = LogStash::Event.new(data)
+
+      expect_any_instance_of(LogStash::Codecs::Plain).to receive(:encode).with(event)
+
+      s3 = LogStash::Outputs::S3.new(minimal_settings)
+      allow(s3).to receive(:test_s3_write)
+      s3.register
+
+      s3.receive(event)
+    end
+  end
+end
diff --git a/spec/plugin_mixins/aws_config_spec.rb b/spec/plugin_mixins/aws_config_spec.rb
new file mode 100644
index 00000000000..d561dcf92b2
--- /dev/null
+++ b/spec/plugin_mixins/aws_config_spec.rb
@@ -0,0 +1,37 @@
+require "spec_helper"
+require "logstash/plugin_mixins/aws_config"
+require 'aws-sdk'
+
+class DummyInputAwsConfig < LogStash::Inputs::Base
+  include LogStash::PluginMixins::AwsConfig
+
+  milestone 1
+
+  def aws_service_endpoint(region)
+    { :dummy_input_aws_config_region => "#{region}.awswebservice.local" }
+  end
+end
+
+describe LogStash::PluginMixins::AwsConfig do
+  it 'should support passing credentials as key, value' do
+    settings = { 'access_key_id' => '1234',  'secret_access_key' => 'secret' }
+
+    config = DummyInputAwsConfig.new(settings)
+    config.aws_options_hash[:access_key_id].should == settings['access_key_id']
+    config.aws_options_hash[:secret_access_key].should == settings['secret_access_key']
+  end
+
+  it 'should support reading configuration from a yaml file' do
+    settings = { 'aws_credentials_file' => File.join(File.dirname(__FILE__), '..', 'support/aws_credentials_file_sample_test.yml') }
+    config = DummyInputAwsConfig.new(settings)
+    config.aws_options_hash[:access_key_id].should == '1234'
+    config.aws_options_hash[:secret_access_key].should == 'secret'
+  end
+
+  it 'should call the class to generate the endpoint configuration' do
+    settings = { 'access_key_id' => '1234',  'secret_access_key' => 'secret', 'region' => 'us-west-2' }
+
+    config = DummyInputAwsConfig.new(settings)
+    config.aws_options_hash[:dummy_input_aws_config_region].should == "us-west-2.awswebservice.local"
+  end
+end
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index 49da3cdc7f7..588e9b859b5 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -47,4 +47,3 @@ def []=(str, value)
   config.extend LogStashHelper
   config.filter_run_excluding :redis => true, :socket => true, :performance => true, :elasticsearch => true, :broken => true, :export_cypher => true
 end
-
diff --git a/spec/support/aws_credentials_file_sample_test.yml b/spec/support/aws_credentials_file_sample_test.yml
new file mode 100644
index 00000000000..d57bd0e6c18
--- /dev/null
+++ b/spec/support/aws_credentials_file_sample_test.yml
@@ -0,0 +1,2 @@
+:access_key_id: '1234'
+:secret_access_key: secret
diff --git a/test-log-update.gz b/test-log-update.gz
new file mode 100644
index 00000000000..e69de29bb2d
diff --git a/tools/Gemfile.jruby-1.9.lock b/tools/Gemfile.jruby-1.9.lock
index 172b1472108..eae9a236261 100644
--- a/tools/Gemfile.jruby-1.9.lock
+++ b/tools/Gemfile.jruby-1.9.lock
@@ -1,5 +1,5 @@
 PATH
-  remote: /Users/colin/dev/src/elasticsearch/logstash
+  remote: /Users/ph/es/logstash
   specs:
     logstash (1.5.0.dev-java)
       addressable
@@ -67,18 +67,18 @@ GEM
       tzinfo (~> 1.1)
     addressable (2.3.6)
     atomic (1.1.16-java)
-    avl_tree (1.1.3)
+    avl_tree (1.2.0)
     awesome_print (1.2.0)
-    aws-sdk (1.54.0)
-      aws-sdk-v1 (= 1.54.0)
-    aws-sdk-v1 (1.54.0)
+    aws-sdk (1.55.0)
+      aws-sdk-v1 (= 1.55.0)
+    aws-sdk-v1 (1.55.0)
       json (~> 1.4)
       nokogiri (>= 1.4.4)
     axiom-types (0.1.1)
       descendants_tracker (~> 0.0.4)
       ice_nine (~> 0.11.0)
       thread_safe (~> 0.3, >= 0.3.1)
-    backports (3.6.1)
+    backports (3.6.3)
     beefcake (0.3.7)
     bindata (2.1.0)
     buftok (0.1)
@@ -114,7 +114,7 @@ GEM
     extlib (0.9.16)
     faraday (0.9.0)
       multipart-post (>= 1.2, < 3)
-    ffi (1.9.5-java)
+    ffi (1.9.6)
     ffi-rzmq (1.0.0)
       ffi
     filewatch (0.5.1)
@@ -156,9 +156,9 @@ GEM
       virtus (~> 1.0)
     metaclass (0.0.4)
     method_source (0.8.2)
-    metriks (0.9.9.6)
+    metriks (0.9.9.7)
       atomic (~> 1.0)
-      avl_tree (~> 1.1.2)
+      avl_tree (~> 1.2.0)
       hitimes (~> 1.1)
     mime-types (1.25.1)
     minitest (5.4.2)
@@ -233,97 +233,6 @@ GEM
     tins (1.3.3)
     treetop (1.4.15)
       polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
-      polyglot (>= 0.3.1)
       polyglot (>= 0.3.1)
     twitter (5.0.0.rc.1)
       buftok (~> 0.1.0)
