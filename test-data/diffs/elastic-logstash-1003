diff --git a/lib/cache.rb b/lib/cache.rb
new file mode 100644
index 00000000000..4c32d1be540
--- /dev/null
+++ b/lib/cache.rb
@@ -0,0 +1,320 @@
+#! /usr/bin/env ruby
+#
+# Copyright (C) 2002  Yoshinori K. Okuji <okuji@enbug.org>
+#
+# You may redistribute it and/or modify it under the same term as Ruby.
+
+# Cache manager based on the LRU algorithm.
+class Cache
+
+  CACHE_OBJECT = Struct.new('CacheObject', :content, :size, :atime)
+  CACHE_VERSION = '0.3'
+
+  include Enumerable
+  
+  def self.version
+    CACHE_VERSION
+  end
+
+  # initialize(max_obj_size = nil, max_size = nil, max_num = nil,
+  #            expiration = nil, &hook)
+  # initialize(hash, &hook)
+  def initialize(*args, &hook)
+    if args.size == 1 and args[0].kind_of?(Hash)
+      @max_obj_size = @max_size = @max_num = @expiration = nil
+      args[0].each do |k, v|
+	k = k.intern if k.respond_to?(:intern)
+	case k
+	when :max_obj_size
+	  @max_obj_size = v
+	when :max_size
+	  @max_size = v
+	when :max_num
+	  @max_num = v
+	when :expiration
+	  @expiration = v
+	end
+      end
+    else
+      @max_obj_size, @max_size, @max_num, @expiration = args
+    end
+
+    # Sanity checks.
+    if @max_obj_size and @max_size and @max_obj_size > @max_size
+      raise ArgumentError, "max_obj_size exceeds max_size (#{@max_obj_size} > #{@max_size})"
+    end
+    if @max_obj_size and @max_obj_size <= 0
+      raise ArgumentError, "invalid max_obj_size `#{@max_obj_size}'"
+    end
+    if @max_size and @max_size <= 0
+      raise ArgumentError, "invalid max_size `#{@max_size}'"
+    end
+    if @max_num and @max_num <= 0
+      raise ArgumentError, "invalid max_num `#{@max_num}'"
+    end
+    if @expiration and @expiration <= 0
+      raise ArgumentError, "invalid expiration `#{@expiration}'"
+    end
+    
+    @hook = hook
+    
+    @objs = {}
+    @size = 0
+    @list = []
+    
+    @hits = 0
+    @misses = 0
+  end
+
+  attr_reader :max_obj_size, :max_size, :max_num, :expiration
+
+  def cached?(key)
+    @objs.include?(key)
+  end
+  alias :include? :cached?
+  alias :member? :cached?
+  alias :key? :cached?
+  alias :has_key? :cached?
+
+  def cached_value?(val)
+    self.each_value do |v|
+      return true if v == val
+    end
+    false
+  end
+  alias :has_value? :cached_value?
+  alias :value? :cached_value?
+
+  def index(val)
+    self.each_pair do |k,v|
+      return k if v == val
+    end
+    nil
+  end
+
+  def keys
+    @objs.keys
+  end
+
+  def length
+    @objs.length
+  end
+  alias :size :length
+
+  def to_hash
+    @objs.dup
+  end
+
+  def values
+    @objs.collect {|key, obj| obj.content}
+  end
+  
+  def invalidate(key)
+    obj = @objs[key]
+    if obj
+      if @hook
+	@hook.call(key, obj.content)
+      end
+      @size -= obj.size
+      @objs.delete(key)
+      @list.each_index do |i|
+	if @list[i] == key
+	  @list.delete_at(i)
+	  break
+	end
+      end
+    elsif block_given?
+      return yield(key)
+    end
+    obj.content
+  end
+  alias :delete :invalidate
+  
+  def invalidate_all()
+    if @hook
+      @objs.each do |key, obj|
+	@hook.call(key, obj)
+      end
+    end
+
+    @objs.clear
+    @list.clear
+    @size = 0
+  end
+  alias :clear :invalidate_all
+  
+  def quiet_delete(key)
+    obj = @objs[key]
+    if obj
+      if @hook
+	#johnar: commenting out hook call
+	#@hook.call(key, obj.content)
+      end
+      @size -= obj.size
+      @objs.delete(key)
+      @list.each_index do |i|
+	if @list[i] == key
+	  @list.delete_at(i)
+	  break
+	end
+      end
+    elsif block_given?
+      return yield(key)
+    end
+    obj.content
+  end
+  alias :delete :invalidate
+  
+  def expire()
+    if @expiration
+      now = Time.now.to_i
+      @list.each_index do |i|
+	key = @list[i]
+	
+	break unless @objs[key].atime + @expiration <= now
+	self.invalidate(key)
+      end
+    end
+#    GC.start
+  end
+	
+  def [](key)
+    self.expire()
+    
+    unless @objs.include?(key)
+      @misses += 1
+      return nil
+    end
+    
+    obj = @objs[key]
+    obj.atime = Time.now.to_i
+
+    @list.each_index do |i|
+      if @list[i] == key
+	@list.delete_at(i)
+	break
+      end
+    end
+    @list.push(key)
+
+    @hits += 1
+    obj.content
+  end
+  
+  def []=(key, obj)
+    self.expire()
+    
+    if self.cached?(key)
+      #johnar: change to quiet_delete so hook isn't called on updates
+	  #self.invalidate(key)
+	  self.quiet_delete(key)
+    end
+
+    size = obj.to_s.size
+    if @max_obj_size and @max_obj_size < size
+      if $DEBUG
+	$stderr.puts("warning: `#{obj.inspect}' isn't cached because its size exceeds #{@max_obj_size}")
+      end
+      return obj
+    end
+    if @max_obj_size.nil? and @max_size and @max_size < size
+      if $DEBUG
+	$stderr.puts("warning: `#{obj.inspect}' isn't cached because its size exceeds #{@max_size}")
+      end
+      return obj
+    end
+      
+    if @max_num and @max_num == @list.size
+      self.invalidate(@list.first)
+    end
+
+    @size += size
+    if @max_size
+      while @max_size < @size
+	self.invalidate(@list.first)
+      end
+    end
+
+    @objs[key] = CACHE_OBJECT.new(obj, size, Time.now.to_i)
+    @list.push(key)
+
+    obj
+  end
+
+  def store(key, value)
+    self[key] = value
+  end
+
+  def each_pair
+    @objs.each do |key, obj|
+      yield key, obj.content
+    end
+    self
+  end
+  alias :each :each_pair
+
+  def each_key
+    @objs.each_key do |key|
+      yield key
+    end
+    self
+  end
+
+  def each_value
+    @objs.each_value do |obj|
+      yield obj.content
+    end
+    self
+  end
+
+  def empty?
+    @objs.empty?
+  end
+
+  def fetch(key, default = nil)
+    val = self[key]
+    if val.nil?
+      if default
+	val = self[key] = default
+      elsif block_given?
+	val = self[key] = yield(key)
+      else
+	raise IndexError, "invalid key `#{key}'"
+      end
+    end
+    val
+  end
+  
+  # The total size of cached objects, the number of cached objects,
+  # the number of cache hits, and the number of cache misses.
+  def statistics()
+    [@size, @list.size, @hits, @misses]
+  end
+end
+
+# Run a test, if executed.
+if __FILE__ == $0
+  cache = Cache.new(100 * 1024, 100 * 1024 * 1024, 256, 1)
+  1000.times do
+    key = rand(1000)
+    cache[key] = key.to_s
+  end
+  1000.times do
+    key = rand(1000)
+    puts cache[key]
+  end
+  sleep 1
+  1000.times do
+    key = rand(1000)
+    puts cache[key]
+  end
+  
+  stat = cache.statistics()
+  hits = stat[2]
+  misses = stat[3]
+  ratio = hits.to_f / (hits + misses)
+  
+  puts "Total size:\t#{stat[0]}"
+  puts "Number:\t\t#{stat[1]}"
+  puts "Hit ratio:\t#{ratio * 100}% (#{hits} / #{hits + misses})"
+end
diff --git a/lib/logstash/filters/multiline.rb b/lib/logstash/filters/multiline.rb
index c2d604ccad1..197695f709a 100644
--- a/lib/logstash/filters/multiline.rb
+++ b/lib/logstash/filters/multiline.rb
@@ -7,6 +7,8 @@
 require "logstash/filters/base"
 require "logstash/namespace"
 require "set"
+require "cache"  #rubygem 'ruby-cache'
+require "stud/interval" # gem stud
 
 # The multiline filter is for combining multiple events from a single source
 # into the same event.
@@ -31,7 +33,7 @@
 # the field is part of a multi-line event
 #
 # The 'what' must be "previous" or "next" and indicates the relation
-# to the multi-line event.
+# to the multi-line event. "streamcache" is for reassembling ongoing streams when messages arrive out-of-order.
 #
 # The 'negate' can be "true" or "false" (defaults false). If true, a 
 # message not matching the pattern will constitute a match of the multiline
@@ -69,8 +71,14 @@ class LogStash::Filters::Multiline < LogStash::Filters::Base
   config :pattern, :validate => :string, :required => true
 
   # If the pattern matched, does event belong to the next or previous event?
-  config :what, :validate => ["previous", "next"], :required => true
+  config :what, :validate => ["previous", "next", "streamcache"], :required => true
 
+  # Cache size, the maximum number of cached messages
+  config :cache_size, :validate => :number, :default => 50000
+  
+  # Cache TTL (seconds)
+  config :cache_ttl, :validate => :number, :default => 5
+  
   # Negate the regexp pattern ('if not matched')
   config :negate, :validate => :boolean, :default => false
   
@@ -118,6 +126,29 @@ def initialize(config = {})
     # This filter needs to keep state.
     @types = Hash.new { |h,k| h[k] = [] }
     @pending = Hash.new
+	
+	# Hook is called when cached objects are invalidated.
+	# This will send reassembled messages back to the pipeline.
+	hook = Proc.new {|key, event| 
+		@logger.info("Calling hook for event output.", :key => key, :event => event)
+		filter_matched(event)
+	}
+	
+	# Create cache, set the maximum number of cached objects and the expiration time 
+	@cache = Cache.new(nil, nil, @cache_size, @cache_ttl, &hook)
+	@logger.debug("StreamCache created.")
+	
+	# this will periodically go through and wipe out expired cache members.
+	def cache_expire
+		@logger.debug("Expiring cache items...")
+		@cache.expire
+	end
+	
+	# Set up the periodic cache expiry thread. Depending on event arrival timing, expiry can take up to 2x TTL time... How to fix?  expire more often?  1/2 the TTL time? Every second?
+    @expire_thread = Thread.new { Stud.interval(@cache_ttl) { cache_expire } }
+	
+
+
   end # def initialize
 
   public
@@ -161,8 +192,7 @@ def filter(event)
     key = event.sprintf(@stream_identity)
     pending = @pending[key]
 
-    @logger.debug("Multiline", :pattern => @pattern, :message => event["message"],
-                  :match => match, :negate => @negate)
+    @logger.debug("Multiline", :pattern => @pattern, :message => event["message"], :match => match, :negate => @negate)
 
     # Add negate option
     match = (match and !@negate) || (!match and @negate)
@@ -183,7 +213,7 @@ def filter(event)
         # this line is not part of the previous event
         # if we have a pending event, it's done, send it.
         # put the current event into pending
-        if pending
+		if pending
           tmp = event.to_hash
           event.overwrite(pending)
           @pending[key] = LogStash::Event.new(tmp)
@@ -213,6 +243,29 @@ def filter(event)
           @pending.delete(key)
         end
       end # if/else match
+	when "streamcache"
+		
+		key = event.sprintf(@stream_identity)
+		cached = @cache[key]
+		#first lookup the stream_identity to see if the stream is already in cache.
+		if cached
+			#if it is, append new message and re-cache
+			event.tag "multiline"
+			cached.append(event)
+			#flatten the event before storing
+			cached["message"] = cached["message"].join("\n") if cached["message"].is_a?(Array)
+			cached["@timestamp"] = cached["@timestamp"].first if cached["@timestamp"].is_a?(Array)
+			@logger.debug("Appending cache item 'what'.", :what => key, :event => cached)
+			@cache.store(key, cached)
+			event.cancel
+		
+		else			
+			#if it's not, add this message to the cache
+			cached = LogStash::Event.new(event.to_hash)
+			@logger.debug("Adding cache item 'what'.", :what => key)
+			@cache.store(key, cached)
+			event.cancel
+		end
     else
       # TODO(sissel): Make this part of the 'register' method.
       @logger.warn("Unknown multiline 'what' value.", :what => @what)
@@ -221,10 +274,16 @@ def filter(event)
     if !event.cancelled?
       event["message"] = event["message"].join("\n") if event["message"].is_a?(Array)
       event["@timestamp"] = event["@timestamp"].first if event["@timestamp"].is_a?(Array)
+	  @logger.info("Outputting event", :message => event["message"])
       filter_matched(event) if match
     end
+	 
   end # def filter
 
+
+ 
+   
+  
   # Flush any pending messages. This is generally used for unit testing only.
   #
   # Note: flush is disabled now; it is preferable to use the multiline codec.
@@ -238,4 +297,6 @@ def __flush
     @pending.clear
     return events
   end # def flush
+  
+ 
 end # class LogStash::Filters::Multiline
diff --git a/lib/logstash/inputs/udp.rb b/lib/logstash/inputs/udp.rb
index 49ebf65eac8..184b38bbe59 100644
--- a/lib/logstash/inputs/udp.rb
+++ b/lib/logstash/inputs/udp.rb
@@ -21,6 +21,12 @@ class LogStash::Inputs::Udp < LogStash::Inputs::Base
 
   # Buffer size
   config :buffer_size, :validate => :number, :default => 8192
+  
+  # I/O workers
+  config :workers, :validate => :number, :default => 2
+  
+  # Queue depth
+  config :queue_size, :validate => :number, :default => 2000
 
   public
   def initialize(params)
@@ -35,6 +41,7 @@ def register
 
   public
   def run(output_queue)
+	@output_queue = output_queue
     begin
       # udp server
       udp_listener(output_queue)
@@ -58,13 +65,24 @@ def udp_listener(output_queue)
     @udp = UDPSocket.new(Socket::AF_INET)
     @udp.bind(@host, @port)
 
+	@input_to_worker = SizedQueue.new(@queue_size)
+	@worker_to_output = SizedQueue.new(@queue_size)	
+
+	@input_workers = @workers.times do
+		Thread.new { inputworker }
+	end
+	
+	#johnarnold: not adding output workers unless I see a reason... one should be fine.
+	#@output_workers = @workers.times do
+		Thread.new { outputworker }
+	#end
+
     loop do
+		#collect datagram message and add to queue
       payload, client = @udp.recvfrom(@buffer_size)
-      @codec.decode(payload) do |event|
-        decorate(event)
-        event["host"] ||= client[3]
-        output_queue << event
-      end
+	  work = [ payload, client ]
+	  @input_to_worker.push(work)
+          
     end
   ensure
     if @udp
@@ -72,7 +90,52 @@ def udp_listener(output_queue)
       @udp.close_write rescue nil
     end
   end # def udp_listener
-
+  
+  def inputworker
+    LogStash::Util::set_thread_name("|worker")
+    begin
+      while true
+        work = @input_to_worker.pop
+		payload = work[0]
+		client = work[1]
+        if payload == LogStash::ShutdownSignal
+          @input_to_worker.push(work)
+          break
+        end
+		
+		@codec.decode(payload) do |event|
+        decorate(event)
+		
+        event["host"] = client[3]
+		@worker_to_output.push(event)
+		
+		end
+      end
+    rescue => e
+      @logger.error("Exception in inputworker", "exception" => e, "backtrace" => e.backtrace)
+    end
+  end # def inputworker
+  
+  
+ def outputworker
+    LogStash::Util::set_thread_name("|worker")
+    begin
+      while true
+        event = @worker_to_output.pop
+		
+        if event == LogStash::ShutdownSignal
+          @worker_to_output.push(payload)
+          break
+        end
+	
+		@output_queue << event
+		
+      end
+    rescue => e
+      @logger.error("Exception in inputworker", "exception" => e, "backtrace" => e.backtrace)
+    end
+  end # def outputworker 
+  
   public
   def teardown
     @udp.close if @udp && !@udp.closed?
