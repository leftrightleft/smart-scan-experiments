diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index d59a3127c77..0f32dcde11f 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -2,11 +2,6 @@
 require "logstash/environment"
 require "logstash/errors"
 require "logstash/config/cpu_core_strategy"
-require "logstash/instrument/collector"
-require "logstash/instrument/metric"
-require "logstash/instrument/periodic_pollers"
-require "logstash/instrument/collector"
-require "logstash/instrument/metric"
 require "logstash/pipeline"
 require "logstash/webserver"
 require "logstash/event_dispatcher"
@@ -19,6 +14,8 @@
 require "socket"
 require "securerandom"
 
+java_import org.logstash.instrument.witness.Witness
+
 LogStash::Environment.load_locale!
 
 class LogStash::Agent
@@ -34,6 +31,8 @@ class LogStash::Agent
   #   :auto_reload [Boolean] - enable reloading of pipelines
   #   :reload_interval [Integer] - reload pipelines every X seconds
   def initialize(settings = LogStash::SETTINGS, source_loader = nil)
+    witness = Witness.new
+    Witness.setInstance(witness)
     @logger = self.class.logger
     @settings = settings
     @auto_reload = setting("config.reload.automatic")
@@ -61,16 +60,12 @@ def initialize(settings = LogStash::SETTINGS, source_loader = nil)
     # Normalize time interval to seconds
     @reload_interval = setting("config.reload.interval") / 1_000_000_000.0
 
-    @collect_metric = setting("metric.collect")
-
-    # Create the collectors and configured it with the library
-    configure_metrics_collectors
+    #TODO: jake - start periodic pollers
 
-    @state_resolver = LogStash::StateResolver.new(metric)
+    @state_resolver = LogStash::StateResolver.new
 
-    @pipeline_reload_metric = metric.namespace([:stats, :pipelines])
-    @instance_reload_metric = metric.namespace([:stats, :reloads])
-    initialize_agent_metrics
+    @witness_pipelines = witness.pipelines
+    @witness_reloads = witness.reloads
 
     @dispatcher = LogStash::EventDispatcher.new(self)
     LogStash::PLUGIN_REGISTRY.hooks.register_emitter(self.class, dispatcher)
@@ -183,7 +178,6 @@ def uptime
   end
 
   def shutdown
-    stop_collecting_metrics
     stop_webserver
     transition_to_stopped
     converge_result = shutdown_pipelines
@@ -403,30 +397,6 @@ def stop_webserver
         @webserver_thread.join
       end
     end
-  end
-
-  def configure_metrics_collectors
-    @collector = LogStash::Instrument::Collector.new
-
-    @metric = if collect_metrics?
-      @logger.debug("Agent: Configuring metric collection")
-      LogStash::Instrument::Metric.new(@collector)
-    else
-      LogStash::Instrument::NullMetric.new(@collector)
-    end
-
-    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(@metric, settings.get("queue.type"), self)
-    @periodic_pollers.start
-  end
-
-  def stop_collecting_metrics
-    @periodic_pollers.stop
-  end
-
-  def collect_metrics?
-    @collect_metric
-  end
-
   def shutdown_pipelines
     logger.debug("Shutting down all pipelines", :pipelines_count => pipelines_count)
 
@@ -471,51 +441,27 @@ def update_metrics(converge_result)
 
   def update_success_metrics(action, action_result)
     case action
-      when LogStash::PipelineAction::Create
-        # When a pipeline is successfully created we create the metric
-        # place holder related to the lifecycle of the pipeline
-        initialize_pipeline_metrics(action)
       when LogStash::PipelineAction::Reload
         update_successful_reload_metrics(action, action_result)
     end
   end
 
   def update_failures_metrics(action, action_result)
-    if action.is_a?(LogStash::PipelineAction::Create)
-      # force to create the metric fields
-      initialize_pipeline_metrics(action)
-    end
 
-    @instance_reload_metric.increment(:failures)
+    @witness_reloads.failure
 
-    @pipeline_reload_metric.namespace([action.pipeline_id, :reloads]).tap do |n|
-      n.increment(:failures)
-      n.gauge(:last_error, { :message => action_result.message, :backtrace => action_result.backtrace})
-      n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
-    end
-  end
-
-  def initialize_agent_metrics
-    @instance_reload_metric.increment(:successes, 0)
-    @instance_reload_metric.increment(:failures, 0)
-  end
-
-  def initialize_pipeline_metrics(action)
-    @pipeline_reload_metric.namespace([action.pipeline_id, :reloads]).tap do |n|
-      n.increment(:successes, 0)
-      n.increment(:failures, 0)
-      n.gauge(:last_error, nil)
-      n.gauge(:last_success_timestamp, nil)
-      n.gauge(:last_failure_timestamp, nil)
-    end
+    witness_pipeline_reloads = @witness_pipelines.pipeline(action.pipeline_id).reloads
+    witness_pipeline_reloads.failure
+    witness_pipeline_reloads.error.message(action_result.message)
+    witness_pipeline_reloads.error.backtrace(action_result.backtrace.to_s)
+    witness_pipeline_reloads.last_failure_timestamp(LogStash::Timestamp.now)
   end
 
   def update_successful_reload_metrics(action, action_result)
-    @instance_reload_metric.increment(:successes)
+    @witness_reloads.success
 
-    @pipeline_reload_metric.namespace([action.pipeline_id, :reloads]).tap do |n|
-      n.increment(:successes)
-      n.gauge(:last_success_timestamp, action_result.executed_at)
-    end
+    witness_pipeline_reloads = @witness_pipelines.pipeline(action.pipeline_id).reloads
+    witness_pipeline_reloads.success
+    witness_pipeline_reloads.last_success_timestamp(action_result.executed_at)
   end
 end # class LogStash::Agent
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 4df9972e5b5..d6fd83899b5 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -32,7 +32,6 @@ module Environment
            Setting::Boolean.new("config.reload.automatic", false),
            Setting::TimeValue.new("config.reload.interval", "3s"), # in seconds
            Setting::Boolean.new("config.support_escapes", false),
-           Setting::Boolean.new("metric.collect", true),
             Setting::String.new("pipeline.id", "main"),
            Setting::Boolean.new("pipeline.system", false),
    Setting::PositiveInteger.new("pipeline.workers", LogStash::Config::CpuCoreStrategy.maximum),
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
deleted file mode 100644
index 4971695a2c9..00000000000
--- a/logstash-core/lib/logstash/instrument/collector.rb
+++ /dev/null
@@ -1,69 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/snapshot"
-require "logstash/instrument/metric_store"
-require "logstash/util/loggable"
-require "concurrent/timer_task"
-require "observer"
-require "singleton"
-require "thread"
-
-module LogStash module Instrument
-  # The Collector is the single point of reference for all
-  # the metrics collection inside logstash, the metrics library will make
-  # direct calls to this class.
-  class Collector
-    include LogStash::Util::Loggable
-
-    SNAPSHOT_ROTATION_TIME_SECS = 1 # seconds
-    SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS = 10 * 60 # seconds
-
-    attr_accessor :agent
-
-    def initialize
-      @metric_store = MetricStore.new
-      @agent = nil
-    end
-
-    # The metric library will call this unique interface
-    # its the job of the collector to update the store with new metric
-    # of update the metric
-    #
-    # If there is a problem with the key or the type of metric we will record an error
-    # but we won't stop processing events, theses errors are not considered fatal.
-    #
-    def push(namespaces_path, key, type, *metric_type_params)
-      begin
-        get(namespaces_path, key, type).execute(*metric_type_params)
-      rescue MetricStore::NamespacesExpectedError => e
-        logger.error("Collector: Cannot record metric", :exception => e)
-      rescue NameError => e
-        logger.error("Collector: Cannot create concrete class for this metric type",
-                     :type => type,
-                     :namespaces_path => namespaces_path,
-                     :key => key,
-                     :metrics_params => metric_type_params,
-                     :exception => e,
-                     :stacktrace => e.backtrace)
-      end
-    end
-
-    def get(namespaces_path, key, type)
-      @metric_store.fetch_or_store(namespaces_path, key) do
-        LogStash::Instrument::MetricType.create(type, namespaces_path, key)
-      end
-    end
-
-    # Snapshot the current Metric Store and return it immediately,
-    # This is useful if you want to get access to the current metric store without
-    # waiting for a periodic call.
-    #
-    # @return [LogStash::Instrument::MetricStore]
-    def snapshot_metric
-      Snapshot.new(@metric_store.dup)
-    end
-
-    def clear(keypath)
-      @metric_store.prune(keypath)
-    end
-  end
-end; end
diff --git a/logstash-core/lib/logstash/instrument/global_metrics.rb b/logstash-core/lib/logstash/instrument/global_metrics.rb
deleted file mode 100644
index dde654d213e..00000000000
--- a/logstash-core/lib/logstash/instrument/global_metrics.rb
+++ /dev/null
@@ -1,13 +0,0 @@
-class GlobalMetrics
-  class Stats(metric)
-    @metric = metric
-  end
-
-  def initialize(metric)
-    @metric = metric
-
-    @pipeline_reloads = metric.namespace([:stats, :pipelines])
-  end
-
-
-end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/instrument/metric.rb b/logstash-core/lib/logstash/instrument/metric.rb
deleted file mode 100644
index d31a8613dad..00000000000
--- a/logstash-core/lib/logstash/instrument/metric.rb
+++ /dev/null
@@ -1,105 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/collector"
-require "concurrent"
-
-module LogStash module Instrument
-  class MetricException < Exception; end
-  class MetricNoKeyProvided < MetricException; end
-  class MetricNoBlockProvided < MetricException; end
-  class MetricNoNamespaceProvided < MetricException; end
-
-  # This class provide the interface between the code, the collector and the format
-  # of the recorded metric.
-  class Metric
-    attr_reader :collector
-
-    def initialize(collector)
-      @collector = collector
-    end
-
-    def increment(namespace, key, value = 1)
-      self.class.validate_key!(key)
-      collector.push(namespace, key, :counter, :increment, value)
-    end
-
-    def decrement(namespace, key, value = 1)
-      self.class.validate_key!(key)
-      collector.push(namespace, key, :counter, :decrement, value)
-    end
-
-    def gauge(namespace, key, value)
-      self.class.validate_key!(key)
-      collector.push(namespace, key, :gauge, :set, value)
-    end
-
-    def time(namespace, key)
-      self.class.validate_key!(key)
-
-      if block_given?
-        timer = TimedExecution.new(self, namespace, key)
-        content = yield
-        timer.stop
-        return content
-      else
-        TimedExecution.new(self, namespace, key)
-      end
-    end
-
-    def report_time(namespace, key, duration)
-      self.class.validate_key!(key)
-      collector.push(namespace, key, :counter, :increment, duration)
-    end
-
-    # This method return a metric instance tied to a specific namespace
-    # so instead of specifying the namespace on every call.
-    #
-    # Example:
-    #   metric.increment(:namespace, :mykey, 200)
-    #   metric.increment(:namespace, :mykey_2, 200)
-    #
-    #   namespaced_metric = metric.namespace(:namespace)
-    #   namespaced_metric.increment(:mykey, 200)
-    #   namespaced_metric.increment(:mykey_2, 200)
-    # ```
-    #
-    # @param name [Array<String>] Name of the namespace
-    # @param name [String] Name of the namespace
-    def namespace(name)
-      raise MetricNoNamespaceProvided if name.nil? || name.empty?
-
-      NamespacedMetric.new(self, name)
-    end
-
-    def self.validate_key!(key)
-      raise MetricNoKeyProvided if key.nil? || key.empty?
-    end
-
-    private
-    # Allow to calculate the execution of a block of code.
-    # This class support 2 differents syntax a block or the return of
-    # the object itself, but in the later case the metric won't be recorded
-    # Until we call `#stop`.
-    #
-    # @see LogStash::Instrument::Metric#time
-    class TimedExecution
-      MILLISECONDS = 1_000.0.freeze
-
-      def initialize(metric, namespace, key)
-        @metric = metric
-        @namespace = namespace
-        @key = key
-        start
-      end
-
-      def start
-        @start_time = Time.now
-      end
-
-      def stop
-        execution_time = (MILLISECONDS * (Time.now - @start_time)).to_i
-        @metric.report_time(@namespace, @key, execution_time)
-        execution_time
-      end
-    end
-  end
-end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_store.rb b/logstash-core/lib/logstash/instrument/metric_store.rb
deleted file mode 100644
index 09e803a46fa..00000000000
--- a/logstash-core/lib/logstash/instrument/metric_store.rb
+++ /dev/null
@@ -1,326 +0,0 @@
-# encoding: utf-8
-require "concurrent"
-require "logstash/instrument/metric_type"
-require "thread"
-
-module LogStash module Instrument
-  # The Metric store the data structure that make sure the data is
-  # saved in a retrievable way, this is a wrapper around multiples ConcurrentHashMap
-  # acting as a tree like structure.
-  class MetricStore
-    class NamespacesExpectedError < StandardError; end
-    class MetricNotFound < StandardError; end
-
-    KEY_PATH_SEPARATOR = "/".freeze
-
-    # Lets me a bit flexible on the coma usage in the path
-    # definition
-    FILTER_KEYS_SEPARATOR = /\s?*,\s*/.freeze
-
-    def initialize
-      # We keep the structured cache to allow
-      # the api to search the content of the differents nodes
-      @store = Concurrent::Map.new
-
-      # This hash has only one dimension
-      # and allow fast retrieval of the metrics
-      @fast_lookup = Concurrent::Map.new
-
-      # This Mutex block the critical section for the
-      # structured hash, it block the zone when we first insert a metric
-      # in the structured hash or when we query it for search or to make
-      # the result available in the API.
-      @structured_lookup_mutex = Mutex.new
-    end
-
-    # This method use the namespace and key to search the corresponding value of
-    # the hash, if it doesn't exist it will create the appropriate namespaces
-    # path in the hash and return `new_value`
-    #
-    # @param [Array] The path where the values should be located
-    # @param [Symbol] The metric key
-    # @return [Object] Return the new_value of the retrieve object in the tree
-    def fetch_or_store(namespaces, key, default_value = nil)
-
-      # We first check in the `@fast_lookup` store to see if we have already see that metrics before,
-      # This give us a `o(1)` access, which is faster than searching through the structured
-      # data store (Which is a `o(n)` operation where `n` is the number of element in the namespace and
-      # the value of the key). If the metric is already present in the `@fast_lookup`, then that value is sent
-      # back directly to the caller.
-      #
-      # BUT. If the value is not present in the `@fast_lookup` the value will be inserted and we assume that we don't
-      # have it in the `@metric_store` for structured search so we add it there too.
-
-      value = @fast_lookup.get(namespaces.dup << key)
-      if value.nil?
-        value = block_given? ? yield(key) : default_value
-        @fast_lookup.put(namespaces.dup << key, value)
-        @structured_lookup_mutex.synchronize do
-            # If we cannot find the value this mean we need to save it in the store.
-          fetch_or_store_namespaces(namespaces).fetch_or_store(key, value)
-        end
-      end
-      return value;
-    end
-
-    # This method allow to retrieve values for a specific path,
-    # This method support the following queries
-    #
-    # stats/pipelines/pipeline_X
-    # stats/pipelines/pipeline_X,pipeline_2
-    # stats/os,jvm
-    #
-    # If you use the `,` on a key the metric store will return the both values at that level
-    #
-    # The returned hash will keep the same structure as it had in the `Concurrent::Map`
-    # but will be a normal ruby hash. This will allow the api to easily serialize the content
-    # of the map
-    #
-    # @param [Array] The path where values should be located
-    # @return [Hash]
-    def get_with_path(path)
-      get(*key_paths(path))
-    end
-
-    # Similar to `get_with_path` but use symbols instead of string
-    #
-    # @param [Array<Symbol>]
-    # @return [Hash]
-    def get(*key_paths)
-      # Normalize the symbols access
-      key_paths.map(&:to_sym)
-      new_hash = Hash.new
-
-      @structured_lookup_mutex.synchronize do
-        get_recursively(key_paths, @store, new_hash)
-      end
-
-      new_hash
-    end
-
-    # Retrieve values like `get`, but don't return them fully nested.
-    # This means that if you call `get_shallow(:foo, :bar)` the result will not
-    # be nested inside of `{:foo {:bar => values}`.
-    #
-    # @param [Array<Symbol>]
-    # @return [Hash]
-    def get_shallow(*key_paths)
-      key_paths.reduce(get(*key_paths)) {|acc, p| acc[p]}
-    end
-
-
-    # Return a hash including the values of the keys given at the path given
-    # 
-    # Example Usage:
-    # extract_metrics(
-    #   [:jvm, :process],
-    #   :open_file_descriptors,
-    #   [:cpu, [:total_in_millis, :percent]]
-    #   [:pipelines, [:one, :two], :size]
-    # )
-    # 
-    # Returns:
-    # # From the jvm.process metrics namespace
-    # {
-    #   :open_file_descriptors => 123
-    #   :cpu => { :total_in_millis => 456, :percent => 789 }
-    #   :pipelines => {
-    #                   :one => {:size => 90210},
-    #                   :two => {:size => 8675309}
-    #                 }
-    # }
-    def extract_metrics(path, *keys)
-      keys.reduce({}) do |acc,k|
-        # Simplify 1-length keys
-        k = k.first if k.is_a?(Array) && k.size == 1
-
-        # If we have array values here we need to recurse
-        # There are two levels of looping here, one for the paths we might pass in
-        # one for the upcoming keys we might pass in
-        if k.is_a?(Array)
-          # We need to build up future executions to extract_metrics
-          # which means building up the path and keys arguments.
-          # We need a nested loop her to execute all permutations of these in case we hit
-          # something like [[:a,:b],[:c,:d]] which produces 4 different metrics
-          next_paths = Array(k.first)
-          next_keys = Array(k[1])
-          rest = k[2..-1]
-          next_paths.each do |next_path|
-            # If there already is a hash at this location use that so we don't overwrite it
-            np_hash = acc[next_path] || {}
-            
-            acc[next_path] = next_keys.reduce(np_hash) do |a,next_key|
-              a.merge! extract_metrics(path + [next_path], [next_key, *rest])
-            end
-          end
-        else # Scalar value
-          res = get_shallow(*path)[k]
-          acc[k] = res ? res.value : nil
-        end
-        
-        acc
-      end
-    end    
-
-    def has_metric?(*path)
-      @fast_lookup[path]
-    end
-
-    # Return all the individuals Metric,
-    # This call mimic a Enum's each if a block is provided
-    #
-    # @param path [String] The search path for metrics
-    # @param [Array] The metric for the specific path
-    def each(path = nil, &block)
-      metrics = if path.nil?
-        get_all
-      else
-        transform_to_array(get_with_path(path))
-      end
-
-      block_given? ? metrics.each(&block) : metrics
-    end
-    alias_method :all, :each
-
-    def prune(path)
-      key_paths = key_paths(path).map(&:to_sym)
-      @structured_lookup_mutex.synchronize do
-        keys_to_delete = @fast_lookup.keys.select {|namespace| (key_paths - namespace[0..-2]).empty? }
-        keys_to_delete.each {|k| @fast_lookup.delete(k) }
-        delete_from_map(@store, key_paths)
-      end
-    end
-
-    def size
-      @fast_lookup.size
-    end
-
-    private
-    def get_all
-      @fast_lookup.values
-    end
-
-    def key_paths(path)
-      path.gsub(/^#{KEY_PATH_SEPARATOR}+/, "").split(KEY_PATH_SEPARATOR)
-    end
-
-    # This method take an array of keys and recursively search the metric store structure
-    # and return a filtered hash of the structure. This method also take into consideration
-    # getting two different branchs.
-    #
-    #
-    # If one part of the `key_paths` contains a filter key with the following format.
-    # "pipeline01, pipeline_02", It know that need to fetch the branch `pipeline01` and `pipeline02`
-    #
-    # Look at the rspec test for more usage.
-    #
-    # @param key_paths [Array<Symbol>] The list of keys part to filter
-    # @param map [Concurrent::Map] The the part of map to search in
-    # @param new_hash [Hash] The hash to populate with the results.
-    # @return Hash
-    def get_recursively(key_paths, map, new_hash)
-      key_candidates = extract_filter_keys(key_paths.shift)
-
-      key_candidates.each do |key_candidate|
-        raise MetricNotFound, "For path: #{key_candidate}. Map keys: #{map.keys}" if map[key_candidate].nil?
-
-        if key_paths.empty? # End of the user requested path
-          if map[key_candidate].is_a?(Concurrent::Map)
-            new_hash[key_candidate] = transform_to_hash(map[key_candidate])
-          else
-            new_hash[key_candidate] = map[key_candidate]
-          end
-        else
-          if map[key_candidate].is_a?(Concurrent::Map)
-            new_hash[key_candidate] = get_recursively(key_paths, map[key_candidate], {})
-          else
-            new_hash[key_candidate] = map[key_candidate]
-          end
-        end
-      end
-      return new_hash
-    end
-
-    def extract_filter_keys(key)
-      key.to_s.strip.split(FILTER_KEYS_SEPARATOR).map(&:to_sym)
-    end
-
-    # Take a hash and recursively flatten it into an array.
-    # This is useful if you are only interested in the leaf of the tree.
-    # Mostly used with `each` to get all the metrics from a specific namespaces
-    #
-    # This could be moved to `LogStash::Util` once this api stabilize
-    #
-    # @return [Array] One dimension array
-     def transform_to_array(map)
-      map.values.collect do |value|
-        value.is_a?(Hash) ? transform_to_array(value) : value
-      end.flatten
-    end
-
-    # Transform the Concurrent::Map hash into a ruby hash format,
-    # This is used to be serialize at the web api layer.
-    #
-    # This could be moved to `LogStash::Util` once this api stabilize
-    #
-    # @return [Hash]
-    def transform_to_hash(map, new_hash = Hash.new)
-      map.each_pair do |key, value|
-        if value.is_a?(Concurrent::Map)
-          new_hash[key] = {}
-          transform_to_hash(value, new_hash[key])
-        else
-          new_hash[key] = value
-        end
-      end
-
-      return new_hash
-    end
-
-    # This method iterate through the namespace path and try to find the corresponding
-    # value for the path, if any part of the path is not found it will
-    # create it.
-    #
-    # @param [Array] The path where values should be located
-    # @raise [ConcurrentMapExpected] Raise if the retrieved object isn't a `Concurrent::Map`
-    # @return [Concurrent::Map] Map where the metrics should be saved
-    def fetch_or_store_namespaces(namespaces_path)
-      path_map = fetch_or_store_namespace_recursively(@store, namespaces_path)
-
-      # This mean one of the namespace and key are colliding
-      # and we have to deal it upstream.
-      unless path_map.is_a?(Concurrent::Map)
-        raise NamespacesExpectedError, "Expecting a `Namespaces` but found class:  #{path_map.class.name} for namespaces_path: #{namespaces_path}"
-      end
-
-      return path_map
-    end
-
-    # Recursively fetch or create the namespace paths through the `MetricStove`
-    # This algorithm use an index to known which keys to search in the map.
-    # This doesn't cloning the array if we want to give a better feedback to the user
-    #
-    # @param [Concurrent::Map] Map to search for the key
-    # @param [Array] List of path to create
-    # @param [Fixnum] Which part from the list to create
-    #
-    def fetch_or_store_namespace_recursively(map, namespaces_path, idx = 0)
-      current = namespaces_path[idx]
-
-      # we are at the end of the namespace path, break out of the recursion
-      return map if current.nil?
-
-      new_map = map.fetch_or_store(current) { Concurrent::Map.new }
-      return fetch_or_store_namespace_recursively(new_map, namespaces_path, idx + 1)
-    end
-
-    def delete_from_map(map, keys)
-      key = keys.first
-      if keys.size == 1
-        map.delete(key)
-      else
-        delete_from_map(map[key], keys[1..-1]) unless map[key].nil?
-      end
-    end
-  end
-end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type.rb b/logstash-core/lib/logstash/instrument/metric_type.rb
deleted file mode 100644
index 85b82de4c3b..00000000000
--- a/logstash-core/lib/logstash/instrument/metric_type.rb
+++ /dev/null
@@ -1,22 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/metric_type/counter"
-require "logstash/instrument/metric_type/gauge"
-
-module LogStash module Instrument
-  module MetricType
-    METRIC_TYPE_LIST = {
-      :counter => LogStash::Instrument::MetricType::Counter,
-      :gauge => LogStash::Instrument::MetricType::Gauge
-    }.freeze
-
-    # Use the string to generate a concrete class for this metrics
-    #
-    # @param [String] The name of the class
-    # @param [Array] Namespaces list
-    # @param [String] The metric key
-    # @raise [NameError] If the class is not found
-    def self.create(type, namespaces, key)
-      METRIC_TYPE_LIST[type].new(namespaces, key)
-    end
-  end
-end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/counter.rb b/logstash-core/lib/logstash/instrument/metric_type/counter.rb
deleted file mode 100644
index 2feeb2d7791..00000000000
--- a/logstash-core/lib/logstash/instrument/metric_type/counter.rb
+++ /dev/null
@@ -1,17 +0,0 @@
-#encoding: utf-8
-java_import org.logstash.instrument.metrics.counter.LongCounter
-
-module LogStash module Instrument module MetricType
-  class Counter < LongCounter
-
-    def initialize(namespaces, key)
-      super(key.to_s)
-
-    end
-
-    def execute(action, value = 1)
-      send(action, value)
-    end
-
-  end
-end; end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/gauge.rb b/logstash-core/lib/logstash/instrument/metric_type/gauge.rb
deleted file mode 100644
index e6492305e4a..00000000000
--- a/logstash-core/lib/logstash/instrument/metric_type/gauge.rb
+++ /dev/null
@@ -1,16 +0,0 @@
-# encoding: utf-8
-java_import org.logstash.instrument.metrics.gauge.LazyDelegatingGauge
-module LogStash module Instrument module MetricType
-  class Gauge < LazyDelegatingGauge
-
-    def initialize(namespaces, key)
-      super(key.to_s)
-    end
-
-    def execute(action, value = nil)
-      send(action, value)
-    end
-
-  end
-end; end; end
-
diff --git a/logstash-core/lib/logstash/instrument/namespaced_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
deleted file mode 100644
index 40afa45424a..00000000000
--- a/logstash-core/lib/logstash/instrument/namespaced_metric.rb
+++ /dev/null
@@ -1,58 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/metric"
-
-module LogStash module Instrument
-  # This class acts a a proxy between the metric library and the user calls.
-  #
-  # This is the class that plugins authors will use to interact with the `MetricStore`
-  # It has the same public interface as `Metric` class but doesnt require to send
-  # the namespace on every call.
-  #
-  # @see Logstash::Instrument::Metric
-  class NamespacedMetric
-    attr_reader :namespace_name
-    # Create metric with a specific namespace
-    #
-    # @param metric [LogStash::Instrument::Metric] The metric instance to proxy
-    # @param namespace [Array] The namespace to use
-    def initialize(metric, namespace_name)
-      @metric = metric
-      @namespace_name = Array(namespace_name)
-    end
-
-    def increment(key, value = 1)
-      @metric.increment(namespace_name, key, value)
-    end
-
-    def decrement(key, value = 1)
-      @metric.decrement(namespace_name, key, value)
-    end
-
-    def gauge(key, value)
-      @metric.gauge(namespace_name, key, value)
-    end
-
-    def report_time(key, duration)
-      @metric.report_time(namespace_name, key, duration)
-    end
-
-    def time(key, &block)
-      @metric.time(namespace_name, key, &block)
-    end
-
-    def collector
-      @metric.collector
-    end
-    
-    def counter(key)
-      collector.get(@namespace_name, key, :counter)
-    end
-
-    def namespace(name)
-      NamespacedMetric.new(metric, namespace_name + Array(name))
-    end
-
-    private
-    attr_reader :metric
-  end
-end; end
diff --git a/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
deleted file mode 100644
index 1a3b6f9c1d1..00000000000
--- a/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
+++ /dev/null
@@ -1,58 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/null_metric"
-
-module LogStash module Instrument
-  # This class acts a a proxy between the metric library and the user calls.
-  #
-  # This is the class that plugins authors will use to interact with the `MetricStore`
-  # It has the same public interface as `Metric` class but doesnt require to send
-  # the namespace on every call.
-  #
-  # @see Logstash::Instrument::Metric
-  class NamespacedNullMetric
-    attr_reader :namespace_name
-    # Create metric with a specific namespace
-    #
-    # @param metric [LogStash::Instrument::Metric] The metric instance to proxy
-    # @param namespace [Array] The namespace to use
-    def initialize(metric = nil, namespace_name = :null)
-      @metric = metric
-      @namespace_name = Array(namespace_name)
-    end
-
-    def increment(key, value = 1)
-    end
-
-    def decrement(key, value = 1)
-    end
-
-    def gauge(key, value)
-    end
-
-    def report_time(key, duration)
-    end
-
-    def time(key, &block)
-      if block_given?
-        yield
-      else
-        ::LogStash::Instrument::NullMetric::NullTimedExecution
-      end
-    end
-
-    def collector
-      @metric.collector
-    end
-
-    def counter(_)
-      ::LogStash::Instrument::NullMetric::NullGauge
-    end
-
-    def namespace(name)
-      NamespacedNullMetric.new(metric, namespace_name + Array(name))
-    end
-
-    private
-    attr_reader :metric
-  end
-end; end
diff --git a/logstash-core/lib/logstash/instrument/null_metric.rb b/logstash-core/lib/logstash/instrument/null_metric.rb
deleted file mode 100644
index f56028d4580..00000000000
--- a/logstash-core/lib/logstash/instrument/null_metric.rb
+++ /dev/null
@@ -1,71 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/metric"
-
-module LogStash module Instrument
-  # This class is used in the context when we disable the metric collection
-  # for specific plugin to replace the `NamespacedMetric` class with this one
-  # which doesn't produce any metric to the collector.
-  class NullMetric
-    attr_reader :namespace_name, :collector
-
-    def initialize(collector = nil)
-      @collector = collector
-    end
-
-    def increment(namespace, key, value = 1)
-      Metric.validate_key!(key)
-    end
-
-    def decrement(namespace, key, value = 1)
-      Metric.validate_key!(key)
-    end
-
-    def gauge(namespace, key, value)
-      Metric.validate_key!(key)
-    end
-
-    def report_time(namespace, key, duration)
-      Metric.validate_key!(key)
-    end
-
-    # We have to manually redefine this method since it can return an
-    # object this object also has to be implemented as a NullObject
-    def time(namespace, key)
-      Metric.validate_key!(key)
-      if block_given?
-        yield
-      else
-        NullTimedExecution
-      end
-    end
-
-    def counter(_)
-      NullGauge
-    end
-
-    def namespace(name)
-      raise MetricNoNamespaceProvided if name.nil? || name.empty?
-      NamespacedNullMetric.new(self, name)
-    end
-
-    def self.validate_key!(key)
-      raise MetricNoKeyProvided if key.nil? || key.empty?
-    end
-
-    private
-
-    class NullGauge
-      def self.increment(_)
-      end
-    end
-
-    # Null implementation of the internal timer class
-    #
-    # @see LogStash::Instrument::TimedExecution`
-    class NullTimedExecution
-      def self.stop
-        0
-      end
-    end
-  end
-end; end
diff --git a/logstash-core/lib/logstash/instrument/snapshot.rb b/logstash-core/lib/logstash/instrument/snapshot.rb
deleted file mode 100644
index 62a12677fdb..00000000000
--- a/logstash-core/lib/logstash/instrument/snapshot.rb
+++ /dev/null
@@ -1,15 +0,0 @@
-# encoding: utf-8
-require "logstash/util/loggable"
-
-module LogStash module Instrument
-  class Snapshot
-    include LogStash::Util::Loggable
-
-    attr_reader :metric_store, :created_at
-
-    def initialize(metric_store, created_at = Time.now)
-      @metric_store = metric_store
-      @created_at = created_at
-    end
-  end
-end; end
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 9b1671b9f0b..b48460b9177 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -12,11 +12,6 @@
 require "logstash/outputs/base"
 require "logstash/shutdown_watcher"
 require "logstash/pipeline_reporter"
-require "logstash/instrument/metric"
-require "logstash/instrument/namespaced_metric"
-require "logstash/instrument/null_metric"
-require "logstash/instrument/namespaced_null_metric"
-require "logstash/instrument/collector"
 require "logstash/instrument/wrapped_write_client"
 require "logstash/util/dead_letter_queue_manager"
 require "logstash/output_delegator"
@@ -29,6 +24,7 @@
 java_import org.logstash.common.DeadLetterQueueFactory
 java_import org.logstash.common.SourceWithMetadata
 java_import org.logstash.common.io.DeadLetterQueueWriter
+java_import org.logstash.instrument.witness.Witness
 
 module LogStash; class BasePipeline
   include LogStash::Util::Loggable
@@ -36,7 +32,7 @@ module LogStash; class BasePipeline
   attr_reader :settings, :config_str, :config_hash, :inputs, :filters, :outputs, :pipeline_id, :lir, :execution_context, :ephemeral_id
   attr_reader :pipeline_config
 
-  def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
+  def initialize(pipeline_config, agent = nil)
     @logger = self.logger
     @mutex = Mutex.new
     @ephemeral_id = SecureRandom.uuid
@@ -132,27 +128,25 @@ def plugin(plugin_type, name, line, column, *args)
 
     raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
     @plugins_by_id[id] = true
-
-    # use NullMetric if called in the BasePipeline context otherwise use the @metric value
-    metric = @metric || Instrument::NullMetric.new
-
-    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
-    # Scope plugins of type 'input' to 'inputs'
-    type_scoped_metric = pipeline_scoped_metric.namespace("#{plugin_type}s".to_sym)
+    witness_plugins = Witness.instance.pipeline(pipeline_id.to_s).plugins
 
     klass = Plugin.lookup(plugin_type, name)
 
     execution_context = ExecutionContext.new(self, @agent, id, klass.config_name, @dlq_writer)
 
     if plugin_type == "output"
-      OutputDelegator.new(@logger, klass, type_scoped_metric, execution_context, OutputDelegatorStrategyRegistry.instance, args)
+      OutputDelegator.new(@logger, klass, witness_plugins.outputs(id), execution_context, OutputDelegatorStrategyRegistry.instance, args)
     elsif plugin_type == "filter"
-      FilterDelegator.new(@logger, klass, type_scoped_metric, execution_context, args)
-    else # input
+      FilterDelegator.new(@logger, klass, witness_plugins.filters(id), execution_context, args)
+    else # input or codec
       input_plugin = klass.new(args)
-      scoped_metric = type_scoped_metric.namespace(id.to_sym)
-      scoped_metric.gauge(:name, input_plugin.config_name)
-      input_plugin.metric = scoped_metric
+      if plugin_type.eql? "input"
+        witness_plugins.inputs(id).name(input_plugin.config_name)
+        input_plugin.metric = witness_plugins.inputs(id).custom
+      elsif plugin_type.eql? "codecs"
+        witness_plugins.codecs(id).name(input_plugin.config_name)
+        input_plugin.metric = witness_plugins.codecs(id).custom
+      end
       input_plugin.execution_context = execution_context
       input_plugin
     end
@@ -190,22 +184,14 @@ module LogStash; class Pipeline < BasePipeline
     :started_at,
     :thread,
     :settings,
-    :metric,
     :filter_queue_client,
     :input_queue_client,
     :queue
 
   MAX_INFLIGHT_WARN_THRESHOLD = 10_000
 
-  def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
+  def initialize(pipeline_config, agent = nil)
     @settings = pipeline_config.settings
-    # This needs to be configured before we call super which will evaluate the code to make
-    # sure the metric instance is correctly send to the plugins to make the namespace scoping work
-    @metric = if namespaced_metric
-      settings.get("metric.collect") ? namespaced_metric : Instrument::NullMetric.new(namespaced_metric.collector)
-    else
-      Instrument::NullMetric.new
-    end
 
     @ephemeral_id = SecureRandom.uuid
     @settings = settings
@@ -226,10 +212,8 @@ def initialize(pipeline_config, namespaced_metric = nil, agent = nil)
     @signal_queue = java.util.concurrent.LinkedBlockingQueue.new
     # Note that @inflight_batches as a central mechanism for tracking inflight
     # batches will fail if we have multiple read clients here.
-    @filter_queue_client.set_events_metric(metric.namespace([:stats, :events]))
-    @filter_queue_client.set_pipeline_metric(
-        metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
-    )
+    @filter_queue_client.set_events_metric(Witness.instance.events)
+    @filter_queue_client.set_pipeline_metric(Witness.instance.pipeline(pipeline_id.to_s).events)
     @drain_queue =  @settings.get_value("queue.drain")
 
 
@@ -413,14 +397,15 @@ def start_workers
 
       max_inflight = batch_size * pipeline_workers
 
-      config_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :config])
-      config_metric.gauge(:workers, pipeline_workers)
-      config_metric.gauge(:batch_size, batch_size)
-      config_metric.gauge(:batch_delay, batch_delay)
-      config_metric.gauge(:config_reload_automatic, @settings.get("config.reload.automatic"))
-      config_metric.gauge(:config_reload_interval, @settings.get("config.reload.interval"))
-      config_metric.gauge(:dead_letter_queue_enabled, dlq_enabled?)
-      config_metric.gauge(:dead_letter_queue_path, @dlq_writer.get_path.to_absolute_path.to_s) if dlq_enabled?
+      witness_config = Witness.instance.pipeline(pipeline_id.to_s).config
+
+      witness_config.workers(pipeline_workers)
+      witness_config.batch_size(batch_size)
+      witness_config.batch_delay(batch_delay)
+      witness_config.config_reload_automatic(@settings.get("config.reload.automatic"))
+      witness_config.config_reload_interval(@settings.get("config.reload.interval"))
+      witness_config.dead_letter_queue_enabled(dlq_enabled?)
+      witness_config.dead_letter_queue_path(@dlq_writer.get_path.to_absolute_path.to_s) if dlq_enabled?
 
 
       @logger.info("Starting pipeline", default_logging_keys(
@@ -728,47 +713,39 @@ def stalling_threads_info
 
   def collect_dlq_stats
     if dlq_enabled?
-      dlq_metric = @metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :dlq])
-      dlq_metric.gauge(:queue_size_in_bytes, @dlq_writer.get_current_queue_size)
+      Witness.instance.pipeline(pipeline_id.to_s).dlq.queue_size_in_bytes(@dlq_writer.get_current_queue_size)
     end
   end
 
   def collect_stats
-    pipeline_metric = @metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :queue])
-    pipeline_metric.gauge(:type, settings.get("queue.type"))
+
+    queue_witness = Witness.instance.pipeline(pipeline_id.to_s).queue
+    queue_witness.type(settings.get("queue.type"))
+
     if @queue.is_a?(LogStash::Util::WrappedAckedQueue) && @queue.queue.is_a?(LogStash::AckedQueue)
       queue = @queue.queue
       dir_path = queue.dir_path
       file_store = Files.get_file_store(Paths.get(dir_path))
 
-      pipeline_metric.namespace([:capacity]).tap do |n|
-        n.gauge(:page_capacity_in_bytes, queue.page_capacity)
-        n.gauge(:max_queue_size_in_bytes, queue.max_size_in_bytes)
-        n.gauge(:max_unread_events, queue.max_unread_events)
-        n.gauge(:queue_size_in_bytes, queue.persisted_size_in_bytes)
-      end
-      pipeline_metric.namespace([:data]).tap do |n|
-        n.gauge(:free_space_in_bytes, file_store.get_unallocated_space)
-        n.gauge(:storage_type, file_store.type)
-        n.gauge(:path, dir_path)
-      end
+      #capacity
+      queue_witness.capacity.page_capacity_in_bytes(queue.page_capacity)
+      queue_witness.capacity.max_queue_size_in_bytes(queue.max_size_in_bytes)
+      queue_witness.capacity.max_unread_events(queue.max_unread_events)
+      queue_witness.capacity.queue_size_in_bytes(queue.persisted_size_in_bytes)
+
+      #data
+      queue_witness.data.free_space_in_bytes(file_store.get_unallocated_space)
+      queue_witness.data.storage_type(file_store.type)
+      queue_witness.data.path(dir_path)
 
-      pipeline_metric.gauge(:events, queue.unread_count)
+      #events
+      queue_witness.events(queue.unread_count)
     end
   end
 
   def clear_pipeline_metrics
-    # TODO(ph): I think the metric should also proxy that call correctly to the collector
-    # this will simplify everything since the null metric would simply just do a noop
-    collector = @metric.collector
-
-    unless collector.nil?
-      # selectively reset metrics we don't wish to keep after reloading
-      # these include metrics about the plugins and number of processed events
-      # we want to keep other metrics like reload counts and error messages
-      collector.clear("stats/pipelines/#{pipeline_id}/plugins")
-      collector.clear("stats/pipelines/#{pipeline_id}/events")
-    end
+    Witness.instance.pipeline(pipeline_id).forget_plugins
+    Witness.instance.pipeline(pipeline_id).forget_events
   end
 
   # Sometimes we log stuff that will dump the pipeline which may contain
@@ -806,7 +783,7 @@ def draining_queue?
   def wrapped_write_client(plugin)
     #need to ensure that metrics are initialized one plugin at a time, else a race condition can exist.
     @mutex.synchronize do
-      LogStash::Instrument::WrappedWriteClient.new(@input_queue_client, self, metric, plugin)
+    LogStash::Instrument::WrappedWriteClient.new(@input_queue_client, self, plugin, @logger)
     end
   end
 end; end
diff --git a/logstash-core/lib/logstash/pipeline_action/create.rb b/logstash-core/lib/logstash/pipeline_action/create.rb
index fe0937831e5..0d27b87f1b0 100644
--- a/logstash-core/lib/logstash/pipeline_action/create.rb
+++ b/logstash-core/lib/logstash/pipeline_action/create.rb
@@ -8,13 +8,9 @@ module LogStash module PipelineAction
   class Create < Base
     include LogStash::Util::Loggable
 
-    # We currently pass around the metric object again this
-    # is needed to correctly create a pipeline, in a future
-    # PR we could pass a factory to create the pipeline so we pass the logic
-    # to create the pipeline instead.
-    def initialize(pipeline_config, metric)
+
+    def initialize(pipeline_config)
       @pipeline_config = pipeline_config
-      @metric = metric
     end
 
     def pipeline_id
@@ -32,7 +28,7 @@ def execution_priority
     # The execute assume that the thread safety access of the pipeline
     # is managed by the caller.
     def execute(agent, pipelines)
-      pipeline = LogStash::Pipeline.new(@pipeline_config, @metric, agent)
+      pipeline = LogStash::Pipeline.new(@pipeline_config, agent)
       
       status = pipeline.start # block until the pipeline is correctly started or crashed
 
diff --git a/logstash-core/lib/logstash/pipeline_action/reload.rb b/logstash-core/lib/logstash/pipeline_action/reload.rb
index e0d8f7fcc97..b86bd257453 100644
--- a/logstash-core/lib/logstash/pipeline_action/reload.rb
+++ b/logstash-core/lib/logstash/pipeline_action/reload.rb
@@ -10,9 +10,8 @@ module LogStash module PipelineAction
   class Reload < Base
     include LogStash::Util::Loggable
 
-    def initialize(pipeline_config, metric)
+    def initialize(pipeline_config)
       @pipeline_config = pipeline_config
-      @metric = metric
     end
 
     def pipeline_id
@@ -40,7 +39,7 @@ def execute(agent, pipelines)
       status = Stop.new(pipeline_id).execute(agent, pipelines)
 
       if status
-        return Create.new(@pipeline_config, @metric).execute(agent, pipelines)
+        return Create.new(@pipeline_config).execute(agent, pipelines)
       else
         return status
       end
diff --git a/logstash-core/lib/logstash/pipeline_settings.rb b/logstash-core/lib/logstash/pipeline_settings.rb
index 7984b5481db..1e82b00ce96 100644
--- a/logstash-core/lib/logstash/pipeline_settings.rb
+++ b/logstash-core/lib/logstash/pipeline_settings.rb
@@ -13,7 +13,6 @@ class PipelineSettings < Settings
       "config.string",
       "dead_letter_queue.enable",
       "dead_letter_queue.max_bytes",
-      "metric.collect",
       "path.config",
       "path.dead_letter_queue",
       "path.queue",
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index 77c277e537c..86c5b2521fa 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -332,7 +332,6 @@
 
     let(:agent_args) do
       {
-        "metric.collect" => true,
         "path.config" => config_file
       }
     end
@@ -395,40 +394,36 @@
       end
 
       it "resets the pipeline metric collector" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:events][:in].value
+        value = Witness.instance.pipeline("main").events.snitch.in
         expect(value).to be <= new_config_generator_counter
       end
 
       it "does not reset the global event count" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/events")[:stats][:events][:in].value
+        value = Witness.instance.events.snitch.in
         expect(value).to be > initial_generator_threshold
       end
 
       it "increases the successful reload count" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
+        value = Witness.instance.pipeline("main").reloads.snitch.successes
         expect(value).to eq(1)
-        instance_value = snapshot.metric_store.get_with_path("/stats")[:stats][:reloads][:successes].value
+        instance_value = Witness.instance.reloads.snitch.successes
         expect(instance_value).to eq(1)
       end
 
       it "does not set the failure reload timestamp" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_failure_timestamp].value
+        value = Witness.instance.pipeline("main").reloads.snitch.last_failure_timestamp
         expect(value).to be(nil)
       end
 
       it "sets the success reload timestamp" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_success_timestamp].value
+        value = Witness.instance.pipeline("main").reloads.snitch.last_success_timestamp
         expect(value).to be_a(Timestamp)
       end
 
       it "does not set the last reload error" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_error].value
+        value = Witness.instance.pipeline("main").reloads.error.snitch.backtrace
+        expect(value).to be(nil)
+        value = Witness.instance.pipeline("main").reloads.error.snitch.message
         expect(value).to be(nil)
       end
     end
@@ -438,33 +433,29 @@
       before(:each) { subject.converge_state_and_update }
 
       it "does not increase the successful reload count" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
+        value = Witness.instance.pipeline("main").reloads.snitch.successes
         expect(value).to eq(0)
       end
 
       it "does not set the successful reload timestamp" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_success_timestamp].value
+        value = Witness.instance.pipeline("main").reloads.snitch.last_success_timestamp
         expect(value).to be(nil)
       end
 
       it "sets the failure reload timestamp" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_failure_timestamp].value
+        value = Witness.instance.pipeline("main").reloads.snitch.last_failure_timestamp
         expect(value).to be_a(Timestamp)
       end
 
       it "sets the last reload error" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_error].value
-        expect(value).to be_a(Hash)
-        expect(value).to include(:message, :backtrace)
+        value = Witness.instance.pipeline("main").reloads.error.snitch.message
+        expect(value).to_not be_nil
+        value = Witness.instance.pipeline("main").reloads.error.snitch.backtrace
+        expect(value).to_not be_nil
       end
 
       it "increases the failed reload count" do
-        snapshot = subject.metric.collector.snapshot_metric
-        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:failures].value
+        value = Witness.instance.pipeline("main").reloads.snitch.failures
         expect(value).to be > 0
       end
     end
@@ -475,7 +466,6 @@
         {
           "config.reload.automatic" => false,
           "pipeline.batch.size" => 1,
-          "metric.collect" => true,
           "path.config" => config_file
         }
       end
@@ -492,17 +482,13 @@ def register
 
       it "does not increase the successful reload count" do
         expect { subject.converge_state_and_update }.to_not change {
-          snapshot = subject.metric.collector.snapshot_metric
-          reload_metrics = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads]
-          reload_metrics[:successes].value
+          Witness.instance.pipeline("main").reloads.snitch.successes
         }
       end
 
       it "increases the failures reload count" do
         expect { subject.converge_state_and_update }.to change {
-          snapshot = subject.metric.collector.snapshot_metric
-          reload_metrics = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads]
-          reload_metrics[:failures].value
+          Witness.instance.pipeline("main").reloads.snitch.failures
         }.by(1)
       end
     end
diff --git a/logstash-core/spec/logstash/instrument/collector_spec.rb b/logstash-core/spec/logstash/instrument/collector_spec.rb
deleted file mode 100644
index b5c9b3073de..00000000000
--- a/logstash-core/spec/logstash/instrument/collector_spec.rb
+++ /dev/null
@@ -1,53 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/collector"
-require "spec_helper"
-
-describe LogStash::Instrument::Collector do
-  subject { LogStash::Instrument::Collector.new }
-  describe "#push" do
-    let(:namespaces_path) { [:root, :pipelines, :pipelines01] }
-    let(:key) { :my_key }
-
-    context "when the `MetricType` exist" do
-      it "store the metric of type `counter`" do
-        subject.push(namespaces_path, key, :counter, :increment)
-      end
-    end
-
-    context "when the `MetricType` doesn't exist" do
-      let(:wrong_type) { :donotexist }
-
-      it "logs an error but dont crash" do
-        expect(subject.logger).to receive(:error)
-          .with("Collector: Cannot create concrete class for this metric type",
-        hash_including({ :type => wrong_type, :namespaces_path => namespaces_path }))
-
-          subject.push(namespaces_path, key, wrong_type, :increment)
-      end
-    end
-
-    context "when there is a conflict with the metric key" do
-      let(:conflicting_namespaces) { [namespaces_path, key].flatten }
-
-      it "logs an error but dont crash" do
-        subject.push(namespaces_path, key, :counter, :increment)
-
-        expect(subject.logger).to receive(:error)
-          .with("Collector: Cannot record metric",
-          hash_including({ :exception => instance_of(LogStash::Instrument::MetricStore::NamespacesExpectedError) }))
-
-          subject.push(conflicting_namespaces, :random_key, :counter, :increment)
-      end
-    end
-  end
-
-  describe "#snapshot_metric" do
-    it "return a `LogStash::Instrument::MetricStore`" do
-      expect(subject.snapshot_metric).to be_kind_of(LogStash::Instrument::Snapshot)
-    end
-
-    it "returns a clone of the metric store" do
-      expect(subject.snapshot_metric).not_to eq(subject.snapshot_metric)
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/instrument/metric_spec.rb b/logstash-core/spec/logstash/instrument/metric_spec.rb
deleted file mode 100644
index 123a47be268..00000000000
--- a/logstash-core/spec/logstash/instrument/metric_spec.rb
+++ /dev/null
@@ -1,111 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/metric"
-require "logstash/instrument/collector"
-require_relative "../../support/matchers"
-require "spec_helper"
-
-describe LogStash::Instrument::Metric do
-  let(:collector) { [] }
-  let(:namespace) { :root }
-
-  subject { LogStash::Instrument::Metric.new(collector) }
-
-  context "#increment" do
-    it "a counter by 1" do
-      metric = subject.increment(:root, :error_rate)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 1)
-    end
-
-    it "a counter by a provided value" do
-      metric = subject.increment(:root, :error_rate, 20)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 20)
-    end
-
-    it "raises an exception if the key is an empty string" do
-      expect { subject.increment(:root, "", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
-    end
-
-    it "raise an exception if the key is nil" do
-      expect { subject.increment(:root, nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
-    end
-  end
-
-  context "#decrement" do
-    it "a counter by 1" do
-      metric = subject.decrement(:root, :error_rate)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 1)
-    end
-
-    it "a counter by a provided value" do
-      metric = subject.decrement(:root, :error_rate, 20)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 20)
-    end
-
-    it "raises an exception if the key is an empty string" do
-      expect { subject.decrement(:root, "", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
-    end
-
-    it "raise an exception if the key is nil" do
-      expect { subject.decrement(:root, nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
-    end
-  end
-
-  context "#gauge" do
-    it "set the value of a key" do
-      metric = subject.gauge(:root, :size_queue, 20)
-      expect(collector).to be_a_metric_event([:root, :size_queue], :gauge, :set, 20)
-    end
-
-    it "raises an exception if the key is an empty string" do
-      expect { subject.gauge(:root, "", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
-    end
-
-    it "raise an exception if the key is nil" do
-      expect { subject.gauge(:root, nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
-    end
-  end
-
-  context "#time" do
-    let(:sleep_time) { 2 }
-    let(:sleep_time_ms) { sleep_time * 1_000 }
-
-    it "records the duration" do
-      subject.time(:root, :duration_ms) { sleep(sleep_time) }
-
-      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5)
-      expect(collector[0]).to match(:root)
-      expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:counter)
-    end
-
-    it "returns the value of the executed block" do
-      expect(subject.time(:root, :testing) { "hello" }).to eq("hello")
-    end
-
-    it "return a TimedExecution" do
-      execution = subject.time(:root, :duration_ms)
-      sleep(sleep_time)
-      execution_time = execution.stop
-
-      expect(execution_time).to eq(collector.last)
-      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 0.1)
-      expect(collector[0]).to match(:root)
-      expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:counter)
-    end
-  end
-
-  context "#namespace" do
-    let(:sub_key) { :my_sub_key }
-
-    it "creates a new metric object and append the `sub_key` to the `base_key`" do
-      expect(subject.namespace(sub_key).namespace_name).to eq([sub_key])
-    end
-
-    it "uses the same collector as the creator class" do
-      child = subject.namespace(sub_key)
-      metric = child.increment(:error_rate)
-      expect(collector).to be_a_metric_event([sub_key, :error_rate], :counter, :increment, 1)
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/instrument/metric_store_spec.rb b/logstash-core/spec/logstash/instrument/metric_store_spec.rb
deleted file mode 100644
index dac026643cb..00000000000
--- a/logstash-core/spec/logstash/instrument/metric_store_spec.rb
+++ /dev/null
@@ -1,274 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/metric_store"
-
-describe LogStash::Instrument::MetricStore do
-  let(:namespaces) { [ :root, :pipelines, :pipeline_01 ] }
-  let(:key) { :events_in }
-  let(:counter) { LogStash::Instrument::MetricType::Counter.new(namespaces, key) }
-
-  context "when the metric object doesn't exist" do
-    it "store the object" do
-      expect(subject.fetch_or_store(namespaces, key, counter)).to eq(counter)
-    end
-
-    it "support a block as argument" do
-      expect(subject.fetch_or_store(namespaces, key) { counter }).to eq(counter)
-    end
-  end
-
-  context "when the metric object exist in the namespace"  do
-    let(:new_counter) { LogStash::Instrument::MetricType::Counter.new(namespaces, key) }
-
-    it "return the object" do
-      subject.fetch_or_store(namespaces, key, counter)
-      expect(subject.fetch_or_store(namespaces, key, new_counter)).to eq(counter)
-    end
-  end
-
-  context "when the namespace end node isn't a map" do
-    let(:conflicting_namespaces) { [:root, :pipelines, :pipeline_01, :events_in] }
-
-    it "raise an exception" do
-      subject.fetch_or_store(namespaces, key, counter)
-      expect { subject.fetch_or_store(conflicting_namespaces, :new_key, counter) }.to raise_error(LogStash::Instrument::MetricStore::NamespacesExpectedError)
-    end
-  end
-
-  context "retrieving events" do
-    let(:metric_events) {
-      [
-        [[:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch"], :event_in, :increment],
-        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_in, :increment],
-        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_out, :increment],
-        [[:node, :sashimi, :pipelines, :pipeline02], :processed_events_out, :increment],
-      ]
-    }
-
-    before :each do
-      # Lets add a few metrics in the store before trying to find them
-      metric_events.each do |namespaces, metric_key, action|
-        metric = subject.fetch_or_store(namespaces, metric_key, LogStash::Instrument::MetricType::Counter.new(namespaces, metric_key))
-        metric.execute(action)
-      end
-    end
-
-    context "#has_metric?" do
-      context "when the path exist" do
-        it "returns true" do
-          expect(subject.has_metric?(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch", :event_in)).to be_truthy
-        end
-      end
-
-      context "when the path doesn't exist" do
-        it "returns false" do
-          expect(subject.has_metric?(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-input-nothing")).to be_falsey
-        end
-      end
-    end
-
-    describe "#get" do
-      context "when the path exist" do
-        it "retrieves end of of a branch" do
-          metrics = subject.get(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch")
-          expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => anything)))))))
-        end
-
-        it "retrieves branch" do
-          metrics = subject.get(:node, :sashimi, :pipelines, :pipeline01)
-          expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything)))))
-        end
-
-        it "allow to retrieve a specific metrics" do
-          metrics = subject.get(:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch", :event_in)
-          expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => a_hash_including(:event_in => be_kind_of(LogStash::Instrument::MetricType::Counter)))))))))
-        end
-
-        context "with filtered keys" do
-          it "allows to retrieve multiple keys on the same level" do
-            metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01,pipeline02")
-            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
-          end
-
-          it "supports space in the keys" do
-            metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01, pipeline02 ")
-            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
-          end
-
-          it "retrieves only the requested keys" do
-            metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01,pipeline02", :processed_events_in)
-            expect(metrics[:node][:sashimi][:pipelines].keys).to include(:pipeline01, :pipeline02)
-          end
-        end
-
-        context "when the path doesnt exist" do
-          it "raise an exception" do
-            expect { subject.get(:node, :sashimi, :dontexist) }.to raise_error(LogStash::Instrument::MetricStore::MetricNotFound, /dontexist/)
-          end
-        end
-      end
-
-      describe "#get_with_path" do
-        context "when the path exist" do
-          it "removes the first `/`" do
-            metrics = subject.get_with_path("/node/sashimi/")
-            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => anything)))
-          end
-
-          it "retrieves end of of a branch" do
-            metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01/plugins/logstash-output-elasticsearch")
-            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => anything)))))))
-          end
-
-          it "retrieves branch" do
-            metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01")
-            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything)))))
-          end
-
-          it "allow to retrieve a specific metrics" do
-            metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01/plugins/logstash-output-elasticsearch/event_in")
-            expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => a_hash_including(:plugins => a_hash_including(:"logstash-output-elasticsearch" => a_hash_including(:event_in => be_kind_of(LogStash::Instrument::MetricType::Counter)))))))))
-          end
-
-          context "with filtered keys" do
-            it "allows to retrieve multiple keys on the same level" do
-              metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01,pipeline02/plugins/logstash-output-elasticsearch/event_in")
-              expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
-            end
-
-            it "supports space in the keys" do
-              metrics = subject.get_with_path("node/sashimi/pipelines/pipeline01, pipeline02 /plugins/logstash-output-elasticsearch/event_in")
-              expect(metrics).to match(a_hash_including(:node => a_hash_including(:sashimi => a_hash_including(:pipelines  => a_hash_including(:pipeline01 => anything, :pipeline02 => anything)))))
-            end
-
-            it "retrieves only the requested keys" do
-              metrics = subject.get(:node, :sashimi, :pipelines, :"pipeline01,pipeline02", :processed_events_in)
-              expect(metrics[:node][:sashimi][:pipelines].keys).to include(:pipeline01, :pipeline02)
-            end
-          end
-        end
-      end
-
-      context "when the path doesnt exist" do
-        it "raise an exception" do
-          expect { subject.get_with_path("node/sashimi/dontexist, pipeline02 /plugins/logstash-output-elasticsearch/event_in") }.to raise_error(LogStash::Instrument::MetricStore::MetricNotFound, /dontexist/)
-        end
-      end
-    end
-
-    describe "get_shallow" do
-      it "should retrieve a path as a single value" do
-        r = subject.get_shallow(:node, :sashimi, :pipelines, :pipeline01, :processed_events_in)
-        expect(r.value).to eql(1)
-      end
-    end
-
-    describe "extract_metrics" do
-      it "should retrieve non-nested values correctly" do
-        r = subject.extract_metrics(
-          [:node, :sashimi, :pipelines, :pipeline01],
-          :processed_events_in,
-          :processed_events_out,
-        )
-        expect(r[:processed_events_in]).to eql(1)
-        expect(r[:processed_events_out]).to eql(1)
-      end
-
-      it "should retrieve nested values correctly alongside non-nested ones" do
-        r = subject.extract_metrics(
-          [:node, :sashimi, :pipelines, :pipeline01],
-          :processed_events_in,
-          [:plugins, :"logstash-output-elasticsearch", :event_in]
-        )
-       expect(r[:processed_events_in]).to eql(1)
-        expect(r[:plugins][:"logstash-output-elasticsearch"][:event_in]).to eql(1)
-      end
-
-      it "should retrieve multiple nested keys at a given location" do
-        r = subject.extract_metrics(
-          [:node, :sashimi, :pipelines],
-          [:pipeline01, [:processed_events_in, :processed_events_out]]
-        )
-
-        expect(r[:pipeline01][:processed_events_in]).to eql(1)
-        expect(r[:pipeline01][:processed_events_out]).to eql(1)
-      end
-
-      it "should retrieve a single key nested in multiple places" do
-        r = subject.extract_metrics(
-          [:node, :sashimi, :pipelines],
-          [[:pipeline01, :pipeline02], :processed_events_out]
-        )
-
-        expect(r[:pipeline01][:processed_events_out]).to eql(1)
-        expect(r[:pipeline02][:processed_events_out]).to eql(1)
-      end
-
-      it "handle overlaps of paths" do
-        r = subject.extract_metrics(
-          [:node, :sashimi, :pipelines],
-          [:pipeline01, :processed_events_in],
-          [[:pipeline01, :pipeline02], :processed_events_out]
-        )
-
-        expect(r[:pipeline01][:processed_events_in]).to eql(1)
-        expect(r[:pipeline01][:processed_events_out]).to eql(1)
-        expect(r[:pipeline02][:processed_events_out]).to eql(1)
-      end
-    end
-
-    describe "#size" do
-      it "returns the number of unique metrics" do
-        expect(subject.size).to eq(metric_events.size)
-      end
-    end
-
-    describe "#each" do
-      it "retrieves all the metric" do
-        expect(subject.each.size).to eq(metric_events.size)
-      end
-
-      it "returns metric types" do
-        metrics = []
-        subject.each { |i| metrics << i }
-        expect(metrics.size).to eq(metric_events.size)
-      end
-
-      it "retrieves all the metrics from a specific branch" do
-        metrics = []
-        subject.each("node/sashimi/pipelines/pipeline01") { |i| metrics << i }
-        expect(metrics.size).to eq(3)
-      end
-    end
-  end
-
-  describe "#prune" do
-    let(:metric_events) {
-      [
-        [[:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch"], :event_in, :increment],
-        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_in, :increment],
-        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_out, :increment],
-        [[:node, :sashimi, :pipelines, :pipeline02], :processed_events_out, :increment],
-      ]
-    }
-
-    before :each do
-      # Lets add a few metrics in the store before trying to find them
-      metric_events.each do |namespaces, metric_key, action|
-        metric = subject.fetch_or_store(namespaces, metric_key, LogStash::Instrument::MetricType::Counter.new(namespaces, metric_key))
-        metric.execute(action)
-      end
-    end
-
-    it "should remove all keys with the same starting path as the argument" do
-      expect(subject.get(:node, :sashimi, :pipelines, :pipeline01)).to be_a(Hash)
-      subject.prune("/node/sashimi/pipelines/pipeline01")
-      expect { subject.get(:node, :sashimi, :pipelines, :pipeline01) }.to raise_error LogStash::Instrument::MetricStore::MetricNotFound
-    end
-
-    it "should keep other metrics on different path branches" do
-      expect(subject.get(:node, :sashimi, :pipelines, :pipeline02)).to be_a(Hash)
-      subject.prune("/node/sashimi/pipelines/pipeline01")
-      expect { subject.get(:node, :sashimi, :pipelines, :pipeline02) }.to_not raise_error
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
deleted file mode 100644
index 82b7c581acc..00000000000
--- a/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
+++ /dev/null
@@ -1,23 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/metric_type/counter"
-require "spec_helper"
-
-describe LogStash::Instrument::MetricType::Counter do
-  let(:namespaces) { [:root, :pipelines, :pipeline_01] }
-  let(:key) { :mykey }
-
-  subject { LogStash::Instrument::MetricType::Counter.new(namespaces, key) }
-
-  describe "#increment" do
-    it "increment the counter" do
-      expect{ subject.increment }.to change { subject.value }.by(1)
-    end
-  end
-
-  context "When serializing to JSON" do
-    it "serializes the value" do
-      expect(LogStash::Json.dump(subject)).to eq("0")
-    end
-  end
-
-end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
deleted file mode 100644
index 69ee278a0e7..00000000000
--- a/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
+++ /dev/null
@@ -1,30 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/metric_type/gauge"
-require "logstash/json"
-require "spec_helper"
-
-describe LogStash::Instrument::MetricType::Gauge do
-  let(:namespaces) { [:root, :pipelines, :pipeline_01] }
-  let(:key) { :mykey }
-  let(:value) { "hello" }
-
-  subject { described_class.new(namespaces, key) }
-
-  before :each do
-    subject.execute(:set, value)
-  end
-
-  describe "#execute" do
-    it "set the value of the gauge" do
-      expect(subject.value).to eq(value)
-    end
-  end
-
-  context "When serializing to JSON" do
-    it "serializes the value" do
-      expect(LogStash::Json.dump(subject)).to eq("\"#{value}\"")
-    end
-  end
-
-
-end
diff --git a/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb b/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
deleted file mode 100644
index b4446afbfce..00000000000
--- a/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
+++ /dev/null
@@ -1,92 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/namespaced_metric"
-require "logstash/instrument/metric"
-require_relative "../../support/matchers"
-require_relative "../../support/shared_examples"
-require "spec_helper"
-
-describe LogStash::Instrument::NamespacedMetric do
-  let(:namespace) { :root }
-  let(:collector) { [] }
-  let(:metric) { LogStash::Instrument::Metric.new(collector) }
-
-  subject { described_class.new(metric, namespace) }
-
-  it "defines the same interface as `Metric`" do
-    expect(described_class).to implement_interface_of(LogStash::Instrument::Metric)
-  end
-
-  it "returns a TimedException when we call without a block" do
-    expect(subject.time(:duration_ms)).to be_kind_of(LogStash::Instrument::Metric::TimedExecution)
-  end
-
-  it "returns the value of the block" do
-    expect(subject.time(:duration_ms) { "hello" }).to eq("hello")
-  end
-
-  it "its doesnt change the original `namespace` when creating a subnamespace" do
-    new_namespace = subject.namespace(:wally)
-
-    expect(subject.namespace_name).to eq([namespace])
-    expect(new_namespace.namespace_name).to eq([:root, :wally])
-  end
-
-  context "#increment" do
-    it "a counter by 1" do
-      metric = subject.increment(:error_rate)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 1)
-    end
-
-    it "a counter by a provided value" do
-      metric = subject.increment(:error_rate, 20)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 20)
-    end
-  end
-
-  context "#decrement" do
-    it "a counter by 1" do
-      metric = subject.decrement(:error_rate)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 1)
-    end
-
-    it "a counter by a provided value" do
-      metric = subject.decrement(:error_rate, 20)
-      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 20)
-    end
-  end
-
-  context "#gauge" do
-    it "set the value of a key" do
-      metric = subject.gauge(:size_queue, 20)
-      expect(collector).to be_a_metric_event([:root, :size_queue], :gauge, :set, 20)
-    end
-  end
-
-  context "#time" do
-    let(:sleep_time) { 2 }
-    let(:sleep_time_ms) { sleep_time * 1_000 }
-
-    it "records the duration" do
-      subject.time(:duration_ms) { sleep(sleep_time) }
-
-      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5)
-      expect(collector[0]).to match([:root])
-      expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:counter)
-    end
-
-    it "return a TimedExecution" do
-      execution = subject.time(:duration_ms)
-      sleep(sleep_time)
-      execution_time = execution.stop
-
-      expect(execution_time).to eq(collector.last)
-      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 0.1)
-      expect(collector[0]).to match([:root])
-      expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:counter)
-    end
-  end
-
-  include_examples "metrics commons operations"
-end
diff --git a/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb b/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb
deleted file mode 100644
index fdd831dfbfc..00000000000
--- a/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb
+++ /dev/null
@@ -1,33 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/namespaced_null_metric"
-require "logstash/instrument/null_metric"
-require_relative "../../support/matchers"
-require "spec_helper"
-
-describe LogStash::Instrument::NamespacedNullMetric do
-  let(:namespace) { :root }
-  let(:collector) { [] }
-  let(:metric) { LogStash::Instrument::NullMetric.new(collector) }
-
-  subject { described_class.new(metric, namespace) }
-
-  it "defines the same interface as `Metric`" do
-    expect(described_class).to implement_interface_of(LogStash::Instrument::NamespacedMetric)
-  end
-
-  it "returns a TimedException when we call without a block" do
-    expect(subject.time(:duration_ms)).to be(LogStash::Instrument::NullMetric::NullTimedExecution)
-  end
-
-  it "returns the value of the block" do
-    expect(subject.time(:duration_ms) { "hello" }).to eq("hello")
-  end
-
-  it "its doesnt change the original `namespace` when creating a subnamespace" do
-    new_namespace = subject.namespace(:wally)
-
-    expect(subject.namespace_name).to eq([namespace])
-    expect(new_namespace.namespace_name).to eq([:root, :wally])
-  end
-
-end
diff --git a/logstash-core/spec/logstash/instrument/null_metric_spec.rb b/logstash-core/spec/logstash/instrument/null_metric_spec.rb
deleted file mode 100644
index 27a861eae69..00000000000
--- a/logstash-core/spec/logstash/instrument/null_metric_spec.rb
+++ /dev/null
@@ -1,23 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/null_metric"
-require "logstash/instrument/namespaced_metric"
-require_relative "../../support/shared_examples"
-require_relative "../../support/matchers"
-require "spec_helper"
-
-describe LogStash::Instrument::NullMetric do
-
-  let(:key) { "test" }
-  let(:collector) { [] }
-  subject { LogStash::Instrument::NullMetric.new(collector) }
-
-  it "defines the same interface as `Metric`" do
-    expect(described_class).to implement_interface_of(LogStash::Instrument::Metric)
-  end
-
-  describe "#namespace" do
-    it "return a NamespacedNullMetric" do
-      expect(subject.namespace(key)).to be_kind_of LogStash::Instrument::NamespacedNullMetric
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/pipeline_action/create_spec.rb b/logstash-core/spec/logstash/pipeline_action/create_spec.rb
index b5918813014..4215525547a 100644
--- a/logstash-core/spec/logstash/pipeline_action/create_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_action/create_spec.rb
@@ -3,11 +3,9 @@
 require_relative "../../support/helpers"
 require_relative "../../support/matchers"
 require "logstash/pipeline_action/create"
-require "logstash/instrument/null_metric"
 require "logstash/inputs/generator"
 
 describe LogStash::PipelineAction::Create do
-  let(:metric) { LogStash::Instrument::NullMetric.new(LogStash::Instrument::Collector.new) }
   let(:pipeline_config) { mock_pipeline_config(:main, "input { generator { id => '123' } } output { null {} }") }
   let(:pipelines) {  Hash.new }
   let(:agent) { double("agent") }
@@ -16,7 +14,7 @@
     clear_data_dir
   end
 
-  subject { described_class.new(pipeline_config, metric) }
+  subject { described_class.new(pipeline_config) }
 
   after do
     pipelines.each do |_, pipeline| 
@@ -29,7 +27,6 @@
     expect(subject.pipeline_id).to eq(:main)
   end
 
-
   context "when we have really short lived pipeline" do
     let(:pipeline_config) { mock_pipeline_config(:main, "input { generator { count => 1 } } output { null {} }") }
 
@@ -76,8 +73,8 @@
     let(:system_pipeline_config) { mock_pipeline_config(:main_2, "input { generator { id => '123' } } output { null {} }", { "pipeline.system" => true }) }
 
     it "should give higher priority to system pipeline" do
-      action_user_pipeline = described_class.new(pipeline_config, metric)
-      action_system_pipeline = described_class.new(system_pipeline_config, metric)
+      action_user_pipeline = described_class.new(pipeline_config)
+      action_system_pipeline = described_class.new(system_pipeline_config)
 
       sorted = [action_user_pipeline, action_system_pipeline].sort
       expect(sorted).to eq([action_system_pipeline, action_user_pipeline])
diff --git a/logstash-core/spec/logstash/pipeline_action/reload_spec.rb b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
index 60bb59686d8..d6aaad98ecd 100644
--- a/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_action/reload_spec.rb
@@ -3,10 +3,8 @@
 require_relative "../../support/helpers"
 require_relative "../../support/matchers"
 require "logstash/pipeline_action/reload"
-require "logstash/instrument/null_metric"
 
 describe LogStash::PipelineAction::Reload do
-  let(:metric) { LogStash::Instrument::NullMetric.new(LogStash::Instrument::Collector.new) }
   let(:pipeline_id) { :main }
   let(:new_pipeline_config) { mock_pipeline_config(pipeline_id, "input { generator { id => 'new' } } output { null {} }", { "pipeline.reloadable" => true}) }
   let(:pipeline_config) { "input { generator {} } output { null {} }" }
@@ -14,7 +12,7 @@
   let(:pipelines) { { pipeline_id => pipeline } }
   let(:agent) { double("agent") }
 
-  subject { described_class.new(new_pipeline_config, metric) }
+  subject { described_class.new(new_pipeline_config) }
 
   before do
     clear_data_dir
diff --git a/logstash-core/spec/logstash/pipeline_action/stop_spec.rb b/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
index e4971ec3352..593faca0fe8 100644
--- a/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_action/stop_spec.rb
@@ -3,7 +3,6 @@
 require_relative "../../support/helpers"
 require "logstash/pipeline_action/stop"
 require "logstash/pipeline"
-require "logstash/instrument/null_metric"
 
 describe LogStash::PipelineAction::Stop do
   let(:pipeline_config) { "input { generator {} } output { null {} }" }
diff --git a/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb b/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
index c8643c52877..06128131542 100644
--- a/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_dlq_commit_spec.rb
@@ -45,7 +45,6 @@ def close() end
       "path.dead_letter_queue" => Dir.mktmpdir
     }
   end
-  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
   let(:test_config) {
     <<-eos
         input { singlegenerator { id => input_id } }
@@ -56,9 +55,10 @@ def close() end
     eos
   }
 
-  subject { mock_pipeline_from_string(test_config, pipeline_settings_obj, metric) }
+  subject { mock_pipeline_from_string(test_config, pipeline_settings_obj) }
 
   before(:each) do
+    Witness.setInstance(Witness.new)
     pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
     allow(LogStash::Plugin).to receive(:lookup).with("input", "singlegenerator").and_return(SingleGeneratorInput)
     allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
diff --git a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
index 59f23ce07e9..d5ec769ed2d 100644
--- a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
@@ -67,11 +67,9 @@ def close
    let(:pipeline_settings) { { "queue.type" => queue_type, "pipeline.workers" => worker_thread_count, "pipeline.id" => pipeline_id} }
 
   let(:pipeline_config) { mock_pipeline_config(pipeline_id, config, pipeline_settings_obj) }
-  subject { described_class.new(pipeline_config, metric) }
+  subject { described_class.new(pipeline_config) }
 
   let(:counting_output) { PipelinePqFileOutput.new({ "id" => output_id }) }
-  let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
-  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
   let(:base_queue_path) { pipeline_settings_obj.get("path.queue") }
   let(:this_queue_folder) { File.join(base_queue_path, SecureRandom.hex(8)) }
 
@@ -91,6 +89,7 @@ def close
 
   before :each do
     FileUtils.mkdir_p(this_queue_folder)
+    Witness.setInstance(Witness.new)
 
     pipeline_settings_obj.set("path.queue", this_queue_folder)
     allow(PipelinePqFileOutput).to receive(:new).with(any_args).and_return(counting_output)
@@ -123,14 +122,13 @@ def close
     # Dir.rm_rf(this_queue_folder)
   end
 
-  let(:collected_metric) { metric_store.get_with_path("stats/pipelines/") }
+  let(:snitch) { Witness.instance.pipeline("main").events.snitch}
 
   it "populates the pipelines core metrics" do
-    _metric = collected_metric[:stats][:pipelines][:main][:events]
-    expect(_metric[:duration_in_millis].value).not_to be_nil
-    expect(_metric[:in].value).to eq(number_of_events)
-    expect(_metric[:filtered].value).to eq(number_of_events)
-    expect(_metric[:out].value).to eq(number_of_events)
+    expect(snitch.duration).not_to be_nil
+    expect(snitch.in).to eq(number_of_events)
+    expect(snitch.filtered).to eq(number_of_events)
+    expect(snitch.out).to eq(number_of_events)
     STDOUT.puts "  queue.type: #{pipeline_settings_obj.get("queue.type")}"
     STDOUT.puts "  queue.page_capacity: #{pipeline_settings_obj.get("queue.page_capacity") / 1024}KB"
     STDOUT.puts "  queue.max_bytes: #{pipeline_settings_obj.get("queue.max_bytes") / 1024}KB"
diff --git a/logstash-core/spec/logstash/pipeline_reporter_spec.rb b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
index 636f98c34ce..f72d7a32053 100644
--- a/logstash-core/spec/logstash/pipeline_reporter_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
@@ -5,6 +5,7 @@
 require_relative "../support/helpers"
 require_relative "../support/mocks_classes"
 
+java_import org.logstash.instrument.witness.Witness
 #TODO: Figure out how to add more tests that actually cover inflight events
 #This will require some janky multithreading stuff
 describe LogStash::PipelineReporter do
@@ -16,6 +17,7 @@
   let(:reporter) { pipeline.reporter }
 
   before do
+    Witness.setInstance(Witness.new)
     allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_call_original
     allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 5be9567dba3..66121eeadb0 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -8,6 +8,8 @@
 require "stud/try"
 require 'timeout'
 
+java_import org.logstash.instrument.witness.Witness
+
 class DummyInput < LogStash::Inputs::Base
   config_name "dummyinput"
   milestone 2
@@ -126,6 +128,7 @@ class TestPipeline < LogStash::Pipeline
   let(:timeout) {120} #seconds
 
   before :each do
+    Witness.setInstance(Witness.new)
     pipeline_workers_setting = LogStash::SETTINGS.get_setting("pipeline.workers")
     allow(pipeline_workers_setting).to receive(:default).and_return(worker_thread_count)
     dlq_enabled_setting = LogStash::SETTINGS.get_setting("dead_letter_queue.enable")
@@ -503,96 +506,7 @@ class TestPipeline < LogStash::Pipeline
     end
   end
 
-  context "metrics" do
-    config = "input { } filter { } output { }"
-
-    let(:settings) { LogStash::SETTINGS.clone }
-    subject { mock_pipeline_from_string(config, settings, metric) }
-
-    after :each do
-      subject.close
-    end
-
-    context "when metric.collect is disabled" do
-      before :each do
-        settings.set("metric.collect", false)
-      end
-
-      context "if namespaced_metric is nil" do
-        let(:metric) { nil }
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-      end
-
-      context "if namespaced_metric is a Metric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
-
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-
-      context "if namespaced_metric is a NullMetric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
-
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(::LogStash::Instrument::NullMetric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-    end
-
-    context "when metric.collect is enabled" do
-      before :each do
-        settings.set("metric.collect", true)
-      end
-
-      context "if namespaced_metric is nil" do
-        let(:metric) { nil }
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-      end
-
-      context "if namespaced_metric is a Metric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
-
-        it "uses a `Metric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::Metric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-
-      context "if namespaced_metric is a NullMetric object" do
-        let(:collector) { ::LogStash::Instrument::Collector.new }
-        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
-
-        it "uses a `NullMetric` object" do
-          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
-        end
-
-        it "uses the same collector" do
-          expect(subject.metric.collector).to be(collector)
-        end
-      end
-    end
-  end
-
-  context "Multiples pipelines" do
+ context "Multiples pipelines" do
     before do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinputgenerator").and_return(DummyInputGenerator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
@@ -774,9 +688,8 @@ class TestPipeline < LogStash::Pipeline
   end
 
   context "when collecting metrics in the pipeline" do
-    let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
 
-    subject { mock_pipeline_from_string(config, pipeline_settings_obj, metric) }
+    subject { mock_pipeline_from_string(config, pipeline_settings_obj) }
 
     let(:pipeline_settings) { { "pipeline.id" => pipeline_id } }
     let(:pipeline_id) { "main" }
@@ -809,7 +722,6 @@ class TestPipeline < LogStash::Pipeline
       EOS
     end
     let(:dummyoutput) { ::LogStash::Outputs::DummyOutput.new({ "id" => dummy_output_id }) }
-    let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
     let(:pipeline_thread) do
       # subject has to be called for the first time outside the thread because it will create a race condition
       # with the subject.ready? call since subject is lazily initialized
@@ -818,6 +730,7 @@ class TestPipeline < LogStash::Pipeline
     end
 
     before :each do
+      Witness.setInstance(Witness.new)
       allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
       allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
@@ -844,63 +757,65 @@ class TestPipeline < LogStash::Pipeline
     end
 
     context "global metric" do
-      let(:collected_metric) { metric_store.get_with_path("stats/events") }
+      let(:snitch) { Witness.instance.events.snitch}
 
       it "populates the different metrics" do
-        expect(collected_metric[:stats][:events][:duration_in_millis].value).not_to be_nil
-        expect(collected_metric[:stats][:events][:in].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:events][:filtered].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:events][:out].value).to eq(number_of_events)
+        expect(snitch.duration).not_to be_nil
+        expect(snitch.in).to eq(number_of_events)
+        expect(snitch.filtered).to eq(number_of_events)
+        expect(snitch.out).to eq(number_of_events)
       end
     end
 
     context "pipelines" do
-      let(:collected_metric) { metric_store.get_with_path("stats/pipelines/") }
+      let(:snitch) { Witness.instance.pipeline("main").events.snitch}
 
       it "populates the pipelines core metrics" do
-        expect(collected_metric[:stats][:pipelines][:main][:events][:duration_in_millis].value).not_to be_nil
-        expect(collected_metric[:stats][:pipelines][:main][:events][:in].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:events][:filtered].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:events][:out].value).to eq(number_of_events)
+        expect(snitch.duration).not_to be_nil
+        expect(snitch.in).to eq(number_of_events)
+        expect(snitch.filtered).to eq(number_of_events)
+        expect(snitch.out).to eq(number_of_events)
       end
 
       it "populates the filter metrics" do
         [dummy_id, dummy_id_other].map(&:to_sym).each do |id|
-          [:in, :out].each do |metric_key|
-            plugin_name = id.to_sym
-            expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:events][metric_key].value).to eq(number_of_events)
-          end
+          plugin_name = id.to_s
+          snitch = Witness.instance.pipeline("main").filters(plugin_name).events.snitch
+          expect(snitch.in).to eq(number_of_events)
+          expect(snitch.out).to eq(number_of_events)
         end
       end
 
       it "populates the output metrics" do
         plugin_name = dummy_output_id.to_sym
+        snitch = Witness.instance.pipeline("main").outputs(plugin_name).events.snitch
 
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:in].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:out].value).to eq(number_of_events)
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:duration_in_millis].value).not_to be_nil
+        expect(snitch.in).to eq(number_of_events)
+        expect(snitch.out).to eq(number_of_events)
+        expect(snitch.duration).not_to be_nil
       end
 
       it "populates the name of the output plugin" do
         plugin_name = dummy_output_id.to_sym
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:name].value).to eq(::LogStash::Outputs::DummyOutput.config_name)
+        expect(Witness.instance.pipeline("main").outputs(plugin_name).snitch.name).to eq(::LogStash::Outputs::DummyOutput.config_name)
       end
 
       it "populates the name of the filter plugin" do
         [dummy_id, dummy_id_other].map(&:to_sym).each do |id|
-          plugin_name = id.to_sym
-          expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:name].value).to eq(LogStash::Filters::DummyFilter.config_name)
+          plugin_name = id.to_s
+          expect(Witness.instance.pipeline("main").filters(plugin_name).snitch.name).to eq(LogStash::Filters::Multiline.config_name)
         end
       end
 
+
       context 'when dlq is disabled' do
         let (:collect_stats) { subject.collect_dlq_stats}
-        let (:collected_stats) { collected_metric[:stats][:pipelines][:main][:dlq]}
-        let (:available_stats) {[:path, :queue_size_in_bytes]}
+        let (:snitch) { Witness.instance.pipeline("main").dlq.snitch}
+
 
-        it 'should show not show any dlq stats' do
+        it 'should show not count any dlq stats' do
           collect_stats
-          expect(collected_stats).to be_nil
+          expect(snitch.queue_size_in_bytes).to be_nil
         end
 
       end
@@ -911,12 +826,12 @@ class TestPipeline < LogStash::Pipeline
         let (:pipeline_dlq_path) { "#{dead_letter_queue_path}/#{pipeline_id}"}
 
         let (:collect_stats) { subject.collect_dlq_stats }
-        let (:collected_stats) { collected_metric[:stats][:pipelines][:main][:dlq]}
+        let (:snitch) { Witness.instance.pipeline("main").dlq.snitch}
 
         it 'should show dlq stats' do
           collect_stats
           # A newly created dead letter queue with no entries will have a size of 1 (the version 'header')
-          expect(collected_stats[:queue_size_in_bytes].value).to eq(1)
+          expect(snitch.queue_size_in_bytes).to eql(1)
         end
       end
     end
