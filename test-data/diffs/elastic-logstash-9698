diff --git a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
index e5b3c3d7b36..316025371aa 100644
--- a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
+++ b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueRWBenchmark.java
@@ -15,12 +15,6 @@
 import org.logstash.ackedqueue.Queueable;
 import org.logstash.ackedqueue.Settings;
 import org.logstash.ackedqueue.SettingsImpl;
-import org.logstash.ackedqueue.io.ByteBufferPageIO;
-import org.logstash.ackedqueue.io.CheckpointIOFactory;
-import org.logstash.ackedqueue.io.FileCheckpointIO;
-import org.logstash.ackedqueue.io.MemoryCheckpointIO;
-import org.logstash.ackedqueue.io.MmapPageIO;
-import org.logstash.ackedqueue.io.PageIOFactory;
 import org.openjdk.jmh.annotations.Benchmark;
 import org.openjdk.jmh.annotations.BenchmarkMode;
 import org.openjdk.jmh.annotations.Fork;
@@ -34,10 +28,6 @@
 import org.openjdk.jmh.annotations.TearDown;
 import org.openjdk.jmh.annotations.Warmup;
 import org.openjdk.jmh.infra.Blackhole;
-import org.openjdk.jmh.runner.Runner;
-import org.openjdk.jmh.runner.RunnerException;
-import org.openjdk.jmh.runner.options.Options;
-import org.openjdk.jmh.runner.options.OptionsBuilder;
 
 @Warmup(iterations = 3, time = 100, timeUnit = TimeUnit.MILLISECONDS)
 @Measurement(iterations = 10, time = 100, timeUnit = TimeUnit.MILLISECONDS)
@@ -48,7 +38,7 @@
 public class QueueRWBenchmark {
 
     private static final int EVENTS_PER_INVOCATION = 500_000;
-    
+
     private static final int BATCH_SIZE = 100;
 
     private static final int ACK_INTERVAL = 1024;
@@ -58,16 +48,14 @@ public class QueueRWBenchmark {
     private ArrayBlockingQueue<Event> queueArrayBlocking;
 
     private Queue queuePersisted;
-    
-    private Queue queueMemory;
 
     private String path;
-    
+
     private ExecutorService exec;
 
     @Setup
-    public void setUp() throws IOException, CloneNotSupportedException {
-        final Settings settingsPersisted = settings(true);
+    public void setUp() throws IOException {
+        final Settings settingsPersisted = settings();
         EVENT.setField("Foo", "Bar");
         EVENT.setField("Foo1", "Bar1");
         EVENT.setField("Foo2", "Bar2");
@@ -76,16 +64,13 @@ public void setUp() throws IOException, CloneNotSupportedException {
         path = settingsPersisted.getDirPath();
         queuePersisted = new Queue(settingsPersisted);
         queueArrayBlocking = new ArrayBlockingQueue<>(ACK_INTERVAL);
-        queueMemory = new Queue(settings(false));
         queuePersisted.open();
-        queueMemory.open();
         exec = Executors.newSingleThreadExecutor();
     }
 
     @TearDown
     public void tearDown() throws IOException {
         queuePersisted.close();
-        queueMemory.close();
         queueArrayBlocking.clear();
         FileUtils.deleteDirectory(new File(path));
         exec.shutdownNow();
@@ -104,29 +89,7 @@ public final void readFromPersistedQueue(final Blackhole blackhole) throws Excep
             }
         });
         for (int i = 0; i < EVENTS_PER_INVOCATION / BATCH_SIZE; ++i) {
-            try (Batch batch = queuePersisted.readBatch(BATCH_SIZE)) {
-                for (final Queueable elem : batch.getElements()) {
-                    blackhole.consume(elem);
-                }
-            }
-        }
-        future.get();
-    }
-
-    @Benchmark
-    @OperationsPerInvocation(EVENTS_PER_INVOCATION)
-    public final void readFromMemoryQueue(final Blackhole blackhole) throws Exception {
-        final Future<?> future = exec.submit(() -> {
-            for (int i = 0; i < EVENTS_PER_INVOCATION; ++i) {
-                try {
-                    this.queueMemory.write(EVENT);
-                } catch (final IOException ex) {
-                    throw new IllegalStateException(ex);
-                }
-            }
-        });
-        for (int i = 0; i < EVENTS_PER_INVOCATION / BATCH_SIZE; ++i) {
-            try (Batch batch = queueMemory.readBatch(BATCH_SIZE)) {
+            try (Batch batch = queuePersisted.readBatch(BATCH_SIZE, TimeUnit.SECONDS.toMillis(1))) {
                 for (final Queueable elem : batch.getElements()) {
                     blackhole.consume(elem);
                 }
@@ -153,31 +116,12 @@ public final void readFromArrayBlockingQueue(final Blackhole blackhole) throws E
         future.get();
     }
 
-    public static void main(final String... args) throws RunnerException {
-        Options opt = new OptionsBuilder()
-            .include(QueueRWBenchmark.class.getSimpleName())
-            .forks(2)
-            .build();
-        new Runner(opt).run();
-    }
-
-    private static Settings settings(final boolean persisted) {
-        final PageIOFactory pageIOFactory;
-        final CheckpointIOFactory checkpointIOFactory;
-        if (persisted) {
-            pageIOFactory = MmapPageIO::new;
-            checkpointIOFactory = FileCheckpointIO::new;
-        } else {
-            pageIOFactory = ByteBufferPageIO::new;
-            checkpointIOFactory = MemoryCheckpointIO::new;
-        }
+    private static Settings settings() {
         return SettingsImpl.fileSettingsBuilder(Files.createTempDir().getPath())
             .capacity(256 * 1024 * 1024)
             .queueMaxBytes(Long.MAX_VALUE)
-            .elementIOFactory(pageIOFactory)
             .checkpointMaxWrites(ACK_INTERVAL)
             .checkpointMaxAcks(ACK_INTERVAL)
-            .checkpointIOFactory(checkpointIOFactory)
             .elementClass(Event.class).build();
     }
 }
diff --git a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java
index a81bf3429ae..c35c37f997c 100644
--- a/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java
+++ b/logstash-core/benchmarks/src/main/java/org/logstash/benchmark/QueueWriteBenchmark.java
@@ -10,8 +10,6 @@
 import org.logstash.ackedqueue.Queue;
 import org.logstash.ackedqueue.Settings;
 import org.logstash.ackedqueue.SettingsImpl;
-import org.logstash.ackedqueue.io.FileCheckpointIO;
-import org.logstash.ackedqueue.io.MmapPageIO;
 import org.openjdk.jmh.annotations.Benchmark;
 import org.openjdk.jmh.annotations.BenchmarkMode;
 import org.openjdk.jmh.annotations.Fork;
@@ -24,10 +22,6 @@
 import org.openjdk.jmh.annotations.State;
 import org.openjdk.jmh.annotations.TearDown;
 import org.openjdk.jmh.annotations.Warmup;
-import org.openjdk.jmh.runner.Runner;
-import org.openjdk.jmh.runner.RunnerException;
-import org.openjdk.jmh.runner.options.Options;
-import org.openjdk.jmh.runner.options.OptionsBuilder;
 
 @Warmup(iterations = 3, time = 100, timeUnit = TimeUnit.MILLISECONDS)
 @Measurement(iterations = 10, time = 100, timeUnit = TimeUnit.MILLISECONDS)
@@ -74,22 +68,12 @@ public final void pushToPersistedQueue() throws Exception {
         }
     }
 
-    public static void main(final String... args) throws RunnerException {
-        Options opt = new OptionsBuilder()
-            .include(QueueWriteBenchmark.class.getSimpleName())
-            .forks(2)
-            .build();
-        new Runner(opt).run();
-    }
-
     private static Settings settings() {
         return SettingsImpl.fileSettingsBuilder(Files.createTempDir().getPath())
             .capacity(256 * 1024 * 1024)
             .queueMaxBytes(Long.MAX_VALUE)
-            .elementIOFactory(MmapPageIO::new)
             .checkpointMaxWrites(1024)
             .checkpointMaxAcks(1024)
-            .checkpointIOFactory(FileCheckpointIO::new)
             .elementClass(Event.class).build();
     }
 }
diff --git a/logstash-core/build.gradle b/logstash-core/build.gradle
index 2fce0e12fc0..3c9a2b68225 100644
--- a/logstash-core/build.gradle
+++ b/logstash-core/build.gradle
@@ -107,6 +107,7 @@ dependencies {
     compile "com.fasterxml.jackson.core:jackson-annotations:${jacksonVersion}"
     compile "com.fasterxml.jackson.module:jackson-module-afterburner:${jacksonVersion}"
     compile "com.fasterxml.jackson.dataformat:jackson-dataformat-cbor:${jacksonVersion}"
+    compile group: 'com.google.guava', name: 'guava', version: '22.0'
     testCompile 'org.apache.logging.log4j:log4j-core:2.6.2:tests'
     testCompile 'org.apache.logging.log4j:log4j-api:2.6.2:tests'
     testCompile 'junit:junit:4.12'
diff --git a/logstash-core/gemspec_jars.rb b/logstash-core/gemspec_jars.rb
index 89babbddb8e..f9da1dc2fcd 100644
--- a/logstash-core/gemspec_jars.rb
+++ b/logstash-core/gemspec_jars.rb
@@ -5,8 +5,9 @@
 gem.requirements << "jar org.apache.logging.log4j:log4j-slf4j-impl, 2.6.2"
 gem.requirements << "jar org.apache.logging.log4j:log4j-api, 2.6.2"
 gem.requirements << "jar org.apache.logging.log4j:log4j-core, 2.6.2"
-gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.9.4"
-gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.9.4"
-gem.requirements << "jar com.fasterxml.jackson.core:jackson-annotations, 2.9.4"
-gem.requirements << "jar com.fasterxml.jackson.module:jackson-module-afterburner, 2.9.4"
-gem.requirements << "jar com.fasterxml.jackson.dataformat:jackson-dataformat-cbor, 2.9.4"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.9.5"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.9.5"
+gem.requirements << "jar com.fasterxml.jackson.core:jackson-annotations, 2.9.5"
+gem.requirements << "jar com.fasterxml.jackson.module:jackson-module-afterburner, 2.9.5"
+gem.requirements << "jar com.fasterxml.jackson.dataformat:jackson-dataformat-cbor, 2.9.5"
+gem.requirements << "jar com.google.guava:guava, 22.0"
diff --git a/logstash-core/lib/logstash-core_jars.rb b/logstash-core/lib/logstash-core_jars.rb
index 63eb1008ee4..6e20b4dc2a0 100644
--- a/logstash-core/lib/logstash-core_jars.rb
+++ b/logstash-core/lib/logstash-core_jars.rb
@@ -2,25 +2,35 @@
 begin
   require 'jar_dependencies'
 rescue LoadError
+  require 'com/fasterxml/jackson/core/jackson-annotations/2.9.5/jackson-annotations-2.9.5.jar'
   require 'org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar'
-  require 'org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar'
+  require 'com/google/guava/guava/22.0/guava-22.0.jar'
+  require 'com/fasterxml/jackson/core/jackson-core/2.9.5/jackson-core-2.9.5.jar'
   require 'org/slf4j/slf4j-api/1.7.21/slf4j-api-1.7.21.jar'
-  require 'com/fasterxml/jackson/core/jackson-annotations/2.9.4/jackson-annotations-2.9.4.jar'
+  require 'com/google/code/findbugs/jsr305/1.3.9/jsr305-1.3.9.jar'
+  require 'com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.9.5/jackson-dataformat-cbor-2.9.5.jar'
+  require 'com/google/j2objc/j2objc-annotations/1.1/j2objc-annotations-1.1.jar'
+  require 'org/codehaus/mojo/animal-sniffer-annotations/1.14/animal-sniffer-annotations-1.14.jar'
+  require 'org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar'
   require 'org/apache/logging/log4j/log4j-slf4j-impl/2.6.2/log4j-slf4j-impl-2.6.2.jar'
-  require 'com/fasterxml/jackson/core/jackson-databind/2.9.4/jackson-databind-2.9.4.jar'
-  require 'com/fasterxml/jackson/core/jackson-core/2.9.4/jackson-core-2.9.4.jar'
-  require 'com/fasterxml/jackson/module/jackson-module-afterburner/2.9.4/jackson-module-afterburner-2.9.4.jar'
-  require 'com/fasterxml/jackson/dataformat/jackson-dataformat-cbor/2.9.4/jackson-dataformat-cbor-2.9.4.jar'
+  require 'com/google/errorprone/error_prone_annotations/2.0.18/error_prone_annotations-2.0.18.jar'
+  require 'com/fasterxml/jackson/core/jackson-databind/2.9.5/jackson-databind-2.9.5.jar'
+  require 'com/fasterxml/jackson/module/jackson-module-afterburner/2.9.5/jackson-module-afterburner-2.9.5.jar'
 end
 
 if defined? Jars
+  require_jar 'com.fasterxml.jackson.core', 'jackson-annotations', '2.9.5'
   require_jar 'org.apache.logging.log4j', 'log4j-core', '2.6.2'
-  require_jar 'org.apache.logging.log4j', 'log4j-api', '2.6.2'
+  require_jar 'com.google.guava', 'guava', '22.0'
+  require_jar 'com.fasterxml.jackson.core', 'jackson-core', '2.9.5'
   require_jar 'org.slf4j', 'slf4j-api', '1.7.21'
-  require_jar 'com.fasterxml.jackson.core', 'jackson-annotations', '2.9.4'
+  require_jar 'com.google.code.findbugs', 'jsr305', '1.3.9'
+  require_jar 'com.fasterxml.jackson.dataformat', 'jackson-dataformat-cbor', '2.9.5'
+  require_jar 'com.google.j2objc', 'j2objc-annotations', '1.1'
+  require_jar 'org.codehaus.mojo', 'animal-sniffer-annotations', '1.14'
+  require_jar 'org.apache.logging.log4j', 'log4j-api', '2.6.2'
   require_jar 'org.apache.logging.log4j', 'log4j-slf4j-impl', '2.6.2'
-  require_jar 'com.fasterxml.jackson.core', 'jackson-databind', '2.9.4'
-  require_jar 'com.fasterxml.jackson.core', 'jackson-core', '2.9.4'
-  require_jar 'com.fasterxml.jackson.module', 'jackson-module-afterburner', '2.9.4'
-  require_jar 'com.fasterxml.jackson.dataformat', 'jackson-dataformat-cbor', '2.9.4'
+  require_jar 'com.google.errorprone', 'error_prone_annotations', '2.0.18'
+  require_jar 'com.fasterxml.jackson.core', 'jackson-databind', '2.9.5'
+  require_jar 'com.fasterxml.jackson.module', 'jackson-module-afterburner', '2.9.5'
 end
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 7f765d6f37d..0e8d6dd56fd 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -45,7 +45,7 @@ module Environment
             Setting::String.new("http.host", "127.0.0.1"),
             Setting::PortRange.new("http.port", 9600..9700),
             Setting::String.new("http.environment", "production"),
-            Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked"]),
+            Setting::String.new("queue.type", "memory", true, ["persisted", "memory"]),
             Setting::Boolean.new("queue.drain", false),
             Setting::Bytes.new("queue.page_capacity", "250mb"),
             Setting::Bytes.new("queue.max_bytes", "1024mb"),
diff --git a/logstash-core/lib/logstash/queue_factory.rb b/logstash-core/lib/logstash/queue_factory.rb
index 70b215557f4..a1574a499f3 100644
--- a/logstash-core/lib/logstash/queue_factory.rb
+++ b/logstash-core/lib/logstash/queue_factory.rb
@@ -19,10 +19,6 @@ def self.create(settings)
       queue_path = ::File.join(settings.get("path.queue"), settings.get("pipeline.id"))
 
       case queue_type
-      when "memory_acked"
-        # memory_acked is used in tests/specs
-        FileUtils.mkdir_p(queue_path)
-        LogStash::Util::WrappedAckedQueue.create_memory_based(queue_path, queue_page_capacity, queue_max_events, queue_max_bytes)
       when "persisted"
         # persisted is the disk based acked queue
         FileUtils.mkdir_p(queue_path)
@@ -31,7 +27,7 @@ def self.create(settings)
         # memory is the legacy and default setting
         LogStash::Util::WrappedSynchronousQueue.new
       else
-        raise ConfigurationError, "Invalid setting `#{queue_type}` for `queue.type`, supported types are: 'memory_acked', 'memory', 'persisted'"
+        raise ConfigurationError, "Invalid setting `#{queue_type}` for `queue.type`, supported types are 'memory' and 'persisted'"
       end
     end
   end
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index 2d5811ed065..3c1d9247e68 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -306,7 +306,7 @@ def execute
     end
 
     # lock path.data before starting the agent
-    @data_path_lock = FileLockFactory.getDefault().obtainLock(setting("path.data"), ".lock");
+    @data_path_lock = FileLockFactory.obtainLock(java.nio.file.Paths.get(setting("path.data")).to_absolute_path, ".lock")
 
     @agent = create_agent(@settings)
 
@@ -351,7 +351,7 @@ def execute
     Stud::untrap("INT", sigint_id) unless sigint_id.nil?
     Stud::untrap("TERM", sigterm_id) unless sigterm_id.nil?
     Stud::untrap("HUP", sighup_id) unless sighup_id.nil?
-    FileLockFactory.getDefault().releaseLock(@data_path_lock) if @data_path_lock
+    FileLockFactory.releaseLock(@data_path_lock) if @data_path_lock
     @log_fd.close if @log_fd
   end # def self.main
 
diff --git a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
index 351ec54b310..7c39aa656b7 100644
--- a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
@@ -20,12 +20,6 @@ class WrappedAckedQueue
     class QueueClosedError < ::StandardError; end
     class NotImplementedError < ::StandardError; end
 
-    def self.create_memory_based(path, capacity, max_events, max_bytes)
-      self.allocate.with_queue(
-        LogStash::AckedMemoryQueue.new(path, capacity, max_events, max_bytes)
-      )
-    end
-
     def self.create_file_based(path, capacity, max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, max_bytes)
       self.allocate.with_queue(
         LogStash::AckedQueue.new(path, capacity, max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, max_bytes)
diff --git a/logstash-core/spec/logstash/instrument/wrapped_write_client_spec.rb b/logstash-core/spec/logstash/instrument/wrapped_write_client_spec.rb
index d01ae4dd24b..14e68f831d2 100644
--- a/logstash-core/spec/logstash/instrument/wrapped_write_client_spec.rb
+++ b/logstash-core/spec/logstash/instrument/wrapped_write_client_spec.rb
@@ -110,19 +110,4 @@ def threaded_read_client
 
     include_examples "queue tests"
   end
-
-  context "AckedMemoryQueue" do
-    let(:queue) { LogStash::Util::WrappedAckedQueue.create_memory_based("", 1024, 10, 4096) }
-
-    before do
-      read_client.set_events_metric(metric.namespace([:stats, :events]))
-      read_client.set_pipeline_metric(metric.namespace([:stats, :pipelines, :main, :events]))
-    end
-
-    after do
-      queue.close
-    end
-
-    include_examples "queue tests"
-  end
 end
diff --git a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
index 65557f824e7..5482a4ba7e1 100644
--- a/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_pq_file_spec.rb
@@ -79,7 +79,7 @@ def close
   let(:number_of_events) { 100_000 }
   let(:page_capacity) { 1 * 1024 * 512 } # 1 128
   let(:max_bytes) { 1024 * 1024 * 1024 } # 1 gb
-  let(:queue_type) { "persisted" } #  "memory" "memory_acked"
+  let(:queue_type) { "persisted" } #  "memory"
   let(:times) { [] }
 
   let(:pipeline_thread) do
diff --git a/logstash-core/spec/logstash/queue_factory_spec.rb b/logstash-core/spec/logstash/queue_factory_spec.rb
index 9182c9c95be..2d21c136f19 100644
--- a/logstash-core/spec/logstash/queue_factory_spec.rb
+++ b/logstash-core/spec/logstash/queue_factory_spec.rb
@@ -8,7 +8,7 @@
   let(:settings_array) do
     [
       LogStash::Setting::WritableDirectory.new("path.queue", Stud::Temporary.pathname),
-      LogStash::Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked"]),
+      LogStash::Setting::String.new("queue.type", "memory", true, ["persisted", "memory"]),
       LogStash::Setting::Bytes.new("queue.page_capacity", "250mb"),
       LogStash::Setting::Bytes.new("queue.max_bytes", "1024mb"),
       LogStash::Setting::Numeric.new("queue.max_events", 0),
@@ -57,18 +57,6 @@
     end
   end
 
-  context "when `queue.type` is `memory_acked`" do
-    before do
-      settings.set("queue.type", "memory_acked")
-    end
-
-    it "returns a `WrappedAckedQueue`" do
-      queue =  subject.create(settings)
-      expect(queue).to be_kind_of(LogStash::Util::WrappedAckedQueue)
-      queue.close
-    end
-  end
-
   context "when `queue.type` is `memory`" do
     before do
       settings.set("queue.type", "memory")
diff --git a/logstash-core/spec/logstash/util/wrapped_acked_queue_spec.rb b/logstash-core/spec/logstash/util/wrapped_acked_queue_spec.rb
index 6d2d2794674..3a12c743c8c 100644
--- a/logstash-core/spec/logstash/util/wrapped_acked_queue_spec.rb
+++ b/logstash-core/spec/logstash/util/wrapped_acked_queue_spec.rb
@@ -30,20 +30,6 @@
     end
   end
 
-  context "memory" do
-    let(:page_capacity) { 1024 }
-    let(:max_events) { 0 }
-    let(:max_bytes) { 0 }
-    let(:path) { Stud::Temporary.directory }
-    let(:queue) { LogStash::Util::WrappedAckedQueue.create_memory_based(path, page_capacity, max_events, max_bytes) }
-
-    after do
-      queue.close
-    end
-
-    include_examples "queue tests"
-  end
-
   context "persisted" do
     let(:page_capacity) { 1024 }
     let(:max_events) { 0 }
diff --git a/logstash-core/src/main/java/JrubyAckedQueueExtService.java b/logstash-core/src/main/java/JrubyAckedQueueExtService.java
index 8b349646e2d..e507d7589b4 100644
--- a/logstash-core/src/main/java/JrubyAckedQueueExtService.java
+++ b/logstash-core/src/main/java/JrubyAckedQueueExtService.java
@@ -1,16 +1,13 @@
+import java.io.IOException;
 import org.jruby.Ruby;
 import org.jruby.runtime.load.BasicLibraryService;
 import org.logstash.ackedqueue.ext.JrubyAckedQueueExtLibrary;
-import org.logstash.ackedqueue.ext.JrubyAckedQueueMemoryExtLibrary;
-
-import java.io.IOException;
 
 public class JrubyAckedQueueExtService implements BasicLibraryService {
     public boolean basicLoad(final Ruby runtime)
             throws IOException
     {
         new JrubyAckedQueueExtLibrary().load(runtime, false);
-        new JrubyAckedQueueMemoryExtLibrary().load(runtime, false);
         return true;
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/FileLockFactory.java b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
index c583cadb610..e3a513a30d9 100644
--- a/logstash-core/src/main/java/org/logstash/FileLockFactory.java
+++ b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
@@ -22,7 +22,6 @@
 import java.io.IOException;
 import java.nio.channels.FileChannel;
 import java.nio.channels.FileLock;
-import java.nio.file.FileSystems;
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
@@ -44,25 +43,13 @@
  */
 public class FileLockFactory {
 
-    /**
-     * Singleton instance
-     */
-    public static final FileLockFactory INSTANCE = new FileLockFactory();
 
     private FileLockFactory() {}
 
     private static final Set<String> LOCK_HELD = Collections.synchronizedSet(new HashSet<>());
     private static final Map<FileLock, String> LOCK_MAP =  Collections.synchronizedMap(new HashMap<>());
 
-    public static final FileLockFactory getDefault() {
-        return FileLockFactory.INSTANCE;
-    }
-
-    public FileLock obtainLock(String lockDir, String lockName) throws IOException {
-        Path dirPath = FileSystems.getDefault().getPath(lockDir);
-
-        // Ensure that lockDir exists and is a directory.
-        // note: this will fail if lockDir is a symlink
+    public static FileLock obtainLock(Path dirPath, String lockName) throws IOException {
         Files.createDirectories(dirPath);
 
         Path lockPath = dirPath.resolve(lockName);
@@ -110,7 +97,7 @@ public FileLock obtainLock(String lockDir, String lockName) throws IOException {
         }
     }
 
-    public void releaseLock(FileLock lock) throws IOException {
+    public static void releaseLock(FileLock lock) throws IOException {
         String lockPath = LOCK_MAP.remove(lock);
         if (lockPath == null) { throw new LockException("Cannot release unobtained lock"); }
         lock.release();
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
index 0bf0720facf..3e582786a90 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Batch.java
@@ -2,29 +2,40 @@
 
 import java.io.Closeable;
 import java.io.IOException;
+import java.util.ArrayList;
 import java.util.List;
 import java.util.concurrent.atomic.AtomicBoolean;
-import org.logstash.ackedqueue.io.LongVector;
 
 public class Batch implements Closeable {
 
     private final List<Queueable> elements;
 
-    private final LongVector seqNums;
+    private final long firstSeqNum;
+
     private final Queue queue;
     private final AtomicBoolean closed;
 
-    public Batch(List<Queueable> elements, LongVector seqNums, Queue q) {
-        this.elements = elements;
-        this.seqNums = seqNums;
+    public Batch(SequencedList<byte[]> serialized, Queue q) {
+        this(
+            serialized.getElements(),
+            serialized.getSeqNums().size() == 0 ? -1L : serialized.getSeqNums().get(0), q
+        );
+    }
+
+    public Batch(List<byte[]> elements, long firstSeqNum, Queue q) {
+        this.elements = deserializeElements(elements, q);
+        this.firstSeqNum = elements.isEmpty() ? -1L : firstSeqNum;
         this.queue = q;
         this.closed = new AtomicBoolean(false);
     }
 
     // close acks the batch ackable events
+    @Override
     public void close() throws IOException {
         if (closed.getAndSet(true) == false) {
-              this.queue.ack(this.seqNums);
+            if (firstSeqNum >= 0L) {
+                this.queue.ack(firstSeqNum, elements.size());
+            }
         } else {
             // TODO: how should we handle double-closing?
             throw new IOException("double closing batch");
@@ -39,9 +50,21 @@ public List<? extends Queueable> getElements() {
         return elements;
     }
 
-    public LongVector getSeqNums() { return this.seqNums; }
-
     public Queue getQueue() {
         return queue;
     }
+
+    /**
+     *
+     * @param serialized Collection of serialized elements
+     * @param q {@link Queue} instance
+     * @return Collection of deserialized {@link Queueable} elements
+     */
+    private static List<Queueable> deserializeElements(List<byte[]> serialized, Queue q) {
+        final List<Queueable> deserialized = new ArrayList<>(serialized.size());
+        for (final byte[] element : serialized) {
+            deserialized.add(q.deserialize(element));
+        }
+        return deserialized;
+    }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
deleted file mode 100644
index 15183ea0554..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
+++ /dev/null
@@ -1,127 +0,0 @@
-package org.logstash.ackedqueue;
-
-import org.logstash.ackedqueue.io.CheckpointIO;
-import org.logstash.ackedqueue.io.PageIO;
-
-import java.io.IOException;
-import java.util.BitSet;
-
-public class HeadPage extends Page {
-
-    // create a new HeadPage object and new page.{pageNum} empty valid data file
-    public HeadPage(int pageNum, Queue queue, PageIO pageIO) {
-        super(pageNum, queue, 0, 0, 0, new BitSet(), pageIO);
-    }
-
-    // create a new HeadPage object from an existing checkpoint and open page.{pageNum} empty valid data file
-    // @param pageIO is expected to be open/recover/create
-    public HeadPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) {
-        super(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO);
-
-        assert checkpoint.getMinSeqNum() == pageIO.getMinSeqNum() && checkpoint.getElementCount() == pageIO.getElementCount() :
-                String.format("checkpoint minSeqNum=%d or elementCount=%d is different than pageIO minSeqNum=%d or elementCount=%d", checkpoint.getMinSeqNum(), checkpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
-
-        // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
-        if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
-            this.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
-        }
-    }
-
-    // verify if data size plus overhead is not greater than the page capacity
-    public boolean hasCapacity(int byteSize) {
-        return this.pageIO.persistedByteCount(byteSize) <= this.pageIO.getCapacity();
-    }
-
-    public boolean hasSpace(int byteSize) {
-        return this.pageIO.hasSpace((byteSize));
-    }
-
-    // NOTE:
-    // we have a page concern inconsistency where readBatch() takes care of the
-    // deserialization and returns a Batch object which contains the deserialized
-    // elements objects of the proper elementClass but HeadPage.write() deals with
-    // a serialized element byte[] and serialization is done at the Queue level to
-    // be able to use the Page.hasSpace() method with the serialized element byte size.
-    //
-    public void write(byte[] bytes, long seqNum, int checkpointMaxWrites) throws IOException {
-        this.pageIO.write(bytes, seqNum);
-
-        if (this.minSeqNum <= 0) {
-            this.minSeqNum = seqNum;
-            this.firstUnreadSeqNum = seqNum;
-        }
-        this.elementCount++;
-
-        // force a checkpoint if we wrote checkpointMaxWrites elements since last checkpoint
-        // the initial condition of an "empty" checkpoint, maxSeqNum() will return -1
-        if (checkpointMaxWrites > 0 && (seqNum >= this.lastCheckpoint.maxSeqNum() + checkpointMaxWrites)) {
-            // did we write more than checkpointMaxWrites elements? if so checkpoint now
-            checkpoint();
-        }
-    }
-
-    public void ensurePersistedUpto(long seqNum) throws IOException {
-        long lastCheckpointUptoSeqNum = this.lastCheckpoint.getMinSeqNum() + this.lastCheckpoint.getElementCount();
-
-        // if the last checkpoint for this headpage already included the given seqNum, no need to fsync/checkpoint
-        if (seqNum > lastCheckpointUptoSeqNum) {
-            // head page checkpoint does a data file fsync
-            checkpoint();
-        }
-    }
-
-    public TailPage behead() throws IOException {
-        checkpoint();
-
-        TailPage tailPage = new TailPage(this);
-
-        // first thing that must be done after beheading is to create a new checkpoint for that new tail page
-        // tail page checkpoint does NOT includes a fsync
-        tailPage.checkpoint();
-
-        // TODO: should we have a better deactivation strategy to avoid too rapid reactivation scenario?
-        Page firstUnreadPage = queue.firstUnreadPage();
-        if (firstUnreadPage == null || (tailPage.getPageNum() > firstUnreadPage.getPageNum())) {
-            // deactivate if this new tailPage is not where the read is occurring
-            tailPage.getPageIO().deactivate();
-        }
-
-        return tailPage;
-    }
-
-    // head page checkpoint, fsync data page if writes occured since last checkpoint
-    // update checkpoint only if it changed since lastCheckpoint
-    public void checkpoint() throws IOException {
-
-         if (this.elementCount > this.lastCheckpoint.getElementCount()) {
-             // fsync & checkpoint if data written since last checkpoint
-
-             this.pageIO.ensurePersisted();
-             forceCheckpoint();
-        } else {
-             Checkpoint checkpoint = new Checkpoint(this.pageNum, this.queue.firstUnackedPageNum(), firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
-             if (! checkpoint.equals(this.lastCheckpoint)) {
-                 // checkpoint only if it changed since last checkpoint
-
-                 // non-dry code with forceCheckpoint() to avoid unnecessary extra new Checkpoint object creation
-                 CheckpointIO io = queue.getCheckpointIO();
-                 io.write(io.headFileName(), checkpoint);
-                 this.lastCheckpoint = checkpoint;
-             }
-         }
-      }
-
-    // unconditionally update head checkpoint
-    public void forceCheckpoint() throws IOException {
-        Checkpoint checkpoint = new Checkpoint(this.pageNum, this.queue.firstUnackedPageNum(), firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
-        CheckpointIO io = queue.getCheckpointIO();
-        io.write(io.headFileName(), checkpoint);
-        this.lastCheckpoint = checkpoint;
-    }
-
-    public void close() throws IOException {
-        checkpoint();
-        this.pageIO.close();
-    }
-
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
index d4aaa8d4e95..a9d307a2bda 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
@@ -1,20 +1,20 @@
 package org.logstash.ackedqueue;
 
+import com.google.common.primitives.Ints;
 import java.io.Closeable;
 import java.io.IOException;
-import java.util.ArrayList;
 import java.util.BitSet;
-import java.util.List;
-import org.logstash.ackedqueue.io.LongVector;
+import org.logstash.ackedqueue.io.CheckpointIO;
 import org.logstash.ackedqueue.io.PageIO;
 
-public abstract class Page implements Closeable {
+public final class Page implements Closeable {
     protected final int pageNum;
     protected long minSeqNum; // TODO: see if we can make it final?
     protected int elementCount;
     protected long firstUnreadSeqNum;
     protected final Queue queue;
     protected PageIO pageIO;
+    private boolean writable;
 
     // bit 0 is minSeqNum
     // TODO: go steal LocalCheckpointService in feature/seq_no from ES
@@ -22,7 +22,7 @@ public abstract class Page implements Closeable {
     protected BitSet ackedSeqNums;
     protected Checkpoint lastCheckpoint;
 
-    public Page(int pageNum, Queue queue, long minSeqNum, int elementCount, long firstUnreadSeqNum, BitSet ackedSeqNums, PageIO pageIO) {
+    public Page(int pageNum, Queue queue, long minSeqNum, int elementCount, long firstUnreadSeqNum, BitSet ackedSeqNums, PageIO pageIO, boolean writable) {
         this.pageNum = pageNum;
         this.queue = queue;
 
@@ -32,40 +32,52 @@ public Page(int pageNum, Queue queue, long minSeqNum, int elementCount, long fir
         this.ackedSeqNums = ackedSeqNums;
         this.lastCheckpoint = new Checkpoint(0, 0, 0, 0, 0);
         this.pageIO = pageIO;
+        this.writable = writable;
+
+        assert this.pageIO != null : "invalid null pageIO";
     }
 
     public String toString() {
         return "pageNum=" + this.pageNum + ", minSeqNum=" + this.minSeqNum + ", elementCount=" + this.elementCount + ", firstUnreadSeqNum=" + this.firstUnreadSeqNum;
     }
 
-    // NOTE:
-    // we have a page concern inconsistency where readBatch() takes care of the
-    // deserialization and returns a Batch object which contains the deserialized
-    // elements objects of the proper elementClass but HeadPage.write() deals with
-    // a serialized element byte[] and serialization is done at the Queue level to
-    // be able to use the Page.hasSpace() method with the serialized element byte size.
-    //
-    // @param limit the batch size limit
-    // @param elementClass the concrete element class for deserialization
-    // @return Batch batch of elements read when the number of elements can be <= limit
-    public Batch readBatch(int limit) throws IOException {
-
+    /**
+     * @param limit the maximum number of elements to read, actual number readcan be smaller
+     * @return {@link SequencedList} collection of serialized elements read
+     * @throws IOException
+     */
+    public SequencedList<byte[]> read(int limit) throws IOException {
         // first make sure this page is activated, activating previously activated is harmless
         this.pageIO.activate();
 
         SequencedList<byte[]> serialized = this.pageIO.read(this.firstUnreadSeqNum, limit);
-        List<byte[]> elements = serialized.getElements();
-        final int count = elements.size();
-        List<Queueable> deserialized = new ArrayList<>(count);
-        for (final byte[] element : elements) {
-            deserialized.add(this.queue.deserialize(element));
-        }
         assert serialized.getSeqNums().get(0) == this.firstUnreadSeqNum :
             String.format("firstUnreadSeqNum=%d != first result seqNum=%d", this.firstUnreadSeqNum, serialized.getSeqNums().get(0));
 
-        this.firstUnreadSeqNum += count;
+        this.firstUnreadSeqNum += serialized.getElements().size();
 
-        return new Batch(deserialized, serialized.getSeqNums(), this.queue);
+        return serialized;
+    }
+
+    public void write(byte[] bytes, long seqNum, int checkpointMaxWrites) throws IOException {
+        if (! this.writable) {
+            throw new IllegalStateException(String.format("page=%d is not writable", this.pageNum));
+        }
+
+        this.pageIO.write(bytes, seqNum);
+
+        if (this.minSeqNum <= 0) {
+            this.minSeqNum = seqNum;
+            this.firstUnreadSeqNum = seqNum;
+        }
+        this.elementCount++;
+
+        // force a checkpoint if we wrote checkpointMaxWrites elements since last checkpoint
+        // the initial condition of an "empty" checkpoint, maxSeqNum() will return -1
+        if (checkpointMaxWrites > 0 && (seqNum >= this.lastCheckpoint.maxSeqNum() + checkpointMaxWrites)) {
+            // did we write more than checkpointMaxWrites elements? if so checkpoint now
+            checkpoint();
+        }
     }
 
     /**
@@ -85,59 +97,157 @@ public boolean isFullyRead() {
     }
 
     public boolean isFullyAcked() {
-        // TODO: it should be something similar to this when we use a proper bitset class like ES
-        // this.ackedSeqNum.firstUnackedBit >= this.elementCount;
-        // TODO: for now use a naive & inefficient mechanism with a simple Bitset
-        return this.elementCount > 0 && this.ackedSeqNums.cardinality() >= this.elementCount;
+        final int cardinality = ackedSeqNums.cardinality();
+        return elementCount > 0 && cardinality == ackedSeqNums.length()
+            && cardinality == elementCount;
     }
 
     public long unreadCount() {
         return this.elementCount <= 0 ? 0 : Math.max(0, (maxSeqNum() - this.firstUnreadSeqNum) + 1);
     }
 
-    // update the page acking bitset. trigger checkpoint on the page if it is fully acked or if we acked more than the
-    // configured threshold checkpointMaxAcks.
-    // note that if the fully acked tail page is the first unacked page, it is not really necessary to also checkpoint
-    // the head page to update firstUnackedPageNum because it will be updated in the next upcoming head page checkpoint
-    // and in a crash condition, the Queue open recovery will detect and purge fully acked pages
-    //
-    // @param seqNums the list of same-page seqNums to ack
-    // @param checkpointMaxAcks the number of acks that will trigger a page checkpoint
-    public void ack(LongVector seqNums, int checkpointMaxAcks) throws IOException {
-        final int count = seqNums.size();
-        for (int i = 0; i < count; ++i) {
-            final long seqNum = seqNums.get(i);
-            // TODO: eventually refactor to use new bit handling class
-
-            assert seqNum >= this.minSeqNum :
-                    String.format("seqNum=%d is smaller than minSeqnum=%d", seqNum, this.minSeqNum);
-
-            assert seqNum < this.minSeqNum + this.elementCount:
-                    String.format("seqNum=%d is greater than minSeqnum=%d + elementCount=%d = %d", seqNum, this.minSeqNum, this.elementCount, this.minSeqNum + this.elementCount);
-            int index = (int)(seqNum - this.minSeqNum);
-
-            this.ackedSeqNums.set(index);
-        }
-
+    /**
+     * update the page acking bitset. trigger checkpoint on the page if it is fully acked or if we acked more than the
+     * configured threshold checkpointMaxAcks.
+     * note that if the fully acked tail page is the first unacked page, it is not really necessary to also checkpoint
+     * the head page to update firstUnackedPageNum because it will be updated in the next upcoming head page checkpoint
+     * and in a crash condition, the Queue open recovery will detect and purge fully acked pages
+     *
+     * @param firstSeqNum Lowest sequence number to ack
+     * @param count Number of elements to ack
+     * @param checkpointMaxAcks number of acks before forcing a checkpoint
+     * @return true if Page and its checkpoint were purged as a result of being fully acked
+     * @throws IOException
+     */
+    public boolean ack(long firstSeqNum, int count, int checkpointMaxAcks) throws IOException {
+        assert firstSeqNum >= this.minSeqNum :
+            String.format("seqNum=%d is smaller than minSeqnum=%d", firstSeqNum, this.minSeqNum);
+        final long maxSeqNum = firstSeqNum + count;
+        assert maxSeqNum <= this.minSeqNum + this.elementCount :
+            String.format(
+                "seqNum=%d is greater than minSeqnum=%d + elementCount=%d = %d", maxSeqNum,
+                this.minSeqNum, this.elementCount, this.minSeqNum + this.elementCount
+            );
+        final int offset = Ints.checkedCast(firstSeqNum - this.minSeqNum);
+        ackedSeqNums.flip(offset, offset + count);
         // checkpoint if totally acked or we acked more than checkpointMaxAcks elements in this page since last checkpoint
         // note that fully acked pages cleanup is done at queue level in Queue.ack()
-        long firstUnackedSeqNum = firstUnackedSeqNum();
-
-        if (isFullyAcked()) {
-            checkpoint();
-
+        final long firstUnackedSeqNum = firstUnackedSeqNum();
+
+        final boolean done = isFullyAcked();
+        if (done) {
+            if (this.writable) {
+                headPageCheckpoint();
+            } else {
+                purge();
+                final CheckpointIO cpIO = queue.getCheckpointIO();
+                cpIO.purge(cpIO.tailFileName(pageNum));
+            }
             assert firstUnackedSeqNum >= this.minSeqNum + this.elementCount - 1:
                     String.format("invalid firstUnackedSeqNum=%d for minSeqNum=%d and elementCount=%d and cardinality=%d", firstUnackedSeqNum, this.minSeqNum, this.elementCount, this.ackedSeqNums.cardinality());
 
-        } else if (checkpointMaxAcks > 0 && (firstUnackedSeqNum >= this.lastCheckpoint.getFirstUnackedSeqNum() + checkpointMaxAcks)) {
+        } else if (checkpointMaxAcks > 0 && firstUnackedSeqNum >= this.lastCheckpoint.getFirstUnackedSeqNum() + checkpointMaxAcks) {
             // did we acked more than checkpointMaxAcks elements? if so checkpoint now
             checkpoint();
         }
+        return done;
+    }
+
+    public void checkpoint() throws IOException {
+        if (this.writable) {
+            headPageCheckpoint();
+        } else {
+            tailPageCheckpoint();
+        }
+    }
+
+    private void headPageCheckpoint() throws IOException {
+        if (this.elementCount > this.lastCheckpoint.getElementCount()) {
+            // fsync & checkpoint if data written since last checkpoint
+
+            this.pageIO.ensurePersisted();
+            this.forceCheckpoint();
+        } else {
+            Checkpoint checkpoint = new Checkpoint(this.pageNum, this.queue.firstUnackedPageNum(), this.firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+            if (! checkpoint.equals(this.lastCheckpoint)) {
+                // checkpoint only if it changed since last checkpoint
+
+                // non-dry code with forceCheckpoint() to avoid unnecessary extra new Checkpoint object creation
+                CheckpointIO io = this.queue.getCheckpointIO();
+                io.write(io.headFileName(), checkpoint);
+                this.lastCheckpoint = checkpoint;
+            }
+        }
+
+    }
+
+    public void tailPageCheckpoint() throws IOException {
+        // since this is a tail page and no write can happen in this page, there is no point in performing a fsync on this page, just stamp checkpoint
+        CheckpointIO io = this.queue.getCheckpointIO();
+        this.lastCheckpoint = io.write(io.tailFileName(this.pageNum), this.pageNum, 0, this.firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+    }
+
+
+    public void ensurePersistedUpto(long seqNum) throws IOException {
+        long lastCheckpointUptoSeqNum = this.lastCheckpoint.getMinSeqNum() + this.lastCheckpoint.getElementCount();
+
+        // if the last checkpoint for this headpage already included the given seqNum, no need to fsync/checkpoint
+        if (seqNum > lastCheckpointUptoSeqNum) {
+            // head page checkpoint does a data file fsync
+            checkpoint();
+        }
+    }
+
+    public void forceCheckpoint() throws IOException {
+        Checkpoint checkpoint = new Checkpoint(this.pageNum, this.queue.firstUnackedPageNum(), this.firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
+        CheckpointIO io = this.queue.getCheckpointIO();
+        io.write(io.headFileName(), checkpoint);
+        this.lastCheckpoint = checkpoint;
+    }
+
+    public void behead() throws IOException {
+        assert this.writable == true : "cannot behead a tail page";
+
+        headPageCheckpoint();
+
+        this.writable = false;
+        this.lastCheckpoint = new Checkpoint(0, 0, 0, 0, 0);
+
+        // first thing that must be done after beheading is to create a new checkpoint for that new tail page
+        // tail page checkpoint does NOT includes a fsync
+        tailPageCheckpoint();
     }
 
-    public abstract void checkpoint() throws IOException;
+    /**
+     * signal that this page is not active and resources can be released
+     * @throws IOException
+     */
+    public void deactivate() throws IOException {
+        this.getPageIO().deactivate();
+    }
 
-    public abstract void close() throws IOException;
+    public boolean hasSpace(int byteSize) {
+        return this.pageIO.hasSpace((byteSize));
+    }
+
+    /**
+     * verify if data size plus overhead is not greater than the page capacity
+     *
+     * @param byteSize the date size to verify
+     * @return true if data plus overhead fit in page
+     */
+    public boolean hasCapacity(int byteSize) {
+        return this.pageIO.persistedByteCount(byteSize) <= this.pageIO.getCapacity();
+    }
+
+    public void close() throws IOException {
+        checkpoint();
+        this.pageIO.close();
+    }
+
+    public void purge() throws IOException {
+        this.pageIO.purge(); // page IO purge calls close
+    }
 
     public int getPageNum() {
         return pageNum;
@@ -151,10 +261,6 @@ public int getElementCount() {
         return elementCount;
     }
 
-    public Queue getQueue() {
-        return queue;
-    }
-
     public PageIO getPageIO() {
         return pageIO;
     }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/PageFactory.java b/logstash-core/src/main/java/org/logstash/ackedqueue/PageFactory.java
new file mode 100644
index 00000000000..403ec3e009a
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/PageFactory.java
@@ -0,0 +1,90 @@
+package org.logstash.ackedqueue;
+
+import org.logstash.ackedqueue.io.PageIO;
+
+import java.io.IOException;
+import java.util.BitSet;
+
+class PageFactory {
+
+    /**
+     * create a new head page object and new page.{@literal {pageNum}} empty valid data file
+     *
+     * @param pageNum the new head page page number
+     * @param queue the {@link Queue} instance
+     * @param pageIO the {@link PageIO} delegate
+     * @return {@link Page} the new head page
+     */
+    public static Page newHeadPage(int pageNum, Queue queue, PageIO pageIO) {
+        return new Page(pageNum, queue, 0, 0, 0, new BitSet(), pageIO, true);
+    }
+
+    /**
+     * create a new head page from an existing {@link Checkpoint} and open page.{@literal {pageNum}} empty valid data file
+     *
+     * @param checkpoint existing head page {@link Checkpoint}
+     * @param queue the {@link Queue} instance
+     * @param pageIO the {@link PageIO} delegate
+     * @return {@link Page} the new head page
+     */
+    public static Page newHeadPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
+        final Page p = new Page(
+                checkpoint.getPageNum(),
+                queue,
+                checkpoint.getMinSeqNum(),
+                checkpoint.getElementCount(),
+                checkpoint.getFirstUnackedSeqNum(),
+                new BitSet(),
+                pageIO,
+                true
+        );
+        try {
+            assert checkpoint.getMinSeqNum() == pageIO.getMinSeqNum() && checkpoint.getElementCount() == pageIO.getElementCount() :
+                    String.format("checkpoint minSeqNum=%d or elementCount=%d is different than pageIO minSeqNum=%d or elementCount=%d", checkpoint.getMinSeqNum(), checkpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
+
+            // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
+            if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
+                p.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
+            }
+
+            return p;
+        } catch (Exception e) {
+            p.close();
+            throw e;
+        }
+    }
+
+    /**
+     * create a new tail page for an exiting Checkpoint and data file
+     *
+     * @param checkpoint existing tail page {@link Checkpoint}
+     * @param queue the {@link Queue} instance
+     * @param pageIO the {@link PageIO} delegate
+     * @return {@link Page} the new tail page
+     */
+    public static Page newTailPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
+        final Page p = new Page(
+                checkpoint.getPageNum(),
+                queue,
+                checkpoint.getMinSeqNum(),
+                checkpoint.getElementCount(),
+                checkpoint.getFirstUnackedSeqNum(),
+                new BitSet(),
+                pageIO,
+                false
+        );
+
+        try {
+            // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
+            if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
+                p.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
+            }
+
+            return p;
+        } catch (Exception e) {
+            p.close();
+            throw e;
+        }
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index f31301b3552..53c7118da85 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -5,11 +5,12 @@
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
 import java.nio.channels.FileLock;
+import java.nio.file.Files;
 import java.nio.file.NoSuchFileException;
+import java.nio.file.Path;
+import java.nio.file.Paths;
 import java.util.ArrayList;
-import java.util.HashSet;
 import java.util.List;
-import java.util.Set;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.Condition;
@@ -20,43 +21,31 @@
 import org.logstash.FileLockFactory;
 import org.logstash.LockException;
 import org.logstash.ackedqueue.io.CheckpointIO;
-import org.logstash.ackedqueue.io.LongVector;
+import org.logstash.ackedqueue.io.FileCheckpointIO;
+import org.logstash.ackedqueue.io.MmapPageIO;
 import org.logstash.ackedqueue.io.PageIO;
-import org.logstash.ackedqueue.io.PageIOFactory;
+import org.logstash.common.FsUtil;
 
-// TODO: Notes
-//
-// - time-based fsync
-//
-// - tragic errors handling
-//   - what errors cause whole queue to be broken
-//   - where to put try/catch for these errors
+public final class Queue implements Closeable {
 
+    private long seqNum;
 
-public class Queue implements Closeable {
-    protected long seqNum;
-    protected HeadPage headPage;
+    protected Page headPage;
 
     // complete list of all non fully acked pages. note that exact sequentially by pageNum cannot be assumed
     // because any fully acked page will be removed from this list potentially creating pageNum gaps in the list.
-    protected final List<TailPage> tailPages;
+    protected final List<Page> tailPages;
 
     // this list serves the only purpose of quickly retrieving the first unread page, operation necessary on every read
     // reads will simply remove the first page from the list when fully read and writes will append new pages upon beheading
-    protected final List<TailPage> unreadTailPages;
-
-    // checkpoints that were not purged in the acking code to keep contiguous checkpoint files
-    // regardless of the correcponding data file purge.
-    protected final Set<Integer> preservedCheckpoints;
+    protected final List<Page> unreadTailPages;
 
     protected volatile long unreadCount;
-    protected volatile long currentByteSize;
 
     private final CheckpointIO checkpointIO;
-    private final PageIOFactory pageIOFactory;
     private final int pageCapacity;
     private final long maxBytes;
-    private final String dirPath;
+    private final Path dirPath;
     private final int maxUnread;
     private final int checkpointMaxAcks;
     private final int checkpointMaxWrites;
@@ -79,41 +68,28 @@ public class Queue implements Closeable {
     private static final Logger logger = LogManager.getLogger(Queue.class);
 
     public Queue(Settings settings) {
-        this(
-            settings.getDirPath(),
-            settings.getCapacity(),
-            settings.getQueueMaxBytes(),
-            settings.getCheckpointIOFactory().build(settings.getDirPath()),
-            settings.getPageIOFactory(),
-            settings.getElementClass(),
-            settings.getMaxUnread(),
-            settings.getCheckpointMaxWrites(),
-            settings.getCheckpointMaxAcks()
-        );
-    }
-
-    private Queue(String dirPath, int pageCapacity, long maxBytes, CheckpointIO checkpointIO,
-        PageIOFactory pageIOFactory, Class<? extends Queueable> elementClass, int maxUnread,
-        int checkpointMaxWrites, int checkpointMaxAcks) {
-        this.dirPath = dirPath;
-        this.pageCapacity = pageCapacity;
-        this.maxBytes = maxBytes;
-        this.checkpointIO = checkpointIO;
-        this.pageIOFactory = pageIOFactory;
-        this.elementClass = elementClass;
+        try {
+            final Path queueDir = Paths.get(settings.getDirPath());
+            Files.createDirectories(queueDir);
+            this.dirPath = queueDir.toRealPath();
+        } catch (final IOException ex) {
+            throw new IllegalStateException(ex);
+        }
+        this.pageCapacity = settings.getCapacity();
+        this.maxBytes = settings.getQueueMaxBytes();
+        this.checkpointIO = new FileCheckpointIO(dirPath);
+        this.elementClass = settings.getElementClass();
         this.tailPages = new ArrayList<>();
         this.unreadTailPages = new ArrayList<>();
-        this.preservedCheckpoints = new HashSet<>();
         this.closed = new AtomicBoolean(true); // not yet opened
-        this.maxUnread = maxUnread;
-        this.checkpointMaxAcks = checkpointMaxAcks;
-        this.checkpointMaxWrites = checkpointMaxWrites;
-        this.unreadCount = 0;
-        this.currentByteSize = 0;
+        this.maxUnread = settings.getMaxUnread();
+        this.checkpointMaxAcks = settings.getCheckpointMaxAcks();
+        this.checkpointMaxWrites = settings.getCheckpointMaxWrites();
+        this.unreadCount = 0L;
 
         // retrieve the deserialize method
         try {
-            Class[] cArg = new Class[1];
+            final Class<?>[] cArg = new Class<?>[1];
             cArg[0] = byte[].class;
             this.deserializeMethod = this.elementClass.getDeclaredMethod("deserialize", cArg);
         } catch (NoSuchMethodException e) {
@@ -122,7 +98,7 @@ private Queue(String dirPath, int pageCapacity, long maxBytes, CheckpointIO chec
     }
 
     public String getDirPath() {
-        return this.dirPath;
+        return this.dirPath.toString();
     }
 
     public long getMaxBytes() {
@@ -133,10 +109,6 @@ public long getMaxUnread() {
         return this.maxUnread;
     }
 
-    public long getCurrentByteSize() {
-        return this.currentByteSize;
-    }
-
     public long getPersistedByteSize() {
         lock.lock();
         try {
@@ -161,9 +133,10 @@ public long getUnreadCount() {
         return this.unreadCount;
     }
 
-    // moved queue opening logic in open() method until we have something in place to used in-memory checkpoints for testing
-    // because for now we need to pass a Queue instance to the Page and we don't want to trigger a Queue recovery when
-    // testing Page
+    /**
+     * Open an existing {@link Queue} or create a new one in the configured path.
+     * @throws IOException
+     */
     public void open() throws IOException {
         final int headPageNum;
 
@@ -172,7 +145,7 @@ public void open() throws IOException {
         lock.lock();
         try {
             // verify exclusive access to the dirPath
-            this.dirLock = FileLockFactory.getDefault().obtainLock(this.dirPath, LOCK_NAME);
+            this.dirLock = FileLockFactory.obtainLock(this.dirPath, LOCK_NAME);
 
             Checkpoint headCheckpoint;
             try {
@@ -182,6 +155,8 @@ public void open() throws IOException {
 
                 logger.debug("No head checkpoint found at: {}, creating new head page", checkpointIO.headFileName());
 
+                this.ensureDiskAvailable(this.maxBytes);
+
                 this.seqNum = 0;
                 headPageNum = 0;
 
@@ -193,16 +168,34 @@ public void open() throws IOException {
 
             // at this point we have a head checkpoint to figure queue recovery
 
+            // as we load pages, compute actuall disk needed substracting existing pages size to the required maxBytes
+            long diskNeeded = this.maxBytes;
+
             // reconstruct all tail pages state upto but excluding the head page
             for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
-
-                // all tail checkpoints in the sequence should exist, if not abort mission with a NoSuchFileException
-                Checkpoint cp = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));
+                final String cpFileName = checkpointIO.tailFileName(pageNum);
+                if (!dirPath.resolve(cpFileName).toFile().exists()) {
+                    continue;
+                }
+                final Checkpoint cp = this.checkpointIO.read(cpFileName);
 
                 logger.debug("opening tail page: {}, in: {}, with checkpoint: {}", pageNum, this.dirPath, cp.toString());
 
-                PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
-                addIO(cp, pageIO);
+                PageIO pageIO = new MmapPageIO(pageNum, this.pageCapacity, this.dirPath);
+                // important to NOT pageIO.open() just yet, we must first verify if it is fully acked in which case
+                // we can purge it and we don't care about its integrity for example if it is of zero-byte file size.
+                if (cp.isFullyAcked()) {
+                    purgeTailPage(cp, pageIO);
+                } else {
+                    pageIO.open(cp.getMinSeqNum(), cp.getElementCount());
+                    addTailPage(PageFactory.newTailPage(cp, this, pageIO));
+                    diskNeeded -= (long)pageIO.getHead();
+                }
+
+                // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
+                if (cp.maxSeqNum() > this.seqNum) {
+                    this.seqNum = cp.maxSeqNum();
+                }
             }
 
             // transform the head page into a tail page only if the headpage is non-empty
@@ -210,9 +203,11 @@ public void open() throws IOException {
 
             logger.debug("opening head page: {}, in: {}, with checkpoint: {}", headCheckpoint.getPageNum(), this.dirPath, headCheckpoint.toString());
 
-            PageIO pageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
+            PageIO pageIO = new MmapPageIO(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
             pageIO.recover(); // optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data
 
+            ensureDiskAvailable(diskNeeded - (long)pageIO.getHead());
+
             if (pageIO.getMinSeqNum() != headCheckpoint.getMinSeqNum() || pageIO.getElementCount() != headCheckpoint.getElementCount()) {
                 // the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes
 
@@ -226,26 +221,30 @@ public void open() throws IOException {
                 }
                 headCheckpoint = new Checkpoint(headCheckpoint.getPageNum(), headCheckpoint.getFirstUnackedPageNum(), firstUnackedSeqNum, pageIO.getMinSeqNum(), pageIO.getElementCount());
             }
-            this.headPage = new HeadPage(headCheckpoint, this, pageIO);
+            this.headPage = PageFactory.newHeadPage(headCheckpoint, this, pageIO);
 
             if (this.headPage.getMinSeqNum() <= 0 && this.headPage.getElementCount() <= 0) {
                 // head page is empty, let's keep it as-is
-
-                this.currentByteSize += pageIO.getCapacity();
-
                 // but checkpoint it to update the firstUnackedPageNum if it changed
                 this.headPage.checkpoint();
             } else {
-                // head page is non-empty, transform it into a tail page and create a new empty head page
-                addPage(headCheckpoint, this.headPage.behead());
+                // head page is non-empty, transform it into a tail page
+                this.headPage.behead();
 
-                headPageNum = headCheckpoint.getPageNum() + 1;
-                newCheckpointedHeadpage(headPageNum);
+                if (headCheckpoint.isFullyAcked()) {
+                    purgeTailPage(headCheckpoint, pageIO);
+                } else {
+                    addTailPage(this.headPage);
+                }
 
                 // track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum
                 if (headCheckpoint.maxSeqNum() > this.seqNum) {
                     this.seqNum = headCheckpoint.maxSeqNum();
                 }
+
+                // create a new empty head page
+                headPageNum = headCheckpoint.getPageNum() + 1;
+                newCheckpointedHeadpage(headPageNum);
             }
 
             // only activate the first tail page
@@ -263,96 +262,68 @@ public void open() throws IOException {
         }
     }
 
-    // TODO: addIO and addPage are almost identical - we should refactor to DRY it up.
-
-    // addIO is basically the same as addPage except that it avoid calling PageIO.open
-    // before actually purging the page if it is fully acked. This avoid dealing with
-    // zero byte page files that are fully acked.
-    // see issue #7809
-    private void addIO(Checkpoint checkpoint, PageIO pageIO) throws IOException {
-        if (checkpoint.isFullyAcked()) {
-            // first make sure any fully acked page per the checkpoint is purged if not already
-            try { pageIO.purge(); } catch (NoSuchFileException e) { /* ignore */ }
-
-            // we want to keep all the "middle" checkpoints between the first unacked tail page and the head page
-            // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
-            // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first
-            // unacked tail page and the head page. we did however purge the data file to free disk resources.
-
-            if (this.tailPages.size() == 0) {
-                // this is the first tail page and it is fully acked so just purge it
-                this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));
-            }
-        } else {
-            pageIO.open(checkpoint.getMinSeqNum(), checkpoint.getElementCount());
-            TailPage page = new TailPage(checkpoint, this, pageIO);
-
-            this.tailPages.add(page);
-            this.unreadTailPages.add(page);
-            this.unreadCount += page.unreadCount();
-            this.currentByteSize += page.getPageIO().getCapacity();
+    /**
+     * delete files for the given page
+     *
+     * @param checkpoint the tail page {@link Checkpoint}
+     * @param pageIO the tail page {@link PageIO}
+     * @throws IOException
+     */
+    private void purgeTailPage(Checkpoint checkpoint, PageIO pageIO) throws IOException {
+        try {
+            pageIO.purge();
+        } catch (NoSuchFileException e) { /* ignore */ }
 
-            // for now deactivate all tail pages, we will only reactivate the first one at the end
-            page.getPageIO().deactivate();
-        }
+        // we want to keep all the "middle" checkpoints between the first unacked tail page and the head page
+        // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
+        // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first
+        // unacked tail page and the head page. we did however purge the data file to free disk resources.
 
-        // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
-        if (checkpoint.maxSeqNum() > this.seqNum) {
-            this.seqNum = checkpoint.maxSeqNum();
+        if (this.tailPages.size() == 0) {
+            // this is the first tail page and it is fully acked so just purge it
+            this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));
         }
     }
 
-    // add a read tail page into this queue structures but also verify that this tail page
-    // is not fully acked in which case it will be purged
-    private void addPage(Checkpoint checkpoint, TailPage page) throws IOException {
-        if (checkpoint.isFullyAcked()) {
-            // first make sure any fully acked page per the checkpoint is purged if not already
-            try { page.getPageIO().purge(); } catch (NoSuchFileException e) { /* ignore */ }
-
-            // we want to keep all the "middle" checkpoints between the first unacked tail page and the head page
-            // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
-            // we will remove any prepended fully acked tail pages but keep all other checkpoints between the first
-            // unacked tail page and the head page. we did however purge the data file to free disk resources.
-
-            if (this.tailPages.size() == 0) {
-                // this is the first tail page and it is fully acked so just purge it
-                this.checkpointIO.purge(this.checkpointIO.tailFileName(checkpoint.getPageNum()));
-            }
-        } else {
-            this.tailPages.add(page);
-            this.unreadTailPages.add(page);
-            this.unreadCount += page.unreadCount();
-            this.currentByteSize += page.getPageIO().getCapacity();
-
-            // for now deactivate all tail pages, we will only reactivate the first one at the end
-            page.getPageIO().deactivate();
-        }
+    /**
+     * add a not fully-acked tail page into this queue structures and un-mmap it.
+     *
+     * @param page the tail {@link Page}
+     * @throws IOException
+     */
+    private void addTailPage(Page page) throws IOException {
+        this.tailPages.add(page);
+        this.unreadTailPages.add(page);
+        this.unreadCount += page.unreadCount();
 
-        // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
-        if (checkpoint.maxSeqNum() > this.seqNum) {
-            this.seqNum = checkpoint.maxSeqNum();
-        }
+        // for now deactivate all tail pages, we will only reactivate the first one at the end
+        page.getPageIO().deactivate();
     }
 
-    // create a new empty headpage for the given pageNum and immediately checkpoint it
-    // @param pageNum the page number of the new head page
+    /**
+     * create a new empty headpage for the given pageNum and immediately checkpoint it
+     *
+     * @param pageNum the page number of the new head page
+     * @throws IOException
+     */
     private void newCheckpointedHeadpage(int pageNum) throws IOException {
-        PageIO headPageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
+        PageIO headPageIO = new MmapPageIO(pageNum, this.pageCapacity, this.dirPath);
         headPageIO.create();
-        this.headPage = new HeadPage(pageNum, this, headPageIO);
+        this.headPage = PageFactory.newHeadPage(pageNum, this, headPageIO);
         this.headPage.forceCheckpoint();
-        this.currentByteSize += headPageIO.getCapacity();
     }
 
-    // @param element the Queueable object to write to the queue
-    // @return long written sequence number
+    /**
+     * write a {@link Queueable} element to the queue. Note that the element will always be written and the queue full
+     * condition will be checked and waited on **after** the write operation.
+     *
+     * @param element the {@link Queueable} element to write
+     * @return the written sequence number
+     * @throws IOException
+     */
     public long write(Queueable element) throws IOException {
         byte[] data = element.serialize();
 
-        if (! this.headPage.hasCapacity(data.length)) {
-            throw new IOException("data to be written is bigger than page capacity");
-        }
-
         // the write strategy with regard to the isFull() state is to assume there is space for this element
         // and write it, then after write verify if we just filled the queue and wait on the notFull condition
         // *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case
@@ -362,6 +333,9 @@ public long write(Queueable element) throws IOException {
 
         lock.lock();
         try {
+            if (! this.headPage.hasCapacity(data.length)) {
+                throw new IOException("data to be written is bigger than page capacity");
+            }
 
             // create a new head page if the current does not have sufficient space left for data to be written
             if (! this.headPage.hasSpace(data.length)) {
@@ -371,20 +345,12 @@ public long write(Queueable element) throws IOException {
                 int newHeadPageNum = this.headPage.pageNum + 1;
 
                 if (this.headPage.isFullyAcked()) {
-                    // purge the old headPage because its full and fully acked
-                    // there is no checkpoint file to purge since just creating a new TailPage from a HeadPage does
-                    // not trigger a checkpoint creation in itself
-                    TailPage tailPage = new TailPage(this.headPage);
-                    tailPage.purge();
-                    currentByteSize -= tailPage.getPageIO().getCapacity();
+                    // here we can just purge the data file and avoid beheading since we do not need
+                    // to add this fully hacked page into tailPages. a new head page will just be created.
+                    // TODO: we could possibly reuse the same page file but just rename it?
+                    this.headPage.purge();
                 } else {
-                    // beheading includes checkpoint+fsync if required
-                    TailPage tailPage = this.headPage.behead();
-
-                    this.tailPages.add(tailPage);
-                    if (! tailPage.isFullyRead()) {
-                        this.unreadTailPages.add(tailPage);
-                    }
+                    behead();
                 }
 
                 // create new head page
@@ -394,7 +360,7 @@ public long write(Queueable element) throws IOException {
             long seqNum = this.seqNum += 1;
             this.headPage.write(data, seqNum, this.checkpointMaxWrites);
             this.unreadCount++;
-            
+
             notEmpty.signal();
 
             // now check if we reached a queue full state and block here until it is not full
@@ -424,33 +390,63 @@ public long write(Queueable element) throws IOException {
         }
     }
 
+    /**
+     * mark head page as read-only (behead) and add it to the tailPages and unreadTailPages collections accordingly
+     * also deactivate it if it's not next-in-line for reading
+     *
+     * @throws IOException
+     */
+    private void behead() throws IOException {
+        // beheading includes checkpoint+fsync if required
+        this.headPage.behead();
+        this.tailPages.add(this.headPage);
+
+        if (! this.headPage.isFullyRead()) {
+            if (!this.unreadTailPages.isEmpty()) {
+                // there are already other unread pages so this new one is not next in line and we can deactivate
+                this.headPage.deactivate();
+            }
+            this.unreadTailPages.add(this.headPage);
+        } else {
+            // it is fully read so we can deactivate
+            this.headPage.deactivate();
+        }
+    }
+
     /**
      * <p>Checks if the Queue is full, with "full" defined as either of:</p>
      * <p>Assuming a maximum size of the queue larger than 0 is defined:</p>
      * <ul>
-     *     <li>The sum of the size of all allocated pages is more than the allowed maximum Queue 
+     *     <li>The sum of the size of all allocated pages is more than the allowed maximum Queue
      *     size</li>
-     *     <li>The sum of the size of all allocated pages equal to the allowed maximum Queue size 
+     *     <li>The sum of the size of all allocated pages equal to the allowed maximum Queue size
      *     and the current head page has no remaining capacity.</li>
      * </ul>
      * <p>or assuming a max unread count larger than 0, is defined "full" is also defined as:</p>
      * <ul>
-     *     <li>The current number of unread events exceeds or is equal to the configured maximum 
+     *     <li>The current number of unread events exceeds or is equal to the configured maximum
      *     number of allowed unread events.</li>
      * </ul>
      * @return True iff the queue is full
      */
     public boolean isFull() {
-        if (this.maxBytes > 0L && (
-            this.currentByteSize > this.maxBytes 
-                || this.currentByteSize == this.maxBytes && !headPage.hasSpace(1)
-        )) {
-            return true;
-        } else {
-            return ((this.maxUnread > 0) && this.unreadCount >= this.maxUnread);
+        lock.lock();
+        try {
+            if (this.maxBytes > 0L && isMaxBytesReached()) {
+                return true;
+            } else {
+                return (this.maxUnread > 0 && this.unreadCount >= this.maxUnread);
+            }
+        } finally {
+            lock.unlock();
         }
     }
 
+    private boolean isMaxBytesReached() {
+        final long persistedByteSize = getPersistedByteSize();
+        return ((persistedByteSize > this.maxBytes) || (persistedByteSize == this.maxBytes && !this.headPage.hasSpace(1)));
+    }
+
     /**
      * Queue is considered empty if it does not contain any tail page and the headpage has no element or all
      * elements are acked
@@ -469,7 +465,9 @@ public boolean isEmpty() {
 
     }
 
-    // @return true if the queue is fully acked, which implies that it is fully read which works as an "empty" state.
+    /**
+     * @return true if the queue is fully acked, which implies that it is fully read which works as an "empty" state.
+     */
     public boolean isFullyAcked() {
         lock.lock();
         try {
@@ -479,7 +477,12 @@ public boolean isFullyAcked() {
         }
     }
 
-    // @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
+    /**
+     * guarantee persistence up to a given sequence number.
+     *
+     * @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
+     * @throws IOException
+     */
     public void ensurePersistedUpto(long seqNum) throws IOException{
         lock.lock();
         try {
@@ -489,207 +492,150 @@ public void ensurePersistedUpto(long seqNum) throws IOException{
         }
     }
 
-    // non-blockin queue read
-    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
-    // @return Batch the batch containing 1 or more element up to the required limit or null of no elements were available
-    public Batch nonBlockReadBatch(int limit) throws IOException {
+    /**
+     * non-blocking queue read
+     *
+     * @param limit read the next batch of size up to this limit. the returned batch size can be smaller than the requested limit if fewer elements are available
+     * @return {@link Batch} the batch containing 1 or more element up to the required limit or null of no elements were available
+     * @throws IOException
+     */
+    public synchronized Batch nonBlockReadBatch(int limit) throws IOException {
         lock.lock();
         try {
-            Page p = firstUnreadPage();
-            return (p == null) ? null : _readPageBatch(p, limit);
+            Page p = nextReadPage();
+            return (isHeadPage(p) && p.isFullyRead()) ? null : readPageBatch(p, limit, 0L);
         } finally {
             lock.unlock();
         }
     }
 
-    // blocking readBatch notes:
-    //   the queue close() notifies all pending blocking read so that they unblock if the queue is being closed.
-    //   this means that all blocking read methods need to verify for the queue close condition.
-    //
-    // blocking queue read until elements are available for read
-    // @param limit read the next bach of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
-    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
-    public Batch readBatch(int limit) throws IOException {
-        Page p;
-
+    /**
+     *
+     * @param limit size limit of the batch to read. returned {@link Batch} can be smaller.
+     * @param timeout the maximum time to wait in milliseconds on write operations
+     * @return the read {@link Batch} or null if no element upon timeout
+     * @throws IOException
+     */
+    public synchronized Batch readBatch(int limit, long timeout) throws IOException {
         lock.lock();
         try {
-            while ((p = firstUnreadPage()) == null && !isClosed()) {
-                try {
-                    notEmpty.await();
-                } catch (InterruptedException e) {
-                    // the thread interrupt() has been called while in the await() blocking call.
-                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
-                    // to any upstream calls on it. for now our choice is to simply return null and set back
-                    // the Thread.interrupted() flag so it can be checked upstream.
-
-                    // set back the interrupted flag
-                    Thread.currentThread().interrupt();
-
-                    return null;
-                }
-            }
-
-            // need to check for close since it is a condition for exiting the while loop
-            return (isClosed()) ? null : _readPageBatch(p, limit);
+            return readPageBatch(nextReadPage(), limit, timeout);
         } finally {
             lock.unlock();
         }
     }
 
-    // blocking queue read until elements are available for read or the given timeout is reached.
-    // @param limit read the next batch of size up to this limit. the returned batch size can be smaller than than the requested limit if fewer elements are available
-    // @param timeout the maximum time to wait in milliseconds
-    // @return Batch the batch containing 1 or more element up to the required limit or null if no elements were available or the blocking call was interrupted
-    public Batch readBatch(int limit, long timeout) throws IOException {
-        Page p;
-
-        lock.lock();
-        try {
-            // wait only if queue is empty
-            if ((p = firstUnreadPage()) == null) {
+    /**
+     * read a {@link Batch} from the given {@link Page}. If the page is a head page, try to maximize the
+     * batch size by waiting for writes.
+     * @param p the {@link Page} to read from.
+     * @param limit size limit of the batch to read.
+     * @param timeout  the maximum time to wait in milliseconds on write operations.
+     * @return {@link Batch} with read elements or null if nothing was read
+     * @throws IOException
+     */
+    private Batch readPageBatch(Page p, int limit, long timeout) throws IOException {
+        int left = limit;
+        final List<byte[]> elements = new ArrayList<>(limit);
+
+        // NOTE: the tricky thing here is that upon entering this method, if p is initially a head page
+        // it could become a tail page upon returning from the notEmpty.await call.
+        long firstSeqNum = -1L;
+        while (left > 0) {
+            if (isHeadPage(p) && p.isFullyRead()) {
+                boolean elapsed;
+                // a head page is fully read but can be written to so let's wait for more data
                 try {
-                    notEmpty.await(timeout, TimeUnit.MILLISECONDS);
+                    elapsed = !notEmpty.await(timeout, TimeUnit.MILLISECONDS);
                 } catch (InterruptedException e) {
-                    // the thread interrupt() has been called while in the await() blocking call.
-                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
-                    // to any upstream calls on it. for now our choice is to simply return null and set back
-                    // the Thread.interrupted() flag so it can be checked upstream.
-
                     // set back the interrupted flag
                     Thread.currentThread().interrupt();
-
-                    return null;
+                    break;
                 }
 
-                // if after returning from wait queue is still empty, or the queue was closed return null
-                if ((p = firstUnreadPage()) == null || isClosed()) { return null; }
+                if ((elapsed && p.isFullyRead()) || isClosed()) {
+                    break;
+                }
             }
 
-            return _readPageBatch(p, limit);
-        } finally {
-            lock.unlock();
-        }
-    }
-
-    private Batch _readPageBatch(Page p, int limit) throws IOException {
-        boolean wasFull = isFull();
+            if (! p.isFullyRead()) {
+                boolean wasFull = isFull();
 
-        Batch b = p.readBatch(limit);
-        this.unreadCount -= b.size();
+                final SequencedList<byte[]> serialized = p.read(left);
+                int n = serialized.getElements().size();
+                assert n > 0 : "page read returned 0 elements";
+                elements.addAll(serialized.getElements());
+                if (firstSeqNum == -1L) {
+                    firstSeqNum = serialized.getSeqNums().get(0);
+                }
 
-        if (p.isFullyRead()) { removeUnreadPage(p); }
-        if (wasFull) { notFull.signalAll(); }
+                this.unreadCount -= n;
+                left -= n;
 
-        return b;
-    }
+                if (wasFull) {
+                    notFull.signalAll();
+                }
+            }
 
-    private static class TailPageResult {
-        public TailPage page;
-        public int index;
+            if (isTailPage(p) && p.isFullyRead()) {
+                break;
+            }
+        }
 
-        public TailPageResult(TailPage page, int index) {
-            this.page = page;
-            this.index = index;
+        if (isTailPage(p) && p.isFullyRead()) {
+            removeUnreadPage(p);
         }
+
+        return new Batch(elements, firstSeqNum, this);
     }
 
-    // perform a binary search through tail pages to find in which page this seqNum falls into
-    private TailPageResult binaryFindPageForSeqnum(long seqNum) {
+    /**
+     * perform a binary search through tail pages to find in which page this seqNum falls into
+     *
+     * @param seqNum the sequence number to search for in the tail pages
+     * @return Index of the found {@link Page} in {@link #tailPages}
+     */
+    private int binaryFindPageForSeqnum(final long seqNum) {
         int lo = 0;
         int hi = this.tailPages.size() - 1;
         while (lo <= hi) {
-            int mid = lo + (hi - lo) / 2;
-            TailPage p = this.tailPages.get(mid);
-
-            if (seqNum < p.getMinSeqNum()) {
+            final int mid = lo + (hi - lo) / 2;
+            final Page p = this.tailPages.get(mid);
+            final long pMinSeq = p.getMinSeqNum();
+            if (seqNum < pMinSeq) {
                 hi = mid - 1;
-            } else if (seqNum >= (p.getMinSeqNum() + p.getElementCount())) {
+            } else if (seqNum >= pMinSeq + (long) p.getElementCount()) {
                 lo = mid + 1;
             } else {
-                return new TailPageResult(p, mid);
+                return mid;
             }
         }
-        return null;
-    }
-
-    // perform a linear search through tail pages to find in which page this seqNum falls into
-    private TailPageResult linearFindPageForSeqnum(long seqNum) {
-        for (int i = 0; i < this.tailPages.size(); i++) {
-            TailPage p = this.tailPages.get(i);
-            if (p.getMinSeqNum() > 0 && seqNum >= p.getMinSeqNum() && seqNum < p.getMinSeqNum() + p.getElementCount()) {
-                return new TailPageResult(p, i);
-            }
-        }
-        return null;
+        throw new IllegalArgumentException(
+            String.format("Sequence number %d not found in any page", seqNum)
+        );
     }
 
-    // ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from
-    // same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks
-    // acks since last checkpoint it will also trigger a checkpoint.
-    // @param seqNums the list of same-page sequence numbers to ack
-    public void ack(LongVector seqNums) throws IOException {
+    /**
+     * ack a list of seqNums that are assumed to be all part of the same page, leveraging the fact that batches are also created from
+     * same-page elements. A fully acked page will trigger a checkpoint for that page. Also if a page has more than checkpointMaxAcks
+     * acks since last checkpoint it will also trigger a checkpoint.
+     *
+     * @param firstAckSeqNum First Sequence Number to Ack
+     * @param ackCount Number of Elements to Ack
+     * @throws IOException
+     */
+    public void ack(final long firstAckSeqNum, final int ackCount) throws IOException {
         // as a first implementation we assume that all batches are created from the same page
-        // so we will avoid multi pages acking here for now
-
-        // find the page to ack by traversing from oldest tail page
-        long firstAckSeqNum = seqNums.get(0);
-
         lock.lock();
         try {
-            TailPageResult result = null;
-
-            if (this.tailPages.size() > 0) {
-                // short-circuit: first check in the first tail page as it is the most likely page where acking will happen
-                TailPage p = this.tailPages.get(0);
-                if (p.getMinSeqNum() > 0 && firstAckSeqNum >= p.getMinSeqNum() && firstAckSeqNum < p.getMinSeqNum() + p.getElementCount()) {
-                    result = new TailPageResult(p, 0);
-                } else {
-                    // dual search strategy: if few tail pages search linearly otherwise perform binary search
-                    result = (this.tailPages.size() > 3) ? binaryFindPageForSeqnum(firstAckSeqNum) : linearFindPageForSeqnum(firstAckSeqNum);
-                }
-            }
-
-            if (result == null) {
-                // if not found then it is in head page
-                assert this.headPage.getMinSeqNum() > 0 && firstAckSeqNum >= this.headPage.getMinSeqNum() && firstAckSeqNum < this.headPage.getMinSeqNum() + this.headPage.getElementCount():
-                        String.format("seqNum=%d is not in head page with minSeqNum=%d", firstAckSeqNum, this.headPage.getMinSeqNum());
-
-                // page acking checkpoints fully acked pages
-                this.headPage.ack(seqNums, this.checkpointMaxAcks);
+            if (containsSeq(headPage, firstAckSeqNum)) {
+                this.headPage.ack(firstAckSeqNum, ackCount, this.checkpointMaxAcks);
             } else {
-                // page acking also checkpoints fully acked pages or upon reaching the checkpointMaxAcks threshold
-                result.page.ack(seqNums, this.checkpointMaxAcks);
-
-                // cleanup fully acked tail page
-                if (result.page.isFullyAcked()) {
-                    boolean wasFull = isFull();
-
-                    this.tailPages.remove(result.index);
-
-                    // remove page data file regardless if it is the first or a middle tail page to free resources
-                    result.page.purge();
-                    this.currentByteSize -= result.page.getPageIO().getCapacity();
-
-                    if (result.index != 0) {
-                        // this an in-between page, we don't purge it's checkpoint to preserve checkpoints sequence on disk
-                        // save that checkpoint so that if it becomes the first checkpoint it can be purged later on.
-                        this.preservedCheckpoints.add(result.page.getPageNum());
-                    } else {
-                        // if this is the first page also remove checkpoint file
-                        this.checkpointIO.purge(this.checkpointIO.tailFileName(result.page.getPageNum()));
-
-                        // check if there are preserved checkpoints file next to this one and delete them
-                        int nextPageNum = result.page.getPageNum() + 1;
-                        while (preservedCheckpoints.remove(nextPageNum)) {
-                            this.checkpointIO.purge(this.checkpointIO.tailFileName(nextPageNum));
-                            nextPageNum++;
-                        }
-                    }
-
-                    if (wasFull) { notFull.signalAll(); }
+                final int resultIndex = binaryFindPageForSeqnum(firstAckSeqNum);
+                if (tailPages.get(resultIndex).ack(firstAckSeqNum, ackCount, this.checkpointMaxAcks)) {
+                    this.tailPages.remove(resultIndex);
+                    notFull.signalAll();
                 }
-
                 this.headPage.checkpoint();
             }
         } finally {
@@ -701,9 +647,12 @@ public CheckpointIO getCheckpointIO() {
         return this.checkpointIO;
     }
 
-    // deserialize a byte array into the required element class.
-    // @param bytes the byte array to deserialize
-    // @return Queueable the deserialized byte array into the required Queueable interface implementation concrete class
+    /**
+     *  deserialize a byte array into the required element class.
+     *
+     * @param bytes the byte array to deserialize
+     * @return {@link Queueable} the deserialized byte array into the required Queueable interface implementation concrete class
+     */
     public Queueable deserialize(byte[] bytes) {
         try {
             return (Queueable)this.deserializeMethod.invoke(this.elementClass, bytes);
@@ -712,6 +661,7 @@ public Queueable deserialize(byte[] bytes) {
         }
     }
 
+    @Override
     public void close() throws IOException {
         // TODO: review close strategy and exception handling and resiliency of first closing tail pages if crash in the middle
 
@@ -721,7 +671,7 @@ public void close() throws IOException {
                 // TODO: not sure if we need to do this here since the headpage close will also call ensurePersisted
                 ensurePersistedUpto(this.seqNum);
 
-                for (TailPage p : this.tailPages) { p.close(); }
+                for (Page p : this.tailPages) { p.close(); }
                 this.headPage.close();
 
                 // release all referenced objects
@@ -740,7 +690,7 @@ public void close() throws IOException {
 
             } finally {
                 try {
-                    FileLockFactory.getDefault().releaseLock(this.dirLock);
+                    FileLockFactory.releaseLock(this.dirLock);
                 } catch (IOException e) {
                     // log error and ignore
                     logger.error("Queue close releaseLock failed, error={}", e.getMessage());
@@ -751,25 +701,41 @@ public void close() throws IOException {
         }
     }
 
-    protected Page firstUnreadPage() throws IOException {
-        // look at head page if no unreadTailPages
-        return (this.unreadTailPages.isEmpty()) ? (this.headPage.isFullyRead() ? null : this.headPage) : this.unreadTailPages.get(0);
+    /**
+     * return the {@link Page} for the next read operation.
+     * @return {@link Page} will be either a read-only tail page or the head page.
+     */
+    public Page nextReadPage() {
+        lock.lock();
+        try {
+            // look at head page if no unreadTailPages
+            return (this.unreadTailPages.isEmpty()) ?  this.headPage : this.unreadTailPages.get(0);
+        } finally {
+            lock.unlock();
+        }
     }
 
     private void removeUnreadPage(Page p) {
-        // HeadPage is not part of the unreadTailPages, just ignore
-        if (p instanceof TailPage){
-            // the page to remove should always be the first one
-            assert this.unreadTailPages.get(0) == p : String.format("unread page is not first in unreadTailPages list");
-            this.unreadTailPages.remove(0);
+        if (! this.unreadTailPages.isEmpty()) {
+            Page firstUnread = this.unreadTailPages.get(0);
+            assert p.pageNum <= firstUnread.pageNum : String.format("fully read pageNum=%d is greater than first unread pageNum=%d", p.pageNum, firstUnread.pageNum);
+            if (firstUnread == p) {
+                // it is possible that when starting to read from a head page which is beheaded will not be inserted in the unreadTailPages list
+                this.unreadTailPages.remove(0);
+            }
         }
     }
 
-    protected int firstUnackedPageNum() {
-        if (this.tailPages.isEmpty()) {
-            return this.headPage.getPageNum();
+    public int firstUnackedPageNum() {
+        lock.lock();
+        try {
+            if (this.tailPages.isEmpty()) {
+                return this.headPage.getPageNum();
+            }
+            return this.tailPages.get(0).getPageNum();
+        } finally {
+            lock.unlock();
         }
-        return this.tailPages.get(0).getPageNum();
     }
 
     public long getAckedCount() {
@@ -795,7 +761,35 @@ public long getUnackedCount() {
         }
     }
 
-    protected boolean isClosed() {
+    private boolean isClosed() {
         return this.closed.get();
     }
+
+    /**
+     * @param p the {@link Page} to verify if it is the head page
+     * @return true if the given {@link Page} is the head page
+     */
+    private boolean isHeadPage(Page p) {
+        return p == this.headPage;
+    }
+
+    /**
+     * @param p the {@link Page} to verify if it is a tail page
+     * @return true if the given {@link Page} is a tail page
+     */
+    private boolean isTailPage(Page p) {
+        return !isHeadPage(p);
+    }
+
+    private void ensureDiskAvailable(final long diskNeeded) throws IOException {
+        if (!FsUtil.hasFreeSpace(this.dirPath, diskNeeded)) {
+            throw new IOException("Not enough free disk space available to allocate persisted queue.");
+        }
+    }
+
+    private static boolean containsSeq(final Page page, final long seqNum) {
+        final long pMinSeq = page.getMinSeqNum();
+        final long pMaxSeq = pMinSeq + (long) page.getElementCount();
+        return seqNum >= pMinSeq && seqNum < pMaxSeq;
+    }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java
index 06b8639d5b0..fb0d8e3dff1 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/QueueRuntimeException.java
@@ -2,28 +2,8 @@
 
 public class QueueRuntimeException extends RuntimeException {
 
-    public static QueueRuntimeException newFormatMessage(String fmt, Object... args) {
-        return new QueueRuntimeException(
-                String.format(fmt, args)
-        );
-    }
-
-    public QueueRuntimeException() {
-    }
-
-    public QueueRuntimeException(String message) {
-        super(message);
-    }
-
     public QueueRuntimeException(String message, Throwable cause) {
         super(message, cause);
     }
 
-    public QueueRuntimeException(Throwable cause) {
-        super(cause);
-    }
-
-    public QueueRuntimeException(String message, Throwable cause, boolean enableSuppression, boolean writableStackTrace) {
-        super(message, cause, enableSuppression, writableStackTrace);
-    }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java
index 09378de1bc7..f2fe8b1dac6 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Settings.java
@@ -1,14 +1,7 @@
 package org.logstash.ackedqueue;
 
-import org.logstash.ackedqueue.io.CheckpointIOFactory;
-import org.logstash.ackedqueue.io.PageIOFactory;
-
 public interface Settings {
 
-    CheckpointIOFactory getCheckpointIOFactory();
-
-    PageIOFactory getPageIOFactory();
-
     Class<? extends Queueable> getElementClass();
 
     String getDirPath();
@@ -22,12 +15,8 @@ public interface Settings {
     int getCheckpointMaxAcks();
 
     int getCheckpointMaxWrites();
-    
-    interface Builder {
 
-        Builder checkpointIOFactory(CheckpointIOFactory factory);
-
-        Builder elementIOFactory(PageIOFactory factory);
+    interface Builder {
 
         Builder elementClass(Class<? extends Queueable> elementClass);
 
@@ -40,7 +29,7 @@ interface Builder {
         Builder checkpointMaxAcks(int checkpointMaxAcks);
 
         Builder checkpointMaxWrites(int checkpointMaxWrites);
-        
+
         Settings build();
 
     }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/SettingsImpl.java b/logstash-core/src/main/java/org/logstash/ackedqueue/SettingsImpl.java
index 0ebcf8aad58..e1501e5cbe1 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/SettingsImpl.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/SettingsImpl.java
@@ -1,12 +1,7 @@
 package org.logstash.ackedqueue;
 
-import org.logstash.ackedqueue.io.CheckpointIOFactory;
-import org.logstash.ackedqueue.io.PageIOFactory;
-
 public class SettingsImpl implements Settings {
     private String dirForFiles;
-    private CheckpointIOFactory checkpointIOFactory;
-    private PageIOFactory pageIOFactory;
     private Class<? extends Queueable> elementClass;
     private int capacity;
     private long queueMaxBytes;
@@ -15,9 +10,7 @@ public class SettingsImpl implements Settings {
     private int checkpointMaxWrites;
 
     public static Builder builder(final Settings settings) {
-        return new BuilderImpl(settings.getDirPath(),
-            settings.getCheckpointIOFactory(),
-            settings.getPageIOFactory(), settings.getElementClass(), settings.getCapacity(),
+        return new BuilderImpl(settings.getDirPath(), settings.getElementClass(), settings.getCapacity(),
             settings.getQueueMaxBytes(), settings.getMaxUnread(), settings.getCheckpointMaxAcks(),
             settings.getCheckpointMaxWrites()
         );
@@ -27,22 +20,10 @@ public static Builder fileSettingsBuilder(final String dirForFiles) {
         return new BuilderImpl(dirForFiles);
     }
 
-    public static Builder memorySettingsBuilder() {
-        return memorySettingsBuilder("");
-    }
-
-    public static Builder memorySettingsBuilder(final String dirForFiles) {
-        return new BuilderImpl(dirForFiles).checkpointMaxAcks(1)
-            .checkpointMaxWrites(1);
-    }
-
-    private SettingsImpl(final String dirForFiles, final CheckpointIOFactory checkpointIOFactory,
-        final PageIOFactory pageIOFactory, final Class<? extends Queueable> elementClass,
+    private SettingsImpl(final String dirForFiles, final Class<? extends Queueable> elementClass,
         final int capacity, final long queueMaxBytes, final int maxUnread,
         final int checkpointMaxAcks, final int checkpointMaxWrites) {
         this.dirForFiles = dirForFiles;
-        this.checkpointIOFactory = checkpointIOFactory;
-        this.pageIOFactory = pageIOFactory;
         this.elementClass = elementClass;
         this.capacity = capacity;
         this.queueMaxBytes = queueMaxBytes;
@@ -60,15 +41,6 @@ public int getCheckpointMaxAcks() {
     public int getCheckpointMaxWrites() {
         return checkpointMaxWrites;
     }
-    
-    @Override
-    public CheckpointIOFactory getCheckpointIOFactory() {
-        return checkpointIOFactory;
-    }
-
-    public PageIOFactory getPageIOFactory() {
-        return pageIOFactory;
-    }
 
     @Override
     public Class<? extends Queueable> getElementClass()  {
@@ -96,7 +68,7 @@ public int getMaxUnread() {
     }
 
     private static final class BuilderImpl implements Builder {
-        
+
         /**
          * The default Queue has a capacity of 0 events, meaning infinite capacity.
          * todo: Remove the ability to set infinite capacity.
@@ -116,20 +88,16 @@ private static final class BuilderImpl implements Builder {
         private static final int DEFAULT_MAX_UNREAD = 0;
 
         /**
-         * Default max number of writes after which we checkpoint.
+         * Default max number of acknowledgements after which we checkpoint.
          */
         private static final int DEFAULT_CHECKPOINT_MAX_ACKS = 1024;
 
         /**
-         * Default number of acknowledgements after which we checkpoint.
+         * Default max number of writes after which we checkpoint.
          */
         private static final int DEFAULT_CHECKPOINT_MAX_WRITES = 1024;
-        
-        private final String dirForFiles;
-
-        private final CheckpointIOFactory checkpointIOFactory;
 
-        private final PageIOFactory pageIOFactory;
+        private final String dirForFiles;
 
         private final Class<? extends Queueable> elementClass;
 
@@ -142,20 +110,17 @@ private static final class BuilderImpl implements Builder {
         private final int checkpointMaxAcks;
 
         private final int checkpointMaxWrites;
-        
+
         private BuilderImpl(final String dirForFiles) {
-            this(dirForFiles, null, null, null, DEFAULT_CAPACITY, DEFAULT_MAX_QUEUE_BYTES,
+            this(dirForFiles, null, DEFAULT_CAPACITY, DEFAULT_MAX_QUEUE_BYTES,
                 DEFAULT_MAX_UNREAD, DEFAULT_CHECKPOINT_MAX_ACKS, DEFAULT_CHECKPOINT_MAX_WRITES
             );
         }
 
-        private BuilderImpl(final String dirForFiles, final CheckpointIOFactory checkpointIOFactory,
-            final PageIOFactory pageIOFactory, final Class<? extends Queueable> elementClass,
+        private BuilderImpl(final String dirForFiles, final Class<? extends Queueable> elementClass,
             final int capacity, final long queueMaxBytes, final int maxUnread,
             final int checkpointMaxAcks, final int checkpointMaxWrites) {
             this.dirForFiles = dirForFiles;
-            this.checkpointIOFactory = checkpointIOFactory;
-            this.pageIOFactory = pageIOFactory;
             this.elementClass = elementClass;
             this.capacity = capacity;
             this.queueMaxBytes = queueMaxBytes;
@@ -164,31 +129,11 @@ private BuilderImpl(final String dirForFiles, final CheckpointIOFactory checkpoi
             this.checkpointMaxWrites = checkpointMaxWrites;
         }
 
-        @Override
-        public Builder checkpointIOFactory(final CheckpointIOFactory factory) {
-            return new BuilderImpl(
-                this.dirForFiles, factory, this.pageIOFactory, this.elementClass, this.capacity,
-                this.queueMaxBytes, this.maxUnread, this.checkpointMaxAcks,
-                this.checkpointMaxWrites
-            );
-        }
-
-        @Override
-        public Builder elementIOFactory(final PageIOFactory factory) {
-            return new BuilderImpl(
-                this.dirForFiles, this.checkpointIOFactory, factory, this.elementClass,
-                this.capacity,
-                this.queueMaxBytes, this.maxUnread, this.checkpointMaxAcks,
-                this.checkpointMaxWrites
-            );
-        }
-
         @Override
         public Builder elementClass(final Class<? extends Queueable> elementClass) {
             return new BuilderImpl(
-                this.dirForFiles, this.checkpointIOFactory, this.pageIOFactory, elementClass,
-                this.capacity,
-                this.queueMaxBytes, this.maxUnread, this.checkpointMaxAcks,
+                this.dirForFiles, elementClass, this.capacity, this.queueMaxBytes, this.maxUnread,
+                this.checkpointMaxAcks,
                 this.checkpointMaxWrites
             );
         }
@@ -196,25 +141,23 @@ public Builder elementClass(final Class<? extends Queueable> elementClass) {
         @Override
         public Builder capacity(final int capacity) {
             return new BuilderImpl(
-                this.dirForFiles, this.checkpointIOFactory, this.pageIOFactory, this.elementClass,
-                capacity, this.queueMaxBytes, this.maxUnread, this.checkpointMaxAcks,
-                this.checkpointMaxWrites
+                this.dirForFiles, this.elementClass, capacity, this.queueMaxBytes, this.maxUnread,
+                this.checkpointMaxAcks, this.checkpointMaxWrites
             );
         }
 
         @Override
         public Builder queueMaxBytes(final long size) {
             return new BuilderImpl(
-                this.dirForFiles, this.checkpointIOFactory, this.pageIOFactory, this.elementClass,
-                this.capacity, size, this.maxUnread, this.checkpointMaxAcks,
-                this.checkpointMaxWrites
+                this.dirForFiles, this.elementClass, this.capacity, size, this.maxUnread,
+                this.checkpointMaxAcks, this.checkpointMaxWrites
             );
         }
 
         @Override
         public Builder maxUnread(final int maxUnread) {
             return new BuilderImpl(
-                this.dirForFiles, this.checkpointIOFactory, this.pageIOFactory, this.elementClass,
+                this.dirForFiles, this.elementClass,
                 this.capacity, this.queueMaxBytes, maxUnread, this.checkpointMaxAcks,
                 this.checkpointMaxWrites
             );
@@ -223,7 +166,7 @@ public Builder maxUnread(final int maxUnread) {
         @Override
         public Builder checkpointMaxAcks(final int checkpointMaxAcks) {
             return new BuilderImpl(
-                this.dirForFiles, this.checkpointIOFactory, this.pageIOFactory, this.elementClass,
+                this.dirForFiles, this.elementClass,
                 this.capacity, this.queueMaxBytes, this.maxUnread, checkpointMaxAcks,
                 this.checkpointMaxWrites
             );
@@ -232,18 +175,16 @@ public Builder checkpointMaxAcks(final int checkpointMaxAcks) {
         @Override
         public Builder checkpointMaxWrites(final int checkpointMaxWrites) {
             return new BuilderImpl(
-                this.dirForFiles, this.checkpointIOFactory, this.pageIOFactory, this.elementClass,
-                this.capacity, this.queueMaxBytes, this.maxUnread, this.checkpointMaxAcks,
-                checkpointMaxWrites
+                this.dirForFiles, this.elementClass, this.capacity, this.queueMaxBytes,
+                this.maxUnread, this.checkpointMaxAcks, checkpointMaxWrites
             );
         }
 
         @Override
         public Settings build() {
             return new SettingsImpl(
-                this.dirForFiles, this.checkpointIOFactory, this.pageIOFactory, this.elementClass,
-                this.capacity, this.queueMaxBytes, this.maxUnread, this.checkpointMaxAcks,
-                this.checkpointMaxWrites
+                this.dirForFiles, this.elementClass, this.capacity, this.queueMaxBytes,
+                this.maxUnread, this.checkpointMaxAcks, this.checkpointMaxWrites
             );
         }
     }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java
deleted file mode 100644
index c7b03c07855..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java
+++ /dev/null
@@ -1,48 +0,0 @@
-package org.logstash.ackedqueue;
-
-import org.logstash.ackedqueue.io.CheckpointIO;
-import org.logstash.ackedqueue.io.PageIO;
-
-import java.io.IOException;
-import java.util.BitSet;
-
-public class TailPage extends Page {
-
-    // create a new TailPage object from a HeadPage object
-    public TailPage(HeadPage page) {
-        super(page.pageNum, page.queue, page.minSeqNum, page.elementCount, page.firstUnreadSeqNum, page.ackedSeqNums, page.pageIO);
-    }
-
-    // create a new TailPage object for an exiting Checkpoint and data file
-    // @param pageIO the PageIO object is expected to be open/recover/create
-    public TailPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) {
-        super(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO);
-
-        // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
-        if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
-            this.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
-        }
-    }
-
-    public void checkpoint() throws IOException {
-        // TODO: not concurrent for first iteration:
-
-        // since this is a tail page and no write can happen in this page, there is no point in performing a fsync on this page, just stamp checkpoint
-        CheckpointIO io = queue.getCheckpointIO();
-        this.lastCheckpoint = io.write(io.tailFileName(this.pageNum), this.pageNum, 0, firstUnackedSeqNum(), this.minSeqNum, this.elementCount);
-    }
-
-    // delete all IO files associated with this page
-    public void purge() throws IOException {
-        if (this.pageIO != null) {
-            this.pageIO.purge(); // page IO purge calls close
-        }
-    }
-
-    public void close() throws IOException {
-        checkpoint();
-        if (this.pageIO != null) {
-            this.pageIO.close();
-        }
-    }
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java
index 93fb624ba75..ea8df32dc80 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedBatchExtLibrary.java
@@ -1,6 +1,5 @@
 package org.logstash.ackedqueue.ext;
 
-import java.util.List;
 import org.jruby.Ruby;
 import org.jruby.RubyClass;
 import org.jruby.RubyModule;
@@ -15,7 +14,6 @@
 import org.logstash.RubyUtil;
 import org.logstash.ackedqueue.Batch;
 import org.logstash.Event;
-import org.logstash.ackedqueue.Queueable;
 import org.logstash.ext.JrubyEventExtLibrary;
 import java.io.IOException;
 
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
index f9083ff877d..e2896c5b16f 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
@@ -19,8 +19,6 @@
 import org.logstash.ackedqueue.Batch;
 import org.logstash.ackedqueue.Queue;
 import org.logstash.ackedqueue.SettingsImpl;
-import org.logstash.ackedqueue.io.FileCheckpointIO;
-import org.logstash.ackedqueue.io.MmapPageIO;
 import org.logstash.ext.JrubyEventExtLibrary;
 
 public class JrubyAckedQueueExtLibrary implements Library {
@@ -72,8 +70,6 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
                     .queueMaxBytes(queueMaxBytes)
                     .checkpointMaxAcks(checkpointMaxAcks)
                     .checkpointMaxWrites(checkpointMaxWrites)
-                    .elementIOFactory(MmapPageIO::new)
-                    .checkpointIOFactory(FileCheckpointIO::new)
                     .elementClass(Event.class)
                     .build()
             );
@@ -102,7 +98,7 @@ public IRubyObject ruby_dir_path(ThreadContext context) {
 
         @JRubyMethod(name = "current_byte_size")
         public IRubyObject ruby_current_byte_size(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getCurrentByteSize());
+            return context.runtime.newFixnum(queue.getPersistedByteSize());
         }
 
         @JRubyMethod(name = "persisted_size_in_bytes")
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
deleted file mode 100644
index a79e1546fb7..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
+++ /dev/null
@@ -1,194 +0,0 @@
-package org.logstash.ackedqueue.ext;
-
-import java.io.IOException;
-import org.jruby.Ruby;
-import org.jruby.RubyBoolean;
-import org.jruby.RubyClass;
-import org.jruby.RubyFixnum;
-import org.jruby.RubyModule;
-import org.jruby.RubyObject;
-import org.jruby.anno.JRubyClass;
-import org.jruby.anno.JRubyMethod;
-import org.jruby.runtime.Arity;
-import org.jruby.runtime.ObjectAllocator;
-import org.jruby.runtime.ThreadContext;
-import org.jruby.runtime.builtin.IRubyObject;
-import org.jruby.runtime.load.Library;
-import org.logstash.RubyUtil;
-import org.logstash.Event;
-import org.logstash.ackedqueue.Batch;
-import org.logstash.ackedqueue.Queue;
-import org.logstash.ackedqueue.SettingsImpl;
-import org.logstash.ackedqueue.io.ByteBufferPageIO;
-import org.logstash.ackedqueue.io.MemoryCheckpointIO;
-import org.logstash.ext.JrubyEventExtLibrary;
-
-public class JrubyAckedQueueMemoryExtLibrary implements Library {
-
-    public void load(Ruby runtime, boolean wrap) throws IOException {
-        RubyModule module = runtime.defineModule("LogStash");
-
-        RubyClass clazz = runtime.defineClassUnder("AckedMemoryQueue", runtime.getObject(), new ObjectAllocator() {
-            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
-                return new RubyAckedMemoryQueue(runtime, rubyClass);
-            }
-        }, module);
-
-        clazz.defineAnnotatedMethods(RubyAckedMemoryQueue.class);
-    }
-
-    // TODO:
-    // as a simplified first prototyping implementation, the Settings class is not exposed and the queue elements
-    // are assumed to be logstash Event.
-
-
-    @JRubyClass(name = "AckedMemoryQueue", parent = "Object")
-    public static class RubyAckedMemoryQueue extends RubyObject {
-        private Queue queue;
-
-        public RubyAckedMemoryQueue(Ruby runtime, RubyClass klass) {
-            super(runtime, klass);
-        }
-
-        public Queue getQueue() {
-            return this.queue;
-        }
-
-        // def initialize
-        @JRubyMethod(name = "initialize", optional = 4)
-        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
-        {
-            args = Arity.scanArgs(context.runtime, args, 4, 0);
-
-            int capacity = RubyFixnum.num2int(args[1]);
-            int maxUnread = RubyFixnum.num2int(args[2]);
-            long queueMaxBytes = RubyFixnum.num2long(args[3]);
-            this.queue = new Queue(
-                SettingsImpl.memorySettingsBuilder(args[0].asJavaString())
-                    .capacity(capacity)
-                    .maxUnread(maxUnread)
-                    .queueMaxBytes(queueMaxBytes)
-                    .elementIOFactory(ByteBufferPageIO::new)
-                    .checkpointIOFactory(MemoryCheckpointIO::new)
-                    .elementClass(Event.class)
-                    .build()
-            );
-            return context.nil;
-        }
-
-        @JRubyMethod(name = "max_unread_events")
-        public IRubyObject ruby_max_unread_events(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getMaxUnread());
-        }
-
-        @JRubyMethod(name = "max_size_in_bytes")
-        public IRubyObject ruby_max_size_in_bytes(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getMaxBytes());
-        }
-
-        @JRubyMethod(name = "page_capacity")
-        public IRubyObject ruby_page_capacity(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getPageCapacity());
-        }
-
-        @JRubyMethod(name = "dir_path")
-        public IRubyObject ruby_dir_path(ThreadContext context) {
-            return context.runtime.newString(queue.getDirPath());
-        }
-
-        @JRubyMethod(name = "current_byte_size")
-        public IRubyObject ruby_current_byte_size(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getCurrentByteSize());
-        }
-
-        @JRubyMethod(name = "persisted_size_in_bytes")
-        public IRubyObject ruby_persisted_size_in_bytes(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getPersistedByteSize());
-        }
-
-        @JRubyMethod(name = "acked_count")
-        public IRubyObject ruby_acked_count(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getAckedCount());
-        }
-
-        @JRubyMethod(name = "unread_count")
-        public IRubyObject ruby_unread_count(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getUnreadCount());
-        }
-
-        @JRubyMethod(name = "unacked_count")
-        public IRubyObject ruby_unacked_count(ThreadContext context) {
-            return context.runtime.newFixnum(queue.getUnackedCount());
-        }
-
-        @JRubyMethod(name = "open")
-        public IRubyObject ruby_open(ThreadContext context)
-        {
-            try {
-                this.queue.getCheckpointIO().purge();
-                this.queue.open();
-            } catch (IOException e) {
-                throw RubyUtil.newRubyIOError(context.runtime, e);
-            }
-
-            return context.nil;
-        }
-
-        @JRubyMethod(name = {"write", "<<"}, required = 1)
-        public IRubyObject ruby_write(ThreadContext context, IRubyObject event)
-        {
-            if (!(event instanceof JrubyEventExtLibrary.RubyEvent)) {
-                throw context.runtime.newTypeError("wrong argument type " + event.getMetaClass() + " (expected LogStash::Event)");
-            }
-
-            long seqNum;
-            try {
-                seqNum = this.queue.write(((JrubyEventExtLibrary.RubyEvent) event).getEvent());
-            } catch (IOException e) {
-                throw RubyUtil.newRubyIOError(context.runtime, e);
-            }
-
-            return context.runtime.newFixnum(seqNum);
-        }
-
-        @JRubyMethod(name = "read_batch", required = 2)
-        public IRubyObject ruby_read_batch(ThreadContext context, IRubyObject limit, IRubyObject timeout)
-        {
-            Batch b;
-
-            try {
-                b = this.queue.readBatch(RubyFixnum.num2int(limit), RubyFixnum.num2int(timeout));
-            } catch (IOException e) {
-                throw RubyUtil.newRubyIOError(context.runtime, e);
-            }
-
-            // TODO: return proper Batch object
-            return (b == null) ? context.nil : new JrubyAckedBatchExtLibrary.RubyAckedBatch(context.runtime, b);
-        }
-
-        @JRubyMethod(name = "is_fully_acked?")
-        public IRubyObject ruby_is_fully_acked(ThreadContext context)
-        {
-            return RubyBoolean.newBoolean(context.runtime, this.queue.isFullyAcked());
-        }
-
-        @JRubyMethod(name = "is_empty?")
-        public IRubyObject ruby_is_empty(ThreadContext context)
-        {
-            return RubyBoolean.newBoolean(context.runtime, this.queue.isEmpty());
-        }
-
-        @JRubyMethod(name = "close")
-        public IRubyObject ruby_close(ThreadContext context)
-        {
-            try {
-                this.queue.close();
-            } catch (IOException e) {
-                throw RubyUtil.newRubyIOError(context.runtime, e);
-            }
-
-            return context.nil;
-        }
-
-    }
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/AbstractByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/AbstractByteBufferPageIO.java
deleted file mode 100644
index 6c632ecd651..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/AbstractByteBufferPageIO.java
+++ /dev/null
@@ -1,289 +0,0 @@
-package org.logstash.ackedqueue.io;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.zip.CRC32;
-import org.apache.logging.log4j.LogManager;
-import org.apache.logging.log4j.Logger;
-import org.logstash.ackedqueue.SequencedList;
-
-public abstract class AbstractByteBufferPageIO implements PageIO {
-
-    public class PageIOInvalidElementException extends IOException {
-        public PageIOInvalidElementException(String message) { super(message); }
-    }
-
-    public class PageIOInvalidVersionException extends IOException {
-        public PageIOInvalidVersionException(String message) { super(message); }
-    }
-
-    public static final byte VERSION_ONE = 1;
-    public static final int VERSION_SIZE = Byte.BYTES;
-    public static final int CHECKSUM_SIZE = Integer.BYTES;
-    public static final int LENGTH_SIZE = Integer.BYTES;
-    public static final int SEQNUM_SIZE = Long.BYTES;
-    public static final int HEADER_SIZE = 1;     // version byte
-    public static final int MIN_CAPACITY = VERSION_SIZE + SEQNUM_SIZE + LENGTH_SIZE + 1 + CHECKSUM_SIZE; // header overhead plus elements overhead to hold a single 1 byte element
-
-    // Size of: Header + Sequence Number + Length + Checksum
-    public static final int WRAPPER_SIZE = HEADER_SIZE + SEQNUM_SIZE + LENGTH_SIZE + CHECKSUM_SIZE;
-
-    public static final boolean VERIFY_CHECKSUM = true;
-    public static final boolean STRICT_CAPACITY = true;
-
-    private static final Logger logger = LogManager.getLogger(AbstractByteBufferPageIO.class);
-
-    protected int capacity; // page capacity is an int per the ByteBuffer class.
-    protected final int pageNum;
-    protected long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
-    protected int elementCount;
-    protected int head; // head is the write position and is an int per ByteBuffer class position
-    protected byte version;
-    private CRC32 checkSummer;
-    private final IntVector offsetMap;
-
-    public AbstractByteBufferPageIO(int pageNum, int capacity) {
-        this.minSeqNum = 0;
-        this.elementCount = 0;
-        this.version = 0;
-        this.head = 0;
-        this.pageNum = pageNum;
-        this.capacity = capacity;
-        this.offsetMap = new IntVector();
-        this.checkSummer = new CRC32();
-    }
-
-    // @return the concrete class buffer
-    protected abstract ByteBuffer getBuffer();
-
-    @Override
-    public void open(long minSeqNum, int elementCount) throws IOException {
-        getBuffer().position(0);
-        this.version = getBuffer().get();
-        validateVersion(this.version);
-        this.head = 1;
-
-        this.minSeqNum = minSeqNum;
-        this.elementCount = elementCount;
-
-        if (this.elementCount > 0) {
-            // verify first seqNum to be same as expected minSeqNum
-            long seqNum = getBuffer().getLong();
-            if (seqNum != this.minSeqNum) { throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum)); }
-
-            // reset back position to first seqNum
-            getBuffer().position(this.head);
-
-            for (int i = 0; i < this.elementCount; i++) {
-                // verify that seqNum must be of strict + 1 increasing order
-                readNextElement(this.minSeqNum + i, !VERIFY_CHECKSUM);
-            }
-        }
-    }
-
-    // recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes
-    // to reflect what it recovered from the page
-    @Override
-    public void recover() throws IOException {
-        getBuffer().position(0);
-        this.version = getBuffer().get();
-        validateVersion(this.version);
-        this.head = 1;
-
-        // force minSeqNum to actual first element seqNum
-        this.minSeqNum = getBuffer().getLong();
-        // reset back position to first seqNum
-        getBuffer().position(this.head);
-
-        // reset elementCount to 0 and increment to octal number of valid elements found
-        this.elementCount = 0;
-
-        for (int i = 0; ; i++) {
-            try {
-                // verify that seqNum must be of strict + 1 increasing order
-                readNextElement(this.minSeqNum + i, VERIFY_CHECKSUM);
-                this.elementCount += 1;
-            } catch (PageIOInvalidElementException e) {
-                // simply stop at first invalid element
-                logger.debug("PageIO recovery element index:{}, readNextElement exception: {}", i, e.getMessage());
-                break;
-            }
-        }
-
-        // if we were not able to read any element just reset minSeqNum to zero
-        if (this.elementCount <= 0) {
-            this.minSeqNum = 0;
-        }
-    }
-
-    // we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check
-    // and if an unexpected version byte is read throw PageIOInvalidVersionException
-    private void validateVersion(byte version) throws PageIOInvalidVersionException {
-        if (version != VERSION_ONE) {
-            throw new PageIOInvalidVersionException(String.format("Expected page version=%d but found version=%d", VERSION_ONE, version));
-        }
-    }
-
-    // read and validate next element at page head
-    // @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
-    private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws PageIOInvalidElementException {
-        // if there is no room for the seqNum and length bytes stop here
-        // TODO: I know this isn't a great exception message but at the time of writing I couldn't come up with anything better :P
-        if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) { throw new PageIOInvalidElementException("cannot read seqNum and length bytes past buffer capacity"); }
-
-        int elementOffset = this.head;
-        int newHead = this.head;
-        ByteBuffer buffer = getBuffer();
-
-        long seqNum = buffer.getLong();
-        newHead += SEQNUM_SIZE;
-
-        if (seqNum != expectedSeqNum) { throw new PageIOInvalidElementException(String.format("Element seqNum %d is expected to be %d", seqNum, expectedSeqNum)); }
-
-        int length = buffer.getInt();
-        newHead += LENGTH_SIZE;
-
-        // length must be > 0
-        if (length <= 0) { throw new PageIOInvalidElementException("Element invalid length"); }
-
-        // if there is no room for the proposed data length and checksum just stop here
-        if (newHead + length + CHECKSUM_SIZE > capacity) { throw new PageIOInvalidElementException("cannot read element payload and checksum past buffer capacity"); }
-
-        if (verifyChecksum) {
-            // read data and compute checksum;
-            this.checkSummer.reset();
-            final int prevLimit = buffer.limit();
-            buffer.limit(buffer.position() + length);
-            this.checkSummer.update(buffer);
-            buffer.limit(prevLimit);
-            int checksum = buffer.getInt();
-            int computedChecksum = (int) this.checkSummer.getValue();
-            if (computedChecksum != checksum) { throw new PageIOInvalidElementException("Element invalid checksum"); }
-        }
-
-        // at this point we recovered a valid element
-        this.offsetMap.add(elementOffset);
-        this.head = newHead + length + CHECKSUM_SIZE;
-
-        buffer.position(this.head);
-    }
-
-    @Override
-    public void create() throws IOException {
-        getBuffer().position(0);
-        getBuffer().put(VERSION_ONE);
-        this.head = 1;
-        this.minSeqNum = 0L;
-        this.elementCount = 0;
-    }
-
-    @Override
-    public void write(byte[] bytes, long seqNum) {
-        write(bytes, seqNum, bytes.length, checksum(bytes));
-    }
-
-    protected int write(byte[] bytes, long seqNum, int length, int checksum) {
-        // since writes always happen at head, we can just append head to the offsetMap
-        assert this.offsetMap.size() == this.elementCount :
-                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
-
-        int initialHead = this.head;
-        ByteBuffer buffer = getBuffer();
-
-        buffer.position(this.head);
-        buffer.putLong(seqNum);
-        buffer.putInt(length);
-        buffer.put(bytes);
-        buffer.putInt(checksum);
-        this.head += persistedByteCount(bytes.length);
-
-        assert this.head == buffer.position() :
-                String.format("head=%d != buffer position=%d", this.head, buffer.position());
-
-        if (this.elementCount <= 0) {
-            this.minSeqNum = seqNum;
-        }
-        this.offsetMap.add(initialHead);
-        this.elementCount++;
-
-        return initialHead;
-    }
-
-    @Override
-    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
-        assert seqNum >= this.minSeqNum :
-                String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
-        assert seqNum <= maxSeqNum() :
-                String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
-
-        List<byte[]> elements = new ArrayList<>();
-        final LongVector seqNums = new LongVector(limit);
-
-        int offset = this.offsetMap.get((int)(seqNum - this.minSeqNum));
-
-        ByteBuffer buffer = getBuffer();
-        buffer.position(offset);
-
-        for (int i = 0; i < limit; i++) {
-            long readSeqNum = buffer.getLong();
-
-            assert readSeqNum == (seqNum + i) :
-                    String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
-
-            int readLength = buffer.getInt();
-            byte[] readBytes = new byte[readLength];
-            buffer.get(readBytes);
-            int checksum = buffer.getInt();
-            int computedChecksum = checksum(readBytes);
-            if (computedChecksum != checksum) {
-                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
-            }
-
-            elements.add(readBytes);
-            seqNums.add(readSeqNum);
-
-            if (seqNum + i >= maxSeqNum()) {
-                break;
-            }
-        }
-
-        return new SequencedList<>(elements, seqNums);
-    }
-
-    @Override
-    public int getCapacity() { return this.capacity; }
-
-    @Override
-    public long getMinSeqNum() { return this.minSeqNum; }
-
-    @Override
-    public int getElementCount() { return this.elementCount; }
-
-    @Override
-    public boolean hasSpace(int bytes) {
-        int bytesLeft = this.capacity - this.head;
-        return persistedByteCount(bytes) <= bytesLeft;
-    }
-
-    @Override
-    public int persistedByteCount(int byteCount) {
-        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
-    }
-
-    @Override
-    public int getHead() {
-        return this.head;
-    }
-
-    protected int checksum(byte[] bytes) {
-        checkSummer.reset();
-        checkSummer.update(bytes, 0, bytes.length);
-        return (int) checkSummer.getValue();
-    }
-
-    private long maxSeqNum() {
-        return this.minSeqNum + this.elementCount - 1;
-    }
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferCleaner.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferCleaner.java
new file mode 100644
index 00000000000..176c3c63bac
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferCleaner.java
@@ -0,0 +1,16 @@
+package org.logstash.ackedqueue.io;
+
+import java.nio.MappedByteBuffer;
+
+/**
+ * Function that forces garbage collection of a {@link MappedByteBuffer}.
+ */
+@FunctionalInterface
+public interface ByteBufferCleaner {
+
+    /**
+     * Forces garbage collection of given buffer.
+     * @param buffer ByteBuffer to GC
+     */
+    void clean(MappedByteBuffer buffer);
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferPageIO.java
deleted file mode 100644
index 6b77f5e20ae..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/ByteBufferPageIO.java
+++ /dev/null
@@ -1,53 +0,0 @@
-package org.logstash.ackedqueue.io;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-
-public class ByteBufferPageIO extends AbstractByteBufferPageIO {
-
-    private final ByteBuffer buffer;
-
-    public ByteBufferPageIO(int pageNum, int capacity, String path) throws IOException {
-        this(capacity, new byte[0]);
-    }
-
-    public ByteBufferPageIO(int capacity) throws IOException {
-        this(capacity, new byte[0]);
-    }
-
-    public ByteBufferPageIO(int capacity, byte[] initialBytes) throws IOException {
-        super(0, capacity);
-
-        if (initialBytes.length > capacity) {
-            throw new IOException("initial bytes greater than capacity");
-        }
-
-        this.buffer = ByteBuffer.allocate(capacity);
-        this.buffer.put(initialBytes);
-    }
-
-    @Override
-    public void deactivate() { /* nothing */ }
-
-    @Override
-    public void activate() { /* nyet */ }
-
-    @Override
-    public void ensurePersisted() { /* nada */ }
-
-    @Override
-    public void purge() { /* zilch */ }
-
-    @Override
-    public void close() { /* don't look here */ }
-
-
-    @Override
-    protected ByteBuffer getBuffer() { return this.buffer; }
-
-    // below public methods only used by tests
-
-    public int getWritePosition() { return this.head; }
-
-    public byte[] dump() { return this.buffer.array(); }
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIO.java
index e60f9127351..c3b2e5de604 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIO.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIO.java
@@ -14,8 +14,6 @@ public interface CheckpointIO {
 
     void purge(String fileName) throws IOException;
 
-    void purge() throws IOException;
-
     // @return the head page checkpoint file name
     String headFileName();
 
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIOFactory.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIOFactory.java
deleted file mode 100644
index b3e43aaf80e..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/CheckpointIOFactory.java
+++ /dev/null
@@ -1,6 +0,0 @@
-package org.logstash.ackedqueue.io;
-
-@FunctionalInterface
-public interface CheckpointIOFactory {
-    CheckpointIO build(String dirPath);
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/FileCheckpointIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/FileCheckpointIO.java
index 0c7d91f2a04..c4660ee9658 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/FileCheckpointIO.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/FileCheckpointIO.java
@@ -1,18 +1,14 @@
 package org.logstash.ackedqueue.io;
 
-import java.nio.channels.FileChannel;
-import org.logstash.ackedqueue.Checkpoint;
-import org.logstash.common.io.BufferedChecksumStreamInput;
-import org.logstash.common.io.InputStreamStreamInput;
-
-import java.io.ByteArrayInputStream;
 import java.io.FileOutputStream;
 import java.io.IOException;
 import java.nio.ByteBuffer;
+import java.nio.channels.FileChannel;
 import java.nio.file.Files;
 import java.nio.file.Path;
-import java.nio.file.Paths;
+import java.nio.file.StandardCopyOption;
 import java.util.zip.CRC32;
+import org.logstash.ackedqueue.Checkpoint;
 
 public class FileCheckpointIO implements CheckpointIO {
 //    Checkpoint file structure
@@ -42,20 +38,16 @@ public class FileCheckpointIO implements CheckpointIO {
 
     private static final String HEAD_CHECKPOINT = "checkpoint.head";
     private static final String TAIL_CHECKPOINT = "checkpoint.";
-    private final String dirPath;
+    private final Path dirPath;
 
-    public FileCheckpointIO(String dirPath) {
+    public FileCheckpointIO(Path dirPath) {
         this.dirPath = dirPath;
     }
 
     @Override
     public Checkpoint read(String fileName) throws IOException {
         return read(
-            new BufferedChecksumStreamInput(
-                new InputStreamStreamInput(
-                    new ByteArrayInputStream(Files.readAllBytes(Paths.get(dirPath, fileName)))
-                )
-            )
+            ByteBuffer.wrap(Files.readAllBytes(dirPath.resolve(fileName)))
         );
     }
 
@@ -70,24 +62,20 @@ public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, l
     public void write(String fileName, Checkpoint checkpoint) throws IOException {
         write(checkpoint, buffer);
         buffer.flip();
-        try (FileOutputStream out = new FileOutputStream(Paths.get(dirPath, fileName).toFile())) {
+        final Path tmpPath = dirPath.resolve(fileName + ".tmp");
+        try (FileOutputStream out = new FileOutputStream(tmpPath.toFile())) {
             out.getChannel().write(buffer);
             out.getFD().sync();
         }
+        Files.move(tmpPath, dirPath.resolve(fileName), StandardCopyOption.ATOMIC_MOVE);
     }
 
     @Override
     public void purge(String fileName) throws IOException {
-        Path path = Paths.get(dirPath, fileName);
+        Path path = dirPath.resolve(fileName);
         Files.delete(path);
     }
 
-    @Override
-    public void purge() throws IOException {
-        // TODO: dir traversal and delete all checkpoints?
-        throw new UnsupportedOperationException("purge() is not supported");
-    }
-
     // @return the head page checkpoint file name
     @Override
     public String headFileName() {
@@ -100,17 +88,18 @@ public String tailFileName(int pageNum) {
         return TAIL_CHECKPOINT + pageNum;
     }
 
-    private Checkpoint read(BufferedChecksumStreamInput crcsi) throws IOException {
-        int version = (int) crcsi.readShort();
+    public static Checkpoint read(ByteBuffer data) throws IOException {
+        int version = (int) data.getShort();
         // TODO - build reader for this version
-        int pageNum = crcsi.readInt();
-        int firstUnackedPageNum = crcsi.readInt();
-        long firstUnackedSeqNum = crcsi.readLong();
-        long minSeqNum = crcsi.readLong();
-        int elementCount = crcsi.readInt();
-
-        int calcCrc32 = (int)crcsi.getChecksum();
-        int readCrc32 = crcsi.readInt();
+        int pageNum = data.getInt();
+        int firstUnackedPageNum = data.getInt();
+        long firstUnackedSeqNum = data.getLong();
+        long minSeqNum = data.getLong();
+        int elementCount = data.getInt();
+        final CRC32 crc32 = new CRC32();
+        crc32.update(data.array(), 0, BUFFER_SIZE - Integer.BYTES);
+        int calcCrc32 = (int) crc32.getValue();
+        int readCrc32 = data.getInt();
         if (readCrc32 != calcCrc32) {
             throw new IOException(String.format("Checkpoint checksum mismatch, expected: %d, actual: %d", calcCrc32, readCrc32));
         }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java
index 601d66086d6..033bf9fe2f0 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/LongVector.java
@@ -24,6 +24,22 @@ public void add(final long num) {
         data[count++] = num;
     }
 
+    /**
+     * Store the {@code long[]} to the underlying {@code long[]}, resizing it if necessary.
+     * @param nums {@code long[]} to store
+     */
+    public void add(final LongVector nums) {
+        if (data.length < count + nums.size()) {
+            final long[] old = data;
+            data = new long[(data.length << 1) + nums.size()];
+            System.arraycopy(old, 0, data, 0, old.length);
+        }
+        for (int i = 0; i < nums.size(); i++) {
+            data[count + i] = nums.get(i);
+        }
+        count += nums.size();
+    }
+
     /**
      * Get value stored at given index.
      * @param index Array index (only values smaller than {@link LongVector#count} are valid)
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MemoryCheckpointIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MemoryCheckpointIO.java
deleted file mode 100644
index 681ec9a75e9..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MemoryCheckpointIO.java
+++ /dev/null
@@ -1,81 +0,0 @@
-package org.logstash.ackedqueue.io;
-
-import org.logstash.ackedqueue.Checkpoint;
-
-import java.io.IOException;
-import java.nio.file.NoSuchFileException;
-import java.util.HashMap;
-import java.util.Map;
-
-public class MemoryCheckpointIO implements CheckpointIO {
-
-    private final String HEAD_CHECKPOINT = "checkpoint.head";
-    private final String TAIL_CHECKPOINT = "checkpoint.";
-
-    private static final Map<String, Map<String, Checkpoint>> sources = new HashMap<>();
-
-    private final String dirPath;
-
-    public static void clearSources() {
-        sources.clear();
-    }
-
-    public MemoryCheckpointIO(String dirPath) {
-        this.dirPath = dirPath;
-    }
-
-    @Override
-    public Checkpoint read(String fileName) throws IOException {
-
-        Checkpoint cp = null;
-        Map<String, Checkpoint> ns = this.sources.get(dirPath);
-        if (ns != null) {
-           cp = ns.get(fileName);
-        }
-        if (cp == null) { throw new NoSuchFileException("no memory checkpoint for dirPath: " + this.dirPath + ", fileName: " + fileName); }
-        return cp;
-    }
-
-    @Override
-    public Checkpoint write(String fileName, int pageNum, int firstUnackedPageNum, long firstUnackedSeqNum, long minSeqNum, int elementCount) throws IOException {
-        Checkpoint checkpoint = new Checkpoint(pageNum, firstUnackedPageNum, firstUnackedSeqNum, minSeqNum, elementCount);
-        write(fileName, checkpoint);
-        return checkpoint;
-    }
-
-    @Override
-    public void write(String fileName, Checkpoint checkpoint) throws IOException {
-        Map<String, Checkpoint> ns = this.sources.get(dirPath);
-        if (ns == null) {
-            ns = new HashMap<>();
-            this.sources.put(this.dirPath, ns);
-        }
-        ns.put(fileName, checkpoint);
-    }
-
-    @Override
-    public void purge(String fileName) {
-        Map<String, Checkpoint> ns = this.sources.get(dirPath);
-        if (ns != null) {
-           ns.remove(fileName);
-        }
-    }
-
-    @Override
-    public void purge() {
-        this.sources.remove(this.dirPath);
-    }
-
-    // @return the head page checkpoint file name
-    @Override
-    public String headFileName() {
-        return HEAD_CHECKPOINT;
-    }
-
-    // @return the tail page checkpoint file name for given page number
-    @Override
-    public String tailFileName(int pageNum) {
-        return TAIL_CHECKPOINT + pageNum;
-    }
-
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
index 46bd79f358a..1a2bc414caa 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/io/MmapPageIO.java
@@ -1,93 +1,196 @@
 package org.logstash.ackedqueue.io;
 
-import sun.misc.Cleaner;
-import sun.nio.ch.DirectBuffer;
-
 import java.io.File;
 import java.io.IOException;
 import java.io.RandomAccessFile;
 import java.nio.MappedByteBuffer;
 import java.nio.channels.FileChannel;
 import java.nio.file.Files;
-import java.nio.file.Paths;
+import java.nio.file.Path;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.zip.CRC32;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.ackedqueue.SequencedList;
+import sun.misc.Cleaner;
+import sun.nio.ch.DirectBuffer;
+
+public final class MmapPageIO implements PageIO {
+
+    public static final byte VERSION_ONE = (byte) 1;
+    public static final int VERSION_SIZE = Byte.BYTES;
+    public static final int CHECKSUM_SIZE = Integer.BYTES;
+    public static final int LENGTH_SIZE = Integer.BYTES;
+    public static final int SEQNUM_SIZE = Long.BYTES;
+    public static final int MIN_CAPACITY = VERSION_SIZE + SEQNUM_SIZE + LENGTH_SIZE + 1 + CHECKSUM_SIZE; // header overhead plus elements overhead to hold a single 1 byte element
+    public static final int HEADER_SIZE = 1;     // version byte
+    public static final boolean VERIFY_CHECKSUM = true;
 
-// TODO: this essentially a copy of ByteBufferPageIO and should be DRY'ed - temp impl to test file based stress test
+    private static final Logger LOGGER = LogManager.getLogger(MmapPageIO.class);
 
-@SuppressWarnings("sunapi")
-public class MmapPageIO extends AbstractByteBufferPageIO {
+    /**
+     * Cleaner function for forcing unmapping of backing {@link MmapPageIO#buffer}.
+     */
+    private static final ByteBufferCleaner BUFFER_CLEANER = byteBuffer -> {
+        Cleaner c = ((DirectBuffer) byteBuffer).cleaner();
+        if (c != null) {
+            c.clean();
+        }
+    };
+
+    private final File file;
 
-    private File file;
+    private final CRC32 checkSummer;
 
-    private FileChannel channel;
-    protected MappedByteBuffer buffer;
+    private final IntVector offsetMap;
 
-    public MmapPageIO(int pageNum, int capacity, String dirPath) {
-        super(pageNum, capacity);
+    private int capacity; // page capacity is an int per the ByteBuffer class.
+    private long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
+    private int elementCount;
+    private int head; // head is the write position and is an int per ByteBuffer class position
+    private byte version;
 
-        this.file = Paths.get(dirPath, "page." + pageNum).toFile();
+    private MappedByteBuffer buffer;
+
+    public MmapPageIO(int pageNum, int capacity, Path dirPath) {
+        this.minSeqNum = 0;
+        this.elementCount = 0;
+        this.version = 0;
+        this.head = 0;
+        this.capacity = capacity;
+        this.offsetMap = new IntVector();
+        this.checkSummer = new CRC32();
+        this.file = dirPath.resolve("page." + pageNum).toFile();
     }
 
     @Override
     public void open(long minSeqNum, int elementCount) throws IOException {
-        mapFile(STRICT_CAPACITY);
-        super.open(minSeqNum, elementCount);
+        mapFile();
+        buffer.position(0);
+        this.version = buffer.get();
+        validateVersion(this.version);
+        this.head = 1;
+
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+
+        if (this.elementCount > 0) {
+            // verify first seqNum to be same as expected minSeqNum
+            long seqNum = buffer.getLong();
+            if (seqNum != this.minSeqNum) {
+                throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum));
+            }
+
+            // reset back position to first seqNum
+            buffer.position(this.head);
+
+            for (int i = 0; i < this.elementCount; i++) {
+                // verify that seqNum must be of strict + 1 increasing order
+                readNextElement(this.minSeqNum + i, !VERIFY_CHECKSUM);
+            }
+        }
+    }
+
+    @Override
+    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
+        assert seqNum >= this.minSeqNum :
+            String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
+        assert seqNum <= maxSeqNum() :
+            String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
+
+        List<byte[]> elements = new ArrayList<>();
+        final LongVector seqNums = new LongVector(limit);
+
+        int offset = this.offsetMap.get((int) (seqNum - this.minSeqNum));
+
+        buffer.position(offset);
+
+        for (int i = 0; i < limit; i++) {
+            long readSeqNum = buffer.getLong();
+
+            assert readSeqNum == (seqNum + i) :
+                String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
+
+            int readLength = buffer.getInt();
+            byte[] readBytes = new byte[readLength];
+            buffer.get(readBytes);
+            int checksum = buffer.getInt();
+            int computedChecksum = checksum(readBytes);
+            if (computedChecksum != checksum) {
+                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
+            }
+
+            elements.add(readBytes);
+            seqNums.add(readSeqNum);
+
+            if (seqNum + i >= maxSeqNum()) {
+                break;
+            }
+        }
+
+        return new SequencedList<>(elements, seqNums);
     }
 
     // recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes
     // to reflect what it recovered from the page
     @Override
     public void recover() throws IOException {
-        mapFile(!STRICT_CAPACITY);
-        super.recover();
-    }
+        mapFile();
+        buffer.position(0);
+        this.version = buffer.get();
+        validateVersion(this.version);
+        this.head = 1;
 
-    // memory map data file to this.buffer and read initial version byte
-    // @param strictCapacity if true verify that data file size is same as configured page capacity, if false update page capacity to actual file size
-    private void mapFile(boolean strictCapacity) throws IOException {
-        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
+        // force minSeqNum to actual first element seqNum
+        this.minSeqNum = buffer.getLong();
+        // reset back position to first seqNum
+        buffer.position(this.head);
 
-        if (raf.length() > Integer.MAX_VALUE) {
-            throw new IOException("Page file too large " + this.file);
-        }
-        int pageFileCapacity = (int)raf.length();
+        // reset elementCount to 0 and increment to octal number of valid elements found
+        this.elementCount = 0;
 
-        if (strictCapacity && this.capacity != pageFileCapacity) {
-            throw new IOException("Page file size " + pageFileCapacity + " different to configured page capacity " + this.capacity + " for " + this.file);
+        for (int i = 0; ; i++) {
+            try {
+                // verify that seqNum must be of strict + 1 increasing order
+                readNextElement(this.minSeqNum + i, VERIFY_CHECKSUM);
+                this.elementCount += 1;
+            } catch (MmapPageIO.PageIOInvalidElementException e) {
+                // simply stop at first invalid element
+                LOGGER.debug("PageIO recovery element index:{}, readNextElement exception: {}", i, e.getMessage());
+                break;
+            }
         }
 
-        // update capacity to actual raf length
-        this.capacity = pageFileCapacity;
-
-        if (this.capacity < MIN_CAPACITY) { throw new IOException(String.format("Page file size is too small to hold elements")); }
-
-        this.channel = raf.getChannel();
-        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
-        raf.close();
-        this.buffer.load();
+        // if we were not able to read any element just reset minSeqNum to zero
+        if (this.elementCount <= 0) {
+            this.minSeqNum = 0;
+        }
     }
 
     @Override
     public void create() throws IOException {
-        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
-        this.channel = raf.getChannel();
-        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
-        raf.close();
-
-        super.create();
+        try (RandomAccessFile raf = new RandomAccessFile(this.file, "rw")) {
+            this.buffer = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+        }
+        buffer.position(0);
+        buffer.put(VERSION_ONE);
+        this.head = 1;
+        this.minSeqNum = 0L;
+        this.elementCount = 0;
     }
 
     @Override
-    public void deactivate() throws IOException {
+    public void deactivate() {
         close(); // close can be called multiple times
     }
 
     @Override
     public void activate() throws IOException {
-        if (this.channel == null) {
-            RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
-            this.channel = raf.getChannel();
-            this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
-            raf.close();
+        if (this.buffer == null) {
+            try (RandomAccessFile raf = new RandomAccessFile(this.file, "rw")) {
+                this.buffer = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+            }
             this.buffer.load();
         }
         // TODO: do we need to check is the channel is still open? not sure how it could be closed
@@ -102,29 +205,192 @@ public void ensurePersisted() {
     public void purge() throws IOException {
         close();
         Files.delete(this.file.toPath());
+        this.head = 0;
+    }
+
+    @Override
+    public void write(byte[] bytes, long seqNum) {
+        write(bytes, seqNum, bytes.length, checksum(bytes));
     }
 
     @Override
-    public void close() throws IOException {
+    public void close() {
         if (this.buffer != null) {
             this.buffer.force();
+            BUFFER_CLEANER.clean(buffer);
 
-            // calling the cleaner() method releases resources held by this direct buffer which would be held until GC otherwise.
-            // see https://github.com/elastic/logstash/pull/6740
-            Cleaner cleaner = ((DirectBuffer) this.buffer).cleaner();
-            if (cleaner != null) { cleaner.clean(); }
-
-        }
-        if (this.channel != null) {
-            if (this.channel.isOpen()) { this.channel.force(false); }
-            this.channel.close(); // close can be called multiple times
         }
-        this.channel = null;
         this.buffer = null;
     }
 
     @Override
-    protected MappedByteBuffer getBuffer() {
-        return this.buffer;
+    public int getCapacity() {
+        return this.capacity;
+    }
+
+    @Override
+    public long getMinSeqNum() {
+        return this.minSeqNum;
+    }
+
+    @Override
+    public int getElementCount() {
+        return this.elementCount;
+    }
+
+    @Override
+    public boolean hasSpace(int bytes) {
+        int bytesLeft = this.capacity - this.head;
+        return persistedByteCount(bytes) <= bytesLeft;
+    }
+
+    @Override
+    public int persistedByteCount(int byteCount) {
+        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
+    }
+
+    @Override
+    public int getHead() {
+        return this.head;
+    }
+
+    private int checksum(byte[] bytes) {
+        checkSummer.reset();
+        checkSummer.update(bytes, 0, bytes.length);
+        return (int) checkSummer.getValue();
+    }
+
+    private long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+
+    // memory map data file to this.buffer and read initial version byte
+    private void mapFile() throws IOException {
+        try (RandomAccessFile raf = new RandomAccessFile(this.file, "rw")) {
+
+            if (raf.length() > Integer.MAX_VALUE) {
+                throw new IOException("Page file too large " + this.file);
+            }
+            int pageFileCapacity = (int) raf.length();
+
+            // update capacity to actual raf length. this can happen if a page size was changed on a non empty queue directory for example.
+            this.capacity = pageFileCapacity;
+
+            if (this.capacity < MIN_CAPACITY) {
+                throw new IOException(String.format("Page file size is too small to hold elements"));
+            }
+            this.buffer = raf.getChannel().map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+        }
+        this.buffer.load();
+    }
+
+    // read and validate next element at page head
+    // @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
+    private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws MmapPageIO.PageIOInvalidElementException {
+        // if there is no room for the seqNum and length bytes stop here
+        // TODO: I know this isn't a great exception message but at the time of writing I couldn't come up with anything better :P
+        if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
+            throw new MmapPageIO.PageIOInvalidElementException(
+                "cannot read seqNum and length bytes past buffer capacity");
+        }
+
+        int elementOffset = this.head;
+        int newHead = this.head;
+
+        long seqNum = buffer.getLong();
+        newHead += SEQNUM_SIZE;
+
+        if (seqNum != expectedSeqNum) {
+            throw new MmapPageIO.PageIOInvalidElementException(
+                String.format("Element seqNum %d is expected to be %d", seqNum, expectedSeqNum));
+        }
+
+        int length = buffer.getInt();
+        newHead += LENGTH_SIZE;
+
+        // length must be > 0
+        if (length <= 0) {
+            throw new MmapPageIO.PageIOInvalidElementException("Element invalid length");
+        }
+
+        // if there is no room for the proposed data length and checksum just stop here
+        if (newHead + length + CHECKSUM_SIZE > capacity) {
+            throw new MmapPageIO.PageIOInvalidElementException(
+                "cannot read element payload and checksum past buffer capacity");
+        }
+
+        if (verifyChecksum) {
+            // read data and compute checksum;
+            this.checkSummer.reset();
+            final int prevLimit = buffer.limit();
+            buffer.limit(buffer.position() + length);
+            this.checkSummer.update(buffer);
+            buffer.limit(prevLimit);
+            int checksum = buffer.getInt();
+            int computedChecksum = (int) this.checkSummer.getValue();
+            if (computedChecksum != checksum) {
+                throw new MmapPageIO.PageIOInvalidElementException(
+                    "Element invalid checksum");
+            }
+        }
+
+        // at this point we recovered a valid element
+        this.offsetMap.add(elementOffset);
+        this.head = newHead + length + CHECKSUM_SIZE;
+
+        buffer.position(this.head);
+    }
+
+    private int write(byte[] bytes, long seqNum, int length, int checksum) {
+        // since writes always happen at head, we can just append head to the offsetMap
+        assert this.offsetMap.size() == this.elementCount :
+            String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
+
+        int initialHead = this.head;
+        buffer.position(this.head);
+        buffer.putLong(seqNum);
+        buffer.putInt(length);
+        buffer.put(bytes);
+        buffer.putInt(checksum);
+        this.head += persistedByteCount(bytes.length);
+
+        assert this.head == buffer.position() :
+            String.format("head=%d != buffer position=%d", this.head, buffer.position());
+
+        if (this.elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(initialHead);
+        this.elementCount++;
+
+        return initialHead;
+    }
+
+    // we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check
+    // and if an unexpected version byte is read throw PageIOInvalidVersionException
+    private static void validateVersion(byte version)
+        throws MmapPageIO.PageIOInvalidVersionException {
+        if (version != VERSION_ONE) {
+            throw new MmapPageIO.PageIOInvalidVersionException(String
+                .format("Expected page version=%d but found version=%d", VERSION_ONE, version));
+        }
+    }
+
+    public static final class PageIOInvalidElementException extends IOException {
+
+        private static final long serialVersionUID = 1L;
+
+        public PageIOInvalidElementException(String message) {
+            super(message);
+        }
+    }
+
+    public static final class PageIOInvalidVersionException extends IOException {
+
+        private static final long serialVersionUID = 1L;
+
+        public PageIOInvalidVersionException(String message) {
+            super(message);
+        }
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIOFactory.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIOFactory.java
deleted file mode 100644
index dffe219b9dc..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/PageIOFactory.java
+++ /dev/null
@@ -1,8 +0,0 @@
-package org.logstash.ackedqueue.io;
-
-import java.io.IOException;
-
-@FunctionalInterface
-public interface PageIOFactory {
-    PageIO build(int pageNum, int capacity, String dirPath) throws IOException;
-}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStream.java b/logstash-core/src/main/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStream.java
deleted file mode 100644
index 8a94389380d..00000000000
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStream.java
+++ /dev/null
@@ -1,289 +0,0 @@
-package org.logstash.ackedqueue.io.wip;
-
-import java.util.Collections;
-import org.logstash.ackedqueue.Checkpoint;
-import org.logstash.ackedqueue.SequencedList;
-import org.logstash.ackedqueue.io.LongVector;
-import org.logstash.common.io.BufferedChecksumStreamInput;
-import org.logstash.common.io.BufferedChecksumStreamOutput;
-import org.logstash.common.io.ByteArrayStreamOutput;
-import org.logstash.common.io.ByteBufferStreamInput;
-import org.logstash.ackedqueue.io.PageIO;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-import java.util.ArrayList;
-import java.util.List;
-
-public class MemoryPageIOStream implements PageIO {
-    static final int CHECKSUM_SIZE = Integer.BYTES;
-    static final int LENGTH_SIZE = Integer.BYTES;
-    static final int SEQNUM_SIZE = Long.BYTES;
-    static final int MIN_RECORD_SIZE = SEQNUM_SIZE + LENGTH_SIZE + CHECKSUM_SIZE;
-    static final int VERSION_SIZE = Integer.BYTES;
-
-    private final byte[] buffer;
-    private final int capacity;
-    private int writePosition;
-    private int readPosition;
-    private int elementCount;
-    private long minSeqNum;
-    private ByteBufferStreamInput streamedInput;
-    private ByteArrayStreamOutput streamedOutput;
-    private BufferedChecksumStreamOutput crcWrappedOutput;
-    private final List<Integer> offsetMap;
-    private String dirPath = "";
-    private String headerDetails = "";
-
-    public int persistedByteCount(byte[] data) {
-        return persistedByteCount(data.length);
-    }
-
-    @Override
-    public int persistedByteCount(int length) {
-        return MIN_RECORD_SIZE + length;
-    }
-
-    public MemoryPageIOStream(int pageNum, int capacity, String dirPath) throws IOException {
-        this(capacity, new byte[capacity]);
-        this.dirPath = dirPath;
-    }
-
-    public MemoryPageIOStream(int capacity, String dirPath) throws IOException {
-        this(capacity, new byte[capacity]);
-        this.dirPath = dirPath;
-    }
-
-    public MemoryPageIOStream(int capacity) throws IOException {
-        this(capacity, new byte[capacity]);
-    }
-
-    public MemoryPageIOStream(int capacity, byte[] initialBytes) throws IOException {
-        this.capacity = capacity;
-        if (initialBytes.length > capacity) {
-            throw new IOException("initial bytes greater than capacity");
-        }
-        buffer = initialBytes;
-        offsetMap = new ArrayList<>();
-        streamedInput = new ByteBufferStreamInput(ByteBuffer.wrap(buffer));
-        streamedOutput = new ByteArrayStreamOutput(buffer);
-        crcWrappedOutput = new BufferedChecksumStreamOutput(streamedOutput);
-    }
-
-    @Override
-    public void recover() throws IOException {
-        throw new UnsupportedOperationException("recover() is not supported");
-    }
-
-    @Override
-    public void open(long minSeqNum, int elementCount) throws IOException {
-        this.minSeqNum = minSeqNum;
-        this.elementCount = elementCount;
-        writePosition = verifyHeader();
-        readPosition = writePosition;
-        if (elementCount > 0) {
-            long seqNumRead;
-            BufferedChecksumStreamInput in = new BufferedChecksumStreamInput(streamedInput);
-            for (int i = 0; i < this.elementCount; i++) {
-                if (writePosition + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
-                    throw new IOException(String.format("cannot read seqNum and length bytes past buffer capacity"));
-                }
-
-                seqNumRead = in.readLong();
-
-                //verify that the buffer starts with the min sequence number
-                if (i == 0 && seqNumRead != this.minSeqNum) {
-                    String msg = String.format("Page minSeqNum mismatch, expected: %d, actual: %d", this.minSeqNum, seqNumRead);
-                    throw new IOException(msg);
-                }
-
-                in.resetDigest();
-                byte[] bytes = in.readByteArray();
-                int actualChecksum = (int) in.getChecksum();
-                int expectedChecksum = in.readInt();
-
-                if (actualChecksum != expectedChecksum) {
-                    // explode with tragic error
-                }
-
-                offsetMap.add(writePosition);
-                writePosition += persistedByteCount(bytes);
-            }
-            setReadPoint(this.minSeqNum);
-        }
-    }
-
-    @Override
-    public void create() throws IOException {
-        writePosition = addHeader();
-        readPosition = writePosition;
-        this.minSeqNum = 1L;
-        this.elementCount = 0;
-    }
-
-    @Override
-    public boolean hasSpace(int byteSize) {
-        return this.capacity >= writePosition + persistedByteCount(byteSize);
-    }
-
-    @Override
-    public void write(byte[] bytes, long seqNum) throws IOException {
-        int pos = this.writePosition;
-        int writeLength = persistedByteCount(bytes);
-        writeToBuffer(seqNum, bytes, writeLength);
-        writePosition += writeLength;
-        assert writePosition == streamedOutput.getPosition() :
-                String.format("writePosition=%d != streamedOutput position=%d", writePosition, streamedOutput.getPosition());
-        if (elementCount <= 0) {
-            this.minSeqNum = seqNum;
-        }
-        this.offsetMap.add(pos);
-        elementCount++;
-    }
-
-    @Override
-    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
-        if (elementCount == 0) {
-            return new SequencedList<>(Collections.emptyList(), new LongVector(0));
-        }
-        setReadPoint(seqNum);
-        return read(limit);
-    }
-
-    @Override
-    public int getCapacity() {
-        return capacity;
-    }
-
-    @Override
-    public int getHead() {
-        return writePosition;
-    }
-
-    @Override
-    public void deactivate() {
-        // do nothing
-    }
-
-    @Override
-    public void activate() {
-        // do nothing
-    }
-
-    @Override
-    public void ensurePersisted() {
-        // do nothing
-    }
-
-    @Override
-    public void purge() throws IOException {
-        // do nothing
-    }
-
-    @Override
-    public void close() throws IOException {
-        // TBD
-    }
-
-    //@Override
-    public void setPageHeaderDetails(String details) {
-        headerDetails = details;
-    }
-
-    public int getWritePosition() {
-        return writePosition;
-    }
-
-    public int getElementCount() {
-        return elementCount;
-    }
-
-    public long getMinSeqNum() {
-        return minSeqNum;
-    }
-
-    // used in tests
-    public byte[] getBuffer() {
-        return buffer;
-    }
-
-    // used in tests
-    public String readHeaderDetails() throws IOException {
-        int tempPosition = readPosition;
-        streamedInput.movePosition(0);
-        int ver = streamedInput.readInt();
-        String details = new String(streamedInput.readByteArray());
-        streamedInput.movePosition(tempPosition);
-        return details;
-    }
-
-    private void setReadPoint(long seqNum) {
-        int readPosition = offsetMap.get(calcRelativeSeqNum(seqNum));
-        streamedInput.movePosition(readPosition);
-    }
-
-    private int calcRelativeSeqNum(long seqNum) {
-        return (int) (seqNum - minSeqNum);
-    }
-
-    private int addHeader() throws IOException {
-        streamedOutput.writeInt(Checkpoint.VERSION);
-        byte[] details = headerDetails.getBytes();
-        streamedOutput.writeByteArray(details);
-        return VERSION_SIZE + LENGTH_SIZE + details.length;
-    }
-
-    private int verifyHeader() throws IOException {
-        int ver = streamedInput.readInt();
-        if (ver != Checkpoint.VERSION) {
-            String msg = String.format("Page version mismatch, expecting: %d, this version: %d", Checkpoint.VERSION, ver);
-            throw new IOException(msg);
-        }
-        int len = streamedInput.readInt();
-        streamedInput.skip(len);
-        return VERSION_SIZE + LENGTH_SIZE + len;
-    }
-
-    private void writeToBuffer(long seqNum, byte[] data, int len) throws IOException {
-        streamedOutput.setWriteWindow(writePosition, len);
-        crcWrappedOutput.writeLong(seqNum);
-        crcWrappedOutput.resetDigest();
-        crcWrappedOutput.writeByteArray(data);
-        long checksum = crcWrappedOutput.getChecksum();
-        crcWrappedOutput.writeInt((int) checksum);
-        crcWrappedOutput.flush();
-        crcWrappedOutput.close();
-    }
-
-    private SequencedList<byte[]> read(int limit) throws IOException {
-        int upto = available(limit);
-        List<byte[]> elements = new ArrayList<>(upto);
-        final LongVector seqNums = new LongVector(upto);
-        for (int i = 0; i < upto; i++) {
-            long seqNum = readSeqNum();
-            byte[] data = readData();
-            skipChecksum();
-            elements.add(data);
-            seqNums.add(seqNum);
-        }
-        return new SequencedList<>(elements, seqNums);
-    }
-
-    private long readSeqNum() throws IOException {
-        return streamedInput.readLong();
-    }
-
-    private byte[] readData() throws IOException {
-        return streamedInput.readByteArray();
-    }
-
-    private void skipChecksum() {
-        streamedInput.skip(CHECKSUM_SIZE);
-    }
-
-    private int available(int sought) {
-        if (elementCount < 1) return 0;
-        if (elementCount < sought) return elementCount;
-        return sought;
-    }
-}
diff --git a/logstash-core/src/main/java/org/logstash/common/FsUtil.java b/logstash-core/src/main/java/org/logstash/common/FsUtil.java
new file mode 100644
index 00000000000..227a47489ae
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/FsUtil.java
@@ -0,0 +1,59 @@
+package org.logstash.common;
+
+import java.io.File;
+import java.io.IOException;
+import java.nio.file.Path;
+import java.util.Arrays;
+import java.util.HashSet;
+import java.util.Set;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+
+/**
+ * File System Utility Methods.
+ */
+public final class FsUtil {
+
+    private FsUtil() {
+    }
+
+    private static final boolean IS_WINDOWS = System.getProperty("os.name").startsWith("Windows");
+    private static final Logger logger = LogManager.getLogger(FsUtil.class);
+
+    /**
+     * Checks if the request number of bytes of free disk space are available under the given
+     * path.
+     * @param path Directory to check
+     * @param size Bytes of free space requested
+     * @return True iff the
+     * @throws IOException on failure to determine free space for given path's partition
+     */
+    public static boolean hasFreeSpace(final Path path, final long size)
+        throws IOException
+    {
+        final Set<File> partitionRoots = new HashSet<>(Arrays.asList(File.listRoots()));
+
+        // crawl up file path until we find a root partition
+        File location = path.toFile().getCanonicalFile();
+        while (!partitionRoots.contains(location)) {
+            location = location.getParentFile();
+            if (location == null) {
+                throw new IllegalStateException(String.format("Unable to determine the partition that contains '%s'", path));
+            }
+        }
+
+        final long freeSpace = location.getFreeSpace();
+
+        if (freeSpace == 0L && IS_WINDOWS) {
+            // On Windows, SUBST'ed drives report 0L from getFreeSpace().
+            // The API doc says "The number of unallocated bytes on the partition or 0L if the abstract pathname does not name a partition."
+            // There is no straightforward fix for this and it seems a fix is included in Java 9.
+            // One alternative is to launch and parse a DIR command and look at the reported free space.
+            // This is a temporary fix to get the CI tests going which relies on SUBST'ed drives to manage long paths.
+            logger.warn("Cannot retrieve free space on " +  location.toString() +  ". This is probably a SUBST'ed drive.");
+            return true;
+        }
+
+        return location.getFreeSpace() >= size;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/execution/MemoryReadBatch.java b/logstash-core/src/main/java/org/logstash/execution/MemoryReadBatch.java
new file mode 100644
index 00000000000..26ef1c6bec3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/execution/MemoryReadBatch.java
@@ -0,0 +1,58 @@
+package org.logstash.execution;
+
+import org.jruby.RubyArray;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.logstash.ext.JrubyEventExtLibrary;
+
+import java.util.LinkedHashSet;
+
+import static org.logstash.RubyUtil.RUBY;
+
+public final class MemoryReadBatch implements QueueBatch {
+
+    private final LinkedHashSet<IRubyObject> events;
+
+    public MemoryReadBatch(final LinkedHashSet<IRubyObject> events) {
+        this.events = events;
+    }
+
+    public static boolean isCancelled(final IRubyObject event) {
+        return ((JrubyEventExtLibrary.RubyEvent) event).getEvent().isCancelled();
+    }
+
+    public static MemoryReadBatch create(LinkedHashSet<IRubyObject> events) {
+        return new MemoryReadBatch(events);
+    }
+
+    public static MemoryReadBatch create() {
+        return create(new LinkedHashSet<>());
+    }
+
+    @Override
+    public RubyArray to_a() {
+        ThreadContext context = RUBY.getCurrentContext();
+        final RubyArray result = context.runtime.newArray(events.size());
+        for (final IRubyObject event : events) {
+            if (!isCancelled(event)) {
+                result.add(event);
+            }
+        }
+        return result;
+    }
+
+    @Override
+    public void merge(final IRubyObject event) {
+        events.add(event);
+    }
+
+    @Override
+    public int filteredSize() {
+        return events.size();
+    }
+
+    @Override
+    public void close() {
+        // no-op
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/execution/QueueBatch.java b/logstash-core/src/main/java/org/logstash/execution/QueueBatch.java
new file mode 100644
index 00000000000..b83a5212e82
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/execution/QueueBatch.java
@@ -0,0 +1,13 @@
+package org.logstash.execution;
+
+import org.jruby.RubyArray;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.io.IOException;
+
+public interface QueueBatch {
+    int filteredSize();
+    RubyArray to_a();
+    void merge(IRubyObject event);
+    void close() throws IOException;
+}
diff --git a/logstash-core/src/test/java/org/logstash/FileLockFactoryMain.java b/logstash-core/src/test/java/org/logstash/FileLockFactoryMain.java
index b285ea246d5..84aa5bc2e85 100644
--- a/logstash-core/src/test/java/org/logstash/FileLockFactoryMain.java
+++ b/logstash-core/src/test/java/org/logstash/FileLockFactoryMain.java
@@ -1,6 +1,7 @@
 package org.logstash;
 
 import java.io.IOException;
+import java.nio.file.Paths;
 
 /*
  * This program is used to test the FileLockFactory in cross-process/JVM.
@@ -9,7 +10,7 @@ public class FileLockFactoryMain {
 
     public static void main(String[] args) {
         try {
-            FileLockFactory.getDefault().obtainLock(args[0], args[1]);
+            FileLockFactory.obtainLock(Paths.get(args[0]), args[1]);
             System.out.println("File locked");
             // Sleep enough time until this process is killed.
             Thread.sleep(Long.MAX_VALUE);
diff --git a/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
index c1487f7e501..96988a395c6 100644
--- a/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
+++ b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
@@ -1,5 +1,6 @@
 package org.logstash;
 
+import java.nio.file.Path;
 import org.junit.After;
 import org.junit.Before;
 import org.junit.Rule;
@@ -12,8 +13,6 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.nio.channels.FileLock;
-import java.nio.file.FileSystems;
-import java.nio.file.Path;
 import java.nio.file.Paths;
 import java.util.concurrent.Executors;
 import java.util.concurrent.ExecutorService;
@@ -27,7 +26,7 @@
 
 public class FileLockFactoryTest {
     @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
-    private String lockDir;
+    private Path lockDir;
     private final String LOCK_FILE = ".test";
 
     private FileLock lock;
@@ -36,13 +35,13 @@ public class FileLockFactoryTest {
 
     @Before
     public void setUp() throws Exception {
-        lockDir = temporaryFolder.newFolder("lock").getPath();
+        lockDir = temporaryFolder.newFolder("lock").toPath();
         executor = Executors.newSingleThreadExecutor();
     }
 
     @Before
     public void lockFirst() throws Exception {
-        lock = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        lock = FileLockFactory.obtainLock(lockDir, LOCK_FILE);
         assertThat(lock.isValid(), is(equalTo(true)));
         assertThat(lock.isShared(), is(equalTo(false)));
     }
@@ -62,50 +61,50 @@ public void ObtainLockOnNonLocked() throws IOException {
 
     @Test(expected = LockException.class)
     public void ObtainLockOnLocked() throws IOException {
-        FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        FileLockFactory.obtainLock(lockDir, LOCK_FILE);
     }
 
     @Test
     public void ObtainLockOnOtherLocked() throws IOException {
-        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, ".test2");
+        FileLock lock2 = FileLockFactory.obtainLock(lockDir, ".test2");
         assertThat(lock2.isValid(), is(equalTo(true)));
         assertThat(lock2.isShared(), is(equalTo(false)));
     }
 
     @Test
     public void LockReleaseLock() throws IOException {
-        FileLockFactory.getDefault().releaseLock(lock);
+        FileLockFactory.releaseLock(lock);
     }
 
     @Test
     public void LockReleaseLockObtainLock() throws IOException {
-        FileLockFactory.getDefault().releaseLock(lock);
+        FileLockFactory.releaseLock(lock);
 
-        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        FileLock lock2 = FileLockFactory.obtainLock(lockDir, LOCK_FILE);
         assertThat(lock2.isValid(), is(equalTo(true)));
         assertThat(lock2.isShared(), is(equalTo(false)));
     }
 
     @Test
     public void LockReleaseLockObtainLockRelease() throws IOException {
-        FileLockFactory.getDefault().releaseLock(lock);
+        FileLockFactory.releaseLock(lock);
 
-        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        FileLock lock2 = FileLockFactory.obtainLock(lockDir, LOCK_FILE);
         assertThat(lock2.isValid(), is(equalTo(true)));
         assertThat(lock2.isShared(), is(equalTo(false)));
 
-        FileLockFactory.getDefault().releaseLock(lock2);
+        FileLockFactory.releaseLock(lock2);
     }
 
     @Test(expected = LockException.class)
     public void ReleaseNullLock() throws IOException {
-        FileLockFactory.getDefault().releaseLock(null);
-     }
+        FileLockFactory.releaseLock(null);
+    }
 
     @Test(expected = LockException.class)
     public void ReleaseUnobtainedLock() throws IOException {
-        FileLockFactory.getDefault().releaseLock(lock);
-        FileLockFactory.getDefault().releaseLock(lock);
+        FileLockFactory.releaseLock(lock);
+        FileLockFactory.releaseLock(lock);
     }
 
     @Test
@@ -119,7 +118,7 @@ public void crossJvmObtainLockOnLocked() throws Exception {
             Paths.get(System.getProperty("java.home"), "bin", "java").toString(),
             "-cp", System.getProperty("java.class.path"),
             Class.forName("org.logstash.FileLockFactoryMain").getName(),
-            lockDir, lockFile
+            lockDir.toString(), lockFile
         };
 
         try {
@@ -137,7 +136,7 @@ public void crossJvmObtainLockOnLocked() throws Exception {
 
             try {
                 // Try to obtain the lock held by the children process.
-                FileLockFactory.getDefault().obtainLock(lockDir, lockFile);
+                FileLockFactory.obtainLock(lockDir, lockFile);
                 fail("Should have threw an exception");
             } catch (LockException e) {
                 // Expected exception as the file is already locked.
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
deleted file mode 100644
index 3b0653e8d6a..00000000000
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
+++ /dev/null
@@ -1,135 +0,0 @@
-package org.logstash.ackedqueue;
-
-import org.junit.Test;
-import org.logstash.ackedqueue.io.PageIO;
-
-import java.io.IOException;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.MatcherAssert.assertThat;
-import static org.logstash.ackedqueue.QueueTestHelpers.singleElementCapacityForByteBufferPageIO;
-import static org.mockito.Mockito.mock;
-
-public class HeadPageTest {
-
-    @Test
-    public void newHeadPage() throws IOException {
-        Settings s = TestSettings.volatileQueueSettings(100);
-        // Close method on HeadPage requires an instance of Queue that has already been opened.
-        Queue q = mock(Queue.class);
-        PageIO pageIO = s.getPageIOFactory().build(0, 100, "dummy");
-        pageIO.create();
-        try(final HeadPage p = new HeadPage(0, q, pageIO)) {
-            assertThat(p.getPageNum(), is(equalTo(0)));
-            assertThat(p.isFullyRead(), is(true));
-            assertThat(p.isFullyAcked(), is(false));
-            assertThat(p.hasSpace(10), is(true));
-            assertThat(p.hasSpace(100), is(false));
-        }
-    }
-
-    @Test
-    public void pageWrite() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-
-        Settings s = TestSettings.volatileQueueSettings(singleElementCapacityForByteBufferPageIO(element));
-        try(Queue q = new Queue(s)) {
-            q.open();
-            HeadPage p = q.headPage;
-
-            assertThat(p.hasSpace(element.serialize().length), is(true));
-            p.write(element.serialize(), 0, 1);
-
-            assertThat(p.hasSpace(element.serialize().length), is(false));
-            assertThat(p.isFullyRead(), is(false));
-            assertThat(p.isFullyAcked(), is(false));
-        }
-    }
-
-    @Test
-    public void pageWriteAndReadSingle() throws IOException {
-        long seqNum = 1L;
-        Queueable element = new StringElement("foobarbaz");
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-
-        Settings s = TestSettings.volatileQueueSettings(singleElementCapacity);
-        try(Queue q = new Queue(s)) {
-            q.open();
-            HeadPage p = q.headPage;
-
-            assertThat(p.hasSpace(element.serialize().length), is(true));
-            p.write(element.serialize(), seqNum, 1);
-
-            Batch b = p.readBatch(1);
-
-            assertThat(b.getElements().size(), is(equalTo(1)));
-            assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
-
-            assertThat(p.hasSpace(element.serialize().length), is(false));
-            assertThat(p.isFullyRead(), is(true));
-            assertThat(p.isFullyAcked(), is(false));
-        }
-    }
-
-    @Test
-    public void inEmpty() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-
-        Settings s = TestSettings.volatileQueueSettings(1000);
-        try(Queue q = new Queue(s)) {
-            q.open();
-            HeadPage p = q.headPage;
-
-            assertThat(p.isEmpty(), is(true));
-            p.write(element.serialize(), 1, 1);
-            assertThat(p.isEmpty(), is(false));
-            Batch b = q.readBatch(1);
-            assertThat(p.isEmpty(), is(false));
-            b.close();
-            assertThat(p.isEmpty(), is(true));
-        }
-    }
-
-    @Test
-    public void pageWriteAndReadMulti() throws IOException {
-        long seqNum = 1L;
-        Queueable element = new StringElement("foobarbaz");
-
-        Settings s = TestSettings.volatileQueueSettings(singleElementCapacityForByteBufferPageIO(element));
-        try(Queue q = new Queue(s)) {
-            q.open();
-            HeadPage p = q.headPage;
-
-            assertThat(p.hasSpace(element.serialize().length), is(true));
-            p.write(element.serialize(), seqNum, 1);
-
-            Batch b = p.readBatch(10);
-
-            assertThat(b.getElements().size(), is(equalTo(1)));
-            assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
-
-            assertThat(p.hasSpace(element.serialize().length), is(false));
-            assertThat(p.isFullyRead(), is(true));
-            assertThat(p.isFullyAcked(), is(false));
-        }
-    }
-
-    // disabled test until we figure what to do in this condition
-//    @Test
-//    public void pageViaQueueOpenForHeadCheckpointWithoutSupportingPageFiles() throws Exception {
-//        URL url = FileCheckpointIOTest.class.getResource("checkpoint.head");
-//        String dirPath = Paths.get(url.toURI()).getParent().toString();
-//        Queueable element = new StringElement("foobarbaz");
-//        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-//        Settings s = TestSettings.persistedQueueSettings(singleElementCapacity, dirPath);
-//        TestQueue q = new TestQueue(s);
-//        try {
-//            q.open();
-//        } catch (NoSuchFileException e) {
-//            assertThat(e.getMessage(), containsString("checkpoint.2"));
-//        }
-//        HeadPage p = q.getHeadPage();
-//        assertThat(p, is(equalTo(null)));
-//    }
-}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
index a27cbccc589..b90b49ca98e 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -23,8 +23,7 @@
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
-import org.logstash.ackedqueue.io.AbstractByteBufferPageIO;
-import org.logstash.ackedqueue.io.LongVector;
+import org.logstash.ackedqueue.io.MmapPageIO;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
@@ -32,7 +31,7 @@
 import static org.hamcrest.CoreMatchers.nullValue;
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.junit.Assert.fail;
-import static org.logstash.ackedqueue.QueueTestHelpers.singleElementCapacityForByteBufferPageIO;
+import static org.logstash.ackedqueue.QueueTestHelpers.computeCapacityForMmapPageIO;
 
 public class QueueTest {
 
@@ -59,7 +58,7 @@ public void tearDown() throws Exception {
 
     @Test
     public void newQueue() throws IOException {
-        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(10))) {
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(10, dataPath))) {
             q.open();
 
             assertThat(q.nonBlockReadBatch(1), nullValue());
@@ -68,7 +67,7 @@ public void newQueue() throws IOException {
 
     @Test
     public void singleWriteRead() throws IOException {
-        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(100))) {
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(100, dataPath))) {
             q.open();
 
             Queueable element = new StringElement("foobarbaz");
@@ -91,10 +90,10 @@ public void singleWriteRead() throws IOException {
     @Test(timeout = 5000)
     public void writeToFullyAckedHeadpage() throws IOException {
         final Queueable element = new StringElement("foobarbaz");
-        final int page = element.serialize().length * 2 + AbstractByteBufferPageIO.MIN_CAPACITY;
+        final int page = element.serialize().length * 2 + MmapPageIO.MIN_CAPACITY;
         // Queue that can only hold one element per page.
-        try (Queue q = new TestQueue(
-            TestSettings.volatileQueueSettings(page, page * 2 - 1))) {
+        try (Queue q = new Queue(
+            TestSettings.persistedQueueSettings(page, page * 2 - 1, dataPath))) {
             q.open();
             for (int i = 0; i < 5; ++i) {
                 q.write(element);
@@ -107,6 +106,18 @@ public void writeToFullyAckedHeadpage() throws IOException {
         }
     }
 
+    @Test
+    public void canReadBatchZeroSize() throws IOException {
+        final int page = MmapPageIO.MIN_CAPACITY;
+        try (Queue q = new Queue(
+            TestSettings.persistedQueueSettings(page, page * 2 - 1, dataPath))) {
+            q.open();
+            try (Batch b = q.readBatch(0, 500L)) {
+                assertThat(b.getElements().size(), is(0));
+            }
+        }
+    }
+
     /**
      * This test ensures that the {@link Queue} functions properly when pagesize is equal to overall
      * queue size (i.e. there is only a single page).
@@ -116,8 +127,8 @@ public void writeToFullyAckedHeadpage() throws IOException {
     public void writeWhenPageEqualsQueueSize() throws IOException {
         final Queueable element = new StringElement("foobarbaz");
         // Queue that can only hold one element per page.
-        try (Queue q = new TestQueue(
-            TestSettings.volatileQueueSettings(1024, 1024L))) {
+        try (Queue q = new Queue(
+            TestSettings.persistedQueueSettings(1024, 1024L, dataPath))) {
             q.open();
             for (int i = 0; i < 3; ++i) {
                 q.write(element);
@@ -132,7 +143,7 @@ public void writeWhenPageEqualsQueueSize() throws IOException {
 
     @Test
     public void singleWriteMultiRead() throws IOException {
-        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(100))) {
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(100, dataPath))) {
             q.open();
 
             Queueable element = new StringElement("foobarbaz");
@@ -148,7 +159,7 @@ public void singleWriteMultiRead() throws IOException {
 
     @Test
     public void multiWriteSamePage() throws IOException {
-        try (Queue q = new TestQueue(TestSettings.volatileQueueSettings(100))) {
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(100, dataPath))) {
             q.open();
             List<Queueable> elements = Arrays
                 .asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"),
@@ -174,9 +185,8 @@ public void multiWriteSamePage() throws IOException {
     @Test
     public void writeMultiPage() throws IOException {
         List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(elements.get(0));
-        try (TestQueue q = new TestQueue(
-            TestSettings.volatileQueueSettings(2 * singleElementCapacity))) {
+        try (Queue q = new Queue(
+            TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements.get(0), 2), dataPath))) {
             q.open();
 
             for (Queueable e : elements) {
@@ -184,30 +194,30 @@ public void writeMultiPage() throws IOException {
             }
 
             // total of 2 pages: 1 head and 1 tail
-            assertThat(q.getTailPages().size(), is(1));
+            assertThat(q.tailPages.size(), is(1));
 
-            assertThat(q.getTailPages().get(0).isFullyRead(), is(false));
-            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));
-            assertThat(q.getHeadPage().isFullyRead(), is(false));
-            assertThat(q.getHeadPage().isFullyAcked(), is(false));
+            assertThat(q.tailPages.get(0).isFullyRead(), is(false));
+            assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
+            assertThat(q.headPage.isFullyRead(), is(false));
+            assertThat(q.headPage.isFullyAcked(), is(false));
 
             Batch b = q.nonBlockReadBatch(10);
             assertThat(b.getElements().size(), is(2));
 
-            assertThat(q.getTailPages().size(), is(1));
+            assertThat(q.tailPages.size(), is(1));
 
-            assertThat(q.getTailPages().get(0).isFullyRead(), is(true));
-            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));
-            assertThat(q.getHeadPage().isFullyRead(), is(false));
-            assertThat(q.getHeadPage().isFullyAcked(), is(false));
+            assertThat(q.tailPages.get(0).isFullyRead(), is(true));
+            assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
+            assertThat(q.headPage.isFullyRead(), is(false));
+            assertThat(q.headPage.isFullyAcked(), is(false));
 
             b = q.nonBlockReadBatch(10);
             assertThat(b.getElements().size(), is(2));
 
-            assertThat(q.getTailPages().get(0).isFullyRead(), is(true));
-            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));
-            assertThat(q.getHeadPage().isFullyRead(), is(true));
-            assertThat(q.getHeadPage().isFullyAcked(), is(false));
+            assertThat(q.tailPages.get(0).isFullyRead(), is(true));
+            assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
+            assertThat(q.headPage.isFullyRead(), is(true));
+            assertThat(q.headPage.isFullyAcked(), is(false));
 
             b = q.nonBlockReadBatch(10);
             assertThat(b, nullValue());
@@ -218,9 +228,8 @@ public void writeMultiPage() throws IOException {
     @Test
     public void writeMultiPageWithInOrderAcking() throws IOException {
         List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(elements.get(0));
-        try (TestQueue q = new TestQueue(
-            TestSettings.volatileQueueSettings(2 * singleElementCapacity))) {
+        try (Queue q = new Queue(
+            TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements.get(0), 2), dataPath))) {
             q.open();
 
             for (Queueable e : elements) {
@@ -230,29 +239,29 @@ public void writeMultiPageWithInOrderAcking() throws IOException {
             Batch b = q.nonBlockReadBatch(10);
 
             assertThat(b.getElements().size(), is(2));
-            assertThat(q.getTailPages().size(), is(1));
+            assertThat(q.tailPages.size(), is(1));
 
             // lets keep a ref to that tail page before acking
-            TailPage tailPage = q.getTailPages().get(0);
+            Page tailPage = q.tailPages.get(0);
 
             assertThat(tailPage.isFullyRead(), is(true));
 
             // ack first batch which includes all elements from tailPages
             b.close();
 
-            assertThat(q.getTailPages().size(), is(0));
+            assertThat(q.tailPages.size(), is(0));
             assertThat(tailPage.isFullyRead(), is(true));
             assertThat(tailPage.isFullyAcked(), is(true));
 
             b = q.nonBlockReadBatch(10);
 
             assertThat(b.getElements().size(), is(2));
-            assertThat(q.getHeadPage().isFullyRead(), is(true));
-            assertThat(q.getHeadPage().isFullyAcked(), is(false));
+            assertThat(q.headPage.isFullyRead(), is(true));
+            assertThat(q.headPage.isFullyAcked(), is(false));
 
             b.close();
 
-            assertThat(q.getHeadPage().isFullyAcked(), is(true));
+            assertThat(q.headPage.isFullyAcked(), is(true));
         }
     }
 
@@ -260,16 +269,15 @@ public void writeMultiPageWithInOrderAcking() throws IOException {
     public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
         List<Queueable> elements1 = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"));
         List<Queueable> elements2 = Arrays.asList(new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(elements1.get(0));
 
         Settings settings = SettingsImpl.builder(
-            TestSettings.volatileQueueSettings(2 * singleElementCapacity)
+            TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements1.get(0), 2), dataPath)
         ).checkpointMaxWrites(1024) // arbitrary high enough threshold so that it's not reached (default for TestSettings is 1)
-        .build();
-        try (TestQueue q = new TestQueue(settings)) {
+            .build();
+        try (Queue q = new Queue(settings)) {
             q.open();
 
-            assertThat(q.getHeadPage().getPageNum(), is(0));
+            assertThat(q.headPage.getPageNum(), is(0));
             Checkpoint c = q.getCheckpointIO().read("checkpoint.head");
             assertThat(c.getPageNum(), is(0));
             assertThat(c.getElementCount(), is(0));
@@ -288,7 +296,7 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
             assertThat(c.getFirstUnackedSeqNum(), is(0L));
             assertThat(c.getFirstUnackedPageNum(), is(0));
 
-        //  assertThat(elements1.get(1).getSeqNum(), is(2L));
+            //  assertThat(elements1.get(1).getSeqNum(), is(2L));
             q.ensurePersistedUpto(2);
 
             c = q.getCheckpointIO().read("checkpoint.head");
@@ -350,7 +358,7 @@ public void randomAcking() throws IOException {
 
         // 10 tests of random queue sizes
         for (int loop = 0; loop < 10; loop++) {
-            int page_count = random.nextInt(10000) + 1;
+            int page_count = random.nextInt(100) + 1;
 
             // String format call below needs to at least print one digit
             final int digits = Math.max((int) Math.ceil(Math.log10(page_count)), 1);
@@ -360,16 +368,15 @@ public void randomAcking() throws IOException {
             for (int i = 0; i < page_count; i++) {
                 elements.add(new StringElement(String.format("%0" + digits + "d", i)));
             }
-            int singleElementCapacity = singleElementCapacityForByteBufferPageIO(elements.get(0));
-            try (TestQueue q = new TestQueue(
-                TestSettings.volatileQueueSettings(singleElementCapacity))) {
+            try (Queue q = new Queue(
+                TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(elements.get(0)), dataPath))) {
                 q.open();
 
                 for (Queueable e : elements) {
                     q.write(e);
                 }
 
-                assertThat(q.getTailPages().size(), is(page_count - 1));
+                assertThat(q.tailPages.size(), is(page_count - 1));
 
                 // first read all elements
                 List<Batch> batches = new ArrayList<>();
@@ -383,24 +390,24 @@ public void randomAcking() throws IOException {
                 for (Batch b : batches) {
                     b.close();
                 }
-                
-                assertThat(q.getTailPages().size(), is(0));
+
+                assertThat(q.tailPages.size(), is(0));
             }
         }
     }
 
-    @Test(timeout = 5000)
+    @Test(timeout = 50_000)
     public void reachMaxUnread() throws IOException, InterruptedException, ExecutionException {
         Queueable element = new StringElement("foobarbaz");
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
+        int singleElementCapacity = computeCapacityForMmapPageIO(element);
 
         Settings settings = SettingsImpl.builder(
-            TestSettings.volatileQueueSettings(singleElementCapacity)
+            TestSettings.persistedQueueSettings(singleElementCapacity, dataPath)
         ).maxUnread(2) // 2 so we know the first write should not block and the second should
-        .build();
-        try (TestQueue q = new TestQueue(settings)) {
+            .build();
+        try (Queue q = new Queue(settings)) {
             q.open();
-            
+
             long seqNum = q.write(element);
             assertThat(seqNum, is(1L));
             assertThat(q.isFull(), is(false));
@@ -428,7 +435,7 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution
             }
 
             // since we did not ack and pages hold a single item
-            assertThat(q.getTailPages().size(), is(ELEMENT_COUNT));
+            assertThat(q.tailPages.size(), is(ELEMENT_COUNT));
         }
     }
 
@@ -438,10 +445,10 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,
 
         // TODO: add randomized testing on the page size (but must be > single element size)
         Settings settings = SettingsImpl.builder(
-            TestSettings.volatileQueueSettings(256) // 256 is arbitrary, large enough to hold a few elements
+            TestSettings.persistedQueueSettings(256, dataPath) // 256 is arbitrary, large enough to hold a few elements
         ).maxUnread(2)
-        .build(); // 2 so we know the first write should not block and the second should
-        try (TestQueue q = new TestQueue(settings)) {
+            .build(); // 2 so we know the first write should not block and the second should
+        try (Queue q = new Queue(settings)) {
             q.open();
 
             // perform first non-blocking write
@@ -472,24 +479,24 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,
             }
 
             // all batches are acked, no tail pages should exist
-            assertThat(q.getTailPages().size(), is(0));
+            assertThat(q.tailPages.size(), is(0));
 
             // the last read unblocked the last write so some elements (1 unread and maybe some acked) should be in the head page
-            assertThat(q.getHeadPage().getElementCount() > 0L, is(true));
-            assertThat(q.getHeadPage().unreadCount(), is(1L));
+            assertThat(q.headPage.getElementCount() > 0L, is(true));
+            assertThat(q.headPage.unreadCount(), is(1L));
             assertThat(q.unreadCount, is(1L));
         }
     }
 
-    @Test(timeout = 5000)
+    @Test(timeout = 50_000)
     public void reachMaxSizeTest() throws IOException, InterruptedException {
         Queueable element = new StringElement("0123456789"); // 10 bytes
 
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-
         // allow 10 elements per page but only 100 events in total
-        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
-        try (TestQueue q = new TestQueue(settings)) {
+        Settings settings = TestSettings.persistedQueueSettings(
+            computeCapacityForMmapPageIO(element, 10), computeCapacityForMmapPageIO(element, 100), dataPath
+        );
+        try (Queue q = new Queue(settings)) {
             q.open();
 
             int elementCount = 99; // should be able to write 99 events before getting full
@@ -508,16 +515,16 @@ public void reachMaxSizeTest() throws IOException, InterruptedException {
         }
     }
 
-    @Test(timeout = 5000)
+    @Test(timeout = 50_000)
     public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedException, ExecutionException {
 
         Queueable element = new StringElement("0123456789"); // 10 bytes
 
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-
         // allow 10 elements per page but only 100 events in total
-        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
-        try (TestQueue q = new TestQueue(settings)) {
+        Settings settings = TestSettings.persistedQueueSettings(
+            computeCapacityForMmapPageIO(element, 10), computeCapacityForMmapPageIO(element, 100), dataPath
+        );
+        try (Queue q = new Queue(settings)) {
             q.open();
             // should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full
             final long elementCount = 99;
@@ -525,39 +532,39 @@ public void ackingMakesQueueNotFullAgainTest() throws IOException, InterruptedEx
                 q.write(element);
             }
             assertThat(q.isFull(), is(false));
-            
+
             // we expect this next write call to block so let's wrap it in a Future
             Future<Long> future = executor.submit(() -> q.write(element));
             assertThat(future.isDone(), is(false));
-            
+
             while (!q.isFull()) {
                 Thread.sleep(10);
             }
             assertThat(q.isFull(), is(true));
-            
-            Batch b = q.readBatch(10); // read 1 page (10 events)
+
+            Batch b = q.readBatch(10, TimeUnit.SECONDS.toMillis(1)); // read 1 page (10 events)
             b.close();  // purge 1 page
-            
+
             while (q.isFull()) { Thread.sleep(10); }
             assertThat(q.isFull(), is(false));
-            
+
             assertThat(future.get(), is(elementCount + 1));
         }
     }
 
-    @Test(timeout = 5000)
+    @Test(timeout = 50_000)
     public void resumeWriteOnNoLongerFullQueueTest() throws IOException, InterruptedException, ExecutionException {
         Queueable element = new StringElement("0123456789"); // 10 bytes
 
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-
         // allow 10 elements per page but only 100 events in total
-        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
-        try (TestQueue q = new TestQueue(settings)) {
+        Settings settings = TestSettings.persistedQueueSettings(
+            computeCapacityForMmapPageIO(element, 10),  computeCapacityForMmapPageIO(element, 100), dataPath
+        );
+        try (Queue q = new Queue(settings)) {
             q.open();
             // should be able to write 90 + 9 events (9 pages + 1 head-page) before getting full
             int elementCount = 99;
-            for (int i = 0; i < elementCount; i++) { 
+            for (int i = 0; i < elementCount; i++) {
                 q.write(element);
             }
 
@@ -565,7 +572,7 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted
 
             // read 1 page (10 events) here while not full yet so that the read will not singal the not full state
             // we want the batch closing below to signal the not full state
-            Batch b = q.readBatch(10);
+            Batch b = q.readBatch(10, TimeUnit.SECONDS.toMillis(1));
 
             // we expect this next write call to block so let's wrap it in a Future
             Future<Long> future = executor.submit(() -> q.write(element));
@@ -582,16 +589,16 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted
         }
     }
 
-    @Test(timeout = 5000)
+    @Test(timeout = 50_000)
     public void queueStillFullAfterPartialPageAckTest() throws IOException, InterruptedException {
 
         Queueable element = new StringElement("0123456789"); // 10 bytes
 
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-
         // allow 10 elements per page but only 100 events in total
-        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
-        try (TestQueue q = new TestQueue(settings)) {
+        Settings settings = TestSettings.persistedQueueSettings(
+            computeCapacityForMmapPageIO(element, 10), computeCapacityForMmapPageIO(element, 100), dataPath
+        );
+        try (Queue q = new Queue(settings)) {
             q.open();
 
             int ELEMENT_COUNT = 99; // should be able to write 99 events before getting full
@@ -608,7 +615,7 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup
             }
             assertThat(q.isFull(), is(true));
 
-            Batch b = q.readBatch(9); // read 90% of page (9 events)
+            Batch b = q.readBatch(9, TimeUnit.SECONDS.toMillis(1)); // read 90% of page (9 events)
             b.close();  // this should not purge a page
 
             assertThat(q.isFull(), is(true)); // queue should still be full
@@ -619,7 +626,7 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup
     public void queueStableUnderStressHugeCapacity() throws Exception {
         stableUnderStress(100_000);
     }
-    
+
     @Test
     public void queueStableUnderStressLowCapacity() throws Exception {
         stableUnderStress(50);
@@ -645,12 +652,11 @@ public void testAckedCount() throws IOException {
         }
 
         long secondSeqNum;
-        long thirdSeqNum;
         try(Queue q = new Queue(settings)){
             q.open();
 
             secondSeqNum = q.write(element2);
-            thirdSeqNum = q.write(element3);
+            q.write(element3);
 
             b = q.nonBlockReadBatch(1);
             assertThat(b.getElements().size(), is(1));
@@ -661,9 +667,7 @@ public void testAckedCount() throws IOException {
             assertThat(b.getElements().get(0), is(element2));
             assertThat(b.getElements().get(1), is(element3));
 
-            final LongVector seqs = new LongVector(1);
-            seqs.add(firstSeqNum);
-            q.ack(seqs);
+            q.ack(firstSeqNum, 1);
         }
 
         try(Queue q = new Queue(settings)) {
@@ -672,28 +676,25 @@ public void testAckedCount() throws IOException {
             b = q.nonBlockReadBatch(2);
             assertThat(b.getElements().size(), is(2));
 
-            final LongVector seqs = new LongVector(2);
-            seqs.add(secondSeqNum);
-            seqs.add(thirdSeqNum);
-            q.ack(seqs);
+            q.ack(secondSeqNum, 2);
 
             assertThat(q.getAckedCount(), equalTo(0L));
             assertThat(q.getUnackedCount(), equalTo(0L));
         }
     }
 
-    @Test(timeout = 5000)
+    @Test(timeout = 50_000)
     public void concurrentWritesTest() throws IOException, InterruptedException, ExecutionException {
 
         final int WRITER_COUNT = 5;
 
         final ExecutorService executorService = Executors.newFixedThreadPool(WRITER_COUNT);
         // very small pages to maximize page creation
-        Settings settings = TestSettings.volatileQueueSettings(100);
-        try (TestQueue q = new TestQueue(settings)) {
+        Settings settings = TestSettings.persistedQueueSettings(100, dataPath);
+        try (Queue q = new Queue(settings)) {
             q.open();
 
-            int ELEMENT_COUNT = 10000;
+            int ELEMENT_COUNT = 1000;
             AtomicInteger element_num = new AtomicInteger(0);
 
             // we expect this next write call to block so let's wrap it in a Future
@@ -714,7 +715,7 @@ public void concurrentWritesTest() throws IOException, InterruptedException, Exe
             int read_count = 0;
 
             while (read_count < ELEMENT_COUNT * WRITER_COUNT) {
-                Batch b = q.readBatch(BATCH_SIZE);
+                Batch b = q.readBatch(BATCH_SIZE, TimeUnit.SECONDS.toMillis(1));
                 read_count += b.size();
                 b.close();
             }
@@ -724,7 +725,7 @@ public void concurrentWritesTest() throws IOException, InterruptedException, Exe
                 assertThat(result, is(ELEMENT_COUNT));
             }
 
-            assertThat(q.getTailPages().isEmpty(), is(true));
+            assertThat(q.tailPages.isEmpty(), is(true));
             assertThat(q.isFullyAcked(), is(true));
         } finally {
             executorService.shutdownNow();
@@ -735,9 +736,8 @@ public void concurrentWritesTest() throws IOException, InterruptedException, Exe
     @Test
     public void fullyAckedHeadPageBeheadingTest() throws IOException {
         Queueable element = new StringElement("foobarbaz1");
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-        try (TestQueue q = new TestQueue(
-            TestSettings.volatileQueueSettings(2 * singleElementCapacity))) {
+        try (Queue q = new Queue(
+            TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element, 2), dataPath))) {
             q.open();
 
             Batch b;
@@ -752,8 +752,8 @@ public void fullyAckedHeadPageBeheadingTest() throws IOException {
             b.close();
 
             // head page should be full and fully acked
-            assertThat(q.getHeadPage().isFullyAcked(), is(true));
-            assertThat(q.getHeadPage().hasSpace(element.serialize().length), is(false));
+            assertThat(q.headPage.isFullyAcked(), is(true));
+            assertThat(q.headPage.hasSpace(element.serialize().length), is(false));
             assertThat(q.isFullyAcked(), is(true));
 
             // write extra element to trigger beheading
@@ -761,13 +761,27 @@ public void fullyAckedHeadPageBeheadingTest() throws IOException {
 
             // since head page was fully acked it should not have created a new tail page
 
-            assertThat(q.getTailPages().isEmpty(), is(true));
-            assertThat(q.getHeadPage().getPageNum(), is(1));
+            assertThat(q.tailPages.isEmpty(), is(true));
+            assertThat(q.headPage.getPageNum(), is(1));
             assertThat(q.firstUnackedPageNum(), is(1));
             assertThat(q.isFullyAcked(), is(false));
         }
     }
 
+    @Test
+    public void getsPersistedByteSizeCorrectly() throws Exception {
+        Settings settings = TestSettings.persistedQueueSettings(100, dataPath);
+        try (Queue queue = new Queue(settings)) {
+            queue.open();
+            long seqNum = 0;
+            for (int i = 0; i < 50; ++i) {
+                seqNum = queue.write(new StringElement("foooo"));
+            }
+            queue.ensurePersistedUpto(seqNum);
+            assertThat(queue.getPersistedByteSize(), is(1063L));
+        }
+    }
+
     @Test
     public void getsPersistedByteSizeCorrectlyForUnopened() throws Exception {
         Settings settings = TestSettings.persistedQueueSettings(100, dataPath);
@@ -779,20 +793,19 @@ public void getsPersistedByteSizeCorrectlyForUnopened() throws Exception {
     @Test
     public void getsPersistedByteSizeCorrectlyForFullyAckedDeletedTailPages() throws Exception {
         final Queueable element = new StringElement("0123456789"); // 10 bytes
-        final int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-        final Settings settings = TestSettings.persistedQueueSettings(singleElementCapacity, dataPath);
+        final Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element), dataPath);
 
         try (Queue q = new Queue(settings)) {
             q.open();
 
             q.write(element);
-            Batch b1 = q.readBatch(2);
+            Batch b1 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
             q.write(element);
-            Batch b2 = q.readBatch(2);
+            Batch b2 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
             q.write(element);
-            Batch b3 = q.readBatch(2);
+            Batch b3 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
             q.write(element);
-            Batch b4 = q.readBatch(2);
+            Batch b4 = q.readBatch(2, TimeUnit.SECONDS.toMillis(1));
 
             assertThat(q.tailPages.size(), is(3));
             assertThat(q.getPersistedByteSize() > 0, is(true));
@@ -814,21 +827,26 @@ public void getsPersistedByteSizeCorrectlyForFullyAckedDeletedTailPages() throws
 
     private void stableUnderStress(final int capacity) throws IOException {
         Settings settings = TestSettings.persistedQueueSettings(capacity, dataPath);
-        final ExecutorService exec = Executors.newScheduledThreadPool(2);
+        final int concurrent = 2;
+        final ExecutorService exec = Executors.newScheduledThreadPool(concurrent);
+        final int count = 20_000;
         try (Queue queue = new Queue(settings)) {
-            final int count = 20_000;
-            final int concurrent = 2;
             queue.open();
-            final Future<Integer>[] futures = new Future[concurrent];
+            final List<Future<Integer>> futures = new ArrayList<>(concurrent);
+            final AtomicInteger counter = new AtomicInteger(0);
             for (int c = 0; c < concurrent; ++c) {
-                futures[c] = exec.submit(() -> {
+                futures.add(exec.submit(() -> {
                     int i = 0;
                     try {
-                        while (i < count / concurrent) {
-                            final Batch batch = queue.readBatch(1);
-                            for (final Queueable elem : batch.getElements()) {
-                                if (elem != null) {
-                                    ++i;
+                        while (counter.get() < count) {
+                            try (final Batch batch = queue.readBatch(
+                                50, TimeUnit.SECONDS.toMillis(1L))
+                            ) {
+                                for (final Queueable elem : batch.getElements()) {
+                                    if (elem != null) {
+                                        counter.incrementAndGet();
+                                        ++i;
+                                    }
                                 }
                             }
                         }
@@ -836,39 +854,40 @@ private void stableUnderStress(final int capacity) throws IOException {
                     } catch (final IOException ex) {
                         throw new IllegalStateException(ex);
                     }
-                });
+                }));
             }
+            final Queueable evnt = new StringElement("foo");
             for (int i = 0; i < count; ++i) {
                 try {
-                    final Queueable evnt = new StringElement("foo");
                     queue.write(evnt);
                 } catch (final IOException ex) {
                     throw new IllegalStateException(ex);
                 }
             }
             assertThat(
-                Arrays.stream(futures).map(i -> {
+                futures.stream().map(i -> {
                     try {
-                        return i.get(2L, TimeUnit.MINUTES);
+                        return i.get(5L, TimeUnit.MINUTES);
                     } catch (final InterruptedException | ExecutionException | TimeoutException ex) {
                         throw new IllegalStateException(ex);
                     }
                 }).reduce((x, y) -> x + y).orElse(0),
-                is(20_000)
+                is(count)
             );
+            assertThat(queue.isFullyAcked(), is(true));
         }
     }
 
     @Test
     public void inEmpty() throws IOException {
-        try(Queue q = new Queue(TestSettings.volatileQueueSettings(1000))) {
+        try(Queue q = new Queue(TestSettings.persistedQueueSettings(1000, dataPath))) {
             q.open();
             assertThat(q.isEmpty(), is(true));
 
             q.write(new StringElement("foobarbaz"));
             assertThat(q.isEmpty(), is(false));
 
-            Batch b = q.readBatch(1);
+            Batch b = q.readBatch(1, TimeUnit.SECONDS.toMillis(1));
             assertThat(q.isEmpty(), is(false));
 
             b.close();
@@ -879,30 +898,29 @@ public void inEmpty() throws IOException {
     @Test
     public void testZeroByteFullyAckedPageOnOpen() throws IOException {
         Queueable element = new StringElement("0123456789"); // 10 bytes
-        int singleElementCapacity = singleElementCapacityForByteBufferPageIO(element);
-        Settings settings = TestSettings.persistedQueueSettings(singleElementCapacity, dataPath);
+        Settings settings = TestSettings.persistedQueueSettings(computeCapacityForMmapPageIO(element), dataPath);
 
         // the goal here is to recreate a condition where the queue has a tail page of size zero with
         // a checkpoint that indicates it is full acknowledged
         // see issue #7809
 
-        try(TestQueue q = new TestQueue(settings)) {
+        try(Queue q = new Queue(settings)) {
             q.open();
 
             Queueable element1 = new StringElement("0123456789");
             Queueable element2 = new StringElement("9876543210");
 
             // write 2 elements to force a new page.
-            q.write(element1);
+            final long firstSeq = q.write(element1);
             q.write(element2);
-            assertThat(q.getTailPages().size(), is(1));
+            assertThat(q.tailPages.size(), is(1));
 
             // work directly on the tail page and not the queue to avoid habing the queue purge the page
             // but make sure the tail page checkpoint marks it as fully acked
-            TailPage tp = q.getTailPages().get(0);
-            Batch b = tp.readBatch(1);
+            Page tp = q.tailPages.get(0);
+            Batch b = new Batch(tp.read(1), q);
             assertThat(b.getElements().get(0), is(element1));
-            tp.ack(b.getSeqNums(), 1);
+            tp.ack(firstSeq, 1, 1);
             assertThat(tp.isFullyAcked(), is(true));
 
         }
@@ -914,17 +932,91 @@ public void testZeroByteFullyAckedPageOnOpen() throws IOException {
         c.truncate(0);
         c.close();
 
-        try(TestQueue q = new TestQueue(settings)) {
+        try(Queue q = new Queue(settings)) {
             // here q.open used to crash with:
             // java.io.IOException: Page file size 0 different to configured page capacity 27 for ...
             // because page.0 ended up as a zero byte file but its checkpoint says it's fully acked
             q.open();
             assertThat(q.getUnackedCount(), is(1L));
-            assertThat(q.getTailPages().size(), is(1));
-            assertThat(q.getTailPages().get(0).isFullyAcked(), is(false));
-            assertThat(q.getTailPages().get(0).elementCount, is(1));
-            assertThat(q.getHeadPage().elementCount, is(0));
+            assertThat(q.tailPages.size(), is(1));
+            assertThat(q.tailPages.get(0).isFullyAcked(), is(false));
+            assertThat(q.tailPages.get(0).elementCount, is(1));
+            assertThat(q.headPage.elementCount, is(0));
         }
     }
 
+    @Test
+    public void pageCapacityChangeOnExistingQueue() throws IOException {
+        final Queueable element = new StringElement("foobarbaz1");
+        final int ORIGINAL_CAPACITY = computeCapacityForMmapPageIO(element, 2);
+        final int NEW_CAPACITY = computeCapacityForMmapPageIO(element, 10);
+
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(ORIGINAL_CAPACITY, dataPath))) {
+            q.open();
+            q.write(element);
+        }
+
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(NEW_CAPACITY, dataPath))) {
+            q.open();
+            assertThat(q.tailPages.get(0).getPageIO().getCapacity(), is(ORIGINAL_CAPACITY));
+            assertThat(q.headPage.getPageIO().getCapacity(), is(NEW_CAPACITY));
+            q.write(element);
+        }
+
+        try (Queue q = new Queue(TestSettings.persistedQueueSettings(NEW_CAPACITY, dataPath))) {
+            q.open();
+            assertThat(q.tailPages.get(0).getPageIO().getCapacity(), is(ORIGINAL_CAPACITY));
+            assertThat(q.tailPages.get(1).getPageIO().getCapacity(), is(NEW_CAPACITY));
+            assertThat(q.headPage.getPageIO().getCapacity(), is(NEW_CAPACITY));
+
+            // will read only within a page boundary
+            Batch b1 = q.readBatch( 10, TimeUnit.SECONDS.toMillis(1));
+            assertThat(b1.size(), is(1));
+            b1.close();
+
+            // will read only within a page boundary
+            Batch b2 = q.readBatch( 10, TimeUnit.SECONDS.toMillis(1));
+            assertThat(b2.size(), is(1));
+            b2.close();
+
+            assertThat(q.tailPages.size(), is(0));
+        }
+    }
+
+
+    @Test(timeout = 5000)
+    public void maximizeBatch() throws IOException {
+
+        // very small pages to maximize page creation
+        Settings settings = TestSettings.persistedQueueSettings(1000, dataPath);
+        try (Queue q = new Queue(settings)) {
+            q.open();
+
+            Callable<Void> writer = () -> {
+                Thread.sleep(500); // sleep 500 ms
+                q.write(new StringElement("E2"));
+                return null;
+            };
+
+            // write one element now and schedule the 2nd write in 500ms
+            q.write(new StringElement("E1"));
+            executor.submit(writer);
+
+            // issue a batch read with a 1s timeout, the batch should contain both elements since
+            // the timeout is greater than the 2nd write delay
+            Batch b = q.readBatch(10, TimeUnit.SECONDS.toMillis(1));
+
+            assertThat(b.size(), is(2));
+        }
+    }
+
+    @Test(expected = IOException.class)
+    public void throwsWhenNotEnoughDiskFree() throws Exception {
+        Settings settings = SettingsImpl.builder(TestSettings.persistedQueueSettings(100, dataPath))
+            .queueMaxBytes(Long.MAX_VALUE)
+            .build();
+        try (Queue queue = new Queue(settings)) {
+            queue.open();
+        }
+    }
 }
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTestHelpers.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTestHelpers.java
index 252291c175b..dadb7811f0e 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTestHelpers.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTestHelpers.java
@@ -1,8 +1,7 @@
 package org.logstash.ackedqueue;
 
-import org.logstash.ackedqueue.io.ByteBufferPageIO;
-
 import java.io.IOException;
+import org.logstash.ackedqueue.io.MmapPageIO;
 
 /**
  * Class containing common methods to help DRY up acked queue tests.
@@ -10,18 +9,22 @@
 public class QueueTestHelpers {
 
     /**
-     * Returns the minimum capacity required for {@link ByteBufferPageIO}
-     * @return int - minimum capacity required
+     * Returns the capacity required for the supplied element
+     * @param element
+     * @return int - capacity required for the supplied element
+     * @throws IOException Throws if a serialization error occurs
      */
-    public static final int BYTE_BUF_PAGEIO_MIN_CAPACITY = ByteBufferPageIO.WRAPPER_SIZE;
+    public static int computeCapacityForMmapPageIO(final Queueable element) throws IOException {
+        return computeCapacityForMmapPageIO(element, 1);
+    }
 
     /**
-     * Returns the {@link ByteBufferPageIO} capacity required for the supplied element
+     * Returns the {@link MmapPageIO} capacity require to hold a multiple elements including all headers and other metadata.
      * @param element
-     * @return int - capacity required for the supplied element
+     * @return int - capacity required for the supplied number of elements
      * @throws IOException Throws if a serialization error occurs
      */
-    public static int singleElementCapacityForByteBufferPageIO(final Queueable element) throws IOException {
-        return ByteBufferPageIO.WRAPPER_SIZE + element.serialize().length;
+    public static int computeCapacityForMmapPageIO(final Queueable element, int count) throws IOException {
+        return MmapPageIO.HEADER_SIZE + (count * (MmapPageIO.SEQNUM_SIZE + MmapPageIO.LENGTH_SIZE + element.serialize().length + MmapPageIO.CHECKSUM_SIZE));
     }
 }
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java b/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java
deleted file mode 100644
index 16d53ef7b00..00000000000
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/TestQueue.java
+++ /dev/null
@@ -1,17 +0,0 @@
-package org.logstash.ackedqueue;
-
-import java.util.List;
-
-public class TestQueue extends Queue {
-    public TestQueue(Settings settings) {
-        super(settings);
-    }
-
-    public HeadPage getHeadPage() {
-        return this.headPage;
-    }
-
-    public List<TailPage> getTailPages() {
-        return this.tailPages;
-    }
-}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
index dac914b43dd..c2fde02f725 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
@@ -1,36 +1,14 @@
 package org.logstash.ackedqueue;
 
-import org.logstash.ackedqueue.io.ByteBufferPageIO;
-import org.logstash.ackedqueue.io.CheckpointIOFactory;
-import org.logstash.ackedqueue.io.FileCheckpointIO;
-import org.logstash.ackedqueue.io.MemoryCheckpointIO;
-import org.logstash.ackedqueue.io.MmapPageIO;
-import org.logstash.ackedqueue.io.PageIOFactory;
-
 public class TestSettings {
 
-    public static Settings volatileQueueSettings(int capacity) {
-        MemoryCheckpointIO.clearSources();
-        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
-        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
-        return SettingsImpl.memorySettingsBuilder().capacity(capacity).elementIOFactory(pageIOFactory)
-            .checkpointIOFactory(checkpointIOFactory).elementClass(StringElement.class).build();
-    }
-
-    public static Settings volatileQueueSettings(int capacity, long size) {
-        MemoryCheckpointIO.clearSources();
-        PageIOFactory pageIOFactory = (pageNum, pageSize, path) -> new ByteBufferPageIO(pageNum, pageSize, path);
-        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
-        return SettingsImpl.memorySettingsBuilder().capacity(capacity).queueMaxBytes(size)
-            .elementIOFactory(pageIOFactory).checkpointIOFactory(checkpointIOFactory)
-            .elementClass(StringElement.class).build();
+    public static Settings persistedQueueSettings(int capacity, String folder) {
+        return SettingsImpl.fileSettingsBuilder(folder).capacity(capacity)
+            .checkpointMaxWrites(1).elementClass(StringElement.class).build();
     }
 
-    public static Settings persistedQueueSettings(int capacity, String folder) {
-        PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
-        CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
-        return SettingsImpl.fileSettingsBuilder(folder).capacity(capacity).elementIOFactory(pageIOFactory)
-            .checkpointMaxWrites(1).checkpointIOFactory(checkpointIOFactory)
-            .elementClass(StringElement.class).build();
+    public static Settings persistedQueueSettings(int capacity, long size, String folder) {
+        return SettingsImpl.fileSettingsBuilder(folder).capacity(capacity)
+            .queueMaxBytes(size).elementClass(StringElement.class).build();
     }
 }
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/ByteBufferPageIOTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/ByteBufferPageIOTest.java
deleted file mode 100644
index cf2eefe6ce5..00000000000
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/io/ByteBufferPageIOTest.java
+++ /dev/null
@@ -1,381 +0,0 @@
-package org.logstash.ackedqueue.io;
-
-import org.junit.Test;
-import org.junit.runner.RunWith;
-import org.junit.runners.Parameterized;
-import org.junit.runners.Parameterized.Parameter;
-import org.junit.runners.Parameterized.Parameters;
-import org.logstash.ackedqueue.Queueable;
-import org.logstash.ackedqueue.SequencedList;
-import org.logstash.ackedqueue.StringElement;
-
-import java.io.IOException;
-import java.util.Arrays;
-import java.util.Collection;
-import java.util.List;
-import java.util.concurrent.Callable;
-import java.util.stream.Collectors;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.MatcherAssert.assertThat;
-import static org.logstash.ackedqueue.QueueTestHelpers.BYTE_BUF_PAGEIO_MIN_CAPACITY;
-import static org.logstash.ackedqueue.QueueTestHelpers.singleElementCapacityForByteBufferPageIO;
-
-public class ByteBufferPageIOTest {
-
-    // convert any checked exceptions into uncheck RuntimeException
-    public static <V> V uncheck(Callable<V> callable) {
-        try {
-            return callable.call();
-        } catch (RuntimeException e) {
-            throw e;
-        } catch (Exception e) {
-            throw new RuntimeException(e);
-        }
-    }
-
-    private interface BufferGenerator {
-        byte[] generate() throws IOException;
-    }
-
-    private static int CAPACITY = 1024;
-
-    private static ByteBufferPageIO newEmptyPageIO() throws IOException {
-        return newEmptyPageIO(CAPACITY);
-    }
-
-    private static ByteBufferPageIO newEmptyPageIO(int capacity) throws IOException {
-        ByteBufferPageIO io = new ByteBufferPageIO(capacity);
-        io.create();
-        return io;
-    }
-
-    private static ByteBufferPageIO newPageIO(int capacity, byte[] bytes) throws IOException {
-        return new ByteBufferPageIO(capacity, bytes);
-    }
-
-    private Queueable buildStringElement(String str) {
-        return new StringElement(str);
-    }
-
-    @Test
-    public void getWritePosition() throws IOException {
-        assertThat(newEmptyPageIO().getWritePosition(), is(equalTo(1)));
-    }
-
-    @Test
-    public void getElementCount() throws IOException {
-        assertThat(newEmptyPageIO().getElementCount(), is(equalTo(0)));
-    }
-
-    @Test
-    public void getStartSeqNum() throws IOException {
-        assertThat(newEmptyPageIO().getMinSeqNum(), is(equalTo(0L)));
-    }
-
-    @Test
-    public void hasSpace() throws IOException {
-        assertThat(newEmptyPageIO(BYTE_BUF_PAGEIO_MIN_CAPACITY).hasSpace(0), is(true));
-        assertThat(newEmptyPageIO(BYTE_BUF_PAGEIO_MIN_CAPACITY).hasSpace(1), is(false));
-    }
-
-    @Test
-    public void hasSpaceAfterWrite() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        long seqNum = 1L;
-
-        ByteBufferPageIO io = newEmptyPageIO(singleElementCapacityForByteBufferPageIO(element));
-
-        assertThat(io.hasSpace(element.serialize().length), is(true));
-        io.write(element.serialize(), seqNum);
-        assertThat(io.hasSpace(element.serialize().length), is(false));
-        assertThat(io.hasSpace(1), is(false));
-    }
-
-    @Test
-    public void write() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        long seqNum = 42L;
-        ByteBufferPageIO io = newEmptyPageIO();
-        io.write(element.serialize(), seqNum);
-        assertThat(io.getWritePosition(), is(equalTo(singleElementCapacityForByteBufferPageIO(element))));
-        assertThat(io.getElementCount(), is(equalTo(1)));
-        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
-    }
-
-    @Test
-    public void openValidState() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        long seqNum = 42L;
-        ByteBufferPageIO io = newEmptyPageIO();
-        io.write(element.serialize(), seqNum);
-
-        byte[] initialState = io.dump();
-        io = newPageIO(initialState.length, initialState);
-        io.open(seqNum, 1);
-        assertThat(io.getElementCount(), is(equalTo(1)));
-        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
-    }
-
-    @Test
-    public void recoversValidState() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        long seqNum = 42L;
-        ByteBufferPageIO io = newEmptyPageIO();
-        io.write(element.serialize(), seqNum);
-
-        byte[] initialState = io.dump();
-        io = newPageIO(initialState.length, initialState);
-        io.recover();
-        assertThat(io.getElementCount(), is(equalTo(1)));
-        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
-    }
-
-    @Test
-    public void recoverEmptyWriteRecover() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        long seqNum = 42L;
-        ByteBufferPageIO io = newEmptyPageIO();
-        byte[] initialState = io.dump();
-
-        io = newPageIO(initialState.length, initialState);
-        io.recover();
-        assertThat(io.getElementCount(), is(equalTo(0)));
-
-        io.write(element.serialize(), seqNum);
-        initialState = io.dump();
-
-        io = newPageIO(initialState.length, initialState);
-        io.recover();
-        assertThat(io.getElementCount(), is(equalTo(1)));
-        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
-    }
-
-    @Test
-    public void recoverNonEmptyWriteRecover() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-
-        ByteBufferPageIO io = newEmptyPageIO();
-        io.write(element.serialize(), 1L);
-        byte[] initialState = io.dump();
-
-        io = newPageIO(initialState.length, initialState);
-        io.recover();
-        assertThat(io.getElementCount(), is(equalTo(1)));
-
-        io.write(element.serialize(), 2L);
-        initialState = io.dump();
-
-        io = newPageIO(initialState.length, initialState);
-        io.recover();
-        assertThat(io.getElementCount(), is(equalTo(2)));
-    }
-
-    @Test(expected = IOException.class)
-    public void openUnexpectedSeqNum() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        long seqNum = 42L;
-        ByteBufferPageIO io = newEmptyPageIO();
-        io.write(element.serialize(), seqNum);
-
-        byte[] initialState = io.dump();
-        newPageIO(initialState.length, initialState);
-        io.open(1L, 1);
-    }
-
-    @RunWith(Parameterized.class)
-    public static class SingleInvalidElementTest {
-
-        private static final List<BufferGenerator> singleGenerators = Arrays.asList(
-            // invalid length
-            () -> {
-                Queueable element = new StringElement("foobarbaz");
-                ByteBufferPageIO io = newEmptyPageIO();
-                byte[] bytes = element.serialize();
-                io.write(bytes, 1L, 514, io.checksum(bytes));
-                return io.dump();
-            },
-
-            // invalid checksum
-            () -> {
-                Queueable element = new StringElement("foobarbaz");
-                ByteBufferPageIO io = newEmptyPageIO();
-                byte[] bytes = element.serialize();
-                io.write(bytes, 1L, bytes.length, 77);
-                return io.dump();
-            },
-
-            // invalid payload
-            () -> {
-                Queueable element = new StringElement("foobarbaz");
-                ByteBufferPageIO io = newEmptyPageIO();
-                byte[] bytes = element.serialize();
-                int checksum = io.checksum(bytes);
-                bytes[1] = 0x01;
-                io.write(bytes, 1L, bytes.length, checksum);
-                return io.dump();
-            }
-        );
-
-        @Parameters
-        public static Collection<byte[]> singleElementParameters() {
-            return singleGenerators.stream().map(g -> uncheck(g::generate)).collect(Collectors.toList());
-        }
-
-        @Parameter
-        public byte[] singleElementParameter;
-
-        @Test
-        public void openInvalidSingleElement() throws IOException {
-            // none of these should generate an exception with open()
-
-            ByteBufferPageIO io = newPageIO(singleElementParameter.length, singleElementParameter);
-            io.open(1L, 1);
-
-            assertThat(io.getElementCount(), is(equalTo(1)));
-            assertThat(io.getMinSeqNum(), is(equalTo(1L)));
-        }
-
-        @Test
-        public void recoverInvalidSingleElement() throws IOException {
-            for (BufferGenerator generator : singleGenerators) {
-                byte[] bytes = generator.generate();
-                ByteBufferPageIO io = newPageIO(bytes.length, bytes);
-                io.recover();
-
-                assertThat(io.getElementCount(), is(equalTo(0)));
-            }
-        }
-    }
-
-    @RunWith(Parameterized.class)
-    public static class DoubleInvalidElementTest {
-
-        private static final List<BufferGenerator> doubleGenerators = Arrays.asList(
-            // invalid length
-            () -> {
-                Queueable element = new StringElement("foobarbaz");
-                ByteBufferPageIO io = newEmptyPageIO();
-                byte[] bytes = element.serialize();
-                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
-                io.write(bytes, 2L, 514, io.checksum(bytes));
-                return io.dump();
-            },
-
-            // invalid checksum
-            () -> {
-                Queueable element = new StringElement("foobarbaz");
-                ByteBufferPageIO io = newEmptyPageIO();
-                byte[] bytes = element.serialize();
-                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
-                io.write(bytes, 2L, bytes.length, 77);
-                return io.dump();
-            },
-
-            // invalid payload
-            () -> {
-                Queueable element = new StringElement("foobarbaz");
-                ByteBufferPageIO io = newEmptyPageIO();
-                byte[] bytes = element.serialize();
-                int checksum = io.checksum(bytes);
-                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
-                bytes[1] = 0x01;
-                io.write(bytes, 2L, bytes.length, checksum);
-                return io.dump();
-            }
-        );
-
-        @Parameters
-        public static Collection<byte[]> doubleElementParameters() {
-            return doubleGenerators.stream().map(g -> uncheck(g::generate)).collect(Collectors.toList());
-        }
-
-        @Parameter
-        public byte[] doubleElementParameter;
-
-        @Test
-        public void openInvalidDoubleElement() throws IOException {
-            // none of these will generate an exception with open()
-
-            ByteBufferPageIO io = newPageIO(doubleElementParameter.length, doubleElementParameter);
-            io.open(1L, 2);
-
-            assertThat(io.getElementCount(), is(equalTo(2)));
-            assertThat(io.getMinSeqNum(), is(equalTo(1L)));
-        }
-
-        @Test
-        public void recoverInvalidDoubleElement() throws IOException {
-            ByteBufferPageIO io = newPageIO(doubleElementParameter.length, doubleElementParameter);
-            io.recover();
-
-            assertThat(io.getElementCount(), is(equalTo(1)));
-         }
-    }
-
-    @Test(expected = AbstractByteBufferPageIO.PageIOInvalidElementException.class)
-    public void openInvalidDeqNumDoubleElement() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        ByteBufferPageIO io = newEmptyPageIO();
-        byte[] bytes = element.serialize();
-        io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
-        io.write(bytes, 3L, bytes.length, io.checksum(bytes));
-
-        io = newPageIO(io.dump().length, io.dump());
-        io.open(1L, 2);
-    }
-
-    @Test
-    public void recoverInvalidDeqNumDoubleElement() throws IOException {
-        Queueable element = new StringElement("foobarbaz");
-        ByteBufferPageIO io = newEmptyPageIO();
-        byte[] bytes = element.serialize();
-        io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
-        io.write(bytes, 3L, bytes.length, io.checksum(bytes));
-
-        io = newPageIO(io.dump().length, io.dump());
-        io.recover();
-
-        assertThat(io.getElementCount(), is(equalTo(1)));
-    }
-
-    @Test
-    public void writeRead() throws IOException {
-        long seqNum = 42L;
-        Queueable element = buildStringElement("foobarbaz");
-        ByteBufferPageIO io = newEmptyPageIO();
-        io.write(element.serialize(), seqNum);
-        SequencedList<byte[]> result = io.read(seqNum, 1);
-        assertThat(result.getElements().size(), is(equalTo(1)));
-        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
-        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
-        assertThat(readElement.toString(), is(equalTo(element.toString())));
-    }
-
-    @Test
-    public void writeReadMulti() throws IOException {
-        Queueable element1 = buildStringElement("foo");
-        Queueable element2 = buildStringElement("bar");
-        Queueable element3 = buildStringElement("baz");
-        Queueable element4 = buildStringElement("quux");
-        ByteBufferPageIO io = newEmptyPageIO();
-        io.write(element1.serialize(), 40L);
-        io.write(element2.serialize(), 41L);
-        io.write(element3.serialize(), 42L);
-        io.write(element4.serialize(), 43L);
-        int batchSize = 11;
-        SequencedList<byte[]> result = io.read(40L, batchSize);
-        assertThat(result.getElements().size(), is(equalTo(4)));
-
-        assertThat(result.getSeqNums().get(0), is(equalTo(40L)));
-        assertThat(result.getSeqNums().get(1), is(equalTo(41L)));
-        assertThat(result.getSeqNums().get(2), is(equalTo(42L)));
-        assertThat(result.getSeqNums().get(3), is(equalTo(43L)));
-
-        assertThat(StringElement.deserialize(result.getElements().get(0)).toString(), is(equalTo(element1.toString())));
-        assertThat(StringElement.deserialize(result.getElements().get(1)).toString(), is(equalTo(element2.toString())));
-        assertThat(StringElement.deserialize(result.getElements().get(2)).toString(), is(equalTo(element3.toString())));
-        assertThat(StringElement.deserialize(result.getElements().get(3)).toString(), is(equalTo(element4.toString())));
-    }
-}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileCheckpointIOTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileCheckpointIOTest.java
index a6a6455a7f4..af56301fdf8 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileCheckpointIOTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileCheckpointIOTest.java
@@ -5,8 +5,6 @@
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 import org.logstash.ackedqueue.Checkpoint;
-import org.logstash.ackedqueue.io.CheckpointIO;
-import org.logstash.ackedqueue.io.FileCheckpointIO;
 
 import java.net.URL;
 import java.nio.file.Files;
@@ -18,7 +16,7 @@
 import static org.hamcrest.MatcherAssert.assertThat;
 
 public class FileCheckpointIOTest {
-    private String checkpointFolder;
+    private Path checkpointFolder;
     private CheckpointIO io;
 
     @Rule
@@ -27,16 +25,15 @@ public class FileCheckpointIOTest {
     @Before
     public void setUp() throws Exception {
         checkpointFolder = temporaryFolder
-                .newFolder("checkpoints")
-                .getPath();
+            .newFolder("checkpoints")
+            .toPath();
         io = new FileCheckpointIO(checkpointFolder);
     }
 
     @Test
     public void read() throws Exception {
         URL url = this.getClass().getResource("checkpoint.head");
-        String dirPath = Paths.get(url.toURI()).getParent().toString();
-        io = new FileCheckpointIO(dirPath);
+        io = new FileCheckpointIO(Paths.get(url.toURI()).getParent());
         Checkpoint chk = io.read("checkpoint.head");
         assertThat(chk.getMinSeqNum(), is(8L));
     }
@@ -45,11 +42,11 @@ public void read() throws Exception {
     public void write() throws Exception {
         io.write("checkpoint.head", 6, 2, 10L, 8L, 200);
         io.write("checkpoint.head", 6, 2, 10L, 8L, 200);
-        Path fullFileName = Paths.get(checkpointFolder, "checkpoint.head");
+        Path fullFileName = checkpointFolder.resolve("checkpoint.head");
         byte[] contents = Files.readAllBytes(fullFileName);
         URL url = this.getClass().getResource("checkpoint.head");
         Path path = Paths.get(url.toURI());
         byte[] compare = Files.readAllBytes(path);
         assertThat(contents, is(equalTo(compare)));
     }
-}
\ No newline at end of file
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
index fbc7db370f1..d8e6d61c197 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/io/FileMmapIOTest.java
@@ -1,12 +1,12 @@
 package org.logstash.ackedqueue.io;
 
+import java.nio.file.Path;
 import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;
 import org.junit.rules.TemporaryFolder;
 import org.logstash.ackedqueue.SequencedList;
 import org.logstash.ackedqueue.StringElement;
-import org.logstash.ackedqueue.io.MmapPageIO;
 
 import java.util.ArrayList;
 import java.util.List;
@@ -16,7 +16,7 @@
 import static org.hamcrest.MatcherAssert.assertThat;
 
 public class FileMmapIOTest {
-    private String folder;
+    private Path folder;
     private MmapPageIO writeIo;
     private MmapPageIO readIo;
     private int pageNum;
@@ -29,7 +29,7 @@ public void setUp() throws Exception {
         pageNum = 0;
         folder = temporaryFolder
                 .newFolder("pages")
-                .getPath();
+                .toPath();
         writeIo = new MmapPageIO(pageNum, 1024, folder);
         readIo = new MmapPageIO(pageNum, 1024, folder);
     }
@@ -53,4 +53,4 @@ public void roundTrip() throws Exception {
         }
         assertThat(readList, is(equalTo(list)));
     }
-}
\ No newline at end of file
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/MemoryCheckpointTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/MemoryCheckpointTest.java
deleted file mode 100644
index 4acf329d9ff..00000000000
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/io/MemoryCheckpointTest.java
+++ /dev/null
@@ -1,86 +0,0 @@
-package org.logstash.ackedqueue.io;
-
-import java.io.IOException;
-import java.nio.file.NoSuchFileException;
-import org.junit.Before;
-import org.junit.Test;
-import org.logstash.ackedqueue.Checkpoint;
-import org.logstash.ackedqueue.SettingsImpl;
-import org.logstash.ackedqueue.Settings;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.MatcherAssert.assertThat;
-
-public class MemoryCheckpointTest {
-
-    private CheckpointIO io;
-
-    @Before
-    public void setUp() {
-        CheckpointIOFactory factory = (dirPath) -> new MemoryCheckpointIO(dirPath);
-        Settings settings = 
-            SettingsImpl.memorySettingsBuilder().checkpointIOFactory(factory).build();
-        this.io = settings.getCheckpointIOFactory().build(settings.getDirPath());
-    }
-
-    @Test
-    public void writeNewReadExisting() throws IOException {
-        io.write("checkpoint.head", 1, 2, 3, 4, 5);
-
-        Checkpoint checkpoint = io.read("checkpoint.head");
-
-        assertThat(checkpoint.getPageNum(), is(equalTo(1)));
-        assertThat(checkpoint.getFirstUnackedPageNum(), is(equalTo(2)));
-        assertThat(checkpoint.getFirstUnackedSeqNum(), is(equalTo(3L)));
-        assertThat(checkpoint.getMinSeqNum(), is(equalTo(4L)));
-        assertThat(checkpoint.getElementCount(), is(equalTo(5)));
-    }
-
-    @Test(expected = NoSuchFileException.class)
-    public void readNonexistent() throws IOException {
-        io.read("checkpoint.invalid");
-    }
-
-    @Test
-    public void readWriteDirPathNamespaced() throws IOException {
-        CheckpointIO io1 = new MemoryCheckpointIO("path1");
-        CheckpointIO io2 = new MemoryCheckpointIO("path2");
-        io1.write("checkpoint.head", 1, 0, 0, 0, 0);
-        io2.write("checkpoint.head", 2, 0, 0, 0, 0);
-
-        Checkpoint checkpoint;
-
-        checkpoint = io1.read("checkpoint.head");
-        assertThat(checkpoint.getPageNum(), is(equalTo(1)));
-
-        checkpoint = io2.read("checkpoint.head");
-        assertThat(checkpoint.getPageNum(), is(equalTo(2)));
-    }
-
-    @Test(expected = NoSuchFileException.class)
-    public void purgeDirPathNamespaced1() throws IOException {
-        CheckpointIO io1 = new MemoryCheckpointIO("path1");
-        CheckpointIO io2 = new MemoryCheckpointIO("path2");
-        io1.write("checkpoint.head", 1, 0, 0, 0, 0);
-        io2.write("checkpoint.head", 2, 0, 0, 0, 0);
-
-        io1.purge("checkpoint.head");
-
-        Checkpoint checkpoint = io1.read("checkpoint.head");
-    }
-
-    @Test
-    public void purgeDirPathNamespaced2() throws IOException {
-        CheckpointIO io1 = new MemoryCheckpointIO("path1");
-        CheckpointIO io2 = new MemoryCheckpointIO("path2");
-        io1.write("checkpoint.head", 1, 0, 0, 0, 0);
-        io2.write("checkpoint.head", 2, 0, 0, 0, 0);
-
-        io1.purge("checkpoint.head");
-
-        Checkpoint checkpoint;
-        checkpoint = io2.read("checkpoint.head");
-        assertThat(checkpoint.getPageNum(), is(equalTo(2)));
-    }
-}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStreamTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStreamTest.java
deleted file mode 100644
index 7b3a63b6a65..00000000000
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/io/wip/MemoryPageIOStreamTest.java
+++ /dev/null
@@ -1,188 +0,0 @@
-package org.logstash.ackedqueue.io.wip;
-
-import org.junit.Test;
-import org.logstash.ackedqueue.Queueable;
-import org.logstash.ackedqueue.SequencedList;
-import org.logstash.ackedqueue.StringElement;
-import org.logstash.ackedqueue.io.wip.MemoryPageIOStream;
-
-import java.io.IOException;
-import java.nio.ByteBuffer;
-
-import static org.hamcrest.CoreMatchers.equalTo;
-import static org.hamcrest.CoreMatchers.is;
-import static org.hamcrest.MatcherAssert.assertThat;
-
-public class MemoryPageIOStreamTest {
-
-    private final int CAPACITY = 1024;
-    private final int EMPTY_HEADER_SIZE = Integer.BYTES + Integer.BYTES;
-
-    private byte[] empty_page_with_header() {
-        byte[] result = new byte[CAPACITY];
-        // version = 1, details = ABC
-        ByteBuffer.wrap(result).put(new byte[]{0, 0, 0, 1, 0, 0, 0, 3, 65, 66, 67});
-        return result;
-    }
-
-    private MemoryPageIOStream subject() throws IOException {
-        return subject(CAPACITY);
-    }
-
-    private MemoryPageIOStream subject(int size) throws IOException {
-        MemoryPageIOStream io = new MemoryPageIOStream(size);
-        io.create();
-        return io;
-    }
-
-    private MemoryPageIOStream subject(byte[] bytes, long seqNum, int count) throws IOException {
-        MemoryPageIOStream io = new MemoryPageIOStream(bytes.length, bytes);
-        io.open(seqNum, count);
-        return io;
-    }
-
-    private Queueable buildStringElement(String str) {
-        return new StringElement(str);
-    }
-
-    @Test
-    public void getWritePosition() throws Exception {
-        assertThat(subject().getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE)));
-        assertThat(subject(empty_page_with_header(), 1L, 0).getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE + 3)));
-    }
-
-    @Test
-    public void getElementCount() throws Exception {
-        assertThat(subject().getElementCount(), is(equalTo(0)));
-        assertThat(subject(empty_page_with_header(), 1L, 0).getElementCount(), is(equalTo(0)));
-    }
-
-    @Test
-    public void getStartSeqNum() throws Exception {
-        assertThat(subject().getMinSeqNum(), is(equalTo(1L)));
-        assertThat(subject(empty_page_with_header(), 1L, 0).getMinSeqNum(), is(equalTo(1L)));
-    }
-
-    @Test
-    public void readHeaderDetails() throws Exception {
-        MemoryPageIOStream io = new MemoryPageIOStream(CAPACITY);
-        io.setPageHeaderDetails("ABC");
-        io.create();
-        assertThat(io.readHeaderDetails(), is(equalTo("ABC")));
-        assertThat(io.getWritePosition(), is(equalTo(EMPTY_HEADER_SIZE + 3)));
-    }
-
-    @Test
-    public void hasSpace() throws Exception {
-        assertThat(subject().hasSpace(10), is(true));
-    }
-
-    @Test
-    public void write() throws Exception {
-        long seqNum = 42L;
-        Queueable element = new StringElement("foobarbaz");
-        MemoryPageIOStream subj = subject();
-        subj.write(element.serialize(), seqNum);
-        assertThat(subj.getElementCount(), is(equalTo(1)));
-        assertThat(subj.getMinSeqNum(), is(equalTo(seqNum)));
-    }
-
-    @Test
-    public void writeUntilFull() throws Exception {
-        long seqNum = 42L;
-        Queueable element = new StringElement("foobarbaz");
-        byte[] data = element.serialize();
-        int bufferSize = 120;
-        MemoryPageIOStream subj = subject(bufferSize);
-        while (subj.hasSpace(data.length)) {
-            subj.write(data, seqNum);
-            seqNum++;
-        }
-        int recordSize = subj.persistedByteCount(data.length);
-        int remains = bufferSize - subj.getWritePosition();
-        assertThat(recordSize, is(equalTo(25))); // element=9 + seqnum=8 + length=4 + crc=4
-        assertThat(subj.getElementCount(), is(equalTo(4)));
-        boolean noSpaceLeft = remains < recordSize;
-        assertThat(noSpaceLeft, is(true));
-    }
-
-    @Test
-    public void read() throws Exception {
-        MemoryPageIOStream subj = subject();
-        SequencedList<byte[]> result = subj.read(1L, 1);
-        assertThat(result.getElements().isEmpty(), is(true));
-    }
-
-    @Test
-    public void writeRead() throws Exception {
-        long seqNum = 42L;
-        Queueable element = buildStringElement("foobarbaz");
-        MemoryPageIOStream subj = subject();
-        subj.write(element.serialize(), seqNum);
-        SequencedList<byte[]> result = subj.read(seqNum, 1);
-        assertThat(result.getElements().size(), is(equalTo(1)));
-        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
-        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
-        assertThat(readElement.toString(), is(equalTo(element.toString())));
-    }
-
-    @Test
-    public void writeReadEmptyElement() throws Exception {
-        long seqNum = 1L;
-        Queueable element = buildStringElement("");
-        MemoryPageIOStream subj = subject();
-        subj.write(element.serialize(), seqNum);
-        SequencedList<byte[]> result = subj.read(seqNum, 1);
-        assertThat(result.getElements().size(), is(equalTo(1)));
-        Queueable readElement = StringElement.deserialize(result.getElements().get(0));
-        assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
-        assertThat(readElement.toString(), is(equalTo(element.toString())));
-    }
-
-    @Test
-    public void writeReadMulti() throws Exception {
-        Queueable element1 = buildStringElement("foo");
-        Queueable element2 = buildStringElement("bar");
-        Queueable element3 = buildStringElement("baz");
-        Queueable element4 = buildStringElement("quux");
-        MemoryPageIOStream subj = subject();
-        subj.write(element1.serialize(), 40L);
-        subj.write(element2.serialize(), 42L);
-        subj.write(element3.serialize(), 44L);
-        subj.write(element4.serialize(), 46L);
-        int batchSize = 11;
-        SequencedList<byte[]> result = subj.read(40L, batchSize);
-        assertThat(result.getElements().size(), is(equalTo(4)));
-
-        assertThat(result.getSeqNums().get(0), is(equalTo(40L)));
-        assertThat(result.getSeqNums().get(1), is(equalTo(42L)));
-        assertThat(result.getSeqNums().get(2), is(equalTo(44L)));
-        assertThat(result.getSeqNums().get(3), is(equalTo(46L)));
-
-        assertThat(StringElement.deserialize(result.getElements().get(0)).toString(), is(equalTo(element1.toString())));
-        assertThat(StringElement.deserialize(result.getElements().get(1)).toString(), is(equalTo(element2.toString())));
-        assertThat(StringElement.deserialize(result.getElements().get(2)).toString(), is(equalTo(element3.toString())));
-        assertThat(StringElement.deserialize(result.getElements().get(3)).toString(), is(equalTo(element4.toString())));
-    }
-
-    @Test
-    public void readFromFirstUnackedSeqNum() throws Exception {
-        long seqNum = 10L;
-        String[] values = new String[]{"aaa", "bbb", "ccc", "ddd", "eee", "fff", "ggg", "hhh", "iii", "jjj"};
-        MemoryPageIOStream stream = subject(300);
-        for (String val : values) {
-            Queueable element = buildStringElement(val);
-            stream.write(element.serialize(), seqNum);
-            seqNum++;
-        }
-        MemoryPageIOStream subj = subject(stream.getBuffer(), 10L, 10);
-        int batchSize = 3;
-        seqNum = 13L;
-        SequencedList<byte[]> result = subj.read(seqNum, batchSize);
-        for (int i = 0; i < 3; i++) {
-            Queueable ele = StringElement.deserialize(result.getElements().get(i));
-            assertThat(result.getSeqNums().get(i), is(equalTo(seqNum + i)));
-            assertThat(ele.toString(), is(equalTo(values[i + 3])));
-        }
-    }
-}
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/stress/Concurrent.java b/logstash-core/src/test/java/org/logstash/stress/Concurrent.java
deleted file mode 100644
index 3fc97fbaedd..00000000000
--- a/logstash-core/src/test/java/org/logstash/stress/Concurrent.java
+++ /dev/null
@@ -1,185 +0,0 @@
-package org.logstash.stress;
-
-import java.io.IOException;
-import java.time.Duration;
-import java.time.Instant;
-import java.util.ArrayList;
-import java.util.Collection;
-import java.util.Collections;
-import java.util.List;
-import java.util.concurrent.ConcurrentLinkedQueue;
-import java.util.stream.Collectors;
-import org.logstash.ackedqueue.Batch;
-import org.logstash.ackedqueue.SettingsImpl;
-import org.logstash.ackedqueue.Queue;
-import org.logstash.ackedqueue.Settings;
-import org.logstash.ackedqueue.StringElement;
-import org.logstash.ackedqueue.io.ByteBufferPageIO;
-import org.logstash.ackedqueue.io.CheckpointIOFactory;
-import org.logstash.ackedqueue.io.FileCheckpointIO;
-import org.logstash.ackedqueue.io.MemoryCheckpointIO;
-import org.logstash.ackedqueue.io.MmapPageIO;
-import org.logstash.ackedqueue.io.PageIOFactory;
-
-public class Concurrent {
-    final static int ELEMENT_COUNT = 2000000;
-    final static int BATCH_SIZE = 1000;
-    static Settings settings;
-
-    public static Settings memorySettings(int capacity) {
-        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
-        CheckpointIOFactory checkpointIOFactory = (source) -> new MemoryCheckpointIO(source);
-        return SettingsImpl.memorySettingsBuilder().capacity(capacity).elementIOFactory(pageIOFactory)
-            .checkpointIOFactory(checkpointIOFactory).elementClass(StringElement.class).build();
-    }
-
-    public static Settings fileSettings(int capacity) {
-        PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
-        CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
-        return SettingsImpl.memorySettingsBuilder("/tmp/queue").capacity(capacity)
-            .elementIOFactory(pageIOFactory)
-            .checkpointIOFactory(checkpointIOFactory).elementClass(StringElement.class).build();
-    }
-
-    public static Thread producer(Queue q, List<StringElement> input) {
-        return new Thread(() -> {
-            try {
-                for (StringElement element : input) {
-                    q.write(element);
-                }
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        });
-
-    }
-
-    public static void oneProducersOneConsumer() throws IOException, InterruptedException {
-        List<StringElement> input = new ArrayList<>();
-        List<StringElement> output = new ArrayList<>();
-
-        Instant start = Instant.now();
-
-        Queue q = new Queue(settings);
-        q.getCheckpointIO().purge();
-        q.open();
-
-        System.out.print("stating single producers and single consumers stress test... ");
-
-        for (int i = 0; i < ELEMENT_COUNT; i++) {
-            input.add(new StringElement(new Integer(i).toString()));
-        }
-
-        Thread consumer = new Thread(() -> {
-            int consumedCount = 0;
-
-            try {
-                while (consumedCount < ELEMENT_COUNT) {
-                    Batch b = q.readBatch(BATCH_SIZE);
-//                    if (b.getElements().size() < BATCH_SIZE) {
-//                        System.out.println("read small batch=" + b.getElements().size());
-//                    } else {
-//                        System.out.println("read batch size=" + b.getElements().size());
-//                    }
-                    output.addAll((List<StringElement>) b.getElements());
-                    b.close();
-                    consumedCount += b.getElements().size();
-                }
-            } catch (IOException e) {
-                throw new RuntimeException(e);
-            }
-        });
-        consumer.start();
-
-        Thread producer = producer(q, input);
-        producer.start();
-
-        consumer.join();
-        q.close();
-
-        Instant end = Instant.now();
-
-        if (! input.equals(output)) {
-            System.out.println("ERROR: input and output are not equal");
-        } else {
-            System.out.println("SUCCESS, result size=" + output.size() + ", elapsed=" + Duration.between(start, end) + ", rate=" + (new Float(ELEMENT_COUNT) / Duration.between(start, end).toMillis()) * 1000);
-        }
-    }
-
-    public static void oneProducersOneMultipleConsumer() throws IOException, InterruptedException {
-        final List<StringElement> input = new ArrayList<>();
-        final Collection<StringElement> output = new ConcurrentLinkedQueue();
-        final int CONSUMERS = 5;
-        List<Thread> consumers = new ArrayList<>();
-
-        Instant start = Instant.now();
-
-        Queue q = new Queue(settings);
-        q.getCheckpointIO().purge();
-        q.open();
-
-        System.out.print("stating single producers and multiple consumers stress test... ");
-
-        for (int i = 0; i < ELEMENT_COUNT; i++) {
-            input.add(new StringElement(new Integer(i).toString()));
-        }
-
-        for (int i = 0; i < CONSUMERS; i++) {
-            consumers.add(new Thread(() -> {
-                try {
-                    while (output.size() < ELEMENT_COUNT) {
-                        Batch b = q.readBatch(BATCH_SIZE);
-//                        if (b.getElements().size() < BATCH_SIZE) {
-//                            System.out.println("read small batch=" + b.getElements().size());
-//                        } else {
-//                            System.out.println("read batch size=" + b.getElements().size());
-//                        }
-                        output.addAll((List<StringElement>) b.getElements());
-                        b.close();
-                    }
-                    // everything is read, close queue here since other consumers might be blocked trying to get next batch
-                    q.close();
-                } catch (IOException e) {
-                    throw new RuntimeException(e);
-                }
-            }));
-        }
-
-        consumers.forEach(c -> c.start());
-
-        Thread producer = producer(q, input);
-        producer.start();
-
-        // gotta hate exception handling in lambdas
-        consumers.forEach(c -> {try{c.join();} catch(InterruptedException e) {throw new RuntimeException(e);}});
-        q.close();
-
-        Instant end = Instant.now();
-
-        List<StringElement> result = output.stream().collect(Collectors.toList());
-        Collections.sort(result, (p1, p2) -> Integer.valueOf(p1.toString()).compareTo(Integer.valueOf(p2.toString())));
-
-        if (! input.equals(result)) {
-            System.out.println("ERROR: input and output are not equal");
-        } else {
-            System.out.println("SUCCESS, result size=" + output.size() + ", elapsed=" + Duration.between(start, end) + ", rate=" + (new Float(ELEMENT_COUNT) / Duration.between(start, end).toMillis()) * 1000);
-        }
-    }
-
-
-    public static void main(String[] args) throws IOException, InterruptedException {
-        System.out.println(">>> starting in-memory stress test");
-
-        settings = memorySettings(1024 * 1024); // 1MB
-        oneProducersOneConsumer();
-        oneProducersOneMultipleConsumer();
-
-        System.out.println("\n>>> starting file-based stress test in /tmp/queue");
-
-        settings = fileSettings(1024 * 1024); // 1MB
-
-        oneProducersOneConsumer();
-        oneProducersOneMultipleConsumer();
-    }
-
-}
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index c323ec86b29..379a88c22e0 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -57,7 +57,7 @@ def puts(payload)
     # Some tests mess with the settings. This ensures one test cannot pollute another
     LogStash::SETTINGS.reset
 
-    LogStash::SETTINGS.set("queue.type", "memory_acked")
+    LogStash::SETTINGS.set("queue.type", "persisted")
     LogStash::SETTINGS.set("queue.page_capacity", 1024 * 1024)
     LogStash::SETTINGS.set("queue.max_events", 250)
   end
