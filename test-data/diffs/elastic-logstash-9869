diff --git a/docs/static/azure-module.asciidoc b/docs/static/azure-module.asciidoc
index 778cef23417..d06d888548a 100644
--- a/docs/static/azure-module.asciidoc
+++ b/docs/static/azure-module.asciidoc
@@ -1,12 +1,14 @@
 [role="xpack"]
 [[azure-module]]
-=== Azure Module  [Experimental]
+=== Azure Module 
 experimental[]
 
 The https://azure.microsoft.com/en-us/overview/what-is-azure/[Microsoft Azure]
 module in Logstash helps you easily integrate your Azure activity logs and SQL
 diagnostic logs with the Elastic Stack. 
 
+image::static/images/azure-flow.png["Azure Work Flow",width="80%"]
+
 You can monitor your Azure cloud environments and SQL DB deployments with
 deep operational insights across multiple Azure subscriptions. You can explore
 the health of your infrastructure in real-time, accelerating root cause analysis
@@ -25,7 +27,7 @@ information.
 
 The Azure module uses the
 {logstash-ref}/plugins-inputs-azure_event_hubs.html[Logstash Azure Event Hubs
-plugin] to consume data from Azure Event Hubs. The module taps directly into the
+input plugin] to consume data from Azure Event Hubs. The module taps directly into the
 Azure dashboard, parses and indexes events into Elasticsearch, and installs a
 suite of {kib} dashboards to help you start exploring your data immediately.   
 
@@ -50,13 +52,6 @@ These {kib} dashboards are available and ready for you to use. You can use them
 
 * *SQL DB Queries*. Info about SQL Database queries and performance. 
 
-==== Azure_event_hubs plugin
-
-The Azure module uses the `azure_event_hubs` plugin. Basic understanding of the
-plugin and options is helpful when you set up the Azure module. See
-{logstash-ref}/plugins-inputs-azure_event_hubs.html[azure_event_hubs plugin
-documentation] for more information about configurations and options. 
-
 [[azure-module-prereqs]]
 ==== Prerequisites
 
@@ -64,104 +59,79 @@ Azure Monitor enabled with Azure Event Hubs and the Elastic Stack are required
 for this module.
 
 [[azure-elk-prereqs]]
-===== Elastic prereqs
+===== Elastic prerequisites
 
-Logstash, Elasticsearch, and Kibana should be installed and running. The
-products are https://www.elastic.co/downloads[available to download] and easy to
-install. 
+The instructions below assume that you have {ls}, {es}, and {kib} running locally. 
+You can also run {ls}, {es}, and {kib} on separate hosts. 
 
 The Elastic Stack version 6.4 (or later) is required for this module. 
 
-NOTE: Logstash, Elasticsearch, and Kibana must run locally. You can also run 
-Elasticsearch, Kibana and Logstash on separate hosts to consume data from Azure.
+The Azure module uses the `azure_event_hubs` input plugin to consume logs and
+metrics from your Azure environment. It is installed by default with {ls} 6.4
+(or later). Basic understanding of the plugin and options is helpful when you
+set up the Azure module.  
+See the {logstash-ref}/plugins-inputs-azure_event_hubs.html[azure_event_hubs input
+plugin documentation] for more information. 
+
+Elastic products are https://www.elastic.co/downloads[available to download] and
+easy to install. 
 
 [[azure-prereqs]]
-===== Azure prereqs
+===== Azure prerequisites
 
 Azure Monitor should be configured to stream logs to one or more Event Hubs. 
+Logstash will need to access these Event Hubs instances to consume your Azure logs and metrics.
 See <<azure-resources>> at the end of this topic for links to Microsoft Azure documentation.
 
-
-[[azure-module-setup]]
-==== Set up the module
-
-Modify this command for your environment, and run it from the Logstash
-directory.
-
-["source","shell",subs="attributes"]
------
-bin/logstash --modules azure --setup  \
-  -M "azure.var.elasticsearch.username={username}"  \
-  -M "azure.var.elasticsearch.password={pwd}"  \
-  -M "azure.var.kibana.username={username}"  \
-  -M "azure.var.kibana.password={pwd}"  \
-  -M "azure.var.elasticsearch.hosts={hostname}"  \
-  -M "azure.var.kibana.host={hostname}"
------
-
-The `--modules azure` option starts a Logstash pipeline for ingestion from Azure
-Event Hubs. The `--setup` option creates an `azure-*` index pattern in
-Elasticsearch and imports Kibana dashboards and visualizations. 
-
-NOTE: The `--setup` option is intended only for first-time setup. If you include
-`--setup` on subsequent runs, your existing Kibana dashboards will be
-overwritten.
-
-
 [[configuring-azure]]
 ==== Configure the module
 
-You can specify <<azure_config_options,options>> for the Logstash Azure module in the
-`logstash.yml` configuration file or with overrides through the command line. 
+Specify <<azure_config_options,options>> for the Logstash Azure module in the
+`logstash.yml` configuration file. 
 
-* *Command line configuration.* You can pass configuration options and run the module from the command line.
-The command line configuration uses the configuration options you provide and supports only one Event Hub.
-
-* *Basic configuration.* You can use the `logstash.yml` file to configure inputs from multiple Event Hubs that share the same configuration. 
+* *Basic configuration.* You can use the `logstash.yml` file to configure inputs from multiple Event Hubs that share the same configuration.
+Basic configuration is recommended for most use cases. 
 
 * *Advanced configuration.*  The advanced configuration is available for deployments where different Event Hubs
 require different configurations. The `logstash.yml` file holds your settings. Advanced configuration is not necessary or
 recommended for most use cases. 
 
-See {logstash-ref}/plugins-inputs-azure_event_hubs.html[azure_event_hubs plugin
+See the {logstash-ref}/plugins-inputs-azure_event_hubs.html[azure_event_hubs input plugin
 documentation] for more information about basic and advanced configuration
 models. 
 
-[[command-line-sample]]
-===== Command line configuration sample
-
-You can use the command line to set up the basic configuration for a single
-Event Hub. This command starts the Azure module with command line arguments,
-bypassing any settings in `logstash.yml`.
-
-["source","shell",subs="attributes"]
------
-bin/logstash --modules azure -M "azure.var.elasticsearch.host=es.mycloud.com" -M "azure.var.input.azure_event_hubs.threads=8" -M "azure.var.input.azure_event_hubs.consumer_group=logstash" -M "azure.var.input.azure_event_hubs.decorate_events=true" -M "azure.var.input.azure_event_hubs.event_hub_connections=Endpoint=sb://example1...EntityPath=insights-logs-errors" -M "azure.var.input.azure_event_hubs.storage_connection=DefaultEndpointsProtocol=https;AccountName=example...."
------
-
-The command line configuration requires the `azure.var.input.azure_event_hubs.` prefix before a configuration option.
-Notice the notation for the `threads` option. 
-
 [[basic-config-sample]]
 ===== Basic configuration sample
 
 The configuration in the `logstash.yml` file is shared between Event Hubs.
+Basic configuration is recommended for most use cases
 
 ["source","shell",subs="attributes"]
 -----
 modules:
   - name: azure
-    var.elasticsearch.hosts: "localhost:9200"
-    var.kibana.host: "localhost:5601"
-    var.input.azure_event_hubs.threads: 8
-    var.input.azure_event_hubs.decorate_events: true
-    var.input.azure_event_hubs.consumer_group: "logstash"
-    var.input.azure_event_hubs.storage_connection: "DefaultEndpointsProtocol=https;AccountName=example...."
+    var.elasticsearch.hosts: localhost:9200
+    var.kibana.host: localhost:5601
+    var.input.azure_event_hubs.consumer_group: "logstash" <1>
+    var.input.azure_event_hubs.storage_connection: "DefaultEndpointsProtocol=https;AccountName=instance1..." <2>
+    var.input.azure_event_hubs.threads: 9 <3>
     var.input.azure_event_hubs.event_hub_connections:
-      - "Endpoint=sb://example1...EntityPath=insights-logs-errors"
-      - "Endpoint=sb://example2...EntityPath=insights-metrics-pt1m"
+      - "Endpoint=sb://...EntityPath=insights-operational-logs" <4>
+      - "Endpoint=sb://...EntityPath=insights-metrics-pt1m" <5>
+      - "Endpoint=sb://...EntityPath=insights-logs-blocks"
+      - "Endpoint=sb://...EntityPath=insights-logs-databasewaitstatistics"
+      - "Endpoint=sb://...EntityPath=insights-logs-errors"
+      - "Endpoint=sb://...EntityPath=insights-logs-querystoreruntimestatistics"
+      - "Endpoint=sb://...EntityPath=insights-logs-querystorewaitstatistics"
+      - "Endpoint=sb://...EntityPath=insights-logs-timeouts"
 -----
 
+<1> The `consumer_group` (optional) is highly recommended. See <<azure_best_practices>>.
+<2> The `storage_connection` (optional) sets the Azure Blob Storage connection for tracking processing state for Event Hubs when scaling out a deployment with multiple Logstash instances. See <<scaling-blob>> for additional details.
+<3> See <<azure_best_practices>> for guidelines on choosing an appropriate number of threads.
+<4> This connection sets up the consumption of Activity Logs. By default, Azure Monitor uses the `insights-operational-logs` Event Hub name. Make sure this matches the name of the Event Hub specified for Activity Logs.
+<5> This connection and the ones below set up the consumption of SQL DB diagnostic logs and metrics. By default, Azure Monitor uses all these different Event Hub names.
+
 The basic configuration requires the `var.input.azure_event_hubs.` prefix
 before a configuration option. 
 Notice the notation for the `threads` option. 
@@ -169,44 +139,88 @@ Notice the notation for the `threads` option.
 [[adv-config-sample]]
 ===== Advanced configuration sample
 
-Advanced configuration in the `logstash.yml` file supports Event Hub specific options. 
-Advanced configuration is not necessary or recommended for most use cases. Use
-it only if it is required for your deployment scenario. 
+Advanced configuration in the `logstash.yml` file supports Event Hub specific
+options. Advanced configuration is available for more granular tuning of
+threading and Blob Storage usage across multiple Event Hubs. Advanced
+configuration is not necessary or recommended for most use cases. Use it only if
+it is required for your deployment scenario. 
 
 You must define the `header` array with `name` in the first position. You can
 define other options in any order. The per Event Hub configuration takes
 precedence. Any values not defined per Event Hub use the global config value.  
 
-In this example 'consumer_group' will be applied to each of the configured Event
-Hubs. Note that 'decorate_events' is defined in both the 'global' and per Event Hub
-configuration. The per Event Hub configuration takes precedence, and the
-global configuration is effectively ignored.
+In this example `threads`, `consumer_group`, and `storage_connection` will be
+applied to each of the configured Event Hubs. Note that `decorate_events` is
+defined in both the global and per Event Hub configuration. The per Event Hub
+configuration takes precedence, and the global configuration is effectively
+ignored when the per Event Hub setting is present.
 
 ["source","shell",subs="attributes"]
 -----
 modules:
   - name: azure
-    var.elasticsearch.hosts: "localhost:9200"
-    var.kibana.host: "localhost:5601"
-    var.input.azure_event_hubs.threads: 8
-    var.input.azure_event_hubs.decorate_events: true
-    var.input.azure_event_hubs.consumer_group: logstash
+    var.elasticsearch.hosts: localhost:9200
+    var.kibana.host: localhost:5601
+    var.input.azure_event_hubs.decorate_events: true <1>
+    var.input.azure_event_hubs.threads: 9 <2>
+    var.input.azure_event_hubs.consumer_group: "logstash"
+    var.input.azure_event_hubs.storage_connection: "DefaultEndpointsProtocol=https;AccountName=instance1..."
     var.input.azure_event_hubs.event_hubs:
-      - ["name",                      "event_hub_connection",      "storage_connection",                                      "initial_position", "decorate_events"]
-      - ["insights-operational-logs", "Endpoint=sb://example1...", "DefaultEndpointsProtocol=https;AccountName=example1....", "HEAD",             "true"]
-      - ["insights-metrics-pt1m",     "Endpoint=sb://example2...", "DefaultEndpointsProtocol=https;AccountName=example2....", "TAIL",             "true"]
-      - ["insights-logs-errors",      "Endpoint=sb://example3...", "DefaultEndpointsProtocol=https;AccountName=example3....", "TAIL",             "false"]
-      - ["insights-operational-logs", "Endpoint=sb://example4...", "DefaultEndpointsProtocol=https;AccountName=example4....", "HEAD",             "true"]
+      - ["name",                                    "initial_position",  "storage_container",  "decorate_events",  "event_hub_connection"]                                   <3>
+      - ["insights-operational-logs",                 "TAIL",              "activity-logs1",    "true",             "Endpoint=sb://...EntityPath=insights-operational-logs"]
+      - ["insights-operational-logs",                 "TAIL",              "activity_logs2",<4>   "true",             "Endpoint=sb://...EntityPath=insights-operational-logs"] 
+      - ["insights-metrics-pt1m",                     "TAIL",              "dbmetrics",         "true",             "Endpoint=sb://...EntityPath=insights-metrics-pt1m"]
+      - ["insights-logs-blocks",                      "TAIL",              "dbblocks",          "true",             "Endpoint=sb://...EntityPath=insights-logs-blocks"]
+      - ["insights-logs-databasewaitstatistics",      "TAIL",              "dbwaitstats",       "false",            "Endpoint=sb://...EntityPath=insights-logs-databasewaitstatistics"]
+      - ["insights-logs-errors",                      "HEAD",              "dberrors",          "true",             "Endpoint=sb://...EntityPath=insights-logs-errors"
+      - ["insights-logs-querystoreruntimestatistics", "TAIL",              "dbstoreruntime",    "true",             "Endpoint=sb://...EntityPath=insights-logs-querystoreruntimestatistics"]
+      - ["insights-logs-querystorewaitstatistics",    "TAIL",              "dbstorewaitstats",  "true",             "Endpoint=sb://...EntityPath=insights-logs-querystorewaitstatistics"]
+      - ["insights-logs-timeouts",                    "TAIL",              "dbtimeouts",        "true",             "Endpoint=sb://...EntityPath=insights-logs-timeouts"]
 -----
 
+<1> You can specify global Event Hub options. They will be overridden by any configurations specified in the event_hubs option.
+<2> See <<azure_best_practices>> for guidelines on choosing an appropriate number of threads.
+<3> The header array must be defined with name in the first position. Other options can be defined in any order. The per Event Hub configuration takes precedence. Any values not defined per Event Hub use the global config value.
+<4> This enables consuming from a second Activity Logs Event Hub that uses a different Blob Storage container. This is necessary to avoid the offsets from the first insights-operational-logs from overwriting the offsets for the second insights-operational-logs.
+
 The advanced configuration doesn't require a prefix before a per Event Hub
-configuration option. Notice the notation for the `decorate_events` option. 
+configuration option. Notice the notation for the `initial_position` option. 
+
+[[scaling-blob]]
+===== Scale Event Hub consumption
+
+An https://azure.microsoft.com/en-us/services/storage/blobs[Azure Blob Storage
+account] is an essential part of Azure-to-Logstash configuration. 
+It is required for users who want to scale out multiple {ls} instances to consume from Event Hubs.
+
+A Blob Storage account is a central location that enables multiple instances of
+{ls} to work together to process events. It records the
+offset (location) of processed events. On restart, {ls} resumes processing
+exactly where it left off.
+
+Configuration notes:
+
+*  A Blob Storage account is highly recommended for use with this module, and is
+likely required for production servers.
+* The `storage_connection` option passes the blob storage connection string. 
+* Configure all {ls} instances to use the same `storage_connection` to get the
+benefits of shared processing.
+
+Sample Blob Storage connection string:
+
+[source,text]
+----
+DefaultEndpointsProtocol=https;AccountName=logstash;AccountKey=ETOPnkd/hDAWidkEpPZDiXffQPku/SZdXhPSLnfqdRTalssdEuPkZwIcouzXjCLb/xPZjzhmHfwRCGo0SBSw==;EndpointSuffix=core.windows.net
+----
+
+Find the connection string to Blob Storage here: 
+https://portal.azure.com[Azure Portal]`-> Blob Storage account -> Access keys`.
 
 [[azure_best_practices]]
 ===== Best practices
 
-Here are some guidelines to help you avoid data conflicts that can cause lost
-events.
+Here are some guidelines to help you achieve a successful deployment, and avoid
+data conflicts that can cause lost events.
 
 * **Create a {ls} consumer group.** 
 Create a new consumer group specifically for {ls}. Do not use the $default or
@@ -224,10 +238,63 @@ sure that at least one of these options is different per Event Hub:
 ** storage_connection
 ** storage_container (defaults to Event Hub name if not defined)
 ** consumer_group
+* **Set number of threads correctly.** 
+The number of threads should equal the number of Event Hubs plus one (or more).
+Each Event Hub needs at least one thread. An additional thread is needed to help
+coordinate the other threads. The number of threads should not exceed the number of Event Hubs multiplied by the
+number of partitions per Event Hub plus one. Threads are
+currently  available only as a global setting.
+** Sample: Event Hubs = 4. Partitions on each Event Hub = 3.
+Minimum threads is 5 (4 Event Hubs plus one). Maximum threads is 13 (4 Event
+Hubs times 3 partitions plus one). 
+** If you're collecting activity logs from only one specified event hub instance,
+then only 2 threads (1 Event Hub plus one) are required.
+
+[[azure-module-setup]]
+==== Set up and run the module
+
+Be sure that the `logstash.yml` file is <<configuring-azure,configured correctly>>. 
+
+===== First time setup
+
+Run this command from the Logstash directory:
 
+["source","shell",subs="attributes"]
+-----
+bin/logstash --setup
+-----
+
+The `--modules azure` option starts a Logstash pipeline for ingestion from Azure
+Event Hubs. The `--setup` option creates an `azure-*` index pattern in
+Elasticsearch and imports Kibana dashboards and visualizations. 
+
+===== Subsequent starts
+
+Run this command from the Logstash directory:
+
+["source","shell",subs="attributes"]
+-----
+bin/logstash 
+-----
+
+NOTE: The `--setup` option is intended only for first-time setup. If you include
+`--setup` on subsequent runs, your existing Kibana dashboards will be
+overwritten.
+
+[[exploring-data-azure]]
+==== Explore your data
+When the Logstash Azure module starts receiving events, you can begin using the
+packaged Kibana dashboards to explore and visualize your data. 
+
+To explore your data with Kibana:
+
+. Open a browser to http://localhost:5601[http://localhost:5601] (username:
+  "elastic"; password: "{pwd}")
+. Click *Dashboard*.
+. Click *[Azure Monitor] Overview*.
 
 [[azure_config_options]]
-===== Configuration options
+==== Configuration options
 
 NOTE: All Event Hubs options are common to both basic and advanced
 configurations, with the following exceptions. The basic configuration uses
@@ -303,7 +370,7 @@ When first reading from an Event Hub, start from this position:
 * `look_back` reads `end` minus a number of seconds worth of pre-existing events.
 You control the number of seconds using the `initial_position_look_back` option.
 
-Note: If `storage_connection` is set, the `initial_position` value is used only
+If `storage_connection` is set, the `initial_position` value is used only
 the first time Logstash reads from the Event Hub.
 
 
@@ -368,33 +435,12 @@ Total number of threads used to process events. The value you set here applies
 to all Event Hubs. Even with advanced configuration, this value is a global
 setting, and can't be set per event hub. 
 
+The number of threads should be the number of Event Hubs plus one or more. 
+See <<azure_best_practices>> for more information.
+
 
 include::shared-module-options.asciidoc[]
     
-[[run-azure]]
-==== Start the module 
-
-. Be sure that the `logstash.yml` file is <<configuring-azure,configured correctly>>. 
-. Run this command from the Logstash directory:
-
-["source","shell",subs="attributes"]
------
-bin/logstash
------
-
-[[exploring-data-azure]]
-==== Explore your data
-When the Logstash Azure module starts receiving events, you can begin using the
-packaged Kibana dashboards to explore and visualize your data. 
-
-To explore your data with Kibana:
-
-. Open a browser to http://localhost:5601[http://localhost:5601] (username:
-  "elastic"; password: "{pwd}")
-. Click *Dashboard*.
-. Select the dashboard you want to see.
-
-
 ==== Azure module schema
 
 This module reads data from the Azure Event Hub and adds some additional structure to the data for Activity Logs and SQL Diagnostics. The original data is always preserved and any data added or parsed will be namespaced under 'azure'. For example, 'azure.subscription' may have been parsed from a longer more complex URN.
diff --git a/docs/static/images/azure-flow.png b/docs/static/images/azure-flow.png
new file mode 100644
index 00000000000..e89eacbe256
Binary files /dev/null and b/docs/static/images/azure-flow.png differ
diff --git a/docs/static/modules.asciidoc b/docs/static/modules.asciidoc
index eaa4c961c22..97375825ba8 100644
--- a/docs/static/modules.asciidoc
+++ b/docs/static/modules.asciidoc
@@ -4,6 +4,13 @@
 Logstash modules provide a quick, end-to-end solution for ingesting data and
 visualizing it with purpose-built dashboards.
 
+These modules are available:
+
+* <<connecting-to-cloud,Elastic Cloud>>
+* <<arcsight-module>>
+* <<netflow-module>>
+* <<azure-module, Microsoft Azure Module>>
+
 Each module comes pre-packaged with Logstash configurations, Kibana dashboards,
 and other meta files that make it easier for you to set up the Elastic Stack for
 specific use cases or data sources.
