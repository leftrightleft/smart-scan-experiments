diff --git a/Makefile b/Makefile
index 1fe081722ff..bcd78e7b803 100644
--- a/Makefile
+++ b/Makefile
@@ -13,7 +13,8 @@ JRUBY_CMD=java -jar $(JRUBY)
 ELASTICSEARCH_URL=http://download.elasticsearch.org/elasticsearch/elasticsearch
 ELASTICSEARCH=vendor/jar/elasticsearch-$(ELASTICSEARCH_VERSION)
 TYPESDB=vendor/collectd/types.db
-TYPESDB_URL=https://collectd.org/files/collectd-5.4.0.tar.gz
+COLLECTD_VERSION=5.4.0
+TYPESDB_URL=https://collectd.org/files/collectd-$(COLLECTD_VERSION).tar.gz
 GEOIP=vendor/geoip/GeoLiteCity.dat
 GEOIP_URL=http://logstash.objects.dreamhost.com/maxmind/GeoLiteCity-2013-01-18.dat.gz
 KIBANA_URL=https://download.elasticsearch.org/kibana/kibana/kibana-latest.tar.gz
@@ -115,7 +116,7 @@ vendor:
 vendor/jar: | vendor
 	$(QUIET)mkdir $@
 
-build-jruby: $(JRUBY)
+vendor-jruby: $(JRUBY)
 
 $(JRUBY): | vendor/jar
 	$(QUIET)echo " ==> Downloading jruby $(JRUBY_VERSION)"
@@ -144,6 +145,7 @@ vendor-geoip: $(GEOIP)
 $(GEOIP): | vendor/geoip
 	$(QUIET)$(DOWNLOAD_COMMAND) $@.tmp.gz $(GEOIP_URL)
 	$(QUIET)gzip -dc $@.tmp.gz > $@.tmp
+	$(QUIET)rm "$@.tmp.gz"
 	$(QUIET)mv $@.tmp $@
 
 vendor/collectd: | vendor
@@ -153,9 +155,8 @@ vendor/collectd: | vendor
 vendor-collectd: $(TYPESDB)
 $(TYPESDB): | vendor/collectd
 	$(QUIET)$(DOWNLOAD_COMMAND) $@.tar.gz $(TYPESDB_URL)
-	$(QUIET)mkdir $@.tmpdir
-	$(QUIET)tar zxf $@.tar.gz -C $@.tmpdir
-	$(QUIET)find $@.tmpdir -type f -name types.db -exec mv -i {} $@ \;
+	$(QUIET)tar zxf $@.tar.gz -O "collectd-$(COLLECTD_VERSION)/src/types.db" > $@
+	$(QUIET)rm $@.tar.gz
 
 # Always run vendor/bundle
 .PHONY: fix-bundler
@@ -418,7 +419,7 @@ prepare-tarball: vendor/ua-parser/regexes.yaml
 prepare-tarball:
 	@echo "=> Preparing tarball"
 	$(QUIET)$(MAKE) $(WORKDIR)
-	$(QUIET)rsync -a --relative bin lib spec locales patterns vendor/bundle/jruby vendor/geoip vendor/jar vendor/kibana vendor/ua-parser vendor/collectd LICENSE README.md $(WORKDIR)
+	$(QUIET)rsync -a --relative bin lib spec locales patterns vendor/bundle/jruby vendor/geoip vendor/jar vendor/kibana vendor/ua-parser vendor/collectd LICENSE README.md --exclude 'vendor/bundle/jruby/1.9/cache' --exclude 'vendor/bundle/jruby/1.9/gems/*/doc' --exclude 'vendor/jar/elasticsearch-$(ELASTICSEARCH_VERSION).tar.gz'  $(WORKDIR)
 	$(QUIET)sed -i -e 's/^LOGSTASH_VERSION = .*/LOGSTASH_VERSION = "$(VERSION)"/' $(WORKDIR)/lib/logstash/version.rb
 
 .PHONY: tarball
diff --git a/lib/logstash/inputs/generator.rb b/lib/logstash/inputs/generator.rb
index 7da3613e01b..d95258d5b07 100644
--- a/lib/logstash/inputs/generator.rb
+++ b/lib/logstash/inputs/generator.rb
@@ -83,7 +83,6 @@ def run(queue)
         queue << event
       end
     end
-    sleep 3
   end # def run
 
   public
diff --git a/lib/logstash/monkeypatches-for-debugging.rb b/lib/logstash/monkeypatches-for-debugging.rb
index 33d6527c878..683c0a106bd 100644
--- a/lib/logstash/monkeypatches-for-debugging.rb
+++ b/lib/logstash/monkeypatches-for-debugging.rb
@@ -1,16 +1,39 @@
 # encoding: utf-8
 if $DEBUGLIST.include?("require")
+  ROOT = File.dirname(__FILE__)
   module Kernel
     alias_method :require_debug, :require
 
     def require(path)
+      start = Time.now
       result = require_debug(path)
+      duration = Time.now - start
+
       origin = caller[1]
       if origin =~ /rubygems\/custom_require/
         origin = caller[3]
+        if origin.nil?
+          STDERR.puts "Unknown origin"
+          STDERR.puts caller.join("\n")
+        end
+      end
+      origin = origin.gsub(/:[0-9]+:in .*/, "") if origin
+
+      # Only print require() calls that did actual work.
+      # require() returns true on load, false if already loaded.
+      if result
+        source = caller[0]
+        #p source.include?("/lib/polyglot.rb:63:in `require'") => source
+        if source.include?("/lib/polyglot.rb:63:in `require'")
+          source = caller[1]
+        end
+
+        #target = $LOADED_FEATURES.grep(/#{path}/).first
+        #puts path
+        #puts caller.map { |c| "  #{c}" }.join("\n")
+        #fontsize = [10, duration * 48].max
+        puts "#{duration},#{path},#{source}"
       end
-      puts "require(\"#{path}\")" if result
-      #puts "require(\"#{path}\") => #{result} (from: #{origin})"
       #puts caller.map { |c| " => #{c}" }.join("\n")
     end
 
diff --git a/lib/logstash/runner.rb b/lib/logstash/runner.rb
index e3ab0576dbd..6431e0244b0 100644
--- a/lib/logstash/runner.rb
+++ b/lib/logstash/runner.rb
@@ -48,6 +48,7 @@ module Cabin::Mixins::Logger
 require "logstash/namespace"
 require "logstash/program"
 require "i18n" # gem 'i18n'
+I18n.enforce_available_locales = true
 I18n.load_path << File.expand_path(
   File.join(File.dirname(__FILE__), "../../locales/en.yml")
 )
diff --git a/misc/patterns/mysql b/misc/patterns/mysql
deleted file mode 100644
index 0c84b0e81d5..00000000000
--- a/misc/patterns/mysql
+++ /dev/null
@@ -1,11 +0,0 @@
-MYSQL_ERROR_TIMESTAMP %{NUMBER:date} %{TIME}
-MYSQL_ERORR_LOG_CONTENT_P1 \[%{WORD:mysql_error_log_level}\] %{GREEDYDATA:mysql_error_log_content}
-MYSQL_ERORR_LOG_CONTENT_P2 %{GREEDYDATA:mysql_error_log_content}
-MYSQL_ERROR_LOG %{MYSQL_ERROR_TIMESTAMP} (%{MYSQL_ERORR_LOG_CONTENT_P1}|%{MYSQL_ERORR_LOG_CONTENT_P2})
-
-MYSQL_SLOW_FROM ^# User@Host: %{USER:user}\[[^\]]+\] @ %{HOST:host} \[%{IP:ip_addr}?]
-MYSQL_SLOW_STAT ^# Query_time: %{NUMBER:duration:float} \s*Lock_time: %{NUMBER:lock_wait:float} \s*Rows_sent: %{NUMBER:results:int} \s*Rows_examined: %{NUMBER:scanned:int}
-MYSQL_SLOW_TIMESTAMP ^SET timestamp=%{NUMBER:timestamp};
-MYSQL_SLOW_DB ^use %{WORD:db_name};
-MYSQL_SLOW_QUERY ^%{WORD:action}%{SPACE}%{GREEDYDATA};
-
diff --git a/misc/patterns/php5 b/misc/patterns/php5
deleted file mode 100644
index 4c1c6599c27..00000000000
--- a/misc/patterns/php5
+++ /dev/null
@@ -1,6 +0,0 @@
-PHP_LOG_CONTENT (.+)
-PHP_DATE_TIME %{MONTHDAY}-%{MONTH}-%{YEAR}\s+%{TIME}
-PHP_TZ_NAME [A-Z]{3}
-PHP_ERROR_LOG \s*+\[%{PHP_DATE_TIME:timestamp} %{PHP_TZ_NAME}\] PHP %{LOGLEVEL:php_log_level} error: %{PHP_LOG_CONTENT:php_log_content}
-PHP_FPM_ERROR_LOG \[%{PHP_DATE_TIME:timestamp}\] %{LOGLEVEL:php_log_level}: (\[%{GREEDYDATA:php_fpm_pool}\] child %{POSINT}, %{GREEDYDATA:php_log_content}|%{GREEDYDATA:php_log_content})
-PHP_FPM_SLOW_LOG \[%{GREEDYDATA:stack_addr}\] %{GREEDYDATA:func_name} %{UNIXPATH:script_path}
diff --git a/misc/patterns/redis b/misc/patterns/redis
deleted file mode 100644
index d0bdff481ab..00000000000
--- a/misc/patterns/redis
+++ /dev/null
@@ -1,2 +0,0 @@
-REDISLOG_WITH_LEVEL \[%{POSINT:pid}\] %{REDISTIMESTAMP:timestamp} # %{LOGLEVEL:redis_log_level} %{GREEDYDATA}
-REDISLOG_FIXED (%{REDISLOG}|%{REDISLOG_WITH_LEVEL})
diff --git a/misc/pipeline-thoughts.md b/misc/pipeline-thoughts.md
deleted file mode 100644
index 5e74401b5d3..00000000000
--- a/misc/pipeline-thoughts.md
+++ /dev/null
@@ -1,56 +0,0 @@
-## Terms
-
-* input = source = emitter = sender
-* filter = decorator = processor 
-* output = destination = sink = consumer = receiver
-
-In this pipeline model, I will call any input, filter, or output a 'station'
-
-## Properties
-
-* inputs produce messages
-* filters modify or drop messages
-* outputs consume messages
-
-Filters have both producer and consumer properties.
-
-## Pipeline stall strategies
-
-Which has the buffer? The input or the output?
-
-* In TCP, the sender stalls if the receiver stops acking.
-* In a Ruby SizedQueue, the sender stalls (SizedQueue#push) if receiver stops popping.
-
-In both cases above, inaction by the receiver causes the sender to stall. This is nice because throttling requires no negotiation. 
-
-Further, stall behavior can be modified simply by writing a filter that changes its behavior when a stall is detected. For example, instead of blocking the pipeline, a stall-managing filter could choose to drop messages so as to unblock upstream stations.
-
-## Parallelization strategies
-
-* Every station can run a tunable number of workers.
-  * Input rationale: For slow consumers like bunny/amqp, logstash users have observed that 4 amqp inputs work faster than 1 amqp input, even with prefetch >100
-  * Filter rationale: CPU-intensive filters like parsers benefit from parallelization
-  * Output rationale: Same for inputs. Slow-in-code outputs can often mitigated simply by running more of those slow things.
-* A worker is one process/thread.
-
-Currently logstash implements all filters in a single worker thread. This causes order problems when using the multiline filter. If instead each filter could have a tunable number of workers, we could leave multline at 1 worker and use 10 for grok and date processing.
-
-## Maintaining Order
-
-When introducing parallelism, the order of messages will be lost without care. This can matter in cases like with logstash's multline filter. In general, this may not be an issue.
-
-## Station plumbing
-
-In this scenario, station plumbing is considered only for in-process communication. External plumbing is trivially achieved by implementing networked inputs and outputs.
-
-* Is Ruby's SizedQueue fast enough? How do MRI and JRuby's SizedQueue implementation performances vary?
-* File descriptors require syscalls to ship messages, probably not good at high performance.
-
-## Station data model
-
-* worker count
-* metrics
-
-## Pipeline data model
-
-* ordered list of stations
diff --git a/misc/pl.rb b/misc/pl.rb
deleted file mode 100644
index 768a4954e73..00000000000
--- a/misc/pl.rb
+++ /dev/null
@@ -1,91 +0,0 @@
-# pipeline tests
-
-$: << "lib"
-require "logstash/config/file"
-config = LogStash::Config::File.new(nil, ARGV[0])
-agent = LogStash::Agent.new
-inputs, filters, outputs = agent.instance_eval { parse_config(config) }
-
-inputs.collect(&:register)
-filters.collect(&:register)
-outputs.collect(&:register)
-
-i2f = SizedQueue.new(16)
-f2o  = SizedQueue.new(16)
-i2f = f2o if filters.empty?
-
-input_threads = inputs.collect do |i| 
-  t = Thread.new do
-    begin
-      i.run(i2f)
-    rescue => e
-      puts :input => i.class, :exception => e
-    end
-  end
-  t[:name] = i.class
-  t
-end
-
-#input_supervisor_thread = Thread.new do
-  #while true 
-    #input_threads.collect(&:join)
-    #i2f << :shutdown
-  #end
-#end
-
-filter_thread = Thread.new(filters) do |filters|
-  if filters.any?
-    event = i2f.pop
-    filters.each do |filter|
-      filter.filter(event)
-    end
-    f2o << event
-  end
-end
-filter_thread[:name] = "filterworker"
-
-output_thread = Thread.new do
-  begin
-    while true 
-      event = f2o.pop
-      outputs.each do |output|
-        output.receive(event)
-      end
-    end
-  rescue => e
-    puts :output_thread => e
-  end
-end
-output_thread[:name] = "outputworker"
-
-def twait(thread)
-  begin
-    puts :waiting => thread[:name]
-    thread.join
-    puts :donewaiting => thread[:name]
-  rescue => e
-    puts thread => e
-  end
-end
-
-def shutdown(input, filter, output)
-  input.each do |i|
-    i.raise("SHUTDOWN")
-    twait(i)
-  end
-
-  #filter.raise("SHUTDOWN")
-  #twait(filter)
-  output.raise("SHUTDOWN")
-  twait(output)
-end
-
-trap("INT") do
-  puts "SIGINT"; shutdown(input_threads, filter_thread, output_thread)
-  exit 1
-end
-
-#[*input_threads, filter_thread, output_thread].collect(&:join)
-sleep 30
-
-
diff --git a/misc/pl2.rb b/misc/pl2.rb
deleted file mode 100644
index 34aa7a2c334..00000000000
--- a/misc/pl2.rb
+++ /dev/null
@@ -1,118 +0,0 @@
-$: << "lib"
-require "logstash/config/file"
-
-class Pipeline
-  class ShutdownSignal; end
-
-  def initialize(configstr)
-    # hacks for now to parse a config string
-    config = LogStash::Config::File.new(nil, configstr)
-    agent = LogStash::Agent.new
-    @inputs, @filters, @outputs = agent.instance_eval { parse_config(config) }
-
-    @inputs.collect(&:register)
-    @filters.collect(&:register)
-    @outputs.collect(&:register)
-
-    @input_to_filter = SizedQueue(16)
-    @filter_to_output = SizedQueue(16)
-
-    # If no filters, pipe inputs to outputs
-    if @filters.empty?
-      input_to_filter = filter_to_output
-    end
-  end
-
-  def run
-    # one thread per input
-    @input_threads = @inputs.collect do |input|
-      Thread.new(input) do |input|
-        inputworker(input)
-      end
-    end
-
-    # one filterworker thread
-    #@filter_threads = @filters.collect do |input
-    # TODO(sissel): THIS IS WHERE I STOPPED WORKING
-
-    # one outputworker thread
-
-    # Now monitor input threads state
-    # if all inputs are terminated, send shutdown signal to @input_to_filter
-  end
-
-  def inputworker(plugin)
-    begin
-      plugin.run(@input_to_filter)
-    rescue ShutdownSignal
-      plugin.teardown
-    rescue => e
-      @logger.error("Exception in plugin #{plugin.class}, restarting plugin.",
-                    "plugin" => plugin.inspect, "exception" => e)
-      plugin.teardown
-      retry
-    end
-  end # def 
-
-  def filterworker
-    begin
-      while true
-        event << @input_to_filter
-        break if event == :shutdown
-        @filters.each do |filter|
-          filter.filter(event)
-        end
-        next if event.cancelled?
-        @filter_to_output << event
-      end
-    rescue => e
-      @logger.error("Exception in plugin #{plugin.class}",
-                    "plugin" => plugin.inspect, "exception" => e)
-    end
-    @filters.each(&:teardown)
-  end # def filterworker
-
-  def outputworker
-    begin
-      while true
-        event << @filter_to_output
-        break if event == :shutdown
-        @outputs.each do |output|
-          output.receive(event)
-        end
-      end
-    rescue => e
-      @logger.error("Exception in plugin #{plugin.class}",
-                    "plugin" => plugin.inspect, "exception" => e)
-    end
-    @outputs.each(&:teardown)
-  end # def filterworker
-end # class Pipeline
-
-def twait(thread)
-  begin
-    puts :waiting => thread[:name]
-    thread.join
-    puts :donewaiting => thread[:name]
-  rescue => e
-    puts thread => e
-  end
-end
-
-def shutdown(input, filter, output)
-  input.each do |i|
-    i.raise("SHUTDOWN")
-  end
-
-  #filter.raise("SHUTDOWN")
-  #twait(filter)
-  output.raise("SHUTDOWN")
-  twait(output)
-end
-
-trap("INT") do
-  puts "SIGINT"; shutdown(input_threads, filter_thread, output_thread)
-  exit 1
-end
-
-
diff --git a/misc/presentation-description.txt b/misc/presentation-description.txt
deleted file mode 100644
index 7afcbe33c2a..00000000000
--- a/misc/presentation-description.txt
+++ /dev/null
@@ -1,24 +0,0 @@
-logstash: get awesome with your logs.
-
-This talk will introduce the free and open source tool, logstash, and cover how
-it can be used to debug and analyze problems with your infrastructure and your
-business: centralize your event and log collection, analyze data, and correlate
-failures. It will also cover some experiences and best practices to help you
-get value the most out of your code.
-
---
-
-Logstash is an open source, free, and scalable tool that can help you get a
-grip on your logs and events. Search and analyze your infrastructure with ease,
-Let logstash be a crystal ball for viewing events in real-time across your
-infrastructure and your business, Logstash acts as a pipeline, so you can
-easily automate reactions and alerts to create a self-healing and monitored
-infrastructure.
-
-This talk introduces logstash and covers how it can be used to debug and
-analyze problems with your infrastructure and your business: centralize your
-event and log collection, analyze data, and correlate failures.
-
-This talk targets folks software engineers, sysadmins, and engineering managers.
-
-Project site: http://logstash.net/
diff --git a/misc/rate.sh b/misc/rate.sh
deleted file mode 100755
index 9ff14b95345..00000000000
--- a/misc/rate.sh
+++ /dev/null
@@ -1,25 +0,0 @@
-#!/bin/zsh
-
-if [ "$#" -ne 1 ] ; then
-  echo "Usage; $0 logfile"
-  exit 1
-fi
-logfile="$1"
-
-pid=$(ps -u $USER -f | awk '/bin.logstash -[f]/ {print $2}')
-fileno=$(lsof -nPp $pid | grep -F "$logfile" |  awk '{ print int($4) }')
-pos=$(awk '/pos:/ {print $2}' /proc/$pid/fdinfo/$fileno)
-size=$(ls -ld "$logfile" | awk '{print $5}')
-starttime=$(awk '{print $22}' /proc/$pid/stat)
-curtime=$(awk '{print $1}' /proc/uptime)
-lines=$(dd if="$logfile" bs=$pos count=1 2> /dev/null | wc -l)
-percent=$(printf "%.2f%%" $(( ($pos / ($size + 0.0)) * 100 )))
-
-duration=$(($curtime - ($starttime / 100.)))
-rate=$(( $lines / (0.0 + $duration) ))
-
-ps --no-header -o "pid user args" -p $pid
-echo "Duration: $duration"
-echo "Lines: $lines (position: $pos, $percent)"
-echo "Rate: $rate" 
-
diff --git a/misc/screencast/000.intro b/misc/screencast/000.intro
deleted file mode 100644
index f8c5d9fc2a6..00000000000
--- a/misc/screencast/000.intro
+++ /dev/null
@@ -1,21 +0,0 @@
-%K Escape
-1GdGi
-tail -f is nice, but it doesn't scale.
-
-Plus, the output is just a stream of text. Aren't logs really messages?
-
-Enter logstash. 
-
-logstash gives you a pipe metaphor similar to the unix model.  Stuff goes in; stuff gets modified; stuff goes out. Think: sed.
-
-Powershell built on the unix pipe model by allowing you to pipe objects instead of just text. (If you haven't seen powershell yet, go check it out, it is awesome)
-
-Let's take that piped object model and apply it to logs, events, and the network.
-
-* Input from files, processes, etc. 
-* Parse it and package it into an object.
-* Ship it to anything willing to listen.
-
-If we provide a framework for doing this, you can easily ship logs to message queues, databases, archive servers, web browsers, etc. 
-
-Let's show a bit of logstash.
diff --git a/misc/screencast/001.config b/misc/screencast/001.config
deleted file mode 100644
index b2e25c9151d..00000000000
--- a/misc/screencast/001.config
+++ /dev/null
@@ -1,37 +0,0 @@
-cd ~/projects/logstash
-%E rm ~/projects/logstash/etc/logstash-demo.yaml
-
-vi etc/logstash-demo.yaml
-:set paste
-
-%K control+l
-i
-# Remember that logstash provides a way to specify inputs, filters, and
-# outputs. For this demo, I'll just show inputs + outputs
----
-inputs:
-  # You can also tag inputs for easier handling later in your pipeline.
-  linux-syslog: # this is the 'linux-syslog' tag
-  - /var/log/messages # watch /var/log/messages (uses eventmachine-tail)
-  - /var/log/kern.log
-  - /var/log/auth.log
-  - /var/log/user.log
-  apache-access: # similar, different tag.
-  - /var/log/apache2/access.log
-  apache-error:
-  - /var/log/apache2/access.log
-  #other:
-  #- amqp://myamqpserver/fanout/rawlogs # an amqp fanout as input
-  #- amqp://myamqpserver/topic/rawlogs # an amqp topic as input
-  #- syslog:///  # take input via syslog protocol over the network
-outputs:
-  #- amqp://myamqpserver/topic/logs  # broadcast logs to an AMQP topic
-  #- mongodb://mongoserver/logs      # store events in mongodb
-  #- stdout:///                      # send to stdout (like tail -f, but better)
-  #- syslog://syslogserver/          # send to another syslog server
-  - websocket:///                    # send to websockets
-%E sleep 3
-
-%K Escape
-
-ZZ
diff --git a/misc/screencast/002.webdemo b/misc/screencast/002.webdemo
deleted file mode 100644
index ff712f66e39..00000000000
--- a/misc/screencast/002.webdemo
+++ /dev/null
@@ -1,41 +0,0 @@
-%K control+a c
-%E sleep 2
-cd ~/projects/logstash
-export RUBYLIB=lib
-ruby bin/logstash -f etc/logstash-demo.yaml
-
-%E sleep 2
-# Now let's pop open google chrome (supports WebSockets) and watch
-# some logs...
-
-%E xdotool search --title " - Google Chrome" windowactivate --sync %@
-%K control+l BackSpace
-http://snack.home/~jls/ws
-
-%E logger -p 1 -t demo "This log is coming to you live."; sleep 2
-%E logger -p 1 -t demo "Any log being received on a logstash input can be viewed here, or stored in a database, or shipped elsewhere for processing."; sleep 2;
-%E logger -p 1 -t demo "Everything is piped input -> filter -> output."; sleep 2;
-%E logger -p 1 -t demo "The output of one can be the input of another. Chain by chain. "; sleep 2;
-%E logger -p 1 -t demo "The way you deal with logs is about to change."; sleep 2;
-
-%E xdotool search --onlyvisible gnome-terminal windowsize --usehints 70 7 windowactivate --sync windowmove 3000 0 
-%K control+minus
-%K control+a c
-%E sleep 2
-# Now we can watch logs in the browser...
-curl -o /dev/null http://snack.home/~jls/something
-!!
-!!
-!!
-
-logger -p 1 -t logging-example 'Hello world!'
-logger -p 1 -t logging-example "Welcome to logstash. $RANDOM"
-!!
-!!
-!!
-%E sleep 2
-
-
-# It's fast, too.
-seq 15 | xargs -n1 logger -p 1 -t fastlogs "real time feeds == :)"
-
diff --git a/misc/screencast/README b/misc/screencast/README
deleted file mode 100644
index e07052716e2..00000000000
--- a/misc/screencast/README
+++ /dev/null
@@ -1,3 +0,0 @@
-The code here was used to automatically direct a screencast demonstrating logstash.
-
-The resulting video is here: http://www.youtube.com/watch?v=Fi7OaiNqPCc
diff --git a/misc/screencast/run.rb b/misc/screencast/run.rb
deleted file mode 100644
index 748c78cec03..00000000000
--- a/misc/screencast/run.rb
+++ /dev/null
@@ -1,52 +0,0 @@
-#!/usr/bin/env ruby
-#
-
-require "rubygems"
-
-#if ENV["DISPLAY"] != ":1" 
-  #puts "$DISPLAY is wrong."
-  #exit 1
-#end
-
-def type(string)
-  system("xdotool", "type", "--clearmodifiers", "--delay", "100", string)
-  puts "Typing: #{string}"
-  #puts string.inspect
-  #$stdout.flush
-end
-
-def run(string)
-  command = string[3..-1].chomp
-  system(command)
-end
-
-def key(string)
-  keyseq = string[3..-1].chomp.split(/ +/)
-  system("xdotool", "key", "--clearmodifiers",  *keyseq)
-  puts keyseq.inspect
-  #puts string.inspect
-  #$stdout.flush
-end
-
-handlers = [
-  [/^[,]/m, proc { |s| type(s); sleep(0.4) } ], # comma
-  [/^[.;:?!]+/m, proc { |s| type(s); sleep(1) } ], # punctuation
-  [/^[\n]{2}/m, proc { |s| type(s); sleep(1) } ], # new paragraph
-  #[/^[\n](?! *[*-])/m, proc { |s| type(" ") } ], # continuation of a paragraph
-  #[/^[\n](?= *[*-])/m, proc { |s| type("\n") } ], # lists or other itemized things
-  [/^[\n]/m, proc { |s| type(s) } ], # lists or other itemized things
-  [/^%E[^\n]*\n/m, proc { |s| run(s) } ], # execute a command
-  [/^%K[^\n]*\n/m, proc { |s| key(s) } ], # type a specific keystroke
-  [/^[^,.;:?!\n]+/m, proc { |s| type(s) } ], # otherwise just type it
-] 
-
-data = $stdin.read
-while data.length > 0
-  match, func = handlers.collect { |re, f| [re.match(data), f] }\
-                        .select { |m,f| m.begin(0) == 0 rescue false }.first
-  str = match.to_s
-  func.call(str)
-  $stdout.flush
-  #sleep 3
-  data = data[match.end(0)..-1]
-end
diff --git a/misc/shipper_config_generator.py b/misc/shipper_config_generator.py
deleted file mode 100644
index 77edbff7fd7..00000000000
--- a/misc/shipper_config_generator.py
+++ /dev/null
@@ -1,416 +0,0 @@
-#!/usr/bin/env python
-"""
-generate logstash shipper configuration file
-"""
-import logging
-import os
-import re
-import subprocess
-import sys
-import httplib
-import socket
-
-from subprocess import Popen, CalledProcessError
-from subprocess import STDOUT, PIPE
-
-logging.getLogger("shipper_config_generator").setLevel(logging.DEBUG)
-
-PWD = os.path.dirname(os.path.realpath(__file__))
-
-
-def _get_first_ip_addr_by_sock():
-    sock = socket.socket(socket.AF_INET, socket.SOCK_STREAM)
-    sock.connect(('baidu.com', 80))
-    ip_addr = sock.getsockname()[0]
-    return ip_addr
-   
-def _get_first_ip_addr_by_http():
-    conn = httplib.HTTPConnection(host='ifconfig.me', port=80, timeout=3.0)
-    conn.request('GET', '/ip')
-    resp = conn.getresponse()
-    ip_addr = resp.read().strip()
-    return ip_addr
-  
-def get_first_ip_addr():
-    try:
-        return _get_first_ip_addr_by_http()
-    except Exception:
-        return _get_first_ip_addr_by_sock()
-
-
-# this function copy from Python 2.7 subprocess.py::check_output
-def func_check_output(*popenargs, **kwargs):
-    r"""Run command with arguments and return its output as a byte string.
-
-    If the exit code was non-zero it raises a CalledProcessError.  The
-    CalledProcessError object will have the return code in the returncode
-    attribute and output in the output attribute.
-
-    The arguments are the same as for the Popen constructor.  Example:
-
-    >>> check_output(["ls", "-l", "/dev/null"])
-    'crw-rw-rw- 1 root root 1, 3 Oct 18  2007 /dev/null\n'
-
-    The stdout argument is not allowed as it is used internally.
-    To capture standard error in the result, use stderr=STDOUT.
-
-    >>> check_output(["/bin/sh", "-c",
-    ...               "ls -l non_existent_file ; exit 0"],
-    ...              stderr=STDOUT)
-    'ls: non_existent_file: No such file or directory\n'
-    """
-    if 'stdout' in kwargs:
-        raise ValueError('stdout argument not allowed, it will be overridden.')
-    process = Popen(stdout=PIPE, *popenargs, **kwargs)
-    output, unused_err = process.communicate()
-    retcode = process.poll()
-    if retcode:
-        cmd = kwargs.get("args")
-        if cmd is None:
-            cmd = popenargs[0]
-        raise CalledProcessError(retcode, cmd, output=output)
-    return output
-
-
-def check_output(*popenargs, **kwargs):
-    if sys.version_info[0] == 2 and sys.version_info[1] < 7:
-        _check_output = func_check_output
-    else:
-        _check_output = subprocess.check_output
-
-    return _check_output(*popenargs, **kwargs)
-
-def check_output_wrapper(s):
-    return check_output(s, shell=True).strip()
-
-"""
-Usage: 
-s = "ps aux | grep redis-server | grep -v 'grep' | awk '{print $NF}'"
-print check_output(s, shell=True)
-print check_output_wrapper(s)
-"""
-
-
-def get_redis_log_full_path_list():
-    log_full_path_list = set()
-    ls_rds_inst = "ps aux | grep redis-server | grep -v 'grep' | awk '{print $NF}'"
-    
-    for config_path in check_output_wrapper(ls_rds_inst).split():
-        if not os.path.exists(config_path):
-           sys.stderr.write('[redis] %s not exists or not a absolute path \n' % config_path)
-           continue
-
-        with open(config_path) as f:
-            for line in f.readlines():
-                if line.startswith('logfile'):
-                    splits = line.split()
-                    if len(splits) == 2:
-                        key, val = splits[0], splits[1].strip()
-                        if os.path.exists(val):
-                            log_full_path_list.add(val)
-    return log_full_path_list
-
-def get_php_fpm_log_full_path_list():
-    error_log_full_path_list = set()
-    slow_log_full_path_list = set()
-
-    get_config_abs_path = "ps aux |grep php-fpm |grep master | awk '{print $NF}' | tr -d '()'"
-
-    for config_path in check_output_wrapper(get_config_abs_path).split():
-        config_path = config_path.strip().strip('"').strip("'")
-        if not config_path:
-            continue
-        if not os.path.exists(config_path):
-            sys.stderr.write('[php-fpm] %s not exits or not a absolute path \n' % config_path)
-            continue
-
-        s = file(config_path).read()
-        pool_name_list = [i for i in re.findall('^\[(?P<pool_name>\w+)\]', s, re.MULTILINE)
-                    if i != 'global']
-        if len(pool_name_list) != 1:
-            sys.stderr.write("[php-fpm] %s php-fpm log detector doesn't supports multiple pool \n" % config_path)
-            continue
-        pool_name = pool_name_list[0]
-
-        with open(config_path) as f:
-            for line in f.readlines():
-                if line.startswith('error_log'):
-                    splits = line.split('=')
-                    error_log = splits[-1].strip().strip(';').replace('$pool',  pool_name)
-                    if not os.path.exists(error_log):
-                        sys.stderr.write('[php-fpm] %s not exits or not a absolute path \n' % error_log)
-                        continue
-                    error_log_full_path_list.add(error_log)
-
-                if line.startswith('slowlog'):
-                    splits = line.split('=')
-                    slow_log = splits[-1].strip().strip(';').replace('$pool', pool_name)
-                    if not os.path.exists(slow_log):
-                        sys.stderr.write('[php-fpm] %s not exits or not a absolute path \n' % slow_log)
-                        continue
-                    slow_log_full_path_list.add(slow_log)
-
-    return error_log_full_path_list, slow_log_full_path_list
-
-
-def get_mysql_log_full_path_list():
-    error_list = set()
-    slow_list = set()
-
-    for pid in check_output_wrapper('pidof mysqld').split():
-        pid = pid.strip()
-        meta = {
-            'config-file': None,
-            'error-log': None,
-            'slow-log': None,
-            }
-        for line in check_output_wrapper('ps -p %s -f | grep %s' % (pid, pid)).split():
-            line = line.strip().replace('_', '-')
-            if line.startswith("--defaults-file"):
-                meta['config-file'] = line.replace('--defaults-file=', '')
-            elif line.startswith('--log-error'):
-                meta['error-log'] = line.replace('--log-error=', '')
-            elif line.startswith('--slow-query-log-file'):
-                meta['slow-log'] = line.replace('--slow-query-log-file=', '')
-
-        if meta['config-file']:
-            with open(meta['config-file']) as f:
-                for line in f.readlines():
-                    line = line.replace('_', '-')
-                    if line.startswith('slow-query-log-file'):
-                        meta['slow-log'] = line.replace('slow-query-log-file', '').replace('=', '').strip()
-                    elif line.startswith('log-error'):
-                        meta['error-log'] = line.replace('error-log', '').replace('=', '').strip()                    
-
-        if meta['slow-log']:
-            slow_list.add(meta['slow-log'])
-        if meta['error-log']:
-            error_list.add(meta['error-log'])
-            
-    return list(error_list), list(slow_list)
-
-
-TEMPLATE_INPUT_FILE = """  file {{
-    charset => 'UTF-8'
-    type => '{logstash_type}'
-    path => '{file_path}'
-    format => 'plain'
-  }}
-"""
-
-def generte_input_file_block(file_path, logstash_type):
-     return TEMPLATE_INPUT_FILE.format(
-          logstash_type=logstash_type,
-          file_path=file_path, 
-          )
-
-CONFIG_TEMPLATE_FILTER_PHP = """  multiline {{
-    type => 'php-error'
-    pattern => '^(\s|#|Stack)'
-    what => 'previous'
-  }}
-
-  multiline {{
-    type => 'php-fpm-slow'
-    pattern => '^$'
-    what => 'previous'
-    negate => true
-  }}
-
-  grok {{
-    type => 'php-error'
-    patterns_dir => '{patterns_dir}'
-    pattern => '%{{PHP_ERROR_LOG}}'
-    singles => true
-  }}
-
-  grok {{
-    type => 'php-fpm-error'
-    patterns_dir => '{patterns_dir}'
-    pattern => '%{{PHP_FPM_ERROR_LOG}}'
-    singles => true
-  }}
-
-  grok {{
-    type => 'php-fpm-slow'
-    patterns_dir => '{patterns_dir}'
-    pattern => '%{{PHP_FPM_SLOW_LOG}}'
-    singles => true
-  }}
-"""
-
-CONFIG_TEMPLATE_INPUTS = """input {{
-  stdin {{
-    type => 'stdin-type'
-  }}
-
-  tcp {{
-    type => 'test-pattern'
-    host => '127.0.0.1'
-    port => 9100
-    mode => server
-    debug => true
-    format => plain
-  }}
-
-{input_blocks}
-
-}}
-
-"""
-
-CONFIG_TEMPLATE_FILTERS_PREFIX = """filter {{
-"""
-CONFIG_TEMPLATE_FILTERS_SUFFIX = """  date {{
-    match => ['timestamp', 'dd-MMM-YYYY HH:mm:ss z', 'dd-MMM-YYYY HH:mm:ss']
-  }}
-
-  mutate {{
-    replace => ["@source_host", "DEFAULT_SOURCE_HOST"]
-  }}
-
-}}
-
-"""
-
-CONFIG_TEMPLATE_FILTER_MYSQL = """  multiline {{
-    type => 'mysql-slow'
-    pattern => "^# User@Host: "
-    negate => true
-    what => previous
-  }}
-
-  multiline {{
-    type => 'mysql-error'
-    what => previous
-    pattern => '^\s'
-  }}
-
-  grok {{
-    type => 'mysql-error'
-    patterns_dir => '{patterns_dir}'
-    pattern => '%{{MYSQL_ERROR_LOG}}'
-  }}
-
-  grep {{
-    type => 'mysql-slow'
-    match => [ "@message", "^# Time: " ]
-    negate => true
-  }}
-
-  grok {{
-    type => 'mysql-slow'
-    singles => true
-    patterns_dir => '{patterns_dir}'
-    pattern => [
-      "%{{MYSQL_SLOW_FROM}}",
-      "%{{MYSQL_SLOW_STAT}}",
-      "%{{MYSQL_SLOW_TIMESTAMP}}",
-      "%{{MYSQL_SLOW_DB}}",
-      "%{{MYSQL_SLOW_QUERY}}"
-     ]
-   }}
-
-  date {{
-    type => 'mysql-slow'
-    match => ['timestamp', 'YYddMM HH:mm:ss']
-  }}
-
-  mutate {{
-    type => 'mysql-slow'
-    remove => "timestamp"
-  }}
-"""
-
-CONFIG_TEMPLATE_FILTER_REDIS = """  grok {{
-    type => 'redis'
-    patterns_dir => '{patterns_dir}'
-    pattern => '%{{REDISLOG_FIXED}}'
-  }}
-"""
-
-
-CONFIG_TEMPLATE_OUTPUTS = """output {{
-#  stdout {{
-#    debug => true
-#    debug_format => "json"
-#  }}
-
-  redis {{
-    host => "{output_redis_host}"
-    port => {output_redis_port}
-    data_type => "list"
-    key => "logstash"
-  }}
-
-{output_blocks}
-}}
-"""
-
-def main():
-    output_redis_host = '10.20.60.85'
-    output_redis_port = 6380
-    patterns_dir = '/usr/local/logstash/patterns'
-
-
-    chunks = []
-    for path in get_redis_log_full_path_list():
-        sys.stdout.write("%s %s found \n" % ("redis", path))
-        chunks.append(generte_input_file_block(path, "redis"))
-
-    error_list, slow_list = get_php_fpm_log_full_path_list()
-    for path in error_list:
-        sys.stdout.write("%s %s found \n" % ("php-fpm-error", path))
-        chunks.append(generte_input_file_block(path, "php-fpm-error"))
-    for path in slow_list:
-        sys.stdout.write("%s %s found \n" % ("php-fpm-slow", path))
-        chunks.append(generte_input_file_block(path, "php-fpm-slow"))
-
-    error_list, slow_list = get_mysql_log_full_path_list()
-    for path in error_list:
-        sys.stdout.write("%s %s found \n" % ("mysql-error", path))
-        chunks.append(generte_input_file_block(path, "mysql-error"))
-    for path in slow_list:
-        sys.stdout.write("%s %s found \n" % ("mysql-slow", path))
-        chunks.append(generte_input_file_block(path, "mysql-slow"))    
-    input_blocks = '\n'.join(chunks)
-
-    t = CONFIG_TEMPLATE_INPUTS + \
-        CONFIG_TEMPLATE_FILTERS_PREFIX + \
-        CONFIG_TEMPLATE_FILTER_REDIS + \
-        '\n' + \
-        CONFIG_TEMPLATE_FILTER_MYSQL + \
-        '\n' + \
-        CONFIG_TEMPLATE_FILTER_PHP + \
-        CONFIG_TEMPLATE_FILTERS_SUFFIX + \
-        CONFIG_TEMPLATE_OUTPUTS
-
-    output_blocks = ""
-    content = t.format(
-         input_blocks=input_blocks, 
-        output_blocks=output_blocks,
-        patterns_dir=patterns_dir,
-         output_redis_host=output_redis_host,
-         output_redis_port=output_redis_port,
-         )
-
-    ip_addr = None
-    try:
-        ip_addr = get_first_ip_addr()
-    except Exception:
-        pass
-    if ip_addr:
-        content = content.replace("DEFAULT_SOURCE_HOST", ip_addr)
-
-    FOLDER_PARENT = os.path.dirname(PWD)
-    save_to_prefix = os.path.join(FOLDER_PARENT, "conf")
-    save_to = os.path.join(save_to_prefix, "shipper-dev.conf")
-    
-    # save_to = os.path.join(PWD, 'shipper.conf')
-    
-    with open(save_to, 'w') as f:
-         f.write(content)
-    sys.stdout.write("save to %s \n" % save_to)
-
-if __name__ == '__main__':
-    main()
diff --git a/require-analyze.rb b/require-analyze.rb
new file mode 100644
index 00000000000..f69d858aa45
--- /dev/null
+++ b/require-analyze.rb
@@ -0,0 +1,22 @@
+require "csv"
+
+#0.003,psych/nodes/mapping,/Users/jls/.rvm/rubies/jruby-1.7.8/lib/ruby/shared/psych/nodes.rb:6:in `(root)'
+
+durations = {}
+durations.default = 0
+
+CSV.foreach(ARGV[0]) do |duration, path, source|
+  source, line, where = source.split(":")
+  #{"0.002"=>"/Users/jls/projects/logstash/vendor/bundle/jruby/1.9/gems/clamp-0.6.3/lib/clamp.rb"}
+  if source.include?("jruby/1.9/gems")
+    # Get the gem name
+    source = source.gsub(/.*\/jruby\/1.9\/gems/, "")[/[^\/]+/]
+  elsif source.include?("/lib/logstash/")
+    source = source.gsub(/^.*(\/lib\/logstash\/)/, "/lib/logstash/")
+  end
+  durations[source] += duration.to_f
+end
+
+durations.sort_by { |k,v| v }.each do |k,v| 
+  puts "#{v} #{k}"
+end
