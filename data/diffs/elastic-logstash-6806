diff --git a/CHANGELOG.md b/CHANGELOG.md
index 5f8bf0bb62c..a0bba3fe06b 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -140,7 +140,7 @@ pre-releases.
   - Lumberjack: This input was not handling backpressure properly from downstream plugins and
     would continue to accept data, eventually running out of memory. We added a circuit breaker to stop
     accepting new connections when we detect this situation. Please note that `max_clients` setting 
-    intoduced in v0.1.9 has been deprecated. This setting temporarily solved the problem by configuring
+    introduced in v0.1.9 has been deprecated. This setting temporarily solved the problem by configuring
     an upper limit to the number of LSF connections (#12).
   - Http: Added new input to receive data via http(s).
   - File: Fixed a critical bug where new files being added to a dir being watched would crash LS.
@@ -263,7 +263,7 @@ pre-releases.
     - Added IAM roles support so you can securely read and write events from S3 without providing your
       AWS credentials (#1575). 
     - Added support for using temporary credentials obtained from AWS STS (#1946)
-    - AWS credentials can be specfied through environment variables (#1619)  
+    - AWS credentials can be specified through environment variables (#1619)  
   - RabbitMQ: 
     - Fixed march_hare client uses incorrect connection url (LOGSTASH-2276)
     - Use Bunny 1.5.0+ (#1894)
@@ -326,7 +326,7 @@ pre-releases.
       the ability to capture failed requests from Elasticsearch and retry them. Error codes like 
       429 (too many requests) will now be retried by default for 3 times. The number of retries and the
       interval between consecutive retries can be configured (#1631)
-    - Logstash does not create a "message.raw" by default whic is usually not_analyzed; this
+    - Logstash does not create a "message.raw" by default which is usually not_analyzed; this
       helps save disk space (#11)
     - Added sniffing config to be able to list machines in the cluster while using the transport client (#22) 
     - Deprecate the usage of index_type configuration. Added document_type to be consistent
@@ -373,7 +373,7 @@ pre-releases.
   - bugfix: file: fixed debian 7 path issue
 
 ### codecs
-  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming imputs
+  - improvement: stdin/tcp: automatically select json_line and line codecs with the tcp and stdin streaming inputs
   - improvement: collectd: add support for NaN values
 
 ### outputs
@@ -522,7 +522,7 @@ pre-releases.
 ### outputs
   - bugfix: elasticsearch: flush any buffered events on logstash shutdown
     (#1175)
-  - feature: riemann: Automatically map event fields to rieman event fields
+  - feature: riemann: Automatically map event fields to riemann event fields
     (logstash-contrib#15, Byron Pezan)
   - bugfix: lumberjack: fix off-by-one errors causing writes to another
     logstash agent to block indefinitely
@@ -789,7 +789,7 @@ pre-releases.
     (LOGSTASH-1423, #692, #739; Jordan Sissel, Bernd Ahlers)
 
 ### patterns
-  - improvement: added IPV6 suppot to IP pattern (#623)
+  - improvement: added IPV6 support to IP pattern (#623)
 
 ## 1.2.1 (September 7, 2013)
 ### general
@@ -1172,7 +1172,7 @@ pre-releases.
    matched. (LOGSTASH-705)
  - feature: kv: Adds field_split, value_split, prefix, and container
    settings. (#225, patch by Alex Wheeler)
- - bugfix: mutate: rename on a nonexistant field now does nothing as expected.
+ - bugfix: mutate: rename on a nonexistent field now does nothing as expected.
    (LOGSTASH-757)
  - bugfix: grok: don't tag an event with _grokparsefailure if it's already so
    (#248, patch by Greg Brockman)
@@ -1228,7 +1228,7 @@ pre-releases.
    Gemfile will be purged in the near future.
  - amqp plugins are now marked 'unsupported' as there is no active maintainer
    nor is there source of active support in the community. If you're interested
-   in maintainership, please email the mailling list or contact Jordan!
+   in maintainership, please email the mailing list or contact Jordan!
 
 ### inputs
  - irc: now stores irc nick
diff --git a/bin/logstash b/bin/logstash
index 3fa98ba66ce..f899a2839e3 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -11,7 +11,7 @@
 # Supported environment variables:
 #   LS_JVM_OPTS="xxx" path to file with JVM options
 #   LS_JAVA_OPTS="xxx" to append extra options to the defaults JAVA_OPTS provided by logstash
-#   JAVA_OPTS="xxx" to *completely override* the defauls set of JAVA_OPTS provided by logstash
+#   JAVA_OPTS="xxx" to *completely override* the default set of JAVA_OPTS provided by logstash
 #
 # Development environment variables:
 #   USE_RUBY=1 to force use the local "ruby" command to launch logstash instead of using the vendored JRuby
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index d070f6c6fa5..3fb6e39d8e8 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -157,7 +157,7 @@ setup_ruby() {
 
 setup() {
   # first check if we want to use drip, which can be used in vendored jruby mode
-  # and also when setting USE_RUBY=1 if the ruby interpretor is in fact jruby
+  # and also when setting USE_RUBY=1 if the ruby interpreter is in fact jruby
   if [ "$JAVACMD" ] ; then
     if [ "$(basename $JAVACMD)" = "drip" ] ; then
       DRIP_JAVACMD=1
diff --git a/bin/ruby b/bin/ruby
index ab7396423fc..b1fd647ce26 100755
--- a/bin/ruby
+++ b/bin/ruby
@@ -7,7 +7,7 @@
 # Supported environment variables:
 #   LS_JVM_OPTS="xxx" path to file with JVM options
 #   LS_JAVA_OPTS="xxx" to append extra options to the defaults JAVA_OPTS provided by logstash
-#   JAVA_OPTS="xxx" to *completely override* the defauls set of JAVA_OPTS provided by logstash
+#   JAVA_OPTS="xxx" to *completely override* the default set of JAVA_OPTS provided by logstash
 #
 # Development environment variables:
 #   USE_RUBY=1 to force use the local "ruby" command to launch logstash instead of using the vendored JRuby
diff --git a/ci/ci_setup.sh b/ci/ci_setup.sh
index 887225c96cb..9974055658d 100755
--- a/ci/ci_setup.sh
+++ b/ci/ci_setup.sh
@@ -3,7 +3,7 @@ set -e
 
 ##
 # Note this setup needs a system ruby to be available, this can not
-# be done here as is higly system dependant.
+# be done here as is highly system dependant.
 ##
 
 #squid proxy work, so if there is a proxy it can be cached.
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index ebef1ac9213..4ee6ee8e5c9 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -55,7 +55,7 @@ to support installing or manipulating Logstash, such as via Puppet.
 ==== Default Logging Level
 
 The default log severity level changed to `INFO` instead of `WARN` to match Elasticsearch. Existing logs
-(in core and plugins) were too noisy at the `INFO` level, so we auditted our log messages and switched some of them to
+(in core and plugins) were too noisy at the `INFO` level, so we audited our log messages and switched some of them to
 `DEBUG` level.
 
 You can use the new `logstash.yml` file to configure the `log.level` setting or continue to pass the new
@@ -197,7 +197,7 @@ that flexibility can cause issues when handling upgrades.
 Non-standard plugins must always be checked for compatibility, but some bundled plugins are upgraded in order to remain 
 compatible with the tools or frameworks that they use for communication. For example, the
 <<plugins-inputs-kafka, Kafka Input>> and <<plugins-outputs-kafka, Kafka Output>> plugins serve as a primary example of 
-such compatibilty changes. The latest version of the Kafka plugins is only compatible with Kafka 0.10, but as the 
+such compatibility changes. The latest version of the Kafka plugins is only compatible with Kafka 0.10, but as the 
 compatibility matrices show: earlier plugin versions are required for earlier versions of Kafka (e.g., Kafka 0.9).
 
 Automatic upgrades generally lead to improved features and support, but network layer changes like those above may make part
diff --git a/docs/static/contributing-patch.asciidoc b/docs/static/contributing-patch.asciidoc
index 470841574f2..c126e0bdc1f 100644
--- a/docs/static/contributing-patch.asciidoc
+++ b/docs/static/contributing-patch.asciidoc
@@ -15,7 +15,7 @@ Plugins are subclasses of a Logstash base class. A plugin's base class defines c
 ==== Input Plugins
 
 Input plugins ingest data from an external source. Input plugins are always associated with a codec. An input plugin 
-always has an associated codec plugin. Input and codec plugins operate in conjuction to create a Logstash event and add 
+always has an associated codec plugin. Input and codec plugins operate in conjunction to create a Logstash event and add 
 that event to the processing queue. An input codec is a subclass of the `LogStash::Inputs::Base` class.
 
 .Input API
@@ -276,7 +276,7 @@ describe LogStash::Outputs::ZeroMQ do
   end
 end
 
-. To add the missing test, use an instance of the ZeroMQ output and a substitute logger. This examle uses an RSpec feature 
+. To add the missing test, use an instance of the ZeroMQ output and a substitute logger. This example uses an RSpec feature 
 called _test doubles_ as the substitute logger.
 +
 Add the following lines to `zeromq_spec.rb`, after `describe LogStash::Outputs::ZeroMQ do` and before `context "when in 
@@ -326,7 +326,7 @@ end
 To run this test:
 
 . Open a terminal window
-. Mavigate to the cloned plugin folder
+. Navigate to the cloned plugin folder
 . The first time you run the test, run the command `bundle install`
 . Run the command `bundle exec rspec`
 
diff --git a/docs/static/event-api.asciidoc b/docs/static/event-api.asciidoc
index 39ac6d77767..2311e2b7e0c 100644
--- a/docs/static/event-api.asciidoc
+++ b/docs/static/event-api.asciidoc
@@ -50,7 +50,7 @@ event.get("[foo][bar]") # => {"a" => 1, "b" => 2}
 event.get("[foo][bar]") # =>  {"a" => 1, "b" => 2, "c" => [1, 2]}
 --------------------------------------------------
 
-Accessing @metdata
+Accessing @metadata
 
 [source,ruby]
 --------------------------------------------------
diff --git a/docs/static/include/pluginbody.asciidoc b/docs/static/include/pluginbody.asciidoc
index 0dc2260a923..88e14779f3d 100644
--- a/docs/static/include/pluginbody.asciidoc
+++ b/docs/static/include/pluginbody.asciidoc
@@ -874,7 +874,7 @@ end
 ----------------------------------
 
 It is appropriate to change these values to fit your plugin. In particular,
-`s.name` and `s.summary` shoud reflect your plugin's name and behavior.
+`s.name` and `s.summary` should reflect your plugin's name and behavior.
 
 `s.licenses` and `s.version` are also important and will come into play when
 you are ready to publish your plugin.
diff --git a/lib/bootstrap/bundler.rb b/lib/bootstrap/bundler.rb
index c90cd67488a..2fc69e31067 100644
--- a/lib/bootstrap/bundler.rb
+++ b/lib/bootstrap/bundler.rb
@@ -31,7 +31,7 @@ def set_key(key, value, hash, file)
       # This patch makes rubygems fetch directly from the remote servers
       # the dependencies he need and might not have downloaded in a local
       # repository. This basically enabled the offline feature to work as
-      # we remove the gems from the vendor directory before packacing.
+      # we remove the gems from the vendor directory before packaging.
       ::Bundler::Source::Rubygems.module_exec do
         def cached_gem(spec)
           cached_built_in_gem(spec)
diff --git a/lib/pluginmanager/custom_gem_indexer.rb b/lib/pluginmanager/custom_gem_indexer.rb
index 0d7d273de2f..1bfd86e241f 100644
--- a/lib/pluginmanager/custom_gem_indexer.rb
+++ b/lib/pluginmanager/custom_gem_indexer.rb
@@ -24,7 +24,7 @@ def copy_to_local_source(temporary_directory)
         local_source
       end
 
-      # *WARNING*: Bundler need to not be activated at this point because it wont find anything that
+      # *WARNING*: Bundler need to not be activated at this point because it won't find anything that
       # is not defined in the gemfile/lock combo
       #
       # This takes a folder with a special structure, will generate an index
diff --git a/lib/pluginmanager/offline_plugin_packager.rb b/lib/pluginmanager/offline_plugin_packager.rb
index 13f8e4c8b02..c4cd6cf24d6 100644
--- a/lib/pluginmanager/offline_plugin_packager.rb
+++ b/lib/pluginmanager/offline_plugin_packager.rb
@@ -28,8 +28,8 @@ class OfflinePluginPackager
     LOGSTASH_DIR = "logstash"
     DEPENDENCIES_DIR = ::File.join(LOGSTASH_DIR, "dependencies")
 
-    # To make sure we have the maximun compatibility
-    # we will ignore theses gems and they wont be included in the pack
+    # To make sure we have the maximum compatibility
+    # we will ignore theses gems and they won't be included in the pack
     IGNORE_GEMS_IN_PACK = %w(
       logstash-core
       logstash-core-plugin-api
diff --git a/lib/pluginmanager/pack_installer/local.rb b/lib/pluginmanager/pack_installer/local.rb
index ff75b971026..8912cf9f76d 100644
--- a/lib/pluginmanager/pack_installer/local.rb
+++ b/lib/pluginmanager/pack_installer/local.rb
@@ -36,7 +36,7 @@ def execute
       # minus the support for additionals source and doing local resolution only.
       ::Bundler::LogstashInjector.inject!(pack)
 
-      # When successfull its safe to install the gem and their specifications in the bundle directory
+      # When successful its safe to install the gem and their specifications in the bundle directory
       pack.gems.each do |packed_gem|
         PluginManager.ui.debug("Installing, #{packed_gem.name}, version: #{packed_gem.version} file: #{packed_gem.file}")
         LogStash::PluginManager::GemInstaller::install(packed_gem.file, packed_gem.plugin?)
diff --git a/lib/pluginmanager/update.rb b/lib/pluginmanager/update.rb
index d315ce560af..583b6c954e2 100644
--- a/lib/pluginmanager/update.rb
+++ b/lib/pluginmanager/update.rb
@@ -46,7 +46,7 @@ def update_gems!
     previous_gem_specs_map = find_latest_gem_specs
 
     # remove any version constrain from the Gemfile so the plugin(s) can be updated to latest version
-    # calling update without requiremend will remove any previous requirements
+    # calling update without requirements will remove any previous requirements
     plugins = plugins_to_update(previous_gem_specs_map)
     # Skipping the major version validation when using a local cache as we can have situations
     # without internet connection.
diff --git a/lib/pluginmanager/util.rb b/lib/pluginmanager/util.rb
index debd709d256..a521f2bac6a 100644
--- a/lib/pluginmanager/util.rb
+++ b/lib/pluginmanager/util.rb
@@ -77,7 +77,7 @@ def self.plugin_file?(plugin)
   end
 
   # retrieve gem specs for all or specified name valid logstash plugins locally installed
-  # @param name [String] specific plugin name to find or nil for all plungins
+  # @param name [String] specific plugin name to find or nil for all plugins
   # @return [Array<Gem::Specification>] all local logstash plugin gem specs
   def self.find_plugins_gem_specs(name = nil)
     specs = name ? Gem::Specification.find_all_by_name(name) : Gem::Specification.find_all
@@ -85,7 +85,7 @@ def self.find_plugins_gem_specs(name = nil)
   end
 
   # list of all locally installed plugins specs specified in the Gemfile.
-  # note that an installed plugin dependecies like codecs will not be listed, only those
+  # note that an installed plugin dependencies like codecs will not be listed, only those
   # specifically listed in the Gemfile.
   # @param gemfile [LogStash::Gemfile] the gemfile to validate against
   # @return [Array<Gem::Specification>] list of plugin specs
@@ -97,13 +97,13 @@ def self.all_installed_plugins_gem_specs(gemfile)
 
   # @param plugin [String] plugin name
   # @param gemfile [LogStash::Gemfile] the gemfile to validate against
-  # @return [Boolean] true if the plugin is an installed logstash plugin and spefificed in the Gemfile
+  # @return [Boolean] true if the plugin is an installed logstash plugin and specified in the Gemfile
   def self.installed_plugin?(plugin, gemfile)
     !!gemfile.find(plugin) && find_plugins_gem_specs(plugin).any?
   end
 
   # @param plugin_list [Array] array of [plugin name, version] tuples
-  # @return [Array] array of [plugin name, version, ...] tuples when duplciate names have been merged and non duplicate version requirements added
+  # @return [Array] array of [plugin name, version, ...] tuples when duplicate names have been merged and non duplicate version requirements added
   def self.merge_duplicates(plugin_list)
 
     # quick & dirty naive dedup for now
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 61725af198c..b367dba5eec 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -61,7 +61,7 @@ def initialize(settings = LogStash::SETTINGS)
   end
 
   def execute
-    @thread = Thread.current # this var is implicilty used by Stud.stop?
+    @thread = Thread.current # this var is implicitly used by Stud.stop?
     @logger.debug("starting agent")
 
     start_pipelines
@@ -298,7 +298,7 @@ def reload_pipeline!(id)
     end
 
     # check if this pipeline is not reloadable. it should not happen as per the check below
-    # but keep it here as a safety net if a reloadable pipeline was releoaded with a non reloadable pipeline
+    # but keep it here as a safety net if a reloadable pipeline was reloaded with a non reloadable pipeline
     if !old_pipeline.reloadable?
       @logger.error("pipeline is not reloadable", :pipeline => id)
       return
@@ -370,7 +370,7 @@ def upgrade_pipeline(pipeline_id, settings, new_config)
       return
     end
 
-    # pipeline started successfuly, update reload success metrics
+    # pipeline started successfully, update reload success metrics
     @instance_reload_metric.increment(:successes)
     @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
       n.increment(:successes)
@@ -424,7 +424,7 @@ def start_pipelines
     @pipelines.each do |id, pipeline|
       start_pipeline(id)
       pipeline.collect_stats
-      # no reloads yet, initalize all the reload metrics
+      # no reloads yet, initialize all the reload metrics
       init_pipeline_reload_metrics(id)
     end
   end
diff --git a/logstash-core/lib/logstash/api/modules/base.rb b/logstash-core/lib/logstash/api/modules/base.rb
index 9e84dc91811..d8ad5e0c7f9 100644
--- a/logstash-core/lib/logstash/api/modules/base.rb
+++ b/logstash-core/lib/logstash/api/modules/base.rb
@@ -31,7 +31,7 @@ def initialize(app=nil, agent)
         end
 
         not_found do
-          # We cannot raise here because it wont be catched by the `error` handler.
+          # We cannot raise here because it won't be catched by the `error` handler.
           # So we manually create a new instance of NotFound and just pass it down.
           respond_with(NotFoundError.new)
         end
diff --git a/logstash-core/lib/logstash/api/rack_app.rb b/logstash-core/lib/logstash/api/rack_app.rb
index 79965e431ed..8b2d1905559 100644
--- a/logstash-core/lib/logstash/api/rack_app.rb
+++ b/logstash-core/lib/logstash/api/rack_app.rb
@@ -80,7 +80,7 @@ def self.app(logger, agent, environment)
           # Custom logger object. Rack CommonLogger does not work with cabin
           use ApiLogger, logger
 
-          # In test env we want errors to propogate up the chain
+          # In test env we want errors to propagate up the chain
           # so we get easy to understand test failures.
           # In production / dev we don't want a bad API endpoint
           # to crash the process
diff --git a/logstash-core/lib/logstash/config/config_ast.rb b/logstash-core/lib/logstash/config/config_ast.rb
index ce6295a6933..16f47fc1680 100644
--- a/logstash-core/lib/logstash/config/config_ast.rb
+++ b/logstash-core/lib/logstash/config/config_ast.rb
@@ -60,20 +60,20 @@ def recursive_select_parent(results=[], klass)
 
 module LogStash; module Config; module AST
 
-  def self.defered_conditionals=(val)
-    @defered_conditionals = val
+  def self.deferred_conditionals=(val)
+    @deferred_conditionals = val
   end
 
-  def self.defered_conditionals
-    @defered_conditionals
+  def self.deferred_conditionals
+    @deferred_conditionals
   end
 
-  def self.defered_conditionals_index
-    @defered_conditionals_index
+  def self.deferred_conditionals_index
+    @deferred_conditionals_index
   end
 
-  def self.defered_conditionals_index=(val)
-    @defered_conditionals_index = val
+  def self.deferred_conditionals_index=(val)
+    @deferred_conditionals_index = val
   end
 
   def self.plugin_instance_index
@@ -92,8 +92,8 @@ def text_value_for_comments
 
   class Config < Node
     def compile
-      LogStash::Config::AST.defered_conditionals = []
-      LogStash::Config::AST.defered_conditionals_index = 0
+      LogStash::Config::AST.deferred_conditionals = []
+      LogStash::Config::AST.deferred_conditionals_index = 0
       LogStash::Config::AST.plugin_instance_index = 0
       code = []
 
@@ -136,7 +136,7 @@ def compile
 
       code += definitions.join("\n").split("\n", -1).collect { |l| "  #{l}" }
 
-      code += LogStash::Config::AST.defered_conditionals
+      code += LogStash::Config::AST.deferred_conditionals
 
       return code.join("\n")
     end
@@ -402,7 +402,7 @@ def compile
       type = recursive_select_parent(PluginSection).first.plugin_type.text_value
 
       if type == "filter"
-        i = LogStash::Config::AST.defered_conditionals_index += 1
+        i = LogStash::Config::AST.deferred_conditionals_index += 1
         source = <<-CODE
           @generated_objects[:cond_func_#{i}] = lambda do |input_events|
             result = []
@@ -415,7 +415,7 @@ def compile
             result
           end
         CODE
-        LogStash::Config::AST.defered_conditionals << source
+        LogStash::Config::AST.deferred_conditionals << source
 
         <<-CODE
           events = @generated_objects[:cond_func_#{i}].call(events)
diff --git a/logstash-core/lib/logstash/config/mixin.rb b/logstash-core/lib/logstash/config/mixin.rb
index 768a9bcc759..a22074b7b59 100644
--- a/logstash-core/lib/logstash/config/mixin.rb
+++ b/logstash-core/lib/logstash/config/mixin.rb
@@ -373,7 +373,7 @@ def process_parameter_value(value, config_settings)
     end
 
     def validate_check_parameter_values(params)
-      # Filter out parametrs that match regexp keys.
+      # Filter out parameters that match regexp keys.
       # These are defined in plugins like this:
       #   config /foo.*/ => ...
       all_params_valid = true
@@ -438,7 +438,7 @@ def validate_value(value, validator)
         end
         result = value.first
       elsif validator.is_a?(Symbol)
-        # TODO(sissel): Factor this out into a coersion method?
+        # TODO(sissel): Factor this out into a coercion method?
         # TODO(sissel): Document this stuff.
         value = hash_or_array(value)
 
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index c17bf0e657e..8f5f9d33380 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -59,7 +59,7 @@ module Environment
   SETTINGS.register Setting::WritableDirectory.new("path.queue", default_queue_file_path)
   
   SETTINGS.on_post_process do |settings|
-    # If the data path is overriden but the queue path isn't recompute the queue path
+    # If the data path is overridden but the queue path isn't recompute the queue path
     # We need to do this at this stage because of the weird execution order
     # our monkey-patched Clamp follows
     if settings.set?("path.data") && !settings.set?("path.queue")
diff --git a/logstash-core/lib/logstash/event.rb b/logstash-core/lib/logstash/event.rb
index 90bdb3b3fe0..6e892d27826 100644
--- a/logstash-core/lib/logstash/event.rb
+++ b/logstash-core/lib/logstash/event.rb
@@ -7,10 +7,10 @@
 require "logstash/timestamp"
 require "logstash/string_interpolation"
 
-# transcient pipeline events for normal in-flow signaling as opposed to
+# transient pipeline events for normal in-flow signaling as opposed to
 # flow altering exceptions. for now having base classes is adequate and
 # in the future it might be necessary to refactor using like a BaseEvent
-# class to have a common interface for all pileline events to support
+# class to have a common interface for all pipeline events to support
 # eventual queueing persistence for example, TBD.
 module LogStash
   class SignalEvent
diff --git a/logstash-core/lib/logstash/filter_delegator.rb b/logstash-core/lib/logstash/filter_delegator.rb
index e5adee779c1..092f01c2542 100644
--- a/logstash-core/lib/logstash/filter_delegator.rb
+++ b/logstash-core/lib/logstash/filter_delegator.rb
@@ -42,7 +42,7 @@ def multi_filter(events)
       new_events = @filter.multi_filter(events)
       clock.stop
 
-      # There is no garantee in the context of filter
+      # There is no guarantee in the context of filter
       # that EVENTS_INT == EVENTS_OUT, see the aggregates and
       # the split filter
       c = new_events.count { |event| !event.cancelled? }
diff --git a/logstash-core/lib/logstash/inputs/base.rb b/logstash-core/lib/logstash/inputs/base.rb
index ca769278893..6715aaa8cf8 100644
--- a/logstash-core/lib/logstash/inputs/base.rb
+++ b/logstash-core/lib/logstash/inputs/base.rb
@@ -89,7 +89,7 @@ def do_stop
     stop
   end
 
-  # stop? should never be overriden
+  # stop? should never be overridden
   public
   def stop?
     @stop_called.value
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
index 25ee3b7e746..08e72599f3d 100644
--- a/logstash-core/lib/logstash/instrument/collector.rb
+++ b/logstash-core/lib/logstash/instrument/collector.rb
@@ -29,7 +29,7 @@ def initialize
     # of update the metric
     #
     # If there is a problem with the key or the type of metric we will record an error
-    # but we wont stop processing events, theses errors are not considered fatal.
+    # but we won't stop processing events, theses errors are not considered fatal.
     #
     def push(namespaces_path, key, type, *metric_type_params)
       begin
diff --git a/logstash-core/lib/logstash/instrument/metric.rb b/logstash-core/lib/logstash/instrument/metric.rb
index 0f0f1b744eb..d31a8613dad 100644
--- a/logstash-core/lib/logstash/instrument/metric.rb
+++ b/logstash-core/lib/logstash/instrument/metric.rb
@@ -77,7 +77,7 @@ def self.validate_key!(key)
     private
     # Allow to calculate the execution of a block of code.
     # This class support 2 differents syntax a block or the return of
-    # the object itself, but in the later case the metric wont be recorded
+    # the object itself, but in the later case the metric won't be recorded
     # Until we call `#stop`.
     #
     # @see LogStash::Instrument::Metric#time
diff --git a/logstash-core/lib/logstash/instrument/metric_store.rb b/logstash-core/lib/logstash/instrument/metric_store.rb
index 3967cefbfb7..9393cd0ed74 100644
--- a/logstash-core/lib/logstash/instrument/metric_store.rb
+++ b/logstash-core/lib/logstash/instrument/metric_store.rb
@@ -73,7 +73,7 @@ def fetch_or_store(namespaces, key, default_value = nil)
     # If you use the `,` on a key the metric store will return the both values at that level
     #
     # The returned hash will keep the same structure as it had in the `Concurrent::Map`
-    # but will be a normal ruby hash. This will allow the api to easily seriliaze the content
+    # but will be a normal ruby hash. This will allow the api to easily serialize the content
     # of the map
     #
     # @param [Array] The path where values should be located
@@ -131,7 +131,7 @@ def get_shallow(*key_paths)
     # }
     def extract_metrics(path, *keys)
       keys.reduce({}) do |acc,k|
-        # Simplifiy 1-length keys
+        # Simplify 1-length keys
         k = k.first if k.is_a?(Array) && k.size == 1
 
         # If we have array values here we need to recurse
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb b/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb
index 23c7a3ce44b..28199b2a5cc 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb
@@ -9,7 +9,7 @@ class Cgroup
     include LogStash::Util::Loggable
 
     CONTROL_GROUP_RE = Regexp.compile("\\d+:([^:,]+(?:,[^:,]+)?):(/.*)");
-    CONTROLLER_SEPERATOR_RE = ","
+    CONTROLLER_SEPARATOR_RE = ","
 
     PROC_SELF_CGROUP_FILE = Pathname.new("/proc/self/cgroup")
     PROC_CGROUP_CPU_DIR = Pathname.new("/sys/fs/cgroup/cpu")
@@ -36,8 +36,8 @@ def control_groups
 
         read_proc_self_cgroup_lines.each do |line|
           matches = CONTROL_GROUP_RE.match(line)
-          # multiples controlles, same hierachy
-          controllers = matches[1].split(CONTROLLER_SEPERATOR_RE)
+          # multiples controls, same hierarchy
+          controllers = matches[1].split(CONTROLLER_SEPARATOR_RE)
           controllers.each_with_object(response) { |controller| response[controller] = matches[2] }
         end
 
diff --git a/logstash-core/lib/logstash/java_integration.rb b/logstash-core/lib/logstash/java_integration.rb
index 670ceaae650..fb5b9eb8c1b 100644
--- a/logstash-core/lib/logstash/java_integration.rb
+++ b/logstash-core/lib/logstash/java_integration.rb
@@ -1,7 +1,7 @@
 # encoding: utf-8
 require "java"
 
-# this is mainly for usage with JrJackson json parsing in :raw mode which genenerates
+# this is mainly for usage with JrJackson json parsing in :raw mode which generates
 # Java::JavaUtil::ArrayList and Java::JavaUtil::LinkedHashMap native objects for speed.
 # these object already quacks like their Ruby equivalents Array and Hash but they will
 # not test for is_a?(Array) or is_a?(Hash) and we do not want to include tests for
@@ -35,7 +35,7 @@ def self.===(other)
   # this bug makes has_key? (and all its aliases) return false for a key that has a nil value.
   # Only LinkedHashMap is patched here because patching the Map interface is not working.
   # TODO find proper fix, and submit upstream
-  # releavant JRuby files:
+  # relevant JRuby files:
   # https://github.com/jruby/jruby/blob/master/core/src/main/ruby/jruby/java/java_ext/java.util.rb
   # https://github.com/jruby/jruby/blob/master/core/src/main/java/org/jruby/java/proxies/MapJavaProxy.java
   def has_key?(key)
diff --git a/logstash-core/lib/logstash/patches/cabin.rb b/logstash-core/lib/logstash/patches/cabin.rb
index bb44fa2a421..5d5fc712156 100644
--- a/logstash-core/lib/logstash/patches/cabin.rb
+++ b/logstash-core/lib/logstash/patches/cabin.rb
@@ -9,7 +9,7 @@
   # Basically, the following is wastes tons of effort creating objects that are
   # never used if the log level hides the log:
   #
-  #     logger.debug("something happend", :what => Happened)
+  #     logger.debug("something happened", :what => Happened)
   #
   # This is shown to be 4x faster:
   #
diff --git a/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb b/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
index ccd43ac1f86..eeb841dd874 100644
--- a/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
+++ b/logstash-core/lib/logstash/patches/stronger_openssl_defaults.rb
@@ -54,7 +54,7 @@ def self.__default_options
   #
   # This monkeypatch doesn't enforce a `VERIFY_MODE` on the SSLContext,
   # SSLContext are both used for the client and the server implementation,
-  # If set the `verify_mode` to peer the server wont accept any connection,
+  # If set the `verify_mode` to peer the server won't accept any connection,
   # because it will try to verify the client certificate, this is a protocol
   # details implemented at the plugin level.
   #
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index b2dff61d619..b73c0e5d3eb 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -149,7 +149,7 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     @input_queue_client = @queue.write_client
     @filter_queue_client = @queue.read_client
     @signal_queue = Queue.new
-    # Note that @infilght_batches as a central mechanism for tracking inflight
+    # Note that @inflight_batches as a central mechanism for tracking inflight
     # batches will fail if we have multiple read clients here.
     @filter_queue_client.set_events_metric(metric.namespace([:stats, :events]))
     @filter_queue_client.set_pipeline_metric(
@@ -419,7 +419,7 @@ def start_inputs
     # first make sure we can register all input plugins
     register_plugins(@inputs)
 
-    # then after all input plugins are sucessfully registered, start them
+    # then after all input plugins are successfully registered, start them
     @inputs.each { |input| start_input(input) }
   end
 
@@ -465,11 +465,11 @@ def inputworker(plugin)
   # @param before_stop [Proc] code block called before performing stop operation on input plugins
   def shutdown(&before_stop)
     # shutdown can only start once the pipeline has completed its startup.
-    # avoid potential race conditoon between the startup sequence and this
+    # avoid potential race condition between the startup sequence and this
     # shutdown method which can be called from another thread at any time
     sleep(0.1) while !ready?
 
-    # TODO: should we also check against calling shutdown multiple times concurently?
+    # TODO: should we also check against calling shutdown multiple times concurrently?
 
     before_stop.call if block_given?
 
diff --git a/logstash-core/lib/logstash/plugin.rb b/logstash-core/lib/logstash/plugin.rb
index 841a39e2baf..9d448c30dba 100644
--- a/logstash-core/lib/logstash/plugin.rb
+++ b/logstash-core/lib/logstash/plugin.rb
@@ -20,7 +20,7 @@ class LogStash::Plugin
   config :enable_metric, :validate => :boolean, :default => true
 
   # Add a unique `ID` to the plugin configuration. If no ID is specified, Logstash will generate one. 
-  # It is strongly recommended to set this ID in your configuration. This is particulary useful 
+  # It is strongly recommended to set this ID in your configuration. This is particularly useful 
   # when you have two or more plugins of the same type, for example, if you have 2 grok filters. 
   # Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.
   #
diff --git a/logstash-core/lib/logstash/plugins/hooks_registry.rb b/logstash-core/lib/logstash/plugins/hooks_registry.rb
index 1d917225669..93ba026a263 100644
--- a/logstash-core/lib/logstash/plugins/hooks_registry.rb
+++ b/logstash-core/lib/logstash/plugins/hooks_registry.rb
@@ -6,17 +6,17 @@ class HooksRegistry
     java_import "java.util.concurrent.CopyOnWriteArrayList"
 
     def initialize
-      @registered_emmitters = ConcurrentHashMap.new
+      @registered_emitters = ConcurrentHashMap.new
       @registered_hooks = ConcurrentHashMap.new
     end
 
     def register_emitter(emitter_scope, dispatcher)
-      @registered_emmitters.put(emitter_scope, dispatcher)
+      @registered_emitters.put(emitter_scope, dispatcher)
       sync_hooks
     end
 
     def remove_emitter(emitter_scope)
-      @registered_emmitters.remove(emitter_scope)
+      @registered_emitters.remove(emitter_scope)
     end
 
     def register_hooks(emitter_scope, callback)
@@ -28,8 +28,8 @@ def register_hooks(emitter_scope, callback)
       sync_hooks
     end
 
-    def emmitters_count
-      @registered_emmitters.size
+    def emitters_count
+      @registered_emitters.size
     end
 
     def hooks_count(emitter_scope = nil)
@@ -43,7 +43,7 @@ def hooks_count(emitter_scope = nil)
 
     private
     def sync_hooks
-      @registered_emmitters.each do |emitter, dispatcher|
+      @registered_emitters.each do |emitter, dispatcher|
         listeners = @registered_hooks.get(emitter)
 
         unless listeners.nil?
diff --git a/logstash-core/lib/logstash/plugins/registry.rb b/logstash-core/lib/logstash/plugins/registry.rb
index 4b98068098c..622b5da3539 100644
--- a/logstash-core/lib/logstash/plugins/registry.rb
+++ b/logstash-core/lib/logstash/plugins/registry.rb
@@ -111,7 +111,7 @@ def execute_universal_plugins
 
     def load_available_plugins
       GemRegistry.logstash_plugins.each do |plugin_context|
-        # When a plugin has a HOOK_FILE defined, its the responsability of the plugin
+        # When a plugin has a HOOK_FILE defined, its the responsibility of the plugin
         # to register itself to the registry of available plugins.
         #
         # Legacy plugin will lazy register themselves
@@ -205,7 +205,7 @@ def size
     private
     # lookup a plugin by type and name in the existing LogStash module namespace
     # ex.: namespace_lookup("filter", "grok") looks for LogStash::Filters::Grok
-    # @param type [String] plugin type, "input", "ouput", "filter"
+    # @param type [String] plugin type, "input", "output", "filter"
     # @param name [String] plugin name, ex.: "grok"
     # @return [Class] the plugin class or raises NameError
     # @raise NameError if plugin class does not exist or is invalid
diff --git a/logstash-core/lib/logstash/util.rb b/logstash-core/lib/logstash/util.rb
index 88f8b999200..0f7ea7817f0 100644
--- a/logstash-core/lib/logstash/util.rb
+++ b/logstash-core/lib/logstash/util.rb
@@ -141,7 +141,7 @@ def self.hash_merge_many(*hashes)
   end # def hash_merge_many
 
 
-  # nomalize method definition based on platform.
+  # normalize method definition based on platform.
   # normalize is used to convert an object create through
   # json deserialization from JrJackson in :raw mode to pure Ruby
   # to support these pure Ruby object monkey patches.
diff --git a/logstash-core/lib/logstash/util/prctl.rb b/logstash-core/lib/logstash/util/prctl.rb
index 8d8bb5c7826..ffb57201452 100644
--- a/logstash-core/lib/logstash/util/prctl.rb
+++ b/logstash-core/lib/logstash/util/prctl.rb
@@ -4,7 +4,7 @@ module LibC
   extend FFI::Library
   ffi_lib 'c'
 
-  # Ok so the 2nd arg isn't really a string... but whaatever
+  # Ok so the 2nd arg isn't really a string... but whatever
   attach_function :prctl, [:int, :string, :long, :long, :long], :int
 end
 
diff --git a/logstash-core/lib/logstash/util/retryable.rb b/logstash-core/lib/logstash/util/retryable.rb
index 04df5ce8c4a..1a932dd48c4 100644
--- a/logstash-core/lib/logstash/util/retryable.rb
+++ b/logstash-core/lib/logstash/util/retryable.rb
@@ -3,7 +3,7 @@ module LogStash
   module Retryable
     # execute retryable code block
     # @param [Hash] options retryable options
-    # @option options [Fixnum] :tries retries to perform, default 1, set to 0 for infite retries. 1 means that upon exception the block will be retried once
+    # @option options [Fixnum] :tries retries to perform, default 1, set to 0 for infinite retries. 1 means that upon exception the block will be retried once
     # @option options [Fixnum] :base_sleep seconds to sleep on first retry, default 1
     # @option options [Fixnum] :max_sleep max seconds to sleep upon exponential backoff, default 1
     # @option options [Exception] :rescue exception class list to retry on, defaults is Exception, which retries on any Exception.
diff --git a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
index b733d221f46..9d34869a8dc 100644
--- a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
@@ -63,7 +63,7 @@ def push(obj)
     #
     # @param [Object] Object to add to the queue
     # @param [Integer] Time in milliseconds to wait before giving up
-    # @return [Boolean] True if adding was successfull if not it return false
+    # @return [Boolean] True if adding was successful if not it return false
     def offer(obj, timeout_ms)
       raise NotImplementedError.new("The offer method is not implemented. There is no non blocking write operation yet.")
     end
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
index e0124fb67c4..9a5f39044f4 100644
--- a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -18,12 +18,12 @@ def push(obj)
     end
     alias_method(:<<, :push)
 
-    # Offer an object to the queue, wait for the specified amout of time.
-    # If adding to the queue was successfull it wil return true, false otherwise.
+    # Offer an object to the queue, wait for the specified amount of time.
+    # If adding to the queue was successful it wil return true, false otherwise.
     #
     # @param [Object] Object to add to the queue
     # @param [Integer] Time in milliseconds to wait before giving up
-    # @return [Boolean] True if adding was successfull if not it return false
+    # @return [Boolean] True if adding was successful if not it return false
     def offer(obj, timeout_ms)
       @queue.offer(obj, timeout_ms, TimeUnit::MILLISECONDS)
     end
@@ -58,7 +58,7 @@ class ReadClient
       def initialize(queue, batch_size = 125, wait_for = 250)
         @queue = queue
         @mutex = Mutex.new
-        # Note that @infilght_batches as a central mechanism for tracking inflight
+        # Note that @inflight_batches as a central mechanism for tracking inflight
         # batches will fail if we have multiple read clients in the pipeline.
         @inflight_batches = {}
 
diff --git a/logstash-core/locales/en.yml b/logstash-core/locales/en.yml
index e06f3d49978..a00ce406a99 100644
--- a/logstash-core/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -71,9 +71,9 @@ en:
         Logstash is not able to start since configuration auto reloading was enabled but the configuration contains plugins that don't support it. Quitting...
     web_api:
       cant_bind_to_port: |-
-        Logstash tried to bind to port %{port}, but the port is already in use. You can specify a new port by launching logtash with the --http.port option."
+        Logstash tried to bind to port %{port}, but the port is already in use. You can specify a new port by launching logstash with the --http.port option."
       cant_bind_to_port_in_range: |-
-        Logstash tried to bind to port range %{http_ports}, but all the ports are already in use. You can specify a new port by launching logtash with the --http.port option."
+        Logstash tried to bind to port range %{http_ports}, but all the ports are already in use. You can specify a new port by launching logstash with the --http.port option."
       hot_threads:
         title: |-
           ::: {%{hostname}}
diff --git a/logstash-core/spec/api/spec_helper.rb b/logstash-core/spec/api/spec_helper.rb
index a984f13cd5f..193eff2d916 100644
--- a/logstash-core/spec/api/spec_helper.rb
+++ b/logstash-core/spec/api/spec_helper.rb
@@ -31,7 +31,7 @@ def stop_webserver; end
 ##
 # Class used to wrap and manage the execution of an agent for test,
 # this helps a lot in order to have a more integrated test for the
-# web api, could be also used for other use cases if generalized enought
+# web api, could be also used for other use cases if generalized enough
 ##
 class LogStashRunner
 
diff --git a/logstash-core/spec/logstash/config/mixin_spec.rb b/logstash-core/spec/logstash/config/mixin_spec.rb
index cb20f88aedd..530ed2f64c4 100644
--- a/logstash-core/spec/logstash/config/mixin_spec.rb
+++ b/logstash-core/spec/logstash/config/mixin_spec.rb
@@ -192,7 +192,7 @@
         expect(clone.uri.to_s).to eql(uri_hidden)
       end
 
-      it "should make the real URI object availale under #uri" do
+      it "should make the real URI object available under #uri" do
         expect(subject.uri.uri).to be_a(::URI)
       end
 
diff --git a/logstash-core/spec/logstash/event_dispatcher_spec.rb b/logstash-core/spec/logstash/event_dispatcher_spec.rb
index 263b3b23a5f..cda57efaf22 100644
--- a/logstash-core/spec/logstash/event_dispatcher_spec.rb
+++ b/logstash-core/spec/logstash/event_dispatcher_spec.rb
@@ -65,7 +65,7 @@ def method_exists_with_arguments(argument1, argument2, argument3)
       emitter.method_exists
     end
 
-    it "allows to remove a listner to an emitter" do
+    it "allows to remove a listener to an emitter" do
       expect(listener).to receive(:method_exists).with(emitter).once
       emitter.dispatcher.add_listener(listener)
       emitter.method_exists
diff --git a/logstash-core/spec/logstash/event_spec.rb b/logstash-core/spec/logstash/event_spec.rb
index 1ed0a9777a5..c798907087a 100644
--- a/logstash-core/spec/logstash/event_spec.rb
+++ b/logstash-core/spec/logstash/event_spec.rb
@@ -108,7 +108,7 @@
       expect(e.to_hash).to include("foo" => nil)
     end
 
-    # BigDecinal is now natively converted by JRuby, see https://github.com/elastic/logstash/pull/4838
+    # BigDecimal is now natively converted by JRuby, see https://github.com/elastic/logstash/pull/4838
     it "should set BigDecimal" do
       e = LogStash::Event.new()
       e.set("[foo]", BigDecimal.new(1))
@@ -238,8 +238,8 @@
 
   context "to_hash" do
     let (:source_hash) {  {"a" => 1, "b" => [1, 2, 3, {"h" => 1, "i" => "baz"}], "c" => {"d" => "foo", "e" => "bar", "f" => [4, 5, "six"]}} }
-    let (:source_hash_with_matada) {  source_hash.merge({"@metadata" => {"a" => 1, "b" => 2}}) }
-    subject { LogStash::Event.new(source_hash_with_matada) }
+    let (:source_hash_with_metadata) {  source_hash.merge({"@metadata" => {"a" => 1, "b" => 2}}) }
+    subject { LogStash::Event.new(source_hash_with_metadata) }
 
     it "should include @timestamp and @version" do
       h = subject.to_hash
@@ -266,7 +266,7 @@
       h = subject.to_hash_with_metadata
       h.delete("@timestamp")
       h.delete("@version")
-      expect(h).to eq(source_hash_with_matada)
+      expect(h).to eq(source_hash_with_metadata)
     end
   end
 
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb
index 4fa42670994..639ab0d8a9d 100644
--- a/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb
@@ -74,7 +74,7 @@
   end
 
   context ".get_all" do
-    context "when we can retreive the stats" do
+    context "when we can retrieve the stats" do
       let(:cpuacct_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
       let(:cpuacct_usage) { 1982 }
       let(:cpu_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
diff --git a/logstash-core/spec/logstash/json_spec.rb b/logstash-core/spec/logstash/json_spec.rb
index 056325ac91b..3340c2e68ff 100644
--- a/logstash-core/spec/logstash/json_spec.rb
+++ b/logstash-core/spec/logstash/json_spec.rb
@@ -37,7 +37,7 @@
 
     ### JRuby specific
     # Former expectation in this code were removed because of https://github.com/rspec/rspec-mocks/issues/964
-    # as soon as is fix we can re introduce them if decired, however for now the completeness of the test
+    # as soon as is fix we can re introduce them if desired, however for now the completeness of the test
     # is also not affected as the conversion would not work if the expectation where not meet.
     ###
     context "jruby deserialize" do
diff --git a/logstash-core/spec/logstash/legacy_ruby_event_spec.rb b/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
index 3cdd33d12d8..481087ec768 100644
--- a/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
+++ b/logstash-core/spec/logstash/legacy_ruby_event_spec.rb
@@ -191,7 +191,7 @@
         end
 
         it "should return unknown patterns as UTF-8" do
-          expect(subject.sprintf("%{unkown_pattern}").encoding).to eq(Encoding::UTF_8)
+          expect(subject.sprintf("%{unknown_pattern}").encoding).to eq(Encoding::UTF_8)
         end
       end
     end
@@ -579,7 +579,7 @@
   end
 
   describe "using hash input from deserialized json" do
-    # this is to test the case when JrJackson deserialises Json and produces
+    # this is to test the case when JrJackson deserializes Json and produces
     # native Java Collections objects for efficiency
     it_behaves_like "all event tests" do
       subject{LogStash::Event.new(LogStash::Json.load(LogStash::Json.dump(event_hash)))}
diff --git a/logstash-core/spec/logstash/legacy_ruby_timestamp_spec.rb b/logstash-core/spec/logstash/legacy_ruby_timestamp_spec.rb
index 196b895c39e..33e5c644fce 100644
--- a/logstash-core/spec/logstash/legacy_ruby_timestamp_spec.rb
+++ b/logstash-core/spec/logstash/legacy_ruby_timestamp_spec.rb
@@ -40,10 +40,10 @@
   end
 
   it "should support to_json and ignore arguments" do
-    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :argumnents => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
+    expect(LogStash::Timestamp.parse_iso8601("2014-09-23T00:00:00-0800").to_json(:some => 1, :arguments => "test")).to eq("\"2014-09-23T08:00:00.000Z\"")
   end
 
-  it "should support timestamp comparaison" do
+  it "should support timestamp comparison" do
    current = LogStash::Timestamp.new(Time.now) 
    future = LogStash::Timestamp.new(Time.now + 100)
 
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 0deb7a609ce..552b62618dd 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -134,7 +134,7 @@ class TestPipeline < LogStash::Pipeline
       CONFIG
     end
 
-    it "should not propage cancelled events from filter to output" do
+    it "should not propagate cancelled events from filter to output" do
       abort_on_exception_state = Thread.abort_on_exception
       Thread.abort_on_exception = true
 
@@ -389,7 +389,7 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(logger).to receive(:warn)
 
-      # pipeline must be first called outside the thread context because it lazyly initialize and will create a
+      # pipeline must be first called outside the thread context because it lazily initialize and will create a
       # race condition if called in the thread
       p = pipeline
       t = Thread.new { p.run }
@@ -411,7 +411,7 @@ class TestPipeline < LogStash::Pipeline
     end
   end
 
-  context "compiled filter funtions" do
+  context "compiled filter functions" do
     context "new events should propagate down the filters" do
       config <<-CONFIG
         filter {
@@ -630,7 +630,7 @@ class TestPipeline < LogStash::Pipeline
 
     it "should handle evaluating different config" do
       # When the functions are compiled from the AST it will generate instance
-      # variables that are unique to the actual config, the intances are pointing
+      # variables that are unique to the actual config, the instances are pointing
       # to conditionals and/or plugins.
       #
       # Before the `defined_singleton_method`, the definition of the method was
diff --git a/logstash-core/spec/logstash/plugin_spec.rb b/logstash-core/spec/logstash/plugin_spec.rb
index fcaf0e6fa76..c6eda09c5ac 100644
--- a/logstash-core/spec/logstash/plugin_spec.rb
+++ b/logstash-core/spec/logstash/plugin_spec.rb
@@ -42,12 +42,12 @@ def self.reloadable?
     end
   end
 
-  it "should fail lookup on inexisting type" do
+  it "should fail lookup on nonexistent type" do
     #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
     expect { LogStash::Plugin.lookup("badbadtype", "badname") }.to raise_error(LogStash::PluginLoadingError)
   end
 
-  it "should fail lookup on inexisting name" do
+  it "should fail lookup on nonexistent name" do
     #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
     expect { LogStash::Plugin.lookup("filter", "badname") }.to raise_error(LogStash::PluginLoadingError)
   end
@@ -217,7 +217,7 @@ def register; end
           config_name "simple_plugin"
 
           config :host, :validate => :string
-          config :export, :validte => :boolean
+          config :export, :validate => :boolean
 
           def register; end
         end
diff --git a/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb b/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
index c45a219dbee..7a20e16945e 100644
--- a/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
+++ b/logstash-core/spec/logstash/plugins/hooks_registry_spec.rb
@@ -35,12 +35,12 @@ def work?
   let(:listener) { DummyListener.new }
 
   it "allow to register an emitter" do
-    expect { subject.register_emitter(emitter.class, emitter.dispatcher) }.to change { subject.emmitters_count }.by(1)
+    expect { subject.register_emitter(emitter.class, emitter.dispatcher) }.to change { subject.emitters_count }.by(1)
   end
 
   it "allow to remove an emitter" do
     subject.register_emitter(emitter.class, emitter.dispatcher)
-    expect { subject.remove_emitter(emitter.class)}.to change { subject.emmitters_count }.by(-1)
+    expect { subject.remove_emitter(emitter.class)}.to change { subject.emitters_count }.by(-1)
   end
 
   it "allow to register hooks to emitters" do
diff --git a/logstash-core/spec/logstash/runner_spec.rb b/logstash-core/spec/logstash/runner_spec.rb
index 8463adbf2a3..e64412ebdc1 100644
--- a/logstash-core/spec/logstash/runner_spec.rb
+++ b/logstash-core/spec/logstash/runner_spec.rb
@@ -141,7 +141,7 @@ def run(args); end
 
     context "with a good configuration" do
       let(:pipeline_string) { "input { } filter { } output { }" }
-      it "should exit successfuly" do
+      it "should exit successfully" do
         expect(subject.run(args)).to eq(0)
       end
     end
diff --git a/logstash-core/spec/logstash/settings/port_range_spec.rb b/logstash-core/spec/logstash/settings/port_range_spec.rb
index 05afd21edbb..6085b026da1 100644
--- a/logstash-core/spec/logstash/settings/port_range_spec.rb
+++ b/logstash-core/spec/logstash/settings/port_range_spec.rb
@@ -56,7 +56,7 @@
     end
   end
 
-  context "when the value is an unkown type" do
+  context "when the value is an unknown type" do
     subject { LogStash::Setting::PortRange.new("mynewtest", 0.1) }
 
 
diff --git a/logstash-core/spec/logstash/util_spec.rb b/logstash-core/spec/logstash/util_spec.rb
index 4cd56139fbf..df974775a75 100644
--- a/logstash-core/spec/logstash/util_spec.rb
+++ b/logstash-core/spec/logstash/util_spec.rb
@@ -37,7 +37,7 @@ class TestKlass
       expect(LogStash::Util.stringify_symbols([1, :a])).to eq([1, "a"])
     end
 
-    it "should convert innner array symbol values to strings" do
+    it "should convert inner array symbol values to strings" do
       expect(LogStash::Util.stringify_symbols({:a => [1, :b]})).to eq({"a" => [1, "b"]})
       expect(LogStash::Util.stringify_symbols([:a, [1, :b]])).to eq(["a", [1, "b"]])
     end
diff --git a/logstash-core/spec/logstash/webserver_spec.rb b/logstash-core/spec/logstash/webserver_spec.rb
index a5bd42f0b8c..cbf14542d1f 100644
--- a/logstash-core/spec/logstash/webserver_spec.rb
+++ b/logstash-core/spec/logstash/webserver_spec.rb
@@ -139,7 +139,7 @@ def free_ports(servers)
     expect(logger).to have_received(:debug).with(message)
   end
 
-  it "reponds to <<" do
+  it "responds to <<" do
     subject << message
     expect(logger).to have_received(:debug).with(message)
   end
diff --git a/logstash-core/src/main/java/org/logstash/Accessors.java b/logstash-core/src/main/java/org/logstash/Accessors.java
index c4cfd492c7d..705f3b21623 100644
--- a/logstash-core/src/main/java/org/logstash/Accessors.java
+++ b/logstash-core/src/main/java/org/logstash/Accessors.java
@@ -154,7 +154,7 @@ private Object store(Object target, String key, Object value) {
             if (i >= size) {
                 // grow array by adding trailing null items
                 // this strategy reflects legacy Ruby impl behaviour and is backed by specs
-                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbritary big numbers
+                // TODO: (colin) this is potentially dangerous, and could produce OOM using arbitrary big numbers
                 // TODO: (colin) should be guard against this?
                 for (int j = size; j < i; j++) {
                     ((List<Object>) target).add(null);
diff --git a/logstash-core/src/main/java/org/logstash/Event.java b/logstash-core/src/main/java/org/logstash/Event.java
index fc6369f5033..206ce651a0a 100644
--- a/logstash-core/src/main/java/org/logstash/Event.java
+++ b/logstash-core/src/main/java/org/logstash/Event.java
@@ -252,12 +252,12 @@ public static Event[] fromJson(String json)
             int i = 0;
             for (Object e : (List)o) {
                 if (!(e instanceof Map)) {
-                    throw new IOException("incompatible inner json array object type=" + e.getClass().getName() + " , only hash map is suppoted");
+                    throw new IOException("incompatible inner json array object type=" + e.getClass().getName() + " , only hash map is supported");
                 }
                 result[i++] = new Event((Map)e);
             }
         } else {
-            throw new IOException("incompatible json object type=" + o.getClass().getName() + " , only hash map or arrays are suppoted");
+            throw new IOException("incompatible json object type=" + o.getClass().getName() + " , only hash map or arrays are supported");
         }
 
         return result;
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
index 7296b54d10e..1b725c894a6 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
@@ -82,7 +82,7 @@ public TailPage behead() throws IOException {
         // TODO: should we have a better deactivation strategy to avoid too rapid reactivation scenario?
         Page firstUnreadPage = queue.firstUnreadPage();
         if (firstUnreadPage == null || (tailPage.getPageNum() > firstUnreadPage.getPageNum())) {
-            // deactivate if this new tailPage is not where the read is occuring
+            // deactivate if this new tailPage is not where the read is occurring
             tailPage.getPageIO().deactivate();
         }
 
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
index 7db89ebdb45..a7b7d7d47cd 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Page.java
@@ -10,7 +10,7 @@
 
 public abstract class Page implements Closeable {
     protected final int pageNum;
-    protected long minSeqNum; // TODO: see if we can meke it final?
+    protected long minSeqNum; // TODO: see if we can make it final?
     protected int elementCount;
     protected long firstUnreadSeqNum;
     protected final Queue queue;
@@ -74,7 +74,7 @@ public boolean isFullyRead() {
     public boolean isFullyAcked() {
         // TODO: it should be something similar to this when we use a proper bitset class like ES
         // this.ackedSeqNum.firstUnackedBit >= this.elementCount;
-        // TODO: for now use a naive & inneficient mechanism with a simple Bitset
+        // TODO: for now use a naive & inefficient mechanism with a simple Bitset
         return this.elementCount > 0 && this.ackedSeqNums.cardinality() >= this.elementCount;
     }
 
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index 4bcef21c77c..9137fa54891 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -36,7 +36,7 @@ public class Queue implements Closeable {
     protected long seqNum;
     protected HeadPage headPage;
 
-    // complete list of all non fully acked pages. note that exact sequenciality by pageNum cannot be assumed
+    // complete list of all non fully acked pages. note that exact sequentially by pageNum cannot be assumed
     // because any fully acked page will be removed from this list potentially creating pageNum gaps in the list.
     protected final List<TailPage> tailPages;
 
@@ -282,7 +282,7 @@ private void add(Checkpoint checkpoint, TailPage page) throws IOException {
         }
     }
 
-    // create a new empty headpage for the given pageNum and imidiately checkpoint it
+    // create a new empty headpage for the given pageNum and immediately checkpoint it
     // @param pageNum the page number of the new head page
     private void newCheckpointedHeadpage(int pageNum) throws IOException {
         PageIO headPageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
@@ -305,7 +305,7 @@ public long write(Queueable element) throws IOException {
         // the write strategy with regard to the isFull() state is to assume there is space for this element
         // and write it, then after write verify if we just filled the queue and wait on the notFull condition
         // *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case
-        // holding an element in memory while wainting for the notFull condition would mean always having the current write
+        // holding an element in memory while waiting for the notFull condition would mean always having the current write
         // element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible
         // to write the current element.
 
@@ -313,7 +313,7 @@ public long write(Queueable element) throws IOException {
         try {
             boolean wasEmpty = (firstUnreadPage() == null);
 
-            // create a new head page if the current does not have suffient space left for data to be written
+            // create a new head page if the current does not have sufficient space left for data to be written
             if (! this.headPage.hasSpace(data.length)) {
                 // beheading includes checkpoint+fsync if required
                 TailPage tailPage = this.headPage.behead();
@@ -362,7 +362,7 @@ public long write(Queueable element) throws IOException {
 
     // @return true if the queue is deemed at full capacity
     public boolean isFull() {
-        // TODO: I am not sure if having unreadCount as volatile is sufficient here. all unreadCount updates are done inside syncronized
+        // TODO: I am not sure if having unreadCount as volatile is sufficient here. all unreadCount updates are done inside synchronized
         // TODO: sections, I believe that to only read the value here, having it as volatile is sufficient?
         if ((this.maxBytes > 0) && this.currentByteSize >= this.maxBytes) {
             return true;
@@ -381,7 +381,7 @@ public boolean isFullyAcked() {
         }
     }
 
-    // @param seqNum the element sequence number upper bound for which persistence should be garanteed (by fsync'ing)
+    // @param seqNum the element sequence number upper bound for which persistence should be guaranteed (by fsync'ing)
     public void ensurePersistedUpto(long seqNum) throws IOException{
         lock.lock();
         try {
@@ -464,7 +464,7 @@ public Batch readBatch(int limit, long timeout) throws IOException {
                     return null;
                 }
 
-                // if after returnining from wait queue is still empty, or the queue was closed return null
+                // if after returning from wait queue is still empty, or the queue was closed return null
                 if ((p = firstUnreadPage()) == null || isClosed()) { return null; }
             }
 
@@ -534,7 +534,7 @@ public void ack(List<Long> seqNums) throws IOException {
         // as a first implementation we assume that all batches are created from the same page
         // so we will avoid multi pages acking here for now
 
-        // find the page to ack by travesing from oldest tail page
+        // find the page to ack by traversing from oldest tail page
         long firstAckSeqNum = seqNums.get(0);
 
         lock.lock();
@@ -547,7 +547,7 @@ public void ack(List<Long> seqNums) throws IOException {
                 if (p.getMinSeqNum() > 0 && firstAckSeqNum >= p.getMinSeqNum() && firstAckSeqNum < p.getMinSeqNum() + p.getElementCount()) {
                     result = new TailPageResult(p, 0);
                 } else {
-                    // dual search strategy: if few tail pages search linearily otherwise perform binary search
+                    // dual search strategy: if few tail pages search linearly otherwise perform binary search
                     result = (this.tailPages.size() > 3) ? binaryFindPageForSeqnum(firstAckSeqNum) : linearFindPageForSeqnum(firstAckSeqNum);
                 }
             }
@@ -598,7 +598,7 @@ public CheckpointIO getCheckpointIO() {
 
     // deserialize a byte array into the required element class.
     // @param bytes the byte array to deserialize
-    // @return Queueable the deserialized byte array into the required Queuable interface implementation concrete class
+    // @return Queueable the deserialized byte array into the required Queueable interface implementation concrete class
     public Queueable deserialize(byte[] bytes) {
         try {
             return (Queueable)this.deserializeMethod.invoke(this.elementClass, bytes);
@@ -613,7 +613,7 @@ public void close() throws IOException {
         if (closed.getAndSet(true) == false) {
             lock.lock();
             try {
-                // TODO: not sure if we need to do this here since the headpage close will also call ensurePersited
+                // TODO: not sure if we need to do this here since the headpage close will also call ensurePersisted
                 ensurePersistedUpto(this.seqNum);
 
                 for (TailPage p : this.tailPages) { p.close(); }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/AbstractByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/AbstractByteBufferPageIO.java
index 19282e37c41..9c927b4e63e 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/AbstractByteBufferPageIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/AbstractByteBufferPageIO.java
@@ -99,7 +99,7 @@ public void recover() throws IOException {
         // reset back position to first seqNum
         getBuffer().position(this.head);
 
-        // reset elementCount to 0 and increment to actal number of valid elements found
+        // reset elementCount to 0 and increment to octal number of valid elements found
         this.elementCount = 0;
 
         for (int i = 0; ; i++) {
@@ -128,7 +128,7 @@ private void validateVersion(byte version) throws PageIOInvalidVersionException
         }
     }
 
-    // read and vadidate next element at page head
+    // read and validate next element at page head
     // @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
     private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws PageIOInvalidElementException {
         // if there is no room for the seqNum and length bytes stop here
diff --git a/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
index d133ae15ff3..28b17f34e2b 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
@@ -30,7 +30,7 @@ public ByteBufferPageIO(int capacity, byte[] initialBytes) throws IOException {
     public void deactivate() { /* nothing */ }
 
     @Override
-    public void activate() { /* niet */ }
+    public void activate() { /* nyet */ }
 
     @Override
     public void ensurePersisted() { /* nada */ }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
index 1d2bd530ee5..7287f9e1d15 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
@@ -42,7 +42,7 @@ public void recover() throws IOException {
     }
 
     // memory map data file to this.buffer and read initial version byte
-    // @param strictCapacity if true verify that data file size is same as confgured page capcity, if false update page capcity to actual file size
+    // @param strictCapacity if true verify that data file size is same as configured page capacity, if false update page capacity to actual file size
     private void mapFile(boolean strictCapacity) throws IOException {
         RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
 
@@ -55,7 +55,7 @@ private void mapFile(boolean strictCapacity) throws IOException {
             throw new IOException("Page file size " + pageFileCapacity + " different to configured page capacity " + this.capacity + " for " + this.file);
         }
 
-        // update capacity to actual raf lenght
+        // update capacity to actual raf length
         this.capacity = pageFileCapacity;
 
         if (this.capacity < MIN_CAPACITY) { throw new IOException(String.format("Page file size is too small to hold elements")); }
diff --git a/logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java b/logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
index 76fe2b582c3..46f2da41ca0 100644
--- a/logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
+++ b/logstash-core/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
@@ -209,7 +209,7 @@ public static IRubyObject ruby_at(ThreadContext context, IRubyObject recv, IRuby
                 IRubyObject epoch = args[0];
 
                 if (epoch instanceof RubyBigDecimal) {
-                    // bug in JRuby prevents correcly parsing a BigDecimal fractional part, see https://github.com/elastic/logstash/issues/4565
+                    // bug in JRuby prevents correctly parsing a BigDecimal fractional part, see https://github.com/elastic/logstash/issues/4565
                     double usec = ((RubyBigDecimal)epoch).frac().convertToFloat().getDoubleValue() * 1000000;
                     t = (RubyTime)RubyTime.at(context, context.runtime.getTime(), ((RubyBigDecimal)epoch).to_int(), new RubyFloat(context.runtime, usec));
                 } else {
diff --git a/logstash-core/src/test/java/org/logstash/RubyfierTest.java b/logstash-core/src/test/java/org/logstash/RubyfierTest.java
index e5bba62148c..df14a70729f 100644
--- a/logstash-core/src/test/java/org/logstash/RubyfierTest.java
+++ b/logstash-core/src/test/java/org/logstash/RubyfierTest.java
@@ -58,7 +58,7 @@ public void testDeepListWithString()
 
         RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyString.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals("foo", rubyArray.toJavaArray()[0].toString());
     }
@@ -98,7 +98,7 @@ public void testDeepListWithInteger()
 
         RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyFixnum.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1L, ((RubyFixnum)rubyArray.toJavaArray()[0]).getLongValue());
     }
@@ -138,7 +138,7 @@ public void testDeepListWithFloat()
 
         RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
@@ -178,7 +178,7 @@ public void testDeepListWithDouble()
 
         RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
@@ -219,7 +219,7 @@ public void testDeepListWithBigDecimal()
 
         RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elements to Java types \o/
         assertEquals(RubyBigDecimal.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyBigDecimal)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
diff --git a/logstash-core/src/test/java/org/logstash/StringInterpolationTest.java b/logstash-core/src/test/java/org/logstash/StringInterpolationTest.java
index 8a28ee64939..2559a55b360 100644
--- a/logstash-core/src/test/java/org/logstash/StringInterpolationTest.java
+++ b/logstash-core/src/test/java/org/logstash/StringInterpolationTest.java
@@ -51,7 +51,7 @@ public void testMissingKey() throws IOException {
     }
 
     @Test
-    public void testDateFormater() throws IOException {
+    public void testDateFormatter() throws IOException {
         Event event = getTestEvent();
         String path = "/full/%{+YYYY}";
         StringInterpolation si = StringInterpolation.getInstance();
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
index c3bdd49607c..d779337d6d1 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -199,7 +199,7 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements1.get(0).serialize().length);
 
         Settings settings = TestSettings.volatileQueueSettings(2 * singleElementCapacity);
-        settings.setCheckpointMaxWrites(1024); // arbritary high enough threshold so that it's not reached (default for TestSettings is 1)
+        settings.setCheckpointMaxWrites(1024); // arbitrary high enough threshold so that it's not reached (default for TestSettings is 1)
         TestQueue q = new TestQueue(settings);
         q.open();
 
diff --git a/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java b/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
index c238c8c3612..4c14edbe207 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
@@ -113,8 +113,8 @@ public void openValidState() throws IOException {
         ByteBufferPageIO io = newEmptyPageIO();
         io.write(element.serialize(), seqNum);
 
-        byte[] inititalState = io.dump();
-        io = newPageIO(inititalState.length, inititalState);
+        byte[] initialState = io.dump();
+        io = newPageIO(initialState.length, initialState);
         io.open(seqNum, 1);
         assertThat(io.getElementCount(), is(equalTo(1)));
         assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
@@ -127,8 +127,8 @@ public void recoversValidState() throws IOException {
         ByteBufferPageIO io = newEmptyPageIO();
         io.write(element.serialize(), seqNum);
 
-        byte[] inititalState = io.dump();
-        io = newPageIO(inititalState.length, inititalState);
+        byte[] initialState = io.dump();
+        io = newPageIO(initialState.length, initialState);
         io.recover();
         assertThat(io.getElementCount(), is(equalTo(1)));
         assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
@@ -139,16 +139,16 @@ public void recoverEmptyWriteRecover() throws IOException {
         Queueable element = new StringElement("foobarbaz");
         long seqNum = 42L;
         ByteBufferPageIO io = newEmptyPageIO();
-        byte[] inititalState = io.dump();
+        byte[] initialState = io.dump();
 
-        io = newPageIO(inititalState.length, inititalState);
+        io = newPageIO(initialState.length, initialState);
         io.recover();
         assertThat(io.getElementCount(), is(equalTo(0)));
 
         io.write(element.serialize(), seqNum);
-        inititalState = io.dump();
+        initialState = io.dump();
 
-        io = newPageIO(inititalState.length, inititalState);
+        io = newPageIO(initialState.length, initialState);
         io.recover();
         assertThat(io.getElementCount(), is(equalTo(1)));
         assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
@@ -160,16 +160,16 @@ public void recoverNonEmptyWriteRecover() throws IOException {
 
         ByteBufferPageIO io = newEmptyPageIO();
         io.write(element.serialize(), 1L);
-        byte[] inititalState = io.dump();
+        byte[] initialState = io.dump();
 
-        io = newPageIO(inititalState.length, inititalState);
+        io = newPageIO(initialState.length, initialState);
         io.recover();
         assertThat(io.getElementCount(), is(equalTo(1)));
 
         io.write(element.serialize(), 2L);
-        inititalState = io.dump();
+        initialState = io.dump();
 
-        io = newPageIO(inititalState.length, inititalState);
+        io = newPageIO(initialState.length, initialState);
         io.recover();
         assertThat(io.getElementCount(), is(equalTo(2)));
     }
@@ -181,8 +181,8 @@ public void openUnexpectedSeqNum() throws IOException {
         ByteBufferPageIO io = newEmptyPageIO();
         io.write(element.serialize(), seqNum);
 
-        byte[] inititalState = io.dump();
-        newPageIO(inititalState.length, inititalState);
+        byte[] initialState = io.dump();
+        newPageIO(initialState.length, initialState);
         io.open(1L, 1);
     }
 
diff --git a/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java b/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
index a77d8586d06..8961b47c9b7 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
@@ -40,7 +40,7 @@ public void writeNewReadExisting() throws IOException {
     }
 
     @Test(expected = NoSuchFileException.class)
-    public void readInnexisting() throws IOException {
+    public void readNonexistent() throws IOException {
         io.read("checkpoint.invalid");
     }
 
diff --git a/logstash-core/src/test/java/org/logstash/stress/Concurent.java b/logstash-core/src/test/java/org/logstash/stress/Concurent.java
index cf53720cd0d..aa8c018976f 100644
--- a/logstash-core/src/test/java/org/logstash/stress/Concurent.java
+++ b/logstash-core/src/test/java/org/logstash/stress/Concurent.java
@@ -13,7 +13,7 @@
 import java.util.concurrent.ConcurrentLinkedQueue;
 import java.util.stream.Collectors;
 
-public class Concurent {
+public class Concurrent {
     final static int ELEMENT_COUNT = 2000000;
     final static int BATCH_SIZE = 1000;
     static Settings settings;
diff --git a/qa/Rakefile b/qa/Rakefile
index fe52f107d9c..589a230c641 100644
--- a/qa/Rakefile
+++ b/qa/Rakefile
@@ -33,7 +33,7 @@ namespace :qa do
       experimental = (ENV['LS_QA_EXPERIMENTAL_OS'].to_s.downcase || "false") == "true"
       machines = config.select_names_for(args[:platform], {"experimental" => experimental})
 
-      puts user_feedback_string_for("bootstraping", args[:platform], machines, {"experimental" => experimental})
+      puts user_feedback_string_for("bootstrapping", args[:platform], machines, {"experimental" => experimental})
 
       options = {:debug => ENV['LS_QA_DEBUG']}
       LogStash::VagrantHelpers.destroy(machines, options)
diff --git a/qa/acceptance/spec/lib/artifact_operation_spec.rb b/qa/acceptance/spec/lib/artifact_operation_spec.rb
index fdebf23de31..f541f3a9ffa 100644
--- a/qa/acceptance/spec/lib/artifact_operation_spec.rb
+++ b/qa/acceptance/spec/lib/artifact_operation_spec.rb
@@ -4,7 +4,7 @@
 require_relative '../shared_examples/running'
 require_relative '../shared_examples/updated'
 
-# This tests verify that the generated artifacts could be used properly in a relase, implements https://github.com/elastic/logstash/issues/5070
+# This tests verify that the generated artifacts could be used properly in a release, implements https://github.com/elastic/logstash/issues/5070
 describe "artifacts operation" do
   config = ServiceTester.configuration
   config.servers.each do |address|
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb
index f8bd007f6b0..2423ded084c 100644
--- a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb
@@ -22,7 +22,7 @@
 
     # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
     xcontext "when the plugin is installed" do
-      it "succesfully removes it" do
+      it "successfully removes it" do
         result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
         expect(logstash).to have_installed?("logstash-filter-qatest")
 
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
index 9eb4939a31f..54ec8a6a72b 100644
--- a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
@@ -22,7 +22,7 @@
 
     # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
     xcontext "when the plugin is installed" do
-      it "succesfully uninstall it" do
+      it "successfully uninstall it" do
         result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
         expect(logstash).to have_installed?("logstash-filter-qatest")
 
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
index 12d317db325..7b9f4880c05 100644
--- a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
@@ -17,13 +17,13 @@
 
     before do
       logstash.run_command_in_path("bin/logstash-plugin install --no-verify --version #{previous_version} #{plugin_name}")
-      # Logstash wont update when we have a pinned versionin the gemfile so we remove them
+      # Logstash won't update when we have a pinned version in the gemfile so we remove them
       logstash.replace_in_gemfile(',[[:space:]]"0.1.0"', "")
       expect(logstash).to have_installed?(plugin_name, previous_version)
     end
 
     context "update a specific plugin" do
-      it "has executed succesfully" do
+      it "has executed successfully" do
         cmd = logstash.run_command_in_path("bin/logstash-plugin update --no-verify #{plugin_name}")
         expect(cmd.stdout).to match(/Updating #{plugin_name}/)
         expect(logstash).not_to have_installed?(plugin_name, previous_version)
@@ -31,7 +31,7 @@
     end
 
     context "update all the plugins" do
-      it "has executed succesfully" do
+      it "has executed successfully" do
         logstash.run_command_in_path("bin/logstash-plugin update --no-verify")
         expect(logstash).to have_installed?(plugin_name, "0.1.1")
       end
diff --git a/qa/integration/fixtures/logstash-dummy-pack/.gitiginore b/qa/integration/fixtures/logstash-dummy-pack/.gitignore
similarity index 100%
rename from qa/integration/fixtures/logstash-dummy-pack/.gitiginore
rename to qa/integration/fixtures/logstash-dummy-pack/.gitignore
diff --git a/qa/integration/specs/01_logstash_bin_smoke_spec.rb b/qa/integration/specs/01_logstash_bin_smoke_spec.rb
index cbda3b63fd5..840fdd3bfa3 100644
--- a/qa/integration/specs/01_logstash_bin_smoke_spec.rb
+++ b/qa/integration/specs/01_logstash_bin_smoke_spec.rb
@@ -52,7 +52,7 @@
       tmp_data_path = File.join(tmp_path, "data")
       FileUtils.mkdir_p(tmp_data_path)
       @ls1.spawn_logstash("-f", config1, "--path.data", tmp_data_path)
-      sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started succesfully at this point
+      sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
       expect(is_port_open?(9600)).to be true
 
       @ls2.spawn_logstash("-f", config2, "--path.data", tmp_data_path)
@@ -70,18 +70,18 @@
       tmp_data_path_2 = File.join(tmp_path_2, "data")
       FileUtils.mkdir_p(tmp_data_path_2)
       @ls1.spawn_logstash("-f", config1, "--path.data", tmp_data_path_1)
-      sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started succesfully at this point
+      sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
       expect(is_port_open?(9600)).to be true
 
       @ls2.spawn_logstash("-f", config2, "--path.data", tmp_data_path_2)
-      sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started succesfully at this point
+      sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started successfully at this point
       expect(@ls2.exited?).to be(false)
     end
 
     it "can be started on the same box with automatically trying different ports for HTTP server" do
       if @ls2.settings.feature_flag != "persistent_queues"
         @ls1.spawn_logstash("-f", config1)
-        sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started succesfully at this point
+        sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
         expect(is_port_open?(9600)).to be true
 
         puts "will try to start the second LS instance on 9601"
@@ -91,7 +91,7 @@
         tmp_data_path = File.join(tmp_path, "data")
         FileUtils.mkdir_p(tmp_data_path)
         @ls2.spawn_logstash("-f", config2, "--path.data", tmp_data_path)
-        sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started succesfully at this point
+        sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started successfully at this point
         expect(is_port_open?(9601)).to be true
         expect(@ls1.process_id).not_to eq(@ls2.process_id)
       else
@@ -103,7 +103,7 @@
         IO.write(File.join(path, "logstash.yml"), YAML.dump(settings))
 
         @ls1.spawn_logstash("--path.settings", path, "-f", config1)
-        sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started succesfully at this point
+        sleep(0.1) until File.exist?(file_config1) && File.size(file_config1) > 0 # Everything is started successfully at this point
         expect(is_port_open?(9600)).to be true
 
         puts "will try to start the second LS instance on 9601"
@@ -115,7 +115,7 @@
         settings = persistent_queue_settings.merge({ "path.data" => data })
         IO.write(File.join(path, "logstash.yml"), YAML.dump(settings))
         @ls2.spawn_logstash("--path.settings", path, "-f", config2)
-        sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started succesfully at this point
+        sleep(0.1) until File.exist?(file_config2) && File.size(file_config2) > 0 # Everything is started successfully at this point
         expect(is_port_open?(9601)).to be true
 
         expect(@ls1.process_id).not_to eq(@ls2.process_id)
diff --git a/qa/integration/specs/beats_input_spec.rb b/qa/integration/specs/beats_input_spec.rb
index 0e3736de485..926f2cbb678 100644
--- a/qa/integration/specs/beats_input_spec.rb
+++ b/qa/integration/specs/beats_input_spec.rb
@@ -43,7 +43,7 @@
     end
 
 
-    it "sucessfully send events" do
+    it "successfully send events" do
       logstash_service.start_background(logstash_config)
       process = filebeat_service.run(filebeat_config_path)
 
diff --git a/qa/integration/specs/cli/install_spec.rb b/qa/integration/specs/cli/install_spec.rb
index b9352e4c87a..d1c02037c54 100644
--- a/qa/integration/specs/cli/install_spec.rb
+++ b/qa/integration/specs/cli/install_spec.rb
@@ -26,7 +26,7 @@ def gem_in_lock_file?(pattern, lock_file)
     let(:install_command) { "bin/logstash-plugin install" }
     let(:change_dir) { true }
 
-    # When you are on anything by linux we wont disable the internet with seccomp
+    # When you are on anything by linux we won't disable the internet with seccomp
     if RbConfig::CONFIG["host_os"] == "linux"
       context "without internet connection (linux seccomp wrapper)" do
 
diff --git a/qa/platform_config.rb b/qa/platform_config.rb
index 37073b2bb50..6cfb9716bd6 100644
--- a/qa/platform_config.rb
+++ b/qa/platform_config.rb
@@ -27,7 +27,7 @@ def configure_bootstrap_scripts(data)
       @bootstrap = OpenStruct.new(:privileged     => "sys/#{type}/bootstrap.sh",
                                   :non_privileged => "sys/#{type}/user_bootstrap.sh")
       ##
-      # for now the only specific boostrap scripts are ones need
+      # for now the only specific bootstrap scripts are ones need
       # with privileged access level, whenever others are also
       # required we can update this section as well with the same pattern.
       ##
diff --git a/qa/scripts/windows/integration/logstash_simple_integration.ps1 b/qa/scripts/windows/integration/logstash_simple_integration.ps1
index fb861cd2e98..02cd4f41489 100644
--- a/qa/scripts/windows/integration/logstash_simple_integration.ps1
+++ b/qa/scripts/windows/integration/logstash_simple_integration.ps1
@@ -26,7 +26,7 @@ $LS_CONFIG="test.conf"
 $LS_BRANCH=$env:LS_BRANCH
 $Logstash_path = "$Main_path\logstash"
 $Logstash_zip_file = "$Download_path\logstash.zip"
-$Logstas_URL = "https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash/$LS_BRANCH/nightly/JDK7/logstash-latest-SNAPSHOT.zip"
+$Logstash_URL = "https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash/$LS_BRANCH/nightly/JDK7/logstash-latest-SNAPSHOT.zip"
 
 ## ----------------------------------------
 
@@ -42,7 +42,7 @@ $ES_URL = "https://download.elastic.co/elasticsearch/elasticsearch/elasticsearch
 ## Download and unzip Logstash
 
 md -Path $Logstash_path
-(New-Object System.Net.WebClient).DownloadFile($Logstas_URL, $Logstash_zip_file)
+(New-Object System.Net.WebClient).DownloadFile($Logstash_URL, $Logstash_zip_file)
 [System.IO.Compression.ZipFile]::ExtractToDirectory($Logstash_zip_file, $Download_path)
 ri $Logstash_zip_file
 mv "$Download_path\log*\*" $Logstash_path
diff --git a/rakelib/test.rake b/rakelib/test.rake
index 8cf7c0d1559..0f2a74eeffc 100644
--- a/rakelib/test.rake
+++ b/rakelib/test.rake
@@ -1,4 +1,4 @@
-# we need to call exit explicity  in order to set the proper exit code, otherwise
+# we need to call exit explicitly  in order to set the proper exit code, otherwise
 # most common CI systems can not know whats up with this tests.
 
 require "pluginmanager/util"
@@ -12,7 +12,7 @@ namespace "test" do
     mkdir "data" unless File.directory?("data")
     mkdir "data/queue" unless File.directory?("data/queue")
 
-    # Need to be run here as because if run aftewarse (after the bundler.setup task) then the report got wrong
+    # Need to be run here as because if run afterwards (after the bundler.setup task) then the report got wrong
     # numbers and misses files. There is an issue with our setup! method as this does not happen with the regular
     # bundler.setup used in regular bundler flows.
     Rake::Task["test:setup-simplecov"].invoke if ENV['COVERAGE']
@@ -50,7 +50,7 @@ namespace "test" do
   desc "run all installed plugins specs"
   task "plugins" => ["setup"] do
     plugins_to_exclude = ENV.fetch("EXCLUDE_PLUGIN", "").split(",")
-    # grab all spec files using the live plugins gem specs. this allows correclty also running the specs
+    # grab all spec files using the live plugins gem specs. this allows correctly also running the specs
     # of a local plugin dir added using the Gemfile :path option. before this, any local plugin spec would
     # not be run because they were not under the vendor/bundle/jruby/1.9/gems path
     test_files = LogStash::PluginManager.find_plugins_gem_specs.map do |spec|
@@ -86,7 +86,7 @@ namespace "test" do
         add_filter pattern
       end
 
-      add_group "bootstrap", "bootstrap/" # This module is used during bootstraping of LS
+      add_group "bootstrap", "bootstrap/" # This module is used during bootstrapping of LS
       add_group "plugin manager", "pluginmanager/" # Code related to the plugin manager
       add_group "core" do |src_file| # The LS core codebase
         /logstash\/\w+.rb/.match(src_file.filename)
diff --git a/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb b/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
index dd636f37143..084205bf405 100644
--- a/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
+++ b/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
@@ -32,7 +32,7 @@
 
   context "when the remote host is unreachable" do
     it "returns false and yield a debug message" do
-      # To make sure we really try to connect to a failling host we have to let it through webmock
+      # To make sure we really try to connect to a failing host we have to let it through webmock
       host ="#{Time.now.to_i.to_s}-do-not-exist.com"
       WebMock.disable_net_connect!(:allow => host)
       ENV["LOGSTASH_PACK_URL"] = "http://#{host}"
diff --git a/spec/unit/plugin_manager/prepare_offline_pack_spec.rb b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
index bcda5993c02..f86be95bdd2 100644
--- a/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
+++ b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
@@ -28,7 +28,7 @@
     allow(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, anything).and_return(offline_plugin_packager)
   end
 
-  context "when not debbuging" do
+  context "when not debugging" do
     before do
       @before_debug_value = ENV["DEBUG"]
       ENV["DEBUG"] = nil
diff --git a/spec/unit/plugin_manager/util_spec.rb b/spec/unit/plugin_manager/util_spec.rb
index 10824e56adc..669a186886f 100644
--- a/spec/unit/plugin_manager/util_spec.rb
+++ b/spec/unit/plugin_manager/util_spec.rb
@@ -28,13 +28,13 @@
     end
 
     context "fetch plugin info" do
-      it "should search for the last version infomation non prerelease" do
+      it "should search for the last version information non prerelease" do
         version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name)
         expect(version_info["number"]).to eq("1.0.7")
       end
 
 
-      it "should search for the last version infomation with prerelease" do
+      it "should search for the last version information with prerelease" do
         version_info = LogStash::PluginManager.fetch_latest_version_info(plugin_name, :pre => true)
         expect(version_info["number"]).to eq("2.0.0.pre")
       end
diff --git a/tools/logstash-docgen/README.md b/tools/logstash-docgen/README.md
index 374d81574c1..b2a19abd47a 100644
--- a/tools/logstash-docgen/README.md
+++ b/tools/logstash-docgen/README.md
@@ -26,7 +26,7 @@ logstash-input-kafka > FAIL
 Exceptions: XXXX
 ```
 
-The generator will try to generate the doc for all the plugins defined in the *Gemfile* and installed in Logstash, if anything goes wrong it wont
+The generator will try to generate the doc for all the plugins defined in the *Gemfile* and installed in Logstash, if anything goes wrong it won't
 stop the generation of the other plugin. The Task will also report any errors with stacktraces at the end, if one plugin fail the build,
 you can interrupt the process and it will output the current errors before exiting.
 
@@ -53,7 +53,7 @@ Usages:
 
 ```sh
 bin/logstash-docgen --all # will generate the doc for all the plugins
-bin/logstash-docgen logtash-input-file logstash-input-s3 # generate doc for 2 plugins
+bin/logstash-docgen logstash-input-file logstash-input-s3 # generate doc for 2 plugins
 ```
 
 **See:** `bin/logstash-docgen --help` for complete usage.
diff --git a/tools/logstash-docgen/lib/logstash/docgen/dependency_lookup.rb b/tools/logstash-docgen/lib/logstash/docgen/dependency_lookup.rb
index 7aecb05694d..227f041096d 100644
--- a/tools/logstash-docgen/lib/logstash/docgen/dependency_lookup.rb
+++ b/tools/logstash-docgen/lib/logstash/docgen/dependency_lookup.rb
@@ -37,7 +37,7 @@ def match_core(requirements)
         .select { |v| requirements.any? { |requirement| requirement.satisfied_by?(v) } }
     end
 
-    # Remove betas/aphas and reverse sort
+    # Remove betas/alphas and reverse sort
     def clean_versions(gemspecs)
       gemspecs.collect(&:to_s)
         .collect { |v| v.gsub(PRERELEASES_RE, '') } # remove beta, alphas and snapshots
diff --git a/tools/logstash-docgen/lib/logstash/docgen/dynamic_parser.rb b/tools/logstash-docgen/lib/logstash/docgen/dynamic_parser.rb
index 48a875e9c83..486d71b7ca1 100644
--- a/tools/logstash-docgen/lib/logstash/docgen/dynamic_parser.rb
+++ b/tools/logstash-docgen/lib/logstash/docgen/dynamic_parser.rb
@@ -31,7 +31,7 @@ def parse
 
     # Find all the modules included by the specified class
     # and use `source_location` to find the actual file on disk.
-    # We need to cleanup the values for evalued modules or system module.
+    # We need to cleanup the values for evaluated modules or system module.
     # `included_modules` will return the list of module in the order they appear.
     # this is important because modules can override the documentation of some
     # option.
diff --git a/tools/logstash-docgen/lib/logstash/docgen/github_generator.rb b/tools/logstash-docgen/lib/logstash/docgen/github_generator.rb
index 59887e4dda3..2c15293bc8a 100644
--- a/tools/logstash-docgen/lib/logstash/docgen/github_generator.rb
+++ b/tools/logstash-docgen/lib/logstash/docgen/github_generator.rb
@@ -14,7 +14,7 @@ module LogStash module Docgen
   # generation on a specific plugin.
   #
   # Since the doc generation need access to the current library/dependency and we
-  # dont want to polute the main exection namespace with libraries that could be incompatible
+  # dont want to pollute the main execution namespace with libraries that could be incompatible
   # each execution of the doc is is own process.
   #
   # Its a lot slower, but we know for sure that it uses the latest dependency for each plugins.
diff --git a/tools/logstash-docgen/lib/logstash/docgen/logstash_generator.rb b/tools/logstash-docgen/lib/logstash/docgen/logstash_generator.rb
index 59cff330f1a..93ce1faff8c 100644
--- a/tools/logstash-docgen/lib/logstash/docgen/logstash_generator.rb
+++ b/tools/logstash-docgen/lib/logstash/docgen/logstash_generator.rb
@@ -11,7 +11,7 @@ module LogStash module Docgen
   # This class is used to generate the documentation inside a working logstash
   # directory, it will take all the installed gemspec and gerate the documentation for them.
   #
-  # In pratice we will install **all the plugins** before running this generator.
+  # In practice we will install **all the plugins** before running this generator.
   # This class is invoked inside logstash with a rake task named: `docs:generate-plugins`
   #
   # There is also code to generate `Index` but for now we will still handle them manually.
diff --git a/tools/logstash-docgen/lib/logstash/docgen/parser.rb b/tools/logstash-docgen/lib/logstash/docgen/parser.rb
index f7230d5f4b1..720345171ac 100644
--- a/tools/logstash-docgen/lib/logstash/docgen/parser.rb
+++ b/tools/logstash-docgen/lib/logstash/docgen/parser.rb
@@ -31,7 +31,7 @@ def from_master_json
   # This class acts as the transformation point between the format
   # and the data.
   #
-  # At the beginning of the PoC, I was targetting multiples different format: asciidoc, manpage,
+  # At the beginning of the PoC, I was targeting multiples different format: asciidoc, manpage,
   # since we only support 1 format now, we could probably remove it.
   class Document
     attr_reader :context, :format
@@ -118,7 +118,7 @@ def canonical_name
       "#{CANONICAL_NAME_PREFIX}-#{section}-#{config_name}"
     end
 
-    # Developper can declare options in the order they want
+    # Developer can declare options in the order they want
     # `Hash` keys are sorted by default in the order of creation.
     # But we force a sort options name for the documentation.
     def config
@@ -188,7 +188,7 @@ def self.generate_for_plugin(plugin_source_path, options = {})
     Document.new(context, format)
   end
 
-  # Note that Gem::Specifcation has an internal cache.
+  # Note that Gem::Specification has an internal cache.
   def self.load_plugin_specification(plugin_source_path)
     gemspec = Dir.glob(::File.join(plugin_source_path, "*.gemspec")).first
     raise "Cannot find the gemspec in #{plugin_source_path}" if gemspec.nil?
diff --git a/tools/logstash-docgen/lib/logstash/docgen/static_parser.rb b/tools/logstash-docgen/lib/logstash/docgen/static_parser.rb
index 578543aebe5..de4d9953f52 100644
--- a/tools/logstash-docgen/lib/logstash/docgen/static_parser.rb
+++ b/tools/logstash-docgen/lib/logstash/docgen/static_parser.rb
@@ -164,7 +164,7 @@ def read_file(file)
       @cached_read[file] ||= File.read(file)
     end
 
-    # Let's try to extract a meaninful name for the classes
+    # Let's try to extract a meaningful name for the classes
     # We need to support theses format:
     #
     # class LogStash::Inputs::File # legacy
diff --git a/tools/logstash-docgen/spec/logstash/docgen/task_runner_spec.rb b/tools/logstash-docgen/spec/logstash/docgen/task_runner_spec.rb
index 52deaa06c71..c28d71c7a19 100644
--- a/tools/logstash-docgen/spec/logstash/docgen/task_runner_spec.rb
+++ b/tools/logstash-docgen/spec/logstash/docgen/task_runner_spec.rb
@@ -6,7 +6,7 @@
 
   context "#success?" do
     let(:name) { :making_stuff }
-    let(:error) { OpenStruct.new(:message => "Something bad, OOOPS!") }
+    let(:error) { OpenStruct.new(:message => "Something bad, OOPS!") }
 
     it "returns true when no errors was passed to the class" do
       expect(subject.new(name).success?).to be_truthy
diff --git a/tools/paquet/lib/paquet/rspec/tasks.rb b/tools/paquet/lib/paquet/rspec/tasks.rb
index 29a66e0d830..de4aa9213a6 100644
--- a/tools/paquet/lib/paquet/rspec/tasks.rb
+++ b/tools/paquet/lib/paquet/rspec/tasks.rb
@@ -7,13 +7,13 @@
 require "paquet/gem"
 
 # This class add new rake methods to a an existing ruby gem,
-# these methods allow developpers to create a Uber gem, a uber gem is
+# these methods allow developers to create a Uber gem, a uber gem is
 # a tarball that contains the current gems and one or more of his dependencies.
 #
 # This Tool will take care of looking at the current dependency tree defined in the Gemspec and the gemfile
 # and will traverse all graph and download the gem file into a specified directory.
 #
-# By default, the tool wont fetch everything and the developper need to declare what gems he want to download.
+# By default, the tool won't fetch everything and the developer need to declare what gems he want to download.
 module Paquet
   class Task < Rake::TaskLib
     def initialize(target_path, cache_path = nil, &block)
diff --git a/tools/paquet/spec/paquet/dependency_spec.rb b/tools/paquet/spec/paquet/dependency_spec.rb
index aeab6663aa0..0ed02f9fe08 100644
--- a/tools/paquet/spec/paquet/dependency_spec.rb
+++ b/tools/paquet/spec/paquet/dependency_spec.rb
@@ -30,7 +30,7 @@
     end
   end
 
-  it "return a meaninful string" do
+  it "return a meaningful string" do
     expect(subject.to_s).to eq("#{name}-#{version}")
   end
 end
diff --git a/tools/paquet/spec/paquet/utils_spec.rb b/tools/paquet/spec/paquet/utils_spec.rb
index de636526d7a..d2c0f207408 100644
--- a/tools/paquet/spec/paquet/utils_spec.rb
+++ b/tools/paquet/spec/paquet/utils_spec.rb
@@ -46,7 +46,7 @@
     let(:redirect_response) { instance_double("Net::HTTP::Response", :code => "302", :headers => { "location" => "https://localhost:8888/new_path" }) }
     let(:response_ok) { instance_double("Net::HTTP::Response", :code => "200") }
 
-    context "less than the maximun of redirection" do
+    context "less than the maximum of redirection" do
       let(:redirect_url) { "https://localhost:8898/redirect/my-file.txt"}
 
       before do
