diff --git a/.github/PULL_REQUEST_TEMPLATE.md.unused b/.github/PULL_REQUEST_TEMPLATE.md.unused
new file mode 100644
index 00000000000..a1538275aec
--- /dev/null
+++ b/.github/PULL_REQUEST_TEMPLATE.md.unused
@@ -0,0 +1 @@
+Thanks for contributing to Logstash! If you haven't already signed our CLA, here's a handy link: https://www.elastic.co/contributor-agreement/
diff --git a/Gemfile b/Gemfile
index c896ca06fa3..6ca8ab91fc0 100644
--- a/Gemfile
+++ b/Gemfile
@@ -6,6 +6,7 @@ gem "logstash-core", :path => "./logstash-core"
 gem "logstash-core-queue-jruby", :path => "./logstash-core-queue-jruby"
 gem "logstash-core-event-java", :path => "./logstash-core-event-java"
 gem "logstash-core-plugin-api", :path => "./logstash-core-plugin-api"
+gem "paquet", "~> 0.2.0"
 gem "ruby-progressbar", "~> 1.8.1"
 gem "builder", "~> 3.2.2"
 gem "file-dependencies", "0.1.6"
@@ -24,7 +25,7 @@ gem "rack-test", :require => "rack/test", :group => :development
 gem "flores", "~> 0.0.6", :group => :development
 gem "term-ansicolor", "~> 1.3.2", :group => :development
 gem "docker-api", "1.31.0", :group => :development
-gem "pleaserun", "~>0.0.27"
+gem "pleaserun", "~>0.0.28"
 gem "logstash-input-heartbeat"
 gem "logstash-codec-collectd"
 gem "logstash-output-xmpp"
@@ -92,7 +93,7 @@ gem "logstash-input-twitter"
 gem "logstash-input-udp"
 gem "logstash-input-unix"
 gem "logstash-input-xmpp"
-gem "logstash-input-kafka", "< 6.0.0"
+gem "logstash-input-kafka"
 gem "logstash-input-beats"
 gem "logstash-output-cloudwatch"
 gem "logstash-output-csv"
@@ -101,7 +102,7 @@ gem "logstash-output-file"
 gem "logstash-output-graphite"
 gem "logstash-output-http"
 gem "logstash-output-irc"
-gem "logstash-output-kafka", "< 6.0.0"
+gem "logstash-output-kafka"
 gem "logstash-output-nagios"
 gem "logstash-output-null"
 gem "logstash-output-pagerduty"
diff --git a/Gemfile.jruby-1.9.lock b/Gemfile.jruby-1.9.lock
deleted file mode 100644
index 47e595a025e..00000000000
--- a/Gemfile.jruby-1.9.lock
+++ /dev/null
@@ -1,736 +0,0 @@
-PATH
-  remote: ./logstash-core
-  specs:
-    logstash-core (5.1.2-java)
-      chronic_duration (= 0.10.6)
-      clamp (~> 0.6.5)
-      concurrent-ruby (= 1.0.0)
-      filesize (= 0.0.4)
-      gems (~> 0.8.3)
-      i18n (= 0.6.9)
-      jar-dependencies
-      jrjackson (~> 0.4.0)
-      jrmonitor (~> 0.4.2)
-      jruby-openssl (= 0.9.16)
-      logstash-core-event-java (= 5.1.2)
-      logstash-core-queue-jruby (= 5.1.2)
-      minitar (~> 0.5.4)
-      pry (~> 0.10.1)
-      puma (~> 2.16)
-      ruby-maven (~> 3.3.9)
-      rubyzip (~> 1.1.7)
-      sinatra (~> 1.4, >= 1.4.6)
-      stud (~> 0.0.19)
-      thread_safe (~> 0.3.5)
-      treetop (< 1.5.0)
-
-PATH
-  remote: ./logstash-core-event-java
-  specs:
-    logstash-core-event-java (5.1.2-java)
-      jar-dependencies
-      ruby-maven (~> 3.3.9)
-
-PATH
-  remote: ./logstash-core-plugin-api
-  specs:
-    logstash-core-plugin-api (2.1.20-java)
-      logstash-core (= 5.1.2)
-
-PATH
-  remote: ./logstash-core-queue-jruby
-  specs:
-    logstash-core-queue-jruby (5.1.2-java)
-
-GEM
-  remote: https://rubygems.org/
-  specs:
-    addressable (2.3.8)
-    arr-pm (0.0.10)
-      cabin (> 0)
-    atomic (1.1.99-java)
-    avl_tree (1.2.1)
-      atomic (~> 1.1)
-    awesome_print (1.7.0)
-    aws-sdk (2.3.22)
-      aws-sdk-resources (= 2.3.22)
-    aws-sdk-core (2.3.22)
-      jmespath (~> 1.0)
-    aws-sdk-resources (2.3.22)
-      aws-sdk-core (= 2.3.22)
-    aws-sdk-v1 (1.66.0)
-      json (~> 1.4)
-      nokogiri (>= 1.4.4)
-    backports (3.6.8)
-    benchmark-ips (2.7.2)
-    bindata (2.3.4)
-    buftok (0.2.0)
-    builder (3.2.2)
-    cabin (0.9.0)
-    childprocess (0.5.9)
-      ffi (~> 1.0, >= 1.0.11)
-    chronic_duration (0.10.6)
-      numerizer (~> 0.1.1)
-    ci_reporter (2.0.0)
-      builder (>= 2.1.2)
-    ci_reporter_rspec (1.0.0)
-      ci_reporter (~> 2.0)
-      rspec (>= 2.14, < 4)
-    cinch (2.3.3)
-    clamp (0.6.5)
-    coderay (1.1.1)
-    concurrent-ruby (1.0.0-java)
-    diff-lcs (1.2.5)
-    docile (1.1.5)
-    docker-api (1.31.0)
-      excon (>= 0.38.0)
-      json
-    domain_name (0.5.20161129)
-      unf (>= 0.0.5, < 1.0.0)
-    dotenv (2.1.1)
-    edn (1.1.1)
-    elasticsearch (1.1.0)
-      elasticsearch-api (= 1.1.0)
-      elasticsearch-transport (= 1.1.0)
-    elasticsearch-api (1.1.0)
-      multi_json
-    elasticsearch-transport (1.1.0)
-      faraday
-      multi_json
-    equalizer (0.0.10)
-    excon (0.54.0)
-    faraday (0.9.2)
-      multipart-post (>= 1.2, < 3)
-    ffi (1.9.14-java)
-    file-dependencies (0.1.6)
-      minitar
-    filesize (0.0.4)
-    filewatch (0.9.0)
-    fivemat (1.3.2)
-    flores (0.0.7)
-    fpm (1.3.3)
-      arr-pm (~> 0.0.9)
-      backports (>= 2.6.2)
-      cabin (>= 0.6.0)
-      childprocess
-      clamp (~> 0.6)
-      ffi
-      json (>= 1.7.7)
-    gelfd (0.2.0)
-    gem_publisher (1.5.0)
-    gems (0.8.3)
-    hitimes (1.2.4-java)
-    http (0.9.9)
-      addressable (~> 2.3)
-      http-cookie (~> 1.0)
-      http-form_data (~> 1.0.1)
-      http_parser.rb (~> 0.6.0)
-    http-cookie (1.0.3)
-      domain_name (~> 0.5)
-    http-form_data (1.0.1)
-    http_parser.rb (0.6.0-java)
-    i18n (0.6.9)
-    insist (1.0.0)
-    jar-dependencies (0.3.7)
-    jls-grok (0.11.4)
-      cabin (>= 0.6.0)
-    jls-lumberjack (0.0.26)
-      concurrent-ruby
-    jmespath (1.3.1)
-    jrjackson (0.4.2-java)
-    jrmonitor (0.4.2)
-    jruby-openssl (0.9.16-java)
-    jruby-stdin-channel (0.2.0-java)
-    json (1.8.3-java)
-    kramdown (1.13.1)
-    logstash-codec-cef (4.1.0-java)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-collectd (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-dots (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-edn (3.0.2)
-      edn
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-edn_lines (3.0.2)
-      edn
-      logstash-codec-line
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-es_bulk (3.0.3)
-      logstash-codec-line
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-fluent (3.0.2-java)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      msgpack-jruby
-    logstash-codec-graphite (3.0.2)
-      logstash-codec-line
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-json (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-json_lines (3.0.2)
-      logstash-codec-line (>= 2.1.0)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-line (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-msgpack (3.0.2-java)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      msgpack-jruby
-    logstash-codec-multiline (3.0.3)
-      jls-grok (~> 0.11.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-patterns-core
-    logstash-codec-netflow (3.1.2)
-      bindata (>= 1.5.0)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-plain (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-codec-rubydebug (3.0.2)
-      awesome_print
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-devutils (1.2.1-java)
-      fivemat
-      gem_publisher
-      insist (= 1.0.0)
-      kramdown
-      logstash-core-plugin-api (>= 2.0, <= 2.99)
-      minitar
-      rake
-      rspec (~> 3.0)
-      rspec-wait
-      stud (>= 0.0.20)
-    logstash-filter-clone (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-csv (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-date (3.1.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-dns (3.0.3)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      lru_redux (~> 1.1.0)
-    logstash-filter-drop (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-fingerprint (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      murmurhash3
-    logstash-filter-geoip (4.0.4-java)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-grok (3.3.0)
-      jls-grok (~> 0.11.3)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-patterns-core
-    logstash-filter-json (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-kv (3.1.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-metrics (4.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      metriks
-      thread_safe
-    logstash-filter-mutate (3.1.3)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-ruby (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-filter-date
-    logstash-filter-sleep (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-split (3.1.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-syslog_pri (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-throttle (4.0.1)
-      atomic
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      thread_safe
-    logstash-filter-urldecode (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-useragent (3.0.3)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      lru_redux (~> 1.1.0)
-      user_agent_parser (>= 2.0.0)
-    logstash-filter-uuid (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-filter-xml (4.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      nokogiri
-      xml-simple
-    logstash-input-beats (3.1.12-java)
-      concurrent-ruby (>= 0.9.2, <= 1.0.0)
-      jar-dependencies (~> 0.3.4)
-      logstash-codec-multiline (>= 2.0.5)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      thread_safe (~> 0.3.5)
-    logstash-input-couchdb_changes (3.1.0)
-      json
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (>= 0.0.22)
-    logstash-input-elasticsearch (4.0.0)
-      elasticsearch (~> 1.0, >= 1.0.6)
-      logstash-codec-json
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-exec (3.1.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (~> 0.0.22)
-    logstash-input-file (4.0.0)
-      addressable
-      filewatch (~> 0.8, >= 0.8.1)
-      logstash-codec-multiline (~> 3.0)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-ganglia (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (~> 0.0.22)
-    logstash-input-gelf (3.0.2)
-      gelfd (= 0.2.0)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (~> 0.0.22)
-    logstash-input-generator (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-graphite (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-input-tcp
-    logstash-input-heartbeat (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud
-    logstash-input-http (3.0.3)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      puma (~> 2.16, >= 2.16.0)
-      rack (~> 1)
-      stud
-    logstash-input-http_poller (3.1.0)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-http_client (>= 2.2.4, < 5.0.0)
-      rufus-scheduler (~> 3.0.9)
-      stud (~> 0.0.22)
-    logstash-input-imap (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      mail (~> 2.6.3)
-      mime-types (= 2.6.2)
-      stud (~> 0.0.22)
-    logstash-input-irc (3.0.2)
-      cinch
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (~> 0.0.22)
-    logstash-input-jdbc (4.1.3)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      rufus-scheduler
-      sequel
-      tzinfo
-      tzinfo-data
-    logstash-input-kafka (5.1.0)
-      logstash-codec-json
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (>= 0.0.22, < 0.1.0)
-    logstash-input-log4j (3.0.3-java)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-lumberjack (3.1.1)
-      concurrent-ruby
-      jls-lumberjack (~> 0.0.26)
-      logstash-codec-multiline (~> 3.0)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-pipe (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (~> 0.0.22)
-    logstash-input-rabbitmq (5.2.0)
-      logstash-codec-json
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-rabbitmq_connection (>= 4.2.0, < 5.0.0)
-    logstash-input-redis (3.1.1)
-      logstash-codec-json
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      redis
-    logstash-input-s3 (3.1.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-aws
-      stud (~> 0.0.18)
-    logstash-input-snmptrap (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      snmp
-    logstash-input-sqs (3.0.2)
-      logstash-codec-json
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-aws (>= 1.0.0)
-    logstash-input-stdin (3.2.0)
-      concurrent-ruby
-      jruby-stdin-channel
-      logstash-codec-line
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-syslog (3.2.0)
-      concurrent-ruby
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-filter-date
-      logstash-filter-grok
-      stud (>= 0.0.22, < 0.1.0)
-      thread_safe
-    logstash-input-tcp (4.1.0)
-      logstash-codec-json
-      logstash-codec-json_lines
-      logstash-codec-line
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-twitter (3.0.3)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (>= 0.0.22, < 0.1)
-      twitter (= 5.15.0)
-    logstash-input-udp (3.1.0)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud (~> 0.0.22)
-    logstash-input-unix (3.0.2)
-      logstash-codec-line
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-input-xmpp (3.1.1)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      xmpp4r (= 0.5)
-    logstash-mixin-aws (4.2.0)
-      aws-sdk (~> 2.3.0)
-      aws-sdk-v1 (>= 1.61.0)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-mixin-http_client (4.0.3)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      manticore (>= 0.5.2, < 1.0.0)
-    logstash-mixin-rabbitmq_connection (4.2.1-java)
-      march_hare (~> 2.20.0)
-      stud (~> 0.0.22)
-    logstash-output-cloudwatch (3.0.3)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-aws (>= 1.0.0)
-      rufus-scheduler (~> 3.0.9)
-    logstash-output-csv (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-filter-json
-      logstash-input-generator
-      logstash-output-file
-    logstash-output-elasticsearch (5.4.0-java)
-      cabin (~> 0.6)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      manticore (>= 0.5.4, < 1.0.0)
-      stud (~> 0.0, >= 0.0.17)
-    logstash-output-file (4.0.1)
-      logstash-codec-json_lines
-      logstash-codec-line
-      logstash-core-plugin-api (>= 2.0.0, < 2.99)
-    logstash-output-graphite (3.1.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-http (3.1.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-http_client (>= 2.2.1, < 5.0.0)
-    logstash-output-irc (3.0.2)
-      cinch
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-kafka (5.1.1)
-      logstash-codec-json
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-nagios (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-null (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-pagerduty (3.0.3)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-pipe (3.0.2)
-      logstash-codec-plain
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-rabbitmq (4.0.4-java)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-rabbitmq_connection (>= 4.1.1, < 5.0.0)
-    logstash-output-redis (3.0.3)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      redis
-      stud
-    logstash-output-s3 (3.2.0)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-aws
-      stud (~> 0.0.22)
-    logstash-output-sns (4.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-aws (>= 1.0.0)
-    logstash-output-sqs (3.0.2)
-      aws-sdk
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-mixin-aws (>= 1.0.0)
-      stud
-    logstash-output-statsd (3.1.1)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      logstash-input-generator
-      statsd-ruby (= 1.2.0)
-    logstash-output-stdout (3.1.0)
-      logstash-codec-line
-      logstash-core-plugin-api (>= 1.60.1, < 2.99)
-    logstash-output-tcp (4.0.0)
-      logstash-codec-json
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      stud
-    logstash-output-udp (3.0.2)
-      logstash-codec-json
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    logstash-output-webhdfs (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      snappy (= 0.0.12)
-      webhdfs
-    logstash-output-xmpp (3.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-      xmpp4r (= 0.5)
-    logstash-patterns-core (4.0.2)
-      logstash-core-plugin-api (>= 1.60, <= 2.99)
-    lru_redux (1.1.0)
-    mail (2.6.4)
-      mime-types (>= 1.16, < 4)
-    manticore (0.6.0-java)
-    march_hare (2.20.0-java)
-    memoizable (0.4.2)
-      thread_safe (~> 0.3, >= 0.3.1)
-    method_source (0.8.2)
-    metriks (0.9.9.7)
-      atomic (~> 1.0)
-      avl_tree (~> 1.2.0)
-      hitimes (~> 1.1)
-    mime-types (2.6.2)
-    minitar (0.5.4)
-    msgpack-jruby (1.4.1-java)
-    multi_json (1.12.1)
-    multipart-post (2.0.0)
-    murmurhash3 (0.1.6-java)
-    mustache (0.99.8)
-    naught (1.1.0)
-    nokogiri (1.6.8.1-java)
-    numerizer (0.1.1)
-    octokit (3.8.0)
-      sawyer (~> 0.6.0, >= 0.5.3)
-    pleaserun (0.0.27)
-      cabin (> 0)
-      clamp
-      dotenv
-      insist
-      mustache (= 0.99.8)
-      stud
-    polyglot (0.3.5)
-    pry (0.10.4-java)
-      coderay (~> 1.1.0)
-      method_source (~> 0.8.1)
-      slop (~> 3.4)
-      spoon (~> 0.0)
-    puma (2.16.0-java)
-    rack (1.6.5)
-    rack-protection (1.5.3)
-      rack
-    rack-test (0.6.3)
-      rack (>= 1.0)
-    rake (12.0.0)
-    redis (3.3.2)
-    rspec (3.1.0)
-      rspec-core (~> 3.1.0)
-      rspec-expectations (~> 3.1.0)
-      rspec-mocks (~> 3.1.0)
-    rspec-core (3.1.7)
-      rspec-support (~> 3.1.0)
-    rspec-expectations (3.1.2)
-      diff-lcs (>= 1.2.0, < 2.0)
-      rspec-support (~> 3.1.0)
-    rspec-mocks (3.1.3)
-      rspec-support (~> 3.1.0)
-    rspec-support (3.1.2)
-    rspec-wait (0.0.9)
-      rspec (>= 3, < 4)
-    ruby-maven (3.3.12)
-      ruby-maven-libs (~> 3.3.9)
-    ruby-maven-libs (3.3.9)
-    ruby-progressbar (1.8.1)
-    rubyzip (1.1.7)
-    rufus-scheduler (3.0.9)
-      tzinfo
-    sawyer (0.6.0)
-      addressable (~> 2.3.5)
-      faraday (~> 0.8, < 0.10)
-    sequel (4.41.0)
-    simple_oauth (0.3.1)
-    simplecov (0.12.0)
-      docile (~> 1.1.0)
-      json (>= 1.8, < 3)
-      simplecov-html (~> 0.10.0)
-    simplecov-html (0.10.0)
-    sinatra (1.4.7)
-      rack (~> 1.5)
-      rack-protection (~> 1.4)
-      tilt (>= 1.3, < 3)
-    slop (3.6.0)
-    snappy (0.0.12-java)
-      snappy-jars (~> 1.1.0)
-    snappy-jars (1.1.0.1.2-java)
-    snmp (1.2.0)
-    spoon (0.0.6)
-      ffi
-    statsd-ruby (1.2.0)
-    stud (0.0.22)
-    term-ansicolor (1.3.2)
-      tins (~> 1.0)
-    thread_safe (0.3.5-java)
-    tilt (2.0.5)
-    tins (1.6.0)
-    treetop (1.4.15)
-      polyglot
-      polyglot (>= 0.3.1)
-    twitter (5.15.0)
-      addressable (~> 2.3)
-      buftok (~> 0.2.0)
-      equalizer (= 0.0.10)
-      faraday (~> 0.9.0)
-      http (>= 0.4, < 0.10)
-      http_parser.rb (~> 0.6.0)
-      json (~> 1.8)
-      memoizable (~> 0.4.0)
-      naught (~> 1.0)
-      simple_oauth (~> 0.3.0)
-    tzinfo (1.2.2)
-      thread_safe (~> 0.1)
-    tzinfo-data (1.2016.10)
-      tzinfo (>= 1.0.0)
-    unf (0.1.4-java)
-    user_agent_parser (2.3.0)
-    webhdfs (0.8.0)
-      addressable
-    xml-simple (1.1.5)
-    xmpp4r (0.5)
-
-PLATFORMS
-  java
-
-DEPENDENCIES
-  benchmark-ips
-  builder (~> 3.2.2)
-  ci_reporter_rspec (= 1.0.0)
-  docker-api (= 1.31.0)
-  file-dependencies (= 0.1.6)
-  flores (~> 0.0.6)
-  fpm (~> 1.3.3)
-  gems (~> 0.8.3)
-  logstash-codec-cef
-  logstash-codec-collectd
-  logstash-codec-dots
-  logstash-codec-edn
-  logstash-codec-edn_lines
-  logstash-codec-es_bulk
-  logstash-codec-fluent
-  logstash-codec-graphite
-  logstash-codec-json
-  logstash-codec-json_lines
-  logstash-codec-line
-  logstash-codec-msgpack
-  logstash-codec-multiline
-  logstash-codec-netflow
-  logstash-codec-plain
-  logstash-codec-rubydebug
-  logstash-core!
-  logstash-core-event-java!
-  logstash-core-plugin-api!
-  logstash-core-queue-jruby!
-  logstash-devutils
-  logstash-filter-clone
-  logstash-filter-csv
-  logstash-filter-date
-  logstash-filter-dns
-  logstash-filter-drop
-  logstash-filter-fingerprint
-  logstash-filter-geoip
-  logstash-filter-grok
-  logstash-filter-json
-  logstash-filter-kv
-  logstash-filter-metrics
-  logstash-filter-mutate
-  logstash-filter-ruby
-  logstash-filter-sleep
-  logstash-filter-split
-  logstash-filter-syslog_pri
-  logstash-filter-throttle
-  logstash-filter-urldecode
-  logstash-filter-useragent
-  logstash-filter-uuid
-  logstash-filter-xml
-  logstash-input-beats
-  logstash-input-couchdb_changes
-  logstash-input-elasticsearch
-  logstash-input-exec
-  logstash-input-file
-  logstash-input-ganglia
-  logstash-input-gelf
-  logstash-input-generator
-  logstash-input-graphite
-  logstash-input-heartbeat
-  logstash-input-http
-  logstash-input-http_poller
-  logstash-input-imap
-  logstash-input-irc
-  logstash-input-jdbc
-  logstash-input-kafka (< 6.0.0)
-  logstash-input-log4j
-  logstash-input-lumberjack
-  logstash-input-pipe
-  logstash-input-rabbitmq
-  logstash-input-redis
-  logstash-input-s3
-  logstash-input-snmptrap
-  logstash-input-sqs
-  logstash-input-stdin
-  logstash-input-syslog
-  logstash-input-tcp
-  logstash-input-twitter
-  logstash-input-udp
-  logstash-input-unix
-  logstash-input-xmpp
-  logstash-output-cloudwatch
-  logstash-output-csv
-  logstash-output-elasticsearch
-  logstash-output-file
-  logstash-output-graphite
-  logstash-output-http
-  logstash-output-irc
-  logstash-output-kafka (< 6.0.0)
-  logstash-output-nagios
-  logstash-output-null
-  logstash-output-pagerduty
-  logstash-output-pipe
-  logstash-output-rabbitmq
-  logstash-output-redis
-  logstash-output-s3
-  logstash-output-sns
-  logstash-output-sqs
-  logstash-output-statsd
-  logstash-output-stdout
-  logstash-output-tcp
-  logstash-output-udp
-  logstash-output-webhdfs
-  logstash-output-xmpp
-  octokit (= 3.8.0)
-  pleaserun (~> 0.0.27)
-  rack-test
-  rspec (~> 3.1.0)
-  ruby-progressbar (~> 1.8.1)
-  rubyzip (~> 1.1.7)
-  simplecov
-  stud (~> 0.0.22)
-  term-ansicolor (~> 1.3.2)
-  tins (= 1.6)
diff --git a/LICENSE b/LICENSE
index 43976b73b2b..40f7cd2fb3e 100644
--- a/LICENSE
+++ b/LICENSE
@@ -1,4 +1,4 @@
-Copyright (c) 2012–2016 Elasticsearch <http://www.elastic.co>
+Copyright (c) 2012–2017 Elasticsearch <http://www.elastic.co>
 
 Licensed under the Apache License, Version 2.0 (the "License");
 you may not use this file except in compliance with the License.
diff --git a/NOTICE.TXT b/NOTICE.TXT
index 0b8a9475f05..a6cf9dfb96d 100644
--- a/NOTICE.TXT
+++ b/NOTICE.TXT
@@ -1,5 +1,571 @@
-Elasticsearch
-Copyright 2012-2015 Elasticsearch
+Logstash
+Copyright 2012-2017 Elasticsearch
+
+This product includes software developed by The Apache Software Foundation (http://www.apache.org/).
+
+==========================================================================
+Third party libraries bundled by the Logstash project:
+
+==========================================================================
+RubyGem: edn Version: 1.1.1
+Copyright (c) 2012 Relevance Inc & Clinton N. Dreisbach
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: awesome_print Version: 1.7.0
+Copyright (c) 2010-2013 Michael Dvorkin
+http://www.dvorkin.net
+%w(mike dvorkin.net) * "@" || "twitter.com/mid"
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: lru_redux Version: 1.1.0
+Copyright (c) 2013 Sam Saffron
+
+MIT License
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: user_agent_parser Version: 2.3.0
+Copyright (c) 2012 Tim Lucas
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+==========================================================================
+RubyGem: elasticsearch Version: 1.1.0
+   Copyright 2013 Elasticsearch
+
+   Licensed under the Apache License, Version 2.0 (the "License");
+   you may not use this file except in compliance with the License.
+   You may obtain a copy of the License at
+
+       http://www.apache.org/licenses/LICENSE-2.0
+
+   Unless required by applicable law or agreed to in writing, software
+   distributed under the License is distributed on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+   See the License for the specific language governing permissions and
+   limitations under the License.
+
+==========================================================================
+RubyGem: addressable Version: 2.3.8
+
+                              Apache License
+                        Version 2.0, January 2004
+                     http://www.apache.org/licenses/
+
+TERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION
+
+1. Definitions.
+
+   "License" shall mean the terms and conditions for use, reproduction,
+   and distribution as defined by Sections 1 through 9 of this document.
+
+   "Licensor" shall mean the copyright owner or entity authorized by
+   the copyright owner that is granting the License.
+
+   "Legal Entity" shall mean the union of the acting entity and all
+   other entities that control, are controlled by, or are under common
+   control with that entity. For the purposes of this definition,
+   "control" means (i) the power, direct or indirect, to cause the
+   direction or management of such entity, whether by contract or
+   otherwise, or (ii) ownership of fifty percent (50%) or more of the
+   outstanding shares, or (iii) beneficial ownership of such entity.
+
+   "You" (or "Your") shall mean an individual or Legal Entity
+   exercising permissions granted by this License.
+
+   "Source" form shall mean the preferred form for making modifications,
+   including but not limited to software source code, documentation
+   source, and configuration files.
+
+   "Object" form shall mean any form resulting from mechanical
+   transformation or translation of a Source form, including but
+   not limited to compiled object code, generated documentation,
+   and conversions to other media types.
+
+   "Work" shall mean the work of authorship, whether in Source or
+   Object form, made available under the License, as indicated by a
+   copyright notice that is included in or attached to the work
+   (an example is provided in the Appendix below).
+
+   "Derivative Works" shall mean any work, whether in Source or Object
+   form, that is based on (or derived from) the Work and for which the
+   editorial revisions, annotations, elaborations, or other modifications
+   represent, as a whole, an original work of authorship. For the purposes
+   of this License, Derivative Works shall not include works that remain
+   separable from, or merely link (or bind by name) to the interfaces of,
+   the Work and Derivative Works thereof.
+
+   "Contribution" shall mean any work of authorship, including
+   the original version of the Work and any modifications or additions
+   to that Work or Derivative Works thereof, that is intentionally
+   submitted to Licensor for inclusion in the Work by the copyright owner
+   or by an individual or Legal Entity authorized to submit on behalf of
+   the copyright owner. For the purposes of this definition, "submitted"
+   means any form of electronic, verbal, or written communication sent
+   to the Licensor or its representatives, including but not limited to
+   communication on electronic mailing lists, source code control systems,
+   and issue tracking systems that are managed by, or on behalf of, the
+   Licensor for the purpose of discussing and improving the Work, but
+   excluding communication that is conspicuously marked or otherwise
+   designated in writing by the copyright owner as "Not a Contribution."
+
+   "Contributor" shall mean Licensor and any individual or Legal Entity
+   on behalf of whom a Contribution has been received by Licensor and
+   subsequently incorporated within the Work.
+
+2. Grant of Copyright License. Subject to the terms and conditions of
+   this License, each Contributor hereby grants to You a perpetual,
+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+   copyright license to reproduce, prepare Derivative Works of,
+   publicly display, publicly perform, sublicense, and distribute the
+   Work and such Derivative Works in Source or Object form.
+
+3. Grant of Patent License. Subject to the terms and conditions of
+   this License, each Contributor hereby grants to You a perpetual,
+   worldwide, non-exclusive, no-charge, royalty-free, irrevocable
+   (except as stated in this section) patent license to make, have made,
+   use, offer to sell, sell, import, and otherwise transfer the Work,
+   where such license applies only to those patent claims licensable
+   by such Contributor that are necessarily infringed by their
+   Contribution(s) alone or by combination of their Contribution(s)
+   with the Work to which such Contribution(s) was submitted. If You
+   institute patent litigation against any entity (including a
+   cross-claim or counterclaim in a lawsuit) alleging that the Work
+   or a Contribution incorporated within the Work constitutes direct
+   or contributory patent infringement, then any patent licenses
+   granted to You under this License for that Work shall terminate
+   as of the date such litigation is filed.
+
+4. Redistribution. You may reproduce and distribute copies of the
+   Work or Derivative Works thereof in any medium, with or without
+   modifications, and in Source or Object form, provided that You
+   meet the following conditions:
+
+   (a) You must give any other recipients of the Work or
+       Derivative Works a copy of this License; and
+
+   (b) You must cause any modified files to carry prominent notices
+       stating that You changed the files; and
+
+   (c) You must retain, in the Source form of any Derivative Works
+       that You distribute, all copyright, patent, trademark, and
+       attribution notices from the Source form of the Work,
+       excluding those notices that do not pertain to any part of
+       the Derivative Works; and
+
+   (d) If the Work includes a "NOTICE" text file as part of its
+       distribution, then any Derivative Works that You distribute must
+       include a readable copy of the attribution notices contained
+       within such NOTICE file, excluding those notices that do not
+       pertain to any part of the Derivative Works, in at least one
+       of the following places: within a NOTICE text file distributed
+       as part of the Derivative Works; within the Source form or
+       documentation, if provided along with the Derivative Works; or,
+       within a display generated by the Derivative Works, if and
+       wherever such third-party notices normally appear. The contents
+       of the NOTICE file are for informational purposes only and
+       do not modify the License. You may add Your own attribution
+       notices within Derivative Works that You distribute, alongside
+       or as an addendum to the NOTICE text from the Work, provided
+       that such additional attribution notices cannot be construed
+       as modifying the License.
+
+   You may add Your own copyright statement to Your modifications and
+   may provide additional or different license terms and conditions
+   for use, reproduction, or distribution of Your modifications, or
+   for any such Derivative Works as a whole, provided Your use,
+   reproduction, and distribution of the Work otherwise complies with
+   the conditions stated in this License.
+
+5. Submission of Contributions. Unless You explicitly state otherwise,
+   any Contribution intentionally submitted for inclusion in the Work
+   by You to the Licensor shall be under the terms and conditions of
+   this License, without any additional terms or conditions.
+   Notwithstanding the above, nothing herein shall supersede or modify
+   the terms of any separate license agreement you may have executed
+   with Licensor regarding such Contributions.
+
+6. Trademarks. This License does not grant permission to use the trade
+   names, trademarks, service marks, or product names of the Licensor,
+   except as required for reasonable and customary use in describing the
+   origin of the Work and reproducing the content of the NOTICE file.
+
+7. Disclaimer of Warranty. Unless required by applicable law or
+   agreed to in writing, Licensor provides the Work (and each
+   Contributor provides its Contributions) on an "AS IS" BASIS,
+   WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or
+   implied, including, without limitation, any warranties or conditions
+   of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A
+   PARTICULAR PURPOSE. You are solely responsible for determining the
+   appropriateness of using or redistributing the Work and assume any
+   risks associated with Your exercise of permissions under this License.
+
+8. Limitation of Liability. In no event and under no legal theory,
+   whether in tort (including negligence), contract, or otherwise,
+   unless required by applicable law (such as deliberate and grossly
+   negligent acts) or agreed to in writing, shall any Contributor be
+   liable to You for damages, including any direct, indirect, special,
+   incidental, or consequential damages of any character arising as a
+   result of this License or out of the use or inability to use the
+   Work (including but not limited to damages for loss of goodwill,
+   work stoppage, computer failure or malfunction, or any and all
+   other commercial damages or losses), even if such Contributor
+   has been advised of the possibility of such damages.
+
+9. Accepting Warranty or Additional Liability. While redistributing
+   the Work or Derivative Works thereof, You may choose to offer,
+   and charge a fee for, acceptance of support, warranty, indemnity,
+   or other liability obligations and/or rights consistent with this
+   License. However, in accepting such obligations, You may act only
+   on Your own behalf and on Your sole responsibility, not on behalf
+   of any other Contributor, and only if You agree to indemnify,
+   defend, and hold each Contributor harmless for any liability
+   incurred by, or claims asserted against, such Contributor by reason
+   of your accepting any such warranty or additional liability.
+
+END OF TERMS AND CONDITIONS
+
+APPENDIX: How to apply the Apache License to your work.
+
+   To apply the Apache License to your work, attach the following
+   boilerplate notice, with the fields enclosed by brackets "[]"
+   replaced with your own identifying information. (Don't include
+   the brackets!)  The text should be enclosed in the appropriate
+   comment syntax for the file format. We also recommend that a
+   file or class name and description of purpose be included on the
+   same "printed page" as the copyright notice for easier
+   identification within third-party archives.
+
+Copyright [yyyy] [name of copyright owner]
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+==========================================================================
+RubyGem: rufus-scheduler Version: 3.0.9
+
+Copyright (c) 2005-2014, John Mettraux, jmettraux@gmail.com
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to deal
+in the Software without restriction, including without limitation the rights
+to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN
+THE SOFTWARE.
+
+
+==========================================================================
+RubyGem: mail Version: 2.6.4
+Copyright (c) 2009-2016 Mikel Lindsaar
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+'Software'), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED 'AS IS', WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT.
+IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY
+CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT,
+TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE
+SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: cinch Version: 2.3.3
+Copyright (c) 2010 Lee Jarvis, Dominik Honnef
+Copyright (c) 2011 Dominik Honnef
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: sequel Version: 4.42.1
+Copyright (c) 2007-2008 Sharon Rosner
+Copyright (c) 2008-2016 Jeremy Evans
+
+Permission is hereby granted, free of charge, to any person obtaining a copy
+of this software and associated documentation files (the "Software"), to
+deal in the Software without restriction, including without limitation the
+rights to use, copy, modify, merge, publish, distribute, sublicense, and/or
+sell copies of the Software, and to permit persons to whom the Software is
+furnished to do so, subject to the following conditions:
+  
+The above copyright notice and this permission notice shall be included in
+all copies or substantial portions of the Software.
+   
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL
+THE AUTHORS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER 
+IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN
+CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: tzinfo Version: 1.2.2
+Copyright (c) 2005-2014 Philip Ross
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of 
+this software and associated documentation files (the "Software"), to deal in 
+the Software without restriction, including without limitation the rights to 
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
+of the Software, and to permit persons to whom the Software is furnished to do 
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all 
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR 
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER 
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, 
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN 
+THE SOFTWARE.
+
+==========================================================================
+RubyGem: tzinfo-data Version: 1.2016.10
+Copyright (c) 2005-2016 Philip Ross
+
+Permission is hereby granted, free of charge, to any person obtaining a copy of 
+this software and associated documentation files (the "Software"), to deal in 
+the Software without restriction, including without limitation the rights to 
+use, copy, modify, merge, publish, distribute, sublicense, and/or sell copies 
+of the Software, and to permit persons to whom the Software is furnished to do 
+so, subject to the following conditions:
+
+The above copyright notice and this permission notice shall be included in all 
+copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR 
+IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, 
+FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE 
+AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER 
+LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, 
+OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN 
+THE SOFTWARE.
+
+==========================================================================
+RubyGem: redis Version: 3.3.2
+Copyright (c) 2009 Ezra Zygmuntowicz
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+==========================================================================
+RubyGem: twitter Version: 5.15.0
+Copyright (c) 2006-2015 Erik Michaels-Ober, John Nunemaker, Wynn Netherland, Steve Richert, Steve Agalloco
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: jar-dependencies Version: 0.3.9
+Copyright (c) 2014 Christian Meier
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
+
+==========================================================================
+RubyGem: cabin Version: 0.9.0
+Copyright 2011 Jordan Sissel
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
+
+
+==========================================================================
+RubyGem: statsd-ruby Version: 1.2.0
+Copyright (c) 2011, 2012, 2013 Rein Henrichs
+
+Permission is hereby granted, free of charge, to any person obtaining
+a copy of this software and associated documentation files (the
+"Software"), to deal in the Software without restriction, including
+without limitation the rights to use, copy, modify, merge, publish,
+distribute, sublicense, and/or sell copies of the Software, and to
+permit persons to whom the Software is furnished to do so, subject to
+the following conditions:
+
+The above copyright notice and this permission notice shall be
+included in all copies or substantial portions of the Software.
+
+THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND,
+EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF
+MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND
+NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE
+LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION
+OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION
+WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.
 
-This product includes software developed by The Apache Software
-Foundation (http://www.apache.org/).
\ No newline at end of file
diff --git a/bin/cpdump b/bin/cpdump
index a49fde72c1d..5bbca5122c8 100755
--- a/bin/cpdump
+++ b/bin/cpdump
@@ -1,7 +1,7 @@
-#!/usr/bin/env vendor/jruby/bin/jruby
+#!/usr/bin/env bin/ruby
 
 require_relative "../lib/bootstrap/environment"
-LogStash::Bundler.setup!({:without => [:build]})
+LogStash::Bundler.setup!({:without => [:build, :development]})
 require "logstash-core"
 require "logstash/environment"
 require "logstash/settings"
diff --git a/bin/lock b/bin/lock
new file mode 100755
index 00000000000..a8a0529a943
--- /dev/null
+++ b/bin/lock
@@ -0,0 +1,9 @@
+#!/usr/bin/env bin/ruby
+
+require_relative "../lib/bootstrap/environment"
+LogStash::Bundler.setup!({:without => [:build, :development]})
+require "logstash-core"
+
+lock = Java::OrgLogstash::FileLockFactory.getDefault.obtainLock(ARGV[0], ".lock")
+puts("locking " + File.join(ARGV[0], ".lock"))
+sleep
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index 3777514a31a..d070f6c6fa5 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -28,6 +28,26 @@ export LOGSTASH_HOME
 SINCEDB_DIR=${LOGSTASH_HOME}
 export SINCEDB_DIR
 
+# This block will iterate over the command-line args Logstash was started with
+# It will find the argument _after_ --path.settings and use that to attempt
+# to derive the location of an acceptable jvm.options file
+# It will do nothing if this is not found.
+# This fix is for #6379
+if [ -z "$LS_JVM_OPTS" ]; then
+  found=0
+  for i in "$@"; do
+     if [ $found -eq 1 ]; then
+       if [ -r "${i}/jvm.options" ]; then
+         export LS_JVM_OPTS="${i}/jvm.options"
+         break
+       fi
+     fi
+     if [ "$i" = "--path.settings" ]; then
+       found=1
+     fi
+  done
+fi
+
 parse_jvm_options() {
   if [ -f "$1" ]; then
     echo "$(grep "^-" "$1" | tr '\n' ' ')"
@@ -135,15 +155,6 @@ setup_ruby() {
   VENDORED_JRUBY=
 }
 
-jruby_opts() {
-  printf "%s" "--1.9"
-  for i in $JAVA_OPTS ; do
-    if [ -z "$i" ]; then
-      printf "%s" " -J$i"
-    fi
-  done
-}
-
 setup() {
   # first check if we want to use drip, which can be used in vendored jruby mode
   # and also when setting USE_RUBY=1 if the ruby interpretor is in fact jruby
@@ -179,8 +190,8 @@ ruby_exec() {
     # $VENDORED_JRUBY is non-empty so use the vendored JRuby
 
     if [ "$DEBUG" ] ; then
-      echo "DEBUG: exec ${JRUBY_BIN} $(jruby_opts) $@"
+      echo "DEBUG: exec ${JRUBY_BIN} $@"
     fi
-    exec "${JRUBY_BIN}" $(jruby_opts) "$@"
+    exec "${JRUBY_BIN}" "$@"
   fi
 }
diff --git a/bin/ruby b/bin/ruby
new file mode 100755
index 00000000000..ab7396423fc
--- /dev/null
+++ b/bin/ruby
@@ -0,0 +1,24 @@
+#!/bin/sh
+# Run a ruby script using the logstash jruby launcher
+#
+# Usage:
+#   bin/ruby [arguments]
+#
+# Supported environment variables:
+#   LS_JVM_OPTS="xxx" path to file with JVM options
+#   LS_JAVA_OPTS="xxx" to append extra options to the defaults JAVA_OPTS provided by logstash
+#   JAVA_OPTS="xxx" to *completely override* the defauls set of JAVA_OPTS provided by logstash
+#
+# Development environment variables:
+#   USE_RUBY=1 to force use the local "ruby" command to launch logstash instead of using the vendored JRuby
+#   DEBUG=1 to output debugging information
+
+# use faster starting JRuby options see https://github.com/jruby/jruby/wiki/Improving-startup-time
+export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1"
+
+unset CDPATH
+
+. "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+setup
+
+ruby_exec "$@"
diff --git a/bin/system-install b/bin/system-install
index 5790a5a41e8..80daf9148fe 100755
--- a/bin/system-install
+++ b/bin/system-install
@@ -5,17 +5,60 @@ unset CDPATH
 setup
 
 if [ -z "$1" ]; then
-  [ -r ${LOGSTASH_HOME}/config/startup.options ] && . ${LOGSTASH_HOME}/config/startup.options
-  [ -r /etc/logstash/startup.options ] && . /etc/logstash/startup.options
+  if [ -r /etc/logstash/startup.options ]; then
+    OPTIONS_PATH=/etc/logstash/startup.options
+  elif [ -r ${LOGSTASH_HOME}/config/startup.options ]; then
+    OPTIONS_PATH=${LOGSTASH_HOME}/config/startup.options
+  fi
+elif [ "$1" == "-h" ] || [ "$1" == "--help" ]; then
+  echo "Usage: system-install [OPTIONSFILE] [STARTUPTYPE] [VERSION]"
+  echo
+  echo "NOTE: These arguments are ordered, and co-dependent"
+  echo
+  echo "OPTIONSFILE: Full path to a startup.options file"
+  echo "OPTIONSFILE is required if STARTUPTYPE is specified, but otherwise looks first"
+  echo "in $LOGSTASH_HOME/config/startup.options and then /etc/logstash/startup.options"
+  echo "Last match wins"
+  echo
+  echo "STARTUPTYPE: e.g. sysv, upstart, systemd, etc."
+  echo "OPTIONSFILE is required to specify a STARTUPTYPE."
+  echo
+  echo "VERSION: The specified version of STARTUPTYPE to use.  The default is usually"
+  echo "preferred here, so it can safely be omitted."
+  echo "Both OPTIONSFILE & STARTUPTYPE are required to specify a VERSION."
+  echo
+  echo "For more information, see https://github.com/jordansissel/pleaserun"
+  exit 0
 else
   if [ -r $1 ]; then
     echo "Using provided startup.options file: ${1}"
-    . $1
+    OPTIONS_PATH=$1
   else
     echo "$1 is not a file path"
+    echo "To manually specify a startup style, put the path to startup.options as the "
+    echo "first argument, followed by the startup style (sysv, upstart, systemd)"
+    exit 1
   fi
 fi
 
+# Read in the env vars in the selected startup.options file...
+. ${OPTIONS_PATH}
+
+old_IFS=$IFS
+IFS=$'\n'
+lines=($(grep -v ^# ${OPTIONS_PATH} | tr -d '"' | grep -v '^LS_OPTS=' | grep \= | grep -v '\=$' | grep -v '\=\"\"$'))
+IFS=$old_IFS
+
+ENV_VAR_ARGS=()
+
+for line in ${lines[@]}; do
+  var=$(echo $line | awk -F\= '{print $1}')
+  if [ "x${!var}" != "x" ]; then
+    ENV_VAR_ARGS+=('--environment-variables')
+    ENV_VAR_ARGS+=("${var}=${!var}")
+  fi
+done
+
 # bin/logstash-plugin is a short lived ruby script thus we can use aggressive "faster starting JRuby options"
 # see https://github.com/jruby/jruby/wiki/Improving-startup-time
 export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify -X-C -Xcompile.invokedynamic=false"
@@ -27,9 +70,22 @@ else
   opts=("--log" "$tempfile" "--overwrite" "--install" "--name" "${SERVICE_NAME}" "--user" "${LS_USER}" "--group" "${LS_GROUP}" "--description" "${SERVICE_DESCRIPTION}" "--nice" "${LS_NICE}" "--limit-open-files" "${LS_OPEN_FILES}" "--prestart" "${PRESTART}")
 fi
 
+if [[ $2 ]]; then
+  echo "Manually creating startup for specified platform: ${2}"
+  opts+=('--platform')
+  opts+=($2)
+fi
+
+if [[ $3 ]]; then
+  echo "Manually creating startup for specified platform (${2}) version: ${3}"
+  opts+=('--version')
+  opts+=($3)
+fi
+
+allopts=("${ENV_VAR_ARGS[@]}" "${opts[@]}")
 program="$(cd `dirname $0`/..; pwd)/bin/logstash"
 
-$(ruby_exec "${LOGSTASH_HOME}/lib/systeminstall/pleasewrap.rb" "${opts[@]}" ${program} ${LS_OPTS})
+$(ruby_exec "${LOGSTASH_HOME}/lib/systeminstall/pleasewrap.rb" "${allopts[@]}" ${program} ${LS_OPTS})
 exit_code=$?
 
 if [ $exit_code -ne 0 ]; then
diff --git a/ci/ci_integration.sh b/ci/ci_integration.sh
index ff343cf8438..de26ea120f8 100755
--- a/ci/ci_integration.sh
+++ b/ci/ci_integration.sh
@@ -16,6 +16,7 @@ rake artifact:tar
 cd build
 echo "Extracting logstash tar file in build/"
 tar xf *.tar.gz
+
 cd ../qa/integration
 # to install test dependencies
 bundle install
diff --git a/ci/travis_integration_install.sh b/ci/travis_integration_install.sh
index 2b8a63ac419..3f174da3f3f 100755
--- a/ci/travis_integration_install.sh
+++ b/ci/travis_integration_install.sh
@@ -18,8 +18,9 @@ rake artifact:tar
 cd build
 echo "Extracting logstash tar file in build/"
 tar xf *.tar.gz
+
 cd ../qa/integration
 pwd
 echo $BUNDLE_GEMFILE
 # to install test dependencies
-bundle install --gemfile="Gemfile"
\ No newline at end of file
+bundle install --gemfile="Gemfile"
diff --git a/config/startup.options b/config/startup.options
index 9d35f798dcd..4d56976ad61 100644
--- a/config/startup.options
+++ b/config/startup.options
@@ -1,7 +1,8 @@
 ################################################################################
 # These settings are ONLY used by $LS_HOME/bin/system-install to create a custom
-# startup script for Logstash.  It should automagically use the init system
-# (systemd, upstart, sysv, etc.) that your Linux distribution uses.
+# startup script for Logstash and is not used by Logstash itself. It should
+# automagically use the init system (systemd, upstart, sysv, etc.) that your
+# Linux distribution uses.
 #
 # After changing anything here, you need to re-run $LS_HOME/bin/system-install
 # as root to push the changes to the init script.
diff --git a/docs/index.asciidoc b/docs/index.asciidoc
index ecf8f3ae4cc..51d78e37769 100644
--- a/docs/index.asciidoc
+++ b/docs/index.asciidoc
@@ -1,10 +1,10 @@
 [[logstash-reference]]
 = Logstash Reference
 
-:branch:                5.x
-:major-version:         5.x
-:logstash_version:      5.1.1
-:elasticsearch_version: 5.1.1
+:branch:                master
+:major-version:         6.x
+:logstash_version:      6.0.0-alpha1
+:elasticsearch_version: 6.0.0-alpha1
 :docker-image:          docker.elastic.co/logstash/logstash:{logstash_version}
 
 //////////
@@ -14,10 +14,10 @@ release-state can be: released | prerelease | unreleased
 
 :jdk:                   1.8.0
 :guide:                 https://www.elastic.co/guide/en/elasticsearch/guide/current/
-:ref:                   https://www.elastic.co/guide/en/elasticsearch/reference/5.x/
-:xpack:                 https://www.elastic.co/guide/en/x-pack/current/
-:logstash:              https://www.elastic.co/guide/en/logstash/5.x/
-:filebeat:              https://www.elastic.co/guide/en/beats/filebeat/5.x/
+:ref:                   https://www.elastic.co/guide/en/elasticsearch/reference/current/
+:xpack:                https://www.elastic.co/guide/en/x-pack/current
+:logstash:              https://www.elastic.co/guide/en/logstash/current/
+:filebeat:              https://www.elastic.co/guide/en/beats/filebeat/current/
 :lsissue:               https://github.com/elastic/logstash/issues/
 :security:              X-Pack Security
 :stack:                 https://www.elastic.co/guide/en/elastic-stack/current/
@@ -37,123 +37,119 @@ volume and variety of data.
 // The pass blocks here point to the correct repository for the edit links in the guide.
 
 // Introduction
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc ?>]
+
 include::static/introduction.asciidoc[]
 
 // Glossary and core concepts go here
 
 // Getting Started with Logstash
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc ?>]
+
 include::static/getting-started-with-logstash.asciidoc[]
 
 // Advanced LS Pipelines
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc ?>]
+
 include::static/advanced-pipeline.asciidoc[]
 
 // Processing Pipeline
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc ?>]
+
 include::static/life-of-an-event.asciidoc[]
 
 // Lostash setup
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc ?>]
+
 include::static/setting-up-logstash.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc ?>]
+
 include::static/docker.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/settings-file.asciidoc ?>]
+
 include::static/settings-file.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/command-line-flags.asciidoc ?>]
+
 include::static/command-line-flags.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/logging.asciidoc ?>]
+
 include::static/logging.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/persistent-queues.asciidoc ?>]
+
 include::static/persistent-queues.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/shutdown.asciidoc ?>]
+
 include::static/shutdown.asciidoc[]
 
 // Breaking Changes
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc ?>]
+
 include::static/breaking-changes.asciidoc[]
 
 // Upgrading Logstash
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc ?>]
+
 include::static/upgrading.asciidoc[]
 
 // Configuring Logstash
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc ?>]
+
 include::static/configuration.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/reloading-config.asciidoc ?>]
+
 include::static/reloading-config.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/managing-multiline-events.asciidoc ?>]
+
 include::static/managing-multiline-events.asciidoc[]
 
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/glob-support.asciidoc ?>]
+
 include::static/glob-support.asciidoc[]
 
 // Deploying & Scaling
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc ?>]
+
 include::static/deploying.asciidoc[]
 
 // Troubleshooting performance
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc ?>]
+
 include::static/performance-checklist.asciidoc[]
 
 // Monitoring APIs
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc ?>]
+
 include::static/monitoring-apis.asciidoc[]
 
 // Working with Plugins
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc ?>]
+
 include::static/plugin-manager.asciidoc[]
 
 // These files do their own pass blocks
-pass::[<?edit_url?>]
+
 include::plugins/inputs.asciidoc[]
 include::plugins/outputs.asciidoc[]
 include::plugins/filters.asciidoc[]
 include::plugins/codecs.asciidoc[]
 
+:edit_url:
+
 // Contributing to Logstash
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/contributing-to-logstash.asciidoc ?>]
+
 include::static/contributing-to-logstash.asciidoc[]
 
-// This is in the pluginbody.asciidoc itself
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/include/pluginbody.asciidoc ?>]
 include::static/input.asciidoc[]
 include::static/codec.asciidoc[]
 include::static/filter.asciidoc[]
 include::static/output.asciidoc[]
 
 // Contributing a Patch to a Logstash Plugin
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/contributing-patch.asciidoc ?>]
+
 include::static/contributing-patch.asciidoc[]
 
 // Logstash Community Maintainer Guide
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/maintainer-guide.asciidoc ?>]
+
 include::static/maintainer-guide.asciidoc[]
 
 // A space is necessary here ^^^
-pass::[<?edit_url?>]
 
-// Submitting a Plugin
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/submitting-a-plugin.asciidoc ?>]
-include::static/submitting-a-plugin.asciidoc[]
 
+// Submitting a Plugin
 
-// This is in the pluginbody.asciidoc itself
-// pass::[<?edit_url?>]
+include::static/submitting-a-plugin.asciidoc[]
 
 // Glossary of Terms
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/glossary.asciidoc ?>]
+
 include::static/glossary.asciidoc[]
 
-// Release Notes
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/releasenotes.asciidoc ?>]
-include::static/releasenotes.asciidoc[]
+// This is in the pluginbody.asciidoc itself
+//
+
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index a18980fb8e2..5d3629f3033 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -129,8 +129,8 @@ before-and-after examples. In Logstash 2.x, you may have run something:
 bin/logstash --config my.conf --pipeline-workers 8 <1>
 bin/logstash -f my.conf -w 8 <2>
 ----------------------------------
-1. Long form options `config` and `pipeline-workers` are used here.
-2. Short form options `f` and `w` (aliases for the former` are used here.
+<1> Long form options `config` and `pipeline-workers` are used here.
+<2> Short form options `f` and `w` (aliases for the former` are used here.
 
 But, in Logstash 5.0, this becomes:
 
@@ -139,8 +139,8 @@ But, in Logstash 5.0, this becomes:
 bin/logstash --path.config my.conf --pipeline.workers 8 <1>
 bin/logstash -f my.conf -w 8 <2>
 ----------------------------------
-1. Long form options are changed to reflect the new options.
-2. Short form options are unchanged.
+<1> Long form options are changed to reflect the new options.
+<2> Short form options are unchanged.
 
 NOTE: None of the short form options have changed!
 
diff --git a/docs/static/command-line-flags.asciidoc b/docs/static/command-line-flags.asciidoc
index 97c793a26c6..4a896bbdfdf 100644
--- a/docs/static/command-line-flags.asciidoc
+++ b/docs/static/command-line-flags.asciidoc
@@ -46,7 +46,7 @@ With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/t
   Sets the number of pipeline workers to run. This option sets the number of workers that will,
   in parallel, execute the filter and output stages of the pipeline. If you find that events are
   backing up, or that  the CPU is not saturated, consider increasing this number to better utilize
-  machine processing power. The default is 8.
+  machine processing power. The default is the number of the host's CPU cores.
 
 *`-b, --pipeline.batch.size SIZE`*::
   Size of batches the pipeline is to work in. This option defines the maximum number of events an
diff --git a/docs/static/configuration.asciidoc b/docs/static/configuration.asciidoc
index 759567acff5..9ac8526114a 100644
--- a/docs/static/configuration.asciidoc
+++ b/docs/static/configuration.asciidoc
@@ -89,7 +89,7 @@ types are supported.
 [[array]]
 ==== Array
 
-This type is now mostly deprecated in favor of using a standard type like `string` with the plugin defining the `:list => true` property for better type checking. It is still needed to handle lists of hashes or mixed types where type checking is not desired. 
+This type is now mostly deprecated in favor of using a standard type like `string` with the plugin defining the `:list => true` property for better type checking. It is still needed to handle lists of hashes or mixed types where type checking is not desired.
 
 Example:
 
diff --git a/docs/static/deploying.asciidoc b/docs/static/deploying.asciidoc
index 687a5aba1c8..dc6fa74faf5 100644
--- a/docs/static/deploying.asciidoc
+++ b/docs/static/deploying.asciidoc
@@ -63,7 +63,7 @@ nodes. By default, Logstash uses the HTTP protocol to move data into the cluster
 You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the
 indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR
 files and has no performance disadvantages compared to the transport or node protocols. You can secure communications
-that use the HTTP REST APIs by using {xpack}xpack-security.html[{security}], which supports SSL and HTTP basic authentication.
+that use the HTTP REST APIs by using {xpack}/xpack-security.html[{security}], which supports SSL and HTTP basic authentication.
 
 When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically
 load-balance indexing requests across a
diff --git a/docs/static/getting-started-with-logstash.asciidoc b/docs/static/getting-started-with-logstash.asciidoc
index bc9d056e95f..cb29f66261c 100644
--- a/docs/static/getting-started-with-logstash.asciidoc
+++ b/docs/static/getting-started-with-logstash.asciidoc
@@ -81,7 +81,7 @@ Save the repository definition to  +/etc/apt/sources.list.d/elastic-{major-versi
 
 ["source","sh",subs="attributes,callouts"]
 --------------------------------------------------
-echo "deb https://artifacts.elastic.co/packages/{major-version}/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-{major-version}.list
+echo "deb https://artifacts.elastic.co/packages/{major-version}-prerelease/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-{major-version}.list
 --------------------------------------------------
 
 [WARNING]
@@ -124,7 +124,7 @@ in a file with a `.repo` suffix, for example `logstash.repo`
 --------------------------------------------------
 [logstash-{major-version}]
 name=Elastic repository for {major-version} packages
-baseurl=https://artifacts.elastic.co/packages/{major-version}/yum
+baseurl=https://artifacts.elastic.co/packages/{major-version}-prerelease/yum
 gpgcheck=1
 gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
 enabled=1
diff --git a/docs/static/include/pluginbody.asciidoc b/docs/static/include/pluginbody.asciidoc
index 4a31f014e48..0dc2260a923 100644
--- a/docs/static/include/pluginbody.asciidoc
+++ b/docs/static/include/pluginbody.asciidoc
@@ -1,4 +1,4 @@
-pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/include/pluginbody.asciidoc ?>]
+
 
 === How to write a Logstash {plugintype} plugin
 
@@ -469,14 +469,15 @@ There are several configuration attributes:
 
 * `:validate` - allows you to enforce passing a particular data type to Logstash
 for this configuration option, such as `:string`, `:password`, `:boolean`,
-`:number`, `:array`, `:hash`, `:path` (a file-system path), `uri` (starting in 5.0.0), `:codec` (since
-1.2.0), `:bytes` (starting in 1.5.0).  Note that this also works as a coercion 
+`:number`, `:array`, `:hash`, `:path` (a file-system path), `uri`, `:codec` (since
+1.2.0), `:bytes`.  Note that this also works as a coercion 
 in that if I specify "true" for boolean (even though technically a string), it
 will become a valid boolean in the config.  This coercion works for the
 `:number` type as well where "1.2" becomes a float and "22" is an integer.
 * `:default` - lets you specify a default value for a parameter
-* `:required` - whether or not this parameter is mandatory (a Boolean `true` or `false`)
-* `:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable. 
+* `:required` - whether or not this parameter is mandatory (a Boolean `true` or
+* `:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable.
+`false`)
 * `:deprecated` - informational (also a Boolean `true` or `false`)
 * `:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.
 
@@ -1299,4 +1300,4 @@ the Logstash repository. When the acceptance guidelines are completed, we will
 facilitate the move to the logstash-plugins organization using the recommended
 https://help.github.com/articles/transferring-a-repository/#transferring-from-a-user-to-an-organization[github process].
 
-pass::[<?edit_url?>]
+
diff --git a/docs/static/logging.asciidoc b/docs/static/logging.asciidoc
index c601bd3f2da..67560e1b053 100644
--- a/docs/static/logging.asciidoc
+++ b/docs/static/logging.asciidoc
@@ -27,8 +27,8 @@ Slow-log for Logstash adds the ability to log when a specific event takes an abn
 through the pipeline. Just like the normal application log, you can find slow-logs in your `--path.logs` directory.
 Slowlog is configured in the `logstash.yml` settings file with the following options:
 
+[source,yaml]
 ------------------------------
-[source]
 slowlog.threshold.warn (default: -1)
 slowlog.threshold.info (default: -1)
 slowlog.threshold.debug (default: -1)
@@ -42,12 +42,12 @@ can be specified using the following time units: `nanos` (nanoseconds), `micros`
 
 Here is an example:
 
+[source,yaml]
 ------------------------------
-[source]
 slowlog.threshold.warn: 2s
-slowlog.threshold.info 1s
-slowlog.threshold.debug 500ms
-slowlog.threshold.trace 100ms
+slowlog.threshold.info: 1s
+slowlog.threshold.debug: 500ms
+slowlog.threshold.trace: 100ms
 ------------------------------
 
 In the above configuration, events that take longer than two seconds to be processed within a filter will be logged.
diff --git a/docs/static/managing-multiline-events.asciidoc b/docs/static/managing-multiline-events.asciidoc
index e9ee8bc12fa..83507c85e6d 100644
--- a/docs/static/managing-multiline-events.asciidoc
+++ b/docs/static/managing-multiline-events.asciidoc
@@ -10,7 +10,7 @@ pipeline is the {logstash}plugins-codecs-multiline.html[multiline codec], which
 a simple set of rules.
 
 
-The most important aspects of configuring either multiline plugin are the following:
+The most important aspects of configuring the multiline codec are the following:
 
 * The `pattern` option specifies a regular expression. Lines that match the specified regular expression are considered
 either continuations of a previous line or the start of a new multiline event. You can use
@@ -20,16 +20,10 @@ value in the `pattern` option are part of the previous line. The `next` value sp
 in the `pattern` option are part of the following line.* The `negate` option applies the multiline codec to lines that
 _do not_ match the regular expression specified in the `pattern` option.
 
-See the full documentation for the {logstash}plugins-codecs-multiline.html[multiline codec] or the
-{logstash}plugins-filters-multiline.html[multiline filter] plugin for more information on configuration options.
+See the full documentation for the {logstash}plugins-codecs-multiline.html[multiline codec] plugin for more information
+on configuration options.
 
-NOTE: For more complex needs, the {logstash}plugins-filters-multiline.html[multiline filter] performs a similar task at
-the filter stage of processing, where the Logstash instance aggregates multiple inputs.
-The multiline filter plugin is not thread-safe. Avoid using multiple filter workers with the multiline filter. You can
-track the progress of upgrades to the functionality of the multiline codec at
-https://github.com/logstash-plugins/logstash-codec-multiline/issues/10[this Github issue].
-
-==== Examples of Multiline Plugin Configuration
+==== Examples of Multiline Codec Configuration
 
 The examples in this section cover the following use cases:
 
diff --git a/docs/static/monitoring-apis.asciidoc b/docs/static/monitoring-apis.asciidoc
index 406736be8fd..bc0b3606bf6 100644
--- a/docs/static/monitoring-apis.asciidoc
+++ b/docs/static/monitoring-apis.asciidoc
@@ -80,13 +80,17 @@ Where `<types>` is optional and specifies the types of node info you want to ret
 You can limit the info that's returned by combining any of the following types in a comma-separated list:
 
 [horizontal]
-`pipeline`::
+<<node-pipeline-info,`pipeline`>>::
 Gets pipeline-specific information and settings.
-`os`::
+<<node-os-info,`os`>>::
 Gets node-level info about the OS.
-`jvm`::
+<<node-jvm-info,`jvm`>>::
 Gets node-level JVM info, including info about threads.
 
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
+[[node-pipeline-info]]
 ==== Pipeline Info
 
 The following request returns a JSON document that shows pipeline info, such as the number of workers,
@@ -115,6 +119,7 @@ Example response:
   }
 --------------------------------------------------
 
+[[node-os-info]]
 ==== OS Info
 
 The following request returns a JSON document that shows the OS name, architecture, version, and
@@ -133,11 +138,12 @@ Example response:
   "os": {
     "name": "Mac OS X",
     "arch": "x86_64",
-    "version": "10.11.2",
+    "version": "10.12.1",
     "available_processors": 8
   }
 --------------------------------------------------
 
+[[node-jvm-info]]
 ==== JVM Info
 
 The following request returns a JSON document that shows node-level JVM stats, such as the JVM process id, version,
@@ -154,12 +160,12 @@ Example response:
 --------------------------------------------------
 {
   "jvm": {
-    "pid": 8187,
+    "pid": 59616,
     "version": "1.8.0_65",
     "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
     "vm_version": "1.8.0_65",
     "vm_vendor": "Oracle Corporation",
-    "start_time_in_millis": 1474305161631,
+    "start_time_in_millis": 1484251185878,
     "mem": {
       "heap_init_in_bytes": 268435456,
       "heap_max_in_bytes": 1037959168,
@@ -171,6 +177,7 @@ Example response:
       "ConcurrentMarkSweep"
     ]
   }
+}
 --------------------------------------------------
 
 [[plugins-api]]
@@ -186,6 +193,9 @@ This API basically returns the output of running the `bin/logstash-plugin list -
 GET /_node/plugins
 --------------------------------------------------
 
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
 The output is a JSON document.
 
 Example response:
@@ -193,11 +203,15 @@ Example response:
 ["source","js",subs="attributes"]
 --------------------------------------------------
 {
-  "total": 91,
+  "total": 92,
   "plugins": [
+    {
+      "name": "logstash-codec-cef",
+      "version": "4.1.2"
+    },
     {
       "name": "logstash-codec-collectd",
-      "version": "3.0.2"
+      "version": "3.0.3"
     },
     {
       "name": "logstash-codec-dots",
@@ -230,15 +244,22 @@ Where `<types>` is optional and specifies the types of stats you want to return.
 By default, all stats are returned. You can limit the info that's returned by combining any of the following types in a comma-separated list:
 
 [horizontal]
-`jvm`::
-Gets JVM stats, including stats about threads, memory usage, and garbage collectors.
-`process`::
+<<jvm-stats,`jvm`>>::
+Gets JVM stats, including stats about threads, memory usage, garbage collectors,
+and uptime.
+<<process-stats,`process`>>::
 Gets process stats, including stats about file descriptors, memory consumption, and CPU usage.
-`mem`::
-Gets memory usage stats.
-`pipeline`::
+<<pipeline-stats,`pipeline`>>::
 Gets runtime stats about the Logstash pipeline.
+<<reload-stats,`reloads`>>::
+Gets runtime stats about config reload successes and failures.
+<<os-stats,`os`>>::
+Gets runtime stats about cgroups when Logstash is running in a container.
 
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
+[[jvm-stats]]
 ==== JVM Stats
 
 The following request returns a JSON document containing JVM stats:
@@ -255,34 +276,34 @@ Example response:
 {
   "jvm": {
     "threads": {
-      "count": 33,
-      "peak_count": 34
+      "count": 35,
+      "peak_count": 36
     },
     "mem": {
-      "heap_used_in_bytes": 276974824,
-      "heap_used_percent": 13,
+      "heap_used_in_bytes": 318691184,
+      "heap_used_percent": 15,
       "heap_committed_in_bytes": 519045120,
       "heap_max_in_bytes": 2075918336,
-      "non_heap_used_in_bytes": 182122272,
-      "non_heap_committed_in_bytes": 193372160,
+      "non_heap_used_in_bytes": 189382304,
+      "non_heap_committed_in_bytes": 200728576,
       "pools": {
         "survivor": {
           "peak_used_in_bytes": 8912896,
-          "used_in_bytes": 11182152,
+          "used_in_bytes": 9538656,
           "peak_max_in_bytes": 35782656,
           "max_in_bytes": 71565312,
           "committed_in_bytes": 17825792
         },
         "old": {
-          "peak_used_in_bytes": 111736080,
-          "used_in_bytes": 171282544,
+          "peak_used_in_bytes": 106946320,
+          "used_in_bytes": 181913072,
           "peak_max_in_bytes": 715849728,
           "max_in_bytes": 1431699456,
           "committed_in_bytes": 357957632
         },
         "young": {
           "peak_used_in_bytes": 71630848,
-          "used_in_bytes": 94510128,
+          "used_in_bytes": 127239456,
           "peak_max_in_bytes": 286326784,
           "max_in_bytes": 572653568,
           "committed_in_bytes": 143261696
@@ -292,18 +313,20 @@ Example response:
     "gc": {
       "collectors": {
         "old": {
-          "collection_time_in_millis": 48,
+          "collection_time_in_millis": 58,
           "collection_count": 2
         },
         "young": {
-          "collection_time_in_millis": 316,
-          "collection_count": 23
+          "collection_time_in_millis": 338,
+          "collection_count": 26
         }
       }
-    }
+    },
+    "uptime_in_millis": 382701
   }
 --------------------------------------------------
 
+[[process-stats]]
 ==== Process Stats
 
 The following request returns a JSON document containing process stats:
@@ -319,25 +342,35 @@ Example response:
 --------------------------------------------------
 {
   "process": {
-    "open_file_descriptors": 60,
-    "peak_open_file_descriptors": 65,
+    "open_file_descriptors": 164,
+    "peak_open_file_descriptors": 166,
     "max_file_descriptors": 10240,
     "mem": {
-      "total_virtual_in_bytes": 5364461568
+      "total_virtual_in_bytes": 5399474176
     },
     "cpu": {
-      "total_in_millis": 101294404000,
-      "percent": 0
+      "total_in_millis": 72810537000,
+      "percent": 0,
+      "load_average": {
+        "1m": 2.41943359375
+      }
     }
   }
+}
 --------------------------------------------------
 
 [[pipeline-stats]]
 ==== Pipeline Stats
 
-The following request returns a JSON document containing pipeline stats, including the number of events that were
-input, filtered, or output by the pipeline. The request also returns stats for each configured input, filter, or
-output stage, and info about whether config reload (if configured) failed or succeeded.
+The following request returns a JSON document containing pipeline stats,
+including:
+
+* the number of events that were input, filtered, or output by the pipeline
+* stats for each configured input, filter, or output stage
+* info about config reload successes and failures
+(when <<reloading-config,config reload>> is enabled)
+* info about the persistent queue (when
+<<persistent-queues,persistent queues>> are enabled)
 
 [source,js]
 --------------------------------------------------
@@ -351,45 +384,46 @@ Example response:
 {
   "pipeline": {
     "events": {
-      "duration_in_millis": 7863504,
-      "in": 100,
-      "filtered": 100,
-      "out": 100
+      "duration_in_millis": 6304989,
+      "in": 200,
+      "filtered": 200,
+      "out": 200
     },
     "plugins": {
       "inputs": [],
       "filters": [
         {
-          "id": "grok_20e5cb7f7c9e712ef9750edf94aefb465e3e361b-2",
+          "id": "4e3d4bed6ba821ebb47f4752bb757b04a754d736-2",
           "events": {
-            "duration_in_millis": 48,
-            "in": 100,
-            "out": 100
+            "duration_in_millis": 113,
+            "in": 200,
+            "out": 200
           },
-          "matches": 100,
+          "matches": 200,
           "patterns_per_field": {
             "message": 1
           },
           "name": "grok"
         },
         {
-          "id": "geoip_20e5cb7f7c9e712ef9750edf94aefb465e3e361b-3",
+          "id": "4e3d4bed6ba821ebb47f4752bb757b04a754d736-3",
           "events": {
-            "duration_in_millis": 141,
-            "in": 100,
-            "out": 100
+            "duration_in_millis": 526,
+            "in": 200,
+            "out": 200
           },
           "name": "geoip"
         }
       ],
       "outputs": [
         {
-          "id": "20e5cb7f7c9e712ef9750edf94aefb465e3e361b-4",
+          "id": "4e3d4bed6ba821ebb47f4752bb757b04a754d736-4",
           "events": {
-            "in": 100,
-            "out": 100
+            "duration_in_millis": 2312,
+            "in": 200,
+            "out": 200
           },
-          "name": "elasticsearch"
+          "name": "stdout"
         }
       ]
     },
@@ -399,12 +433,83 @@ Example response:
       "last_success_timestamp": null,
       "last_failure_timestamp": null,
       "failures": 0
+    },
+    "queue": {
+      "events": 26,
+      "type": "persisted",
+      "capacity": {
+        "page_capacity_in_bytes": 262144000,
+        "max_queue_size_in_bytes": 4294967296,
+        "max_unread_events": 0
+      },
+      "data": {
+        "path": "/path/to/data/queue",
+        "free_space_in_bytes": 123027787776,
+        "storage_type": "hfs"
+      }
     }
   }
+}
 --------------------------------------------------
 
-See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
+[[reload-stats]]
+==== Reload Stats
+
+The following request returns a JSON document that shows info about config reload successes and failures.
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/reloads
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "reloads": {
+    "successes": 0,
+    "failures": 0
+  }
+}
+--------------------------------------------------
+
+[[os-stats]]
+==== OS Stats
+
+When Logstash is running in a container, the following request returns a JSON document that
+contains cgroup information to give you a more accurate view of CPU load, including whether
+the container is being throttled. 
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/os
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "os" : {
+    "cgroup" : { 
+      "cpuacct" : {
+        "control_group" : "/elastic1",
+        "usage_nanos" : 378477588075
+                },
+      "cpu" : {
+        "control_group" : "/elastic1",
+        "cfs_period_micros" : 1000000,
+        "cfs_quota_micros" : 800000,
+        "stat" : {
+          "number_of_elapsed_periods" : 4157,
+          "number_of_times_throttled" : 460,
+          "time_throttled_nanos" : 581617440755
+        }
+      }    
+    }
+  }
+--------------------------------------------------
 
 
 [[hot-threads-api]]
@@ -429,13 +534,12 @@ Example response:
 [source,js]
 --------------------------------------------------
 {
-  "hot_threads": {
-    "time": "2016-09-19T10:44:13-07:00",
+    "time": "2017-01-12T12:09:45-08:00",
     "busiest_threads": 3,
     "threads": [
       {
         "name": "LogStash::Runner",
-        "percent_of_cpu_time": 0.17,
+        "percent_of_cpu_time": 1.07,
         "state": "timed_waiting",
         "traces": [
           "java.lang.Object.wait(Native Method)",
@@ -451,38 +555,37 @@ Example response:
         ]
       },
       {
-        "name": "Ruby-0-Thread-17",
-        "percent_of_cpu_time": 0.11,
-        "state": "timed_waiting",
-        "path": "/Users/username/logstash-5.0.0/logstash-core/lib/logstash/pipeline.rb:471",
+        "name": "[main]>worker7",
+        "percent_of_cpu_time": 0.71,
+        "state": "waiting",
         "traces": [
-          "java.lang.Object.wait(Native Method)",
-          "org.jruby.RubyThread.sleep(RubyThread.java:1002)",
-          "org.jruby.RubyKernel.sleep(RubyKernel.java:803)",
-          "org.jruby.RubyKernel$INVOKER$s$0$1$sleep.call(RubyKernel$INVOKER$s$0$1$sleep.gen)",
-          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:667)",
-          "org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:206)",
-          "org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:168)",
-          "rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb:84)",
-          "rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb)",
-          "org.jruby.ast.executable.AbstractScript.__file__(AbstractScript.java:46)"
+          "sun.misc.Unsafe.park(Native Method)",
+          "java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:897)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1222)",
+          "java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)",
+          "org.jruby.RubyThread.lockInterruptibly(RubyThread.java:1470)",
+          "org.jruby.ext.thread.Mutex.lock(Mutex.java:91)",
+          "org.jruby.ext.thread.Mutex.synchronize(Mutex.java:147)",
+          "org.jruby.ext.thread.Mutex$INVOKER$i$0$0$synchronize.call(Mutex$INVOKER$i$0$0$synchronize.gen)"
         ]
       },
       {
-        "name": "[main]-pipeline-manager",
-        "percent_of_cpu_time": 0.04,
-        "state": "timed_waiting",
+        "name": "[main]>worker3",
+        "percent_of_cpu_time": 0.71,
+        "state": "waiting",
         "traces": [
-          "java.lang.Object.wait(Native Method)",
-          "java.lang.Thread.join(Thread.java:1253)",
-          "org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)",
-          "org.jruby.RubyThread.join(RubyThread.java:697)",
-          "org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)",
-          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)",
-          "org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)",
-          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:683)",
-          "org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:286)",
-          "org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:81)"
+          "sun.misc.Unsafe.park(Native Method)",
+          "java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:897)",
+          "java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1222)",
+          "java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)",
+          "org.jruby.RubyThread.lockInterruptibly(RubyThread.java:1470)",
+          "org.jruby.ext.thread.Mutex.lock(Mutex.java:91)",
+          "org.jruby.ext.thread.Mutex.synchronize(Mutex.java:147)",
+          "org.jruby.ext.thread.Mutex$INVOKER$i$0$0$synchronize.call(Mutex$INVOKER$i$0$0$synchronize.gen)"
         ]
       }
     ]
@@ -497,6 +600,9 @@ The parameters allowed are:
 `human`:: 	            If true, returns plain text instead of JSON format. The default is false.
 `ignore_idle_threads`:: If true, does not return idle threads. The default is true.
 
+See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
+Logstash monitoring APIs.
+
 You can use the `?human` parameter to return the document in a human-readable format.
 
 [source,js]
@@ -508,10 +614,10 @@ Example of a human-readable response:
 
 [source,js]
 --------------------------------------------------
-::: {}
-Hot threads at 2016-07-26T18:46:18-07:00, busiestThreads=3:
-================================================================================
- 0.15 % of cpu usage by timed_waiting thread named 'LogStash::Runner'
+ ::: {}
+ Hot threads at 2017-01-12T12:10:15-08:00, busiestThreads=3: 
+ ================================================================================
+ 1.02 % of cpu usage, state: timed_waiting, thread name: 'LogStash::Runner' 
 	java.lang.Object.wait(Native Method)
 	java.lang.Thread.join(Thread.java:1253)
 	org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)
@@ -523,32 +629,29 @@ Hot threads at 2016-07-26T18:46:18-07:00, busiestThreads=3:
 	org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)
 	org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)
  --------------------------------------------------------------------------------
- 0.11 % of cpu usage by timed_waiting thread named 'Ruby-0-Thread-17'
- /Users/username/BuildTesting/logstash-5.0.0logstash-core/lib/logstash/pipeline.rb:471
-	java.lang.Object.wait(Native Method)
-	org.jruby.RubyThread.sleep(RubyThread.java:1002)
-	org.jruby.RubyKernel.sleep(RubyKernel.java:803)
-	org.jruby.RubyKernel$INVOKER$s$0$1$sleep.call(RubyKernel$INVOKER$s$0$1$sleep.gen)
-	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:667)
-	org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:206)
-	org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:168)
-	rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/BuildTesting/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb:84)
-	rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/BuildTesting/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb)
-	org.jruby.ast.executable.AbstractScript.__file__(AbstractScript.java:46)
+ 0.71 % of cpu usage, state: waiting, thread name: '[main]>worker7' 
+	sun.misc.Unsafe.park(Native Method)
+	java.util.concurrent.locks.LockSupport.park(LockSupport.java:175)
+	java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:836)
+	java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireInterruptibly(AbstractQueuedSynchronizer.java:897)
+	java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1222)
+	java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:335)
+	org.jruby.RubyThread.lockInterruptibly(RubyThread.java:1470)
+	org.jruby.ext.thread.Mutex.lock(Mutex.java:91)
+	org.jruby.ext.thread.Mutex.synchronize(Mutex.java:147)
+	org.jruby.ext.thread.Mutex$INVOKER$i$0$0$synchronize.call(Mutex$INVOKER$i$0$0$synchronize.gen)
  --------------------------------------------------------------------------------
- 0.04 % of cpu usage by timed_waiting thread named '[main]-pipeline-manager'
-	java.lang.Object.wait(Native Method)
-	java.lang.Thread.join(Thread.java:1253)
-	org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)
-	org.jruby.RubyThread.join(RubyThread.java:697)
-	org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)
-	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)
-	org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)
-	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:683)
-	org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:286)
-	org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:81)
+ 0.71 % of cpu usage, state: timed_waiting, thread name: '[main]>worker3' 
+	sun.misc.Unsafe.park(Native Method)
+	java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:215)
+	java.util.concurrent.SynchronousQueue$TransferStack.awaitFulfill(SynchronousQueue.java:460)
+	java.util.concurrent.SynchronousQueue$TransferStack.transfer(SynchronousQueue.java:362)
+	java.util.concurrent.SynchronousQueue.poll(SynchronousQueue.java:941)
+	sun.reflect.GeneratedMethodAccessor6.invoke(Unknown Source)
+	sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
+	java.lang.reflect.Method.invoke(Method.java:497)
+	org.jruby.javasupport.JavaMethod.invokeDirectWithExceptionHandling(JavaMethod.java:466)
+	org.jruby.javasupport.JavaMethod.invokeDirect(JavaMethod.java:324)
 
 --------------------------------------------------
 
-See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
diff --git a/docs/static/offline-plugins.asciidoc b/docs/static/offline-plugins.asciidoc
index 38d1364a1bc..48012af78a2 100644
--- a/docs/static/offline-plugins.asciidoc
+++ b/docs/static/offline-plugins.asciidoc
@@ -1,17 +1,91 @@
 [[offline-plugins]]
 === Offline Plugin Management
 
-The Logstash <<working-with-plugins,plugin manager>> was introduced in the 1.5 release. This section discusses setting up
-local repositories of plugins for use on systems without access to the Internet.
+The Logstash <<working-with-plugins,plugin manager>> provides support for preparing offline plugin packs that you can
+use to install Logstash plugins on systems that don't have Internet access. 
 
-The procedures in this section require a staging machine running Logstash that has access to a public or private Rubygems
-server. This staging machine downloads and packages the files used for offline installation.
+This procedure requires a staging machine running Logstash that has access to a public or
+<<private-rubygem,private Rubygems>> server. The staging machine downloads and packages all the files and dependencies
+required for offline installation.
 
-See the <<private-rubygem,Private Gem Repositories>> section for information on setting up your own private
-Rubygems server.
+NOTE: If you used offline plugin management prior to Logstash 5.2, you used the `pack` and `unpack` subcommands. Those
+subcommands are now deprecated, but the procedure for using them is still available in the documentation 
+<<building-offline-packages-deprecated,here>>.
 
+[[building-offline-packs]]
 [float]
-=== Building the Offline Package
+=== Building Offline Plugin Packs
+
+An _offline plugin pack_ is a compressed file that contains all the plugins your offline Logstash installation requires,
+along with the dependencies for those plugins.
+
+To build an offline plugin pack:
+
+. Make sure all the plugins that you want to package are installed on the staging server and that the staging server can
+access the Internet.
+
+. Run the `bin/logstash-plugin prepare-offline-pack` subcommand to package the plugins and dependencies:
++
+[source, shell]
+-------------------------------------------------------------------------------
+bin/logstash-plugin prepare-offline-pack --output OUTPUT [PLUGINS]
+-------------------------------------------------------------------------------
++
+where:
++
+* `OUTPUT` specifies the location where the compressed plugin pack will be written. The default location is
++/LOGSTASH_HOME/logstash-offline-plugins-{logstash_version}.zip+.
+* `[PLUGINS]` specifies one or more plugins that you want to include in the pack.
+
+Examples:
+
+["source","sh",subs="attributes"]
+-------------------------------------------------------------------------------
+bin/logstash-plugin prepare-offline-pack logstash-input-beats <1>
+bin/logstash-plugin prepare-offline-pack logstash-filter-* <2>
+bin/logstash-plugin prepare-offline-pack logstash-filter-* logstash-input-beats <3>
+-------------------------------------------------------------------------------
+<1> Packages the Beats input plugin and any dependencies.
+<2> Uses a wildcard to package all filter plugins and any dependencies.
+<3> Packages all filter plugins, the Beats input plugin, and any dependencies.
+
+NOTE: Downloading all dependencies for the specified plugins may take some time, depending on the plugins listed.
+
+[[installing-offline-packs]]
+[float]
+=== Installing Offline Plugin Packs
+
+To install an offline plugin pack:
+
+. Move the compressed bundle to the machine where you want to install the plugins.
+
+. Run the `bin/logstash-plugin install` subcommand to install the packaged plugins:
++
+["source","sh",subs="attributes"]
+-------------------------------------------------------------------------------
+bin/logstash-plugin install file:///path/to/logstash-offline-plugins-{logstash_version}.zip
+-------------------------------------------------------------------------------
++
+Where +path/to/logstash-offline-plugins-{logstash_version}.zip+ is the path to the offline plugin pack.
+
+[float]
+=== Updating Offline Plugins
+
+To update offline plugins, you update the plugins on the staging server and then use the same process that you followed to
+build and install the plugin pack:
+
+. On the staging server, run the `bin/logstash-plugin update` subcommand to update the plugins. See <<updating-plugins>>.
+
+. Create a new version of the plugin pack. See <<building-offline-packs>>.
+
+. Install the new version of the plugin pack. See <<installing-offline-packs>>.
+
+
+[[building-offline-packages-deprecated]]
+[float]
+=== Building the Offline Package (Deprecated Procedure)
+
+deprecated[5.2, Starting with Logstash 5.2, the `pack` and `unpack` commands are deprecated and replaced by the `prepare-offline-pack` and `install` commands]
 
 Working with offline plugins requires you to create an _offline package_, which is a compressed file that contains all of
 the plugins your offline Logstash installation requires, along with the dependencies for those plugins.
@@ -30,7 +104,9 @@ NOTE: Downloading all dependencies for the specified plugins may take some time,
 `bin/logstash-plugin unpack` subcommand to make the packaged plugins available.
 
 [float]
-=== Install or Update a local plugin
+=== Install or Update a local plugin (Deprecated Procedure)
+
+deprecated[5.2]
 
 To install or update a local plugin, use the `--local` option with the install and update commands, as in the following
 examples:
@@ -54,6 +130,8 @@ examples:
 [[managing-packs]]
 === Managing Plugin Packs
 
+deprecated[5.2]
+
 The `pack` and `unpack` subcommands for `bin/logstash-plugin` take the following options:
 
 [horizontal]
diff --git a/docs/static/persistent-queues.asciidoc b/docs/static/persistent-queues.asciidoc
index 73b24134a2f..cf0113b1e26 100644
--- a/docs/static/persistent-queues.asciidoc
+++ b/docs/static/persistent-queues.asciidoc
@@ -1,77 +1,74 @@
 [[persistent-queues]]
 === Persistent Queues
 
-WARNING: This functionality is in beta and is subject to change. It should be deployed in production at your own risk.
+WARNING: This functionality is in beta and is subject to change. Deployment in production is at your own risk.
 
 By default, Logstash uses in-memory bounded queues between pipeline stages
-(input → filter and filter → output) to buffer events. The size of these 
-in-memory queues is fixed and not configurable. If Logstash terminates unsafely,
-either as the result of a software failure or the user forcing an unsafe
-shutdown, it's possible to lose queued events. 
-
-To prevent event loss in these scenarios, you can configure Logstash to use
-persistent queues. With persistent queues enabled, Logstash persists buffered
-events to disk instead of storing them in memory. 
-
-Persistent queues are also useful for Logstash deployments that require high
-throughput and resiliency. Instead of deploying and managing a message
-broker, such as Redis, RabbitMQ, or Apache Kafka, to handle a mismatch in
-cadence between the shipping stage and the relatively expensive processing
-stage, you can enable persistent queues to buffer events on disk. The queue size
-is variable and configurable, which means that you have more control over
-managing situations that can result in back pressure to the source. See <<backpressure-persistent-queues>>. 
-
-[[persistent-queues-advantages]]
-==== Advantages of Persistent Queues
-
-Using persistent queues:
-
-* Provides protection from losing in-flight messages when the Logstash process is shut down or the machine is restarted.
-* Handles the surge of events without having to use an external queueing mechanism like Redis or Kafka.
-* Provides an at-least-once message delivery guarantee. If Logstash is restarted
-while events are in-flight, Logstash will attempt to deliver messages stored
-in the persistent queue until delivery succeeds at least once. In other words,
-messages stored in the persistent queue may be duplicated, but not lost.
+(inputs → pipeline workers) to buffer events. The size of these in-memory
+queues is fixed and not configurable. If Logstash experiences a temporary
+machine failure, the contents of the in-memory queue will be lost. Temporary machine
+failures are scenarios where Logstash or its host machine are terminated
+abnormally but are capable of being restarted. 
+
+In order to protect against data loss during abnormal termination, Logstash has
+a persistent queue feature which will store the message queue on disk.
+Persistent queues provide durability of data within Logstash.
+
+Persistent queues are also useful for Logstash deployments that need large buffers.
+Instead of deploying and managing a message broker, such as Redis, RabbitMQ, or
+Apache Kafka, to facilitate a buffered publish-subscriber model, you can enable
+persistent queues to buffer events on disk and remove the message broker.
+
+In summary, the two benefits of enabling persistent queues are as follows:
+
+* Provides protection from in-flight message loss when the Logstash process is abnormally terminated.
+* Absorbs bursts of events without needing an external buffering mechanism like Redis or Apache Kafka.
 
 [[persistent-queues-limitations]]
 ==== Limitations of Persistent Queues
 
-The current implementation of persistent queues has the following limitations:
+The following are problems not solved by the persistent queue feature:
 
-* This version does not enable full end-to-end resiliency, except for messages
-sent to the <<plugins-inputs-beats,beats>> input. For other inputs, Logstash
-only acknowledges delivery of messages in the filter and output stages, and not
-all the way back to the input or source.
-* It does not handle permanent disk or machine failures. The data persisted to disk is not replicated, so it is still a single point of failure.
+* Input plugins that do not use a request-response protocol cannot be protected from data loss. For example: tcp, udp, zeromq push+pull, and many other inputs do not have a mechanism to acknowledge receipt to the sender. Plugins such as beats and http, which *do* have a acknowledgement capability, are well protected by this queue.
+* It does not handle permanent machine failures such as disk corruption, disk failure, and machine loss. The data persisted to disk is not replicated.
 
 [[persistent-queues-architecture]]
 ==== How Persistent Queues Work
 
-The persistent queue sits between the input and filter stages in the same
+The queue sits between the input and filter stages in the same
 process:
 
-input → persistent queue → filter + output 
-
-The input stage reads data from the configured data source and writes events to
-the persistent queue for processing. As events pass through the pipeline,
-Logstash pulls a batch of events from the persistent queue for processing them
-in the filter and output stages. As events are processed, Logstash uses a
-checkpoint file to track which events are successfully acknowledged (ACKed) as
-processed by Logstash. An event is recorded as ACKed in the checkpoint file if
-the event is successfully sent to the last output stage in the pipeline;
-Logstash does not wait for the output to acknowledge delivery. 
-
-During a normal, controlled shutdown (*CTRL+C*), Logstash finishes
-processing the current in-flight events (that is, the events being processed by
-the filter and output stages, not the queued events), finalizes the ACKing
-of these events, and then terminates the Logstash process. Upon restart,
-Logstash uses the checkpoint file to pick up where it left off in the persistent
-queue and processes the events in the backlog. 
-
-If Logstash crashes or experiences an uncontrolled shutdown, any in-flight
-events are left as unACKed in the persistent queue. Upon restart, Logstash will
-replay the events from its history, potentially leading to duplicate data being
-written to the output.
+input → queue → filter + output 
+
+When an input has events ready to process, it writes them to the queue. When
+the write to the queue is successful, the input can send an acknowledgement to
+its data source.
+
+When processing events from the queue, Logstash acknowledges events as
+completed, within the queue, only after filters and outputs have completed.
+The queue keeps a record of events that have been processed by the pipeline.
+An event is recorded as processed (in this document, called "acknowledged" or
+"ACKed") if, and only if, the event has been processed completely by the
+Logstash pipeline. 
+
+What does acknowledged mean? This means the event has been handled by all
+configured filters and outputs. For example, if you have only one output,
+Elasticsearch, an event is ACKed when the Elasticsearch output has successfully
+sent this event to Elasticsearch. 
+
+During a normal shutdown (*CTRL+C* or SIGTERM), Logstash will stop reading
+from the queue and will finish processing the in-flight events being processed
+by the filters and outputs. Upon restart, Logstash will resume processing the
+events in the persistent queue as well as accepting new events from inputs.
+
+If Logstash is abnormally terminated, any in-flight events will not have been
+ACKed and will be reprocessed by filters and outputs when Logstash is
+restarted. Logstash processes events in batches, so it is possible
+that for any given batch, some of that batch may have been successfully
+completed, but not recorded as ACKed, when an abnormal termination occurs.
+
+For more details specific behaviors of queue writes and acknowledgement, see 
+<<durability-persistent-queues>>.
 
 [[configuring-persistent-queues]]
 ==== Configuring Persistent Queues
@@ -79,18 +76,20 @@ written to the output.
 To configure persistent queues, you can specify the following options in the
 Logstash <<logstash-settings-file,settings file>>:
 
-* `queue.type`: Specify `persisted` to enable persistent queues. By default, persistent queues are disabled (`queue.type: memory`).
+* `queue.type`: Specify `persisted` to enable persistent queues. By default, persistent queues are disabled (default: `queue.type: memory`).
 * `path.queue`: The directory path where the data files will be stored. By default, the files are stored in `path.data/queue`. 
-* `queue.page_capacity`: The size of the page data file. The queue data consists of append-only data files separated into pages. The default size is 250mb. 
-* `queue.max_events`:  The maximum number of unread events that are allowed in the queue. The default is 0 (unlimited).
+* `queue.page_capacity`: The maximum size of a queue page in bytes. The queue data consists of append-only files called "pages". The default size is 250mb. Changing this value is unlikely to have performance benefits.
+// Technically, I know, this isn't "maximum number of events" it's really maximum number of events not yet read by the pipeline worker. We only use this for testing and users generally shouldn't be setting this.
+* `queue.max_events`:  The maximum number of events that are allowed in the queue. The default is 0 (unlimited). This value is used internally for the Logstash test suite.
 * `queue.max_bytes`: The total capacity of the queue in number of bytes. The
 default is 1024mb (1gb). Make sure the capacity of your disk drive is greater
-than the value you specify here. If both `queue.max_events` and 
+than the value you specify here.
+
+If both `queue.max_events` and 
 `queue.max_bytes` are specified, Logstash uses whichever criteria is reached
-first. 
+first. See <<backpressure-persistent-queue>> for behavior when these queue limits are reached.
 
-You can also specify options that control when the checkpoint file gets updated (`queue.checkpoint.acks`, `queue.checkpoint.writes`, and
-`queue.checkpoint.interval`). See <<durability-persistent-queues>>.
+You can also specify options that control when the checkpoint file gets updated (`queue.checkpoint.acks`, `queue.checkpoint.writes`). See <<durability-persistent-queues>>.
 
 Example configuration:
 
@@ -98,24 +97,22 @@ Example configuration:
 queue.type: persisted
 queue.max_bytes: 4gb 
 
-[[backpressure-persistent-queues]]
+[[backpressure-persistent-queue]]
 ==== Handling Back Pressure
 
-Logstash has a built-in mechanism that exerts back pressure on the data flow 
-when the queue is full. This mechanism helps Logstash control the rate of data
-flow at the input stage without overwhelming downstream stages and outputs like
-Elasticsearch.
+When the queue is full, Logstash puts back pressure on the inputs to stall data
+flowing into Logstash. This mechanism helps Logstash control the rate of data
+flow at the input stage without overwhelming outputs like Elasticsearch.
 
-You can control when back pressure happens by using the `queue.max_bytes` 
-setting to configure the capacity of the queue on disk. The following example
-sets the total capacity of the queue to 8gb:
+Use `queue.max_bytes` setting to configure the total capacity of the queue on
+disk. The following example sets the total capacity of the queue to 8gb:
 
 [source, yaml]
 queue.type: persisted
 queue.max_bytes: 8gb
 
-With these settings specified, Logstash will buffer unACKed events on disk until 
-the size of the queue reaches 8gb. When the queue is full of unACKed events, and
+With these settings specified, Logstash will buffer events on disk until the
+size of the queue reaches 8gb. When the queue is full of unACKed events, and
 the size limit has been reached, Logstash will no longer accept new events. 
 
 Each input handles back pressure independently. For example, when the
@@ -128,24 +125,38 @@ events.
 [[durability-persistent-queues]]
 ==== Controlling Durability
 
+Durability is a property of storage writes that ensures data will be available after it's written.
+
 When the persistent queue feature is enabled, Logstash will store events on
-disk. The persistent queue exposes the trade-off between performance and
-durability by providing the following configuration options:
+disk. Logstash commits to disk in a mechanism called checkpointing.
+
+To discuss durability, we need to introduce a few details about how the persistent queue is implemented.
+
+First, the queue itself is a set of pages. There are two kinds of pages: head pages and tail pages. The head page is where new events are written. There is only one head page. When the head page is of a certain size (see `queue.page_capacity`), it becomes a tail page, and a new head page is created. Tail pages are immutable, and the head page is append-only. 
+Second, the queue records details about itself (pages, acknowledgements, etc) in a separate file called a checkpoint file.
+
+When recording a checkpoint, Logstash will:
 
-* `queue.checkpoint.writes`: The number of writes to the queue to trigger an
-fsync to disk. This configuration controls the durability from the producer
-side. Keep in mind that a disk flush is a relatively heavy operation that will
-affect throughput if performed after every write. For instance, if you want to
-ensure that all messages in Logstash's queue are durable, you can set
-`queue.checkpoint.writes: 1`. However, this setting can severely impact
-performance.
+* Call fsync on the head page.
+* Atomically write to disk the current state of the queue.
 
-* `queue.checkpoint.acks`: The number of ACKs to the queue to trigger an fsync to disk. This configuration controls the durability from the consumer side.
+The following settings are available to let you tune durability:
 
-The process of checkpointing is atomic, which means any update to the file is
-saved if successful.
+* `queue.checkpoint.writes`: Logstash will checkpoint after this many writes into the queue. Currently, one event counts as one write, but this may change in future releases.
+* `queue.checkpoint.acks`: Logstah will checkpoint after this many events are acknowledged. This configuration controls the durability at the processing (filter + output)
+part of Logstash.
+
+Disk writes have a resource cost. Tuning the above values higher or lower will trade durability for performance. For instance, if you want to the strongest durability for all input events, you can set `queue.checkpoint.writes: 1`.
+
+The process of checkpointing is atomic, which means any update to the file is saved if successful.
 
 If Logstash is terminated, or if there is a hardware level failure, any data
 that is buffered in the persistent queue, but not yet checkpointed, is lost.
 To avoid this possibility, you can set `queue.checkpoint.writes: 1`, but keep in
 mind that this setting can severely impact performance.
+
+[[garbage-collection]]
+==== Disk Garbage Collection
+
+On disk, the queue is stored as a set of pages where each page is one file. Each page can be at most `queue.page_capacity` in size. Pages are deleted (garbage collected) after all events in that page have been ACKed. If an older page has at least one event that is not yet ACKed, that entire page will remain on disk until all events in that page are successfully processed. Each page containing unprocessed events will count against the `queue.max_bytes` byte size.
+
diff --git a/docs/static/plugin-manager.asciidoc b/docs/static/plugin-manager.asciidoc
index f59626ccfb3..72ed107eec7 100644
--- a/docs/static/plugin-manager.asciidoc
+++ b/docs/static/plugin-manager.asciidoc
@@ -16,7 +16,7 @@ available in your deployment:
 [source,shell]
 ----------------------------------
 bin/logstash-plugin list <1>
-bin/logstash-plugin list --info <2>
+bin/logstash-plugin list --verbose <2>
 bin/logstash-plugin list '*namefragment*' <3>
 bin/logstash-plugin list --group output <4>
 ----------------------------------
diff --git a/docs/static/submitting-a-plugin.asciidoc b/docs/static/submitting-a-plugin.asciidoc
index d85db91a8ff..8de77da375e 100644
--- a/docs/static/submitting-a-plugin.asciidoc
+++ b/docs/static/submitting-a-plugin.asciidoc
@@ -104,4 +104,4 @@ the Logstash repository. When the acceptance guidelines are completed, we will
 facilitate the move to the logstash-plugins organization using the recommended
 https://help.github.com/articles/transferring-a-repository/#transferring-from-a-user-to-an-organization[github process].
 
-pass::[<?edit_url?>]
+
diff --git a/lib/pluginmanager/bundler/logstash_injector.rb b/lib/pluginmanager/bundler/logstash_injector.rb
index 1ac2e0ebe68..45dcc83a594 100644
--- a/lib/pluginmanager/bundler/logstash_injector.rb
+++ b/lib/pluginmanager/bundler/logstash_injector.rb
@@ -5,26 +5,50 @@
 require "bundler/dependency"
 require "bundler/dsl"
 require "bundler/injector"
+require "bundler/shared_helpers"
+require "pluginmanager/gemfile"
+require "pathname"
+
 
 # This class cannot be in the logstash namespace, because of the way the DSL
 # class interact with the other libraries
+module Bundler
+  module SharedHelpers
+    def default_bundle_dir
+      Pathname.new(LogStash::Environment::LOGSTASH_HOME)
+    end
+  end
+end
+
 module Bundler
   class LogstashInjector < ::Bundler::Injector
     def self.inject!(new_deps, options = { :gemfile => LogStash::Environment::GEMFILE, :lockfile => LogStash::Environment::LOCKFILE })
       gemfile = options.delete(:gemfile)
       lockfile = options.delete(:lockfile)
 
-      bundler_format = Array(new_deps).collect { |plugin|  ::Bundler::Dependency.new(plugin.name, "=#{plugin.version}")}
+      bundler_format = new_deps.plugins.collect(&method(:dependency))
+      dependencies = new_deps.dependencies.collect(&method(:dependency))
 
       injector = new(bundler_format)
-      injector.inject(gemfile, lockfile)
+
+      # Some of the internal classes requires to be inside the LOGSTASH_HOME to find the relative
+      # path of the core gems.
+      Dir.chdir(LogStash::Environment::LOGSTASH_HOME) do
+        injector.inject(gemfile, lockfile, dependencies)
+      end
     end
 
+    def self.dependency(plugin)
+      ::Bundler::Dependency.new(plugin.name, "=#{plugin.version}")
+    end
 
     # This class is pretty similar to what bundler's injector class is doing
     # but we only accept a local resolution of the dependencies instead of calling rubygems.
     # so we removed `definition.resolve_remotely!`
-    def inject(gemfile_path, lockfile_path)
+    #
+    # And managing the gemfile is down by using our own Gemfile parser, this allow us to
+    # make it work with gems that are already defined in the gemfile.
+    def inject(gemfile_path, lockfile_path, dependencies)
       if Bundler.settings[:frozen]
         # ensure the lock and Gemfile are synced
         Bundler.definition.ensure_equivalent_gemfile_and_lockfile(true)
@@ -33,16 +57,31 @@ def inject(gemfile_path, lockfile_path)
       end
 
       builder = Dsl.new
-      builder.eval_gemfile(gemfile_path)
+      gemfile = LogStash::Gemfile.new(File.new(gemfile_path, "r+")).load
 
-      @new_deps -= builder.dependencies
+      begin
+        @new_deps.each do |dependency|
+          gemfile.update(dependency.name, dependency.requirement)
+        end
 
-      builder.eval_gemfile("injected gems", new_gem_lines) if @new_deps.any?
-      definition = builder.to_definition(lockfile_path, {})
-      append_to(gemfile_path) if @new_deps.any?
-      definition.lock(lockfile_path)
+        # If the dependency is defined in the gemfile, lets try to update the version with the one we have
+        # with the pack.
+        dependencies.each do |dependency|
+          if gemfile.defined_in_gemfile?(dependency.name)
+            gemfile.update(dependency.name, dependency.requirement)
+          end
+        end
 
-      return @new_deps
+        builder.eval_gemfile("bundler file", gemfile.generate())
+        definition = builder.to_definition(lockfile_path, {})
+        definition.lock(lockfile_path)
+        gemfile.save
+      rescue => e
+        # the error should be handled elsewhere but we need to get the original file if we dont
+        # do this logstash will be in an inconsistent state
+        gemfile.restore!
+        raise e
+      end
     ensure
       Bundler.settings[:frozen] = "1" if frozen
     end
diff --git a/lib/pluginmanager/command.rb b/lib/pluginmanager/command.rb
index 1a7c88bedc9..fa4d40dc8fc 100644
--- a/lib/pluginmanager/command.rb
+++ b/lib/pluginmanager/command.rb
@@ -6,7 +6,7 @@ def gemfile
 
   # If set in debug mode we will raise an exception and display the stacktrace
   def report_exception(readable_message, exception)
-    if ENV["DEBUG"]
+    if debug?
       raise exception
     else
       signal_error("#{readable_message}, message: #{exception.message}")
@@ -14,7 +14,7 @@ def report_exception(readable_message, exception)
   end
 
   def display_bundler_output(output)
-    if ENV['DEBUG'] && output
+    if debug? && output
       # Display what bundler did in the last run
       $stderr.puts("Bundler output")
       $stderr.puts(output)
@@ -35,4 +35,8 @@ def relative_path(path)
     require "pathname"
     ::Pathname.new(path).relative_path_from(::Pathname.new(LogStash::Environment::LOGSTASH_HOME)).to_s
   end
+
+  def debug?
+    ENV["DEBUG"]
+  end
 end
diff --git a/lib/pluginmanager/custom_gem_indexer.rb b/lib/pluginmanager/custom_gem_indexer.rb
new file mode 100644
index 00000000000..0d7d273de2f
--- /dev/null
+++ b/lib/pluginmanager/custom_gem_indexer.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+require "stud/temporary"
+
+module LogStash module PluginManager
+  class CustomGemIndexer
+    GEMS_DIR = "gems"
+
+    class << self
+      # Copy the file to a specific format that `Gem::Indexer` can understand
+      # See `#update_in_memory_index`
+      def copy_to_local_source(temporary_directory)
+        local_source = Stud::Temporary.pathname
+        local_source_gems = ::File.join(local_source, GEMS_DIR)
+
+        FileUtils.mkdir_p(local_source_gems)
+        PluginManager.ui.debug("Creating the index structure format from #{temporary_directory} to #{local_source}")
+
+        Dir.glob(::File.join(temporary_directory, "**", "*.gem")).each do |file|
+          destination = ::File.join(local_source_gems, ::File.basename(file))
+          FileUtils.cp(file, destination)
+        end
+
+        local_source
+      end
+
+      # *WARNING*: Bundler need to not be activated at this point because it wont find anything that
+      # is not defined in the gemfile/lock combo
+      #
+      # This takes a folder with a special structure, will generate an index
+      # similar to what rubygems do and make them available in the local program,
+      # we use this **side effect** to validate theses gems with the current gemfile/lock.
+      # Bundler will assume they are system gems and will use them when doing resolution checks.
+      #
+      #.
+      # ├── gems
+      # │   ├── addressable-2.4.0.gem
+      # │   ├── cabin-0.9.0.gem
+      # │   ├── ffi-1.9.14-java.gem
+      # │   ├── gemoji-1.5.0.gem
+      # │   ├── launchy-2.4.3-java.gem
+      # │   ├── logstash-output-elasticsearch-5.2.0-java.gem
+      # │   ├── logstash-output-secret-0.1.0.gem
+      # │   ├── manticore-0.6.0-java.gem
+      # │   ├── spoon-0.0.6.gem
+      # │   └── stud-0.0.22.gem
+      #
+      # Right now this work fine, but I think we could also use Bundler's SourceList classes to handle the same thing
+      def update_in_memory_index!(local_source)
+        PluginManager.ui.debug("Generating indexes in #{local_source}")
+        indexer = ::Gem::Indexer.new(local_source, { :build_modern => true})
+        indexer.ui = ::Gem::SilentUI.new unless ENV["DEBUG"]
+        indexer.generate_index
+      end
+
+      def index(path)
+        local_source = copy_to_local_source(path)
+        update_in_memory_index!(local_source)
+        local_source
+      end
+    end
+  end
+end end
diff --git a/lib/pluginmanager/errors.rb b/lib/pluginmanager/errors.rb
index 260c5815cbc..691c8b67500 100644
--- a/lib/pluginmanager/errors.rb
+++ b/lib/pluginmanager/errors.rb
@@ -1,6 +1,8 @@
 # encoding: utf-8
 module LogStash module PluginManager
     class PluginManagerError < StandardError; end
+    class PluginNotFoundError < PluginManagerError; end
+    class UnpackablePluginError < PluginManagerError; end
     class FileNotFoundError < PluginManagerError; end
     class InvalidPackError < PluginManagerError; end
     class InstallError < PluginManagerError
diff --git a/lib/pluginmanager/gem_installer.rb b/lib/pluginmanager/gem_installer.rb
index 4a1cf020983..abb686d50db 100644
--- a/lib/pluginmanager/gem_installer.rb
+++ b/lib/pluginmanager/gem_installer.rb
@@ -2,6 +2,7 @@
 require "pluginmanager/ui"
 require "pathname"
 require "rubygems/package"
+require "fileutils"
 
 module LogStash module PluginManager
   # Install a physical gem package to the appropriate location inside logstash
@@ -12,11 +13,13 @@ class GemInstaller
     GEM_HOME = Pathname.new(::File.join(LogStash::Environment::BUNDLE_DIR, "jruby", "1.9"))
     SPECIFICATIONS_DIR = "specifications"
     GEMS_DIR = "gems"
+    CACHE_DIR = "cache"
 
     attr_reader :gem_home
 
     def initialize(gem_file, display_post_install_message = false, gem_home = GEM_HOME)
-      @gem = ::Gem::Package.new(gem_file)
+      @gem_file = gem_file
+      @gem = ::Gem::Package.new(@gem_file)
       @gem_home = Pathname.new(gem_home)
       @display_post_install_message = display_post_install_message
     end
@@ -26,6 +29,7 @@ def install
       extract_files
       write_specification
       display_post_install_message
+      copy_gem_file_to_cache
     end
 
     def self.install(gem_file, display_post_install_message = false, gem_home = GEM_HOME)
@@ -41,6 +45,10 @@ def spec_dir
       gem_home.join(SPECIFICATIONS_DIR)
     end
 
+    def cache_dir
+      gem_home.join(CACHE_DIR)
+    end
+
     def spec_file
       spec_dir.join("#{spec.full_name}.gemspec")
     end
@@ -69,10 +77,16 @@ def display_post_install_message?
       @display_post_install_message && !spec.post_install_message.nil?
     end
 
+    def copy_gem_file_to_cache
+      destination = ::File.join(cache_dir, ::File.basename(@gem_file))
+      FileUtils.cp(@gem_file, destination)
+    end
+
     def create_destination_folders
       FileUtils.mkdir_p(gem_home)
       FileUtils.mkdir_p(gem_dir)
       FileUtils.mkdir_p(spec_dir)
+      FileUtils.mkdir_p(cache_dir)
     end
   end
 end end
diff --git a/lib/pluginmanager/gemfile.rb b/lib/pluginmanager/gemfile.rb
index c0b996dffd7..86af9a40826 100644
--- a/lib/pluginmanager/gemfile.rb
+++ b/lib/pluginmanager/gemfile.rb
@@ -71,6 +71,10 @@ def restore
       @gemset = @original_backup
     end
 
+    def defined_in_gemfile?(name)
+      @gemset.find_gem(name)
+    end
+
     def restore!
       restore
       save
diff --git a/lib/pluginmanager/install.rb b/lib/pluginmanager/install.rb
index e7ba94158ce..476b00ec7be 100644
--- a/lib/pluginmanager/install.rb
+++ b/lib/pluginmanager/install.rb
@@ -20,6 +20,9 @@ class LogStash::PluginManager::Install < LogStash::PluginManager::Command
   # but the argument parsing does not support it for now so currently if specifying --version only
   # one plugin name can be also specified.
   def execute
+    # Turn off any jar dependencies lookup when running with `--local`
+    ENV["JARS_SKIP"] = "true" if local?
+
     # This is a special flow for PACK related plugins,
     # if we dont detect an pack we will just use the normal `Bundle install` Strategy`
     # this could be refactored into his own strategy
@@ -155,6 +158,7 @@ def install_gems_list!(install_list)
   # Bundler 2.0, will have support for plugins source we could create a .gem source
   # to support it.
   def extract_local_gems_plugins
+    FileUtils.mkdir_p(LogStash::Environment::CACHE_PATH)
     plugins_arg.collect do |plugin|
       # We do the verify before extracting the gem so we dont have to deal with unused path
       if verify?
@@ -162,6 +166,9 @@ def extract_local_gems_plugins
         signal_error("Installation aborted, verification failed for #{plugin}") unless LogStash::PluginManager.logstash_plugin?(plugin, version)
       end
 
+      # Make the original .gem available for the prepare-offline-pack,
+      # paquet will lookup in the cache directory before going to rubygems.
+      FileUtils.cp(plugin, ::File.join(LogStash::Environment::CACHE_PATH, ::File.basename(plugin)))
       package, path = LogStash::Rubygems.unpack(plugin, LogStash::Environment::LOCAL_GEM_PATH)
       [package.spec.name, package.spec.version, { :path => relative_path(path) }]
     end
diff --git a/lib/pluginmanager/main.rb b/lib/pluginmanager/main.rb
index fd1f75b9807..fed9c4cddc6 100644
--- a/lib/pluginmanager/main.rb
+++ b/lib/pluginmanager/main.rb
@@ -6,8 +6,6 @@
 ENV["GEM_HOME"] = ENV["GEM_PATH"] = LogStash::Environment.logstash_gem_home
 Gem.use_paths(LogStash::Environment.logstash_gem_home)
 
-LogStash::Bundler.setup!({:without => [:build, :development]})
-
 module LogStash
   module PluginManager
   end
@@ -23,6 +21,7 @@ module PluginManager
 require "pluginmanager/pack"
 require "pluginmanager/unpack"
 require "pluginmanager/generate"
+require "pluginmanager/prepare_offline_pack"
 
 module LogStash
   module PluginManager
@@ -33,10 +32,11 @@ class Main < Clamp::Command
       subcommand "install", "Install a Logstash plugin", LogStash::PluginManager::Install
       subcommand "remove", "Remove a Logstash plugin", LogStash::PluginManager::Remove
       subcommand "update", "Update a plugin", LogStash::PluginManager::Update
-      subcommand "pack", "Package currently installed plugins", LogStash::PluginManager::Pack
-      subcommand "unpack", "Unpack packaged plugins", LogStash::PluginManager::Unpack
+      subcommand "pack", "Package currently installed plugins, Deprecated: Please use prepare-offline-pack instead", LogStash::PluginManager::Pack
+      subcommand "unpack", "Unpack packaged plugins, Deprecated: Please use prepare-offline-pack instead", LogStash::PluginManager::Unpack
       subcommand "generate", "Create the foundation for a new plugin", LogStash::PluginManager::Generate
       subcommand "uninstall", "Uninstall a plugin. Deprecated: Please use remove instead", LogStash::PluginManager::Remove
+      subcommand "prepare-offline-pack", "Create an archive of specified plugins to use for offline installation", LogStash::PluginManager::PrepareOfflinePack
     end
   end
 end
diff --git a/lib/pluginmanager/offline_plugin_packager.rb b/lib/pluginmanager/offline_plugin_packager.rb
new file mode 100644
index 00000000000..a0d21413570
--- /dev/null
+++ b/lib/pluginmanager/offline_plugin_packager.rb
@@ -0,0 +1,120 @@
+# encoding: utf-8
+require "pluginmanager/ui"
+require "pluginmanager/errors"
+require "bootstrap/environment"
+require "bootstrap/util/compress"
+require "paquet"
+require "stud/temporary"
+require "fileutils"
+
+module LogStash module PluginManager
+  class SpecificationHelpers
+    WILDCARD = "*"
+    WILDCARD_INTO_RE = ".*"
+
+    def self.find_by_name_with_wildcards(pattern)
+      re = transform_pattern_into_re(pattern)
+      ::Gem::Specification.find_all.select do |specification|
+        specification.name =~ re
+      end
+    end
+
+    def self.transform_pattern_into_re(pattern)
+      Regexp.new("^#{pattern.gsub(WILDCARD, WILDCARD_INTO_RE)}$")
+    end
+  end
+
+  class OfflinePluginPackager
+    LOGSTASH_DIR = "logstash"
+    DEPENDENCIES_DIR = ::File.join(LOGSTASH_DIR, "dependencies")
+
+    # To make sure we have the maximun compatibility
+    # we will ignore theses gems and they wont be included in the pack
+    IGNORE_GEMS_IN_PACK = %w(
+      logstash-core
+      logstash-core-queue-jruby
+      logstash-core-event-java
+      logstash-core-plugin-api
+      jar-dependencies
+    )
+
+    INVALID_PLUGINS_TO_EXPLICIT_PACK = IGNORE_GEMS_IN_PACK.collect { |name| /^#{name}/ } + [
+      /mixin/
+    ]
+
+    def initialize(plugins_to_package, target)
+      @plugins_to_package = Array(plugins_to_package)
+      @target = target
+
+      validate_plugins!
+    end
+
+    def validate_plugins!
+      @plugins_to_package.each do |plugin_name|
+        if INVALID_PLUGINS_TO_EXPLICIT_PACK.any? { |invalid_name| plugin_name =~ invalid_name }
+          raise UnpackablePluginError, "Cannot explicitly pack `#{plugin_name}` for offline installation"
+        end
+      end
+    end
+
+    def generate_temporary_path
+      Stud::Temporary.pathname
+    end
+
+    def explicitly_declared_plugins_specs
+      @plugins_to_package.collect do |plugin_pattern|
+        specs = SpecificationHelpers.find_by_name_with_wildcards(plugin_pattern)
+
+        if specs.size > 0
+          specs
+        else
+          raise LogStash::PluginManager::PluginNotFoundError, "Cannot find plugins matching: `#{plugin_pattern}`"
+        end
+      end.flatten
+    end
+
+    def execute
+      temp_path = generate_temporary_path
+      packet_gem = Paquet::Gem.new(temp_path, LogStash::Environment::CACHE_PATH)
+
+      explicit_plugins_specs = explicitly_declared_plugins_specs
+
+      explicit_plugins_specs.each do |spec|
+        packet_gem.add(spec.name)
+      end
+
+      IGNORE_GEMS_IN_PACK.each do |gem_name|
+        packet_gem.ignore(gem_name)
+      end
+
+      packet_gem.pack
+
+      prepare_package(explicit_plugins_specs, temp_path)
+      LogStash::Util::Zip.compress(temp_path, @target)
+    ensure
+      FileUtils.rm_rf(temp_path)
+    end
+
+    def prepare_package(explicit_plugins, temp_path)
+      FileUtils.mkdir_p(::File.join(temp_path, LOGSTASH_DIR))
+      FileUtils.mkdir_p(::File.join(temp_path, DEPENDENCIES_DIR))
+
+      explicit_path = ::File.join(temp_path, LOGSTASH_DIR)
+      dependencies_path = ::File.join(temp_path, DEPENDENCIES_DIR)
+
+      Dir.glob(::File.join(temp_path, "*.gem")).each do |gem_file|
+        filename = ::File.basename(gem_file)
+
+        if explicit_plugins.any? { |spec| filename =~ /^#{spec.name}/ }
+          FileUtils.mv(gem_file, ::File.join(explicit_path, filename))
+        else
+          FileUtils.mv(gem_file, ::File.join(dependencies_path, filename))
+        end
+      end
+    end
+
+    def self.package(plugins_args, target)
+      OfflinePluginPackager.new(plugins_args, target).execute
+    end
+  end
+end end
diff --git a/lib/pluginmanager/pack.rb b/lib/pluginmanager/pack.rb
index 18b46e18511..2f00728199d 100644
--- a/lib/pluginmanager/pack.rb
+++ b/lib/pluginmanager/pack.rb
@@ -8,6 +8,8 @@ class LogStash::PluginManager::Pack < LogStash::PluginManager::PackCommand
   option "--overwrite", :flag, "Overwrite a previously generated package file", :default => false
 
   def execute
+    signal_deprecation_warning_for_pack
+
     puts("Packaging plugins for offline usage")
 
     validate_target_file
diff --git a/lib/pluginmanager/pack_command.rb b/lib/pluginmanager/pack_command.rb
index 2409b212f97..3f4ed0b383c 100644
--- a/lib/pluginmanager/pack_command.rb
+++ b/lib/pluginmanager/pack_command.rb
@@ -10,4 +10,12 @@ def archive_manager
   def file_extension
     zip? ? ".zip" : ".tar.gz"
   end
+
+  def signal_deprecation_warning_for_pack
+  message =<<-EOS
+The pack and the unpack command are now deprecated and will be removed in a future version of Logstash.
+See the `prepare-offline-pack` to update your workflow. You can get documentation about this by running `bin/logstash-plugin prepare-offline-pack --help`
+  EOS
+  puts message
+  end
 end
diff --git a/lib/pluginmanager/pack_fetch_strategy/repository.rb b/lib/pluginmanager/pack_fetch_strategy/repository.rb
index 777bd59843a..4898e32f298 100644
--- a/lib/pluginmanager/pack_fetch_strategy/repository.rb
+++ b/lib/pluginmanager/pack_fetch_strategy/repository.rb
@@ -10,12 +10,17 @@
 
 module LogStash module PluginManager module PackFetchStrategy
   class Repository
-    ELASTIC_PACK_BASE_URI = ENV["LOGSTASH_PACK_URL"] || "https://artifacts.elastic.co/downloads/logstash-plugins"
+    DEFAULT_PACK_URL = "https://artifacts.elastic.co/downloads/logstash-plugins"
     PACK_EXTENSION = "zip"
 
     class << self
+      def elastic_pack_base_uri
+        env_url = ENV["LOGSTASH_PACK_URL"]
+        (env_url.nil? || env_url.empty?) ? DEFAULT_PACK_URL : env_url
+      end
+
       def pack_uri(plugin_name)
-        url = "#{ELASTIC_PACK_BASE_URI}/#{plugin_name}/#{plugin_name}-#{LOGSTASH_VERSION}.#{PACK_EXTENSION}"
+        url = "#{elastic_pack_base_uri}/#{plugin_name}/#{plugin_name}-#{LOGSTASH_VERSION}.#{PACK_EXTENSION}"
         URI.parse(url)
       end
 
diff --git a/lib/pluginmanager/pack_installer/local.rb b/lib/pluginmanager/pack_installer/local.rb
index 702eb408754..ff75b971026 100644
--- a/lib/pluginmanager/pack_installer/local.rb
+++ b/lib/pluginmanager/pack_installer/local.rb
@@ -2,6 +2,7 @@
 require "pluginmanager/ui"
 require "pluginmanager/bundler/logstash_injector"
 require "pluginmanager/gem_installer"
+require "pluginmanager/custom_gem_indexer"
 require "pluginmanager/errors"
 require "pluginmanager/pack_installer/pack"
 require "bootstrap/util/compress"
@@ -10,7 +11,6 @@
 module LogStash module PluginManager module PackInstaller
   class Local
     PACK_EXTENSION = ".zip"
-    GEMS_DIR = "gems"
     LOGSTASH_PATTERN_RE = /logstash\/?/
 
     attr_reader :local_file
@@ -29,13 +29,12 @@ def execute
       pack = LogStash::PluginManager::PackInstaller::Pack.new(uncompressed_path)
       raise PluginManager::InvalidPackError, "The pack must contains at least one plugin" unless pack.valid?
 
-      local_source = move_to_local_source(uncompressed_path)
-      update_in_memory_index(local_source)
+      local_source = LogStash::PluginManager::CustomGemIndexer.index(uncompressed_path)
 
       # Try to add the gems to the current gemfile and lock file, if successful
       # both of them will be updated. This injector is similar to Bundler's own injector class
       # minus the support for additionals source and doing local resolution only.
-      added = ::Bundler::LogstashInjector.inject!(pack.plugins)
+      ::Bundler::LogstashInjector.inject!(pack)
 
       # When successfull its safe to install the gem and their specifications in the bundle directory
       pack.gems.each do |packed_gem|
@@ -44,7 +43,7 @@ def execute
       end
       PluginManager.ui.info("Install successful")
     rescue ::Bundler::BundlerError => e
-      raise PluginManager::InstallError.new(e), "An error occurent went installing plugins"
+      raise PluginManager::InstallError.new(e), "An error occurred went installing plugins"
     ensure
       FileUtils.rm_rf(uncompressed_path) if uncompressed_path && Dir.exist?(uncompressed_path)
       FileUtils.rm_rf(local_source) if local_source && Dir.exist?(local_source)
@@ -64,48 +63,5 @@ def uncompress(source)
     def valid_format?(local_file)
       ::File.extname(local_file).downcase == PACK_EXTENSION
     end
-
-    # Copy the file to a specific format that `Gem::Indexer` can understand
-    # See `#update_in_memory_index`
-    def move_to_local_source(temporary_directory)
-      local_source = Stud::Temporary.pathname
-      local_source_gems = ::File.join(local_source, GEMS_DIR)
-
-      FileUtils.mkdir_p(local_source_gems)
-      PluginManager.ui.debug("Creating the index structure format from #{temporary_directory} to #{local_source}")
-
-      Dir.glob(::File.join(temporary_directory, "**", "*.gem")).each do |file|
-        destination = ::File.join(local_source_gems, ::File.basename(file))
-        FileUtils.cp(file, destination)
-      end
-
-      local_source
-    end
-
-    # This takes a folder with a special structure, will generate an index
-    # similar to what rubygems do and make them available in the local program,
-    # we use this **side effect** to validate theses gems with the current gemfile/lock.
-    # Bundler will assume they are system gems and will use them when doing resolution checks.
-    #
-    #.
-    # ├── gems
-    # │   ├── addressable-2.4.0.gem
-    # │   ├── cabin-0.9.0.gem
-    # │   ├── ffi-1.9.14-java.gem
-    # │   ├── gemoji-1.5.0.gem
-    # │   ├── launchy-2.4.3-java.gem
-    # │   ├── logstash-output-elasticsearch-5.2.0-java.gem
-    # │   ├── logstash-output-secret-0.1.0.gem
-    # │   ├── manticore-0.6.0-java.gem
-    # │   ├── spoon-0.0.6.gem
-    # │   └── stud-0.0.22.gem
-    #
-    # Right now this work fine, but I think we could also use Bundler's SourceList classes to handle the same thing
-    def update_in_memory_index(local_source)
-      PluginManager.ui.debug("Generating indexes in #{local_source}")
-      indexer = ::Gem::Indexer.new(local_source, { :build_modern => true})
-      indexer.ui = ::Gem::SilentUI.new unless ENV["DEBUG"]
-      indexer.generate_index
-    end
   end
 end end end
diff --git a/lib/pluginmanager/pack_installer/pack.rb b/lib/pluginmanager/pack_installer/pack.rb
index 7c9b5bbe8e7..3090472dbf2 100644
--- a/lib/pluginmanager/pack_installer/pack.rb
+++ b/lib/pluginmanager/pack_installer/pack.rb
@@ -67,7 +67,6 @@ def initialize(source)
     end
 
     def plugins
-      require "pry"
       gems.select { |gem| !gem.dependency? }
     end
 
diff --git a/lib/pluginmanager/prepare_offline_pack.rb b/lib/pluginmanager/prepare_offline_pack.rb
new file mode 100644
index 00000000000..8cd9736bcf5
--- /dev/null
+++ b/lib/pluginmanager/prepare_offline_pack.rb
@@ -0,0 +1,61 @@
+# encoding: utf-8
+require "pluginmanager/command"
+require "pluginmanager/errors"
+
+class LogStash::PluginManager::PrepareOfflinePack < LogStash::PluginManager::Command
+  parameter "[PLUGIN] ...", "plugin name(s)", :attribute_name => :plugins_arg
+  option "--output", "OUTPUT", "output file", :default => ::File.join(LogStash::Environment::LOGSTASH_HOME, "logstash-offline-plugins-#{LOGSTASH_VERSION}.zip")
+
+  def execute
+    validate_arguments!
+
+    # We need to start bundler, dependencies so  the plugins are available for the prepare
+    LogStash::Bundler.setup!({:without => [:build, :development]})
+
+    # manually require paquet since its an external dependency
+    require "pluginmanager/offline_plugin_packager"
+    require "paquet"
+    require "paquet/shell_ui"
+
+    # Override the shell output with the one from the plugin manager
+    # To silence some of debugs/info statements
+    Paquet.ui = Paquet::SilentUI unless debug?
+
+    FileUtils.rm_rf(output) if ::File.exist?(output)
+
+    LogStash::PluginManager::OfflinePluginPackager.package(plugins_arg, output)
+
+    message = <<-EOS
+Offline package created at: #{output}
+
+You can install it with this command `bin/logstash-plugin install file://#{::File.expand_path(output)}`
+    EOS
+
+    LogStash::PluginManager::ui.info(message)
+  rescue LogStash::PluginManager::UnpackablePluginError => e
+    report_exception("Offline package", e)
+  rescue LogStash::PluginManager::PluginNotFoundError => e
+    report_exception("Cannot create the offline archive", e)
+  end
+
+  def validate_arguments!
+    if plugins_arg.size == 0
+      message = <<-EOS
+You need to specify at least one plugin or use a wildcard expression.
+
+Examples:
+bin/logstash-plugin prepare-offline-pack logstash-input-beats
+bin/logstash-plugin prepare-offline-pack logstash-filter-jdbc logstash-input-beats
+bin/logstash-plugin prepare-offline-pack logstash-filter-*
+bin/logstash-plugin prepare-offline-pack logstash-filter-* logstash-input-beats
+
+You can get a list of the installed plugin by running `bin/logstash-plugin list`
+
+The prepare offline will pack the currently installed plugins and their dependencies
+for offline installation.
+EOS
+      signal_usage_error(message)
+    end
+  end
+end
+
diff --git a/lib/pluginmanager/unpack.rb b/lib/pluginmanager/unpack.rb
index 7937e7d2e24..f1f7221e171 100644
--- a/lib/pluginmanager/unpack.rb
+++ b/lib/pluginmanager/unpack.rb
@@ -8,6 +8,8 @@ class LogStash::PluginManager::Unpack < LogStash::PluginManager::PackCommand
   parameter "file", "the package file name", :attribute_name => :package_file, :required => true
 
   def execute
+    signal_deprecation_warning_for_pack
+
     puts("Unpacking #{package_file}")
 
     FileUtils.rm_rf(LogStash::Environment::CACHE_PATH)
diff --git a/lib/pluginmanager/update.rb b/lib/pluginmanager/update.rb
index e5de20cd494..2007c8ad7bd 100644
--- a/lib/pluginmanager/update.rb
+++ b/lib/pluginmanager/update.rb
@@ -14,6 +14,9 @@ class LogStash::PluginManager::Update < LogStash::PluginManager::Command
   option "--local", :flag, "force local-only plugin update. see bin/logstash-plugin package|unpack", :default => false
 
   def execute
+    # Turn off any jar dependencies lookup when running with `--local`
+    ENV["JARS_SKIP"] = "true" if local?
+
     # remove "system" local gems used by LS
     local_gems = gemfile.locally_installed_gems.map(&:name) - NON_PLUGIN_LOCAL_GEMS
 
diff --git a/lib/pluginmanager/utils/downloader.rb b/lib/pluginmanager/utils/downloader.rb
index 7c04dfb1fcf..0d520febfaa 100644
--- a/lib/pluginmanager/utils/downloader.rb
+++ b/lib/pluginmanager/utils/downloader.rb
@@ -46,7 +46,7 @@ def fetch(redirect_count = 0)
 
       begin
         FileUtils.mkdir_p(download_to)
-        downloaded_file = ::File.open(::File.join(download_to, ::File.basename(remote_file_uri.path)), "w")
+        downloaded_file = ::File.open(::File.join(download_to, ::File.basename(remote_file_uri.path)), "wb")
 
         HttpClient.start(remote_file_uri) do |http|
           request = Net::HTTP::Get.new(remote_file_uri.path)
diff --git a/logstash-core-event-java/lib/logstash-core-event-java/version.rb b/logstash-core-event-java/lib/logstash-core-event-java/version.rb
index eb41e5e8a43..f3030c840dd 100644
--- a/logstash-core-event-java/lib/logstash-core-event-java/version.rb
+++ b/logstash-core-event-java/lib/logstash-core-event-java/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_EVENT_JAVA_VERSION = "5.1.2"
+LOGSTASH_CORE_EVENT_JAVA_VERSION = "6.0.0-alpha1"
diff --git a/logstash-core-event-java/spec/event_spec.rb b/logstash-core-event-java/spec/event_spec.rb
index 3402270c92b..1ed0a9777a5 100644
--- a/logstash-core-event-java/spec/event_spec.rb
+++ b/logstash-core-event-java/spec/event_spec.rb
@@ -61,6 +61,16 @@
       expect(e.get("[foo][2]")).to eq(1.0)
       expect(e.get("[foo][3]")).to be_nil
     end
+
+    context "negative array values" do
+      it "should index from the end of the array" do
+        list = ["bar", 1, 1.0]
+        e = LogStash::Event.new({"foo" => list})
+        expect(e.get("[foo][-3]")).to eq(list[-3])
+        expect(e.get("[foo][-2]")).to eq(list[-2])
+        expect(e.get("[foo][-1]")).to eq(list[-1])
+      end
+    end
   end
 
   context "#set" do
diff --git a/logstash-core-event-java/src/main/java/org/logstash/Accessors.java b/logstash-core-event-java/src/main/java/org/logstash/Accessors.java
index bdf6c622b83..c4cfd492c7d 100644
--- a/logstash-core-event-java/src/main/java/org/logstash/Accessors.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Accessors.java
@@ -34,10 +34,12 @@ public Object del(String reference) {
                 return ((Map<String, Object>) target).remove(field.getKey());
             } else if (target instanceof List) {
                 int i = Integer.parseInt(field.getKey());
-                if (i < 0 || i >= ((List) target).size()) {
+                try {
+                    int offset = listIndex(i, ((List) target).size());
+                    return ((List)target).remove(offset);
+                } catch (IndexOutOfBoundsException e) {
                     return null;
                 }
-                return ((List<Object>) target).remove(i);
             } else {
                 throw newCollectionException(target);
             }
@@ -112,10 +114,13 @@ private Object findCreateTarget(FieldReference field) {
     }
 
     private boolean foundInList(List<Object> target, int index) {
-        if (index < 0 || index >= target.size()) {
+        try {
+            int offset = listIndex(index, target.size());
+            return target.get(offset) != null;
+        } catch (IndexOutOfBoundsException e) {
             return false;
         }
-        return target.get(index) != null;
+
     }
 
     private boolean foundInMap(Map<String, Object> target, String key) {
@@ -127,12 +132,12 @@ private Object fetch(Object target, String key) {
             Object result = ((Map<String, Object>) target).get(key);
             return result;
         } else if (target instanceof List) {
-            int i = Integer.parseInt(key);
-            if (i < 0 || i >= ((List) target).size()) {
+            try {
+                int offset = listIndex(Integer.parseInt(key), ((List) target).size());
+                return ((List<Object>) target).get(offset);
+            } catch (IndexOutOfBoundsException e) {
                 return null;
             }
-            Object result = ((List<Object>) target).get(i);
-            return result;
         } else if (target == null) {
             return null;
         } else {
@@ -156,7 +161,8 @@ private Object store(Object target, String key, Object value) {
                 }
                 ((List<Object>) target).add(value);
             } else {
-                ((List<Object>) target).set(i, value);
+                int offset = listIndex(i, ((List) target).size());
+                ((List<Object>) target).set(offset, value);
             }
         } else {
             throw newCollectionException(target);
@@ -174,4 +180,23 @@ private boolean isCollection(Object target) {
     private ClassCastException newCollectionException(Object target) {
         return new ClassCastException("expecting List or Map, found "  + target.getClass());
     }
+
+    /* 
+     * Returns a positive integer offset for a list of known size.
+     *
+     * @param i if positive, and offset from the start of the list. If negative, the offset from the end of the list, where -1 means the last element.
+     * @param size the size of the list.
+     * @return the positive integer offset for the list given by index i.
+     */
+    public static int listIndex(int i, int size) {
+        if (i >= size || i < -size) {
+            throw new IndexOutOfBoundsException("Index " + i + " is out of bounds for a list with size " + size);
+        }
+
+        if (i < 0) { // Offset from the end of the array.
+            return size + i;
+        } else {
+            return i;
+        }
+    }
 }
diff --git a/logstash-core-event-java/src/test/java/org/logstash/AccessorsTest.java b/logstash-core-event-java/src/test/java/org/logstash/AccessorsTest.java
index df0a56c5f09..634ef9ad88a 100644
--- a/logstash-core-event-java/src/test/java/org/logstash/AccessorsTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/AccessorsTest.java
@@ -1,5 +1,11 @@
 package org.logstash;
 
+import org.junit.experimental.theories.DataPoint;
+import org.junit.Rule;
+import org.junit.rules.ExpectedException;
+import org.junit.experimental.theories.Theories;
+import org.junit.experimental.theories.Theory;
+import org.junit.runner.RunWith;
 import org.junit.Test;
 
 import static org.junit.Assert.*;
@@ -207,4 +213,38 @@ public void testStaleTargetCache() throws Exception {
         assertEquals(accessors.get("[foo][bar]"), null);
         assertEquals(accessors.get("[foo]"), "boom");
     }
+
+    @RunWith(Theories.class)
+    public static class TestListIndexFailureCases {
+      private static final int size = 10;
+
+      @DataPoint
+      public static final int tooLarge = size;
+
+      @DataPoint
+      public static final int tooLarge1 = size+1;
+
+      @DataPoint
+      public static final int tooLargeNegative = -size - 1;
+
+      @Rule
+      public ExpectedException exception = ExpectedException.none();
+
+      @Theory
+      public void testListIndexOutOfBounds(int i) {
+        exception.expect(IndexOutOfBoundsException.class);
+        Accessors.listIndex(i, size);
+      }
+    }
+
+    public static class TestListIndex {
+      public void testListIndexOutOfBounds() {
+        assertEquals(Accessors.listIndex(0, 10), 0);
+        assertEquals(Accessors.listIndex(1, 10), 1);
+        assertEquals(Accessors.listIndex(9, 10), 9);
+        assertEquals(Accessors.listIndex(-1, 10), 9);
+        assertEquals(Accessors.listIndex(-9, 10), 1);
+        assertEquals(Accessors.listIndex(-10, 10), 0);
+      }
+    }
 }
diff --git a/logstash-core-event/lib/logstash-core-event/version.rb b/logstash-core-event/lib/logstash-core-event/version.rb
index bb4bb9cb7a9..74914785722 100644
--- a/logstash-core-event/lib/logstash-core-event/version.rb
+++ b/logstash-core-event/lib/logstash-core-event/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_EVENT_VERSION = "5.1.2"
+LOGSTASH_CORE_EVENT_VERSION = "6.0.0-alpha1"
diff --git a/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
index d6eb52f96b4..e0ebc222533 100644
--- a/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
+++ b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
@@ -1 +1,2 @@
-LOGSTASH_CORE_PLUGIN_API = "2.1.20"
+# encoding: utf-8
+LOGSTASH_CORE_PLUGIN_API = "2.1.16"
diff --git a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
index d1453e3451e..820aa8de189 100644
--- a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
+++ b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
@@ -17,7 +17,7 @@ Gem::Specification.new do |gem|
   gem.require_paths = ["lib"]
   gem.version       = LOGSTASH_CORE_PLUGIN_API
 
-  gem.add_runtime_dependency "logstash-core", "5.1.2"
+  gem.add_runtime_dependency "logstash-core", "6.0.0-alpha1"
 
   # Make sure we dont build this gem from a non jruby
   # environment.
diff --git a/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/version.rb b/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/version.rb
index a35e199357f..225cfbbad2f 100644
--- a/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/version.rb
+++ b/logstash-core-queue-jruby/lib/logstash-core-queue-jruby/version.rb
@@ -1,3 +1,3 @@
 # encoding: utf-8
 
-LOGSTASH_CORE_QUEUE_JRUBY_VERSION = "5.1.2"
+LOGSTASH_CORE_QUEUE_JRUBY_VERSION = "6.0.0-alpha1"
diff --git a/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
index cf63c360c9d..e36f425ee3f 100644
--- a/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
+++ b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
@@ -87,6 +87,46 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
             return context.nil;
         }
 
+        @JRubyMethod(name = "max_unread_events")
+        public IRubyObject ruby_max_unread_events(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxUnread());
+        }
+
+        @JRubyMethod(name = "max_size_in_bytes")
+        public IRubyObject ruby_max_size_in_bytes(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxBytes());
+        }
+
+        @JRubyMethod(name = "page_capacity")
+        public IRubyObject ruby_page_capacity(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getPageCapacity());
+        }
+
+        @JRubyMethod(name = "dir_path")
+        public IRubyObject ruby_dir_path(ThreadContext context) {
+            return context.runtime.newString(queue.getDirPath());
+        }
+
+        @JRubyMethod(name = "current_byte_size")
+        public IRubyObject ruby_current_byte_size(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getCurrentByteSize());
+        }
+
+        @JRubyMethod(name = "acked_count")
+        public IRubyObject ruby_acked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getAckedCount());
+        }
+
+        @JRubyMethod(name = "unacked_count")
+        public IRubyObject ruby_unacked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnackedCount());
+        }
+
+        @JRubyMethod(name = "unread_count")
+        public IRubyObject ruby_unread_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnreadCount());
+        }
+
         @JRubyMethod(name = "open")
         public IRubyObject ruby_open(ThreadContext context)
         {
diff --git a/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
index 90fe314823d..db80f228454 100644
--- a/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
+++ b/logstash-core-queue-jruby/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueMemoryExtLibrary.java
@@ -81,6 +81,46 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
             return context.nil;
         }
 
+        @JRubyMethod(name = "max_unread_events")
+        public IRubyObject ruby_max_unread_events(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxUnread());
+        }
+
+        @JRubyMethod(name = "max_size_in_bytes")
+        public IRubyObject ruby_max_size_in_bytes(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getMaxBytes());
+        }
+
+        @JRubyMethod(name = "page_capacity")
+        public IRubyObject ruby_page_capacity(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getPageCapacity());
+        }
+
+        @JRubyMethod(name = "dir_path")
+        public IRubyObject ruby_dir_path(ThreadContext context) {
+            return context.runtime.newString(queue.getDirPath());
+        }
+
+        @JRubyMethod(name = "current_byte_size")
+        public IRubyObject ruby_current_byte_size(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getCurrentByteSize());
+        }
+
+        @JRubyMethod(name = "acked_count")
+        public IRubyObject ruby_acked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getAckedCount());
+        }
+
+        @JRubyMethod(name = "unread_count")
+        public IRubyObject ruby_unread_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnreadCount());
+        }
+
+        @JRubyMethod(name = "unacked_count")
+        public IRubyObject ruby_unacked_count(ThreadContext context) {
+            return context.runtime.newFixnum(queue.getUnackedCount());
+        }
+
         @JRubyMethod(name = "open")
         public IRubyObject ruby_open(ThreadContext context)
         {
diff --git a/logstash-core/build.gradle b/logstash-core/build.gradle
index 625b73933a9..0d1c318ccf2 100644
--- a/logstash-core/build.gradle
+++ b/logstash-core/build.gradle
@@ -110,7 +110,6 @@ idea {
 }
 
 dependencies {
-    runtime 'org.apache.logging.log4j:log4j-1.2-api:2.6.2'
     compile 'org.apache.logging.log4j:log4j-api:2.6.2'
     compile 'org.apache.logging.log4j:log4j-core:2.6.2'
     compile 'com.fasterxml.jackson.core:jackson-core:2.7.4'
diff --git a/logstash-core/gemspec_jars.rb b/logstash-core/gemspec_jars.rb
index 15f2967a4b6..b18f7aa2e5d 100644
--- a/logstash-core/gemspec_jars.rb
+++ b/logstash-core/gemspec_jars.rb
@@ -2,7 +2,6 @@
 # runtime dependencies to generate this gemspec dependencies file to be eval'ed by the gemspec
 # for the jar-dependencies requirements.
 
-gem.requirements << "jar org.apache.logging.log4j:log4j-1.2-api, 2.6.2"
 gem.requirements << "jar org.apache.logging.log4j:log4j-api, 2.6.2"
 gem.requirements << "jar org.apache.logging.log4j:log4j-core, 2.6.2"
 gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.4"
diff --git a/logstash-core/lib/logstash-core/version.rb b/logstash-core/lib/logstash-core/version.rb
index 44931418c81..9cd610ab455 100644
--- a/logstash-core/lib/logstash-core/version.rb
+++ b/logstash-core/lib/logstash-core/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_VERSION = "5.1.2"
+LOGSTASH_CORE_VERSION = "6.0.0-alpha1"
diff --git a/logstash-core/lib/logstash-core_jars.rb b/logstash-core/lib/logstash-core_jars.rb
index f548c9fd35c..b759e1d5833 100644
--- a/logstash-core/lib/logstash-core_jars.rb
+++ b/logstash-core/lib/logstash-core_jars.rb
@@ -5,7 +5,6 @@
   require 'org/apache/logging/log4j/log4j-core/2.6.2/log4j-core-2.6.2.jar'
   require 'org/apache/logging/log4j/log4j-api/2.6.2/log4j-api-2.6.2.jar'
   require 'com/fasterxml/jackson/core/jackson-core/2.7.4/jackson-core-2.7.4.jar'
-  require 'org/apache/logging/log4j/log4j-1.2-api/2.6.2/log4j-1.2-api-2.6.2.jar'
   require 'com/fasterxml/jackson/core/jackson-annotations/2.7.0/jackson-annotations-2.7.0.jar'
   require 'com/fasterxml/jackson/core/jackson-databind/2.7.4/jackson-databind-2.7.4.jar'
 end
@@ -14,7 +13,6 @@
   require_jar( 'org.apache.logging.log4j', 'log4j-core', '2.6.2' )
   require_jar( 'org.apache.logging.log4j', 'log4j-api', '2.6.2' )
   require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.4' )
-  require_jar( 'org.apache.logging.log4j', 'log4j-1.2-api', '2.6.2' )
   require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
   require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.4' )
 end
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index ebdefefffb2..86406007cba 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -52,7 +52,8 @@ def initialize(settings = LogStash::SETTINGS)
     # Create the collectors and configured it with the library
     configure_metrics_collectors
 
-    @reload_metric = metric.namespace([:stats, :pipelines])
+    @pipeline_reload_metric = metric.namespace([:stats, :pipelines])
+    @instance_reload_metric = metric.namespace([:stats, :reloads])
 
     @dispatcher = LogStash::EventDispatcher.new(self)
     LogStash::PLUGIN_REGISTRY.hooks.register_emitter(self.class, dispatcher)
@@ -87,13 +88,13 @@ def execute
   # @param pipeline_id [String] pipeline string identifier
   # @param settings [Hash] settings that will be passed when creating the pipeline.
   #   keys should be symbols such as :pipeline_workers and :pipeline_batch_delay
-  def register_pipeline(pipeline_id, settings = @settings)
+  def register_pipeline(settings)
     pipeline_settings = settings.clone
-    pipeline_settings.set("pipeline.id", pipeline_id)
+    pipeline_id = pipeline_settings.get("pipeline.id")
 
     pipeline = create_pipeline(pipeline_settings)
     return unless pipeline.is_a?(LogStash::Pipeline)
-    if @auto_reload && pipeline.non_reloadable_plugins.any?
+    if @auto_reload && !pipeline.reloadable?
       @logger.error(I18n.t("logstash.agent.non_reloadable_config_register"),
                     :pipeline_id => pipeline_id,
                     :plugins => pipeline.non_reloadable_plugins.map(&:class))
@@ -109,7 +110,8 @@ def reload_state!
         begin
           reload_pipeline!(pipeline_id)
         rescue => e
-          @reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
+          @instance_reload_metric.increment(:failures)
+          @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
             n.increment(:failures)
             n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
             n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
@@ -128,7 +130,6 @@ def uptime
   end
 
   def stop_collecting_metrics
-    @collector.stop
     @periodic_pollers.stop
   end
 
@@ -176,6 +177,12 @@ def id_path
     @id_path ||= ::File.join(settings.get("path.data"), "uuid")
   end
 
+  def running_pipelines
+    @upgrade_mutex.synchronize do
+      @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }
+    end
+  end
+
   def running_pipelines?
     @upgrade_mutex.synchronize do
       @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }.any?
@@ -200,14 +207,13 @@ def configure_metrics_collectors
     @collector = LogStash::Instrument::Collector.new
 
     @metric = if collect_metrics?
-                @logger.debug("Agent: Configuring metric collection")
-                LogStash::Instrument::Metric.new(@collector)
-              else
-                LogStash::Instrument::NullMetric.new(@collector)
-              end
-
+      @logger.debug("Agent: Configuring metric collection")
+      LogStash::Instrument::Metric.new(@collector)
+    else
+      LogStash::Instrument::NullMetric.new(@collector)
+    end
 
-    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(@metric)
+    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(@metric, settings.get("queue.type"), self)
     @periodic_pollers.start
   end
 
@@ -223,30 +229,40 @@ def collect_metrics?
     @collect_metric
   end
 
-  def create_pipeline(settings, config=nil)
+  def increment_reload_failures_metrics(id, config, exception)
+    @instance_reload_metric.increment(:failures)
+    @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+      n.increment(:failures)
+      n.gauge(:last_error, { :message => exception.message, :backtrace => exception.backtrace})
+      n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+    end
+    if @logger.debug?
+      @logger.error("Cannot load an invalid configuration.", :reason => exception.message, :backtrace => exception.backtrace)
+    else
+      @logger.error("Cannot load an invalid configuration.", :reason => exception.message)
+    end
+  end
+
+  # create a new pipeline with the given settings and config, if the pipeline initialization failed
+  # increment the failures metrics
+  # @param settings [Settings] the setting for the new pipelines
+  # @param config [String] the configuration string or nil to fetch the configuration per settings
+  # @return [Pipeline] the new pipeline or nil if it failed
+  def create_pipeline(settings, config = nil)
     if config.nil?
       begin
         config = fetch_config(settings)
       rescue => e
         @logger.error("failed to fetch pipeline configuration", :message => e.message)
-        return
+        return nil
       end
     end
 
     begin
       LogStash::Pipeline.new(config, settings, metric)
     rescue => e
-      @reload_metric.namespace([settings.get("pipeline.id").to_sym, :reloads]).tap do |n|
-        n.increment(:failures)
-        n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
-        n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
-      end
-      if @logger.debug?
-        @logger.error("fetched an invalid config", :config => config, :reason => e.message, :backtrace => e.backtrace)
-      else
-        @logger.error("fetched an invalid config", :config => config, :reason => e.message)
-      end
-      return
+      increment_reload_failures_metrics(settings.get("pipeline.id"), config, e)
+      return nil
     end
   end
 
@@ -254,30 +270,89 @@ def fetch_config(settings)
     @config_loader.format_config(settings.get("path.config"), settings.get("config.string"))
   end
 
-  # since this method modifies the @pipelines hash it is
-  # wrapped in @upgrade_mutex in the parent call `reload_state!`
+  # reload_pipeline trys to reloads the pipeline with id using a potential new configuration if it changed
+  # since this method modifies the @pipelines hash it is wrapped in @upgrade_mutex in the parent call `reload_state!`
+  # @param id [String] the pipeline id to reload
   def reload_pipeline!(id)
     old_pipeline = @pipelines[id]
     new_config = fetch_config(old_pipeline.settings)
+
     if old_pipeline.config_str == new_config
-      @logger.debug("no configuration change for pipeline",
-                    :pipeline => id, :config => new_config)
+      @logger.debug("no configuration change for pipeline", :pipeline => id, :config => new_config)
       return
     end
 
-    new_pipeline = create_pipeline(old_pipeline.settings, new_config)
+    # check if this pipeline is not reloadable. it should not happen as per the check below
+    # but keep it here as a safety net if a reloadable pipeline was releoaded with a non reloadable pipeline
+    if !old_pipeline.reloadable?
+      @logger.error("pipeline is not reloadable", :pipeline => id)
+      return
+    end
 
-    return if new_pipeline.nil?
+    # BasePipeline#initialize will compile the config, and load all plugins and raise an exception
+    # on an invalid configuration
+    begin
+      pipeline_validator = LogStash::BasePipeline.new(new_config, old_pipeline.settings)
+    rescue => e
+      increment_reload_failures_metrics(id, new_config, e)
+      return
+    end
 
-    if new_pipeline.non_reloadable_plugins.any?
-      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"),
-                    :pipeline_id => id,
-                    :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
+    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort
+    if !pipeline_validator.reloadable?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => id, :plugins => pipeline_validator.non_reloadable_plugins.map(&:class))
+      # TODO: in the original code the failure metrics were not incremented, should we do it here?
+      # increment_reload_failures_metrics(id, new_config, e)
+      return
+    end
+
+    # we know configis valid so we are fairly comfortable to first stop old pipeline and then start new one
+    upgrade_pipeline(id, old_pipeline.settings, new_config)
+  end
+
+  # upgrade_pipeline first stops the old pipeline and starts the new one
+  # this method exists only for specs to be able to expects this to be executed
+  # @params pipeline_id [String] the pipeline id to upgrade
+  # @params settings [Settings] the settings for the new pipeline
+  # @params new_config [String] the new pipeline config
+  def upgrade_pipeline(pipeline_id, settings, new_config)
+    @logger.warn("fetched new config for pipeline. upgrading..", :pipeline => pipeline_id, :config => new_config)
+
+    # first step: stop the old pipeline.
+    # IMPORTANT: a new pipeline with same settings should not be instantiated before the previous one is shutdown
+
+    stop_pipeline(pipeline_id)
+    reset_pipeline_metrics(pipeline_id)
+
+    # second step create and start a new pipeline now that the old one is shutdown
+
+    new_pipeline = create_pipeline(settings, new_config)
+    if new_pipeline.nil?
+      # this is a scenario where the configuration is valid (compilable) but the new pipeline refused to start
+      # and at this point NO pipeline is running
+      @logger.error("failed to create the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
+      return
+    end
+
+    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort. this should normally not
+    # happen since the check should be done in reload_pipeline! prior to get here.
+    if !new_pipeline.reloadable?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => pipeline_id, :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
+      return
+    end
+
+    @pipelines[pipeline_id] = new_pipeline
+
+    if !start_pipeline(pipeline_id)
+      @logger.error("failed to start the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
       return
-    else
-      @logger.warn("fetched new config for pipeline. upgrading..",
-                   :pipeline => id, :config => new_pipeline.config_str)
-      upgrade_pipeline(id, new_pipeline)
+    end
+
+    # pipeline started successfuly, update reload success metrics
+    @instance_reload_metric.increment(:successes)
+    @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
+      n.increment(:successes)
+      n.gauge(:last_success_timestamp, LogStash::Timestamp.now)
     end
   end
 
@@ -291,7 +366,8 @@ def start_pipeline(id)
       begin
         pipeline.run
       rescue => e
-        @reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+        @instance_reload_metric.increment(:failures)
+        @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
           n.increment(:failures)
           n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
           n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
@@ -302,7 +378,7 @@ def start_pipeline(id)
     while true do
       if !t.alive?
         return false
-      elsif pipeline.ready?
+      elsif pipeline.running?
         return true
       else
         sleep 0.01
@@ -319,8 +395,11 @@ def stop_pipeline(id)
   end
 
   def start_pipelines
-    @pipelines.each do |id, _|
+    @instance_reload_metric.increment(:successes, 0)
+    @instance_reload_metric.increment(:failures, 0)
+    @pipelines.each do |id, pipeline|
       start_pipeline(id)
+      pipeline.collect_stats
       # no reloads yet, initalize all the reload metrics
       init_pipeline_reload_metrics(id)
     end
@@ -335,18 +414,6 @@ def running_pipeline?(pipeline_id)
     thread.is_a?(Thread) && thread.alive?
   end
 
-  def upgrade_pipeline(pipeline_id, new_pipeline)
-    stop_pipeline(pipeline_id)
-    reset_pipeline_metrics(pipeline_id)
-    @pipelines[pipeline_id] = new_pipeline
-    if start_pipeline(pipeline_id) # pipeline started successfuly
-      @reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
-        n.increment(:successes)
-        n.gauge(:last_success_timestamp, LogStash::Timestamp.now)
-      end
-    end
-  end
-
   def clean_state?
     @pipelines.empty?
   end
@@ -356,7 +423,7 @@ def setting(key)
   end
 
   def init_pipeline_reload_metrics(id)
-    @reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+    @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
       n.increment(:successes, 0)
       n.increment(:failures, 0)
       n.gauge(:last_error, nil)
diff --git a/logstash-core/lib/logstash/api/commands/default_metadata.rb b/logstash-core/lib/logstash/api/commands/default_metadata.rb
index 119c0c66727..4436adf1350 100644
--- a/logstash-core/lib/logstash/api/commands/default_metadata.rb
+++ b/logstash-core/lib/logstash/api/commands/default_metadata.rb
@@ -20,7 +20,9 @@ def version
         end
 
         def http_address
-          service.agent.webserver.address
+          @http_address ||= service.get_shallow(:http_address).value
+        rescue ::LogStash::Instrument::MetricStore::MetricNotFound, NoMethodError => e
+          nil
         end
       end
     end
diff --git a/logstash-core/lib/logstash/api/commands/node.rb b/logstash-core/lib/logstash/api/commands/node.rb
index 816d6be8f8b..e52e6c94fb5 100644
--- a/logstash-core/lib/logstash/api/commands/node.rb
+++ b/logstash-core/lib/logstash/api/commands/node.rb
@@ -17,11 +17,12 @@ def all(selected_fields=[])
           payload
         end
 
-        def pipeline
-          extract_metrics(
-            [:stats, :pipelines, :main, :config],
+        def pipeline(pipeline_id = LogStash::SETTINGS.get("pipeline.id").to_sym)
+          stats = extract_metrics(
+            [:stats, :pipelines, pipeline_id, :config],
             :workers, :batch_size, :batch_delay, :config_reload_automatic, :config_reload_interval
           )
+          stats.merge(:id => pipeline_id)
         end
 
         def os
diff --git a/logstash-core/lib/logstash/api/commands/stats.rb b/logstash-core/lib/logstash/api/commands/stats.rb
index f5ef39043bc..c9a59f878c2 100644
--- a/logstash-core/lib/logstash/api/commands/stats.rb
+++ b/logstash-core/lib/logstash/api/commands/stats.rb
@@ -3,6 +3,9 @@
 require 'logstash/util/thread_dump'
 require_relative "hot_threads_reporter"
 
+java_import java.nio.file.Files
+java_import java.nio.file.Paths
+
 module LogStash
   module Api
     module Commands
@@ -16,10 +19,14 @@ def jvm
             ),
             :mem => memory,
             :gc => gc,
-            :uptime_in_millis => service.get_shallow(:jvm, :uptime_in_millis)
+            :uptime_in_millis => service.get_shallow(:jvm, :uptime_in_millis),
           }
         end
 
+        def reloads
+          service.get_shallow(:stats, :reloads)
+        end
+
         def process
           extract_metrics(
             [:jvm, :process],
@@ -38,9 +45,10 @@ def events
           )
         end
 
-        def pipeline
-          stats = service.get_shallow(:stats, :pipelines)
-          PluginsStats.report(stats)
+        def pipeline(pipeline_id = LogStash::SETTINGS.get("pipeline.id").to_sym)
+          stats = service.get_shallow(:stats, :pipelines, pipeline_id)
+          stats = PluginsStats.report(stats)
+          stats.merge(:id => pipeline_id)
         end
 
         def memory
@@ -61,6 +69,14 @@ def memory
           }
         end
 
+        def os
+          service.get_shallow(:os)
+        rescue
+          # The only currently fetch OS information is about the linux
+          # containers.
+          {}
+        end
+
         def gc
           service.get_shallow(:jvm, :gc)
         end
@@ -83,9 +99,6 @@ def plugin_stats(stats, plugin_type)
           end
 
           def report(stats)
-            # Only one pipeline right now.
-            stats = stats[:main]
-
             {
               :events => stats[:events],
               :plugins => {
@@ -94,6 +107,7 @@ def report(stats)
                 :outputs => plugin_stats(stats, :outputs)
               },
               :reloads => stats[:reloads],
+              :queue => stats[:queue]
             }
           end
         end # module PluginsStats
diff --git a/logstash-core/lib/logstash/api/modules/node_stats.rb b/logstash-core/lib/logstash/api/modules/node_stats.rb
index 239aaa87cd1..f56efe81c59 100644
--- a/logstash-core/lib/logstash/api/modules/node_stats.rb
+++ b/logstash-core/lib/logstash/api/modules/node_stats.rb
@@ -13,11 +13,16 @@ class NodeStats < ::LogStash::Api::Modules::Base
             :jvm => jvm_payload,
             :process => process_payload,
             :pipeline => pipeline_payload,
+            :reloads => reloads,
+            :os => os_payload
           }
           respond_with(payload, {:filter => params["filter"]})
         end
 
         private
+        def os_payload
+          @stats.os
+        end
 
         def events_payload
           @stats.events
@@ -27,6 +32,10 @@ def jvm_payload
           @stats.jvm
         end
 
+        def reloads
+          @stats.reloads
+        end
+
         def process_payload
           @stats.process
         end
diff --git a/logstash-core/lib/logstash/api/modules/stats.rb b/logstash-core/lib/logstash/api/modules/stats.rb
index eee3d0b8b65..a35c9f062b7 100644
--- a/logstash-core/lib/logstash/api/modules/stats.rb
+++ b/logstash-core/lib/logstash/api/modules/stats.rb
@@ -30,8 +30,9 @@ def stats_command
             :jvm => {
               :timestamp => stats_command.started_at,
               :uptime_in_millis => stats_command.uptime,
-              :memory => stats_command.memory
-            }
+              :memory => stats_command.memory,
+            },
+            :os => stats_command.os
           }
           respond_with(payload, {:filter => params["filter"]})
         end
diff --git a/logstash-core/lib/logstash/config/mixin.rb b/logstash-core/lib/logstash/config/mixin.rb
index 09cbd7a019f..bc4114eb6cc 100644
--- a/logstash-core/lib/logstash/config/mixin.rb
+++ b/logstash-core/lib/logstash/config/mixin.rb
@@ -47,6 +47,23 @@ def self.included(base)
     base.extend(LogStash::Config::Mixin::DSL)
   end
 
+  # Recursive method to replace environment variable references in parameters
+  def deep_replace(value)
+    if (value.is_a?(Hash))
+      value.each do |valueHashKey, valueHashValue|
+        value[valueHashKey.to_s] = deep_replace(valueHashValue)
+      end
+    else
+      if (value.is_a?(Array))
+        value.each_index do | valueArrayIndex|
+          value[valueArrayIndex] = deep_replace(value[valueArrayIndex])
+        end
+      else
+        return replace_env_placeholders(value)
+      end
+    end
+  end
+
   def config_init(params)
     # Validation will modify the values inside params if necessary.
     # For example: converting a string to a number, etc.
@@ -105,19 +122,7 @@ def config_init(params)
 
     # Resolve environment variables references
     params.each do |name, value|
-      if (value.is_a?(Hash))
-        value.each do |valueHashKey, valueHashValue|
-          value[valueHashKey.to_s] = replace_env_placeholders(valueHashValue)
-        end
-      else
-        if (value.is_a?(Array))
-          value.each_index do |valueArrayIndex|
-            value[valueArrayIndex] = replace_env_placeholders(value[valueArrayIndex])
-          end
-        else
-          params[name.to_s] = replace_env_placeholders(value)
-        end
-      end
+      params[name.to_s] = deep_replace(value)
     end
 
 
@@ -268,6 +273,7 @@ def validate(params)
       return is_valid
     end # def validate
 
+    # TODO: Remove in 6.0
     def print_version_notice
       return if @@version_notice_given
 
@@ -288,14 +294,10 @@ def print_version_notice
           end
         end
       rescue LogStash::PluginNoVersionError
-        # If we cannot find a version in the currently installed gems we
-        # will display this message. This could happen in the test, if you 
-        # create an anonymous class to test a plugin.
-        self.logger.warn(I18n.t("logstash.plugin.no_version",
-                                :type => @plugin_type,
-                                :name => @config_name,
-                                :LOGSTASH_VERSION => LOGSTASH_VERSION))
-      ensure 
+        # This can happen because of one of the following:
+        # - The plugin is loaded from the plugins.path and contains no gemspec.
+        # - The plugin is defined in a universal plugin, so the loaded plugin doesn't correspond to an actual gemspec.
+      ensure
         @@version_notice_given = true
       end
     end
diff --git a/logstash-core/lib/logstash/filter_delegator.rb b/logstash-core/lib/logstash/filter_delegator.rb
index f54fc0179a9..e5adee779c1 100644
--- a/logstash-core/lib/logstash/filter_delegator.rb
+++ b/logstash-core/lib/logstash/filter_delegator.rb
@@ -9,7 +9,8 @@ class FilterDelegator
       :threadsafe?,
       :do_close,
       :do_stop,
-      :periodic_flush
+      :periodic_flush,
+      :reloadable?
     ]
     def_delegators :@filter, *DELEGATED_METHODS
 
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
index c6946781fb5..25ee3b7e746 100644
--- a/logstash-core/lib/logstash/instrument/collector.rb
+++ b/logstash-core/lib/logstash/instrument/collector.rb
@@ -11,12 +11,8 @@ module LogStash module Instrument
   # The Collector is the single point of reference for all
   # the metrics collection inside logstash, the metrics library will make
   # direct calls to this class.
-  #
-  # This class is an observable responsable of periodically emitting view of the system
-  # to other components like the internal metrics pipelines.
   class Collector
     include LogStash::Util::Loggable
-    include Observable
 
     SNAPSHOT_ROTATION_TIME_SECS = 1 # seconds
     SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS = 10 * 60 # seconds
@@ -26,7 +22,6 @@ class Collector
     def initialize
       @metric_store = MetricStore.new
       @agent = nil
-      start_periodic_snapshotting
     end
 
     # The metric library will call this unique interface
@@ -43,8 +38,6 @@ def push(namespaces_path, key, type, *metric_type_params)
         end
 
         metric.execute(*metric_type_params)
-
-        changed # we had changes coming in so we can notify the observers
       rescue MetricStore::NamespacesExpectedError => e
         logger.error("Collector: Cannot record metric", :exception => e)
       rescue NameError => e
@@ -58,51 +51,13 @@ def push(namespaces_path, key, type, *metric_type_params)
       end
     end
 
-    # Monitor the `Concurrent::TimerTask` this update is triggered on every successful or not
-    # run of the task, TimerTask implement Observable and the collector acts as
-    # the observer and will keep track if something went wrong in the execution.
-    #
-    # @param [Time] Time of execution
-    # @param [result] Result of the execution
-    # @param [Exception] Exception
-    def update(time_of_execution, result, exception)
-      return true if exception.nil?
-      logger.error("Collector: Something went wrong went sending data to the observers",
-                   :execution_time => time_of_execution,
-                   :result => result,
-                   :exception => exception.class.name)
-    end
-
     # Snapshot the current Metric Store and return it immediately,
     # This is useful if you want to get access to the current metric store without
     # waiting for a periodic call.
     #
     # @return [LogStash::Instrument::MetricStore]
     def snapshot_metric
-      Snapshot.new(@metric_store)
-    end
-
-    # Configure and start the periodic task for snapshotting the `MetricStore`
-    def start_periodic_snapshotting
-      @snapshot_task = Concurrent::TimerTask.new { publish_snapshot }
-      @snapshot_task.execution_interval = SNAPSHOT_ROTATION_TIME_SECS
-      @snapshot_task.timeout_interval = SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS
-      @snapshot_task.add_observer(self)
-      @snapshot_task.execute
-    end
-
-    def stop
-      @snapshot_task.shutdown
-    end
-
-    # Create a snapshot of the MetricStore and send it to to the registered observers
-    # The observer will receive the following signature in the update methode.
-    #
-    # `#update(created_at, metric_store)`
-    def publish_snapshot
-      created_at = Time.now
-      logger.debug("Collector: Sending snapshot to observers", :created_at => created_at) if logger.debug?
-      notify_observers(snapshot_metric)
+      Snapshot.new(@metric_store.dup)
     end
 
     def clear(keypath)
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/base.rb b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
index 916f0c88df0..b66c50b58e3 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
@@ -12,6 +12,8 @@ class Base
       :polling_timeout => 120
     }
 
+    attr_reader :metric
+
     public
     def initialize(metric, options = {})
       @metric = metric
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb b/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb
new file mode 100644
index 00000000000..23c7a3ce44b
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/cgroup.rb
@@ -0,0 +1,137 @@
+# encoding: utf-8
+require "pathname"
+require "logstash/util/loggable"
+
+# Logic from elasticsearch/core/src/main/java/org/elasticsearch/monitor/os/OsProbe.java
+# Move to ruby to remove any existing dependency
+module LogStash module Instrument module PeriodicPoller
+  class Cgroup
+    include LogStash::Util::Loggable
+
+    CONTROL_GROUP_RE = Regexp.compile("\\d+:([^:,]+(?:,[^:,]+)?):(/.*)");
+    CONTROLLER_SEPERATOR_RE = ","
+
+    PROC_SELF_CGROUP_FILE = Pathname.new("/proc/self/cgroup")
+    PROC_CGROUP_CPU_DIR = Pathname.new("/sys/fs/cgroup/cpu")
+    PROC_CGROUP_CPUACCT_DIR = Pathname.new("/sys/fs/cgroup/cpuacct")
+
+    GROUP_CPUACCT = "cpuacct"
+    CPUACCT_USAGE_FILE = "cpuacct.usage"
+
+    GROUP_CPU = "cpu"
+    CPU_FS_PERIOD_US_FILE = "cpu.cfs_period_us"
+    CPU_FS_QUOTA_US_FILE = "cpu.cfs_quota_us"
+
+    CPU_STATS_FILE = "cpu.stat"
+
+    class << self
+      def are_cgroup_available?
+        [::File.exist?(PROC_SELF_CGROUP_FILE),
+         Dir.exist?(PROC_CGROUP_CPU_DIR),
+         Dir.exist?(PROC_CGROUP_CPUACCT_DIR)].all?
+      end
+
+      def control_groups
+        response = {}
+
+        read_proc_self_cgroup_lines.each do |line|
+          matches = CONTROL_GROUP_RE.match(line)
+          # multiples controlles, same hierachy
+          controllers = matches[1].split(CONTROLLER_SEPERATOR_RE)
+          controllers.each_with_object(response) { |controller| response[controller] = matches[2] }
+        end
+
+        response
+      end
+
+      def read_first_line(path)
+        IO.readlines(path).first
+      end
+
+      def cgroup_cpuacct_usage_nanos(control_group)
+        read_first_line(::File.join(PROC_CGROUP_CPUACCT_DIR, control_group, CPUACCT_USAGE_FILE)).to_i
+      end
+
+      def cgroup_cpu_fs_period_micros(control_group)
+        read_first_line(::File.join(PROC_CGROUP_CPUACCT_DIR, control_group, CPU_FS_PERIOD_US_FILE)).to_i
+      end
+
+      def cgroup_cpu_fs_quota_micros(control_group)
+        read_first_line(::File.join(PROC_CGROUP_CPUACCT_DIR, control_group,  CPU_FS_QUOTA_US_FILE)).to_i
+      end
+
+      def read_proc_self_cgroup_lines
+        IO.readlines(PROC_SELF_CGROUP_FILE)
+      end
+
+      class CpuStats
+        attr_reader :number_of_elapsed_periods, :number_of_times_throttled, :time_throttled_nanos
+
+        def initialize(number_of_elapsed_periods, number_of_times_throttled, time_throttled_nanos)
+          @number_of_elapsed_periods = number_of_elapsed_periods
+          @number_of_times_throttled = number_of_times_throttled
+          @time_throttled_nanos = time_throttled_nanos
+        end
+      end
+
+      def read_sys_fs_cgroup_cpuacct_cpu_stat(control_group)
+        IO.readlines(::File.join(PROC_CGROUP_CPU_DIR, control_group, CPU_STATS_FILE))
+      end
+
+      def cgroup_cpuacct_cpu_stat(control_group)
+        lines = read_sys_fs_cgroup_cpuacct_cpu_stat(control_group);
+
+        number_of_elapsed_periods = -1;
+        number_of_times_throttled = -1;
+        time_throttled_nanos = -1;
+
+        lines.each do |line|
+          fields = line.split(/\s+/)
+          case fields.first
+          when "nr_periods" then number_of_elapsed_periods = fields[1].to_i
+          when "nr_throttled" then number_of_times_throttled= fields[1].to_i
+          when "throttled_time" then time_throttled_nanos = fields[1].to_i
+          end
+        end
+
+        CpuStats.new(number_of_elapsed_periods, number_of_times_throttled, time_throttled_nanos)
+      end
+
+      def get_all
+       groups = control_groups
+       return if groups.empty?
+
+       cgroups_stats = {
+         :cpuacct => {},
+         :cpu => {}
+       }
+
+       cpuacct_group = groups[GROUP_CPUACCT]
+       cgroups_stats[:cpuacct][:control_group] = cpuacct_group
+       cgroups_stats[:cpuacct][:usage_nanos] = cgroup_cpuacct_usage_nanos(cpuacct_group)
+
+       cpu_group = groups[GROUP_CPU]
+       cgroups_stats[:cpu][:control_group] = cpu_group
+       cgroups_stats[:cpu][:cfs_period_micros] = cgroup_cpu_fs_period_micros(cpu_group)
+       cgroups_stats[:cpu][:cfs_quota_micros] = cgroup_cpu_fs_quota_micros(cpu_group)
+
+       cpu_stats = cgroup_cpuacct_cpu_stat(cpu_group)
+
+       cgroups_stats[:cpu][:stat] = {
+         :number_of_elapsed_periods => cpu_stats.number_of_elapsed_periods,
+         :number_of_times_throttled => cpu_stats.number_of_times_throttled,
+         :time_throttled_nanos => cpu_stats.time_throttled_nanos
+       }
+
+       cgroups_stats
+      rescue => e
+        logger.debug("Error, cannot retrieve cgroups information", :exception => e.class.name, :message => e.message) if logger.debug?
+        nil
+      end
+
+      def get
+        are_cgroup_available? ? get_all : nil
+      end
+    end
+  end
+end end end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
index c1186cf9e9e..707553aa513 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
@@ -39,7 +39,6 @@ def self.get(gc_name)
 
     def initialize(metric, options = {})
       super(metric, options)
-      @metric = metric
       @load_average = LoadAverage.create
     end
 
@@ -114,7 +113,7 @@ def collect_load_average
 
       metric.gauge([:jvm, :process, :cpu], :load_average, load_average) unless load_average.nil?
     end
-    
+
     def collect_jvm_metrics(data)
       runtime_mx_bean = ManagementFactory.getRuntimeMXBean()
       metric.gauge([:jvm], :uptime_in_millis, runtime_mx_bean.getUptime())
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/os.rb b/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
index 8ad09dfc7d7..7f2a334b5ba 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/os.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require "logstash/instrument/periodic_poller/base"
+require "logstash/instrument/periodic_poller/cgroup"
 
 module LogStash module Instrument module PeriodicPoller
   class Os < Base
@@ -8,6 +9,26 @@ def initialize(metric, options = {})
     end
 
     def collect
+      collect_cgroup
+    end
+
+    def collect_cgroup
+      if stats = Cgroup.get
+        save_metric([:os], :cgroup, stats)
+      end
+    end
+
+    # Recursive function to create the Cgroups values form the created hash
+    def save_metric(namespace, k, v)
+      if v.is_a?(Hash)
+        v.each do |new_key, new_value|
+          n = namespace.dup
+          n << k.to_sym
+          save_metric(n, new_key, new_value)
+        end
+      else
+        metric.gauge(namespace, k.to_sym, v)
+      end
     end
   end
 end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/pq.rb b/logstash-core/lib/logstash/instrument/periodic_poller/pq.rb
new file mode 100644
index 00000000000..d0028031f1f
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/pq.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/base"
+
+module LogStash module Instrument module PeriodicPoller
+  class PersistentQueue < Base
+    def initialize(metric, queue_type, agent, options = {})
+      super(metric, options)
+      @metric = metric
+      @queue_type = queue_type
+      @agent = agent
+    end
+
+    def collect
+      pipeline_id, pipeline = @agent.running_pipelines.first
+      unless pipeline.nil?
+        pipeline.collect_stats
+      end
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_pollers.rb b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
index 09c4feebd57..0ce6d406448 100644
--- a/logstash-core/lib/logstash/instrument/periodic_pollers.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_pollers.rb
@@ -1,6 +1,7 @@
 # encoding: utf-8
 require "logstash/instrument/periodic_poller/os"
 require "logstash/instrument/periodic_poller/jvm"
+require "logstash/instrument/periodic_poller/pq"
 
 module LogStash module Instrument
   # Each PeriodPoller manager his own thread to do the poller
@@ -9,10 +10,11 @@ module LogStash module Instrument
   class PeriodicPollers
     attr_reader :metric
 
-    def initialize(metric)
+    def initialize(metric, queue_type, pipelines)
       @metric = metric
       @periodic_pollers = [PeriodicPoller::Os.new(metric),
-                          PeriodicPoller::JVM.new(metric)]
+                           PeriodicPoller::JVM.new(metric),
+                           PeriodicPoller::PersistentQueue.new(metric, queue_type, pipelines)]
     end
 
     def start
diff --git a/logstash-core/lib/logstash/logging/logger.rb b/logstash-core/lib/logstash/logging/logger.rb
index 74b51cfeec9..378adaa0c82 100644
--- a/logstash-core/lib/logstash/logging/logger.rb
+++ b/logstash-core/lib/logstash/logging/logger.rb
@@ -106,7 +106,7 @@ def as_data(plugin_params, event, took_in_nanos)
         {
           :plugin_params => plugin_params,
           :took_in_nanos => took_in_nanos,
-          :took_in_millis => took_in_nanos / 1000,
+          :took_in_millis => took_in_nanos / 1000000,
           :event => event.to_json
         }
       end
diff --git a/logstash-core/lib/logstash/output_delegator.rb b/logstash-core/lib/logstash/output_delegator.rb
index 39a7fdb1f0a..23166f0cf0a 100644
--- a/logstash-core/lib/logstash/output_delegator.rb
+++ b/logstash-core/lib/logstash/output_delegator.rb
@@ -29,6 +29,10 @@ def config_name
     @output_class.config_name
   end
 
+  def reloadable?
+    @output_class.reloadable?
+  end
+
   def concurrency
     @output_class.concurrency
   end
@@ -39,7 +43,9 @@ def register
 
   def multi_receive(events)
     @metric_events.increment(:in, events.length)
+    clock = @metric_events.time(:duration_in_millis)
     @strategy.multi_receive(events)
+    clock.stop
     @metric_events.increment(:out, events.length)
   end
 
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 6b244c80312..e68a0c7ac8d 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -5,8 +5,6 @@
 require "logstash/namespace"
 require "logstash/errors"
 require "logstash-core/logstash-core"
-require "logstash/util/wrapped_acked_queue"
-require "logstash/util/wrapped_synchronous_queue"
 require "logstash/event"
 require "logstash/config/file"
 require "logstash/filters/base"
@@ -21,43 +19,22 @@
 require "logstash/instrument/collector"
 require "logstash/output_delegator"
 require "logstash/filter_delegator"
+require "logstash/queue_factory"
 
-module LogStash; class Pipeline
+module LogStash; class BasePipeline
   include LogStash::Util::Loggable
 
-  attr_reader :inputs,
-    :filters,
-    :outputs,
-    :worker_threads,
-    :events_consumed,
-    :events_filtered,
-    :reporter,
-    :pipeline_id,
-    :started_at,
-    :thread,
-    :config_str,
-    :config_hash,
-    :settings,
-    :metric,
-    :filter_queue_client,
-    :input_queue_client
-
-  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
+  attr_reader :config_str, :config_hash, :inputs, :filters, :outputs, :pipeline_id
 
-  RELOAD_INCOMPATIBLE_PLUGINS = [
-    "LogStash::Inputs::Stdin"
-  ]
-
-  def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
+  def initialize(config_str, settings)
     @logger = self.logger
     @config_str = config_str
     @config_hash = Digest::SHA1.hexdigest(@config_str)
     # Every time #plugin is invoked this is incremented to give each plugin
     # a unique id when auto-generating plugin ids
     @plugin_counter ||= 0
-    @settings = settings
-    @pipeline_id = @settings.get_value("pipeline.id") || self.object_id
-    @reporter = PipelineReporter.new(@logger, self)
+
+    @pipeline_id = settings.get_value("pipeline.id") || self.object_id
 
     # A list of plugins indexed by id
     @plugins_by_id = {}
@@ -65,36 +42,104 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     @filters = nil
     @outputs = nil
 
-    @worker_threads = []
-
-    # This needs to be configured before we evaluate the code to make
-    # sure the metric instance is correctly send to the plugins to make the namespace scoping work
-    @metric = namespaced_metric.nil? ? Instrument::NullMetric.new : namespaced_metric
-
     grammar = LogStashConfigParser.new
-    @config = grammar.parse(config_str)
-    if @config.nil?
-      raise ConfigurationError, grammar.failure_reason
-    end
-    # This will compile the config to ruby and evaluate the resulting code.
-    # The code will initialize all the plugins and define the
-    # filter and output methods.
-    code = @config.compile
-    @code = code
+    parsed_config = grammar.parse(config_str)
+    raise(ConfigurationError, grammar.failure_reason) if parsed_config.nil?
+
+    config_code = parsed_config.compile
 
-    # The config code is hard to represent as a log message...
-    # So just print it.
+    # config_code = BasePipeline.compileConfig(config_str)
 
-    if @settings.get_value("config.debug") && @logger.debug?
-      @logger.debug("Compiled pipeline code", :code => code)
+    if settings.get_value("config.debug") && @logger.debug?
+      @logger.debug("Compiled pipeline code", :code => config_code)
     end
 
+    # Evaluate the config compiled code that will initialize all the plugins and define the
+    # filter and output methods.
     begin
-      eval(code)
+      eval(config_code)
     rescue => e
+      # TODO: the original code rescue e but does nothing with it, should we re-raise to have original exception details!?
       raise
     end
-    @queue = build_queue_from_settings
+  end
+
+  def plugin(plugin_type, name, *args)
+    @plugin_counter += 1
+
+    # Collapse the array of arguments into a single merged hash
+    args = args.reduce({}, &:merge)
+
+    id = if args["id"].nil? || args["id"].empty?
+      args["id"] = "#{@config_hash}-#{@plugin_counter}"
+    else
+      args["id"]
+    end
+
+    raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
+    @plugins_by_id[id] = true
+
+    # use NullMetric if called in the BasePipeline context otherwise use the @metric value
+    metric = @metric || Instrument::NullMetric.new
+
+    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
+    # Scope plugins of type 'input' to 'inputs'
+    type_scoped_metric = pipeline_scoped_metric.namespace("#{plugin_type}s".to_sym)
+
+    klass = Plugin.lookup(plugin_type, name)
+
+    if plugin_type == "output"
+      OutputDelegator.new(@logger, klass, type_scoped_metric,  OutputDelegatorStrategyRegistry.instance, args)
+    elsif plugin_type == "filter"
+      FilterDelegator.new(@logger, klass, type_scoped_metric, args)
+    else # input
+      input_plugin = klass.new(args)
+      input_plugin.metric = type_scoped_metric.namespace(id)
+      input_plugin
+    end
+  end
+
+  def reloadable?
+    non_reloadable_plugins.empty?
+  end
+
+  def non_reloadable_plugins
+    (inputs + filters + outputs).select { |plugin| !plugin.reloadable? }
+  end
+end; end
+
+module LogStash; class Pipeline < BasePipeline
+  attr_reader \
+    :worker_threads,
+    :events_consumed,
+    :events_filtered,
+    :reporter,
+    :started_at,
+    :thread,
+    :settings,
+    :metric,
+    :filter_queue_client,
+    :input_queue_client,
+    :queue
+
+  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
+
+  def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
+    # This needs to be configured before we call super which will evaluate the code to make
+    # sure the metric instance is correctly send to the plugins to make the namespace scoping work
+    @metric = if namespaced_metric
+      settings.get("metric.collect") ? namespaced_metric : Instrument::NullMetric.new(namespaced_metric.collector)
+    else
+      Instrument::NullMetric.new
+    end
+
+    @settings = settings
+    @reporter = PipelineReporter.new(@logger, self)
+    @worker_threads = []
+
+    super(config_str, settings)
+
+    @queue = LogStash::QueueFactory.create(settings)
     @input_queue_client = @queue.write_client
     @filter_queue_client = @queue.read_client
     @signal_queue = Queue.new
@@ -115,32 +160,6 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     @flushing = Concurrent::AtomicReference.new(false)
   end # def initialize
 
-  def build_queue_from_settings
-    queue_type = settings.get("queue.type")
-    queue_page_capacity = settings.get("queue.page_capacity")
-    queue_max_bytes = settings.get("queue.max_bytes")
-    queue_max_events = settings.get("queue.max_events")
-    checkpoint_max_acks = settings.get("queue.checkpoint.acks")
-    checkpoint_max_writes = settings.get("queue.checkpoint.writes")
-    checkpoint_max_interval = settings.get("queue.checkpoint.interval")
-
-    if queue_type == "memory_acked"
-      # memory_acked is used in tests/specs
-      LogStash::Util::WrappedAckedQueue.create_memory_based("", queue_page_capacity, queue_max_events, queue_max_bytes)
-    elsif queue_type == "memory"
-      # memory is the legacy and default setting
-      LogStash::Util::WrappedSynchronousQueue.new()
-    elsif queue_type == "persisted"
-      # persisted is the disk based acked queue
-      queue_path = settings.get("path.queue")
-      LogStash::Util::WrappedAckedQueue.create_file_based(queue_path, queue_page_capacity, queue_max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, queue_max_bytes)
-    else
-      raise(ConfigurationError, "invalid queue.type setting")
-    end
-  end
-
-  private :build_queue_from_settings
-
   def ready?
     @ready.value
   end
@@ -428,41 +447,6 @@ def shutdown_workers
     @outputs.each(&:do_close)
   end
 
-  def plugin(plugin_type, name, *args)
-    @plugin_counter += 1
-
-    # Collapse the array of arguments into a single merged hash
-    args = args.reduce({}, &:merge)
-
-    id = if args["id"].nil? || args["id"].empty?
-           args["id"] = "#{@config_hash}-#{@plugin_counter}"
-         else
-           args["id"]
-         end
-
-    raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
-    
-    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
-
-    klass = Plugin.lookup(plugin_type, name)
-
-    # Scope plugins of type 'input' to 'inputs'
-    type_scoped_metric = pipeline_scoped_metric.namespace("#{plugin_type}s".to_sym)
-    plugin = if plugin_type == "output"
-               OutputDelegator.new(@logger, klass, type_scoped_metric,
-                                   OutputDelegatorStrategyRegistry.instance,
-                                   args)
-             elsif plugin_type == "filter"
-               FilterDelegator.new(@logger, klass, type_scoped_metric, args)
-             else # input
-               input_plugin = klass.new(args)
-               input_plugin.metric = type_scoped_metric.namespace(id)
-               input_plugin
-             end
-    
-    @plugins_by_id[id] = plugin
-  end
-
   # for backward compatibility in devutils for the rspec helpers, this method is not used
   # in the pipeline anymore.
   def filter(event, &block)
@@ -543,9 +527,27 @@ def stalling_threads_info
       .each {|t| t.delete("status") }
   end
 
-  def non_reloadable_plugins
-    (inputs + filters + outputs).select do |plugin|
-      RELOAD_INCOMPATIBLE_PLUGINS.include?(plugin.class.name)
+  def collect_stats
+    pipeline_metric = @metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :queue])
+    pipeline_metric.gauge(:type, settings.get("queue.type"))
+
+    if @queue.is_a?(LogStash::Util::WrappedAckedQueue) && @queue.queue.is_a?(LogStash::AckedQueue)
+      queue = @queue.queue
+      dir_path = queue.dir_path
+      file_store = Files.get_file_store(Paths.get(dir_path))
+
+      pipeline_metric.namespace([:capacity]).tap do |n|
+        n.gauge(:page_capacity_in_bytes, queue.page_capacity)
+        n.gauge(:max_queue_size_in_bytes, queue.max_size_in_bytes)
+        n.gauge(:max_unread_events, queue.max_unread_events)
+      end
+      pipeline_metric.namespace([:data]).tap do |n|
+        n.gauge(:free_space_in_bytes, file_store.get_unallocated_space)
+        n.gauge(:storage_type, file_store.type)
+        n.gauge(:path, dir_path)
+      end
+
+      pipeline_metric.gauge(:events, queue.unread_count)
     end
   end
 
@@ -561,5 +563,4 @@ def inspect
       :flushing => @flushing
     }
   end
-
 end end
diff --git a/logstash-core/lib/logstash/plugin.rb b/logstash-core/lib/logstash/plugin.rb
index 83dd76a2a79..1e8334b00d0 100644
--- a/logstash-core/lib/logstash/plugin.rb
+++ b/logstash-core/lib/logstash/plugin.rb
@@ -22,13 +22,14 @@ class LogStash::Plugin
   # Add a unique `ID` to the plugin instance, this `ID` is used for tracking
   # information for a specific configuration of the plugin.
   #
-  # ```
+  # [source,ruby]
+  # ---------------------------------------------------------------------------------------------------
   # output {
   #  stdout {
   #    id => "ABC"
   #  }
   # }
-  # ```
+  # ---------------------------------------------------------------------------------------------------
   #
   # If you don't explicitely set this variable Logstash will generate a unique name.
   config :id, :validate => :string
@@ -94,6 +95,14 @@ def inspect
     end
   end
 
+  def reloadable?
+    self.class.reloadable?
+  end
+
+  def self.reloadable?
+    true
+  end
+
   def debug_info
     [self.class.to_s, original_params]
   end
diff --git a/logstash-core/lib/logstash/queue_factory.rb b/logstash-core/lib/logstash/queue_factory.rb
new file mode 100644
index 00000000000..a3a28cf0353
--- /dev/null
+++ b/logstash-core/lib/logstash/queue_factory.rb
@@ -0,0 +1,36 @@
+# encoding: utf-8
+require "fileutils"
+require "logstash/event"
+require "logstash/namespace"
+require "logstash/util/wrapped_acked_queue"
+require "logstash/util/wrapped_synchronous_queue"
+
+module LogStash
+  class QueueFactory
+    def self.create(settings)
+      queue_type = settings.get("queue.type")
+      queue_page_capacity = settings.get("queue.page_capacity")
+      queue_max_bytes = settings.get("queue.max_bytes")
+      queue_max_events = settings.get("queue.max_events")
+      checkpoint_max_acks = settings.get("queue.checkpoint.acks")
+      checkpoint_max_writes = settings.get("queue.checkpoint.writes")
+      checkpoint_max_interval = settings.get("queue.checkpoint.interval")
+
+      case queue_type
+      when "memory_acked"
+        # memory_acked is used in tests/specs
+        LogStash::Util::WrappedAckedQueue.create_memory_based("", queue_page_capacity, queue_max_events, queue_max_bytes)
+      when "memory"
+        # memory is the legacy and default setting
+        LogStash::Util::WrappedSynchronousQueue.new
+      when "persisted"
+        # persisted is the disk based acked queue
+        queue_path = ::File.join(settings.get("path.queue"), settings.get("pipeline.id"))
+        FileUtils.mkdir_p(queue_path)
+        LogStash::Util::WrappedAckedQueue.create_file_based(queue_path, queue_page_capacity, queue_max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, queue_max_bytes)
+      else
+        raise ConfigurationError, "Invalid setting `#{queue_type}` for `queue.type`, supported types are: 'memory_acked', 'memory', 'persisted'"
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index eeed1eb6bcd..924d42a8f48 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -170,7 +170,12 @@ def run(args)
     rescue => e
       # abort unless we're just looking for the help
       unless cli_help?(args)
-        $stderr.puts "ERROR: Failed to load settings file from \"path.settings\". Aborting... path.setting=#{LogStash::SETTINGS.get("path.settings")}, exception=#{e.class}, message=>#{e.message}"
+        if e.kind_of?(Psych::Exception)
+          yaml_file_path = ::File.join(LogStash::SETTINGS.get("path.settings"), "logstash.yml")
+          $stderr.puts "ERROR: Failed to parse YAML file \"#{yaml_file_path}\". Please confirm if the YAML structure is valid (e.g. look for incorrect usage of whitespace or indentation). Aborting... parser_error=>#{e.message}"
+        else
+          $stderr.puts "ERROR: Failed to load settings file from \"path.settings\". Aborting... path.setting=#{LogStash::SETTINGS.get("path.settings")}, exception=#{e.class}, message=>#{e.message}"
+        end
         return 1
       end
     end
@@ -256,7 +261,7 @@ def execute
 
     @agent = create_agent(@settings)
 
-    @agent.register_pipeline("main", @settings)
+    @agent.register_pipeline(@settings)
 
     # enable sigint/sigterm before starting the agent
     # to properly handle a stalled agent
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
index 20cf6bff9ed..c1250835d05 100644
--- a/logstash-core/lib/logstash/settings.rb
+++ b/logstash-core/lib/logstash/settings.rb
@@ -489,6 +489,32 @@ def coerce(value)
         Util::TimeValue.from_value(value).to_nanos
       end
     end
+
+    class ArrayCoercible < Coercible
+      def initialize(name, klass, default, strict=true, &validator_proc)
+        @element_class = klass
+        super(name, ::Array, default, strict, &validator_proc)
+      end
+
+      def coerce(value)
+        Array(value)
+      end
+
+      protected
+      def validate(input)
+        if !input.is_a?(@klass)
+          raise ArgumentError.new("Setting \"#{@name}\" must be a #{@klass}. Received: #{input} (#{input.class})")
+        end
+
+        unless input.all? {|el| el.kind_of?(@element_class) }
+          raise ArgumentError.new("Values of setting \"#{@name}\" must be #{@element_class}. Received: #{input.map(&:class)}")
+        end
+
+        if @validator_proc && !@validator_proc.call(input)
+          raise ArgumentError.new("Failed to validate setting \"#{@name}\" with value: #{input}")
+        end
+      end
+    end
   end
 
 
diff --git a/logstash-core/lib/logstash/shutdown_watcher.rb b/logstash-core/lib/logstash/shutdown_watcher.rb
index 6fffc270f27..10de81db1b6 100644
--- a/logstash-core/lib/logstash/shutdown_watcher.rb
+++ b/logstash-core/lib/logstash/shutdown_watcher.rb
@@ -44,7 +44,7 @@ def start
         @reports << pipeline_report_snapshot
         @reports.delete_at(0) if @reports.size > @report_every # expire old report
         if cycle_number == (@report_every - 1) # it's report time!
-          logger.warn(@reports.last)
+          logger.warn(@reports.last.to_s)
 
           if shutdown_stalled?
             logger.error("The shutdown process appears to be stalled due to busy or blocked plugins. Check the logs for more information.") if stalled_count == 0
diff --git a/logstash-core/lib/logstash/util/decorators.rb b/logstash-core/lib/logstash/util/decorators.rb
index 4f5c2910c7e..f5e4ac5dabd 100644
--- a/logstash-core/lib/logstash/util/decorators.rb
+++ b/logstash-core/lib/logstash/util/decorators.rb
@@ -34,8 +34,9 @@ def add_fields(fields,event, pluginname)
 
     # tags is an array of string. sprintf syntax can be used.
     def add_tags(new_tags, event, pluginname)
-      tags = event.get("tags")
-      tags = tags.nil? ? [] : Array(tags)
+      return if new_tags.empty?
+
+      tags = Array(event.get("tags")) # note that Array(nil) => []
 
       new_tags.each do |new_tag|
         new_tag = event.sprintf(new_tag)
diff --git a/logstash-core/lib/logstash/util/safe_uri.rb b/logstash-core/lib/logstash/util/safe_uri.rb
index add29b6ee67..76b50d27e9a 100644
--- a/logstash-core/lib/logstash/util/safe_uri.rb
+++ b/logstash-core/lib/logstash/util/safe_uri.rb
@@ -47,5 +47,10 @@ def sanitized
   def ==(other)
     other.is_a?(::LogStash::Util::SafeURI) ? @uri == other.uri : false
   end
+
+  def clone
+    cloned_uri = uri.clone
+    self.class.new(cloned_uri)
+  end
 end
 
diff --git a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
index ffac9eaab4c..5dd40c7b889 100644
--- a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
@@ -33,6 +33,8 @@ def self.create_file_based(path, capacity, max_events, checkpoint_max_writes, ch
 
     private_class_method :new
 
+    attr_reader :queue
+
     def with_queue(queue)
       @queue = queue
       @queue.open
@@ -130,10 +132,19 @@ def set_batch_dimensions(batch_size, wait_for)
 
       def set_events_metric(metric)
         @event_metric = metric
+        define_initial_metrics_values(@event_metric)
       end
 
       def set_pipeline_metric(metric)
         @pipeline_metric = metric
+        define_initial_metrics_values(@pipeline_metric)
+      end
+
+      def define_initial_metrics_values(namespaced_metric)
+        namespaced_metric.report_time(:duration_in_millis, 0)
+        namespaced_metric.increment(:filtered, 0)
+        namespaced_metric.increment(:in, 0)
+        namespaced_metric.increment(:out, 0)
       end
 
       def inflight_batches
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
index 98503d960f1..e36d88eb45f 100644
--- a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -79,10 +79,19 @@ def set_batch_dimensions(batch_size, wait_for)
 
       def set_events_metric(metric)
         @event_metric = metric
+        define_initial_metrics_values(@event_metric)
       end
 
       def set_pipeline_metric(metric)
         @pipeline_metric = metric
+        define_initial_metrics_values(@pipeline_metric)
+      end
+
+      def define_initial_metrics_values(namespaced_metric)
+        namespaced_metric.report_time(:duration_in_millis, 0)
+        namespaced_metric.increment(:filtered, 0)
+        namespaced_metric.increment(:in, 0)
+        namespaced_metric.increment(:out, 0)
       end
 
       def inflight_batches
diff --git a/logstash-core/lib/logstash/version.rb b/logstash-core/lib/logstash/version.rb
index 6b0ff092f1b..83a9cb085d6 100644
--- a/logstash-core/lib/logstash/version.rb
+++ b/logstash-core/lib/logstash/version.rb
@@ -11,4 +11,4 @@
 #       eventually this file should be in the root logstash lib fir and dependencies in logstash-core should be
 #       fixed.
 
-LOGSTASH_VERSION = "5.1.2"
+LOGSTASH_VERSION = "6.0.0-alpha1"
diff --git a/logstash-core/lib/logstash/webserver.rb b/logstash-core/lib/logstash/webserver.rb
index b7b99b602a0..0531cb84395 100644
--- a/logstash-core/lib/logstash/webserver.rb
+++ b/logstash-core/lib/logstash/webserver.rb
@@ -87,9 +87,17 @@ def start_webserver(port)
       @server = ::Puma::Server.new(app, events)
       @server.add_tcp_listener(http_host, port)
 
-      logger.info("Successfully started Logstash API endpoint", :port => @port)
+      logger.info("Successfully started Logstash API endpoint", :port => port)
+
+      set_http_address_metric("#{http_host}:#{port}")
 
       @server.run.join
     end
+
+    private
+    def set_http_address_metric(value)
+      return unless @agent.metric
+      @agent.metric.gauge([], :http_address, value)
+    end
   end
 end
diff --git a/logstash-core/locales/en.yml b/logstash-core/locales/en.yml
index d1c31e51006..cda5bdf5f8e 100644
--- a/logstash-core/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -44,9 +44,6 @@ en:
         %{plugin} plugin is using the 'milestone' method to declare the version
         of the plugin this method is deprecated in favor of declaring the
         version inside the gemspec.
-      no_version: >-
-        %{name} plugin doesn't have a version. This plugin isn't well
-         supported by the community and likely has no maintainer.
       version:
         0-9-x:
          Using version 0.9.x %{type} plugin '%{name}'. This plugin should work but
diff --git a/logstash-core/logstash-core.gemspec b/logstash-core/logstash-core.gemspec
index 7c11218fc23..0d4107159d4 100644
--- a/logstash-core/logstash-core.gemspec
+++ b/logstash-core/logstash-core.gemspec
@@ -46,7 +46,7 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "rubyzip", "~> 1.1.7"
   gem.add_runtime_dependency "thread_safe", "~> 0.3.5" #(Apache 2.0 license)
 
-  gem.add_runtime_dependency "jrjackson", "~> 0.4.0" #(Apache 2.0 license)
+  gem.add_runtime_dependency "jrjackson", "~> 0.4.2" #(Apache 2.0 license)
 
   gem.add_runtime_dependency "jar-dependencies"
   # as of Feb 3rd 2016, the ruby-maven gem is resolved to version 3.3.3 and that version
diff --git a/logstash-core/spec/api/lib/api/node_stats_spec.rb b/logstash-core/spec/api/lib/api/node_stats_spec.rb
index 1602599e1ce..bfe695a8d01 100644
--- a/logstash-core/spec/api/lib/api/node_stats_spec.rb
+++ b/logstash-core/spec/api/lib/api/node_stats_spec.rb
@@ -80,7 +80,11 @@
         "filtered" => Numeric,
         "out" => Numeric
      }
-    }
+   },
+   "reloads" => {
+     "successes" => Numeric,
+     "failures" => Numeric
+   }
   }
 
   test_api_and_resources(root_structure)
diff --git a/logstash-core/spec/api/spec_helper.rb b/logstash-core/spec/api/spec_helper.rb
index 31ec6f27113..a984f13cd5f 100644
--- a/logstash-core/spec/api/spec_helper.rb
+++ b/logstash-core/spec/api/spec_helper.rb
@@ -20,7 +20,9 @@ def read_fixture(name)
 module LogStash
   class DummyAgent < Agent
     def start_webserver
-      @webserver = Struct.new(:address).new("#{Socket.gethostname}:#{::LogStash::WebServer::DEFAULT_PORTS.first}")
+      http_address = "#{Socket.gethostname}:#{::LogStash::WebServer::DEFAULT_PORTS.first}"
+      @webserver = Struct.new(:address).new(http_address)
+      self.metric.gauge([], :http_address, http_address)
     end
     def stop_webserver; end
   end
@@ -57,7 +59,7 @@ def initialize
   def start
     # We start a pipeline that will generate a finite number of events
     # before starting the expectations
-    agent.register_pipeline("main", @settings)
+    agent.register_pipeline(@settings)
     @agent_task = Stud::Task.new { agent.execute }
     @agent_task.wait
   end
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index 98eafba9d9a..5d1b840292f 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -9,6 +9,7 @@
 describe LogStash::Agent do
 
   let(:agent_settings) { LogStash::SETTINGS }
+  let(:default_pipeline_id) { LogStash::SETTINGS.get("pipeline.id") }
   let(:agent_args) { {} }
   let(:pipeline_settings) { agent_settings.clone }
   let(:pipeline_args) { {} }
@@ -41,7 +42,6 @@
   end
 
   describe "register_pipeline" do
-    let(:pipeline_id) { "main" }
     let(:config_string) { "input { } filter { } output { }" }
     let(:agent_args) do
       {
@@ -57,7 +57,7 @@
         expect(arg1).to eq(config_string)
         expect(arg2.to_hash).to include(agent_args)
       end
-      subject.register_pipeline(pipeline_id, agent_settings)
+      subject.register_pipeline(agent_settings)
     end
   end
 
@@ -90,10 +90,9 @@
           "path.config" => config_file
         }
       end
-      let(:pipeline_id) { "main" }
 
       before(:each) do
-        subject.register_pipeline(pipeline_id, pipeline_settings)
+        subject.register_pipeline(pipeline_settings)
       end
 
       context "if state is clean" do
@@ -155,7 +154,7 @@
 
           it "does not try to reload the pipeline" do
             t = Thread.new { subject.execute }
-            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.running?
             expect(subject).to_not receive(:reload_pipeline!)
             File.open(config_file, "w") { |f| f.puts second_pipeline_config }
             subject.reload_state!
@@ -173,7 +172,7 @@
 
           it "tries to reload the pipeline" do
             t = Thread.new { subject.execute }
-            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.running?
             expect(subject).to receive(:reload_pipeline!).once.and_call_original
             File.open(config_file, "w") { |f| f.puts second_pipeline_config }
             subject.reload_state!
@@ -195,17 +194,16 @@
           "path.config" => config_file,
         }
       end
-      let(:pipeline_id) { "main" }
 
       before(:each) do
-        subject.register_pipeline(pipeline_id, pipeline_settings)
+        subject.register_pipeline(pipeline_settings)
       end
 
       context "if state is clean" do
         it "should periodically reload_state" do
           allow(subject).to receive(:clean_state?).and_return(false)
           t = Thread.new { subject.execute }
-          sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+          sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.running?
           expect(subject).to receive(:reload_state!).at_least(2).times
           sleep 0.1
           Stud.stop!(t)
@@ -220,7 +218,7 @@
 
           it "does not upgrade the new config" do
             t = Thread.new { subject.execute }
-            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.running?
             expect(subject).to_not receive(:upgrade_pipeline)
             File.open(config_file, "w") { |f| f.puts second_pipeline_config }
             sleep 0.1
@@ -235,7 +233,7 @@
 
           it "does upgrade the new config" do
             t = Thread.new { subject.execute }
-            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.running?
             expect(subject).to receive(:upgrade_pipeline).once.and_call_original
             File.open(config_file, "w") { |f| f.puts second_pipeline_config }
             sleep 0.1
@@ -249,7 +247,6 @@
   end
 
   describe "#reload_state!" do
-    let(:pipeline_id) { "main" }
     let(:first_pipeline_config) { "input { } filter { } output { }" }
     let(:second_pipeline_config) { "input { generator {} } filter { } output { }" }
     let(:pipeline_args) { {
@@ -259,13 +256,13 @@
     } }
 
     before(:each) do
-      subject.register_pipeline(pipeline_id, pipeline_settings)
+      subject.register_pipeline(pipeline_settings)
     end
 
     context "when fetching a new state" do
       it "upgrades the state" do
         expect(subject).to receive(:fetch_config).and_return(second_pipeline_config)
-        expect(subject).to receive(:upgrade_pipeline).with(pipeline_id, kind_of(LogStash::Pipeline))
+        expect(subject).to receive(:upgrade_pipeline).with(default_pipeline_id, kind_of(LogStash::Settings), second_pipeline_config)
         subject.reload_state!
       end
     end
@@ -285,7 +282,6 @@
       "config.reload.interval" => 0.01,
       "config.string" => pipeline_config
     } }
-    let(:pipeline_id) { "main" }
 
     context "environment variable templating" do
       before :each do
@@ -299,14 +295,13 @@
 
       it "doesn't upgrade the state" do
         allow(subject).to receive(:fetch_config).and_return(pipeline_config)
-        subject.register_pipeline(pipeline_id, pipeline_settings)
-        expect(subject.pipelines[pipeline_id].inputs.first.message).to eq("foo-bar")
+        subject.register_pipeline(pipeline_settings)
+        expect(subject.pipelines[default_pipeline_id].inputs.first.message).to eq("foo-bar")
       end
     end
   end
 
   describe "#upgrade_pipeline" do
-    let(:pipeline_id) { "main" }
     let(:pipeline_config) { "input { } filter { } output { }" }
     let(:pipeline_args) { {
       "config.string" => pipeline_config,
@@ -315,7 +310,7 @@
     let(:new_pipeline_config) { "input { generator {} } output { }" }
 
     before(:each) do
-      subject.register_pipeline(pipeline_id, pipeline_settings)
+      subject.register_pipeline(pipeline_settings)
     end
 
     after(:each) do
@@ -330,14 +325,14 @@
       end
 
       it "leaves the state untouched" do
-        subject.send(:"reload_pipeline!", pipeline_id)
-        expect(subject.pipelines[pipeline_id].config_str).to eq(pipeline_config)
+        subject.send(:"reload_pipeline!", default_pipeline_id)
+        expect(subject.pipelines[default_pipeline_id].config_str).to eq(pipeline_config)
       end
 
       context "and current state is empty" do
         it "should not start a pipeline" do
           expect(subject).to_not receive(:start_pipeline)
-          subject.send(:"reload_pipeline!", pipeline_id)
+          subject.send(:"reload_pipeline!", default_pipeline_id)
         end
       end
     end
@@ -350,13 +345,13 @@
         allow(subject).to receive(:start_pipeline)
       end
       it "updates the state" do
-        subject.send(:"reload_pipeline!", pipeline_id)
-        expect(subject.pipelines[pipeline_id].config_str).to eq(new_config)
+        subject.send(:"reload_pipeline!", default_pipeline_id)
+        expect(subject.pipelines[default_pipeline_id].config_str).to eq(new_config)
       end
       it "starts the pipeline" do
         expect(subject).to receive(:stop_pipeline)
         expect(subject).to receive(:start_pipeline)
-        subject.send(:"reload_pipeline!", pipeline_id)
+        subject.send(:"reload_pipeline!", default_pipeline_id)
       end
     end
   end
@@ -430,7 +425,7 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
       Thread.abort_on_exception = true
 
       @t = Thread.new do
-        subject.register_pipeline("main",  pipeline_settings)
+        subject.register_pipeline(pipeline_settings)
         subject.execute
       end
 
@@ -479,7 +474,9 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
       it "increases the successful reload count" do
         snapshot = subject.metric.collector.snapshot_metric
         value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
+        instance_value = snapshot.metric_store.get_with_path("/stats")[:stats][:reloads][:successes].value
         expect(value).to eq(1)
+        expect(instance_value).to eq(1)
       end
 
       it "does not set the failure reload timestamp" do
diff --git a/logstash-core/spec/logstash/filters/base_spec.rb b/logstash-core/spec/logstash/filters/base_spec.rb
index 246c11a286e..b3e27a333f3 100644
--- a/logstash-core/spec/logstash/filters/base_spec.rb
+++ b/logstash-core/spec/logstash/filters/base_spec.rb
@@ -248,7 +248,21 @@ def filter(event)
     end
   end
 
- describe "remove_field on deep objects" do
+  describe "remove_field on tags" do
+    config <<-CONFIG
+    filter {
+      noop {
+        remove_field => ["tags"]
+      }
+    }
+    CONFIG
+
+    sample("tags" => "foo") do
+      reject { subject }.include?("tags")
+    end
+  end
+
+  describe "remove_field on deep objects" do
     config <<-CONFIG
     filter {
       noop {
diff --git a/logstash-core/spec/logstash/instrument/collector_spec.rb b/logstash-core/spec/logstash/instrument/collector_spec.rb
index 2a9979d0caa..b5c9b3073de 100644
--- a/logstash-core/spec/logstash/instrument/collector_spec.rb
+++ b/logstash-core/spec/logstash/instrument/collector_spec.rb
@@ -45,5 +45,9 @@
     it "return a `LogStash::Instrument::MetricStore`" do
       expect(subject.snapshot_metric).to be_kind_of(LogStash::Instrument::Snapshot)
     end
+
+    it "returns a clone of the metric store" do
+      expect(subject.snapshot_metric).not_to eq(subject.snapshot_metric)
+    end
   end
 end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb
new file mode 100644
index 00000000000..4fa42670994
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/cgroup_spec.rb
@@ -0,0 +1,148 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/cgroup"
+require "spec_helper"
+
+describe LogStash::Instrument::PeriodicPoller::Cgroup do
+  subject { described_class }
+
+  context ".are_cgroup_available?" do
+    context "all the file exist" do
+      before do
+        allow(::File).to receive(:exist?).with(subject::PROC_SELF_CGROUP_FILE).and_return(true)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPU_DIR).and_return(true)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPUACCT_DIR).and_return(true)
+      end
+
+      it "returns true" do
+        expect(subject.are_cgroup_available?).to be_truthy
+      end
+    end
+
+    context "not all the file exist" do
+      before do
+        allow(::File).to receive(:exist?).with(subject::PROC_SELF_CGROUP_FILE).and_return(true)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPU_DIR).and_return(false)
+        allow(::Dir).to receive(:exist?).with(subject::PROC_CGROUP_CPUACCT_DIR).and_return(true)
+      end
+
+      it "returns false" do
+        expect(subject.are_cgroup_available?).to be_falsey
+      end
+    end
+  end
+
+  context ".control_groups" do
+    let(:proc_self_cgroup_content) {
+      %w(14:name=systemd,holaunlimited:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+13:pids:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+12:hugetlb:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+11:net_prio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+10:perf_event:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+9:net_cls:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+8:freezer:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+7:devices:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+6:memory:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+5:blkio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+4:cpuacct:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+3:cpu:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+2:cpuset:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+1:name=openrc:/docker) }
+
+    before do
+      allow(subject).to receive(:read_proc_self_cgroup_lines).and_return(proc_self_cgroup_content)
+    end
+
+    it "returns the control groups" do
+      expect(subject.control_groups).to match({
+        "name=systemd" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "holaunlimited" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "pids" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "hugetlb" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "net_prio" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "perf_event" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "net_cls" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "freezer" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "devices" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "memory" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "blkio" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "cpuacct" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "cpu" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "cpuset" => "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61",
+        "name=openrc" => "/docker"
+      })
+    end
+  end
+
+  context ".get_all" do
+    context "when we can retreive the stats" do
+      let(:cpuacct_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+      let(:cpuacct_usage) { 1982 }
+      let(:cpu_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+      let(:cfs_period_micros) { 500 }
+      let(:cfs_quota_micros) { 98 }
+      let(:cpu_stats_number_of_periods) { 1 }
+      let(:cpu_stats_number_of_time_throttled) { 2 }
+      let(:cpu_stats_time_throttled_nanos) { 3 }
+      let(:proc_self_cgroup_content) {
+        %W(14:name=systemd,holaunlimited:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+13:pids:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+12:hugetlb:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+11:net_prio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+10:perf_event:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+9:net_cls:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+8:freezer:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+7:devices:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+6:memory:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+5:blkio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+4:cpuacct:#{cpuacct_control_group}
+3:cpu:#{cpu_control_group}
+2:cpuset:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+1:name=openrc:/docker) }
+      let(:cpu_stat_file_content) {
+        [
+          "nr_periods #{cpu_stats_number_of_periods}",
+          "nr_throttled #{cpu_stats_number_of_time_throttled}",
+          "throttled_time #{cpu_stats_time_throttled_nanos}"
+        ]
+      }
+
+      before do
+        allow(subject).to receive(:read_proc_self_cgroup_lines).and_return(proc_self_cgroup_content)
+        allow(subject).to receive(:read_sys_fs_cgroup_cpuacct_cpu_stat).and_return(cpu_stat_file_content)
+
+        allow(subject).to receive(:cgroup_cpuacct_usage_nanos).with(cpuacct_control_group).and_return(cpuacct_usage)
+        allow(subject).to receive(:cgroup_cpu_fs_period_micros).with(cpu_control_group).and_return(cfs_period_micros)
+        allow(subject).to receive(:cgroup_cpu_fs_quota_micros).with(cpu_control_group).and_return(cfs_quota_micros)
+      end
+
+      it "returns all the stats" do
+        expect(subject.get_all).to match(
+          :cpuacct => {
+            :control_group => cpuacct_control_group,
+            :usage_nanos => cpuacct_usage,
+          },
+          :cpu => {
+            :control_group => cpu_control_group,
+            :cfs_period_micros => cfs_period_micros,
+            :cfs_quota_micros => cfs_quota_micros,
+            :stat => {
+                :number_of_elapsed_periods => cpu_stats_number_of_periods,
+                :number_of_times_throttled => cpu_stats_number_of_time_throttled,
+                :time_throttled_nanos => cpu_stats_time_throttled_nanos
+            }
+          }
+        )
+      end
+    end
+
+    context "when an exception is raised" do
+      before do
+        allow(subject).to receive(:control_groups).and_raise("Something went wrong")
+      end
+
+      it "returns nil" do
+        expect(subject.get_all).to be_nil
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/os_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/os_spec.rb
new file mode 100644
index 00000000000..a8772aa6106
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/os_spec.rb
@@ -0,0 +1,85 @@
+# encoding: utf-8
+require "logstash/instrument/periodic_poller/os"
+require "logstash/instrument/metric"
+require "logstash/instrument/collector"
+
+describe LogStash::Instrument::PeriodicPoller::Os do
+  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+
+  context "recorded cgroup metrics (mocked cgroup env)" do
+    subject { described_class.new(metric, {})}
+
+    let(:snapshot_store) { metric.collector.snapshot_metric.metric_store }
+    let(:os_metrics) { snapshot_store.get_shallow(:os) }
+
+    let(:cpuacct_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+    let(:cpuacct_usage) { 1982 }
+    let(:cpu_control_group) { "/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61" }
+    let(:cpu_period_micros) { 500 }
+    let(:cpu_quota_micros) { 98 }
+    let(:cpu_stats_number_of_periods) { 1 }
+    let(:cpu_stats_number_of_time_throttled) { 2 }
+    let(:cpu_stats_time_throttled_nanos) { 3 }
+    let(:proc_self_cgroup_content) {
+      %W(14:name=systemd,holaunlimited:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+13:pids:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+12:hugetlb:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+11:net_prio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+10:perf_event:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+9:net_cls:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+8:freezer:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+7:devices:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+6:memory:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+5:blkio:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+4:cpuacct:#{cpuacct_control_group}
+3:cpu:#{cpu_control_group}
+2:cpuset:/docker/a10687343f90e97bbb1f7181bd065a42de96c40c4aa91764a9d526ea30475f61
+1:name=openrc:/docker) }
+    let(:cpu_stat_file_content) {
+      [
+        "nr_periods #{cpu_stats_number_of_periods}",
+        "nr_throttled #{cpu_stats_number_of_time_throttled}",
+        "throttled_time #{cpu_stats_time_throttled_nanos}"
+      ]
+    }
+
+    before do
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:are_cgroup_available?).and_return(true)
+
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:read_proc_self_cgroup_lines).and_return(proc_self_cgroup_content)
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:read_sys_fs_cgroup_cpuacct_cpu_stat).and_return(cpu_stat_file_content)
+
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:cgroup_cpuacct_usage_nanos).with(cpuacct_control_group).and_return(cpuacct_usage)
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:cgroup_cpu_fs_period_micros).with(cpu_control_group).and_return(cpu_period_micros)
+      allow(LogStash::Instrument::PeriodicPoller::Cgroup).to receive(:cgroup_cpu_fs_quota_micros).with(cpu_control_group).and_return(cpu_quota_micros)
+
+      subject.collect
+    end
+
+    def mval(*metric_path)
+      metric_path.reduce(os_metrics) {|acc,k| acc[k]}.value
+    end
+
+    it "should have a value for #{[:cgroup, :cpuacc, :control_group]} that is a String" do
+      expect(mval(:cgroup, :cpuacct, :control_group)).to be_a(String)
+    end
+
+    it "should have a value for #{[:cgroup, :cpu, :control_group]} that is a String" do
+      expect(mval(:cgroup, :cpu, :control_group)).to be_a(String)
+    end
+
+    [
+      [:cgroup, :cpuacct, :usage_nanos],
+      [:cgroup, :cpu, :cfs_period_micros],
+      [:cgroup, :cpu, :cfs_quota_micros],
+      [:cgroup, :cpu, :stat, :number_of_elapsed_periods],
+      [:cgroup, :cpu, :stat, :number_of_times_throttled],
+      [:cgroup, :cpu, :stat, :time_throttled_nanos]
+    ].each do |path|
+      path = Array(path)
+      it "should have a value for #{path} that is Numeric" do
+        expect(mval(*path)).to be_a(Numeric)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/output_delegator_spec.rb b/logstash-core/spec/logstash/output_delegator_spec.rb
index 56dbd5168f7..e86c0556f71 100644
--- a/logstash-core/spec/logstash/output_delegator_spec.rb
+++ b/logstash-core/spec/logstash/output_delegator_spec.rb
@@ -45,16 +45,24 @@
     context "after having received a batch of events" do
       before do
         subject.register
-        subject.multi_receive(events)
       end
 
       it "should pass the events through" do
-        expect(out_inst).to have_received(:multi_receive).with(events)
+        expect(out_inst).to receive(:multi_receive).with(events)
+        subject.multi_receive(events)
       end
 
       it "should increment the number of events received" do
-        expect(subject.metric_events).to have_received(:increment).with(:in, events.length)
-        expect(subject.metric_events).to have_received(:increment).with(:out, events.length)
+        expect(subject.metric_events).to receive(:increment).with(:in, events.length)
+        expect(subject.metric_events).to receive(:increment).with(:out, events.length)
+        subject.multi_receive(events)
+      end
+
+      it "should record the `duration_in_millis`" do
+        clock = spy("clock")
+        expect(subject.metric_events).to receive(:time).with(:duration_in_millis).and_return(clock)
+        expect(clock).to receive(:stop)
+        subject.multi_receive(events)
       end
     end
 
diff --git a/logstash-core/spec/logstash/pipeline_reporter_spec.rb b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
index 40e821d6941..68b48181996 100644
--- a/logstash-core/spec/logstash/pipeline_reporter_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
@@ -2,31 +2,7 @@
 require "spec_helper"
 require "logstash/pipeline"
 require "logstash/pipeline_reporter"
-
-class DummyOutput < LogStash::Outputs::Base
-
-  config_name "dummyoutput"
-  milestone 2
-
-  attr_reader :num_closes, :events
-
-  def initialize(params={})
-    super
-    @num_closes = 0
-    @events = []
-  end
-
-  def register
-  end
-
-  def receive(event)
-    @events << event
-  end
-
-  def close
-    @num_closes += 1
-  end
-end
+require_relative "../support/mocks_classes"
 
 #TODO: Figure out how to add more tests that actually cover inflight events
 #This will require some janky multithreading stuff
@@ -39,7 +15,7 @@ def close
   let(:reporter) { pipeline.reporter }
 
   before do
-    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_call_original
     allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
 
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 919dffaa52d..791ca1f8e87 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -49,7 +49,7 @@ def close
   end
 end
 
-class DummyOutputMore < DummyOutput
+class DummyOutputMore < ::LogStash::Outputs::DummyOutput
   config_name "dummyoutputmore"
 end
 
@@ -158,7 +158,7 @@ class TestPipeline < LogStash::Pipeline
     before(:each) do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummysafefilter").and_return(DummySafeFilter)
     end
@@ -258,7 +258,7 @@ class TestPipeline < LogStash::Pipeline
     before(:each) do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     end
 
 
@@ -313,7 +313,7 @@ class TestPipeline < LogStash::Pipeline
       before(:each) do
         allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
         allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-        allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+        allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       end
 
       let(:config) { "input { dummyinput {} } output { dummyoutput {} }"}
@@ -378,12 +378,12 @@ class TestPipeline < LogStash::Pipeline
     let(:pipeline_settings) { { "pipeline.batch.size" => batch_size, "pipeline.workers" => 1 } }
     let(:pipeline) { LogStash::Pipeline.new(config, pipeline_settings_obj) }
     let(:logger) { pipeline.logger }
-    let(:warning_prefix) { /CAUTION: Recommended inflight events max exceeded!/ }
+    let(:warning_prefix) { Regexp.new("CAUTION: Recommended inflight events max exceeded!") }
 
     before(:each) do
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinput").and_return(DummyInput)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(logger).to receive(:warn)
       thread = Thread.new { pipeline.run }
       pipeline.shutdown
@@ -435,28 +435,87 @@ class TestPipeline < LogStash::Pipeline
   end
 
   context "metrics" do
-    config <<-CONFIG
-    input { }
-    filter { }
-    output { }
-    CONFIG
-
-    it "uses a `NullMetric` object if `metric.collect` is set to false" do
-      settings = double("LogStash::SETTINGS")
-
-      allow(settings).to receive(:get_value).with("pipeline.id").and_return("main")
-      allow(settings).to receive(:get_value).with("metric.collect").and_return(false)
-      allow(settings).to receive(:get_value).with("config.debug").and_return(false)
-      allow(settings).to receive(:get).with("queue.type").and_return("memory")
-      allow(settings).to receive(:get).with("queue.page_capacity").and_return(1024 * 1024)
-      allow(settings).to receive(:get).with("queue.max_events").and_return(250)
-      allow(settings).to receive(:get).with("queue.max_bytes").and_return(1024 * 1024 * 1024)
-      allow(settings).to receive(:get).with("queue.checkpoint.acks").and_return(1024)
-      allow(settings).to receive(:get).with("queue.checkpoint.writes").and_return(1024)
-      allow(settings).to receive(:get).with("queue.checkpoint.interval").and_return(1000)
-
-      pipeline = LogStash::Pipeline.new(config, settings)
-      expect(pipeline.metric).to be_kind_of(LogStash::Instrument::NullMetric)
+    config = "input { } filter { } output { }"
+
+    let(:settings) { LogStash::SETTINGS.clone }
+    subject { LogStash::Pipeline.new(config, settings, metric) }
+
+    context "when metric.collect is disabled" do
+      before :each do
+        settings.set("metric.collect", false)
+      end
+
+      context "if namespaced_metric is nil" do
+        let(:metric) { nil }
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+      end
+
+      context "if namespaced_metric is a Metric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
+
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
+
+      context "if namespaced_metric is a NullMetric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
+
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(::LogStash::Instrument::NullMetric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
+    end
+
+    context "when metric.collect is enabled" do
+      before :each do
+        settings.set("metric.collect", true)
+      end
+
+      context "if namespaced_metric is nil" do
+        let(:metric) { nil }
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+      end
+
+      context "if namespaced_metric is a Metric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::Metric.new(collector) }
+
+        it "uses a `Metric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::Metric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
+
+      context "if namespaced_metric is a NullMetric object" do
+        let(:collector) { ::LogStash::Instrument::Collector.new }
+        let(:metric) { ::LogStash::Instrument::NullMetric.new(collector) }
+
+        it "uses a `NullMetric` object" do
+          expect(subject.metric).to be_a(LogStash::Instrument::NullMetric)
+        end
+
+        it "uses the same collector" do
+          expect(subject.metric.collector).to be(collector)
+        end
+      end
     end
   end
 
@@ -465,7 +524,7 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("input", "dummyinputgenerator").and_return(DummyInputGenerator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputmore").and_return(DummyOutputMore)
     end
 
@@ -501,14 +560,14 @@ class TestPipeline < LogStash::Pipeline
       }
       EOS
     end
-    let(:output) { DummyOutput.new }
+    let(:output) { ::LogStash::Outputs::DummyOutput.new }
 
     before do
-      allow(DummyOutput).to receive(:new).with(any_args).and_return(output)
+      allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(output)
       allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     end
 
     it "flushes the buffered contents of the filter" do
@@ -531,7 +590,7 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     end
 
     let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
@@ -643,22 +702,22 @@ class TestPipeline < LogStash::Pipeline
       }
       EOS
     end
-    let(:dummyoutput) { DummyOutput.new({ "id" => dummy_output_id }) }
+    let(:dummyoutput) { ::LogStash::Outputs::DummyOutput.new({ "id" => dummy_output_id }) }
     let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
 
     before :each do
-      allow(DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
+      allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
       allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
 
       Thread.new { subject.run }
       # make sure we have received all the generated events
       wait(3).for do
         # give us a bit of time to flush the events
-        dummyoutput.events.size < number_of_events
-      end.to be_falsey
+        dummyoutput.events.size >= number_of_events
+      end.to be_truthy
     end
 
     after :each do
@@ -697,12 +756,15 @@ class TestPipeline < LogStash::Pipeline
 
       it "populates the output metrics" do
         plugin_name = dummy_output_id.to_sym
+
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:in].value).to eq(number_of_events)
         expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:out].value).to eq(number_of_events)
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:duration_in_millis].value).not_to be_nil
       end
 
       it "populates the name of the output plugin" do
         plugin_name = dummy_output_id.to_sym
-        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:name].value).to eq(DummyOutput.config_name)
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:name].value).to eq(::LogStash::Outputs::DummyOutput.config_name)
       end
 
       it "populates the name of the filter plugin" do
@@ -719,7 +781,7 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
-      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
     end
 
     let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
diff --git a/logstash-core/spec/logstash/plugin_spec.rb b/logstash-core/spec/logstash/plugin_spec.rb
index 2313ca27d95..fcaf0e6fa76 100644
--- a/logstash-core/spec/logstash/plugin_spec.rb
+++ b/logstash-core/spec/logstash/plugin_spec.rb
@@ -7,6 +7,41 @@
 require "logstash/filters/base"
 
 describe LogStash::Plugin do
+  context "reloadable" do
+    context "by default" do
+      subject do
+        Class.new(LogStash::Plugin) do
+        end
+      end
+
+      it "makes .reloadable? return true" do
+        expect(subject.reloadable?).to be_truthy
+      end
+
+      it "makes #reloadable? return true" do
+        expect(subject.new({}).reloadable?).to be_truthy
+      end
+    end
+
+    context "user can overrides" do
+      subject do
+        Class.new(LogStash::Plugin) do
+          def self.reloadable?
+            false
+          end
+        end
+      end
+
+      it "makes .reloadable? return true" do
+        expect(subject.reloadable?).to be_falsey
+      end
+
+      it "makes #reloadable? return true" do
+        expect(subject.new({}).reloadable?).to be_falsey
+      end
+    end
+  end
+
   it "should fail lookup on inexisting type" do
     #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
     expect { LogStash::Plugin.lookup("badbadtype", "badname") }.to raise_error(LogStash::PluginLoadingError)
@@ -106,12 +141,8 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
       one_notice.validate({})
     end
 
-    it "warns the user if we can't find a defined version" do
-      expect_any_instance_of(LogStash::Logging::Logger).to receive(:warn)
-        .once
-        .with(/plugin doesn't have a version/)
-
-      subject.validate({})
+    it "doesn't raise an exception if no version is found" do
+      expect { subject.validate({}) }.not_to raise_error
     end
 
 
diff --git a/logstash-core/spec/logstash/queue_factory_spec.rb b/logstash-core/spec/logstash/queue_factory_spec.rb
new file mode 100644
index 00000000000..fa0e2160d11
--- /dev/null
+++ b/logstash-core/spec/logstash/queue_factory_spec.rb
@@ -0,0 +1,76 @@
+# encoding: utf-8
+require "logstash/queue_factory"
+require "logstash/settings"
+require "stud/temporary"
+
+describe LogStash::QueueFactory do
+  let(:pipeline_id) { "my_pipeline" }
+  let(:settings_array) do
+    [
+      LogStash::Setting::WritableDirectory.new("path.queue", Stud::Temporary.pathname),
+      LogStash::Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked"]),
+      LogStash::Setting::Bytes.new("queue.page_capacity", "250mb"),
+      LogStash::Setting::Bytes.new("queue.max_bytes", "1024mb"),
+      LogStash::Setting::Numeric.new("queue.max_events", 0),
+      LogStash::Setting::Numeric.new("queue.checkpoint.acks", 1024),
+      LogStash::Setting::Numeric.new("queue.checkpoint.writes", 1024),
+      LogStash::Setting::Numeric.new("queue.checkpoint.interval", 1000),
+      LogStash::Setting::String.new("pipeline.id", pipeline_id)
+    ]
+  end
+
+  let(:settings) do
+    s = LogStash::Settings.new
+
+    settings_array.each do |setting|
+      s.register(setting)
+    end
+    s
+  end
+
+  subject { described_class }
+
+  context "when `queue.type` is `persisted`" do
+    before do
+      settings.set("queue.type", "persisted")
+    end
+
+    it "returns a `WrappedAckedQueue`" do
+      expect(subject.create(settings)).to be_kind_of(LogStash::Util::WrappedAckedQueue)
+    end
+
+    describe "per pipeline id subdirectory creation" do
+      let(:queue_path) { ::File.join(settings.get("path.queue"), pipeline_id) }
+
+      after :each do
+        FileUtils.rmdir(queue_path)
+      end
+
+      it "creates a queue directory based on the pipeline id" do
+        expect(Dir.exist?(queue_path)).to be_falsey
+        queue = subject.create(settings)
+        expect(Dir.exist?(queue_path)).to be_truthy
+      end
+    end
+  end
+
+  context "when `queue.type` is `memory_acked`" do
+    before do
+      settings.set("queue.type", "memory_acked")
+    end
+
+    it "returns a `WrappedAckedQueue`" do
+      expect(subject.create(settings)).to be_kind_of(LogStash::Util::WrappedAckedQueue)
+    end
+  end
+
+  context "when `queue.type` is `memory`" do
+    before do
+      settings.set("queue.type", "memory")
+    end
+
+    it "returns a `WrappedAckedQueue`" do
+      expect(subject.create(settings)).to be_kind_of(LogStash::Util::WrappedSynchronousQueue)
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/array_coercible_spec.rb b/logstash-core/spec/logstash/settings/array_coercible_spec.rb
new file mode 100644
index 00000000000..7146ff0950a
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/array_coercible_spec.rb
@@ -0,0 +1,46 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::ArrayCoercible do
+  subject { described_class.new("option", element_class, value) }
+  let(:value) { [ ] }
+  let(:element_class) { Object }
+
+  context "when given a non array value" do
+    let(:value) { "test" }
+    describe "the value" do
+      it "is converted to an array with that single element" do
+        expect(subject.value).to eq(["test"])
+      end
+    end
+  end
+
+  context "when given an array value" do
+    let(:value) { ["test"] }
+    describe "the value" do
+      it "is not modified" do
+        expect(subject.value).to eq(value)
+      end
+    end
+  end
+
+  describe "initialization" do
+    subject { described_class }
+    let(:element_class) { Fixnum }
+    context "when given values of incorrect element class" do
+      let(:value) { "test" }
+
+      it "will raise an exception" do
+        expect { described_class.new("option", element_class, value) }.to raise_error(ArgumentError)
+      end
+    end
+    context "when given values of correct element class" do
+      let(:value) { 1 }
+
+      it "will not raise an exception" do
+        expect { described_class.new("option", element_class, value) }.not_to raise_error
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/util/safe_uri_spec.rb b/logstash-core/spec/logstash/util/safe_uri_spec.rb
new file mode 100644
index 00000000000..b8e5e546a31
--- /dev/null
+++ b/logstash-core/spec/logstash/util/safe_uri_spec.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require "logstash/util/safe_uri"
+require "spec_helper"
+
+module LogStash module Util
+  describe SafeURI do
+    describe "#clone" do
+      subject { LogStash::Util::SafeURI.new("http://localhost:9200/uri?q=s") }
+      it "allows modifying uri parameters" do
+        cloned_safe_uri = subject.clone
+        cloned_safe_uri.path = "/cloned"
+        cloned_safe_uri.query = "a=b"
+        expect(subject.path).to eq("/uri")
+        expect(subject.query).to eq("q=s")
+        expect(cloned_safe_uri.path).to eq("/cloned")
+        expect(cloned_safe_uri.query).to eq("a=b")
+      end
+    end
+  end
+end end
diff --git a/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
index d7b403ed0b0..b9b65fe8886 100644
--- a/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
+++ b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
@@ -63,7 +63,30 @@ def poll(*) shift(); end
             batch = read_client.take_batch
             read_client.close_batch(batch)
             store = collector.snapshot_metric.metric_store
-            expect(store.size).to eq(0)
+
+            expect(store.get_shallow(:events, :in).value).to eq(0)
+            expect(store.get_shallow(:events, :in)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:events, :out).value).to eq(0)
+            expect(store.get_shallow(:events, :out)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:events, :filtered).value).to eq(0)
+            expect(store.get_shallow(:events, :filtered)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:events, :duration_in_millis).value).to eq(0)
+            expect(store.get_shallow(:events, :duration_in_millis)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:pipeline, :in).value).to eq(0)
+            expect(store.get_shallow(:pipeline, :in)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:pipeline, :duration_in_millis).value).to eq(0)
+            expect(store.get_shallow(:pipeline, :duration_in_millis)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:pipeline, :out).value).to eq(0)
+            expect(store.get_shallow(:pipeline, :out)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
+
+            expect(store.get_shallow(:pipeline, :filtered).value).to eq(0)
+            expect(store.get_shallow(:pipeline, :filtered)).to be_kind_of(LogStash::Instrument::MetricType::Counter)
           end
         end
 
@@ -73,15 +96,22 @@ def poll(*) shift(); end
             5.times {|i| batch.push("value-#{i}")}
             write_client.push_batch(batch)
             read_batch = read_client.take_batch
-            sleep(0.1) # simulate some work?
-            read_client.close_batch(batch)
+            sleep(0.1) # simulate some work for the `duration_in_millis`
+            # TODO: this interaction should be cleaned in an upcoming PR,
+            # This is what the current pipeline does.
+            read_client.add_filtered_metrics(read_batch)
+            read_client.add_output_metrics(read_batch)
+            read_client.close_batch(read_batch)
             store = collector.snapshot_metric.metric_store
 
-            expect(store.size).to eq(4)
             expect(store.get_shallow(:events, :in).value).to eq(5)
+            expect(store.get_shallow(:events, :out).value).to eq(5)
+            expect(store.get_shallow(:events, :filtered).value).to eq(5)
             expect(store.get_shallow(:events, :duration_in_millis).value).to be > 0
             expect(store.get_shallow(:pipeline, :in).value).to eq(5)
             expect(store.get_shallow(:pipeline, :duration_in_millis).value).to be > 0
+            expect(store.get_shallow(:pipeline, :out).value).to eq(5)
+            expect(store.get_shallow(:pipeline, :filtered).value).to eq(5)
           end
         end
       end
diff --git a/logstash-core/spec/support/mocks_classes.rb b/logstash-core/spec/support/mocks_classes.rb
index 5ae99a68f0b..6aa4c70c7ad 100644
--- a/logstash-core/spec/support/mocks_classes.rb
+++ b/logstash-core/spec/support/mocks_classes.rb
@@ -12,7 +12,7 @@ class DummyOutput < LogStash::Outputs::Base
     def initialize(params={})
       super
       @num_closes = 0
-      @events = Queue.new
+      @events = []
     end
 
     def register
@@ -23,7 +23,7 @@ def receive(event)
     end
 
     def close
-      @num_closes = 1
+      @num_closes += 1
     end
   end
 
diff --git a/logstash-core/src/main/java/org/logstash/FileLockFactory.java b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
new file mode 100644
index 00000000000..58f7fd75746
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
@@ -0,0 +1,121 @@
+// this class is largely inspired by Lucene FSLockFactory and friends, below is the Lucene original Apache 2.0 license:
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.logstash;
+
+import java.io.IOException;
+import java.nio.channels.FileChannel;
+import java.nio.channels.FileLock;
+import java.nio.file.FileSystems;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * FileLockFactory provides a way to obtain an exclusive file lock for a given dir path and lock name.
+ * The obtainLock() method will return a Filelock object which should be released using the releaseLock()
+ * method. Normally the returned FileLock object should not be manipulated directly. Only the obtainLock()
+ * and releaseLock() methods should be use to gain and release exclusive access.
+ * This is threadsafe and will work across threads on the same JVM and will also work across processes/JVM.
+ *
+ * TODO: because of the releaseLock() method, strictly speaking this class is not only a factory anymore,
+ * maybe we should rename it FileLockManager?
+ */
+public class FileLockFactory {
+
+    /**
+     * Singleton instance
+     */
+    public static final FileLockFactory INSTANCE = new FileLockFactory();
+
+    private FileLockFactory() {}
+
+    private static final Set<String> LOCK_HELD = Collections.synchronizedSet(new HashSet<>());
+    private static final Map<FileLock, String> LOCK_MAP =  Collections.synchronizedMap(new HashMap<>());
+
+    public static final FileLockFactory getDefault() {
+        return FileLockFactory.INSTANCE;
+    }
+
+    public FileLock obtainLock(String lockDir, String lockName) throws IOException {
+        Path dirPath = FileSystems.getDefault().getPath(lockDir);
+
+        // Ensure that lockDir exists and is a directory.
+        // note: this will fail if lockDir is a symlink
+        Files.createDirectories(dirPath);
+
+        Path lockPath = dirPath.resolve(lockName);
+
+        try {
+            Files.createFile(lockPath);
+        } catch (IOException ignore) {
+            // we must create the file to have a truly canonical path.
+            // if it's already created, we don't care. if it cant be created, it will fail below.
+        }
+
+        // fails if the lock file does not exist
+        final Path realLockPath = lockPath.toRealPath();
+
+        if (LOCK_HELD.add(realLockPath.toString())) {
+            FileChannel channel = null;
+            FileLock lock = null;
+            try {
+                channel = FileChannel.open(realLockPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE);
+                lock = channel.tryLock();
+                if (lock != null) {
+                    LOCK_MAP.put(lock, realLockPath.toString());
+                    return lock;
+                } else {
+                    throw new LockException("Lock held by another program: " + realLockPath);
+                }
+            } finally {
+                if (lock == null) { // not successful - clear up and move out
+                    try {
+                        if (channel != null) {
+                            channel.close();
+                        }
+                    } catch (Throwable t) {
+                        // suppress any channel close exceptions
+                    }
+
+                    boolean removed = LOCK_HELD.remove(realLockPath.toString());
+                    if (removed == false) {
+                        throw new LockException("Lock path was cleared but never marked as held: " + realLockPath);
+                    }
+                }
+            }
+        } else {
+            throw new LockException("Lock held by this virtual machine: " + realLockPath);
+        }
+    }
+
+    public void releaseLock(FileLock lock) throws IOException {
+        String lockPath = LOCK_MAP.remove(lock);
+        if (lockPath == null) { throw new LockException("Cannot release unobtained lock"); }
+        lock.release();
+        Boolean removed = LOCK_HELD.remove(lockPath);
+        if (removed == false) { throw new LockException("Lock path was not marked as held: " + lockPath); }
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/LockException.java b/logstash-core/src/main/java/org/logstash/LockException.java
new file mode 100644
index 00000000000..fad548440d4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/LockException.java
@@ -0,0 +1,13 @@
+package org.logstash;
+
+import java.io.IOException;
+
+public class LockException extends IOException {
+    public LockException(String message) {
+        super(message);
+    }
+
+    public LockException(String message, Throwable cause) {
+        super(message, cause);
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
index f87d57c3e7d..7296b54d10e 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/HeadPage.java
@@ -11,15 +11,15 @@ public class HeadPage extends Page {
     // create a new HeadPage object and new page.{pageNum} empty valid data file
     public HeadPage(int pageNum, Queue queue, PageIO pageIO) throws IOException {
         super(pageNum, queue, 0, 0, 0, new BitSet(), pageIO);
-        pageIO.create();
     }
 
     // create a new HeadPage object from an existing checkpoint and open page.{pageNum} empty valid data file
+    // @param pageIO is expected to be open/recover/create
     public HeadPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
         super(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO);
 
-        // open the data file and reconstruct the IO object internal state
-        pageIO.open(checkpoint.getMinSeqNum(), checkpoint.getElementCount());
+        assert checkpoint.getMinSeqNum() == pageIO.getMinSeqNum() && checkpoint.getElementCount() == pageIO.getElementCount() :
+                String.format("checkpoint minSeqNum=%d or elementCount=%d is different than pageIO minSeqNum=%d or elementCount=%d", checkpoint.getMinSeqNum(), checkpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
 
         // this page ackedSeqNums bitset is a new empty bitset, if we have some acked elements, set them in the bitset
         if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index 6f9b48bcb7c..981e9af00d5 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -1,5 +1,7 @@
 package org.logstash.ackedqueue;
 
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
 import org.logstash.common.io.CheckpointIO;
 import org.logstash.common.io.PageIO;
 import org.logstash.common.io.PageIOFactory;
@@ -60,22 +62,24 @@ public class Queue implements Closeable {
     private final Method deserializeMethod;
 
     // thread safety
-    final Lock lock = new ReentrantLock();
-    final Condition notFull  = lock.newCondition();
-    final Condition notEmpty = lock.newCondition();
+    private final Lock lock = new ReentrantLock();
+    private final Condition notFull  = lock.newCondition();
+    private final Condition notEmpty = lock.newCondition();
+
+    private static final Logger logger = LogManager.getLogger(Queue.class);
 
     public Queue(Settings settings) {
         this(
-                settings.getDirPath(),
-                settings.getCapacity(),
-                settings.getQueueMaxBytes(),
-                settings.getCheckpointIOFactory().build(settings.getDirPath()),
-                settings.getPageIOFactory(),
-                settings.getElementClass(),
-                settings.getMaxUnread(),
-                settings.getCheckpointMaxWrites(),
-                settings.getCheckpointMaxAcks(),
-                settings.getCheckpointMaxInterval()
+            settings.getDirPath(),
+            settings.getCapacity(),
+            settings.getQueueMaxBytes(),
+            settings.getCheckpointIOFactory().build(settings.getDirPath()),
+            settings.getPageIOFactory(),
+            settings.getElementClass(),
+            settings.getMaxUnread(),
+            settings.getCheckpointMaxWrites(),
+            settings.getCheckpointMaxAcks(),
+            settings.getCheckpointMaxInterval()
         );
     }
 
@@ -106,6 +110,30 @@ public Queue(String dirPath, int pageCapacity, long maxBytes, CheckpointIO check
         }
     }
 
+    public String getDirPath() {
+        return this.dirPath;
+    }
+
+    public long getMaxBytes() {
+        return this.maxBytes;
+    }
+
+    public long getMaxUnread() {
+        return this.maxUnread;
+    }
+
+    public long getCurrentByteSize() {
+        return this.currentByteSize;
+    }
+
+    public int getPageCapacity() {
+        return this.pageCapacity;
+    }
+
+    public long getUnreadCount() {
+        return this.unreadCount;
+    }
+
     // moved queue opening logic in open() method until we have something in place to used in-memory checkpoints for testing
     // because for now we need to pass a Queue instance to the Page and we don't want to trigger a Queue recovery when
     // testing Page
@@ -118,11 +146,8 @@ public void open() throws IOException {
         try {
             headCheckpoint = this.checkpointIO.read(checkpointIO.headFileName());
         } catch (NoSuchFileException e) {
-            headCheckpoint = null;
-        }
+            // if there is no head checkpoint, create a new headpage and checkpoint it and exit method
 
-        // if there is no head checkpoint, create a new headpage and checkpoint it and exit method
-        if (headCheckpoint == null) {
             this.seqNum = 0;
             headPageNum = 0;
 
@@ -138,33 +163,45 @@ public void open() throws IOException {
         for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
 
             // all tail checkpoints in the sequence should exist, if not abort mission with a NoSuchFileException
-            Checkpoint tailCheckpoint = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));
+            Checkpoint cp = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));
 
             PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
+            pageIO.open(cp.getMinSeqNum(), cp.getElementCount());
 
-            add(tailCheckpoint, pageIO);
+            add(cp, new TailPage(cp, this, pageIO));
         }
 
         // transform the head page into a tail page only if the headpage is non-empty
         // in both cases it will be checkpointed to track any changes in the firstUnackedPageNum when reconstructing the tail pages
 
-        if (headCheckpoint.getMinSeqNum() <= 0 && headCheckpoint.getElementCount() <= 0) {
+        PageIO pageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
+        pageIO.recover(); // optimistically recovers the head page data file and set minSeqNum and elementCount to the actual read/recovered data
+
+        if (pageIO.getMinSeqNum() != headCheckpoint.getMinSeqNum() || pageIO.getElementCount() != headCheckpoint.getElementCount()) {
+            // the recovered page IO shows different minSeqNum or elementCount than the checkpoint, use the page IO attributes
+
+            logger.warn("recovered head data page {} is different than checkpoint, using recovered page information", headCheckpoint.getPageNum());
+            logger.debug("head checkpoint minSeqNum={} or elementCount={} is different than head pageIO minSeqNum={} or elementCount={}", headCheckpoint.getMinSeqNum(), headCheckpoint.getElementCount(), pageIO.getMinSeqNum(), pageIO.getElementCount());
+
+            long firstUnackedSeqNum = headCheckpoint.getFirstUnackedSeqNum();
+            if (firstUnackedSeqNum < pageIO.getMinSeqNum()) {
+                logger.debug("head checkpoint firstUnackedSeqNum={} is < head pageIO minSeqNum={}, using pageIO minSeqNum", firstUnackedSeqNum, pageIO.getMinSeqNum());
+                firstUnackedSeqNum = pageIO.getMinSeqNum();
+            }
+            headCheckpoint = new Checkpoint(headCheckpoint.getPageNum(), headCheckpoint.getFirstUnackedPageNum(), firstUnackedSeqNum, pageIO.getMinSeqNum(), pageIO.getElementCount());
+        }
+        this.headPage = new HeadPage(headCheckpoint, this, pageIO);
+
+        if (this.headPage.getMinSeqNum() <= 0 && this.headPage.getElementCount() <= 0) {
             // head page is empty, let's keep it as-is
 
-            PageIO headPageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
-            this.headPage = new HeadPage(headCheckpoint, this, headPageIO);
-            this.currentByteSize += headPageIO.getCapacity();
+            this.currentByteSize += pageIO.getCapacity();
 
             // but checkpoint it to update the firstUnackedPageNum if it changed
             this.headPage.checkpoint();
         } else {
             // head page is non-empty, transform it into a tail page and create a new empty head page
-
-            PageIO pageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
-
-            TailPage p = new TailPage(headCheckpoint, this, pageIO);
-            p.checkpoint();
-            add(headCheckpoint, pageIO);
+            add(headCheckpoint, this.headPage.behead());
 
             headPageNum = headCheckpoint.getPageNum() + 1;
             newCheckpointedHeadpage(headPageNum);
@@ -183,10 +220,12 @@ public void open() throws IOException {
         this.closed.set(false);
     }
 
-    private void add(Checkpoint checkpoint, PageIO pageIO) throws IOException {
+    // add a read tail page into this queue structures but also verify that this tail page
+    // is not fully acked in which case it will be purged
+    private void add(Checkpoint checkpoint, TailPage page) throws IOException {
         if (checkpoint.isFullyAcked()) {
             // first make sure any fully acked page per the checkpoint is purged if not already
-            try { pageIO.purge(); } catch (NoSuchFileException e) { /* ignore */ }
+            try { page.getPageIO().purge(); } catch (NoSuchFileException e) { /* ignore */ }
 
             // we want to keep all the "middle" checkpoints between the first unacked tail page and the head page
             // to always have a contiguous sequence of checkpoints which helps figuring queue integrity. for this
@@ -199,30 +238,30 @@ private void add(Checkpoint checkpoint, PageIO pageIO) throws IOException {
             } else {
                 // create a tail page with a null PageIO and add it to tail pages but not unreadTailPages
                 // since it is fully read because also fully acked
+                // TODO: I don't like this null pageIO tail page...
                 this.tailPages.add(new TailPage(checkpoint, this, null));
             }
         } else {
-            TailPage p = new TailPage(checkpoint, this, pageIO);
-            this.tailPages.add(p);
-            this.unreadTailPages.add(p);
-            this.unreadCount += p.unreadCount();
-            this.currentByteSize += pageIO.getCapacity();
+            this.tailPages.add(page);
+            this.unreadTailPages.add(page);
+            this.unreadCount += page.unreadCount();
+            this.currentByteSize += page.getPageIO().getCapacity();
 
             // for now deactivate all tail pages, we will only reactivate the first one at the end
-            pageIO.deactivate();
+            page.getPageIO().deactivate();
         }
 
         // track the seqNum as we rebuild tail pages, prevent empty pages with a minSeqNum of 0 to reset seqNum
         if (checkpoint.maxSeqNum() > this.seqNum) {
             this.seqNum = checkpoint.maxSeqNum();
         }
-
     }
 
     // create a new empty headpage for the given pageNum and imidiately checkpoint it
     // @param pageNum the page number of the new head page
     private void newCheckpointedHeadpage(int pageNum) throws IOException {
         PageIO headPageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
+        headPageIO.create();
         this.headPage = new HeadPage(pageNum, this, headPageIO);
         this.headPage.forceCheckpoint();
         this.currentByteSize += headPageIO.getCapacity();
@@ -585,6 +624,19 @@ protected int firstUnackedPageNum() {
         return this.tailPages.get(0).getPageNum();
     }
 
+    public long getAckedCount() {
+        return headPage.ackedSeqNums.cardinality() + tailPages.stream()
+                .mapToLong(page -> page.ackedSeqNums.cardinality())
+                .sum();
+    }
+
+    public long getUnackedCount() {
+        long headPageCount = (headPage.getElementCount() - headPage.ackedSeqNums.cardinality());
+        long tailPagesCount = tailPages.stream()
+                .mapToLong(page -> (page.getElementCount() - page.ackedSeqNums.cardinality())).sum();
+        return headPageCount + tailPagesCount;
+    }
+
     protected long nextSeqNum() {
         return this.seqNum += 1;
     }
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java b/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java
index 6ebccbf1d87..41afe05adb9 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/TailPage.java
@@ -14,6 +14,7 @@ public TailPage(HeadPage page) {
     }
 
     // create a new TailPage object for an exiting Checkpoint and data file
+    // @param pageIO the PageIO object is expected to be open/recover/create
     public TailPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOException {
         super(checkpoint.getPageNum(), queue, checkpoint.getMinSeqNum(), checkpoint.getElementCount(), checkpoint.getFirstUnackedSeqNum(), new BitSet(), pageIO);
 
@@ -21,12 +22,6 @@ public TailPage(Checkpoint checkpoint, Queue queue, PageIO pageIO) throws IOExce
         if (checkpoint.getFirstUnackedSeqNum() > checkpoint.getMinSeqNum()) {
             this.ackedSeqNums.flip(0, (int) (checkpoint.getFirstUnackedSeqNum() - checkpoint.getMinSeqNum()));
         }
-
-        if (pageIO != null) {
-            // open the data file and reconstruct the IO object internal state
-            pageIO.open(checkpoint.getMinSeqNum(), checkpoint.getElementCount());
-        }
-
     }
 
     public void checkpoint() throws IOException {
diff --git a/logstash-core/src/main/java/org/logstash/common/io/AbstractByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/AbstractByteBufferPageIO.java
new file mode 100644
index 00000000000..bdc7714f647
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/io/AbstractByteBufferPageIO.java
@@ -0,0 +1,277 @@
+package org.logstash.common.io;
+
+import org.logstash.ackedqueue.SequencedList;
+
+import java.io.IOException;
+import java.nio.ByteBuffer;
+import java.util.ArrayList;
+import java.util.List;
+import java.util.zip.CRC32;
+import java.util.zip.Checksum;
+
+public abstract class AbstractByteBufferPageIO implements PageIO {
+
+    public class PageIOInvalidElementException extends IOException {
+        public PageIOInvalidElementException() { super(); }
+        public PageIOInvalidElementException(String message) { super(message); }
+    }
+
+    public class PageIOInvalidVersionException extends IOException {
+        public PageIOInvalidVersionException(String message) { super(message); }
+    }
+
+    public static final byte VERSION_ONE = 1;
+    public static final int VERSION_SIZE = Byte.BYTES;
+    public static final int CHECKSUM_SIZE = Integer.BYTES;
+    public static final int LENGTH_SIZE = Integer.BYTES;
+    public static final int SEQNUM_SIZE = Long.BYTES;
+    public static final int MIN_RECORD_SIZE = SEQNUM_SIZE + CHECKSUM_SIZE;
+    public static final int HEADER_SIZE = 1;     // version byte
+    public static final int MIN_CAPACITY = VERSION_SIZE + SEQNUM_SIZE + LENGTH_SIZE + 1 + CHECKSUM_SIZE; // header overhead plus elements overhead to hold a single 1 byte element
+    public static final List<byte[]> EMPTY_READ = new ArrayList<>(0);
+
+    public static final boolean VERIFY_CHECKSUM = true;
+    public static final boolean STRICT_CAPACITY = true;
+
+    protected int capacity; // page capacity is an int per the ByteBuffer class.
+    protected final int pageNum;
+    protected final List<Integer> offsetMap; // has to be extendable
+    protected long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
+    protected int elementCount;
+    protected int head; // head is the write position and is an int per ByteBuffer class position
+    protected byte version;
+    protected Checksum checkSummer;
+
+    public AbstractByteBufferPageIO(int pageNum, int capacity) {
+        this.minSeqNum = 0;
+        this.elementCount = 0;
+        this.version = 0;
+        this.head = 0;
+        this.pageNum = pageNum;
+        this.capacity = capacity;
+        this.offsetMap = new ArrayList<>();
+        this.checkSummer = new CRC32();
+    }
+
+    // @return the concrete class buffer
+    protected abstract ByteBuffer getBuffer();
+
+    @Override
+    public void open(long minSeqNum, int elementCount) throws IOException {
+        getBuffer().position(0);
+        this.version = getBuffer().get();
+        validateVersion(this.version);
+        this.head = 1;
+
+        this.minSeqNum = minSeqNum;
+        this.elementCount = elementCount;
+
+        if (this.elementCount > 0) {
+            // verify first seqNum to be same as expected minSeqNum
+            long seqNum = getBuffer().getLong();
+            if (seqNum != this.minSeqNum) { throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum)); }
+
+            // reset back position to first seqNum
+            getBuffer().position(this.head);
+
+            for (int i = 0; i < this.elementCount; i++) {
+                // verify that seqNum must be of strict + 1 increasing order
+                readNextElement(this.minSeqNum + i, !VERIFY_CHECKSUM);
+            }
+        }
+    }
+
+    // recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes
+    // to reflect what it recovered from the page
+    @Override
+    public void recover() throws IOException {
+        getBuffer().position(0);
+        this.version = getBuffer().get();
+        validateVersion(this.version);
+        this.head = 1;
+
+        // force minSeqNum to actual first element seqNum
+        this.minSeqNum = getBuffer().getLong();
+        // reset back position to first seqNum
+        getBuffer().position(this.head);
+
+        // reset elementCount to 0 and increment to actal number of valid elements found
+        this.elementCount = 0;
+
+        for (int i = 0; ; i++) {
+            try {
+                // verify that seqNum must be of strict + 1 increasing order
+                readNextElement(this.minSeqNum + i, VERIFY_CHECKSUM);
+                this.elementCount += 1;
+            } catch (PageIOInvalidElementException e) {
+                // simply stop at first invalid element
+                break;
+            }
+        }
+
+        // if we were not able to read any element just reset minSeqNum to zero
+        if (this.elementCount <= 0) {
+            this.minSeqNum = 0;
+        }
+    }
+
+    // we don't have different versions yet so simply check if the version is VERSION_ONE for basic integrity check
+    // and if an unexpected version byte is read throw PageIOInvalidVersionException
+    private void validateVersion(byte version) throws PageIOInvalidVersionException {
+        if (version != VERSION_ONE) {
+            throw new PageIOInvalidVersionException(String.format("Expected page version=%d but found version=%d", VERSION_ONE, version));
+        }
+    }
+
+    // read and vadidate next element at page head
+    // @param verifyChecksum if true the actual element data will be read + checksumed and compared to written checksum
+    private void readNextElement(long expectedSeqNum, boolean verifyChecksum) throws PageIOInvalidElementException {
+        // if there is no room for the seqNum and length bytes stop here
+        // TODO: I know this isn't a great exception message but at the time of writing I couldn't come up with anything better :P
+        if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) { throw new PageIOInvalidElementException("cannot read seqNum and length bytes past buffer capacity"); }
+
+        int elementOffset = this.head;
+        ByteBuffer buffer = getBuffer();
+
+        long seqNum = buffer.getLong();
+        this.head += SEQNUM_SIZE;
+
+        if (seqNum != expectedSeqNum) { throw new PageIOInvalidElementException(String.format("Element seqNum %d is expected to be %d", seqNum, expectedSeqNum)); }
+
+        int length = buffer.getInt();
+        this.head += LENGTH_SIZE;
+
+        // length must be > 0
+        if (length <= 0) { throw new PageIOInvalidElementException("Element invalid length"); }
+
+        // if there is no room for the proposed data length and checksum just stop here
+        if (this.head + length + CHECKSUM_SIZE > capacity) { throw new PageIOInvalidElementException("cannot read element payload and checksum past buffer capacity"); }
+
+        if (verifyChecksum) {
+            // read data and compute checksum;
+            byte[] readBytes = new byte[length];
+            buffer.get(readBytes);
+            int checksum = buffer.getInt();
+            int computedChecksum = checksum(readBytes);
+            if (computedChecksum != checksum) { throw new PageIOInvalidElementException("Element invalid checksum"); }
+        }
+
+        // at this point we recovered a valid element
+        this.offsetMap.add(elementOffset);
+        this.head += length + CHECKSUM_SIZE;
+
+        buffer.position(head);
+    }
+
+    @Override
+    public void create() throws IOException {
+        getBuffer().position(0);
+        getBuffer().put(VERSION_ONE);
+        this.head = 1;
+        this.minSeqNum = 0L;
+        this.elementCount = 0;
+    }
+
+    @Override
+    public void write(byte[] bytes, long seqNum) throws IOException {
+        write(bytes, seqNum, bytes.length, checksum(bytes));
+    }
+
+    protected int write(byte[] bytes, long seqNum, int length, int checksum) {
+        // since writes always happen at head, we can just append head to the offsetMap
+        assert this.offsetMap.size() == this.elementCount :
+                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
+
+        int initialHead = this.head;
+        ByteBuffer buffer = getBuffer();
+
+        buffer.position(this.head);
+        buffer.putLong(seqNum);
+        buffer.putInt(length);
+        buffer.put(bytes);
+        buffer.putInt(checksum);
+        this.head += persistedByteCount(bytes.length);
+
+        assert this.head == buffer.position() :
+                String.format("head=%d != buffer position=%d", this.head, buffer.position());
+
+        if (this.elementCount <= 0) {
+            this.minSeqNum = seqNum;
+        }
+        this.offsetMap.add(initialHead);
+        this.elementCount++;
+
+        return initialHead;
+    }
+
+    @Override
+    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
+        assert seqNum >= this.minSeqNum :
+                String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
+        assert seqNum <= maxSeqNum() :
+                String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
+
+        List<byte[]> elements = new ArrayList<>();
+        List<Long> seqNums = new ArrayList<>();
+
+        int offset = this.offsetMap.get((int)(seqNum - this.minSeqNum));
+
+        ByteBuffer buffer = getBuffer();
+        buffer.position(offset);
+
+        for (int i = 0; i < limit; i++) {
+            long readSeqNum = buffer.getLong();
+
+            assert readSeqNum == (seqNum + i) :
+                    String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
+
+            int readLength = buffer.getInt();
+            byte[] readBytes = new byte[readLength];
+            buffer.get(readBytes);
+            int checksum = buffer.getInt();
+            int computedChecksum = checksum(readBytes);
+            if (computedChecksum != checksum) {
+                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
+            }
+
+            elements.add(readBytes);
+            seqNums.add(readSeqNum);
+
+            if (seqNum + i >= maxSeqNum()) {
+                break;
+            }
+        }
+
+        return new SequencedList<byte[]>(elements, seqNums);
+    }
+
+    @Override
+    public int getCapacity() { return this.capacity; }
+
+    @Override
+    public long getMinSeqNum() { return this.minSeqNum; }
+
+    @Override
+    public int getElementCount() { return this.elementCount; }
+
+    @Override
+    public boolean hasSpace(int bytes) {
+        int bytesLeft = this.capacity - this.head;
+        return persistedByteCount(bytes) <= bytesLeft;
+    }
+
+    @Override
+    public int persistedByteCount(int byteCount) {
+        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
+    }
+
+    protected int checksum(byte[] bytes) {
+        checkSummer.reset();
+        checkSummer.update(bytes, 0, bytes.length);
+        return (int) checkSummer.getValue();
+    }
+
+    private long maxSeqNum() {
+        return this.minSeqNum + this.elementCount - 1;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
index e2619a5b3e5..184f9f16635 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/ByteBufferPageIO.java
@@ -2,6 +2,7 @@
 
 import org.logstash.ackedqueue.Queueable;
 import org.logstash.ackedqueue.SequencedList;
+import sun.reflect.generics.reflectiveObjects.NotImplementedException;
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
@@ -10,27 +11,9 @@
 import java.util.zip.CRC32;
 import java.util.zip.Checksum;
 
-// TODO: currently assuming continuous seqNum is the byte buffer where we can deduct the maxSeqNum from the min + count.
-// TODO: we could change this and support non-continuous seqNums but I am not sure we should.
-// TODO: checksum is not currently computed.
-
-public class ByteBufferPageIO implements PageIO {
-    public static final byte VERSION = 1;
-    public static final int CHECKSUM_SIZE = Integer.BYTES;
-    public static final int LENGTH_SIZE = Integer.BYTES;
-    public static final int SEQNUM_SIZE = Long.BYTES;
-    public static final int MIN_RECORD_SIZE = SEQNUM_SIZE + LENGTH_SIZE + CHECKSUM_SIZE;
-    public static final int HEADER_SIZE = 1;     // version byte
-    static final List<byte[]> EMPTY_READ = new ArrayList<>(0);
-
-    private final int capacity;
-    private final List<Integer> offsetMap; // has to be extendable
+public class ByteBufferPageIO extends AbstractByteBufferPageIO {
+
     private final ByteBuffer buffer;
-    private long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
-    private int elementCount;
-    private int head;
-    private byte version;
-    private Checksum checkSummer;
 
     public ByteBufferPageIO(int pageNum, int capacity, String path) throws IOException {
         this(capacity, new byte[0]);
@@ -41,208 +24,41 @@ public ByteBufferPageIO(int capacity) throws IOException {
     }
 
     public ByteBufferPageIO(int capacity, byte[] initialBytes) throws IOException {
-        this.capacity = capacity;
+        super(0, capacity);
+
         if (initialBytes.length > capacity) {
             throw new IOException("initial bytes greater than capacity");
         }
 
         this.buffer = ByteBuffer.allocate(capacity);
         this.buffer.put(initialBytes);
-
-        this.offsetMap = new ArrayList<>();
-        this.checkSummer = new CRC32();
-    }
-
-    @Override
-    public void open(long minSeqNum, int elementCount) throws IOException {
-        this.minSeqNum = minSeqNum;
-        this.elementCount = elementCount;
-
-        this.buffer.position(0);
-        this.version = this.buffer.get();
-        this.head = 1;
-
-        if (this.elementCount > 0) {
-
-            // TODO: refactor the read logic below to DRY with the read() method.
-
-            // set head by skipping over all elements
-            for (int i = 0; i < this.elementCount; i++) {
-                if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
-                    throw new IOException(String.format("cannot read seqNum and length bytes past buffer capacity"));
-                }
-
-                long seqNum = this.buffer.getLong();
-
-                if (i == 0 && seqNum != this.minSeqNum) {
-                    throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum));
-                }
-
-                this.offsetMap.add(head);
-                this.head += SEQNUM_SIZE;
-
-
-                int length = this.buffer.getInt();
-                this.head += LENGTH_SIZE;
-
-                if (this.head + length + CHECKSUM_SIZE > capacity) {
-                    throw new IOException(String.format("cannot read element payload and checksum past buffer capacity"));
-                }
-
-                // skip over data
-                this.head += length;
-                this.head += CHECKSUM_SIZE;
-
-                this.buffer.position(head);
-            }
-        }
-    }
-
-    @Override
-    public void create() throws IOException {
-        this.buffer.position(0);
-        this.buffer.put(VERSION);
-        this.head = 1;
-        this.minSeqNum = 0L;
-        this.elementCount = 0;
-    }
-
-    @Override
-    public int getCapacity() {
-        return this.capacity;
-    }
-
-    public long getMinSeqNum() {
-        return this.minSeqNum;
-    }
-
-    @Override
-    public boolean hasSpace(int bytes) {
-        int bytesLeft = this.capacity - this.head;
-        return persistedByteCount(bytes) <= bytesLeft;
-    }
-
-    @Override
-    public void write(byte[] bytes, long seqNum) throws IOException {
-        // since writes always happen at head, we can just append head to the offsetMap
-        assert this.offsetMap.size() == this.elementCount :
-                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
-
-        int initialHead = this.head;
-
-        this.buffer.position(this.head);
-        this.buffer.putLong(seqNum);
-        this.buffer.putInt(bytes.length);
-        this.buffer.put(bytes);
-        this.buffer.putInt(checksum(bytes));
-        this.head += persistedByteCount(bytes.length);
-        assert this.head == this.buffer.position() :
-                String.format("head=%d != buffer position=%d", this.head, this.buffer.position());
-
-        if (this.elementCount <= 0) {
-            this.minSeqNum = seqNum;
-        }
-        this.offsetMap.add(initialHead);
-        this.elementCount++;
-    }
-
-    @Override
-    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
-        assert seqNum >= this.minSeqNum :
-                String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
-        assert seqNum <= maxSeqNum() :
-                String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
-
-        List<byte[]> elements = new ArrayList<>();
-        List<Long> seqNums = new ArrayList<>();
-
-        int offset = this.offsetMap.get((int)(seqNum - this.minSeqNum));
-
-        this.buffer.position(offset);
-
-        for (int i = 0; i < limit; i++) {
-            long readSeqNum = this.buffer.getLong();
-
-            assert readSeqNum == (seqNum + i) :
-                    String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
-
-            int readLength = this.buffer.getInt();
-            byte[] readBytes = new byte[readLength];
-            this.buffer.get(readBytes);
-            int checksum = this.buffer.getInt();
-            int computedChecksum = checksum(readBytes);
-            if (computedChecksum != checksum) {
-                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
-            }
-
-            elements.add(readBytes);
-            seqNums.add(readSeqNum);
-
-            if (seqNum + i >= maxSeqNum()) {
-                break;
-            }
-        }
-
-        return new SequencedList<>(elements, seqNums);
     }
 
     @Override
-    public void deactivate() {
-        // nothing to do
-    }
+    public void deactivate() { /* nothing */ }
 
     @Override
-    public void activate() {
-        // nothing to do
-    }
+    public void activate() { /* niet */ }
 
     @Override
-    public void ensurePersisted() {
-        // nothing to do
-    }
+    public void ensurePersisted() { /* nada */ }
 
     @Override
-    public void purge() throws IOException {
-        // do nothing
-    }
+    public void purge() { /* zilch */ }
 
     @Override
-    public void close() throws IOException {
-        // TODO: not sure if we need to do something here since in-memory pages are ephemeral
-    }
+    public void close() { /* don't look here */ }
 
-    private int checksum(byte[] bytes) {
-        checkSummer.reset();
-        checkSummer.update(bytes, 0, bytes.length);
-        return (int) checkSummer.getValue();
-    }
-
-    // TODO: static method for tests - should refactor
-    public static int _persistedByteCount(int byteCount) {
-        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
-    }
 
     @Override
-    public int persistedByteCount(int byteCount) {
-        return ByteBufferPageIO._persistedByteCount(byteCount);
-    }
-
-    private long maxSeqNum() {
-        return this.minSeqNum + this.elementCount - 1;
-    }
-
+    protected ByteBuffer getBuffer() { return this.buffer; }
 
     // below public methods only used by tests
 
-    public int getWritePosition() {
-        return this.head;
-    }
+    // TODO: static method for tests - should refactor
+    public static int _persistedByteCount(int byteCount) { return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE; }
 
-    public int getElementCount() {
-        return this.elementCount;
-    }
+    public int getWritePosition() { return this.head; }
 
-    public byte[] dump() {
-        return this.buffer.array();
-    }
+    public byte[] dump() { return this.buffer.array(); }
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java b/logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java
index 6d79aad2be4..5388003fc41 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/FileCheckpointIO.java
@@ -5,8 +5,13 @@
 import java.io.IOException;
 import java.io.InputStream;
 import java.nio.file.Files;
+import java.nio.file.OpenOption;
 import java.nio.file.Path;
 import java.nio.file.Paths;
+import static java.nio.file.StandardOpenOption.CREATE;
+import static java.nio.file.StandardOpenOption.WRITE;
+import static java.nio.file.StandardOpenOption.TRUNCATE_EXISTING;
+import static java.nio.file.StandardOpenOption.DSYNC;
 
 public class FileCheckpointIO  implements CheckpointIO {
 //    Checkpoint file structure
@@ -29,6 +34,7 @@ public class FileCheckpointIO  implements CheckpointIO {
     private final String dirPath;
     private final String HEAD_CHECKPOINT = "checkpoint.head";
     private final String TAIL_CHECKPOINT = "checkpoint.";
+    private final OpenOption[] WRITE_OPTIONS = new OpenOption[] { WRITE, CREATE, TRUNCATE_EXISTING, DSYNC };
 
     public FileCheckpointIO(String dirPath) {
         this.dirPath = dirPath;
@@ -53,7 +59,7 @@ public void write(String fileName, Checkpoint checkpoint) throws IOException {
         Path path = Paths.get(dirPath, fileName);
         final byte[] buffer = new byte[BUFFER_SIZE];
         write(checkpoint, buffer);
-        Files.write(path, buffer);
+        Files.write(path, buffer, WRITE_OPTIONS);
     }
 
     @Override
diff --git a/logstash-core/src/main/java/org/logstash/common/io/MemoryCheckpointIO.java b/logstash-core/src/main/java/org/logstash/common/io/MemoryCheckpointIO.java
index 4723e2ea550..409095eb4c6 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/MemoryCheckpointIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/MemoryCheckpointIO.java
@@ -3,6 +3,7 @@
 import org.logstash.ackedqueue.Checkpoint;
 
 import java.io.IOException;
+import java.nio.file.NoSuchFileException;
 import java.util.HashMap;
 import java.util.Map;
 
@@ -25,7 +26,9 @@ public MemoryCheckpointIO(String dirPath) {
 
     @Override
     public Checkpoint read(String fileName) throws IOException {
-        return this.sources.get(fileName);
+        Checkpoint cp = this.sources.get(fileName);
+        if (cp == null) { throw new NoSuchFileException("no memory checkpoint for " + fileName); }
+        return cp;
     }
 
     @Override
diff --git a/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java b/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
index 0b86477f80b..a3d2763ddd5 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/MmapPageIO.java
@@ -1,199 +1,74 @@
 package org.logstash.common.io;
 
-import org.logstash.ackedqueue.Queueable;
-import org.logstash.ackedqueue.SequencedList;
-
 import java.io.File;
 import java.io.IOException;
 import java.io.RandomAccessFile;
 import java.nio.MappedByteBuffer;
 import java.nio.channels.FileChannel;
 import java.nio.file.Files;
-import java.nio.file.Path;
 import java.nio.file.Paths;
-import java.util.ArrayList;
-import java.util.List;
-import java.util.zip.CRC32;
-import java.util.zip.Checksum;
 
 // TODO: this essentially a copy of ByteBufferPageIO and should be DRY'ed - temp impl to test file based stress test
 
-public class MmapPageIO implements PageIO {
-    public static final byte VERSION = 1;
-    public static final int CHECKSUM_SIZE = Integer.BYTES;
-    public static final int LENGTH_SIZE = Integer.BYTES;
-    public static final int SEQNUM_SIZE = Long.BYTES;
-    public static final int MIN_RECORD_SIZE = SEQNUM_SIZE + CHECKSUM_SIZE;
-    public static final int HEADER_SIZE = 1;     // version byte
-    static final List<byte[]> EMPTY_READ = new ArrayList<>(0);
-
-    private final int capacity;
-    private final String dirPath;
-    private final int pageNum;
-    private final List<Integer> offsetMap; // has to be extendable
-
-    private MappedByteBuffer buffer;
+public class MmapPageIO extends AbstractByteBufferPageIO {
+
     private File file;
     private FileChannel channel;
-
-    private long minSeqNum; // TODO: to make minSeqNum final we have to pass in the minSeqNum in the constructor and not set it on first write
-    private int elementCount;
-    private int head;
-    private byte version;
-    private Checksum checkSummer;
+    protected MappedByteBuffer buffer;
 
     public MmapPageIO(int pageNum, int capacity, String dirPath) throws IOException {
-        this.pageNum = pageNum;
-        this.capacity = capacity;
-        this.dirPath = dirPath;
-        this.offsetMap = new ArrayList<>();
-        this.checkSummer = new CRC32();
+        super(pageNum, capacity);
+
+        this.file = Paths.get(dirPath, "page." + pageNum).toFile();
     }
 
     @Override
     public void open(long minSeqNum, int elementCount) throws IOException {
-        this.minSeqNum = minSeqNum;
-        this.elementCount = elementCount;
-
-        this.file = buildPath().toFile();
-        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
-        this.channel = raf.getChannel();
-        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
-        raf.close();
-        this.buffer.load();
-
-        this.buffer.position(0);
-        this.version = this.buffer.get();
-        this.head = 1;
-
-        if (this.elementCount > 0) {
-
-            // TODO: refactor the read logic below to DRY with the read() method.
-
-            // set head by skipping over all elements
-            for (int i = 0; i < this.elementCount; i++) {
-                if (this.head + SEQNUM_SIZE + LENGTH_SIZE > capacity) {
-                    throw new IOException(String.format("cannot read seqNum and length bytes past buffer capacity"));
-                }
-
-                long seqNum = this.buffer.getLong();
+        mapFile(STRICT_CAPACITY);
+        super.open(minSeqNum, elementCount);
+    }
 
-                if (i == 0 && seqNum != this.minSeqNum) {
-                    throw new IOException(String.format("first seqNum=%d is different than minSeqNum=%d", seqNum, this.minSeqNum));
-                }
+    // recover will overwrite/update/set this object minSeqNum, capacity and elementCount attributes
+    // to reflect what it recovered from the page
+    @Override
+    public void recover() throws IOException {
+        mapFile(!STRICT_CAPACITY);
+        super.recover();
+    }
 
-                this.offsetMap.add(head);
-                this.head += SEQNUM_SIZE;
+    // memory map data file to this.buffer and read initial version byte
+    // @param strictCapacity if true verify that data file size is same as confgured page capcity, if false update page capcity to actual file size
+    private void mapFile(boolean strictCapacity) throws IOException {
+        RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
 
+        if (raf.length() > Integer.MAX_VALUE) {
+            throw new IOException("Page file too large " + this.file);
+        }
+        int pageFileCapacity = (int)raf.length();
 
-                int length = this.buffer.getInt();
-                this.head += LENGTH_SIZE;
+        if (strictCapacity && this.capacity != pageFileCapacity) {
+            throw new IOException("Page file size " + pageFileCapacity + " different to configured page capacity " + this.capacity + " for " + this.file);
+        }
 
-                if (this.head + length + CHECKSUM_SIZE > capacity) {
-                    throw new IOException(String.format("cannot read element payload and checksum past buffer capacity"));
-                }
+        // update capacity to actual raf lenght
+        this.capacity = pageFileCapacity;
 
-                // skip over data
-                this.head += length;
-                this.head += CHECKSUM_SIZE;
+        if (this.capacity < MIN_CAPACITY) { throw new IOException(String.format("Page file size is too small to hold elements")); }
 
-                this.buffer.position(head);
-            }
-        }
+        this.channel = raf.getChannel();
+        this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
+        raf.close();
+        this.buffer.load();
     }
 
     @Override
     public void create() throws IOException {
-        this.file = buildPath().toFile();
         RandomAccessFile raf = new RandomAccessFile(this.file, "rw");
         this.channel = raf.getChannel();
         this.buffer = this.channel.map(FileChannel.MapMode.READ_WRITE, 0, this.capacity);
         raf.close();
 
-        this.buffer.position(0);
-        this.buffer.put(VERSION);
-        this.head = 1;
-        this.minSeqNum = 0;
-        this.elementCount = 0;
-    }
-
-    @Override
-    public int getCapacity() {
-        return this.capacity;
-    }
-
-    public long getMinSeqNum() {
-        return this.minSeqNum;
-    }
-
-    @Override
-    public boolean hasSpace(int bytes) {
-        int bytesLeft = this.capacity - this.head;
-        return persistedByteCount(bytes) <= bytesLeft;
-    }
-
-    @Override
-    public void write(byte[] bytes, long seqNum) throws IOException {
-        // since writes always happen at head, we can just append head to the offsetMap
-        assert this.offsetMap.size() == this.elementCount :
-                String.format("offsetMap size=%d != elementCount=%d", this.offsetMap.size(), this.elementCount);
-
-        int initialHead = this.head;
-
-        this.buffer.position(this.head);
-        this.buffer.putLong(seqNum);
-        this.buffer.putInt(bytes.length);
-        this.buffer.put(bytes);
-        this.buffer.putInt(checksum(bytes));
-        this.head += persistedByteCount(bytes.length);
-        assert this.head == this.buffer.position() :
-                String.format("head=%d != buffer position=%d", this.head, this.buffer.position());
-
-        if (this.elementCount <= 0) {
-            this.minSeqNum = seqNum;
-        }
-        this.offsetMap.add(initialHead);
-        this.elementCount++;
-    }
-
-    @Override
-    public SequencedList<byte[]> read(long seqNum, int limit) throws IOException {
-        assert seqNum >= this.minSeqNum :
-                String.format("seqNum=%d < minSeqNum=%d", seqNum, this.minSeqNum);
-        assert seqNum <= maxSeqNum() :
-                String.format("seqNum=%d is > maxSeqNum=%d", seqNum, maxSeqNum());
-
-        List<byte[]> elements = new ArrayList<>();
-        List<Long> seqNums = new ArrayList<>();
-
-        int offset = this.offsetMap.get((int)(seqNum - this.minSeqNum));
-
-        this.buffer.position(offset);
-
-        for (int i = 0; i < limit; i++) {
-            long readSeqNum = this.buffer.getLong();
-
-            assert readSeqNum == (seqNum + i) :
-                    String.format("unmatched seqNum=%d to readSeqNum=%d", seqNum + i, readSeqNum);
-
-            int readLength = this.buffer.getInt();
-            byte[] readBytes = new byte[readLength];
-            this.buffer.get(readBytes);
-            int checksum = this.buffer.getInt();
-            int computedChecksum = checksum(readBytes);
-            if (computedChecksum != checksum) {
-                throw new IOException(String.format("computed checksum=%d != checksum for file=%d", computedChecksum, checksum));
-            }
-
-            elements.add(readBytes);
-            seqNums.add(readSeqNum);
-
-            if (seqNum + i >= maxSeqNum()) {
-                break;
-            }
-        }
-
-        return new SequencedList<byte[]>(elements, seqNums);
+        super.create();
     }
 
     @Override
@@ -221,7 +96,7 @@ public void ensurePersisted() {
     @Override
     public void purge() throws IOException {
         close();
-        Files.delete(buildPath());
+        Files.delete(this.file.toPath());
     }
 
     @Override
@@ -233,22 +108,8 @@ public void close() throws IOException {
         this.buffer = null;
     }
 
-    private int checksum(byte[] bytes) {
-        checkSummer.reset();
-        checkSummer.update(bytes, 0, bytes.length);
-        return (int) checkSummer.getValue();
-    }
-
     @Override
-    public int persistedByteCount(int byteCount) {
-        return SEQNUM_SIZE + LENGTH_SIZE + byteCount + CHECKSUM_SIZE;
-    }
-
-    private long maxSeqNum() {
-        return this.minSeqNum + this.elementCount - 1;
-    }
-
-    private Path buildPath() {
-        return Paths.get(this.dirPath, "page." + this.pageNum);
+    protected MappedByteBuffer getBuffer() {
+        return this.buffer;
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/PageIO.java b/logstash-core/src/main/java/org/logstash/common/io/PageIO.java
index 0fa3ede204b..5796c6c1d1d 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/PageIO.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/PageIO.java
@@ -10,10 +10,20 @@
 public interface PageIO extends Closeable {
 
     // the concrete class should be constructed with the pageNum, capacity and dirPath attributes
+    // and open/recover/create must first be called to setup with physical data file
+    //
+    // TODO: we should probably refactor this with a factory to force the creation of a fully
+    //       initialized concrete object with either open/recover/create instead of allowing
+    //       a partially initialized object using the concrete class constructor. Not sure.
 
     // open an existing data container and reconstruct internal state if required
     void open(long minSeqNum, int elementCount) throws IOException;
 
+    // optimistically recover an existing data container and reconstruct internal state
+    // with the actual read/recovered data. this is only useful when reading a head page
+    // data file since tail pages are read-only.
+    void recover() throws IOException;
+
     // create a new empty data file
     void create() throws IOException;
 
@@ -44,4 +54,10 @@ public interface PageIO extends Closeable {
 
     // delete/unlink/remove data file
     void purge() throws IOException;
+
+    // @return the data container elements count
+    int getElementCount();
+
+    // @return the data container min sequence number
+    long getMinSeqNum();
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/wip/MemoryPageIOStream.java b/logstash-core/src/main/java/org/logstash/common/io/wip/MemoryPageIOStream.java
index 5c8ec153727..3d0e66762cb 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/wip/MemoryPageIOStream.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/wip/MemoryPageIOStream.java
@@ -8,6 +8,7 @@
 import org.logstash.common.io.ByteArrayStreamOutput;
 import org.logstash.common.io.ByteBufferStreamInput;
 import org.logstash.common.io.PageIO;
+import sun.reflect.generics.reflectiveObjects.NotImplementedException;
 
 import java.io.IOException;
 import java.nio.ByteBuffer;
@@ -69,6 +70,11 @@ public MemoryPageIOStream(int capacity, byte[] initialBytes) throws IOException
         crcWrappedOutput = new BufferedChecksumStreamOutput(streamedOutput);
     }
 
+    @Override
+    public void recover() throws IOException {
+        throw new NotImplementedException();
+    }
+
     @Override
     public void open(long minSeqNum, int elementCount) throws IOException {
         this.minSeqNum = minSeqNum;
diff --git a/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
new file mode 100644
index 00000000000..f11c97dd2f6
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
@@ -0,0 +1,91 @@
+package org.logstash;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.nio.channels.FileLock;
+import java.nio.file.FileSystems;
+import java.nio.file.Path;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+
+public class FileLockFactoryTest {
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+    private String lockDir;
+    private final String LOCK_FILE = ".test";
+
+    private FileLock lock;
+
+    @Before
+    public void setUp() throws Exception {
+        lockDir = temporaryFolder.newFolder("lock").getPath();
+    }
+
+    @Before
+    public void lockFirst() throws Exception {
+        lock = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock.isValid(), is(equalTo(true)));
+        assertThat(lock.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void ObtainLockOnNonLocked() throws IOException {
+        // empty to just test the lone @Before lockFirst() test
+    }
+
+    @Test(expected = LockException.class)
+    public void ObtainLockOnLocked() throws IOException {
+        FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+    }
+
+    @Test
+    public void ObtainLockOnOtherLocked() throws IOException {
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, ".test2");
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void LockReleaseLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+    }
+
+    @Test
+    public void LockReleaseLockObtainLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void LockReleaseLockObtainLockRelease() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+
+        FileLockFactory.getDefault().releaseLock(lock2);
+    }
+
+    @Test(expected = LockException.class)
+    public void ReleaseNullLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(null);
+     }
+
+    @Test(expected = LockException.class)
+    public void ReleaseUnobtainedLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+        FileLockFactory.getDefault().releaseLock(lock);
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
index 5115c69a5d1..efdc1229faa 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
@@ -2,15 +2,10 @@
 
 import org.junit.Test;
 import org.logstash.common.io.ByteBufferPageIO;
-import org.logstash.common.io.FileCheckpointIOTest;
 import org.logstash.common.io.PageIO;
 
 import java.io.IOException;
-import java.net.URL;
-import java.nio.file.NoSuchFileException;
-import java.nio.file.Paths;
 
-import static org.hamcrest.CoreMatchers.containsString;
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.MatcherAssert.assertThat;
@@ -19,9 +14,10 @@ public class HeadPageTest {
 
     @Test
     public void newHeadPage() throws IOException {
-        Settings s = TestSettings.getSettings(100);
+        Settings s = TestSettings.volatileQueueSettings(100);
         Queue q = new Queue(s);
         PageIO pageIO = s.getPageIOFactory().build(0, 100, "dummy");
+        pageIO.create();
         HeadPage p = new HeadPage(0, q, pageIO);
 
         assertThat(p.getPageNum(), is(equalTo(0)));
@@ -36,7 +32,7 @@ public void pageWrite() throws IOException {
         Queueable element = new StringElement("foobarbaz");
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
 
-        Settings s = TestSettings.getSettings(singleElementCapacity);
+        Settings s = TestSettings.volatileQueueSettings(singleElementCapacity);
         Queue q = new Queue(s);
         q.open();
         HeadPage p = q.headPage;
@@ -55,7 +51,7 @@ public void pageWriteAndReadSingle() throws IOException {
         Queueable element = new StringElement("foobarbaz");
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
 
-        Settings s = TestSettings.getSettings(singleElementCapacity);
+        Settings s = TestSettings.volatileQueueSettings(singleElementCapacity);
         Queue q = new Queue(s);
         q.open();
         HeadPage p = q.headPage;
@@ -79,7 +75,7 @@ public void pageWriteAndReadMulti() throws IOException {
         Queueable element = new StringElement("foobarbaz");
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
 
-        Settings s = TestSettings.getSettings(singleElementCapacity);
+        Settings s = TestSettings.volatileQueueSettings(singleElementCapacity);
         Queue q = new Queue(s);
         q.open();
         HeadPage p = q.headPage;
@@ -104,7 +100,7 @@ public void pageWriteAndReadMulti() throws IOException {
 //        String dirPath = Paths.get(url.toURI()).getParent().toString();
 //        Queueable element = new StringElement("foobarbaz");
 //        int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
-//        Settings s = TestSettings.getSettingsCheckpointFilePageMemory(singleElementCapacity, dirPath);
+//        Settings s = TestSettings.persistedQueueSettings(singleElementCapacity, dirPath);
 //        TestQueue q = new TestQueue(s);
 //        try {
 //            q.open();
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
index 13a777a7d63..a2965758ba6 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -1,9 +1,14 @@
 package org.logstash.ackedqueue;
 
+import org.junit.Before;
+import org.junit.Rule;
 import org.junit.Test;
+import static org.junit.Assert.fail;
+import org.junit.rules.TemporaryFolder;
 import org.logstash.common.io.ByteBufferPageIO;
 
 import java.io.IOException;
+import java.nio.file.NoSuchFileException;
 import java.util.ArrayList;
 import java.util.Arrays;
 import java.util.Collections;
@@ -22,10 +27,18 @@
 import static org.hamcrest.MatcherAssert.assertThat;
 
 public class QueueTest {
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+
+    private String dataPath;
+
+    @Before
+    public void setUp() throws Exception {
+        dataPath = temporaryFolder.newFolder("data").getPath();
+    }
 
     @Test
     public void newQueue() throws IOException {
-        Queue q = new TestQueue(TestSettings.getSettings(10));
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(10));
         q.open();
 
         assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));
@@ -33,7 +46,7 @@ public void newQueue() throws IOException {
 
     @Test
     public void singleWriteRead() throws IOException {
-        Queue q = new TestQueue(TestSettings.getSettings(100));
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));
         q.open();
 
         Queueable element = new StringElement("foobarbaz");
@@ -48,7 +61,7 @@ public void singleWriteRead() throws IOException {
 
     @Test
     public void singleWriteMultiRead() throws IOException {
-        Queue q = new TestQueue(TestSettings.getSettings(100));
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));
         q.open();
 
         Queueable element = new StringElement("foobarbaz");
@@ -63,7 +76,7 @@ public void singleWriteMultiRead() throws IOException {
 
     @Test
     public void multiWriteSamePage() throws IOException {
-        Queue q = new TestQueue(TestSettings.getSettings(100));
+        Queue q = new TestQueue(TestSettings.volatileQueueSettings(100));
         q.open();
 
         List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"));
@@ -89,7 +102,7 @@ public void writeMultiPage() throws IOException {
         List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
 
-        TestQueue q = new TestQueue(TestSettings.getSettings(2 * singleElementCapacity));
+        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));
         q.open();
 
         for (Queueable e : elements) {
@@ -132,7 +145,7 @@ public void writeMultiPageWithInOrderAcking() throws IOException {
         List<Queueable> elements = Arrays.asList(new StringElement("foobarbaz1"), new StringElement("foobarbaz2"), new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
 
-        TestQueue q = new TestQueue(TestSettings.getSettings(2 * singleElementCapacity));
+        TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(2 * singleElementCapacity));
         q.open();
 
         for (Queueable e : elements) {
@@ -173,7 +186,7 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
         List<Queueable> elements2 = Arrays.asList(new StringElement("foobarbaz3"), new StringElement("foobarbaz4"));
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements1.get(0).serialize().length);
 
-        Settings settings = TestSettings.getSettings(2 * singleElementCapacity);
+        Settings settings = TestSettings.volatileQueueSettings(2 * singleElementCapacity);
         settings.setCheckpointMaxWrites(1024); // arbritary high enough threshold so that it's not reached (default for TestSettings is 1)
         TestQueue q = new TestQueue(settings);
         q.open();
@@ -227,7 +240,12 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
         Batch b = q.nonBlockReadBatch(10);
         b.close();
 
-        assertThat(q.getCheckpointIO().read("checkpoint.0"), is(nullValue()));
+        try {
+            q.getCheckpointIO().read("checkpoint.0");
+            fail("expected NoSuchFileException thrown");
+        } catch (NoSuchFileException e) {
+            // nothing
+        }
 
         c = q.getCheckpointIO().read("checkpoint.head");
         assertThat(c.getPageNum(), is(equalTo(1)));
@@ -263,7 +281,7 @@ public void randomAcking() throws IOException {
             }
             int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(elements.get(0).serialize().length);
 
-            TestQueue q = new TestQueue(TestSettings.getSettings(singleElementCapacity));
+            TestQueue q = new TestQueue(TestSettings.volatileQueueSettings(singleElementCapacity));
             q.open();
 
             for (Queueable e : elements) {
@@ -294,7 +312,7 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution
         Queueable element = new StringElement("foobarbaz");
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
 
-        Settings settings = TestSettings.getSettings(singleElementCapacity);
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity);
         settings.setMaxUnread(2); // 2 so we know the first write should not block and the second should
         TestQueue q = new TestQueue(settings);
         q.open();
@@ -342,7 +360,7 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,
         Queueable element = new StringElement("foobarbaz");
 
         // TODO: add randomized testing on the page size (but must be > single element size)
-        Settings settings = TestSettings.getSettings(256); // 256 is arbitrary, large enough to hold a few elements
+        Settings settings = TestSettings.volatileQueueSettings(256); // 256 is arbitrary, large enough to hold a few elements
 
         settings.setMaxUnread(2); // 2 so we know the first write should not block and the second should
         TestQueue q = new TestQueue(settings);
@@ -397,7 +415,7 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
 
         // allow 10 elements per page but only 100 events in total
-        Settings settings = TestSettings.getSettings(singleElementCapacity * 10, singleElementCapacity * 100);
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
 
         TestQueue q = new TestQueue(settings);
         q.open();
@@ -432,7 +450,7 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
 
         // allow 10 elements per page but only 100 events in total
-        Settings settings = TestSettings.getSettings(singleElementCapacity * 10, singleElementCapacity * 100);
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
 
         TestQueue q = new TestQueue(settings);
         q.open();
@@ -475,7 +493,7 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
 
         // allow 10 elements per page but only 100 events in total
-        Settings settings = TestSettings.getSettings(singleElementCapacity * 10, singleElementCapacity * 100);
+        Settings settings = TestSettings.volatileQueueSettings(singleElementCapacity * 10, singleElementCapacity * 100);
 
         TestQueue q = new TestQueue(settings);
         q.open();
@@ -507,4 +525,52 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup
         executor.shutdown();
     }
 
-}
\ No newline at end of file
+    @Test
+    public void testAckedCount() throws IOException {
+        Settings settings = TestSettings.persistedQueueSettings(100, dataPath);
+        Queue q = new Queue(settings);
+        q.open();
+
+        Queueable element1 = new StringElement("foobarbaz");
+        Queueable element2 = new StringElement("wowza");
+        Queueable element3 = new StringElement("third");
+        long firstSeqNum = q.write(element1);
+
+        Batch b = q.nonBlockReadBatch(1);
+        assertThat(b.getElements().size(), is(equalTo(1)));
+
+        q.close();
+
+        q = new Queue(settings);
+        q.open();
+
+        long secondSeqNum = q.write(element2);
+        long thirdSeqNum = q.write(element3);
+
+        b = q.nonBlockReadBatch(1);
+        assertThat(b.getElements().size(), is(equalTo(1)));
+        assertThat(b.getElements().get(0), is(equalTo(element1)));
+
+        b = q.nonBlockReadBatch(2);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+        assertThat(b.getElements().get(0), is(equalTo(element2)));
+        assertThat(b.getElements().get(1), is(equalTo(element3)));
+
+        q.ack(Collections.singletonList(firstSeqNum));
+        q.close();
+
+        q = new Queue(settings);
+        q.open();
+
+        b = q.nonBlockReadBatch(2);
+        assertThat(b.getElements().size(), is(equalTo(2)));
+
+        q.ack(Arrays.asList(secondSeqNum, thirdSeqNum));
+
+        assertThat(q.getAckedCount(), equalTo(0L));
+        assertThat(q.getUnackedCount(), equalTo(0L));
+
+        q.close();
+    }
+
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
index cda45aa3e64..ccc7066abb1 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/TestSettings.java
@@ -4,11 +4,12 @@
 import org.logstash.common.io.CheckpointIOFactory;
 import org.logstash.common.io.FileCheckpointIO;
 import org.logstash.common.io.MemoryCheckpointIO;
+import org.logstash.common.io.MmapPageIO;
 import org.logstash.common.io.PageIOFactory;
 
 public class TestSettings {
 
-    public static Settings getSettings(int capacity) {
+    public static Settings volatileQueueSettings(int capacity) {
         MemoryCheckpointIO.clearSources();
         Settings s = new MemorySettings();
         PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
@@ -20,7 +21,7 @@ public static Settings getSettings(int capacity) {
         return s;
     }
 
-    public static Settings getSettings(int capacity, long size) {
+    public static Settings volatileQueueSettings(int capacity, long size) {
         MemoryCheckpointIO.clearSources();
         Settings s = new MemorySettings();
         PageIOFactory pageIOFactory = (pageNum, pageSize, path) -> new ByteBufferPageIO(pageNum, pageSize, path);
@@ -33,12 +34,13 @@ public static Settings getSettings(int capacity, long size) {
         return s;
     }
 
-    public static Settings getSettingsCheckpointFilePageMemory(int capacity, String folder) {
+    public static Settings persistedQueueSettings(int capacity, String folder) {
         Settings s = new FileSettings(folder);
-        PageIOFactory pageIOFactory = (pageNum, size, path) -> new ByteBufferPageIO(pageNum, size, path);
+        PageIOFactory pageIOFactory = (pageNum, size, path) -> new MmapPageIO(pageNum, size, path);
         CheckpointIOFactory checkpointIOFactory = (source) -> new FileCheckpointIO(source);
         s.setCapacity(capacity);
         s.setElementIOFactory(pageIOFactory);
+        s.setCheckpointMaxWrites(1);
         s.setCheckpointIOFactory(checkpointIOFactory);
         s.setElementClass(StringElement.class);
         return s;
diff --git a/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java b/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
index 9edd134bbe2..5dc7713ba94 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/ByteBufferPageIOTest.java
@@ -1,34 +1,58 @@
 package org.logstash.common.io;
 
 import org.junit.Test;
+import org.junit.runner.RunWith;
+import org.junit.runners.Parameterized;
+import org.junit.runners.Parameterized.Parameter;
+import org.junit.runners.Parameterized.Parameters;
 import org.logstash.ackedqueue.Queueable;
 import org.logstash.ackedqueue.SequencedList;
 import org.logstash.ackedqueue.StringElement;
 
 import java.io.IOException;
+import java.io.UncheckedIOException;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.Collection;
 import java.util.List;
+import java.util.concurrent.Callable;
+import java.util.stream.Collectors;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.MatcherAssert.assertThat;
 
-
 public class ByteBufferPageIOTest {
 
-    private final int CAPACITY = 1024;
-    private int MIN_CAPACITY = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(0);
+    // convert any checked exceptions into uncheck RuntimeException
+    public static <V> V uncheck(Callable<V> callable) {
+        try {
+            return callable.call();
+        } catch (RuntimeException e) {
+            throw e;
+        } catch (Exception e) {
+            throw new RuntimeException(e);
+        }
+    }
+
+    private interface BufferGenerator {
+        byte[] generate() throws IOException;
+    }
+
+    private static int CAPACITY = 1024;
+    private static int MIN_CAPACITY = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(0);
 
-    private ByteBufferPageIO subject() throws IOException {
-        return subject(CAPACITY);
+    private static ByteBufferPageIO newEmptyPageIO() throws IOException {
+        return newEmptyPageIO(CAPACITY);
     }
 
-    private ByteBufferPageIO subject(int capacity) throws IOException {
+    private static ByteBufferPageIO newEmptyPageIO(int capacity) throws IOException {
         ByteBufferPageIO io = new ByteBufferPageIO(capacity);
         io.create();
         return io;
     }
 
-    private ByteBufferPageIO subject(int capacity, byte[] bytes) throws IOException {
+    private static ByteBufferPageIO newPageIO(int capacity, byte[] bytes) throws IOException {
         return new ByteBufferPageIO(capacity, bytes);
     }
 
@@ -38,23 +62,23 @@ private Queueable buildStringElement(String str) {
 
     @Test
     public void getWritePosition() throws IOException {
-        assertThat(subject().getWritePosition(), is(equalTo(1)));
+        assertThat(newEmptyPageIO().getWritePosition(), is(equalTo(1)));
     }
 
     @Test
     public void getElementCount() throws IOException {
-        assertThat(subject().getElementCount(), is(equalTo(0)));
+        assertThat(newEmptyPageIO().getElementCount(), is(equalTo(0)));
     }
 
     @Test
     public void getStartSeqNum() throws IOException {
-        assertThat(subject().getMinSeqNum(), is(equalTo(0L)));
+        assertThat(newEmptyPageIO().getMinSeqNum(), is(equalTo(0L)));
     }
 
     @Test
     public void hasSpace() throws IOException {
-        assertThat(subject(MIN_CAPACITY).hasSpace(0), is(true));
-        assertThat(subject(MIN_CAPACITY).hasSpace(1), is(false));
+        assertThat(newEmptyPageIO(MIN_CAPACITY).hasSpace(0), is(true));
+        assertThat(newEmptyPageIO(MIN_CAPACITY).hasSpace(1), is(false));
     }
 
     @Test
@@ -63,64 +87,228 @@ public void hasSpaceAfterWrite() throws IOException {
         int singleElementCapacity = ByteBufferPageIO.HEADER_SIZE + ByteBufferPageIO._persistedByteCount(element.serialize().length);
         long seqNum = 1L;
 
-        ByteBufferPageIO subject = subject(singleElementCapacity);
+        ByteBufferPageIO io = newEmptyPageIO(singleElementCapacity);
 
-        assertThat(subject.hasSpace(element.serialize().length), is(true));
-        subject.write(element.serialize(), seqNum);
-        assertThat(subject.hasSpace(element.serialize().length), is(false));
-        assertThat(subject.hasSpace(1), is(false));
+        assertThat(io.hasSpace(element.serialize().length), is(true));
+        io.write(element.serialize(), seqNum);
+        assertThat(io.hasSpace(element.serialize().length), is(false));
+        assertThat(io.hasSpace(1), is(false));
     }
 
     @Test
     public void write() throws IOException {
         Queueable element = new StringElement("foobarbaz");
         long seqNum = 42L;
-        ByteBufferPageIO subj = subject();
-        subj.create();
-        subj.write(element.serialize(), seqNum);
-        assertThat(subj.getWritePosition(), is(equalTo(ByteBufferPageIO.HEADER_SIZE +  ByteBufferPageIO._persistedByteCount(element.serialize().length))));
-        assertThat(subj.getElementCount(), is(equalTo(1)));
-        assertThat(subj.getMinSeqNum(), is(equalTo(seqNum)));
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+        assertThat(io.getWritePosition(), is(equalTo(ByteBufferPageIO.HEADER_SIZE +  ByteBufferPageIO._persistedByteCount(element.serialize().length))));
+        assertThat(io.getElementCount(), is(equalTo(1)));
+        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
+    }
+
+    @Test
+    public void openValidState() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        long seqNum = 42L;
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+
+        byte[] inititalState = io.dump();
+        io = newPageIO(inititalState.length, inititalState);
+        io.open(seqNum, 1);
+        assertThat(io.getElementCount(), is(equalTo(1)));
+        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
     }
 
     @Test
     public void recoversValidState() throws IOException {
         Queueable element = new StringElement("foobarbaz");
         long seqNum = 42L;
-        ByteBufferPageIO subject = subject();
-        subject.create();
-        subject.write(element.serialize(), seqNum);
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
 
-        byte[] inititalState = subject.dump();
-        subject = subject(inititalState.length, inititalState);
-        subject.open(seqNum, 1);
-        assertThat(subject.getElementCount(), is(equalTo(1)));
-        assertThat(subject.getMinSeqNum(), is(equalTo(seqNum)));
+        byte[] inititalState = io.dump();
+        io = newPageIO(inititalState.length, inititalState);
+        io.recover();
+        assertThat(io.getElementCount(), is(equalTo(1)));
+        assertThat(io.getMinSeqNum(), is(equalTo(seqNum)));
     }
 
     @Test(expected = IOException.class)
-    public void recoversInvalidState() throws IOException {
+    public void openUnexpectedSeqNum() throws IOException {
         Queueable element = new StringElement("foobarbaz");
         long seqNum = 42L;
-        ByteBufferPageIO subject = subject();
-        subject.create();
-        subject.write(element.serialize(), seqNum);
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+
+        byte[] inititalState = io.dump();
+        newPageIO(inititalState.length, inititalState);
+        io.open(1L, 1);
+    }
+
+    @RunWith(Parameterized.class)
+    public static class SingleInvalidElementTest {
+
+        private static final List<BufferGenerator> singleGenerators = Arrays.asList(
+            // invalid length
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes, 1L, 514, io.checksum(bytes));
+                return io.dump();
+            },
+
+            // invalid checksum
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes, 1L, bytes.length, 77);
+                return io.dump();
+            },
+
+            // invalid payload
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                int checksum = io.checksum(bytes);
+                bytes[1] = 0x01;
+                io.write(bytes, 1L, bytes.length, checksum);
+                return io.dump();
+            }
+        );
+
+        @Parameters
+        public static Collection<byte[]> singleElementParameters() {
+            return singleGenerators.stream().map(g -> uncheck(g::generate)).collect(Collectors.toList());
+        }
+
+        @Parameter
+        public byte[] singleElementParameter;
+
+        @Test
+        public void openInvalidSingleElement() throws IOException {
+            // none of these should generate an exception with open()
+
+            ByteBufferPageIO io = newPageIO(singleElementParameter.length, singleElementParameter);
+            io.open(1L, 1);
+
+            assertThat(io.getElementCount(), is(equalTo(1)));
+            assertThat(io.getMinSeqNum(), is(equalTo(1L)));
+        }
+
+        @Test
+        public void recoverInvalidSingleElement() throws IOException {
+            for (BufferGenerator generator : singleGenerators) {
+                byte[] bytes = generator.generate();
+                ByteBufferPageIO io = newPageIO(bytes.length, bytes);
+                io.recover();
 
-        byte[] inititalState = subject.dump();
-        subject(inititalState.length, inititalState);
-        subject.open(1L, 1);
+                assertThat(io.getElementCount(), is(equalTo(0)));
+            }
+        }
     }
 
-    // TODO: add other invalid initial states
+    @RunWith(Parameterized.class)
+    public static class DoubleInvalidElementTest {
+
+        private static final List<BufferGenerator> doubleGenerators = Arrays.asList(
+            // invalid length
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+                io.write(bytes, 2L, 514, io.checksum(bytes));
+                return io.dump();
+            },
+
+            // invalid checksum
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+                io.write(bytes, 2L, bytes.length, 77);
+                return io.dump();
+            },
+
+            // invalid payload
+            () -> {
+                Queueable element = new StringElement("foobarbaz");
+                ByteBufferPageIO io = newEmptyPageIO();
+                byte[] bytes = element.serialize();
+                int checksum = io.checksum(bytes);
+                io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+                bytes[1] = 0x01;
+                io.write(bytes, 2L, bytes.length, checksum);
+                return io.dump();
+            }
+        );
+
+        @Parameters
+        public static Collection<byte[]> doubleElementParameters() {
+            return doubleGenerators.stream().map(g -> uncheck(g::generate)).collect(Collectors.toList());
+        }
+
+        @Parameter
+        public byte[] doubleElementParameter;
+
+        @Test
+        public void openInvalidDoubleElement() throws IOException {
+            // none of these will generate an exception with open()
+
+            ByteBufferPageIO io = newPageIO(doubleElementParameter.length, doubleElementParameter);
+            io.open(1L, 2);
+
+            assertThat(io.getElementCount(), is(equalTo(2)));
+            assertThat(io.getMinSeqNum(), is(equalTo(1L)));
+        }
+
+        @Test
+        public void recoverInvalidDoubleElement() throws IOException {
+            ByteBufferPageIO io = newPageIO(doubleElementParameter.length, doubleElementParameter);
+            io.recover();
+
+            assertThat(io.getElementCount(), is(equalTo(1)));
+         }
+    }
+
+    @Test(expected = AbstractByteBufferPageIO.PageIOInvalidElementException.class)
+    public void openInvalidDeqNumDoubleElement() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        ByteBufferPageIO io = newEmptyPageIO();
+        byte[] bytes = element.serialize();
+        io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+        io.write(bytes, 3L, bytes.length, io.checksum(bytes));
+
+        io = newPageIO(io.dump().length, io.dump());
+        io.open(1L, 2);
+    }
+
+    @Test
+    public void recoverInvalidDeqNumDoubleElement() throws IOException {
+        Queueable element = new StringElement("foobarbaz");
+        ByteBufferPageIO io = newEmptyPageIO();
+        byte[] bytes = element.serialize();
+        io.write(bytes.clone(), 1L, bytes.length, io.checksum(bytes));
+        io.write(bytes, 3L, bytes.length, io.checksum(bytes));
+
+        io = newPageIO(io.dump().length, io.dump());
+        io.recover();
+
+        assertThat(io.getElementCount(), is(equalTo(1)));
+    }
 
     @Test
     public void writeRead() throws IOException {
         long seqNum = 42L;
         Queueable element = buildStringElement("foobarbaz");
-        ByteBufferPageIO subj = subject();
-        subj.create();
-        subj.write(element.serialize(), seqNum);
-        SequencedList<byte[]> result = subj.read(seqNum, 1);
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element.serialize(), seqNum);
+        SequencedList<byte[]> result = io.read(seqNum, 1);
         assertThat(result.getElements().size(), is(equalTo(1)));
         Queueable readElement = StringElement.deserialize(result.getElements().get(0));
         assertThat(result.getSeqNums().get(0), is(equalTo(seqNum)));
@@ -133,14 +321,13 @@ public void writeReadMulti() throws IOException {
         Queueable element2 = buildStringElement("bar");
         Queueable element3 = buildStringElement("baz");
         Queueable element4 = buildStringElement("quux");
-        ByteBufferPageIO subj = subject();
-        subj.create();
-        subj.write(element1.serialize(), 40L);
-        subj.write(element2.serialize(), 41L);
-        subj.write(element3.serialize(), 42L);
-        subj.write(element4.serialize(), 43L);
+        ByteBufferPageIO io = newEmptyPageIO();
+        io.write(element1.serialize(), 40L);
+        io.write(element2.serialize(), 41L);
+        io.write(element3.serialize(), 42L);
+        io.write(element4.serialize(), 43L);
         int batchSize = 11;
-        SequencedList<byte[]> result = subj.read(40L, batchSize);
+        SequencedList<byte[]> result = io.read(40L, batchSize);
         assertThat(result.getElements().size(), is(equalTo(4)));
 
         assertThat(result.getSeqNums().get(0), is(equalTo(40L)));
@@ -153,5 +340,4 @@ public void writeReadMulti() throws IOException {
         assertThat(StringElement.deserialize(result.getElements().get(2)).toString(), is(equalTo(element3.toString())));
         assertThat(StringElement.deserialize(result.getElements().get(3)).toString(), is(equalTo(element4.toString())));
     }
-
 }
\ No newline at end of file
diff --git a/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java b/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
index d2629bd4ba1..843b73713a5 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/MemoryCheckpointTest.java
@@ -2,11 +2,13 @@
 
 import org.junit.Before;
 import org.junit.Test;
+import static org.junit.Assert.fail;
 import org.logstash.ackedqueue.Checkpoint;
 import org.logstash.ackedqueue.MemorySettings;
 import org.logstash.ackedqueue.Settings;
 
 import java.io.IOException;
+import java.nio.file.NoSuchFileException;
 
 import static org.hamcrest.CoreMatchers.equalTo;
 import static org.hamcrest.CoreMatchers.is;
@@ -37,9 +39,8 @@ public void writeNewReadExisting() throws IOException {
         assertThat(checkpoint.getElementCount(), is(equalTo(5)));
     }
 
-    @Test
+    @Test(expected = NoSuchFileException.class)
     public void readInnexisting() throws IOException {
-        Checkpoint checkpoint = io.read("checkpoint.invalid");
-        assertThat(checkpoint, is(equalTo(null)));
+        io.read("checkpoint.invalid");
     }
 }
diff --git a/qa/integration/.gitignore b/qa/integration/.gitignore
index e7775797e47..01f9fbcf19f 100644
--- a/qa/integration/.gitignore
+++ b/qa/integration/.gitignore
@@ -1,2 +1,4 @@
 /services/installed
 /fixtures/certificates
+/fixtures/offline.o
+/fixtures/offline
diff --git a/qa/integration/.ruby-version b/qa/integration/.ruby-version
deleted file mode 100644
index cd53e881769..00000000000
--- a/qa/integration/.ruby-version
+++ /dev/null
@@ -1 +0,0 @@
-jruby-9.1.5.0
diff --git a/qa/integration/fixtures/install_spec.yml b/qa/integration/fixtures/install_spec.yml
new file mode 100644
index 00000000000..cbfc784af81
--- /dev/null
+++ b/qa/integration/fixtures/install_spec.yml
@@ -0,0 +1,3 @@
+---
+services:
+  - logstash
diff --git a/qa/integration/fixtures/logstash-dummy-pack/.gitiginore b/qa/integration/fixtures/logstash-dummy-pack/.gitiginore
new file mode 100644
index 00000000000..0bd659d183a
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/.gitiginore
@@ -0,0 +1,3 @@
+vendor/bundle
+dependencies/
+build/
diff --git a/qa/integration/fixtures/logstash-dummy-pack/Gemfile b/qa/integration/fixtures/logstash-dummy-pack/Gemfile
new file mode 100644
index 00000000000..f40ab78d3d1
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/Gemfile
@@ -0,0 +1,4 @@
+source 'https://rubygems.org'
+gemspec
+
+gem "paquet", :path => "#{File.dirname(__FILE__)}/../../../../tools/paquet"
diff --git a/qa/integration/fixtures/logstash-dummy-pack/Rakefile b/qa/integration/fixtures/logstash-dummy-pack/Rakefile
new file mode 100644
index 00000000000..bb55078e69e
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/Rakefile
@@ -0,0 +1,14 @@
+# encoding: utf-8
+require "paquet"
+
+TARGET_DIRECTORY = File.join(File.dirname(__FILE__), "dependencies")
+
+Paquet::Task.new(TARGET_DIRECTORY) do
+  pack "manticore"
+  pack "gemoji"
+
+  # Everything not defined here will be assumed to be provided
+  # by the target installation
+  ignore "logstash-core-plugin-api"
+  ignore "logstash-core"
+end
diff --git a/qa/integration/fixtures/logstash-dummy-pack/bundle.sh b/qa/integration/fixtures/logstash-dummy-pack/bundle.sh
new file mode 100755
index 00000000000..bc16b58dcfc
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/bundle.sh
@@ -0,0 +1,20 @@
+bundle install --path vendor
+bundle exec rake vendor
+bundle exec rake paquet:vendor
+rm -rf build/
+mkdir -p build/logstash-dummy-pack/logstash/
+cp -r dependencies build/logstash-dummy-pack/logstash/
+gem build logstash-output-secret.gemspec
+mv logstash-output-secret*.gem build/logstash-dummy-pack/logstash/
+
+# Generate stuff for a uber zip
+mkdir -p build/logstash-dummy-pack/elasticsearch
+touch build/logstash-dummy-pack/elasticsearch/README.md
+
+mkdir -p build/logstash-dummy-pack/kibana
+touch build/logstash-dummy-pack/kibana/README.md
+
+cd build/
+zip -r logstash-dummy-pack.zip logstash-dummy-pack
+cp *.zip ../
+cd ..
diff --git a/qa/integration/fixtures/logstash-dummy-pack/lib/logstash/outputs/secret.rb b/qa/integration/fixtures/logstash-dummy-pack/lib/logstash/outputs/secret.rb
new file mode 100644
index 00000000000..ed621ac3259
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/lib/logstash/outputs/secret.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# An secret output that does nothing.
+class LogStash::Outputs::Secret < LogStash::Outputs::Base
+  config_name "secret"
+
+  public
+  def register
+  end # def register
+
+  public
+  def receive(event)
+    return "Event received"
+  end # def event
+end # class LogStash::Outputs::Secret
diff --git a/qa/integration/fixtures/logstash-dummy-pack/logstash-dummy-pack.zip b/qa/integration/fixtures/logstash-dummy-pack/logstash-dummy-pack.zip
new file mode 100644
index 00000000000..f2dbc63c8d6
Binary files /dev/null and b/qa/integration/fixtures/logstash-dummy-pack/logstash-dummy-pack.zip differ
diff --git a/qa/integration/fixtures/logstash-dummy-pack/logstash-output-secret.gemspec b/qa/integration/fixtures/logstash-dummy-pack/logstash-output-secret.gemspec
new file mode 100644
index 00000000000..af9a1e2b192
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/logstash-output-secret.gemspec
@@ -0,0 +1,33 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-output-secret'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'Write a short summary, because Rubygems requires one.'
+  s.description   = 'Write a longer description or delete this line.'
+  s.homepage      = 'https://github.com/ph/secret'
+  s.authors       = ['Pier-Hugues Pellerin']
+  s.email         = 'phpellerin@gmail.com'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "output" }
+  s.post_install_message =<<eos
+This plugins will require the configuration of XXXXX in the logstash.yml
+
+Make sure you double check your configuration
+eos
+
+
+  # Gem dependencies
+  s.add_runtime_dependency "manticore"
+  s.add_runtime_dependency "gemoji", "< 2.0"
+
+
+  s.add_development_dependency "paquet"
+  s.add_development_dependency "rake"
+end
diff --git a/qa/integration/fixtures/logstash-dummy-pack/spec/outputs/secret_spec.rb b/qa/integration/fixtures/logstash-dummy-pack/spec/outputs/secret_spec.rb
new file mode 100644
index 00000000000..341b7a2501d
--- /dev/null
+++ b/qa/integration/fixtures/logstash-dummy-pack/spec/outputs/secret_spec.rb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/outputs/secret"
+require "logstash/codecs/plain"
+require "logstash/event"
+
+describe LogStash::Outputs::Secret do
+  let(:sample_event) { LogStash::Event.new }
+  let(:output) { LogStash::Outputs::Secret.new }
+
+  before do
+    output.register
+  end
+
+  describe "receive message" do
+    subject { output.receive(sample_event) }
+
+    it "returns a string" do
+      expect(subject).to eq("Event received")
+    end
+  end
+end
diff --git a/qa/integration/fixtures/logstash-filter-qatest-0.1.1.gem b/qa/integration/fixtures/logstash-filter-qatest-0.1.1.gem
new file mode 100644
index 00000000000..3dd8ba42d1f
Binary files /dev/null and b/qa/integration/fixtures/logstash-filter-qatest-0.1.1.gem differ
diff --git a/qa/integration/fixtures/offline_wrapper/.gitignore b/qa/integration/fixtures/offline_wrapper/.gitignore
new file mode 100644
index 00000000000..e4d959a4151
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/.gitignore
@@ -0,0 +1 @@
+offine
diff --git a/qa/integration/fixtures/offline_wrapper/Makefile b/qa/integration/fixtures/offline_wrapper/Makefile
new file mode 100644
index 00000000000..448c4e434fe
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/Makefile
@@ -0,0 +1,8 @@
+OBJECTS=offline offline.o
+
+default: offline
+
+clean:
+	rm -f $(OBJECTS)
+offline: offline.o
+	$(CC) -o $@ $<
diff --git a/qa/integration/fixtures/offline_wrapper/README.md b/qa/integration/fixtures/offline_wrapper/README.md
new file mode 100644
index 00000000000..34e35214aa5
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/README.md
@@ -0,0 +1,31 @@
+
+# offline with seccomp
+
+This is a little hack I wrote while trying to see if seccomp could be used to
+help me more easily test programs in "offline mode" without having to actually
+disable networking on my whole laptop.
+
+Building:
+
+```
+make offline
+```
+
+Usage:
+
+```
+./offline <program> [args]
+```
+
+Example:
+
+```
+% nc localhost 10000
+Ncat: Connection refused.
+
+% ./offline nc localhost 10000
+Ncat: Permission denied.
+
+% ./offline host google.com
+host: isc_socket_bind: permission denied
+```
diff --git a/qa/integration/fixtures/offline_wrapper/offline.c b/qa/integration/fixtures/offline_wrapper/offline.c
new file mode 100644
index 00000000000..b335729876d
--- /dev/null
+++ b/qa/integration/fixtures/offline_wrapper/offline.c
@@ -0,0 +1,98 @@
+#include <linux/seccomp.h>
+#include <linux/filter.h>
+#include <sys/syscall.h>
+#include <stddef.h>
+#include <sys/types.h>
+#include <errno.h>
+#include <stdio.h>
+#include <sys/prctl.h>
+#include <unistd.h>
+
+#include <sys/socket.h>
+#include <sys/un.h>
+#include <netinet/in.h>
+
+struct sock_filter reject_connect_and_bind[] = {
+  // LD|W|ABS == Load Word at ABSolute offset
+  // Load the syscall number
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, (offsetof(struct seccomp_data, nr))),
+
+  // JMP|JEQ|K Do a jump after comparing EQuality of the loaded value and a
+  // constant. If equal, jump 2 positions forward, if not equal, do not jump(zero jump).
+  // Is it the `connect` syscall?
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, __NR_connect, 2, 0),
+  // Is it the `bind` syscall?
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, __NR_bind, 1, 0),
+
+  // RET|K Return a constant.
+  // Neither bind nor connect? Allow it.
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+
+  // Fun fact. `connect` and `bind` take the same arguments, so we can process them the same way.
+  // int bind(int sockfd, const struct sockaddr *addr, socklen_t addrlen)
+  // int connect(int sockfd, const struct sockaddr *addr, socklen_t addrlen)
+
+  // Ideally, we'd load the 2nd arg (sockaddr struct) and look at the `sa_family` member to see
+  // what kind of socket address is to be used. However, BPF/seccomp doesn't allow you to 
+  // dereference pointers... so let's try relying on the sockaddr_len argument.
+
+  // Load third argument to the syscall (addrlen)
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, offsetof(struct seccomp_data, args[2])),
+
+  // Try filtering based on the sockaddr len. This isn't great, but may be better than nothing.
+  //   - Reject sockaddr_in and sockaddr_in6
+  //   - Allow everything else (unix sockets, etc)
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, sizeof(struct sockaddr_in), 1, 0),
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, sizeof(struct sockaddr_in6), 0, 1),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ERRNO|(EACCES&SECCOMP_RET_DATA)),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+};
+
+struct sock_filter reject_inet_socket[] = {
+  // LD|W|ABS == Load Word at ABSolute offset
+  // Load the syscall number
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, (offsetof(struct seccomp_data, nr))),
+
+  // Is it the `socket` syscall?
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, __NR_socket, 1, 0),
+  // Not `socket` call? Allow it.
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+
+  // Load first argument to the syscall (domain)
+  BPF_STMT(BPF_LD|BPF_W|BPF_ABS, offsetof(struct seccomp_data, args[0])),
+
+  // Reject INET and INET6
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, AF_INET, 1, 0),
+  BPF_JUMP(BPF_JMP|BPF_JEQ|BPF_K, AF_INET6, 0, 1),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ERRNO|(EACCES&SECCOMP_RET_DATA)),
+  BPF_STMT(BPF_RET|BPF_K, SECCOMP_RET_ALLOW),
+};
+
+int main(int argc, char **argv) {
+  struct sock_filter *filter = reject_inet_socket;
+  unsigned short count = sizeof(reject_inet_socket) / sizeof(filter[0]);
+
+  struct sock_fprog prog = {
+    .len = count,
+    .filter = filter,
+  };
+
+  if (prctl(PR_SET_NO_NEW_PRIVS, 1, 0, 0, 0)) {
+    perror("seccomp PR_SET_NO_NEW_PRIVS");
+    return 1;
+  }
+
+  if (prctl(PR_SET_SECCOMP, SECCOMP_MODE_FILTER, &prog)) {
+    perror("seccomp");
+    return 1;
+  }
+
+  if (argc == 1) {
+    printf("Usage: %s <program> [args]\n", argv[0]);
+    return 1;
+  }
+
+  argv++;
+  execvp(argv[0], argv);
+  return 0;
+}
diff --git a/qa/integration/fixtures/prepare_offline_pack_spec.yml b/qa/integration/fixtures/prepare_offline_pack_spec.yml
new file mode 100644
index 00000000000..cbfc784af81
--- /dev/null
+++ b/qa/integration/fixtures/prepare_offline_pack_spec.yml
@@ -0,0 +1,3 @@
+---
+services:
+  - logstash
diff --git a/qa/integration/framework/helpers.rb b/qa/integration/framework/helpers.rb
index 23a911e091e..5692dec5e0c 100644
--- a/qa/integration/framework/helpers.rb
+++ b/qa/integration/framework/helpers.rb
@@ -1,7 +1,8 @@
 # encoding: utf-8
 # Helper module for all tests
-
 require "flores/random"
+require "fileutils"
+require "zip"
 
 def wait_for_port(port, retry_attempts)
   tries = retry_attempts
@@ -42,4 +43,66 @@ def config_to_temp_file(config)
 def random_port
   # 9600-9700 is reserved in Logstash HTTP server, so we don't want that
   Flores::Random.integer(9701..15000)
-end  
\ No newline at end of file
+end
+
+class Pack
+  PLUGINS_PATH = "logstash"
+  DEPENDENCIES_PATH = File.join("logstash", "dependencies")
+  GEM_EXTENSION = ".gem"
+
+  def initialize(target)
+    @target = target
+  end
+
+  def plugins
+    @plugins ||= extract_gems_data(File.join(@target, PLUGINS_PATH))
+  end
+
+  def dependencies
+    @dependencies ||= extract_gems_data(File.join(@target, DEPENDENCIES_PATH))
+  end
+
+  def glob_gems
+    "*#{GEM_EXTENSION}"
+  end
+
+  def extract_gems_data(path)
+    Dir.glob(File.join(path, glob_gems)).collect { |gem_file| extract_gem_data_from_file(gem_file) }
+  end
+
+  def extract_gem_data_from_file(gem_file)
+    gem = File.basename(gem_file.downcase, GEM_EXTENSION)
+
+    parts = gem.split("-")
+
+    if gem.match(/java/)
+      platform = parts.pop
+      version = parts.pop
+      name = parts.join("-")
+
+      OpenStruct.new(:name => name, :version => version, :platform => platform)
+    else
+      version = parts.pop
+      name = parts.join("-")
+
+      OpenStruct.new(:name => name, :version => version, :platform => nil)
+    end
+  end
+end
+
+def extract(source, target, pattern = nil)
+  raise "Directory #{target} exist" if ::File.exist?(target)
+  Zip::File.open(source) do |zip_file|
+    zip_file.each do |file|
+      path = ::File.join(target, file.name)
+      FileUtils.mkdir_p(::File.dirname(path))
+      zip_file.extract(file, path) if pattern.nil? || pattern =~ file.name
+    end
+  end
+end
+
+def unpack(zip)
+  target = Stud::Temporary.pathname
+  extract(zip, target)
+  Pack.new(target)
+end
diff --git a/qa/integration/integration_tests.gemspec b/qa/integration/integration_tests.gemspec
index 5a44f769538..8a95b4a015a 100644
--- a/qa/integration/integration_tests.gemspec
+++ b/qa/integration/integration_tests.gemspec
@@ -20,4 +20,5 @@ Gem::Specification.new do |s|
   s.add_development_dependency 'pry'
   s.add_development_dependency 'logstash-devutils'
   s.add_development_dependency 'flores'
+  s.add_development_dependency 'rubyzip'
 end
diff --git a/qa/integration/services/kafka_setup.sh b/qa/integration/services/kafka_setup.sh
index 3a3b283300d..def69cf6bfb 100755
--- a/qa/integration/services/kafka_setup.sh
+++ b/qa/integration/services/kafka_setup.sh
@@ -8,7 +8,7 @@ if [ -n "${KAFKA_VERSION+1}" ]; then
     echo "KAFKA_VERSION is $KAFKA_VERSION"
     version=$KAFKA_VERSION
 else
-    version=0.10.0.1
+    version=0.10.1.0
 fi
 
 KAFKA_HOME=$INSTALL_DIR/kafka
diff --git a/qa/integration/services/logstash_service.rb b/qa/integration/services/logstash_service.rb
index b873ab89bbc..cd4e054af26 100644
--- a/qa/integration/services/logstash_service.rb
+++ b/qa/integration/services/logstash_service.rb
@@ -199,6 +199,10 @@ def plugin_cli
     PluginCli.new(@logstash_home)
   end
 
+  def lock_file
+    File.join(@logstash_home, "Gemfile.jruby-1.9.lock")
+  end
+
   class PluginCli
     class ProcessStatus < Struct.new(:exit_code, :stderr_and_stdout); end
 
@@ -209,12 +213,23 @@ class ProcessStatus < Struct.new(:exit_code, :stderr_and_stdout); end
 
     def initialize(logstash_home)
       @logstash_plugin = File.join(logstash_home, LOGSTASH_PLUGIN)
+      @logstash_home = logstash_home
     end
 
     def remove(plugin_name)
       run("remove #{plugin_name}")
     end
 
+    def prepare_offline_pack(plugins, output_zip = nil)
+      plugins = Array(plugins)
+
+      if output_zip.nil?
+        run("prepare-offline-pack #{plugins.join(" ")}")
+      else
+        run("prepare-offline-pack --output #{output_zip} #{plugins.join(" ")}")
+      end
+    end
+
     def list(plugin_name, verbose = false)
       run("list #{plugin_name} #{verbose ? "--verbose" : ""}")
     end
@@ -223,19 +238,33 @@ def install(plugin_name)
       run("install #{plugin_name}")
     end
 
-    def run(command)
+    def run_raw(cmd_parameters, change_dir = true)
       out = Tempfile.new("content")
       out.sync = true
-      process = ChildProcess.build(logstash_plugin,*command.split(" "))
+
+      parts = cmd_parameters.split(" ")
+      cmd = parts.shift
+
+      process = ChildProcess.build(cmd, *parts)
       process.io.stdout = process.io.stderr = out
 
       Bundler.with_clean_env do
-        process.start
+        if change_dir
+          Dir.chdir(@logstash_home) do
+            process.start
+          end
+        else
+          process.start
+        end
       end
 
       process.poll_for_exit(TIMEOUT_MAXIMUM)
       out.rewind
       ProcessStatus.new(process.exit_code, out.read)
     end
+
+    def run(command)
+      run_raw("#{logstash_plugin} #{command}")
+    end
   end
 end
diff --git a/qa/integration/services/service.rb b/qa/integration/services/service.rb
index f2c3525d2f6..63b898d30d9 100644
--- a/qa/integration/services/service.rb
+++ b/qa/integration/services/service.rb
@@ -1,6 +1,8 @@
 # Base class for a service like Kafka, ES, Logstash
 class Service
 
+  attr_reader :settings
+
   def initialize(name, settings)
     @name = name
     @settings = settings
diff --git a/qa/integration/specs/cli/install_spec.rb b/qa/integration/specs/cli/install_spec.rb
new file mode 100644
index 00000000000..b9352e4c87a
--- /dev/null
+++ b/qa/integration/specs/cli/install_spec.rb
@@ -0,0 +1,97 @@
+# encoding: utf-8
+require_relative "../../framework/fixture"
+require_relative "../../framework/settings"
+require_relative "../../services/logstash_service"
+require_relative "../../framework/helpers"
+require "logstash/devutils/rspec/spec_helper"
+require "stud/temporary"
+require "fileutils"
+
+def gem_in_lock_file?(pattern, lock_file)
+  content =  File.read(lock_file)
+  content.match(pattern)
+end
+
+describe "CLI > logstash-plugin install" do
+
+  before(:all) do
+    @fixture = Fixture.new(__FILE__)
+    @logstash = @fixture.get_service("logstash")
+    @logstash_plugin = @logstash.plugin_cli
+    @pack_directory =  File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "logstash-dummy-pack"))
+  end
+
+  shared_examples "install from a pack" do
+    let(:pack) { "file://#{File.join(@pack_directory, "logstash-dummy-pack.zip")}" }
+    let(:install_command) { "bin/logstash-plugin install" }
+    let(:change_dir) { true }
+
+    # When you are on anything by linux we wont disable the internet with seccomp
+    if RbConfig::CONFIG["host_os"] == "linux"
+      context "without internet connection (linux seccomp wrapper)" do
+
+        let(:offline_wrapper_path) { File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "offline_wrapper")) }
+
+        before do
+          Dir.chdir(offline_wrapper_path) do
+            system("make clean")
+            system("make")
+          end
+        end
+
+        let(:offline_wrapper_cmd) { File.join(offline_wrapper_path, "offline") }
+
+        it "successfully install the pack" do
+          execute = @logstash_plugin.run_raw("#{offline_wrapper_cmd} #{install_command} #{pack}", change_dir)
+
+          expect(execute.stderr_and_stdout).to match(/Install successful/)
+          expect(execute.exit_code).to eq(0)
+
+          installed = @logstash_plugin.list("logstash-output-secret")
+          expect(installed.stderr_and_stdout).to match(/logstash-output-secret/)
+
+          expect(gem_in_lock_file?(/gemoji/, @logstash.lock_file)).to be_truthy
+        end
+      end
+    else
+
+      context "with internet connection" do
+        it "successfully install the pack" do
+          execute = @logstash_plugin.run_raw("#{install_command} #{pack}", change_dir)
+
+          expect(execute.stderr_and_stdout).to match(/Install successful/)
+          expect(execute.exit_code).to eq(0)
+
+          installed = @logstash_plugin.list("logstash-output-secret")
+          expect(installed.stderr_and_stdout).to match(/logstash-output-secret/)
+
+          expect(gem_in_lock_file?(/gemoji/, @logstash.lock_file)).to be_truthy
+        end
+      end
+    end
+  end
+
+  context "pack" do
+    context "when the command is run in the `$LOGSTASH_HOME`" do
+      include_examples "install from a pack"
+    end
+
+    context "when the command is run outside of the `$LOGSTASH_HOME`" do
+      include_examples "install from a pack" do
+        let(:change_dir) { false }
+        let(:install_command) { "#{@logstash.logstash_home}/bin/logstash-plugin install" }
+
+        before :all do
+          @current = Dir.pwd
+          tmp = Stud::Temporary.pathname
+          FileUtils.mkdir_p(tmp)
+          Dir.chdir(tmp)
+        end
+
+        after :all do
+          Dir.chdir(@current)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/integration/specs/cli/prepare_offline_pack_spec.rb b/qa/integration/specs/cli/prepare_offline_pack_spec.rb
new file mode 100644
index 00000000000..c4f19f5a8b7
--- /dev/null
+++ b/qa/integration/specs/cli/prepare_offline_pack_spec.rb
@@ -0,0 +1,88 @@
+# encoding: utf-8
+require_relative "../../framework/fixture"
+require_relative "../../framework/settings"
+require_relative "../../services/logstash_service"
+require_relative "../../framework/helpers"
+require "logstash/devutils/rspec/spec_helper"
+
+describe "CLI > logstash-plugin prepare-offline-pack" do
+  before(:all) do
+    @fixture = Fixture.new(__FILE__)
+    @logstash_plugin = @fixture.get_service("logstash").plugin_cli
+  end
+
+  let(:temporary_zip_file) do
+    p = Stud::Temporary.pathname
+    FileUtils.mkdir_p(p)
+    File.join(p, "mypack.zip")
+  end
+
+  context "creating a pack for specific plugins" do
+    let(:plugins_to_pack) { %w(logstash-input-beats logstash-output-elasticsearch) }
+
+    it "successfully create a pack" do
+      execute = @logstash_plugin.prepare_offline_pack(plugins_to_pack, temporary_zip_file)
+
+      expect(execute.exit_code).to eq(0)
+      expect(execute.stderr_and_stdout).to match(/Offline package created at/)
+      expect(execute.stderr_and_stdout).to match(/#{temporary_zip_file}/)
+
+      unpacked = unpack(temporary_zip_file)
+
+      expect(unpacked.plugins.collect(&:name)).to include(*plugins_to_pack)
+      expect(unpacked.plugins.size).to eq(2)
+
+      expect(unpacked.dependencies.size).to be > 0
+    end
+  end
+
+  context "create a pack from a wildcard" do
+    let(:plugins_to_pack) { %w(logstash-filter-*) }
+
+    it "successfully create a pack" do
+      execute = @logstash_plugin.prepare_offline_pack(plugins_to_pack, temporary_zip_file)
+
+      expect(execute.exit_code).to eq(0)
+      expect(execute.stderr_and_stdout).to match(/Offline package created at/)
+      expect(execute.stderr_and_stdout).to match(/#{temporary_zip_file}/)
+
+      unpacked = unpack(temporary_zip_file)
+
+      filters = @logstash_plugin.list(plugins_to_pack.first).stderr_and_stdout.split("\n")
+
+      expect(unpacked.plugins.collect(&:name)).to include(*filters)
+      expect(unpacked.plugins.size).to eq(filters.size)
+
+      expect(unpacked.dependencies.size).to be > 0
+    end
+  end
+
+  context "create a pack with a locally installed .gem" do
+    let(:plugin_to_pack) { "logstash-filter-qatest" }
+
+    before do
+      @logstash_plugin.install(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "logstash-filter-qatest-0.1.1.gem"))
+
+      # assert that the plugins is correctly installed
+      execute = @logstash_plugin.list(plugin_to_pack)
+
+      expect(execute.stderr_and_stdout).to match(/#{plugin_to_pack}/)
+      expect(execute.exit_code).to eq(0)
+    end
+
+    it "successfully create a pack" do
+      execute = @logstash_plugin.prepare_offline_pack(plugin_to_pack, temporary_zip_file)
+
+      expect(execute.stderr_and_stdout).to match(/Offline package created at/)
+      expect(execute.stderr_and_stdout).to match(/#{temporary_zip_file}/)
+      expect(execute.exit_code).to eq(0)
+
+      unpacked = unpack(temporary_zip_file)
+
+      expect(unpacked.plugins.collect(&:name)).to include(plugin_to_pack)
+      expect(unpacked.plugins.size).to eq(1)
+
+      expect(unpacked.dependencies.size).to eq(0)
+    end
+  end
+end
diff --git a/qa/integration/specs/cli/remove_spec.rb b/qa/integration/specs/cli/remove_spec.rb
index f390b61d00d..95981265d5a 100644
--- a/qa/integration/specs/cli/remove_spec.rb
+++ b/qa/integration/specs/cli/remove_spec.rb
@@ -5,40 +5,87 @@
 require_relative '../../framework/helpers'
 require "logstash/devutils/rspec/spec_helper"
 
-describe "Test removal of plugin" do
+describe "CLI > logstash-plugin remove" do
   before(:all) do
     @fixture = Fixture.new(__FILE__)
     @logstash_plugin = @fixture.get_service("logstash").plugin_cli
   end
 
-  context "when no other plugins depends on this plugin" do
-    it "successfully remove the plugin" do
-      execute = @logstash_plugin.remove("logstash-input-twitter")
+    if RbConfig::CONFIG["host_os"] == "linux"
+      context "without internet connection (linux seccomp wrapper)" do
 
-      expect(execute.exit_code).to eq(0)
-      expect(execute.stderr_and_stdout).to match(/Successfully removed logstash-input-twitter/)
+        let(:offline_wrapper_path) { File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "fixtures", "offline_wrapper")) }
+        let(:offline_wrapper_cmd) { File.join(offline_wrapper_path, "offline") }
 
-      presence_check = @logstash_plugin.list("logstash-input-twitter")
-      expect(presence_check.exit_code).to eq(1)
-      expect(presence_check.stderr_and_stdout).to match(/ERROR: No plugins found/)
+        before do
+          Dir.chdir(offline_wrapper_path) do
+            system("make clean")
+            system("make")
+          end
+        end
 
-      @logstash_plugin.install("logstash-input-twitter")
-    end
-  end
+        context "when no other plugins depends on this plugin" do
+          it "successfully remove the plugin" do
+            execute = @logstash_plugin.run_raw("#{offline_wrapper_cmd} bin/logstash-plugin remove logstash-input-twitter")
+
+            expect(execute.exit_code).to eq(0)
+            expect(execute.stderr_and_stdout).to match(/Successfully removed logstash-input-twitter/)
+
+            presence_check = @logstash_plugin.list("logstash-input-twitter")
+            expect(presence_check.exit_code).to eq(1)
+            expect(presence_check.stderr_and_stdout).to match(/ERROR: No plugins found/)
+
+            @logstash_plugin.install("logstash-input-twitter")
+          end
+        end
+
+        context "when other plugins depends on this plugin" do
+          it "refuses to remove the plugin and display the plugin that depends on it." do
+            execute = @logstash_plugin.run_raw("#{offline_wrapper_cmd} bin/logstash-plugin remove logstash-codec-json")
+
+            expect(execute.exit_code).to eq(1)
+            expect(execute.stderr_and_stdout).to match(/Failed to remove "logstash-codec-json"/)
+            expect(execute.stderr_and_stdout).to match(/logstash-input-beats/) # one of the dependency
+            expect(execute.stderr_and_stdout).to match(/logstash-output-udp/) # one of the dependency
 
-  context "when other plugins depends on this plugin" do
-    it "refuses to remove the plugin and display the plugin that depends on it." do
-      execute = @logstash_plugin.remove("logstash-codec-json")
+            presence_check = @logstash_plugin.list("logstash-codec-json")
 
-      expect(execute.exit_code).to eq(1)
-      expect(execute.stderr_and_stdout).to match(/Failed to remove "logstash-codec-json"/)
-      expect(execute.stderr_and_stdout).to match(/logstash-input-beats/) # one of the dependency
-      expect(execute.stderr_and_stdout).to match(/logstash-output-udp/) # one of the dependency
+            expect(presence_check.exit_code).to eq(0)
+            expect(presence_check.stderr_and_stdout).to match(/logstash-codec-json/)
+          end
+        end
 
-      presence_check = @logstash_plugin.list("logstash-codec-json")
+      end
+    else
+      context "when no other plugins depends on this plugin" do
+        it "successfully remove the plugin" do
+          execute = @logstash_plugin.remove("logstash-input-twitter")
 
-      expect(presence_check.exit_code).to eq(0)
-      expect(presence_check.stderr_and_stdout).to match(/logstash-codec-json/)
+          expect(execute.exit_code).to eq(0)
+          expect(execute.stderr_and_stdout).to match(/Successfully removed logstash-input-twitter/)
+
+          presence_check = @logstash_plugin.list("logstash-input-twitter")
+          expect(presence_check.exit_code).to eq(1)
+          expect(presence_check.stderr_and_stdout).to match(/ERROR: No plugins found/)
+
+          @logstash_plugin.install("logstash-input-twitter")
+        end
+      end
+
+      context "when other plugins depends on this plugin" do
+        it "refuses to remove the plugin and display the plugin that depends on it." do
+          execute = @logstash_plugin.remove("logstash-codec-json")
+
+          expect(execute.exit_code).to eq(1)
+          expect(execute.stderr_and_stdout).to match(/Failed to remove "logstash-codec-json"/)
+          expect(execute.stderr_and_stdout).to match(/logstash-input-beats/) # one of the dependency
+          expect(execute.stderr_and_stdout).to match(/logstash-output-udp/) # one of the dependency
+
+          presence_check = @logstash_plugin.list("logstash-codec-json")
+
+          expect(presence_check.exit_code).to eq(0)
+          expect(presence_check.stderr_and_stdout).to match(/logstash-codec-json/)
+        end
+      end
     end
-  end
 end
diff --git a/qa/integration/specs/monitoring_api_spec.rb b/qa/integration/specs/monitoring_api_spec.rb
index f9045c49d2c..69b7a3a7517 100644
--- a/qa/integration/specs/monitoring_api_spec.rb
+++ b/qa/integration/specs/monitoring_api_spec.rb
@@ -47,4 +47,26 @@
     end
   end
 
+  it "can retrieve queue stats" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_with_stdin
+    logstash_service.wait_for_logstash
+
+    Stud.try(max_retry.times, RSpec::Expectations::ExpectationNotMetError) do
+      result = logstash_service.monitoring_api.node_stats
+      expect(result["pipeline"]["queue"]).not_to be_nil
+      if logstash_service.settings.feature_flag == "persistent_queues"
+        expect(result["pipeline"]["queue"]["type"]).to eq "persisted"
+        expect(result["pipeline"]["queue"]["data"]["free_space_in_bytes"]).not_to be_nil
+        expect(result["pipeline"]["queue"]["data"]["storage_type"]).not_to be_nil
+        expect(result["pipeline"]["queue"]["data"]["path"]).not_to be_nil
+        expect(result["pipeline"]["queue"]["events"]).not_to be_nil
+        expect(result["pipeline"]["queue"]["capacity"]["page_capacity_in_bytes"]).not_to be_nil
+        expect(result["pipeline"]["queue"]["capacity"]["max_queue_size_in_bytes"]).not_to be_nil
+        expect(result["pipeline"]["queue"]["capacity"]["max_unread_events"]).not_to be_nil
+      else
+        expect(result["pipeline"]["queue"]["type"]).to eq "memory"
+      end
+    end
+  end
 end
diff --git a/qa/integration/specs/reload_config_spec.rb b/qa/integration/specs/reload_config_spec.rb
index 530e75a2a7f..24f37aba942 100644
--- a/qa/integration/specs/reload_config_spec.rb
+++ b/qa/integration/specs/reload_config_spec.rb
@@ -64,11 +64,14 @@
     
     # check reload stats
     reload_stats = logstash_service.monitoring_api.pipeline_stats["reloads"]
+    instance_reload_stats = logstash_service.monitoring_api.node_stats["reloads"]
     expect(reload_stats["successes"]).to eq(1)
     expect(reload_stats["failures"]).to eq(0)
     expect(reload_stats["last_success_timestamp"].blank?).to be false
     expect(reload_stats["last_error"]).to eq(nil)
     
+    expect(instance_reload_stats["successes"]).to eq(1)
+    expect(instance_reload_stats["failures"]).to eq(0)
     # parse the results and validate
     re = JSON.load(File.new(output_file2))
     expect(re["clientip"]).to eq("74.125.176.147")
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-0.1.1.gem b/qa/support/logstash-filter-qatest/logstash-filter-qatest-0.1.1.gem
new file mode 100644
index 00000000000..3dd8ba42d1f
Binary files /dev/null and b/qa/support/logstash-filter-qatest/logstash-filter-qatest-0.1.1.gem differ
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index 27ebdc946d0..1937bf4e262 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -60,6 +60,7 @@ namespace "artifact" do
     @exclude_paths << "bin/bundle"
     @exclude_paths << "bin/rspec"
     @exclude_paths << "bin/rspec.bat"
+    @exclude_paths << "bin/lock"
 
     @exclude_paths
   end
@@ -84,25 +85,25 @@ namespace "artifact" do
   task "all" => ["prepare", "build"]
 
   desc "Build a tar.gz of default logstash plugins with all dependencies"
-  task "tar" => ["prepare", "generate_build_metadata"] do
+  task "tar" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
     puts("[artifact:tar] Building tar.gz of default plugins")
     build_tar
   end
 
   desc "Build a zip of default logstash plugins with all dependencies"
-  task "zip" => ["prepare", "generate_build_metadata"] do
+  task "zip" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
     puts("[artifact:zip] Building zip of default plugins")
     build_zip
   end
 
   desc "Build an RPM of logstash with all dependencies"
-  task "rpm" => ["prepare", "generate_build_metadata"] do
+  task "rpm" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
     puts("[artifact:rpm] building rpm package")
     package("centos", "5")
   end
 
   desc "Build a DEB of logstash with all dependencies"
-  task "deb" => ["prepare", "generate_build_metadata"] do
+  task "deb" => ["prepare", "generate_build_metadata", "license:generate-notice-file"] do
     puts("[artifact:deb] building deb package")
     package("ubuntu", "12.04")
   end
@@ -112,6 +113,7 @@ namespace "artifact" do
     Rake::Task["artifact:build-logstash-core"].invoke
     Rake::Task["artifact:build-logstash-core-event"].invoke
     Rake::Task["artifact:build-logstash-core-plugin-api"].invoke
+    Rake::Task["artifact:build-logstash-core-queue-jruby"].invoke
   end
 
   # "all-plugins" version of tasks
@@ -220,6 +222,24 @@ namespace "artifact" do
     end
   end
 
+  task "build-logstash-core-queue-jruby" do
+    # regex which matches a Gemfile gem definition for the logstash-core gem and captures the :path option
+    gem_line_regex = /^\s*gem\s+["']logstash-core-queue-jruby["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
+
+    lines = File.readlines("Gemfile")
+    matches = lines.select{|line| line[gem_line_regex]}
+    abort("ERROR: Gemfile format error, need a single logstash-core-queue-jruby gem specification") if matches.size != 1
+
+    path = matches.first[gem_line_regex, 1]
+
+    if path
+      Rake::Task["plugin:build-local-core-gem"].invoke("logstash-core-queue-jruby", path)
+    else
+      puts "The Gemfile should reference \"logstash-core-queue-jruby\" gem locally through :path, but found instead: #{matches}"
+      exit(1)
+    end
+  end
+
   task "prepare" do
     if ENV['SKIP_PREPARE'] != "1"
       ["bootstrap", "plugin:install-default", "artifact:clean-bundle-config"].each {|task| Rake::Task[task].invoke }
@@ -432,5 +452,4 @@ namespace "artifact" do
       out.cleanup
     end
   end # def package
-
 end
diff --git a/rakelib/dependency.rake b/rakelib/dependency.rake
index 56471a99640..323e76ff275 100644
--- a/rakelib/dependency.rake
+++ b/rakelib/dependency.rake
@@ -9,7 +9,7 @@ namespace "dependency" do
   end # task rbx-stdlib
 
   task "archive-tar-minitar" do
-    Rake::Task["gem:require"].invoke("minitar", ">= 0")
+    Rake::Task["gem:require"].invoke("minitar", "0.5.4")
   end # task archive-minitar
 
   task "stud" do
diff --git a/rakelib/license.rake b/rakelib/license.rake
new file mode 100644
index 00000000000..fb6ba92be7a
--- /dev/null
+++ b/rakelib/license.rake
@@ -0,0 +1,84 @@
+require "rubygems/specification"
+require "bootstrap/environment"
+require_relative "default_plugins"
+require 'rubygems'
+
+namespace "license" do
+
+  SKIPPED_DEPENDENCIES = [
+    "logstash-core-plugin-api"
+  ]
+
+  GEM_INSTALL_PATH = File.join(LogStash::Environment.logstash_gem_home, "gems")
+
+  NOTICE_FILE_PATH = File.join(LogStash::Environment::LOGSTASH_HOME, "NOTICE.TXT")
+
+  desc "Generate a license/notice file for default plugin dependencies"
+  task "generate-notice-file" => ["bootstrap", "plugin:install-default"] do
+    puts("[license:generate-notice-file] Generating notice file for default plugin dependencies")
+    generate_notice_file
+  end
+
+  def generate_notice_file
+    File.open(NOTICE_FILE_PATH,'w') do |file|
+      begin
+        add_logstash_header(file)
+        add_dependencies_licenses(file)
+      rescue => e
+        raise "Unable to generate notice file, #{e}"
+      end
+    end
+  end
+
+  def add_logstash_header(notice_file)
+    copyright_year = Time.now.year
+    notice_file << "Logstash\n"
+    notice_file << "Copyright 2012-#{copyright_year} Elasticsearch\n"
+    notice_file << "\nThis product includes software developed by The Apache Software Foundation (http://www.apache.org/).\n"
+    notice_file << "\n==========================================================================\n"
+    notice_file << "Third party libraries bundled by the Logstash project:\n\n"
+  end
+
+  def add_dependencies_licenses(notice_file)
+    # to keep track of all the plugins we've traversed
+    seen_dependencies = Hash.new
+    LogStash::RakeLib::DEFAULT_PLUGINS.each do |plugin|
+      gemspec = Gem::Specification.find_all_by_name(plugin)[0]
+      gemspec.runtime_dependencies.each do |dep|
+        name = dep.name
+        next if SKIPPED_DEPENDENCIES.include?(name) || seen_dependencies.key?(name)
+        seen_dependencies[name] = true
+        # ignore all the runtime logstash-* plugin dependencies
+        next if name.start_with?("logstash")
+        path = gem_home(dep.to_spec)
+        dep.to_spec.licenses.each do |license|
+          notice = ""
+          license = ""
+          Dir.glob(File.join(path, '*LICENSE*')) do |path|
+            notice << File.read(path)
+            notice << "\n"
+          end
+          Dir.glob(File.join(path, '*NOTICE*')) do |path|
+            license << File.read(path)
+            license << "\n"
+          end
+
+          if !notice.empty? || !license.empty?
+            notice_file << "==========================================================================\n"
+            notice_file << "RubyGem: #{name} Version: #{dep.to_spec.version}\n"
+            notice_file << notice
+            notice_file << license
+          end
+        end
+      end
+    end
+  end
+
+  def gem_home(spec)
+    spec_base_name = "#{spec.name}-#{spec.version}"
+    if spec.platform == "java"
+      spec_base_name += "-java"
+    end
+    File.join(GEM_INSTALL_PATH, "#{spec_base_name}")
+  end
+end
diff --git a/spec/unit/license_spec.rb b/spec/unit/license_spec.rb
index 88fe74d4ba3..fdc5ea80065 100644
--- a/spec/unit/license_spec.rb
+++ b/spec/unit/license_spec.rb
@@ -15,7 +15,9 @@
                    /artistic 2.*/,
                    /ruby/,
                    /lgpl/,
-                   /epl/])
+                   /epl/,
+                   /elastic/i
+  ])
   }
 
   ##
diff --git a/spec/unit/plugin_manager/offline_plugin_packager_spec.rb b/spec/unit/plugin_manager/offline_plugin_packager_spec.rb
new file mode 100644
index 00000000000..feca32dfaff
--- /dev/null
+++ b/spec/unit/plugin_manager/offline_plugin_packager_spec.rb
@@ -0,0 +1,132 @@
+# encoding: utf-8
+require "pluginmanager/offline_plugin_packager"
+require "stud/temporary"
+require "bootstrap/util/compress"
+require "fileutils"
+require "spec_helper"
+
+def retrieve_packaged_plugins(path)
+  Dir.glob(::File.join(path, "logstash", "*.gem"))
+end
+
+def retrieve_dependencies_gems(path)
+  Dir.glob(::File.join(path, "logstash", "dependencies", "*.gem"))
+end
+
+describe LogStash::PluginManager::SpecificationHelpers do
+  subject { described_class }
+
+  context "when it find gems" do
+    it "returns filtered results" do
+      expect(subject.find_by_name_with_wildcards("logstash-filter-*").all? { |spec| spec.name =~ /logstash-filter-/ }).to be_truthy
+    end
+  end
+
+  context "when it doesn't find gems" do
+    it "doesnt return gemspecs" do
+      expect(subject.find_by_name_with_wildcards("donotexistatall").size).to eq(0)
+    end
+  end
+end
+
+describe LogStash::PluginManager::OfflinePluginPackager do
+  before do
+    WebMock.allow_net_connect!
+  end
+
+  subject { described_class }
+
+  let(:temporary_dir) { Stud::Temporary.pathname }
+  let(:target) { ::File.join(temporary_dir, "my-pack.zip")}
+  let(:extract_to) { Stud::Temporary.pathname }
+
+  context "when the plugins doesn't" do
+    let(:plugins_args) { "idotnotexist" }
+
+    it "raise an exception" do
+      expect { subject.package(plugins_args, target) }.to raise_error(LogStash::PluginManager::PluginNotFoundError)
+    end
+  end
+
+  context "when the plugins is a core gem" do
+    %W(
+    logstash-core
+    logstash-core-event-java
+    logstash-core-plugin-api
+    logstash-core-queue-jruby).each do |plugin_name|
+      it "raise an exception with plugin: #{plugin_name}" do
+        expect { subject.package(plugin_name, target) }.to raise_error(LogStash::PluginManager::UnpackablePluginError)
+      end
+    end
+  end
+
+  context "when the plugins exist" do
+    before :all do
+      Paquet.ui = Paquet::SilentUI
+    end
+
+    before do
+      FileUtils.mkdir_p(temporary_dir)
+
+      subject.package(plugins_args, target)
+      LogStash::Util::Zip.extract(target, extract_to)
+    end
+
+    context "one plugin specified" do
+      let(:plugins_args) { ["logstash-input-stdin"] }
+
+      it "creates a pack with the plugin" do
+        expect(retrieve_packaged_plugins(extract_to).size).to eq(1)
+        expect(retrieve_packaged_plugins(extract_to)).to include(/logstash-input-stdin/)
+        expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+      end
+    end
+
+    context "multiples plugins" do
+      let(:plugins_args) { ["logstash-input-stdin", "logstash-input-beats"] }
+
+      it "creates pack with the plugins" do
+        expect(retrieve_packaged_plugins(extract_to).size).to eq(2)
+
+        plugins_args.each do |plugin_name|
+          expect(retrieve_packaged_plugins(extract_to)).to include(/#{plugin_name}/)
+        end
+
+        expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+      end
+    end
+
+    context "with wildcards" do
+      let(:plugins_args) { ["logstash-filter-*"] }
+
+      it "creates a pack with the plugins" do
+        expect(retrieve_packaged_plugins(extract_to).size).to eq(LogStash::PluginManager::SpecificationHelpers.find_by_name_with_wildcards(plugins_args.first).size)
+
+        retrieve_packaged_plugins(extract_to).each do |gem_file|
+          expect(gem_file).to match(/logstash-filter-.+/)
+        end
+
+        expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+      end
+    end
+
+    context "with wildcards and normal plugins" do
+      let(:plugins_args) { ["logstash-filter-*", "logstash-input-beats"] }
+
+      it "creates a pack with the plugins" do
+        groups = retrieve_packaged_plugins(extract_to).group_by { |gem_file| ::File.basename(gem_file).split("-")[1] }
+
+        expect(groups["filter"].size).to eq(LogStash::PluginManager::SpecificationHelpers.find_by_name_with_wildcards(plugins_args.first).size)
+
+        groups["filter"].each do |gem_file|
+          expect(gem_file).to match(/logstash-filter-.+/)
+        end
+
+        expect(groups["input"].size).to eq(1)
+        expect(groups["input"]).to include(/logstash-input-beats/)
+
+        expect(retrieve_dependencies_gems(extract_to).size).to be > 0
+      end
+    end
+  end
+end
diff --git a/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb b/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
index 395f1c0b816..dd636f37143 100644
--- a/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
+++ b/spec/unit/plugin_manager/pack_fetch_strategy/repository_spec.rb
@@ -11,7 +11,7 @@
 
   context "#plugin_uri" do
     it "generate an url from a name" do
-      matched = URI.parse("#{subject::ELASTIC_PACK_BASE_URI}/#{plugin_name}/#{plugin_name}-#{LOGSTASH_VERSION}.#{subject::PACK_EXTENSION}")
+      matched = URI.parse("#{subject.elastic_pack_base_uri}/#{plugin_name}/#{plugin_name}-#{LOGSTASH_VERSION}.#{subject::PACK_EXTENSION}")
       expect(subject.pack_uri(plugin_name)).to eq(matched)
     end
   end
@@ -35,8 +35,43 @@
       # To make sure we really try to connect to a failling host we have to let it through webmock
       host ="#{Time.now.to_i.to_s}-do-not-exist.com"
       WebMock.disable_net_connect!(:allow => host)
-      stub_const("LogStash::PluginManager::PackFetchStrategy::Repository::ELASTIC_PACK_BASE_URI", "http://#{host}")
+      ENV["LOGSTASH_PACK_URL"] = "http://#{host}"
       expect(subject.get_installer_for(plugin_name)).to be_falsey
+      ENV["LOGSTASH_PACK_URL"] = nil
+    end
+  end
+
+  context "pack repository url" do
+    context "when `LOGSTASH_PACK_URL` is set in ENV" do
+      before do
+        ENV["LOGSTASH_PACK_URL"] = url
+      end
+
+      after do
+        ENV.delete("LOGSTASH_PACK_URL")
+      end
+
+      context "value is a string" do
+        let(:url) { "http://testing.dev" }
+
+        it "return the configured string" do
+          expect(subject.elastic_pack_base_uri).to eq(url)
+        end
+      end
+
+      context "value is an empty string" do
+        let(:url) { "" }
+
+        it "return the default" do
+          expect(subject.elastic_pack_base_uri).to eq(subject::DEFAULT_PACK_URL)
+        end
+      end
+    end
+
+    context "when `LOGSTASH_PACK_URL` is not set in ENV" do
+      it "return the default" do
+        expect(subject.elastic_pack_base_uri).to eq(subject::DEFAULT_PACK_URL)
+      end
     end
   end
 end
diff --git a/spec/unit/plugin_manager/pack_installer/local_spec.rb b/spec/unit/plugin_manager/pack_installer/local_spec.rb
index 9fb799cf956..b99712a4c07 100644
--- a/spec/unit/plugin_manager/pack_installer/local_spec.rb
+++ b/spec/unit/plugin_manager/pack_installer/local_spec.rb
@@ -49,7 +49,7 @@
       let(:local_file) { ::File.join(::File.dirname(__FILE__), "..", "..", "..", "support", "pack", "valid-pack.zip") }
 
       it "install the gems" do
-        expect(::Bundler::LogstashInjector).to receive(:inject!).with(be_kind_of(Array)).and_return([])
+        expect(::Bundler::LogstashInjector).to receive(:inject!).with(be_kind_of(LogStash::PluginManager::PackInstaller::Pack)).and_return([])
 
         expect(::LogStash::PluginManager::GemInstaller).to receive(:install).with(/logstash-input-packtest/, anything)
         expect(::LogStash::PluginManager::GemInstaller).to receive(:install).with(/logstash-input-packtestdep/, anything)
diff --git a/spec/unit/plugin_manager/prepare_offline_pack_spec.rb b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
new file mode 100644
index 00000000000..bcda5993c02
--- /dev/null
+++ b/spec/unit/plugin_manager/prepare_offline_pack_spec.rb
@@ -0,0 +1,99 @@
+# encoding: utf-8
+require "spec_helper"
+require "pluginmanager/main"
+require "pluginmanager/prepare_offline_pack"
+require "pluginmanager/offline_plugin_packager"
+require "stud/temporary"
+require "fileutils"
+
+# This Test only handle the interaction with the OfflinePluginPackager class
+# any test for bundler will need to be done as rats test
+describe LogStash::PluginManager::PrepareOfflinePack do
+  before do
+    WebMock.allow_net_connect!
+  end
+
+  subject { described_class.new(cmd, {}) }
+
+  let(:temporary_dir) { Stud::Temporary.pathname }
+  let(:tmp_zip_file) { ::File.join(temporary_dir, "myspecial.zip") }
+  let(:offline_plugin_packager) { double("offline_plugin_packager") }
+  let(:cmd_args) { ["--output", tmp_zip_file, "logstash-input-stdin"] }
+  let(:cmd) { "install" }
+
+  before do
+    FileUtils.mkdir_p(temporary_dir)
+
+    allow(LogStash::Bundler).to receive(:invoke!).and_return(nil)
+    allow(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, anything).and_return(offline_plugin_packager)
+  end
+
+  context "when not debbuging" do
+    before do
+      @before_debug_value = ENV["DEBUG"]
+      ENV["DEBUG"] = nil
+    end
+
+    after do
+      ENV["DEBUG"] = @before_debug_value
+    end
+
+    it "silences paquet ui reporter" do
+      expect(Paquet).to receive(:ui=).with(Paquet::SilentUI)
+      subject.run(cmd_args)
+    end
+
+    context "when file target already exist" do
+      before do
+        FileUtils.touch(tmp_zip_file)
+      end
+
+      it "overrides the file" do
+        expect(FileUtils).to receive(:rm_rf).with(tmp_zip_file)
+        subject.run(cmd_args)
+      end
+    end
+
+    context "when trying to use a core gem" do
+      let(:exception) { LogStash::PluginManager::UnpackablePluginError }
+
+      before do
+        allow(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, anything).and_raise(exception)
+      end
+
+      it "catches the error" do
+        expect(subject).to receive(:report_exception).with("Offline package", be_kind_of(exception)).and_return(nil)
+        subject.run(cmd_args)
+      end
+    end
+
+    context "when trying to pack a plugin that doesnt exist" do
+      let(:exception) { LogStash::PluginManager::PluginNotFoundError }
+
+      before do
+        allow(LogStash::PluginManager::OfflinePluginPackager).to receive(:package).with(anything, anything).and_raise(exception)
+      end
+
+      it "catches the error" do
+        expect(subject).to receive(:report_exception).with("Cannot create the offline archive", be_kind_of(exception)).and_return(nil)
+        subject.run(cmd_args)
+      end
+    end
+  end
+
+  context "when debugging" do
+    before do
+      @before_debug_value = ENV["DEBUG"]
+      ENV["DEBUG"] = "1"
+    end
+
+    after do
+      ENV["DEBUG"] = @before_debug_value
+    end
+
+    it "doesn't silence paquet ui reporter" do
+      expect(Paquet).not_to receive(:ui=).with(Paquet::SilentUI)
+      subject.run(cmd_args)
+    end
+  end
+end
diff --git a/tools/logstash-docgen/templates/index-codecs.asciidoc.erb b/tools/logstash-docgen/templates/index-codecs.asciidoc.erb
index 3cc116a4b98..af0659be09b 100644
--- a/tools/logstash-docgen/templates/index-codecs.asciidoc.erb
+++ b/tools/logstash-docgen/templates/index-codecs.asciidoc.erb
@@ -1,7 +1,7 @@
 [[codec-plugins]]
 == Codec plugins
 
-A codec plugin changes the data representation of an event. Codecs are essentially stream filters that can operate as part 
+A codec plugin changes the data representation of an event. Codecs are essentially stream filters that can operate as part
 of an input or output.
 
 The following codec plugins are available:
@@ -14,8 +14,8 @@ The following codec plugins are available:
 |=======================================================================
 
 <% plugins.each do |plugin| %>
-pass::[<?edit_url <%=plugin.edit_url%> ?>]
+:edit_url: <%=plugin.edit_url%>
 include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
 <% end %>
 
-pass::[<?edit_url?>]
+:edit_url:
diff --git a/tools/logstash-docgen/templates/index-filters.asciidoc.erb b/tools/logstash-docgen/templates/index-filters.asciidoc.erb
index f5e82b253c8..47340c5c272 100644
--- a/tools/logstash-docgen/templates/index-filters.asciidoc.erb
+++ b/tools/logstash-docgen/templates/index-filters.asciidoc.erb
@@ -1,7 +1,7 @@
 [[filter-plugins]]
 == Filter plugins
 
-A filter plugin performs intermediary processing on an event. Filters are often applied conditionally depending on the 
+A filter plugin performs intermediary processing on an event. Filters are often applied conditionally depending on the
 characteristics of the event.
 
 The following filter plugins are available:
@@ -14,8 +14,8 @@ The following filter plugins are available:
 |=======================================================================
 
 <% plugins.each do |plugin| %>
-pass::[<?edit_url <%=plugin.edit_url%> ?>]
+:edit_url: <%=plugin.edit_url%>
 include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
 <% end %>
 
-pass::[<?edit_url?>]
+:edit_url:
diff --git a/tools/logstash-docgen/templates/index-inputs.asciidoc.erb b/tools/logstash-docgen/templates/index-inputs.asciidoc.erb
index 20e70ae6901..ecc477537d1 100644
--- a/tools/logstash-docgen/templates/index-inputs.asciidoc.erb
+++ b/tools/logstash-docgen/templates/index-inputs.asciidoc.erb
@@ -13,8 +13,8 @@ The following input plugins are available:
 |=======================================================================
 
 <% plugins.each do |plugin| %>
-pass::[<?edit_url <%=plugin.edit_url%> ?>]
+:edit_url: <%=plugin.edit_url%>
 include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
 <% end %>
 
-pass::[<?edit_url?>]
+:edit_url:
diff --git a/tools/logstash-docgen/templates/index-outputs.asciidoc.erb b/tools/logstash-docgen/templates/index-outputs.asciidoc.erb
index b64ff85842a..6d6d241bab6 100644
--- a/tools/logstash-docgen/templates/index-outputs.asciidoc.erb
+++ b/tools/logstash-docgen/templates/index-outputs.asciidoc.erb
@@ -13,8 +13,8 @@ The following output plugins are available:
 |=======================================================================
 
 <% plugins.each do |plugin| %>
-pass::[<?edit_url <%=plugin.edit_url%> ?>]
+:edit_url: <%=plugin.edit_url%>
 include::<%=plugin.type%>/<%=plugin.name%>.asciidoc[]
 <% end %>
 
-pass::[<?edit_url?>]
+:edit_url:
diff --git a/tools/logstash-docgen/templates/plugin-doc.asciidoc.erb b/tools/logstash-docgen/templates/plugin-doc.asciidoc.erb
index 999b52ea4cd..a9c9933dc5d 100644
--- a/tools/logstash-docgen/templates/plugin-doc.asciidoc.erb
+++ b/tools/logstash-docgen/templates/plugin-doc.asciidoc.erb
@@ -4,7 +4,6 @@
 Version: <%=version%>
 Released on: <%=release_date%>
 <%=changelog_url%>[Changelog]
-Compatible: <%= supported_logstash %>
 
 <% unless default_plugin? %>
 NOTE: This plugin does not ship with Logstash by default, but it is easy to install by running `bin/logstash-plugin install logstash-<%= section %>-<%= name %>`.
diff --git a/tools/paquet/.gitignore b/tools/paquet/.gitignore
new file mode 100644
index 00000000000..ebcca5e3bda
--- /dev/null
+++ b/tools/paquet/.gitignore
@@ -0,0 +1,6 @@
+/.bundle/
+/.yardoc
+/Gemfile.lock
+/spec/support/dependencies
+/spec/support/.bundle
+/spec/support/*.lock
diff --git a/tools/paquet/Gemfile b/tools/paquet/Gemfile
new file mode 100644
index 00000000000..67dedf9c429
--- /dev/null
+++ b/tools/paquet/Gemfile
@@ -0,0 +1,4 @@
+source 'https://rubygems.org'
+
+# Specify your gem's dependencies in paquet.gemspec
+gemspec
diff --git a/tools/paquet/LICENSE b/tools/paquet/LICENSE
new file mode 100644
index 00000000000..43976b73b2b
--- /dev/null
+++ b/tools/paquet/LICENSE
@@ -0,0 +1,13 @@
+Copyright (c) 2012–2016 Elasticsearch <http://www.elastic.co>
+
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/tools/paquet/README.md b/tools/paquet/README.md
new file mode 100644
index 00000000000..ac61e149a0f
--- /dev/null
+++ b/tools/paquet/README.md
@@ -0,0 +1,70 @@
+# Paquet
+
+This gem allow a developer to create a uber gem, a uber gem is a gem that content the current gem and his dependencies and is distributed as a tarball.
+
+This tool allow to define what will be bundler and what should be ignored, it uses the dependencies defined in the gemspec and gemfile to know what to download.
+
+Note that by default no gems will be bundled.
+
+
+## Installation
+
+Add this line to your application's Gemfile:
+
+```ruby
+gem 'paquet'
+```
+
+And then execute:
+
+    $ bundle
+
+## Usage
+Define the dependencies in your Rakefile
+
+```ruby
+# encoding: utf-8
+require "paquet"
+
+TARGET_DIRECTORY = File.join(File.dirname(__FILE__), "dependencies")
+
+Paquet::Task.new(TARGET_DIRECTORY) do
+  pack "manticore"
+  pack "launchy"
+  pack "gemoji"
+  pack "logstash-output-elasticsearch"
+
+  # Everything not defined here will be assumed to be provided
+  # by the target installation
+  ignore "logstash-core-plugin-api"
+  ignore "logstash-core"
+end
+```
+
+And run
+
+```
+bundle exec rake paquet:vendor
+```
+
+The dependencies will be downloaded in your target directory.
+
+## Project Principles
+
+* Community: If a newbie has a bad time, it's a bug.
+* Software: Make it work, then make it right, then make it fast.
+* Technology: If it doesn't do a thing today, we can make it do it tomorrow.
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports,
+complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and
+maintainers or community members  saying "send patches or die" - you will not
+see that here.
+
+It is more important to me that you are able to contribute.
+
+For more information about contributing, see the
+[CONTRIBUTING](../CONTRIBUTING.md) file.
diff --git a/tools/paquet/Rakefile b/tools/paquet/Rakefile
new file mode 100644
index 00000000000..43022f711e2
--- /dev/null
+++ b/tools/paquet/Rakefile
@@ -0,0 +1,2 @@
+require "bundler/gem_tasks"
+task :default => :spec
diff --git a/tools/paquet/lib/paquet.rb b/tools/paquet/lib/paquet.rb
new file mode 100644
index 00000000000..0e19b6221eb
--- /dev/null
+++ b/tools/paquet/lib/paquet.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require "paquet/version"
+require "paquet/shell_ui"
+require "paquet/gem"
+require "paquet/dependency"
+require "paquet/rspec/tasks"
+
+module Paquet
+end
diff --git a/tools/paquet/lib/paquet/dependency.rb b/tools/paquet/lib/paquet/dependency.rb
new file mode 100644
index 00000000000..08fc739cc7a
--- /dev/null
+++ b/tools/paquet/lib/paquet/dependency.rb
@@ -0,0 +1,19 @@
+module Paquet
+  class Dependency
+    attr_reader :name, :version, :platform
+
+    def initialize(name, version, platform)
+      @name = name
+      @version = version
+      @platform = platform
+    end
+
+    def to_s
+      "#{name}-#{version}"
+    end
+
+    def ruby?
+      platform == "ruby"
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/gem.rb b/tools/paquet/lib/paquet/gem.rb
new file mode 100644
index 00000000000..88219651546
--- /dev/null
+++ b/tools/paquet/lib/paquet/gem.rb
@@ -0,0 +1,109 @@
+# encoding: utf-8
+require "paquet/dependency"
+require "paquet/shell_ui"
+require "paquet/utils"
+
+module Paquet
+  class Gem
+    RUBYGEMS_URI = "https://rubygems.org/downloads"
+
+    attr_reader :gems, :ignores
+
+    def initialize(target_path, cache = nil)
+      @target_path = target_path
+      @gems = []
+      @ignores = []
+      @cache = cache
+    end
+
+    def add(name)
+      @gems << name
+    end
+
+    def ignore(name)
+      @ignores << name
+    end
+
+    def pack
+      Paquet::ui.info("Cleaning existing target path: #{@target_path}")
+
+      FileUtils.rm_rf(@target_path)
+      FileUtils.mkdir_p(@target_path)
+
+      package_gems(collect_required_gems)
+    end
+
+    def package_gems(collect_required_gems)
+      gems_to_package = collect_required_gems
+        .collect { |gem| gem_full_name(gem) }
+        .uniq
+
+      if use_cache?
+        gems_to_package.each do |gem_name|
+          if gem_file = find_in_cache(gem_name)
+            destination = File.join(@target_path, File.basename(gem_file))
+            FileUtils.cp(gem_file, destination)
+            Paquet::ui.info("Vendoring: #{gem_name}, from cache: #{gem_file}")
+          else
+            download_gem(gem_name)
+          end
+        end
+      else
+        gems_to_package.each do |gem_name|
+          download_gem(gem_name)
+        end
+      end
+    end
+
+    def use_cache?
+      @cache
+    end
+
+    def find_in_cache(gem_name)
+      filename = File.join(@cache, gem_name)
+      File.exist?(filename) ? filename : nil
+    end
+
+    def size
+      @gems.size
+    end
+
+    def ignore?(name)
+      ignores.include?(name)
+    end
+
+    def collect_required_gems()
+      candidates = []
+      @gems.each do |name|
+        candidates += resolve_dependencies(name)
+      end
+      candidates.flatten
+    end
+
+    def resolve_dependencies(name)
+      return [] if ignore?(name)
+
+      spec = ::Gem::Specification.find_by_name(name)
+      current_dependency = Dependency.new(name, spec.version, spec.platform)
+      dependencies = spec.dependencies.select { |dep| dep.type == :runtime }
+
+      if dependencies.size == 0
+        [current_dependency]
+      else
+        [dependencies.collect { |spec| resolve_dependencies(spec.name) }, current_dependency].flatten.uniq { |s| s.name }
+      end
+    end
+
+    def gem_full_name(gem)
+      gem.ruby? ? "#{gem.name}-#{gem.version}.gem" : "#{gem.name}-#{gem.version}-#{gem.platform}.gem" 
+    end
+
+    def download_gem(gem_name)
+      source = "#{RUBYGEMS_URI}/#{gem_name}"
+      destination = File.join(@target_path, gem_name)
+
+      Paquet::ui.info("Vendoring: #{gem_name}, downloading: #{source}")
+      Paquet::Utils::download_file(source, destination)
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/rspec/tasks.rb b/tools/paquet/lib/paquet/rspec/tasks.rb
new file mode 100644
index 00000000000..29a66e0d830
--- /dev/null
+++ b/tools/paquet/lib/paquet/rspec/tasks.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "bundler"
+require "rake"
+require "rake/tasklib"
+require "fileutils"
+require "net/http"
+require "paquet/gem"
+
+# This class add new rake methods to a an existing ruby gem,
+# these methods allow developpers to create a Uber gem, a uber gem is
+# a tarball that contains the current gems and one or more of his dependencies.
+#
+# This Tool will take care of looking at the current dependency tree defined in the Gemspec and the gemfile
+# and will traverse all graph and download the gem file into a specified directory.
+#
+# By default, the tool wont fetch everything and the developper need to declare what gems he want to download.
+module Paquet
+  class Task < Rake::TaskLib
+    def initialize(target_path, cache_path = nil, &block)
+      @gem = Gem.new(target_path, cache_path)
+
+      instance_eval(&block)
+
+      namespace :paquet do
+        desc "Build a pack with #{@gem.size} gems: #{@gem.gems.join(",")}"
+        task :vendor do
+          @gem.pack
+        end
+      end
+    end
+
+    def pack(name)
+      @gem.add(name)
+    end
+
+    def ignore(name)
+      @gem.ignore(name)
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/shell_ui.rb b/tools/paquet/lib/paquet/shell_ui.rb
new file mode 100644
index 00000000000..331c37bef88
--- /dev/null
+++ b/tools/paquet/lib/paquet/shell_ui.rb
@@ -0,0 +1,37 @@
+# encoding: utf-8
+module Paquet
+  class SilentUI
+    class << self
+      def debug(message)
+      end
+      def info(message)
+      end
+    end
+  end
+
+  class ShellUi
+    def debug(message)
+      report_message(:debug, message) if debug?
+    end
+
+    def info(message)
+      report_message(:info, message)
+    end
+
+    def report_message(level, message)
+      puts "[#{level.upcase}]: #{message}"
+    end
+
+    def debug?
+      ENV["DEBUG"]
+    end
+  end
+
+  def self.ui
+    @logger ||= ShellUi.new
+  end
+
+  def self.ui=(new_output)
+    @logger = new_output
+  end
+end
diff --git a/tools/paquet/lib/paquet/utils.rb b/tools/paquet/lib/paquet/utils.rb
new file mode 100644
index 00000000000..44fd98b7455
--- /dev/null
+++ b/tools/paquet/lib/paquet/utils.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+require "fileutils"
+require "uri"
+
+module Paquet
+  class Utils
+    HTTPS_SCHEME = "https"
+    REDIRECTION_LIMIT = 5
+
+    def self.download_file(source, destination, counter = REDIRECTION_LIMIT)
+      raise "Too many redirection" if counter == 0
+
+      begin
+        f = File.open(destination, "w")
+
+        uri = URI.parse(source)
+
+        http = Net::HTTP.new(uri.host, uri.port, )
+        http.use_ssl = uri.scheme ==  HTTPS_SCHEME
+
+        response = http.get(uri.path)
+
+        case response
+        when Net::HTTPSuccess
+          f.write(response.body)
+        when Net::HTTPRedirection
+          counter -= 1
+          download_file(response['location'], destination, counter)
+        else
+          raise "Response not handled: #{response.class}, path: #{uri.path}"
+        end
+        f.path
+      rescue => e
+        FileUtils.rm_rf(f.path) rescue nil
+        raise e
+      ensure
+        f.close
+      end
+    end
+  end
+end
diff --git a/tools/paquet/lib/paquet/version.rb b/tools/paquet/lib/paquet/version.rb
new file mode 100644
index 00000000000..14b1b5ab010
--- /dev/null
+++ b/tools/paquet/lib/paquet/version.rb
@@ -0,0 +1,3 @@
+module Paquet
+  VERSION = "0.2.0"
+end
diff --git a/tools/paquet/paquet.gemspec b/tools/paquet/paquet.gemspec
new file mode 100644
index 00000000000..b195778d0ee
--- /dev/null
+++ b/tools/paquet/paquet.gemspec
@@ -0,0 +1,28 @@
+# coding: utf-8
+lib = File.expand_path('../lib', __FILE__)
+$LOAD_PATH.unshift(lib) unless $LOAD_PATH.include?(lib)
+require 'paquet/version'
+
+Gem::Specification.new do |spec|
+  spec.name          = "paquet"
+  spec.version       = Paquet::VERSION
+  spec.authors       = ["Elastic"]
+  spec.email         = ["info@elastic.co"]
+  spec.license       = "Apache License (2.0)"
+
+  spec.summary       = %q{Rake helpers to create a uber gem}
+  spec.description   = %q{This gem add a few rake tasks to create a uber gems that will be shipped as a zip}
+  spec.homepage      = "https://github.com/elastic/logstash"
+
+
+  spec.files         = Dir.glob(File.join(File.dirname(__FILE__), "lib", "**", "*.rb"))
+
+  spec.bindir        = "exe"
+  spec.executables   = spec.files.grep(%r{^exe/}) { |f| File.basename(f) }
+  spec.require_paths = ["lib"]
+
+  spec.add_development_dependency "rspec"
+  spec.add_development_dependency "pry"
+  spec.add_development_dependency "webmock", "~> 2.2.0"
+  spec.add_development_dependency "stud"
+end
diff --git a/tools/paquet/spec/integration/paquet_spec.rb b/tools/paquet/spec/integration/paquet_spec.rb
new file mode 100644
index 00000000000..b5b52f1ecdb
--- /dev/null
+++ b/tools/paquet/spec/integration/paquet_spec.rb
@@ -0,0 +1,74 @@
+# encoding: utf-8
+require "bundler"
+require "fileutils"
+require "stud/temporary"
+
+describe "Pack the dependencies", :integration => true do
+  let(:path) { File.expand_path(File.join(File.dirname(__FILE__), "..", "support")) }
+  let(:vendor_path) { Stud::Temporary.pathname }
+  let(:dependencies_path) { File.join(path, "dependencies") }
+  let(:bundler_cmd) { "bundle install --path #{vendor_path}"}
+  let(:rake_cmd) { "bundler exec rake paquet:vendor" }
+  let(:bundler_config) { File.join(path, ".bundler") }
+  let(:cache_path) { File.join(path, "cache") }
+  let(:cache_flores_gem) { File.join(cache_path, "flores-0.0.6.gem")}
+  let(:dummy_checksum_content) { "hello world" }
+
+  before do
+    FileUtils.mkdir_p(cache_path)
+  end
+
+  context "with gems in cache" do
+    before do
+      File.open(cache_flores_gem, "w") { |f| f.write(dummy_checksum_content) }
+
+      FileUtils.rm_rf(bundler_config)
+      FileUtils.rm_rf(vendor_path)
+
+      Bundler.with_clean_env do
+        Dir.chdir(path) do
+          system(bundler_cmd)
+          system(rake_cmd)
+        end
+      end
+    end
+
+    after do
+      FileUtils.rm_rf(cache_flores_gem)
+    end
+
+    it "download the dependencies" do
+      downloaded_dependencies = Dir.glob(File.join(dependencies_path, "*.gem"))
+
+      expect(downloaded_dependencies.size).to eq(2)
+      expect(downloaded_dependencies).to include(/flores-0\.0\.6/,/stud/)
+      expect(downloaded_dependencies).not_to include(/logstash-devutils/)
+
+      expect(File.read(Dir.glob(File.join(dependencies_path, "flores*.gem")).first)).to eq(dummy_checksum_content)
+    end
+  end
+
+  context "without cached gems" do
+    before do
+      FileUtils.rm_rf(bundler_config)
+      FileUtils.rm_rf(vendor_path)
+
+      Bundler.with_clean_env do
+        Dir.chdir(path) do
+          system(bundler_cmd)
+          system(rake_cmd)
+        end
+      end
+    end
+
+    it "download the dependencies" do
+      downloaded_dependencies = Dir.glob(File.join(dependencies_path, "*.gem"))
+
+      expect(downloaded_dependencies.size).to eq(2)
+      expect(downloaded_dependencies).to include(/flores-0\.0\.6/,/stud/)
+      expect(downloaded_dependencies).not_to include(/logstash-devutils/)
+
+      expect(File.read(Dir.glob(File.join(dependencies_path, "flores*.gem")).first)).not_to eq(dummy_checksum_content)
+    end
+  end
+end
diff --git a/tools/paquet/spec/paquet/dependency_spec.rb b/tools/paquet/spec/paquet/dependency_spec.rb
new file mode 100644
index 00000000000..aeab6663aa0
--- /dev/null
+++ b/tools/paquet/spec/paquet/dependency_spec.rb
@@ -0,0 +1,36 @@
+# encoding: utf-8
+require "paquet/dependency"
+
+describe Paquet::Dependency do
+  let(:name) { "mygem" }
+  let(:version) { "1.2.3" }
+  let(:platform) { "ruby" }
+
+  subject { described_class.new(name, version, platform) }
+
+  it "returns the name" do
+    expect(subject.name).to eq(name)
+  end
+
+  it "returns the version" do
+    expect(subject.version).to eq(version)
+  end
+
+  context "when the platform is mri" do
+    it "returns true" do
+      expect(subject.ruby?).to be_truthy
+    end
+  end
+
+  context "platform is jruby" do
+    let(:platform) { "java"}
+
+    it "returns false" do
+      expect(subject.ruby?).to be_falsey
+    end
+  end
+
+  it "return a meaninful string" do
+    expect(subject.to_s).to eq("#{name}-#{version}")
+  end
+end
diff --git a/tools/paquet/spec/paquet/gem_spec.rb b/tools/paquet/spec/paquet/gem_spec.rb
new file mode 100644
index 00000000000..b18bc731f36
--- /dev/null
+++ b/tools/paquet/spec/paquet/gem_spec.rb
@@ -0,0 +1,67 @@
+# encoding: utf-8
+require "paquet/gem"
+require "stud/temporary"
+require "fileutils"
+
+describe Paquet::Gem do
+  let(:target_path) { Stud::Temporary.pathname }
+  let(:dummy_gem) { "dummy-gem" }
+
+  subject { described_class.new(target_path) }
+
+  it "adds gem to pack" do
+    subject.add(dummy_gem)
+    expect(subject.gems).to include(dummy_gem)
+  end
+
+  it "allows to ignore gems" do
+    subject.ignore(dummy_gem)
+    expect(subject.ignore?(dummy_gem))
+  end
+
+  it "keeps track of the number of gem to pack" do
+    expect { subject.add(dummy_gem) }.to change { subject.size }.by(1)
+  end
+
+  context "when not configuring cache" do
+    it "use_cache? returns false" do
+      expect(subject.use_cache?).to be_truthy
+    end
+  end
+
+  context "when configuring cache" do
+    let(:cache_path) do
+      p = Stud::Temporary.pathname
+      FileUtils.mkdir_p(p)
+      p
+    end
+
+    subject { described_class.new(target_path, cache_path) }
+
+    it "use_cache? returns true" do
+      expect(subject.use_cache?).to be_truthy
+    end
+
+    context "#find_in_cache" do
+      let(:gem_full_name) { "super-lib-0.1.0.gem" }
+
+      context "when the gem is in cache directory" do
+        let(:gem_file_path) { File.join(cache_path, gem_full_name) }
+
+        before do
+          FileUtils.touch(gem_file_path)
+        end
+
+        it "returns true" do
+          expect(subject.find_in_cache(gem_full_name)).to match(gem_file_path)
+        end
+      end
+
+      context "when the gem is not in the cache directory" do
+        it "returns false" do
+          expect(subject.find_in_cache(gem_full_name)).to be_falsey
+        end
+      end
+    end
+  end
+end
diff --git a/tools/paquet/spec/paquet/shell_ui_spec.rb b/tools/paquet/spec/paquet/shell_ui_spec.rb
new file mode 100644
index 00000000000..398274e9445
--- /dev/null
+++ b/tools/paquet/spec/paquet/shell_ui_spec.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require "paquet/shell_ui"
+
+describe Paquet::ShellUi do
+  let(:message) { "hello world" }
+
+  subject { described_class.new }
+
+  context "when debug is on" do
+    before :all do
+      @debug = ENV["debug"]
+      ENV["DEBUG"] = "1"
+    end
+
+    after :all do
+      ENV["DEBUG"] = @debug
+    end
+
+    it "show the debug statement" do
+      expect(subject).to receive(:puts).with("[DEBUG]: #{message}")
+      subject.debug(message)
+    end
+  end
+
+  context "not in debug" do
+    before :all do
+      @debug = ENV["debug"]
+      ENV["DEBUG"] = nil
+    end
+
+    after :all do
+      ENV["DEBUG"] = @debug
+    end
+
+    it "doesnt show the debug statement" do
+      expect(subject).not_to receive(:puts).with("[DEBUG]: #{message}")
+      subject.debug(message)
+    end
+  end
+end
diff --git a/tools/paquet/spec/paquet/utils_spec.rb b/tools/paquet/spec/paquet/utils_spec.rb
new file mode 100644
index 00000000000..de636526d7a
--- /dev/null
+++ b/tools/paquet/spec/paquet/utils_spec.rb
@@ -0,0 +1,93 @@
+# encoding: utf-8
+require "paquet/utils"
+require "stud/temporary"
+require "spec_helper"
+
+describe Paquet::Utils do
+  subject { described_class }
+
+  let(:url) { "https://localhost:8898/my-file.txt"}
+  let(:destination) do
+    p = Stud::Temporary.pathname
+    FileUtils.mkdir_p(p)
+    File.join(p, "tmp-file")
+  end
+
+  let(:content) { "its halloween, halloween!" }
+
+  context "when the file exist" do
+    before do
+      stub_request(:get, url).to_return(
+        { :status => 200,
+          :body => content,
+          :headers => {}}
+      )
+    end
+
+    it "download the file to local temporary file" do
+      expect(File.read(subject.download_file(url, destination))).to match(content)
+    end
+
+    context "when an exception occur" do
+      let(:temporary_path) { Stud::Temporary.pathname }
+
+      before do
+        expect(URI).to receive(:parse).with(anything).and_raise("something went wrong")
+      end
+
+      it "deletes the temporary file" do
+        subject.download_file(url, destination) rescue nil
+        expect(File.exist?(destination)).to be_falsey
+      end
+    end
+  end
+
+  context "on redirection" do
+    let(:redirect_response) { instance_double("Net::HTTP::Response", :code => "302", :headers => { "location" => "https://localhost:8888/new_path" }) }
+    let(:response_ok) { instance_double("Net::HTTP::Response", :code => "200") }
+
+    context "less than the maximun of redirection" do
+      let(:redirect_url) { "https://localhost:8898/redirect/my-file.txt"}
+
+      before do
+        stub_request(:get, url).to_return(
+          { :status => 302, :headers => { "location" => redirect_url }}
+        )
+
+        stub_request(:get, url).to_return(
+          { :status => 200, :body => content }
+        )
+      end
+
+      it "follows the redirection" do
+        expect(File.read(subject.download_file(url, destination))).to match(content)
+      end
+    end
+
+    context "too many redirection" do
+      before do
+        stub_request(:get, url).to_return(
+          { :status => 302, :headers => { "location" => url }}
+        )
+      end
+
+      it "raises an exception" do
+        expect { subject.download_file(url, destination) }.to raise_error(/Too many redirection/)
+      end
+    end
+  end
+
+  [404, 400, 401, 500].each do |code|
+    context "When the server return #{code}" do
+      before do
+        stub_request(:get, url).to_return(
+          { :status => code }
+        )
+      end
+
+      it "raises an exception" do
+        expect { subject.download_file(url, destination) }.to raise_error(/Response not handled/)
+      end
+    end
+  end
+end
diff --git a/tools/paquet/spec/spec_helper.rb b/tools/paquet/spec/spec_helper.rb
new file mode 100644
index 00000000000..a2d0aa45d42
--- /dev/null
+++ b/tools/paquet/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "webmock/rspec"
diff --git a/tools/paquet/spec/support/Gemfile b/tools/paquet/spec/support/Gemfile
new file mode 100644
index 00000000000..884cbedd8cc
--- /dev/null
+++ b/tools/paquet/spec/support/Gemfile
@@ -0,0 +1,4 @@
+source "https://rubygems.org"
+gemspec
+gem "paquet", :path => "../../"
+
diff --git a/tools/paquet/spec/support/Rakefile b/tools/paquet/spec/support/Rakefile
new file mode 100644
index 00000000000..5e8040d8521
--- /dev/null
+++ b/tools/paquet/spec/support/Rakefile
@@ -0,0 +1,12 @@
+# encoding: utf-8
+require "paquet"
+
+TARGET_DIRECTORY = File.join(File.dirname(__FILE__), "dependencies")
+CACHE_PATH = File.join(File.dirname(__FILE__), "cache")
+
+Paquet::Task.new(TARGET_DIRECTORY, CACHE_PATH) do
+  pack "stud"
+  pack "flores"
+
+  ignore "logstash-devutils"
+end
diff --git a/tools/paquet/spec/support/paquet.gemspec b/tools/paquet/spec/support/paquet.gemspec
new file mode 100644
index 00000000000..668d109a74a
--- /dev/null
+++ b/tools/paquet/spec/support/paquet.gemspec
@@ -0,0 +1,17 @@
+# coding: utf-8
+
+Gem::Specification.new do |spec|
+  spec.name          = "paquet-test"
+  spec.version       = "0.0.0"
+  spec.authors       = ["Elastic"]
+  spec.email         = ["info@elastic.co"]
+  spec.license       = "Apache License (2.0)"
+
+  spec.summary       = %q{testing gem}
+  spec.description   = %q{testing gem}
+  spec.homepage      = "https://github.com/elastic/logstash"
+
+  spec.add_runtime_dependency "stud"
+  spec.add_runtime_dependency "flores", "0.0.6"
+  spec.add_runtime_dependency "logstash-devutils", "0.0.6"
+end
diff --git a/versions.yml b/versions.yml
index d8e00540f43..1c44f7e4c39 100644
--- a/versions.yml
+++ b/versions.yml
@@ -1,7 +1,7 @@
 ---
-logstash: 5.1.2
-logstash-core: 5.1.2
-logstash-core-event: 5.1.2
-logstash-core-event-java: 5.1.2
-logstash-core-queue-jruby: 5.1.2
-logstash-core-plugin-api: 2.1.20
+logstash: 6.0.0-alpha1
+logstash-core: 6.0.0-alpha1
+logstash-core-event: 6.0.0-alpha1
+logstash-core-event-java: 6.0.0-alpha1
+logstash-core-queue-jruby: 6.0.0-alpha1
+logstash-core-plugin-api: 2.1.16
