diff --git a/config/logstash.yml b/config/logstash.yml
index 0e07005b0f6..4a9d9cf510f 100644
--- a/config/logstash.yml
+++ b/config/logstash.yml
@@ -40,6 +40,9 @@
 #
 # pipeline.workers: 2
 #
+pipeline.dead_letter_pipeline: 'dlq'
+pipeline.failure_handler: "dead_letter"
+
 # How many events to retrieve from inputs before sending to filters+workers
 #
 # pipeline.batch.size: 125
@@ -78,7 +81,7 @@
 # of ECS for the individual pipeline in its `pipelines.yml` definition. Setting
 # it here will set the default for _all_ pipelines, including new ones.
 #
-# pipeline.ecs_compatibility: v8
+# pipeline.ecs_compatibility: disabled
 #
 # ------------ Pipeline Configuration Settings --------------
 #
@@ -97,7 +100,7 @@
 # Periodically check if the configuration has changed and reload the pipeline
 # This can also be triggered manually through the SIGHUP signal
 #
-# config.reload.automatic: false
+#config.reload.automatic: true
 #
 # How often to check if the pipeline configuration has changed (in seconds)
 # Note that the unit value (s) is required. Values without a qualifier (e.g. 60) 
@@ -246,7 +249,7 @@
 # ------------ Dead-Letter Queue Settings --------------
 # Flag to turn on dead-letter queue.
 #
-# dead_letter_queue.enable: false
+# dead_letter_queue.enable: true
 
 # If using dead_letter_queue.enable: true, the maximum size of each dead letter queue. Entries
 # will be dropped if they would increase the size of the dead letter queue beyond this setting.
diff --git a/config/pipelines.yml b/config/pipelines.yml
index 3dd60017a6c..a6cc2e2b772 100644
--- a/config/pipelines.yml
+++ b/config/pipelines.yml
@@ -6,90 +6,35 @@
 #
 # Example of two pipelines:
 #
-# - pipeline.id: test
-#   pipeline.workers: 1
-#   pipeline.batch.size: 1
-#   config.string: "input { generator {} } filter { sleep { time => 1 } } output { stdout { codec => dots } }"
-# - pipeline.id: another_test
-#   queue.type: persisted
-#   path.config: "/tmp/logstash/*.config"
-#
-# Available options:
-#
-#   # name of the pipeline
-#   pipeline.id: mylogs
-#
-#   # The configuration string to be used by this pipeline
-#   config.string: "input { generator {} } filter { sleep { time => 1 } } output { stdout { codec => dots } }"
-#
-#   # The path from where to read the configuration text
-#   path.config: "/etc/conf.d/logstash/myconfig.cfg"
-#
-#   # How many worker threads execute the Filters+Outputs stage of the pipeline
-#   pipeline.workers: 1 (actually defaults to number of CPUs)
-#
-#   # How many events to retrieve from inputs before sending to filters+workers
-#   pipeline.batch.size: 125
-#
-#   # How long to wait in milliseconds while polling for the next event
-#   # before dispatching an undersized batch to filters+outputs
-#   pipeline.batch.delay: 50
-#
-#   Set the pipeline event ordering. Options are "auto" (the default), "true" # #   or "false".
-#   "auto" automatically enables ordering if the 'pipeline.workers' setting
-#   is also set to '1', and disables otherwise.
-#   "true" enforces ordering on a pipeline and prevents logstash from starting
-#   a pipeline with multiple workers allocated.
-#   "false" disable any extra processing necessary for preserving ordering.
-#
-#   pipeline.ordered: auto
-#
-#   # Internal queuing model, "memory" for legacy in-memory based queuing and
-#   # "persisted" for disk-based acked queueing. Defaults is memory
-#   queue.type: memory
-#
-#   # If using queue.type: persisted, the page data files size. The queue data consists of
-#   # append-only data files separated into pages. Default is 64mb
-#   queue.page_capacity: 64mb
-#
-#   # If using queue.type: persisted, the maximum number of unread events in the queue.
-#   # Default is 0 (unlimited)
-#   queue.max_events: 0
-#
-#   # If using queue.type: persisted, the total capacity of the queue in number of bytes.
-#   # Default is 1024mb or 1gb
-#   queue.max_bytes: 1024mb
-#
-#   # If using queue.type: persisted, the maximum number of acked events before forcing a checkpoint
-#   # Default is 1024, 0 for unlimited
-#   queue.checkpoint.acks: 1024
-#
-#   # If using queue.type: persisted, the maximum number of written events before forcing a checkpoint
-#   # Default is 1024, 0 for unlimited
-#   queue.checkpoint.writes: 1024
-#
-#   # If using queue.type: persisted, the interval in milliseconds when a checkpoint is forced on the head page
-#   # Default is 1000, 0 for no periodic checkpoint.
-#   queue.checkpoint.interval: 1000
-#
-#   # Enable Dead Letter Queueing for this pipeline.
-#   dead_letter_queue.enable: false
-#
-#   If using dead_letter_queue.enable: true, the maximum size of dead letter queue for this pipeline. Entries
-#   will be dropped if they would increase the size of the dead letter queue beyond this setting.
-#   Default is 1024mb
-#   dead_letter_queue.max_bytes: 1024mb
-#
-#   If using dead_letter_queue.enable: true, the interval in milliseconds where if no further events eligible for the DLQ
-#   have been created, a dead letter queue file will be written. A low value here will mean that more, smaller, queue files
-#   may be written, while a larger value will introduce more latency between items being "written" to the dead letter queue, and
-#   being available to be read by the dead_letter_queue input when items are are written infrequently.
-#   Default is 5000.
-#
-#   dead_letter_queue.flush_interval: 5000
 
-#
-#   If using dead_letter_queue.enable: true, the directory path where the data files will be stored.
-#   Default is path.data/dead_letter_queue
-#
-#   path.dead_letter_queue:
+- pipeline.id: stdin
+  pipeline.workers: 1
+  pipeline.dead_letter_pipeline: 'print'
+  pipeline.batch.size: 1
+  config.string: 'input { stdin {  } } filter{ json {source => "message" target => ["the_target"] } sleep { time => 0.1 add_tag => "slept" }if [a] != 0 { mutate { add_tag => ["foo"]}} } output {elasticsearch { cloud_id => "es8:XX" cloud_auth => "elastic:YY" data_stream => false }}'
+- pipeline.id: dli
+  config.string: 'input { dead_letter_pipeline { }} output { elasticsearch { cloud_id => "es8:XX" cloud_auth => "elastic:YY" data_stream => false index => "logstash_dli"}}'
+- pipeline.id: repair
+  queue.type: memory
+  pipeline.dead_letter_pipeline: 'dli'
+  config.string: "input { dead_letter_pipeline { }} filter { json { source => '[event][original]' } mutate { remove_field => 'geoip'}  mutate { remove_field => ['error','message'] add_tag => 'repaired' } } output { elasticsearch  { cloud_id => 'es8:XX' cloud_auth => 'elastic:YY' data_stream => false }}"
+- pipeline.id: tcp
+  pipeline.workers: 1
+  pipeline.dead_letter_pipeline: 'dli'
+  config.string: 'input { tcp { port => 2323 }} filter{ json {source => "message" target => ["the_target"] }} output {elasticsearch { cloud_id => "es8:XX" cloud_auth => "elastic:YY" data_stream => false }}'
+- pipeline.id: tcp2
+  pipeline.workers: 1
+  pipeline.dead_letter_pipeline: 'print'
+  config.string: 'input { tcp { port => 2324 }} filter{ json {source => "message" target => ["the_target"] } }output {elasticsearch { cloud_id => "es8:XX" cloud_auth => "elastic:YY" data_stream => false }}'
+- pipeline.id: print
+  pipeline.workers: 1
+  config.string: 'input { dead_letter_pipeline {}} output { stdout { codec => rubydebug { metadata => true }}}'
+- pipeline.id: tcp_broken
+  pipeline.workers: 1
+  pipeline.dead_letter_pipeline: 'print'
+  config.string: 'input { tcp { port => 2325 codec => json { target => "marget" }}} filter{ split { field => "results" } if [a] > 1 {mutate { add_tag  => "a is big" } } }output {elasticsearch { cloud_id => "es8:XX" cloud_auth => "elastic:YY" data_stream => false }}'
+- pipeline.id: tcp_working
+  pipeline.workers: 1
+  pipeline.dead_letter_pipeline: 'print'
+  config.string: 'input { tcp { port => 2326 codec => json { target => "marget" }}} filter{ split { field => "results" } mutate { add_tag  => "a is big" } } output { if [a] > 1 {stdout { codec => rubydebug }}}'
+
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 0147a1cf61f..a3e7d777412 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -61,6 +61,8 @@ module Environment
            Setting::Boolean.new("pipeline.separate_logs", false),
    Setting::CoercibleString.new("pipeline.ordered", "auto", true, ["auto", "true", "false"]),
    Setting::CoercibleString.new("pipeline.ecs_compatibility", "v8", true, %w(disabled v1 v8)),
+            Setting::String.new("pipeline.dead_letter_pipeline", nil, false).nullable,
+            Setting::String.new("pipeline.failure_handler", "explode", true, ["explode", "drop", "dead_letter"]),
                     Setting.new("path.plugins", Array, []),
     Setting::NullableString.new("interactive", nil, false),
            Setting::Boolean.new("config.debug", false),
diff --git a/logstash-core/lib/logstash/filters/base.rb b/logstash-core/lib/logstash/filters/base.rb
index bbda8b6e0c1..d5af2be8e3b 100644
--- a/logstash-core/lib/logstash/filters/base.rb
+++ b/logstash-core/lib/logstash/filters/base.rb
@@ -156,10 +156,42 @@ def filter(event)
   public
   def do_filter(event, &block)
     time = java.lang.System.nanoTime
-    filter(event, &block)
+    begin
+      filter(event, &block)
+    rescue => cause
+      on_failure(event, cause)
+    end
     @slow_logger.on_event("event processing time", @original_params, event, java.lang.System.nanoTime - time)
   end
 
+  # Replace with failure_handler
+  def on_failure(event, cause)
+    on_failure_dlq(event, cause)
+  end
+  #
+  #
+  def on_failure_dlq(event, cause)
+    failure_message = {
+        "message" => cause.message,
+        "type"=> cause.class.to_s,
+        "stack_trace" => cause.backtrace,
+        "source" => {
+            "plugin_name" => config_name,
+            "plugin_id" => id
+        }
+    }
+    # Need to figure out the correct event shape here.
+    execution_context.dlq_writer.write(event, failure_message)
+    event.cancel
+  end
+
+  def on_failure_drop(event, cause)
+    event.cancel
+  end
+
+  def on_failure_crash(event, cause)
+    raise cause
+  end
 
   # in 1.5.0 multi_filter is meant to be used in the generated filter function in LogStash::Config::AST::Plugin only
   # and is temporary until we refactor the filter method interface to accept events list and return events list,
diff --git a/logstash-core/lib/logstash/java_pipeline.rb b/logstash-core/lib/logstash/java_pipeline.rb
index 3ca76e52074..9f0a7fb2834 100644
--- a/logstash-core/lib/logstash/java_pipeline.rb
+++ b/logstash-core/lib/logstash/java_pipeline.rb
@@ -257,7 +257,8 @@ def start_workers
       config_metric.gauge(:config_reload_automatic, settings.get("config.reload.automatic"))
       config_metric.gauge(:config_reload_interval, settings.get("config.reload.interval").to_nanos)
       config_metric.gauge(:dead_letter_queue_enabled, dlq_enabled?)
-      config_metric.gauge(:dead_letter_queue_path, dlq_writer.get_path.to_absolute_path.to_s) if dlq_enabled?
+      # config_metric.gauge(:dead_letter_queue_path, dlq_writer.get_path.to_absolute_path.to_s) if dlq_enabled?
+
       config_metric.gauge(:ephemeral_id, ephemeral_id)
       config_metric.gauge(:hash, lir.unique_hash)
       config_metric.gauge(:graph, ::LogStash::Config::LIRSerializer.serialize(lir))
diff --git a/logstash-core/lib/logstash/outputs/base.rb b/logstash-core/lib/logstash/outputs/base.rb
index 231d19f1e52..75a66505575 100644
--- a/logstash-core/lib/logstash/outputs/base.rb
+++ b/logstash-core/lib/logstash/outputs/base.rb
@@ -106,6 +106,41 @@ def multi_receive(events)
     end
   end
 
+  def do_receive(event)
+    begin
+      receive(event)
+    rescue => e
+      on_failure(event, e)
+    end
+  end
+
+  # Replace with failure_handler
+  def on_failure(event, cause)
+    on_failure_dlq(event, cause)
+  end
+
+  def on_failure_dlq(event, cause)
+    failure_message = {
+        "message" => cause.message,
+        "type"=> cause.class.to_s,
+        "stack_trace" => cause.backtrace,
+        "source" => {
+            "plugin_name" => config_name,
+            "plugin_id" => id
+        }
+    }
+    # Need to figure out the correct event shape here.
+    execution_context.dlq_writer.write(event, failure_message)
+  end
+
+  def on_failure_drop(event, cause)
+    event.cancel
+  end
+
+  def on_failure_crash(event, cause)
+    raise cause
+  end
+
   def workers_not_supported(message=nil)
     raise "This plugin (#{self.class.name}) is using the obsolete '#workers_not_supported' method. If you installed this plugin specifically on this Logstash version, it is not compatible. If you are a plugin author, please see https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_output_plugin.html for more info"
   end
diff --git a/logstash-core/lib/logstash/plugins/builtin.rb b/logstash-core/lib/logstash/plugins/builtin.rb
index 1fb4c9fcbe6..aa0fa3b9da0 100644
--- a/logstash-core/lib/logstash/plugins/builtin.rb
+++ b/logstash-core/lib/logstash/plugins/builtin.rb
@@ -16,9 +16,11 @@
 # under the License.
 
 module ::LogStash::Plugins::Builtin
+  require 'logstash/plugins/builtin/dead_letter_pipeline/input'
   require 'logstash/plugins/builtin/pipeline/input'
   require 'logstash/plugins/builtin/pipeline/output'
 
   LogStash::PLUGIN_REGISTRY.add(:input, "pipeline", LogStash::Plugins::Builtin::Pipeline::Input)
   LogStash::PLUGIN_REGISTRY.add(:output, "pipeline", LogStash::Plugins::Builtin::Pipeline::Output)
+  LogStash::PLUGIN_REGISTRY.add(:input, "dead_letter_pipeline", LogStash::Plugins::Builtin::DeadLetterPipeline::Input)
 end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/plugins/builtin/dead_letter_pipeline/input.rb b/logstash-core/lib/logstash/plugins/builtin/dead_letter_pipeline/input.rb
new file mode 100644
index 00000000000..aa241c2915f
--- /dev/null
+++ b/logstash-core/lib/logstash/plugins/builtin/dead_letter_pipeline/input.rb
@@ -0,0 +1,62 @@
+module ::LogStash; module Plugins; module Builtin; module DeadLetterPipeline; class Input < ::LogStash::Inputs::Base
+  include org.logstash.plugins.pipeline.PipelineInput
+  config_name "dead_letter_pipeline"
+
+  attr_reader :pipeline_bus, :address
+
+  def register
+    # May as well set this up here, writers won't do anything until
+    # @running is set to false
+    @running = java.util.concurrent.atomic.AtomicBoolean.new(false)
+    @address = execution_context.pipeline.pipeline_id
+    @pipeline_bus = execution_context.agent.pipeline_bus
+    listen_successful = pipeline_bus.listen(self, address)
+    if !listen_successful
+      raise ::LogStash::ConfigurationError, "Internal input at '#{@address}' already bound! Addresses must be globally unique across pipelines."
+    end
+    # add address to the plugin stats
+    metric.gauge(:address, address)
+
+  end
+
+  def run(queue)
+    @queue = queue
+    @running.set(true)
+
+    while @running.get()
+      sleep 0.1
+    end
+  end
+
+  def running?
+    @running && @running.get()
+  end
+
+  # Returns false if the receive failed due to a stopping input
+  # To understand why this value is useful see Internal.send_to
+  # Note, this takes a java Stream, not a ruby array
+  def internalReceive(events)
+    return false if !@running.get()
+
+    # TODO This should probably push a batch at some point in the future when doing so
+    # buys us some efficiency
+    events.forEach do |event|
+      decorate(event)
+      @queue << event
+    end
+
+    true
+  end
+
+  def stop
+    pipeline_bus.unlisten(self, address)
+    # We stop receiving events _after_ we unlisten to pick up any events sent by upstream outputs that
+    # have not yet stopped
+    @running.set(false) if @running # If register wasn't yet called, no @running!
+  end
+
+  def isRunning
+    @running.get
+  end
+
+end; end; end; end; end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/plugins/builtin/pipeline/output.rb b/logstash-core/lib/logstash/plugins/builtin/pipeline/output.rb
index 07107cb602d..ec274ceb713 100644
--- a/logstash-core/lib/logstash/plugins/builtin/pipeline/output.rb
+++ b/logstash-core/lib/logstash/plugins/builtin/pipeline/output.rb
@@ -40,6 +40,7 @@ def multi_receive(events)
   end
 
   def close
+    puts "Unregistering #{self} from #{@send_to}"
     pipeline_bus.unregisterSender(self, @send_to)
   end
 end; end; end; end; end
\ No newline at end of file
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
index 1df5315ca01..ec52d9b123c 100644
--- a/logstash-core/lib/logstash/settings.rb
+++ b/logstash-core/lib/logstash/settings.rb
@@ -62,6 +62,8 @@ def self.included(base)
       "pipeline.workers",
       "pipeline.ordered",
       "pipeline.ecs_compatibility",
+      "pipeline.dead_letter_pipeline",
+      "pipeline.failure_handler",
       "queue.checkpoint.acks",
       "queue.checkpoint.interval",
       "queue.checkpoint.writes",
diff --git a/logstash-core/src/main/java/org/logstash/common/AbstractDeadLetterQueueWriterExt.java b/logstash-core/src/main/java/org/logstash/common/AbstractDeadLetterQueueWriterExt.java
index 53a881e95fb..635e19a9fed 100644
--- a/logstash-core/src/main/java/org/logstash/common/AbstractDeadLetterQueueWriterExt.java
+++ b/logstash-core/src/main/java/org/logstash/common/AbstractDeadLetterQueueWriterExt.java
@@ -21,6 +21,8 @@
 package org.logstash.common;
 
 import java.io.IOException;
+import java.util.Map;
+
 import org.jruby.Ruby;
 import org.jruby.RubyBoolean;
 import org.jruby.RubyClass;
@@ -29,7 +31,8 @@
 import org.jruby.anno.JRubyMethod;
 import org.jruby.runtime.ThreadContext;
 import org.jruby.runtime.builtin.IRubyObject;
-import org.logstash.common.io.DeadLetterQueueWriter;
+import org.logstash.common.dlq.DeadLetterPipelineWriter;
+import org.logstash.common.dlq.IDeadLetterQueueWriter;
 import org.logstash.ext.JrubyEventExtLibrary;
 
 @JRubyClass(name = "AbstractDeadLetterQueueWriter")
@@ -141,7 +144,7 @@ public static final class PluginDeadLetterQueueWriterExt
 
         private IRubyObject writerWrapper;
 
-        private DeadLetterQueueWriter innerWriter;
+        private IDeadLetterQueueWriter innerWriter;
 
         private IRubyObject pluginId;
 
@@ -160,9 +163,9 @@ public AbstractDeadLetterQueueWriterExt.PluginDeadLetterQueueWriterExt initializ
             final ThreadContext context, final IRubyObject innerWriter, final IRubyObject pluginId,
             final IRubyObject pluginType) {
             writerWrapper = innerWriter;
-            if (writerWrapper.getJavaClass().equals(DeadLetterQueueWriter.class)) {
+            if (writerWrapper.getJavaClass().equals(DeadLetterPipelineWriter.class)) {
                 this.innerWriter = writerWrapper.toJava(
-                    DeadLetterQueueWriter.class
+                    DeadLetterPipelineWriter.class
                 );
             }
             this.pluginId = pluginId;
@@ -192,14 +195,22 @@ protected IRubyObject getInnerWriter(final ThreadContext context) {
         }
 
         @Override
+        @SuppressWarnings("unchecked")
         protected IRubyObject doWrite(final ThreadContext context, final IRubyObject event,
             final IRubyObject reason) {
             if (hasOpenWriter()) {
                 try {
-                    innerWriter.writeEntry(
-                        ((JrubyEventExtLibrary.RubyEvent) event).getEvent(),
-                        pluginIdString, pluginTypeString, reason.asJavaString()
-                    );
+                    if (reason.getJavaClass().isAssignableFrom(Map.class)){
+                        innerWriter.writeEntry(
+                                ((JrubyEventExtLibrary.RubyEvent) event).getEvent(),
+                                reason.toJava(Map.class)
+                        );
+                    }else{
+                        innerWriter.writeEntry(
+                                ((JrubyEventExtLibrary.RubyEvent) event).getEvent(),
+                                pluginIdString, pluginTypeString, reason.asJavaString()
+                        );
+                    }
                 } catch (final IOException ex) {
                     throw new IllegalStateException(ex);
                 }
diff --git a/logstash-core/src/main/java/org/logstash/common/DLQWriterAdapter.java b/logstash-core/src/main/java/org/logstash/common/DLQWriterAdapter.java
index 1fbc906bafd..4c63bb9f4a6 100644
--- a/logstash-core/src/main/java/org/logstash/common/DLQWriterAdapter.java
+++ b/logstash-core/src/main/java/org/logstash/common/DLQWriterAdapter.java
@@ -23,15 +23,16 @@
 import co.elastic.logstash.api.DeadLetterQueueWriter;
 import co.elastic.logstash.api.Event;
 import co.elastic.logstash.api.Plugin;
+import org.logstash.common.dlq.IDeadLetterQueueWriter;
 
 import java.io.IOException;
 import java.util.Objects;
 
 public class DLQWriterAdapter implements DeadLetterQueueWriter {
 
-    private final org.logstash.common.io.DeadLetterQueueWriter dlqWriter;
+    private final IDeadLetterQueueWriter dlqWriter;
 
-    public DLQWriterAdapter(org.logstash.common.io.DeadLetterQueueWriter dlqWriter) {
+    public DLQWriterAdapter(IDeadLetterQueueWriter dlqWriter) {
         this.dlqWriter = Objects.requireNonNull(dlqWriter);
     }
 
diff --git a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
index 88e8175936c..7813859eac2 100644
--- a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
+++ b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
@@ -40,7 +40,10 @@
 
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
+import org.logstash.common.dlq.DeadLetterPipelineWriter;
+import org.logstash.common.dlq.IDeadLetterQueueWriter;
 import org.logstash.common.io.DeadLetterQueueWriter;
+import org.logstash.plugins.pipeline.PipelineBus;
 
 import java.io.IOException;
 import java.nio.file.Paths;
@@ -54,7 +57,7 @@
 public class DeadLetterQueueFactory {
 
     private static final Logger logger = LogManager.getLogger(DeadLetterQueueFactory.class);
-    private static final ConcurrentHashMap<String, DeadLetterQueueWriter> REGISTRY = new ConcurrentHashMap<>();
+    private static final ConcurrentHashMap<String, IDeadLetterQueueWriter> REGISTRY = new ConcurrentHashMap<>();
     private static final long MAX_SEGMENT_SIZE_BYTES = 10 * 1024 * 1024;
 
     /**
@@ -77,15 +80,15 @@ private DeadLetterQueueFactory() {
      * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
      * @return The write manager for the specific id's dead-letter-queue context
      */
-    public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval) {
+    public static IDeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval) {
         return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize, flushInterval));
     }
 
-    public static DeadLetterQueueWriter release(String id) {
+    public static IDeadLetterQueueWriter release(String id) {
         return REGISTRY.remove(id);
     }
 
-    private static DeadLetterQueueWriter newWriter(final String id, final String dlqPath, final long maxQueueSize, final Duration flushInterval) {
+    private static IDeadLetterQueueWriter newWriter(final String id, final String dlqPath, final long maxQueueSize, final Duration flushInterval) {
         try {
             return new DeadLetterQueueWriter(Paths.get(dlqPath, id), MAX_SEGMENT_SIZE_BYTES, maxQueueSize, flushInterval);
         } catch (IOException e) {
@@ -93,4 +96,17 @@ private static DeadLetterQueueWriter newWriter(final String id, final String dlq
         }
         return null;
     }
+
+    /**
+     * DLQ Pipeline retrieval
+     *
+     */
+    public static IDeadLetterQueueWriter getWriter(String sourcePipeline, String deadLetterPipeline, PipelineBus pipelineBus) {
+        return REGISTRY.computeIfAbsent(sourcePipeline, key -> newPipelineWriter(sourcePipeline, deadLetterPipeline, pipelineBus));
+    }
+
+    private static IDeadLetterQueueWriter newPipelineWriter(String sourcePipeline, String deadLetterPipeline, PipelineBus pipelineBus){
+        return new DeadLetterPipelineWriter(sourcePipeline, deadLetterPipeline, pipelineBus);
+    }
+
 }
diff --git a/logstash-core/src/main/java/org/logstash/common/dlq/DeadLetterPipelineWriter.java b/logstash-core/src/main/java/org/logstash/common/dlq/DeadLetterPipelineWriter.java
new file mode 100644
index 00000000000..a7f82de08fc
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/dlq/DeadLetterPipelineWriter.java
@@ -0,0 +1,104 @@
+package org.logstash.common.dlq;
+
+import com.google.common.collect.Lists;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+import org.logstash.plugins.pipeline.PipelineBus;
+import org.logstash.plugins.pipeline.PipelineOutput;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.concurrent.atomic.AtomicBoolean;
+import java.util.concurrent.atomic.LongAdder;
+
+import static org.logstash.RubyUtil.RUBY;
+
+
+public class DeadLetterPipelineWriter implements IDeadLetterQueueWriter {
+    private static final Logger LOGGER = LogManager.getLogger(DeadLetterPipelineWriter.class);
+    private final String sourcePipeline;
+    private final String deadLetterPipeline;
+    private final PipelineBus pipelineBus;
+    private final PipelineOutput deadLetterPipelineOutput;
+    private final LongAdder eventsWritten = new LongAdder();
+    private final AtomicBoolean isOpen = new AtomicBoolean(true);
+
+    public DeadLetterPipelineWriter(final String sourcePipeline, final String deadLetterPipeline, final PipelineBus pipelineBus) {
+        this.sourcePipeline = sourcePipeline;
+        this.deadLetterPipeline = deadLetterPipeline;
+        this.pipelineBus = pipelineBus;
+
+        deadLetterPipelineOutput = new PipelineOutput() {
+            @Override
+            public int hashCode() {
+                return super.hashCode();
+            }
+
+            public String toString(){
+                return String.format("Dead Letter Pipeline: [Dead Letter Pipeline: %s, Originating Pipeline: %s]",
+                                     deadLetterPipeline, sourcePipeline);
+            }
+        };
+        LOGGER.warn("Registering failure pipeline from {} to {}", sourcePipeline, deadLetterPipeline);
+        pipelineBus.registerSender(deadLetterPipelineOutput, Lists.newArrayList(deadLetterPipeline));
+    }
+
+    @Override
+    public void writeEntry(Event event, Map<String, Object> metadata) throws IOException {
+        LOGGER.debug("Sending event {} to {}", event, deadLetterPipeline);
+        Event newEvent = new Event();
+        String errorKey = "[error]";
+        String dlpRoute = (String) event.getField("[@metadata][dlp_route]");
+        if (dlpRoute == null || dlpRoute.isEmpty()){
+            dlpRoute = deadLetterPipeline;
+            newEvent.setField("[event][original]", event.toJson());
+        } else {
+            newEvent.setField("[event][original]", event.getField("[event][original]"));
+            String[] path = dlpRoute.split(",");
+            for (String pathPart : path){
+                if (pathPart.equals(deadLetterPipeline)){
+                    LOGGER.warn("DLP Cycle detected: Event{} has already been through {}, path={}", event, pathPart, dlpRoute);
+                    return;
+                }
+            }
+            dlpRoute = String.format("%s,%s", dlpRoute, deadLetterPipeline);
+        }
+        newEvent.setField(errorKey, metadata);
+        newEvent.setField("[@metadata][dlp_route]", dlpRoute);
+        newEvent.setField(String.format("%s[source][pipeline]", errorKey), sourcePipeline);
+        JrubyEventExtLibrary.RubyEvent er = JrubyEventExtLibrary.RubyEvent.newRubyEvent(RUBY, newEvent);
+        pipelineBus.sendEvents(deadLetterPipelineOutput, Lists.newArrayList(er), true);
+        eventsWritten.increment();
+    }
+
+    @Override
+    public void writeEntry(Event event, String pluginName, String pluginId, String reason) throws IOException {
+        Map<String, Object> metadata = new HashMap<>();
+        Map<String, String> source = new HashMap<>();
+        source.put("plugin_name", pluginName);
+        source.put("plugin_id", pluginId);
+        metadata.put("source", source);
+        metadata.put("message", reason);
+        writeEntry(event, metadata);
+    }
+
+    @Override
+    public void close() {
+        if (isOpen.compareAndSet(true, false)) {
+            pipelineBus.unregisterSender(deadLetterPipelineOutput, Lists.newArrayList(deadLetterPipeline));
+        }
+    }
+
+    @Override
+    public boolean isOpen() {
+        return isOpen.get();
+    }
+
+    @Override
+    public long getCurrentQueueSize() {
+        return eventsWritten.longValue();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/dlq/IDeadLetterQueueWriter.java b/logstash-core/src/main/java/org/logstash/common/dlq/IDeadLetterQueueWriter.java
new file mode 100644
index 00000000000..d30f5878ddc
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/dlq/IDeadLetterQueueWriter.java
@@ -0,0 +1,14 @@
+package org.logstash.common.dlq;
+
+import org.logstash.Event;
+import java.io.IOException;
+import java.util.Map;
+
+
+public interface IDeadLetterQueueWriter {
+    void writeEntry(Event event, String pluginName, String pluginId, String reason) throws IOException;
+    void writeEntry(Event event, Map<String, Object> reason) throws IOException;
+    void close();
+    boolean isOpen();
+    long getCurrentQueueSize();
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/failure/DLQFailureHandler.java b/logstash-core/src/main/java/org/logstash/common/failure/DLQFailureHandler.java
new file mode 100644
index 00000000000..f802d620f5d
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/failure/DLQFailureHandler.java
@@ -0,0 +1,42 @@
+package org.logstash.common.failure;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.Event;
+import org.logstash.common.dlq.IDeadLetterQueueWriter;
+
+import java.io.IOException;
+import java.util.HashMap;
+import java.util.Map;
+
+public class DLQFailureHandler implements FailureHandler{
+
+    private static final Logger LOGGER = LogManager.getLogger(DLQFailureHandler.class);
+    private final IDeadLetterQueueWriter writer;
+
+    public DLQFailureHandler(IDeadLetterQueueWriter writer){
+        this.writer = writer;
+    }
+
+    public void handle(final Event event, final Exception exception) {
+        Map<String, Object> failureMetadata = new HashMap<>();
+        fillFailureMetadata(failureMetadata, exception);
+        handle(event, failureMetadata);
+    }
+
+    @Override
+    public void handle(final Event event, final Map<String, Object> failureMetadata, final Exception exception) {
+        fillFailureMetadata(failureMetadata, exception);
+        handle(event, failureMetadata);
+    }
+
+    @Override
+    public void handle(final Event event, final Map<String, Object> failureMetadata) {
+        try{
+            LOGGER.warn("Writing {} to DLQ -> {}", event.toMap(), failureMetadata);
+            writer.writeEntry(event, failureMetadata);
+        } catch (IOException e){
+            LOGGER.error("Unable to write to DLQ", e);
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/failure/DestructiveFailureHandler.java b/logstash-core/src/main/java/org/logstash/common/failure/DestructiveFailureHandler.java
new file mode 100644
index 00000000000..13d69bd2ce3
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/failure/DestructiveFailureHandler.java
@@ -0,0 +1,30 @@
+package org.logstash.common.failure;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.Event;
+
+import java.util.Map;
+
+public class DestructiveFailureHandler implements FailureHandler {
+
+    private static final Logger LOGGER = LogManager.getLogger(DestructiveFailureHandler.class);
+
+    @Override
+    public void handle(Event event, Exception exception){
+        LOGGER.error("Failure, blowing up the pipeline {}", event.toMap(), exception);
+        throw new RuntimeException(exception);
+    }
+
+    @Override
+    public void handle(Event event, Map<String, Object> failureMetadata, Exception exception){
+        LOGGER.error("Failure, blowing up the pipeline {} - {}", event.toMap(), failureMetadata, exception);
+        throw new RuntimeException(exception);
+    }
+
+    @Override
+    public void handle(Event event, Map<String, Object> failureMetadata) {
+        LOGGER.error("Failure, blowing up the pipeline {} - {}", event.toMap(), failureMetadata);
+        throw new RuntimeException(failureMetadata.toString());
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/failure/DropFailureHandler.java b/logstash-core/src/main/java/org/logstash/common/failure/DropFailureHandler.java
new file mode 100644
index 00000000000..9217fefec74
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/failure/DropFailureHandler.java
@@ -0,0 +1,30 @@
+package org.logstash.common.failure;
+
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.Event;
+
+import java.util.Map;
+
+public class DropFailureHandler implements FailureHandler {
+
+    private static final Logger LOGGER = LogManager.getLogger(DropFailureHandler.class);
+
+    @Override
+    public void handle(Event event, Exception exception) {
+        LOGGER.error("Dropping {} - {}", event.toMap(), exception);
+        event.cancel();
+    }
+
+    @Override
+    public void handle(Event event, Map<String, Object> failureMetadata, Exception exception) {
+        LOGGER.error("Dropping {} - {}", event.toMap(), failureMetadata, exception);
+        event.cancel();
+    }
+
+    @Override
+    public void handle(Event event, Map<String, Object> failureMetadata) {
+        LOGGER.error("Dropping {} - {}", event.toMap(), failureMetadata);
+        event.cancel();
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/failure/FailureHandler.java b/logstash-core/src/main/java/org/logstash/common/failure/FailureHandler.java
new file mode 100644
index 00000000000..1332173294e
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/common/failure/FailureHandler.java
@@ -0,0 +1,24 @@
+package org.logstash.common.failure;
+
+import org.logstash.Event;
+
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.Map;
+
+// General Interface to handle failures. *Extremely* rough WIP
+// Rubify to add to ExecutionContext
+
+public interface FailureHandler {
+    void handle(Event event, Exception exception);
+    void handle(Event event, Map<String, Object> failureMetadata, Exception exception);
+    void handle(Event event, Map<String, Object> failureMetadata);
+
+    default void fillFailureMetadata(Map<String, Object> failureMetadata, Exception exception){
+        failureMetadata.put("message", exception.getMessage());
+        failureMetadata.put("type", exception.getClass().toString());
+        failureMetadata.put("stack_trace", Arrays.toString(exception.getStackTrace()));
+        Map<String, String> source = new HashMap<>();
+        failureMetadata.put("source", source);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
index b1d2e2cea62..ed0cc20d639 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
@@ -46,6 +46,7 @@
 import java.nio.file.StandardCopyOption;
 import java.time.Duration;
 import java.time.Instant;
+import java.util.Map;
 import java.util.concurrent.Executors;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
@@ -63,11 +64,12 @@
 import org.logstash.FieldReference;
 import org.logstash.FileLockFactory;
 import org.logstash.Timestamp;
+import org.logstash.common.dlq.IDeadLetterQueueWriter;
 
 import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
 import static org.logstash.common.io.RecordIOReader.SegmentStatus;
 
-public final class DeadLetterQueueWriter implements Closeable {
+public final class DeadLetterQueueWriter implements Closeable, IDeadLetterQueueWriter {
 
     @VisibleForTesting
     static final String SEGMENT_FILE_PATTERN = "%d.log";
@@ -126,6 +128,12 @@ public void writeEntry(Event event, String pluginName, String pluginId, String r
         writeEntry(new DLQEntry(event, pluginName, pluginId, reason));
     }
 
+    @Override
+
+    public void writeEntry(Event event, Map<String, Object> reason) throws IOException {
+        writeEntry(new DLQEntry(event, "-", "-", reason.toString()));
+    }
+
     @Override
     public void close() {
         if (open.compareAndSet(true, false)) {
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/CompiledPipeline.java b/logstash-core/src/main/java/org/logstash/config/ir/CompiledPipeline.java
index d31794cde4a..1bd70fb77ae 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/CompiledPipeline.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/CompiledPipeline.java
@@ -24,10 +24,12 @@
 import org.jruby.RubyArray;
 import org.jruby.RubyHash;
 import org.jruby.runtime.builtin.IRubyObject;
+import org.logstash.Event;
 import org.logstash.RubyUtil;
 import org.logstash.Rubyfier;
 import org.logstash.common.EnvironmentVariableProvider;
 import org.logstash.common.SourceWithMetadata;
+import org.logstash.common.failure.FailureHandler;
 import org.logstash.config.ir.compiler.AbstractFilterDelegatorExt;
 import org.logstash.config.ir.compiler.AbstractOutputDelegatorExt;
 import org.logstash.config.ir.compiler.ComputeStepSyntaxElement;
@@ -36,7 +38,6 @@
 import org.logstash.config.ir.compiler.EventCondition;
 import org.logstash.config.ir.compiler.RubyIntegration;
 import org.logstash.config.ir.compiler.SplitDataset;
-import org.logstash.config.ir.expression.*;
 import org.logstash.config.ir.graph.SeparatorVertex;
 import org.logstash.config.ir.graph.IfVertex;
 import org.logstash.config.ir.graph.PluginVertex;
@@ -47,8 +48,6 @@
 import org.logstash.plugins.ConfigVariableExpander;
 import org.logstash.secret.store.SecretStore;
 
-import java.lang.reflect.Constructor;
-import java.lang.reflect.InvocationTargetException;
 import java.util.*;
 import java.util.stream.Collectors;
 import java.util.stream.Stream;
@@ -66,6 +65,7 @@ public final class CompiledPipeline {
 
     private static final Logger LOGGER = LogManager.getLogger(CompiledPipeline.class);
 
+    private FailureHandler failureHandler = null;
     /**
      * Compiler for conditional expressions that turn {@link IfVertex} into {@link EventCondition}.
      */
@@ -100,16 +100,28 @@ public CompiledPipeline(
             final PipelineIR pipelineIR,
             final RubyIntegration.PluginFactory pluginFactory)
     {
-        this(pipelineIR, pluginFactory, null);
+        this(pipelineIR, pluginFactory, null, null);
     }
 
     public CompiledPipeline(
             final PipelineIR pipelineIR,
             final RubyIntegration.PluginFactory pluginFactory,
             final SecretStore secretStore)
+    {
+        this(pipelineIR, pluginFactory, secretStore, null);
+    }
+
+
+    public CompiledPipeline(
+            final PipelineIR pipelineIR,
+            final RubyIntegration.PluginFactory pluginFactory,
+            final SecretStore secretStore,
+            final FailureHandler failureHandler)
     {
         this.pipelineIR = pipelineIR;
         this.pluginFactory = pluginFactory;
+        // Failure Handler here is to catch weird pipeline bugs, such as field reference comparisons.
+        this.failureHandler = failureHandler;
         try (ConfigVariableExpander cve = new ConfigVariableExpander(
                 secretStore,
                 EnvironmentVariableProvider.defaultProvider())) {
@@ -299,19 +311,38 @@ public int compute(final Collection<RubyEvent> batch, final boolean flush, final
                 // send batch one-by-one as single-element batches down the filters
                 for (final RubyEvent e : batch) {
                     filterBatch.set(0, e);
-                    _compute(filterBatch, outputBatch, flush, shutdown);
+                    Event copy = e.getEvent().clone();
+                    try {
+                        _compute(filterBatch, outputBatch, flush, shutdown);
+                    }
+                    catch (Exception ex){
+                        LOGGER.error("Error in filters", ex);
+                        failureHandler.handle(copy, ex);
+                    }
                 }
-                compiledOutputs.compute(outputBatch, flush, shutdown);
+                computeOutputs(outputBatch, flush, shutdown);
                 return outputBatch.size();
             } else if (flush || shutdown) {
+                // TODO: Handle errors here.
                 @SuppressWarnings({"unchecked"}) final RubyArray<RubyEvent> outputBatch = RubyUtil.RUBY.newArray();
                 _compute(EMPTY_ARRAY, outputBatch, flush, shutdown);
-                compiledOutputs.compute(outputBatch, flush, shutdown);
+                computeOutputs(outputBatch, flush, shutdown);
                 return outputBatch.size();
             }
             return 0;
         }
 
+        private void computeOutputs(RubyArray<RubyEvent> outputBatch, boolean flush, boolean shutdown) {
+            try {
+                compiledOutputs.compute(outputBatch, flush, shutdown);
+            }catch (Exception ex){
+                LOGGER.error("Error in outputs", ex);
+                for (Object re : outputBatch.toArray()) {
+                    failureHandler.handle(((RubyEvent)re).getEvent(), ex);
+                }
+            }
+        }
+
         private void _compute(final RubyArray<RubyEvent> batch, final RubyArray<RubyEvent> outputBatch, final boolean flush, final boolean shutdown) {
             final Collection<RubyEvent> result = compiledFilters.compute(batch, flush, shutdown);
             copyNonCancelledEvents(result, outputBatch);
diff --git a/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java b/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java
index 8c8a91327e7..68be63eb0e7 100644
--- a/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java
+++ b/logstash-core/src/main/java/org/logstash/config/ir/compiler/EventCondition.java
@@ -56,6 +56,7 @@
 import org.logstash.config.ir.expression.binary.RegexEq;
 import org.logstash.config.ir.expression.unary.Not;
 import org.logstash.config.ir.expression.unary.Truthy;
+import org.logstash.execution.PipelineException;
 import org.logstash.ext.JrubyEventExtLibrary;
 
 /**
@@ -424,28 +425,31 @@ private static EventCondition compareConstants(final ValueExpression left,
 
         private static EventCondition compareFields(final EventValueExpression left,
                                                     final EventValueExpression right, final Predicate<Integer> operator) {
-            final FieldReference one = FieldReference.from(left.getFieldName());
-            final FieldReference other = FieldReference.from(right.getFieldName());
-            return event -> {
-                final Event javaEvent = event.getEvent();
-                return operator.test(
-                        compare(
-                                javaEvent.getUnconvertedField(one), javaEvent.getUnconvertedField(other)
-                        )
-                );
-            };
+            return event ->
+                    operator.test(compare(mandatoryObjectFromValueExpression(left, event.getEvent()),
+                                          mandatoryObjectFromValueExpression(right, event.getEvent())));
+        }
+
+        private static Object mandatoryObjectFromValueExpression(EventValueExpression valueExpression, Event event) {
+            FieldReference fieldReference = FieldReference.from(valueExpression.getFieldName());
+            Object object = event.getUnconvertedField(fieldReference);
+            if (object == null){
+                throw new IllegalStateException(
+                        String.format("Line:%d, Column:%d: Invalid comparison - Field reference %s not in event",
+                                      valueExpression.getSourceWithMetadata().getLine(),
+                                      valueExpression.getSourceWithMetadata().getColumn(),
+                                      valueExpression.getFieldName()));
+            }
+            return object;
         }
 
         @SuppressWarnings("unchecked")
         private static EventCondition compareFieldToConstant(final EventValueExpression left,
                                                              final ValueExpression right, final Predicate<Integer> operator) {
-            final FieldReference one = FieldReference.from(left.getFieldName());
             final Comparable<IRubyObject> other =
                     (Comparable<IRubyObject>) Rubyfier.deep(RubyUtil.RUBY, right.get());
-            return event -> {
-                final Event javaEvent = event.getEvent();
-                return operator.test(compare(javaEvent.getUnconvertedField(one), other));
-            };
+            return event ->
+                 operator.test(compare(mandatoryObjectFromValueExpression(left, event.getEvent()), other));
         }
 
         @SuppressWarnings("unchecked")
@@ -694,6 +698,9 @@ private static String getUnexpectedTypeDetails(Object unexpected) {
                     details = (expression.getSourceWithMetadata() != null) ? expression.getSourceWithMetadata().toString()
                                                                            : expression.toString();
                 } else {
+                    if (unexpected == null){
+                        return "null";
+                    }
                     details = unexpected.toString();
                 }
                 return String.format("%s:%s", unexpected.getClass(), details);
diff --git a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
index 139648b72ff..ae8a2a99159 100644
--- a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
@@ -53,6 +53,7 @@
 import org.logstash.common.DeadLetterQueueFactory;
 import org.logstash.common.EnvironmentVariableProvider;
 import org.logstash.common.SourceWithMetadata;
+import org.logstash.common.failure.DLQFailureHandler;
 import org.logstash.config.ir.ConfigCompiler;
 import org.logstash.config.ir.InvalidIRException;
 import org.logstash.config.ir.PipelineConfig;
@@ -64,6 +65,7 @@
 import org.logstash.instrument.metrics.MetricKeys;
 import org.logstash.instrument.metrics.NullMetricExt;
 import org.logstash.plugins.ConfigVariableExpander;
+import org.logstash.plugins.pipeline.PipelineBus;
 import org.logstash.secret.store.SecretStore;
 import org.logstash.secret.store.SecretStoreExt;
 
@@ -133,7 +135,9 @@ public class AbstractPipelineExt extends RubyBasicObject {
 
     private AbstractMetricExt metric;
 
-    private IRubyObject dlqWriter;
+    protected IRubyObject dlqWriter;
+
+    protected IRubyObject failureHandler;
 
     private PipelineReporterExt reporter;
 
@@ -274,18 +278,13 @@ public final IRubyObject lir(final ThreadContext context) {
         return JavaUtil.convertJavaToUsableRubyObject(context.runtime, lir);
     }
 
-    @JRubyMethod(name = "dlq_writer")
-    public final IRubyObject dlqWriter(final ThreadContext context) {
+    protected final IRubyObject dlqWriter(final ThreadContext context, final IRubyObject agent) {
         if (dlqWriter == null) {
-            if (dlqEnabled(context).isTrue()) {
-                dlqWriter = JavaUtil.convertJavaToUsableRubyObject(
-                    context.runtime,
-                    DeadLetterQueueFactory.getWriter(
-                        pipelineId.asJavaString(),
-                        getSetting(context, "path.dead_letter_queue").asJavaString(),
-                        getSetting(context, "dead_letter_queue.max_bytes").convertToInteger().getLongValue(),
-                        Duration.ofMillis(getSetting(context, "dead_letter_queue.flush_interval").convertToInteger().getLongValue()))
-                    );
+            if (!getSetting(context, "pipeline.dead_letter_pipeline").isNil()) {
+                PipelineBus pipelineBus = agent.callMethod(context, "pipeline_bus").toJava(PipelineBus.class);
+                dlqWriter = getDeadLetterPipelineWriter(context, pipelineBus);
+            } else if (dlqEnabled(context).isTrue()) {
+                dlqWriter = getDlqFileWriter(context);
             } else {
                 dlqWriter = RubyUtil.DUMMY_DLQ_WRITER_CLASS.callMethod(context, "new");
             }
@@ -293,6 +292,33 @@ public final IRubyObject dlqWriter(final ThreadContext context) {
         return dlqWriter;
     }
 
+
+    @JRubyMethod(name = "dlq_writer")
+    public final IRubyObject dlqWriter(final ThreadContext context) {
+        return dlqWriter;
+    }
+
+    private IRubyObject getDeadLetterPipelineWriter(ThreadContext context, PipelineBus pipelineBus){
+        return JavaUtil.convertJavaToUsableRubyObject(
+                context.runtime,
+                DeadLetterQueueFactory.getWriter(
+                        pipelineId.asJavaString(),
+                        getSetting(context, "pipeline.dead_letter_pipeline").asJavaString(),
+                        pipelineBus)
+        );
+    }
+
+    private IRubyObject getDlqFileWriter(ThreadContext context) {
+        return JavaUtil.convertJavaToUsableRubyObject(
+                context.runtime,
+                DeadLetterQueueFactory.getWriter(
+                        pipelineId.asJavaString(),
+                        getSetting(context, "path.dead_letter_queue").asJavaString(),
+                        getSetting(context, "dead_letter_queue.max_bytes").convertToInteger().getLongValue(),
+                        Duration.ofMillis(getSetting(context, "dead_letter_queue.flush_interval").convertToInteger().getLongValue()))
+        );
+    }
+
     @JRubyMethod(name = "dlq_enabled?")
     public final IRubyObject dlqEnabled(final ThreadContext context) {
         return getSetting(context, "dead_letter_queue.enable");
diff --git a/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java
index a4fd279b91a..44d074bc812 100644
--- a/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/JavaBasePipelineExt.java
@@ -20,6 +20,7 @@
 
 package org.logstash.execution;
 
+import com.headius.invokebinder.transform.Drop;
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
 import org.jruby.Ruby;
@@ -33,6 +34,11 @@
 import org.jruby.runtime.builtin.IRubyObject;
 import org.logstash.RubyUtil;
 import org.logstash.common.IncompleteSourceWithMetadataException;
+import org.logstash.common.dlq.IDeadLetterQueueWriter;
+import org.logstash.common.failure.DLQFailureHandler;
+import org.logstash.common.failure.DestructiveFailureHandler;
+import org.logstash.common.failure.DropFailureHandler;
+import org.logstash.common.failure.FailureHandler;
 import org.logstash.config.ir.CompiledPipeline;
 import org.logstash.execution.queue.QueueWriter;
 import org.logstash.ext.JRubyWrappedWriteClientExt;
@@ -79,10 +85,11 @@ public JavaBasePipelineExt initialize(final ThreadContext context, final IRubyOb
                 ).initialize(context, pipelineId(), metric()),
                 new ExecutionContextFactoryExt(
                     context.runtime, RubyUtil.EXECUTION_CONTEXT_FACTORY_CLASS
-                ).initialize(context, args[3], this, dlqWriter(context)),
+                ).initialize(context, args[3], this, dlqWriter(context, args[3])),
                 RubyUtil.FILTER_DELEGATOR_CLASS
             ),
-            getSecretStore(context)
+            getSecretStore(context),
+            getFailureHandler(context)
         );
         inputs = RubyArray.newArray(context.runtime, lirExecution.inputs());
         filters = RubyArray.newArray(context.runtime, lirExecution.filters());
@@ -96,6 +103,20 @@ public JavaBasePipelineExt initialize(final ThreadContext context, final IRubyOb
         return this;
     }
 
+    public FailureHandler getFailureHandler(final ThreadContext context){
+        String failureHandler = getSetting(context, "pipeline.failure_handler").asJavaString();
+        switch (failureHandler){
+            case "dead_letter":
+                return new DLQFailureHandler(dlqWriter.toJava(IDeadLetterQueueWriter.class));
+            case "drop":
+                return new DropFailureHandler();
+            case "explode":
+                return new DestructiveFailureHandler();
+            default:
+                throw new IllegalArgumentException("Invalid value for failure handler " + failureHandler);
+        }
+    }
+
     @JRubyMethod(name = "lir_execution")
     public IRubyObject lirExecution(final ThreadContext context) {
         return JavaUtil.convertJavaToUsableRubyObject(context.runtime, lirExecution);
diff --git a/logstash-core/src/main/java/org/logstash/execution/PipelineException.java b/logstash-core/src/main/java/org/logstash/execution/PipelineException.java
new file mode 100644
index 00000000000..1d8ba7d9c30
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/execution/PipelineException.java
@@ -0,0 +1,9 @@
+package org.logstash.execution;
+
+public class PipelineException extends RuntimeException {
+    private static final long serialVersionUID = 1L;
+
+    public PipelineException(final String message){
+        super(message);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/plugins/pipeline/AddressState.java b/logstash-core/src/main/java/org/logstash/plugins/pipeline/AddressState.java
index 205282f9a4a..14ecb053781 100644
--- a/logstash-core/src/main/java/org/logstash/plugins/pipeline/AddressState.java
+++ b/logstash-core/src/main/java/org/logstash/plugins/pipeline/AddressState.java
@@ -91,4 +91,12 @@ boolean hasOutput(PipelineOutput output) {
     public Set<PipelineOutput> getOutputs() {
         return outputs;
     }
+
+    @Override
+    public String toString() {
+        return "AddressState{" +
+                "outputs=" + outputs +
+                ", input=" + input +
+                '}';
+    }
 }
