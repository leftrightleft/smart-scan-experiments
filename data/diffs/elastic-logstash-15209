diff --git a/docs/gs-index.asciidoc b/docs/gs-index.asciidoc
index feec48b8713..e499c7a7691 100644
--- a/docs/gs-index.asciidoc
+++ b/docs/gs-index.asciidoc
@@ -1,5 +1,5 @@
 [[logstash-reference]]
-= Logstash Reference
+= {ls} Reference
 
 :branch:                5.4
 :major-version:         5.4
@@ -22,15 +22,15 @@ release-state can be: released | prerelease | unreleased
 :security:              X-Pack Security
 
 [[introduction]]
-== Logstash Introduction
+== {ls} Introduction
 
-Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically
+{ls} is an open source data collection engine with real-time pipelining capabilities. {ls} can dynamically
 unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all
 your data for diverse advanced downstream analytics and visualization use cases.
 
-While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
+While {ls} originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
 type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many
-native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater
+native codecs further simplifying the ingestion process. {ls} accelerates your insights by harnessing a greater
 volume and variety of data.
 
 include::static/introduction.asciidoc[]
diff --git a/docs/include/attributes-ls.asciidoc b/docs/include/attributes-ls.asciidoc
index 714982cadef..831b0bbf222 100644
--- a/docs/include/attributes-ls.asciidoc
+++ b/docs/include/attributes-ls.asciidoc
@@ -1,6 +1,6 @@
 /////
-These settings control attributes for Logstash core content 
-in the Logstash Reference (LSR) only.
+These settings control attributes for {ls} core content 
+in the {ls} Reference (LSR) only.
  
 Shared attributes for the plugin docs (in the LSR and VPR) should
 go in /docs/include/attributes-lsplugins.asciidoc instead 
diff --git a/docs/include/attributes-lsplugins.asciidoc b/docs/include/attributes-lsplugins.asciidoc
index 674bcc03c86..55ce40859bb 100644
--- a/docs/include/attributes-lsplugins.asciidoc
+++ b/docs/include/attributes-lsplugins.asciidoc
@@ -10,4 +10,4 @@ Text is written to accommodate multiple versions because plugins are not stack v
 /////
 
 
-:ecs-default: When the `ecs_compatibility` option for this plugin is not explicitly set, its effective value depends on the `pipeline.ecs_compatibility` setting for the pipeline in `pipelines.yml`, or globally in {logstash-ref}/logstash-settings-file.html[`logstash.yml`], allowing you to specify your preferred behavior at the plugin, pipeline, or system level. If no preference is specified, the default value is `v8` for Logstash 8 or `disabled` for all earlier releases of Logstash. For more information about ECS compatibility settings in Logstash and plugins, see {logstash-ref}/ecs-ls.html[ECS in Logstash].
+:ecs-default: When the `ecs_compatibility` option for this plugin is not explicitly set, its effective value depends on the `pipeline.ecs_compatibility` setting for the pipeline in `pipelines.yml`, or globally in {logstash-ref}/logstash-settings-file.html[`logstash.yml`], allowing you to specify your preferred behavior at the plugin, pipeline, or system level. If no preference is specified, the default value is `v8` for {ls} 8 or `disabled` for all earlier releases of {ls}. For more information about ECS compatibility settings in {ls} and plugins, see {logstash-ref}/ecs-ls.html[ECS in {ls}].
diff --git a/docs/include/filter.asciidoc b/docs/include/filter.asciidoc
index 23035490f5d..de8cbba9f32 100644
--- a/docs/include/filter.asciidoc
+++ b/docs/include/filter.asciidoc
@@ -133,10 +133,10 @@ endif::[]
   * Value type is {logstash-ref}/configuration-file-structure.html#string[string]
   * There is no default value for this setting.
 
-Add a unique `ID` to the plugin configuration. If no ID is specified, Logstash will generate one.
+Add a unique `ID` to the plugin configuration. If no ID is specified, {ls} will generate one.
 It is strongly recommended to set this ID in your configuration. This is particularly useful
 when you have two or more plugins of the same type, for example, if you have 2 {plugin} filters.
-Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.
+Adding a named ID in this case will help in monitoring {ls} when using the monitoring APIs.
 
 
 ["source","json",subs="attributes"]
diff --git a/docs/include/input.asciidoc b/docs/include/input.asciidoc
index d0b9b5cd057..50692cbb29a 100644
--- a/docs/include/input.asciidoc
+++ b/docs/include/input.asciidoc
@@ -68,7 +68,7 @@ ifndef::default_codec[]
   * Default value is `"plain"`
 endif::[]
 
-The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your Logstash pipeline.
+The codec used for input data. Input codecs are a convenient method for decoding your data before it enters the input, without needing a separate filter in your {ls} pipeline.
 endif::no_codec[]
 
 
@@ -98,10 +98,10 @@ endif::[]
   * Value type is {logstash-ref}/configuration-file-structure.html#string[string]
   * There is no default value for this setting.
 
-Add a unique `ID` to the plugin configuration. If no ID is specified, Logstash will generate one.
+Add a unique `ID` to the plugin configuration. If no ID is specified, {ls} will generate one.
 It is strongly recommended to set this ID in your configuration. This is particularly useful
 when you have two or more plugins of the same type, for example, if you have 2 {plugin} inputs.
-Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.
+Adding a named ID in this case will help in monitoring {ls} when using the monitoring APIs.
 
 ["source","json",subs="attributes"]
 ---------------------------------------------------------------------------------------------------
@@ -152,22 +152,22 @@ If you try to set a type on an event that already has one (for
 example when you send an event from a shipper to an indexer) then
 a new input will not override the existing type. A type set at
 the shipper stays with that event for its life even
-when sent to another Logstash server.
+when sent to another {ls} server.
 
 ifeval::["{type}"=="input"]
 ifeval::["{plugin}"=="beats"]
 
 ifeval::["{versioned_docs}"!="true"]
 NOTE: The Beats shipper automatically sets the `type` field on the event.
-You cannot override this setting in the Logstash config. If you specify
+You cannot override this setting in the {ls} config. If you specify
 a setting for the <<plugins-inputs-beats-type,`type`>> config option in
-Logstash, it is ignored.
+{ls}, it is ignored.
 endif::[]
 ifeval::["{versioned_docs}"=="true"]
 NOTE: The Beats shipper automatically sets the `type` field on the event.
-You cannot override this setting in the Logstash config. If you specify
+You cannot override this setting in the {ls} config. If you specify
 a setting for the <<{version}-plugins-inputs-beats-type,`type`>> config option in
-Logstash, it is ignored.
+{ls}, it is ignored.
 endif::[]
 
 endif::[]
diff --git a/docs/include/output.asciidoc b/docs/include/output.asciidoc
index 5d8090645af..622cd396cc2 100644
--- a/docs/include/output.asciidoc
+++ b/docs/include/output.asciidoc
@@ -46,7 +46,7 @@ ifndef::default_codec[]
   * Default value is `"plain"`
 endif::[]
 
-The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output without needing a separate filter in your Logstash pipeline.
+The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output without needing a separate filter in your {ls} pipeline.
 endif::no_codec[]
 
 ifeval::["{versioned_docs}"!="true"]
@@ -75,10 +75,10 @@ endif::[]
   * Value type is {logstash-ref}/configuration-file-structure.html#string[string]
   * There is no default value for this setting.
 
-Add a unique `ID` to the plugin configuration. If no ID is specified, Logstash will generate one.
+Add a unique `ID` to the plugin configuration. If no ID is specified, {ls} will generate one.
 It is strongly recommended to set this ID in your configuration. This is particularly useful
 when you have two or more plugins of the same type. For example, if you have 2 {plugin} outputs.
-Adding a named ID in this case will help in monitoring Logstash when using the monitoring APIs.
+Adding a named ID in this case will help in monitoring {ls} when using the monitoring APIs.
 
 ["source","json",subs="attributes"]
 ---------------------------------------------------------------------------------------------------
diff --git a/docs/include/version-list-intro.asciidoc b/docs/include/version-list-intro.asciidoc
index c396d201c99..85a9264d13f 100644
--- a/docs/include/version-list-intro.asciidoc
+++ b/docs/include/version-list-intro.asciidoc
@@ -11,4 +11,4 @@ To see which version of the plugin you have installed, run `bin/logstash-plugin
 list --verbose`. 
 
 NOTE: Versioned plugin documentation is not available for plugins released prior
-to Logstash 6.0.
+to {ls} 6.0.
diff --git a/docs/index.asciidoc b/docs/index.asciidoc
index ef4f1f7b86a..50b66651453 100644
--- a/docs/index.asciidoc
+++ b/docs/index.asciidoc
@@ -1,5 +1,5 @@
 [[logstash-reference]]
-= Logstash Reference
+= {ls} Reference
 
 include::{docs-root}/shared/versions/stack/{source_branch}.asciidoc[]
 include::{docs-root}/shared/attributes.asciidoc[]
@@ -22,22 +22,22 @@ include::./include/attributes-lsplugins.asciidoc[]
 
 
 [[introduction]]
-== Logstash Introduction
+== {ls} Introduction
 
-Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically
+{ls} is an open source data collection engine with real-time pipelining capabilities. {ls} can dynamically
 unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all
 your data for diverse advanced downstream analytics and visualization use cases.
 
-While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
+While {ls} originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
 type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many
-native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater
+native codecs further simplifying the ingestion process. {ls} accelerates your insights by harnessing a greater
 volume and variety of data.
 
 // The pass blocks here point to the correct repository for the edit links in the guide.
 
 // Introduction
 
-// Getting Started with Logstash
+// Getting Started with {ls}
 include::static/getting-started-with-logstash.asciidoc[]
 
 // Advanced LS Pipelines
@@ -52,7 +52,7 @@ include::static/ecs-compatibility.asciidoc[]
 // Processing details
 include::static/processing-info.asciidoc[]
 
-// Logstash setup
+// {ls} setup
 include::static/setting-up-logstash.asciidoc[]
 
 include::static/settings-file.asciidoc[]
@@ -71,7 +71,7 @@ include::static/logging.asciidoc[]
 
 include::static/shutdown.asciidoc[]
 
-// Upgrading Logstash
+// Upgrading {ls}
 include::static/upgrading.asciidoc[]
 
 // Configuring pipelines
@@ -80,7 +80,7 @@ include::static/pipeline-configuration.asciidoc[]
 // Security
 include::static/security/logstash.asciidoc[]
 
-// Advanced Logstash Configuration
+// Advanced {ls} Configuration
 include::static/configuration-advanced.asciidoc[]
 
 include::static/multiple-pipelines.asciidoc[]
@@ -102,9 +102,9 @@ include::static/field-reference.asciidoc[]
 //Ref, but not appear in the main TOC. The `exclude`attribute was carrying
 //forward for all subsequent topics under the `configuration.asciidoc` heading.
 //This include should remain after includes for all other topics under the
-//`Advanced Logstash Configuration` heading.
+//`Advanced {ls} Configuration` heading.
 
-// Logstash-to-Logstash
+// {ls}-to-{ls}
 include::static/ls-ls-config.asciidoc[]
 
 // Centralized configuration managements
@@ -112,7 +112,7 @@ include::static/config-management.asciidoc[]
 
 include::static/management/configuring-centralized-pipelines.asciidoc[]
 
-// Working with Logstash Modules
+// Working with {ls} Modules
 include::static/modules.asciidoc[]
 
 include::static/arcsight-module.asciidoc[]
@@ -173,7 +173,7 @@ include::static/config-details.asciidoc[]
 
 include::static/troubleshoot/troubleshooting.asciidoc[]
 
-// Contributing to Logstash
+// Contributing to {ls}
 :edit_url:
 include::static/contributing-to-logstash.asciidoc[]
 
@@ -185,7 +185,7 @@ include::static/filter.asciidoc[]
 
 include::static/output.asciidoc[]
 
-// Logstash Community Maintainer Guide
+// {ls} Community Maintainer Guide
 include::static/maintainer-guide.asciidoc[]
 
 // Plugin doc guidelines
@@ -200,7 +200,7 @@ include::static/contributing-patch.asciidoc[]
 
 include::static/contribute-core.asciidoc[]
 
-// Contributing to Logstash - JAVA EDITION
+// Contributing to {ls} - JAVA EDITION
 :edit_url:
 include::static/contributing-java-plugin.asciidoc[]
 
diff --git a/docs/static/advanced-pipeline.asciidoc b/docs/static/advanced-pipeline.asciidoc
index 648e4d58713..57081fac402 100644
--- a/docs/static/advanced-pipeline.asciidoc
+++ b/docs/static/advanced-pipeline.asciidoc
@@ -1,10 +1,10 @@
 [[advanced-pipeline]]
-=== Parsing Logs with Logstash
+=== Parsing Logs with {ls}
 
-In <<first-event>>, you created a basic Logstash pipeline to test your Logstash setup. In the real world, a Logstash
+In <<first-event>>, you created a basic {ls} pipeline to test your {ls} setup. In the real world, a {ls}
 pipeline is a bit more complex: it typically has one or more input, filter, and output plugins.
 
-In this section, you create a Logstash pipeline that uses Filebeat to take Apache web logs as input, parses those
+In this section, you create a {ls} pipeline that uses Filebeat to take Apache web logs as input, parses those
 logs to create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Rather than
 defining the pipeline configuration at the command line, you'll define the pipeline in a config file.
 
@@ -13,22 +13,22 @@ download the sample data set used in this example. Unpack the file.
 
 
 [[configuring-filebeat]]
-==== Configuring Filebeat to Send Log Lines to Logstash
+==== Configuring Filebeat to Send Log Lines to {ls}
 
-Before you create the Logstash pipeline, you'll configure Filebeat to send log lines to Logstash.
+Before you create the {ls} pipeline, you'll configure Filebeat to send log lines to {ls}.
 The https://github.com/elastic/beats/tree/main/filebeat[Filebeat] client is a lightweight, resource-friendly tool
-that collects logs from files on the server and forwards these logs to your Logstash instance for processing.
+that collects logs from files on the server and forwards these logs to your {ls} instance for processing.
 Filebeat is designed for reliability and low latency. Filebeat has a light resource footprint on the host machine,
-and the {logstash-ref}/plugins-inputs-beats.html[`Beats input`] plugin minimizes the resource demands on the Logstash
+and the {logstash-ref}/plugins-inputs-beats.html[`Beats input`] plugin minimizes the resource demands on the {ls}
 instance.
 
 NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your
-Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
+{ls} instance. For the purposes of this tutorial, {ls} and Filebeat are running on the
 same machine.
 
-The default Logstash installation includes the {logstash-ref}/plugins-inputs-beats.html[`Beats input`] plugin. The Beats
-input plugin enables Logstash to receive events from the Elastic Beats framework, which means that any Beat written
-to work with the Beats framework, such as Packetbeat and Metricbeat, can also send event data to Logstash.
+The default {ls} installation includes the {logstash-ref}/plugins-inputs-beats.html[`Beats input`] plugin. The Beats
+input plugin enables {ls} to receive events from the Elastic Beats framework, which means that any Beat written
+to work with the Beats framework, such as Packetbeat and Metricbeat, can also send event data to {ls}.
 
 To install Filebeat on your data source machine, download the appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page]. You can also refer to
 {filebeat-ref}/filebeat-installation-configuration.html[Filebeat quick start] for additional
@@ -65,12 +65,12 @@ NOTE: If you run Filebeat as root, you need to change ownership of the configura
 {beats-ref}/config-file-permissions.html[Config File Ownership and Permissions]
 in the _Beats Platform Reference_).
 
-Filebeat will attempt to connect on port 5044. Until Logstash starts with an active Beats plugin, there
+Filebeat will attempt to connect on port 5044. Until {ls} starts with an active Beats plugin, there
 won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
 
-==== Configuring Logstash for Filebeat Input
+==== Configuring {ls} for Filebeat Input
 
-Next, you create a Logstash configuration pipeline that uses the Beats input plugin to receive
+Next, you create a {ls} configuration pipeline that uses the Beats input plugin to receive
 events from Beats.
 
 The following text represents the skeleton of a configuration pipeline:
@@ -93,9 +93,9 @@ output {
 This skeleton is non-functional, because the input and output sections don’t have any valid options defined.
 
 To get started, copy and paste the skeleton configuration pipeline into a file named `first-pipeline.conf` in your home
-Logstash directory.
+{ls} directory.
 
-Next, configure your Logstash instance to use the Beats input plugin by adding the following lines to the `input` section
+Next, configure your {ls} instance to use the Beats input plugin by adding the following lines to the `input` section
 of the `first-pipeline.conf` file:
 
 [source,json]
@@ -105,8 +105,8 @@ of the `first-pipeline.conf` file:
     }
 --------------------------------------------------------------------------------
 
-You'll configure Logstash to write to Elasticsearch later. For now, you can add the following line
-to the `output` section so that the output is printed to stdout when you run Logstash:
+You'll configure {ls} to write to Elasticsearch later. For now, you can add the following line
+to the `output` section so that the output is printed to stdout when you run {ls}:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -141,19 +141,19 @@ bin/logstash -f first-pipeline.conf --config.test_and_exit
 
 The `--config.test_and_exit` option parses your configuration file and reports any errors.
 
-If the configuration file passes the configuration test, start Logstash with the following command:
+If the configuration file passes the configuration test, start {ls} with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
 bin/logstash -f first-pipeline.conf --config.reload.automatic
 --------------------------------------------------------------------------------
 
-The `--config.reload.automatic` option enables automatic config reloading so that you don't have to stop and restart Logstash
+The `--config.reload.automatic` option enables automatic config reloading so that you don't have to stop and restart {ls}
 every time you modify the configuration file.
 
-As Logstash starts up, you might see one or more warning messages about Logstash ignoring the `pipelines.yml` file. You
+As {ls} starts up, you might see one or more warning messages about {ls} ignoring the `pipelines.yml` file. You
 can safely ignore this warning. The `pipelines.yml` file is used for running <<multiple-pipelines,multiple pipelines>>
-in a single Logstash instance. For the examples shown here, you are running a single pipeline.
+in a single {ls} instance. For the examples shown here, you are running a single pipeline.
 
 If your pipeline is working correctly, you should see a series of events like the following written to the console:
 
@@ -195,7 +195,7 @@ is not ideal. You want to parse the log messages to create specific, named field
 To do this, you'll use the `grok` filter plugin.
 
 The {logstash-ref}/plugins-filters-grok.html[`grok`] filter plugin is one of several plugins that are available by default in
-Logstash. For details on how to manage Logstash plugins, see the <<working-with-plugins,reference documentation>> for
+{ls}. For details on how to manage {ls} plugins, see the <<working-with-plugins,reference documentation>> for
 the plugin manager.
 
 The `grok` filter plugin enables you to parse the unstructured log data into something structured and queryable.
@@ -261,7 +261,7 @@ output {
 }
 --------------------------------------------------------------------------------
 
-Save your changes. Because you've enabled automatic config reloading, you don't have to restart Logstash to
+Save your changes. Because you've enabled automatic config reloading, you don't have to restart {ls} to
 pick up your changes. However, you do need to force Filebeat to read the log file from scratch. To do this,
 go to the terminal window where Filebeat is running and press Ctrl+C to shut down Filebeat. Then delete the
 Filebeat registry file. For example, run:
@@ -281,10 +281,10 @@ Next, restart Filebeat with the following command:
 sudo ./filebeat -e -c filebeat.yml -d "publish"
 --------------------------------------------------------------------------------
 
-There might be a slight delay before Filebeat begins processing events if it needs to wait for Logstash to reload the
+There might be a slight delay before Filebeat begins processing events if it needs to wait for {ls} to reload the
 config file.
 
-After Logstash applies the grok pattern, the events will have the following JSON representation:
+After {ls} applies the grok pattern, the events will have the following JSON representation:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -333,7 +333,7 @@ In addition to parsing log data for better searches, filter plugins can derive s
 data. As an example, the {logstash-ref}/plugins-filters-geoip.html[`geoip`] plugin looks up IP addresses, derives geographic
 location information from the addresses, and adds that location information to the logs.
 
-Configure your Logstash instance to use the `geoip` filter plugin by adding the following lines to the `filter` section
+Configure your {ls} instance to use the `geoip` filter plugin by adding the following lines to the `filter` section
 of the `first-pipeline.conf` file:
 
 [source,json]
@@ -416,7 +416,7 @@ your data into Elasticsearch.
 
 TIP: {ess-leadin}
 
-The Logstash pipeline can index the data into an
+The {ls} pipeline can index the data into an
 Elasticsearch cluster. Edit the `first-pipeline.conf` file and replace the entire `output` section with the following
 text:
 
@@ -429,8 +429,8 @@ output {
 }
 --------------------------------------------------------------------------------
 
-With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes that
-Logstash and Elasticsearch are running on the same instance. You can specify a remote Elasticsearch instance by using
+With this configuration, {ls} uses http protocol to connect to Elasticsearch. The above example assumes that
+{ls} and Elasticsearch are running on the same instance. You can specify a remote Elasticsearch instance by using
 the `hosts` configuration to specify something like `hosts => [ "es-machine:9092" ]`.
 
 At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
@@ -470,7 +470,7 @@ sudo ./filebeat -e -c filebeat.yml -d "publish"
 [[testing-initial-pipeline]]
 ===== Testing Your Pipeline
 
-Now that the Logstash pipeline is configured to index the data into an
+Now that the {ls} pipeline is configured to index the data into an
 Elasticsearch cluster, you can query Elasticsearch.
 
 Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin.
@@ -481,7 +481,7 @@ Replace $DATE with the current date, in YYYY.MM.DD format:
 curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=response=200'
 --------------------------------------------------------------------------------
 
-NOTE: The date used in the index name is based on UTC, not the timezone where Logstash is running.
+NOTE: The date used in the index name is based on UTC, not the timezone where {ls} is running.
 If the query returns `index_not_found_exception`, make sure that `logstash-$DATE` reflects the actual
 name of the index. To see a list of available indexes, use this query: `curl 'localhost:9200/_cat/indices?v'`.
 
@@ -665,10 +665,10 @@ learn how to create a pipeline that uses multiple input and output plugins.
 === Stitching Together Multiple Input and Output Plugins
 
 The information you need to manage often comes from several disparate sources, and use cases can require multiple
-destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these
+destinations for your data. Your {ls} pipeline can use multiple input and output plugins to handle these
 requirements.
 
-In this section, you create a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
+In this section, you create a {ls} pipeline that takes input from a Twitter feed and the Filebeat client, then
 sends the information to an Elasticsearch cluster as well as writing the information directly to a file.
 
 [float]
@@ -689,7 +689,7 @@ key and secret, as well as your access token and secret. See the docs for the {l
 
 Like you did earlier when you worked on <<advanced-pipeline>>, create a config file (called `second-pipeline.conf`) that
 contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make
-sure you pass in the correct config file name when you run Logstash.
+sure you pass in the correct config file name when you run {ls}.
 
 Add the following lines to the `input` section of the `second-pipeline.conf` file, substituting your values for the
 placeholder values shown here:
@@ -707,11 +707,11 @@ placeholder values shown here:
 
 [float]
 [[configuring-lsf]]
-==== Configuring Filebeat to Send Log Lines to Logstash
+==== Configuring Filebeat to Send Log Lines to {ls}
 
 As you learned earlier in <<configuring-filebeat>>, the https://github.com/elastic/beats/tree/main/filebeat[Filebeat]
 client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your
-Logstash instance for processing.
+{ls} instance for processing.
 
 After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
 directory, and replace the contents with the following lines. Make sure `paths` points to your syslog:
@@ -735,7 +735,7 @@ Save your changes.
 To keep the configuration simple, you won't specify TLS/SSL settings as you would in a real world
 scenario.
 
-Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the `input` section
+Configure your {ls} instance to use the Filebeat input plugin by adding the following lines to the `input` section
 of the `second-pipeline.conf` file:
 
 [source,json]
@@ -747,12 +747,12 @@ of the `second-pipeline.conf` file:
 
 [float]
 [[logstash-file-output]]
-==== Writing Logstash Data to a File
+==== Writing {ls} Data to a File
 
-You can configure your Logstash pipeline to write data directly to a file with the
+You can configure your {ls} pipeline to write data directly to a file with the
 {logstash-ref}/plugins-outputs-file.html[`file`] output plugin.
 
-Configure your Logstash instance to use the `file` output plugin by adding the following lines to the `output` section
+Configure your {ls} instance to use the `file` output plugin by adding the following lines to the `output` section
 of the `second-pipeline.conf` file:
 
 [source,json]
@@ -769,7 +769,7 @@ of the `second-pipeline.conf` file:
 Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as
 providing redundant points of entry into the cluster when a particular node is unavailable.
 
-To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the `output` section of the `second-pipeline.conf` file to read:
+To configure your {ls} instance to write to multiple Elasticsearch nodes, edit the `output` section of the `second-pipeline.conf` file to read:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -781,7 +781,7 @@ output {
 --------------------------------------------------------------------------------
 
 Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the `hosts`
-parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
+parameter lists multiple IP addresses, {ls} load-balances requests across the list of addresses. Also note that
 the default port for Elasticsearch is `9200` and can be omitted in the configuration above.
 
 [float]
@@ -814,7 +814,7 @@ output {
 }
 --------------------------------------------------------------------------------
 
-Logstash is consuming data from the Twitter feed you configured, receiving data from Filebeat, and
+{ls} is consuming data from the Twitter feed you configured, receiving data from Filebeat, and
 indexing this information to three nodes in an Elasticsearch cluster as well as writing to a file.
 
 At the data source machine, run Filebeat with the following command:
@@ -824,7 +824,7 @@ At the data source machine, run Filebeat with the following command:
 sudo ./filebeat -e -c filebeat.yml -d "publish"
 --------------------------------------------------------------------------------
 
-Filebeat will attempt to connect on port 5044. Until Logstash starts with an active Beats plugin, there
+Filebeat will attempt to connect on port 5044. Until {ls} starts with an active Beats plugin, there
 won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
 
 To verify your configuration, run the following command:
@@ -835,7 +835,7 @@ bin/logstash -f second-pipeline.conf --config.test_and_exit
 --------------------------------------------------------------------------------
 
 The `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file
-passes the configuration test, start Logstash with the following command:
+passes the configuration test, start {ls} with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
diff --git a/docs/static/arcsight-module.asciidoc b/docs/static/arcsight-module.asciidoc
index eb2c148c475..b6981b588bc 100644
--- a/docs/static/arcsight-module.asciidoc
+++ b/docs/static/arcsight-module.asciidoc
@@ -1,18 +1,18 @@
 [role="xpack"]
 [[arcsight-module]]
-=== Logstash ArcSight Module
+=== {ls} ArcSight Module
 
 ++++
 <titleabbrev>ArcSight Module</titleabbrev>
 ++++
 
-NOTE: The Logstash ArcSight module is an
+NOTE: The {ls} ArcSight module is an
 https://www.elastic.co/products/x-pack[{xpack}] feature under the Basic License
 and is therefore free to use. Please contact
 mailto:arcsight@elastic.co[arcsight@elastic.co] for questions or more
 information.
 
-The Logstash ArcSight module enables you to easily integrate your ArcSight data with the Elastic Stack.
+The {ls} ArcSight module enables you to easily integrate your ArcSight data with the Elastic Stack.
 With a single command, the module taps directly into the ArcSight Smart Connector or the Event Broker,
 parses and indexes the security events into Elasticsearch, and installs a suite of Kibana dashboards
 to get you exploring your data immediately.
@@ -20,7 +20,7 @@ to get you exploring your data immediately.
 [[arcsight-prereqs]]
 ==== Prerequisites
 
-These instructions assume that Logstash, Elasticsearch, and Kibana are already
+These instructions assume that {ls}, Elasticsearch, and Kibana are already
 installed. The products you need are https://www.elastic.co/downloads[available
 to download] and easy to install. The Elastic Stack 5.6 (or later) and {xpack} are required for
 this module.   If you are using the Elastic Stack 6.2 and earlier, please see
@@ -30,14 +30,14 @@ for those versions.
 [[arcsight-architecture]]
 ==== Deployment Architecture
 
-The Logstash ArcSight module understands CEF (Common Event Format), and can
+The {ls} ArcSight module understands CEF (Common Event Format), and can
 accept, enrich, and index these events for analysis on the Elastic Stack. ADP
 contains two core data collection components for data streaming:
 
 * The _Smart Connectors (SC)_ are edge log collectors that parse and normalize
-data to CEF prior to publishing to the Logstash receiver.
+data to CEF prior to publishing to the {ls} receiver.
 * The _Event Broker_ is the central hub for incoming data and is based on
-open source Apache Kafka. The Logstash ArcSight module can consume directly from
+open source Apache Kafka. The {ls} ArcSight module can consume directly from
 Event Broker topics.
 
 [[arcsight-getting-started-smartconnector]]
@@ -54,14 +54,14 @@ image::static/images/arcsight-diagram-smart-connectors.svg[ArcSight Smart Connec
 Smart Connector has been configured to publish ArcSight data (to TCP port `5000`) using the CEF syslog
 destination.
 
-NOTE: Logstash, Elasticsearch, and Kibana must run locally. Note that you can also run
-Elasticsearch, Kibana and Logstash on separate hosts to consume data from ArcSight.
+NOTE: {ls}, Elasticsearch, and Kibana must run locally. Note that you can also run
+Elasticsearch, Kibana and {ls} on separate hosts to consume data from ArcSight.
 
 [[arcsight-instructions-smartconnector]]
 ===== Instructions for Smart Connector
 
-. Start the Logstash ArcSight module by running the following command in the
-Logstash install directory with your respective Smart Connector host and port:
+. Start the {ls} ArcSight module by running the following command in the
+{ls} install directory with your respective Smart Connector host and port:
 +
 ["source","shell",subs="attributes"]
 -----
@@ -73,10 +73,10 @@ bin/logstash --modules arcsight --setup \
 +
 --
 
-The `--modules arcsight` option spins up an ArcSight CEF-aware Logstash
+The `--modules arcsight` option spins up an ArcSight CEF-aware {ls}
 pipeline for ingestion. The `--setup` option creates an `arcsight-*` index
 pattern in Elasticsearch and imports Kibana dashboards and visualizations. On
-subsequent module runs or when scaling out the Logstash deployment,
+subsequent module runs or when scaling out the {ls} deployment,
 the `--setup` option should be omitted to avoid overwriting the existing Kibana
 dashboards.
 
@@ -103,18 +103,18 @@ the Event Broker event stream.
 
 image::static/images/arcsight-diagram-adp.svg[ArcSight Event Broker architecture]
 
-By default, the Logstash ArcSight module consumes from the Event Broker "eb-cef" topic.
+By default, the {ls} ArcSight module consumes from the Event Broker "eb-cef" topic.
 For additional settings, see <<arcsight-module-config>>. Consuming from a
 secured Event Broker port is possible, see <<arcsight-module-config>>.
 
-NOTE: Logstash, Elasticsearch, and Kibana must run locally. Note that you can also run
-Elasticsearch, Kibana and Logstash on separate hosts to consume data from ArcSight.
+NOTE: {ls}, Elasticsearch, and Kibana must run locally. Note that you can also run
+Elasticsearch, Kibana and {ls} on separate hosts to consume data from ArcSight.
 
 [[arcsight-instructions-eventbroker]]
 ===== Instructions for Event Broker
 
-. Start the Logstash ArcSight module by running the following command in the
-Logstash install directory with your respective Event Broker host and port:
+. Start the {ls} ArcSight module by running the following command in the
+{ls} install directory with your respective Event Broker host and port:
 +
 ["source","shell",subs="attributes"]
 -----
@@ -125,10 +125,10 @@ bin/logstash --modules arcsight --setup \
 -----
 +
 --
-The `--modules arcsight` option spins up an ArcSight CEF-aware Logstash
+The `--modules arcsight` option spins up an ArcSight CEF-aware {ls}
 pipeline for ingestion. The `--setup` option creates an `arcsight-*` index
 pattern in Elasticsearch and imports Kibana dashboards and visualizations. On
-subsequent module runs or when scaling out the Logstash deployment,
+subsequent module runs or when scaling out the {ls} deployment,
 the `--setup` option should be omitted to avoid overwriting the existing Kibana
 dashboards.
 
@@ -146,7 +146,7 @@ control the behavior of the ArcSight module.
 
 [[exploring-data-arcsight]]
 ==== Exploring Your Security Data
-Once the Logstash ArcSight module starts receiving events, you can immediately
+Once the {ls} ArcSight module starts receiving events, you can immediately
 begin using the packaged Kibana dashboards to explore and visualize your
 security data. The dashboards rapidly accelerate the time and effort required
 for security analysts and operators to gain situational and behavioral insights
@@ -196,7 +196,7 @@ causing the elevated count of failures?
 [[configuring-arcsight]]
 ==== Configuring the Module
 
-You can specify additional options for the Logstash ArcSight module in the
+You can specify additional options for the {ls} ArcSight module in the
 `logstash.yml` configuration file or with overrides through the command line
 like in the getting started. For more information about configuring modules, see
 <<logstash-modules>>.
@@ -219,16 +219,16 @@ modules:
 -----
 
 [[arcsight-module-config]]
-===== Logstash ArcSight Module Configuration Options
+===== {ls} ArcSight Module Configuration Options
 
 The ArcSight module provides the following settings for configuring the behavior
 of the module. These settings include ArcSight-specific options plus common
-options that are supported by all Logstash modules.
+options that are supported by all {ls} modules.
 
 When you override a setting at the command line, remember to prefix the setting
 with the module name, for example, `arcsight.var.inputs` instead of `var.inputs`.
 
-If you don't specify configuration settings, Logstash uses the defaults.
+If you don't specify configuration settings, {ls} uses the defaults.
 
 *ArcSight Module Options*
 
@@ -239,7 +239,7 @@ If you don't specify configuration settings, Logstash uses the defaults.
 * Default value is "eventbroker"
 --
 +
-Set the input(s) to expose for the Logstash ArcSight module. Valid settings are
+Set the input(s) to expose for the {ls} ArcSight module. Valid settings are
 "eventbroker", "smartconnector", or "eventbroker,smartconnector" (exposes both
 inputs concurrently).
 
@@ -384,7 +384,7 @@ KafkaClient {
 Please note that specifying `jaas_path` and `kerberos_config` here will add these
 to the global JVM system properties. This means if you have multiple Kafka inputs,
 all of them would be sharing the same `jaas_path` and `kerberos_config`.
-If this is not desirable, you would have to run separate instances of Logstash on
+If this is not desirable, you would have to run separate instances of {ls} on
 different JVM instances.
 
 
diff --git a/docs/static/azure-module.asciidoc b/docs/static/azure-module.asciidoc
index 14c9caea3a8..799e32fa753 100644
--- a/docs/static/azure-module.asciidoc
+++ b/docs/static/azure-module.asciidoc
@@ -10,7 +10,7 @@ experimental[]
 deprecated[7.8.0, "We recommend using the Azure modules in {filebeat-ref}/filebeat-module-azure.html[{Filebeat}] and {metricbeat-ref}/metricbeat-module-azure.html[{metricbeat}], which are compliant with the {ecs-ref}/index.html[Elastic Common Schema (ECS)]"]
 
 The https://azure.microsoft.com/en-us/overview/what-is-azure/[Microsoft Azure]
-module in Logstash helps you easily integrate your Azure activity logs and SQL
+module in {ls} helps you easily integrate your Azure activity logs and SQL
 diagnostic logs with the Elastic Stack. 
 
 image::static/images/azure-flow.png["Azure Work Flow",width="80%"]
@@ -26,7 +26,7 @@ and decreasing overall time to resolution. The Azure module helps you:
 * Monitor and optimize your SQL DB deployments.
 
 The Azure module uses the
-{logstash-ref}/plugins-inputs-azure_event_hubs.html[Logstash Azure Event Hubs
+{logstash-ref}/plugins-inputs-azure_event_hubs.html[{ls} Azure Event Hubs
 input plugin] to consume data from Azure Event Hubs. The module taps directly into the
 Azure dashboard, parses and indexes events into Elasticsearch, and installs a
 suite of {kib} dashboards to help you start exploring your data immediately.   
@@ -80,13 +80,13 @@ easy to install.
 ===== Azure prerequisites
 
 Azure Monitor should be configured to stream logs to one or more Event Hubs. 
-Logstash will need to access these Event Hubs instances to consume your Azure logs and metrics.
+{ls} will need to access these Event Hubs instances to consume your Azure logs and metrics.
 See <<azure-resources>> at the end of this topic for links to Microsoft Azure documentation.
 
 [[configuring-azure]]
 ==== Configure the module
 
-Specify <<azure_config_options,options>> for the Logstash Azure module in the
+Specify <<azure_config_options,options>> for the {ls} Azure module in the
 `logstash.yml` configuration file. 
 
 * *Basic configuration.* You can use the `logstash.yml` file to configure inputs from multiple Event Hubs that share the same configuration.
@@ -126,7 +126,7 @@ modules:
       - "Endpoint=sb://...EntityPath=insights-logs-timeouts"
 -----
 <1> The `consumer_group` (optional) is highly recommended. See <<azure_best_practices>>.
-<2> The `storage_connection` (optional) sets the Azure Blob Storage connection for tracking processing state for Event Hubs when scaling out a deployment with multiple Logstash instances. See <<scaling-blob>> for additional details.
+<2> The `storage_connection` (optional) sets the Azure Blob Storage connection for tracking processing state for Event Hubs when scaling out a deployment with multiple {ls} instances. See <<scaling-blob>> for additional details.
 <3> See <<azure_best_practices>> for guidelines on choosing an appropriate number of threads.
 <4> This connection sets up the consumption of Activity Logs. By default, Azure Monitor uses the `insights-operational-logs` Event Hub name. Make sure this matches the name of the Event Hub specified for Activity Logs.
 <5> This connection and the ones below set up the consumption of SQL DB diagnostic logs and metrics. By default, Azure Monitor uses all these different Event Hub names.
@@ -188,7 +188,7 @@ configuration option. Notice the notation for the `initial_position` option.
 ===== Scale Event Hub consumption
 
 An https://azure.microsoft.com/en-us/services/storage/blobs[Azure Blob Storage
-account] is an essential part of Azure-to-Logstash configuration. 
+account] is an essential part of Azure-to-{ls} configuration. 
 It is required for users who want to scale out multiple {ls} instances to consume from Event Hubs.
 
 A Blob Storage account is a central location that enables multiple instances of
@@ -282,20 +282,20 @@ Be sure that the `logstash.yml` file is <<configuring-azure,configured correctly
 
 ===== First time setup
 
-Run this command from the Logstash directory:
+Run this command from the {ls} directory:
 
 ["source","shell",subs="attributes"]
 -----
 bin/logstash --setup
 -----
 
-The `--modules azure` option starts a Logstash pipeline for ingestion from Azure
+The `--modules azure` option starts a {ls} pipeline for ingestion from Azure
 Event Hubs. The `--setup` option creates an `azure-*` index pattern in
 Elasticsearch and imports Kibana dashboards and visualizations. 
 
 ===== Subsequent starts
 
-Run this command from the Logstash directory:
+Run this command from the {ls} directory:
 
 ["source","shell",subs="attributes"]
 -----
@@ -308,7 +308,7 @@ overwritten.
 
 [[exploring-data-azure]]
 ==== Explore your data
-When the Logstash Azure module starts receiving events, you can begin using the
+When the {ls} Azure module starts receiving events, you can begin using the
 packaged Kibana dashboards to explore and visualize your data. 
 
 To explore your data with Kibana:
@@ -368,7 +368,7 @@ Writing checkpoints too frequently can slow down processing unnecessarily.
 * Default value is `$Default` 
 
 Consumer group used to read the Event Hub(s). Create a consumer group
-specifically for Logstash. Then ensure that all instances of Logstash use that
+specifically for {ls}. Then ensure that all instances of {ls} use that
 consumer group so that they can work together properly.
 
 
@@ -396,7 +396,7 @@ When first reading from an Event Hub, start from this position:
 You control the number of seconds using the `initial_position_look_back` option.
 
 If `storage_connection` is set, the `initial_position` value is used only
-the first time Logstash reads from the Event Hub.
+the first time {ls} reads from the Event Hub.
 
 
 [[azure_initial_position_look_back]]
@@ -428,7 +428,7 @@ requires more memory.
 * No default value 
 
 Connection string for blob account storage. Blob account storage persists the
-offsets between restarts, and ensures that multiple instances of Logstash
+offsets between restarts, and ensures that multiple instances of {ls}
 process different partitions. 
 When this value is set, restarts resume where processing left off.
 When this value is not set, the `initial_position` value is used on every restart.
diff --git a/docs/static/best-practice.asciidoc b/docs/static/best-practice.asciidoc
index a49b8f56704..5351fca499c 100644
--- a/docs/static/best-practice.asciidoc
+++ b/docs/static/best-practice.asciidoc
@@ -10,7 +10,7 @@ https://github.com/elastic/logstash/issues, or
 
 // After merge, update PR link to link directly to this topic in GH
 
-Also check out the https://discuss.elastic.co/c/logstash[Logstash discussion
+Also check out the https://discuss.elastic.co/c/logstash[{ls} discussion
 forum].
 
 [discrete] 
@@ -50,7 +50,7 @@ You can manage pipelines in a {ls} instance using either local pipeline configur
 {logstash-ref}/configuring-centralized-pipelines.html[centralized pipeline management]
 in {kib}.
 
-After you configure Logstash to use centralized pipeline management, you can
+After you configure {ls} to use centralized pipeline management, you can
 no longer specify local pipeline configurations. The `pipelines.yml` file and
 settings such as `path.config` and `config.string` are inactive when centralized
 pipeline management is enabled.
diff --git a/docs/static/breaking-changes-60.asciidoc b/docs/static/breaking-changes-60.asciidoc
index 6d4ab321577..8c43b7c8585 100644
--- a/docs/static/breaking-changes-60.asciidoc
+++ b/docs/static/breaking-changes-60.asciidoc
@@ -4,10 +4,10 @@
 Here are the breaking changes for 6.0. 
 
 [discrete]
-==== Changes in Logstash Core
+==== Changes in {ls} Core
 
-These changes can affect any instance of Logstash that uses impacted features.
-Changes to Logstash Core are plugin agnostic.
+These changes can affect any instance of {ls} that uses impacted features.
+Changes to {ls} Core are plugin agnostic.
 
 [discrete]
 ===== Application Settings
@@ -30,7 +30,7 @@ Changes to Logstash Core are plugin agnostic.
 * Configurations provided with `-f` or `config.path` will not be appended with `stdin` input and `stdout` output automatically.
 
 [discrete]
-===== List of plugins bundled with Logstash
+===== List of plugins bundled with {ls}
 
 The following plugins were removed from the default bundle based on usage data. You can still install these plugins manually:
 
diff --git a/docs/static/breaking-changes-70.asciidoc b/docs/static/breaking-changes-70.asciidoc
index f4520137abe..c23f3b21bef 100644
--- a/docs/static/breaking-changes-70.asciidoc
+++ b/docs/static/breaking-changes-70.asciidoc
@@ -4,10 +4,10 @@
 Here are the breaking changes for {ls} 7.0. 
 
 [discrete]
-==== Changes in Logstash Core
+==== Changes in {ls} Core
 
-These changes can affect any instance of Logstash that uses impacted features.
-Changes to Logstash Core are plugin agnostic.
+These changes can affect any instance of {ls} that uses impacted features.
+Changes to {ls} Core are plugin agnostic.
 
 [discrete]
 [[java-exec-default]]
@@ -32,9 +32,9 @@ know.
 As of 7.0, Beats fields conform to the {ecs-ref}/index.html[Elastic Common
 Schema (ECS)].
 
-If you upgrade Logstash before you upgrade Beats, the payloads continue to use
-the pre-ECS schema. If you upgrade your Beats before you upgrade Logstash, then
-you'll get payloads with the ECS schema in advance of any Logstash upgrade.
+If you upgrade {ls} before you upgrade Beats, the payloads continue to use
+the pre-ECS schema. If you upgrade your Beats before you upgrade {ls}, then
+you'll get payloads with the ECS schema in advance of any {ls} upgrade.
 
 If you see mapping conflicts after upgrade, that is an indication that the
 Beats/ECS change is influencing the data reaching existing indices. 
@@ -46,7 +46,7 @@ Beats/ECS change is influencing the data reaching existing indices.
 
 The Field Reference parser, which is used to interpret references to fields in
 your pipelines and plugins, was made to be more strict and will now reject
-inputs that are either ambiguous or illegal. Since 6.4, Logstash has emitted
+inputs that are either ambiguous or illegal. Since 6.4, {ls} has emitted
 warnings when encountering input that is ambiguous, and allowed an early opt-in
 of strict-mode parsing either by providing the command-line flag
 `--field-reference-parser STRICT` or by adding `config.field_reference.parser:
@@ -59,7 +59,7 @@ Here's an example.
 [source,txt]
 -----
 logstash-6.7.0 % echo "hello"| bin/logstash -e 'filter { mutate { replace => { "message" => "%{[[]]message]} you" } } }'
-[2019-04-05T16:52:18,691][WARN ][org.logstash.FieldReference] Detected ambiguous Field Reference `[[]]message]`, which we expanded to the path `[message]`; in a future release of Logstash, ambiguous Field References will not be expanded.
+[2019-04-05T16:52:18,691][WARN ][org.logstash.FieldReference] Detected ambiguous Field Reference `[[]]message]`, which we expanded to the path `[message]`; in a future release of {ls}, ambiguous Field References will not be expanded.
 {
        "message" => "hello you",
       "@version" => "1",
@@ -75,16 +75,16 @@ logstash-6.7.0 % echo "hello"| bin/logstash -e 'filter { mutate { replace => { "
 -----
 logstash-7.0.0 % echo "hello"| bin/logstash -e 'filter { mutate { replace => { "message" => "%{[[]]message]} you" } } }'
 [2019-04-05T16:48:09,135][FATAL][logstash.runner          ] An unexpected error occurred! {:error=>java.lang.IllegalStateException: org.logstash.FieldReference$IllegalSyntaxException: Invalid FieldReference: `[[]]message]`
-[2019-04-05T16:48:09,167][ERROR][org.logstash.Logstash    ] java.lang.IllegalStateException: Logstash stopped processing because of an error: (SystemExit) exit
+[2019-04-05T16:48:09,167][ERROR][org.logstash.{ls}    ] java.lang.IllegalStateException: {ls} stopped processing because of an error: (SystemExit) exit
 -----
 
   
 [discrete]
-==== Changes in Logstash Plugins
+==== Changes in {ls} Plugins
 
 With 7.0.0, we have taken the opportunity to upgrade a number of bundled plugins
 to their newest major version, absorbing their breaking changes into the
-Logstash distribution.
+{ls} distribution.
 
 While these upgrades included new features and important fixes, only the
 breaking changes are called out below.
@@ -135,7 +135,7 @@ Here are the breaking changes for  input plugins.
 * Removed obsolete `target_field_for_codec` option
 * Changed default value of `add_hostname` to false
 
-NOTE: In Beats 7.0.0, the fields exported by Beats _to_ the Logstash Beats Input
+NOTE: In Beats 7.0.0, the fields exported by Beats _to_ the {ls} Beats Input
 conform to the {ecs-ref}/index.html[Elastic Common Schema (ECS)]. Many of the
 exported fields have been renamed, so you may need to modify your pipeline
 configurations to access them at their new locations prior to upgrading your
diff --git a/docs/static/breaking-changes-80.asciidoc b/docs/static/breaking-changes-80.asciidoc
index dbcab3d0d5f..8e5a9daefb5 100644
--- a/docs/static/breaking-changes-80.asciidoc
+++ b/docs/static/breaking-changes-80.asciidoc
@@ -15,7 +15,7 @@ Here are the breaking changes for 8.0.
 {ls} must have a copy of the {es} CA that signed the cluster's certificates.
 When a new {es} cluster is started up _without_ dedicated certificates, it generates its own default self-signed Certificate Authority at startup.
 
-Our hosted {ess} simplifies safe, secure communication between Logstash and Elasticsearch. 
+Our hosted {ess} simplifies safe, secure communication between {ls} and Elasticsearch. 
 {ess} uses certificates signed by standard publicly trusted certificate authorities, and therefore setting a cacert is not necessary.
 
 For more information, see {logstash-ref}/ls-security.html#es-security-on[{es} security on by default]. 
@@ -23,9 +23,9 @@ For more information, see {logstash-ref}/ls-security.html#es-security-on[{es} se
 [discrete]
 [[bc-java-11-minimum]]
 ===== Java 11 minimum
-Logstash requires Java 11 or later.
-By default, Logstash will run with the bundled JDK, which has been verified to
-work with each specific version of Logstash, and generally provides the best
+{ls} requires Java 11 or later.
+By default, {ls} will run with the bundled JDK, which has been verified to
+work with each specific version of {ls}, and generally provides the best
 performance and reliability.
 
 [discrete]
@@ -40,15 +40,15 @@ The value of `JAVA_HOME` will be ignored.
 [[bc-ecs-compatibility]]
 ===== ECS compatibility is now on by default
 Many plugins can now be run in a mode that avoids implicit conflict with the Elastic Common Schema.
-This mode is controlled individually with each plugin's `ecs_compatibility` option, which defaults to the value of the Logstash `pipeline.ecs_compatibility` setting.
-In Logstash 8, this compatibility mode will be on-by-default for all pipelines. https://github.com/elastic/logstash/issues/11623[#11623]
+This mode is controlled individually with each plugin's `ecs_compatibility` option, which defaults to the value of the {ls} `pipeline.ecs_compatibility` setting.
+In {ls} 8, this compatibility mode will be on-by-default for all pipelines. https://github.com/elastic/logstash/issues/11623[#11623]
 
-If you wish to _lock in_ a pipeline's behaviour from Logstash 7.x before upgrading to Logstash 8, you can set  `pipeline.ecs_compatibility: disabled` to its definition in `pipelines.yml` (or globally in `logstash.yml`).
+If you wish to _lock in_ a pipeline's behaviour from {ls} 7.x before upgrading to {ls} 8, you can set  `pipeline.ecs_compatibility: disabled` to its definition in `pipelines.yml` (or globally in `logstash.yml`).
 
 [discrete]
 [[bc-ruby-engine]]
 ===== Ruby Execution Engine removed
-The Java Execution Engine has been the default engine since Logstash 7.0, and works with plugins written in either Ruby or Java.
+The Java Execution Engine has been the default engine since {ls} 7.0, and works with plugins written in either Ruby or Java.
 Removal of the Ruby Execution Engine will not affect the ability to run existing pipelines. https://github.com/elastic/logstash/pull/12517[#12517]
 
 [discrete]
diff --git a/docs/static/breaking-changes-pre63.asciidoc b/docs/static/breaking-changes-pre63.asciidoc
index 854b2195bad..4174a36c900 100644
--- a/docs/static/breaking-changes-pre63.asciidoc
+++ b/docs/static/breaking-changes-pre63.asciidoc
@@ -1,7 +1,7 @@
 [[breaking-pq]]
-=== Breaking change across PQ versions prior to Logstash 6.3.0
+=== Breaking change across PQ versions prior to {ls} 6.3.0
 
-If you are upgrading from Logstash 6.2.x or any earlier version (including 5.x)
+If you are upgrading from {ls} 6.2.x or any earlier version (including 5.x)
 and have the persistent queue enabled, we strongly recommend that you drain or
 delete the persistent queue before you upgrade. See <<draining-pqs>>
 for information and instructions.
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index 1a548d6bcf5..0a3d0f244a3 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -8,7 +8,7 @@ to 8.y). On occasion, we are forced to break compatibility within a given major
 to ensure correctness of operation.
 
 This section covers the changes that you need to be aware of when migrating to
-Logstash 8.0.0 and later.
+{ls} 8.0.0 and later.
 
 NOTE: Migrating directly between non-consecutive major versions (6.x to
 8.x) is not recommended. 
diff --git a/docs/static/codec.asciidoc b/docs/static/codec.asciidoc
index f3ed7fd7e77..f9bbe3472bd 100644
--- a/docs/static/codec.asciidoc
+++ b/docs/static/codec.asciidoc
@@ -10,6 +10,6 @@
 
 :getstarted: Let's step through creating a {plugintype} plugin using the https://github.com/logstash-plugins/logstash-codec-example/[example {plugintype} plugin].
 
-:methodheader: pass:m[Logstash codecs must implement the `register` method, and the `decode` method or the `encode` method (or both).]
+:methodheader: pass:m[{ls} codecs must implement the `register` method, and the `decode` method or the `encode` method (or both).]
 
 include::include/pluginbody.asciidoc[]
diff --git a/docs/static/config-management.asciidoc b/docs/static/config-management.asciidoc
index 2960f651cd6..5f0f9ccd2b1 100644
--- a/docs/static/config-management.asciidoc
+++ b/docs/static/config-management.asciidoc
@@ -1,13 +1,13 @@
 [[config-management]]
-== Managing Logstash
+== Managing {ls}
 
-Logstash provides configuration management features to make it easier for you to
+{ls} provides configuration management features to make it easier for you to
 manage updates to your configuration over time.
 
-The topics in this section describe Logstash configuration management features
+The topics in this section describe {ls} configuration management features
 only. For information about other config management tools, such as Puppet and
 Chef, see the documentation for those projects. Also take a look at the
-https://forge.puppet.com/elastic/logstash[Logstash Puppet module documentation].
+https://forge.puppet.com/elastic/logstash[{ls} Puppet module documentation].
 
 :edit_url!:
 include::management/centralized-pipelines.asciidoc[]
diff --git a/docs/static/configuration-advanced.asciidoc b/docs/static/configuration-advanced.asciidoc
index 54d02619b70..c6a5db4214e 100644
--- a/docs/static/configuration-advanced.asciidoc
+++ b/docs/static/configuration-advanced.asciidoc
@@ -1,5 +1,5 @@
 [[configuration-advanced]]
-== Advanced Logstash Configurations
+== Advanced {ls} Configurations
 
 You can take {ls} beyond basic configuration to handle more advanced
 requirements, such as multiple pipelines, communication between {ls} pipelines,
diff --git a/docs/static/contrib-acceptance.asciidoc b/docs/static/contrib-acceptance.asciidoc
index 086b0266c89..f5b7657229c 100644
--- a/docs/static/contrib-acceptance.asciidoc
+++ b/docs/static/contrib-acceptance.asciidoc
@@ -10,7 +10,7 @@ The plugin name must be unique and in this format: `logstash-plugintype-pluginna
 If the plugin name is more than one word, separate words after plugin type with underscores. 
 Example: _logstash-output-elastic_app_search_
 * **Documentation.** Documentation is a required component of your plugin. 
-If we list your plugin in the Logstash Reference, we point to your documentation--a readme.md, docs/index.asciidoc, or both--in your plugin repo.
+If we list your plugin in the {ls} Reference, we point to your documentation--a readme.md, docs/index.asciidoc, or both--in your plugin repo.
 * **Code Review.** Your plugin must be reviewed by members of the community for coherence, quality, readability, stability and security.
 * **Tests.** Your plugin must contain tests to be accepted. You can refer to http://betterspecs.org/ for examples. 
 ** Step 1. Enable travis on your account
diff --git a/docs/static/contribute-core.asciidoc b/docs/static/contribute-core.asciidoc
index 4e6ae6e03b8..df7b940b13a 100644
--- a/docs/static/contribute-core.asciidoc
+++ b/docs/static/contribute-core.asciidoc
@@ -1,10 +1,10 @@
 [[contribute-to-core]]
-=== Extending Logstash core
+=== Extending {ls} core
 
-We also welcome contributions and bug fixes to the Logstash core feature set.
+We also welcome contributions and bug fixes to the {ls} core feature set.
 
 Please read through our
 https://github.com/elastic/logstash/blob/main/CONTRIBUTING.md[contribution]
-guide, and the Logstash
+guide, and the {ls}
 https://github.com/elastic/logstash/blob/main/README.md[readme]
 document.
diff --git a/docs/static/contributing-java-plugin.asciidoc b/docs/static/contributing-java-plugin.asciidoc
index 2e4b2cba280..fbd301f15ad 100644
--- a/docs/static/contributing-java-plugin.asciidoc
+++ b/docs/static/contributing-java-plugin.asciidoc
@@ -8,13 +8,13 @@ you a head start.
 Native support for Java plugins in {ls} consists of several components:
 
 * Extensions to the Java execution engine to support running Java plugins in
-Logstash pipelines
+{ls} pipelines
 * APIs for developing Java plugins. 
 The APIs are in the `co.elastic.logstash.api` package. 
 A Java plugin might break if it references classes or specific concrete
 implementations of API interfaces outside that package. The implementation of
 classes outside of the API package may change at any time.
-* Tooling to automate the packaging and deployment of Java plugins in Logstash.
+* Tooling to automate the packaging and deployment of Java plugins in {ls}.
 
 [discrete]
 === Process overview
@@ -24,7 +24,7 @@ Here are the steps:
 . Set up your environment. 
 . Code the plugin.
 . Package and deploy the plugin.
-. Run Logstash with your new plugin.
+. Run {ls} with your new plugin.
 
 [float]
 ==== Let's get started 
diff --git a/docs/static/contributing-patch.asciidoc b/docs/static/contributing-patch.asciidoc
index 9af4942cb11..2a315cbfc06 100644
--- a/docs/static/contributing-patch.asciidoc
+++ b/docs/static/contributing-patch.asciidoc
@@ -1,7 +1,7 @@
 [[contributing-patch-plugin]]
-=== Contributing a patch to a Logstash plugin
+=== Contributing a patch to a {ls} plugin
 
-This section discusses the information you need to know to successfully contribute a patch to a Logstash plugin.
+This section discusses the information you need to know to successfully contribute a patch to a {ls} plugin.
 
 Each plugin defines its own configuration options. These control the behavior of the plugin to some degree. 
 Configuration option definitions commonly include:
@@ -10,13 +10,13 @@ Configuration option definitions commonly include:
 * Default value
 * Any required flags
 
-Plugins are subclasses of a Logstash base class. A plugin's base class defines common configuration and methods.
+Plugins are subclasses of a {ls} base class. A plugin's base class defines common configuration and methods.
 
 [[contrib-patch-input]]
 ==== Input plugins
 
 Input plugins ingest data from an external source. Input plugins are always associated with a codec. An input plugin 
-always has an associated codec plugin. Input and codec plugins operate in conjunction to create a Logstash event and add 
+always has an associated codec plugin. Input and codec plugins operate in conjunction to create a {ls} event and add 
 that event to the processing queue. An input codec is a subclass of the `LogStash::Inputs::Base` class.
 
 [[input-api]]
@@ -97,7 +97,7 @@ multiple execution paths or varying input data.
 [[rspec]]
 ===== RSpec framework
 
-Logstash uses Rspec, a Ruby testing framework, to define and run the test suite. What follows is a summary of various 
+{ls} uses Rspec, a Ruby testing framework, to define and run the test suite. What follows is a summary of various 
 sources.
 
 .Rspec Example
diff --git a/docs/static/contributing-to-logstash.asciidoc b/docs/static/contributing-to-logstash.asciidoc
index c1c51ab3d5a..94700150998 100644
--- a/docs/static/contributing-to-logstash.asciidoc
+++ b/docs/static/contributing-to-logstash.asciidoc
@@ -1,7 +1,7 @@
 [[contributing-to-logstash]]
-== Contributing to Logstash
+== Contributing to {ls}
 
-You can add your own input, codec, filter, or output plugins to Logstash. 
+You can add your own input, codec, filter, or output plugins to {ls}. 
 
 include::contrib-acceptance.asciidoc[]
 
@@ -9,7 +9,7 @@ include::contrib-acceptance.asciidoc[]
 [[add-plugin]]
 === Add a plugin
 
-Plugins can be developed and deployed independently of the Logstash core. 
+Plugins can be developed and deployed independently of the {ls} core. 
 Here are some documents to guide you through the process of coding, deploying, and sharing your plugin:
 
 * Write a new plugin 
@@ -35,7 +35,7 @@ You have three options for shutting down a plugin: `stop`, `stop?`, and `close`.
 * Call the `stop` method from outside the plugin thread. This method signals the plugin to stop.
 * The `stop?` method returns `true` when the `stop` method has already been called for that plugin.
 * The `close` method performs final bookkeeping and cleanup after the plugin's `run` method and the plugin's thread both
-exit. The `close` method is a a new name for the method known as `teardown` in previous versions of Logstash.
+exit. The `close` method is a a new name for the method known as `teardown` in previous versions of {ls}.
 
 The `shutdown`, `finished`, `finished?`, `running?`, and `terminating?` methods are redundant and no longer present in the
 Plugin Base class.
diff --git a/docs/static/core-plugins/inputs/java_generator.asciidoc b/docs/static/core-plugins/inputs/java_generator.asciidoc
index b888a8fa7ea..e6c97c5748c 100644
--- a/docs/static/core-plugins/inputs/java_generator.asciidoc
+++ b/docs/static/core-plugins/inputs/java_generator.asciidoc
@@ -21,7 +21,7 @@ include::{include_path}/plugin_header-core.asciidoc[]
 Generate synthetic log events.
 
 This plugin generates a stream of synthetic events that can be used to test the correctness or performance of a
-Logstash pipeline.
+{ls} pipeline.
 
 
 [id="plugins-{type}s-{plugin}-options"]
diff --git a/docs/static/core-plugins/outputs/java_stdout.asciidoc b/docs/static/core-plugins/outputs/java_stdout.asciidoc
index 315a109b8dc..dc2a1f790bb 100644
--- a/docs/static/core-plugins/outputs/java_stdout.asciidoc
+++ b/docs/static/core-plugins/outputs/java_stdout.asciidoc
@@ -18,10 +18,10 @@ include::{include_path}/plugin_header-core.asciidoc[]
 
 ==== Description
 
-Prints events to the STDOUT of the shell running Logstash. This output is convenient for debugging
+Prints events to the STDOUT of the shell running {ls}. This output is convenient for debugging
 plugin configurations by providing instant access to event data after it has passed through the inputs and filters.
 
-For example, the following output configuration in conjunction with the Logstash `-e` command-line flag, will
+For example, the following output configuration in conjunction with the {ls} `-e` command-line flag, will
 allow you to see the results of your event pipeline for quick iteration.
 [source,ruby]
     output {
diff --git a/docs/static/dead-letter-queues.asciidoc b/docs/static/dead-letter-queues.asciidoc
index 5d2806ccd8a..a983bc56163 100644
--- a/docs/static/dead-letter-queues.asciidoc
+++ b/docs/static/dead-letter-queues.asciidoc
@@ -14,10 +14,10 @@ See <<dlq-size>> and <<dlq-clear>> for more info.
 [[dead-letter-how]]
 ==== How the dead letter queue works
 
-By default, when Logstash encounters an event that it cannot process because the
-data contains a mapping error or some other issue, the Logstash pipeline 
+By default, when {ls} encounters an event that it cannot process because the
+data contains a mapping error or some other issue, the {ls} pipeline 
 either hangs or drops the unsuccessful event. In order to protect against data
-loss in this situation, you can <<configuring-dlq,configure Logstash>> to write
+loss in this situation, you can <<configuring-dlq,configure {ls}>> to write
 unsuccessful events to a dead letter queue instead of dropping them.
 
 NOTE: The dead letter queue is currently supported only for the
@@ -30,7 +30,7 @@ metadata that describes the reason the event could not be processed, information
 about the plugin that wrote the event, and the timestamp when the event
 entered the dead letter queue.
 
-To process events in the dead letter queue, create a Logstash pipeline
+To process events in the dead letter queue, create a {ls} pipeline
 configuration that uses the
 <<plugins-inputs-dead_letter_queue,`dead_letter_queue` input plugin>> to read
 from the queue. See <<processing-dlq-events>> for more information.
@@ -71,7 +71,7 @@ the `dead_letter_queue_enable` option in the `logstash.yml`
 dead_letter_queue.enable: true
 -------------------------------------------------------------------------------
 
-Dead letter queues are stored as files in the local directory of the Logstash
+Dead letter queues are stored as files in the local directory of the {ls}
 instance. By default, the dead letter queue files are stored in
 `path.data/dead_letter_queue`. Each pipeline has a separate queue. For example,
 the dead letter queue for the `main` pipeline is stored in
@@ -109,7 +109,7 @@ dead_letter_queue.flush_interval: 5000
 -------------------------------------------------------------------------------
 
 NOTE: You may not use the same `dead_letter_queue` path for two different
-Logstash instances.
+{ls} instances.
 
 [[file-rotation]]
 ===== File rotation
@@ -198,7 +198,7 @@ output {
 <1> The path to the top-level directory containing the dead letter queue. This
 directory contains a separate folder for each pipeline that writes to the dead
 letter queue. To find the path to this directory, look at the `logstash.yml`
-<<logstash-settings-file,settings file>>. By default, Logstash creates the
+<<logstash-settings-file,settings file>>. By default, {ls} creates the
 `dead_letter_queue` directory under the location used for persistent
 storage (`path.data`), for example, `LOGSTASH_HOME/data/dead_letter_queue`.
 However, if `path.dead_letter_queue` is set, it uses that location instead.
@@ -256,7 +256,7 @@ but the data cannot be processed because it contains a mapping error:
 {"geoip":{"location":"home"}}
 --------------------------------------------------------------------------------
 
-Indexing fails because the Logstash output plugin expects a `geo_point` object in
+Indexing fails because the {ls} output plugin expects a `geo_point` object in
 the `location` field, but the value is a string. The failed event is written to
 the dead letter queue, along with metadata about the error that caused the
 failure:
@@ -266,7 +266,7 @@ failure:
 {
    "@metadata" => {
     "dead_letter_queue" => {
-       "entry_time" => #<Java::OrgLogstash::Timestamp:0x5b5dacd5>,
+       "entry_time" => #<Java::Org{ls}::Timestamp:0x5b5dacd5>,
         "plugin_id" => "fb80f1925088497215b8d037e622dec5819b503e-4",
       "plugin_type" => "elasticsearch",
            "reason" => "Could not index event to Elasticsearch. status: 400, action: [\"index\", {:_id=>nil, :_index=>\"logstash-2017.06.22\", :_type=>\"doc\", :_routing=>nil}, 2017-06-22T01:29:29.804Z My-MacBook-Pro-2.local {\"geoip\":{\"location\":\"home\"}}], response: {\"index\"=>{\"_index\"=>\"logstash-2017.06.22\", \"_type\"=>\"doc\", \"_id\"=>\"AVzNayPze1iR9yDdI2MD\", \"status\"=>400, \"error\"=>{\"type\"=>\"mapper_parsing_exception\", \"reason\"=>\"failed to parse\", \"caused_by\"=>{\"type\"=>\"illegal_argument_exception\", \"reason\"=>\"illegal latitude value [266.30859375] for geoip.location\"}}}}"
diff --git a/docs/static/deploying.asciidoc b/docs/static/deploying.asciidoc
index be5d46800dc..40d01db32ec 100644
--- a/docs/static/deploying.asciidoc
+++ b/docs/static/deploying.asciidoc
@@ -1,5 +1,5 @@
 [[deploying-and-scaling]]
-== Deploying and Scaling Logstash
+== Deploying and Scaling {ls}
 
 The Elastic Stack is used for tons of use cases, from operational log and
 metrics analytics, to enterprise and application search. Making sure your data
@@ -7,7 +7,7 @@ gets scalably, durably, and securely transported to Elasticsearch is extremely
 important, especially for mission critical environments.
 
 The goal of this document is to highlight the most common architecture patterns
-for Logstash and how to effectively scale as your deployment grows. The focus
+for {ls} and how to effectively scale as your deployment grows. The focus
 will be around the operational log, metrics, and security analytics use cases
 because they tend to require larger scale deployments. The deploying and scaling
 recommendations provided here may vary based on your own requirements.
@@ -29,10 +29,10 @@ and index your data.
 image::static/images/deploy1.png[]
 
 [float]
-==== Introducing Logstash
-What are the main benefits for integrating Logstash into your architecture?
+==== Introducing {ls}
+What are the main benefits for integrating {ls} into your architecture?
 
-* Scale through ingestion spikes - Logstash has an adaptive disk-based
+* Scale through ingestion spikes - {ls} has an adaptive disk-based
 buffering system that will absorb incoming throughput, therefore mitigating
 backpressure
 * Ingest from other data sources like databases, S3, or messaging queues
@@ -43,7 +43,7 @@ backpressure
 [[scaling-ingest]]
 === Scaling Ingest
 
-Beats and Logstash make ingest awesome. Together, they provide a comprehensive
+Beats and {ls} make ingest awesome. Together, they provide a comprehensive
 solution that is scalable and resilient. What can you expect?
 
 * Horizontal scalability, high availability, and variable load handling
@@ -51,13 +51,13 @@ solution that is scalable and resilient. What can you expect?
 * End-to-end secure transport with authentication and wire encryption
 
 [float]
-==== Beats and Logstash
+==== Beats and {ls}
 
 Beats run across thousands of edge host servers, collecting, tailing, and
-shipping logs to Logstash. Logstash serves as the centralized streaming
+shipping logs to {ls}. {ls} serves as the centralized streaming
 engine for data unification and enrichment. The
 <<plugins-inputs-beats,Beats input plugin>> exposes a secure,
-acknowledgement-based endpoint for Beats to send data to Logstash.
+acknowledgement-based endpoint for Beats to send data to {ls}.
 
 image::static/images/deploy2.png[]
 
@@ -69,17 +69,17 @@ details on resiliency.
 [float]
 ==== Scalability
 
-Logstash is horizontally scalable and can form groups of nodes running the same
-pipeline. Logstash’s adaptive buffering capabilities will facilitate smooth
-streaming even through variable throughput loads. If the Logstash layer becomes
+{ls} is horizontally scalable and can form groups of nodes running the same
+pipeline. {ls}’s adaptive buffering capabilities will facilitate smooth
+streaming even through variable throughput loads. If the {ls} layer becomes
 an ingestion bottleneck, simply add more nodes to scale out. Here are a few
 general recommendations:
 
 * Beats should {filebeat-ref}/elasticsearch-output.html#_loadbalance[load balance] across a group of
-Logstash nodes.
-* A minimum of two Logstash nodes are recommended for high availability.
-* It’s common to deploy just one Beats input per Logstash node, but multiple
-Beats inputs can also be deployed per Logstash node to expose independent
+{ls} nodes.
+* A minimum of two {ls} nodes are recommended for high availability.
+* It’s common to deploy just one Beats input per {ls} node, but multiple
+Beats inputs can also be deployed per {ls} node to expose independent
 endpoints for different data sources.
 
 [float]
@@ -88,12 +88,12 @@ endpoints for different data sources.
 When using https://www.elastic.co/products/beats/filebeat[Filebeat] or
 https://www.elastic.co/products/beats/winlogbeat[Winlogbeat] for log collection
 within this ingest flow, *at-least-once delivery* is guaranteed. Both the
-communication protocols, from Filebeat or Winlogbeat to Logstash, and from
-Logstash to Elasticsearch, are synchronous and support acknowledgements. The
+communication protocols, from Filebeat or Winlogbeat to {ls}, and from
+{ls} to Elasticsearch, are synchronous and support acknowledgements. The
 other Beats don’t yet have support for acknowledgements.
 
-Logstash persistent queues provide protection across node failures. For
-disk-level resiliency in Logstash, it’s important to ensure disk redundancy.
+{ls} persistent queues provide protection across node failures. For
+disk-level resiliency in {ls}, it’s important to ensure disk redundancy.
 For on-premise deployments, it's recommended that you configure RAID. When
 running in the cloud or a containerized environment, it’s recommended that you
 use persistent disks with replication strategies that reflect your data SLAs.
@@ -105,7 +105,7 @@ guarantees. For more details, see the
 [float]
 ==== Processing
 
-Logstash will commonly extract fields with <<plugins-filters-grok,grok>> or
+{ls} will commonly extract fields with <<plugins-filters-grok,grok>> or
 <<plugins-filters-dissect,dissect>>, augment
 <<plugins-filters-geoip,geographical>> info, and can further enrich events with
 <<plugins-filters-translate,file>>, <<plugins-filters-jdbc_streaming,database>>,
@@ -119,8 +119,8 @@ Make sure to check out the other <<filter-plugins,available filter plugins>>.
 Enterprise-grade security is available across the entire delivery chain.
 
 * Wire encryption is recommended for both the transport from
-{filebeat-ref}/configuring-ssl-logstash.html[Beats to Logstash] and from
-{logstash-ref}/ls-security.html[Logstash to Elasticsearch].
+{filebeat-ref}/configuring-ssl-logstash.html[Beats to {ls}] and from
+{logstash-ref}/ls-security.html[{ls} to Elasticsearch].
 * There’s a wealth of security options when communicating with Elasticsearch
 including basic authentication, TLS, PKI, LDAP, AD, and other custom realms.
 To enable Elasticsearch security, see 
@@ -129,12 +129,12 @@ To enable Elasticsearch security, see
 [float]
 ==== Monitoring
 
-When running Logstash 5.2 or greater,
+When running {ls} 5.2 or greater,
 the https://www.elastic.co/products/x-pack/monitoring[Monitoring UI] provides
 deep visibility into your deployment metrics, helping observe performance and
 alleviate bottlenecks as you scale. Monitoring is an X-Pack feature under the
 Basic License and is therefore *free to use*. To get started, see
-{logstash-ref}/monitoring-logstash.html[Monitoring Logstash].
+{logstash-ref}/monitoring-logstash.html[Monitoring {ls}].
 
 If external monitoring is preferred, there are <<monitoring,Monitoring APIs>>
 that return point-in-time metrics snapshots.
@@ -152,8 +152,8 @@ image::static/images/deploy3.png[]
 [float]
 ==== TCP, UDP, and HTTP Protocols
 
-The TCP, UDP, and HTTP protocols are common ways to feed data into Logstash.
-Logstash can expose endpoint listeners with the respective
+The TCP, UDP, and HTTP protocols are common ways to feed data into {ls}.
+{ls} can expose endpoint listeners with the respective
 <<plugins-inputs-tcp,TCP>>, <<plugins-inputs-udp,UDP>>, and
 <<plugins-inputs-http,HTTP>> input plugins. The data sources enumerated below
 are typically ingested through one of these three protocols.
@@ -163,7 +163,7 @@ connectivity issues may result in data loss.
 
 For high availability scenarios, a third-party hardware or software load
 balancer, like HAProxy, should be added to fan out traffic to a group of
-Logstash nodes.
+{ls} nodes.
 
 [float]
 ==== Network and Security Data
@@ -174,12 +174,12 @@ ingestion points.
 
 * Network wire data - collect and analyze network traffic with
 https://www.elastic.co/products/beats/packetbeat[Packetbeat].
-* Netflow v5/v9/v10 - Logstash understands data from Netflow/IPFIX exporters
+* Netflow v5/v9/v10 - {ls} understands data from Netflow/IPFIX exporters
 with the <<plugins-codecs-netflow,Netflow codec>>.
-* Nmap - Logstash accepts and parses Nmap XML data with the
+* Nmap - {ls} accepts and parses Nmap XML data with the
 <<plugins-codecs-nmap,Nmap codec>>.
-* SNMP trap - Logstash has a native <<plugins-inputs-snmptrap,SNMP trap input>>.
-* CEF - Logstash accepts and parses CEF data from systems like Arcsight
+* SNMP trap - {ls} has a native <<plugins-inputs-snmptrap,SNMP trap input>>.
+* CEF - {ls} accepts and parses CEF data from systems like Arcsight
 SmartConnectors with the <<plugins-codecs-cef,CEF codec>>. See this
 https://www.elastic.co/blog/integrating-elastic-stack-with-arcsight-siem-part-1[blog series]
 for more details.
@@ -188,20 +188,20 @@ for more details.
 ==== Centralized Syslog Servers
 
 Existing syslog server technologies like rsyslog and syslog-ng generally send
-syslog over to Logstash TCP or UDP endpoints for extraction, processing, and
+syslog over to {ls} TCP or UDP endpoints for extraction, processing, and
 persistence. If the data format conforms to RFC3164, it can be fed directly
-to the <<plugins-inputs-syslog,Logstash syslog input>>.
+to the <<plugins-inputs-syslog,{ls} syslog input>>.
 
 [float]
 ==== Infrastructure & Application Data and IoT
 
 Infrastructure and application metrics can be collected with
 https://www.elastic.co/products/beats/metricbeat[Metricbeat], but applications
-can also send webhooks to a Logstash HTTP input or have metrics polled from an
+can also send webhooks to a {ls} HTTP input or have metrics polled from an
 HTTP endpoint with the <<plugins-inputs-http_poller,HTTP poller input plugin>>.
 
 For applications that log with log4j2, it’s recommended to use the
-SocketAppender to send JSON to the Logstash TCP input. Alternatively, log4j2
+SocketAppender to send JSON to the {ls} TCP input. Alternatively, log4j2
 can also log to a file for collection with FIlebeat. Usage of the log4j1
 SocketAppender is not recommended.
 
@@ -215,28 +215,28 @@ telemetry data through one of these protocols.
 If you are leveraging message queuing technologies as part of your existing
 infrastructure, getting that data into the Elastic Stack is easy. For existing
 users who are utilizing an external queuing layer like Redis or RabbitMQ just
-for data buffering with Logstash, it’s recommended to use Logstash persistent
+for data buffering with {ls}, it’s recommended to use {ls} persistent
 queues instead of an external queuing layer. This will help with overall ease
 of management by removing an unnecessary layer of complexity in your ingest
 architecture.
 
 For users who want to integrate data from existing Kafka deployments or require
 the underlying usage of ephemeral storage, Kafka can serve as a data hub where
-Beats can persist to and Logstash nodes can consume from.
+Beats can persist to and {ls} nodes can consume from.
 
 image::static/images/deploy4.png[]
 
-The other TCP, UDP, and HTTP sources can persist to Kafka with Logstash as a
+The other TCP, UDP, and HTTP sources can persist to Kafka with {ls} as a
 conduit to achieve high availability in lieu of a load balancer. A group of
-Logstash nodes can then consume from topics with the
+{ls} nodes can then consume from topics with the
 <<plugins-inputs-kafka,Kafka input>> to further transform and enrich the data in
 transit.
 
 [float]
 ==== Resiliency and Recovery
 
-When Logstash consumes from Kafka, persistent queues should be enabled and will
-add transport resiliency to mitigate the need for reprocessing during Logstash
+When {ls} consumes from Kafka, persistent queues should be enabled and will
+add transport resiliency to mitigate the need for reprocessing during {ls}
 node failures. In this context, it’s recommended to use the default persistent
 queue disk allocation size `queue.max_bytes: 1GB`.
 
@@ -246,7 +246,7 @@ be reprocessed from Kafka in the case of disaster recovery and reconciliation.
 [float]
 ==== Other Messaging Queue Integrations
 
-Although an additional queuing layer is not required, Logstash can consume from
+Although an additional queuing layer is not required, {ls} can consume from
 a myriad of other message queuing technologies like
 <<plugins-inputs-rabbitmq,RabbitMQ>> and <<plugins-inputs-redis,Redis>>. It also
 supports ingestion from hosted queuing services like
diff --git a/docs/static/doc-for-plugin.asciidoc b/docs/static/doc-for-plugin.asciidoc
index e33b2e63431..14fd0d38031 100644
--- a/docs/static/doc-for-plugin.asciidoc
+++ b/docs/static/doc-for-plugin.asciidoc
@@ -5,13 +5,13 @@ Documentation is a required component of your plugin.
 Quality documentation with good examples contributes to the adoption of your plugin.
 
 The documentation that you write for your plugin will be generated and published
-in the {logstash-ref}/index.html[Logstash Reference] and the
-{lsplugindocs}[Logstash Versioned Plugin Reference].
+in the {logstash-ref}/index.html[{ls} Reference] and the
+{lsplugindocs}[{ls} Versioned Plugin Reference].
 
 .Plugin listing in {ls} Reference
 [NOTE]
 =====
-We may list your plugin in the {logstash-ref}/index.html[Logstash Reference] if
+We may list your plugin in the {logstash-ref}/index.html[{ls} Reference] if
 it meets our <<plugin-acceptance,requirements and quality standards>>. 
 When we list your plugin, we point to _your_ documentation--a readme.md, docs/index.asciidoc, or both--in your plugin repo.
 For more info on this option, see <<plugin-listing>>. 
@@ -31,7 +31,7 @@ The <<plugin-generator,plugin generation utility>> creates a starter file for yo
 ==== Heading IDs
 
 Format heading anchors with variables that can support generated IDs. This approach
-creates unique IDs when the {lsplugindocs}[Logstash Versioned Plugin Reference]
+creates unique IDs when the {lsplugindocs}[{ls} Versioned Plugin Reference]
 is built. Unique heading IDs are required to avoid duplication over multiple versions of a plugin.
 
 *Example*
@@ -46,7 +46,7 @@ Instead, use variables to define it:
 ==== Configuration models
 ----------------------------------
 
-If you hardcode an ID, the {lsplugindocs}[Logstash Versioned Plugin Reference]
+If you hardcode an ID, the {lsplugindocs}[{ls} Versioned Plugin Reference]
 builds correctly the first time. The second time the doc build runs, the ID
 is flagged as a duplicate, and the build fails.
 
@@ -77,9 +77,9 @@ Points to this heading in the same file:
 ==== Configuration models
 ----------------------------------
 
-===== Link to content in the Logstash Reference Guide
+===== Link to content in the {ls} Reference Guide
 
-Use external link syntax for links that point to documentation for other plugins or content in the Logstash Reference Guide.
+Use external link syntax for links that point to documentation for other plugins or content in the {ls} Reference Guide.
 
 *Examples*
 [source,txt]
@@ -104,10 +104,10 @@ If you want your link to display as {logstash-ref}/getting-started-with-logstash
 {logstash-ref}/getting-started-with-logstash.html
 -----
 
-If you want your link to display as {logstash-ref}/getting-started-with-logstash.html[Getting Started with Logstash], use this format:
+If you want your link to display as {logstash-ref}/getting-started-with-logstash.html[Getting Started with {ls}], use this format:
 [source,txt]
 -----
-{logstash-ref}/getting-started-with-logstash.html[Getting Started with Logstash]
+{logstash-ref}/getting-started-with-logstash.html[Getting Started with {ls}]
 -----
 
 ===== Link to data type descriptions
@@ -152,7 +152,7 @@ match => {
 ==== Where's my doc?
 
 Plugin documentation goes through several steps before it gets published in the 
-{lsplugindocs}[Logstash Versioned Plugin Reference] and the {logstash-ref}/index.html[Logstash Reference].
+{lsplugindocs}[{ls} Versioned Plugin Reference] and the {logstash-ref}/index.html[{ls} Reference].
 
 Here's an overview of the workflow:
 
@@ -161,7 +161,7 @@ Here's an overview of the workflow:
 * Wait for the continuous integration build to complete successfully.
 * Publish the plugin to https://rubygems.org.
 * A script detects the new or changed version, and picks up the `index.asciidoc` file for inclusion in the doc build.
-* The documentation for your new plugin is published in the {lsplugindocs}[Logstash Versioned Plugin Reference].
+* The documentation for your new plugin is published in the {lsplugindocs}[{ls} Versioned Plugin Reference].
 
 We're not done yet. 
 
@@ -169,8 +169,8 @@ We're not done yet.
 (We sometimes package plugin docs between releases if we make significant changes to plugin documentation or add a new plugin.)
 * The script detects the new or changed version, and picks up the `index.asciidoc` file for inclusion in the doc build.
 * We create a pull request, and merge the new and changed content into the appropriate version branches.
-* For a new plugin, we add a link to the list of plugins in the {logstash-ref}/index.html[Logstash Reference].
-* The documentation for your new (or changed) plugin is published in the {logstash-ref}/index.html[Logstash Reference].
+* For a new plugin, we add a link to the list of plugins in the {logstash-ref}/index.html[{ls} Reference].
+* The documentation for your new (or changed) plugin is published in the {logstash-ref}/index.html[{ls} Reference].
 
 ===== Documentation or plugin updates
 
@@ -187,5 +187,5 @@ For tips on contributing and changelog guidelines, see
 https://github.com/elastic/logstash/blob/main/CONTRIBUTING.md#logstash-plugin-changelog-guidelines[CONTRIBUTING.md].
 
 For general information about contributing, see
-{logstash-ref}/contributing-to-logstash.html[Contributing to Logstash].
+{logstash-ref}/contributing-to-logstash.html[Contributing to {ls}].
 
diff --git a/docs/static/docker.asciidoc b/docs/static/docker.asciidoc
index 0c636d008b1..904fd1f4563 100644
--- a/docs/static/docker.asciidoc
+++ b/docs/static/docker.asciidoc
@@ -1,6 +1,6 @@
 [[docker]]
-=== Running Logstash on Docker
-Docker images for Logstash are available from the Elastic Docker
+=== Running {ls} on Docker
+Docker images for {ls} are available from the Elastic Docker
 registry. The base image is https://hub.docker.com/_/ubuntu/[ubuntu:20.04].
 
 A list of all published Docker images and tags is available at
@@ -16,12 +16,12 @@ Elastic license levels.
 
 ==== Pulling the image
 
-Obtaining Logstash for Docker is as simple as issuing a +docker
+Obtaining {ls} for Docker is as simple as issuing a +docker
 pull+ command against the Elastic Docker registry.
 
 ifeval::["{release-state}"=="unreleased"]
 
-However, version {logstash_version} of Logstash has not yet been
+However, version {logstash_version} of {ls} has not yet been
 released, so no Docker image is currently available for this version.
 
 endif::[]
@@ -50,7 +50,7 @@ Run the following commands to verify the container image signature for {ls} v{ve
 
 ifeval::["{release-state}"=="unreleased"]
 
-Version {logstash_version} of Logstash has not yet been
+Version {logstash_version} of {ls} has not yet been
 released, so no Docker image is currently available for this version.
 
 endif::[]
@@ -79,15 +79,15 @@ The following checks were performed on each of these signatures:
 endif::[]
 
 [[docker-config]]
-=== Configuring Logstash for Docker
+=== Configuring {ls} for Docker
 
-Logstash differentiates between two types of configuration:
+{ls} differentiates between two types of configuration:
 <<config-setting-files,Settings and Pipeline Configuration>>.
 
 ==== Pipeline Configuration
 
 It is essential to place your pipeline configuration where it can be
-found by Logstash. By default, the container will look in
+found by {ls}. By default, the container will look in
 +/usr/share/logstash/pipeline/+ for pipeline configuration files.
 
 In this example we use a bind-mounted volume to provide the
@@ -99,9 +99,9 @@ docker run --rm -it -v ~/pipeline/:/usr/share/logstash/pipeline/ {docker-image}
 --------------------------------------------
 
 Every file in the host directory +~/pipeline/+ will then be parsed
-by Logstash as pipeline configuration.
+by {ls} as pipeline configuration.
 
-If you don't provide configuration to Logstash, it will run with a
+If you don't provide configuration to {ls}, it will run with a
 minimal config that listens for messages from the
 <<plugins-inputs-beats,Beats input plugin>> and echoes any that are
 received to `stdout`. In this case, the startup logs will be similar
@@ -109,12 +109,12 @@ to the following:
 
 ["source","text"]
 --------------------------------------------
-Sending Logstash logs to /usr/share/logstash/logs which is now configured via log4j2.properties.
+Sending {ls} logs to /usr/share/logstash/logs which is now configured via log4j2.properties.
 [2016-10-26T05:11:34,992][INFO ][logstash.inputs.beats    ] Beats inputs: Starting input listener {:address=>"0.0.0.0:5044"}
 [2016-10-26T05:11:35,068][INFO ][logstash.pipeline        ] Starting pipeline {"id"=>"main", "pipeline.workers"=>4, "pipeline.batch.size"=>125, "pipeline.batch.delay"=>5, "pipeline.max_inflight"=>500}
 [2016-10-26T05:11:35,078][INFO ][org.logstash.beats.Server] Starting server on port: 5044
 [2016-10-26T05:11:35,078][INFO ][logstash.pipeline        ] Pipeline main started
-[2016-10-26T05:11:35,105][INFO ][logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
+[2016-10-26T05:11:35,105][INFO ][logstash.agent           ] Successfully started {ls} API endpoint {:port=>9600}
 --------------------------------------------
 
 This is the default configuration for the image, defined in
@@ -132,7 +132,7 @@ also possible to use environment variables to define settings.
 [[docker-bind-mount-settings]]
 ===== Bind-mounted settings files
 
-Settings files can also be provided through bind-mounts. Logstash
+Settings files can also be provided through bind-mounts. {ls}
 expects to find them at +/usr/share/logstash/config/+.
 
 It's possible to provide an entire directory containing all needed
@@ -176,9 +176,9 @@ that you don't retain the example config from the base image.
 [[docker-env-config]]
 ===== Environment variable configuration
 
-Under Docker, Logstash settings can be configured via environment
+Under Docker, {ls} settings can be configured via environment
 variables. When the container starts, a helper process checks the environment
-for variables that can be mapped to Logstash settings. Settings that are found
+for variables that can be mapped to {ls} settings. Settings that are found
 in the environment override those in the `logstash.yml` as the container starts up.
 
 For compatibility with container orchestration systems, these environment
@@ -189,7 +189,7 @@ Some example translations are shown here:
 
 .Example Docker Environment Variables
 [horizontal]
-**Environment Variable**:: **Logstash Setting**
+**Environment Variable**:: **{ls} Setting**
 `PIPELINE_WORKERS`:: `pipeline.workers`
 `LOG_LEVEL`:: `log.level`
 `MONITORING_ENABLED`:: `monitoring.enabled`
@@ -201,7 +201,7 @@ NOTE: Defining settings with environment variables causes `logstash.yml` to
 be modified in place. This behaviour is likely undesirable if `logstash.yml` was
 bind-mounted from the host system. Thus, it is not recommended to
 combine the bind-mount technique with the environment variable technique. It
-is best to choose a single method for defining Logstash settings.
+is best to choose a single method for defining {ls} settings.
 
 ==== Docker defaults
 The following settings have different default values when using the Docker
@@ -224,6 +224,6 @@ be "masked" by the new file.
 
 ==== Logging Configuration
 
-Under Docker, Logstash logs go to standard output by default. To
+Under Docker, {ls} logs go to standard output by default. To
 change this behaviour, use any of the techniques above to replace the
 file at +/usr/share/logstash/config/log4j2.properties+.
diff --git a/docs/static/ecs-compatibility.asciidoc b/docs/static/ecs-compatibility.asciidoc
index 10d4a220228..5dcaab16e98 100644
--- a/docs/static/ecs-compatibility.asciidoc
+++ b/docs/static/ecs-compatibility.asciidoc
@@ -1,5 +1,5 @@
 [[ecs-ls]]
-=== ECS in Logstash
+=== ECS in {ls}
 
 // LS8 will ship with ECS v8, but until ECS v8 is ready we rely on ECS v1 as an approximation.
 :ls8-ecs-major-version: v8
diff --git a/docs/static/env-vars.asciidoc b/docs/static/env-vars.asciidoc
index 91604861c42..6865fd01574 100644
--- a/docs/static/env-vars.asciidoc
+++ b/docs/static/env-vars.asciidoc
@@ -3,14 +3,14 @@
 
 ==== Overview
 
-* You can set environment variable references in the configuration for Logstash plugins by using `${var}`.
-* At Logstash startup, each reference is replaced by the value of the environment variable.
+* You can set environment variable references in the configuration for {ls} plugins by using `${var}`.
+* At {ls} startup, each reference is replaced by the value of the environment variable.
 * The replacement is case-sensitive.
-* References to undefined variables raise a Logstash configuration error.
-* You can give a default value by using the form `${var:default value}`. Logstash uses the default value if the
+* References to undefined variables raise a {ls} configuration error.
+* You can give a default value by using the form `${var:default value}`. {ls} uses the default value if the
 environment variable is undefined.
 * You can add environment variable references in any plugin option type: string, number, boolean, array, or hash.
-* Environment variables are immutable. If you update the environment variable, you'll have to restart Logstash to pick up the updated value.
+* Environment variables are immutable. If you update the environment variable, you'll have to restart {ls} to pick up the updated value.
 
 ==== Examples
 
@@ -37,7 +37,7 @@ Now let's set the value of `TCP_PORT`:
 export TCP_PORT=12345
 ----
 
-At startup, Logstash uses this configuration:
+At startup, {ls} uses this configuration:
 
 [source,ruby]
 ----------------------------------
@@ -48,7 +48,7 @@ input {
 }
 ----------------------------------
 
-If the `TCP_PORT` environment variable is not set, Logstash returns a configuration error.
+If the `TCP_PORT` environment variable is not set, {ls} returns a configuration error.
 
 You can fix this problem by specifying a default value:
 
@@ -61,7 +61,7 @@ input {
 }
 ----
 
-Now, instead of returning a configuration error if the variable is undefined, Logstash uses the default:
+Now, instead of returning a configuration error if the variable is undefined, {ls} uses the default:
 
 [source,ruby]
 ----
@@ -72,7 +72,7 @@ input {
 }
 ----
 
-If the environment variable is defined, Logstash uses the value specified for the variable instead of the default.
+If the environment variable is defined, {ls} uses the value specified for the variable instead of the default.
 
 ===== Setting the value of a tag
 
@@ -94,7 +94,7 @@ Let's set the value of `ENV_TAG`:
 export ENV_TAG="tag2"
 ----
 
-At startup, Logstash uses this configuration:
+At startup, {ls} uses this configuration:
 
 [source,ruby]
 ----
@@ -127,7 +127,7 @@ Let's set the value of `HOME`:
 export HOME="/path"
 ----
 
-At startup, Logstash uses the following configuration:
+At startup, {ls} uses the following configuration:
 
 [source,ruby]
 ----
diff --git a/docs/static/event-api.asciidoc b/docs/static/event-api.asciidoc
index 59ad7c937c8..6104e5d3dbf 100644
--- a/docs/static/event-api.asciidoc
+++ b/docs/static/event-api.asciidoc
@@ -1,22 +1,22 @@
 [[event-api]]
 === Event API
 
-This section is targeted for plugin developers and users of Logstash's Ruby filter. Below we document recent 
-changes (starting with version 5.0) in the way users have been accessing Logstash's event based data in 
+This section is targeted for plugin developers and users of {ls}'s Ruby filter. Below we document recent 
+changes (starting with version 5.0) in the way users have been accessing {ls}'s event based data in 
 custom plugins and in the Ruby filter. Note that <<event-dependent-configuration>> 
-data flow in Logstash's config files -- using <<logstash-config-field-references>> -- is 
+data flow in {ls}'s config files -- using <<logstash-config-field-references>> -- is 
 not affected by this change, and will continue to use existing syntax.
 
 [float]
 ==== Event Object
 
-Event is the main object that encapsulates data flow internally in Logstash and provides an API for the plugin 
+Event is the main object that encapsulates data flow internally in {ls} and provides an API for the plugin 
 developers to interact with the event's content. Typically, this API is used in plugins and in a Ruby filter to 
-retrieve data and use it for transformations. Event object contains the original data sent to Logstash and any additional 
-fields created during Logstash's filter stages.
+retrieve data and use it for transformations. Event object contains the original data sent to {ls} and any additional 
+fields created during {ls}'s filter stages.
 
 In 5.0, we've re-implemented the Event class and its supporting classes in pure Java. Since Event is a critical component 
-in data processing,  a rewrite in Java improves performance and provides efficient serialization when storing data on disk. For the most part, this change aims at keeping backward compatibility and is transparent to the users. To this extent we've updated and published most of the plugins in Logstash's ecosystem to adhere to the new API changes. However, if you are maintaining a custom plugin, or have a Ruby filter, this change will affect you. The aim of this guide is to describe the new API and provide examples to migrate to the new changes.
+in data processing,  a rewrite in Java improves performance and provides efficient serialization when storing data on disk. For the most part, this change aims at keeping backward compatibility and is transparent to the users. To this extent we've updated and published most of the plugins in {ls}'s ecosystem to adhere to the new API changes. However, if you are maintaining a custom plugin, or have a Ruby filter, this change will affect you. The aim of this guide is to describe the new API and provide examples to migrate to the new changes.
 
 [float]
 ==== Event API
@@ -34,7 +34,7 @@ The getter is a read-only access of field-based data in an Event.
 **Returns:** Value for this field or nil if the field does not exist. Returned values could be a string, 
 numeric or timestamp scalar value.
 
-`field` is a structured field sent to Logstash or created after the transformation process. `field` can also 
+`field` is a structured field sent to {ls} or created after the transformation process. `field` can also 
 be a nested <<field-references-deepdive,field reference>> such as `[field][bar]`.
 
 Examples:
diff --git a/docs/static/event-data.asciidoc b/docs/static/event-data.asciidoc
index 78713346f56..f21271c9f27 100644
--- a/docs/static/event-data.asciidoc
+++ b/docs/static/event-data.asciidoc
@@ -1,16 +1,16 @@
 [[event-dependent-configuration]]
 === Accessing event data and fields
 
-A Logstash pipeline usually has three stages: inputs -> filters -> outputs. 
+A {ls} pipeline usually has three stages: inputs -> filters -> outputs. 
 Inputs generate events, filters modify them, and outputs ship them elsewhere.
 
 All events have properties. 
 For example, an Apache access log has properties
 like status code (200, 404), request path ("/", "index.html"), HTTP verb
 (GET, POST), client IP address, and so forth. 
-Logstash calls these properties "fields".
+{ls} calls these properties "fields".
 
-Some configuration options in Logstash require the existence of fields in
+Some configuration options in {ls} require the existence of fields in
 order to function.  
 Because inputs generate events, there are no fields to
 evaluate within the input block--they do not exist yet!
@@ -22,7 +22,7 @@ These configuration options depend on events and fields, and therefore, work onl
 [[logstash-config-field-references]]
 ==== Field references
 
-When you need to refer to a field by name, you can use the Logstash <<field-references-deepdive,field reference syntax>>. 
+When you need to refer to a field by name, you can use the {ls} <<field-references-deepdive,field reference syntax>>. 
 
 The basic syntax to access a field is `[fieldname]`. 
 If you are referring to a **top-level field**, you can omit the `[]` and simply
@@ -60,7 +60,7 @@ For more detailed information, see <<field-references-deepdive>>.
 [[sprintf]]
 ==== sprintf format
 
-The field reference format is also used in what Logstash calls 'sprintf format'. 
+The field reference format is also used in what {ls} calls 'sprintf format'. 
 This format enables you to embed field values in other strings. 
 For example, the statsd output has an 'increment' setting that enables you to
 keep a count of apache logs by status code:
@@ -92,7 +92,7 @@ output {
 NOTE: The sprintf format continues to support http://joda-time.sourceforge.net/apidocs/org/joda/time/format/DateTimeFormat.html[deprecated joda time format] strings as well using the `%{+FORMAT}` syntax.
 These formats are not directly interchangeable, and we advise you to begin using the more modern Java Time format.
 
-NOTE: A Logstash timestamp represents an instant on the UTC-timeline, so using sprintf formatters will produce results that may not align with your machine-local timezone.
+NOTE: A {ls} timestamp represents an instant on the UTC-timeline, so using sprintf formatters will produce results that may not align with your machine-local timezone.
 
 [discrete]
 [[conditionals]]
@@ -101,7 +101,7 @@ NOTE: A Logstash timestamp represents an instant on the UTC-timeline, so using s
 Sometimes you want to filter or output an event only under
 certain conditions. For that, you can use a conditional.
 
-Conditionals in Logstash look and act the same way they do in programming
+Conditionals in {ls} look and act the same way they do in programming
 languages. Conditionals support `if`, `else if` and `else` statements
 and can be nested.
 
@@ -222,7 +222,7 @@ See <<date-time>> for more details and an example.
 [[metadata]]
 ==== The @metadata field
 
-In Logstash, there is a special field called `@metadata`.  The contents
+In {ls}, there is a special field called `@metadata`.  The contents
 of `@metadata` are not part of any of your events at output time, which
 makes it great to use for conditionals, or extending and building event fields
 with field reference and `sprintf` formatting.
diff --git a/docs/static/field-reference.asciidoc b/docs/static/field-reference.asciidoc
index 830c0912e28..dead61a66bb 100644
--- a/docs/static/field-reference.asciidoc
+++ b/docs/static/field-reference.asciidoc
@@ -2,7 +2,7 @@
 == Field References Deep Dive
 
 It is often useful to be able to refer to a field or collection of fields by name. To do this,
-you can use the Logstash field reference syntax.
+you can use the {ls} field reference syntax.
 
 The syntax to access a field specifies the entire path to the field, with each fragment wrapped in square brackets.
 When a field name contains square brackets, they must be properly <<formal-grammar-escape-sequences, _escaped_>>.
@@ -36,14 +36,14 @@ Below is the formal grammar of the Field Reference, with notes and examples.
 [[formal-grammar-field-reference-literal]]
 ==== Field Reference Literal
 
-A _Field Reference Literal_ is a sequence of one or more _Path Fragments_ that can be used directly in Logstash pipeline <<conditionals,conditionals>> without any additional quoting (e.g. `[request]`, `[response][status]`).
+A _Field Reference Literal_ is a sequence of one or more _Path Fragments_ that can be used directly in {ls} pipeline <<conditionals,conditionals>> without any additional quoting (e.g. `[request]`, `[response][status]`).
 
 [source,antlr]
 fieldReferenceLiteral
   : ( pathFragment )+
   ;
 
-NOTE: In Logstash 7.x and earlier, a quoted value (such as `["foo"]`) is
+NOTE: In {ls} 7.x and earlier, a quoted value (such as `["foo"]`) is
 considered a field reference and isn't treated as a single element array. This
 behavior might cause confusion in conditionals, such as `[message] in ["foo",
 "bar"]` compared to `[message] in ["foo"]`. We discourage using names with
@@ -140,7 +140,7 @@ An _Embedded Field Reference_ is a _Field Reference_ that is itself wrapped in s
 === Escape Sequences
 
 For {ls} to reference a field whose name contains a character that has special meaning in the field reference grammar, the character must be escaped.
-Logstash can be globally configured to use one of two field reference escape modes:
+{ls} can be globally configured to use one of two field reference escape modes:
 
  - `none` (default): no escape sequence processing is done. Fields containing literal square brackets cannot be referenced by the Event API.
  - `percent`: URI-style percent encoding of UTF-8 bytes. The left square bracket (`[`) is expressed as `%5B`, and the right square bracket (`]`) is expressed as `%5D`.
diff --git a/docs/static/filter.asciidoc b/docs/static/filter.asciidoc
index e12712c8b3d..bc5a063b0b8 100644
--- a/docs/static/filter.asciidoc
+++ b/docs/static/filter.asciidoc
@@ -9,6 +9,6 @@
 
 :getstarted: Let's step through creating a {plugintype} plugin using the https://github.com/logstash-plugins/logstash-filter-example/[example {plugintype} plugin].
 
-:methodheader: pass:m[Logstash filters must implement the `register` and `filter` methods.]
+:methodheader: pass:m[{ls} filters must implement the `register` and `filter` methods.]
 
 include::include/pluginbody.asciidoc[]
diff --git a/docs/static/getting-started-with-logstash.asciidoc b/docs/static/getting-started-with-logstash.asciidoc
index 851de32c508..18a1e630a8a 100644
--- a/docs/static/getting-started-with-logstash.asciidoc
+++ b/docs/static/getting-started-with-logstash.asciidoc
@@ -1,7 +1,7 @@
 [[getting-started-with-logstash]]
-== Getting Started with Logstash
+== Getting Started with {ls}
 
-This section guides you through the process of installing Logstash and verifying that everything is running properly.
+This section guides you through the process of installing {ls} and verifying that everything is running properly.
 After learning how to stash your first event, you go on to create a more advanced pipeline that takes Apache web logs as
 input, parses the logs, and writes the parsed data to an Elasticsearch cluster. Then you learn how to stitch together multiple input and output plugins to unify data from a variety of disparate sources.
 
@@ -10,13 +10,13 @@ This section includes the following topics:
 * <<ls-jvm>>
 * <<installing-logstash>>
 * <<first-event>>
-* {logstash-ref}/advanced-pipeline.html[Parsing Logs with Logstash]
+* {logstash-ref}/advanced-pipeline.html[Parsing Logs with {ls}]
 * {logstash-ref}/multiple-input-output-plugins.html[Stitching Together Multiple Input and Output Plugins]
 
 include::jvm.asciidoc[]
 
 [[installing-logstash]]
-=== Installing Logstash
+=== Installing {ls}
 
 [discrete]
 [[installing-binary]]
@@ -24,10 +24,10 @@ include::jvm.asciidoc[]
 
 The {ls} binaries are available from 
 https://www.elastic.co/downloads/logstash[https://www.elastic.co/downloads].
-Download the Logstash installation file for your host environment--TAR.GZ, DEB,
+Download the {ls} installation file for your host environment--TAR.GZ, DEB,
 ZIP, or RPM. 
 
-Unpack the file. Do not install Logstash into a directory path that
+Unpack the file. Do not install {ls} into a directory path that
 contains colon (:) characters.
 
 [NOTE]
@@ -43,7 +43,7 @@ Alternatively, you can download an `oss` package, which contains only features
 that are available under the Apache 2.0 license. 
 --
 
-On supported Linux operating systems, you can use a package manager to install Logstash.
+On supported Linux operating systems, you can use a package manager to install {ls}.
 
 [discrete]
 [[package-repositories]]
@@ -51,9 +51,9 @@ On supported Linux operating systems, you can use a package manager to install L
 
 We also have repositories available for APT and YUM based distributions. Note
 that we only provide binary packages, but no source packages, as the packages
-are created as part of the Logstash build.
+are created as part of the {ls} build.
 
-We have split the Logstash package repositories by version into separate urls
+We have split the {ls} package repositories by version into separate urls
 to avoid accidental upgrades across major versions. For all {major-version}.y
 releases use {major-version} as version number.
 
@@ -68,14 +68,14 @@ to sign all our packages. It is available from https://pgp.mit.edu.
 [NOTE]
 --
 When installing from a package repository (or from the DEB or RPM installation file),
-you will need to run Logstash as a service. Please refer to
-{logstash-ref}/running-logstash.html[Running Logstash as a Service] for more
+you will need to run {ls} as a service. Please refer to
+{logstash-ref}/running-logstash.html[Running {ls} as a Service] for more
 information.
 
-For testing purposes, you may still run Logstash from the command line, but
+For testing purposes, you may still run {ls} from the command line, but
 you may need to define the default setting options (described in
-{logstash-ref}/dir-layout.html[Logstash Directory Layout]) manually. Please
-refer to {logstash-ref}/running-logstash-command-line.html[Running Logstash from the Command Line]
+{logstash-ref}/dir-layout.html[{ls} Directory Layout]) manually. Please
+refer to {logstash-ref}/running-logstash-command-line.html[Running {ls} from the Command Line]
 for more information.
 --
 
@@ -84,7 +84,7 @@ for more information.
 
 ifeval::["{release-state}"=="unreleased"]
 
-Version {logstash_version} of Logstash has not yet been released.
+Version {logstash_version} of {ls} has not yet been released.
 
 endif::[]
 
@@ -132,7 +132,7 @@ endif::[]
 
 [WARNING]
 ==================================================
-Use the `echo` method described above to add the Logstash repository.  Do not
+Use the `echo` method described above to add the {ls} repository.  Do not
 use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not
 provide a source package. If you have added the `deb-src` entry, you will see an
 error like the following:
@@ -151,7 +151,7 @@ it with:
 sudo apt-get update && sudo apt-get install logstash
 --------------------------------------------------
 
-See {logstash-ref}/running-logstash.html[Running Logstash] for details about managing Logstash as a system service.
+See {logstash-ref}/running-logstash.html[Running {ls}] for details about managing {ls} as a system service.
 
 endif::[]
 
@@ -160,7 +160,7 @@ endif::[]
 
 ifeval::["{release-state}"=="unreleased"]
 
-Version {logstash_version} of Logstash has not yet been released.
+Version {logstash_version} of {ls} has not yet been released.
 
 endif::[]
 
@@ -222,25 +222,25 @@ sudo yum install logstash
 WARNING: The repositories do not work with older rpm based distributions
          that still use RPM v3, like CentOS5.
 
-See the {logstash-ref}/running-logstash.html[Running Logstash] document for managing Logstash as a system service.
+See the {logstash-ref}/running-logstash.html[Running {ls}] document for managing {ls} as a system service.
 
 endif::[]
 
 [discrete]
 ==== Docker
 
-Images are available for running Logstash as a Docker container. They are
+Images are available for running {ls} as a Docker container. They are
 available from the Elastic Docker registry.
 
-See <<docker,Running Logstash on Docker>> for
-details on how to configure and run Logstash Docker containers.
+See <<docker,Running {ls} on Docker>> for
+details on how to configure and run {ls} Docker containers.
 
 [[first-event]]
 === Stashing Your First Event
 
-First, let's test your Logstash installation by running the most basic _Logstash pipeline_.
+First, let's test your {ls} installation by running the most basic _{ls} pipeline_.
 
-A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
+A {ls} pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
 plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
 the data to a destination.
 
@@ -248,7 +248,7 @@ the data to a destination.
 
 image::static/images/basic_logstash_pipeline.png[]
 
-To test your Logstash installation, run the most basic Logstash pipeline. 
+To test your {ls} installation, run the most basic {ls} pipeline. 
 
 **MacOS, Linux**
 
@@ -305,13 +305,13 @@ command line lets you quickly test configurations without having to edit a file
 The pipeline in the example takes input from the standard input, `stdin`, and moves that input to the standard output,
 `stdout`, in a structured format.
 
-After starting Logstash, wait until you see "Pipeline main started" and then enter `hello world` at the command prompt:
+After starting {ls}, wait until you see "Pipeline main started" and then enter `hello world` at the command prompt:
 
 [source,shell]
 hello world
 2013-11-21T01:22:14.405+0000 0.0.0.0 hello world
 
-Logstash adds timestamp and IP address information to the message. Exit Logstash by issuing a *CTRL-D* command in the
-shell where Logstash is running.
+{ls} adds timestamp and IP address information to the message. Exit {ls} by issuing a *CTRL-D* command in the
+shell where {ls} is running.
 
-Congratulations! You've created and run a basic Logstash pipeline. Next, you learn how to create a more realistic pipeline.
+Congratulations! You've created and run a basic {ls} pipeline. Next, you learn how to create a more realistic pipeline.
diff --git a/docs/static/glob-support.asciidoc b/docs/static/glob-support.asciidoc
index 3e28e9cb57b..f0ddc1f9801 100644
--- a/docs/static/glob-support.asciidoc
+++ b/docs/static/glob-support.asciidoc
@@ -1,7 +1,7 @@
 [[glob-support]]
 === Glob Pattern Support
 
-Logstash supports the following patterns wherever glob patterns are allowed:
+{ls} supports the following patterns wherever glob patterns are allowed:
 
 *`*`*::
 Match any file. You can also use an `*` to restrict other values in the glob.
diff --git a/docs/static/include/javapluginpkg.asciidoc b/docs/static/include/javapluginpkg.asciidoc
index 7f23d2f0aff..0a048cd2d02 100644
--- a/docs/static/include/javapluginpkg.asciidoc
+++ b/docs/static/include/javapluginpkg.asciidoc
@@ -9,7 +9,7 @@ plugin development, the procedure for packaging Java plugins as Ruby gems
 has been automated through a custom task in the Gradle build file provided
 with the example Java plugins. The following sections describe how to
 configure and execute that packaging task as well as how to install the
-packaged Java plugin in Logstash.
+packaged Java plugin in {ls}.
 
 [float]
 ==== Configuring the Gradle packaging task
@@ -27,7 +27,7 @@ group                      'org.logstashplugins' // must match the package of th
 version                    "${file("VERSION").text.trim()}" // read from required VERSION file
 description                = "Example Java filter implementation"
 pluginInfo.licenses        = ['Apache-2.0'] // list of SPDX license IDs
-pluginInfo.longDescription = "This gem is a Logstash plugin required to be installed on top of the Logstash core pipeline using \$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program"
+pluginInfo.longDescription = "This gem is a {ls} plugin required to be installed on top of the {ls} core pipeline using \$LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program"
 pluginInfo.authors         = ['Elasticsearch']
 pluginInfo.email           = ['info@elastic.co']
 pluginInfo.homepage        = "http://www.elastic.co/guide/en/logstash/current/index.html"
@@ -43,7 +43,7 @@ You should configure the values above for your plugin.
 root of your plugin's codebase.
 * `pluginInfo.pluginType` should be set to one of `input`, `filter`, `codec`,
 or `output`.
-* `pluginInfo.pluginName` must match the name specified on the `@LogstashPlugin`
+* `pluginInfo.pluginName` must match the name specified on the `@{ls}Plugin`
 annotation on the main plugin class. The Gradle packaging task will validate
 that and return an error if they do not match.
 
@@ -52,7 +52,7 @@ that and return an error if they do not match.
 
 Several Ruby source files along with a `gemspec` file and a `Gemfile` are
 required to package the plugin as a Ruby gem. These Ruby files are used only
-for defining the Ruby gem structure or at Logstash startup time to register
+for defining the Ruby gem structure or at {ls} startup time to register
 the Java plugin. They are not used during runtime event processing. The
 Gradle packaging task automatically generates all of these files based on
 the values configured in the section above.
@@ -70,10 +70,10 @@ That task will produce a gem file in the root directory of your
 plugin's codebase with the name `logstash-{plugintype}-<pluginName>-<version>.gem`
 
 [float]
-==== Installing the Java plugin in Logstash
+==== Installing the Java plugin in {ls}
 
 After you have packaged your Java plugin as a Ruby gem, you can install it in
-Logstash with this command:
+{ls} with this command:
 
 [source,shell]
 -----
diff --git a/docs/static/include/javapluginsetup.asciidoc b/docs/static/include/javapluginsetup.asciidoc
index ead8972bd0c..9a1c1e3177f 100644
--- a/docs/static/include/javapluginsetup.asciidoc
+++ b/docs/static/include/javapluginsetup.asciidoc
@@ -1,5 +1,5 @@
-To develop a new Java {plugintype} for Logstash, you write a new Java class that
-conforms to the Logstash Java {pluginclass} API, package it, and install it with the
+To develop a new Java {plugintype} for {ls}, you write a new Java class that
+conforms to the {ls} Java {pluginclass} API, package it, and install it with the
 logstash-plugin utility. We'll go through each of those steps.
 
 [float]
@@ -9,36 +9,36 @@ logstash-plugin utility. We'll go through each of those steps.
 ==== Copy the example repo
 
 Start by copying the {pluginrepo}. The plugin API is currently part of the
-Logstash codebase so you must have a local copy of that available. You can
-obtain a copy of the Logstash codebase with the following `git` command:
+{ls} codebase so you must have a local copy of that available. You can
+obtain a copy of the {ls} codebase with the following `git` command:
 
 [source,shell]
 -----
 git clone --branch <branch_name> --single-branch https://github.com/elastic/logstash.git <target_folder>
 -----
 
-The `branch_name` should correspond to the version of Logstash containing the
+The `branch_name` should correspond to the version of {ls} containing the
 preferred revision of the Java plugin API. 
 
 NOTE: The GA version of the Java plugin API is available in the `7.2`
-and later branches of the Logstash codebase.
+and later branches of the {ls} codebase.
 
-Specify the `target_folder` for your local copy of the Logstash codebase. If you
+Specify the `target_folder` for your local copy of the {ls} codebase. If you
 do not specify `target_folder`, it defaults to a new folder called `logstash`
 under your current folder.
 
 [float]
 ==== Generate the .jar file
 
-After you have obtained a copy of the appropriate revision of the Logstash
+After you have obtained a copy of the appropriate revision of the {ls}
 codebase, you need to compile it to generate the .jar file containing the Java
-plugin API. From the root directory of your Logstash codebase ($LS_HOME), you
+plugin API. From the root directory of your {ls} codebase ($LS_HOME), you
 can compile it with `./gradlew assemble` (or `gradlew.bat assemble` if you're
 running on Windows). This should produce the
 `$LS_HOME/logstash-core/build/libs/logstash-core-x.y.z.jar` where `x`, `y`, and
-`z` refer to the version of Logstash.
+`z` refer to the version of {ls}.
 
-After you have successfully compiled Logstash, you need to tell your Java plugin
+After you have successfully compiled {ls}, you need to tell your Java plugin
 where to find the `logstash-core-x.y.z.jar` file. Create a new file named
 `gradle.properties` in the root folder of your plugin project. That file should
 have a single line:
@@ -48,5 +48,5 @@ have a single line:
 LOGSTASH_CORE_PATH=<target_folder>/logstash-core
 -----
 
-where `target_folder` is the root folder of your local copy of the Logstash codebase.
+where `target_folder` is the root folder of your local copy of the {ls} codebase.
 
diff --git a/docs/static/include/pluginbody.asciidoc b/docs/static/include/pluginbody.asciidoc
index 2a3460556ab..08d7504098a 100644
--- a/docs/static/include/pluginbody.asciidoc
+++ b/docs/static/include/pluginbody.asciidoc
@@ -1,8 +1,8 @@
 [id="{plugintype}-new-plugin"]
 
-=== How to write a Logstash {plugintype} plugin
+=== How to write a {ls} {plugintype} plugin
 
-To develop a new {plugintype} for Logstash, build a self-contained Ruby gem
+To develop a new {plugintype} for {ls}, build a self-contained Ruby gem
 whose source code lives in its own GitHub repository. The Ruby gem can then be
 hosted and shared on RubyGems.org. You can use the example {plugintype}
 implementation as a starting point. (If you're unfamiliar with
@@ -14,7 +14,7 @@ https://www.ruby-lang.org/en/documentation/quickstart/[].)
 {getstarted}
 
 ===== Create a GitHub repo for your new plugin
-Each Logstash plugin lives in its own GitHub repository. To create a new repository for your plugin:
+Each {ls} plugin lives in its own GitHub repository. To create a new repository for your plugin:
 
 . Log in to GitHub.
 . Click the **Repositories** tab. You'll see a list of other repositories you've forked or contributed to.
@@ -27,9 +27,9 @@ Each Logstash plugin lives in its own GitHub repository. To create a new reposit
 
 ===== Use the plugin generator tool
 
-You can create your own Logstash plugin in seconds! The `generate` subcommand of `bin/logstash-plugin` creates the foundation 
-for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you 
-can start adding custom code to process data with Logstash.
+You can create your own {ls} plugin in seconds! The `generate` subcommand of `bin/logstash-plugin` creates the foundation 
+for a new {ls} plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you 
+can start adding custom code to process data with {ls}.
 
 For more information, see <<plugin-generator>>
 
@@ -118,7 +118,7 @@ require "socket" # for Socket.gethostname
 class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
   config_name "example"
 
-  # If undefined, Logstash will complain, even if codec is unused.
+  # If undefined, {ls} will complain, even if codec is unused.
   default :codec, "plain"
 
   # The message string to use in the event.
@@ -225,7 +225,7 @@ require "logstash/namespace"
 class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
 
   # Setting the config_name here is required. This is how you
-  # configure this {plugintype} from your Logstash config.
+  # configure this {plugintype} from your {ls} config.
   #
   # {plugintype} {
   #   {pluginname} { message => "My message..." }
@@ -276,9 +276,9 @@ class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
   config_name "example"
 
   # This sets the concurrency behavior of this plugin. By default it is :legacy, which was the standard
-  # way concurrency worked before Logstash 2.4
+  # way concurrency worked before {ls} 2.4
   # 
-  # You should explicitly set it to either :single or :shared as :legacy will be removed in Logstash 6.0
+  # You should explicitly set it to either :single or :shared as :legacy will be removed in {ls} 6.0
   # 
   # When configured as :single a single instance of the Output will be shared among the
   # pipeline worker threads. Access to the `#multi_receive/#multi_receive_encoded/#receive` method will be synchronized
@@ -310,7 +310,7 @@ Now let's take a line-by-line look at the example plugin.
 
 ===== `require` Statements
 
-Logstash {plugintype} plugins require parent classes defined in
+{ls} {plugintype} plugins require parent classes defined in
 +logstash/pass:attributes[{plugintype}]s/base+ and logstash/namespace:
 
 [source,ruby]
@@ -321,7 +321,7 @@ require "logstash/namespace"
 ----------------------------------
 
 Of course, the plugin you build may depend on other code, or even gems. Just put
-them here along with these Logstash dependencies.
+them here along with these {ls} dependencies.
 
 ==== Plugin Body
 
@@ -354,7 +354,7 @@ This is the name your plugin will call inside the {plugintype} configuration
 block.
 
 If you set +config_name "pass:attributes[{pluginname}]"+ in your plugin code,
-the corresponding Logstash configuration block would need to look like this:
+the corresponding {ls} configuration block would need to look like this:
 
 // /////////////////////////////////////////////////////////////////////////////
 // If encode_method is NOT defined (not a codec)
@@ -402,11 +402,11 @@ endif::encode_method[]
   config :variable_name, :validate => :variable_type, :default => "Default value", :required => boolean, :deprecated => boolean, :obsolete => string
 ----------------------------------
 The configuration, or `config` section allows you to define as many (or as few)
-parameters as are needed to enable Logstash to process events.
+parameters as are needed to enable {ls} to process events.
 
 There are several configuration attributes:
 
-* `:validate` - allows you to enforce passing a particular data type to Logstash
+* `:validate` - allows you to enforce passing a particular data type to {ls}
 for this configuration option, such as `:string`, `:password`, `:boolean`,
 `:number`, `:array`, `:hash`, `:path` (a file-system path), `uri`, `:codec` (since
 1.2.0), `:bytes`.  Note that this also works as a coercion 
@@ -438,7 +438,7 @@ ifdef::register_method[]
   end # def register
 ----------------------------------
 
-The Logstash `register` method is like an `initialize` method. It was originally
+The {ls} `register` method is like an `initialize` method. It was originally
 created to enforce having `super` called, preventing headaches for newbies.
 (Note: It may go away in favor of `initialize`, in conjunction with some
 enforced testing to ensure `super` is called.)
@@ -478,7 +478,7 @@ end # def filter
 ----------------------------------
 The plugin's `filter` method is where the actual filtering work takes place!
 Inside the `filter` method you can refer to the event data using the `Event`
-object. Event is the main object that encapsulates data flow internally in Logstash 
+object. Event is the main object that encapsulates data flow internally in {ls} 
 and provides an <<event-api, API>> for the plugin developers to interact with the 
 event's content.
 
@@ -498,7 +498,7 @@ Note that configuration variables are now in scope as instance variables, like
   filter_matched(event)
 ----------------------------------
 Calling the `filter_matched` method upon successful execution of the plugin will
-ensure that any fields or tags added through the Logstash configuration for this
+ensure that any fields or tags added through the {ls} configuration for this
 filter will be handled correctly. For example, any `add_field`, `remove_field`,
 `add_tag` and/or `remove_tag` actions will be performed at this time.
 
@@ -747,7 +747,7 @@ codec plugin:
 
 ** `sha1` is the sha1 signature used to verify the integrity of the file
 referenced by `url`.
-** `url` is the address from where Logstash will download the file.
+** `url` is the address from where {ls} will download the file.
 ** `files` is an optional array of files to extract from the downloaded file.
 Note that while tar archives can use absolute or relative paths, treat them as
 absolute in this array.  If `files` is not present, all files will be
@@ -783,7 +783,7 @@ Deprecations are noted in the `logstash-deprecation.log` file in the
 ===== Add a Gemfile
 
 Gemfiles allow Ruby's Bundler to maintain the dependencies for your plugin.
-Currently, all we'll need is the Logstash gem, for testing, but if you require
+Currently, all we'll need is the {ls} gem, for testing, but if you require
 other gems, you should add them in here.
 
 TIP: See http://bundler.io/gemfile.html[Bundler's Gemfile page] for more details.
@@ -809,8 +809,8 @@ Gem::Specification.new do |s|
   s.name = 'logstash-{plugintype}-{pluginname}'
   s.version = '0.1.0'
   s.licenses = ['Apache License (2.0)']
-  s.summary = "This {plugintype} does x, y, z in Logstash"
-  s.description = "This gem is a logstash plugin required to be installed on top of the Logstash core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program"
+  s.summary = "This {plugintype} does x, y, z in {ls}"
+  s.description = "This gem is a logstash plugin required to be installed on top of the {ls} core pipeline using $LS_HOME/bin/logstash-plugin install gemname. This gem is not a stand-alone program"
   s.authors = ["Elastic"]
   s.email = 'info@elastic.co'
   s.homepage = "http://www.elastic.co/guide/en/logstash/current/index.html"
@@ -836,7 +836,7 @@ It is appropriate to change these values to fit your plugin. In particular,
 `s.licenses` and `s.version` are also important and will come into play when
 you are ready to publish your plugin.
 
-Logstash and all its plugins are licensed under
+{ls} and all its plugins are licensed under
 https://github.com/elastic/logstash/blob/main/LICENSE.txt[Apache License, version 2 ("ALv2")].
 If you make your plugin publicly available via http://rubygems.org[RubyGems.org],
 please make sure to have this line in your gemspec:
@@ -856,7 +856,7 @@ a gem are only used for testing, then it would be a development dependency.
 [NOTE]
 =========================
 You can also have versioning requirements for your dependencies--including other
-Logstash plugins:
+{ls} plugins:
 
 [source,ruby]
 [subs="attributes"]
@@ -895,13 +895,13 @@ file at http://mvnrepository.com and download the specified version.
 ==== Document your plugin
 Documentation is an important part of your plugin. All plugin documentation is
 rendered and placed in the 
-{logstash-ref}[Logstash Reference] and the {lsplugindocs}[Versioned plugin docs].
+{logstash-ref}[{ls} Reference] and the {lsplugindocs}[Versioned plugin docs].
 
 See <<plugin-doc>> for tips and guidelines.
 
 ==== Add Tests
 
-Logstash loves tests. Lots of tests. If you're using your new {plugintype}
+{ls} loves tests. Lots of tests. If you're using your new {plugintype}
 plugin in a production environment, you'll want to have some tests to ensure you
 are not breaking any existing functionality.
 
@@ -981,9 +981,9 @@ this case, `0.1.0`.
 
 ===== Test installation
 
-You should test install your plugin into a clean installation of Logstash.
+You should test install your plugin into a clean installation of {ls}.
 Download the latest version from the
-https://www.elastic.co/downloads/logstash/[Logstash downloads page].
+https://www.elastic.co/downloads/logstash/[{ls} downloads page].
 
 . Untar and cd in to the directory:
 +
@@ -1005,7 +1005,7 @@ environment, and `0.1.0` with the correct version number from the gemspec file.
 bin/logstash-plugin install /my/logstash/plugins/logstash-{plugintype}-{pluginname}/logstash-{plugintype}-{pluginname}-0.1.0.gem
 ----------------------------------
 +
-* After running this, you should see feedback from Logstash that it was
+* After running this, you should see feedback from {ls} that it was
 successfully installed:
 +
 [source,sh]
@@ -1018,7 +1018,7 @@ Successfully installed 'logstash-{plugintype}-{pluginname}' with version '0.1.0'
 +
 [TIP]
 =======
-You can also use the Logstash plugin tool to determine which plugins are
+You can also use the {ls} plugin tool to determine which plugins are
 currently available:
 
 [source,sh]
@@ -1029,7 +1029,7 @@ Depending on what you have installed, you might see a short or long list of
 plugins: inputs, codecs, filters and outputs.
 =======
 +
-. Now try running Logstash with a simple configuration passed in via the
+. Now try running {ls} with a simple configuration passed in via the
 command-line, using the `-e` flag.
 [NOTE]
 Your results will depend on what your {plugintype} plugin is designed to do.
@@ -1076,7 +1076,7 @@ bin/logstash -e 'input { stdin{ codec => {pluginname}{}} } output {stdout { code
 The {pluginname} {plugintype} plugin will append the contents of `append` (which
 by default appends ", Hello World!")
 
-After starting Logstash, type something, for example "Random output string".
+After starting {ls}, type something, for example "Random output string".
 The resulting output message field contents should be,
 "Random output string, Hello World!":
 
@@ -1157,17 +1157,17 @@ verify that your target is receiving the expected results.
 
 endif::receive_method[]
 
-Congratulations! You've built, deployed and successfully run a Logstash
+Congratulations! You've built, deployed and successfully run a {ls}
 {plugintype}.
 
 ==== Submitting your plugin to http://rubygems.org[RubyGems.org] and https://github.com/logstash-plugins[logstash-plugins]
 
-Logstash uses http://rubygems.org[RubyGems.org] as its repository for all plugin
+{ls} uses http://rubygems.org[RubyGems.org] as its repository for all plugin
 artifacts. Once you have developed your new plugin, you can make it available to
-Logstash users by simply publishing it to RubyGems.org.
+{ls} users by simply publishing it to RubyGems.org.
 
 ===== Licensing
-Logstash and all its plugins are licensed under
+{ls} and all its plugins are licensed under
 https://github.com/elasticsearch/logstash/blob/main/LICENSE[Apache License, version 2 ("ALv2")].
 If you make your plugin publicly available via http://rubygems.org[RubyGems.org],
 please make sure to have this line in your gemspec:
@@ -1220,7 +1220,7 @@ in your local repository.
 . Publishes the gem to RubyGems.org
 ========
 
-That's it! Your plugin is published! Logstash users can now install your plugin
+That's it! Your plugin is published! {ls} users can now install your plugin
 by running:
 
 [source,sh]
@@ -1240,12 +1240,12 @@ we always welcome new plugins!
 Some of the many benefits of having your plugin in the logstash-plugins
 repository are:
 
-* **Discovery.** Your plugin will appear in the {logstash-ref}[Logstash Reference],
-where Logstash users look first for plugins and documentation.
+* **Discovery.** Your plugin will appear in the {logstash-ref}[{ls} Reference],
+where {ls} users look first for plugins and documentation.
 * **Documentation.** Your plugin documentation will automatically be added to the
-{logstash-ref}[Logstash Reference].
+{logstash-ref}[{ls} Reference].
 * **Testing.** With our testing infrastructure, your plugin will be continuously
-tested against current and future releases of Logstash.  As a result, users will
+tested against current and future releases of {ls}.  As a result, users will
 have the assurance that if incompatibilities arise, they will be quickly
 discovered and corrected.
 
@@ -1256,12 +1256,12 @@ coherence, quality, readability, stability and security.
 * **Tests.** Your plugin must contain tests to be accepted.  These tests are also
 subject to code review for scope and completeness.  It's ok if you don't know
 how to write tests -- we will guide you. We are working on publishing a guide to
-creating tests for Logstash which will make it easier.  In the meantime, you can
+creating tests for {ls} which will make it easier.  In the meantime, you can
 refer to http://betterspecs.org/ for examples.
 
 To begin migrating your plugin to logstash-plugins, simply create a new
 https://github.com/elasticsearch/logstash/issues[issue] in
-the Logstash repository. When the acceptance guidelines are completed, we will
+the {ls} repository. When the acceptance guidelines are completed, we will
 facilitate the move to the logstash-plugins organization using the recommended
 https://help.github.com/articles/transferring-a-repository/#transferring-from-a-user-to-an-organization[github process].
 
diff --git a/docs/static/ingest-convert.asciidoc b/docs/static/ingest-convert.asciidoc
index b263faebde4..56202ee163d 100644
--- a/docs/static/ingest-convert.asciidoc
+++ b/docs/static/ingest-convert.asciidoc
@@ -3,10 +3,10 @@
 
 After implementing {ref}/ingest.html[ingest] pipelines to parse your data, you
 might decide that you want to take advantage of the richer transformation
-capabilities in Logstash. For example, you may need to use Logstash instead of
+capabilities in {ls}. For example, you may need to use {ls} instead of
 ingest pipelines if you want to:
 
-* Ingest from more inputs. Logstash can natively ingest data from many other
+* Ingest from more inputs. {ls} can natively ingest data from many other
 sources like TCP, UDP, syslog, and relational databases.
 
 * Use multiple outputs. Ingest node was designed to only support Elasticsearch
@@ -14,15 +14,15 @@ as an output, but you may want to use more than one output. For example, you may
 want to archive your incoming data to S3 as well as indexing it in
 Elasticsearch.
 
-* Take advantage of the richer transformation capabilities in Logstash, such as
+* Take advantage of the richer transformation capabilities in {ls}, such as
 external lookups.
 
 * Use the persistent queue feature to handle spikes when ingesting data (from
 Beats and other sources).
 
-To make it easier for you to migrate your configurations, Logstash provides an
+To make it easier for you to migrate your configurations, {ls} provides an
 ingest pipeline conversion tool. The conversion tool takes the ingest pipeline
-definition as input and, when possible, creates the equivalent Logstash
+definition as input and, when possible, creates the equivalent {ls}
 configuration as output. 
 
 See <<ingest-converter-limitations>> for a full list of tool limitations.
@@ -30,7 +30,7 @@ See <<ingest-converter-limitations>> for a full list of tool limitations.
 [[ingest-converter-run]]
 ==== Running the tool
 
-You'll find the conversion tool in the `bin` directory of your Logstash
+You'll find the conversion tool in the `bin` directory of your {ls}
 installation. See <<dir-layout>> to find the location of `bin` on your system.
 
 To run the conversion tool, use the following command:
@@ -45,7 +45,7 @@ Where:
 * `INPUT_FILE_URI` is a file URI that specifies the full path to the JSON file
 that defines the ingest node pipeline. 
 
-* `OUTPUT_FILE_URI` is the file URI of the Logstash DSL file that will be
+* `OUTPUT_FILE_URI` is the file URI of the {ls} DSL file that will be
 generated by the tool.
 
 * `--append-stdio` is an optional flag that adds stdin and stdout sections to
diff --git a/docs/static/input.asciidoc b/docs/static/input.asciidoc
index 7c3f720e749..a7ed08b7251 100644
--- a/docs/static/input.asciidoc
+++ b/docs/static/input.asciidoc
@@ -7,6 +7,6 @@
 
 :getstarted: Let's step through creating an {plugintype} plugin using the https://github.com/logstash-plugins/logstash-input-example/[example {plugintype} plugin].
 
-:methodheader: pass:m[Logstash inputs must implement two main methods: `register` and `run`.]
+:methodheader: pass:m[{ls} inputs must implement two main methods: `register` and `run`.]
 
 include::include/pluginbody.asciidoc[]
diff --git a/docs/static/introduction.asciidoc b/docs/static/introduction.asciidoc
index e1bc7bc4c16..a4ee9c1624a 100644
--- a/docs/static/introduction.asciidoc
+++ b/docs/static/introduction.asciidoc
@@ -1,6 +1,6 @@
 [float]
 [[power-of-logstash]]
-== The Power of Logstash
+== The Power of {ls}
 
 *The ingestion workhorse for Elasticsearch and more*
 
@@ -17,9 +17,9 @@ Over 200 plugins available, plus the flexibility of creating and contributing yo
 image:static/images/logstash.png[]
 
 [float]
-== Logstash Loves Data
+== {ls} Loves Data
 
-Collect more, so you can know more. Logstash welcomes data of all shapes and sizes.
+Collect more, so you can know more. {ls} welcomes data of all shapes and sizes.
 
 [float]
 === Logs and Metrics
@@ -65,17 +65,17 @@ Explore an expansive breadth of other data.
 
 * In this age of technological advancement, the massive IoT world unleashes endless use cases through capturing and
 harnessing data from connected sensors.
-* Logstash is the common event collection backbone for ingestion of data shipped from mobile devices to intelligent
+* {ls} is the common event collection backbone for ingestion of data shipped from mobile devices to intelligent
 homes, connected vehicles, healthcare sensors, and many other industry specific applications.
 
 [float]
 == Easily Enrich Everything
 
 The better the data, the better the knowledge. Clean and transform your data during ingestion to gain near real-time
-insights immediately at index or output time. Logstash comes out-of-box with many aggregations and mutations along
+insights immediately at index or output time. {ls} comes out-of-box with many aggregations and mutations along
 with pattern matching, geo mapping, and dynamic lookup capabilities.
 
-* {logstash-ref}/plugins-filters-grok.html[Grok] is the bread and butter of Logstash filters and is used ubiquitously to derive
+* {logstash-ref}/plugins-filters-grok.html[Grok] is the bread and butter of {ls} filters and is used ubiquitously to derive
 structure out of unstructured data. Enjoy a wealth of integrated patterns aimed to help quickly resolve web, systems,
 networking, and other types of event formats.
 * Expand your horizons by deciphering {logstash-ref}/plugins-filters-geoip.html[geo coordinates] from IP addresses, normalizing
diff --git a/docs/static/java-codec.asciidoc b/docs/static/java-codec.asciidoc
index fd03eef94f3..ff6726480b0 100644
--- a/docs/static/java-codec.asciidoc
+++ b/docs/static/java-codec.asciidoc
@@ -14,7 +14,7 @@
 
 //:getstarted: Let's step through creating a {plugintype} plugin using the https://github.com/logstash-plugins/logstash-codec-example/[example {plugintype} plugin].
 
-//:methodheader: Logstash codecs must implement the `register` method, and the `decode` method or the `encode` method (or both).
+//:methodheader: {ls} codecs must implement the `register` method, and the `decode` method or the `encode` method (or both).
 
 [[java-codec-plugin]]
 
@@ -41,7 +41,7 @@ Let's look at the main class in that codec filter:
  
 [source,java]
 -----
-@LogstashPlugin(name="java_codec_example")
+@{ls}Plugin(name="java_codec_example")
 public class JavaCodecExample implements Codec {
 
     public static final PluginConfigSpec<String> DELIMITER_CONFIG =
@@ -115,20 +115,20 @@ Let's step through and examine each part of that class.
 
 [source,java]
 -----
-@LogstashPlugin(name="java_codec_example")
+@{ls}Plugin(name="java_codec_example")
 public class JavaCodecExample implements Codec {
 -----
 
 Notes about the class declaration:
 
-* All Java plugins must be annotated with the `@LogstashPlugin` annotation. Additionally:
+* All Java plugins must be annotated with the `@{ls}Plugin` annotation. Additionally:
 ** The `name` property of the annotation must be supplied and defines the name of the plugin as it will be used
-   in the Logstash pipeline definition. For example, this codec would be referenced in the codec section of the
-   an appropriate input or output in the Logstash pipeline defintion as `codec => java_codec_example { }`
+   in the {ls} pipeline definition. For example, this codec would be referenced in the codec section of the
+   an appropriate input or output in the {ls} pipeline defintion as `codec => java_codec_example { }`
 ** The value of the `name` property must match the name of the class excluding casing and underscores.
 * The class must implement the `co.elastic.logstash.api.Codec` interface.
 * Java plugins may not be created in the `org.logstash` or `co.elastic.logstash` packages to prevent potential
-clashes with classes in Logstash itself.
+clashes with classes in {ls} itself.
 
 [float]
 ===== Plugin settings
@@ -153,7 +153,7 @@ defines the delimiter on which the codec will split events. It is not a required
 setting and if it is not explicitly  set, its default value will be `,`.
 
 The `configSchema` method must return a list of all settings that the plugin
-supports. The Logstash execution engine  will validate that all required
+supports. The {ls} execution engine  will validate that all required
 settings are present and that no unsupported settings are present.
 
 [float]
@@ -187,7 +187,7 @@ character set.
 Any additional initialization may occur in the constructor as well. If there are
 any unrecoverable errors encountered in the configuration or initialization of
 the codec plugin, a descriptive exception should be thrown. The exception will
-be logged and will prevent Logstash from starting.
+be logged and will prevent {ls} from starting.
 
 [float]
 ==== Codec methods
@@ -267,7 +267,7 @@ partially-supplied byte streams across calls to `decode`.
 
 The `encode` method encodes an event into a sequence of bytes and writes it into
 the specified `OutputStream`. Because a single codec instance is shared across
-all pipeline workers in the output stage of the Logstash pipeline, codecs should
+all pipeline workers in the output stage of the {ls} pipeline, codecs should
 _not_ retain state across calls to their `encode` methods.
 
 [float]
@@ -284,7 +284,7 @@ public Codec cloneCodec() {
 The `cloneCodec` method should return an identical instance of the codec with the exception of its ID. Because codecs
 may be stateful across calls to their `decode` methods, input plugins that are multi-threaded should use a separate
 instance of each codec via the `cloneCodec` method for each of their threads. Because a single codec instance is shared
-across all pipeline workers in the output stage of the Logstash pipeline, codecs should _not_ retain state across calls
+across all pipeline workers in the output stage of the {ls} pipeline, codecs should _not_ retain state across calls
 to their `encode` methods. In the example above, the codec is cloned with the same delimiter but a different ID.
 
 [float]
@@ -313,9 +313,9 @@ test] that you can use as a template for your own.
 include::include/javapluginpkg.asciidoc[]
 
 [float]
-=== Run Logstash with the Java codec plugin
+=== Run {ls} with the Java codec plugin
 
-To test the plugin, start Logstash with:
+To test the plugin, start {ls} with:
 
 [source,java]
 -----
@@ -323,7 +323,7 @@ echo "foo,bar" | bin/logstash -e 'input { java_stdin { codec => java_codec_examp
 -----
 
 
-The expected Logstash output (excluding initialization) with the configuration above is:
+The expected {ls} output (excluding initialization) with the configuration above is:
 
 [source,txt]
 -----
@@ -344,9 +344,9 @@ The expected Logstash output (excluding initialization) with the configuration a
 [float]
 === Feedback
 
-If you have any feedback on Java plugin support in Logstash, please comment on our 
+If you have any feedback on Java plugin support in {ls}, please comment on our 
 https://github.com/elastic/logstash/issues/9215[main Github issue] or post in the 
-https://discuss.elastic.co/c/logstash[Logstash forum].
+https://discuss.elastic.co/c/logstash[{ls} forum].
 
 :pluginrepo!:
 :sversion!:
diff --git a/docs/static/java-filter.asciidoc b/docs/static/java-filter.asciidoc
index a756fb73e1d..706345a895e 100644
--- a/docs/static/java-filter.asciidoc
+++ b/docs/static/java-filter.asciidoc
@@ -27,7 +27,7 @@ to `day_of_week: "yadnoM"`. Let's look at the main class in that example filter:
  
 [source,java]
 -----
-@LogstashPlugin(name = "java_filter_example")
+@{ls}Plugin(name = "java_filter_example")
 public class JavaFilterExample implements Filter {
 
     public static final PluginConfigSpec<String> SOURCE_CONFIG =
@@ -78,20 +78,20 @@ Let's step through and examine each part of that class.
 
 [source,java]
 -----
-@LogstashPlugin(name = "java_filter_example")
+@{ls}Plugin(name = "java_filter_example")
 public class JavaFilterExample implements Filter {
 -----
 
 Notes about the class declaration:
 
-* All Java plugins must be annotated with the `@LogstashPlugin` annotation. Additionally:
+* All Java plugins must be annotated with the `@{ls}Plugin` annotation. Additionally:
 ** The `name` property of the annotation must be supplied and defines the name of the plugin as it will be used
-   in the Logstash pipeline definition. For example, this filter would be referenced in the filter section of the
-   Logstash pipeline defintion as `filter { java_filter_example => { .... } }`
+   in the {ls} pipeline definition. For example, this filter would be referenced in the filter section of the
+   {ls} pipeline defintion as `filter { java_filter_example => { .... } }`
 ** The value of the `name` property must match the name of the class excluding casing and underscores.
 * The class must implement the `co.elastic.logstash.api.Filter` interface.
 * Java plugins may not be created in the `org.logstash` or `co.elastic.logstash` packages to prevent potential
-clashes with classes in Logstash itself.
+clashes with classes in {ls} itself.
 
 [float]
 ==== Plugin settings
@@ -114,7 +114,7 @@ the name of the field in each event that will be reversed. It is not a required
 set, its default value will be `message`.
 
 The `configSchema` method must return a list of all settings that the plugin supports. In a future phase of the
-Java plugin project, the Logstash execution engine will validate that all required settings are present and that
+Java plugin project, the {ls} execution engine will validate that all required settings are present and that
 no unsupported settings are present.
 
 [float]
@@ -141,7 +141,7 @@ a local variable so that it can be used later in the `filter` method.
 Any additional initialization may occur in the constructor as well. If there are
 any unrecoverable errors encountered in the configuration or initialization of
 the filter plugin, a descriptive exception should be thrown. The exception will
-be logged and will prevent Logstash from starting.
+be logged and will prevent {ls} from starting.
 
 [float]
 ==== Filter method
@@ -160,7 +160,7 @@ public Collection<Event> filter(Collection<Event> events, FilterMatchListener ma
     return events;
 -----
 
-Finally, we come to the `filter` method that is invoked by the Logstash
+Finally, we come to the `filter` method that is invoked by the {ls}
 execution engine on batches of events as they flow through the event processing
 pipeline. The events to be filtered are supplied in the `events` argument and
 the method should return a collection of filtered events. Filters may perform a
@@ -247,9 +247,9 @@ unit test] that you can use as a template for your own.
 include::include/javapluginpkg.asciidoc[]
 
 [float]
-=== Run Logstash with the Java filter plugin
+=== Run {ls} with the Java filter plugin
 
-The following is a minimal Logstash configuration that can be used to test that
+The following is a minimal {ls} configuration that can be used to test that
 the Java filter plugin is correctly installed and functioning.
  
 [source,java]
@@ -265,8 +265,8 @@ output {
 }
 -----
 
-Copy the above Logstash configuration to a file such as `java_filter.conf`.
-Start Logstash with:
+Copy the above {ls} configuration to a file such as `java_filter.conf`.
+Start {ls} with:
 
 [source,shell]
 -----
@@ -274,7 +274,7 @@ bin/logstash -f /path/to/java_filter.conf
 -----
 
 
-The expected Logstash output (excluding initialization) with the configuration
+The expected {ls} output (excluding initialization) with the configuration
 above is:
 
 [source,txt]
@@ -291,9 +291,9 @@ above is:
 [float]
 === Feedback
 
-If you have any feedback on Java plugin support in Logstash, please comment on our 
+If you have any feedback on Java plugin support in {ls}, please comment on our 
 https://github.com/elastic/logstash/issues/9215[main Github issue] or post in the 
-https://discuss.elastic.co/c/logstash[Logstash forum].
+https://discuss.elastic.co/c/logstash[{ls} forum].
 
 :pluginrepo!:
 :sversion!:
diff --git a/docs/static/java-input.asciidoc b/docs/static/java-input.asciidoc
index cf287fc5bc8..ccc957953ef 100644
--- a/docs/static/java-input.asciidoc
+++ b/docs/static/java-input.asciidoc
@@ -24,7 +24,7 @@ terminating. Let's look at the main class in the  example input.
  
 [source,java]
 ----- 
-@LogstashPlugin(name="java_input_example")
+@{ls}Plugin(name="java_input_example")
 public class JavaInputExample implements Input {
 
     public static final PluginConfigSpec<Long> EVENT_COUNT_CONFIG =
@@ -90,20 +90,20 @@ Let's step through and examine each part of that class.
 
 [source,java]
 -----
-@LogstashPlugin(name="java_input_example")
+@{ls}Plugin(name="java_input_example")
 public class JavaInputExample implements Input {
 -----
 
 Notes about the class declaration:
 
-* All Java plugins must be annotated with the `@LogstashPlugin` annotation. Additionally:
+* All Java plugins must be annotated with the `@{ls}Plugin` annotation. Additionally:
 ** The `name` property of the annotation must be supplied and defines the name of the plugin as it will be used
-   in the Logstash pipeline definition. For example, this input would be referenced in the input section of the
-   Logstash pipeline defintion as `input { java_input_example => { .... } }`
+   in the {ls} pipeline definition. For example, this input would be referenced in the input section of the
+   {ls} pipeline defintion as `input { java_input_example => { .... } }`
 ** The value of the `name` property must match the name of the class excluding casing and underscores.
 * The class must implement the `co.elastic.logstash.api.Input` interface.
 * Java plugins may not be created in the `org.logstash` or `co.elastic.logstash` packages to prevent potential
-clashes with classes in Logstash itself.
+clashes with classes in {ls} itself.
 
 [float]
 ==== Plugin settings
@@ -133,7 +133,7 @@ if it is not explicitly set, the settings default to `3` and  `message`,
 respectively.
 
 The `configSchema` method must return a list of all settings that the plugin
-supports. In a future phase of the Java plugin project, the Logstash execution
+supports. In a future phase of the Java plugin project, the {ls} execution
 engine will validate that all required settings are present and that no
 unsupported settings are present.
 
@@ -163,7 +163,7 @@ the `start` method.
 Any additional initialization may occur in the constructor as well. If there are
 any unrecoverable errors encountered in the configuration or initialization of
 the input plugin, a descriptive exception should be thrown. The exception will
-be logged and will prevent Logstash from starting.
+be logged and will prevent {ls} from starting.
 
 [float]
 ==== Start method
@@ -256,9 +256,9 @@ test] that you can use as a template for your own.
 include::include/javapluginpkg.asciidoc[]
 
 [float]
-=== Running Logstash with the Java input plugin
+=== Running {ls} with the Java input plugin
 
-The following is a minimal Logstash configuration that can be used to test that
+The following is a minimal {ls} configuration that can be used to test that
 the Java input plugin is correctly installed and functioning.
 
 [source,java]
@@ -271,7 +271,7 @@ output {
 }
 -----
 
-Copy the above Logstash configuration to a file such as `java_input.conf`. 
+Copy the above {ls} configuration to a file such as `java_input.conf`. 
 Start {ls} with:
 
 [source,shell]
@@ -280,7 +280,7 @@ bin/logstash -f /path/to/java_input.conf
 -----
 
 
-The expected Logstash output (excluding initialization) with the configuration above is:
+The expected {ls} output (excluding initialization) with the configuration above is:
 
 [source,txt]
 -----
@@ -304,9 +304,9 @@ The expected Logstash output (excluding initialization) with the configuration a
 [float]
 === Feedback
 
-If you have any feedback on Java plugin support in Logstash, please comment on our 
+If you have any feedback on Java plugin support in {ls}, please comment on our 
 https://github.com/elastic/logstash/issues/9215[main Github issue] or post in the 
-https://discuss.elastic.co/c/logstash[Logstash forum].
+https://discuss.elastic.co/c/logstash[{ls} forum].
 
 :pluginrepo!:
 :sversion!:
diff --git a/docs/static/java-output.asciidoc b/docs/static/java-output.asciidoc
index a7a0d6e0c31..2fae49be60c 100644
--- a/docs/static/java-output.asciidoc
+++ b/docs/static/java-output.asciidoc
@@ -26,7 +26,7 @@ The example output plugin prints events to the console using the event's
  
 [source,java]
 -----
-@LogstashPlugin(name = "java_output_example")
+@{ls}Plugin(name = "java_output_example")
 public class JavaOutputExample implements Output {
 
     public static final PluginConfigSpec<String> PREFIX_CONFIG =
@@ -87,20 +87,20 @@ Let's step through and examine each part of that class.
 
 [source,java]
 -----
-@LogstashPlugin(name="java_output_example")
+@{ls}Plugin(name="java_output_example")
 public class JavaOutputExample implements Output {
 -----
 
 Notes about the class declaration:
 
-* All Java plugins must be annotated with the `@LogstashPlugin` annotation. Additionally:
+* All Java plugins must be annotated with the `@{ls}Plugin` annotation. Additionally:
 ** The `name` property of the annotation must be supplied and defines the name of the plugin as it will be used
-   in the Logstash pipeline definition. For example, this output would be referenced in the output section of the
-   Logstash pipeline definition as `output { java_output_example => { .... } }`
+   in the {ls} pipeline definition. For example, this output would be referenced in the output section of the
+   {ls} pipeline definition as `output { java_output_example => { .... } }`
 ** The value of the `name` property must match the name of the class excluding casing and underscores.
 * The class must implement the `co.elastic.logstash.api.Output` interface.
 * Java plugins may not be created in the `org.logstash` or `co.elastic.logstash` packages to prevent potential
-clashes with classes in Logstash itself.
+clashes with classes in {ls} itself.
 
 [float]
 ==== Plugin settings
@@ -125,7 +125,7 @@ defines an optional prefix to include in the output of the event. The setting is
 not required and if it is not explicitly set, it defaults to the empty string.
 
 The `configSchema` method must return a list of all settings that the plugin
-supports. In a future phase of the Java plugin project, the Logstash execution
+supports. In a future phase of the Java plugin project, the {ls} execution
 engine will validate that all required settings are present and that no
 unsupported settings are present.
 
@@ -160,7 +160,7 @@ defined that is useful for unit testing with a `Stream` other than `System.out`.
 Any additional initialization may occur in the constructor as well. If there are
 any unrecoverable errors encountered in the configuration or initialization of
 the output plugin, a descriptive exception should be thrown. The exception will
-be logged and will prevent Logstash from starting.
+be logged and will prevent {ls} from starting.
 
 [float]
 ==== Output method
@@ -238,9 +238,9 @@ test] that you can use as a template for your own.
 include::include/javapluginpkg.asciidoc[]
 
 [float]
-=== Running Logstash with the Java output plugin
+=== Running {ls} with the Java output plugin
 
-The following is a minimal Logstash configuration that can be used to test that
+The following is a minimal {ls} configuration that can be used to test that
 the Java output plugin is correctly installed and functioning.
 
 [source,java]
@@ -253,8 +253,8 @@ output {
 }
 -----
 
-Copy the above Logstash configuration to a file such as `java_output.conf`.
-Logstash should then be started with:
+Copy the above {ls} configuration to a file such as `java_output.conf`.
+{ls} should then be started with:
 
 [source,txt]
 -----
@@ -262,7 +262,7 @@ bin/logstash -f /path/to/java_output.conf
 -----
 
 
-The expected Logstash output (excluding initialization) with the configuration
+The expected {ls} output (excluding initialization) with the configuration
 above is:
 
 [source,txt]
@@ -273,9 +273,9 @@ above is:
 [float]
 === Feedback
 
-If you have any feedback on Java plugin support in Logstash, please comment on our 
+If you have any feedback on Java plugin support in {ls}, please comment on our 
 https://github.com/elastic/logstash/issues/9215[main Github issue] or post in the 
-https://discuss.elastic.co/c/logstash[Logstash forum].
+https://discuss.elastic.co/c/logstash[{ls} forum].
 
 :pluginrepo!:
 :sversion!:  
diff --git a/docs/static/jvm.asciidoc b/docs/static/jvm.asciidoc
index 88765d9077f..c9fcbdc6151 100644
--- a/docs/static/jvm.asciidoc
+++ b/docs/static/jvm.asciidoc
@@ -24,7 +24,7 @@ Adoptium Eclipse Temurin 17, the latest long term support (LTS) release of the J
 
 Use the LS_JAVA_HOME environment variable if you want to use a JDK other than the
 version that is bundled. 
-If you have the LS_JAVA_HOME environment variable set to use a custom JDK, Logstash
+If you have the LS_JAVA_HOME environment variable set to use a custom JDK, {ls}
 will continue to use the JDK version you have specified, even after you upgrade.
 =====
 
diff --git a/docs/static/keystore.asciidoc b/docs/static/keystore.asciidoc
index ce6a9f0b128..219c53b91de 100644
--- a/docs/static/keystore.asciidoc
+++ b/docs/static/keystore.asciidoc
@@ -1,9 +1,9 @@
 [[keystore]]
 === Secrets keystore for secure settings
 
-When you configure Logstash, you might need to specify sensitive settings or
+When you configure {ls}, you might need to specify sensitive settings or
 configuration, such as passwords. Rather than relying on file system permissions
-to protect these values, you can use the Logstash keystore to securely store
+to protect these values, you can use the {ls} keystore to securely store
 secret values for use in configuration settings.
 
 After adding a key and its secret value to the keystore, you can use the key in
@@ -37,10 +37,10 @@ In `logstash.yml`, use:
 xpack.management.elasticsearch.password: ${ES_PWD}
 -----  
 
-Notice that the Logstash keystore differs from the Elasticsearch keystore.
+Notice that the {ls} keystore differs from the Elasticsearch keystore.
 Whereas the Elasticsearch keystore lets you store `elasticsearch.yml` values by
-name, the Logstash keystore lets you specify arbitrary names that you
-can reference in the Logstash configuration.
+name, the {ls} keystore lets you specify arbitrary names that you
+can reference in the {ls} configuration.
 
 NOTE: There are some configuration fields that have no secret meaning, so not every field could leverage
 the secret store for variables substitution. Plugin's `id` field is a field of this kind
@@ -49,7 +49,7 @@ NOTE: Referencing keystore data from `pipelines.yml` or the command line (`-e`)
 is not currently supported.
 
 NOTE: Referencing keystore data from {logstash-ref}/logstash-centralized-pipeline-management.html[centralized pipeline management]
-requires each Logstash deployment to have a local copy of the keystore.
+requires each {ls} deployment to have a local copy of the keystore.
 
 NOTE: The {ls} keystore needs to be protected, but the {ls} user must
 have access to the file. While most things in {ls} can be protected with
@@ -57,7 +57,7 @@ have access to the file. While most things in {ls} can be protected with
 {ls} user. Use `chown logstash:root <keystore> && chmod 0600
 <keystore>`.
 
-When Logstash parses the settings (`logstash.yml`) or configuration
+When {ls} parses the settings (`logstash.yml`) or configuration
 (`/etc/logstash/conf.d/*.conf`), it resolves keys from the keystore before
 resolving environment variables.
 
@@ -66,11 +66,11 @@ resolving environment variables.
 [[keystore-password]]
 === Keystore password
 
-You can protect access to the Logstash keystore by storing a password in an
-environment variable called `LOGSTASH_KEYSTORE_PASS`. If you create the Logstash
+You can protect access to the {ls} keystore by storing a password in an
+environment variable called `LOGSTASH_KEYSTORE_PASS`. If you create the {ls}
 keystore after setting this variable, the keystore will be password protected.
 This means that the environment variable needs to be accessible to the running
-instance of Logstash. This environment variable must also be correctly set for
+instance of {ls}. This environment variable must also be correctly set for
 any users who need to issue keystore commands (add, list, remove, etc.).
 
 Using a keystore password is recommended, but optional. The data will be encrypted even if you
@@ -88,11 +88,11 @@ set -o history
 bin/logstash-keystore create
 --------------------------------------------------
 
-This setup requires the user running Logstash to have the environment variable
+This setup requires the user running {ls} to have the environment variable
 `LOGSTASH_KEYSTORE_PASS=mypassword` defined. If the environment variable is not defined,
-Logstash cannot access the the keystore.
+{ls} cannot access the the keystore.
 
-When you run Logstash from an RPM or DEB package installation, the environment
+When you run {ls} from an RPM or DEB package installation, the environment
 variables are sourced from `/etc/sysconfig/logstash`.
 
 NOTE: You might need to create `/etc/sysconfig/logstash`. This file should be
@@ -102,14 +102,14 @@ line.
 
 For other distributions, such as Docker or ZIP, see the documentation for your
 runtime environment (Windows, Docker, etc) to learn how to set the
-environment variable for the user that runs Logstash. Ensure that the
+environment variable for the user that runs {ls}. Ensure that the
 environment variable (and thus the password) is only accessible to that user.
 
 [discrete]
 [[keystore-location]]
 === Keystore location
 
-The keystore must be located in the Logstash `path.settings` directory. This is
+The keystore must be located in the {ls} `path.settings` directory. This is
 the same directory that contains the `logstash.yml` file. When performing any
 operation against the keystore, it is recommended to set `path.settings` for the
 keystore command. For example, to create a keystore on a RPM/DEB installation:
diff --git a/docs/static/life-of-an-event.asciidoc b/docs/static/life-of-an-event.asciidoc
index b0fa05b6d76..e017db836db 100644
--- a/docs/static/life-of-an-event.asciidoc
+++ b/docs/static/life-of-an-event.asciidoc
@@ -1,7 +1,7 @@
 [[pipeline]]
-== How Logstash Works
+== How {ls} Works
 
-The Logstash event processing pipeline has three stages: inputs -> filters ->
+The {ls} event processing pipeline has three stages: inputs -> filters ->
 outputs. Inputs generate events, filters modify them, and outputs ship them
 elsewhere. Inputs and outputs support codecs that enable you to encode or decode
 the data as it enters or exits the pipeline without having to use a separate
@@ -9,7 +9,7 @@ filter.
 
 [float]
 === Inputs
-You use inputs to get data into Logstash. Some of the more commonly-used inputs
+You use inputs to get data into {ls}. Some of the more commonly-used inputs
 are:
 
 * *file*: reads from a file on the filesystem, much like the UNIX command
@@ -17,8 +17,8 @@ are:
 * *syslog*: listens on the well-known port 514 for syslog messages and parses
 according to the RFC3164 format
 * *redis*: reads from a redis server, using both redis channels and redis lists.
-Redis is often used as a "broker" in a centralized Logstash installation, which
-queues Logstash events from remote Logstash "shippers".
+Redis is often used as a "broker" in a centralized {ls} installation, which
+queues {ls} events from remote {ls} "shippers".
 * *beats*: processes events sent by https://www.elastic.co/downloads/beats[Beats].
 
 For more information about the available inputs, see
@@ -26,13 +26,13 @@ For more information about the available inputs, see
 
 [float]
 === Filters
-Filters are intermediary processing devices in the Logstash pipeline. You can
+Filters are intermediary processing devices in the {ls} pipeline. You can
 combine filters with conditionals to perform an action on an event if it meets
 certain criteria. Some useful filters include:
 
 * *grok*: parse and structure arbitrary text. Grok is currently the best way in
-Logstash to parse unstructured log data into something structured and queryable.
-With 120 patterns built-in to Logstash, it's more than likely you'll find one
+{ls} to parse unstructured log data into something structured and queryable.
+With 120 patterns built-in to {ls}, it's more than likely you'll find one
 that meets your needs!
 * *mutate*: perform general transformations on event fields. You can rename,
 remove, replace, and modify fields in your events.
@@ -46,7 +46,7 @@ For more information about the available filters, see
 
 [float]
 === Outputs
-Outputs are the final phase of the Logstash pipeline. An event can pass through
+Outputs are the final phase of the {ls} pipeline. An event can pass through
 multiple outputs, but once all output processing is complete, the event has
 finished its execution. Some commonly used outputs include:
 
@@ -81,19 +81,19 @@ For more information about the available codecs, see
 [[execution-model]]
 === Execution Model
 
-The Logstash event processing pipeline coordinates the execution of inputs,
+The {ls} event processing pipeline coordinates the execution of inputs,
 filters, and outputs.
 
-Each input stage in the Logstash pipeline runs in its own thread. Inputs write
+Each input stage in the {ls} pipeline runs in its own thread. Inputs write
 events to a central queue that is either in memory (default) or on disk. Each
 pipeline worker thread takes a batch of events off this queue, runs the batch of
 events through the configured filters, and then runs the filtered events through
 any outputs. The size of the batch and number of pipeline worker threads are
 configurable (see <<tuning-logstash>>).
 
-By default, Logstash uses in-memory bounded queues between pipeline stages
-(input → filter and filter → output) to buffer events. If Logstash terminates
+By default, {ls} uses in-memory bounded queues between pipeline stages
+(input → filter and filter → output) to buffer events. If {ls} terminates
 unsafely, any events that are stored in memory will be lost. To help prevent data
-loss, you can enable Logstash to persist in-flight events to disk. See
+loss, you can enable {ls} to persist in-flight events to disk. See
 <<persistent-queues>> for more information.
 
diff --git a/docs/static/listing-a-plugin.asciidoc b/docs/static/listing-a-plugin.asciidoc
index 10bb16cdb6c..3774f941484 100644
--- a/docs/static/listing-a-plugin.asciidoc
+++ b/docs/static/listing-a-plugin.asciidoc
@@ -1,12 +1,12 @@
 [[plugin-listing]]
 === List your plugin
 
-The {logstash-ref}[Logstash Reference] is the first place {ls} users look for plugins and documentation. 
+The {logstash-ref}[{ls} Reference] is the first place {ls} users look for plugins and documentation. 
 If your plugin meets the <<plugin-acceptance,quality and acceptance guidelines>>, we may be able to list it in the guide.
 
 The plugin source and documentation will continue to live in your repo, and we will direct users there. 
 
-If you would like to have your plugin included in the {logstash-ref}[Logstash Reference], create a new https://github.com/elasticsearch/logstash/issues[issue] in the Logstash repository with the following information:
+If you would like to have your plugin included in the {logstash-ref}[{ls} Reference], create a new https://github.com/elasticsearch/logstash/issues[issue] in the {ls} repository with the following information:
 
 * Title: `PluginListing: <your-plugin-name>`
 * Body:
diff --git a/docs/static/logging.asciidoc b/docs/static/logging.asciidoc
index cf399d0cfc5..86ce40991ba 100644
--- a/docs/static/logging.asciidoc
+++ b/docs/static/logging.asciidoc
@@ -1,8 +1,8 @@
 [[logging]]
 === Logging
 
-Logstash emits internal logs during its operation, which are placed in `LS_HOME/logs` (or `/var/log/logstash` for
-DEB/RPM). The default logging level is `INFO`. Logstash's logging framework is based on
+{ls} emits internal logs during its operation, which are placed in `LS_HOME/logs` (or `/var/log/logstash` for
+DEB/RPM). The default logging level is `INFO`. {ls}'s logging framework is based on
 http://logging.apache.org/log4j/2.x/[Log4j 2 framework], and much of its functionality is exposed directly to users.
 
 You can configure logging for a particular subsystem, module, or plugin.
@@ -13,26 +13,26 @@ example, if you are debugging issues with Elasticsearch Output, you can increase
 log levels just for that component. This approach reduces noise from
 excessive logging and helps you focus on the problem area.
 
-You can configure logging using the `log4j2.properties` file or the Logstash API.
+You can configure logging using the `log4j2.properties` file or the {ls} API.
 
 * *`log4j2.properties` file.*  Changes made through the `log4j2.properties`
-file require you to restart Logstash for the changes to take effect.  Changes *persist*
+file require you to restart {ls} for the changes to take effect.  Changes *persist*
 through subsequent restarts. 
 * *Logging API.* Changes made through the Logging API are effective immediately 
-without a restart. The changes *do not persist* after Logstash
+without a restart. The changes *do not persist* after {ls}
 is restarted.
 
 [[log4j2]]
 ==== Log4j2 configuration
 
-Logstash ships with a `log4j2.properties` file with out-of-the-box settings, including logging to console. You
+{ls} ships with a `log4j2.properties` file with out-of-the-box settings, including logging to console. You
 can modify this file to change the rotation policy, type, and other
 https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers[log4j2
 configuration]. 
 
-You must restart Logstash to apply any changes that you make to
+You must restart {ls} to apply any changes that you make to
 this file.
-Changes to `log4j2.properties` persist after Logstash is restarted.
+Changes to `log4j2.properties` persist after {ls} is restarted.
 
 Here's an example using `outputs.elasticsearch`:
 
@@ -50,12 +50,12 @@ the logger name is obtained by lowercasing the full class name and replacing dou
 
 ==== Logging APIs
 
-For temporary logging changes, modifying the `log4j2.properties` file and restarting Logstash leads to unnecessary
+For temporary logging changes, modifying the `log4j2.properties` file and restarting {ls} leads to unnecessary
 downtime. Instead, you can dynamically update logging levels through the logging API. These settings are effective
 immediately and do not need a restart. 
 
-NOTE: By default, the logging API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
-instance, you need to launch Logstash with the `--api.http.port` flag specified to bind to a different port. See
+NOTE: By default, the logging API attempts to bind to `tcp:9600`. If this port is already in use by another {ls}
+instance, you need to launch {ls} with the `--api.http.port` flag specified to bind to a different port. See
 <<command-line-flags>> for more information.
 
 ===== Retrieve list of logging configurations
@@ -114,7 +114,7 @@ curl -XPUT 'localhost:9600/_node/logging?pretty' -H 'Content-Type: application/j
 '
 --------------------------------------------------
 
-While this setting is in effect, Logstash emits DEBUG-level logs for __all__ the Elasticsearch outputs
+While this setting is in effect, {ls} emits DEBUG-level logs for __all__ the Elasticsearch outputs
 specified in your configuration. Please note this new setting is transient and will not survive a restart.
 
 NOTE: If you want logging changes to persist after a restart, add them to `log4j2.properties` instead. 
@@ -135,7 +135,7 @@ You can specify the log file location using `--path.logs` setting.
 
 ==== Slowlog
 
-Slowlog for Logstash adds the ability to log when a specific event takes an abnormal amount of time to make its way
+Slowlog for {ls} adds the ability to log when a specific event takes an abnormal amount of time to make its way
 through the pipeline. Just like the normal application log, you can find slowlogs in your `--path.logs` directory.
 Slowlog is configured in the `logstash.yml` settings file with the following options:
 
diff --git a/docs/static/logstash-glossary.asciidoc b/docs/static/logstash-glossary.asciidoc
index d184899f37a..e3916d93eb3 100644
--- a/docs/static/logstash-glossary.asciidoc
+++ b/docs/static/logstash-glossary.asciidoc
@@ -1,75 +1,75 @@
 == Glossary
-Logstash Glossary
+{ls} Glossary
 
 apache ::
-	A very common open source web server application, which produces logs easily consumed by Logstash (Apache Common/Combined Log Format).
+	A very common open source web server application, which produces logs easily consumed by {ls} (Apache Common/Combined Log Format).
 
 agent ::
-	An invocation of Logstash with a particular configuration, allowing it to operate as a "shipper", a "collector", or a combination of functionalities.
+	An invocation of {ls} with a particular configuration, allowing it to operate as a "shipper", a "collector", or a combination of functionalities.
 
 
 broker ::
-	An intermediary used in a multi-tiered Logstash deployment which allows a queueing mechanism to be used. Examples of brokers are Redis, RabbitMQ, and Apache Kafka. This pattern is a common method of building fault-tolerance into a Logstash architecture.
+	An intermediary used in a multi-tiered {ls} deployment which allows a queueing mechanism to be used. Examples of brokers are Redis, RabbitMQ, and Apache Kafka. This pattern is a common method of building fault-tolerance into a {ls} architecture.
 
 buffer::
-	Within Logstash, a temporary storage area where events can queue up, waiting to be processed. The default queue size is 20 events, but it is not recommended to increase this, as Logstash is not designed to operate as a queueing mechanism.
+	Within {ls}, a temporary storage area where events can queue up, waiting to be processed. The default queue size is 20 events, but it is not recommended to increase this, as {ls} is not designed to operate as a queueing mechanism.
 
 centralized::
-	A configuration of Logstash in which the Logstash agent, input and output sources live on multiple machines, and the pipeline passes through these tiers.
+	A configuration of {ls} in which the {ls} agent, input and output sources live on multiple machines, and the pipeline passes through these tiers.
 
 codec::
-	A Logstash plugin which works within an input or output plugin, and usually aims to serialize or deserialize data flowing through the Logstash pipeline. A common example is the JSON codec, which allows Logstash inputs to receive data which arrives in JSON format, or output event data in JSON format.
+	A {ls} plugin which works within an input or output plugin, and usually aims to serialize or deserialize data flowing through the {ls} pipeline. A common example is the JSON codec, which allows {ls} inputs to receive data which arrives in JSON format, or output event data in JSON format.
 
 collector::
-	An instance of Logstash which receives external events from another instance of Logstash, or perhaps some other client, either remote or local.
+	An instance of {ls} which receives external events from another instance of {ls}, or perhaps some other client, either remote or local.
 
 conditional::
-	In a computer programming context, a control flow which executes certain actions based on true/false values of a statement (called the condition). Often expressed in the form of "if ... then ... (elseif ...) else". Logstash has built-in conditionals to allow users control of the plugin pipeline.
+	In a computer programming context, a control flow which executes certain actions based on true/false values of a statement (called the condition). Often expressed in the form of "if ... then ... (elseif ...) else". {ls} has built-in conditionals to allow users control of the plugin pipeline.
 
 elasticsearch::
 	An open-source, Lucene-based, RESTful search and analytics engine written in Java, with supported clients in various languages such as Perl, Python, Ruby, Java, etc.
 
 event::
-	In Logstash parlance, a single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the Logstash pipeline.
+	In {ls} parlance, a single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the {ls} pipeline.
 
 field::
-	A data point (often a key-value pair) within a full Logstash event, e.g. "timestamp", "message", "hostname", "ipaddress". Also used to describe a key-value pair in Elasticsearch.
+	A data point (often a key-value pair) within a full {ls} event, e.g. "timestamp", "message", "hostname", "ipaddress". Also used to describe a key-value pair in Elasticsearch.
 
 file::
-	A resource storing binary data (which might be text, image, application, etc.) on a physical storage media. In the Logstash context, a common input source which monitors a growing collection of text-based log lines.
+	A resource storing binary data (which might be text, image, application, etc.) on a physical storage media. In the {ls} context, a common input source which monitors a growing collection of text-based log lines.
 
 filter:
-	An intermediary processing mechanism in the Logstash pipeline. Typically, filters act upon event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to configuration rules. The second phase of the typical Logstash pipeline (inputs->filters->outputs).
+	An intermediary processing mechanism in the {ls} pipeline. Typically, filters act upon event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to configuration rules. The second phase of the typical {ls} pipeline (inputs->filters->outputs).
 
 fluentd::
-	Like Logstash, another open-source tool for collecting logs and events, with plugins to extend functionality.
+	Like {ls}, another open-source tool for collecting logs and events, with plugins to extend functionality.
 
 ganglia::
-	A scalable, distributed monitoring system suitable for large clusters. Logstash features both an input and an output to enable reading from, and writing to Ganglia.
+	A scalable, distributed monitoring system suitable for large clusters. {ls} features both an input and an output to enable reading from, and writing to Ganglia.
 
 graphite::
-	A highly-scalable realtime graphing application, which presents graphs through web interfaces. Logstash provides an output which ships event data to Graphite for visualization.
+	A highly-scalable realtime graphing application, which presents graphs through web interfaces. {ls} provides an output which ships event data to Graphite for visualization.
 
 heka::
-	An open-source event processing system developed by Mozilla and often compared to Logstash.
+	An open-source event processing system developed by Mozilla and often compared to {ls}.
 
 index::
 	An index can be seen as a named collection of documents in Elasticsearch which are available for searching and querying. It is a logical namespace which maps to one or more primary shards and can have zero or more replica shards.
 
 indexer::
-	Refers to a Logstash instance which is tasked with interfacing with an Elasticsearch cluster in order to index event data.
+	Refers to a {ls} instance which is tasked with interfacing with an Elasticsearch cluster in order to index event data.
 
 input::
-	The means for ingesting data into Logstash. Inputs allow users to pull data from files, network sockets, other applications, etc. The initial phase of the typical Logstash pipeline (inputs->filters->outputs).
+	The means for ingesting data into {ls}. Inputs allow users to pull data from files, network sockets, other applications, etc. The initial phase of the typical {ls} pipeline (inputs->filters->outputs).
 
 jar / jarfile::
-	A packaging method for Java libraries. Since Logstash runs on the JRuby runtime environment, it is possible to use these Java libraries to provide extra functionality to Logstash.
+	A packaging method for Java libraries. Since {ls} runs on the JRuby runtime environment, it is possible to use these Java libraries to provide extra functionality to {ls}.
 
 java::
 	An object-oriented programming language popular for its flexibility, extendability and portability.
 
 jRuby:
-	JRuby is a 100% Java implementation of the Ruby programming language, which allows Ruby to run in the JVM. Logstash typically runs in JRuby, which provides it with a fast, extensible runtime environment.
+	JRuby is a 100% Java implementation of the Ruby programming language, which allows Ruby to run in the JVM. {ls} typically runs in JRuby, which provides it with a fast, extensible runtime environment.
 
 kibana::
 	A visual tool for viewing time-based data which has been stored in Elasticsearch. Kibana features a powerful set of functionality based on panels which query Elasticsearch in different ways.
@@ -80,53 +80,53 @@ log::
 log4j::
 	A very common Java-based logging utility.
 
-Logstash::
+{ls}::
 	An application which offers a powerful data processing pipeline, allowing users to consume information from various sources, enrich the data, and output it to any number of other sources.
 
 lumberjack::
-	A protocol for shipping logs from one location to another, in a secure and optimized manner. Also the (deprecated) name of a software application, now known as Logstash Forwarder (LSF).
+	A protocol for shipping logs from one location to another, in a secure and optimized manner. Also the (deprecated) name of a software application, now known as {ls} Forwarder (LSF).
 
 output::
-	The means for passing event data out of Logstash into other applications, network endpoints, files, etc. The last phase of the typical Logstash pipeline (inputs->filters->outputs).
+	The means for passing event data out of {ls} into other applications, network endpoints, files, etc. The last phase of the typical {ls} pipeline (inputs->filters->outputs).
 
 pipeline::
-	A term used to describe the flow of events through the Logstash workflow. The pipeline typically consists of a series of inputs, filters, and outputs.
+	A term used to describe the flow of events through the {ls} workflow. The pipeline typically consists of a series of inputs, filters, and outputs.
 
 plugin::
-	A generic term referring to an input, codec, filter, or output which extends basic Logstash functionality.
+	A generic term referring to an input, codec, filter, or output which extends basic {ls} functionality.
 
 redis::
-	An open-source key-value store and cache which is often used in conjunction with Logstash as a message broker.
+	An open-source key-value store and cache which is often used in conjunction with {ls} as a message broker.
 
 ruby::
-	A popular, open-source, object-oriented programming language in which Logstash is implemented.
+	A popular, open-source, object-oriented programming language in which {ls} is implemented.
 
 shell::
 	A command-line interface to an operating system.
 
 shipper::
-	An instance of Logstash which send events to another instance of Logstash, or some other application.
+	An instance of {ls} which send events to another instance of {ls}, or some other application.
 
 statsd::
-	A network daemon for aggregating statistics, such as counters and timers, and shipping over UDP to backend services, such as Graphite or Datadog. Logstash provides an output to statsd.
+	A network daemon for aggregating statistics, such as counters and timers, and shipping over UDP to backend services, such as Graphite or Datadog. {ls} provides an output to statsd.
 
 stdin::
-	An I/O stream providing input to a software application. In Logstash, an input which receives data from this stream.
+	An I/O stream providing input to a software application. In {ls}, an input which receives data from this stream.
 
 stdout::
-	An I/O stream producing output from a software application. In Logstash, an output which produces data from this stream.
+	An I/O stream producing output from a software application. In {ls}, an output which produces data from this stream.
 
 syslog::
-	A popular method for logging messages from a computer. The standard is somewhat loose, but Logstash has tools (input, grok patterns) to make this simpler.
+	A popular method for logging messages from a computer. The standard is somewhat loose, but {ls} has tools (input, grok patterns) to make this simpler.
 
 standalone::
-	A configuration of Logstash in which the Logstash agent, input and output sources typically live on the same host machine.
+	A configuration of {ls} in which the {ls} agent, input and output sources typically live on the same host machine.
 
 thread::
-	Parallel sequences of execution within a process which allow a computer to perform several tasks simultaneously, in a multi-processor environment. Logstash takes advantage of this functionality, by specifying the "-w" flag
+	Parallel sequences of execution within a process which allow a computer to perform several tasks simultaneously, in a multi-processor environment. {ls} takes advantage of this functionality, by specifying the "-w" flag
 
 type::
 	In Elasticsearch type, a type can be compared to a table in a relational database. Each type has a list of fields that can be specified for documents of that type. The mapping defines how each field in the document is analyzed. To index documents, it is required to specify both an index and a type.
 
 worker::
-	The filter thread model used by Logstash, where each worker receives an event and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety).
+	The filter thread model used by {ls}, where each worker receives an event and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive (permitting that we have thread safety).
diff --git a/docs/static/ls-ls-config.asciidoc b/docs/static/ls-ls-config.asciidoc
index a7adc5b4784..4bd748d5580 100644
--- a/docs/static/ls-ls-config.asciidoc
+++ b/docs/static/ls-ls-config.asciidoc
@@ -1,14 +1,14 @@
 [[ls-to-ls]]
-== Logstash-to-Logstash communication
+== {ls}-to-{ls} communication
 
 {ls}-to-{ls} communication is available if you need to have one {ls} instance communicate with another {ls} instance. 
-Implementing Logstash-to-Logstash communication can add complexity to your environment, but you may need it if the data path crosses network or firewall boundaries. 
+Implementing {ls}-to-{ls} communication can add complexity to your environment, but you may need it if the data path crosses network or firewall boundaries. 
 However, we suggest you don't implement unless it is strictly required.
 
 NOTE: If you are looking for information on connecting multiple pipelines within
-one Logstash instance, see <<pipeline-to-pipeline>>.
+one {ls} instance, see <<pipeline-to-pipeline>>.
 
-Logstash-to-Logstash communication can be achieved in one of two ways: 
+{ls}-to-{ls} communication can be achieved in one of two ways: 
 
 * <<lumberjack-considerations,Lumberjack output to Beats input>>
 * <<http-considerations,HTTP output to HTTP input>>
@@ -20,7 +20,7 @@ Before you implement the Lumberjack to Beats configuration, keep these points in
 
 * Lumberjack to Beats provides high availability, but does not provide load balancing. 
 The Lumberjack output plugin allows defining multiple output hosts for high availability, but instead of load-balancing between all output hosts, it falls back to one host on the list in the case of failure.
-* If you need a proxy between the Logstash instances, TCP proxy is the only option.
+* If you need a proxy between the {ls} instances, TCP proxy is the only option.
 * There's no explicit way to exert back pressure back to the beats input.
 
 Ready to see more configuration details? See <<ls-to-ls-lumberjack>>.
@@ -31,14 +31,14 @@ This approach relies on the use of <<plugins-outputs-http,http output>> to <<plu
 Take these considerations into account before you implement:
 
 * HTTP does not provide built-in high availability. You will need to implement your own load balancer in between the HTTP output and the HTTP input. 
-* If you need a proxy between the Logstash instances, you can use any HTTP proxy. 
+* If you need a proxy between the {ls} instances, you can use any HTTP proxy. 
 * The HTTP input adds connection information to events, and this may be data you don't want.
 
 For now, <<plugins-outputs-http,http output>> to <<plugins-inputs-http,http input>> with manual configuration may be the best path forward if these limitations don't apply to your use case.
 
 Ready to see more configuration details? See <<ls-to-ls-http>>.
 
-NOTE: In the future, we may replace the implementation of Logstash-to-Logstash with a purpose-build HTTP implementation, which would deprecate the use of Lumberjack and Beats, or the use of the HTTP Input and Output plugins.
+NOTE: In the future, we may replace the implementation of {ls}-to-{ls} with a purpose-build HTTP implementation, which would deprecate the use of Lumberjack and Beats, or the use of the HTTP Input and Output plugins.
 
 include::ls-ls-lumberjack.asciidoc[]
 include::ls-ls-http.asciidoc[]
diff --git a/docs/static/ls-ls-http.asciidoc b/docs/static/ls-ls-http.asciidoc
index 6af1af7f772..db751232a25 100644
--- a/docs/static/ls-ls-http.asciidoc
+++ b/docs/static/ls-ls-http.asciidoc
@@ -1,7 +1,7 @@
 [[ls-to-ls-http]]
-=== Logstash-to-Logstash: HTTP output to HTTP input
+=== {ls}-to-{ls}: HTTP output to HTTP input
 
-HTTP output to HTTP input is an alternative to the Lumberjack output to Beats input approach for Logstash-to-Logstash communication. 
+HTTP output to HTTP input is an alternative to the Lumberjack output to Beats input approach for {ls}-to-{ls} communication. 
 This approach relies on the use of <<plugins-outputs-http,http output>> to <<plugins-inputs-http,http input>> plugins.
 
 NOTE: Check out these <<http-considerations,considerations>> before you implement {ls}-to-{ls} using HTTP. 
@@ -11,16 +11,16 @@ For now, <<plugins-outputs-http,http output>> to <<plugins-inputs-http,http inpu
 [[overview-http-http]]
 ==== Configuration overview
 
-To use the HTTP protocol to connect two Logstash instances:
+To use the HTTP protocol to connect two {ls} instances:
 
-. Configure the downstream (server) Logstash to use HTTP input
-. Configure the upstream (client) Logstash to use HTTP output
+. Configure the downstream (server) {ls} to use HTTP input
+. Configure the upstream (client) {ls} to use HTTP output
 . Secure the communication between HTTP input and HTTP output
 
 [[configure-downstream-logstash-http-input]]
-===== Configure the downstream Logstash to use HTTP input
+===== Configure the downstream {ls} to use HTTP input
 
-Configure the HTTP input on the downstream (receiving) Logstash to receive connections. 
+Configure the HTTP input on the downstream (receiving) {ls} to receive connections. 
 The minimum configuration requires these options:
 
 * `port` - To set a custom port.
@@ -37,11 +37,11 @@ input {
 ----
 
 [[configure-upstream-logstash-http-output]]
-===== Configure the upstream Logstash to use HTTP output
+===== Configure the upstream {ls} to use HTTP output
 
-In order to obtain the best performance when sending data from one Logstash to another, the data needs to be batched and compressed. As such, the upstream Logstash (the sending Logstash) needs to be configured with these options:
+In order to obtain the best performance when sending data from one {ls} to another, the data needs to be batched and compressed. As such, the upstream {ls} (the sending {ls}) needs to be configured with these options:
 
-* `url` - The receiving Logstash.
+* `url` - The receiving {ls}.
 * `http_method` - Set to `post`.
 * `retry_non_idempotent` - Set to `true`, in order to retry failed events.
 * `format` - Set to `json_batch` to batch the data.
@@ -61,23 +61,23 @@ output {
 ----
 
 [[securing-logstash-to-logstash]]
-===== Secure Logstash to Logstash
+===== Secure {ls} to {ls}
 
-It is important that you secure the communication between Logstash instances. 
-Use SSL/TLS mutual authentication in order to ensure that the upstream Logstash instance sends encrypted data to a trusted downstream Logstash instance, and vice versa. 
+It is important that you secure the communication between {ls} instances. 
+Use SSL/TLS mutual authentication in order to ensure that the upstream {ls} instance sends encrypted data to a trusted downstream {ls} instance, and vice versa. 
 
-. Create a certificate authority (CA) in order to sign the certificates that you plan to use between Logstash instances. Creating a correct SSL/TLS infrastructure is outside the scope of this document.
+. Create a certificate authority (CA) in order to sign the certificates that you plan to use between {ls} instances. Creating a correct SSL/TLS infrastructure is outside the scope of this document.
 +
 TIP: We recommend you use the {ref}/certutil.html[elasticsearch-certutil] tool to generate your certificates.
 
-. Configure the downstream (receiving) Logstash to use SSL. 
+. Configure the downstream (receiving) {ls} to use SSL. 
 Add these settings to the HTTP Input configuration:
 +
- * `ssl`: When set to `true`, it enables Logstash use of SSL/TLS
- * `ssl_key`: Specifies the key that Logstash uses to authenticate with the client.
- * `ssl_certificate`: Specifies the certificate that Logstash uses to authenticate with the client.
- * `ssl_certificate_authorities`: Configures Logstash to trust any certificates signed by the specified CA.
- * `ssl_verify_mode`:  Specifies whether Logstash server verifies the client certificate against the CA.
+ * `ssl`: When set to `true`, it enables {ls} use of SSL/TLS
+ * `ssl_key`: Specifies the key that {ls} uses to authenticate with the client.
+ * `ssl_certificate`: Specifies the certificate that {ls} uses to authenticate with the client.
+ * `ssl_certificate_authorities`: Configures {ls} to trust any certificates signed by the specified CA.
+ * `ssl_verify_mode`:  Specifies whether {ls} server verifies the client certificate against the CA.
 +
 For example:
 +
@@ -96,12 +96,12 @@ input {
 }
 ----
 
-. Configure the upstream (sending) Logstash to use SSL. 
+. Configure the upstream (sending) {ls} to use SSL. 
 Add these settings to the HTTP output configuration:
 +
- * `cacert`: Configures the Logstash client to trust any certificates signed by the specified CA.
- * `client_key`: Specifies the key the Logstash client uses to authenticate with the Logstash server.
- * `client_cert`: Specifies the certificate that the Logstash client uses to authenticate to the Logstash server.
+ * `cacert`: Configures the {ls} client to trust any certificates signed by the specified CA.
+ * `client_key`: Specifies the key the {ls} client uses to authenticate with the {ls} server.
+ * `client_cert`: Specifies the certificate that the {ls} client uses to authenticate to the {ls} server.
 +
 For example:
 +
@@ -118,12 +118,12 @@ output {
 }
 ----
 
-. If you would like an additional authentication step, you can also use basic user/password authentication in both Logstash instances:
+. If you would like an additional authentication step, you can also use basic user/password authentication in both {ls} instances:
 +
  * `user`: Sets the username to use for authentication.
  * `password`: Sets the password to use for authentication.
 +
-For example, you would need to add the following to both Logstash instances:
+For example, you would need to add the following to both {ls} instances:
 +
 [source,json]
 ----
diff --git a/docs/static/ls-ls-lumberjack.asciidoc b/docs/static/ls-ls-lumberjack.asciidoc
index 11e8c4fd1b7..cf95f9f0a0f 100644
--- a/docs/static/ls-ls-lumberjack.asciidoc
+++ b/docs/static/ls-ls-lumberjack.asciidoc
@@ -1,21 +1,21 @@
 [[ls-to-ls-lumberjack]]
-=== Logstash-to-Logstash: Lumberjack output to Beats input
+=== {ls}-to-{ls}: Lumberjack output to Beats input
 
-You can set up communication between two Logstash machines by connecting the Lumberjack output to the Beats input. 
+You can set up communication between two {ls} machines by connecting the Lumberjack output to the Beats input. 
 
-Logstash-to-Logstash using Lumberjack and Beats has been our standard approach for {ls}-to-{ls}, and may still be the best option for more robust use cases. 
+{ls}-to-{ls} using Lumberjack and Beats has been our standard approach for {ls}-to-{ls}, and may still be the best option for more robust use cases. 
 
-NOTE: Check out these <<lumberjack-considerations,considerations>> before you implement Logstash-to-Logstash using Lumberjack and Beats. 
+NOTE: Check out these <<lumberjack-considerations,considerations>> before you implement {ls}-to-{ls} using Lumberjack and Beats. 
 
 ==== Configuration overview
 
-Use the Lumberjack protocol to connect two Logstash machines.
+Use the Lumberjack protocol to connect two {ls} machines.
 
 . Generate a trusted SSL certificate (required by the lumberjack protocol).
-. Copy the SSL certificate to the upstream Logstash machine.
-. Copy the SSL certificate and key to the downstream Logstash machine.
-. Set the upstream Logstash machine to use the Lumberjack output to send data.
-. Set the downstream Logstash machine to listen for incoming Lumberjack connections through the Beats input.
+. Copy the SSL certificate to the upstream {ls} machine.
+. Copy the SSL certificate and key to the downstream {ls} machine.
+. Set the upstream {ls} machine to use the Lumberjack output to send data.
+. Set the downstream {ls} machine to listen for incoming Lumberjack connections through the Beats input.
 . Test it.
 
 [[generate-self-signed-cert]]
@@ -34,7 +34,7 @@ where:
 
 * `lumberjack.key` is the name of the SSL key to be created
 * `lumberjack.cert` is the name of the SSL certificate to be created
-* `localhost` is the name of the upstream Logstash computer
+* `localhost` is the name of the upstream {ls} computer
 
 
 This command produces output similar to the following:
@@ -50,14 +50,14 @@ writing new private key to 'lumberjack.key'
 [[copy-cert-key]]
 ===== Copy the SSL certificate and key
 
-Copy the SSL certificate to the upstream Logstash machine.
+Copy the SSL certificate to the upstream {ls} machine.
 
-Copy the SSL certificate and key to the downstream Logstash machine.
+Copy the SSL certificate and key to the downstream {ls} machine.
 
 [[save-cert-ls1]]
-===== Start the upstream Logstash instance
+===== Start the upstream {ls} instance
 
-Start Logstash and generate test events:
+Start {ls} and generate test events:
 
 [source,shell]
 ----
@@ -67,9 +67,9 @@ bin/logstash -e 'input { generator { count => 5 } } output { lumberjack { codec
 This sample command sends five events to mydownstreamhost:5000 using the SSL certificate provided.
 
 [[save-cert-ls2]]
-===== Start the downstream Logstash instance
+===== Start the downstream {ls} instance
 
-Start the downstream instance of Logstash:
+Start the downstream instance of {ls}:
 
 [source,shell]
 ----
@@ -81,7 +81,7 @@ This sample command sets port 5000 to listen for incoming Beats input.
 [[test-ls-to-ls]]
 ===== Verify the communication
 
-Watch the downstream Logstash machine for the incoming events. You should see five incrementing events similar to the following:
+Watch the downstream {ls} machine for the incoming events. You should see five incrementing events similar to the following:
 
 [source,shell]
 ----
diff --git a/docs/static/maintainer-guide.asciidoc b/docs/static/maintainer-guide.asciidoc
index edefdb81c3c..c71e2537db5 100644
--- a/docs/static/maintainer-guide.asciidoc
+++ b/docs/static/maintainer-guide.asciidoc
@@ -1,5 +1,5 @@
 [[community-maintainer]]
-=== Logstash Plugins Community Maintainer Guide
+=== {ls} Plugins Community Maintainer Guide
 
 This document, to be read by new Maintainers, should explain their responsibilities.  It was inspired by the
 http://rfc.zeromq.org/spec:22[C4] document from the ZeroMQ project.  This document is subject to change and suggestions
@@ -8,13 +8,13 @@ through Pull Requests and issues are strongly encouraged.
 [float]
 === Contribution Guidelines
 
-For general guidance around contributing to Logstash Plugins, see the
-https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html[_Contributing to Logstash_] section.
+For general guidance around contributing to {ls} Plugins, see the
+https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html[_Contributing to {ls}_] section.
 
 [float]
 === Document Goals
 
-To help make the Logstash plugins community participation easy with positive feedback.
+To help make the {ls} plugins community participation easy with positive feedback.
 
 To increase diversity.
 
@@ -135,22 +135,22 @@ Each version identifier should be a level-2 header using `##`
 [float]
 ==== Continuous Integration
 
-Plugins are setup with automated continuous integration (CI) environments and there should be a corresponding badge on each Github page.  If it’s missing, please contact the Logstash core team.
+Plugins are setup with automated continuous integration (CI) environments and there should be a corresponding badge on each Github page.  If it’s missing, please contact the {ls} core team.
 
 Every Pull Request opened automatically triggers a CI run.  To conduct a manual run, comment “Jenkins, please test this.” on the Pull Request.
 
 [float]
 === Versioning Plugins
 
-Logstash core and its plugins have separate product development lifecycles. Hence the versioning and release strategy for
+{ls} core and its plugins have separate product development lifecycles. Hence the versioning and release strategy for
 the core and plugins do not have to be aligned. In fact, this was one of our goals during the great separation of plugins
-work in Logstash 1.5.
+work in {ls} 1.5.
 
-At times, there will be changes in core API in Logstash, which will require mass update of plugins to reflect the changes
+At times, there will be changes in core API in {ls}, which will require mass update of plugins to reflect the changes
 in core. However, this does not happen frequently.
 
 For plugins, we would like to adhere to a versioning and release strategy that can better inform our users, about any
-breaking changes to the Logstash configuration formats and functionality.
+breaking changes to the {ls} configuration formats and functionality.
 
 Plugin releases follows a three-placed numbering scheme X.Y.Z. where X denotes a major release version which may break
 compatibility with existing configuration or functionality. Y denotes releases which includes features which are backward
@@ -186,11 +186,11 @@ details.
 === Logging
 
 Although it’s important not to bog down performance with excessive logging, debug level logs can be immensely helpful when
-diagnosing and troubleshooting issues with Logstash.  Please remember to liberally add debug logs wherever it makes sense
+diagnosing and troubleshooting issues with {ls}.  Please remember to liberally add debug logs wherever it makes sense
 as users will be forever gracious.
 
 [source,shell]
-@logger.debug("Logstash loves debug logs!", :actions => actions)
+@logger.debug("{ls} loves debug logs!", :actions => actions)
 
 [float]
 === Contributor License Agreement (CLA) Guidance
@@ -209,7 +209,7 @@ Please make sure the CLA is signed by every Contributor prior to reviewing PRs a
 [float]
 === Need Help?
 
-Ping @logstash-core on Github to get the attention of the Logstash core team.
+Ping @logstash-core on Github to get the attention of the {ls} core team.
 
 [float]
 === Community Administration
diff --git a/docs/static/management/centralized-pipelines.asciidoc b/docs/static/management/centralized-pipelines.asciidoc
index 8e6f29b9d96..c67c10daf15 100644
--- a/docs/static/management/centralized-pipelines.asciidoc
+++ b/docs/static/management/centralized-pipelines.asciidoc
@@ -2,7 +2,7 @@
 === Centralized Pipeline Management
 
 The pipeline management feature centralizes the creation and
-management of Logstash configuration pipelines in {kib}. 
+management of {ls} configuration pipelines in {kib}. 
 
 NOTE: Centralized pipeline management is a subscription feature.
 If you want to try the full set of features, you can activate a free 30-day trial. 
@@ -10,9 +10,9 @@ To view the status of your license, start a trial, or install a new
 license, open the {kib} main menu and click *Stack Management > License Management*.
 For more information, see https://www.elastic.co/subscriptions and {kibana-ref}/managing-licenses.html[License Management].
 
-You can control multiple Logstash instances from the pipeline management UI in
-{kib}. You can add, edit, and delete pipeline configurations. On the Logstash
-side, you simply need to enable configuration management and register Logstash
+You can control multiple {ls} instances from the pipeline management UI in
+{kib}. You can add, edit, and delete pipeline configurations. On the {ls}
+side, you simply need to enable configuration management and register {ls}
 to use the centrally managed pipeline configurations.
 
 IMPORTANT: After you configure {ls} to use centralized pipeline management, you can
@@ -27,14 +27,14 @@ Before using the pipeline management UI, you must:
 * <<configuring-centralized-pipelines, Configure centralized pipeline management>>.
 * If {kib} is protected with basic authentication, make sure your {kib} user has
 the `logstash_admin` role as well as the `logstash_writer` role that you created
-when you <<ls-security,configured Logstash to use basic authentication>>. Additionally,
+when you <<ls-security,configured {ls} to use basic authentication>>. Additionally,
 in order to view (as read-only) non-centrally-managed pipelines in the pipeline management
 UI, make sure your {kib} user has the `monitoring_user` role as well.
 
-To manage Logstash pipelines in {kib}:
+To manage {ls} pipelines in {kib}:
 
 . Open {kib} in your browser and go to the Management tab. If you've set up
-configuration management correctly, you'll see an area for managing Logstash.
+configuration management correctly, you'll see an area for managing {ls}.
 +
 image::static/management/images/centralized_config.png[]
 
@@ -86,26 +86,26 @@ persistent queues are enabled.
 
 * The pipeline configurations and metadata are stored in Elasticsearch. Any
 changes that you make to a pipeline definition are picked up and loaded
-automatically by all Logstash instances registered to use the pipeline. The
-changes are applied immediately. If Logstash is registered to use the pipeline,
-you do not have to restart Logstash to pick up the changes.
+automatically by all {ls} instances registered to use the pipeline. The
+changes are applied immediately. If {ls} is registered to use the pipeline,
+you do not have to restart {ls} to pick up the changes.
 
-* The pipeline runs on all Logstash instances that are registered to use the
-pipeline.  {kib} saves the new configuration, and Logstash will attempt to load
+* The pipeline runs on all {ls} instances that are registered to use the
+pipeline.  {kib} saves the new configuration, and {ls} will attempt to load
 it. There is no validation done at the UI level.
 
-* You need to check the local Logstash logs for configuration errors. If you're
-using the Logstash monitoring feature in {kib}, use the Monitoring tab to
-check the status of your Logstash nodes.
+* You need to check the local {ls} logs for configuration errors. If you're
+using the {ls} monitoring feature in {kib}, use the Monitoring tab to
+check the status of your {ls} nodes.
 
 * You can specify multiple pipeline configurations that run in parallel on the
-same Logstash node.
+same {ls} node.
 
-* If you edit and save a pipeline configuration, Logstash reloads
+* If you edit and save a pipeline configuration, {ls} reloads
 the configuration in the background and continues processing events.
 
-* If you try to delete a pipeline that is running (for example, `apache`) in {kib}, Logstash will
-attempt to stop the pipeline. Logstash waits until all
+* If you try to delete a pipeline that is running (for example, `apache`) in {kib}, {ls} will
+attempt to stop the pipeline. {ls} waits until all
 events have been fully processed by the pipeline. Before you delete a pipeline,
 make sure you understand your data sources. Stopping a pipeline may
 lead to data loss.
diff --git a/docs/static/management/configuring-centralized-pipelines.asciidoc b/docs/static/management/configuring-centralized-pipelines.asciidoc
index d8b6f20a47f..5928fefc317 100644
--- a/docs/static/management/configuring-centralized-pipelines.asciidoc
+++ b/docs/static/management/configuring-centralized-pipelines.asciidoc
@@ -20,11 +20,11 @@ minimum, set:
 * `xpack.management.enabled: true` to enable centralized configuration
 management.
 * `xpack.management.elasticsearch.hosts` to specify the Elasticsearch
-instance that will store the Logstash pipeline configurations and metadata.
+instance that will store the {ls} pipeline configurations and metadata.
 * `xpack.management.pipeline.id` to register the pipelines that you want to
 centrally manage.
 
-. Restart Logstash.
+. Restart {ls}.
 
 . If your Elasticsearch cluster is protected with basic authentication, assign
 the built-in `logstash_admin` role as well as the `logstash_writer` role to any users who will use centralized pipeline
@@ -33,7 +33,7 @@ management. See <<ls-security>> for more information.
 NOTE: Centralized management is disabled until you configure and enable
 {security-features}.
 
-IMPORTANT: After you've configured Logstash to use centralized pipeline
+IMPORTANT: After you've configured {ls} to use centralized pipeline
 management, you can no longer specify local pipeline configurations. This means
 that the `pipelines.yml` file and settings like `path.config` and
 `config.string` are inactive when this feature is enabled.
diff --git a/docs/static/managing-multiline-events.asciidoc b/docs/static/managing-multiline-events.asciidoc
index 0e98f8c0cdc..748d38133bf 100644
--- a/docs/static/managing-multiline-events.asciidoc
+++ b/docs/static/managing-multiline-events.asciidoc
@@ -2,20 +2,20 @@
 === Managing Multiline Events
 
 Several use cases generate events that span multiple lines of text. In order to correctly handle these multiline events,
-Logstash needs to know how to tell which lines are part of a single event.
+{ls} needs to know how to tell which lines are part of a single event.
 
 Multiline event processing is complex and relies on proper event ordering. The best way to guarantee ordered log
 processing is to implement the processing as early in the pipeline as possible.
 
 The <<plugins-codecs-multiline>> codec is the preferred tool for handling multiline events
-in the Logstash pipeline. The multiline codec merges lines from a single input using
+in the {ls} pipeline. The multiline codec merges lines from a single input using
 a simple set of rules.
 
-IMPORTANT: If you are using a Logstash input plugin that supports multiple hosts, such as
+IMPORTANT: If you are using a {ls} input plugin that supports multiple hosts, such as
 the <<plugins-inputs-beats>> input plugin, you should not use the
 <<plugins-codecs-multiline>> codec to handle multiline events. Doing so may result in the
 mixing of streams and corrupted event data. In this situation, you need to handle multiline
-events before sending the event data to Logstash.
+events before sending the event data to {ls}.
 
 The most important aspects of configuring the multiline codec are the following:
 
@@ -49,7 +49,7 @@ Exception in thread "main" java.lang.NullPointerException
         at com.example.myproject.Author.getBookTitles(Author.java:25)
         at com.example.myproject.Bootstrap.main(Bootstrap.java:14)
 
-To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:
+To consolidate these lines into a single event in {ls}, use the following configuration for the multiline codec:
 
 [source,json]
 input {
@@ -72,7 +72,7 @@ example:
 printf ("%10.10ld  \t %10.10ld \t %s\
   %f", w, x, y, z );
 
-To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:
+To consolidate these lines into a single event in {ls}, use the following configuration for the multiline codec:
 
 [source,json]
 input {
@@ -95,7 +95,7 @@ specific activity, as in this example:
 [2015-08-24 11:49:14,389][INFO ][env                      ] [Letha] using [1] data paths, mounts [[/
 (/dev/disk1)]], net usable_space [34.5gb], net total_space [118.9gb], types [hfs]
 
-To consolidate these lines into a single event in Logstash, use the following configuration for the multiline codec:
+To consolidate these lines into a single event in {ls}, use the following configuration for the multiline codec:
 
 [source,json]
 input {
diff --git a/docs/static/mem-queue.asciidoc b/docs/static/mem-queue.asciidoc
index a6612e40798..990be6dc08d 100644
--- a/docs/static/mem-queue.asciidoc
+++ b/docs/static/mem-queue.asciidoc
@@ -1,9 +1,9 @@
 [[memory-queue]]
 === Memory queue 
 
-By default, Logstash uses in-memory bounded queues between pipeline stages (inputs → pipeline workers) to buffer events. 
-If Logstash experiences a temporary machine failure, the contents of the memory queue will be lost. 
-Temporary machine failures are scenarios where Logstash or its host machine are terminated abnormally, but are capable of being restarted. 
+By default, {ls} uses in-memory bounded queues between pipeline stages (inputs → pipeline workers) to buffer events. 
+If {ls} experiences a temporary machine failure, the contents of the memory queue will be lost. 
+Temporary machine failures are scenarios where {ls} or its host machine are terminated abnormally, but are capable of being restarted. 
 
 [[mem-queue-benefits]]
 ==== Benefits of memory queues
@@ -26,7 +26,7 @@ TIP: Consider using <<persistent-queues,persistent queues>> to avoid these limit
 ==== Memory queue size
 
 Memory queue size is not configured directly.
-Instead, it depends on how you have Logstash tuned. 
+Instead, it depends on how you have {ls} tuned. 
 
 Its upper bound is defined by `pipeline.workers` (default: number of CPUs) times the `pipeline.batch.size` (default: 125) events.
 This value, called the "inflight count," determines maximum number of events that can be held in each memory queue.
@@ -57,9 +57,9 @@ This value defaults to the number of the host's CPU cores.
 [[backpressure-mem-queue]]
 ==== Back pressure
 
-When the queue is full, Logstash puts back pressure on the inputs to stall data
-flowing into Logstash. 
-This mechanism helps Logstash control the rate of data flow at the input stage
+When the queue is full, {ls} puts back pressure on the inputs to stall data
+flowing into {ls}. 
+This mechanism helps {ls} control the rate of data flow at the input stage
 without overwhelming outputs like Elasticsearch.
 
 Each input handles back pressure independently. 
diff --git a/docs/static/modules.asciidoc b/docs/static/modules.asciidoc
index b47e24074df..6d87ed8f517 100644
--- a/docs/static/modules.asciidoc
+++ b/docs/static/modules.asciidoc
@@ -1,7 +1,7 @@
 [[logstash-modules]]
-== Working with Logstash Modules
+== Working with {ls} Modules
 
-Logstash modules provide a quick, end-to-end solution for ingesting data and
+{ls} modules provide a quick, end-to-end solution for ingesting data and
 visualizing it with purpose-built dashboards.
 
 These modules are available:
@@ -10,7 +10,7 @@ These modules are available:
 * <<netflow-module,Netflow Module (deprecated)>>
 * <<azure-module, Microsoft Azure Module (deprecated)>>
 
-Each module comes pre-packaged with Logstash configurations, Kibana dashboards,
+Each module comes pre-packaged with {ls} configurations, Kibana dashboards,
 and other meta files that make it easier for you to set up the Elastic Stack for
 specific use cases or data sources.
 
@@ -22,10 +22,10 @@ easier for you to get started. When you run a module, it will:
 . Set up the Kibana dashboards, including the index pattern, searches, and
 visualizations required to visualize your data in Kibana.
 
-. Run the Logstash pipeline with the configurations required to read and parse
+. Run the {ls} pipeline with the configurations required to read and parse
 the data.
 
-image::static/images/logstash-module-overview.png[Logstash modules overview]
+image::static/images/logstash-module-overview.png[{ls} modules overview]
 
 [discrete]
 [[running-logstash-modules]]
@@ -43,7 +43,7 @@ bin/logstash --modules MODULE_NAME --setup [-M "CONFIG_SETTING=VALUE"]
 
 Where:
 
-* `--modules` runs the Logstash module specified by `MODULE_NAME`.
+* `--modules` runs the {ls} module specified by `MODULE_NAME`.
 
 * `-M "CONFIG_SETTING=VALUE"` is optional and overrides the specified
 configuration setting. You can specify multiple overrides. Each override must
@@ -135,7 +135,7 @@ https://support.apple.com/en-us/HT202491[Safely open apps on your Mac].
 ====
 
 You can override module settings by specifying one or more configuration
-overrides when you start Logstash. To specify an override, you use the `-M`
+overrides when you start {ls}. To specify an override, you use the `-M`
 command line option:
 
 [source,shell]
@@ -156,7 +156,7 @@ bin/logstash --modules netflow -M "netflow.var.input.udp.port=3555" -M "netflow.
 ----
 
 Any settings defined in the command line are ephemeral and will not persist across
-subsequent runs of Logstash. If you want to persist a configuration, you need to
+subsequent runs of {ls}. If you want to persist a configuration, you need to
 set it in the `logstash.yml` <<logstash-settings-file,settings file>>.
 
 Settings that you specify at the command line are merged with any settings 
diff --git a/docs/static/monitoring/collectors-legacy.asciidoc b/docs/static/monitoring/collectors-legacy.asciidoc
index 1feb8baa4cc..0fcc8081811 100644
--- a/docs/static/monitoring/collectors-legacy.asciidoc
+++ b/docs/static/monitoring/collectors-legacy.asciidoc
@@ -3,12 +3,12 @@
 [[logstash-monitoring-collectors-legacy]]
 ===== Collectors
 
-Collectors, as their name implies, collect things. In monitoring for Logstash, 
-collectors are just <<pipeline,Inputs>> in the same way that ordinary Logstash 
+Collectors, as their name implies, collect things. In monitoring for {ls}, 
+collectors are just <<pipeline,Inputs>> in the same way that ordinary {ls} 
 configurations provide inputs.
 
 Like monitoring for {es}, each collector can create zero or more monitoring 
-documents. As it is currently implemented, each Logstash node runs two types of 
+documents. As it is currently implemented, each {ls} node runs two types of 
 collectors: one for node stats and one for pipeline stats.
 
 [options="header"]
@@ -18,13 +18,13 @@ collectors: one for node stats and one for pipeline stats.
 | Gathers details about the running node, such as memory utilization and CPU
 usage (for example, `GET /_stats`).
 
-This runs on every Logstash node with monitoring enabled. One common
-failure is that Logstash directories are copied with their `path.data` directory
-included (`./data` by default), which copies the persistent UUID of the Logstash
-node along with it. As a result, it generally appears that one or more Logstash
+This runs on every {ls} node with monitoring enabled. One common
+failure is that {ls} directories are copied with their `path.data` directory
+included (`./data` by default), which copies the persistent UUID of the {ls}
+node along with it. As a result, it generally appears that one or more {ls}
 nodes are failing to collect monitoring data, when in fact they are all really
-misreporting as the _same_ Logstash node. Re-use `path.data` directories only 
-when upgrading Logstash, such that upgraded nodes replace the previous versions.
+misreporting as the _same_ {ls} node. Re-use `path.data` directories only 
+when upgrading {ls}, such that upgraded nodes replace the previous versions.
 | Pipeline Stats | `logstash_state`
 | Gathers details about the node's running pipelines, which powers the 
 Monitoring Pipeline UI.
@@ -32,15 +32,15 @@ Monitoring Pipeline UI.
 
 Per collection interval, which defaults to 10 seconds (`10s`), each collector is
 run. The failure of an individual collector does not impact any other collector. 
-Each collector, as an ordinary Logstash input, creates a separate Logstash event 
-in its isolated monitoring pipeline. The Logstash output then sends the data.
+Each collector, as an ordinary {ls} input, creates a separate {ls} event 
+in its isolated monitoring pipeline. The {ls} output then sends the data.
 
 The collection interval can be configured dynamically and you can also disable 
 data collection. For more information about the configuration options for the 
 collectors, see <<monitoring-settings-legacy>>.
 
 WARNING: Unlike {es} and {kib} monitoring, there is no 
-`xpack.monitoring.collection.enabled` setting on Logstash. You must use the 
+`xpack.monitoring.collection.enabled` setting on {ls}. You must use the 
 `xpack.monitoring.enabled` setting to enable and disable data collection. 
 
 If gaps exist in the monitoring charts in {kib}, it is typically because either 
diff --git a/docs/static/monitoring/monitoring-apis.asciidoc b/docs/static/monitoring/monitoring-apis.asciidoc
index 0f7b7cf0049..f594fa4f711 100644
--- a/docs/static/monitoring/monitoring-apis.asciidoc
+++ b/docs/static/monitoring/monitoring-apis.asciidoc
@@ -11,7 +11,7 @@ about {ls}:
 * <<hot-threads-api>>
 
 
-You can use the root resource to retrieve general information about the Logstash instance, including
+You can use the root resource to retrieve general information about the {ls} instance, including
 the host and version.
 
 [source,js]
@@ -30,17 +30,17 @@ Example response:
 }
 --------------------------------------------------
 
-NOTE: By default, the monitoring API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
-instance, you need to launch Logstash with the `--api.http.port` flag specified to bind to a different port. See
+NOTE: By default, the monitoring API attempts to bind to `tcp:9600`. If this port is already in use by another {ls}
+instance, you need to launch {ls} with the `--api.http.port` flag specified to bind to a different port. See
 <<command-line-flags>> for more information.
 
 [discrete]
 [[monitoring-api-security]]
-==== Securing the Logstash API
+==== Securing the {ls} API
 
 The {ls} Monitoring APIs are not secured by default, but you can configure {ls} to secure them in one of several ways to meet your organization's needs.
 
-You can enable SSL for the Logstash API by setting `api.ssl.enabled: true` in the `logstash.yml`, and providing the relevant keystore settings `api.ssl.keystore.path` and `api.ssl.keystore.password`:
+You can enable SSL for the {ls} API by setting `api.ssl.enabled: true` in the `logstash.yml`, and providing the relevant keystore settings `api.ssl.keystore.path` and `api.ssl.keystore.password`:
 
 [source]
 --------------------------------------------------
@@ -50,7 +50,7 @@ api.ssl.keystore.password: "s3cUr3p4$$w0rd"
 --------------------------------------------------
 
 The keystore must be in either jks or p12 format, and must contain both a certificate and a private key.
-Connecting clients receive this certificate, allowing them to authenticate the Logstash endpoint.
+Connecting clients receive this certificate, allowing them to authenticate the {ls} endpoint.
 
 You can also require HTTP Basic authentication by setting `api.auth.type: basic` in the `logstash.yml`, and providing the relevant credentials `api.auth.basic.username` and `api.auth.basic.password`:
 
@@ -68,7 +68,7 @@ NOTE: Usage of Keystore or Environment or variable replacements is encouraged fo
 [[monitoring-common-options]]
 ==== Common options
 
-The following options can be applied to all of the Logstash monitoring APIs.
+The following options can be applied to all of the {ls} monitoring APIs.
 
 [discrete]
 ===== Pretty results
@@ -79,7 +79,7 @@ will be pretty formatted (use it for debugging only!).
 [discrete]
 ===== Human-readable output
 
-NOTE: For Logstash {logstash_version}, the `human` option is supported for the <<hot-threads-api>>
+NOTE: For {ls} {logstash_version}, the `human` option is supported for the <<hot-threads-api>>
 only. When you specify `human=true`, the results are returned in plain text instead of
 JSON format. The default is false.
 
@@ -116,7 +116,7 @@ Gets node-level info about the OS.
 Gets node-level JVM info, including info about threads.
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
+{ls} monitoring APIs.
 
 [discrete]
 [[node-pipeline-info]]
@@ -252,7 +252,7 @@ Example response:
 [[plugins-api]]
 === Plugins info API
 
-The plugins info API gets information about all Logstash plugins that are currently installed.
+The plugins info API gets information about all {ls} plugins that are currently installed.
 This API basically returns the output of running the `bin/logstash-plugin list --verbose` command.
 
 [source,js]
@@ -261,7 +261,7 @@ curl -XGET 'localhost:9600/_node/plugins?pretty'
 --------------------------------------------------
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
+{ls} monitoring APIs.
 
 The output is a JSON document.
 
@@ -298,7 +298,7 @@ Example response:
 [[node-stats-api]]
 === Node Stats API
 
-The node stats API retrieves runtime stats about Logstash.
+The node stats API retrieves runtime stats about {ls}.
 
 [source,js]
 --------------------------------------------------
@@ -316,22 +316,22 @@ and uptime.
 <<process-stats,`process`>>::
 Gets process stats, including stats about file descriptors, memory consumption, and CPU usage.
 <<event-stats,`events`>>::
-Gets event-related statistics for the Logstash instance (regardless of how many
+Gets event-related statistics for the {ls} instance (regardless of how many
 pipelines were created and destroyed).
 <<flow-stats,`flow`>>::
-Gets flow-related statistics for the Logstash instance (regardless of how many
+Gets flow-related statistics for the {ls} instance (regardless of how many
 pipelines were created and destroyed).
 <<pipeline-stats,`pipelines`>>::
-Gets runtime stats about each Logstash pipeline.
+Gets runtime stats about each {ls} pipeline.
 <<reload-stats,`reloads`>>::
 Gets runtime stats about config reload successes and failures.
 <<os-stats,`os`>>::
-Gets runtime stats about cgroups when Logstash is running in a container.
+Gets runtime stats about cgroups when {ls} is running in a container.
 <<geoip-database-stats,`geoip_download_manager`>>::
 Gets stats for databases used with the <<plugins-filters-geoip, Geoip filter plugin>>.
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
+{ls} monitoring APIs.
 
 [discrete]
 [[jvm-stats]]
@@ -441,7 +441,7 @@ Example response:
 ==== Event stats
 
 The following request returns a JSON document containing event-related statistics
-for the Logstash instance:
+for the {ls} instance:
 
 [source,js]
 --------------------------------------------------
@@ -467,7 +467,7 @@ Example response:
 ==== Flow stats
 
 The following request returns a JSON document containing flow-rates
-for the Logstash instance:
+for the {ls} instance:
 
 [source,js]
 --------------------------------------------------
@@ -507,7 +507,7 @@ Example response:
 NOTE: When the rate for a given flow metric window is infinite, it is presented as a string (either `"Infinity"` or  `"-Infinity"`).
       This occurs when the numerator metric has changed during the window without a change in the rate's denominator metric.
 
-Flow rates provide visibility into how a Logstash instance or an individual pipeline is _currently_ performing relative to _itself_ over time.
+Flow rates provide visibility into how a {ls} instance or an individual pipeline is _currently_ performing relative to _itself_ over time.
 This allows us to attach _meaning_ to the cumulative-value metrics that are also presented by this API, and to determine whether an instance or pipeline is behaving better or worse than it has in the past.
 
 The following flow rates are available for the logstash process as a whole and for each of its pipelines individually.
@@ -1026,7 +1026,7 @@ Example response:
 [[os-stats]]
 ==== OS stats
 
-When Logstash is running in a container, the following request returns a JSON document that
+When {ls} is running in a container, the following request returns a JSON document that
 contains cgroup information to give you a more accurate view of CPU load, including whether
 the container is being throttled.
 
@@ -1077,7 +1077,7 @@ For more info, see <<plugins-filters-geoip-metrics,Database Metrics>> in the Geo
 [[hot-threads-api]]
 === Hot Threads API
 
-The hot threads API gets the current hot threads for Logstash. A hot thread is a
+The hot threads API gets the current hot threads for {ls}. A hot thread is a
 Java thread that has high CPU usage and executes for a longer than normal period
 of time.
 
@@ -1087,7 +1087,7 @@ curl -XGET 'localhost:9600/_node/hot_threads?pretty'
 --------------------------------------------------
 
 The output is a JSON document that contains a breakdown of the top hot threads for
-Logstash.
+{ls}.
 
 Example response:
 
@@ -1127,7 +1127,7 @@ The parameters allowed are:
 `ignore_idle_threads`:: If true, does not return idle threads. The default is true.
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
+{ls} monitoring APIs.
 
 You can use the `?human` parameter to return the document in a human-readable format.
 
diff --git a/docs/static/monitoring/monitoring-ea.asciidoc b/docs/static/monitoring/monitoring-ea.asciidoc
index c8df70c40e2..292f24a4f14 100644
--- a/docs/static/monitoring/monitoring-ea.asciidoc
+++ b/docs/static/monitoring/monitoring-ea.asciidoc
@@ -45,20 +45,20 @@ deploy it to the host where {ls} is running.
 NOTE: If you're using a monitoring cluster, use the {kib} instance connected to
 the monitoring cluster.
 
-. In the query bar, search for and select the **Logstash** integration for
+. In the query bar, search for and select the **{ls}** integration for
 {agent}.
 . Read the overview to make sure you understand integration requirements and
 other considerations.
-. Click **Add Logstash**.
+. Click **Add {ls}**.
 +
 TIP: If you're installing an integration for the first time, you may be prompted
 to install {agent}. Click **Add integration only (skip agent installation)**.
 
 . Configure the integration name and optionally add a description. Make sure you
 configure all required settings:
-* Under **Collect Logstash application and slowlog logs**, modify the log paths
+* Under **Collect {ls} application and slowlog logs**, modify the log paths
 to match your {ls} environment.
-* Under **Collect Logstash node metrics and stats**, make sure the hosts setting
+* Under **Collect {ls} node metrics and stats**, make sure the hosts setting
 points to your {ls} host URLs. By default, the integration collects {ls}
 monitoring metrics from `localhost:9600`. If that host and port number are not
 correct, update the `hosts` setting. If you configured {ls} to use encrypted
diff --git a/docs/static/monitoring/monitoring-internal-legacy.asciidoc b/docs/static/monitoring/monitoring-internal-legacy.asciidoc
index af2a5e7236c..92cea29b9c1 100644
--- a/docs/static/monitoring/monitoring-internal-legacy.asciidoc
+++ b/docs/static/monitoring/monitoring-internal-legacy.asciidoc
@@ -14,21 +14,21 @@ Monitoring {ls} with legacy collection uses these components:
 * <<logstash-monitoring-collectors-legacy,Collectors>>
 * <<logstash-monitoring-output-legacy,Output>>
 
-These pieces live outside of the default Logstash pipeline in a dedicated monitoring
+These pieces live outside of the default {ls} pipeline in a dedicated monitoring
 pipeline. This configuration ensures that all data and processing has a minimal
-impact on ordinary Logstash processing. Existing Logstash features, such as the
+impact on ordinary {ls} processing. Existing {ls} features, such as the
 <<plugins-outputs-elasticsearch,`elasticsearch` output>>, can be reused to
 benefit from its retry policies. 
 
 NOTE: The `elasticsearch` output that is used for monitoring {ls} is 
 configured exclusively through settings found in `logstash.yml`. It is not 
-configured by using anything from the Logstash configurations that might also be 
+configured by using anything from the {ls} configurations that might also be 
 using their own separate `elasticsearch` outputs.
 
 
 The production {es} cluster should be configured to receive {ls} monitoring
 data. This configuration enables the production {es} cluster to add metadata
-(for example, its cluster UUID) to the Logstash monitoring data and then route
+(for example, its cluster UUID) to the {ls} monitoring data and then route
 it to the monitoring clusters. For more information  about typical monitoring
 architectures, see  {ref}/how-monitoring-works.html[How monitoring works] in the
 {ref}[Elasticsearch Reference]. 
@@ -44,16 +44,16 @@ include::monitoring-output-legacy.asciidoc[]
 <titleabbrev>Configure legacy collection</titleabbrev>
 ++++
 
-To monitor Logstash nodes:
+To monitor {ls} nodes:
 
 . Specify where to send monitoring data. This cluster is often referred to as
 the _production cluster_. For examples of typical monitoring architectures, see
 {ref}/how-monitoring-works.html[How monitoring works].
 +
 --
-IMPORTANT: To visualize Logstash as part of the Elastic Stack (as shown in Step
+IMPORTANT: To visualize {ls} as part of the Elastic Stack (as shown in Step
 6), send metrics to your _production_ cluster. Sending metrics to a dedicated
-monitoring cluster will show the Logstash metrics under the _monitoring_ cluster.
+monitoring cluster will show the {ls} metrics under the _monitoring_ cluster.
 
 --
 
@@ -61,7 +61,7 @@ monitoring cluster will show the Logstash metrics under the _monitoring_ cluster
 production cluster. If that setting is `false`, the collection of monitoring data
 is disabled in {es} and data is ignored from all other sources.
 
-. Configure your Logstash nodes to send metrics by setting
+. Configure your {ls} nodes to send metrics by setting
 `xpack.monitoring.enabled` to `true` and specifying the destination {es} node(s)
 as `xpack.monitoring.elasticsearch.hosts` in `logstash.yml`. 
 If {security-features} are enabled, you also need to specify the credentials for
@@ -80,7 +80,7 @@ xpack.monitoring.elasticsearch.password: "changeme"
 <1> If SSL/TLS is enabled on the production cluster, you must
 connect through HTTPS. As of v5.2.1, you can specify multiple
 Elasticsearch hosts as an array as well as specifying a single
-host as a string. If multiple URLs are specified, Logstash
+host as a string. If multiple URLs are specified, {ls}
 can round-robin requests to these production nodes. 
 --
 
@@ -89,7 +89,7 @@ CA certificates that will be used to verify the identity of the nodes
 in the cluster.
 +
 --
-To add a CA certificate to a Logstash node's trusted certificates, you
+To add a CA certificate to a {ls} node's trusted certificates, you
 can specify the location of the PEM encoded certificate with the
 `certificate_authority` setting:
 
@@ -152,11 +152,11 @@ xpack.monitoring.elasticsearch.sniffing: false
 
 --
 
-. Restart your Logstash nodes.
+. Restart your {ls} nodes.
 
 . To verify your monitoring configuration, point your web browser at your {kib}
 host, and select **Monitoring** from the side navigation. Metrics reported from
-your Logstash nodes should be visible in the Logstash section. When security is
+your {ls} nodes should be visible in the {ls} section. When security is
 enabled, to view the monitoring dashboards you must log in to {kib} as a user
 who has the `kibana_user` and `monitoring_user` roles.
 +
diff --git a/docs/static/monitoring/monitoring-output-legacy.asciidoc b/docs/static/monitoring/monitoring-output-legacy.asciidoc
index 349308f62ed..5189b41cf28 100644
--- a/docs/static/monitoring/monitoring-output-legacy.asciidoc
+++ b/docs/static/monitoring/monitoring-output-legacy.asciidoc
@@ -3,9 +3,9 @@
 [[logstash-monitoring-output-legacy]]
 ==== Output
 
-Like all Logstash pipelines, the purpose of the dedicated monitoring pipeline is 
-to send events to outputs. In the case of monitoring for Logstash, the output 
-is always an `elasticsearch` output. However, unlike ordinary Logstash pipelines, 
+Like all {ls} pipelines, the purpose of the dedicated monitoring pipeline is 
+to send events to outputs. In the case of monitoring for {ls}, the output 
+is always an `elasticsearch` output. However, unlike ordinary {ls} pipelines, 
 the output is configured within the `logstash.yml` settings file via the
 `xpack.monitoring.elasticsearch.*` settings.
 
@@ -13,14 +13,14 @@ Other than its unique manner of configuration, this `elasticsearch` output
 behaves like all `elasticsearch` outputs, including its ability to pause data 
 collection when issues exist with the output.
 
-IMPORTANT: It is critical that all Logstash nodes share the same setup. 
+IMPORTANT: It is critical that all {ls} nodes share the same setup. 
 Otherwise, monitoring data might be routed in different ways or to different places.
 
 [float]
 [[logstash-monitoring-default-legacy]]
 ===== Default Configuration
 
-If a Logstash node does not explicitly define a monitoring output setting, 
+If a {ls} node does not explicitly define a monitoring output setting, 
 the following default configuration is used:
 
 [source,yaml]
@@ -28,12 +28,12 @@ the following default configuration is used:
 xpack.monitoring.elasticsearch.hosts: [ "http://localhost:9200" ]
 ---------------------------------------------------
 
-All data produced by monitoring for Logstash is indexed in the monitoring 
+All data produced by monitoring for {ls} is indexed in the monitoring 
 cluster by using the `.monitoring-logstash` template, which is managed by the
 {ref}/es-monitoring-exporters.html[exporters] within {es}. 
 
 If you are working with a cluster that has {security} enabled, extra steps are 
-necessary to properly configure Logstash. For more information, see 
+necessary to properly configure {ls}. For more information, see 
 <<configuring-logstash>>. 
 
 IMPORTANT: When discussing security relative to the `elasticsearch` output, it
diff --git a/docs/static/monitoring/monitoring-ui.asciidoc b/docs/static/monitoring/monitoring-ui.asciidoc
index cca28de6de6..c9d66e7c4dc 100644
--- a/docs/static/monitoring/monitoring-ui.asciidoc
+++ b/docs/static/monitoring/monitoring-ui.asciidoc
@@ -4,20 +4,20 @@
 
 Use the {stack} {monitor-features} to view metrics and gain insight into how
 your {ls} deployment is running. In the overview dashboard, you can see all
-events received and sent by Logstash, plus info about memory usage and uptime:
+events received and sent by {ls}, plus info about memory usage and uptime:
 
-image::static/monitoring/images/overviewstats.png[Logstash monitoring overview dashboard in Kibana]
+image::static/monitoring/images/overviewstats.png[{ls} monitoring overview dashboard in Kibana]
 
 Then you can drill down to see stats about a specific node:
 
-image::static/monitoring/images/nodestats.png[Logstash monitoring node stats dashboard in Kibana]
+image::static/monitoring/images/nodestats.png[{ls} monitoring node stats dashboard in Kibana]
 
 NOTE: A {ls} node is considered unique based on its persistent UUID, which
 is written to the <<logstash-settings-file,`path.data`>> directory when the node
 starts.
 
 Before you can use the monitoring UI,
-<<configuring-logstash, configure Logstash monitoring>>.
+<<configuring-logstash, configure {ls} monitoring>>.
 
 For information about using the Monitoring UI, see
 {kibana-ref}/xpack-monitoring.html[{monitoring} in {kib}].
diff --git a/docs/static/monitoring/monitoring.asciidoc b/docs/static/monitoring/monitoring.asciidoc
index 11c1a4c669f..dce7fb42a19 100644
--- a/docs/static/monitoring/monitoring.asciidoc
+++ b/docs/static/monitoring/monitoring.asciidoc
@@ -1,18 +1,18 @@
 [[monitoring-logstash]]
-== Monitoring Logstash with APIs
+== Monitoring {ls} with APIs
 
-When you run Logstash, it automatically captures runtime metrics that you can
-use to monitor the health and performance of your Logstash deployment.
+When you run {ls}, it automatically captures runtime metrics that you can
+use to monitor the health and performance of your {ls} deployment.
 
-The metrics collected by Logstash include:
+The metrics collected by {ls} include:
 
-* Logstash node info, like pipeline settings, OS info, and JVM info.
+* {ls} node info, like pipeline settings, OS info, and JVM info.
 * Plugin info, including a list of installed plugins.
 * Node stats, like JVM stats, process stats, event-related stats, and pipeline
 runtime stats.
 * Hot threads.
 
-You can use <<monitoring,monitoring APIs>> provided by Logstash
+You can use <<monitoring,monitoring APIs>> provided by {ls}
 to retrieve these metrics. These APIs are available by default without
 requiring any extra configuration.
 
diff --git a/docs/static/monitoring/pipeline-viewer.asciidoc b/docs/static/monitoring/pipeline-viewer.asciidoc
index 0bade27b45c..f5797fee25e 100644
--- a/docs/static/monitoring/pipeline-viewer.asciidoc
+++ b/docs/static/monitoring/pipeline-viewer.asciidoc
@@ -6,7 +6,7 @@
 The pipeline viewer UI offers additional visibility into the behavior and
 performance of complex pipeline configurations.
 Use the pipeline viewer to visualize and monitor the behavior of complex
-Logstash pipeline configurations. You can see and interact with a tree view
+{ls} pipeline configurations. You can see and interact with a tree view
 that illustrates the pipeline topology, data flow, and branching logic.
 
 The pipeline viewer highlights CPU% and event latency in cases where the values
@@ -21,17 +21,17 @@ image::static/monitoring/images/pipeline-tree.png[Pipeline Viewer]
 
 Before using the pipeline viewer:
 
-* <<monitoring-logstash,Configure Logstash monitoring>>.
-* Start the Logstash pipeline that you want to monitor.
+* <<monitoring-logstash,Configure {ls} monitoring>>.
+* Start the {ls} pipeline that you want to monitor.
 
-Logstash begins shipping metrics to the monitoring cluster.
+{ls} begins shipping metrics to the monitoring cluster.
 
 [float]
 ==== View the pipeline
 
 To view the pipeline:
 
-* Kibana -> Monitoring -> Logstash -> Pipelines
+* Kibana -> Monitoring -> {ls} -> Pipelines
 
 Each pipeline is identified by a pipeline ID (`main` by default). For each
 pipeline, you see the pipeline's throughput and the number
@@ -49,8 +49,8 @@ Click the arrow beside a branch name to collapse or expand it.
 ==== Notes and best practices
 
 *Use semantic IDs.*
-Specify semantic IDs when you configure the stages in your Logstash pipeline.
-Otherwise, Logstash generates them for you. Semantic IDs help you identify
+Specify semantic IDs when you configure the stages in your {ls} pipeline.
+Otherwise, {ls} generates them for you. Semantic IDs help you identify
 configurations that are causing bottlenecks. For example, you may have several
 grok filters running in your pipeline. If you have specified semantic IDs, you
 can tell at a glance which filters are slow. Semantic IDs, such as
@@ -70,7 +70,7 @@ its work.
 
 *Versioning.*
 Version information is available from the dropdown list beside the pipeline ID.
-Logstash generates a new version each time you modify a pipeline, and
+{ls} generates a new version each time you modify a pipeline, and
 stores multiple versions of the pipeline stats. Use this information to see how
-changes over time affect throughput and other metrics. Logstash does not store
+changes over time affect throughput and other metrics. {ls} does not store
 multiple versions of the pipeline configurations.
diff --git a/docs/static/monitoring/pipelines/tweets_about_rain.conf b/docs/static/monitoring/pipelines/tweets_about_rain.conf
index 68f60e523ff..7f64b094f61 100644
--- a/docs/static/monitoring/pipelines/tweets_about_rain.conf
+++ b/docs/static/monitoring/pipelines/tweets_about_rain.conf
@@ -7,7 +7,7 @@
 input {
 
   ## Note: you will have to setup the environment variables used
-  ## below. Refer to the Twitter Logstash Input plugin documentation
+  ## below. Refer to the Twitter {ls} Input plugin documentation
   ## for their expected values
   twitter {
     id => "tweet harvester"
diff --git a/docs/static/monitoring/troubleshooting.asciidoc b/docs/static/monitoring/troubleshooting.asciidoc
index 40cf01a2522..3d7d7135da2 100644
--- a/docs/static/monitoring/troubleshooting.asciidoc
+++ b/docs/static/monitoring/troubleshooting.asciidoc
@@ -1,13 +1,13 @@
 [role="xpack"]
 [[monitoring-troubleshooting]]
-=== Troubleshooting monitoring in Logstash
+=== Troubleshooting monitoring in {ls}
 ++++
 <titleabbrev>Troubleshooting</titleabbrev>
 ++++
 
 
 [float]
-==== Logstash Monitoring Not Working After Upgrade
+==== {ls} Monitoring Not Working After Upgrade
 
 When upgrading from older versions, the built-in `logstash_system` user is
 disabled for security reasons. To resume monitoring:
diff --git a/docs/static/multiple-pipelines.asciidoc b/docs/static/multiple-pipelines.asciidoc
index 59abeb40828..c248b4c1f16 100644
--- a/docs/static/multiple-pipelines.asciidoc
+++ b/docs/static/multiple-pipelines.asciidoc
@@ -1,7 +1,7 @@
 [[multiple-pipelines]]
 === Multiple Pipelines
 
-If you need to run more than one pipeline in the same process, Logstash provides a way to do this through a configuration file called `pipelines.yml`.
+If you need to run more than one pipeline in the same process, {ls} provides a way to do this through a configuration file called `pipelines.yml`.
 This file must be placed in the `path.settings` folder and follows this structure:
 
 [source,yaml]
@@ -17,7 +17,7 @@ This file must be placed in the `path.settings` folder and follows this structur
 This file is formatted in YAML and contains a list of dictionaries, where each dictionary describes a pipeline, and each key/value pair specifies a setting for that pipeline. The example shows two different pipelines described by their IDs and  configuration paths. For the first pipeline, the value of `pipeline.workers` is set to 3, while in the other, the persistent queue feature is enabled.
 The value of a setting that is not explicitly set in the `pipelines.yml` file will fall back to the default specified in the `logstash.yml` <<logstash-settings-file,settings file>>.
 
-When you start Logstash without arguments, it will read the `pipelines.yml` file and instantiate all pipelines specified in the file. On the other hand, when you use `-e` or `-f`, Logstash ignores the `pipelines.yml` file and logs a warning about it.
+When you start {ls} without arguments, it will read the `pipelines.yml` file and instantiate all pipelines specified in the file. On the other hand, when you use `-e` or `-f`, {ls} ignores the `pipelines.yml` file and logs a warning about it.
 
 [[multiple-pipeline-usage]]
 ==== Usage Considerations
diff --git a/docs/static/netflow-module.asciidoc b/docs/static/netflow-module.asciidoc
index b00c4e8f6de..024f0498d0e 100644
--- a/docs/static/netflow-module.asciidoc
+++ b/docs/static/netflow-module.asciidoc
@@ -1,5 +1,5 @@
 [[netflow-module]]
-=== Logstash Netflow Module
+=== {ls} Netflow Module
 
 ++++
 <titleabbrev>Netflow Module (deprecated)</titleabbrev>
@@ -7,12 +7,12 @@
 
 deprecated[7.4.0, Replaced by the {filebeat-ref}/filebeat-module-netflow.html[{Filebeat} Netflow Module] which is compliant with the {ecs-ref}/index.html[Elastic Common Schema (ECS)]]
 
-The Logstash Netflow module simplifies the collection, normalization, and
+The {ls} Netflow module simplifies the collection, normalization, and
 visualization of network flow data. With a single command, the module parses
 network flow data, indexes the events into Elasticsearch, and installs a suite
 of Kibana dashboards to get you exploring your data immediately.
 
-Logstash modules support Netflow Version 5 and 9.
+{ls} modules support Netflow Version 5 and 9.
 
 
 ==== What is Flow Data?
@@ -30,7 +30,7 @@ part of.
 ===== Requirements
 
 These instructions assume you have already installed Elastic Stack
-(Logstash, Elasticsearch, and Kibana) version 5.6 or higher. The products you
+({ls}, Elasticsearch, and Kibana) version 5.6 or higher. The products you
 need are https://www.elastic.co/downloads[available to download] and easy to
 install.
 
@@ -41,18 +41,18 @@ NOTE: The {ls} Netflow Module has been deprecated and replaced by the
 {filebeat-ref}/filebeat-module-netflow.html[{Filebeat} Netflow Module], which is
 compliant with the {ecs-ref}/index.html[Elastic Common Schema (ECS)].
 
-. Start the Logstash Netflow module by running the following command in the
-Logstash installation directory:
+. Start the {ls} Netflow module by running the following command in the
+{ls} installation directory:
 +
 [source,shell]
 -----
 bin/logstash --modules netflow --setup -M netflow.var.input.udp.port=NNNN
 -----
 +
-Where `NNNN` is the UDP port on which Logstash will listen for network traffic
-data. If you don't specify a port, Logstash listens on port 2055 by default.
+Where `NNNN` is the UDP port on which {ls} will listen for network traffic
+data. If you don't specify a port, {ls} listens on port 2055 by default.
 +
-The `--modules netflow` option spins up a Netflow-aware Logstash pipeline
+The `--modules netflow` option spins up a Netflow-aware {ls} pipeline
 for ingestion.
 +
 The `--setup` option creates a `netflow-*` index pattern in Elasticsearch and
@@ -75,7 +75,7 @@ security.
 [[exploring-data-netflow]]
 ==== Exploring Your Data
 
-Once the Logstash Netflow module starts processing events, you can immediately
+Once the {ls} Netflow module starts processing events, you can immediately
 begin using the packaged Kibana dashboards to explore and visualize your
 network flow data.
 
@@ -113,12 +113,12 @@ image::static/images/netflow-geo-location.png[Netflow geo location dashboard]
 [[configuring-netflow]]
 ==== Configuring the Module
 
-You can further refine the behavior of the Logstash Netflow module by specifying
+You can further refine the behavior of the {ls} Netflow module by specifying
 settings in the `logstash.yml` settings file, or overriding settings at the
 command line.
 
 For example, the following configuration in the `logstash.yml` file sets
-Logstash to listen on port 9996 for network traffic data:
+{ls} to listen on port 9996 for network traffic data:
 [source,yaml]
 -----
 modules:
@@ -141,13 +141,13 @@ For more information about configuring modules, see
 
 The Netflow module provides the following settings for configuring the behavior
 of the module. These settings include Netflow-specific options plus common
-options that are supported by all Logstash modules.
+options that are supported by all {ls} modules.
 
 When you override a setting at the command line, remember to prefix the setting
 with the module name, for example,  `netflow.var.input.udp.port` instead of
 `var.input.udp.port`.
 
-If you don't specify configuration settings, Logstash uses the defaults.
+If you don't specify configuration settings, {ls} uses the defaults.
 
 *Netflow Options*
 
@@ -158,7 +158,7 @@ If you don't specify configuration settings, Logstash uses the defaults.
 * Default value is 2055.
 --
 +
-Sets the UDP port on which Logstash listens for network traffic data. Although
+Sets the UDP port on which {ls} listens for network traffic data. Although
 2055 is the default for this setting, some devices use ports in the range of
 9995 through 9998, with 9996 being the most commonly used alternative.
 
diff --git a/docs/static/offline-plugins.asciidoc b/docs/static/offline-plugins.asciidoc
index aba0d4627cb..949c395b868 100644
--- a/docs/static/offline-plugins.asciidoc
+++ b/docs/static/offline-plugins.asciidoc
@@ -1,10 +1,10 @@
 [[offline-plugins]]
 === Offline Plugin Management
 
-The Logstash <<working-with-plugins,plugin manager>> provides support for preparing offline plugin packs that you can
-use to install Logstash plugins on systems that don't have Internet access. 
+The {ls} <<working-with-plugins,plugin manager>> provides support for preparing offline plugin packs that you can
+use to install {ls} plugins on systems that don't have Internet access. 
 
-This procedure requires a staging machine running Logstash that has access to a public or
+This procedure requires a staging machine running {ls} that has access to a public or
 <<private-rubygem,private Rubygems>> server. The staging machine downloads and packages all the files and dependencies
 required for offline installation.
 
@@ -12,7 +12,7 @@ required for offline installation.
 [discrete]
 === Building Offline Plugin Packs
 
-An _offline plugin pack_ is a compressed file that contains all the plugins your offline Logstash installation requires,
+An _offline plugin pack_ is a compressed file that contains all the plugins your offline {ls} installation requires,
 along with the dependencies for those plugins.
 
 To build an offline plugin pack:
diff --git a/docs/static/output.asciidoc b/docs/static/output.asciidoc
index d42536d6b85..6091e24e91d 100644
--- a/docs/static/output.asciidoc
+++ b/docs/static/output.asciidoc
@@ -9,6 +9,6 @@
 
 :getstarted: Let's step through creating an {plugintype} plugin using the https://github.com/logstash-plugins/logstash-output-example/[example {plugintype} plugin].
 
-:methodheader: pass:m[Logstash outputs must implement the `register` and `multi_receive` methods.]
+:methodheader: pass:m[{ls} outputs must implement the `register` and `multi_receive` methods.]
 
 include::include/pluginbody.asciidoc[]
diff --git a/docs/static/performance-checklist.asciidoc b/docs/static/performance-checklist.asciidoc
index 4d0824f4a41..6e656769ef9 100644
--- a/docs/static/performance-checklist.asciidoc
+++ b/docs/static/performance-checklist.asciidoc
@@ -1,7 +1,7 @@
 [[performance-tuning]]
 == Performance Tuning
 
-This section includes the following information about tuning Logstash
+This section includes the following information about tuning {ls}
 performance:
 
 * <<performance-troubleshooting>>
@@ -10,7 +10,7 @@ performance:
 [[performance-troubleshooting]]
 === Performance Troubleshooting
 
-You can use these troubleshooting tips to quickly diagnose and resolve Logstash performance problems. 
+You can use these troubleshooting tips to quickly diagnose and resolve {ls} performance problems. 
 Advanced knowledge of pipeline internals is not required to understand this guide. 
 However, the <<pipeline,pipeline documentation>> is recommended reading if you want to go beyond these tips.
 
@@ -26,20 +26,20 @@ sure-fire way to create a confusing situation.
 
 . *Check the performance of input sources and output destinations:*
 +
-* Logstash is only as fast as the services it connects to. Logstash can only consume and produce data as fast as its input and output destinations can!
+* {ls} is only as fast as the services it connects to. {ls} can only consume and produce data as fast as its input and output destinations can!
 
 . *Check system statistics:*
 +
 * CPU
 ** Note whether the CPU is being heavily used. On Linux/Unix, you can run `top -H` to see process statistics broken out by thread, as well as total CPU statistics.
-** If CPU usage is high, skip forward to the section about checking the JVM heap and then read the section about tuning Logstash worker settings.
+** If CPU usage is high, skip forward to the section about checking the JVM heap and then read the section about tuning {ls} worker settings.
 * Memory
-** Be aware of the fact that Logstash runs on the Java VM. This means that Logstash will always use the maximum amount of memory you allocate to it.
-** Look for other applications that use large amounts of memory and may be causing Logstash to swap to disk. This can happen if the total memory used by applications exceeds physical memory.
+** Be aware of the fact that {ls} runs on the Java VM. This means that {ls} will always use the maximum amount of memory you allocate to it.
+** Look for other applications that use large amounts of memory and may be causing {ls} to swap to disk. This can happen if the total memory used by applications exceeds physical memory.
 * I/O Utilization
 ** Monitor disk I/O to check for disk saturation.
-*** Disk saturation can happen if you’re using Logstash plugins (such as the file output) that may saturate your storage.
-*** Disk saturation can also happen if you're encountering a lot of errors that force Logstash to generate large error logs.
+*** Disk saturation can happen if you’re using {ls} plugins (such as the file output) that may saturate your storage.
+*** Disk saturation can also happen if you're encountering a lot of errors that force {ls} to generate large error logs.
 *** On Linux, you can use iostat, dstat, or something similar to monitor disk I/O.
 ** Monitor network I/O for network saturation.
 *** Network saturation can happen if you’re using inputs/outputs that perform a lot of network operations.
@@ -49,17 +49,17 @@ sure-fire way to create a confusing situation.
 +
 include::config-details.asciidoc[tag=heap-size-tips]
 
-. *Tune Logstash worker settings:*
+. *Tune {ls} worker settings:*
 +
 * Begin by scaling up the number of pipeline workers by using the `-w` flag. This will increase the number of threads available for filters and outputs. It is safe to scale this up to a multiple of CPU cores, if need be, as the threads can become idle on I/O.
 * You may also tune the output batch size. For many outputs, such as the Elasticsearch output, this setting will correspond to the size of I/O operations. In the case of the Elasticsearch output, this setting corresponds to the batch size.
 
 [[tuning-logstash]]
-=== Tuning and Profiling Logstash Performance
+=== Tuning and Profiling {ls} Performance
 
-The Logstash defaults are chosen to provide fast, safe performance for most
+The {ls} defaults are chosen to provide fast, safe performance for most
 users. However if you notice performance issues, you may need to modify
-some of the defaults. Logstash provides the following configurable options
+some of the defaults. {ls} provides the following configurable options
 for tuning pipeline performance: `pipeline.workers`, `pipeline.batch.size`, and `pipeline.batch.delay`. For more information about setting these options, see <<logstash-settings-file>>.
 
 Make sure you've read the <<performance-troubleshooting>> before modifying these options.
@@ -68,7 +68,7 @@ Make sure you've read the <<performance-troubleshooting>> before modifying these
 
 * The `pipeline.batch.size` setting defines the maximum number of events an individual worker thread collects before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Some hardware configurations require you to increase JVM heap space in the `jvm.options` config file to avoid performance degradation. (See <<config-setting-files>> for more info.) Values in excess of the optimum range cause performance degradation due to frequent garbage collection or JVM crashes related to out-of-memory exceptions. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, issues https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[bulk requests] for each batch received. Tuning the `pipeline.batch.size` setting adjusts the size of bulk requests sent to Elasticsearch.
 
-* The `pipeline.batch.delay` setting rarely needs to be tuned. This setting adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that Logstash waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, Logstash begins to execute filters and outputs.The maximum time that Logstash waits between receiving an event and processing that event in a filter is the product of the `pipeline.batch.delay` and  `pipeline.batch.size` settings.
+* The `pipeline.batch.delay` setting rarely needs to be tuned. This setting adjusts the latency of the {ls} pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that {ls} waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, {ls} begins to execute filters and outputs.The maximum time that {ls} waits between receiving an event and processing that event in a filter is the product of the `pipeline.batch.delay` and  `pipeline.batch.size` settings.
 
 [float]
 ==== Notes on Pipeline Configuration and Performance
@@ -86,13 +86,13 @@ following suggestions:
 
 * Threads in Java have names and you can use the `jstack`, `top`, and the VisualVM graphical tools to figure out which resources a given thread uses.
 
-* On Linux platforms, Logstash labels all the threads it can with something descriptive. For example, inputs show up as `[base]<inputname`, and pipeline workers show up as `[base]>workerN`, where N is an integer.  Where possible, other threads are also labeled to help you identify their purpose.
+* On Linux platforms, {ls} labels all the threads it can with something descriptive. For example, inputs show up as `[base]<inputname`, and pipeline workers show up as `[base]>workerN`, where N is an integer.  Where possible, other threads are also labeled to help you identify their purpose.
 
 [float]
 [[profiling-the-heap]]
 ==== Profiling the Heap
 
-When tuning Logstash you may have to adjust the heap size. You can use the https://visualvm.github.io/[VisualVM] tool to profile the heap. The *Monitor* pane in particular is useful for checking whether your heap allocation is sufficient for the current workload. The screenshots below show sample *Monitor* panes. The first pane examines a Logstash instance configured with too many inflight events. The second pane examines a Logstash instance configured with an appropriate amount of inflight events. Note that the specific batch sizes used here are most likely not applicable to your specific workload, as the memory demands of Logstash vary in large part based on the type of messages you are sending.
+When tuning {ls} you may have to adjust the heap size. You can use the https://visualvm.github.io/[VisualVM] tool to profile the heap. The *Monitor* pane in particular is useful for checking whether your heap allocation is sufficient for the current workload. The screenshots below show sample *Monitor* panes. The first pane examines a {ls} instance configured with too many inflight events. The second pane examines a {ls} instance configured with an appropriate amount of inflight events. Note that the specific batch sizes used here are most likely not applicable to your specific workload, as the memory demands of {ls} vary in large part based on the type of messages you are sending.
 
 image::static/images/pipeline_overload.png[]
 
diff --git a/docs/static/persistent-queues.asciidoc b/docs/static/persistent-queues.asciidoc
index 51630f9421d..a8360ad5adf 100644
--- a/docs/static/persistent-queues.asciidoc
+++ b/docs/static/persistent-queues.asciidoc
@@ -8,10 +8,10 @@ A {ls} persistent queue helps protect against data loss during abnormal terminat
 
 A persistent queue (PQ):
 
-* Helps protect against message loss during a normal shutdown and when Logstash
+* Helps protect against message loss during a normal shutdown and when {ls}
 is terminated abnormally. 
-If Logstash is restarted while events are in-flight,
-Logstash attempts to deliver messages stored in the persistent queue until
+If {ls} is restarted while events are in-flight,
+{ls} attempts to deliver messages stored in the persistent queue until
 delivery succeeds at least once.  
 * Can absorb bursts of events without needing an external buffering mechanism like Redis or Apache Kafka. 
 
@@ -34,7 +34,7 @@ TIP: Use the local filesystem for data integrity and performance. Network File S
 [[configuring-persistent-queues]]
 ==== Configuring persistent queues
 
-To configure persistent queues, specify options in the Logstash <<logstash-settings-file,settings file>>.
+To configure persistent queues, specify options in the {ls} <<logstash-settings-file,settings file>>.
 Settings are applied to every pipeline.
 
 When you set values for capacity and sizing settings, remember that the value you set is applied _per pipeline_ rather than a total to be shared among all pipelines. 
@@ -46,7 +46,7 @@ TIP: If you want to define values for a specific pipeline, use <<multiple-pipeli
 `queue.page_capacity`:: The queue data consists of append-only files called "pages." This value sets the maximum size of a queue page in bytes. 
 The default size of 64mb is a good value for most users, and changing this value is unlikely to have performance benefits. 
 If you change the page capacity of an existing queue, the new size applies only to the new page.
-`queue.drain`:: Specify `true` if you want Logstash to wait until the persistent queue is drained before shutting down. The amount of time it takes to drain the queue depends on the number of events that have accumulated in the queue. Therefore, you should avoid using this setting unless the queue, even when full, is relatively small and can be drained quickly. 
+`queue.drain`:: Specify `true` if you want {ls} to wait until the persistent queue is drained before shutting down. The amount of time it takes to drain the queue depends on the number of events that have accumulated in the queue. Therefore, you should avoid using this setting unless the queue, even when full, is relatively small and can be drained quickly. 
 `queue.max_events`:: The maximum number of events not yet read by the pipeline worker. The default is 0 (unlimited).
 We use this setting for internal testing. 
 Users generally shouldn't be changing this value.
@@ -86,7 +86,7 @@ If you optimize for data protection, you may impact performance.
 [[pq-size]]
 ===== Queue size
 You can control queue size with the `queue.max_events` and  `queue.max_bytes` settings.
-If both settings are specified, Logstash uses whichever criteria is reached
+If both settings are specified, {ls} uses whichever criteria is reached
 first. 
 See <<backpressure-persistent-queue>> for behavior when queue limits are
 reached.
@@ -157,7 +157,7 @@ queue.max_bytes: 10mb
 Setting `queue.checkpoint.writes` and `queue.checkpoint.acks` to `0` may
 yield maximum performance, but may have potential impact on durability.
 
-In a situation where Logstash is terminated or there is a hardware-level
+In a situation where {ls} is terminated or there is a hardware-level
 failure, any data that has not been checkpointed, is lost. 
 See <<durability-persistent-queues>> to better understand the trade-offs.
 
@@ -170,7 +170,7 @@ Persistent queues can play an important role in your <<pipeline-to-pipeline,pipe
 [[uc-isolator]]
 ====== Use case: PQs and output isolator pattern
 
-Here is a real world use case described by a Logstash user.
+Here is a real world use case described by a {ls} user.
 
 "_In our deployment, we use one pipeline per output, and each pipeline has a
 large PQ. This configuration allows a single output to stall without blocking
@@ -196,7 +196,7 @@ This error indicates that the head page (the oldest in a directory and the one w
 To research and resolve the issue: 
 
 . Identify the queue (or queues) that may be corrupt by checking log files, or running the `pqcheck` utility.
-. Stop Logstash, and wait for it to shut down. 
+. Stop {ls}, and wait for it to shut down. 
 . Run `pqrepair <path>` for each of the corrupted queues.
 
 [[pqcheck]]
@@ -232,9 +232,9 @@ In this case, run `pqrepair` against the specified queue directory.
 *Sample with healthy page file*
 
 This sample represents a healthy queue with three page files. 
-In this sample, Logstash is currently writing to `page.2` as referenced by
+In this sample, {ls} is currently writing to `page.2` as referenced by
 `checkpoint.head`.
-Logstash is reading from `page.0` as referenced by `checkpoint.0`.
+{ls} is reading from `page.0` as referenced by `checkpoint.0`.
 
 [source,txt]
 -----
@@ -257,7 +257,7 @@ checkpoint.0, fully-acked: NO, page.0 size: 67108864  <1>
 
 *Sample with corrupted page file*
 
-If Logstash doesn't start and/or `pqcheck` shows an anomaly, such as `NOT_FOUND` for a page, run `pqrepair` on the queue directory.
+If {ls} doesn't start and/or `pqcheck` shows an anomaly, such as `NOT_FOUND` for a page, run `pqrepair` on the queue directory.
 
 [source,txt]
 -----
@@ -293,7 +293,7 @@ where `<queue_directory>` is the fully qualified path to the persistent queue lo
 There is no output if the utility runs properly.  
 
 The `pqrepair` utility requires write access to the directory. 
-Folder permissions may cause problems when Logstash is run as a service.
+Folder permissions may cause problems when {ls} is run as a service.
 In this situation, use `sudo`.
 
 [source,txt]
@@ -301,7 +301,7 @@ In this situation, use `sudo`.
 /usr/share/logstash$ sudo -u logstash bin/pqrepair /var/lib/logstash/queue/main/
 -----
 
-After you run `pqrepair`, restart Logstash to verify that the repair operation was successful. 
+After you run `pqrepair`, restart {ls} to verify that the repair operation was successful. 
 
  
 [[draining-pqs]]
@@ -317,8 +317,8 @@ Examples include:
 To drain the persistent queue:
 
 . In the `logstash.yml` file, set `queue.drain: true`.
-. Restart Logstash for this setting to take effect.
-. Shutdown Logstash (using CTRL+C or SIGTERM), and wait for the queue to empty.
+. Restart {ls} for this setting to take effect.
+. Shutdown {ls} (using CTRL+C or SIGTERM), and wait for the queue to empty.
 
 [[persistent-queues-architecture]]
 ==== How persistent queues work
@@ -332,30 +332,30 @@ When an input has events ready to process, it writes them to the queue. When
 the write to the queue is successful, the input can send an acknowledgement to
 its data source.
 
-When processing events from the queue, Logstash acknowledges events as
+When processing events from the queue, {ls} acknowledges events as
 completed, within the queue, only after filters and outputs have completed.
 The queue keeps a record of events that have been processed by the pipeline.
 An event is recorded as processed (in this document, called "acknowledged" or
 "ACKed") if, and only if, the event has been processed completely by the
-Logstash pipeline. 
+{ls} pipeline. 
 
 What does acknowledged mean? This means the event has been handled by all
 configured filters and outputs. For example, if you have only one output,
 Elasticsearch, an event is ACKed when the Elasticsearch output has successfully
 sent this event to Elasticsearch. 
 
-During a normal shutdown (*CTRL+C* or SIGTERM), Logstash stops reading
+During a normal shutdown (*CTRL+C* or SIGTERM), {ls} stops reading
 from the queue and finishes processing the in-flight events being processed
-by the filters and outputs. Upon restart, Logstash resumes processing the
+by the filters and outputs. Upon restart, {ls} resumes processing the
 events in the persistent queue as well as accepting new events from inputs.
 
-If Logstash is abnormally terminated, any in-flight events will not have been
-ACKed and will be reprocessed by filters and outputs when Logstash is
-restarted. Logstash processes events in batches, so it is possible
+If {ls} is abnormally terminated, any in-flight events will not have been
+ACKed and will be reprocessed by filters and outputs when {ls} is
+restarted. {ls} processes events in batches, so it is possible
 that for any given batch, some of that batch may have been successfully
 completed, but not recorded as ACKed, when an abnormal termination occurs.
 
-NOTE: If you override the default behavior by setting `drain.queue: true`, Logstash reads from the queue until it is emptied--even after a controlled shutdown. 
+NOTE: If you override the default behavior by setting `drain.queue: true`, {ls} reads from the queue until it is emptied--even after a controlled shutdown. 
 
 For more details specific behaviors of queue writes and acknowledgement, see 
 <<durability-persistent-queues>>.
@@ -364,8 +364,8 @@ For more details specific behaviors of queue writes and acknowledgement, see
 [[backpressure-persistent-queue]]
 ===== Handling back pressure
 
-When the queue is full, Logstash puts back pressure on the inputs to stall data
-flowing into Logstash. This mechanism helps Logstash control the rate of data
+When the queue is full, {ls} puts back pressure on the inputs to stall data
+flowing into {ls}. This mechanism helps {ls} control the rate of data
 flow at the input stage without overwhelming outputs like Elasticsearch.
 
 Use `queue.max_bytes` setting to configure the total capacity of the queue on
@@ -377,15 +377,15 @@ queue.type: persisted
 queue.max_bytes: 8gb
 -----
 
-With these settings specified, Logstash buffers events on disk until the
+With these settings specified, {ls} buffers events on disk until the
 size of the queue reaches 8gb. When the queue is full of unACKed events, and
-the size limit has been reached, Logstash no longer accepts new events. 
+the size limit has been reached, {ls} no longer accepts new events. 
 
 Each input handles back pressure independently. For example, when the
 <<plugins-inputs-beats,beats>> input encounters back pressure, it no longer
 accepts new connections and waits until the persistent queue has space to accept
 more events. After the filter and output stages finish processing existing
-events in the queue and ACKs them, Logstash automatically starts accepting new
+events in the queue and ACKs them, {ls} automatically starts accepting new
 events.
 
 [[durability-persistent-queues]]
@@ -393,24 +393,24 @@ events.
 
 Durability is a property of storage writes that ensures data will be available after it's written.
 
-When the persistent queue feature is enabled, Logstash stores events on
-disk. Logstash commits to disk in a mechanism called _checkpointing_.
+When the persistent queue feature is enabled, {ls} stores events on
+disk. {ls} commits to disk in a mechanism called _checkpointing_.
 
 The queue itself is a set of pages. There are two kinds of pages: head pages and tail pages. The head page is where new events are written. There is only one head page. When the head page is of a certain size (see `queue.page_capacity`), it becomes a tail page, and a new head page is created. Tail pages are immutable, and the head page is append-only. 
 Second, the queue records details about itself (pages, acknowledgements, etc) in a separate file called a checkpoint file.
 
-When recording a checkpoint, Logstash:
+When recording a checkpoint, {ls}:
 
 * Calls `fsync` on the head page.
 * Atomically writes to disk the current state of the queue.
 
 The process of checkpointing is atomic, which means any update to the file is saved if successful.
 
-IMPORTANT: If Logstash is terminated, or if there is a hardware-level failure,
+IMPORTANT: If {ls} is terminated, or if there is a hardware-level failure,
 any data that is buffered in the persistent queue, but not yet checkpointed, is
 lost.
 
-You can force Logstash to checkpoint more frequently by setting
+You can force {ls} to checkpoint more frequently by setting
 `queue.checkpoint.writes`. This setting specifies the maximum number of events
 that may be written to disk before forcing a checkpoint. The default is 1024. To
 ensure maximum durability and avoid data loss in the persistent queue, you can
diff --git a/docs/static/pipeline-config-exps.asciidoc b/docs/static/pipeline-config-exps.asciidoc
index e1364965929..6d036fd46fe 100644
--- a/docs/static/pipeline-config-exps.asciidoc
+++ b/docs/static/pipeline-config-exps.asciidoc
@@ -1,7 +1,7 @@
 
 [[config-examples]]
-=== Logstash configuration examples
-These examples illustrate how you can configure Logstash to filter events, process Apache logs and syslog messages, and use conditionals to control what events are processed by a filter or output.
+=== {ls} configuration examples
+These examples illustrate how you can configure {ls} to filter events, process Apache logs and syslog messages, and use conditionals to control what events are processed by a filter or output.
 
 TIP: If you need help building grok patterns, try out the
 {kibana-ref}/xpack-grokdebugger.html[Grok Debugger]. 
@@ -30,7 +30,7 @@ output {
 }
 ----------------------------------
 
-Run Logstash with this configuration:
+Run {ls} with this configuration:
 
 [source,ruby]
 ----------------------------------
@@ -68,9 +68,9 @@ You should see something returned to stdout that looks like this:
 }
 ----------------------------------
 
-As you can see, Logstash (with help from the `grok` filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This is extremely useful once you start querying and analyzing our log data. For example, you'll be able to easily run reports on HTTP response codes, IP addresses, referrers, and so on. There are quite a few grok patterns included with Logstash out-of-the-box, so it's quite likely if you need to parse a common log format, someone has already done the work for you. For more information, see the list of https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns[Logstash grok patterns] on GitHub.
+As you can see, {ls} (with help from the `grok` filter) was able to parse the log line (which happens to be in Apache "combined log" format) and break it up into many different discrete bits of information. This is extremely useful once you start querying and analyzing our log data. For example, you'll be able to easily run reports on HTTP response codes, IP addresses, referrers, and so on. There are quite a few grok patterns included with {ls} out-of-the-box, so it's quite likely if you need to parse a common log format, someone has already done the work for you. For more information, see the list of https://github.com/logstash-plugins/logstash-patterns-core/tree/main/patterns[{ls} grok patterns] on GitHub.
 
-The other filter used in this example is the `date` filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the `@timestamp` field in this example is set to December 11, 2013, even though Logstash is ingesting the event at some point afterwards. This is handy when backfilling logs. It gives you the ability to tell Logstash "use this value as the timestamp for this event".
+The other filter used in this example is the `date` filter. This filter parses out a timestamp and uses it as the timestamp for the event (regardless of when you're ingesting the log data). You'll notice that the `@timestamp` field in this example is set to December 11, 2013, even though {ls} is ingesting the event at some point afterwards. This is handy when backfilling logs. It gives you the ability to tell {ls} "use this value as the timestamp for this event".
 
 [discrete]
 ==== Processing Apache logs
@@ -115,16 +115,16 @@ Then, create the input file you configured above (in this example, "/tmp/access_
 98.83.179.51 - - [18/May/2011:19:35:08 -0700] "GET /css/main.css HTTP/1.1" 200 1837 "http://www.safesand.com/information.htm" "Mozilla/5.0 (Windows NT 6.0; WOW64; rv:2.0.1) Gecko/20100101 Firefox/4.0.1"
 ----------------------------------
 
-Now, run Logstash with the -f flag to pass in the configuration file:
+Now, run {ls} with the -f flag to pass in the configuration file:
 
 [source,js]
 ----------------------------------
 bin/logstash -f logstash-apache.conf
 ----------------------------------
 
-Now you should see your apache log data in Elasticsearch! Logstash opened and read the specified input file, processing each event it encountered. Any additional lines logged to this file will also be captured, processed by Logstash as events, and stored in Elasticsearch. As an added bonus, they are stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
+Now you should see your apache log data in Elasticsearch! {ls} opened and read the specified input file, processing each event it encountered. Any additional lines logged to this file will also be captured, processed by {ls} as events, and stored in Elasticsearch. As an added bonus, they are stashed with the field "type" set to "apache_access" (this is done by the type => "apache_access" line in the input configuration).
 
-In this configuration, Logstash is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching `*log`), by changing one line in the above configuration:
+In this configuration, {ls} is only watching the apache access_log, but it's easy enough to watch both the access_log and the error_log (actually, any file matching `*log`), by changing one line in the above configuration:
 
 [source,js]
 ----------------------------------
@@ -134,9 +134,9 @@ input {
 ...
 ----------------------------------
 
-When you restart Logstash, it will process both the error and access logs. However, if you inspect your data (using elasticsearch-kopf, perhaps), you'll see that the access_log is broken up into discrete fields, but the error_log isn't. That's because we used a `grok` filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
+When you restart {ls}, it will process both the error and access logs. However, if you inspect your data (using elasticsearch-kopf, perhaps), you'll see that the access_log is broken up into discrete fields, but the error_log isn't. That's because we used a `grok` filter to match the standard combined apache log format and automatically split the data into separate fields. Wouldn't it be nice *if* we could control how a line was parsed, based on its format? Well, we can...
 
-Note that Logstash did not reprocess the events that were already seen in the access_log file. When reading from a file, Logstash saves its position and only processes new lines as they are added. Neat!
+Note that {ls} did not reprocess the events that were already seen in the access_log file. When reading from a file, {ls} saves its position and only processes new lines as they are added. Neat!
 
 [discrete]
 [[using-conditionals]]
@@ -203,9 +203,9 @@ output {
 
 [discrete]
 ==== Processing Syslog messages
-Syslog is one of the most common use cases for Logstash, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line so you can get a feel for what happens.
+Syslog is one of the most common use cases for {ls}, and one it handles exceedingly well (as long as the log lines conform roughly to RFC3164). Syslog is the de facto UNIX networked logging standard, sending messages from client machines to a local file, or to a centralized log server via rsyslog. For this example, you won't need a functioning syslog instance; we'll fake it from the command line so you can get a feel for what happens.
 
-First, let's make a simple configuration file for Logstash + syslog, called 'logstash-syslog.conf'.
+First, let's make a simple configuration file for {ls} + syslog, called 'logstash-syslog.conf'.
 
 [source,ruby]
 ----------------------------------
@@ -239,14 +239,14 @@ output {
 }
 ----------------------------------
 
-Run Logstash with this new configuration:
+Run {ls} with this new configuration:
 
 [source,ruby]
 ----------------------------------
 bin/logstash -f logstash-syslog.conf
 ----------------------------------
 
-Normally, a client machine would connect to the Logstash instance on port 5000 and send its message. For this example, we'll just telnet to Logstash and enter a log line (similar to how we entered log lines into STDIN earlier). Open another shell window to interact with the Logstash syslog input and enter the following command:
+Normally, a client machine would connect to the {ls} instance on port 5000 and send its message. For this example, we'll just telnet to {ls} and enter a log line (similar to how we entered log lines into STDIN earlier). Open another shell window to interact with the {ls} syslog input and enter the following command:
 
 [source,ruby]
 ----------------------------------
@@ -263,7 +263,7 @@ Dec 23 14:30:01 louis CRON[619]: (www-data) CMD (php /usr/share/cacti/site/polle
 Dec 22 18:28:06 louis rsyslogd: [origin software="rsyslogd" swVersion="4.2.0" x-pid="2253" x-info="http://www.rsyslog.com"] rsyslogd was HUPed, type 'lightweight'.
 ----------------------------------
 
-Now you should see the output of Logstash in your original shell as it processes and parses messages!
+Now you should see the output of {ls} in your original shell as it processes and parses messages!
 
 [source,ruby]
 ----------------------------------
diff --git a/docs/static/pipeline-configuration.asciidoc b/docs/static/pipeline-configuration.asciidoc
index 07c3e5edaf1..34e86b4157c 100644
--- a/docs/static/pipeline-configuration.asciidoc
+++ b/docs/static/pipeline-configuration.asciidoc
@@ -2,14 +2,14 @@
 == Creating a {ls} pipeline
 
 You can create a pipeline by stringing together plugins--<<input-plugins,inputs>>, <<output-plugins,outputs>>, <<filter-plugins,filters>>, and sometimes <<codec-plugins,codecs>>--in order to process data. 
-To build a Logstash pipeline, create a config file to specify which plugins you want to use and the settings for each plugin.
+To build a {ls} pipeline, create a config file to specify which plugins you want to use and the settings for each plugin.
 
 A very basic pipeline might contain only an input and an output. 
 Most pipelines include at least one filter plugin because that's where the "transform" part of the ETL (extract, transform, load) magic happens. 
 You can reference event fields in a pipeline and use conditionals to process events when they meet certain criteria. 
 
-Let's step through creating a simple pipeline config on your local machine and then using it to run Logstash. 
-Create a file named "logstash-simple.conf" and save it in the same directory as Logstash.
+Let's step through creating a simple pipeline config on your local machine and then using it to run {ls}. 
+Create a file named "logstash-simple.conf" and save it in the same directory as {ls}.
 
 [source,ruby]
 ----------------------------------
@@ -27,7 +27,7 @@ Then, run {ls} and specify the configuration file with the `-f` flag.
 bin/logstash -f logstash-simple.conf
 -----
 
-Et voilà! Logstash reads the specified configuration file and outputs to both Elasticsearch and stdout. 
+Et voilà! {ls} reads the specified configuration file and outputs to both Elasticsearch and stdout. 
 Before we move on to <<config-examples,more complex examples>>, let's take a look at what's in a pipeline config file.
 
 [[configuration-file-structure]]
@@ -166,12 +166,12 @@ Examples:
 [discrete]
 ==== Codec
 
-A codec is the name of Logstash codec used to represent the data. Codecs can be
+A codec is the name of {ls} codec used to represent the data. Codecs can be
 used in both inputs and outputs.
 
 Input codecs provide a convenient way to decode your data before it enters the input.
 Output codecs provide a convenient way to encode your data before it leaves the output.
-Using an input or output codec eliminates the need for a separate filter in your Logstash pipeline.
+Using an input or output codec eliminates the need for a separate filter in your {ls} pipeline.
 
 A list of available codecs can be found at the <<codec-plugins,Codec Plugins>> page.
 
diff --git a/docs/static/pipeline-pipeline-config.asciidoc b/docs/static/pipeline-pipeline-config.asciidoc
index 679f35128d5..c33652fe3a1 100644
--- a/docs/static/pipeline-pipeline-config.asciidoc
+++ b/docs/static/pipeline-pipeline-config.asciidoc
@@ -1,9 +1,9 @@
 [[pipeline-to-pipeline]]
 === Pipeline-to-pipeline communication
 
-When using the multiple pipeline feature of Logstash, you may want to connect multiple pipelines within the same Logstash instance. This configuration can be useful to isolate the execution of these pipelines, as well as to help break-up the logic of complex pipelines. The `pipeline` input/output enables a number of advanced architectural patterns discussed later in this document.
+When using the multiple pipeline feature of {ls}, you may want to connect multiple pipelines within the same {ls} instance. This configuration can be useful to isolate the execution of these pipelines, as well as to help break-up the logic of complex pipelines. The `pipeline` input/output enables a number of advanced architectural patterns discussed later in this document.
 
-If you need to set up communication _between_ Logstash instances, use either {logstash-ref}/ls-to-ls.html[Logstash-to-Logstash] communications, or an intermediary queue, such as Kafka or Redis.
+If you need to set up communication _between_ {ls} instances, use either {logstash-ref}/ls-to-ls.html[{ls}-to-{ls}] communications, or an intermediary queue, such as Kafka or Redis.
 
 TIP: Persistent queues (PQs) can help keep data moving through pipelines.  
 See <<pq-pline-pline>> to learn how PQs can enhance your
@@ -12,7 +12,7 @@ pipeline-to-pipeline communication strategy.
 [[pipeline-to-pipeline-overview]]
 ==== Configuration overview
 
-Use the `pipeline` input and `pipeline` output to connect two pipelines running within the same Logstash instance. These inputs use a client-server approach, where the `pipeline` input registers a virtual address that a `pipeline` output can connect to.
+Use the `pipeline` input and `pipeline` output to connect two pipelines running within the same {ls} instance. These inputs use a client-server approach, where the `pipeline` input registers a virtual address that a `pipeline` output can connect to.
 
 . Create a 'downstream' pipeline that listens for events on a virtual address.
 . Create an 'upstream' pipeline that produces events, sending them through a `pipeline` output to one or more virtual addresses.
@@ -31,11 +31,11 @@ Here is a simple example of this configuration.
 [[how-pipeline-to-pipeline-works]]
 ===== How it works
 
-The `pipeline` input acts as a virtual server listening on a single virtual address in the local process. Only `pipeline` outputs running on the same local Logstash can send events to this address. Pipeline `outputs` can send events to a list of virtual addresses. A `pipeline` output will be blocked if the downstream pipeline is blocked or unavailable.
+The `pipeline` input acts as a virtual server listening on a single virtual address in the local process. Only `pipeline` outputs running on the same local {ls} can send events to this address. Pipeline `outputs` can send events to a list of virtual addresses. A `pipeline` output will be blocked if the downstream pipeline is blocked or unavailable.
 
 When events are sent across pipelines, their data is fully copied. Modifications to an event in a downstream pipeline do not affect that event in any upstream pipelines.
 
-The `pipeline` plugin may be the most efficient way to communicate between pipelines, but it still incurs a performance cost. Logstash must duplicate each event in full on the Java heap for each downstream pipeline. Using this feature may affect the heap memory utilization of Logstash.
+The `pipeline` plugin may be the most efficient way to communicate between pipelines, but it still incurs a performance cost. {ls} must duplicate each event in full on the Java heap for each downstream pipeline. Using this feature may affect the heap memory utilization of {ls}.
 
 [[delivery-guarantees]]
 ===== Delivery guarantees
@@ -52,13 +52,13 @@ downstream pipeline without blocking any upstream pipelines sending to it.
 These delivery guarantees also inform the shutdown behavior of this feature. When performing a pipeline reload, changes
 will be made immediately as the user requests, even if that means removing a downstream pipeline receiving events from
 an upstream pipeline. This will cause the upstream pipeline to block. You must restore the downstream pipeline to
-cleanly shut down Logstash. You may issue a force kill, but inflight events may be lost unless the persistent queue is
+cleanly shut down {ls}. You may issue a force kill, but inflight events may be lost unless the persistent queue is
 enabled for that pipeline.
 
 [[avoid-cycles]]
 ===== Avoid cycles
 
-When you connect pipelines, keep the data flowing in one direction. Looping data or connecting the pipelines into a cyclic graph can cause problems. Logstash waits for each pipeline's work to complete before shutting down. Pipeline loops can prevent Logstash from shutting down cleanly.
+When you connect pipelines, keep the data flowing in one direction. Looping data or connecting the pipelines into a cyclic graph can cause problems. {ls} waits for each pipeline's work to complete before shutting down. Pipeline loops can prevent {ls} from shutting down cleanly.
 
 [[architectural-patterns]]
 ==== Architectural patterns
@@ -126,7 +126,7 @@ Notice how following the flow of data is a simple due to the fact that each pipe
 [[output-isolator-pattern]]
 ===== The output isolator pattern
 
-You can use the output isolator pattern to prevent Logstash from becoming blocked if one of multiple outputs experiences a temporary failure. Logstash, by default, is blocked when any single output is down. This behavior is important in guaranteeing at-least-once delivery of data. 
+You can use the output isolator pattern to prevent {ls} from becoming blocked if one of multiple outputs experiences a temporary failure. {ls}, by default, is blocked when any single output is down. This behavior is important in guaranteeing at-least-once delivery of data. 
 
 For example, a server might be configured to send log data to both Elasticsearch and an HTTP endpoint. The HTTP endpoint might be frequently unavailable due to regular service or other reasons. In this scenario, data would be paused from sending to Elasticsearch any time the HTTP endpoint is down. 
 
diff --git a/docs/static/plugin-generator.asciidoc b/docs/static/plugin-generator.asciidoc
index 20454384681..34420ceb7c1 100644
--- a/docs/static/plugin-generator.asciidoc
+++ b/docs/static/plugin-generator.asciidoc
@@ -1,9 +1,9 @@
 [[plugin-generator]]
 === Generating plugins
 
-You can create your own Logstash plugin in seconds! The generate subcommand of `bin/logstash-plugin` creates the foundation 
-for a new Logstash plugin with templatized files. 
-It creates the correct directory structure, gemspec files, and dependencies so you can start adding custom code to process data with Logstash.
+You can create your own {ls} plugin in seconds! The generate subcommand of `bin/logstash-plugin` creates the foundation 
+for a new {ls} plugin with templatized files. 
+It creates the correct directory structure, gemspec files, and dependencies so you can start adding custom code to process data with {ls}.
 
 **Example Usage**
 
diff --git a/docs/static/plugin-manager.asciidoc b/docs/static/plugin-manager.asciidoc
index ab58f11ca08..7a0ceff1adf 100644
--- a/docs/static/plugin-manager.asciidoc
+++ b/docs/static/plugin-manager.asciidoc
@@ -26,7 +26,7 @@ https://support.apple.com/en-us/HT202491[Safely open apps on your Mac].
 ====
 
 
-Logstash has a rich collection of input, filter, codec, and output plugins.
+{ls} has a rich collection of input, filter, codec, and output plugins.
 Check out the https://www.elastic.co/support/matrix#matrix_logstash_plugins[Elastic Support Matrix] 
 to see which plugins are supported at various levels. 
 
@@ -55,7 +55,7 @@ offline plugin packs.
 ==== Proxy configuration
 
 Most plugin manager commands require access to the internet to reach https://rubygems.org[RubyGems.org].
-If your organization is behind a firewall, you can set these environments variables to configure Logstash to use your proxy.
+If your organization is behind a firewall, you can set these environments variables to configure {ls} to use your proxy.
 
 [source, shell]
 ----------------------------------
@@ -67,7 +67,7 @@ export https_proxy=http://localhost:3128
 [[listing-plugins]]
 === Listing plugins
 
-Logstash release packages bundle common plugins. To list the plugins currently
+{ls} release packages bundle common plugins. To list the plugins currently
 available in your deployment:
 
 [source,shell]
@@ -88,7 +88,7 @@ bin/logstash-plugin list --group output <4>
 
 When you have access to internet, you can retrieve plugins hosted on the
 https://rubygems.org/[RubyGems.org]public repository and install them on top of
-your Logstash installation.
+your {ls} installation.
 
 [source,shell]
 ----------------------------------
@@ -101,7 +101,7 @@ After a plugin is successfully installed, you can use it in your configuration f
 [[updating-plugins]]
 === Updating plugins
 
-Plugins have their own release cycles and are often released independently of Logstash’s core release cycle. 
+Plugins have their own release cycles and are often released independently of {ls}’s core release cycle. 
 Using the update subcommand you can get the latest version of the plugin.
 
 [source,shell]
@@ -116,7 +116,7 @@ bin/logstash-plugin update logstash-input-github <2>
 [[removing-plugins]]
 === Removing plugins
 
-If you need to remove plugins from your Logstash installation:
+If you need to remove plugins from your {ls} installation:
 
 [source,shell]
 ----------------------------------
@@ -128,7 +128,7 @@ bin/logstash-plugin remove logstash-input-github
 ==== Advanced: Adding a locally built plugin
 
 In some cases, you may want to install plugins which are not yet released and
-not hosted on RubyGems.org. Logstash provides you the option to install a
+not hosted on RubyGems.org. {ls} provides you the option to install a
 locally built plugin which is packaged as a ruby gem. Using a file location:
 
 [source,shell]
@@ -140,7 +140,7 @@ bin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem
 [[installing-local-plugins-path]]
 ==== Advanced: Using `--path.plugins`
 
-Using the Logstash `--path.plugins` flag, you can load a plugin source code located on your file system. Typically this is used by
+Using the {ls} `--path.plugins` flag, you can load a plugin source code located on your file system. Typically this is used by
 developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
 
 The path needs to be in a  specific directory hierarchy: `PATH/logstash/TYPE/NAME.rb`, where TYPE is 'inputs' 'filters', 'outputs' or 'codecs' and NAME is the name of the plugin.
diff --git a/docs/static/private-gem-repo.asciidoc b/docs/static/private-gem-repo.asciidoc
index 930bd3a32f2..5412fba1411 100644
--- a/docs/static/private-gem-repo.asciidoc
+++ b/docs/static/private-gem-repo.asciidoc
@@ -1,7 +1,7 @@
 [[private-rubygem]]
 === Private Gem Repositories
 
-The Logstash plugin manager connects to a Ruby gems repository to install and update Logstash plugins. By default, this
+The {ls} plugin manager connects to a Ruby gems repository to install and update {ls} plugins. By default, this
 repository is http://rubygems.org.
 
 Some use cases are unable to use the default repository, as in the following examples:
@@ -28,7 +28,7 @@ By default, the gemfile's `source` line reads:
 
 [source,shell]
 ----------
-# This is a Logstash generated Gemfile.
+# This is a {ls} generated Gemfile.
 # If you modify this file manually all comments and formatting will be lost.
 
 source "https://rubygems.org"
@@ -38,7 +38,7 @@ To change the source, edit the `source` line to contain your preferred source, a
 
 [source,shell]
 ----------
-# This is a Logstash generated Gemfile.
+# This is a {ls} generated Gemfile.
 # If you modify this file manually all comments and formatting will be lost.
 
 source "https://my.private.repository"
diff --git a/docs/static/redirects.asciidoc b/docs/static/redirects.asciidoc
index a3a86d89859..ca7d55429c6 100644
--- a/docs/static/redirects.asciidoc
+++ b/docs/static/redirects.asciidoc
@@ -34,18 +34,18 @@ from many known apps, such as nginx or apache.
 // HOMEBREW INSTALL 
 
 [role="exclude",id="brew"]
-=== Homebrew (MacOS) for Logstash
+=== Homebrew (MacOS) for {ls}
 
-As of Logstash 8.0, Elastic no longer maintains a homebrew cask containing formulae for installing the Elastic-licensed distribution of Logstash.
-If you want to run the full distribution of Logstash on a Mac, you are encouraged to <<installing-binary,install from a downloaded binary distribution>>.
+As of {ls} 8.0, Elastic no longer maintains a homebrew cask containing formulae for installing the Elastic-licensed distribution of {ls}.
+If you want to run the full distribution of {ls} on a Mac, you are encouraged to <<installing-binary,install from a downloaded binary distribution>>.
 
 You can still install the Apache-licensed OSS distribution with homebrew using the formulae maintained by Homebrew.
 
 [role="exclude",id="brew-start"]
-==== Homebrew for Logstash
+==== Homebrew for {ls}
 
-As of Logstash 8.0, Elastic no longer maintains a Homebrew cask containing formulae for installing the Elastic-licensed distribution of Logstash.
-If you want to run the full distribution of Logstash on a Mac, you are encouraged to <<installing-binary,install from a downloaded binary distribution>>.
+As of {ls} 8.0, Elastic no longer maintains a Homebrew cask containing formulae for installing the Elastic-licensed distribution of {ls}.
+If you want to run the full distribution of {ls} on a Mac, you are encouraged to <<installing-binary,install from a downloaded binary distribution>>.
 
 // UPGRADE FROM OLDER VERSIONS
 
@@ -65,11 +65,11 @@ applies for your upgrade scenario.
 [float]
 ==== Drain the Persistent Queue (version 6.2.x and earlier)
 
-The following applies only if you are upgrading from Logstash version 6.2.x or
+The following applies only if you are upgrading from {ls} version 6.2.x or
 earlier with the persistent queue (PQ) enabled.
 
 We strive to maintain backward compatibility within a given major release. 
-Serialization issues in Logstash 6.2.x and earlier required us to break
+Serialization issues in {ls} 6.2.x and earlier required us to break
 that compatibility in version 6.3.0 to ensure correctness of operation. For more
 technical details, please check our tracking github issue for this
 matter, https://github.com/elastic/logstash/issues/9494[#9494].
@@ -80,13 +80,13 @@ the persistent queue before you upgrade from version 6.2.x and earlier.
 To drain the queue:
 
 . In the logstash.yml file, set `queue.drain: true`.
-. Restart Logstash for this setting to take effect. 
-. Shutdown Logstash (using CTRL+C or SIGTERM), and wait for the queue to empty.
+. Restart {ls} for this setting to take effect. 
+. Shutdown {ls} (using CTRL+C or SIGTERM), and wait for the queue to empty.
 
 When the queue is empty:
 
 . Complete the upgrade.
-. Restart Logstash.
+. Restart {ls}.
 
 We have resolved issues with data incompatibilities for version 6.3 and later. 
 These steps won’t be required for future upgrades.
@@ -95,14 +95,14 @@ These steps won’t be required for future upgrades.
 [role="exclude",id="upgrading-logstash-pqs-6.3"]
 ==== Upgrading from version 6.3 (and later) with Persistent Queues enabled 
 
-Upgrading Logstash with persistent queues enabled is supported. The persistent
-queue directory is self-contained and can be read by a new Logstash instance
-running the same pipeline. You can safely shut down the original Logstash
+Upgrading {ls} with persistent queues enabled is supported. The persistent
+queue directory is self-contained and can be read by a new {ls} instance
+running the same pipeline. You can safely shut down the original {ls}
 instance, spin up a new instance, and set `path.queue` in the `logstash.yml`
 <<logstash-settings-file,settings file>> to point to the original queue directory.
 You can also use a mounted drive to make this workflow easier.
 
-Keep in mind that only one Logstash instance can write to `path.queue`. You
+Keep in mind that only one {ls} instance can write to `path.queue`. You
 cannot have the original instance and the new instance writing to the queue at
 the same time.
 
diff --git a/docs/static/releasenotes.asciidoc b/docs/static/releasenotes.asciidoc
index 762d2814591..80b199de862 100644
--- a/docs/static/releasenotes.asciidoc
+++ b/docs/static/releasenotes.asciidoc
@@ -3,46 +3,46 @@
 
 This section summarizes the changes in the following releases:
 
-* <<logstash-8-8-2,Logstash 8.8.2>>
-* <<logstash-8-8-1,Logstash 8.8.1>>
-* <<logstash-8-8-0,Logstash 8.8.0>>
-* <<logstash-8-7-1,Logstash 8.7.1>>
-* <<logstash-8-7-0,Logstash 8.7.0>>
-* <<logstash-8-6-2,Logstash 8.6.2>>
-* <<logstash-8-6-1,Logstash 8.6.1>>
-* <<logstash-8-6-0,Logstash 8.6.0>>
-* <<logstash-8-5-3,Logstash 8.5.3>>
-* <<logstash-8-5-2,Logstash 8.5.2>>
-* <<logstash-8-5-1,Logstash 8.5.1>>
-* <<logstash-8-5-0,Logstash 8.5.0>>
-* <<logstash-8-4-2,Logstash 8.4.2>>
-* <<logstash-8-4-1,Logstash 8.4.1>>
-* <<logstash-8-4-0,Logstash 8.4.0>>
-* <<logstash-8-3-3,Logstash 8.3.3>>
-* <<logstash-8-3-2,Logstash 8.3.2>>
-* <<logstash-8-3-1,Logstash 8.3.1>>
-* <<logstash-8-3-0,Logstash 8.3.0>>
-* <<logstash-8-2-3,Logstash 8.2.3>>
-* <<logstash-8-2-2,Logstash 8.2.2>>
-* <<logstash-8-2-1,Logstash 8.2.1>>
-* <<logstash-8-2-0,Logstash 8.2.0>>
-* <<logstash-8-1-3,Logstash 8.1.3>>
-* <<logstash-8-1-2,Logstash 8.1.2>>
-* <<logstash-8-1-1,Logstash 8.1.1>>
-* <<logstash-8-1-0,Logstash 8.1.0>>
-* <<logstash-8-0-1,Logstash 8.0.1>>
-* <<logstash-8-0-0,Logstash 8.0.0>>
-* <<logstash-8-0-0-rc2,Logstash 8.0.0-rc2>>
-* <<logstash-8-0-0-rc1,Logstash 8.0.0-rc1>>
-* <<logstash-8-0-0-beta1,Logstash 8.0.0-beta1>>
-* <<logstash-8-0-0-alpha2,Logstash 8.0.0-alpha2>>
-* <<logstash-8-0-0-alpha1,Logstash 8.0.0-alpha1>>
+* <<logstash-8-8-2,{ls} 8.8.2>>
+* <<logstash-8-8-1,{ls} 8.8.1>>
+* <<logstash-8-8-0,{ls} 8.8.0>>
+* <<logstash-8-7-1,{ls} 8.7.1>>
+* <<logstash-8-7-0,{ls} 8.7.0>>
+* <<logstash-8-6-2,{ls} 8.6.2>>
+* <<logstash-8-6-1,{ls} 8.6.1>>
+* <<logstash-8-6-0,{ls} 8.6.0>>
+* <<logstash-8-5-3,{ls} 8.5.3>>
+* <<logstash-8-5-2,{ls} 8.5.2>>
+* <<logstash-8-5-1,{ls} 8.5.1>>
+* <<logstash-8-5-0,{ls} 8.5.0>>
+* <<logstash-8-4-2,{ls} 8.4.2>>
+* <<logstash-8-4-1,{ls} 8.4.1>>
+* <<logstash-8-4-0,{ls} 8.4.0>>
+* <<logstash-8-3-3,{ls} 8.3.3>>
+* <<logstash-8-3-2,{ls} 8.3.2>>
+* <<logstash-8-3-1,{ls} 8.3.1>>
+* <<logstash-8-3-0,{ls} 8.3.0>>
+* <<logstash-8-2-3,{ls} 8.2.3>>
+* <<logstash-8-2-2,{ls} 8.2.2>>
+* <<logstash-8-2-1,{ls} 8.2.1>>
+* <<logstash-8-2-0,{ls} 8.2.0>>
+* <<logstash-8-1-3,{ls} 8.1.3>>
+* <<logstash-8-1-2,{ls} 8.1.2>>
+* <<logstash-8-1-1,{ls} 8.1.1>>
+* <<logstash-8-1-0,{ls} 8.1.0>>
+* <<logstash-8-0-1,{ls} 8.0.1>>
+* <<logstash-8-0-0,{ls} 8.0.0>>
+* <<logstash-8-0-0-rc2,{ls} 8.0.0-rc2>>
+* <<logstash-8-0-0-rc1,{ls} 8.0.0-rc1>>
+* <<logstash-8-0-0-beta1,{ls} 8.0.0-beta1>>
+* <<logstash-8-0-0-alpha2,{ls} 8.0.0-alpha2>>
+* <<logstash-8-0-0-alpha1,{ls} 8.0.0-alpha1>>
 
 
 [[logstash-8-8-2]]
-=== Logstash 8.8.2 Release Notes
+=== {ls} 8.8.2 Release Notes
 
-No user-facing changes in Logstash core.
+No user-facing changes in {ls} core.
 
 ==== Plugins
 
@@ -73,11 +73,11 @@ No user-facing changes in Logstash core.
 
 *Elasticsearch Output - 11.15.8*
 
-* Fix a regression introduced in 11.14.0 which could prevent Logstash 8.8 from establishing a connection to Elasticsearch for Central Management and Monitoring core features https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/1141[#1141]
+* Fix a regression introduced in 11.14.0 which could prevent {ls} 8.8 from establishing a connection to Elasticsearch for Central Management and Monitoring core features https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/1141[#1141]
 
 
 [[logstash-8-8-1]]
-=== Logstash 8.8.1 Release Notes
+=== {ls} 8.8.1 Release Notes
 
 * Remove obsolete notice when using plugins with version < 1.0.0 https://github.com/elastic/logstash/pull/15077[#15077]
 * Docs: Add instructions to verify Docker install images https://github.com/elastic/logstash/pull/15064[#15064]
@@ -158,14 +158,14 @@ No user-facing changes in Logstash core.
 
 
 [[logstash-8-8-0]]
-=== Logstash 8.8.0 Release Notes
+=== {ls} 8.8.0 Release Notes
 
 [[known-issues-8.8.0]]
 ==== Known issues
 
-Logstash 8.8.0 may fail to start when SSL/TLS is enabled
+{ls} 8.8.0 may fail to start when SSL/TLS is enabled
 in monitoring and/or central management, due to a change introduced in version 11.14.0 of the https://github.com/logstash-plugins/logstash-output-elasticsearch[logstash-output-elasticsearch] plugin. 
-When impacted by this issue, Logstash fails to start and logs an error similar to the following:
+When impacted by this issue, {ls} fails to start and logs an error similar to the following:
 
 ```
 [logstash.licensechecker.licensereader] Failed to perform request {:message=>"PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target", :exception=>Manticore::ClientProtocolException, :cause=>#<Java::JavaxNetSsl::SSLHandshakeException: PKIX path building failed: sun.security.provider.certpath.SunCertPathBuilderException: unable to find valid certification path to requested target>}
@@ -199,14 +199,14 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 [[notable-8.8.0]]
 ==== Notable issues fixed
 
-* Fix a race condition that prevents Logstash from updating a pipeline's configuration with in-flight events
+* Fix a race condition that prevents {ls} from updating a pipeline's configuration with in-flight events
   experiencing connection errors. https://github.com/elastic/logstash/issues/14739[#14739]
   This issue primarily manifests following the update of Elasticsearch credentials through Central Management,
   after credentials expired while events were in-flight. It causes the Elasticsearch Output to get stuck attempting
   to send events with the expired credentials instead of using the updated ones.
-  To address this problem, Logstash has improved the pipeline shutdown phase functionality to allow an output plugin
+  To address this problem, {ls} has improved the pipeline shutdown phase functionality to allow an output plugin
   to request the termination of the in-flight batch of events; hence preventing the need for administrators
-  to manually restart Logstash. Furthermore, when used in combination with a persistent queue to prevent data loss,
+  to manually restart {ls}. Furthermore, when used in combination with a persistent queue to prevent data loss,
   the batch is eligible for reprocessing on pipeline restart.
   Plugin developers can now decide whether to make use of such functionality on output plugins. https://github.com/elastic/logstash/pull/14940[#14940]
 
@@ -219,7 +219,7 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 
 *Elasticsearch Filter - 3.15.0*
 
-* Standardize SSL settings to comply with Logstash's naming convention https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/168[#168]
+* Standardize SSL settings to comply with {ls}'s naming convention https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/168[#168]
 
 * Added support for configurable retries with new `retry_on_failure` and `retry_on_status` options https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/160[#160]
 
@@ -229,22 +229,22 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 
 *Beats Input - 6.6.0*
 
-* Standardize SSL settings to comply with Logstash's naming convention https://github.com/logstash-plugins/logstash-input-beats/pull/470[#470]
+* Standardize SSL settings to comply with {ls}'s naming convention https://github.com/logstash-plugins/logstash-input-beats/pull/470[#470]
 
 *Elasticsearch Input - 4.17.0*
 
-* Standardize SSL settings to comply with Logstash's naming convention https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/185[#185]
+* Standardize SSL settings to comply with {ls}'s naming convention https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/185[#185]
 
 *Http Input - 3.7.0*
 
-* Standardize SSL settings to comply with Logstash's naming convention https://github.com/logstash-plugins/logstash-input-http/pull/165[#165]
+* Standardize SSL settings to comply with {ls}'s naming convention https://github.com/logstash-plugins/logstash-input-http/pull/165[#165]
 
 *Kafka Integration - 11.2.1*
 
 * Fix nil exception to empty headers of record during event metadata assignment https://github.com/logstash-plugins/logstash-integration-kafka/pull/140[#140]
 * Added TLS truststore and keystore settings specifically to access the schema registry https://github.com/logstash-plugins/logstash-integration-kafka/pull/137[#137]
 * Added config `group_instance_id` to use the Kafka's consumer static membership feature https://github.com/logstash-plugins/logstash-integration-kafka/pull/135[#135]
-* Changed Kafka client to 3.3.1, requires Logstash >= 8.3.0. 
+* Changed Kafka client to 3.3.1, requires {ls} >= 8.3.0. 
 * Deprecated `default` value for setting `client_dns_lookup` forcing to `use_all_dns_ips` when explicitly used https://github.com/logstash-plugins/logstash-integration-kafka/pull/130[#130]
 * Changed the consumer's poll from using the one that blocks on metadata retrieval to the one that doesn't https://github.com/logstash-plugins/logstash-integration-kafka/pull/133[#136]
 
@@ -254,10 +254,10 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 
 * Fixed race condition during plugin registration phase https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1125[#1125]
 * Added the ability to negatively acknowledge the batch under processing if the plugin is blocked in a retry-error-loop and a shutdown is requested. https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1119[#1119]
-* Standardize SSL settings to comply with Logstash's naming convention https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1118[#1118]
+* Standardize SSL settings to comply with {ls}'s naming convention https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1118[#1118]
 
 [[logstash-8-7-1]]
-=== Logstash 8.7.1 Release Notes
+=== {ls} 8.7.1 Release Notes
 
 [[notable-8.7.1]]
 ==== Performance improvements and notable issues fixed
@@ -296,7 +296,7 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 * Fix failure to load Java dependencies making v7.1.0 unusable https://github.com/logstash-plugins/logstash-integration-aws/pull/24[#24]
 
 [[logstash-8-7-0]]
-=== Logstash 8.7.0 Release Notes
+=== {ls} 8.7.0 Release Notes
 
 [[features-8.7.0]]
 ==== New features and enhancements
@@ -308,12 +308,12 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 ==== Performance improvements and notable issues fixed
 
 * Fix: DLQ writer isn't properly created due to inversion of parameteres https://github.com/elastic/logstash/pull/14900[#14900]
-* Logstash fails to start on OracleLinux7 https://github.com/elastic/logstash/pull/14890[#14890]
+* {ls} fails to start on OracleLinux7 https://github.com/elastic/logstash/pull/14890[#14890]
 * Fix: DLQ age policy isn't executed if the current head segment haven't receives any write https://github.com/elastic/logstash/pull/14878[#14878]
 * Fixes an issue during process shutdown in which the stalled shutdown watcher incorrectly reports `inflight_count` as `0` even when there are events in-flight https://github.com/elastic/logstash/pull/14760[#14760]
 
 [[core-8.7.0]]
-==== Other changes to Logstash core
+==== Other changes to {ls} core
 
 * Allow `dead_letter_queue.retain.age` usage in pipeline settings https://github.com/elastic/logstash/pull/14954[#14954]
 * Improved logging behavior in a docker container https://github.com/elastic/logstash/pull/14949[#14949]
@@ -328,7 +328,7 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 [[docs-8.7.0]]
 ==== Documentation enhancements
 
-* Describe how to use Elastic Agent to monitor Logstash https://github.com/elastic/logstash/pull/14959[#14959]
+* Describe how to use Elastic Agent to monitor {ls} https://github.com/elastic/logstash/pull/14959[#14959]
 * Update Debian/Ubuntu instructions following apt-key deprecation https://github.com/elastic/logstash/pull/14835[#14835]
 
 [[plugins-8.7.0]]
@@ -357,7 +357,7 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 * Technology preview support for allowing events to individually encode a default pipeline with `[@metadata][target_ingest_pipeline]` (as part of a technology preview, this feature may change without notice) https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1113[#1113]
 
 [[logstash-8-6-2]]
-=== Logstash 8.6.2 Release Notes
+=== {ls} 8.6.2 Release Notes
 
 [[dependencies-8.6.2]]
 ==== Updates to dependencies
@@ -379,14 +379,14 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 * Changed `manage_template` default value to `false` when data streams is enabled https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1111[#1111]
 
 [[logstash-8-6-1]]
-=== Logstash 8.6.1 Release Notes
+=== {ls} 8.6.1 Release Notes
 
 [[dependencies-8.6.1]]
 ==== Updates to dependencies
 * Updated snakeyaml to 1.33 https://github.com/elastic/logstash/pull/14848[#14848]
 
 [[logstash-8-6-0]]
-=== Logstash 8.6.0 Release Notes
+=== {ls} 8.6.0 Release Notes
 
 [[features-8.6.0]]
 ==== New features and enhancements
@@ -394,7 +394,7 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 * Extends the flow rates introduced to the Node Stats API in 8.5.0 (which included windows for `current` and `lifetime`)
   to include a Technology Preview of several additional windows such as `last_15_minutes`, `last_24_hours`, etc..
   https://github.com/elastic/logstash/pull/14571[#14571]
-* Logstash introduced instance and pipeline level flow metrics, `growth_bytes` and `growth_events` for persisted queue
+* {ls} introduced instance and pipeline level flow metrics, `growth_bytes` and `growth_events` for persisted queue
   to provide a better visibility about how fast pipeline queue is growing.
   https://github.com/elastic/logstash/pull/14554[#14554]
 
@@ -406,11 +406,11 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 
 [[docs-8.6.0]]
 ==== Documentation enhancements
-* Crafted a guide on how to configure and troubleshooting Logstash on Kubernetes.
+* Crafted a guide on how to configure and troubleshooting {ls} on Kubernetes.
  ** Getting started https://github.com/elastic/logstash/pull/14655[#14655]
  ** Persistent Storage https://github.com/elastic/logstash/pull/14714[#14714]
  ** Stack Monitoring https://github.com/elastic/logstash/pull/14696[#14696]
- ** Securing Logstash https://github.com/elastic/logstash/pull/14737[#14737]
+ ** Securing {ls} https://github.com/elastic/logstash/pull/14737[#14737]
 
 [[plugins-8.6.0]]
 ==== Plugin releases
@@ -442,27 +442,27 @@ xpack.management.elasticsearch.ssl.ca_trusted_fingerprint: "<value>"
 * Feature: expose `dlq_routed` document metric to track the documents routed into DLQ https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1090[#1090]
 
 [[logstash-8-5-3]]
-=== Logstash 8.5.3 Release Notes
+=== {ls} 8.5.3 Release Notes
 
-No user-facing changes in Logstash core.
+No user-facing changes in {ls} core.
 
 [[plugins-8-5-3]]
 ==== Plugins
 
-No user-facing changes in Logstash plugins.
+No user-facing changes in {ls} plugins.
 
 [[logstash-8-5-2]]
-=== Logstash 8.5.2 Release Notes
+=== {ls} 8.5.2 Release Notes
 
-No user-facing changes in Logstash core.
+No user-facing changes in {ls} core.
 
 [[plugins-8-5-2]]
 ==== Plugins
 
-No user-facing changes in Logstash plugins.
+No user-facing changes in {ls} plugins.
 
 [[logstash-8-5-1]]
-=== Logstash 8.5.1 Release Notes
+=== {ls} 8.5.1 Release Notes
 
 [[notable-8.5.1]]
 ==== Notable issues fixed
@@ -507,22 +507,22 @@ No user-facing changes in Logstash plugins.
 
 * DOC: clarify that `http_compression` option only affects _requests_; compressed _responses_ have always been read independent of this setting https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1030[#1030]
 
-* Fix broken link to Logstash Reference https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1085[#1085]
+* Fix broken link to {ls} Reference https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1085[#1085]
 
 * Fixes a possible infinite-retry-loop that could occur when this plugin is configured with an `action` whose value contains a <<sprintf,sprintf-style placeholder>> that fails to be resolved for an individual event.
 Events in this state are routed to the pipeline's <<dead-letter-queues,dead letter queue (DLQ)>> if the DLQ is enabled.
 Otherwise, these events are logged-and-dropped so that the remaining events in the batch can be processed. https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1080[#1080]
 
 [[logstash-8-5-0]]
-=== Logstash 8.5.0 Release Notes
+=== {ls} 8.5.0 Release Notes
 
 [[known-issues-8.5.0]]
 ==== Known issues
 
 Due to a recent change in the Red Hat scan verification process,
-this version of Logstash is not available in the Red Hat Ecosystem Catalog.
+this version of {ls} is not available in the Red Hat Ecosystem Catalog.
 This bug will be fixed in the next release.
-Please use the https://www.docker.elastic.co/r/logstash/logstash[Elastic docker registry] to download the 8.5.0 Logstash image.
+Please use the https://www.docker.elastic.co/r/logstash/logstash[Elastic docker registry] to download the 8.5.0 {ls} image.
 
 [[features-8.5.0]]
 ==== New features and enhancements
@@ -538,7 +538,7 @@ Please use the https://www.docker.elastic.co/r/logstash/logstash[Elastic docker
 [[docs-8.5.0]]
 ==== Documentation Improvements and Fixes
 
-* Add missing reference to full config of Logstash to Logstash over HTTP https://github.com/elastic/logstash/pull/14466[#14466]
+* Add missing reference to full config of {ls} to {ls} over HTTP https://github.com/elastic/logstash/pull/14466[#14466]
 * Describe DLQ's age retention policy https://github.com/elastic/logstash/pull/14340[#14340]
 * Document the cleaning of consumed events from DLQ https://github.com/elastic/logstash/pull/14341[#14341]
 
@@ -560,7 +560,7 @@ Please use the https://www.docker.elastic.co/r/logstash/logstash[Elastic docker
 
 *File Input - 4.4.4*
 
-* Fixes gzip file handling in read mode when run on JDK12+, including JDK17 that is bundled with Logstash 8.4+ https://github.com/logstash-plugins/logstash-input-file/pull/312[#312]
+* Fixes gzip file handling in read mode when run on JDK12+, including JDK17 that is bundled with {ls} 8.4+ https://github.com/logstash-plugins/logstash-input-file/pull/312[#312]
 
 *Http_poller Input - 5.4.0*
 
@@ -573,25 +573,25 @@ Please use the https://www.docker.elastic.co/r/logstash/logstash[Elastic docker
 * Feature: deprecates the `failure_type_logging_whitelist` configuration option, renaming it `silence_errors_in_log` https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1068[#1068]
 
 [[logstash-8-4-2]]
-=== Logstash 8.4.2 Release Notes
+=== {ls} 8.4.2 Release Notes
 
 [[notable-8.4.2]]
 ==== Notable issues fixed
 
 * Fixed the inability to configure "monitoring.cluster_uuid" in docker https://github.com/elastic/logstash/pull/14496[#14496]
 * Disabled DES-CBC3-SHA cipher in some plugins that still supported it https://github.com/elastic/logstash/pull/14501[#14501]
-* Upgraded JRuby the CSV gem to fix a thread leak in Logstash 8.4.0 when using the CSV filter https://github.com/elastic/logstash/pull/14508[#14508] https://github.com/elastic/logstash/pull/14526[#14526]
-* Fixed Windows .bat scripts that prevented the use of the Plugin Manager and Keystore in Logstash 8.3.3/8.4.0 https://github.com/elastic/logstash/pull/14516[#14516]
+* Upgraded JRuby the CSV gem to fix a thread leak in {ls} 8.4.0 when using the CSV filter https://github.com/elastic/logstash/pull/14508[#14508] https://github.com/elastic/logstash/pull/14526[#14526]
+* Fixed Windows .bat scripts that prevented the use of the Plugin Manager and Keystore in {ls} 8.3.3/8.4.0 https://github.com/elastic/logstash/pull/14516[#14516]
 
 [[docs-8-4-2]]
 ==== Documentation improvements
 
-* Added https://www.elastic.co/guide/en/logstash/8.4/winlogbeat-modules.html[documentation for using Winlogbeat] with Logstash https://github.com/elastic/logstash/pull/14512[#14512]
+* Added https://www.elastic.co/guide/en/logstash/8.4/winlogbeat-modules.html[documentation for using Winlogbeat] with {ls} https://github.com/elastic/logstash/pull/14512[#14512]
 
 [[logstash-8-4-1]]
-=== Logstash 8.4.1 Release Notes
+=== {ls} 8.4.1 Release Notes
 
-No user-facing changes in Logstash core.
+No user-facing changes in {ls} core.
 
 ==== Plugins
 
@@ -609,7 +609,7 @@ No user-facing changes in Logstash core.
 
 
 [[logstash-8-4-0]]
-=== Logstash 8.4.0 Release Notes
+=== {ls} 8.4.0 Release Notes
 
 
 [[features-8.4.0]]
@@ -621,7 +621,7 @@ No user-facing changes in Logstash core.
 This release brings significant improvements to help users manage their dead letter queues, including:
 
 * A new `clean_consumed` option on the Dead Letter Queue input plugin.
-It can automatically delete segments from a dead letter queue after all events in the segment have been consumed by a Logstash pipeline.
+It can automatically delete segments from a dead letter queue after all events in the segment have been consumed by a {ls} pipeline.
 * A new age retention policy, enabling the automatic removal of segments from a dead letter queue
 based on the age of events within those segments.
 * Additional dead letter queue metrics available from the monitoring API https://github.com/elastic/logstash/pull/14324[#14324]
@@ -635,21 +635,21 @@ plugins. They all use version 3 of the AWS Ruby SDK.
 [[jdk-8.4.0]]
 ===== JDK17 support
 
-Logstash now comes bundled with JDK17, while still providing compatibility with user-supplied JDK11.
+{ls} now comes bundled with JDK17, while still providing compatibility with user-supplied JDK11.
 The new JDK includes an update pertaining to a potential security vulnerability.
 Please see our link:https://discuss.elastic.co/c/announcements/security-announcements/31[security statement for details].
 
 [[m1-8.4.0]]
-===== Logstash M1 download
+===== {ls} M1 download
 
-Logstash is now available for download on M1 equipped MacOS devices, and comes bundled with M1 native JDK17.
+{ls} is now available for download on M1 equipped MacOS devices, and comes bundled with M1 native JDK17.
 
 [[notable-8.4.0]]
 ==== Notable issues fixed
 
-* Remove `/etc/systemd/system/logstash.service` only when file is installed by Logstash https://github.com/elastic/logstash/pull/14200[#14200]
+* Remove `/etc/systemd/system/logstash.service` only when file is installed by {ls} https://github.com/elastic/logstash/pull/14200[#14200]
 * Fix Arcsight module compatibility with Elasticsearch `8.x` https://github.com/elastic/logstash/pull/13874[#13874]
-* Ensure that timestamp values are serialized with a minimum of 3 decimal places to guarantee that millisecond precision timestamps match those from Logstash `7.x` https://github.com/elastic/logstash/pull/14299[#14299]
+* Ensure that timestamp values are serialized with a minimum of 3 decimal places to guarantee that millisecond precision timestamps match those from {ls} `7.x` https://github.com/elastic/logstash/pull/14299[#14299]
 * Fix issue with native Java plugin thread-safety and concurrency https://github.com/elastic/logstash/pull/14360[#14360]
 * Allow the ability to use Ruby codecs inside native Java plugins https://github.com/elastic/logstash/pull/13523[#13523]
 
@@ -666,7 +666,7 @@ Logstash is now available for download on M1 equipped MacOS devices, and comes b
 
 *Dead Letter Queue Input - 2.0.0*
 
-* Introduce the boolean `clean_consumed` setting to enable the automatic removal of completely consumed segments. Requires Logstash 8.4.0 or above https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/43[#43]
+* Introduce the boolean `clean_consumed` setting to enable the automatic removal of completely consumed segments. Requires {ls} 8.4.0 or above https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/43[#43]
 * Expose metrics about segments and events cleaned by this plugin https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/45[#45]
 
 *Xml Filter - 4.2.0*
@@ -688,7 +688,7 @@ individual plugins:
 aws `3.x` gems.
 
 [[logstash-8-3-3]]
-=== Logstash 8.3.3 Release Notes
+=== {ls} 8.3.3 Release Notes
 
 [[notable-8.3.3]]
 ==== Notable issue fixed
@@ -722,29 +722,29 @@ aws `3.x` gems.
 
 
 [[logstash-8-3-2]]
-=== Logstash 8.3.2 Release Notes
+=== {ls} 8.3.2 Release Notes
 
 No user-facing changes in this release.
 
 [[logstash-8-3-1]]
-=== Logstash 8.3.1 Release Notes
+=== {ls} 8.3.1 Release Notes
 
 [[notable-8.3.1]]
 ==== Notable issues fixed
 
 * We fixed an event serializing incompatibility introduced in 8.3.0 https://github.com/elastic/logstash/pull/14314[#14314]
-  If you're using dead letter queues or persistent queues we recommend that you do not use Logstash 8.3.0 and upgrade to 8.3.1.
+  If you're using dead letter queues or persistent queues we recommend that you do not use {ls} 8.3.0 and upgrade to 8.3.1.
 
 [[logstash-8-3-0]]
-=== Logstash 8.3.0 Release Notes
+=== {ls} 8.3.0 Release Notes
 
 [[known-issue-8-3-0]]
 ==== Known issue
 
 An event serialization bug was discovered, which causes an issue when trying to read dead letter or persistent queues created
-in previous versions of Logstash.
+in previous versions of {ls}.
 
-We recommend not upgrading to Logstash 8.3.0 if you are using dead letter or persistent queues.
+We recommend not upgrading to {ls} 8.3.0 if you are using dead letter or persistent queues.
 
 [[features-8.3.0]]
 ==== New features and enhancements
@@ -757,13 +757,13 @@ These improvements dramatically decrease network load while also giving users th
 
 * Dead Letter Queues can now be configured to drop older events instead of new ones when they're full. The setting "dead_letter_queue.storage_policy" has been introduced for this purpose, and new metrics - such as a counter for dropped events - are now exposed in the API to better monitor the DLQ behavior. https://github.com/elastic/logstash/pull/13923[#13923] https://github.com/elastic/logstash/pull/14058[#14058]
 
-* To improve security of Logstash deployments, 8.3.0 brings a new setting "allow_superuser" that defaults to false. When enabled it prevents Logstash from starting as super user ("root"). This setting will be enabled by default in the future. Consider explicitly enabling it. Otherwise a deprecation log entry will be emitted. https://github.com/elastic/logstash/pull/14046[#14046] https://github.com/elastic/logstash/pull/14089[#14089]
+* To improve security of {ls} deployments, 8.3.0 brings a new setting "allow_superuser" that defaults to false. When enabled it prevents {ls} from starting as super user ("root"). This setting will be enabled by default in the future. Consider explicitly enabling it. Otherwise a deprecation log entry will be emitted. https://github.com/elastic/logstash/pull/14046[#14046] https://github.com/elastic/logstash/pull/14089[#14089]
 
-* Continuing with the focus on security, we've introduced "api.auth.basic.password_policy.mode" to ensure the password used to guard Logstash's HTTP API has a minimum set of strength requirements. By default a warning will be emitted if the defined password doesn't meet the criteria, but in a future release the mode will be set to "ERROR". https://github.com/elastic/logstash/pull/14045[#14045] https://github.com/elastic/logstash/pull/14105[#14105] https://github.com/elastic/logstash/pull/14159[#14159]
+* Continuing with the focus on security, we've introduced "api.auth.basic.password_policy.mode" to ensure the password used to guard {ls}'s HTTP API has a minimum set of strength requirements. By default a warning will be emitted if the defined password doesn't meet the criteria, but in a future release the mode will be set to "ERROR". https://github.com/elastic/logstash/pull/14045[#14045] https://github.com/elastic/logstash/pull/14105[#14105] https://github.com/elastic/logstash/pull/14159[#14159]
 
-* Elasticsearch introduced "security on by default" back in 8.0.0, with TLS enabled by default in its HTTP and transport layers. To facilitate connecting to 8.x clusters, Elasticsearch displays the fingerprint of the Certificate Authority it generates on startup. This release of Logstash introduces support for setting "ca_trusted_fingerprint" in Elasticsearch input, filter and outputs plugins. https://github.com/elastic/logstash/pull/14120[#14120]
+* Elasticsearch introduced "security on by default" back in 8.0.0, with TLS enabled by default in its HTTP and transport layers. To facilitate connecting to 8.x clusters, Elasticsearch displays the fingerprint of the Certificate Authority it generates on startup. This release of {ls} introduces support for setting "ca_trusted_fingerprint" in Elasticsearch input, filter and outputs plugins. https://github.com/elastic/logstash/pull/14120[#14120]
 
-* Technical Preview: Receiving events containing keys with characters that have special meaning to Logstash such as `[` and `]` (for field references) has always causes issues to data ingestion. A new setting in Technical Preview, disabled by default, called "config.field_reference.escape_style" was introduced to handle such special characters by escaping them. https://github.com/elastic/logstash/pull/14044[#14044]
+* Technical Preview: Receiving events containing keys with characters that have special meaning to {ls} such as `[` and `]` (for field references) has always causes issues to data ingestion. A new setting in Technical Preview, disabled by default, called "config.field_reference.escape_style" was introduced to handle such special characters by escaping them. https://github.com/elastic/logstash/pull/14044[#14044]
 
 [[notable-8.3.0]]
 ==== Notable issues fixed
@@ -773,7 +773,7 @@ These improvements dramatically decrease network load while also giving users th
 * Add thread safety around Puma startup/shutdown https://github.com/elastic/logstash/pull/14080[#14080]
 * Add value converters for java.time classes https://github.com/elastic/logstash/pull/13972[#13972]
 * Correct the class reference to the MetricNotFound exception https://github.com/elastic/logstash/pull/13970[#13970]
-* Fix a possible corruption of Persistent Queue during a crash of the Logstash process https://github.com/elastic/logstash/pull/14165[#14165]
+* Fix a possible corruption of Persistent Queue during a crash of the {ls} process https://github.com/elastic/logstash/pull/14165[#14165]
 
 [[dependencies-8.3.0]]
 ==== Updates to dependencies
@@ -790,7 +790,7 @@ These improvements dramatically decrease network load while also giving users th
 
 *Elasticsearch Filter - 3.12.0*
 
-* Add support for `ca_trusted_fingerprint` when run on Logstash 8.3+ https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/158[#158]
+* Add support for `ca_trusted_fingerprint` when run on {ls} 8.3+ https://github.com/logstash-plugins/logstash-filter-elasticsearch/pull/158[#158]
 
 *Fingerprint Filter - 3.4.0*
 
@@ -807,7 +807,7 @@ These improvements dramatically decrease network load while also giving users th
 *Elasticsearch Input - 4.14.0*
 
 * Refactor: switch to using scheduler mixin https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/177[#177]
-* Add support for `ca_trusted_fingerprint` when run on Logstash 8.3+ https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/178[#178]
+* Add support for `ca_trusted_fingerprint` when run on {ls} 8.3+ https://github.com/logstash-plugins/logstash-input-elasticsearch/pull/178[#178]
 
 *Http Input - 3.6.0*
 
@@ -815,7 +815,7 @@ These improvements dramatically decrease network load while also giving users th
 
 *Jms Input - 3.2.2*
 
-* Fix: Remove usage of `java_kind_of?` to allow this plugin to be supported for versions of Logstash using jruby-9.3.x
+* Fix: Remove usage of `java_kind_of?` to allow this plugin to be supported for versions of {ls} using jruby-9.3.x
  https://github.com/logstash-plugins/logstash-input-jms/pull/54[#54]
 
 *S3 Input - 3.8.4*
@@ -848,7 +848,7 @@ These improvements dramatically decrease network load while also giving users th
 
 *Elasticsearch Output - 11.6.0*
 
-* Add support for `ca_trusted_fingerprint` when run on Logstash 8.3+ https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1074[#1074]
+* Add support for `ca_trusted_fingerprint` when run on {ls} 8.3+ https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1074[#1074]
 * Feat: add ssl_supported_protocols option https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1055[#1055]
 * [DOC] Add `v8` to supported values for ecs_compatiblity defaults https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1059[#1059]
 
@@ -863,12 +863,12 @@ These improvements dramatically decrease network load while also giving users th
 * Fix: close server and client sockets on plugin close
 
 [[logstash-8-2-3]]
-=== Logstash 8.2.3 Release Notes
+=== {ls} 8.2.3 Release Notes
 
 * Updated bundled JDK to 11.0.15+10 https://github.com/elastic/logstash/pull/14152[#14152]
 
 [[logstash-8-2-2]]
-=== Logstash 8.2.2 Release Notes
+=== {ls} 8.2.2 Release Notes
 
 [[notable-8.2.2]]
 ==== Notable issues fixed
@@ -876,7 +876,7 @@ These improvements dramatically decrease network load while also giving users th
 * Avoid unnecessary thread synchronization when the Persistent Queue is full https://github.com/elastic/logstash/pull/14141[#14141]
 
 [[logstash-8-2-1]]
-=== Logstash 8.2.1 Release Notes
+=== {ls} 8.2.1 Release Notes
 
 [[notable-8.2.1]]
 ==== Notable issues fixed
@@ -939,11 +939,11 @@ https://github.com/elastic/logstash/pull/13935[#13935]
 
 
 [[logstash-8-2-0]]
-=== Logstash 8.2.0 Release Notes
+=== {ls} 8.2.0 Release Notes
 
 ==== Breaking changes
 
-* Starting with Logstash 8.0 all supported and tested operating systems use system.d so this release removes leftover SysVinit scripts from .deb and .rpm packages https://github.com/elastic/logstash/pull/13954[#13954] https://github.com/elastic/logstash/pull/13955[#13955]
+* Starting with {ls} 8.0 all supported and tested operating systems use system.d so this release removes leftover SysVinit scripts from .deb and .rpm packages https://github.com/elastic/logstash/pull/13954[#13954] https://github.com/elastic/logstash/pull/13955[#13955]
 
 [[notable-8.2.0]]
 ==== Notable issues fixed
@@ -953,7 +953,7 @@ https://github.com/elastic/logstash/pull/13935[#13935]
 * Print bundled JDK's version in launch scripts when `LS_JAVA_HOME` is provided https://github.com/elastic/logstash/pull/13880[#13880]
 * Updated jackson-databind to 2.13.2 in ingest-converter tool https://github.com/elastic/logstash/pull/13900[#13900]
 * Updated google-java-format dependency to 1.13.0 and guava to 31.0.1 in core https://github.com/elastic/logstash/pull/13700[#13700]
-* Multiple documentation improvements related to: Logstash to Logstash communication https://github.com/elastic/logstash/pull/13999[#13999], docker variable injection https://github.com/elastic/logstash/pull/12198[#12198], LS-ES security configuration https://github.com/elastic/logstash/pull/14012[#14012], JDK 11 Bundling https://github.com/elastic/logstash/pull/14022[#14022], and other overall documentation restructuring https://github.com/elastic/logstash/pull/14015[#14015].
+* Multiple documentation improvements related to: {ls} to {ls} communication https://github.com/elastic/logstash/pull/13999[#13999], docker variable injection https://github.com/elastic/logstash/pull/12198[#12198], LS-ES security configuration https://github.com/elastic/logstash/pull/14012[#14012], JDK 11 Bundling https://github.com/elastic/logstash/pull/14022[#14022], and other overall documentation restructuring https://github.com/elastic/logstash/pull/14015[#14015].
 
 
 ==== Plugins
@@ -997,22 +997,22 @@ https://github.com/elastic/logstash/pull/13935[#13935]
 *Http Output - 5.5.0*
 
 * Feat: added `ssl_supported_protocols` option https://github.com/logstash-plugins/logstash-output-http/pull/131[#131]
-* Fix retry indefinitely in termination process. This feature requires Logstash 8.1 https://github.com/logstash-plugins/logstash-output-http/pull/129[#129]
+* Fix retry indefinitely in termination process. This feature requires {ls} 8.1 https://github.com/logstash-plugins/logstash-output-http/pull/129[#129]
 * Docs: Add retry policy description https://github.com/logstash-plugins/logstash-output-http/pull/130[#130]
 * Introduce retryable unknown exceptions for "connection reset by peer" and "timeout" https://github.com/logstash-plugins/logstash-output-http/pull/127[#127]
 
 [[logstash-8-1-3]]
-=== Logstash 8.1.3 Release Notes
+=== {ls} 8.1.3 Release Notes
 
 No user-facing changes in this release.
 
 [[logstash-8-1-2]]
-=== Logstash 8.1.2 Release Notes
+=== {ls} 8.1.2 Release Notes
 
 [[notable-8.1.2]]
 ==== Notable issues fixed
 
-* Fixed issue where Logstash crashed if Central Management couldn't reach Elasticsearch https://github.com/elastic/logstash/pull/13689[#13689]
+* Fixed issue where {ls} crashed if Central Management couldn't reach Elasticsearch https://github.com/elastic/logstash/pull/13689[#13689]
 
 ==== Plugins
 
@@ -1033,14 +1033,14 @@ No user-facing changes in this release.
 * Fix: unable to start with password protected key https://github.com/logstash-plugins/logstash-output-tcp/pull/45[#45]
 
 [[logstash-8-1-1]]
-=== Logstash 8.1.1 Release Notes
+=== {ls} 8.1.1 Release Notes
 
 [[notable-8.1.1]]
 ==== Notable issues fixed
 
 * The `bin/logstash-plugin uninstall <plugin>` command works as expected, successfully uninstalling the specified plugin https://github.com/elastic/logstash/pull/13823[#13823]
-* Logstash CLI tools are now able to use the selected JDK on Windows https://github.com/elastic/logstash/pull/13839[#13839]
-* Logstash can successfully locate the Windows JVM, even if the path includes spaces https://github.com/elastic/logstash/pull/13881[#13881]
+* {ls} CLI tools are now able to use the selected JDK on Windows https://github.com/elastic/logstash/pull/13839[#13839]
+* {ls} can successfully locate the Windows JVM, even if the path includes spaces https://github.com/elastic/logstash/pull/13881[#13881]
 * The GeoIP database lookup will now respect a proxy defined with the http_proxy environment variable. https://github.com/elastic/logstash/pull/13840[#13840]
 
 ==== Updates to dependencies
@@ -1060,7 +1060,7 @@ No user-facing changes in this release.
 *Dead_letter_queue Input - 1.1.11*
 
 * Fix: pre-flight checks before creating DLQ reader https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/35[#35]
-* Fix: avoid Logstash crash on shutdown if DLQ files weren't created https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/33[#33]
+* Fix: avoid {ls} crash on shutdown if DLQ files weren't created https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/33[#33]
 
 *Elasticsearch Input - 4.12.2*
 
@@ -1083,7 +1083,7 @@ No user-facing changes in this release.
 
 
 [[logstash-8-1-0]]
-=== Logstash 8.1.0 Release Notes
+=== {ls} 8.1.0 Release Notes
 
 [[known-issue-8-1-0]]
 ==== Known issue
@@ -1095,7 +1095,7 @@ result in an error:
 Gem::LoadError: You have already activated jruby-openssl 0.12.2, but your Gemfile requires jruby-openssl 0.12.1. Prepending `bundle exec` to your command may solve this.
 ```
 
-Logstash should still run, and other plugin operations, such as `update` and `install`, should work as expected.
+{ls} should still run, and other plugin operations, such as `update` and `install`, should work as expected.
 
 NOTE: The `bin/logstash-plugin list` command may fail with the same error after a failed uninstallation.
 
@@ -1108,9 +1108,9 @@ The `filter-dissect` plugin has recent changes available for update.
 Running `bin/logstash-plugin update logstash-filter-dissect` should mitigate this issue.
 
 
-==== Logstash core 
+==== {ls} core 
 
-No user-facing changes in Logstash core.
+No user-facing changes in {ls} core.
 
 ==== Plugins
 
@@ -1146,7 +1146,7 @@ No user-facing changes in Logstash core.
 * Feat: support ssl_verification_mode option https://github.com/logstash-plugins/logstash-output-http/pull/126[#126]
 
 [[logstash-8-0-1]]
-=== Logstash 8.0.1 Release Notes
+=== {ls} 8.0.1 Release Notes
 
 [[notable-8.0.1]]
 ==== Notable issues fixed
@@ -1155,10 +1155,10 @@ No user-facing changes in Logstash core.
 https://github.com/elastic/logstash/pull/13727[#13727]
 
 * Recently, users running `bin/logstash-plugin` to install or update plugins stumbled upon an issue that would prevent
-Logstash from starting due a third-party dependency update. The dependency was pinned to an older version.
+{ls} from starting due a third-party dependency update. The dependency was pinned to an older version.
 https://github.com/elastic/logstash/issues/13777[#13777]
 
-* Logstash startup and the `pqrepair`/`pqcheck` tools have been improved to handle corrupted files in case of an
+* {ls} startup and the `pqrepair`/`pqcheck` tools have been improved to handle corrupted files in case of an
 unexpected shutdown. https://github.com/elastic/logstash/pull/13692[#13692] https://github.com/elastic/logstash/pull/13721[#13721]
 
 ==== Plugins
@@ -1177,7 +1177,7 @@ unexpected shutdown. https://github.com/elastic/logstash/pull/13692[#13692] http
 
 *Dead_letter_queue Input - 1.1.10*
 
-* Fix, avoid Logstash crash on shutdown if DLQ files weren't created https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/33[#33]
+* Fix, avoid {ls} crash on shutdown if DLQ files weren't created https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/33[#33]
 * Fix `@metadata` get overwritten by reestablishing metadata that stored in DLQ https://github.com/logstash-plugins/logstash-input-dead_letter_queue/pull/34[#34]
 
 *Tcp Input - 6.2.7*
@@ -1196,24 +1196,24 @@ unexpected shutdown. https://github.com/elastic/logstash/pull/13692[#13692] http
 * Fixed to use `reconnect_interval` when establish a connection
 
 [[logstash-8-0-0]]
-=== Logstash 8.0.0 Release Notes
+=== {ls} 8.0.0 Release Notes
 
 The following list are changes in 8.0.0 as compared to 7.17.0, and combines release notes from the 8.0.0-alpha1, -alpha2, -beta1, -rc1 and -rc2 releases.
 
 [[breaking-8.0.0]]
 ==== Breaking changes
 * Many plugins can now be run in a mode that avoids implicit conflict with the Elastic Common Schema (ECS).
-  This mode is controlled individually with each plugin’s ecs_compatibility option, which defaults to the value of the Logstash pipeline.ecs_compatibility setting.
-  In Logstash 8, this compatibility mode will be on-by-default for all pipelines.
-  If you wish to lock in a pipeline’s behavior from Logstash 7.x before upgrading to Logstash 8,
+  This mode is controlled individually with each plugin’s ecs_compatibility option, which defaults to the value of the {ls} pipeline.ecs_compatibility setting.
+  In {ls} 8, this compatibility mode will be on-by-default for all pipelines.
+  If you wish to lock in a pipeline’s behavior from {ls} 7.x before upgrading to {ls} 8,
   you can set `pipeline.ecs_compatibility: disabled` to its definition in `pipelines.yml` (or globally in `logstash.yml`).
-* Starting from Logstash 8.0, the minimum required version of Java to run Logstash is Java 11.
-  By default, Logstash will run with the bundled JDK, which has been verified to work with each specific version of Logstash,
+* Starting from {ls} 8.0, the minimum required version of Java to run {ls} is Java 11.
+  By default, {ls} will run with the bundled JDK, which has been verified to work with each specific version of {ls},
   and generally provides the best performance and reliability.
-* Support for using `JAVA_HOME` to override the path to the JDK that Logstash runs with has been removed for this release.
+* Support for using `JAVA_HOME` to override the path to the JDK that {ls} runs with has been removed for this release.
   In the `8.x` release, users should set the value of `LS_JAVA_HOME` to the path of their preferred JDK if they
   wish to use a version other than the bundled JDK. The value of `JAVA_HOME` will be ignored.
-* The Java Execution Engine has been the default engine since Logstash 7.0, and works with plugins written in either Ruby or Java.
+* The Java Execution Engine has been the default engine since {ls} 7.0, and works with plugins written in either Ruby or Java.
   Removal of the Ruby Execution Engine will not affect the ability to run existing pipelines. https://github.com/elastic/logstash/pull/12517[#12517]
 * We have added support for UTF-16 and other multi-byte-character when reading log files. https://github.com/elastic/logstash/pull/9702[#9702]
 * Setting `config.field_reference.parser` has been removed.
@@ -1228,13 +1228,13 @@ For a more detailed view of these changes please check <<breaking-8.0>>.
 ==== New features and enhancements
 * As processing times speed up, millisecond granularity is not always enough. Inbound data increasingly has sub-millisecond granularity timestamps.
   The pull request https://github.com/elastic/logstash/pull/12797[#12797] allows the internal mechanisms of
-  Logstash that hold moment-in-time data - such as the Logstash Event, the Persistent Queue, the Dead Letter Queue and JSON encoding/decoding - to have nanosecond granularity.
+  {ls} that hold moment-in-time data - such as the {ls} Event, the Persistent Queue, the Dead Letter Queue and JSON encoding/decoding - to have nanosecond granularity.
 * We have added another flag to the Benchmark CLI to allow passing a data file with previously captured data to the custom test case.
   This feature allows users to run the Benchmark CLI in a custom test case with a custom config and a custom dataset. https://github.com/elastic/logstash/pull/12437[#12437]
 
 ==== Plugins
 
-Logstash 8.0.0 includes the same versions of all bundled plugins as Logstash 7.17.0.
+{ls} 8.0.0 includes the same versions of all bundled plugins as {ls} 7.17.0.
 If you upgrade to 7.17 before upgrading to 8.0 (as recommended), you won't see any changes to plugin versions.
 
 *Clone Filter - 4.2.0*
@@ -1366,7 +1366,7 @@ https://github.com/logstash-plugins/logstash-integration-kafka/pull/106[#106]
 Java causes on connection related exceptions will now be extra logged when plugin is logging at debug level
 https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1029[#1029]
 * ECS-related fixes https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1046[#1046]
-** Data Streams requirement on ECS is properly enforced when running on Logstash 8, and warned about when running on Logstash 7.
+** Data Streams requirement on ECS is properly enforced when running on {ls} 8, and warned about when running on {ls} 7.
 ** ECS Compatibility v8 can now be selected
 
 *Core Patterns - 4.3.2*
@@ -1375,47 +1375,47 @@ https://github.com/logstash-plugins/logstash-output-elasticsearch/pull/1029[#102
 
 
 [[logstash-8-0-0-rc2]]
-=== Logstash 8.0.0-rc2 Release Notes
+=== {ls} 8.0.0-rc2 Release Notes
 
 [[notable-8.0.0-rc2]]
 ==== Notable issues fixed
 * Fixed long-standing issue in which the `events.out` count incorrectly included events that had been dropped with the drop filter.
 Now the total out event count includes only events that reach the out stage. https://github.com/elastic/logstash/pull/13593[#13593]
 * Reduced scope and impact of a memory leak that can be caused by using UUIDs or other high-cardinality field names https://github.com/elastic/logstash/pull/13642[#13642]
-* Fixed an issue with the Azure input plugin that caused Logstash to crash when the input was used in a pipeline. https://github.com/elastic/logstash/pull/13603[#13603]
+* Fixed an issue with the Azure input plugin that caused {ls} to crash when the input was used in a pipeline. https://github.com/elastic/logstash/pull/13603[#13603]
 
 ==== Plugin releases
 Plugins align with release 7.17.0
 
 
 [[logstash-8-0-0-rc1]]
-=== Logstash 8.0.0-rc1 Release Notes
+=== {ls} 8.0.0-rc1 Release Notes
 
 ==== Breaking changes
 
 [[rn-ecs-compatibility]]
 ===== ECS compatibility
-Many plugins can now be run in a mode that avoids implicit conflict with the Elastic Common Schema (ECS). This mode is controlled individually with each plugin’s ecs_compatibility option, which defaults to the value of the Logstash pipeline.ecs_compatibility setting. In Logstash 8, this compatibility mode will be on-by-default for all pipelines.
+Many plugins can now be run in a mode that avoids implicit conflict with the Elastic Common Schema (ECS). This mode is controlled individually with each plugin’s ecs_compatibility option, which defaults to the value of the {ls} pipeline.ecs_compatibility setting. In {ls} 8, this compatibility mode will be on-by-default for all pipelines.
 
-If you wish to lock in a pipeline’s behavior from Logstash 7.x before upgrading to Logstash 8, you can set pipeline.ecs_compatibility: disabled to its definition in pipelines.yml (or globally in logstash.yml).
+If you wish to lock in a pipeline’s behavior from {ls} 7.x before upgrading to {ls} 8, you can set pipeline.ecs_compatibility: disabled to its definition in pipelines.yml (or globally in logstash.yml).
 
 ==== New features and enhancements
 
-Logstash Docker images are now based on Ubuntu 20.04.
+{ls} Docker images are now based on Ubuntu 20.04.
 
 ==== Plugin releases
 Plugins align with release 7.16.2
 
 
 [[logstash-8-0-0-beta1]]
-=== Logstash 8.0.0-beta1 Release Notes
+=== {ls} 8.0.0-beta1 Release Notes
 
 ==== Breaking changes
 
 [[rn-java-11-minimum]]
 ===== Java 11 minimum
-Starting from Logstash 8.0, the minimum required version of Java to run Logstash is Java 11. By default, Logstash will
-run with the bundled JDK, which has been verified to work with each specific version of Logstash, and generally
+Starting from {ls} 8.0, the minimum required version of Java to run {ls} is Java 11. By default, {ls} will
+run with the bundled JDK, which has been verified to work with each specific version of {ls}, and generally
 provides the best performance and reliability.
 
 See <<breaking-changes>> for a preview of additional breaking changes coming your way. 
@@ -1425,7 +1425,7 @@ See <<breaking-changes>> for a preview of additional breaking changes coming you
 [[rn-nanosecond-precision]]
 ===== Nanosecond precision
 As processing times speed up, millisecond granularity is not always enough. Inbound data increasingly has sub-millisecond granularity timestamps.
-The pull request https://github.com/elastic/logstash/pull/12797[#12797] allows the internal mechanisms of Logstash that hold moment-in-time data - such as the Logstash Event, the Persistent Queue, the Dead Letter Queue and JSON encoding/decoding - to have nanosecond granularity.
+The pull request https://github.com/elastic/logstash/pull/12797[#12797] allows the internal mechanisms of {ls} that hold moment-in-time data - such as the {ls} Event, the Persistent Queue, the Dead Letter Queue and JSON encoding/decoding - to have nanosecond granularity.
 
 Timestamp precision is limited to the JVM and Platform's available granularity, which in many cases is microseconds.
 
@@ -1459,13 +1459,13 @@ Plugins align with release 7.15.1
 
 
 [[logstash-8-0-0-alpha2]]
-=== Logstash 8.0.0-alpha2 Release Notes
+=== {ls} 8.0.0-alpha2 Release Notes
 
 ==== Breaking changes
 
 [[java-home-breaking-change]]
 ===== Removed support for JAVA_HOME
-Support for using `JAVA_HOME` to override the path to the JDK that Logstash runs with has been removed for this release.
+Support for using `JAVA_HOME` to override the path to the JDK that {ls} runs with has been removed for this release.
 In the `8.x` release, users should set the value of `LS_JAVA_HOME` to the path of their preferred JDK if they
 wish to use a version other than the bundled JDK. The value of `JAVA_HOME` will be ignored.
 
@@ -1473,13 +1473,13 @@ wish to use a version other than the bundled JDK. The value of `JAVA_HOME` will
 Plugins align with release 7.15.0
 
 [[logstash-8-0-0-alpha1]]
-=== Logstash 8.0.0-alpha1 Release Notes
+=== {ls} 8.0.0-alpha1 Release Notes
 
 ==== Breaking changes
 
 [[ruby-engine]]
 ===== Ruby Execution Engine removed
-The Java Execution Engine has been the default engine since Logstash 7.0, and works with plugins written in either Ruby or Java.
+The Java Execution Engine has been the default engine since {ls} 7.0, and works with plugins written in either Ruby or Java.
 Removal of the Ruby Execution Engine will not affect the ability to run existing pipelines. https://github.com/elastic/logstash/pull/12517[#12517]
 
 [[utf-16]]
diff --git a/docs/static/reloading-config.asciidoc b/docs/static/reloading-config.asciidoc
index 4661de54678..81295891038 100644
--- a/docs/static/reloading-config.asciidoc
+++ b/docs/static/reloading-config.asciidoc
@@ -1,9 +1,9 @@
 [[reloading-config]]
 === Reloading the Config File
 
-You can set Logstash to detect and reload configuration changes automatically.
+You can set {ls} to detect and reload configuration changes automatically.
 
-To enable automatic config reloading, start Logstash with the `--config.reload.automatic` (or `-r`)
+To enable automatic config reloading, start {ls} with the `--config.reload.automatic` (or `-r`)
 command-line option specified. For example:
 
 [source,shell]
@@ -14,8 +14,8 @@ bin/logstash -f apache.config --config.reload.automatic
 NOTE: The `--config.reload.automatic` option is not available when you specify the `-e` flag to pass
 in configuration settings from the command-line.
 
-By default, Logstash checks for configuration changes every 3 seconds. To change this interval,
-use the `--config.reload.interval <interval>` option,  where `interval` specifies how often Logstash
+By default, {ls} checks for configuration changes every 3 seconds. To change this interval,
+use the `--config.reload.interval <interval>` option,  where `interval` specifies how often {ls}
 checks the config files for changes (in seconds). 
 
 Note that the unit qualifier (`s`) is required.
@@ -23,9 +23,9 @@ Note that the unit qualifier (`s`) is required.
 [[force-reload]]
 ==== Force reloading the config file
 
-If Logstash is already running without auto-reload enabled, you can force
-Logstash to reload the config file and restart the pipeline. Do this by sending
-a SIGHUP (signal hangup) to the process running Logstash. 
+If {ls} is already running without auto-reload enabled, you can force
+{ls} to reload the config file and restart the pipeline. Do this by sending
+a SIGHUP (signal hangup) to the process running {ls}. 
 For example:
 
 [source,shell]
@@ -33,17 +33,17 @@ For example:
 kill -SIGHUP 14175
 ----------------------------------
 
-Where 14175 is the ID of the process running Logstash.
+Where 14175 is the ID of the process running {ls}.
 
 NOTE: This functionality is not supported on Windows OS.
 
 ==== How automatic config reloading works
 
-When Logstash detects a change in a config file, it stops the current pipeline by stopping
+When {ls} detects a change in a config file, it stops the current pipeline by stopping
 all inputs, and it attempts to create a new pipeline that uses the updated configuration.
-After validating the syntax of the new configuration, Logstash verifies that all inputs
+After validating the syntax of the new configuration, {ls} verifies that all inputs
 and outputs can be initialized (for example, that all required ports are open). If the checks
-are successful, Logstash swaps the existing pipeline with the new pipeline. If the checks
+are successful, {ls} swaps the existing pipeline with the new pipeline. If the checks
 fail, the old pipeline continues to function, and the errors are propagated to the console.
 
 During automatic config reloading, the JVM is not restarted. The creating and swapping of
@@ -52,7 +52,7 @@ pipelines all happens within the same process.
 Changes to <<plugins-filters-grok,grok>> pattern files are also reloaded, but only when
 a change in the config file triggers a reload (or the pipeline is restarted).
 
-In general, Logstash is not watching or monitoring any configuration files used or referenced by inputs,
+In general, {ls} is not watching or monitoring any configuration files used or referenced by inputs,
 filters or outputs.
 
 [[plugins-block-reload]]
diff --git a/docs/static/resiliency.asciidoc b/docs/static/resiliency.asciidoc
index ff347097bda..32f65d0bebc 100644
--- a/docs/static/resiliency.asciidoc
+++ b/docs/static/resiliency.asciidoc
@@ -1,22 +1,22 @@
 [[resiliency]]
 == Queues and data resiliency
 
-By default, Logstash uses <<memory-queue,in-memory bounded queues>> between pipeline stages (inputs → pipeline workers) to buffer events. 
+By default, {ls} uses <<memory-queue,in-memory bounded queues>> between pipeline stages (inputs → pipeline workers) to buffer events. 
 
-As data flows through the event processing pipeline, Logstash may encounter
+As data flows through the event processing pipeline, {ls} may encounter
 situations that prevent it from delivering events to the configured
 output. For example, the data might contain unexpected data types, or
-Logstash might terminate abnormally. 
+{ls} might terminate abnormally. 
 
 To guard against data loss and ensure that events flow through the
-pipeline without interruption, Logstash provides data resiliency
+pipeline without interruption, {ls} provides data resiliency
 features. 
 
 * <<persistent-queues>> protect against data loss by storing events in an
 internal queue on disk. 
 
-* <<dead-letter-queues>> provide on-disk storage for events that Logstash is unable to process so that you can evaluate them. 
+* <<dead-letter-queues>> provide on-disk storage for events that {ls} is unable to process so that you can evaluate them. 
 You can easily reprocess events in the dead letter queue by using the `dead_letter_queue` input plugin.
 
 These resiliency features are disabled by default. To turn on these features,
-you must explicitly enable them in the Logstash <<logstash-settings-file,settings file>>.
+you must explicitly enable them in the {ls} <<logstash-settings-file,settings file>>.
diff --git a/docs/static/running-logstash-command-line.asciidoc b/docs/static/running-logstash-command-line.asciidoc
index 5eba5c5961d..3dc4871f829 100644
--- a/docs/static/running-logstash-command-line.asciidoc
+++ b/docs/static/running-logstash-command-line.asciidoc
@@ -1,5 +1,5 @@
 [[running-logstash-command-line]]
-=== Running Logstash from the Command Line
+=== Running {ls} from the Command Line
 
 [IMPORTANT]
 .macOS Gatekeeper warnings
@@ -27,14 +27,14 @@ https://support.apple.com/en-us/HT202491[Safely open apps on your Mac].
 ====
 
 
-To run Logstash from the command line, use the following command:
+To run {ls} from the command line, use the following command:
 
 [source,shell]
 ----
 bin/logstash [options]
 ----
 
-To run Logstash from the Windows command line, use the following command:
+To run {ls} from the Windows command line, use the following command:
 
 [source,shell]
 ----
@@ -42,11 +42,11 @@ bin/logstash.bat [options]
 ----
 
 Where `options` are <<command-line-flags,command-line>> flags that you can
-specify to control Logstash execution. The location of the `bin` directory
+specify to control {ls} execution. The location of the `bin` directory
 varies by platform. See <<dir-layout>> to find the location of `bin\logstash` on
 your system.
 
-The following example runs Logstash and loads the Logstash config defined in
+The following example runs {ls} and loads the {ls} config defined in
 the `mypipeline.conf` file:
 
 [source,shell]
@@ -56,29 +56,29 @@ bin/logstash -f mypipeline.conf
 
 Any flags that you set at the command line override the corresponding settings
 in <<logstash-settings-file>>, but the file
-itself is not changed. It remains as-is for subsequent Logstash runs.
+itself is not changed. It remains as-is for subsequent {ls} runs.
 
-Specifying command line options is useful when you are testing Logstash.
+Specifying command line options is useful when you are testing {ls}.
 However, in a production environment, we recommend that you use
-<<logstash-settings-file>> to control Logstash execution. Using
+<<logstash-settings-file>> to control {ls} execution. Using
 the settings file makes it easier for you to specify multiple options, and it
 provides you with a single, versionable file that you can use to start up
-Logstash consistently for each run.
+{ls} consistently for each run.
 
 [[command-line-flags]]
 ==== Command-Line Flags
 
-Logstash has the following flags. You can use the `--help` flag to display this information.
+{ls} has the following flags. You can use the `--help` flag to display this information.
 
 *`--node.name NAME`*::
-  Specify the name of this Logstash instance. If no value is given it will default to the current
+  Specify the name of this {ls} instance. If no value is given it will default to the current
   hostname.
 
 *`-f, --path.config CONFIG_PATH`*::
-Load the Logstash config from a specific file or directory. If a directory is given, all
+Load the {ls} config from a specific file or directory. If a directory is given, all
 files in that directory will be concatenated in lexicographical order and then parsed as a
 single config file. Specifying this flag multiple times is not supported. If you specify
-this flag multiple times, Logstash uses the last occurrence (for example, `-f foo -f bar`
+this flag multiple times, {ls} uses the last occurrence (for example, `-f foo -f bar`
 is the same as `-f bar`).
 +
 You can specify wildcards (<<glob-support,globs>>) and any matched files will
@@ -90,7 +90,7 @@ load specific files by name:
 bin/logstash --debug -f '/tmp/{one,two,three}'
 ---------------------------------------------
 +
-With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/two`, and
+With this command, {ls} concatenates three config files, `/tmp/one`, `/tmp/two`, and
 `/tmp/three`, and parses them into a single config.
 
 *`-e, --config.string CONFIG_STRING`*::
@@ -114,7 +114,7 @@ With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/t
 
 *`-M, --modules.variable`*::
   Assign a value to a configurable option for a module.  The format for assigning variables is
-  `-M "MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.KEY_NAME=value"` for Logstash variables. For other
+  `-M "MODULE_NAME.var.PLUGIN_TYPE.PLUGIN_NAME.KEY_NAME=value"` for {ls} variables. For other
   settings, it will be `-M "MODULE_NAME.KEY_NAME.SUB_KEYNAME=value"`.  The `-M` flag can be used
   as many times as is necessary. If no `-M` options are specified, then the default value for
   that setting will be used.  The `-M` flag is only used in conjunction with the `--modules`
@@ -158,20 +158,20 @@ With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/t
   Sets the process default value for  ECS compatibility mode.
   Can be an ECS version like `v1` or `v8`, or `disabled`.
   The default is `v8`.
-  Pipelines defined before Logstash 8 operated without ECS in mind.
-  To ensure a migrated pipeline continues to operate as it did in older releases of Logstash, opt-OUT of ECS for the individual pipeline by setting `pipeline.ecs_compatibility: disabled` in its `pipelines.yml` definition.
+  Pipelines defined before {ls} 8 operated without ECS in mind.
+  To ensure a migrated pipeline continues to operate as it did in older releases of {ls}, opt-OUT of ECS for the individual pipeline by setting `pipeline.ecs_compatibility: disabled` in its `pipelines.yml` definition.
   Using the command-line flag will set the default for _all_ pipelines, including new ones.
   See <<ecs-compatibility>> for more info.
 
 *`--pipeline.unsafe_shutdown`*::
-  Force Logstash to exit during shutdown even if there are still inflight events
-  in memory. By default, Logstash will refuse to quit until all received events
+  Force {ls} to exit during shutdown even if there are still inflight events
+  in memory. By default, {ls} will refuse to quit until all received events
   have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
 
 *`--path.data PATH`*::
-  This should point to a writable directory. Logstash will use this directory whenever it needs to store
+  This should point to a writable directory. {ls} will use this directory whenever it needs to store
   data. Plugins will also have access to this path. The default is the `data` directory under
-  Logstash home.
+  {ls} home.
 
 *`-p, --path.plugins PATH`*::
   A path of where to find custom plugins. This flag can be given multiple times to include
@@ -180,10 +180,10 @@ With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/t
   and `NAME` is the name of the plugin.
 
 *`-l, --path.logs PATH`*::
-  Directory to write Logstash internal logs to.
+  Directory to write {ls} internal logs to.
 
 *`--log.level LEVEL`*::
- Set the log level for Logstash. Possible values are:
+ Set the log level for {ls}. Possible values are:
 * `fatal`: log very severe error messages that will usually be followed by the application aborting
 * `error`: log errors
 * `warn`: log warnings
@@ -200,12 +200,12 @@ With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/t
   Drop to shell instead of running as normal. Valid shells are "irb" and "pry".
 
 *`-V, --version`*::
-  Emit the version of Logstash and its friends, then exit.
+  Emit the version of {ls} and its friends, then exit.
 
 *`-t, --config.test_and_exit`*::
   Check configuration for valid syntax and then exit. Note that grok patterns are not checked for
-  correctness with this flag. Logstash can read multiple config files from a directory. If you combine this
-  flag with `--log.level=debug`, Logstash will log the combined config file, annotating
+  correctness with this flag. {ls} can read multiple config files from a directory. If you combine this
+  flag with `--log.level=debug`, {ls} will log the combined config file, annotating
   each config block with the source file it came from.
 
 *`-r, --config.reload.automatic`*::
@@ -224,22 +224,22 @@ With this command, Logstash concatenates three config files, `/tmp/one`, `/tmp/t
 
 *`--api.http.port HTTP_PORT`*::
   Web API http port. This option specifies the bind port for the metrics REST endpoint. The default is 9600-9700.
-  This setting accepts a range of the format 9600-9700. Logstash will pick up the first available port.
+  This setting accepts a range of the format 9600-9700. {ls} will pick up the first available port.
 
 *`--log.format FORMAT`*::
-   Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text
+   Specify if {ls} should write its own logs in JSON form (one event per line) or in plain text
    (using Ruby's Object#inspect). The default is "plain".
 
 *`--path.settings SETTINGS_DIR`*::
   Set the directory containing the `logstash.yml` <<logstash-settings-file,settings file>> as well
   as the log4j logging configuration. This can also be set through the LS_SETTINGS_DIR environment variable.
-  The default is the `config` directory under Logstash home.
+  The default is the `config` directory under {ls} home.
 
 *`--enable-local-plugin-development`*::
   This flag enables developers to update their local Gemfile without running into issues caused by a frozen lockfile.
   This flag can be helpful when you are developing/testing plugins locally.
 
-NOTE: This flag is for Logstash developers only. End users should not need it.
+NOTE: This flag is for {ls} developers only. End users should not need it.
 
 
 
diff --git a/docs/static/running-logstash-windows.asciidoc b/docs/static/running-logstash-windows.asciidoc
index 7a3819c58e6..c77da200fb4 100644
--- a/docs/static/running-logstash-windows.asciidoc
+++ b/docs/static/running-logstash-windows.asciidoc
@@ -1,12 +1,12 @@
 [[running-logstash-windows]]
-=== Running Logstash on Windows
-Before reading this section, see <<installing-logstash>> to get started.  You also need to be familiar with <<running-logstash-command-line>> as command line options are used to test running Logstash on Windows.
+=== Running {ls} on Windows
+Before reading this section, see <<installing-logstash>> to get started.  You also need to be familiar with <<running-logstash-command-line>> as command line options are used to test running {ls} on Windows.
 
-IMPORTANT: Specifying command line options is useful when you are testing Logstash. However, in a production environment, we recommend that you use <<logstash-settings-file>> to control Logstash execution. Using the settings file makes it easier for you to specify multiple options, and it provides you with a single, versionable file that you can use to start up Logstash consistently for each run.
+IMPORTANT: Specifying command line options is useful when you are testing {ls}. However, in a production environment, we recommend that you use <<logstash-settings-file>> to control {ls} execution. Using the settings file makes it easier for you to specify multiple options, and it provides you with a single, versionable file that you can use to start up {ls} consistently for each run.
 
-Logstash is not started automatically after installation. How to start and stop Logstash on Windows depends on whether you want to run it manually, as a service (with https://nssm.cc/[NSSM]), or run it as a scheduled task. This guide provides an example of some of the ways Logstash can run on Windows.
+{ls} is not started automatically after installation. How to start and stop {ls} on Windows depends on whether you want to run it manually, as a service (with https://nssm.cc/[NSSM]), or run it as a scheduled task. This guide provides an example of some of the ways {ls} can run on Windows.
 
-NOTE: It is recommended to validate your configuration works by running Logstash manually before running Logstash as a service or a scheduled task.
+NOTE: It is recommended to validate your configuration works by running {ls} manually before running {ls} as a service or a scheduled task.
 
 [[running-logstash-windows-validation]]
 ==== Validating JVM prerequisites on Windows
@@ -51,14 +51,14 @@ NOTE: As of the publication of this document, please review this https://github.
 Once you have <<setup-logstash>> and validated JVM pre-requisites, you may proceed.  
 
 NOTE: For the examples listed below, we are running Windows Server 2016, Java 11.0.3,
-have extracted the https://www.elastic.co/downloads/logstash[Logstash ZIP
+have extracted the https://www.elastic.co/downloads/logstash[{ls} ZIP
 package] to +C:{backslash}logstash-{logstash_version}{backslash}+, and using the example
 `syslog.conf` file shown below (stored in
 +C:{backslash}logstash-{logstash_version}{backslash}config{backslash}+).
 
 [[running-logstash-windows-manual]]
-==== Running Logstash manually
-Logstash can be run manually using https://docs.microsoft.com/en-us/powershell/[PowerShell].  Open an Administrative https://docs.microsoft.com/en-us/powershell/[PowerShell] session, then run the following commands:
+==== Running {ls} manually
+{ls} can be run manually using https://docs.microsoft.com/en-us/powershell/[PowerShell].  Open an Administrative https://docs.microsoft.com/en-us/powershell/[PowerShell] session, then run the following commands:
 
 ["source","sh",subs="attributes"]
 -----
@@ -66,20 +66,20 @@ PS C:{backslash}Windows{backslash}system32> cd C:{backslash}logstash-{logstash_v
 PS C:{backslash}logstash-{logstash_version}> .{backslash}bin{backslash}logstash.bat -f .{backslash}config{backslash}syslog.conf
 -----
 
-NOTE: In a production environment, we recommend that you use <<logstash-settings-file>> to control Logstash execution.
+NOTE: In a production environment, we recommend that you use <<logstash-settings-file>> to control {ls} execution.
 
-Wait for the following messages to appear, to confirm Logstash has started successfully:
+Wait for the following messages to appear, to confirm {ls} has started successfully:
 
 ["source","sh",subs="attributes"]
 -----
-[logstash.runner          ] Starting Logstash {"logstash.version"=>"{logstash_version}"}
+[logstash.runner          ] Starting {ls} {"logstash.version"=>"{logstash_version}"}
 [logstash.inputs.udp      ] Starting UDP listener {:address=>"0.0.0.0:514"}
-[logstash.agent           ] Successfully started Logstash API endpoint {:port=>9600}
+[logstash.agent           ] Successfully started {ls} API endpoint {:port=>9600}
 -----
 
 [[running-logstash-windows-nssm]]
-==== Running Logstash as a service with NSSM
-NOTE: It is recommended to validate your configuration works by running Logstash manually before you proceed.
+==== Running {ls} as a service with NSSM
+NOTE: It is recommended to validate your configuration works by running {ls} manually before you proceed.
 
 Download https://nssm.cc/[NSSM], then extract `nssm.exe` from
 `nssm-<version.number>\win64\nssm.exe` to +C:{backslash}logstash-{logstash_version}{backslash}bin{backslash}+.
@@ -98,9 +98,9 @@ Once the `NSSM service installer` window appears, specify the following paramete
 ** In the `Application` tab:
 *** Path: Path to `logstash.bat`: +C:{backslash}logstash-{logstash_version}{backslash}bin{backslash}logstash.bat+
 *** Startup Directory: Path to the `bin` directory: +C:{backslash}logstash-{logstash_version}{backslash}bin+
-*** Arguments: For this example to start Logstash: +-f C:{backslash}logstash-{logstash_version}{backslash}config{backslash}syslog.conf+
+*** Arguments: For this example to start {ls}: +-f C:{backslash}logstash-{logstash_version}{backslash}config{backslash}syslog.conf+
 +
-NOTE: In a production environment, we recommend that you use <<logstash-settings-file>> to control Logstash execution.
+NOTE: In a production environment, we recommend that you use <<logstash-settings-file>> to control {ls} execution.
 
 ** Review and make any changes necessary in the `Details` tab:
 *** Ensure `Startup Type` is set appropriately
@@ -117,8 +117,8 @@ NOTE: In a production environment, we recommend that you use <<logstash-settings
 Once the service has been installed with NSSM, validate and start the service following the https://docs.microsoft.com/en-us/powershell/scripting/samples/managing-services[PowerShell Managing Services] documentation.
 
 [[running-logstash-windows-scheduledtask]]
-==== Running Logstash with Task Scheduler
-NOTE: It is recommended to validate your configuration works by running Logstash manually before you proceed.
+==== Running {ls} with Task Scheduler
+NOTE: It is recommended to validate your configuration works by running {ls} manually before you proceed.
 
 Open the Windows https://docs.microsoft.com/en-us/windows/desktop/taskschd/task-scheduler-start-page[Task Scheduler], then click `Create Task` in the Actions window.  Specify the following parameters in the `Actions` tab:
 
@@ -129,7 +129,7 @@ Open the Windows https://docs.microsoft.com/en-us/windows/desktop/taskschd/task-
 *** Add arguments: +-f C:\logstash-{logstash_version}{backslash}config{backslash}syslog.conf+
 *** Start in: +C:{backslash}logstash-{logstash_version}{backslash}bin{backslash}+
 +
-NOTE: In a production environment, we recommend that you use <<logstash-settings-file>> to control Logstash execution.
+NOTE: In a production environment, we recommend that you use <<logstash-settings-file>> to control {ls} execution.
 
 ** Review and make any changes necessary in the `General`, `Triggers`, `Conditions`, and `Settings` tabs.
 
@@ -137,14 +137,14 @@ NOTE: In a production environment, we recommend that you use <<logstash-settings
 
 ** Once the new task has been created, either wait for it to run on the schedule or select the service then click `Run` to start the task.
 
-NOTE: Logstash can be stopped by selecting the service, then clicking `End` in the Task Scheduler window.
+NOTE: {ls} can be stopped by selecting the service, then clicking `End` in the Task Scheduler window.
 
 [[running-logstash-windows-example]]
-==== Example Logstash Configuration
-We will configure Logstash to listen for syslog messages over port 514 with this configuration (file name is `syslog.conf`):
+==== Example {ls} Configuration
+We will configure {ls} to listen for syslog messages over port 514 with this configuration (file name is `syslog.conf`):
 [source,yaml]
 -----
-# Sample Logstash configuration for receiving
+# Sample {ls} configuration for receiving
 # UDP syslog messages over port 514
 
 input {
diff --git a/docs/static/running-logstash.asciidoc b/docs/static/running-logstash.asciidoc
index 1f876c1f1b8..fe411d736cd 100644
--- a/docs/static/running-logstash.asciidoc
+++ b/docs/static/running-logstash.asciidoc
@@ -1,7 +1,7 @@
 [[running-logstash]]
-=== Running Logstash as a Service on Debian or RPM
+=== Running {ls} as a Service on Debian or RPM
 
-Logstash is not started automatically after installation. Starting and stopping Logstash depends on the
+{ls} is not started automatically after installation. Starting and stopping {ls} depends on the
 init system of the underlying operating system, which is now systemd.
 
 As systemd is now the de-facto init system, here are some common operating systems and versions that
@@ -15,10 +15,10 @@ use it.  This list is intended to be informative, not exhaustive.
 |=======================================================================
 
 [[running-logstash-systemd]]
-==== Running Logstash by Using Systemd
+==== Running {ls} by Using Systemd
 
 Distributions like Debian Jessie, Ubuntu 15.10+, and many of the SUSE derivatives use systemd and the
-`systemctl` command to start and stop services. Logstash places the systemd unit files in `/etc/systemd/system` for both deb and rpm. After installing the package, you can start up Logstash with:
+`systemctl` command to start and stop services. {ls} places the systemd unit files in `/etc/systemd/system` for both deb and rpm. After installing the package, you can start up {ls} with:
 
 [source,sh]
 -------------------------------------------
diff --git a/docs/static/security/basic-auth.asciidoc b/docs/static/security/basic-auth.asciidoc
index 25fda83645c..7b5e7d32f13 100644
--- a/docs/static/security/basic-auth.asciidoc
+++ b/docs/static/security/basic-auth.asciidoc
@@ -1,11 +1,11 @@
 [discrete]
 [[ls-http-auth-basic]]
-=== Configuring Logstash to use basic authentication
+=== Configuring {ls} to use basic authentication
 
-Logstash needs to be able to manage index templates, create indices,
+{ls} needs to be able to manage index templates, create indices,
 and write and delete documents in the indices it creates.
 
-To set up authentication credentials for Logstash:
+To set up authentication credentials for {ls}:
 
 . Use the the **Management > Roles** UI in {kib} or the `role` API to create a
 `logstash_writer` role. For *cluster* privileges, add `manage_index_templates` and `monitor`. 
@@ -31,7 +31,7 @@ POST _security/role/logstash_writer
 <1> The cluster needs the `manage_ilm` privilege if 
 {ref}/getting-started-index-lifecycle-management.html[index lifecycle management]
 is enabled.
-<2> If you use a custom Logstash index pattern, specify your custom pattern
+<2> If you use a custom {ls} index pattern, specify your custom pattern
 instead of the default `logstash-*` pattern.
 <3> If {ref}/getting-started-index-lifecycle-management.html[index lifecycle
 management] is enabled, the role requires the `manage` and `manage_ilm`
@@ -48,13 +48,13 @@ POST _security/user/logstash_internal
 {
   "password" : "x-pack-test-password",
   "roles" : [ "logstash_writer"],
-  "full_name" : "Internal Logstash User"
+  "full_name" : "Internal {ls} User"
 }
 ---------------------------------------------------------------
 
-. Configure Logstash to authenticate as the `logstash_internal` user you just
+. Configure {ls} to authenticate as the `logstash_internal` user you just
 created. You configure credentials separately for each of the {es} plugins in
-your Logstash `.conf` file. For example:
+your {ls} `.conf` file. For example:
 +
 [source,js]
 --------------------------------------------------
diff --git a/docs/static/security/es-security.asciidoc b/docs/static/security/es-security.asciidoc
index 0c985aa1f4b..4c3b6071921 100644
--- a/docs/static/security/es-security.asciidoc
+++ b/docs/static/security/es-security.asciidoc
@@ -23,7 +23,7 @@ Examples:
 * `output {elasticsearch { cloud_id => "<cloud id>" api_key => "<api key>" } }``
 
 For more details, check out the
-{logstash-ref}/connecting-to-cloud.html[Logstash-to-Cloud documentation].
+{logstash-ref}/connecting-to-cloud.html[{ls}-to-Cloud documentation].
 
 {ess-leadin-short}
 =====
@@ -50,7 +50,7 @@ Therefore {ls} needs its own copy of the self-signed CA from the {es} cluster in
 
 Copy the {ref}/configuring-stack-security.html#stack-security-certificates[self-signed CA certificate] from the {es} `config/certs` directory.
 
-Save it to a location that Logstash can access, such as `config/certs` on the {ls} instance. 
+Save it to a location that {ls} can access, such as `config/certs` on the {ls} instance. 
 
 /////
 ToDo: 
diff --git a/docs/static/security/grant-access.asciidoc b/docs/static/security/grant-access.asciidoc
index 0f6ec42274b..2a38d6a9651 100644
--- a/docs/static/security/grant-access.asciidoc
+++ b/docs/static/security/grant-access.asciidoc
@@ -1,12 +1,12 @@
 [discrete]
 [[ls-user-access]]
-=== Granting access to the Logstash indices
+=== Granting access to the {ls} indices
 
-To access the indices Logstash creates, users need the `read` and
+To access the indices {ls} creates, users need the `read` and
 `view_index_metadata` privileges:
 
 . Create a `logstash_reader` role that has the `read` and `view_index_metadata`
-privileges  for the Logstash indices. You can create roles from the
+privileges  for the {ls} indices. You can create roles from the
 **Management > Roles** UI in {kib} or through the `role` API:
 +
 [source, sh]
@@ -17,7 +17,7 @@ POST _security/role/logstash_reader
 }
 ---------------------------------------------------------------
 
-. Assign your Logstash users the `logstash_reader` role. If the Logstash user
+. Assign your {ls} users the `logstash_reader` role. If the {ls} user
 will be using
 {logstash-ref}/logstash-centralized-pipeline-management.html[centralized pipeline management],
 also assign the `logstash_admin` role. You can create and manage users from the
@@ -29,7 +29,7 @@ POST _security/user/logstash_user
 {
   "password" : "x-pack-test-password",
   "roles" : [ "logstash_reader", "logstash_admin"], <1>
-  "full_name" : "Kibana User for Logstash"
+  "full_name" : "Kibana User for {ls}"
 }
 ---------------------------------------------------------------
 <1> `logstash_admin` is a built-in role that provides access to system
diff --git a/docs/static/security/logstash.asciidoc b/docs/static/security/logstash.asciidoc
index 20a7ed11b0a..6eae39f707a 100644
--- a/docs/static/security/logstash.asciidoc
+++ b/docs/static/security/logstash.asciidoc
@@ -5,22 +5,22 @@
 <titleabbrev>Secure your connection</titleabbrev>
 ++++
 
-The Logstash {es} {logstash-ref}/plugins-outputs-elasticsearch.html[output],
+The {ls} {es} {logstash-ref}/plugins-outputs-elasticsearch.html[output],
 {logstash-ref}/plugins-inputs-elasticsearch.html[input], and
 {logstash-ref}/plugins-filters-elasticsearch.html[filter] plugins,  as well as
 {logstash-ref}/monitoring-logstash.html[monitoring] and central management,
 support authentication and encryption over HTTPS.
 
 {es} clusters are secured by default (starting in 8.0). 
-You need to configure authentication credentials for Logstash in order to
+You need to configure authentication credentials for {ls} in order to
 establish communication.
-Logstash throws an exception and the processing pipeline is halted if authentication fails.
+{ls} throws an exception and the processing pipeline is halted if authentication fails.
 
-In addition to configuring authentication credentials for Logstash, you need
-to grant authorized users permission to access the Logstash indices.
+In addition to configuring authentication credentials for {ls}, you need
+to grant authorized users permission to access the {ls} indices.
 
 Security is enabled by default on the {es} cluster (starting in 8.0).
-You must enable TLS/SSL in the {es} output section of the Logstash configuration in order to allow Logstash to communicate with the {es} cluster.
+You must enable TLS/SSL in the {es} output section of the {ls} configuration in order to allow {ls} to communicate with the {es} cluster.
 
 include::es-security.asciidoc[]
 include::basic-auth.asciidoc[]
diff --git a/docs/static/security/ls-monitoring.asciidoc b/docs/static/security/ls-monitoring.asciidoc
index 8f4ebad8531..054e57072da 100644
--- a/docs/static/security/ls-monitoring.asciidoc
+++ b/docs/static/security/ls-monitoring.asciidoc
@@ -2,15 +2,15 @@
 [[ls-monitoring-user]]
 === Configuring credentials for {ls} monitoring
 
-If you want to monitor your Logstash instance with {stack-monitor-features}, and
-store the monitoring data in a secured {es} cluster, you must configure Logstash
+If you want to monitor your {ls} instance with {stack-monitor-features}, and
+store the monitoring data in a secured {es} cluster, you must configure {ls}
 with a username and password for a user with the appropriate permissions.
 
 The {security-features} come preconfigured with a
 {ref}/built-in-users.html[`logstash_system` built-in user]
 for this purpose. This user has the minimum permissions necessary for the
 monitoring function, and _should not_ be used for any other purpose - it is
-specifically _not intended_ for use within a Logstash pipeline.
+specifically _not intended_ for use within a {ls} pipeline.
 
 By default, the `logstash_system` user does not have a password. The user will
 not be enabled until you set a password. See
diff --git a/docs/static/security/pipeline-mgmt.asciidoc b/docs/static/security/pipeline-mgmt.asciidoc
index 97d05730f28..789816e51b1 100644
--- a/docs/static/security/pipeline-mgmt.asciidoc
+++ b/docs/static/security/pipeline-mgmt.asciidoc
@@ -2,9 +2,9 @@
 [[ls-pipeline-management-user]]
 === Configuring credentials for Centralized Pipeline Management
 
-If you plan to use Logstash
+If you plan to use {ls}
 {logstash-ref}/logstash-centralized-pipeline-management.html[centralized pipeline management],
-you need to configure the username and password that Logstash uses for managing
+you need to configure the username and password that {ls} uses for managing
 configurations.
 
 You configure the user and password in the `logstash.yml` configuration file:
diff --git a/docs/static/security/pki-auth.asciidoc b/docs/static/security/pki-auth.asciidoc
index 7b3b8c87e6f..14908b51f62 100644
--- a/docs/static/security/pki-auth.asciidoc
+++ b/docs/static/security/pki-auth.asciidoc
@@ -4,7 +4,7 @@
 
 The `elasticsearch` output supports PKI authentication. To use an X.509
 client-certificate for authentication, you configure the `keystore` and
-`keystore_password` options in your Logstash `.conf` file:
+`keystore_password` options in your {ls} `.conf` file:
 
 [source,js]
 --------------------------------------------------
diff --git a/docs/static/security/tls-encryption.asciidoc b/docs/static/security/tls-encryption.asciidoc
index a8b9f29b8e1..087df6ca7ac 100644
--- a/docs/static/security/tls-encryption.asciidoc
+++ b/docs/static/security/tls-encryption.asciidoc
@@ -1,9 +1,9 @@
 [discrete]
 [[ls-http-ssl]]
-=== Configuring Logstash to use TLS/SSL encryption
+=== Configuring {ls} to use TLS/SSL encryption
 
 If TLS encryption is enabled on an on premise {es} cluster, you need to
-configure the `ssl` and `cacert` options in your Logstash `.conf` file:
+configure the `ssl` and `cacert` options in your {ls} `.conf` file:
 
 [source,js]
 --------------------------------------------------
diff --git a/docs/static/setting-up-logstash.asciidoc b/docs/static/setting-up-logstash.asciidoc
index b7c2864d44c..cdcbd28f233 100644
--- a/docs/static/setting-up-logstash.asciidoc
+++ b/docs/static/setting-up-logstash.asciidoc
@@ -1,9 +1,9 @@
 [[setup-logstash]]
-== Setting Up and Running Logstash
+== Setting Up and Running {ls}
 
 Before reading this section, see <<installing-logstash>> for basic installation instructions to get you started.
 
-This section includes additional information on how to set up and run Logstash, including:
+This section includes additional information on how to set up and run {ls}, including:
 
 * <<dir-layout>>
 * <<config-setting-files>>
@@ -19,9 +19,9 @@ This section includes additional information on how to set up and run Logstash,
 
 
 [[dir-layout]]
-=== Logstash Directory Layout
+=== {ls} Directory Layout
 
-This section describes the default directory structure that is created when you unpack the Logstash installation packages.
+This section describes the default directory structure that is created when you unpack the {ls} installation packages.
 
 [[zip-targz-layout]]
 ==== Directory Layout of `.zip` and `.tar.gz` Archives
@@ -30,20 +30,20 @@ The `.zip` and `.tar.gz` packages are entirely self-contained. All files and
 directories are, by default, contained within the home directory -- the directory
 created when unpacking the archive.
 
-This is very convenient because you don't have to create any directories to start using Logstash, and uninstalling
-Logstash is as easy as removing the home directory.  However, it is advisable to change the default locations of the
+This is very convenient because you don't have to create any directories to start using {ls}, and uninstalling
+{ls} is as easy as removing the home directory.  However, it is advisable to change the default locations of the
 config and the logs directories so that you do not delete important data later on.
 
 [cols="<h,<,<m,<m",options="header",]
 |=======================================================================
 | Type | Description | Default Location | Setting
 | home
-  | Home directory of the Logstash installation.
+  | Home directory of the {ls} installation.
   | `{extract.path}`- Directory created by unpacking the archive
  d|
 
 | bin
-  | Binary scripts, including `logstash` to start Logstash
+  | Binary scripts, including `logstash` to start {ls}
     and `logstash-plugin` to install plugins
   | `{extract.path}/bin`
  d|
@@ -80,12 +80,12 @@ locations for the system:
 |=======================================================================
 | Type | Description | Default Location | Setting
 | home
-  | Home directory of the Logstash installation.
+  | Home directory of the {ls} installation.
   | `/usr/share/logstash`
  d|
 
 | bin
-  | Binary scripts including `logstash` to start Logstash
+  | Binary scripts including `logstash` to start {ls}
     and `logstash-plugin` to install plugins
   | `/usr/share/logstash/bin`
  d|
@@ -96,7 +96,7 @@ locations for the system:
   | `path.settings`
 
 | conf
-  | Logstash pipeline configuration files
+  | {ls} pipeline configuration files
   | `/etc/logstash/conf.d/*.conf`
   | See `/etc/logstash/pipelines.yml`
 
@@ -127,12 +127,12 @@ similar directory layout.
 |=======================================================================
 | Type | Description | Default Location | Setting
 | home
-  | Home directory of the Logstash installation.
+  | Home directory of the {ls} installation.
   | `/usr/share/logstash`
  d|
 
 | bin
-  | Binary scripts, including `logstash` to start Logstash
+  | Binary scripts, including `logstash` to start {ls}
     and `logstash-plugin` to install plugins
   | `/usr/share/logstash/bin`
  d|
@@ -143,7 +143,7 @@ similar directory layout.
   | `path.settings`
 
 | conf
-  | Logstash pipeline configuration files
+  | {ls} pipeline configuration files
   | `/usr/share/logstash/pipeline`
   | `path.config`
 
@@ -159,20 +159,20 @@ similar directory layout.
 
 |=======================================================================
 
-NOTE: Logstash Docker containers do not create log files by default. They log
+NOTE: {ls} Docker containers do not create log files by default. They log
 to standard output.
 
 [[config-setting-files]]
-=== Logstash Configuration Files
+=== {ls} Configuration Files
 
-Logstash has two types of configuration files: _pipeline configuration files_, which define the Logstash processing
-pipeline, and _settings files_, which specify options that control Logstash startup and execution.
+{ls} has two types of configuration files: _pipeline configuration files_, which define the {ls} processing
+pipeline, and _settings files_, which specify options that control {ls} startup and execution.
 
 [[pipeline-config-files]]
 ==== Pipeline Configuration Files
 
-You create pipeline configuration files when you define the stages of your Logstash processing pipeline. On deb and
-rpm, you place the pipeline configuration files in the `/etc/logstash/conf.d` directory. Logstash tries to load only
+You create pipeline configuration files when you define the stages of your {ls} processing pipeline. On deb and
+rpm, you place the pipeline configuration files in the `/etc/logstash/conf.d` directory. {ls} tries to load only
 files with `.conf` extension in the `/etc/logstash/conf.d directory` and ignores all other files.
 
 See <<configuration>> for more info.
@@ -180,16 +180,16 @@ See <<configuration>> for more info.
 [[settings-files]]
 ==== Settings Files
 
-The settings files are already defined in the Logstash installation. Logstash includes the following settings files:
+The settings files are already defined in the {ls} installation. {ls} includes the following settings files:
 
 *`logstash.yml`*::
-  Contains Logstash configuration flags. You can set flags in this file instead of passing the flags at the command
+  Contains {ls} configuration flags. You can set flags in this file instead of passing the flags at the command
   line. Any flags that you set at the command line override the corresponding settings in the `logstash.yml` file. See <<logstash-settings-file>> for more info.
 *`pipelines.yml`*::
-  Contains the framework and instructions for running multiple pipelines in a single Logstash instance. See <<multiple-pipelines>> for more info.
+  Contains the framework and instructions for running multiple pipelines in a single {ls} instance. See <<multiple-pipelines>> for more info.
 *`jvm.options`*::
   Contains JVM configuration flags. Use this file to set initial and maximum values for
-  total heap space. You can also use this file to set the locale for Logstash.
+  total heap space. You can also use this file to set the locale for {ls}.
   Specify each flag on a separate line. All other settings in this file are
   considered expert settings.
 *`log4j2.properties`*:: Contains default settings for `log4j 2` library. See <<log4j2>> for more info.
diff --git a/docs/static/settings-file.asciidoc b/docs/static/settings-file.asciidoc
index e9a5c8e1098..bb990fb48fe 100644
--- a/docs/static/settings-file.asciidoc
+++ b/docs/static/settings-file.asciidoc
@@ -1,10 +1,10 @@
 [[logstash-settings-file]]
 === logstash.yml
 
-You can set options in the Logstash settings file, `logstash.yml`, to control Logstash execution. For example,
+You can set options in the {ls} settings file, `logstash.yml`, to control {ls} execution. For example,
 you can specify pipeline settings, the location of configuration files, logging options, and other settings.
 Most of the settings in the `logstash.yml` file are also available as <<command-line-flags,command-line flags>>
-when you run Logstash. Any flags that you set at the command line override the corresponding settings in the
+when you run {ls}. Any flags that you set at the command line override the corresponding settings in the
 `logstash.yml` file.
 
 The `logstash.yml` file is written in http://yaml.org/[YAML]. Its location varies by platform (see
@@ -74,7 +74,7 @@ The `logstash.yml` file includes the following settings.
 | Machine's hostname
 
 | `path.data`
-| The directory that Logstash and its plugins use for any persistent needs.
+| The directory that {ls} and its plugins use for any persistent needs.
 |`LOGSTASH_HOME/data`
 
 | `pipeline.id`
@@ -105,8 +105,8 @@ increasing this number to better utilize machine processing power.
 | `50`
 
 | `pipeline.unsafe_shutdown`
-| When set to `true`, forces Logstash to exit during shutdown even if there are still inflight events
-  in memory. By default, Logstash will refuse to quit until all received events
+| When set to `true`, forces {ls} to exit during shutdown even if there are still inflight events
+  in memory. By default, {ls} will refuse to quit until all received events
   have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
 | `false`
 
@@ -119,7 +119,7 @@ a|
 Set the pipeline event ordering. Valid options are:
 
 * `auto`. Automatically enables ordering if the `pipeline.workers` setting is `1`, and disables otherwise.
-* `true`. Enforces ordering on the pipeline and prevents Logstash from starting
+* `true`. Enforces ordering on the pipeline and prevents {ls} from starting
 if there are multiple workers.
 * `false`. Disables the processing required to preserve order. Ordering will not be
 guaranteed, but you save the processing cost of preserving order.
@@ -143,7 +143,7 @@ Values other than `disabled` are currently considered BETA, and may produce unin
 | `disabled`
 
 | `path.config`
-| The path to the Logstash config for the main pipeline. If you specify a directory or wildcard,
+| The path to the {ls} config for the main pipeline. If you specify a directory or wildcard,
   config files are read from the directory in alphabetical order.
 | Platform-specific. See <<dir-layout>>.
 
@@ -154,8 +154,8 @@ Values other than `disabled` are currently considered BETA, and may produce unin
 
 | `config.test_and_exit`
 | When set to `true`, checks that the configuration is valid and then exits. Note that grok patterns are not checked for
-  correctness with this setting. Logstash can read multiple config files from a directory. If you combine this
-  setting with `log.level: debug`, Logstash will log the combined config file, annotating
+  correctness with this setting. {ls} can read multiple config files from a directory. If you combine this
+  setting with `log.level: debug`, {ls} will log the combined config file, annotating
   each config block with the source file it came from.
 | `false`
 
@@ -165,7 +165,7 @@ Values other than `disabled` are currently considered BETA, and may produce unin
 | `false`
 
 | `config.reload.interval`
-| How often in seconds Logstash checks the config files for changes. Note that the unit qualifier (`s`) is required.
+| How often in seconds {ls} checks the config files for changes. Note that the unit qualifier (`s`) is required.
 | `3s`
 
 | `config.debug`
@@ -212,7 +212,7 @@ Current options are:
 | 0 (unlimited)
 
 | `queue.max_bytes`
-| The total capacity of the queue (`queue.type: persisted`) in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both `queue.max_events` and `queue.max_bytes` are specified, Logstash uses whichever criteria is reached first.
+| The total capacity of the queue (`queue.type: persisted`) in number of bytes. Make sure the capacity of your disk drive is greater than the value you specify here. If both `queue.max_events` and `queue.max_bytes` are specified, {ls} uses whichever criteria is reached first.
 | 1024mb (1g)
 
 | `queue.checkpoint.acks`
@@ -224,15 +224,15 @@ Current options are:
 | 1024
 
 | `queue.checkpoint.retry`
-| When enabled, Logstash will retry four times per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on Windows platform, filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. (`queue.type: persisted`)
+| When enabled, {ls} will retry four times per attempted checkpoint write for any checkpoint writes that fail. Any subsequent errors are not retried. This is a workaround for failed checkpoint writes that have been seen only on Windows platform, filesystems with non-standard behavior such as SANs and is not recommended except in those specific circumstances. (`queue.type: persisted`)
 | `true`
 
 | `queue.drain`
-| When enabled, Logstash waits until the persistent queue (`queue.type: persisted`) is drained before shutting down.
+| When enabled, {ls} waits until the persistent queue (`queue.type: persisted`) is drained before shutting down.
 | `false`
 
 | `dead_letter_queue.enable`
-| Flag to instruct Logstash to enable the DLQ feature supported by plugins.
+| Flag to instruct {ls} to enable the DLQ feature supported by plugins.
 | `false`
 
 | `dead_letter_queue.max_bytes`
@@ -337,12 +337,12 @@ The log level. Valid options are:
 | `plain`
 
 | `path.logs`
-| The directory where Logstash will write its log to.
+| The directory where {ls} will write its log to.
 | `LOGSTASH_HOME/logs`
 
 | `pipeline.separate_logs`
-|  This a boolean setting to enable separation of logs per pipeline in different log files. If enabled Logstash will create a different log file for each pipeline,
-using the pipeline.id as name of the file. The destination directory is taken from the `path.log`s setting. When there are many pipelines configured in Logstash,
+|  This a boolean setting to enable separation of logs per pipeline in different log files. If enabled {ls} will create a different log file for each pipeline,
+using the pipeline.id as name of the file. The destination directory is taken from the `path.log`s setting. When there are many pipelines configured in {ls},
 separating each log lines per pipeline could be helpful in case you need to troubleshoot what’s happening in a single pipeline, without interference of the other ones.
 | `false`
 
@@ -354,12 +354,12 @@ separating each log lines per pipeline could be helpful in case you need to trou
 | Platform-specific. See <<dir-layout>>.
 
 | `allow_superuser`
-| Setting to `true` to allow or `false` to block running Logstash as a superuser.
+| Setting to `true` to allow or `false` to block running {ls} as a superuser.
 | `true`
 
 | `event_api.tags.illegal`
 | When set to `warn`, allow illegal value assignment to the reserved `tags` field.
-When set to `rename`, Logstash events can't be created with an illegal value in `tags`. This value will be moved to `_tags` and a `_tagsparsefailure` tag is added to indicate the illegal operation. Doing `set` operation with illegal value will throw exception.
+When set to `rename`, {ls} events can't be created with an illegal value in `tags`. This value will be moved to `_tags` and a `_tagsparsefailure` tag is added to indicate the illegal operation. Doing `set` operation with illegal value will throw exception.
 Setting this flag to `warn` is deprecated and will be removed in a future release.
 | `rename`
 |=======================================================================
diff --git a/docs/static/settings/configuration-management-settings.asciidoc b/docs/static/settings/configuration-management-settings.asciidoc
index 450ba0e33f7..79d55fc3040 100644
--- a/docs/static/settings/configuration-management-settings.asciidoc
+++ b/docs/static/settings/configuration-management-settings.asciidoc
@@ -1,6 +1,6 @@
 [role="xpack"]
 [[configuration-management-settings]]
-==== Configuration Management Settings in Logstash
+==== Configuration Management Settings in {ls}
 ++++
 <titleabbrev>Configuration Management Settings</titleabbrev>
 ++++
@@ -8,7 +8,7 @@
 You can set the following `xpack.management` settings in `logstash.yml` to
 enable
 <<logstash-centralized-pipeline-management,centralized pipeline management>>.
-For more information about configuring Logstash, see <<logstash-settings-file>>.
+For more information about configuring {ls}, see <<logstash-settings-file>>.
 
 The following example shows basic settings that assume {es} and {kib} are
 installed on the localhost with basic AUTH enabled, but no SSL. If you're using
@@ -28,31 +28,31 @@ xpack.management.pipeline.id: ["apache", "cloudwatch_logs"]
 `xpack.management.enabled`::
 
 Set to `true` to enable {xpack} centralized configuration management for
-Logstash.
+{ls}.
 
 `xpack.management.logstash.poll_interval`::
 
-How often the Logstash instance polls for pipeline changes from Elasticsearch.
+How often the {ls} instance polls for pipeline changes from Elasticsearch.
 The default is 5s.
 
 `xpack.management.pipeline.id`::
 
 Specify a comma-separated list of pipeline IDs to register for centralized
-pipeline management. After changing this setting, you need to restart Logstash
+pipeline management. After changing this setting, you need to restart {ls}
 to pick up changes.
 Pipeline IDs support `*` as a <<wildcard-in-pipeline-id, wildcard>> for matching multiple IDs
 
 `xpack.management.elasticsearch.hosts`::
 
-The {es} instance that will store the Logstash pipeline configurations and
+The {es} instance that will store the {ls} pipeline configurations and
 metadata. This might be the same {es} instance specified in the `outputs`
-section in your Logstash configuration, or a different one. Defaults to
+section in your {ls} configuration, or a different one. Defaults to
 `http://localhost:9200`.
 
 `xpack.management.elasticsearch.username` and `xpack.management.elasticsearch.password`::
 
 If your {es} cluster is protected with basic authentication, these settings
-provide the username and password that the Logstash instance uses to
+provide the username and password that the {ls} instance uses to
 authenticate for accessing the configuration data. The username you specify here
 should have the built-in `logstash_admin` role and the customized `logstash_writer` role, which provides access to system
 indices for managing configurations. Starting with Elasticsearch version 7.10.0, the
@@ -61,7 +61,7 @@ If a user has created their own roles and granted them access to the .logstash i
 
 `xpack.management.elasticsearch.proxy`::
 
-Optional setting that allows you to specify a proxy URL if Logstash needs to use a proxy
+Optional setting that allows you to specify a proxy URL if {ls} needs to use a proxy
 to reach your Elasticsearch cluster.
 
 `xpack.management.elasticsearch.ssl.ca_trusted_fingerprint`::
@@ -135,7 +135,7 @@ Supported cipher suites vary depending on the Java and protocol versions.
 If you're using {es} in {ecloud}, you should specify the identifier here.
 This setting is an alternative to `xpack.management.elasticsearch.hosts`.
 If `cloud_id` is configured, `xpack.management.elasticsearch.hosts` should not be used.
-This {es} instance will store the Logstash pipeline configurations and metadata.
+This {es} instance will store the {ls} pipeline configurations and metadata.
 
 `xpack.management.elasticsearch.cloud_auth`::
 
diff --git a/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc b/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc
index 0304f7ba674..c9645324a32 100644
--- a/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc
+++ b/docs/static/settings/configuration-wildcard-pipeline-id.asciidoc
@@ -14,6 +14,6 @@ xpack.management.pipeline.id: ["*logs", "*apache*", "tomcat_log"]
 
 In this example, `"*logs"` matches all IDs ending in `logs`. `"*apache*"` matches any IDs with `apache` in the name.
 
-Wildcard in pipeline IDs is available starting with Elasticsearch 7.10. Logstash can pick up new pipeline without a restart if the new pipeline ID matches the wildcard pattern.
+Wildcard in pipeline IDs is available starting with Elasticsearch 7.10. {ls} can pick up new pipeline without a restart if the new pipeline ID matches the wildcard pattern.
 
 
diff --git a/docs/static/settings/monitoring-settings-legacy.asciidoc b/docs/static/settings/monitoring-settings-legacy.asciidoc
index 414dd6361f2..95100601a84 100644
--- a/docs/static/settings/monitoring-settings-legacy.asciidoc
+++ b/docs/static/settings/monitoring-settings-legacy.asciidoc
@@ -6,9 +6,9 @@
 ++++
 
 You can set the following `xpack.monitoring` settings in `logstash.yml` to
-control how monitoring data is collected from your Logstash nodes. However, the
+control how monitoring data is collected from your {ls} nodes. However, the
 defaults work best in most circumstances. For more information about configuring
-Logstash, see <<logstash-settings-file>>.
+{ls}, see <<logstash-settings-file>>.
 
 
 [[monitoring-general-settings-legacy]]
@@ -20,31 +20,31 @@ Monitoring is disabled by default. Set to `true` to enable {xpack} monitoring.
 
 `xpack.monitoring.elasticsearch.hosts`::
 
-The {es} instances that you want to ship your Logstash metrics to. This might be
-the same {es} instance specified in the `outputs` section in your Logstash
+The {es} instances that you want to ship your {ls} metrics to. This might be
+the same {es} instance specified in the `outputs` section in your {ls}
 configuration, or a different one. This is *not* the URL of your dedicated
 monitoring cluster. Even if you are using a dedicated monitoring cluster, the
-Logstash metrics must be routed through your production cluster. You can specify
+{ls} metrics must be routed through your production cluster. You can specify
 a single host as a string, or specify multiple hosts as an array. Defaults to
 `http://localhost:9200`.
 
 NOTE: If your Elasticsearch cluster is configured with dedicated master-eligible
-nodes, Logstash metrics should _not_ be routed to these nodes, as doing so can
+nodes, {ls} metrics should _not_ be routed to these nodes, as doing so can
 create resource contention and impact the stability of the Elasticsearch
 cluster. Therefore, do not include such nodes in
 `xpack.monitoring.elasticsearch.hosts`.
 
 `xpack.monitoring.elasticsearch.proxy`::
 
-The monitoring {es} instance and monitored Logstash can be separated by a proxy.
-To enable Logstash to connect to a proxied {es}, set this value to the URI of the intermediate
+The monitoring {es} instance and monitored {ls} can be separated by a proxy.
+To enable {ls} to connect to a proxied {es}, set this value to the URI of the intermediate
 proxy using the standard URI format, `<protocol>://<host>` for example `http://192.168.1.1`.
 An empty string is treated as if proxy was not set.
 
 `xpack.monitoring.elasticsearch.username` and `xpack.monitoring.elasticsearch.password`::
 
 If your {es} is protected with basic authentication, these settings provide the
-username and password that the Logstash instance uses to authenticate for
+username and password that the {ls} instance uses to authenticate for
 shipping monitoring data.
 
 
@@ -53,7 +53,7 @@ shipping monitoring data.
 
 `xpack.monitoring.collection.interval`::
 
-Controls how often data samples are collected and shipped on the Logstash side.
+Controls how often data samples are collected and shipped on the {ls} side.
 Defaults to `10s`. If you modify the collection interval, set the 
 `xpack.monitoring.min_interval_seconds` option in `kibana.yml` to the same value.
 
@@ -135,8 +135,8 @@ Supported cipher suites vary depending on the Java and protocol versions.
 If you're using {es} in {ecloud}, you should specify the identifier here.
 This setting is an alternative to `xpack.monitoring.elasticsearch.hosts`.
 If `cloud_id` is configured, `xpack.monitoring.elasticsearch.hosts` should not be used.
-The {es} instances that you want to ship your Logstash metrics to. This might be
-the same {es} instance specified in the `outputs` section in your Logstash
+The {es} instances that you want to ship your {ls} metrics to. This might be
+the same {es} instance specified in the `outputs` section in your {ls}
 configuration, or a different one.
 
 `xpack.monitoring.elasticsearch.cloud_auth`::
diff --git a/docs/static/shared-module-options.asciidoc b/docs/static/shared-module-options.asciidoc
index 3e9b2eb12c0..1592faf76de 100644
--- a/docs/static/shared-module-options.asciidoc
+++ b/docs/static/shared-module-options.asciidoc
@@ -11,9 +11,9 @@ The following configuration options are supported by all modules:
 +
 Sets the host(s) of the Elasticsearch cluster. For each host, you must specify
 the hostname and port. For example, "myhost:9200". If given an <<array,array>>,
-Logstash will load balance requests across the hosts specified in the hosts
+{ls} will load balance requests across the hosts specified in the hosts
 parameter. It is important to exclude {ref}/modules-node.html[dedicated master
-nodes] from the hosts list to prevent Logstash from sending bulk requests to the
+nodes] from the hosts list to prevent {ls} from sending bulk requests to the
 master nodes. So this parameter should only reference either data or client
 nodes in Elasticsearch.
 +
diff --git a/docs/static/shutdown.asciidoc b/docs/static/shutdown.asciidoc
index 18835c06664..50da3dca66b 100644
--- a/docs/static/shutdown.asciidoc
+++ b/docs/static/shutdown.asciidoc
@@ -1,5 +1,5 @@
 [[shutdown]]
-=== Shutting Down Logstash
+=== Shutting Down {ls}
 
 If you're running {ls} as a service, use one of the following commands to stop it:
 
@@ -24,11 +24,11 @@ Alternatively, enter *Ctrl-C* in the console.
 
 ==== What Happens During a Controlled Shutdown?
 
-When you attempt to shut down a running Logstash instance, Logstash performs several steps before it can safely shut down. It must:
+When you attempt to shut down a running {ls} instance, {ls} performs several steps before it can safely shut down. It must:
 
 * Stop all input, filter and output plugins
 * Process all in-flight events
-* Terminate the Logstash process
+* Terminate the {ls} process
 
 The following conditions affect the shutdown process:
 
@@ -39,21 +39,21 @@ query.
 
 These situations make the duration and success of the shutdown process unpredictable.
 
-Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
+{ls} has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
 This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
 worker threads.
 
-To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--pipeline.unsafe_shutdown` flag when
-you start Logstash.
+To enable {ls} to forcibly terminate in the case of a stalled shutdown, use the `--pipeline.unsafe_shutdown` flag when
+you start {ls}.
 
-WARNING: Unsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason may result in data loss (unless you've
-enabled Logstash to use <<persistent-queues,persistent queues>>). Shut down
-Logstash safely whenever possible.
+WARNING: Unsafe shutdowns, force-kills of the {ls} process, or crashes of the {ls} process for any other reason may result in data loss (unless you've
+enabled {ls} to use <<persistent-queues,persistent queues>>). Shut down
+{ls} safely whenever possible.
 
 [[shutdown-stall-example]]
 ==== Stall Detection Example
 
-In this example, slow filter execution prevents the pipeline from performing a clean shutdown. Because Logstash is
+In this example, slow filter execution prevents the pipeline from performing a clean shutdown. Because {ls} is
 started with the `--pipeline.unsafe_shutdown` flag, the shutdown results in the loss of 20 events.
 
 ========
@@ -64,7 +64,7 @@ Pipeline main started
 ^CSIGINT received. Shutting down the agent. {:level=>:warn}
 stopping pipeline {:id=>"main", :level=>:warn}
 Received shutdown signal, but pipeline is still waiting for in-flight events
-to be processed. Sending another ^C will force quit Logstash, but this may cause
+to be processed. Sending another ^C will force quit {ls}, but this may cause
 data loss. {:level=>:warn}
 {"inflight_count"=>125, "stalling_thread_info"=>{["LogStash::Filters::Ruby", 
 {"code"=>"sleep 10000"}]=>[{"thread_id"=>19, "name"=>"[main]>worker0", 
@@ -80,4 +80,4 @@ Check the logs for more information. {:level=>:error}
 Forcefully quitting logstash.. {:level=>:fatal}
 ========
 
-When `--pipeline.unsafe_shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
+When `--pipeline.unsafe_shutdown` isn't enabled, {ls} continues to run and produce these reports periodically.
diff --git a/docs/static/submitting-a-plugin.asciidoc b/docs/static/submitting-a-plugin.asciidoc
index b0f2b2280f2..0ae029be4ff 100644
--- a/docs/static/submitting-a-plugin.asciidoc
+++ b/docs/static/submitting-a-plugin.asciidoc
@@ -1,12 +1,12 @@
 [[publish-plugin]]
 === Publish your plugin to RubyGems.org
 
-Logstash uses http://rubygems.org[RubyGems.org] as its repository for all plugin artifacts. 
+{ls} uses http://rubygems.org[RubyGems.org] as its repository for all plugin artifacts. 
 After you have developed your new plugin, you can make it available to
-Logstash users by publishing it to RubyGems.org.
+{ls} users by publishing it to RubyGems.org.
 
 ==== Licensing
-Logstash and all its plugins are licensed under
+{ls} and all its plugins are licensed under
 https://github.com/elasticsearch/logstash/blob/main/LICENSE[Apache License, version 2 ("ALv2")].
 If you make your plugin publicly available via http://rubygems.org[RubyGems.org],
 please make sure to have this line in your gemspec:
@@ -59,7 +59,7 @@ in your local repository.
 . Publishes the gem to RubyGems.org
 ========
 
-That's it! Your plugin is published! Logstash users can now install your plugin
+That's it! Your plugin is published! {ls} users can now install your plugin
 by running:
 
 [source,sh]
diff --git a/docs/static/transforming-data.asciidoc b/docs/static/transforming-data.asciidoc
index 9c2e8125e7d..acfdcb15ef6 100644
--- a/docs/static/transforming-data.asciidoc
+++ b/docs/static/transforming-data.asciidoc
@@ -1,7 +1,7 @@
 [[transformation]]
 == Transforming Data
 
-With over 200 plugins in the Logstash plugin ecosystem, it's sometimes
+With over 200 plugins in the {ls} plugin ecosystem, it's sometimes
 challenging to choose the best plugin to meet your data processing needs.
 In this section, we've collected a list of popular plugins and organized them
 according to their processing capabilities:
@@ -22,9 +22,9 @@ mutating and dropping events.
 
 <<plugins-filters-date,date filter>>::
 
-Parses dates from fields to use as Logstash timestamps for events.
+Parses dates from fields to use as {ls} timestamps for events.
 +
-The following config parses a field called `logdate` to set the Logstash
+The following config parses a field called `logdate` to set the {ls}
 timestamp:
 +
 [source,json]
@@ -122,11 +122,11 @@ filter {
 === Deserializing Data
 
 The plugins described in this section are useful for deserializing data into
-Logstash events.
+{ls} events.
 
 <<plugins-codecs-avro,avro codec>>::
 
-Reads serialized Avro records as Logstash events. This plugin deserializes
+Reads serialized Avro records as {ls} events. This plugin deserializes
 individual Avro records. It is not for reading Avro files. Avro files have a
 unique format that must be handled upon input.
 +
@@ -202,7 +202,7 @@ input {
 
 <<plugins-codecs-protobuf,protobuf codec>>::
 
-Reads protobuf encoded messages and converts them to Logstash events. Requires
+Reads protobuf encoded messages and converts them to {ls} events. Requires
 the protobuf definitions to be compiled as Ruby files. You can compile them by
 using the
 https://github.com/codekitchen/ruby-protocol-buffers[ruby-protoc compiler].
@@ -399,7 +399,7 @@ filter {
 The <<plugins-filters-elasticsearch,elasticsearch filter>> copies fields from previous log events in Elasticsearch to current events.
 +
 The following config shows a complete example of how this filter might
-be used.  Whenever Logstash receives an "end" event, it uses this Elasticsearch
+be used.  Whenever {ls} receives an "end" event, it uses this Elasticsearch
 filter to find the matching "start" event based on some operation identifier.
 Then it copies the `@timestamp` field from the "start" event into a new field on
 the "end" event.  Finally, using a combination of the date filter and the
diff --git a/docs/static/troubleshoot/plugin-tracing.asciidoc b/docs/static/troubleshoot/plugin-tracing.asciidoc
index d2c1067f4f1..cd4ba032291 100644
--- a/docs/static/troubleshoot/plugin-tracing.asciidoc
+++ b/docs/static/troubleshoot/plugin-tracing.asciidoc
@@ -12,7 +12,7 @@ While you can define an "id" for each plugin to facilitate this discovery,
 naming each plugin is not practical, especially for very large pipelines
 containing hundreds of plugins. 
 
-You can use the information from the Logstash APIs to fetch link an auto
+You can use the information from the {ls} APIs to fetch link an auto
 generated ID of a plugin to its declaration (starting with 7.6.0).
 
 Here's how:
diff --git a/docs/static/troubleshoot/troubleshooting.asciidoc b/docs/static/troubleshoot/troubleshooting.asciidoc
index b4c8ee7a0d7..d7fac31fd34 100644
--- a/docs/static/troubleshoot/troubleshooting.asciidoc
+++ b/docs/static/troubleshoot/troubleshooting.asciidoc
@@ -21,7 +21,7 @@ https://github.com/elastic/logstash/issues, or
 [discrete]
 [[discuss]]
 === Discussion forums
-Also check out the https://discuss.elastic.co/c/logstash[Logstash discussion
+Also check out the https://discuss.elastic.co/c/logstash[{ls} discussion
 forum].
 
 include::ts-logstash.asciidoc[]
diff --git a/docs/static/troubleshoot/ts-kafka.asciidoc b/docs/static/troubleshoot/ts-kafka.asciidoc
index e5b4fa3d389..0d8043c8793 100644
--- a/docs/static/troubleshoot/ts-kafka.asciidoc
+++ b/docs/static/troubleshoot/ts-kafka.asciidoc
@@ -107,7 +107,7 @@ NOTE: The default setting of `auto` is the best option for most circumstances an
 
 *Symptoms*
 
-Logstash’s Kafka Input is causing a much higher number of commits to
+{ls}’s Kafka Input is causing a much higher number of commits to
 the offset topic than expected. Often the complaint also mentions redundant
 offset commits where the same offset is committed repeatedly.
 
@@ -132,7 +132,7 @@ this scenario. For example, raising it by 10x will lead to 10x fewer offset comm
 
 *Symptoms*
 
-Logstash Kafka input randomly logs errors from the configured codec and/or reads
+{ls} Kafka input randomly logs errors from the configured codec and/or reads
 events incorrectly (partial reads, mixing data between multiple events etc.).
 
 -----
diff --git a/docs/static/troubleshoot/ts-logstash.asciidoc b/docs/static/troubleshoot/ts-logstash.asciidoc
index 42288c4d3da..2a2efa03734 100644
--- a/docs/static/troubleshoot/ts-logstash.asciidoc
+++ b/docs/static/troubleshoot/ts-logstash.asciidoc
@@ -17,7 +17,7 @@ executable files to the temp directory. This situation causes subsequent failure
 
 [source,sh]
 -----
-[2018-03-25T12:23:01,149][ERROR][org.logstash.Logstash ]
+[2018-03-25T12:23:01,149][ERROR][org.logstash.{ls} ]
 java.lang.IllegalStateException: org.jruby.exceptions.RaiseException:
 (LoadError) Could not load FFI Provider: (NotImplementedError) FFI not
 available: java.lang.UnsatisfiedLinkError: /tmp/jffi5534463206038012403.so:
@@ -39,7 +39,7 @@ Operation not permitted
 
 // https://github.com/elastic/logstash/issues/10496 and https://github.com/elastic/logstash/issues/10498
 
-After an upgrade, Logstash may show warnings similar to these:
+After an upgrade, {ls} may show warnings similar to these:
 
 [source,sh]
 -----
@@ -67,7 +67,7 @@ Try adding these values to the `jvm.options` file.
 
 *Notes:*
 
-* These settings allow Logstash to start without warnings.
+* These settings allow {ls} to start without warnings.
 * This workaround has been tested with simple pipelines. If you have experiences
 to share, please comment in the
 https://github.com/elastic/logstash/issues/10496[issue].
@@ -76,14 +76,14 @@ https://github.com/elastic/logstash/issues/10496[issue].
 [[ts-windows-permission-denied-NUL]]
 ===== 'Permission denied - NUL' errors on Windows
 
-Logstash may not start with some user-supplied versions of the JDK on Windows.  
+{ls} may not start with some user-supplied versions of the JDK on Windows.  
 
 
 *Sample error*
 
 [source,sh]
 -----
-[FATAL] 2022-04-27 15:13:16.650 [main] Logstash - Logstash stopped processing because of an error: (EACCES) Permission denied - NUL
+[FATAL] 2022-04-27 15:13:16.650 [main] {ls} - {ls} stopped processing because of an error: (EACCES) Permission denied - NUL
 org.jruby.exceptions.SystemCallError: (EACCES) Permission denied - NUL
 -----
 
@@ -98,8 +98,8 @@ This issue affects some OpenJDK-derived JVM versions (Adoptium, OpenJDK, and Azu
 
 *Work around*
 
-* Use the {logstash-ref}/getting-started-with-logstash.html#ls-jvm[bundled JDK] included with Logstash
-* Or, try adding this value to the `jvm.options` file, and restarting Logstash
+* Use the {logstash-ref}/getting-started-with-logstash.html#ls-jvm[bundled JDK] included with {ls}
+* Or, try adding this value to the `jvm.options` file, and restarting {ls}
 +
 [source,sh]
 -----
@@ -127,8 +127,8 @@ queue section for more information on remediating problems with persistent queue
 ===== Error response code 429
 
 A `429` message indicates that an application is busy handling other requests. For
-example, Elasticsearch sends a `429` code to notify Logstash (or other indexers)
-that the bulk failed because the ingest queue is full. Logstash will retry sending documents.
+example, Elasticsearch sends a `429` code to notify {ls} (or other indexers)
+that the bulk failed because the ingest queue is full. {ls} will retry sending documents.
 
 *Possible actions*
 
@@ -198,8 +198,8 @@ Inputs and outputs might be affected, too.
 
 *Background*
 
-The different plugins running on Logstash can be quite verbose if the logging level is set to `debug` or `trace`.
-As the logging library used in Logstash is synchronous, heavy logging can affect performances.
+The different plugins running on {ls} can be quite verbose if the logging level is set to `debug` or `trace`.
+As the logging library used in {ls} is synchronous, heavy logging can affect performances.
 
 *Solution*
 
diff --git a/docs/static/troubleshoot/ts-other-issues.asciidoc b/docs/static/troubleshoot/ts-other-issues.asciidoc
index 25727063f20..47bb81d533d 100644
--- a/docs/static/troubleshoot/ts-other-issues.asciidoc
+++ b/docs/static/troubleshoot/ts-other-issues.asciidoc
@@ -8,7 +8,7 @@ Coming soon, and you can help! If you have something to add, please:
 https://github.com/elastic/logstash/issues, or
 * create a pull request with your proposed changes at https://github.com/elastic/logstash.
 
-Also check out the https://discuss.elastic.co/c/logstash[Logstash discussion
+Also check out the https://discuss.elastic.co/c/logstash[{ls} discussion
 forum].
 
 
diff --git a/docs/static/upgrading.asciidoc b/docs/static/upgrading.asciidoc
index 3b1a26da550..c05c9bb9a99 100644
--- a/docs/static/upgrading.asciidoc
+++ b/docs/static/upgrading.asciidoc
@@ -1,25 +1,25 @@
 [[upgrading-logstash]]
-== Upgrading Logstash
+== Upgrading {ls}
 
 [IMPORTANT]
 ===========================================
-Before upgrading Logstash:
+Before upgrading {ls}:
 
 * Consult the <<breaking-changes,breaking changes>> docs.
 * Read the <<releasenotes>>.
 * Test upgrades in a development environment before upgrading your production cluster.
 
-While upgrading Logstash:
+While upgrading {ls}:
 
 * If you use monitoring, you must re-use the data directory when you
-upgrade Logstash. Otherwise, the Logstash node is assigned a new persistent UUID
+upgrade {ls}. Otherwise, the {ls} node is assigned a new persistent UUID
 and becomes a new node in the monitoring data.
 ===========================================
 
 If you're upgrading other products in the stack, also read the
 {stack-ref}/index.html[Elastic Stack Installation and Upgrade Guide]. 
 
-See the following topics for information about upgrading Logstash:
+See the following topics for information about upgrading {ls}:
 
 * <<upgrading-using-package-managers>>
 * <<upgrading-using-direct-download>>
@@ -31,62 +31,62 @@ See the following topics for information about upgrading Logstash:
 
 Fresh installations can and should start with the same version across the Elastic Stack.
 
-Elasticsearch 8.0 does not require Logstash 8.0. An Elasticsearch 8.0 cluster
-will happily receive data from earlier versions of Logstash via the default
+Elasticsearch 8.0 does not require {ls} 8.0. An Elasticsearch 8.0 cluster
+will happily receive data from earlier versions of {ls} via the default
 HTTP communication layer. This provides some flexibility to decide when to
-upgrade Logstash relative to an Elasticsearch upgrade. It may or may not be
+upgrade {ls} relative to an Elasticsearch upgrade. It may or may not be
 convenient for you to upgrade them together, and it is not required to be done
 at the same time as long as Elasticsearch is upgraded first.
 
 You should upgrade in a timely manner to get the performance improvements that
-come with Logstash 8.0, but do so in the way that makes the most sense for your
+come with {ls} 8.0, but do so in the way that makes the most sense for your
 environment.
 
 [discrete]
 ==== When not to upgrade
 
-If any Logstash plugin that you require is not compatible with Logstash 8.0, then you should wait until it is ready
+If any {ls} plugin that you require is not compatible with {ls} 8.0, then you should wait until it is ready
 before upgrading.
 
-Although we make great efforts to ensure compatibility, Logstash 8.0 is not completely backwards compatible. 
-As noted in the Elastic Stack upgrade guide, you should not upgrade Logstash 8.0 before you upgrade Elasticsearch 8.0. 
+Although we make great efforts to ensure compatibility, {ls} 8.0 is not completely backwards compatible. 
+As noted in the Elastic Stack upgrade guide, you should not upgrade {ls} 8.0 before you upgrade Elasticsearch 8.0. 
 This is both
-practical and because some Logstash 8.0 plugins may attempt to use features of Elasticsearch 8.0 that did not exist
+practical and because some {ls} 8.0 plugins may attempt to use features of Elasticsearch 8.0 that did not exist
 in earlier versions. 
 
 For example, if you attempt to send the 8.x template to a cluster before
 Elasticsearch 8.0, then  all indexing likely fail. 
-If you use your own custom template with Logstash, then this issue can be ignored.
+If you use your own custom template with {ls}, then this issue can be ignored.
 
 
 [[upgrading-using-package-managers]]
 === Upgrading using package managers
 
-This procedure uses <<package-repositories,package managers>> to upgrade Logstash.
+This procedure uses <<package-repositories,package managers>> to upgrade {ls}.
 
-. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
+. Shut down your {ls} pipeline, including any inputs that send events to {ls}.
 . Using the directions in the <<package-repositories>> section, update your repository
 links to point to the 8.x repositories.
 . Run the `apt-get upgrade logstash` or `yum update logstash` command as appropriate for your operating system.
 . Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
-some Logstash plugins have changed in the 8.x release.
-. Restart your Logstash pipeline after you have updated your configuration file.
+some {ls} plugins have changed in the 8.x release.
+. Restart your {ls} pipeline after you have updated your configuration file.
 
 [[upgrading-using-direct-download]]
 === Upgrading using a direct download
 
-This procedure downloads the relevant Logstash binaries directly from Elastic.
+This procedure downloads the relevant {ls} binaries directly from Elastic.
 
-. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
-. Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
+. Shut down your {ls} pipeline, including any inputs that send events to {ls}.
+. Download the https://www.elastic.co/downloads/logstash[{ls} installation file] that matches your host environment.
 . Backup your `config/` and `data/` folders in a temporary space.
-. Delete your Logstash directory.
-. Unpack the installation file into the folder that contained the Logstash directory that you just deleted.
+. Delete your {ls} directory.
+. Unpack the installation file into the folder that contained the {ls} directory that you just deleted.
 . Restore the `config/` and `data/` folders that were previously saved, overwriting the folders created during the unpack operation.
 . Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command.
 Configuration options for
-some Logstash plugins have changed.
-. Restart your Logstash pipeline after updating your configuration file.
+some {ls} plugins have changed.
+. Restart your {ls} pipeline after updating your configuration file.
 
 [[upgrading-minor-versions]]
 === Upgrading between minor versions
@@ -102,9 +102,9 @@ not supported.
 
 
 [[upgrading-logstash-8.0]]
-=== Upgrading Logstash to 8.0
+=== Upgrading {ls} to 8.0
 
-Before upgrading Logstash:
+Before upgrading {ls}:
 
 * Read the <<releasenotes>>.
 * Read the <<breaking-changes,breaking changes>> docs. 
@@ -116,7 +116,7 @@ There you can find info on these topics and more:
 ** <<bc-field-ref-parser,Field parser is more strict>>
 
  
-If you are installing Logstash with other components in the Elastic Stack, also see the
+If you are installing {ls} with other components in the Elastic Stack, also see the
 {stack-ref}/index.html[Elastic Stack installation and upgrade documentation].
 
 NOTE: Upgrading between non-consecutive major versions (6.x to 8.x, for example) is not supported. 
