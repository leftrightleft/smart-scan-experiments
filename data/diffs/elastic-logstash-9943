diff --git a/docs/index.asciidoc b/docs/index.asciidoc
index 25352ba5866..b8abc2a3005 100644
--- a/docs/index.asciidoc
+++ b/docs/index.asciidoc
@@ -212,6 +212,14 @@ include::{plugins-repo-dir}/plugins/outputs.asciidoc[]
 include::{plugins-repo-dir}/plugins/filters.asciidoc[]
 include::{plugins-repo-dir}/plugins/codecs.asciidoc[]
 
+// FAQ and Troubleshooting
+
+// :edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/faq.asciidoc
+include::static/best-practice.asciidoc[]
+
+// :edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/troubleshooting.asciidoc
+include::static/troubleshooting.asciidoc[]
+
 :edit_url:
 
 // Contributing to Logstash
@@ -241,6 +249,7 @@ include::static/maintainer-guide.asciidoc[]
 :edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/submitting-a-plugin.asciidoc
 include::static/submitting-a-plugin.asciidoc[]
 
+
 // Glossary of Terms
 
 :edit_url: https://github.com/elastic/logstash/edit/{branch}/docs/static/glossary.asciidoc
diff --git a/docs/static/best-practice.asciidoc b/docs/static/best-practice.asciidoc
new file mode 100644
index 00000000000..2be607b14b5
--- /dev/null
+++ b/docs/static/best-practice.asciidoc
@@ -0,0 +1,112 @@
+[[tips]] 
+== Tips and Best Practices
+
+We are adding more tips and best practices, so please check back soon. 
+If you have something to add, please:
+
+* create an issue at
+https://github.com/elastic/logstash/issues, or
+* create a pull request with your proposed changes at https://github.com/elastic/logstash.
+
+// After merge, update PR link to link directly to this topic in GH
+
+Also check out the https://discuss.elastic.co/c/logstash[Logstash discussion
+forum].
+
+[float] 
+[[tip-cli]] 
+=== Command line
+
+[float] 
+[[tip-windows-cli]] 
+==== Shell commands on Windows OS
+
+Command line examples often show single quotes. 
+On Windows systems, replace a single quote `'` with a double quote `"`. 
+
+*Example*
+
+Instead of:
+
+-----
+bin/logstash -e 'input { stdin { } } output { stdout {} }'
+-----
+
+Use this format on Windows systems:
+
+-----
+bin\logstash -e "input { stdin { } } output { stdout {} }"
+-----
+
+[float]
+[[tip-pipelines]]
+=== Pipelines
+
+[float]
+[[tip-pipeline-mgmt]]
+==== Pipeline management
+
+You can manage pipelines in a {ls} instance using either local pipeline configurations or
+{logstash-ref}/configuring-centralized-pipelines.html[centralized pipeline management]
+in {kib}.
+
+After you configure Logstash to use centralized pipeline management, you can
+no longer specify local pipeline configurations. The `pipelines.yml` file and
+settings such as `path.config` and `config.string` are inactive when centralized
+pipeline management is enabled.
+
+[float]
+[[tip-kafka]]
+=== Kafka
+
+[float]
+[[tip-kafka-settings]]
+==== Kafka settings
+
+[float]
+[[tip-kafka-partitions]]
+===== Partitions per topic
+
+"How many partitions should I use per topic?"
+
+At least the number of {ls} nodes multiplied by consumer threads per node.
+
+Better yet, use a multiple of the above number. Increasing the number of
+partitions for an existing topic is extremely complicated. Partitions have a
+very low overhead. Using 5 to 10 times the number of partitions suggested by the
+first point is generally fine, so long as the overall partition count does not
+exceed 2000.
+
+Err on the side of over-partitioning up to a total 1000
+partitions overall. Try not to exceed 1000 partitions.
+
+[float]
+[[tip-kafka-threads]]
+===== Consumer threads
+
+"How many consumer threads should I configure?"
+
+Lower values tend to be more efficient and have less memory overhead. Try a
+value of `1` then iterate your way up. The value should in general be lower than
+the number of pipeline workers. Values larger than 4 rarely result in
+performance improvement.
+
+[float]
+[[tip-kafka-pq-persist]]
+==== Kafka input and persistent queue (PQ)
+
+[float]
+[[tip-kafka-offset-commit]]
+===== Kafka offset commits
+
+"Does Kafka Input commit offsets only after the event has been safely persisted to the PQ?"
+
+"Does Kafa Input commit offsets only for events that have passed the pipeline fully?"
+
+No, we can’t make that guarantee. Offsets are committed to Kafka periodically. If
+writes to the PQ are slow or blocked, offsets for events that haven’t safely
+reached the PQ can be committed.
+
+
+
+
diff --git a/docs/static/troubleshooting.asciidoc b/docs/static/troubleshooting.asciidoc
new file mode 100644
index 00000000000..6e06d0065d1
--- /dev/null
+++ b/docs/static/troubleshooting.asciidoc
@@ -0,0 +1,262 @@
+[[troubleshooting]] 
+== Troubleshooting Common Problems
+
+We are adding more troubleshooting tips, so please check back soon. If you
+have something to add, please:
+
+* create an issue at
+https://github.com/elastic/logstash/issues, or
+* create a pull request with your proposed changes at https://github.com/elastic/logstash.
+
+// After merge, update PR link to link directly to this topic in GH
+
+Also check out the https://discuss.elastic.co/c/logstash[Logstash discussion
+forum].
+
+
+[float] 
+[[ts-install]] 
+== Installation and setup
+
+
+[float] 
+[[ts-temp-dir]] 
+=== Inaccessible temp directory
+
+Certain versions of the JRuby runtime and libraries
+in certain plugins (the Netty network library in the TCP input, for example) copy
+executable files to the temp directory. This situation causes subsequent failures when
+`/tmp` is mounted `noexec`. 
+
+*Sample error*
+
+-----
+[2018-03-25T12:23:01,149][ERROR][org.logstash.Logstash ]
+java.lang.IllegalStateException: org.jruby.exceptions.RaiseException:
+(LoadError) Could not load FFI Provider: (NotImplementedError) FFI not
+available: java.lang.UnsatisfiedLinkError: /tmp/jffi5534463206038012403.so:
+/tmp/jffi5534463206038012403.so: failed to map segment from shared object:
+Operation not permitted
+-----
+
+*Possible solutions*
+
+* Change setting to mount `/tmp` with `exec`.
+* Specify an alternate directory using the `-Djava.io.tmpdir` setting in the `jvm.options` file.
+ 
+
+[float] 
+[[ts-ingest]] 
+== Data ingestion
+
+[float] 
+[[ts-429]] 
+=== Error response code 429
+
+A `429` message indicates that an application is busy handling other requests. For
+example, Elasticsearch sends a `429` code to notify Logstash (or other indexers)
+that the bulk failed because the ingest queue is full. Logstash will retry sending documents.
+
+*Possible actions*
+
+Check {es} to see if it needs attention.
+
+* {ref}/cluster-stats.html
+* {ref}/es-monitoring.html
+
+*Sample error*
+
+-----
+[2018-08-21T20:05:36,111][INFO ][logstash.outputs.elasticsearch] retrying
+failed action with response code: 429
+({"type"=>"es_rejected_execution_exception", "reason"=>"rejected execution of
+org.elasticsearch.transport.TransportService$7@85be457 on
+EsThreadPoolExecutor[bulk, queue capacity = 200,
+org.elasticsearch.common.util.concurrent.EsThreadPoolExecutor@538c9d8a[Running,
+pool size = 16, active threads = 16, queued tasks = 200, completed tasks =
+685]]"})
+-----
+
+
+[float] 
+[[ts-performance]] 
+== General performance tuning
+
+For general performance tuning tips and guidelines, see <<performance-tuning>>.
+
+
+
+
+
+
+
+[float] 
+[[ts-kafka]] 
+== Common Kafka support issues and solutions
+ 
+[float] 
+[[ts-kafka-timeout]] 
+=== Kafka session timeout issues (input side)
+
+*Symptoms* 
+
+Throughput issues and duplicate event processing {ls} logs warnings:
+
+-----
+[2017-10-18T03:37:59,302][WARN][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
+Auto offset commit failed for group clap_tx1: Commit cannot be completed since
+the group has already rebalanced and assigned the partitions to another member.
+-----
+
+The time between subsequent calls to `poll()` was longer than the
+configured `session.timeout.ms`, which typically implies that the poll loop is
+spending too much time processing messages. You can address this by
+increasing the session timeout or by reducing the maximum size of batches
+returned in `poll()` with `max.poll.records`. 
+
+-----
+[INFO][org.apache.kafka.clients.consumer.internals.ConsumerCoordinator] Revoking
+previously assigned partitions [] for group log-ronline-node09
+`[2018-01-29T14:54:06,485][INFO]`[org.apache.kafka.clients.consumer.internals.ConsumerCoordinator]
+Setting newly assigned partitions [elk-pmbr-9] for group log-pmbr 
+-----
+
+*Background*
+
+Kafka tracks the individual consumers in a consumer group (for example, a number
+of {ls} instances) and tries to give each consumer one or more specific
+partitions of data in the topic they’re consuming. In order to achieve this,
+Kafka tracks whether or not a consumer ({ls} Kafka input thread) is making
+progress on their assigned partition, and reassigns partitions that have not
+made progress in a set timeframe. 
+
+When {ls} requests more events from the Kafka Broker than it can process within
+the timeout, it triggers reassignment of partitions. Reassignment of partitions
+takes time, and can cause duplicate processing of events and significant
+throughput problems. 
+
+*Possible solutions*
+
+* Reduce the number of records per request that {ls} polls from the Kafka Broker in one request,
+* Reduce the number of Kafka input threads, and/or 
+* Increase the relevant timeouts in the Kafka Consumer configuration.
+
+*Details*
+
+The `max_poll_records` option sets the number of records to be pulled in one request.
+If it exceeds the default value of 500, try reducing it. 
+
+The `consumer_threads` option sets the number of input threads. If the value exceeds
+the number of pipeline workers configured in the `logstash.yml` file, it should
+certainly be reduced.  
+If the value is greater than 4, try reducing it to `4` or less if the client has
+the time/resources for it. Try starting with a value of `1`, and then
+incrementing from there to find the optimal performance. 
+
+The `session_timeout_ms` option sets the relevant timeout. Set it to a value
+that ensures that the number of events in `max_poll_records` can be safely
+processed within the time limit. 
+
+-----
+EXAMPLE
+Pipeline throughput is `10k/s` and `max_poll_records` is set to 1k =>. The value
+must be at least 100ms if `consumer_threads` is set to `1`. If it is set to a
+higher value `n`, then the minimum session timeout increases proportionally to
+`n * 100ms`.
+-----
+
+In practice the value must be set much higher than the theoretical value because
+the behavior of the outputs and filters in a pipeline follows a distribution.
+The value should also be higher than the maximum time you expect your outputs to
+stall. The default setting is `10s == 10000ms`. If you are experiencing
+periodic problems with an output that can stall because of load or similar
+effects (such as the Elasticsearch output), there is little downside to
+increasing this value significantly to say `60s`. 
+
+From a performance perspective, decreasing the `max_poll_records` value is preferable
+to increasing the timeout value. Increasing the timeout is your only option if the
+client’s issues are caused by periodically stalling outputs. Check logs for
+evidence of stalling outputs, such as `ES output logging status 429`.
+
+[float] 
+[[ts-kafka-many-offset-commits]] 
+=== Large number of offset commits (Kafka input side)
+
+*Symptoms*
+
+Logstash’s Kafka Input is causing a much higher number of commits to
+the offset topic than expected. Often the complaint also mentions redundant
+offset commits where the same offset is committed repeatedly.
+
+*Solution*
+
+For Kafka Broker versions 0.10.2.1 to 1.0.x: The problem is caused by a bug in
+Kafka. https://issues.apache.org/jira/browse/KAFKA-6362 The client’s best option
+is upgrading their Kafka Brokers to version 1.1 or newer. 
+
+For older versions of
+Kafka or if the above does not fully resolve the issue: The problem can also be
+caused by setting the value for `poll_timeout_ms` too low relative to the rate
+at which the Kafka Brokers receive events themselves (or if Brokers periodically
+idle between receiving bursts of events). Increasing the value set for
+`poll_timeout_ms` proportionally decreases the number of offsets commits in
+this scenario. For example, raising it by 10x will lead to 10x fewer offset commits.
+
+
+[float] 
+[[ts-kafka-codec-errors-input]] 
+=== Codec Errors in Kafka Input (before Plugin Version 6.3.4 only) 
+
+*Symptoms*
+
+Logstash Kafka input randomly logs errors from the configured codec and/or reads
+events incorrectly (partial reads, mixing data between multiple events etc.).
+
+-----
+Log example:  [2018-02-05T13:51:25,773][FATAL][logstash.runner          ] An
+unexpected error occurred! {:error=>#<TypeError: can't convert nil into String>,
+:backtrace=>["org/jruby/RubyArray.java:1892:in `join'",
+"org/jruby/RubyArray.java:1898:in `join'",
+"/usr/share/logstash/logstash-core/lib/logstash/util/buftok.rb:87:in `extract'",
+"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-codec-line-3.0.8/lib/logstash/codecs/line.rb:38:in
+`decode'",
+"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-input-kafka-5.1.11/lib/logstash/inputs/kafka.rb:241:in
+`thread_runner'",
+"file:/usr/share/logstash/vendor/jruby/lib/jruby.jar!/jruby/java/java_ext/java.lang.rb:12:in
+`each'",
+"/usr/share/logstash/vendor/bundle/jruby/1.9/gems/logstash-input-kafka-5.1.11/lib/logstash/inputs/kafka.rb:240:in
+`thread_runner'"]} 
+-----
+
+*Background*
+
+There was a bug in the way the Kafka Input plugin was handling codec instances
+when running on multiple threads (`consumer_threads` set to > 1).
+https://github.com/logstash-plugins/logstash-input-kafka/issues/210 
+
+*Solution*
+
+* Upgrade Kafka Input plugin to v. 6.3.4 or later. 
+* If (and only if) upgrading is not possible, set `consumer_threads` to `1`.
+
+
+[float] 
+[[ts-other]] 
+== Other issues
+
+Coming soon
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
