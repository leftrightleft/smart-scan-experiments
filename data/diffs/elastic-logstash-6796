diff --git a/config/logstash.yml b/config/logstash.yml
index 1cdec6f74d6..bc8e6fad05d 100644
--- a/config/logstash.yml
+++ b/config/logstash.yml
@@ -90,7 +90,7 @@
 # Internal queuing model, "memory" for legacy in-memory based queuing and
 # "persisted" for disk-based acked queueing. Defaults is memory
 #
-# queue.type: memory
+queue.type: memory_batched
 #
 # If using queue.type: persisted, the directory path where the data files will be stored.
 # Default is path.data/queue
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index c17bf0e657e..f5878d91901 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -40,7 +40,7 @@ module Environment
             Setting::String.new("http.host", "127.0.0.1"),
             Setting::PortRange.new("http.port", 9600..9700),
             Setting::String.new("http.environment", "production"),
-            Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked"]),
+            Setting::String.new("queue.type", "memory", true, ["persisted", "memory", "memory_acked", "memory_batched"]),
             Setting::Boolean.new("queue.drain", false),
             Setting::Bytes.new("queue.page_capacity", "250mb"),
             Setting::Bytes.new("queue.max_bytes", "1024mb"),
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index b2dff61d619..161fde426d3 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -5,6 +5,9 @@
 require "logstash/namespace"
 require "logstash/errors"
 require "logstash-core/logstash-core"
+require "logstash/util/wrapped_acked_queue"
+require "logstash/util/wrapped_synchronous_queue"
+require "logstash/util/wrapped_batched_queue"
 require "logstash/event"
 require "logstash/config/file"
 require "logstash/filters/base"
diff --git a/logstash-core/lib/logstash/queue_factory.rb b/logstash-core/lib/logstash/queue_factory.rb
index 70b215557f4..fa6aa333d68 100644
--- a/logstash-core/lib/logstash/queue_factory.rb
+++ b/logstash-core/lib/logstash/queue_factory.rb
@@ -4,6 +4,7 @@
 require "logstash/namespace"
 require "logstash/util/wrapped_acked_queue"
 require "logstash/util/wrapped_synchronous_queue"
+require "logstash/util/wrapped_batched_queue"
 
 module LogStash
   class QueueFactory
@@ -15,6 +16,7 @@ def self.create(settings)
       checkpoint_max_acks = settings.get("queue.checkpoint.acks")
       checkpoint_max_writes = settings.get("queue.checkpoint.writes")
       checkpoint_max_interval = settings.get("queue.checkpoint.interval")
+      pipeline_batch_size = settings.get("pipeline.batch.size")
 
       queue_path = ::File.join(settings.get("path.queue"), settings.get("pipeline.id"))
 
@@ -27,6 +29,8 @@ def self.create(settings)
         # persisted is the disk based acked queue
         FileUtils.mkdir_p(queue_path)
         LogStash::Util::WrappedAckedQueue.create_file_based(queue_path, queue_page_capacity, queue_max_events, checkpoint_max_writes, checkpoint_max_acks, checkpoint_max_interval, queue_max_bytes)
+      when "memory_batched"
+        LogStash::Util::WrappedBatchedQueue.new(pipeline_batch_size)
       when "memory"
         # memory is the legacy and default setting
         LogStash::Util::WrappedSynchronousQueue.new
diff --git a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
index b733d221f46..9d8f6646c92 100644
--- a/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_acked_queue.rb
@@ -57,6 +57,11 @@ def push(obj)
     end
     alias_method(:<<, :push)
 
+    def push_batch(batch)
+      check_closed("write")
+      @queue.write_batch(batch)
+    end
+
     # TODO - fix doc for this noop method
     # Offer an object to the queue, wait for the specified amount of time.
     # If adding to the queue was successful it will return true, false otherwise.
@@ -94,6 +99,10 @@ def read_client()
       ReadClient.new(self)
     end
 
+    def empty?
+      @queue.is_fully_acked?
+    end
+
     def check_closed(action)
       if closed?
         raise QueueClosedError.new("Attempted to #{action} on a closed AckedQueue")
@@ -127,7 +136,7 @@ def close
       end
 
       def empty?
-        @mutex.synchronize { @queue.is_fully_acked? }
+        @mutex.synchronize { @queue.empty? }
       end
 
       def set_batch_dimensions(batch_size, wait_for)
@@ -219,12 +228,6 @@ def stop_clock(batch)
         end
       end
 
-      def add_starting_metrics(batch)
-        return if @event_metric.nil? || @pipeline_metric.nil?
-        @event_metric.increment(:in, batch.starting_size)
-        @pipeline_metric.increment(:in, batch.starting_size)
-      end
-
       def add_filtered_metrics(batch)
         @event_metric.increment(:filtered, batch.filtered_size)
         @pipeline_metric.increment(:filtered, batch.filtered_size)
@@ -318,14 +321,6 @@ def cancelled_size
         # @cancelled.size
       end
 
-      def shutdown_signal_received?
-        false
-      end
-
-      def flush_signal_received?
-        false
-      end
-
       private
 
       def iterating?
@@ -359,9 +354,7 @@ def push_batch(batch)
         if @queue.closed?
           raise QueueClosedError.new("Attempted to write a batch to a closed AckedQueue")
         end
-        batch.each do |event|
-          push(event)
-        end
+        @queue.push_batch(batch)
       end
     end
 
diff --git a/logstash-core/lib/logstash/util/wrapped_batched_queue.rb b/logstash-core/lib/logstash/util/wrapped_batched_queue.rb
new file mode 100644
index 00000000000..92855e52ff8
--- /dev/null
+++ b/logstash-core/lib/logstash/util/wrapped_batched_queue.rb
@@ -0,0 +1,300 @@
+# encoding: utf-8
+
+require "jruby_batched_queue_ext"
+
+module LogStash; module Util
+  class WrappedBatchedQueue
+
+    def initialize(batch_size = 125)
+      @queue = LogStash::BatchedQueue.new(batch_size)
+    end
+
+    # Push an object to the queue if the queue is full
+    # it will block until the object can be added to the queue.
+    #
+    # @param [Object] Object to add to the queue
+    def push(obj)
+      @queue.write(obj)
+    end
+    alias_method(:<<, :push)
+
+    def push_batch(batch)
+      @queue.write_batch(batch)
+    end
+
+    # Offer an object to the queue, wait for the specified amout of time.
+    # If adding to the queue was successfull it wil return true, false otherwise.
+    #
+    # @param [Object] Object to add to the queue
+    # @param [Integer] Time in milliseconds to wait before giving up
+    # @return [Boolean] True if adding was successfull if not it return false
+    def offer(obj, timeout_ms)
+      raise NotImplementedError.new("The offer method is not implemented. There is no non blocking write operation yet.")
+    end
+
+    # Blocking
+    def take
+      raise NotImplementedError.new("The take method is not implemented.")
+    end
+
+    # Block for X millis
+    def poll(millis)
+      raise NotImplementedError.new("The poll method is not implemented.")
+    end
+
+    def read_batch(wait)
+      @queue.read_batch(wait)
+    end
+
+    def write_client
+      WriteClient.new(self)
+    end
+
+    def read_client
+      ReadClient.new(self)
+    end
+
+    def empty?
+      @queue.empty?
+    end
+
+    def close
+      # ignore
+    end
+
+    class ReadClient
+      # We generally only want one thread at a time able to access pop/take/poll operations
+      # from this queue. We also depend on this to be able to block consumers while we snapshot
+      # in-flight buffers
+
+      def initialize(queue, batch_size = 125, wait_for = 250)
+        @queue = queue
+        @mutex = Mutex.new
+        # Note that @infilght_batches as a central mechanism for tracking inflight
+        # batches will fail if we have multiple read clients in the pipeline.
+        @inflight_batches = {}
+
+        # allow the worker thread to report the execution time of the filter + output
+        @inflight_clocks = {}
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def close
+        # noop, compat with acked queue read client
+      end
+
+      def empty?
+        @queue.empty?
+      end
+
+      def set_batch_dimensions(batch_size, wait_for)
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def set_events_metric(metric)
+        @event_metric = metric
+      end
+
+      def set_pipeline_metric(metric)
+        @pipeline_metric = metric
+      end
+
+      def inflight_batches
+        @mutex.synchronize do
+          yield(@inflight_batches)
+        end
+      end
+
+      def current_inflight_batch
+        @inflight_batches.fetch(Thread.current, [])
+      end
+
+      # create a new empty batch
+      # @return [ReadBatch] a new empty read batch
+      def new_batch
+        ReadBatch.new(@queue, @batch_size, @wait_for)
+      end
+
+      def read_batch
+        batch = new_batch
+        @mutex.synchronize { batch.read_next }
+        start_metrics(batch)
+        batch
+      end
+
+      def start_metrics(batch)
+        @mutex.synchronize do
+          # there seems to be concurrency issues with metrics, keep it in the mutex
+          set_current_thread_inflight_batch(batch)
+          start_clock
+        end
+      end
+
+      def set_current_thread_inflight_batch(batch)
+        @inflight_batches[Thread.current] = batch
+      end
+
+      def close_batch(batch)
+        @mutex.synchronize do
+          # there seems to be concurrency issues with metrics, keep it in the mutex
+          @inflight_batches.delete(Thread.current)
+          stop_clock(batch)
+        end
+      end
+
+      def start_clock
+        @inflight_clocks[Thread.current] = [
+          @event_metric.time(:duration_in_millis),
+          @pipeline_metric.time(:duration_in_millis)
+        ]
+      end
+
+      def stop_clock(batch)
+        unless @inflight_clocks[Thread.current].nil?
+          if batch.size > 0
+            # onl/y stop (which also records) the metrics if the batch is non-empty.
+            # start_clock is now called at empty batch creation and an empty batch could
+            # stay empty all the way down to the close_batch call.
+            @inflight_clocks[Thread.current].each(&:stop)
+          end
+          @inflight_clocks.delete(Thread.current)
+        end
+      end
+
+      def add_filtered_metrics(batch)
+        @event_metric.increment(:filtered, batch.filtered_size)
+        @pipeline_metric.increment(:filtered, batch.filtered_size)
+      end
+
+      def add_output_metrics(batch)
+        @event_metric.increment(:out, batch.filtered_size)
+        @pipeline_metric.increment(:out, batch.filtered_size)
+      end
+    end
+
+    class ReadBatch
+      def initialize(queue, size, wait)
+        @queue = queue
+        @size = size
+        @wait = wait
+
+        @originals = Hash.new
+
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        # @cancelled = Hash.new
+
+        @generated = Hash.new
+        @iterating_temp = Hash.new
+        @iterating = false # Atomic Boolean maybe? Although batches are not shared across threads
+        @batch = nil
+      end
+
+      def read_next
+        @batch = @queue.read_batch(@wait)
+        return if @batch.nil?
+        @batch.each { |e| @originals[e] = true }
+      end
+
+      def merge(event)
+        return if event.nil? || @originals.key?(event)
+        # take care not to cause @generated to change during iteration
+        # @iterating_temp is merged after the iteration
+        if iterating?
+          @iterating_temp[event] = true
+        else
+          # the periodic flush could generate events outside of an each iteration
+          @generated[event] = true
+        end
+      end
+
+      def cancel(event)
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        raise("cancel is unsupported")
+        # @cancelled[event] = true
+      end
+
+      def each(&blk)
+        # take care not to cause @originals or @generated to change during iteration
+        @iterating = true
+
+        # below the checks for @cancelled.include?(e) have been replaced by e.cancelled?
+        # TODO: for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        @originals.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @generated.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @iterating = false
+        update_generated
+      end
+
+      def size
+        filtered_size
+      end
+
+      def starting_size
+        @originals.size
+      end
+
+      def filtered_size
+        @originals.size + @generated.size
+      end
+
+      def cancelled_size
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        raise("cancelled_size is unsupported ")
+        # @cancelled.size
+      end
+
+      private
+
+      def iterating?
+        @iterating
+      end
+
+      def update_generated
+        @generated.update(@iterating_temp)
+        @iterating_temp.clear
+      end
+    end
+
+    class WriteClient
+      def initialize(queue)
+        @queue = queue
+      end
+
+      def get_new_batch
+        WriteBatch.new
+      end
+
+      def push(event)
+        @queue.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def push_batch(batch)
+        @queue.push_batch(batch)
+      end
+    end
+
+    class WriteBatch
+      def initialize
+        @events = []
+      end
+
+      def push(event)
+        @events.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def each(&blk)
+        @events.each do |e|
+          blk.call(e)
+        end
+      end
+    end
+  end
+end end
diff --git a/logstash-core/spec/logstash/queue_factory_spec.rb b/logstash-core/spec/logstash/queue_factory_spec.rb
index 9182c9c95be..7ccd00dcfec 100644
--- a/logstash-core/spec/logstash/queue_factory_spec.rb
+++ b/logstash-core/spec/logstash/queue_factory_spec.rb
@@ -15,7 +15,8 @@
       LogStash::Setting::Numeric.new("queue.checkpoint.acks", 1024),
       LogStash::Setting::Numeric.new("queue.checkpoint.writes", 1024),
       LogStash::Setting::Numeric.new("queue.checkpoint.interval", 1000),
-      LogStash::Setting::String.new("pipeline.id", pipeline_id)
+      LogStash::Setting::String.new("pipeline.id", pipeline_id),
+      LogStash::Setting::PositiveInteger.new("pipeline.batch.size", 125),
     ]
   end
 
diff --git a/logstash-core/src/main/java/JrubyBatchedQueueExtService.java b/logstash-core/src/main/java/JrubyBatchedQueueExtService.java
new file mode 100644
index 00000000000..a235b7cb1c0
--- /dev/null
+++ b/logstash-core/src/main/java/JrubyBatchedQueueExtService.java
@@ -0,0 +1,14 @@
+import org.jruby.Ruby;
+import org.jruby.runtime.load.BasicLibraryService;
+import org.logstash.batchedqueue.ext.JrubyBatchedQueueExtLibrary;
+
+import java.io.IOException;
+
+public class JrubyBatchedQueueExtService implements BasicLibraryService {
+    public boolean basicLoad(final Ruby runtime)
+            throws IOException
+    {
+        new JrubyBatchedQueueExtLibrary().load(runtime, false);
+         return true;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index 4bcef21c77c..fde3c3e8350 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -16,11 +16,13 @@
 import java.nio.file.NoSuchFileException;
 import java.util.ArrayList;
 import java.util.List;
+import java.util.concurrent.Callable;
 import java.util.concurrent.TimeUnit;
 import java.util.concurrent.atomic.AtomicBoolean;
 import java.util.concurrent.locks.Condition;
 import java.util.concurrent.locks.Lock;
 import java.util.concurrent.locks.ReentrantLock;
+import java.util.stream.Collectors;
 
 
 // TODO: Notes
@@ -360,6 +362,75 @@ public long write(Queueable element) throws IOException {
         }
     }
 
+    // @param elements the Queueable objects to write to the queue
+    // @return long written sequence number
+    public void write(List<Queueable> elements) throws IOException {
+        List<byte[]> serialized = elements.stream().map(e -> { try { return e.serialize(); } catch (IOException ex) { throw new RuntimeException(ex); }}).collect(Collectors.toList());
+
+        // the write strategy with regard to the isFull() state is to assume there is space for this element
+        // and write it, then after write verify if we just filled the queue and wait on the notFull condition
+        // *after* the write which is both safer for a crash condition, and the queue closing sequence. In the former case
+        // holding an element in memory while wainting for the notFull condition would mean always having the current write
+        // element at risk in the always-full queue state. In the later, when closing a full queue, it would be impossible
+        // to write the current element.
+
+        lock.lock();
+        try {
+            boolean wasEmpty = (firstUnreadPage() == null);
+
+            for (byte[] data : serialized) {
+                if (! this.headPage.hasCapacity(data.length)) {
+                    throw new IOException("data to be written is bigger than page capacity");
+                }
+
+                long seqNum = nextSeqNum();
+
+                // create a new head page if the current does not have suffient space left for data to be written
+                if (! this.headPage.hasSpace(data.length)) {
+                    // beheading includes checkpoint+fsync if required
+                    TailPage tailPage = this.headPage.behead();
+
+                    this.tailPages.add(tailPage);
+                    if (! tailPage.isFullyRead()) {
+                        this.unreadTailPages.add(tailPage);
+                    }
+
+                    // create new head page
+                    newCheckpointedHeadpage(tailPage.pageNum + 1);
+                }
+
+                this.headPage.write(data, seqNum, this.checkpointMaxWrites);
+                this.unreadCount++;
+            }
+
+            // if the queue was empty before write, signal non emptiness
+            if (wasEmpty) { notEmpty.signal(); }
+
+            // now check if we reached a queue full state and block here until it is not full
+            // for the next write or the queue was closed.
+            while (isFull() && !isClosed()) {
+                try {
+                    notFull.await();
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to return normally and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // this is a bit tricky in the case of the queue full condition blocking state.
+                    // TODO: we will want to avoid initiating a new write operation if Thread.interrupted() was called.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return;
+                }
+            }
+        } finally {
+            lock.unlock();
+        }
+    }
+
     // @return true if the queue is deemed at full capacity
     public boolean isFull() {
         // TODO: I am not sure if having unreadCount as volatile is sufficient here. all unreadCount updates are done inside syncronized
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
index 839876bc2f2..982052421bd 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/ext/JrubyAckedQueueExtLibrary.java
@@ -1,6 +1,8 @@
 package org.logstash.ackedqueue.ext;
 
+import org.jruby.RubyArray;
 import org.logstash.Event;
+import org.logstash.ackedqueue.Queueable;
 import org.logstash.ext.JrubyEventExtLibrary;
 import org.jruby.Ruby;
 import org.jruby.RubyClass;
@@ -25,6 +27,8 @@
 import org.logstash.common.io.PageIOFactory;
 
 import java.io.IOException;
+import java.util.List;
+import java.util.stream.Collectors;
 
 public class JrubyAckedQueueExtLibrary implements Library {
 
@@ -156,6 +160,25 @@ public IRubyObject ruby_write(ThreadContext context, IRubyObject event)
             return context.runtime.newFixnum(seqNum);
         }
 
+        @JRubyMethod(name = {"write_batch"}, required = 1)
+        public IRubyObject ruby_write_batch(ThreadContext context, IRubyObject events)
+        {
+            if (!(events instanceof RubyArray)) {
+                throw context.runtime.newTypeError("wrong argument type " + events.getMetaClass() + " (expected Array)");
+            }
+
+            try {
+                List<JrubyEventExtLibrary.RubyEvent> rubyEvents = ((RubyArray)events).getList();
+                List<Queueable> javaEvents = rubyEvents.stream().map(e -> e.getEvent()).collect(Collectors.toList());
+                this.queue.write(javaEvents);
+            } catch (IOException e) {
+                throw context.runtime.newIOErrorFromException(e);
+            }
+
+            return context.nil;
+        }
+
+
         @JRubyMethod(name = "read_batch", required = 2)
         public IRubyObject ruby_read_batch(ThreadContext context, IRubyObject limit, IRubyObject timeout)
         {
diff --git a/logstash-core/src/main/java/org/logstash/batchedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/batchedqueue/Queue.java
new file mode 100644
index 00000000000..a46b8a105e5
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/batchedqueue/Queue.java
@@ -0,0 +1,211 @@
+package org.logstash.batchedqueue;
+
+import java.util.ArrayList;
+import java.util.List;
+import java.util.concurrent.TimeUnit;
+import java.util.concurrent.locks.Condition;
+import java.util.concurrent.locks.Lock;
+import java.util.concurrent.locks.ReentrantLock;
+
+public class Queue<E> {
+
+    // thread safety
+    final Lock lock = new ReentrantLock();
+    final Condition notFull  = lock.newCondition();
+    final Condition notEmpty = lock.newCondition();
+
+    private final int WORKERS = 4;
+
+    final int limit;
+    private List[] batches;
+    private int write_batch;
+    private int read_batch;
+
+    public Queue(int limit) {
+        this.limit = limit;
+        this.batches = new  List[WORKERS];
+        this.write_batch = 0;
+        this.read_batch = 0;
+        for (int i = 0; i < WORKERS; i++) {
+            this.batches[i] = new ArrayList<E>();
+        }
+    }
+
+    private int next_write_batch() {
+        return (this.write_batch + 1) % WORKERS;
+    }
+
+    private void inc_write_batch() {
+        this.write_batch = (this.write_batch + 1) % WORKERS;
+    }
+
+    private void inc_read_batch() {
+        this.read_batch = (this.read_batch + 1) % WORKERS;
+    }
+
+
+    public void write(E element) {
+        lock.lock();
+        try {
+
+            // empty queue shortcut
+            if (_isEmpty()) {
+                this.batches[this.write_batch].add(element);
+                notEmpty.signal();
+                return;
+            }
+
+            boolean interrupted = !_waitNotFull();
+            if (interrupted) { return; }
+
+            if (this.batches[this.write_batch].size() >= this.limit) {
+                inc_write_batch();
+            }
+            this.batches[this.write_batch].add(element);
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    public void write(List<E> elements) {
+        lock.lock();
+        try {
+            int read_pos = 0;
+
+            while (read_pos < elements.size()) {
+
+                boolean interrupted = !_waitNotFull();
+                if (interrupted) { return; }
+
+                int max_writes = this.limit - this.batches[this.write_batch].size();
+                int remaining = elements.size() - read_pos;
+                int write_size = max_writes >= remaining ? remaining : max_writes;
+
+                boolean wasEmpty = _isEmpty();
+
+                this.batches[this.write_batch].addAll(elements.subList(read_pos, read_pos + write_size));
+                read_pos += write_size;
+                if (this.batches[this.write_batch].size() >= this.limit) {
+                    inc_write_batch();
+                }
+
+                if (wasEmpty) {  notEmpty.signal(); }
+            }
+        } finally {
+            lock.unlock();
+        }
+    }
+
+
+
+
+    // returns false if the thread was interrrupted
+    private boolean _waitNotFull() {
+        while (isFull()) {
+            try {
+                notFull.await();
+            } catch (InterruptedException e) {
+                // the thread interrupt() has been called while in the await() blocking call.
+                // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                // to any upstream calls on it. for now our choice is to return normally and set back
+                // the Thread.interrupted() flag so it can be checked upstream.
+
+                // set back the interrupted flag
+                Thread.currentThread().interrupt();
+
+                return false;
+            }
+        }
+        return true;
+    }
+
+    private boolean isFull() {
+        return next_write_batch() == read_batch && this.batches[this.write_batch].size() >= this.limit;
+    }
+
+    private boolean _isEmpty() {
+        return this.read_batch == this.write_batch && this.batches[this.read_batch].isEmpty();
+    }
+
+    public boolean isEmpty() {
+        lock.lock();
+        try {
+            return _isEmpty();
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    public List<E> nonBlockReadBatch() {
+        lock.lock();
+        try {
+            // full queue shortcut
+            if (isFull()) {
+                List<E> batch = swap();
+                notFull.signalAll();
+                return batch;
+            }
+
+            if (_isEmpty()) { return null; }
+
+            return swap();
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    public List<E> readBatch() {
+        return null;
+    }
+
+    public List<E> readBatch(long timeout) {
+        lock.lock();
+        try {
+            while (_isEmpty()) {
+                //System.out.println("*** isEmpty");
+                try {
+                    boolean timedout = !notEmpty.await(timeout, TimeUnit.MILLISECONDS); // await return false when reaching timeout
+                    if (timedout) { break; }
+                } catch (InterruptedException e) {
+                    // the thread interrupt() has been called while in the await() blocking call.
+                    // at this point the interrupted flag is reset and Thread.interrupted() will return false
+                    // to any upstream calls on it. for now our choice is to simply return null and set back
+                    // the Thread.interrupted() flag so it can be checked upstream.
+
+                    // set back the interrupted flag
+                    Thread.currentThread().interrupt();
+
+                    return null;
+                }
+            }
+
+            if (_isEmpty()) { return null; }
+
+            if (isFull()) {
+                //System.out.println("*** isFull");
+                List<E> batch = swap();
+                notFull.signalAll();
+                return batch;
+            }
+
+            return swap();
+        } finally {
+            lock.unlock();
+        }
+    }
+
+    public void close() {
+        // nothing
+    }
+
+
+    private List<E> swap() {
+        List<E> batch = this.batches[this.read_batch];
+        this.batches[this.read_batch] = new ArrayList<>();
+        if (this.read_batch != this.write_batch) {
+            inc_read_batch();
+        }
+        return batch;
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/batchedqueue/ext/JrubyBatchedQueueExtLibrary.java b/logstash-core/src/main/java/org/logstash/batchedqueue/ext/JrubyBatchedQueueExtLibrary.java
new file mode 100644
index 00000000000..70af2192d0c
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/batchedqueue/ext/JrubyBatchedQueueExtLibrary.java
@@ -0,0 +1,110 @@
+package org.logstash.batchedqueue.ext;
+
+import org.jruby.RubyArray;
+import org.logstash.Event;
+import org.logstash.ext.JrubyEventExtLibrary;
+import org.jruby.Ruby;
+import org.jruby.RubyClass;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyModule;
+import org.jruby.RubyObject;
+import org.jruby.RubyBoolean;
+import org.jruby.anno.JRubyClass;
+import org.jruby.anno.JRubyMethod;
+import org.jruby.runtime.Arity;
+import org.jruby.runtime.ObjectAllocator;
+import org.jruby.runtime.ThreadContext;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.jruby.runtime.load.Library;
+import org.logstash.batchedqueue.Queue;
+
+import java.io.IOException;
+import java.util.List;
+
+
+public class JrubyBatchedQueueExtLibrary implements Library {
+
+    public void load(Ruby runtime, boolean wrap) throws IOException {
+        RubyModule module = runtime.defineModule("LogStash");
+
+        RubyClass clazz = runtime.defineClassUnder("BatchedQueue", runtime.getObject(), new ObjectAllocator() {
+            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
+                return new RubyBatchedQueue(runtime, rubyClass);
+            }
+        }, module);
+
+        clazz.defineAnnotatedMethods(RubyBatchedQueue.class);
+    }
+
+    @JRubyClass(name = "BatchedQueue", parent = "Object")
+    public static class RubyBatchedQueue extends RubyObject {
+        private Queue queue;
+
+        public RubyBatchedQueue(Ruby runtime, RubyClass klass) {
+            super(runtime, klass);
+        }
+
+        public Queue getQueue() {
+            return this.queue;
+        }
+
+        // def initialize
+        @JRubyMethod(name = "initialize", required = 1)
+        public IRubyObject ruby_initialize(ThreadContext context, IRubyObject limit)
+        {
+
+            int _limit = RubyFixnum.num2int(limit);
+
+            this.queue = new Queue(_limit);
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = {"write", "<<"}, required = 1)
+        public IRubyObject ruby_write(ThreadContext context, IRubyObject event)
+        {
+            if (!(event instanceof JrubyEventExtLibrary.RubyEvent)) {
+                throw context.runtime.newTypeError("wrong argument type " + event.getMetaClass() + " (expected LogStash::Event)");
+            }
+
+            this.queue.write(event);
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = {"write_batch"}, required = 1)
+        public IRubyObject ruby_write_batch(ThreadContext context, IRubyObject events)
+        {
+            if (!(events instanceof RubyArray)) {
+                throw context.runtime.newTypeError("wrong argument type " + events.getMetaClass() + " (expected Array)");
+            }
+
+            this.queue.write(((RubyArray)events).getList());
+
+            return context.nil;
+        }
+
+        @JRubyMethod(name = "read_batch", required = 1)
+        public IRubyObject ruby_read_batch(ThreadContext context, IRubyObject timeout)
+        {
+            List result = this.queue.readBatch(RubyFixnum.num2int(timeout));
+            return result == null ? context.nil : context.runtime.newArray(result);
+        }
+
+        @JRubyMethod(name = "empty?")
+        public IRubyObject ruby_empty(ThreadContext context)
+        {
+
+            return RubyBoolean.newBoolean(context.runtime, this.queue.isEmpty());
+        }
+
+        @JRubyMethod(name = "close")
+        public IRubyObject ruby_close(ThreadContext context)
+        {
+
+            this.queue.close();
+            return context.nil;
+        }
+
+    }
+}
diff --git a/qa/integration/specs/monitoring_api_spec.rb b/qa/integration/specs/monitoring_api_spec.rb
index 69b7a3a7517..de0f15aa529 100644
--- a/qa/integration/specs/monitoring_api_spec.rb
+++ b/qa/integration/specs/monitoring_api_spec.rb
@@ -65,7 +65,7 @@
         expect(result["pipeline"]["queue"]["capacity"]["max_queue_size_in_bytes"]).not_to be_nil
         expect(result["pipeline"]["queue"]["capacity"]["max_unread_events"]).not_to be_nil
       else
-        expect(result["pipeline"]["queue"]["type"]).to eq "memory"
+        expect(result["pipeline"]["queue"]["type"]).to eq("memory").or(eq("memory_batched"))
       end
     end
   end
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index 44919e332c7..4a683dea711 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -41,9 +41,12 @@ def puts(payload)
     #   logger
     # end
 
-    LogStash::SETTINGS.set("queue.type", "memory_acked")
-    LogStash::SETTINGS.set("queue.page_capacity", 1024 * 1024)
-    LogStash::SETTINGS.set("queue.max_events", 250)
+    # LogStash::SETTINGS.set("queue.type", "memory_acked")
+    # LogStash::SETTINGS.set("queue.page_capacity", 1024 * 1024)
+    # LogStash::SETTINGS.set("queue.max_events", 250)
+
+    LogStash::SETTINGS.set("queue.type", "memory_batched")
+    LogStash::SETTINGS.set("queue.drain", true)
   end
 end
 
