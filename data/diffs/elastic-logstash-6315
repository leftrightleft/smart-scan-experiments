diff --git a/.github/ISSUE_TEMPLATE.md b/.github/ISSUE_TEMPLATE.md
new file mode 100644
index 00000000000..9d85c7cfddb
--- /dev/null
+++ b/.github/ISSUE_TEMPLATE.md
@@ -0,0 +1,11 @@
+Please post all product and debugging questions on our [forum](https://discuss.elastic.co/c/logstash). Your questions will reach our wider community members there, and if we confirm that there is a bug, then we can open a new issue here.
+
+Logstash Plugins are located in a different organization: https://github.com/logstash-plugins. For bugs on specific Logstash plugins, for example, if Redis Output has a defect, please open it in the respective Redis Output repository. 
+
+For all general issues, please provide the following details for fast resolution:
+
+- Version:
+- Operating System:
+- Config File (if you have sensitive info, please remove it):
+- Sample Data:
+- Steps to Reproduce:
diff --git a/.github/PULL_REQUEST_TEMPLATE.md b/.github/PULL_REQUEST_TEMPLATE.md
new file mode 100644
index 00000000000..a1538275aec
--- /dev/null
+++ b/.github/PULL_REQUEST_TEMPLATE.md
@@ -0,0 +1 @@
+Thanks for contributing to Logstash! If you haven't already signed our CLA, here's a handy link: https://www.elastic.co/contributor-agreement/
diff --git a/.gitignore b/.gitignore
index 9360eb39bb9..2715c52735f 100644
--- a/.gitignore
+++ b/.gitignore
@@ -28,3 +28,14 @@ rspec.xml
 .vendor
 integration_run
 .mvn/
+qa/.vm_ssh_config
+qa/.vagrant
+qa/acceptance/.vagrant
+qa/Gemfile.lock
+*.ipr
+*.iws
+*.iml
+.gradle
+.idea
+logs
+qa/integration/services/installed/
diff --git a/.tailor b/.tailor
deleted file mode 100644
index 5e883dba31d..00000000000
--- a/.tailor
+++ /dev/null
@@ -1,8 +0,0 @@
-Tailor.config do |config|
-  config.file_set '*.rb' do |style|
-    style.indentation_spaces 2, :level => :off
-    style.max_line_length 80, :level => :off
-    style.allow_trailing_line_spaces true, :level => :off
-    style.spaces_after_comma false, :level => :off
-  end
-end
diff --git a/.travis.yml b/.travis.yml
new file mode 100644
index 00000000000..f2dfb53f000
--- /dev/null
+++ b/.travis.yml
@@ -0,0 +1,24 @@
+sudo: false
+language: ruby
+cache:
+  directories:
+    - vendor/bundle
+    - ~/.gradle/
+rvm:
+  - jruby-1.7.25
+jdk:
+  - oraclejdk8
+env:
+  - INTEGRATION=true  
+before_install:
+  # Force bundler 1.12.5 because version 1.13 has issues, see https://github.com/fastlane/fastlane/issues/6065#issuecomment-246044617
+  - gem uninstall -i /home/travis/.rvm/gems/jruby-1.7.25@global bundler
+  - gem install bundler -v 1.12.5 --no-rdoc --no-ri --no-document --quiet   
+install:
+  - rake test:install-core
+  - ci/travis_integration_install.sh
+before_script:
+  - echo "--order rand" > .rspec
+script:
+  - rake test:core
+  - ci/travis_integration_run.sh
diff --git a/CHANGELOG.md b/CHANGELOG.md
index 5f8bf0bb62c..ee465b998d3 100644
--- a/CHANGELOG.md
+++ b/CHANGELOG.md
@@ -1,3 +1,236 @@
+## 5.0.0-beta1 (Sep 21, 2016)
+ - Migrated Logstash's internal logging framework to Log4j2. This enhancement provides the following features:
+   - Support changing logging level dynamically at runtime through REST endpoints. New APIs have been exposed 
+     under `_node/logging` to update log levels. Also a new endpoint `_node/logging` was added to return all 
+     existing loggers.
+   - Configurable file rotation policy for logs. Default is per-day.
+   - Support component-level or plugin level log settings.
+   - Unify logging across Logstash's Java and Ruby code.
+   - Logs are now placed in `LS_HOME/logs` dir configurable via `path.logs` setting.
+ - Breaking change: Set default log severity level to `INFO` instead of `WARN` to match Elasticsearch.
+ - Show meaningful error message with an unknown CLI command ([#5748](https://github.com/elastic/logstash/issues/5748))
+ - Monitoring API enhancements
+   - Added `duration_in_millis` metric under `/_node/stats/pipeline/events`
+   - Added JVM GC stats under `/_node/stats/jvm`
+   - Removed the `/_node/mem` resource as it's been properly moved under `/_node/jvm/mem`
+   - Added config reload stats under new resource type `_node/stats/pipeline/reloads`
+   - Added config reload enabled/disabled info to `/_node/pipeline`
+   - Added JVM GC strategy info under `/_node/jvm`
+   - Ensure `?human` option works correctly for `hot_threads` API.
+   - Make sure a non-existing API endpoint correctly returns 404 and a structured error message.
+ - Plugin Developers: Improved nomenclature and methods for 'threadsafe' outputs. Removed `workers_not_supported` 
+    method ([#5662](https://github.com/elastic/logstash/issues/5662))
+
+### Output
+  - Elasticsearch
+    - Breaking Change: Index template for 5.0 has been changed to reflect Elasticsearch's mapping 
+      changes. Most importantly, the subfield for string multi-fields has changed from `.raw` to `.keyword` 
+      to match ES default behavior ([#386](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/386))
+    - Users installing ES 5.x and LS 5.x This change will not affect you and you will continue to use 
+      the ES defaults. Users upgrading from LS 2.x to LS 5.x with ES 5.x LS will not force upgrade the template, 
+      if logstash template already exists. This means you will still use .raw for sub-fields coming from 2.x. 
+      If you choose to use the new template, you will have to reindex your data after the new template is 
+      installed.
+    - Added `check_connection_timeout` parameter which has a default of 10m
+
+## 5.0.0-alpha5 (Aug 2, 2016)
+ - Introduced a performance optimization called bi-values to store both JRuby and Java object types which will
+   benefit plugins written in Ruby.
+ - Added support for specifying a comma-separated list of resources to monitoring APIs. This can be used to 
+   filter API response ([#5609](https://github.com/elastic/logstash/issues/5609))
+ - `/_node/hot_threads?human=true` human option now returns a human readable format, not JSON.
+ - Pipeline stats from `/_node/stats/pipeline` is also included in the parent `/_node/stats` 
+   resource for completeness.
+ 
+### Input
+ - Beats
+   - Reimplemented input in Java and to use asynchronous IO library Netty. These changes resulted in 
+     up to 50% gains in throughput performance while preserving the original functionality ([#92](https://github.com/logstash-plugins/logstash-input-beats/issues/92)).
+ - JDBC
+   - Added support for providing encoding charset for strings not in UTF-8 format. `columns_charset` allows 
+     you to override this encoding setting per-column ([#143](https://github.com/logstash-plugins/logstash-input-jdbc/issues/143))
+ - HTTP Poller
+   - Added meaningful error messages on missing trust/key-store password. Document the creation of a custom keystore.
+
+### Filter
+ - CSV
+   - Added `autodetect_column_names` option to read column names from header.
+ - Throttle
+   - Reimplemented plugin to work with multiple threads, support asynchronous input and properly 
+     tracks past events ([#4](https://github.com/logstash-plugins/logstash-filter-throttle/issues/4))
+
+### Output
+ - Elasticsearch
+   - Added ability to choose different default template based on ES versions ([#401](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/401))
+ - Kafka
+   - Input is a shareable instance across multiple pipeline workers. This ensures efficient use of resources like 
+     broker TCP connections, internal producer buffers, etc ([#79](https://github.com/logstash-plugins/logstash-output-kafka/pull/79))
+   - Added feature to allow regex patterns in topics so you can subscribe to multiple ones.
+
+## 5.0.0-alpha4 (June 28, 2016)
+ - Created a new `LS_HOME/data` directory to store plugin states, Logstash instance UUID and more. This directory 
+   location is configurable via `path.data` ([#5404](https://github.com/elastic/logstash/issues/5404)).
+ - Made `bin/logstash -V/--version` fast on Unix platforms.
+ - Monitoring API: Added hostname, http_address, version as static fields for all APIs ([#5450](https://github.com/elastic/logstash/issues/5450)).
+ - Added time tracking (wall-clock) to all individual filter and output instances. The goal is to help identify 
+   what plugin configurations are consuming the most time. Exposed via `/_node/stats/pipeline`.
+ - Added ` /_node` API which provides static information for OS, JVM and pipeline settings.  
+ - Moved  `_plugins` api to `_node/plugins` endpoint.
+ - Moved `hot_thread` API report to `_node/hot_thread` endpoint.
+ - Add new `:list` property to configuration parameters. This will allow the user to specify one or more values.
+ - Add new URI config validator/type. This allows plugin like the Elasticsearch output to safely URIs for 
+   their configuration. Any password information in the URI will be masked when logged.
+ 
+### Input
+ - Kafka
+   - Added support for Kafka broker 0.10.
+ - HTTP
+   - Fixed a bug where HTTP input plugin blocked stats API ([#51](https://github.com/logstash-plugins/logstash-input-http/issues/51)). 
+ 
+### Output
+ - Elasticsearch
+   - ES output is now fully threadsafe. This means internal resources can be shared among multiple 
+     `output { elasticsearch {} }` instances.
+   - Sniffing improvements so any current connections don't have to be closed/reopened after a sniff round.
+   - Introduced a connection pool to efficiently reuse connections to ES backends.
+   - Added exponential backoff to connection retries with a ceiling of `retry_max_interval` which is the most time to 
+     wait between retries, and `retry_initial_interval` which is the initial amount of time to wait. 
+     `retry_initial_interval` will be increased exponentially between retries until a request succeeds.
+     
+ - Kafka
+   - Added support for Kafka broker 0.10
+   
+### Filter
+ - Grok
+   - Added stats counter on grok matches and failures. This is exposed in `_node/stats/pipeline`
+ - Date
+   - Added stats counter on grok matches and failures. This is exposed in `_node/stats/pipeline`  
+
+## 5.0.0-alpha3 (May 31, 2016)
+ - Breaking Change: Introduced a new way to configure application settings for Logstash through a settings.yml file.
+   This file is typically located in `LS_HOME/config`, or `/etc/logstash` when installed via packages. Logstash will not be 
+   able to start without this file, so please make sure to pass in `path.settings` if you are starting Logstash manually after 
+   installing it via a package (RPM, DEB) ([#4401](https://github.com/elastic/logstash/issues/4401)).
+ - Breaking Change: Most of the long form options (https://www.elastic.co/guide/en/logstash/5.0/command-line-flags.html) 
+   have been renamed to adhere to the yml dot notation to be used in the settings file. Short form options have not been
+   changed ([#4401](https://github.com/elastic/logstash/issues/4401)).
+ - Breaking Change: When Logstash is installed via DEB, RPM packages, it uses /usr/share and /var to install binaries and 
+   config files respectively. Previously it used to install in /opt directory. This change was done to make the user experience 
+   consistent with other Elastic products ([#5101](https://github.com/elastic/logstash/issues/5101)).
+ - Breaking Change: For plugin developers, the Event class has a [new API](https://github.com/elastic/logstash/issues/5141) 
+   to access its data. You will no longer be able to directly use the Event class through the ruby hash paradigm. All the 
+   plugins packaged with Logstash has been updated to use the new API and their versions bumped to the next major.
+ - Added support for systemd so you can now manage Logstash as a service on most Linux distributions ([#5012](https://github.com/elastic/logstash/issues/5012)).
+ - Added new subcommand `generate` to `logstash-plugins` script that bootstraps a new plugin with the right directory structure
+   and all the required files.
+ - Logstash can now emit its log in structured, json format. Specify `--log.format=json` in the settings file or via 
+   the command line ([#1569](https://github.com/elastic/logstash/issues/1569)).
+ - Added more operational information to help run Logstash in production. `_node/stats` now shows file descriptors 
+   and cpu information.
+ - Fixed a bug where Logstash would not shutdown when CTRL-C was used, when using stdin input in configuration ([#1769](https://github.com/elastic/logstash/issues/1769)).
+   
+### Input
+ - RabbitMQ: Removed `verify_ssl` option which was never used previously. To validate SSL certs use the 
+   `ssl_certificate_path` and `ssl_certificate_password` config options ([#82](https://github.com/logstash-plugins/logstash-input-rabbitmq/issues/82)).
+ - Stdin: This plugin is now non-blocking so you can use CTRL-C to stop Logstash.
+ - JDBC: Added `jdbc_password_filepath` parameter for reading password from an external file ([#120](https://github.com/logstash-plugins/logstash-input-jdbc/issues/120)).
+ 
+### Filter
+ - XML:
+   - Breaking: New configuration `suppress_empty` which defaults to `true`. Changed default behaviour of the plugin 
+     in favor of avoiding mapping conflicts when reaching elasticsearch ([#24](https://github.com/logstash-plugins/logstash-filter-xml/issues/24)).
+   - New configuration `force_content`. By default the filter expands attributes differently from content in xml 
+     elements. This option allows you to force text content and attributes to always parse to a hash value ([#16](https://github.com/logstash-plugins/logstash-filter-xml/issues/16)).
+   - Fixed a bug that ensure `target` is set when storing xml content in the event (`store_xml => true`).
+
+## 5.0.0-alpha2 (May 3, 2016
+### general
+ - Added `--preserve` option to `bin/logstash-plugin` install command. This allows us to preserve gem options 
+   which are already specified in `Gemfile`, which would have been previously overwritten.
+ - When running any plugin related commands you can now use DEBUG=1, to give the user a bit more 
+   information about what bundler is doing.
+ - Added reload support to the init script so you can do `service logstash reload`
+ - Fixed use of KILL_ON_STOP_TIMEOUT variable in init scripts which allows Logstash to force stop (#4991).
+ - Upgrade to JRuby 1.7.25.
+ - Filenames for Debian and RPM artifacts have been renamed to match Elasticsearch's naming scheme. The metadata 
+   is still the same, so upgrades will not be affected. If you have automated downloads for Logstash, please make
+   sure you have the updated URLs with the new names ([#5100](https://github.com/elastic/logstash/issues/5100)).  
+
+### Input
+ - Kafka: Fixed an issue where Snappy and LZ4 compression were not working.
+
+### Filter
+ - GeoIP: Added support for GeoIP2 city database and support for IPv6 lookups ([#23](https://github.com/logstash-plugins/logstash-filter-geoip/issues/23))
+
+### Output
+ - Elasticsearch: Added support for specifying ingest pipelines ([#410](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/410))
+ - Kafka: Fixed an issue where Snappy and LZ4 compression were not working ([#50](https://github.com/logstash-plugins/logstash-output-kafka/issues/50)).  
+
+
+## 5.0.0-alpha1 (April 5, 2016)
+### general
+ - Added APIs to monitor the Logstash pipeline. You can now query information/stats about event 
+   flow, JVM, and hot_threads.
+ - Added dynamic config, a new feature to track config file for changes and restart the 
+   pipeline (same process) with updated config changes. This feature can be enabled in two 
+   ways: Passing a CLI long-form option `--auto-reload` or with short-form `-r`. Another 
+   option, `--reload-interval <seconds>` controls how often LS should check the config files 
+   for changes. Alternatively, if you don't start with the CLI option, you can send SIGHUP 
+   or `kill -1` signal to LS to reload the config file, and restart the pipeline ([#4513](https://github.com/elastic/logstash/issues/4513)).
+ - Added support to evaluate environment variables inside the Logstash config. You can also specify a 
+   default if the variable is not defined. The syntax is `${myVar:default}` ([#3944](https://github.com/elastic/logstash/issues/3944)).
+ - Improved throughput performance across the board (up by 2x in some configs) by implementing Event 
+   representation in Java. Event is the main object that encapsulates data as it flows through 
+   Logstash and provides APIs for the plugins to perform processing. This change also enables 
+   faster serialization for future persistence work ([4191](https://github.com/elastic/logstash/issues/4191)).
+ - Added ability to configure custom garbage collection log file using `$LS_LOG_DIR`.
+ - `bin/plugin` in renamed to `bin/logstash-plugin`. This was renamed to prevent `PATH` being polluted 
+   when other components of the Elastic stack are installed on the same instance ([#4891](https://github.com/elastic/logstash/pull/4891)).
+ - Fixed a bug where new pipeline might break plugins by calling the `register` method twice causing 
+   undesired behavior ([#4851](https://github.com/elastic/logstash/issues/4851)).
+ - Made `JAVA_OPTS` and `LS_JAVA_OPTS` work consistently on Windows  ([#4758](https://github.com/elastic/logstash/pull/4758)).
+ - Fixed bug where specifying JMX parameters in `LS_JAVA_OPTS` caused Logstash not to restart properly
+   ([#4319](https://github.com/elastic/logstash/issues/4319)).
+ - Fixed a bug where upgrading plugins with Manticore threw an error and sometimes corrupted installation ([#4818](https://github.com/elastic/logstash/issues/4818)).
+ - Removed milestone warning that was displayed when the `--pluginpath` option was used to load plugins ([#4562](https://github.com/elastic/logstash/issues/4562)).
+ - Upgraded to JRuby 1.7.24.
+ - Reverted default output workers to 1. Perviously we had made output workers the same as number of pipeline
+   workers ([#4877](https://github.com/elastic/logstash/issues/4877)).
+   
+### input
+ - Beats
+   - Enhanced to verify client certificates against CA ([#8](https://github.com/logstash-plugins/logstash-input-beats/issues/8)).
+ - RabbitMQ
+   - Breaking Change: Metadata is now disabled by default because it was regressing performance.
+   - Improved performance by using an internal queue and bulk ACKs.
+ - Redis
+   - Increased the batch_size to 100 by default. This provides a big jump in throughput and 
+     reduction in CPU utilization ([#25](https://github.com/logstash-plugins/logstash-input-redis/issues/25)).
+ - JDBC
+   - Added retry connection feature ([#91](https://github.com/logstash-plugins/logstash-input-jdbc/issues/91)).
+ - Kafka
+   - Breaking: Added support for 0.9 consumer API. This plugin now supports SSL based encryption. This release 
+     changed a lot of configuration, so it is not backward compatible. Also, this version will not work with 
+     Kafka 0.8 broker
+   
+### filter
+  - DNS: 
+    - Improved performance by adding caches to both successful and failed requests.
+    - Added support for retrying with the `:max_retries` setting.
+    - Lowered the default value of timeout from 2 to 0.5 seconds.
+
+### output   
+  - Elasticsearch
+    - Bumped minimum manticore version to 0.5.4 which fixes a memory leak when sniffing 
+      is used ([#392](https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/392)).
+    - Fixed bug when updating documents with doc_as_upsert and scripting.
+    - Made error messages more verbose and easier to parse by humans.
+    - Retryable failures are now logged at the info level instead of warning.
+  - Kafka
+    - Breaking: Added support for 0.9 API. This plugin now supports SSL based encryption. This release 
+      changed a lot of configuration, so it is not backward compatible. Also, this version will not work with 
+      Kafka 0.8 broker      
+
 ## 1.5.5 (Oct 29, 2015)
 ### general
  - Update to JRuby 1.7.22
diff --git a/CONTRIBUTING.md b/CONTRIBUTING.md
index d325328823b..e9eab8029ac 100644
--- a/CONTRIBUTING.md
+++ b/CONTRIBUTING.md
@@ -10,7 +10,7 @@ That said, some basic guidelines, which you are free to ignore :)
 
 ## Want to learn?
 
-Want to lurk about and see what others are doing with Logstash? 
+Want to lurk about and see what others are doing with Logstash?
 
 * The irc channel (#logstash on irc.freenode.org) is a good place for this
 * The [forum](https://discuss.elastic.co/c/logstash) is also
@@ -18,7 +18,7 @@ Want to lurk about and see what others are doing with Logstash?
 
 ## Got Questions?
 
-Have a problem you want Logstash to solve for you? 
+Have a problem you want Logstash to solve for you?
 
 * You can ask a question in the [forum](https://discuss.elastic.co/c/logstash)
 * Alternately, you are welcome to join the IRC channel #logstash on
@@ -36,12 +36,32 @@ If you think you found a bug, it probably is a bug.
 * If it is specific to a plugin, please file it in the respective repository under [logstash-plugins](https://github.com/logstash-plugins)
 * or ask the [forum](https://discuss.elastic.co/c/logstash).
 
+## Issue Prioritization
+The Logstash team takes time to digest, consider solutions, and weigh applicability of issues to both the broad
+Logstash user base and our own goals for the project. Through this process, we assign issues a priority using GitHub
+labels. Below is a description of priority labels.
+
+* P1: A high priority issue that affects almost all Logstash users. Bugs that would cause data loss, security
+issues and test failures. Workarounds for P1s generally don’t exist without a code change. A P1 issue is usually
+stop the world kinda scenario, so we need to make sure P1s are properly triaged and being worked upon.
+* P2: A broadly applicable, high visibility issue that enhances Logstash usability for a majority of users.
+* P3: Nice-to-have bug fixes or functionality.  Workarounds for P3s generally exist.
+* P4: Anything not in above, catch-all label.
+
 # Contributing Documentation and Code Changes
 
-If you have a bugfix or new feature that you would like to contribute to
-logstash, and you think it will take more than a few minutes to produce the fix
-(ie; write code), it is worth discussing the change with the Logstash users and developers first! You can reach us via [GitHub](https://github.com/elastic/logstash/issues), the [forum](https://discuss.elastic.co/c/logstash), or via IRC (#logstash on freenode irc)
-Please note that Pull Requests without tests will not be merged. If you would like to contribute but do not have experience with writing tests, please ping us on IRC/forum or create a PR and ask our help.
+If you have a bugfix or new feature that you would like to contribute to Logstash, and you think it will take
+more than a few minutes to produce the fix (ie; write code), it is worth discussing the change with the Logstash
+users and developers first! You can reach us via [GitHub](https://github.com/elastic/logstash/issues), the [forum](https://discuss.elastic.co/c/logstash), or via IRC (#logstash on freenode irc)
+Please note that Pull Requests without tests will not be merged. If you would like to contribute but do not have
+experience with writing tests, please ping us on IRC/forum or create a PR and ask our help.
+
+If you would like to contribute to Logstash, but don't know where to start, you can use the GitHub labels "adoptme"
+and "low hanging fruit". Issues marked with these labels are relatively easy, and provides a good starting
+point to contribute to Logstash.
+
+See: https://github.com/elastic/logstash/labels/adoptme
+https://github.com/elastic/logstash/labels/low%20hanging%20fruit
 
 ## Contributing to plugins
 
@@ -61,5 +81,3 @@ Check our [documentation](https://www.elastic.co/guide/en/logstash/current/contr
    request](https://help.github.com/articles/using-pull-requests). In the pull
    request, describe what your changes do and mention any bugs/issues related
    to the pull request.
-
-
diff --git a/Gemfile b/Gemfile
index 679cbf93403..9e2c0c8d835 100644
--- a/Gemfile
+++ b/Gemfile
@@ -2,25 +2,114 @@
 # If you modify this file manually all comments and formatting will be lost.
 
 source "https://rubygems.org"
-gem "logstash-core", "3.0.0.dev", :path => "./logstash-core"
-# gem "logstash-core-event", "3.0.0.dev", :path => "./logstash-core-event"
-gem "logstash-core-event-java", "3.0.0.dev", :path => "./logstash-core-event-java"
-gem "logstash-core-plugin-api", "1.0.0", :path => "./logstash-core-plugin-api"
+gem "logstash-core", :path => "./logstash-core"
+gem "logstash-core-event-java", :path => "./logstash-core-event-java"
+gem "logstash-core-plugin-api", :path => "./logstash-core-plugin-api"
 gem "file-dependencies", "0.1.6"
 gem "ci_reporter_rspec", "1.0.0", :group => :development
 gem "simplecov", :group => :development
 gem "coveralls", :group => :development
-# Tins 1.7 requires the ruby 2.0 platform to install,
-# this gem is a dependency of term-ansi-color which is a dependency of coveralls.
-# 1.6 is the last supported version on jruby.
 gem "tins", "1.6", :group => :development
 gem "rspec", "~> 3.1.0", :group => :development
-gem "logstash-devutils", "~> 0.0.15", :group => :development
+gem "logstash-devutils", "~> 1.1", :group => :development
 gem "benchmark-ips", :group => :development
 gem "octokit", "3.8.0", :group => :build
-gem "stud", "~> 0.0.21", :group => :build
+gem "stud", "~> 0.0.22", :group => :build
 gem "fpm", "~> 1.3.3", :group => :build
 gem "rubyzip", "~> 1.1.7", :group => :build
 gem "gems", "~> 0.8.3", :group => :build
 gem "rack-test", :require => "rack/test", :group => :development
 gem "flores", "~> 0.0.6", :group => :development
+gem "term-ansicolor", "~> 1.3.2", :group => :development
+gem "docker-api", "1.31.0", :group => :development
+gem "pleaserun"
+gem "logstash-input-heartbeat"
+gem "logstash-codec-collectd"
+gem "logstash-output-xmpp"
+gem "logstash-codec-dots"
+gem "logstash-codec-edn"
+gem "logstash-codec-edn_lines"
+gem "logstash-codec-fluent"
+gem "logstash-codec-es_bulk"
+gem "logstash-codec-graphite"
+gem "logstash-codec-json"
+gem "logstash-codec-json_lines"
+gem "logstash-codec-line"
+gem "logstash-codec-msgpack"
+gem "logstash-codec-multiline"
+gem "logstash-codec-netflow"
+gem "logstash-codec-plain"
+gem "logstash-codec-rubydebug"
+gem "logstash-filter-clone"
+gem "logstash-filter-csv"
+gem "logstash-filter-date"
+gem "logstash-filter-dns"
+gem "logstash-filter-drop"
+gem "logstash-filter-fingerprint"
+gem "logstash-filter-geoip"
+gem "logstash-filter-grok"
+gem "logstash-filter-json"
+gem "logstash-filter-kv"
+gem "logstash-filter-metrics"
+gem "logstash-filter-mutate"
+gem "logstash-filter-ruby"
+gem "logstash-filter-sleep"
+gem "logstash-filter-split"
+gem "logstash-filter-syslog_pri"
+gem "logstash-filter-throttle"
+gem "logstash-filter-urldecode"
+gem "logstash-filter-useragent"
+gem "logstash-filter-uuid"
+gem "logstash-filter-xml"
+gem "logstash-input-couchdb_changes"
+gem "logstash-input-elasticsearch"
+gem "logstash-input-exec"
+gem "logstash-input-file"
+gem "logstash-input-ganglia"
+gem "logstash-input-gelf"
+gem "logstash-input-generator"
+gem "logstash-input-graphite"
+gem "logstash-input-http"
+gem "logstash-input-http_poller"
+gem "logstash-input-imap"
+gem "logstash-input-irc"
+gem "logstash-input-jdbc"
+gem "logstash-input-log4j"
+gem "logstash-input-lumberjack"
+gem "logstash-input-pipe"
+gem "logstash-input-rabbitmq"
+gem "logstash-input-redis"
+gem "logstash-input-s3"
+gem "logstash-input-snmptrap"
+gem "logstash-input-sqs"
+gem "logstash-input-stdin"
+gem "logstash-input-syslog"
+gem "logstash-input-tcp"
+gem "logstash-input-twitter"
+gem "logstash-input-udp"
+gem "logstash-input-unix"
+gem "logstash-input-xmpp"
+gem "logstash-input-kafka"
+gem "logstash-input-beats"
+gem "logstash-output-cloudwatch"
+gem "logstash-output-csv"
+gem "logstash-output-elasticsearch"
+gem "logstash-output-file"
+gem "logstash-output-graphite"
+gem "logstash-output-http"
+gem "logstash-output-irc"
+gem "logstash-output-kafka"
+gem "logstash-output-nagios"
+gem "logstash-output-null"
+gem "logstash-output-pagerduty"
+gem "logstash-output-pipe"
+gem "logstash-output-rabbitmq"
+gem "logstash-output-redis"
+gem "logstash-output-s3"
+gem "logstash-output-sns"
+gem "logstash-output-sqs"
+gem "logstash-output-statsd"
+gem "logstash-output-stdout"
+gem "logstash-output-tcp"
+gem "logstash-output-udp"
+gem "logstash-output-webhdfs"
diff --git a/Gemfile.jruby-1.9.lock b/Gemfile.jruby-1.9.lock
index 69163d4626a..b4bdc9e0d90 100644
--- a/Gemfile.jruby-1.9.lock
+++ b/Gemfile.jruby-1.9.lock
@@ -1,21 +1,21 @@
 PATH
   remote: ./logstash-core
   specs:
-    logstash-core (3.0.0.dev-java)
-      cabin (~> 0.8.0)
+    logstash-core (5.1.0-java)
       chronic_duration (= 0.10.6)
       clamp (~> 0.6.5)
       concurrent-ruby (= 1.0.0)
       filesize (= 0.0.4)
       gems (~> 0.8.3)
       i18n (= 0.6.9)
-      jrjackson (~> 0.3.7)
-      jruby-monitoring (~> 0.1)
-      jruby-openssl (= 0.9.13)
-      logstash-core-event-java (~> 3.0.0.dev)
+      jar-dependencies (~> 0.3.4)
+      jrjackson (~> 0.4.0)
+      jrmonitor (~> 0.4.2)
+      jruby-openssl (= 0.9.16)
+      logstash-core-event-java (= 5.1.0)
       minitar (~> 0.5.4)
       pry (~> 0.10.1)
-      puma (~> 2.15, >= 2.15.3)
+      puma (~> 2.16)
       rubyzip (~> 1.1.7)
       sinatra (~> 1.4, >= 1.4.6)
       stud (~> 0.0.19)
@@ -25,18 +25,39 @@ PATH
 PATH
   remote: ./logstash-core-event-java
   specs:
-    logstash-core-event-java (3.0.0.dev-java)
+    logstash-core-event-java (5.1.0-java)
       jar-dependencies
       ruby-maven (~> 3.3.9)
 
+PATH
+  remote: ./logstash-core-plugin-api
+  specs:
+    logstash-core-plugin-api (2.1.12-java)
+      logstash-core (= 5.1.0)
+
 GEM
   remote: https://rubygems.org/
   specs:
     addressable (2.3.8)
     arr-pm (0.0.10)
       cabin (> 0)
-    backports (3.6.7)
-    benchmark-ips (2.3.0)
+    atomic (1.1.99-java)
+    avl_tree (1.2.1)
+      atomic (~> 1.1)
+    awesome_print (1.7.0)
+    aws-sdk (2.3.22)
+      aws-sdk-resources (= 2.3.22)
+    aws-sdk-core (2.3.22)
+      jmespath (~> 1.0)
+    aws-sdk-resources (2.3.22)
+      aws-sdk-core (= 2.3.22)
+    aws-sdk-v1 (1.66.0)
+      json (~> 1.4)
+      nokogiri (>= 1.4.4)
+    backports (3.6.8)
+    benchmark-ips (2.7.2)
+    bindata (2.3.1)
+    buftok (0.2.0)
     builder (3.2.2)
     cabin (0.8.1)
     childprocess (0.5.9)
@@ -48,26 +69,38 @@ GEM
     ci_reporter_rspec (1.0.0)
       ci_reporter (~> 2.0)
       rspec (>= 2.14, < 4)
+    cinch (2.3.2)
     clamp (0.6.5)
-    coderay (1.1.0)
+    coderay (1.1.1)
     concurrent-ruby (1.0.0-java)
-    coveralls (0.8.10)
-      json (~> 1.8)
-      rest-client (>= 1.6.8, < 2)
-      simplecov (~> 0.11.0)
+    coveralls (0.8.15)
+      json (>= 1.8, < 3)
+      simplecov (~> 0.12.0)
       term-ansicolor (~> 1.3)
       thor (~> 0.19.1)
-      tins (~> 1.6.0)
+      tins (>= 1.6.0, < 2)
     diff-lcs (1.2.5)
     docile (1.1.5)
-    domain_name (0.5.20160128)
+    domain_name (0.5.20160615)
       unf (>= 0.0.5, < 1.0.0)
+    edn (1.1.1)
+    elasticsearch (1.1.0)
+      elasticsearch-api (= 1.1.0)
+      elasticsearch-transport (= 1.1.0)
+    elasticsearch-api (1.1.0)
+      multi_json
+    elasticsearch-transport (1.1.0)
+      faraday
+      multi_json
+    equalizer (0.0.10)
     faraday (0.9.2)
       multipart-post (>= 1.2, < 3)
-    ffi (1.9.10-java)
+    ffi (1.9.14-java)
     file-dependencies (0.1.6)
       minitar
     filesize (0.0.4)
+    filewatch (0.9.0)
+    fivemat (1.3.2)
     flores (0.0.6)
     fpm (1.3.3)
       arr-pm (~> 0.0.9)
@@ -77,37 +110,417 @@ GEM
       clamp (~> 0.6)
       ffi
       json (>= 1.7.7)
+    gelfd (0.2.0)
     gem_publisher (1.5.0)
     gems (0.8.3)
+    hitimes (1.2.4-java)
+    http (0.9.9)
+      addressable (~> 2.3)
+      http-cookie (~> 1.0)
+      http-form_data (~> 1.0.1)
+      http_parser.rb (~> 0.6.0)
     http-cookie (1.0.2)
       domain_name (~> 0.5)
+    http-form_data (1.0.1)
+    http_parser.rb (0.6.0-java)
     i18n (0.6.9)
     insist (1.0.0)
-    jar-dependencies (0.3.2)
-    jrjackson (0.3.8)
-    jruby-monitoring (0.3.0)
-    jruby-openssl (0.9.13-java)
+    jar-dependencies (0.3.5)
+    jls-grok (0.11.3)
+      cabin (>= 0.6.0)
+    jls-lumberjack (0.0.26)
+      concurrent-ruby
+    jmespath (1.3.1)
+    jrjackson (0.4.0-java)
+    jrmonitor (0.4.2)
+    jruby-openssl (0.9.16-java)
+    jruby-stdin-channel (0.2.0-java)
     json (1.8.3-java)
-    kramdown (1.9.0)
-    logstash-devutils (0.0.18-java)
+    kramdown (1.12.0)
+    logstash-codec-collectd (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-dots (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn (3.0.2)
+      edn
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-edn_lines (3.0.2)
+      edn
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-es_bulk (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-fluent (3.0.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-graphite (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-json_lines (3.0.2)
+      logstash-codec-line (>= 2.1.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-line (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-msgpack (3.0.2-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      msgpack-jruby
+    logstash-codec-multiline (3.0.2)
+      jls-grok (~> 0.11.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-codec-netflow (3.1.2)
+      bindata (>= 1.5.0)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-plain (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-codec-rubydebug (3.0.2)
+      awesome_print
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-devutils (1.0.2-java)
+      fivemat
       gem_publisher
       insist (= 1.0.0)
       kramdown
+      logstash-core-plugin-api (~> 2.0)
       minitar
       rake
-      rspec (~> 3.1.0)
+      rspec (~> 3.0)
       rspec-wait
       stud (>= 0.0.20)
+    logstash-filter-clone (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-csv (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-date (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-dns (3.0.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+    logstash-filter-drop (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-fingerprint (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      murmurhash3
+    logstash-filter-geoip (4.0.3-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-grok (3.2.1)
+      jls-grok (~> 0.11.3)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-patterns-core
+    logstash-filter-json (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-kv (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-metrics (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      metriks
+      thread_safe
+    logstash-filter-mutate (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-ruby (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+    logstash-filter-sleep (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-split (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-syslog_pri (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-throttle (4.0.0)
+      atomic
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe
+    logstash-filter-urldecode (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-useragent (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      lru_redux (~> 1.1.0)
+      user_agent_parser (>= 2.0.0)
+    logstash-filter-uuid (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-filter-xml (4.0.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      nokogiri
+      xml-simple
+    logstash-input-beats (3.1.4)
+      concurrent-ruby (>= 0.9.2, <= 1.0.0)
+      jar-dependencies (~> 0.3.4)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      thread_safe (~> 0.3.5)
+    logstash-input-couchdb_changes (3.0.2)
+      json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22)
+    logstash-input-elasticsearch (3.0.2)
+      elasticsearch (~> 1.0, >= 1.0.6)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-exec (3.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-file (3.0.3)
+      addressable
+      filewatch (~> 0.8, >= 0.8.1)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-ganglia (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-gelf (3.0.2)
+      gelfd (= 0.2.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-generator (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-graphite (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-tcp
+    logstash-input-heartbeat (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-input-http (3.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      puma (~> 2.16, >= 2.16.0)
+      rack (~> 1)
+      stud
+    logstash-input-http_poller (3.1.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 2.2.4, < 5.0.0)
+      rufus-scheduler (~> 3.0.9)
+      stud (~> 0.0.22)
+    logstash-input-imap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      mail (~> 2.6.3)
+      mime-types (= 2.6.2)
+      stud (~> 0.0.22)
+    logstash-input-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-jdbc (4.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      rufus-scheduler
+      sequel
+      tzinfo
+      tzinfo-data
+    logstash-input-kafka (5.0.4)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1.0)
+    logstash-input-log4j (3.0.3-java)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-lumberjack (3.1.1)
+      concurrent-ruby
+      jls-lumberjack (~> 0.0.26)
+      logstash-codec-multiline (~> 3.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-rabbitmq (5.1.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.1.1, < 5.0.0)
+    logstash-input-redis (3.1.1)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+    logstash-input-s3 (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.18)
+    logstash-input-snmptrap (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snmp
+    logstash-input-sqs (3.0.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-input-stdin (3.1.1)
+      concurrent-ruby
+      jruby-stdin-channel
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-syslog (3.0.2)
+      concurrent-ruby
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-date
+      logstash-filter-grok
+      stud (>= 0.0.22, < 0.1.0)
+      thread_safe
+    logstash-input-tcp (4.0.3)
+      logstash-codec-json
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-twitter (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (>= 0.0.22, < 0.1)
+      twitter (= 5.15.0)
+    logstash-input-udp (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud (~> 0.0.22)
+    logstash-input-unix (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-input-xmpp (3.1.1)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-mixin-aws (4.1.0)
+      aws-sdk (~> 2.3.0)
+      aws-sdk-v1 (>= 1.61.0)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-mixin-http_client (4.0.3)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.2, < 1.0.0)
+    logstash-mixin-rabbitmq_connection (4.1.2-java)
+      march_hare (~> 2.15.0)
+      stud (~> 0.0.22)
+    logstash-output-cloudwatch (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      rufus-scheduler (~> 3.0.9)
+    logstash-output-csv (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-filter-json
+      logstash-input-generator
+      logstash-output-file
+    logstash-output-elasticsearch (4.1.3-java)
+      cabin (~> 0.6)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      manticore (>= 0.5.4, < 1.0.0)
+      stud (~> 0.0, >= 0.0.17)
+    logstash-output-file (3.0.2)
+      logstash-codec-json_lines
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-graphite (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-http (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-http_client (>= 2.2.1, < 5.0.0)
+    logstash-output-irc (3.0.2)
+      cinch
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-kafka (5.0.4)
+      logstash-codec-json
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-nagios (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-null (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pagerduty (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-pipe (3.0.2)
+      logstash-codec-plain
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-rabbitmq (4.0.4-java)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-rabbitmq_connection (>= 4.1.1, < 5.0.0)
+    logstash-output-redis (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      redis
+      stud
+    logstash-output-s3 (3.1.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws
+      stud (~> 0.0.22)
+    logstash-output-sns (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+    logstash-output-sqs (3.0.2)
+      aws-sdk
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-mixin-aws (>= 1.0.0)
+      stud
+    logstash-output-statsd (3.1.1)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      logstash-input-generator
+      statsd-ruby (= 1.2.0)
+    logstash-output-stdout (3.0.2)
+      logstash-codec-line
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-tcp (3.2.0)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      stud
+    logstash-output-udp (3.0.2)
+      logstash-codec-json
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    logstash-output-webhdfs (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      snappy (= 0.0.12)
+      webhdfs
+    logstash-output-xmpp (3.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+      xmpp4r (= 0.5)
+    logstash-patterns-core (4.0.2)
+      logstash-core-plugin-api (>= 1.60, <= 2.99)
+    lru_redux (1.1.0)
+    mail (2.6.4)
+      mime-types (>= 1.16, < 4)
+    manticore (0.6.0-java)
+    march_hare (2.15.0-java)
+    memoizable (0.4.2)
+      thread_safe (~> 0.3, >= 0.3.1)
     method_source (0.8.2)
-    mime-types (2.99)
+    metriks (0.9.9.7)
+      atomic (~> 1.0)
+      avl_tree (~> 1.2.0)
+      hitimes (~> 1.1)
+    mime-types (2.6.2)
     minitar (0.5.4)
+    msgpack-jruby (1.4.1-java)
+    multi_json (1.12.1)
     multipart-post (2.0.0)
-    netrc (0.11.0)
+    murmurhash3 (0.1.6-java)
+    mustache (0.99.8)
+    naught (1.1.0)
+    nokogiri (1.6.8-java)
     numerizer (0.1.1)
     octokit (3.8.0)
       sawyer (~> 0.6.0, >= 0.5.3)
+    pleaserun (0.0.24)
+      cabin (> 0)
+      clamp
+      insist
+      mustache (= 0.99.8)
+      stud
     polyglot (0.3.5)
-    pry (0.10.3-java)
+    pry (0.10.4-java)
       coderay (~> 1.1.0)
       method_source (~> 0.8.1)
       slop (~> 3.4)
@@ -118,11 +531,8 @@ GEM
       rack
     rack-test (0.6.3)
       rack (>= 1.0)
-    rake (10.5.0)
-    rest-client (1.8.0)
-      http-cookie (>= 1.0.2, < 2.0)
-      mime-types (>= 1.16, < 3.0)
-      netrc (~> 0.7)
+    rake (11.2.2)
+    redis (3.3.1)
     rspec (3.1.0)
       rspec-core (~> 3.1.0)
       rspec-expectations (~> 3.1.0)
@@ -135,18 +545,22 @@ GEM
     rspec-mocks (3.1.3)
       rspec-support (~> 3.1.0)
     rspec-support (3.1.2)
-    rspec-wait (0.0.8)
-      rspec (>= 2.11, < 3.5)
-    ruby-maven (3.3.9)
-      ruby-maven-libs (~> 3.3.1)
-    ruby-maven-libs (3.3.3)
+    rspec-wait (0.0.9)
+      rspec (>= 3, < 4)
+    ruby-maven (3.3.12)
+      ruby-maven-libs (~> 3.3.9)
+    ruby-maven-libs (3.3.9)
     rubyzip (1.1.7)
+    rufus-scheduler (3.0.9)
+      tzinfo
     sawyer (0.6.0)
       addressable (~> 2.3.5)
       faraday (~> 0.8, < 0.10)
-    simplecov (0.11.2)
+    sequel (4.37.0)
+    simple_oauth (0.3.1)
+    simplecov (0.12.0)
       docile (~> 1.1.0)
-      json (~> 1.8)
+      json (>= 1.8, < 3)
       simplecov-html (~> 0.10.0)
     simplecov-html (0.10.0)
     sinatra (1.4.7)
@@ -154,19 +568,44 @@ GEM
       rack-protection (~> 1.4)
       tilt (>= 1.3, < 3)
     slop (3.6.0)
-    spoon (0.0.4)
+    snappy (0.0.12-java)
+      snappy-jars (~> 1.1.0)
+    snappy-jars (1.1.0.1.2-java)
+    snmp (1.2.0)
+    spoon (0.0.6)
       ffi
+    statsd-ruby (1.2.0)
     stud (0.0.22)
     term-ansicolor (1.3.2)
       tins (~> 1.0)
     thor (0.19.1)
     thread_safe (0.3.5-java)
-    tilt (2.0.2)
+    tilt (2.0.5)
     tins (1.6.0)
     treetop (1.4.15)
       polyglot
       polyglot (>= 0.3.1)
+    twitter (5.15.0)
+      addressable (~> 2.3)
+      buftok (~> 0.2.0)
+      equalizer (= 0.0.10)
+      faraday (~> 0.9.0)
+      http (>= 0.4, < 0.10)
+      http_parser.rb (~> 0.6.0)
+      json (~> 1.8)
+      memoizable (~> 0.4.0)
+      naught (~> 1.0)
+      simple_oauth (~> 0.3.0)
+    tzinfo (1.2.2)
+      thread_safe (~> 0.1)
+    tzinfo-data (1.2016.6)
+      tzinfo (>= 1.0.0)
     unf (0.1.4-java)
+    user_agent_parser (2.3.0)
+    webhdfs (0.8.0)
+      addressable
+    xml-simple (1.1.5)
+    xmpp4r (0.5)
 
 PLATFORMS
   java
@@ -179,13 +618,105 @@ DEPENDENCIES
   flores (~> 0.0.6)
   fpm (~> 1.3.3)
   gems (~> 0.8.3)
-  logstash-core (= 3.0.0.dev)!
-  logstash-core-event-java (= 3.0.0.dev)!
-  logstash-devutils (~> 0.0.15)
+  logstash-codec-collectd
+  logstash-codec-dots
+  logstash-codec-edn
+  logstash-codec-edn_lines
+  logstash-codec-es_bulk
+  logstash-codec-fluent
+  logstash-codec-graphite
+  logstash-codec-json
+  logstash-codec-json_lines
+  logstash-codec-line
+  logstash-codec-msgpack
+  logstash-codec-multiline
+  logstash-codec-netflow
+  logstash-codec-plain
+  logstash-codec-rubydebug
+  logstash-core!
+  logstash-core-event-java!
+  logstash-core-plugin-api!
+  logstash-devutils
+  logstash-filter-clone
+  logstash-filter-csv
+  logstash-filter-date
+  logstash-filter-dns
+  logstash-filter-drop
+  logstash-filter-fingerprint
+  logstash-filter-geoip
+  logstash-filter-grok
+  logstash-filter-json
+  logstash-filter-kv
+  logstash-filter-metrics
+  logstash-filter-mutate
+  logstash-filter-ruby
+  logstash-filter-sleep
+  logstash-filter-split
+  logstash-filter-syslog_pri
+  logstash-filter-throttle
+  logstash-filter-urldecode
+  logstash-filter-useragent
+  logstash-filter-uuid
+  logstash-filter-xml
+  logstash-input-beats (= 3.1.0.beta4)
+  logstash-input-couchdb_changes
+  logstash-input-elasticsearch
+  logstash-input-exec
+  logstash-input-file
+  logstash-input-ganglia
+  logstash-input-gelf
+  logstash-input-generator
+  logstash-input-graphite
+  logstash-input-heartbeat
+  logstash-input-http
+  logstash-input-http_poller
+  logstash-input-imap
+  logstash-input-irc
+  logstash-input-jdbc
+  logstash-input-kafka
+  logstash-input-log4j
+  logstash-input-lumberjack
+  logstash-input-pipe
+  logstash-input-rabbitmq
+  logstash-input-redis
+  logstash-input-s3
+  logstash-input-snmptrap
+  logstash-input-sqs
+  logstash-input-stdin
+  logstash-input-syslog
+  logstash-input-tcp
+  logstash-input-twitter
+  logstash-input-udp
+  logstash-input-unix
+  logstash-input-xmpp
+  logstash-output-cloudwatch
+  logstash-output-csv
+  logstash-output-elasticsearch
+  logstash-output-file
+  logstash-output-graphite
+  logstash-output-http
+  logstash-output-irc
+  logstash-output-kafka
+  logstash-output-nagios
+  logstash-output-null
+  logstash-output-pagerduty
+  logstash-output-pipe
+  logstash-output-rabbitmq
+  logstash-output-redis
+  logstash-output-s3
+  logstash-output-sns
+  logstash-output-sqs
+  logstash-output-statsd
+  logstash-output-stdout
+  logstash-output-tcp
+  logstash-output-udp
+  logstash-output-webhdfs
+  logstash-output-xmpp
   octokit (= 3.8.0)
+  pleaserun
   rack-test
   rspec (~> 3.1.0)
   rubyzip (~> 1.1.7)
   simplecov
-  stud (~> 0.0.21)
+  stud (~> 0.0.22)
   tins (= 1.6)
diff --git a/Makefile b/Makefile
deleted file mode 100644
index db263b80aff..00000000000
--- a/Makefile
+++ /dev/null
@@ -1,2 +0,0 @@
-%: 
-	rake $@
diff --git a/README.md b/README.md
index 92229c33dcb..e8aeedf903d 100644
--- a/README.md
+++ b/README.md
@@ -2,10 +2,9 @@
 
 ### Build status
 
-| Branch | master | 2.x | 2.1
+| Test | master | 5.0 | 2.3 |
 |---|---|---|---|
-| core | [![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/logstash_regression_master/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_2x/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_2x/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_21/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/logstash_regression_21/) |
-| integration | [![Build Status](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/badge/icon)](http://build-eu-00.elastic.co/view/LS%20Master/job/Logstash_Master_Default_Plugins/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_2x/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_2x/) | [![Build Status](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_21/badge/icon)](http://build-eu-00.elastic.co/view/LS%202.x/job/Logstash_Default_Plugins_21/) |
+| core | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=master)](https://travis-ci.org/elastic/logstash) | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=2.3)](https://travis-ci.org/elastic/logstash) | [![Build Status](https://travis-ci.org/elastic/logstash.svg?branch=2.3)](https://travis-ci.org/elastic/logstash) |
 
 Logstash is a tool for managing events and logs. You can use it to collect
 logs, parse them, and store them for later use (like, for searching).  If you
@@ -172,4 +171,4 @@ see that here.
 It is more important to me that you are able to contribute.
 
 For more information about contributing, see the
-[CONTRIBUTING](CONTRIBUTING.md) file.
+[CONTRIBUTING](.github/CONTRIBUTING.md) file.
diff --git a/ROADMAP.md b/ROADMAP.md
new file mode 100644
index 00000000000..9583b301af1
--- /dev/null
+++ b/ROADMAP.md
@@ -0,0 +1,15 @@
+## Logstash Roadmap
+
+[Logstash](https://www.elastic.co/products/logstash "Logstash") is an open-source data collection engine, developed directly on Github and distributed under the Apache License 2.0. The roadmap is defined by the core themes of performance, resiliency, and manageability, along with the overall plugin ecosystem. User requirements are the main driving force behind our development efforts. If a user has a bad time, it’s a bug!  All submitted [issues](https://github.com/elastic/logstash/issues/new "New Issue"), suggestions, or ideas are greatly encouraged and appreciated.
+
+### Logstash Core
+
+With an open development model, the core roadmap can generally be distilled in Github. Long term roadmap features can be viewed with the "[roadmap](https://github.com/elastic/logstash/labels/roadmap "Roadmap Features")" label. The features and bug fixes targeted for an upcoming release can be viewed with the specific release label, e.g. "[v5.0.0](https://github.com/elastic/logstash/issues?utf8=%E2%9C%93&q=is%3Aissue+label%3Av5.0.0 "Logstash 5.0 Release")". Please note that feature timing and priorities are susceptible to change and therefore not guaranteed.
+
+### Logstash Plugins
+
+The [Logstash Plugins](https://github.com/logstash-plugins "Logstash Plugins") ecosystem enables the innovation of additional integrations and processing capabilities for the core engine. Plugins in this organization are either maintained by Elastic or the community. Community maintained plugins have a special banner in the documentation page, e.g. [Salesforce input](https://www.elastic.co/guide/en/logstash/current/plugins-inputs-salesforce.html "Salesforce Input Plugin"). Many awesome humans have taken the roadmap into their own hands by becoming community maintainers. If you’re actively working with specific community plugins and would like to get more involved, feel free to reach out in this [forum thread](https://discuss.elastic.co/t/logstash-plugins-community-maintainers/35953 "Community Maintainers").
+
+## Latest Changes
+
+For a list of latest changes across Logstash and its plugins, see the [release notes](https://www.elastic.co/guide/en/logstash/current/releasenotes.html "Current Release Notes").
diff --git a/acceptance_spec/acceptance/install_spec.rb b/acceptance_spec/acceptance/install_spec.rb
deleted file mode 100644
index 45efc1bf6fb..00000000000
--- a/acceptance_spec/acceptance/install_spec.rb
+++ /dev/null
@@ -1,95 +0,0 @@
-require_relative '../spec_helper_acceptance'
-
-branch = ENV['LS_BRANCH'] || 'master'
-build_url = 'https://s3-eu-west-1.amazonaws.com/build-eu.elasticsearch.org/logstash'
-
-describe "Logstash class:" do
-
-  case fact('osfamily')
-  when 'RedHat'
-    core_package_name    = 'logstash'
-    service_name         = 'logstash'
-    core_url             = "#{build_url}/#{branch}/nightly/JDK7/logstash-latest-SNAPSHOT.rpm"
-    pid_file             = '/var/run/logstash.pid'
-  when 'Debian'
-    core_package_name    = 'logstash'
-    service_name         = 'logstash'
-    core_url             = "#{build_url}/#{branch}/nightly/JDK7/logstash-latest-SNAPSHOT.deb"
-    pid_file             = '/var/run/logstash.pid'
-  end
-
-  context "Install Nightly core package" do
-
-    it 'should run successfully' do
-      pp = "class { 'logstash': package_url => '#{core_url}', java_install => true }
-            logstash::configfile { 'basic_config': content => 'input { tcp { port => 2000 } } output { stdout { } } ' }
-           "
-
-      # Run it twice and test for idempotency
-      apply_manifest(pp, :catch_failures => true)
-      sleep 20
-      expect(apply_manifest(pp, :catch_failures => true).exit_code).to be_zero
-
-    end
-
-    describe package(core_package_name) do
-      it { should be_installed }
-    end
-
-    describe service(service_name) do
-      it { should be_enabled }
-      it { should be_running }
-    end
-
-    describe file(pid_file) do
-      it { should be_file }
-      its(:content) { should match /[0-9]+/ }
-    end
-
-    describe port(2000) do
-      it {
-        sleep 30
-        should be_listening
-      }
-    end
-
-  end
-
-  context "ensure we are still running" do
-
-    describe service(service_name) do
-      it {
-        sleep 30
-        should be_running
-      }
-    end
-
-    describe port(2000) do
-      it { should be_listening }
-    end
-
-  end
-
-  describe "module removal" do
-
-    it 'should run successfully' do
-      pp = "class { 'logstash': ensure => 'absent' }"
-
-      # Run it twice and test for idempotency
-      apply_manifest(pp, :catch_failures => true)
-
-    end
-
-    describe service(service_name) do
-      it { should_not be_enabled }
-      it { should_not be_running }
-    end
-
-    describe package(core_package_name) do
-      it { should_not be_installed }
-    end
-
-  end
-
-end
-
diff --git a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml b/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
deleted file mode 100644
index 4f33d28f788..00000000000
--- a/acceptance_spec/acceptance/nodesets/centos-6-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  centos-6-x64:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: el-6-x86_64
-    image: electrical/centos:6.4
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'yum install -y wget ntpdate rubygems ruby-augeas ruby-devel augeas-devel logrotate'
-      - 'touch /etc/sysconfig/network'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml b/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
deleted file mode 100644
index 7b2df7423e5..00000000000
--- a/acceptance_spec/acceptance/nodesets/debian-6-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  debian-6:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: debian-6-amd64
-    image: electrical/debian:6.0.8
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
-      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml b/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
deleted file mode 100644
index 704b6c7d424..00000000000
--- a/acceptance_spec/acceptance/nodesets/debian-7-x64.yml
+++ /dev/null
@@ -1,16 +0,0 @@
-HOSTS:
-  debian-7:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: debian-7-amd64
-    image: electrical/debian:7.3
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq lsb-release wget net-tools ruby rubygems ruby1.8-dev libaugeas-dev libaugeas-ruby ntpdate locales-all logrotate'
-      - 'REALLY_GEM_UPDATE_SYSTEM=1 gem update --system --no-ri --no-rdoc'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
deleted file mode 100644
index 4d6879e74b0..00000000000
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1204-x64.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-HOSTS:
-  ubuntu-12-04:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: ubuntu-12.04-amd64
-    image: electrical/ubuntu:12.04
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq ruby1.8-dev libaugeas-dev libaugeas-ruby ruby rubygems lsb-release wget net-tools curl logrotate'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml b/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
deleted file mode 100644
index 0f6e1772f29..00000000000
--- a/acceptance_spec/acceptance/nodesets/ubuntu-server-1404-x64.yml
+++ /dev/null
@@ -1,15 +0,0 @@
-HOSTS:
-  ubuntu-14-04:
-    roles:
-      - master
-      - database
-      - dashboard
-    platform: ubuntu-14.04-amd64
-    image: electrical/ubuntu:14.04
-    hypervisor: docker
-    docker_cmd: '["/sbin/init"]'
-    docker_image_commands:
-      - 'apt-get install -yq ruby ruby1.9.1-dev libaugeas-dev libaugeas-ruby lsb-release wget net-tools curl logrotate'
-    docker_preserve_image: true
-CONFIG:
-  type: foss
diff --git a/acceptance_spec/spec_helper_acceptance.rb b/acceptance_spec/spec_helper_acceptance.rb
deleted file mode 100644
index 752743b4aa5..00000000000
--- a/acceptance_spec/spec_helper_acceptance.rb
+++ /dev/null
@@ -1,70 +0,0 @@
-require 'beaker-rspec'
-require 'pry'
-require 'securerandom'
-
-files_dir = ENV['files_dir'] || '/home/jenkins/puppet'
-
-proxy_host = ENV['BEAKER_PACKAGE_PROXY'] || ''
-
-if !proxy_host.empty?
-  gem_proxy = "http_proxy=#{proxy_host}" unless proxy_host.empty?
-
-  hosts.each do |host|
-    on host, "echo 'export http_proxy='#{proxy_host}'' >> /root/.bashrc"
-    on host, "echo 'export https_proxy='#{proxy_host}'' >> /root/.bashrc"
-    on host, "echo 'export no_proxy=\"localhost,127.0.0.1,localaddress,.localdomain.com,#{host.name}\"' >> /root/.bashrc"
-  end
-else
-  gem_proxy = ''
-end
-
-hosts.each do |host|
-  # Install Puppet
-  if host.is_pe?
-    install_pe
-  else
-    puppetversion = ENV['VM_PUPPET_VERSION']
-    on host, "#{gem_proxy} gem install puppet --no-ri --no-rdoc --version '~> #{puppetversion}'"
-    on host, "mkdir -p #{host['distmoduledir']}"
-
-    if fact('osfamily') == 'Suse'
-      install_package host, 'rubygems ruby-devel augeas-devel libxml2-devel'
-      on host, "#{gem_proxy} gem install ruby-augeas --no-ri --no-rdoc"
-    end
-
-  end
-
-  # on debian/ubuntu nodes ensure we get the latest info
-  # Can happen we have stalled data in the images
-  if fact('osfamily') == 'Debian'
-    on host, "apt-get update"
-  end
-
-end
-
-RSpec.configure do |c|
-  # Project root
-  proj_root = File.expand_path(File.join(File.dirname(__FILE__), '..'))
-
-  # Readable test descriptions
-  c.formatter = :documentation
-
-  # Configure all nodes in nodeset
-  c.before :suite do
-    # Install module and dependencies
-
-    hosts.each do |host|
-
-      on host, puppet('module','install','elasticsearch-logstash'), { :acceptable_exit_codes => [0,1] }
-
-      if fact('osfamily') == 'Debian'
-        scp_to(host, "#{files_dir}/puppetlabs-apt-1.4.2.tar.gz", '/tmp/puppetlabs-apt-1.4.2.tar.gz')
-        on host, puppet('module','install','/tmp/puppetlabs-apt-1.4.2.tar.gz'), { :acceptable_exit_codes => [0,1] }
-      end
-      if fact('osfamily') == 'Suse'
-        on host, puppet('module','install','darin-zypprepo'), { :acceptable_exit_codes => [0,1] }
-      end
-
-    end
-  end
-end
diff --git a/bin/logstash b/bin/logstash
index 978bc8e112b..45d65afc384 100755
--- a/bin/logstash
+++ b/bin/logstash
@@ -9,7 +9,7 @@
 # See 'bin/logstash help' for a list of commands.
 #
 # Supported environment variables:
-#   LS_HEAP_SIZE="xxx" size for the -Xmx${LS_HEAP_SIZE} maximum Java heap size option, default is "1g"
+#   LS_JVM_OPTS="xxx" path to file with JVM options
 #   LS_JAVA_OPTS="xxx" to append extra options to the defaults JAVA_OPTS provided by logstash
 #   JAVA_OPTS="xxx" to *completely override* the defauls set of JAVA_OPTS provided by logstash
 #
@@ -46,4 +46,11 @@ fi
 . "$(cd `dirname $SOURCEPATH`/..; pwd)/bin/logstash.lib.sh"
 setup
 
-ruby_exec "${LOGSTASH_HOME}/lib/bootstrap/environment.rb" "logstash/runner.rb" "$@"
+if [ "$1" = "-V" ] || [ "$1" = "--version" ];
+then
+  LOGSTASH_VERSION_FILE="${LOGSTASH_HOME}/logstash-core/lib/logstash/version.rb"
+  LOGSTASH_VERSION="$(sed -ne 's/^LOGSTASH_VERSION = "\([^*]*\)"$/\1/p' $LOGSTASH_VERSION_FILE)"
+  echo "logstash $LOGSTASH_VERSION"
+else
+  ruby_exec "${LOGSTASH_HOME}/lib/bootstrap/environment.rb" "logstash/runner.rb" "$@"
+fi
diff --git a/bin/logstash.lib.sh b/bin/logstash.lib.sh
index c2058e5d5c7..3777514a31a 100755
--- a/bin/logstash.lib.sh
+++ b/bin/logstash.lib.sh
@@ -25,9 +25,14 @@ fi
 
 LOGSTASH_HOME=$(cd `dirname $SOURCEPATH`/..; pwd)
 export LOGSTASH_HOME
+SINCEDB_DIR=${LOGSTASH_HOME}
+export SINCEDB_DIR
 
-# Defaults you can override with environment variables
-LS_HEAP_SIZE="${LS_HEAP_SIZE:=1g}"
+parse_jvm_options() {
+  if [ -f "$1" ]; then
+    echo "$(grep "^-" "$1" | tr '\n' ' ')"
+  fi
+}
 
 setup_java() {
   if [ -z "$JAVACMD" ] ; then
@@ -50,45 +55,34 @@ setup_java() {
 
   if [ "$JAVA_OPTS" ] ; then
     echo "WARNING: Default JAVA_OPTS will be overridden by the JAVA_OPTS defined in the environment. Environment JAVA_OPTS are $JAVA_OPTS"  1>&2
-  else
-    # There are no JAVA_OPTS set from the client, we set a predefined
-    # set of options that think are good in general
-    JAVA_OPTS="-XX:+UseParNewGC"
-    JAVA_OPTS="$JAVA_OPTS -XX:+UseConcMarkSweepGC"
-    JAVA_OPTS="$JAVA_OPTS -Djava.awt.headless=true"
+  fi
 
-    JAVA_OPTS="$JAVA_OPTS -XX:CMSInitiatingOccupancyFraction=75"
-    JAVA_OPTS="$JAVA_OPTS -XX:+UseCMSInitiatingOccupancyOnly"
-    # Causes the JVM to dump its heap on OutOfMemory.
-    JAVA_OPTS="$JAVA_OPTS -XX:+HeapDumpOnOutOfMemoryError"
-    # The path to the heap dump location
-    # This variable needs to be isolated since it may contain spaces
-    HEAP_DUMP_PATH="-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof"
+  # Set a default GC log file for use by jvm.options _before_ it's called.
+  if [ -z "$LS_GC_LOG_FILE" ] ; then
+    LS_GC_LOG_FILE="./logstash-gc.log"
   fi
 
+  # Set the initial JVM options from the jvm.options file.  Look in
+  # /etc/logstash first, and break if that file is found readable there.
+  if [ -z "$LS_JVM_OPTS" ]; then
+      for jvm_options in /etc/logstash/jvm.options \
+                        "$LOGSTASH_HOME"/config/jvm.options;
+                         do
+          if [ -r "$jvm_options" ]; then
+              LS_JVM_OPTS=$jvm_options
+              break
+          fi
+      done
+  fi
+  # use the defaults, first, then override with anything provided
+  LS_JAVA_OPTS="$(parse_jvm_options "$LS_JVM_OPTS") $LS_JAVA_OPTS"
+
   if [ "$LS_JAVA_OPTS" ] ; then
     # The client set the variable LS_JAVA_OPTS, choosing his own
     # set of java opts.
     JAVA_OPTS="$JAVA_OPTS $LS_JAVA_OPTS"
   fi
 
-  if [ "$LS_HEAP_SIZE" ] ; then
-    JAVA_OPTS="$JAVA_OPTS -Xmx${LS_HEAP_SIZE}"
-  fi
-
-  if [ "$LS_USE_GC_LOGGING" ] ; then
-    if [ -z "$LS_GC_LOG_FILE" ] ; then
-      LS_GC_LOG_FILE="./logstash-gc.log"
-    fi
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCDetails"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCTimeStamps"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintClassHistogram"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintTenuringDistribution"
-    JAVA_OPTS="$JAVA_OPTS -XX:+PrintGCApplicationStoppedTime"
-    JAVA_OPTS="$JAVA_OPTS -Xloggc:${LS_GC_LOG_FILE}"
-    echo "Writing garbage collection logs to ${LS_GC_LOG_FILE}"
-  fi
-
   export JAVACMD
   export JAVA_OPTS
 }
@@ -113,7 +107,11 @@ setup_drip() {
   if [ "$USE_RUBY" = "1" ] ; then
     export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify"
   else
-    JAVA_OPTS="$JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+    if [ -z "$JAVA_OPTS" ] ; then
+      LS_JAVA_OPTS="$LS_JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+    else
+      JAVA_OPTS="$JAVA_OPTS -XX:+TieredCompilation -XX:TieredStopAtLevel=1 -noverify"
+    fi
   fi
   export JAVACMD
   export DRIP_INIT_CLASS="org.jruby.main.DripMain"
@@ -140,7 +138,9 @@ setup_ruby() {
 jruby_opts() {
   printf "%s" "--1.9"
   for i in $JAVA_OPTS ; do
-    printf "%s" " -J$i"
+    if [ -z "$i" ]; then
+      printf "%s" " -J$i"
+    fi
   done
 }
 
@@ -179,8 +179,8 @@ ruby_exec() {
     # $VENDORED_JRUBY is non-empty so use the vendored JRuby
 
     if [ "$DEBUG" ] ; then
-      echo "DEBUG: exec ${JRUBY_BIN} $(jruby_opts) "-J$HEAP_DUMP_PATH" $@"
+      echo "DEBUG: exec ${JRUBY_BIN} $(jruby_opts) $@"
     fi
-    exec "${JRUBY_BIN}" $(jruby_opts) "-J$HEAP_DUMP_PATH" "$@"
+    exec "${JRUBY_BIN}" $(jruby_opts) "$@"
   fi
 }
diff --git a/bin/setup.bat b/bin/setup.bat
index 40993179168..f2ac30383f8 100644
--- a/bin/setup.bat
+++ b/bin/setup.bat
@@ -15,6 +15,12 @@ goto finally
 
 :setup_jruby
 REM setup_java()
+if not defined JAVA_HOME IF EXIST %ProgramData%\Oracle\java\javapath\java.exe (
+    for /f "tokens=2 delims=[]" %%a in ('dir %ProgramData%\Oracle\java\javapath\java.exe') do @set JAVA_EXE=%%a
+)
+if defined JAVA_EXE set JAVA_HOME=%JAVA_EXE:\bin\java.exe=%
+if defined JAVA_EXE echo Using JAVA_HOME=%JAVA_HOME% retrieved from %ProgramData%\Oracle\java\javapath\java.exe
+
 if not defined JAVA_HOME goto missing_java_home
 REM ***** JAVA options *****
 
@@ -24,7 +30,9 @@ if "%LS_HEAP_SIZE%" == "" (
 
 IF NOT "%JAVA_OPTS%" == "" (
     ECHO JAVA_OPTS was set to [%JAVA_OPTS%]. Logstash will trust these options, and not set any defaults that it might usually set
-) ELSE (
+    goto opts_defined
+)
+
     SET JAVA_OPTS=%JAVA_OPTS% -Xmx%LS_HEAP_SIZE%
 
     REM Enable aggressive optimizations in the JVM
@@ -52,7 +60,8 @@ IF NOT "%JAVA_OPTS%" == "" (
     REM The path to the heap dump location, note directory must exists and have enough
     REM space for a full heap dump.
     SET JAVA_OPTS=%JAVA_OPTS% -XX:HeapDumpPath="$LS_HOME/heapdump.hprof"
-)
+:opts_defined
+
 
 IF NOT "%LS_JAVA_OPTS%" == "" (
     ECHO LS_JAVA_OPTS was set to [%LS_JAVA_OPTS%]. This will be appended to the JAVA_OPTS [%JAVA_OPTS%]
diff --git a/bin/system-install b/bin/system-install
new file mode 100755
index 00000000000..5790a5a41e8
--- /dev/null
+++ b/bin/system-install
@@ -0,0 +1,41 @@
+#!/bin/bash
+
+unset CDPATH
+. "$(cd `dirname $0`/..; pwd)/bin/logstash.lib.sh"
+setup
+
+if [ -z "$1" ]; then
+  [ -r ${LOGSTASH_HOME}/config/startup.options ] && . ${LOGSTASH_HOME}/config/startup.options
+  [ -r /etc/logstash/startup.options ] && . /etc/logstash/startup.options
+else
+  if [ -r $1 ]; then
+    echo "Using provided startup.options file: ${1}"
+    . $1
+  else
+    echo "$1 is not a file path"
+  fi
+fi
+
+# bin/logstash-plugin is a short lived ruby script thus we can use aggressive "faster starting JRuby options"
+# see https://github.com/jruby/jruby/wiki/Improving-startup-time
+export JRUBY_OPTS="$JRUBY_OPTS -J-XX:+TieredCompilation -J-XX:TieredStopAtLevel=1 -J-noverify -X-C -Xcompile.invokedynamic=false"
+
+tempfile=$(mktemp)
+if [ "x${PRESTART}" == "x" ]; then
+  opts=("--log" "$tempfile" "--overwrite" "--install" "--name" "${SERVICE_NAME}" "--user" "${LS_USER}" "--group" "${LS_GROUP}" "--description" "${SERVICE_DESCRIPTION}" "--nice" "${LS_NICE}" "--limit-open-files" "${LS_OPEN_FILES}")
+else
+  opts=("--log" "$tempfile" "--overwrite" "--install" "--name" "${SERVICE_NAME}" "--user" "${LS_USER}" "--group" "${LS_GROUP}" "--description" "${SERVICE_DESCRIPTION}" "--nice" "${LS_NICE}" "--limit-open-files" "${LS_OPEN_FILES}" "--prestart" "${PRESTART}")
+fi
+
+program="$(cd `dirname $0`/..; pwd)/bin/logstash"
+
+$(ruby_exec "${LOGSTASH_HOME}/lib/systeminstall/pleasewrap.rb" "${opts[@]}" ${program} ${LS_OPTS})
+exit_code=$?
+
+if [ $exit_code -ne 0 ]; then
+  cat $tempfile
+  echo "Unable to install system startup script for Logstash."
+else
+  echo "Successfully created system startup script for Logstash"
+fi
+rm $tempfile
diff --git a/bot/check_pull_changelog.rb b/bot/check_pull_changelog.rb
deleted file mode 100644
index 7e8ac7e1f21..00000000000
--- a/bot/check_pull_changelog.rb
+++ /dev/null
@@ -1,89 +0,0 @@
-require "octokit"
-##
-# This script will validate that any pull request submitted against a github 
-# repository will contains changes to CHANGELOG file.
-#
-# If not the case, an helpful text will be commented on the pull request
-# If ok, a thanksful message will be commented also containing a @mention to 
-# acts as a trigger for review notification by a human.
-## 
-
-
-@bot="" # Put here your bot github username
-@password="" # Put here your bot github password
-
-@repository="logstash/logstash"
-@mention="@jordansissel"
-
-@missing_changelog_message = <<MISSING_CHANGELOG
-Hello, I'm #{@bot}, I'm here to help you accomplish your pull request submission quest
-
-You still need to accomplish these tasks:
-
-* Please add a changelog information
-
-Also note that your pull request name will appears in the details section 
-of the release notes, so please make it clear
-MISSING_CHANGELOG
-
-@ok_changelog_message = <<OK_CHANGELOG
-You successfully completed the pre-requisite quest (aka updating CHANGELOG)
-
-Also note that your pull request name will appears in the details section 
-of the release notes, so please make it clear, if not already done.
-
-#{@mention} Dear master, would you please have a look to this humble request
-OK_CHANGELOG
-
-#Connect to Github
-@client=Octokit::Client.new(:login => @bot, :password => @password)
-
-
-#For each open pull
-Octokit.pull_requests(@repository).each do |pull|
-  #Get botComment
-  botComment = nil
-  @client.issue_comments(@repository, pull.number, {
-    :sort => "created",
-    :direction => "desc"
-  }).each do |comment|
-    if comment.user.login == @bot
-      botComment = comment
-      break
-    end
-  end
-
-  if !botComment.nil? and botComment.body.start_with?("[BOT-OK]")
-    #Pull already validated by bot, nothing to do
-    puts "Pull request #{pull.number}, already ok for bot"
-  else
-    #Firt encounter, or previous [BOT-WARN] status
-    #Check for changelog
-    warnOnMissingChangeLog = true
-    @client.pull_request_files(@repository, pull.number).each do |changedFile|
-      if changedFile.filename  == "CHANGELOG"
-        if changedFile.additions.to_i > 0
-          #Changelog looks good
-          warnOnMissingChangeLog = false
-        else
-          #No additions, means crazy deletion
-          warnOnMissingChangeLog = true
-        end
-      end
-    end
-    if warnOnMissingChangeLog
-      if botComment.nil?
-        puts "Pull request #{pull.number}, adding bot warning"
-        @client.add_comment(@repository, pull.number, "[BOT-WARN] #{@missing_changelog_message}")
-      else
-        puts "Pull request #{pull.number}, already warned, no changes yet"
-      end
-    else
-      if !botComment.nil?
-        @client.delete_comment(@repository,botComment.id)
-      end
-      puts "Pull request #{pull.number}, adding bot ok"
-      @client.add_comment(@repository, pull.number, "[BOT-OK] #{@ok_changelog_message}")
-    end
-  end
-end
diff --git a/ci/ci_acceptance.sh b/ci/ci_acceptance.sh
new file mode 100755
index 00000000000..5665fe5cc4a
--- /dev/null
+++ b/ci/ci_acceptance.sh
@@ -0,0 +1,49 @@
+#!/usr/bin/env bash
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+SELECTED_TEST_SUITE=$1
+
+if [[ $SELECTED_TEST_SUITE == $"redhat" ]]; then
+  echo "Generating the RPM, make sure you start with a clean environment before generating other packages."
+  rake artifact:rpm
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["redhat"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:redhat
+  bundle exec rake qa:vm:halt["redhat"]
+elif [[ $SELECTED_TEST_SUITE == $"debian" ]]; then
+  echo "Generating the DEB, make sure you start with a clean environment before generating other packages."
+  rake artifact:deb
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup["debian"]
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:debian
+  bundle exec rake qa:vm:halt["debian"]
+elif [[ $SELECTED_TEST_SUITE == $"all" ]]; then
+  echo "Building Logstash artifacts"
+  rake artifact:all
+
+  echo "Acceptance: Installing dependencies"
+  cd qa
+  bundle install
+
+  echo "Acceptance: Running the tests"
+  bundle exec rake qa:vm:setup
+  bundle exec rake qa:vm:ssh_config
+  bundle exec rake qa:acceptance:all
+  bundle exec rake qa:vm:halt
+  cd ..
+fi
diff --git a/ci/ci_integration.sh b/ci/ci_integration.sh
index 139408fefc9..ff343cf8438 100755
--- a/ci/ci_integration.sh
+++ b/ci/ci_integration.sh
@@ -1,3 +1,23 @@
 #!/bin/sh
-rake test:install-default
-rake test:integration
+set -e
+
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
+echo "Running integration tests from qa/integration"
+if [[ ! -d "build" ]]; then
+  mkdir build
+fi  
+rm -rf build/*  
+echo "Building logstash tar file in build/"
+rake artifact:tar
+cd build
+echo "Extracting logstash tar file in build/"
+tar xf *.tar.gz
+cd ../qa/integration
+# to install test dependencies
+bundle install
+# runs all tests
+rspec
diff --git a/ci/ci_setup.sh b/ci/ci_setup.sh
index fea695cb2c5..887225c96cb 100755
--- a/ci/ci_setup.sh
+++ b/ci/ci_setup.sh
@@ -1,4 +1,5 @@
 #!/usr/bin/env bash
+set -e
 
 ##
 # Note this setup needs a system ruby to be available, this can not
@@ -13,6 +14,11 @@ rm -rf vendor       # make sure there are no vendorized dependencies
 rm -rf .bundle
 rm -rf spec/reports # no stale spec reports from previous executions
 
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
 # Setup the environment
 rake bootstrap # Bootstrap your logstash instance
 
diff --git a/ci/ci_test.sh b/ci/ci_test.sh
index c0eadda6424..a7f62d151bb 100755
--- a/ci/ci_test.sh
+++ b/ci/ci_test.sh
@@ -1,10 +1,16 @@
 #!/usr/bin/env bash
+set -e
 
 ##
 # Keep in mind to run ci/ci_setup.sh if you need to setup/clean up your environment before
 # running the test suites here.
 ##
 
+# Since we are using the system jruby, we need to make sure our jvm process
+# uses at least 1g of memory, If we don't do this we can get OOM issues when
+# installing gems. See https://github.com/elastic/logstash/issues/5179
+export JRUBY_OPTS="-J-Xmx1g"
+
 SELECTED_TEST_SUITE=$1
 
 if [[ $SELECTED_TEST_SUITE == $"core-fail-fast" ]]; then
diff --git a/ci/travis_integration_install.sh b/ci/travis_integration_install.sh
new file mode 100755
index 00000000000..2b8a63ac419
--- /dev/null
+++ b/ci/travis_integration_install.sh
@@ -0,0 +1,25 @@
+#!/usr/bin/env bash
+set -e
+
+# This file sets up the environment for travis integration tests
+
+
+if [[ "$INTEGRATION" != "true" ]]; then
+    exit
+fi
+  
+echo "Setting up integration tests"
+if [[ ! -d "build" ]]; then
+    mkdir build
+fi  
+rm -rf build/*  
+echo "Building logstash tar file in build/"
+rake artifact:tar
+cd build
+echo "Extracting logstash tar file in build/"
+tar xf *.tar.gz
+cd ../qa/integration
+pwd
+echo $BUNDLE_GEMFILE
+# to install test dependencies
+bundle install --gemfile="Gemfile"
\ No newline at end of file
diff --git a/ci/travis_integration_run.sh b/ci/travis_integration_run.sh
new file mode 100755
index 00000000000..f65c712b1f4
--- /dev/null
+++ b/ci/travis_integration_run.sh
@@ -0,0 +1,10 @@
+#!/usr/bin/env bash
+set -e
+
+if [[ "$INTEGRATION" != "true" ]]; then
+    exit
+fi
+
+echo "Running integration tests from qa/integration directory"
+cd qa/integration
+rspec
diff --git a/config/jvm.options b/config/jvm.options
new file mode 100644
index 00000000000..68abc1ad17e
--- /dev/null
+++ b/config/jvm.options
@@ -0,0 +1,74 @@
+## JVM configuration
+
+# Xms represents the initial size of total heap space
+# Xmx represents the maximum size of total heap space
+
+-Xms256m
+-Xmx1g
+
+################################################################
+## Expert settings
+################################################################
+##
+## All settings below this section are considered
+## expert settings. Don't tamper with them unless
+## you understand what you are doing
+##
+################################################################
+
+## GC configuration
+-XX:+UseParNewGC
+-XX:+UseConcMarkSweepGC
+-XX:CMSInitiatingOccupancyFraction=75
+-XX:+UseCMSInitiatingOccupancyOnly
+
+## optimizations
+
+# disable calls to System#gc
+-XX:+DisableExplicitGC
+
+## Locale
+# Set the locale language
+#-Duser.language=en
+
+# Set the locale country
+#-Duser.country=US
+
+# Set the locale variant, if any
+#-Duser.variant=
+
+## basic
+
+# set the I/O temp directory
+#-Djava.io.tmpdir=$HOME
+
+# set to headless, just in case
+-Djava.awt.headless=true
+
+# ensure UTF-8 encoding by default (e.g. filenames)
+-Dfile.encoding=UTF-8
+
+# use our provided JNA always versus the system one
+#-Djna.nosys=true
+
+## heap dumps
+
+# generate a heap dump when an allocation from the Java heap fails
+# heap dumps are created in the working directory of the JVM
+-XX:+HeapDumpOnOutOfMemoryError
+
+# specify an alternative path for heap dumps
+# ensure the directory exists and has sufficient space
+#-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof
+
+## GC logging
+#-XX:+PrintGCDetails
+#-XX:+PrintGCTimeStamps
+#-XX:+PrintGCDateStamps
+#-XX:+PrintClassHistogram
+#-XX:+PrintTenuringDistribution
+#-XX:+PrintGCApplicationStoppedTime
+
+# log GC status to a file with time stamps
+# ensure the directory exists
+#-Xloggc:${LS_GC_LOG_FILE}
diff --git a/config/log4j2.properties b/config/log4j2.properties
new file mode 100644
index 00000000000..52026f20d3c
--- /dev/null
+++ b/config/log4j2.properties
@@ -0,0 +1,40 @@
+status = error
+name = LogstashPropertiesConfig
+
+appender.console.type = Console
+appender.console.name = plain_console
+appender.console.layout.type = PatternLayout
+appender.console.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %m%n
+
+appender.json_console.type = Console
+appender.json_console.name = json_console
+appender.json_console.layout.type = JSONLayout
+appender.json_console.layout.compact = true
+appender.json_console.layout.eventEol = true
+
+appender.rolling.type = RollingFile
+appender.rolling.name = plain_rolling
+appender.rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.rolling.policies.type = Policies
+appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.rolling.policies.time.interval = 1
+appender.rolling.policies.time.modulate = true
+appender.rolling.layout.type = PatternLayout
+appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %.10000m%n
+
+appender.json_rolling.type = RollingFile
+appender.json_rolling.name = json_rolling
+appender.json_rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.json_rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.json_rolling.policies.type = Policies
+appender.json_rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.json_rolling.policies.time.interval = 1
+appender.json_rolling.policies.time.modulate = true
+appender.json_rolling.layout.type = JSONLayout
+appender.json_rolling.layout.compact = true
+appender.json_rolling.layout.eventEol = true
+
+rootLogger.level = ${sys:ls.log.level}
+rootLogger.appenderRef.console.ref = ${sys:ls.log.format}_console
+rootLogger.appenderRef.rolling.ref = ${sys:ls.log.format}_rolling
diff --git a/config/logstash.yml b/config/logstash.yml
new file mode 100644
index 00000000000..bce9f417e36
--- /dev/null
+++ b/config/logstash.yml
@@ -0,0 +1,115 @@
+# Settings file in YAML
+#
+# Settings can be specified either in hierarchical form, e.g.:
+#
+#   pipeline:
+#     batch:
+#       size: 125
+#       delay: 5
+#
+# Or as flat keys:
+#
+#   pipeline.batch.size: 125
+#   pipeline.batch.delay: 5
+#
+# ------------  Node identity ------------
+#
+# Use a descriptive name for the node:
+#
+# node.name: test
+#
+# If omitted the node name will default to the machine's host name
+#
+# ------------ Data path ------------------
+#
+# Which directory should be used by logstash and its plugins
+# for any persistent needs. Defaults to LOGSTASH_HOME/data
+#
+# path.data:
+#
+# ------------ Pipeline Settings --------------
+#
+# Set the number of workers that will, in parallel, execute the filters+outputs
+# stage of the pipeline.
+#
+# This defaults to the number of the host's CPU cores.
+#
+# pipeline.workers: 2
+#
+# How many workers should be used per output plugin instance
+#
+# pipeline.output.workers: 1
+#
+# How many events to retrieve from inputs before sending to filters+workers
+#
+# pipeline.batch.size: 125
+#
+# How long to wait before dispatching an undersized batch to filters+workers
+# Value is in milliseconds.
+#
+# pipeline.batch.delay: 5
+#
+# Force Logstash to exit during shutdown even if there are still inflight
+# events in memory. By default, logstash will refuse to quit until all
+# received events have been pushed to the outputs.
+#
+# WARNING: enabling this can lead to data loss during shutdown
+#
+# pipeline.unsafe_shutdown: false
+#
+# ------------ Pipeline Configuration Settings --------------
+#
+# Where to fetch the pipeline configuration for the main pipeline
+#
+# path.config:
+#
+# Pipeline configuration string for the main pipeline
+#
+# config.string:
+#
+# At startup, test if the configuration is valid and exit (dry run)
+#
+# config.test_and_exit: false
+#
+# Periodically check if the configuration has changed and reload the pipeline
+# This can also be triggered manually through the SIGHUP signal
+#
+# config.reload.automatic: false
+#
+# How often to check if the pipeline configuration has changed (in seconds)
+#
+# config.reload.interval: 3
+#
+# Show fully compiled configuration as debug log message
+# NOTE: --log.level must be 'debug'
+#
+# config.debug: false
+#
+# ------------ Metrics Settings --------------
+#
+# Bind address for the metrics REST endpoint
+#
+# http.host: "127.0.0.1"
+#
+# Bind port for the metrics REST endpoint, this option also accept a range
+# (9600-9700) and logstash will pick up the first available ports.
+#
+# http.port: 9600-9700
+#
+# ------------ Debugging Settings --------------
+#
+# Options for log.level:
+#   * fatal
+#   * error
+#   * warn
+#   * info (default)
+#   * debug
+#   * trace
+#
+# log.level: info
+# path.logs:
+#
+# ------------ Other Settings --------------
+#
+# Where to find custom plugins
+# path.plugins: []
diff --git a/config/startup.options b/config/startup.options
new file mode 100644
index 00000000000..9d35f798dcd
--- /dev/null
+++ b/config/startup.options
@@ -0,0 +1,52 @@
+################################################################################
+# These settings are ONLY used by $LS_HOME/bin/system-install to create a custom
+# startup script for Logstash.  It should automagically use the init system
+# (systemd, upstart, sysv, etc.) that your Linux distribution uses.
+#
+# After changing anything here, you need to re-run $LS_HOME/bin/system-install
+# as root to push the changes to the init script.
+################################################################################
+
+# Override Java location
+JAVACMD=/usr/bin/java
+
+# Set a home directory
+LS_HOME=/usr/share/logstash
+
+# logstash settings directory, the path which contains logstash.yml
+LS_SETTINGS_DIR="${LS_HOME}/config"
+
+# Arguments to pass to logstash
+LS_OPTS="--path.settings ${LS_SETTINGS_DIR}"
+
+# Arguments to pass to java
+LS_JAVA_OPTS=""
+
+# pidfiles aren't used the same way for upstart and systemd; this is for sysv users.
+LS_PIDFILE=/var/run/logstash.pid
+
+# user and group id to be invoked as
+LS_USER=logstash
+LS_GROUP=logstash
+
+# Enable GC logging by uncommenting the appropriate lines in the GC logging
+# section in jvm.options
+LS_GC_LOG_FILE=/var/log/logstash/gc.log
+
+# Open file limit
+LS_OPEN_FILES=16384
+
+# Nice level
+LS_NICE=19
+
+# Change these to have the init script named and described differently
+# This is useful when running multiple instances of Logstash on the same
+# physical box or vm
+SERVICE_NAME="logstash"
+SERVICE_DESCRIPTION="logstash"
+
+# If you need to run a command or script before launching Logstash, put it
+# between the lines beginning with `read` and `EOM`, and uncomment those lines.
+###
+## read -r -d '' PRESTART << EOM
+## EOM
diff --git a/patterns/.gitkeep b/data/.gitkeep
similarity index 100%
rename from patterns/.gitkeep
rename to data/.gitkeep
diff --git a/docs/asciidoc_index.rb b/docs/asciidoc_index.rb
deleted file mode 100644
index be1b94403f8..00000000000
--- a/docs/asciidoc_index.rb
+++ /dev/null
@@ -1,35 +0,0 @@
-#!/usr/bin/env ruby
-
-require "erb"
-
-if ARGV.size != 2
-  $stderr.puts "No path given to search for plugin docs"
-  $stderr.puts "Usage: #{$0} plugin_doc_dir type"
-  exit 1
-end
-
-
-def plugins(glob)
-  plugins=Hash.new []
-  files = Dir.glob(glob)
-  files.collect { |f| File.basename(f).gsub(".asciidoc", "") }.each {|plugin|
-    first_letter = plugin[0,1]
-    plugins[first_letter] += [plugin]
-  }
-  return Hash[plugins.sort]
-end # def plugins
-
-basedir = ARGV[0]
-type = ARGV[1]
-
-docs = plugins(File.join(basedir, "#{type}/*.asciidoc"))
-template_path = File.join(File.dirname(__FILE__), "index-#{type}.asciidoc.erb")
-template = File.new(template_path).read
-erb = ERB.new(template, nil, "-")
-
-path = "#{basedir}/#{type}.asciidoc"
-
-File.open(path, "w") do |out|
-  html = erb.result(binding)
-  out.puts(html)
-end
diff --git a/docs/index.asciidoc b/docs/index.asciidoc
new file mode 100644
index 00000000000..8e3100fafaa
--- /dev/null
+++ b/docs/index.asciidoc
@@ -0,0 +1,150 @@
+[[logstash-reference]]
+= Logstash Reference
+
+:branch:                5.x
+:major-version:         5.x
+:logstash_version:      5.1.0
+:elasticsearch_version: 5.1.0
+:docker-image:          docker.elastic.co/logstash/logstash:{logstash_version}
+
+//////////
+release-state can be: released | prerelease | unreleased
+//////////
+:release-state:  unreleased
+
+:jdk:                   1.8.0
+:guide:                 https://www.elastic.co/guide/en/elasticsearch/guide/current/
+:ref:                   https://www.elastic.co/guide/en/elasticsearch/reference/5.x/
+:xpack:                 https://www.elastic.co/guide/en/x-pack/current/
+:logstash:              https://www.elastic.co/guide/en/logstash/5.x/
+:filebeat:              https://www.elastic.co/guide/en/beats/filebeat/5.x/
+:lsissue:               https://github.com/elastic/logstash/issues/
+:security:              X-Pack Security
+:stack:                 https://www.elastic.co/guide/en/elastic-stack/current/
+
+[[introduction]]
+== Logstash Introduction
+
+Logstash is an open source data collection engine with real-time pipelining capabilities. Logstash can dynamically
+unify data from disparate sources and normalize the data into destinations of your choice. Cleanse and democratize all
+your data for diverse advanced downstream analytics and visualization use cases.
+
+While Logstash originally drove innovation in log collection, its capabilities extend well beyond that use case. Any
+type of event can be enriched and transformed with a broad array of input, filter, and output plugins, with many
+native codecs further simplifying the ingestion process. Logstash accelerates your insights by harnessing a greater
+volume and variety of data.
+
+// The pass blocks here point to the correct repository for the edit links in the guide.
+
+// Introduction
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/introduction.asciidoc ?>]
+include::static/introduction.asciidoc[]
+
+// Glossary and core concepts go here
+
+// Getting Started with Logstash
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/getting-started-with-logstash.asciidoc ?>]
+include::static/getting-started-with-logstash.asciidoc[]
+
+// Advanced LS Pipelines
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/advanced-pipeline.asciidoc ?>]
+include::static/advanced-pipeline.asciidoc[]
+
+// Processing Pipeline
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/life-of-an-event.asciidoc ?>]
+include::static/life-of-an-event.asciidoc[]
+
+// Lostash setup
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/setting-up-logstash.asciidoc ?>]
+include::static/setting-up-logstash.asciidoc[]
+
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/docker.asciidoc ?>]
+include::static/docker.asciidoc[]
+
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/settings-file.asciidoc ?>]
+include::static/settings-file.asciidoc[]
+
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/command-line-flags.asciidoc ?>]
+include::static/command-line-flags.asciidoc[]
+
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/logging.asciidoc ?>]
+include::static/logging.asciidoc[]
+
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/stalled-shutdown.asciidoc ?>]
+include::static/stalled-shutdown.asciidoc[]
+
+// Breaking Changes
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/breaking-changes.asciidoc ?>]
+include::static/breaking-changes.asciidoc[]
+
+// Upgrading Logstash
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/upgrading.asciidoc ?>]
+include::static/upgrading.asciidoc[]
+
+// Configuring Logstash
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/configuration.asciidoc ?>]
+include::static/configuration.asciidoc[]
+
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/reloading-config.asciidoc ?>]
+include::static/reloading-config.asciidoc[]
+
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/managing-multiline-events.asciidoc ?>]
+include::static/managing-multiline-events.asciidoc[]
+
+// Deploying & Scaling
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/deploying.asciidoc ?>]
+include::static/deploying.asciidoc[]
+
+// Troubleshooting performance
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/performance-checklist.asciidoc ?>]
+include::static/performance-checklist.asciidoc[]
+
+// Monitoring APIs
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/monitoring-apis.asciidoc ?>]
+include::static/monitoring-apis.asciidoc[]
+
+// Working with Plugins
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/plugin-manager.asciidoc ?>]
+include::static/plugin-manager.asciidoc[]
+
+// These files do their own pass blocks
+pass::[<?edit_url?>]
+include::plugins/inputs.asciidoc[]
+include::plugins/outputs.asciidoc[]
+include::plugins/filters.asciidoc[]
+include::plugins/codecs.asciidoc[]
+
+// Contributing to Logstash
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/contributing-to-logstash.asciidoc ?>]
+include::static/contributing-to-logstash.asciidoc[]
+
+// This is in the pluginbody.asciidoc itself
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/include/pluginbody.asciidoc ?>]
+include::static/input.asciidoc[]
+include::static/codec.asciidoc[]
+include::static/filter.asciidoc[]
+include::static/output.asciidoc[]
+
+// Contributing a Patch to a Logstash Plugin
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/contributing-patch.asciidoc ?>]
+include::static/contributing-patch.asciidoc[]
+
+// Logstash Community Maintainer Guide
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/maintainer-guide.asciidoc ?>]
+include::static/maintainer-guide.asciidoc[]
+
+// A space is necessary here ^^^
+pass::[<?edit_url?>]
+
+// Submitting a Plugin
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/submitting-a-plugin.asciidoc ?>]
+include::static/submitting-a-plugin.asciidoc[]
+
+
+// This is in the pluginbody.asciidoc itself
+// pass::[<?edit_url?>]
+
+// Glossary of Terms
+pass::[<?edit_url https://github.com/elastic/logstash/edit/master/docs/static/glossary.asciidoc ?>]
+include::static/glossary.asciidoc[]
+
diff --git a/docs/plugin-doc.html.erb b/docs/plugin-doc.html.erb
deleted file mode 100644
index c236314e0af..00000000000
--- a/docs/plugin-doc.html.erb
+++ /dev/null
@@ -1,80 +0,0 @@
----
-title: logstash docs for <%= section %>s/<%= name %>
-layout: content_right
----
-<h2><%= name %></h2>
-<h3>Milestone: <a href="../plugin-milestones"><%= @milestone %></a></h3>
-<% if is_contrib_plugin -%>
-<% end -%>
-
-<%= description %>
-
-<h3> Synopsis </h3>
-
-A sample configuration file is shown here:
-
-<pre><code><% if section == "codec" -%>
-# with an input plugin:
-# you can also use this codec with an output.
-input { 
-<% if name == "json_lines" -%>
-  udp {
-    port =&gt; 1234
-<% else -%>
-  file {
-<% end -%>
-    codec =&gt; <%= synopsis -%>
-  }
-}
-<% else -%>
-<%= section %> {
-  <%= synopsis -%>
-}
-<% end -%></code></pre>
-
-<h3> Details </h3>
-
-<% sorted_attributes.each do |name, config| -%>
-<%
-     if name.is_a?(Regexp)
-       name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
-       is_regexp = true
-     else
-       is_regexp = false
-     end
--%>
-<h4> 
-  <a name="<%= name %>">
-    <%= name %><%= " (required setting)" if config[:required] %>
-    <%= " <strong>DEPRECATED</strong>" if config[:deprecated] %>
-</a>
-</h4>
-
-<ul>
-<% if config[:deprecated] -%>
-  <li> DEPRECATED WARNING: This configuration item is deprecated and may not be included in later versions.</li>
-<% end -%>
-<% if is_regexp -%>
-  <li> The configuration attribute name here is anything that matches the above regular expression. </li>
-<% end -%>
-<% if config[:validate].is_a?(Symbol) -%>
-  <li> Value type is <a href="../configuration#<%= config[:validate] %>"><%= config[:validate] %></a> </li>
-<% elsif config[:validate].nil? -%>
-  <li> Value type is <a href="../configuration#string">string</a> </li>
-<% elsif config[:validate].is_a?(Array) -%>
-  <li> Value can be any of: <%= config[:validate].map(&:inspect).join(", ") %> </li>
-<% end -%>
-<% if config.include?(:default) -%>
-  <li> Default value is <%= config[:default].inspect %> </li>
-<% else -%>
-  <li> There is no default value for this setting. </li>
-<% end -%>
-</ul>
-
-<%= config[:description] %>
-
-<% end -%>
-
-<hr>
-
-This is documentation from <a href="https://github.com/logstash/logstash/blob/v<%= LOGSTASH_VERSION %>/<%= file %>"><%= file %></a>
diff --git a/docs/plugin-synopsis.html.erb b/docs/plugin-synopsis.html.erb
deleted file mode 100644
index 92465227fa8..00000000000
--- a/docs/plugin-synopsis.html.erb
+++ /dev/null
@@ -1,24 +0,0 @@
-<%= name %> {
-<% sorted_attributes.each do |name, config|
-   next if config[:deprecated]
-   if config[:validate].is_a?(Array) 
-     annotation = "string, one of #{config[:validate].inspect}"
-   elsif config[:validate] == :path
-     annotation = "a valid filesystem path"
-   else 
-     annotation = "#{config[:validate]}"
-   end
-
-   if name.is_a?(Regexp)
-     name = "/" + name.to_s.gsub(/^\(\?-mix:/, "").gsub(/\)$/, "") + "/"
-   end
-   if config[:required]
-     annotation += " (required)"
-   else
-     annotation += " (optional)"
-   end
-   annotation += ", default: #{config[:default].inspect}" if config.include?(:default)
--%>
-<%= "  " if section == "codec" %>    <a href="#<%= name %>"><%= name %></a> => ... # <%= annotation %>
-<% end -%>
-<%= "  " if section == "codec" %>  }
diff --git a/docs/static/advanced-pipeline.asciidoc b/docs/static/advanced-pipeline.asciidoc
index 7e541ce5eea..5f43c702fff 100644
--- a/docs/static/advanced-pipeline.asciidoc
+++ b/docs/static/advanced-pipeline.asciidoc
@@ -1,22 +1,79 @@
 [[advanced-pipeline]]
-=== Setting Up an Advanced Logstash Pipeline
+=== Parsing Logs with Logstash
 
-A Logstash pipeline in most use cases has one or more input, filter, and output plugins. The scenarios in this section
-build Logstash configuration files to specify these plugins and discuss what each plugin is doing.
+In <<first-event>>, you created a basic Logstash pipeline to test your Logstash setup. In the real world, a Logstash
+pipeline is a bit more complex: it typically has one or more input, filter, and output plugins.  
 
-The Logstash configuration file defines your _Logstash pipeline_. When you start a Logstash instance, use the
-`-f <path/to/file>` option to specify the configuration file that defines that instance’s pipeline.
+In this section, you create a Logstash pipeline that uses Filebeat to take Apache web logs as input, parses those
+logs to create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Rather than
+defining the pipeline configuration at the command line, you'll define the pipeline in a config file. 
 
-A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
-plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
-the data to a destination.
+To get started, go https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here] to
+download the sample data set used in this example. Unpack the file.
 
-image::static/images/basic_logstash_pipeline.png[]
 
-The following text represents the skeleton of a configuration pipeline:
+[[configuring-filebeat]]
+==== Configuring Filebeat to Send Log Lines to Logstash
+
+Before you create the Logstash pipeline, you'll configure Filebeat to send log lines to Logstash.  
+The https://github.com/elastic/beats/tree/master/filebeat[Filebeat] client is a lightweight, resource-friendly tool
+that collects logs from files on the server and forwards these logs to your Logstash instance for processing.
+Filebeat is designed for reliability and low latency. Filebeat has a light resource footprint on the host machine,
+and the {logstash}plugins-inputs-beats.html[`Beats input`] plugin minimizes the resource demands on the Logstash
+instance.
+
+NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your
+Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
+same machine.
+
+The default Logstash installation includes the {logstash}plugins-inputs-beats.html[`Beats input`] plugin. The Beats
+input plugin enables Logstash to receive events from the Elastic Beats framework, which means that any Beat written
+to work with the Beats framework, such as Packetbeat and Metricbeat, can also send event data to Logstash. 
+
+To install Filebeat on your data source machine, download the appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page]. You can also refer to
+{filebeat}filebeat-getting-started.html[Getting Started with Filebeat] in the Beats documentation for additional
+installation instructions.
+
+After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
+directory, and replace the contents with the following lines. Make sure `paths` points to the example Apache log file,
+`logstash-tutorial.log`, that you downloaded earlier: 
+
+[source,yaml]
+--------------------------------------------------------------------------------
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /path/to/file/logstash-tutorial.log <1>
+output.logstash:
+  hosts: ["localhost:5043"]
+--------------------------------------------------------------------------------
+
+<1> Absolute path to the file or files that Filebeat processes.
+
+Save your changes. 
+
+To keep the configuration simple, you won't specify TLS/SSL settings as you would in a real world
+scenario.
+
+At the data source machine, run Filebeat with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
+sudo ./filebeat -e -c filebeat.yml -d "publish"
+--------------------------------------------------------------------------------
+
+Filebeat will attempt to connect on port 5403. Until Logstash starts with an active Beats plugin, there
+won’t be any answer on that port, so any messages you see regarding failure to connect on that port are normal for now.
+
+==== Configuring Logstash for Filebeat Input
+
+Next, you create a Logstash configuration pipeline that uses the Beats input plugin to receive
+events from Beats.
+
+The following text represents the skeleton of a configuration pipeline:
+
+[source,json]
+--------------------------------------------------------------------------------
 # The # character at the beginning of a line indicates a comment. Use
 # comments to describe your configuration.
 input {
@@ -30,58 +87,110 @@ output {
 }
 --------------------------------------------------------------------------------
 
-This skeleton is non-functional, because the input and output sections don’t have any valid options defined. The
-examples in this tutorial build configuration files to address specific use cases.
-
-Paste the skeleton into a file named `first-pipeline.conf` in your home Logstash directory.
+This skeleton is non-functional, because the input and output sections don’t have any valid options defined. 
 
-[[parsing-into-es]]
-==== Parsing Apache Logs into Elasticsearch
+To get started, copy and paste the skeleton configuration pipeline into a file named `first-pipeline.conf` in your home
+Logstash directory. 
 
-This example creates a Logstash pipeline that takes Apache web logs as input, parses those logs to create specific,
-named fields from the logs, and writes the parsed data to an Elasticsearch cluster.
+Next, configure your Logstash instance to use the Beats input plugin by adding the following lines to the `input` section
+of the `first-pipeline.conf` file:
 
-You can download the sample data set used in this example
-https://download.elastic.co/demos/logstash/gettingstarted/logstash-tutorial.log.gz[here]. Unpack this file.
+[source,json]
+--------------------------------------------------------------------------------
+    beats {
+        port => "5043"
+    }
+--------------------------------------------------------------------------------
 
-[float]
-[[configuring-file-input]]
-==== Configuring Logstash for File Input
+You'll configure Logstash to write to Elasticsearch later. For now, you can add the following line
+to the `output` section so that the output is printed to stdout when you run Logstash: 
 
-To start your Logstash pipeline, configure the Logstash instance to read from a file using the
-{logstash}plugins-inputs-file.html[file] input plugin.
+[source,json]
+--------------------------------------------------------------------------------
+    stdout { codec => rubydebug }
+--------------------------------------------------------------------------------
 
-Edit the `first-pipeline.conf` file to add the following text:
+When you're done, the contents of `first-pipeline.conf` should look like this:
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
-    file {
-        path => "/path/to/logstash-tutorial.log"
-        start_position => beginning <1>
-        ignore_older => 0 <2>
+    beats {
+        port => "5043"
     }
 }
+# The filter part of this file is commented out to indicate that it is
+# optional.
+# filter {
+#
+# }
+output {
+    stdout { codec => rubydebug }
+}
 --------------------------------------------------------------------------------
 
-<1> The default behavior of the file input plugin is to monitor a file for new information, in a manner similar to the
-UNIX `tail -f` command. To change this default behavior and process the entire file, we need to specify the position
-where Logstash starts processing the file.
-<2> The default behavior of the file input plugin is to ignore files whose last modification is greater than 86400s. To change this default behavior and process the tutorial file (which date can be much older than a day), we need to specify to not ignore old files.
+To verify your configuration, run the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f first-pipeline.conf --config.test_and_exit
+--------------------------------------------------------------------------------
+
+The `--config.test_and_exit` option parses your configuration file and reports any errors.
+
+If the configuration file passes the configuration test, start Logstash with the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+bin/logstash -f first-pipeline.conf --config.reload.automatic
+--------------------------------------------------------------------------------
+
+The `--config.reload.automatic` option enables automatic config reloading so that you don't have to stop and restart Logstash
+every time you modify the configuration file.
+
+If your pipeline is working correctly, you should see a series of events like the following written to the console:
+
+[source,json]
+--------------------------------------------------------------------------------
+{
+    "@timestamp" => 2016-10-11T20:54:06.733Z,
+        "offset" => 325,
+      "@version" => "1",
+          "beat" => {
+        "hostname" => "My-MacBook-Pro.local",
+            "name" => "My-MacBook-Pro.local"
+    },
+    "input_type" => "log",
+          "host" => "My-MacBook-Pro.local",
+        "source" => "/path/to/file/logstash-tutorial.log",
+       "message" => "83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "type" => "log",
+          "tags" => [
+        [0] "beats_input_codec_plain_applied"
+    ]
+}
+...
+
+--------------------------------------------------------------------------------
 
-Replace `/path/to/` with the actual path to the location of `logstash-tutorial.log` in your file system.
 
 [float]
 [[configuring-grok-filter]]
-===== Parsing Web Logs with the Grok Filter Plugin
+==== Parsing Web Logs with the Grok Filter Plugin
+
+Now you have a working pipeline that reads log lines from Filebeat. However you'll notice that the format of the log messages
+is not ideal. You want to parse the log messages to create specific, named fields from the logs.
+To do this, you'll use the `grok` filter plugin.
 
 The {logstash}plugins-filters-grok.html[`grok`] filter plugin is one of several plugins that are available by default in
 Logstash. For details on how to manage Logstash plugins, see the <<working-with-plugins,reference documentation>> for
 the plugin manager.
 
-Because the `grok` filter plugin looks for patterns in the incoming log data, configuration requires you to make
-decisions about how to identify the patterns that are of interest to your use case. A representative line from the web
-server log sample looks like this:
+The `grok` filter plugin enables you to parse the unstructured log data into something structured and queryable.
+
+Because the `grok` filter plugin looks for patterns in the incoming log data, configuring the plugin requires you to
+make decisions about how to identify the patterns that are of interest to your use case. A representative line from the
+web server log sample looks like this:
 
 [source,shell]
 --------------------------------------------------------------------------------
@@ -90,8 +199,7 @@ HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-
 Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
 --------------------------------------------------------------------------------
 
-The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. In this tutorial, use
-the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
+The IP address at the beginning of the line is easy to identify, as is the timestamp in brackets. To parse the data, you can use the `%{COMBINEDAPACHELOG}` grok pattern, which structures lines from the Apache log using the following schema:
 
 [horizontal]
 *Information*:: *Field Name*
@@ -107,7 +215,7 @@ Bytes served:: `bytes`
 Referrer URL:: `referrer`
 User agent:: `agent`
 
-Edit the `first-pipeline.conf` file to add the following text:
+Edit the `first-pipeline.conf` file and replace the entire `filter` section with the following text:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -118,47 +226,85 @@ filter {
 }
 --------------------------------------------------------------------------------
 
-After processing, the sample line has the following JSON representation:
+When you're done, the contents of `first-pipeline.conf` should look like this:
 
 [source,json]
 --------------------------------------------------------------------------------
-{
-"clientip" : "83.149.9.216",
-"ident" : ,
-"auth" : ,
-"timestamp" : "04/Jan/2015:05:13:42 +0000",
-"verb" : "GET",
-"request" : "/presentations/logstash-monitorama-2013/images/kibana-search.png",
-"httpversion" : "HTTP/1.1",
-"response" : "200",
-"bytes" : "203023",
-"referrer" : "http://semicomplete.com/presentations/logstash-monitorama-2013/",
-"agent" : "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
+input {
+    beats {
+        port => "5043"
+    }
+}
+filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+}
+output {
+    stdout { codec => rubydebug }
 }
 --------------------------------------------------------------------------------
 
-[float]
-[[indexing-parsed-data-into-elasticsearch]]
-===== Indexing Parsed Data into Elasticsearch
+Save your changes. Because you've enabled automatic config reloading, you don't have to restart Logstash to 
+pick up your changes. However, you do need to force Filebeat to read the log file from scratch. To do this,
+go to the terminal window where Filebeat is running and press Ctrl+C to shut down Filebeat. Then delete the
+Filebeat registry file. For example, run:
 
-Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
-Elasticsearch cluster. Edit the `first-pipeline.conf` file to add the following text after the `input` section:
+[source,shell]
+--------------------------------------------------------------------------------
+sudo rm data/registry
+--------------------------------------------------------------------------------
+
+Since Filebeat stores the state of each file it harvests in the registry, deleting the registry file forces
+Filebeat to read all the files it's harvesting from scratch.
+
+Next, restart Filebeat with the following command:
+
+[source,shell]
+--------------------------------------------------------------------------------
+sudo ./filebeat -e -c filebeat.yml -d "publish"
+--------------------------------------------------------------------------------
+
+After processing the log file with the grok pattern, the events will have the following JSON representation:
 
 [source,json]
 --------------------------------------------------------------------------------
-output {
-    elasticsearch {
-    }
+{
+        "request" => "/presentations/logstash-monitorama-2013/images/kibana-search.png",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+         "offset" => 325,
+           "auth" => "-",
+          "ident" => "-",
+     "input_type" => "log",
+           "verb" => "GET",
+         "source" => "/path/to/file/logstash-tutorial.log",
+        "message" => "83.149.9.216 - - [04/Jan/2015:05:13:42 +0000] \"GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1\" 200 203023 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+           "type" => "log",
+           "tags" => [
+        [0] "beats_input_codec_plain_applied"
+    ],
+       "referrer" => "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
+     "@timestamp" => 2016-10-11T21:04:36.167Z,
+       "response" => "200",
+          "bytes" => "203023",
+       "clientip" => "83.149.9.216",
+       "@version" => "1",
+           "beat" => {
+        "hostname" => "My-MacBook-Pro.local",
+            "name" => "My-MacBook-Pro.local"
+    },
+           "host" => "My-MacBook-Pro.local",
+    "httpversion" => "1.1",
+      "timestamp" => "04/Jan/2015:05:13:42 +0000"
 }
 --------------------------------------------------------------------------------
 
-With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes Logstash
-and Elasticsearch to be running on the same instance. You can specify a remote Elasticsearch instance using `hosts`
-configuration like `hosts => "es-machine:9092"`.
+Notice that the event includes the original message, but the log message is also broken down into specific fields.
+
 
 [float]
 [[configuring-geoip-plugin]]
-===== Enhancing Your Data with the Geoip Filter Plugin
+==== Enhancing Your Data with the Geoip Filter Plugin
 
 In addition to parsing log data for better searches, filter plugins can derive supplementary information from existing
 data. As an example, the {logstash}plugins-filters-geoip.html[`geoip`] plugin looks up IP addresses, derives geographic
@@ -169,32 +315,26 @@ of the `first-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
-geoip {
-    source => "clientip"
-}
+    geoip {
+        source => "clientip"
+    }
 --------------------------------------------------------------------------------
 
-The `geoip` plugin configuration requires data that is already defined as separate fields. Make sure that the `geoip`
-section is after the `grok` section of the configuration file.
+The `geoip` plugin configuration requires you to specify the name of the source field that contains the IP address to look up. In this example, the `clientip` field contains the IP address.
 
-Specify the name of the field that contains the IP address to look up. In this tutorial, the field name is `clientip`.
+Since filters are evaluated in sequence, make sure that the `geoip` section is after the `grok` section of 
+the configuration file and that both the `grok` and `geoip` sections are nested within the `filter` section. 
 
-[float]
-[[testing-initial-pipeline]]
-===== Testing Your Initial Pipeline
-
-At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
-like this:
+When you're done, the contents of `first-pipeline.conf` should look like this:
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
-    file {
-        path => "/Users/palecur/logstash-1.5.2/logstash-tutorial-dataset"
-        start_position => beginning
+    beats {
+        port => "5043"
     }
 }
-filter {
+ filter {
     grok {
         match => { "message" => "%{COMBINEDAPACHELOG}"}
     }
@@ -203,236 +343,372 @@ filter {
     }
 }
 output {
-    elasticsearch {}
-    stdout {}
+    stdout { codec => rubydebug }
 }
 --------------------------------------------------------------------------------
 
-To verify your configuration, run the following command:
+Save your changes. To force Filebeat to read the log file from scratch, as you did earlier, shut down Filebeat (press Ctrl+C), 
+delete the registry file, and then restart Filebeat with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf --configtest
+sudo ./filebeat -e -c filebeat.yml -d "publish"
 --------------------------------------------------------------------------------
 
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
+Notice that the event now contains geographic location information:
 
-[source,shell]
+[source,json]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf
+{
+        "request" => "/presentations/logstash-monitorama-2013/images/kibana-search.png",
+          "agent" => "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "geoip" => {
+              "timezone" => "Europe/Moscow",
+                    "ip" => "83.149.9.216",
+              "latitude" => 55.7522,
+        "continent_code" => "EU",
+             "city_name" => "Moscow",
+         "country_code2" => "RU",
+          "country_name" => "Russia",
+              "dma_code" => nil,
+         "country_code3" => "RU",
+           "region_name" => "Moscow",
+              "location" => [
+            [0] 37.6156,
+            [1] 55.7522
+        ],
+           "postal_code" => "101194",
+             "longitude" => 37.6156,
+           "region_code" => "MOW"
+    },
+    ...
 --------------------------------------------------------------------------------
 
-Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin:
 
-[source,shell]
+[float]
+[[indexing-parsed-data-into-elasticsearch]]
+==== Indexing Your Data into Elasticsearch
+
+Now that the web logs are broken down into specific fields, the Logstash pipeline can index the data into an
+Elasticsearch cluster. Edit the `first-pipeline.conf` file and replace the entire `output` section with the following
+text:
+
+[source,json]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=response=200'
+output {
+    elasticsearch {
+        hosts => [ "localhost:9200" ]
+    }
+}
 --------------------------------------------------------------------------------
 
-Replace $DATE with the current date, in YYYY.MM.DD format.
+With this configuration, Logstash uses http protocol to connect to Elasticsearch. The above example assumes that
+Logstash and Elasticsearch are running on the same instance. You can specify a remote Elasticsearch instance by using
+the `hosts` configuration to specify something like `hosts => [ "es-machine:9092" ]`.
 
-Since our sample has just one 200 HTTP response, we get one hit back:
+At this point, your `first-pipeline.conf` file has input, filter, and output sections properly configured, and looks
+something like this:
 
 [source,json]
 --------------------------------------------------------------------------------
-{"took":2,
-"timed_out":false,
-"_shards":{"total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.5351382,
-  "hits":[{"_index":"logstash-2015.07.30",
-    "_type":"logs",
-    "_id":"AU7gqOky1um3U6ZomFaF",
-    "_score":1.5351382,
-    "_source":{"message":"83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
-      "@version":"1",
-      "@timestamp":"2015-07-30T20:30:41.265Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"83.149.9.216",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:13:45 +0000",
-      "verb":"GET",
-      "request":"/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"52878",
-      "referrer":"\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
-      "agent":"\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\""
-      }
-    }]
-  }
+input {
+    beats {
+        port => "5043"
+    }
+}
+ filter {
+    grok {
+        match => { "message" => "%{COMBINEDAPACHELOG}"}
+    }
+    geoip {
+        source => "clientip"
+    }
+}
+output {
+    elasticsearch {
+        hosts => [ "localhost:9200" ]
+    }
 }
 --------------------------------------------------------------------------------
 
-Try another search for the geographic information derived from the IP address:
+Save your changes. To force Filebeat to read the log file from scratch, as you did earlier, shut down Filebeat (press Ctrl+C), 
+delete the registry file, and then restart Filebeat with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-$DATE/_search?q=geoip.city_name=Buffalo'
+sudo ./filebeat -e -c filebeat.yml -d "publish"
 --------------------------------------------------------------------------------
 
-Replace $DATE with the current date, in YYYY.MM.DD format.
+[float]
+[[testing-initial-pipeline]]
+===== Testing Your Pipeline
 
-Only one of the log entries comes from Buffalo, so the query produces a single response:
+Now that the Logstash pipeline is configured to index the data into an
+Elasticsearch cluster, you can query Elasticsearch.
+
+Try a test query to Elasticsearch based on the fields created by the `grok` filter plugin. 
+Replace $DATE with the current date, in YYYY.MM.DD format:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=response=200'
+--------------------------------------------------------------------------------
+
+NOTE: The date used in the index name is based on UTC, not the timezone where Logstash is running.
+If the query returns `index_not_found_exception`, make sure that `logstash-$DATE` reflects the actual
+name of the index. To see a list of available indexes, use this query: `curl 'localhost:9200/_cat/indices?v'`. 
+
+You should get multiple hits back. For example:
 
 [source,json]
 --------------------------------------------------------------------------------
-{"took":3,
-"timed_out":false,
-"_shards":{
-  "total":5,
-  "successful":5,
-  "failed":0},
-"hits":{"total":1,
-  "max_score":1.03399,
-  "hits":[{"_index":"logstash-2015.07.31",
-    "_type":"logs",
-    "_id":"AU7mK3CVSiMeBsJ0b_EP",
-    "_score":1.03399,
-    "_source":{
-      "message":"108.174.55.234 - - [04/Jan/2015:05:27:45 +0000] \"GET /?flav=rss20 HTTP/1.1\" 200 29941 \"-\" \"-\"",
-      "@version":"1",
-      "@timestamp":"2015-07-31T22:11:22.347Z",
-      "host":"localhost",
-      "path":"/path/to/logstash-tutorial-dataset",
-      "clientip":"108.174.55.234",
-      "ident":"-",
-      "auth":"-",
-      "timestamp":"04/Jan/2015:05:27:45 +0000",
-      "verb":"GET",
-      "request":"/?flav=rss20",
-      "httpversion":"1.1",
-      "response":"200",
-      "bytes":"29941",
-      "referrer":"\"-\"",
-      "agent":"\"-\"",
-      "geoip":{
-        "ip":"108.174.55.234",
-        "country_code2":"US",
-        "country_code3":"USA",
-        "country_name":"United States",
-        "continent_code":"NA",
-        "region_name":"NY",
-        "city_name":"Buffalo",
-        "postal_code":"14221",
-        "latitude":42.9864,
-        "longitude":-78.7279,
-        "dma_code":514,
-        "area_code":716,
-        "timezone":"America/New_York",
-        "real_region_name":"New York",
-        "location":[-78.7279,42.9864]
+{
+  "took" : 21,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 98,
+    "max_score" : 3.745223,
+    "hits" : [
+      {
+        "_index" : "logstash-2016.10.11",
+        "_type" : "log",
+        "_id" : "AVe14gMiYMkU36o_eVsA",
+        "_score" : 3.745223,
+        "_source" : {
+          "request" : "/presentations/logstash-monitorama-2013/images/frontend-response-codes.png",
+          "agent" : "\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "geoip" : {
+            "timezone" : "Europe/Moscow",
+            "ip" : "83.149.9.216",
+            "latitude" : 55.7522,
+            "continent_code" : "EU",
+            "city_name" : "Moscow",
+            "country_code2" : "RU",
+            "country_name" : "Russia",
+            "dma_code" : null,
+            "country_code3" : "RU",
+            "region_name" : "Moscow",
+            "location" : [
+              37.6156,
+              55.7522
+            ],
+            "postal_code" : "101194",
+            "longitude" : 37.6156,
+            "region_code" : "MOW"
+          },
+          "offset" : 2932,
+          "auth" : "-",
+          "ident" : "-",
+          "input_type" : "log",
+          "verb" : "GET",
+          "source" : "/path/to/file/logstash-tutorial.log",
+          "message" : "83.149.9.216 - - [04/Jan/2015:05:13:45 +0000] \"GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1\" 200 52878 \"http://semicomplete.com/presentations/logstash-monitorama-2013/\" \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36\"",
+          "type" : "log",
+          "tags" : [
+            "beats_input_codec_plain_applied"
+          ],
+          "referrer" : "\"http://semicomplete.com/presentations/logstash-monitorama-2013/\"",
+          "@timestamp" : "2016-10-11T22:34:25.317Z",
+          "response" : "200",
+          "bytes" : "52878",
+          "clientip" : "83.149.9.216",
+          "@version" : "1",
+          "beat" : {
+            "hostname" : "My-MacBook-Pro.local",
+            "name" : "My-MacBook-Pro.local"
+          },
+          "host" : "My-MacBook-Pro.local",
+          "httpversion" : "1.1",
+          "timestamp" : "04/Jan/2015:05:13:45 +0000"
+        }
       }
-    }
-  }]
- }
-}
+    }, 
+    ...
+    
+--------------------------------------------------------------------------------
+
+Try another search for the geographic information derived from the IP address.
+Replace $DATE with the current date, in YYYY.MM.DD format:
+
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=geoip.city_name=Buffalo'
 --------------------------------------------------------------------------------
 
+A few log entries come from Buffalo, so the query produces the following response:
+
+[source,json]
+--------------------------------------------------------------------------------
+{
+  "took" : 3,
+  "timed_out" : false,
+  "_shards" : {
+    "total" : 5,
+    "successful" : 5,
+    "failed" : 0
+  },
+  "hits" : {
+    "total" : 3,
+    "max_score" : 2.6390574,
+    "hits" : [
+      {
+        "_index" : "logstash-2016.10.11",
+        "_type" : "log",
+        "_id" : "AVe14gMjYMkU36o_eVtO",
+        "_score" : 2.6390574,
+        "_source" : {
+          "request" : "/?flav=rss20",
+          "agent" : "\"-\"",
+          "geoip" : {
+            "timezone" : "America/New_York",
+            "ip" : "108.174.55.234",
+            "latitude" : 42.9864,
+            "continent_code" : "NA",
+            "city_name" : "Buffalo",
+            "country_code2" : "US",
+            "country_name" : "United States",
+            "dma_code" : 514,
+            "country_code3" : "US",
+            "region_name" : "New York",
+            "location" : [
+              -78.7279,
+              42.9864
+            ],
+            "postal_code" : "14221",
+            "longitude" : -78.7279,
+            "region_code" : "NY"
+          },
+          "offset" : 21471,
+          "auth" : "-",
+          "ident" : "-",
+          "input_type" : "log",
+          "verb" : "GET",
+          "source" : "/path/to/file/logstash-tutorial.log",
+          "message" : "108.174.55.234 - - [04/Jan/2015:05:27:45 +0000] \"GET /?flav=rss20 HTTP/1.1\" 200 29941 \"-\" \"-\"",
+          "type" : "log",
+          "tags" : [
+            "beats_input_codec_plain_applied"
+          ],
+          "referrer" : "\"-\"",
+          "@timestamp" : "2016-10-11T22:34:25.318Z",
+          "response" : "200",
+          "bytes" : "29941",
+          "clientip" : "108.174.55.234",
+          "@version" : "1",
+          "beat" : {
+            "hostname" : "My-MacBook-Pro.local",
+            "name" : "My-MacBook-Pro.local"
+          },
+          "host" : "My-MacBook-Pro.local",
+          "httpversion" : "1.1",
+          "timestamp" : "04/Jan/2015:05:27:45 +0000"
+        }
+      },
+     ...
+     
+--------------------------------------------------------------------------------
+
+If you are using Kibana to visualize your data, you can also explore the Filebeat data in Kibana:
+
+image::static/images/kibana-filebeat-data.png[Discovering Filebeat data in Kibana]
+
+See the {filebeat}filebeat-getting-started.html[Filebeat getting started docs] for info about loading the Kibana
+index pattern for Filebeat.
+
+You've successfully created a pipeline that uses Filebeat to take Apache web logs as input, parses those logs to
+create specific, named fields from the logs, and writes the parsed data to an Elasticsearch cluster. Next, you
+learn how to create a pipeline that uses multiple input and output plugins.
+
 [[multiple-input-output-plugins]]
-==== Multiple Input and Output Plugins
+=== Stitching Together Multiple Input and Output Plugins
 
 The information you need to manage often comes from several disparate sources, and use cases can require multiple
 destinations for your data. Your Logstash pipeline can use multiple input and output plugins to handle these
 requirements.
 
-This example creates a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
+In this section, you create a Logstash pipeline that takes input from a Twitter feed and the Filebeat client, then
 sends the information to an Elasticsearch cluster as well as writing the information directly to a file.
 
 [float]
 [[twitter-configuration]]
-==== Reading from a Twitter feed
+==== Reading from a Twitter Feed
 
-To add a Twitter feed, you need several pieces of information:
+To add a Twitter feed, you use the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin. To
+configure the plugin, you need several pieces of information:
 
-* A _consumer_ key, which uniquely identifies your Twitter app, which is Logstash in this case.
+* A _consumer key_, which uniquely identifies your Twitter app.
 * A _consumer secret_, which serves as the password for your Twitter app.
-* One or more _keywords_ to search in the incoming feed.
+* One or more _keywords_ to search in the incoming feed. The example shows using "cloud" as a keyword, but you can use whatever you want.
 * An _oauth token_, which identifies the Twitter account using this app.
 * An _oauth token secret_, which serves as the password of the Twitter account.
 
-Visit https://dev.twitter.com/apps to set up a Twitter account and generate your consumer key and secret, as well as
-your OAuth token and secret.
+Visit https://dev.twitter.com/apps[https://dev.twitter.com/apps] to set up a Twitter account and generate your consumer
+key and secret, as well as your access token and secret. See the docs for the {logstash}plugins-inputs-twitter.html[`twitter`] input plugin if you're not sure how to generate these keys. 
 
-Use this information to add the following lines to the `input` section of the `first-pipeline.conf` file:
+Like you did earlier when you worked on <<advanced-pipeline>>, create a config file (called `second-pipeline.conf`) that
+contains the skeleton of a configuration pipeline. If you want, you can reuse the file you created earlier, but make
+sure you pass in the correct config file name when you run Logstash. 
+
+Add the following lines to the `input` section of the `second-pipeline.conf` file, substituting your values for the 
+placeholder values shown here:
 
 [source,json]
 --------------------------------------------------------------------------------
-twitter {
-    consumer_key =>
-    consumer_secret =>
-    keywords =>
-    oauth_token =>
-    oauth_token_secret =>
-}
+    twitter {
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
+    }
 --------------------------------------------------------------------------------
 
 [float]
 [[configuring-lsf]]
-==== The Filebeat Client
+==== Configuring Filebeat to Send Log Lines to Logstash
 
-The https://github.com/elastic/beats/tree/master/filebeat[filebeat] client is a lightweight, resource-friendly tool that
-collects logs from files on the server and forwards these logs to your Logstash instance for processing. The
-Filebeat client uses the secure Beats protocol to communicate with your Logstash instance. The
-lumberjack protocol is designed for reliability and low latency. Filebeat uses the computing resources of
-the machine hosting the source data, and the {logstash}plugins-inputs-beats.html[Beats input] plugin minimizes the
-resource demands on the Logstash instance.
+As you learned earlier in <<configuring-filebeat>>, the https://github.com/elastic/beats/tree/master/filebeat[Filebeat]
+client is a lightweight, resource-friendly tool that collects logs from files on the server and forwards these logs to your
+Logstash instance for processing.
 
-NOTE: In a typical use case, Filebeat runs on a separate machine from the machine running your
-Logstash instance. For the purposes of this tutorial, Logstash and Filebeat are running on the
-same machine.
-
-Default Logstash configuration includes the {logstash}plugins-inputs-beats.html[Beats input plugin], which is
-designed to be resource-friendly. To install Filebeat on your data source machine, download the
-appropriate package from the Filebeat https://www.elastic.co/downloads/beats/filebeat[product page].
-
-Create a configuration file for Filebeat similar to the following example:
+After installing Filebeat, you need to configure it. Open the `filebeat.yml` file located in your Filebeat installation
+directory, and replace the contents with the following lines. Make sure `paths` points to your syslog: 
 
 [source,shell]
 --------------------------------------------------------------------------------
-filebeat:
-  prospectors:
-    -
-      paths:
-        - "/path/to/sample-log" <2>
-      fields:
-        type: syslog
-output:
-  logstash:
-    hosts: ["localhost:5043"]
-  tls:
-    certificate: /path/to/ssl-certificate.crt <2>
-    certificate_key: /path/to/ssl-certificate.key
-    certificate_authorities: /path/to/ssl-certificate.crt
-    timeout: 15
-
-<1> Path to the file or files that Filebeat processes.
-<2> Path to the SSL certificate for the Logstash instance.
+filebeat.prospectors:
+- input_type: log
+  paths:
+    - /var/log/*.log <1>
+  fields:
+    type: syslog <2>
+output.logstash:
+  hosts: ["localhost:5043"]
 --------------------------------------------------------------------------------
 
-Save this configuration file as `filebeat.yml`.
+<1> Absolute path to the file or files that Filebeat processes.
+<2> Adds a field called `type` with the value `syslog` to the event.
+
+Save your changes. 
+
+To keep the configuration simple, you won't specify TLS/SSL settings as you would in a real world
+scenario.
 
 Configure your Logstash instance to use the Filebeat input plugin by adding the following lines to the `input` section
-of the `first-pipeline.conf` file:
+of the `second-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
-beats {
-    port => "5043"
-    ssl => true
-    ssl_certificate => "/path/to/ssl-cert" <1>
-    ssl_key => "/path/to/ssl-key" <2>
-}
+    beats {
+        port => "5043"
+    }
 --------------------------------------------------------------------------------
 
-<1> Path to the SSL certificate that the Logstash instance uses to authenticate itself to Filebeat.
-<2> Path to the key for the SSL certificate.
-
 [float]
 [[logstash-file-output]]
 ==== Writing Logstash Data to a File
@@ -441,23 +717,23 @@ You can configure your Logstash pipeline to write data directly to a file with t
 {logstash}plugins-outputs-file.html[`file`] output plugin.
 
 Configure your Logstash instance to use the `file` output plugin by adding the following lines to the `output` section
-of the `first-pipeline.conf` file:
+of the `second-pipeline.conf` file:
 
 [source,json]
 --------------------------------------------------------------------------------
-file {
-    path => /path/to/target/file
-}
+    file {
+        path => "/path/to/target/file"
+    }
 --------------------------------------------------------------------------------
 
 [float]
 [[multiple-es-nodes]]
-==== Writing to multiple Elasticsearch nodes
+==== Writing to Multiple Elasticsearch Nodes
 
 Writing to multiple Elasticsearch nodes lightens the resource demands on a given Elasticsearch node, as well as
 providing redundant points of entry into the cluster when a particular node is unavailable.
 
-To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the output section of the `first-pipeline.conf` file to read:
+To configure your Logstash instance to write to multiple Elasticsearch nodes, edit the `output` section of the `second-pipeline.conf` file to read:
 
 [source,json]
 --------------------------------------------------------------------------------
@@ -470,29 +746,26 @@ output {
 
 Use the IP addresses of three non-master nodes in your Elasticsearch cluster in the host line. When the `hosts`
 parameter lists multiple IP addresses, Logstash load-balances requests across the list of addresses. Also note that
-default port for Elasticsearch is `9200` and can be omitted in the configuration above.
+the default port for Elasticsearch is `9200` and can be omitted in the configuration above.
 
 [float]
 [[testing-second-pipeline]]
 ===== Testing the Pipeline
 
-At this point, your `first-pipeline.conf` file looks like this:
+At this point, your `second-pipeline.conf` file looks like this: 
 
 [source,json]
 --------------------------------------------------------------------------------
 input {
     twitter {
-        consumer_key =>
-        consumer_secret =>
-        keywords =>
-        oauth_token =>
-        oauth_token_secret =>
+        consumer_key => "enter_your_consumer_key_here"
+        consumer_secret => "enter_your_secret_here"
+        keywords => ["cloud"]
+        oauth_token => "enter_your_access_token_here"
+        oauth_token_secret => "enter_your_access_token_secret_here"
     }
     beats {
         port => "5043"
-        ssl => true
-        ssl_certificate => "/path/to/ssl-cert"
-        ssl_key => "/path/to/ssl-key"
     }
 }
 output {
@@ -500,7 +773,7 @@ output {
         hosts => ["IP Address 1:port1", "IP Address 2:port2", "IP Address 3"]
     }
     file {
-        path => /path/to/target/file
+        path => "/path/to/target/file"
     }
 }
 --------------------------------------------------------------------------------
@@ -522,91 +795,41 @@ To verify your configuration, run the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf --configtest
+bin/logstash -f second-pipeline.conf --config.test_and_exit
 --------------------------------------------------------------------------------
 
-The `--configtest` option parses your configuration file and reports any errors. When the configuration file passes
-the configuration test, start Logstash with the following command:
+The `--config.test_and_exit` option parses your configuration file and reports any errors. When the configuration file
+passes the configuration test, start Logstash with the following command:
 
 [source,shell]
 --------------------------------------------------------------------------------
-bin/logstash -f first-pipeline.conf
+bin/logstash -f second-pipeline.conf
 --------------------------------------------------------------------------------
 
 Use the `grep` utility to search in the target file to verify that information is present:
 
 [source,shell]
 --------------------------------------------------------------------------------
-grep Mozilla /path/to/target/file
+grep syslog /path/to/target/file
 --------------------------------------------------------------------------------
 
 Run an Elasticsearch query to find the same information in the Elasticsearch cluster:
 
 [source,shell]
 --------------------------------------------------------------------------------
-curl -XGET 'localhost:9200/logstash-2015.07.30/_search?q=agent=Mozilla'
+curl -XGET 'localhost:9200/logstash-$DATE/_search?pretty&q=fields.type:syslog'
 --------------------------------------------------------------------------------
 
-[[stalled-shutdown]]
-=== Stalled Shutdown Detection
-
-Shutting down a running Logstash instance involves the following steps:
-
-* Stop all input, filter and output plugins
-* Process all in-flight events
-* Terminate the Logstash process
-
-The following conditions affect the shutdown process:
-
-* An input plugin receiving data at a slow pace.
-* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy
-query.
-* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+Replace $DATE with the current date, in YYYY.MM.DD format.
 
-These situations make the duration and success of the shutdown process unpredictable.
+To see data from the Twitter feed, try this query:
 
-Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
-This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
-worker threads.
+[source,shell]
+--------------------------------------------------------------------------------
+curl -XGET 'http://localhost:9200/logstash-$DATE/_search?pretty&q=client:iphone'
+--------------------------------------------------------------------------------
 
-To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--allow-unsafe-shutdown` flag when
-you start Logstash.
+Again, remember to replace $DATE with the current date, in YYYY.MM.DD format. 
 
-[[shutdown-stall-example]]
-==== Stall Detection Example
 
-In this example, slow filter execution prevents the pipeline from clean shutdown. By starting Logstash with the
-`--allow-unsafe-shutdown` flag, quitting with *Ctrl+C* results in an eventual shutdown that loses 20 events.
 
-========
-[source,shell]
-% bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } \
-                     output { stdout { codec => dots } }' -w 1 --allow-unsafe-shutdown
-Default settings used: Filter workers: 1
-Logstash startup completed
-^CSIGINT received. Shutting down the pipeline. {:level=>:warn}
-Received shutdown signal, but pipeline is still waiting for in-flight events
-to be processed. Sending another ^C will force quit Logstash, but this may cause
-data loss. {:level=>:warn}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
-The shutdown process appears to be stalled due to busy or blocked plugins. Check
-    the logs for more information.
-{:level=>:error}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
- {:level=>:warn, "INFLIGHT_EVENT_COUNT"=>{"input_to_filter"=>20, "total"=>20},
- "STALLING_THREADS"=>
- {["LogStash::Filters::Ruby", {"code"=>"sleep 10000"}]=>[{"thread_id"=>15,
- "name"=>"|filterworker.0", "current_call"=>"
- (ruby filter code):1:in `sleep'"}]}}
-Forcefully quitting logstash.. {:level=>:fatal}
-========
-
-When `--allow-unsafe-shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
diff --git a/docs/static/breaking-changes.asciidoc b/docs/static/breaking-changes.asciidoc
index 64b15808d43..cea5b769557 100644
--- a/docs/static/breaking-changes.asciidoc
+++ b/docs/static/breaking-changes.asciidoc
@@ -1,72 +1,268 @@
 [[breaking-changes]]
 == Breaking changes
 
-**Breaking changes in 2.2**
-Although 2.2 is fully compatible with configurations from older versions, there are some architectural 
-changes to the pipeline that users need to take into consideration before deploying in production. 
-These changes are not strictly "breaking" in the semantic versioning sense, but they make Logstash behave differently 
-in runtime, and can also affect performance. We have compiled such a list in the <<_upgrading_logstash_to_2.2>> section. 
-Please review it before deploying 2.2 version.
+This section discusses the changes that you need to be aware of when migrating your application to Logstash 5.0 from the previous major release of Logstash (2.x).
 
-**Changes in 2.0**
+[float]
+=== Changes in Logstash Core
 
-Version 2.0 of Logstash has some changes that are incompatible with previous versions of Logstash. This section discusses
-what you need to be aware of when migrating to this version.
+These changes can impact any instance of Logstash and are plugin agnostic, but only if you are using the features that are impacted.
 
 [float]
-== Elasticsearch Output Default
+==== Application Settings
+
+[IMPORTANT]
+Logstash 5.0 introduces a new way to <<logstash-settings-file, configure application settings>> for Logstash through a
+`logstash.yml` file.
 
-Starting with the 2.0 release of Logstash, the default Logstash output for Elasticsearch is HTTP. To use the `node` or
-`transport` protocols, download the https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-elasticsearch_java.html[Elasticsearch Java plugin]. The
-Logstash HTTP output to Elasticsearch now supports sniffing.
+This file is typically located in `${LS_HOME}/config`, or `/etc/logstash` when installed via packages. Logstash will not be 
+able to start without this file, so please make sure to pass in `--path.settings` if you are starting Logstash manually
+after installing it via a package (RPM, DEB).
 
-NOTE: The `elasticsearch_java` plugin has two versions specific to the version of the underlying Elasticsearch cluster.
-Be sure to specify the correct value for the `--version` option during installation:
-* For Elasticsearch versions before 2.0, use the command
-`bin/plugin install --version 1.5.x logstash-output-elasticsearch_java`
-* For Elasticsearch versions 2.0 and after, use the command
-`bin/plugin install --version 2.0.0 logstash-output-elasticsearch_java`
+[source,bash]
+----------------------------------
+bin/logstash --path.settings /path/to/logstash.yml
+----------------------------------
 
 [float]
-=== Configuration Changes
+==== Release Packages
 
-The Elasticsearch output plugin configuration has the following changes:
+When Logstash 5.0 is installed via DEB or RPM packages, it now uses `/usr/share/logstash` to
+install binaries. Previously it used to install in `/opt/logstash` directory. This change was done to make the user experience consistent with other products in the Elastic Stack.
 
-* The `host` configuration option is now `hosts`, allowing you to specify multiple hosts and associated ports in the
-`myhost:9200` format
-* New options: `bind_host`, `bind_port`, `cluster`, `embedded`, `embedded_http_port`, `port`, `sniffing_delay`
-* The `max_inflight_requests` option, which was deprecated in the 1.5 release, is now removed
-* Since the `hosts` option allows specification of ports for the hosts, the redundant `port` option is now removed
-* The `node_name` and `protocol` options have been moved to the `elasticsearch_java` plugin
+[cols="3", options="header"]
+|===
+| |DEB |RPM
+|Logstash 2.x
+|`/opt/logstash`
+|`/opt/logstash`
+|Logstash 5.0 
+|`/usr/share/logstash`
+|`/usr/share/logstash`
+|===
 
-The following deprecated configuration settings are removed in this release:
+A complete directory layout is described in <<dir-layout>>. This will likely impact any scripts that you may have written
+to support installing or manipulating Logstash, such as via Puppet.
+
+[float]
+==== Default Logging Level
 
-* input plugin configuration settings: `debug`, `format`, `charset`, `message_format`
-* output plugin configuration settings: `type`, `tags`, `exclude_tags`.
-* filter plugin configuration settings: `type`, `tags`, `exclude_tags`.
+The default log severity level changed to `INFO` instead of `WARN` to match Elasticsearch. Existing logs
+(in core and plugins) were too noisy at the `INFO` level, so we auditted our log messages and switched some of them to
+`DEBUG` level.
 
-Configuration files with these settings present are invalid and prevent Logstash from starting.
+You can use the new `logstash.yml` file to configure the `log.level` setting or continue to pass the new
+`--log.level` command line flag.
+
+[source,bash]
+----------------------------------
+bin/logstash --log.level warn
+----------------------------------
 
 [float]
-=== Kafka Output Configuration Changes
+==== Plugin Manager Renamed
+
+`bin/plugin` has been renamed to `bin/logstash-plugin`. This occurred in Logstash 2.3 and it was mainly prevent `PATH` being
+polluted when other components of the Elastic Stack are installed on the same machine. Also, this provides a foundation
+for future change which will allow Elastic Stack packs to be installed via this script.
+
+Logstash 5.0 also adds a `remove` option, which is an alias for the now-deprecated `uninstall` option.
 
-The 2.0 release of Logstash includes a new version of the Kafka output plugin with significant configuration changes.
-Please compare the documentation pages for the
-https://www.elastic.co/guide/en/logstash/1.5/plugins-outputs-kafka.html[Logstash 1.5] and
-https://www.elastic.co/guide/en/logstash/2.0/plugins-outputs-kafka.html[Logstash 2.0] versions of the Kafka output plugin
-and update your configuration files accordingly.
+As with earlier releases, the updated script allows both online and offline plugin installation. For example, to install a
+plugin named “my-plugin”, it’s as simple as running:
+
+[source,bash]
+----------------------------------
+bin/logstash-plugin install my-plugin
+----------------------------------
+
+Similar to the package changes, this is likely to impact and scripts that have been written to follow Logstash
+installations.
+
+Like earlier releases of Logstash, most plugins are bundled directly with Logstash, so no additional action is required
+while upgrading from earlier Logstash releases. However, if you are attempting to install a non-bundled plugin, then make
+sure that it supports Logstash 5.0 before upgrading!
 
 [float]
-=== Metrics Filter Changes
-Prior implementations of the metrics filter plugin used dotted field names. Elasticsearch does not allow field names to
-have dots, beginning with version 2.0, so a change was made to use sub-fields instead of dots in this plugin. Please note
-that these changes make version 3.0.0 of the metrics filter plugin incompatible with previous releases.
+==== Logstash with All Plugins Download
 
+The Logstash All Plugins download option has been removed. For users previously using this option as a convenience for
+offline plugin management purposes (air-gapped environments), please see the <<offline-plugins>> documentation page.
+
+There were 17 plugins removed from 5.0 default bundle. These plugins can still be installed manually for use:
+
+* logstash-codec-oldlogstashjson
+* logstash-input-eventlog
+* logstash-input-log4j
+* logstash-input-zeromq
+* logstash-filter-anonymize
+* logstash-filter-checksum
+* logstash-filter-multiline
+* logstash-output-email
+* logstash-output-exec
+* logstash-output-ganglia
+* logstash-output-gelf
+* logstash-output-hipchat
+* logstash-output-juggernaut
+* logstash-output-lumberjack
+* logstash-output-nagios_nsca
+* logstash-output-opentsdb
+* logstash-output-zeromq
 
 [float]
-=== Filter Worker Default Change
+==== Command Line Interface
+
+Some CLI Options changed in Logstash 5.0. If you were using the “long form” of the <<command-line-flags,options>>,
+then this will impact the way that you launch Logstash. They were changed to match the `logstash.yml` format used to
+simplify future setup, as well as behave in the same way as other products in the Elastic Stack. For example, here’s two
+before-and-after examples. In Logstash 2.x, you may have run something:
+
+[source,bash]
+----------------------------------
+bin/logstash --config my.conf --pipeline-workers 8 <1>
+bin/logstash -f my.conf -w 8 <2>
+----------------------------------
+1. Long form options `config` and `pipeline-workers` are used here.
+2. Short form options `f` and `w` (aliases for the former` are used here.
+
+But, in Logstash 5.0, this becomes:
+
+[source,bash]
+----------------------------------
+bin/logstash --path.config my.conf --pipeline.workers 8 <1>
+bin/logstash -f my.conf -w 8 <2>
+----------------------------------
+1. Long form options are changed to reflect the new options.
+2. Short form options are unchanged.
+
+NOTE: None of the short form options have changed!
+
+[float]
+=== Breaking Changes in Plugins
+
+[float]
+==== Elasticsearch Output `workers` Setting Removed
+
+Starting with Logstash 5.0, the `workers` setting in the Elasticsearch output
+plugin is no longer supported. Pipelines that specify this setting will no
+longer start up. You need to specify the `pipeline.workers` setting at the
+pipeline level instead. For more information about setting
+`pipeline.workers`, see <<logstash-settings-file>>.
+
+[float]
+==== Elasticsearch Output Index Template
+
+The index template for Elasticsearch 5.0 has been changed to reflect
+https://www.elastic.co/guide/en/elasticsearch/reference/5.0/breaking_50_mapping_changes.html[Elasticsearch's mapping changes]. Most
+importantly, the subfield for string multi-fields has changed from `.raw` to `.keyword` to match Elasticsearch's default
+behavior. The impact of this change to various user groups is detailed below:
+
+** New Logstash 5.0 and Elasticsearch 5.0 users: Multi-fields (often called sub-fields) use `.keyword` from the
+outset. In Kibana, you can use `my_field.keyword` to perform aggregations against text-based fields, in the same way that it 
+used to be `my_field.raw`.
+** Existing users with custom templates: Using a custom template means that you control the template completely, and our 
+template changes do not impact you.
+** Existing users with default template: Logstash does not force you to upgrade templates if one already exists. If you
+intend to move to the new template and want to use `.keyword`, you will most likely want to reindex existing data so that it
+also uses the `.keyword` field, unless you are able to transition from `.raw` to `.keyword`. Elasticsearch's
+{ref}docs-reindex.html[reindexing API] can help move your data from using `.raw` subfields to `.keyword`, thereby avoiding any
+transition time. You _can_ use a custom template to get both `.raw` and `.keyword` so that you can wait until all `.raw` data
+has stopped existing before transitioning to only using `.keyword`; this will waste some storage space and memory, but it does
+help users to avoid having to relearn operations.
+
+[float]
+==== Plugin Versions
+
+Logstash is unique amongst the Elastic Stack with respect to its plugins. Unlike Elasticsearch and Kibana, which both 
+require plugins to be targeted to a specific release, Logstash’s plugin ecosystem provides more flexibility so that it can
+support outside ecosystems _within the same release_. Unfortunately, 
+that flexibility can cause issues when handling upgrades.
+
+Non-standard plugins must always be checked for compatibility, but some bundled plugins are upgraded in order to remain 
+compatible with the tools or frameworks that they use for communication. For example, the
+<<plugins-inputs-kafka, Kafka Input>> and <<plugins-outputs-kafka, Kafka Output>> plugins serve as a primary example of 
+such compatibilty changes. The latest version of the Kafka plugins is only compatible with Kafka 0.10, but as the 
+compatibility matrices show: earlier plugin versions are required for earlier versions of Kafka (e.g., Kafka 0.9).
+
+Automatic upgrades generally lead to improved features and support, but network layer changes like those above may make part
+of your architecture incompatible. You should always test your Logstash configurations in a test environment before
+deploying to production, which would catch these kinds of issues. If you do face such an issue, then you should also check
+the specific plugin’s page to see how to get a compatible, older plugin version if necessary.
+
+For example, if you upgrade to Logstash 5.0, but you want to run against Kafka 0.9, then you need to remove the
+bundled plugin(s) that only work with Kafka 0.10 and replace them:
+
+[source,bash]
+----------------------------------
+bin/logstash-plugin remove logstash-input-kafka
+bin/logstash-plugin remove logstash-output-kafka
+bin/logstash-plugin install --version 4.0.0 logstash-input-kafka
+bin/logstash-plugin install --version 4.0.1 logstash-output-kafka
+----------------------------------
+
+The version numbers were found by checking the compatibility matrix for the individual plugins.
+
+[float]
+==== File Input
+
+The <<plugins-inputs-file, File Input>> `SinceDB` file is now saved at `<path.data>/plugins/inputs/file` location,
+where `path.data` is the path defined in the new `logstash.yml` file.
+
+[cols="2", options="header"]
+|===
+| |Default `sincedb_path`
+|Logstash 2.x
+|`$HOME/.sincedb*`
+|Logstash 5.0 
+|`<path.data>/plugins/inputs/file`
+|===
+
+If you have manually specified `sincedb_path` as part of the configuration, this change will not affect you.
+If you are moving from Logstash 2.x to Logstash 5.0, and you would like to use the existing SinceDB file,
+then it must be copied over to `path.data` manually to use the save state (or the path needs to be changed to point to it).
+
+[float]
+=== Ruby Filter and Custom Plugin Developers
+
+With the migration to the new <<event-api>>, we have changed how you can access internal data compared to previous release. 
+The `event` object no longer returns a reference to the data. Instead, it returns a copy. This might change how you perform
+manipulation of your data, especially when working with nested hashes. When working with nested hashes, it’s recommended that 
+you use the <<logstash-config-field-references, `field reference` syntax>> instead of using multiple square brackets.
+
+As part of this change, Logstash has introduced new Getter/Setter APIs for accessing information in the `event` object.
+
+**Examples:**
+
+Prior to Logstash 5.0, you may have used Ruby filters like so:
+
+[source, js]
+----------------------------------
+filter {
+  ruby {
+    codec => "event['name'] = 'Logstash'"
+  }
+  ruby {
+    codec => "event['product']['version'] = event['major'] + '.' + event['minor']"
+  }
+}
+----------------------------------
+
+The above syntax, which uses the `event` object as a reference, is no longer supported in
+Logstash 5.0. Fortunately, the change to make it work is very simple:
+
+[source, js]
+----------------------------------
+filter {
+  ruby {
+    codec => "event.set('name', 'Logstash')"
+  }
+  ruby {
+    codec => "event.set('[product][version]', event.get('major') + '.' + event.get('minor'))"
+  }
+}
+----------------------------------
+
+NOTE: Moving from the old syntax to the new syntax, it can be easy to miss that `['product']['version']` became
+`'[product][version]'`. The quotes moved from inside of the square brackets to outside of the square brackets!
 
-Starting with the 2.0 release of Logstash, the default value of the `filter_workers` configuration option for filter
-plugins is half of the available CPU cores, instead of 1. This change increases parallelism in filter execution for
-resource-intensive filtering operations. You can continue to use the `-w` flag to manually set the value for this option,
-as in previous releases.
+The <<event-api>> documentation describes the available syntax in great detail.
diff --git a/docs/static/command-line-flags.asciidoc b/docs/static/command-line-flags.asciidoc
index 2b08c8a7d2e..2e304247d3a 100644
--- a/docs/static/command-line-flags.asciidoc
+++ b/docs/static/command-line-flags.asciidoc
@@ -1,74 +1,120 @@
 [[command-line-flags]]
-=== Command-line flags
+=== Command-Line Flags
 
 Logstash has the following flags. You can use the `--help` flag to display this information.
 
-[source,shell]
-----------------------------------
--f, --config CONFIGFILE
- Load the Logstash config from a specific file, directory, or a wildcard. If
- given a directory or wildcard, config files will be read from the directory in
- alphabetical order.
-
--e CONFIGSTRING
- Use the given string as the configuration data. Same syntax as the config file.
- If not input is specified, 'stdin { type => stdin }' is default. If no output
- is specified, 'stdout { codec => rubydebug }}' is default.
-
--w, --filterworkers COUNT
- Sets the number of pipeline workers (threads) to run for filter and output
- processing (default: number of cores).
- If you find that events are backing up, or that the CPU is not saturated, consider increasing
- this number to better utilize machine processing power.
-
--b, --pipeline-batch-size SIZE
- This parameter defines the maximum number of events an individual worker thread will collect
- before attempting to execute its filters and outputs. Default is 125 events.
- Larger batch sizes are generally more efficient, but come at the cost of increased memory
- overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
- variable to effectively use the option.
-
--u, --pipeline-batch-delay DELAY_IN_MS
- When creating pipeline event batches, how long to wait while polling for the next event.
- Default is 5ms.
-
--l, --log FILE
- Log to a given path. Default is to log to stdout
-
---verbose
- Increase verbosity to the first level (info), less verbose.
-
---debug
- Increase verbosity to the last level (trace), more verbose.
-
--V, --version
-  Display the version of Logstash.
-
--p, --pluginpath
-  A path of where to find plugins. This flag can be given multiple times to include
+Instead of specifying options at the command line, we recommend that you control Logstash execution
+by specifying options in the Logstash <<logstash-settings-file,settings file>>. Using a settings file
+makes it easier for you to specify mutliple options, and it provides you with a single, versionable
+file that you can use to start up Logstash consistently for each run.
+
+Any flags that you set at the command line override the corresponding settings in the Logstash
+<<logstash-settings-file,settings file>>.
+
+*`--node.name NAME`*::
+  Specify the name of this Logstash instance. If no value is given it will default to the current
+  hostname.
+
+*`-f, --path.config CONFIG_PATH`*::
+  Load the Logstash config from a specific file or directory. If a directory is given, all
+  files in that directory will be concatenated in lexicographical order and then parsed as a
+  single config file. You can also specify wildcards (globs) and any matched files will
+  be loaded in the order described above.
+
+*`-e, --config.string CONFIG_STRING`*::
+  Use the given string as the configuration data. Same syntax as the config file. If no
+  input is specified, then the following is used as the default input:
+  `input { stdin { type => stdin } }` and if no output is specified, then the
+  following is used as the default output: `output { stdout { codec => rubydebug } }`.
+  If you wish to use both defaults, please use the empty string for the `-e` flag.
+  The default is nil.
+
+*`-w, --pipeline.workers COUNT`*::
+  Sets the number of pipeline workers to run. This option sets the number of workers that will,
+  in parallel, execute the filter and output stages of the pipeline. If you find that events are
+  backing up, or that  the CPU is not saturated, consider increasing this number to better utilize
+  machine processing power. The default is 8.
+
+*`-b, --pipeline.batch.size SIZE`*::
+  Size of batches the pipeline is to work in. This option defines the maximum number of events an
+  individual worker thread will collect from inputs before attempting to execute its filters and outputs.
+  The default is 125 events. Larger batch sizes are generally more efficient, but come at the cost of
+  increased memory overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
+  variable to effectively use the option.
+
+*`-u, --pipeline.batch.delay DELAY_IN_MS`*::
+  When creating pipeline batches, how long to wait while polling for the next event. This option defines
+  how long in milliseconds to wait before dispatching an undersized batch to filters and workers.
+  The default is 5ms.
+
+*`--pipeline.unsafe_shutdown`*::
+  Force Logstash to exit during shutdown even if there are still inflight events
+  in memory. By default, Logstash will refuse to quit until all received events
+  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
+
+*`--path.data PATH`*::
+  This should point to a writable directory. Logstash will use this directory whenever it needs to store
+  data. Plugins will also have access to this path. The default is the `data` directory under
+  Logstash home.
+
+*`-p, --path.plugins PATH`*::
+  A path of where to find custom plugins. This flag can be given multiple times to include
   multiple paths. Plugins are expected to be in a specific directory hierarchy:
-  'PATH/logstash/TYPE/NAME.rb' where TYPE is 'inputs' 'filters', 'outputs' or 'codecs'
-  and NAME is the name of the plugin.
-
--t, --configtest
-  Checks configuration and then exit. Note that grok patterns are not checked for
-  correctness with this flag.
-  Logstash can read multiple config files from a directory. If you combine this
-  flag with `--debug`, Logstash will log the combined config file, annotating the
-  individual config blocks with the source file it came from.
-  
--r, --[no-]auto-reload
-  Monitor configuration changes and reload the configuration whenever it is changed.
-  
---reload-interval RELOAD_INTERVAL
-  Specifies how often Logstash checks the config files for changes. The default is every 3 seconds.
-
---http-host WEB_API_HTTP_HOST 
-  Web API binding host (default: "127.0.0.1")
-
---http-port WEB_API_HTTP_PORT
-  Web API http port (default: 9600)
-
--h, --help
+  `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`,
+  and `NAME` is the name of the plugin.
+
+*`-l, --path.logs PATH`*::
+  Directory to Write Logstash internal logs to.
+
+*`--log.level LEVEL`*::
+ Set the log level for Logstash. Possible values are:
+* `fatal`: log very severe error messages that will usually be followed by the application aborting
+* `error`: log errors
+* `warn`: log warnings
+* `info`: log verbose info (this is the default)
+* `debug`: log debugging info (for developers)
+* `trace`: log finer-grained messages beyond debugging info
+
+*`--config.debug`*::
+  Show the fully compiled configuration as a debug log message (you must also have `--log.level=debug` enabled).
+  WARNING: The log message will include any 'password' options passed to plugin configs as plaintext, and may result
+  in plaintext passwords appearing in your logs!
+
+*`-i, --interactive SHELL`*::
+  Drop to shell instead of running as normal. Valid shells are "irb" and "pry".
+
+*`-V, --version`*::
+  Emit the version of Logstash and its friends, then exit.
+
+*`-t, --config.test_and_exit`*::
+  Check configuration for valid syntax and then exit. Note that grok patterns are not checked for
+  correctness with this flag. Logstash can read multiple config files from a directory. If you combine this
+  flag with `--log.level=debug`, Logstash will log the combined config file, annotating
+  each config block with the source file it came from.
+
+*`-r, --config.reload.automatic`*::
+  Monitor configuration changes and reload whenever the configuration is changed.
+  NOTE: Use SIGHUP to manually reload the config. The default is false.
+
+*`--config.reload.interval RELOAD_INTERVAL`*::
+  How frequently to poll the configuration location for changes, in seconds. The default is every 3 seconds.
+
+*`--http.host HTTP_HOST`*::
+  Web API binding host. This option specifies the bind address for the metrics REST endpoint. The default is "127.0.0.1".
+
+*`--http.port HTTP_PORT`*::
+  Web API http port. This option specifies the bind port for the metrics REST endpoint. The default is 9600-9700.
+  This setting accepts a range of the format 9600-9700. Logstash will pick up the first available port.
+
+*`--log.format FORMAT`*::
+   Specify if Logstash should write its own logs in JSON form (one event per line) or in plain text
+   (using Ruby's Object#inspect). The default is "plain".
+
+*`--path.settings SETTINGS_DIR`*::
+  Set the directory containing the `logstash.yml` <<logstash-settings-file,settings file>> as well
+  as the log4j logging configuration. This can also be set through the LS_SETTINGS_DIR environment variable.
+  The default is the `config` directory under Logstash home.
+
+*`-h, --help`*::
   Print help
-----------------------------------
+
diff --git a/docs/static/configuration.asciidoc b/docs/static/configuration.asciidoc
index 72aaeae5731..ec2d21d1aae 100644
--- a/docs/static/configuration.asciidoc
+++ b/docs/static/configuration.asciidoc
@@ -83,25 +83,40 @@ The settings you can configure vary according to the plugin type. For informatio
 === Value Types
 
 A plugin can require that the value for a setting be a
-certain type, such as boolean or hash. The following value
+certain type, such as boolean, list, or hash. The following value
 types are supported.
 
 [[array]]
-[float]
 ==== Array
 
-An array can be a single string value or multiple values. If you specify the same
-setting multiple times, it appends to the array.
+This type is now mostly deprecated in favor of using a standard type like `string` with the plugin defining the `:list => true` property for better type checking. It is still needed to handle lists of hashes or mixed types where type checking is not desired. added[5.0.0-alpha4,The :list property is available for better type checking] 
+
+Example:
+
+[source,js]
+----------------------------------
+  users => [ {id => 1, name => bob}, {id => 2, name => jane} ]
+----------------------------------
+
+[[list]]
+[float]
+==== Lists
+
+added[5.0.0-alpha4,The :list property is available for better type checking]
+
+Not a type in and of itself, but a property types can have.
+This makes it possible to type check multiple values.
+Plugin authors can enable list checking by specifying `:list => true` when declaring an argument.
 
 Example:
 
 [source,js]
 ----------------------------------
   path => [ "/var/log/messages", "/var/log/*.log" ]
-  path => "/data/mysql/mysql.log"
+  uris => [ "http://elastic.co", "http://example.net" ]
 ----------------------------------
 
-This example configures `path` to be an array that contains an element for each of the three strings.
+This example configures `path`, which is a `string` to be a list that contains an element for each of the three strings. It also will configure the `uris` parameter to be a list of URIs, failing if any of the URIs provided are not valid.
 
 
 [[boolean]]
@@ -203,6 +218,23 @@ Example:
   my_password => "password"
 ----------------------------------
 
+[[uri]]
+[float]
+==== URI
+
+added[5.0.0-alpha4]
+
+A URI can be anything from a full URL like 'http://elastic.co/' to a simple identifier
+like 'foobar'. If the URI contains a password such as 'http://user:pass@example.net' the password
+portion of the URI will not be logged or printed.
+
+Example:
+[source,js]
+----------------------------------
+  my_uri => "http://foo:bar@example.net"
+----------------------------------
+
+
 [[path]]
 [float]
 ==== Path
@@ -366,7 +398,7 @@ What's an expression? Comparison tests, boolean logic, and so on!
 You can use the following comparison operators:
 
 * equality: `==`,  `!=`,  `<`,  `>`,  `<=`, `>=`
-* regexp: `=~`, `!~`
+* regexp: `=~`, `!~` (checks a pattern on the right against a string value on the left)
 * inclusion: `in`, `not in`
 
 The supported boolean operators are:
@@ -387,7 +419,7 @@ For example, the following conditional uses the mutate filter to remove the fiel
 ----------------------------------
 filter {
   if [action] == "login" {
-    mutate { remove => "secret" }
+    mutate { remove_field => "secret" }
   }
 }
 ----------------------------------
@@ -406,7 +438,7 @@ output {
 }
 ----------------------------------
 
-The `in` conditional enables you to compare against the value of a field:
+You can use the `in` operator to test whether a field contains a specific string, key, or (for lists) element:
 
 [source,js]
 ----------------------------------
@@ -433,7 +465,7 @@ filter {
 ----------------------------------
 
 You use the `not in` conditional the same way. For example,
-you could use `not in` to only route events to elasticsearch
+you could use `not in` to only route events to Elasticsearch
 when `grok` is successful:
 
 [source,js]
@@ -445,13 +477,20 @@ output {
 }
 ----------------------------------
 
+You can check for the existence of a specific field, but there's currently no way to differentiate between a field that
+doesn't exist versus a field that's simply false. The expression `if [foo]` returns `false` when:
+
+* `[foo]` doesn't exist in the event,
+* `[foo]` exists in the event, but is false, or
+* `[foo]` exists in the event, but is null
+
 For more complex examples, see <<using-conditionals, Using Conditionals>>.
 
 [float]
 [[metadata]]
 ==== The @metadata field
 
-In Logstash 1.5 there is a new, special field, called `@metadata`.  The contents
+In Logstash 1.5 and later, there is a special field called `@metadata`.  The contents
 of `@metadata` will not be part of any of your events at output time, which
 makes it great to use for conditionals, or extending and building event fields
 with field reference and sprintf formatting.
@@ -484,15 +523,16 @@ Let's see what comes out:
 ----------------------------------
 
 $ bin/logstash -f ../test.conf
-Logstash startup completed
+Pipeline main started
 asdf
 {
-       "message" => "asdf",
+    "@timestamp" => 2016-06-30T02:42:51.496Z,
       "@version" => "1",
-    "@timestamp" => "2015-03-18T23:09:29.595Z",
           "host" => "example.com",
-          "show" => "This data will be in the output"
+          "show" => "This data will be in the output",
+       "message" => "asdf"
 }
+
 ----------------------------------
 
 The "asdf" typed in became the `message` field contents, and the conditional
@@ -513,18 +553,18 @@ Let's see what the output looks like with this change:
 [source,ruby]
 ----------------------------------
 $ bin/logstash -f ../test.conf
-Logstash startup completed
+Pipeline main started
 asdf
 {
-       "message" => "asdf",
-      "@version" => "1",
-    "@timestamp" => "2015-03-18T23:10:19.859Z",
-          "host" => "example.com",
-          "show" => "This data will be in the output",
+    "@timestamp" => 2016-06-30T02:46:48.565Z,
      "@metadata" => {
            "test" => "Hello",
         "no_show" => "This data will not be in the output"
-    }
+    },
+      "@version" => "1",
+          "host" => "example.com",
+          "show" => "This data will be in the output",
+       "message" => "asdf"
 }
 ----------------------------------
 
@@ -565,13 +605,13 @@ configuration a sample date string and see what comes out:
 [source,ruby]
 ----------------------------------
 $ bin/logstash -f ../test.conf
-Logstash startup completed
+Pipeline main started
 02/Mar/2014:15:36:43 +0100
 {
-       "message" => "02/Mar/2014:15:36:43 +0100",
+    "@timestamp" => 2014-03-02T14:36:43.000Z,
       "@version" => "1",
-    "@timestamp" => "2014-03-02T14:36:43.000Z",
-          "host" => "example.com"
+          "host" => "example.com",
+       "message" => "02/Mar/2014:15:36:43 +0100"
 }
 ----------------------------------
 
@@ -600,86 +640,59 @@ output {
 ----------------------------------
 
 [[environment-variables]]
-=== Using Environment Variables in Configuration
+=== Using Environment Variables in the Configuration
+
 ==== Overview
 
-* You can set environment variable references into Logstash plugins configuration using `${var}` or `$var`.
-* Each reference will be replaced by environment variable value at Logstash startup.
+* You can set environment variable references in the configuration for Logstash plugins by using `${var}`.
+* At Logstash startup, each reference will be replaced by the value of the environment variable.
 * The replacement is case-sensitive.
 * References to undefined variables raise a Logstash configuration error.
-* A default value can be given by using the form `${var:default value}`.
-* You can add environment variable references in any plugin option type : string, number, boolean, array or hash.
-* Environment variables are immutable. If you update the environment variable, you'll have to restart Logstash to pick the updated value.
+* You can give a default value by using the form `${var:default value}`. Logstash uses the default value if the
+environment variable is undefined.
+* You can add environment variable references in any plugin option type : string, number, boolean, array, or hash.
+* Environment variables are immutable. If you update the environment variable, you'll have to restart Logstash to pick up the updated value.
 
 ==== Examples
 
-[cols="a,a,a"]
-|==================================
-|Logstash config source	|Environment 	|Logstash config result
+The following examples show you how to use environment variables to set the values of some commonly used
+configuration options.
 
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => "$TCP_PORT"
-  }
-}
-----
+===== Setting the TCP Port 
+
+Here's an example that uses an environment variable to set the TCP port:
 
-|
-[source,shell]
-----
-export TCP_PORT=12345
-----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => 12345
-  }
-}
-----
-|
 [source,ruby]
-----
+----------------------------------
 input {
   tcp {
     port => "${TCP_PORT}"
   }
 }
-----
+----------------------------------
+
+Now let's set the value of `TCP_PORT`:
 
-|
 [source,shell]
 ----
 export TCP_PORT=12345
 ----
-|
+
+At startup, Logstash uses the following configuration: 
+
 [source,ruby]
-----
+----------------------------------
 input {
   tcp {
     port => 12345
   }
 }
-----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => "${TCP_PORT}"
-  }
-}
-----
+----------------------------------
+
+If the `TCP_PORT` environment variable is not set, Logstash returns a configuration error.
+
+You can fix this problem by specifying a default value: 
 
-|
-No TCP_PORT defined
-|
-Raise a logstash configuration error
-|
 [source,ruby]
 ----
 input {
@@ -689,9 +702,8 @@ input {
 }
 ----
 
-|
-No TCP_PORT defined
-|
+Now, instead of returning a configuration error if the variable is undefined, Logstash uses the default:
+
 [source,ruby]
 ----
 input {
@@ -700,31 +712,13 @@ input {
   }
 }
 ----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => "${TCP_PORT:54321}"
-  }
-}
-----
 
-|
-[source,shell]
-----
-export TCP_PORT=12345
-----
-|
-[source,ruby]
-----
-input {
-  tcp {
-    port => 12345
-  }
-}
-----
-|
+If the environment variable is defined, Logstash uses the value specified for the variable instead of the default. 
+
+===== Setting the Value of a Tag
+
+Here's an example that uses an environment variable to set the value of a tag:
+
 [source,ruby]
 ----
 filter {
@@ -734,12 +728,15 @@ filter {
 }
 ----
 
-|
+Let's set the value of `ENV_TAG`:
+
 [source,shell]
 ----
 export ENV_TAG="tag2"
 ----
-|
+
+At startup, Logstash uses the following configuration: 
+
 [source,ruby]
 ----
 filter {
@@ -748,7 +745,11 @@ filter {
   }
 }
 ----
-|
+
+===== Setting a File Path
+
+Here's an example that uses an environment variable to set the path to a log file:
+
 [source,ruby]
 ----
 filter {
@@ -759,12 +760,16 @@ filter {
   }
 }
 ----
-|
+
+Let's set the value of `HOME`:
+
 [source,shell]
 ----
 export HOME="/path"
 ----
-|
+
+At startup, Logstash uses the following configuration: 
+
 [source,ruby]
 ----
 filter {
@@ -775,7 +780,7 @@ filter {
   }
 }
 ----
-|==================================
+
 
 [[config-examples]]
 === Logstash Configuration Examples
@@ -951,7 +956,7 @@ This example labels all events using the `type` field, but doesn't actually pars
 Similarly, you can use conditionals to direct events to particular outputs. For example, you could:
 
 * alert nagios of any apache events with status 5xx
-* record any 4xx status to elasticsearch
+* record any 4xx status to Elasticsearch
 * record all status code hits via statsd
 
 To tell nagios about any http event that has a 5xx status code, you
diff --git a/docs/static/contributing-to-logstash.asciidoc b/docs/static/contributing-to-logstash.asciidoc
index 238b26fa4fe..05bdeade5e7 100644
--- a/docs/static/contributing-to-logstash.asciidoc
+++ b/docs/static/contributing-to-logstash.asciidoc
@@ -14,7 +14,7 @@ Since plugins can now be developed and deployed independently of the Logstash
 core, there are documents which guide you through the process of coding and
 deploying your own plugins:
 
-
+* <<plugin-generator,Generating a New Plugin>>
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_input_plugin.html[How to write a Logstash input plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_codec_plugin.html[How to write a Logstash codec plugin]
 * http://www.elasticsearch.org/guide/en/logstash/current/_how_to_write_a_logstash_filter_plugin.html[How to write a Logstash filter plugin]
diff --git a/docs/static/deploying.asciidoc b/docs/static/deploying.asciidoc
index e57cd85554b..d5bff607b65 100644
--- a/docs/static/deploying.asciidoc
+++ b/docs/static/deploying.asciidoc
@@ -63,7 +63,7 @@ nodes. By default, Logstash uses the HTTP protocol to move data into the cluster
 You can use the Elasticsearch HTTP REST APIs to index data into the Elasticsearch cluster. These APIs represent the
 indexed data in JSON. Using the REST APIs does not require the Java client classes or any additional JAR
 files and has no performance disadvantages compared to the transport or node protocols. You can secure communications
-that use the HTTP REST APIs with the {shield}[Shield] plugin, which supports SSL and HTTP basic authentication.
+that use the HTTP REST APIs by using {xpack}xpack-security.html[{security}], which supports SSL and HTTP basic authentication.
 
 When you use the HTTP protocol, you can configure the Logstash Elasticsearch output plugin to automatically
 load-balance indexing requests across a
diff --git a/docs/static/event-api.asciidoc b/docs/static/event-api.asciidoc
new file mode 100644
index 00000000000..39ac6d77767
--- /dev/null
+++ b/docs/static/event-api.asciidoc
@@ -0,0 +1,120 @@
+[[event-api]]
+=== Event API
+
+This section is targeted for plugin developers and users of Logstash's Ruby filter. Below we document recent 
+changes (starting with version 5.0) in the way users have been accessing Logstash's event based data in 
+custom plugins and in the Ruby filter. Note that <<event-dependent-configuration>> 
+data flow in Logstash's config files -- using <<logstash-config-field-references>> -- is 
+not affected by this change, and will continue to use existing syntax.
+
+[float]
+==== Event Object
+
+Event is the main object that encapsulates data flow internally in Logstash and provides an API for the plugin 
+developers to interact with the event's content. Typically, this API is used in plugins and in a Ruby filter to 
+retrieve data and use it for transformations. Event object contains the original data sent to Logstash and any additional 
+fields created during Logstash's filter stages.
+
+In 5.0, we've re-implemented the Event class and its supporting classes in pure Java. Since Event is a critical component 
+in data processing,  a rewrite in Java improves performance and provides efficient serialization when storing data on disk. For the most part, this change aims at keeping backward compatibility and is transparent to the users. To this extent we've updated and published most of the plugins in Logstash's ecosystem to adhere to the new API changes. However, if you are maintaining a custom plugin, or have a Ruby filter, this change will affect you. The aim of this guide is to describe the new API and provide examples to migrate to the new changes.
+
+[float]
+==== Event API
+
+Prior to version 5.0, developers could access and manipulate event data by directly using Ruby hash syntax. For 
+example, `event[field] = foo`. While this is powerful, our goal is to abstract the internal implementation details 
+and provide well-defined getter and setter APIs.
+
+**Get API**
+
+The getter is a read-only access of field-based data in an Event.
+
+**Syntax:** `event.get(field)`
+
+**Returns:** Value for this field or nil if the field does not exist. Returned values could be a string, 
+numeric or timestamp scalar value.
+
+`field` is a structured field sent to Logstash or created after the transformation process. `field` can also 
+be a nested field reference such as `[field][bar]`.
+
+Examples:
+
+[source,ruby]
+--------------------------------------------------
+event.get("foo" ) # => "baz"
+event.get("[foo]") # => "zab"
+event.get("[foo][bar]") # => 1
+event.get("[foo][bar]") # => 1.0
+event.get("[foo][bar]") # =>  [1, 2, 3]
+event.get("[foo][bar]") # => {"a" => 1, "b" => 2}
+event.get("[foo][bar]") # =>  {"a" => 1, "b" => 2, "c" => [1, 2]}
+--------------------------------------------------
+
+Accessing @metdata
+
+[source,ruby]
+--------------------------------------------------
+event.get("[@metadata][foo]") # => "baz"
+--------------------------------------------------
+
+**Set API**
+
+This API can be used to mutate data in an Event. 
+
+**Syntax:** `event.set(field, value)`
+
+**Returns:**  The current Event  after the mutation, which can be used for chainable calls.
+
+Examples:
+
+[source,ruby]
+--------------------------------------------------
+event.set("foo", "baz")
+event.set("[foo]", "zab")
+event.set("[foo][bar]", 1)
+event.set("[foo][bar]", 1.0)
+event.set("[foo][bar]", [1, 2, 3])
+event.set("[foo][bar]", {"a" => 1, "b" => 2})
+event.set("[foo][bar]", {"a" => 1, "b" => 2, "c" => [1, 2]})
+event.set("[@metadata][foo]", "baz")
+--------------------------------------------------
+
+Mutating a collection after setting it in the Event has an undefined behaviour and is not allowed.
+
+[source,ruby]
+--------------------------------------------------
+h = {"a" => 1, "b" => 2, "c" => [1, 2]}
+event.set("[foo][bar]", h)
+
+h["c"] = [3, 4]
+event.get("[foo][bar][c]") # => undefined
+
+Suggested way of mutating collections:
+
+h = {"a" => 1, "b" => 2, "c" => [1, 2]}
+event.set("[foo][bar]", h)
+
+h["c"] = [3, 4]
+event.set("[foo][bar]", h)
+
+# Alternatively,
+event.set("[foo][bar][c]", [3, 4]) 
+--------------------------------------------------
+
+[float]
+==== Ruby Filter
+
+The <<plugins-filters-ruby,Ruby Filter>> can be used to execute any ruby code and manipulate event data using the 
+API described above. For example, using the new API:
+
+[source,ruby]
+--------------------------------------------------
+filter {
+  ruby {
+    code => 'event.set("lowercase_field", event.get("message").downcase)'
+  }  
+}    
+--------------------------------------------------
+
+This filter will lowercase the `message` field, and set it to a new field called `lowercase_field`
+
diff --git a/docs/static/getting-started-with-logstash.asciidoc b/docs/static/getting-started-with-logstash.asciidoc
index 2b2b244a0b7..e57fdc99313 100644
--- a/docs/static/getting-started-with-logstash.asciidoc
+++ b/docs/static/getting-started-with-logstash.asciidoc
@@ -2,13 +2,20 @@
 == Getting Started with Logstash
 
 This section guides you through the process of installing Logstash and verifying that everything is running properly.
-Later sections deal with increasingly complex configurations to address selected use cases.
+After learning how to stash your first event, you go on to create a more advanced pipeline that takes Apache web logs as
+input, parses the logs, and writes the parsed data to an Elasticsearch cluster. Then you learn how to stitch together multiple input and output plugins to unify data from a variety of disparate sources.
+
+This section includes the following topics:
+
+* <<installing-logstash>>
+* <<first-event>>
+* <<advanced-pipeline>>
+* <<multiple-input-output-plugins>>
 
-[float]
 [[installing-logstash]]
-=== Install Logstash
+=== Installing Logstash
 
-NOTE: Logstash requires Java 7 or later. Use the
+NOTE: Logstash requires Java 8 or later. Use the
 http://www.oracle.com/technetwork/java/javase/downloads/index.html[official Oracle distribution] or an open-source
 distribution such as http://openjdk.java.net/[OpenJDK].
 
@@ -20,33 +27,150 @@ java -version
 On systems with Java installed, this command produces output similar to the following:
 
 [source,shell]
-java version "1.7.0_45"
-Java(TM) SE Runtime Environment (build 1.7.0_45-b18)
-Java HotSpot(TM) 64-Bit Server VM (build 24.45-b08, mixed mode)
+java version "1.8.0_65"
+Java(TM) SE Runtime Environment (build 1.8.0_65-b17)
+Java HotSpot(TM) 64-Bit Server VM (build 25.65-b01, mixed mode)
 
 [float]
 [[installing-binary]]
-==== Installing from a downloaded binary
+=== Installing from a Downloaded Binary
 
 Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
-Unpack the file. On supported Linux operating systems, you can <<package-repositories,use a package manager>> to
-install Logstash.
+Unpack the file. Do not install Logstash into a directory path that contains colon (:) characters. 
+
+On supported Linux operating systems, you can use a package manager to install Logstash.
+
+[float]
+[[package-repositories]]
+=== Installing from Package Repositories
+
+We also have repositories available for APT and YUM based distributions. Note
+that we only provide binary packages, but no source packages, as the packages
+are created as part of the Logstash build.
+
+We have split the Logstash package repositories by version into separate urls
+to avoid accidental upgrades across major versions. For all {major-version}.y
+releases use {major-version} as version number.
+
+We use the PGP key
+https://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
+Elastic's Signing Key, with fingerprint
+
+    4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
+
+to sign all our packages. It is available from https://pgp.mit.edu.
+
+[float]
+==== APT
+
+Download and install the Public Signing Key:
+
+[source,sh]
+--------------------------------------------------
+wget -qO - https://artifacts.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
+--------------------------------------------------
+
+You may need to install the `apt-transport-https` package on Debian before proceeding:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get install apt-transport-https
+--------------------------------------------------
+
+Save the repository definition to  +/etc/apt/sources.list.d/elastic-{major-version}.list+:
+
+["source","sh",subs="attributes,callouts"]
+--------------------------------------------------
+echo "deb https://artifacts.elastic.co/packages/{major-version}/apt stable main" | sudo tee -a /etc/apt/sources.list.d/elastic-{major-version}.list
+--------------------------------------------------
+
+[WARNING]
+==================================================
+Use the `echo` method described above to add the Logstash repository.  Do not
+use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not
+provide a source package. If you have added the `deb-src` entry, you will see an
+error like the following:
+
+    Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)
+
+Just delete the `deb-src` entry from the `/etc/apt/sources.list` file and the
+installation should work as expected.
+==================================================
+
+Run `sudo apt-get update` and the repository is ready for use. You can install
+it with:
+
+[source,sh]
+--------------------------------------------------
+sudo apt-get update && sudo apt-get install logstash
+--------------------------------------------------
+
+See the <<running-logstash,Running Logstash>> document for managing Logstash as a system service.
+
+[float]
+==== YUM
+
+Download and install the public signing key:
+
+[source,sh]
+--------------------------------------------------
+rpm --import https://artifacts.elastic.co/GPG-KEY-elasticsearch
+--------------------------------------------------
+
+Add the following in your `/etc/yum.repos.d/` directory
+in a file with a `.repo` suffix, for example `logstash.repo`
+
+["source","sh",subs="attributes,callouts"]
+--------------------------------------------------
+[logstash-{major-version}]
+name=Elastic repository for {major-version} packages
+baseurl=https://artifacts.elastic.co/packages/{major-version}/yum
+gpgcheck=1
+gpgkey=https://artifacts.elastic.co/GPG-KEY-elasticsearch
+enabled=1
+autorefresh=1
+type=rpm-md
+--------------------------------------------------
+
+And your repository is ready for use. You can install it with:
+
+[source,sh]
+--------------------------------------------------
+sudo yum install logstash
+--------------------------------------------------
+
+WARNING: The repositories do not work with older rpm based distributions
+         that still use RPM v3, like CentOS5.
+
+See the <<running-logstash,Running Logstash>> document for managing Logstash as a system service.
 
 [[first-event]]
-=== Stashing Your First Event: Basic Logstash Example
+=== Stashing Your First Event
+
+First, let's test your Logstash installation by running the most basic _Logstash pipeline_.
+
+A Logstash pipeline has two required elements, `input` and `output`, and one optional element, `filter`. The input
+plugins consume data from a source, the filter plugins modify the data as you specify, and the output plugins write
+the data to a destination.
+
+//TODO: REPLACE WITH NEW IMAGE
+
+image::static/images/basic_logstash_pipeline.png[]
 
 To test your Logstash installation, run the most basic Logstash pipeline:
 
-[source,shell]
+["source","sh",subs="attributes"]
+--------------------------------------------------
 cd logstash-{logstash_version}
 bin/logstash -e 'input { stdin { } } output { stdout {} }'
+--------------------------------------------------
 
 The `-e` flag enables you to specify a configuration directly from the command line. Specifying configurations at the
 command line lets you quickly test configurations without having to edit a file between iterations.
-This pipeline takes input from the standard input, `stdin`, and moves that input to the standard output, `stdout`, in a
-structured format.
+The pipeline in the example takes input from the standard input, `stdin`, and moves that input to the standard output,
+`stdout`, in a structured format.
 
-Once "Logstash startup completed" is displayed, type hello world at the command prompt to see Logstash respond:
+After starting Logstash, wait until you see "Pipeline main started" and then enter `hello world` at the command prompt:
 
 [source,shell]
 hello world
@@ -55,5 +179,5 @@ hello world
 Logstash adds timestamp and IP address information to the message. Exit Logstash by issuing a *CTRL-D* command in the
 shell where Logstash is running.
 
-The <<advanced-pipeline,Advanced Tutorial>> expands the capabilities of your Logstash instance to cover broader
-use cases.
+Congratulations! You've created and run a basic Logstash pipeline. Next, you learn how to create a more realistic pipeline.
+
diff --git a/docs/static/glossary.asciidoc b/docs/static/glossary.asciidoc
new file mode 100644
index 00000000000..0fbea1cfa36
--- /dev/null
+++ b/docs/static/glossary.asciidoc
@@ -0,0 +1,79 @@
+[[glossary]]
+== Glossary of Terms
+
+[[glossary-metadata]]@metadata ::
+  A special field for storing content that you don't want to include in output <<glossary-event,events>>. For example, the `@metadata`
+  field is useful for creating transient fields for use in <<glossary-conditional,conditional>> statements.
+    
+[[glossary-codec-plugin]]codec plugin::
+  A Logstash <<glossary-plugin,plugin>> that changes the data representation of an <<glossary-event,event>>. Codecs are essentially stream filters that can operate as part of an input or output. Codecs enable you to separate the transport of messages from the serialization process. Popular codecs include json, msgpack, and plain (text).
+  
+[[glossary-conditional]]conditional::
+  A control flow that executes certain actions based on whether a statement (also called a condition) is true or false. Logstash supports `if`, `else if`, and `else` statements. You can use conditional statements to apply filters and send events to a specific output based on conditions that you specify. 
+    
+[[glossary-event]]event::
+	A single unit of information, containing a timestamp plus additional data. An event arrives via an input, and is subsequently parsed, timestamped, and passed through the Logstash <<glossary-pipeline,pipeline>>.
+    
+[[glossary-field]]field::
+  An <<glossary-event,event>> property. For example, each event in an apache access log has properties, such as a status
+  code (200, 404), request path ("/", "index.html"), HTTP verb (GET, POST), client IP address, and so on. Logstash uses
+  the term "fields" to refer to these properties.
+  
+[[glossary-field-reference]]field reference::
+  A reference to an event <<glossary-field,field>>. This reference may appear in an output block or filter block in the
+  Logstash config file. Field references are typically wrapped in square (`[]`) brackets, for example `[fieldname]`. If
+  you are referring to a top-level field, you can omit the `[]` and simply use the field name. To refer to a nested
+  field, you specify the full path to that field: `[top-level field][nested field]`.
+
+[[glossary-filter-plugin]]filter plugin::
+  A Logstash <<glossary-plugin,plugin>> that performs intermediary processing on an <<glossary-event,event>>. Typically, filters act upon
+  event data after it has been ingested via inputs, by mutating, enriching, and/or modifying the data according to
+  configuration rules. Filters are often applied conditionally depending on the characteristics of the event. Popular
+  filter plugins include grok, mutate, drop, clone, and geoip. Filter stages are optional.
+  
+[[glossary-gem]]gem::
+  A self-contained package of code that's hosted on https://rubygems.org[RubyGems.org]. Logstash <<glossary-plugin,plugins>> are packaged as
+  Ruby Gems. You can use the Logstash <<glossary-plugin-manager,plugin manager>> to manage Logstash gems.
+  
+[[glossary-hot-thread]]hot thread::
+  A Java thread that has high CPU usage and executes for a longer than normal period of time.
+  
+[[glossary-input-plugin]]input plugin::
+  A Logstash <<glossary-plugin,plugin>> that reads <<glossary-event,event>> data from a specific source. Input plugins are the first stage in the Logstash event processing <<glossary-pipeline,pipeline>>. Popular input plugins include file, syslog, redis, and beats.
+  
+[[glossary-indexer]]indexer::
+	A Logstash instance that is tasked with interfacing with an Elasticsearch cluster in order to index <<glossary-event,event>> data.
+    
+[[glossary-message-broker]]message broker::
+  Also referred to as a _message buffer_ or _message queue_, a message broker is external software (such as Redis, Kafka, or RabbitMQ) that stores messages from the Logstash shipper instance as an intermediate store, waiting to be processed by the Logstash indexer instance.
+ 
+[[glossary-output-plugin]]output plugin::
+  A Logstash <<glossary-plugin,plugin>> that writes <<glossary-event,event>> data to a specific destination. Outputs are the final stage in
+  the event <<glossary-pipeline,pipeline>>. Popular output plugins include elasticsearch, file, graphite, and
+  statsd.  
+  
+[[glossary-pipeline]]pipeline::
+  A term used to describe the flow of <<glossary-event,events>> through the Logstash workflow. A pipeline typically consists of a series of
+  input, filter, and output stages. <<glossary-input-plugin,Input>> stages get data from a source and generate events,
+  <<glossary-filter-plugin,filter>> stages, which are optional, modify the event data, and
+  <<glossary-output-plugin,output>> stages write the data to a destination. Inputs and outputs support <<glossary-codec-plugin,codecs>> that enable you to encode or decode the data as it enters or exits the pipeline without having to use
+  a separate filter. 
+  
+[[glossary-plugin]]plugin::
+  A self-contained software package that implements one of the stages in the Logstash event processing
+  <<glossary-pipeline,pipeline>>. The list of available plugins includes <<glossary-input-plugin,input plugins>>,
+  <<glossary-output-plugin,output plugins>>, <<glossary-codec-plugin,codec plugins>>, and
+  <<glossary-filter-plugin,filter plugins>>. The plugins are implemented as Ruby <<glossary-gem,gems>> and hosted on
+  https://rubygems.org[RubyGems.org]. You define the stages of an event processing <<glossary-pipeline,pipeline>> by configuring plugins. 
+ 
+[[glossary-plugin-manager]]plugin manager::
+  Accessed via the `bin/logstash-plugin` script, the plugin manager enables you to manage the lifecycle of
+  <<glossary-plugin,plugins>> in your Logstash deployment. You can install, remove, and upgrade plugins by using the
+  plugin manager Command Line Interface (CLI).
+
+[[shipper]]shipper::
+	An instance of Logstash that send events to another instance of Logstash, or some other application.
+    
+[[worker]]worker::
+	The filter thread model used by Logstash, where each worker receives an <<glossary-event,event>> and applies all filters, in order, before emitting the event to the output queue. This allows scalability across CPUs because many filters are CPU intensive.
+
diff --git a/docs/static/images/basic_logstash_pipeline.png b/docs/static/images/basic_logstash_pipeline.png
index d1b31401a49..61341fc68ac 100644
Binary files a/docs/static/images/basic_logstash_pipeline.png and b/docs/static/images/basic_logstash_pipeline.png differ
diff --git a/docs/static/images/deploy_3.png b/docs/static/images/deploy_3.png
index cda4337fa9d..96bc119c3e0 100644
Binary files a/docs/static/images/deploy_3.png and b/docs/static/images/deploy_3.png differ
diff --git a/docs/static/images/kibana-filebeat-data.png b/docs/static/images/kibana-filebeat-data.png
new file mode 100644
index 00000000000..b3c2d62a068
Binary files /dev/null and b/docs/static/images/kibana-filebeat-data.png differ
diff --git a/docs/static/include/pluginbody.asciidoc b/docs/static/include/pluginbody.asciidoc
index a51c4f74aa9..81274cca0e2 100644
--- a/docs/static/include/pluginbody.asciidoc
+++ b/docs/static/include/pluginbody.asciidoc
@@ -275,40 +275,31 @@ require "logstash/namespace"
 class LogStash::{pluginclass}::{pluginnamecap} < LogStash::{pluginclass}::Base
   config_name "example"
 
-  # If declared logstash will only allow a single instance of this plugin
-  # to exist, regardless of how many CPU cores logstash detects. This is best
-  # used in cases like the File output, where separate threads writing to a single
-  # File would only cause problems.
+  # This sets the concurrency behavior of this plugin. By default it is :legacy, which was the standard
+  # way concurrency worked before Logstash 2.4
+  # 
+  # You should explicitly set it to either :single or :shared as :legacy will be removed in Logstash 6.0
+  # 
+  # When configured as :single a single instance of the Output will be shared among the
+  # pipeline worker threads. Access to the `#multi_receive/#multi_receive_encoded/#receive` method will be synchronized
+  # i.e. only one thread will be active at a time making threadsafety much simpler.
+  # 
+  # You can set this to :shared if your output is threadsafe. This will maximize
+  # concurrency but you will need to make appropriate uses of mutexes in `#multi_receive/#receive`.
   #
-  # respond_to? check needed for backwards compatibility with < 2.2 Logstashes
-  declare_workers_not_supported! if self.respond_to?(:declare_workers_not_supported!)
-
-  # If declared threadsafe logstash will only ever create one
-  # instance of this plugin per pipeline.
-  # That instance will be shared across all workers
-  # It is up to the plugin author to correctly write concurrent code!
-  #
-  # respond_to? check needed for backwards compatibility with < 2.2 Logstashes
-  declare_threadsafe! if self.respond_to?(:declare_threadsafe!)
-
+  # Only the `#multi_receive/#multi_receive_encoded` methods need to actually be threadsafe, the other methods
+  # will only be executed in a single thread
+  concurrency :single
+  
   public
-  def register
-    # Does the same thing as declare_workers_not_supported!
-    # But works in < 2.2 logstashes
-    # workers_not_supported
+  def register    
   end # def register
 
   public
   # Takes an array of events
+  # Must be threadsafe if `concurrency :shared` is set
   def multi_receive(events)
   end # def multi_receive
-
-  public
-  # Needed for logstash < 2.2 compatibility
-  # Takes events one at a time
-  def receive(event)
-  end # def receive
-
 end # class LogStash::{pluginclass}::{pluginnamecap}
 ----------------------------------
 endif::multi_receive_method[]
@@ -474,14 +465,14 @@ There are several configuration attributes:
 
 * `:validate` - allows you to enforce passing a particular data type to Logstash
 for this configuration option, such as `:string`, `:password`, `:boolean`,
-`:number`, `:array`, `:hash`, `:path` (a file-system path), `:codec` (since
+`:number`, `:array`, `:hash`, `:path` (a file-system path), `uri` (starting in 5.0.0), `:codec` (since
 1.2.0), `:bytes` (starting in 1.5.0).  Note that this also works as a coercion
 in that if I specify "true" for boolean (even though technically a string), it
 will become a valid boolean in the config.  This coercion works for the
 `:number` type as well where "1.2" becomes a float and "22" is an integer.
 * `:default` - lets you specify a default value for a parameter
-* `:required` - whether or not this parameter is mandatory (a Boolean `true` or
-`false`)
+* `:required` - whether or not this parameter is mandatory (a Boolean `true` or `false`)
+* `:list` - whether or not this value should be a list of values. Will typecheck the list members, and convert scalars to one element lists. Note that this mostly obviates the array type, though if you need lists of complex objects that will be more suitable. added[5.0.0-alpha4,The :list property is available for better type checking]
 * `:deprecated` - informational (also a Boolean `true` or `false`)
 * `:obsolete` - used to declare that a given setting has been removed and is no longer functioning. The idea is to provide an informed upgrade path to users who are still using a now-removed setting.
 
@@ -883,7 +874,7 @@ time.
 
 **Version messaging from Logstash**
 
-If you start Logstash with the `--verbose` flag, you will see messages like
+If you start Logstash with the `--log.level info` flag, you will see messages like
 these to indicate the relative maturity indicated by the plugin version number:
 
 ** **0.1.x**
diff --git a/docs/static/introduction.asciidoc b/docs/static/introduction.asciidoc
index 9b69959d6a6..597e5b41c58 100644
--- a/docs/static/introduction.asciidoc
+++ b/docs/static/introduction.asciidoc
@@ -27,7 +27,7 @@ Collect more, so you can know more. Logstash welcomes data of all shapes and siz
 Where it all started.
 
 * Handle all types of logging data
-** Easily ingest a multitude of web logs like <<parsing-into-es,Apache>>, and application
+** Easily ingest a multitude of web logs like <<advanced-pipeline,Apache>>, and application
 logs like <<plugins-inputs-log4j,log4j>> for Java
 ** Capture many other log formats like <<plugins-inputs-syslog,syslog>>,
 <<plugins-inputs-eventlog,Windows event logs>>, networking and firewall logs, and more
diff --git a/docs/static/life-of-an-event.asciidoc b/docs/static/life-of-an-event.asciidoc
index 3a5f72055c6..d1431d116b6 100644
--- a/docs/static/life-of-an-event.asciidoc
+++ b/docs/static/life-of-an-event.asciidoc
@@ -1,5 +1,5 @@
 [[pipeline]]
-=== Logstash Processing Pipeline
+== How Logstash Works
 
 The Logstash event processing pipeline has three stages: inputs -> filters ->
 outputs. Inputs generate events, filters modify them, and outputs ship them
@@ -8,7 +8,7 @@ the data as it enters or exits the pipeline without having to use a separate
 filter.
 
 [float]
-==== Inputs
+=== Inputs
 You use inputs to get data into Logstash. Some of the more commonly-used inputs
 are:
 
@@ -25,7 +25,7 @@ For more information about the available inputs, see
 <<input-plugins,Input Plugins>>.
 
 [float]
-==== Filters
+=== Filters
 Filters are intermediary processing devices in the Logstash pipeline. You can
 combine filters with conditionals to perform an action on an event if it meets
 certain criteria. Some useful filters include:
@@ -45,7 +45,7 @@ For more information about the available filters, see
 <<filter-plugins,Filter Plugins>>.
 
 [float]
-==== Outputs
+=== Outputs
 Outputs are the final phase of the Logstash pipeline. An event can pass through
 multiple outputs, but once all output processing is complete, the event has
 finished its execution. Some commonly used outputs include:
@@ -55,7 +55,7 @@ your data in an efficient, convenient, and easily queryable format...
 Elasticsearch is the way to go. Period. Yes, we're biased :)
 * *file*: write event data to a file on disk.
 * *graphite*: send event data to graphite, a popular open source tool for
-storing and graphing metrics. http://graphite.wikidot.com/
+storing and graphing metrics. http://graphite.readthedocs.io/en/latest/
 * *statsd*: send event data to statsd, a service that "listens for statistics,
 like counters and timers, sent over UDP and sends aggregates to one or more
 pluggable backend services". If you're already using statsd, this could be
@@ -65,7 +65,7 @@ For more information about the available outputs, see
 <<output-plugins,Output Plugins>>.
 
 [float]
-==== Codecs
+=== Codecs
 Codecs are basically stream filters that can operate as part of an input or
 output. Codecs enable you to easily separate the transport of your messages from
 the serialization process. Popular codecs include `json`, `msgpack`, and `plain`
@@ -78,16 +78,16 @@ stacktrace messages into a single event.
 For more information about the available codecs, see
 <<codec-plugins,Codec Plugins>>.
 
-[float]
+[[fault-tolerance]]
 === Fault Tolerance
 
 Logstash keeps all events in main memory during processing. Logstash responds to a SIGTERM by attempting to halt inputs and waiting for pending events to finish processing before shutting down. When the pipeline cannot be flushed due to a stuck output or filter, Logstash waits indefinitely. For example, when a pipeline sends output to a database that is unreachable by the Logstash instance, the instance waits indefinitely after receiving a SIGTERM.
 
-To enable Logstash to detect these situations and terminate with a stalled pipeline, use the `--allow-unsafe-shutdown` flag.
+To enable Logstash to detect these situations and terminate with a stalled pipeline, use the `--pipeline.unsafe_shutdown` flag.
 
 WARNING: Unsafe shutdowns, force-kills of the Logstash process, or crashes of the Logstash process for any other reason result in data loss. Shut down Logstash safely whenever possible.
 
-[float]
+[[execution-model]]
 ==== Execution Model
 
 The Logstash pipeline coordinates the execution of inputs, filters, and outputs. The following schematic sketches the data flow of a pipeline:
@@ -119,12 +119,12 @@ num_pipeline_workers.times do
 end
 wait_for_threads_to_terminate()
 
-There are three configurable options in the pipeline, `--pipeline-workers`, `--pipeline-batch-size`, and `--pipeline-batch-delay`.
-The `--pipeline-workers` or `-w` parameter determines how many threads to run for filter and output processing. If you find that events are backing up, or that the CPU is not saturated, consider increasing the value of this parameter to make better use of available processing power. Good results can even be found increasing this number past the number of available processors as these threads may spend significant time in an I/O wait state when writing to external systems. Legal values for this parameter are positive integers.
+There are three configurable options in the pipeline, `--pipeline.workers`, `--pipeline.batch.size`, and `--pipeline.batch.delay`.
+The `--pipeline.workers` or `-w` parameter determines how many threads to run for filter and output processing. If you find that events are backing up, or that the CPU is not saturated, consider increasing the value of this parameter to make better use of available processing power. Good results can even be found increasing this number past the number of available processors as these threads may spend significant time in an I/O wait state when writing to external systems. Legal values for this parameter are positive integers.
 
-The `--pipeline-batch-size` or `-b` parameter defines the maximum number of events an individual worker thread collects before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Some hardware configurations require you to increase JVM heap size by setting the `LS_HEAP_SIZE` variable to avoid performance degradation with this option. Values of this parameter in excess of the optimum range cause performance degradation due to frequent garbage collection or JVM crashes related to out-of-memory exceptions. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, issues https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[bulk requests] for each batch received. Tuning the `-b` parameter adjusts the size of bulk requests sent to Elasticsearch.
+The `--pipeline.batch.size` or `-b` parameter defines the maximum number of events an individual worker thread collects before attempting to execute filters and outputs. Larger batch sizes are generally more efficient, but increase memory overhead. Some hardware configurations require you to increase JVM heap size by setting the `LS_HEAP_SIZE` variable to avoid performance degradation with this option. Values of this parameter in excess of the optimum range cause performance degradation due to frequent garbage collection or JVM crashes related to out-of-memory exceptions. Output plugins can process each batch as a logical unit. The Elasticsearch output, for example, issues https://www.elastic.co/guide/en/elasticsearch/reference/current/docs-bulk.html[bulk requests] for each batch received. Tuning the `-b` parameter adjusts the size of bulk requests sent to Elasticsearch.
 
-The `--pipeline-batch-delay` option rarely needs to be tuned. This option adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that Logstash waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, Logstash beings to execute filters and outputs.The maximum time that Logstash waits between receiving an event and processing that event in a filter is the product of the `pipeline_batch_delay` and  `pipeline_batch_size` settings.
+The `--pipeline.batch.delay` option rarely needs to be tuned. This option adjusts the latency of the Logstash pipeline. Pipeline batch delay is the maximum amount of time in milliseconds that Logstash waits for new messages after receiving an event in the current pipeline worker thread. After this time elapses, Logstash begins to execute filters and outputs.The maximum time that Logstash waits between receiving an event and processing that event in a filter is the product of the `pipeline_batch_delay` and  `pipeline_batch_size` settings.
 
 [float]
 ==== Notes on Pipeline Configuration and Performance
@@ -156,3 +156,4 @@ Examining the in-depth GC statistics with a tool similar to the excellent https:
 
 NOTE: As long as the GC pattern is acceptable, heap sizes that occasionally increase to the maximum are acceptable. Such heap size spikes happen in response to a burst of large events passing through the pipeline. In general practice, maintain a gap between the used amount of heap memory and the maximum.
 This document is not a comprehensive guide to JVM GC tuning. Read the official http://www.oracle.com/webfolder/technetwork/tutorials/obe/java/gc01/index.html[Oracle guide] for more information on the topic. We also recommend reading http://www.semicomplete.com/blog/geekery/debugging-java-performance.html[Debugging Java Performance].
+
diff --git a/docs/static/logging.asciidoc b/docs/static/logging.asciidoc
new file mode 100644
index 00000000000..bc37bd3219c
--- /dev/null
+++ b/docs/static/logging.asciidoc
@@ -0,0 +1,69 @@
+[[logging]]
+=== Logging
+
+Logstash emits internal logs during its operation, which are placed in `LS_HOME/logs`. The default logging level is `INFO`. 
+Logstash's logging framework is based on http://logging.apache.org/log4j/2.x/[Log4j2 framework], and much of its functionality 
+is exposed directly to users.
+
+When debugging problems, particularly problems with plugins, it can be helpful to increase the logging level to `DEBUG` 
+to emit more verbose messages. Previously, you could only set a log level that applied to the entire Logstash product. 
+Starting with 5.0, you can configure logging for a particular subsystem in Logstash. For example, if you are 
+debugging issues with Elasticsearch Output, you can increase log levels just for that component. This way 
+you can reduce noise due to excessive logging and focus on the problem area effectively.
+
+==== Log file location
+
+You can specify the log file location using `--path.logs` setting.
+
+==== Log4j2 Configuration
+
+Logstash ships with a `log4j2.properties` file with out-of-the-box settings. You  can modify this file directly to change the 
+rotation policy, type, and other https://logging.apache.org/log4j/2.x/manual/configuration.html#Loggers[log4j2 configuration]. 
+You must restart Lostash to apply any changes that you make to this file.
+
+==== Logging APIs
+
+You could modify the `log4j2.properties` file and restart your Logstash, but that is both tedious and leads to unnecessary 
+downtime. Instead, you can dynamically update logging levels through the logging API. These settings are effective 
+immediately and do not need a restart. To update logging levels, take the subsystem/module you are interested in and prepend 
+`logger.` to it. For example:
+
+[source,js]
+--------------------------------------------------
+PUT /_node/logging
+{
+    "logger.logstash.outputs.elasticsearch" : "DEBUG"
+}
+--------------------------------------------------
+
+While this setting is in effect, Logstash will begin to emit DEBUG-level logs for __all__ the Elasticsearch outputs 
+specified in your configuration. Please note this new setting is transient and will not survive a restart.
+
+To retrieve a list of logging subsystems available at runtime, you can do a `GET` request to `_node/logging`
+
+[source,js]
+--------------------------------------------------
+GET /_node/logging?pretty
+--------------------------------------------------
+
+Example response:
+
+["source","js"]
+--------------------------------------------------
+{
+...
+"loggers" : {
+   "logstash.registry" : "WARN",
+   "logstash.instrument.periodicpoller.os" : "WARN",
+   "logstash.instrument.collector" : "WARN",
+   "logstash.runner" : "WARN",
+   "logstash.inputs.stdin" : "WARN",
+   "logstash.outputs.stdout" : "WARN",
+   "logstash.agent" : "WARN",
+   "logstash.api.service" : "WARN",
+   "logstash.instrument.periodicpoller.jvm" : "WARN",
+   "logstash.pipeline" : "WARN",
+   "logstash.codecs.line" : "WARN"
+   }
+}
+--------------------------------------------------
diff --git a/docs/static/logstash-docs-home.asciidoc b/docs/static/logstash-docs-home.asciidoc
deleted file mode 100644
index 19bd3281184..00000000000
--- a/docs/static/logstash-docs-home.asciidoc
+++ /dev/null
@@ -1,30 +0,0 @@
-[[logstash-docs-home]]
-== Logstash Documentation
-Pretty self-explanatory, really
-
-=== Downloads and Releases
-* http://www.elasticsearch.org/overview/logstash/download/[Download Logstash 1.4.2]
-* http://www.elasticsearch.org/blog/apt-and-yum-repositories/[package repositories]
-* http://www.elasticsearch.org/blog/logstash-1-4-2/[release notes]
-* https://github.com/elasticsearch/logstash/blob/master/CHANGELOG[view changelog]
-* https://github.com/elasticsearch/puppet-logstash[Puppet Module]
-
-=== Plugins
-* http://elasticsearch.org/#[contrib plugins]
-* http://elasticsearch.org/#[writing your own plugins]
-* http://elasticsearch.org/#[Inputs] / http://elasticsearch.org/#[Filters] / http://elasticsearch.org/#[Outputs]
-* http://elasticsearch.org/#[Codecs]
-* http://elasticsearch.org/#[(more)]
-
-=== HOWTOs, References, Information
-* http://elasticsearch.org/#[Getting Started with Logstash]
-* http://elasticsearch.org/#[Configuration file overview]
-* http://elasticsearch.org/#[Command-line flags]
-* http://elasticsearch.org/#[The life of an event in Logstash]
-* http://elasticsearch.org/#[Using conditional logic]
-* http://elasticsearch.org/#[Glossary]
-* http://elasticsearch.org/#[(more)]
-
-=== About / Videos / Blogs
-* http://elasticsearch.org/#[Videos]
-* http://elasticsearch.org/#[Blogs]
diff --git a/docs/static/maintainer-guide.asciidoc b/docs/static/maintainer-guide.asciidoc
index 5c8253ca8e3..7d3e0c9a029 100644
--- a/docs/static/maintainer-guide.asciidoc
+++ b/docs/static/maintainer-guide.asciidoc
@@ -1,15 +1,17 @@
 [[community-maintainer]]
-== Logstash Plugins Community Maintainer Guide
+=== Logstash Plugins Community Maintainer Guide
 
 This document, to be read by new Maintainers, should explain their responsibilities.  It was inspired by the
 http://rfc.zeromq.org/spec:22[C4] document from the ZeroMQ project.  This document is subject to change and suggestions
 through Pull Requests and issues are strongly encouraged.
 
+[float]
 === Contribution Guidelines
 
 For general guidance around contributing to Logstash Plugins, see the
 https://www.elastic.co/guide/en/logstash/current/contributing-to-logstash.html[_Contributing to Logstash_] section.
 
+[float]
 === Document Goals
 
 To help make the Logstash plugins community participation easy with positive feedback.
@@ -24,6 +26,7 @@ To support the natural life cycle of a plugin.
 To codify the roles and responsibilities of: Maintainers and Contributors with specific focus on patch testing, code
 review, merging and release.
 
+[float]
 === Development Workflow
 
 All Issues and Pull Requests must be tracked using the Github issue tracker.
@@ -32,6 +35,7 @@ The plugin uses the http://www.apache.org/licenses/LICENSE-2.0[Apache 2.0 licens
 patch introduces code which has an incompatible license. Patch ownership and copyright is defined in the Elastic
 https://www.elastic.co/contributor-agreement[Contributor License Agreement] (CLA).
 
+[float]
 ==== Terminology
 
 A "Contributor" is a role a person assumes when providing a patch. Contributors will not have commit access to the
@@ -41,6 +45,7 @@ before a patch can be reviewed. Contributors can add themselves to the plugin Co
 A "Maintainer" is a role a person assumes when maintaining a plugin and keeping it healthy, including triaging issues, and
 reviewing and merging patches.
 
+[float]
 ==== Patch Requirements
 
 A patch is a minimal and accurate answer to exactly one identified and agreed upon problem. It must conform to the
@@ -57,6 +62,7 @@ and any additional lines as necessary for change explanation and rationale.
 A patch is mergeable when it satisfies the above requirements and has been reviewed positively by at least one other
 person.
 
+[float]
 ==== Development Process
 
 A user will log an issue on the issue tracker describing the problem they face or observe with as much detail as possible.
@@ -79,26 +85,61 @@ Maintainers should involve the core team if help is needed to reach consensus.
 
 Review non-source changes such as documentation in the same way as source code changes.
 
+[float]
 ==== Branch Management
 
 The plugin has a master branch that always holds the latest in-progress version and should always build.  Topic branches
 should kept to the minimum.
 
+[float]
 ==== Changelog Management
 
 Every plugin should have a changelog (CHANGELOG.md).  If not, please create one.  When changes are made to a plugin, make sure to include a changelog entry under the respective version to document the change.  The changelog should be easily understood from a user point of view.  As we iterate and release plugins rapidly, users use the changelog as a mechanism for deciding whether to update.
 
-Changes that are not user facing should be tagged as “Internal: ”.  For example:
-
-* "Internal: Refactored specs for better testing"
-* "Default timeout configuration changed from 10s to 5s"
-
+Changes that are not user facing should be tagged as `internal:`.  For example:
+
+[source,markdown]
+ - internal: Refactored specs for better testing
+ - config: Default timeout configuration changed from 10s to 5s
+
+[float]
+===== Detailed format of CHANGELOG.md
+
+Sharing a similar format of CHANGELOG.md in plugins ease readability for users.
+Please see following annotated example and see a concrete example in https://raw.githubusercontent.com/logstash-plugins/logstash-filter-date/master/CHANGELOG.md[logstash-filter-date].
+
+[source,markdown]
+----
+## 1.0.x                              // <1> <2>
+ - change description                 // <3>
+ - tag: change description            // <3> <4>
+ - tag1,tag2: change description      // <3> <5>
+ - tag: Multi-line description        // <3> <6>
+   must be indented and can use
+   additional markdown syntax
+                                      // <7>
+## 1.0.0                              // <8>
+[...]
+
+----
+<1> Latest version is the first line of CHANGELOG.md
+<2> Each version identifier should be a level-2 header using `##`
+<3> One change description is described as a list item using a dash `-` aligned under the version identifier
+<4> One change can be tagged by a word and suffixed by `:`. +
+    Common tags are `bugfix`, `feature`, `doc`, `test` or `internal`.
+<5> One change can have multiple tags separated by a comma and suffixed by `:`
+<6> A multi-line change description must be properly indented
+<7> Please take care to *separate versions with an empty line*
+<8> Previous version identifier
+
+[float]
 ==== Continuous Integration
 
 Plugins are setup with automated continuous integration (CI) environments and there should be a corresponding badge on each Github page.  If it’s missing, please contact the Logstash core team.
 
 Every Pull Request opened automatically triggers a CI run.  To conduct a manual run, comment “Jenkins, please test this.” on the Pull Request.
 
+[float]
 === Versioning Plugins
 
 Logstash core and its plugins have separate product development lifecycles. Hence the versioning and release strategy for
@@ -115,11 +156,13 @@ Plugin releases follows a three-placed numbering scheme X.Y.Z. where X denotes a
 compatibility with existing configuration or functionality. Y denotes releases which includes features which are backward
 compatible. Z denotes releases which includes bug fixes and patches.
 
+[float]
 ==== Changing the version
 
 Version can be changed in the Gemspec, which needs to be associated with a changelog entry. Following this, we can publish
 the gem to RubyGem.org manually. At this point only the core developers can publish a gem.
 
+[float]
 ==== Labeling
 
 Labeling is a critical aspect of maintaining plugins. All issues in GitHub should be labeled correctly so it can:
@@ -139,6 +182,7 @@ details.
 * `needs tests`: Patch has no tests, and cannot be accepted without unit/integration tests
 * `docs`: Documentation related issue/PR
 
+[float]
 === Logging
 
 Although it’s important not to bog down performance with excessive logging, debug level logs can be immensely helpful when
@@ -148,6 +192,7 @@ as users will be forever gracious.
 [source,shell]
 @logger.debug("Logstash loves debug logs!", :actions => actions)
 
+[float]
 === Contributor License Agreement (CLA) Guidance
 
 [qanda]
@@ -161,10 +206,12 @@ Please make sure the CLA is signed by every Contributor prior to reviewing PRs a
      signs the CLA after a PR is submitted, they can refresh the automated CLA checker by pushing another
      comment on the PR after 5 minutes of signing.
 
+[float]
 === Need Help?
 
 Ping @logstash-core on Github to get the attention of the Logstash core team.
 
+[float]
 === Community Administration
 
 The core team is there to support the plugin Maintainers and overall ecosystem.
diff --git a/docs/static/managing-multiline-events.asciidoc b/docs/static/managing-multiline-events.asciidoc
index 7245b3c50c1..e9ee8bc12fa 100644
--- a/docs/static/managing-multiline-events.asciidoc
+++ b/docs/static/managing-multiline-events.asciidoc
@@ -1,7 +1,7 @@
 [[multiline]]
 === Managing Multiline Events
 
-Several use cases generate events that span multiple lines of text. In order to correctly handle these multline events,
+Several use cases generate events that span multiple lines of text. In order to correctly handle these multiline events,
 Logstash needs to know how to tell which lines are part of a single event.
 
 Multiline event processing is complex and relies on proper event ordering. The best way to guarantee ordered log
diff --git a/docs/static/monitoring-apis.asciidoc b/docs/static/monitoring-apis.asciidoc
index 97b2a57c3c4..406736be8fd 100644
--- a/docs/static/monitoring-apis.asciidoc
+++ b/docs/static/monitoring-apis.asciidoc
@@ -1,13 +1,39 @@
 [[monitoring]]
 == Monitoring APIs
 
+experimental[]
+
 Logstash provides the following monitoring APIs to retrieve runtime metrics
 about Logstash:
 
-* <<root-resource-api>>
-* <<stats-info-api>>
-* <<hot-threads-api>>
+* <<node-info-api>>
 * <<plugins-api>>
+* <<node-stats-api>>
+* <<hot-threads-api>>
+
+
+You can use the root resource to retrieve general information about the Logstash instance, including
+the host and version.
+
+[source,js]
+--------------------------------------------------
+GET /
+--------------------------------------------------
+
+Example response:
+
+["source","js",subs="attributes"]
+--------------------------------------------------
+{
+   "host": "skywalker",
+   "version": "{logstash_version}",
+   "http_address": "127.0.0.1:9600"
+}
+--------------------------------------------------
+
+NOTE: By default, the monitoring API attempts to bind to `tcp:9600`. If this port is already in use by another Logstash
+instance, you need to launch Logstash with the `--http.port` flag specified to bind to a different port. See
+<<command-line-flags>> for more information.
 
 [float]
 [[monitoring-common-options]]
@@ -19,14 +45,12 @@ The following options can be applied to all of the Logstash monitoring APIs.
 ==== Pretty Results
 
 When appending `?pretty=true` to any request made, the JSON returned
-will be pretty formatted (use it for debugging only!). Another option is
-to set `?format=yaml` which will cause the result to be returned in the
-(sometimes) more readable yaml format.
+will be pretty formatted (use it for debugging only!).
 
 [float]
 ==== Human-Readable Output
 
-NOTE: For Alpha 1, the `human` option is supported for the <<hot-threads-api>>
+NOTE: For Logstash {logstash_version}, the `human` option is supported for the <<hot-threads-api>>
 only. When you specify `human=true`, the results are returned in plain text instead of
 JSON format. The default is false.
 
@@ -39,50 +63,66 @@ being consumed by a monitoring tool, rather than intended for human
 consumption.  The default for the `human` flag is
 `false`.
 
-[[plugins-api]]
-=== Plugins API
+[[node-info-api]]
+=== Node Info API
 
 experimental[]
 
-The plugins API gets information about all Logstash plugins that are currently installed.
-This API basically returns the output of running the `bin/logstash-plugin list --verbose` command.
+The node info API retrieves information about the node.
 
 [source,js]
 --------------------------------------------------
-GET /_plugins
+GET /_node/<types>
 --------------------------------------------------
 
-The output is a JSON document.
+Where `<types>` is optional and specifies the types of node info you want to return.
 
-Example response:
+You can limit the info that's returned by combining any of the following types in a comma-separated list:
+
+[horizontal]
+`pipeline`::
+Gets pipeline-specific information and settings.
+`os`::
+Gets node-level info about the OS.
+`jvm`::
+Gets node-level JVM info, including info about threads.
+
+==== Pipeline Info
+
+The following request returns a JSON document that shows pipeline info, such as the number of workers,
+batch size, and batch delay:
 
 [source,js]
 --------------------------------------------------
-"total": 102
-"plugins" : [
-  {
-      "name": "logstash-output-pagerduty"
-      "version": "2.0.2"
-  },
-  {
-      "name": "logstash-output-elasticsearch"
-      "version": "2.1.2"
-  }
-....
-] 
+GET /_node/pipeline
 --------------------------------------------------
 
-[[root-resource-api]]
-=== Root Resource API
+If you want to view additional information about the pipeline, such as stats for each configured input, filter,
+or output stage, see the <<pipeline-stats>> section under the <<node-stats-api>>.
 
-experimental[]
+Example response:
+
+["source","js",subs="attributes"]
+--------------------------------------------------
+{
+  "pipeline": {
+    "workers": 8,
+    "batch_size": 125,
+    "batch_delay": 5,
+    "config_reload_automatic": true,
+    "config_reload_interval": 3
+
+  }
+--------------------------------------------------
+
+==== OS Info
 
-The root resource API retrieves general information about the Logstash instance, including
-the host name and version information.
+The following request returns a JSON document that shows the OS name, architecture, version, and
+available processors:
 
 [source,js]
 --------------------------------------------------
-GET /
+GET /_node/os
 --------------------------------------------------
 
 Example response:
@@ -90,56 +130,122 @@ Example response:
 [source,js]
 --------------------------------------------------
 {
-   "hostname": "skywalker",
-    "version" : {
-        "number" : "2.1.0",       
-    }
+  "os": {
+    "name": "Mac OS X",
+    "arch": "x86_64",
+    "version": "10.11.2",
+    "available_processors": 8
   }
 --------------------------------------------------
 
+==== JVM Info
 
-See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
-Logstash monitoring APIs.
+The following request returns a JSON document that shows node-level JVM stats, such as the JVM process id, version,
+VM info, memory usage, and info about garbage collectors:
+
+[source,js]
+--------------------------------------------------
+GET /_node/jvm
+--------------------------------------------------
 
-[[stats-info-api]]
-=== Stats Info API
+Example response:
 
-experimental[]
+[source,js]
+--------------------------------------------------
+{
+  "jvm": {
+    "pid": 8187,
+    "version": "1.8.0_65",
+    "vm_name": "Java HotSpot(TM) 64-Bit Server VM",
+    "vm_version": "1.8.0_65",
+    "vm_vendor": "Oracle Corporation",
+    "start_time_in_millis": 1474305161631,
+    "mem": {
+      "heap_init_in_bytes": 268435456,
+      "heap_max_in_bytes": 1037959168,
+      "non_heap_init_in_bytes": 2555904,
+      "non_heap_max_in_bytes": 0
+    },
+    "gc_collectors": [
+      "ParNew",
+      "ConcurrentMarkSweep"
+    ]
+  }
+--------------------------------------------------
 
-The stats info API retrieves runtime stats about Logstash. 
+[[plugins-api]]
+=== Plugins Info API
 
-// COMMENTED OUT until Logstash supports multiple pipelines: To retrieve all stats for the Logstash instance, use the `_node/stats` endpoint:
+experimental[]
+
+The plugins info API gets information about all Logstash plugins that are currently installed.
+This API basically returns the output of running the `bin/logstash-plugin list --verbose` command.
 
 [source,js]
 --------------------------------------------------
-GET /_node/stats/<types>
+GET /_node/plugins
+--------------------------------------------------
+
+The output is a JSON document.
+
+Example response:
+
+["source","js",subs="attributes"]
+--------------------------------------------------
+{
+  "total": 91,
+  "plugins": [
+    {
+      "name": "logstash-codec-collectd",
+      "version": "3.0.2"
+    },
+    {
+      "name": "logstash-codec-dots",
+      "version": "3.0.2"
+    },
+    {
+      "name": "logstash-codec-edn",
+      "version": "3.0.2"
+    },
+    .
+    .
+    .
+  ]
 --------------------------------------------------
 
-////
-COMMENTED OUT until Logstash supports multiple pipelines: To retrieve all stats per pipeline, use the `_pipelines/stats` endpoint:
+[[node-stats-api]]
+=== Node Stats API
+
+experimental[]
+
+The node stats API retrieves runtime stats about Logstash.
 
 [source,js]
 --------------------------------------------------
-GET /_pipelines/stats/<types>
+GET /_node/stats/<types>
 --------------------------------------------------
-////
 
 Where `<types>` is optional and specifies the types of stats you want to return.
 
-By default, all stats are returned. You can limit this by combining any of the following types: 
+By default, all stats are returned. You can limit the info that's returned by combining any of the following types in a comma-separated list:
 
 [horizontal]
-`events`::
-	Gets event information since startup. 
 `jvm`::
-	Gets JVM stats, including stats about garbage collection. 
+Gets JVM stats, including stats about threads, memory usage, and garbage collectors.
+`process`::
+Gets process stats, including stats about file descriptors, memory consumption, and CPU usage.
+`mem`::
+Gets memory usage stats.
+`pipeline`::
+Gets runtime stats about the Logstash pipeline.
+
+==== JVM Stats
 
-For example, the following request returns a JSON document that shows the number of events
-that were input, filtered, and output by Logstash since startup:
+The following request returns a JSON document containing JVM stats:
 
 [source,js]
 --------------------------------------------------
-GET /_node/stats/events
+GET /_node/stats/jvm
 --------------------------------------------------
 
 Example response:
@@ -147,61 +253,160 @@ Example response:
 [source,js]
 --------------------------------------------------
 {
-    "events": {
-        "in": 91,
-        "filtered": 91,
-        "out": 91
+  "jvm": {
+    "threads": {
+      "count": 33,
+      "peak_count": 34
+    },
+    "mem": {
+      "heap_used_in_bytes": 276974824,
+      "heap_used_percent": 13,
+      "heap_committed_in_bytes": 519045120,
+      "heap_max_in_bytes": 2075918336,
+      "non_heap_used_in_bytes": 182122272,
+      "non_heap_committed_in_bytes": 193372160,
+      "pools": {
+        "survivor": {
+          "peak_used_in_bytes": 8912896,
+          "used_in_bytes": 11182152,
+          "peak_max_in_bytes": 35782656,
+          "max_in_bytes": 71565312,
+          "committed_in_bytes": 17825792
+        },
+        "old": {
+          "peak_used_in_bytes": 111736080,
+          "used_in_bytes": 171282544,
+          "peak_max_in_bytes": 715849728,
+          "max_in_bytes": 1431699456,
+          "committed_in_bytes": 357957632
+        },
+        "young": {
+          "peak_used_in_bytes": 71630848,
+          "used_in_bytes": 94510128,
+          "peak_max_in_bytes": 286326784,
+          "max_in_bytes": 572653568,
+          "committed_in_bytes": 143261696
+        }
+      }
+    },
+    "gc": {
+      "collectors": {
+        "old": {
+          "collection_time_in_millis": 48,
+          "collection_count": 2
+        },
+        "young": {
+          "collection_time_in_millis": 316,
+          "collection_count": 23
+        }
+      }
     }
-}
+  }
 --------------------------------------------------
 
-The following request returns a JSON document containing JVM stats:
+==== Process Stats
+
+The following request returns a JSON document containing process stats:
 
 [source,js]
 --------------------------------------------------
-GET /_node/stats/jvm
+GET /_node/stats/process
 --------------------------------------------------
 
 Example response:
 
 [source,js]
 --------------------------------------------------
-"jvm":{  
-   "timestamp":1453233447702,
-   "uptime_in_millis":211125811,
-   "mem":{  
-      "heap_used_in_bytes":58442576,
-      "heap_used_percent":5,
-      "heap_committed_in_bytes":259522560,
-      "heap_max_in_bytes":1037959168,
-      "non_heap_used_in_bytes":56332256,
-      "non_heap_committed_in_bytes":57475072,
-      "pools":{  
-         "young":{  
-            "used_in_bytes":41672000,
-            "max_in_bytes":286326784,
-            "peak_used_in_bytes":71630848,
-            "peak_max_in_bytes":286326784
-         },
-         "survivor":{  
-            "used_in_bytes":260552,
-            "max_in_bytes":35782656,
-            "peak_used_in_bytes":8912896,
-            "peak_max_in_bytes":35782656
-         },
-         "old":{  
-            "used_in_bytes":16510024,
-            "max_in_bytes":715849728,
-            "peak_used_in_bytes":16510024,
-            "peak_max_in_bytes":715849728
-         }
-      }
-   }
+{
+  "process": {
+    "open_file_descriptors": 60,
+    "peak_open_file_descriptors": 65,
+    "max_file_descriptors": 10240,
+    "mem": {
+      "total_virtual_in_bytes": 5364461568
+    },
+    "cpu": {
+      "total_in_millis": 101294404000,
+      "percent": 0
+    }
+  }
+--------------------------------------------------
+
+[[pipeline-stats]]
+==== Pipeline Stats
+
+The following request returns a JSON document containing pipeline stats, including the number of events that were
+input, filtered, or output by the pipeline. The request also returns stats for each configured input, filter, or
+output stage, and info about whether config reload (if configured) failed or succeeded.
+
+[source,js]
+--------------------------------------------------
+GET /_node/stats/pipeline
+--------------------------------------------------
+
+Example response:
+
+[source,js]
+--------------------------------------------------
+{
+  "pipeline": {
+    "events": {
+      "duration_in_millis": 7863504,
+      "in": 100,
+      "filtered": 100,
+      "out": 100
+    },
+    "plugins": {
+      "inputs": [],
+      "filters": [
+        {
+          "id": "grok_20e5cb7f7c9e712ef9750edf94aefb465e3e361b-2",
+          "events": {
+            "duration_in_millis": 48,
+            "in": 100,
+            "out": 100
+          },
+          "matches": 100,
+          "patterns_per_field": {
+            "message": 1
+          },
+          "name": "grok"
+        },
+        {
+          "id": "geoip_20e5cb7f7c9e712ef9750edf94aefb465e3e361b-3",
+          "events": {
+            "duration_in_millis": 141,
+            "in": 100,
+            "out": 100
+          },
+          "name": "geoip"
+        }
+      ],
+      "outputs": [
+        {
+          "id": "20e5cb7f7c9e712ef9750edf94aefb465e3e361b-4",
+          "events": {
+            "in": 100,
+            "out": 100
+          },
+          "name": "elasticsearch"
+        }
+      ]
+    },
+    "reloads": {
+      "last_error": null,
+      "successes": 0,
+      "last_success_timestamp": null,
+      "last_failure_timestamp": null,
+      "failures": 0
+    }
+  }
 --------------------------------------------------
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
 Logstash monitoring APIs.
 
+
 [[hot-threads-api]]
 === Hot Threads API
 
@@ -217,40 +422,81 @@ GET /_node/hot_threads
 --------------------------------------------------
 
 The output is a JSON document that contains a breakdown of the top hot threads for
-Logstash. The parameters allowed are:
-
-[horizontal]
-`threads`:: 	        The number of hot threads to return. The default is 3. 
-`human`:: 	            If true, returns plain text instead of JSON format. The default is false. 
-`ignore_idle_threads`:: If true, does not return idle threads. The default is true.
+Logstash.
 
 Example response:
 
 [source,js]
 --------------------------------------------------
 {
-  "hostname" : "Example-MBP-2",
-  "time" : "2016-03-08T17:58:18-08:00",
-  "busiest_threads" : 3,
-  "threads" : [ {
-    "name" : "LogStash::Runner",
-    "percent_of_cpu_time" : 16.93,
-    "state" : "timed_waiting",
-    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\tjava.lang.Thread.join(Thread.java:1253)\n\t\torg.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)\n\t\torg.jruby.RubyThread.join(RubyThread.java:697)\n\t\torg.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)\n\t\torg.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)\n\t\torg.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)\n"
-  }, {
-    "name" : "Api Webserver",
-    "percent_of_cpu_time" : 0.39,
-    "state" : "timed_waiting",
-    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\tjava.lang.Thread.join(Thread.java:1253)\n\t\torg.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)\n\t\torg.jruby.RubyThread.join(RubyThread.java:697)\n\t\torg.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)\n\t\torg.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)\n\t\torg.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)\n"
-  }, {
-    "name" : "Ruby-0-Thread-13",
-    "percent_of_cpu_time" : 0.15,
-    "state" : "timed_waiting",
-    "path" : "/Users/suyog/ws/elastic/logstash/build/logstash-3.0.0.dev/vendor/local_gems/f5685da5/logstash-core-3.0.0.dev-java/lib/logstash/pipeline.rb:496",
-    "traces" : "\t\tjava.lang.Object.wait(Native Method)\n\t\torg.jruby.RubyThread.sleep(RubyThread.java:1002)\n\t\torg.jruby.RubyKernel.sleep(RubyKernel.java:803)\n\t\torg.jruby.RubyKernel$INVOKER$s$0$1$sleep.call(RubyKernel$INVOKER$s$0$1$sleep.gen)\n\t\torg.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:667)\n\t\torg.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:206)\n\t\torg.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:168)\n\t\torg.jruby.ast.FCallOneArgNode.interpret(FCallOneArgNode.java:36)\n\t\torg.jruby.ast.NewlineNode.interpret(NewlineNode.java:105)\n\t\torg.jruby.ast.BlockNode.interpret(BlockNode.java:71)\n"
-  } ]
+  "hot_threads": {
+    "time": "2016-09-19T10:44:13-07:00",
+    "busiest_threads": 3,
+    "threads": [
+      {
+        "name": "LogStash::Runner",
+        "percent_of_cpu_time": 0.17,
+        "state": "timed_waiting",
+        "traces": [
+          "java.lang.Object.wait(Native Method)",
+          "java.lang.Thread.join(Thread.java:1253)",
+          "org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)",
+          "org.jruby.RubyThread.join(RubyThread.java:697)",
+          "org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)",
+          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)",
+          "org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)",
+          "org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)",
+          "org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)",
+          "org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)"
+        ]
+      },
+      {
+        "name": "Ruby-0-Thread-17",
+        "percent_of_cpu_time": 0.11,
+        "state": "timed_waiting",
+        "path": "/Users/username/logstash-5.0.0/logstash-core/lib/logstash/pipeline.rb:471",
+        "traces": [
+          "java.lang.Object.wait(Native Method)",
+          "org.jruby.RubyThread.sleep(RubyThread.java:1002)",
+          "org.jruby.RubyKernel.sleep(RubyKernel.java:803)",
+          "org.jruby.RubyKernel$INVOKER$s$0$1$sleep.call(RubyKernel$INVOKER$s$0$1$sleep.gen)",
+          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:667)",
+          "org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:206)",
+          "org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:168)",
+          "rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb:84)",
+          "rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb)",
+          "org.jruby.ast.executable.AbstractScript.__file__(AbstractScript.java:46)"
+        ]
+      },
+      {
+        "name": "[main]-pipeline-manager",
+        "percent_of_cpu_time": 0.04,
+        "state": "timed_waiting",
+        "traces": [
+          "java.lang.Object.wait(Native Method)",
+          "java.lang.Thread.join(Thread.java:1253)",
+          "org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)",
+          "org.jruby.RubyThread.join(RubyThread.java:697)",
+          "org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)",
+          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)",
+          "org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)",
+          "org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:683)",
+          "org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:286)",
+          "org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:81)"
+        ]
+      }
+    ]
+  }
+}
 --------------------------------------------------
 
+The parameters allowed are:
+
+[horizontal]
+`threads`:: 	        The number of hot threads to return. The default is 3.
+`human`:: 	            If true, returns plain text instead of JSON format. The default is false.
+`ignore_idle_threads`:: If true, does not return idle threads. The default is true.
+
 You can use the `?human` parameter to return the document in a human-readable format.
 
 [source,js]
@@ -258,28 +504,50 @@ You can use the `?human` parameter to return the document in a human-readable fo
 GET /_node/hot_threads?human=true
 --------------------------------------------------
 
-Example of a human-readable response: 
+Example of a human-readable response:
 
 [source,js]
 --------------------------------------------------
-::: {Ringo Kid}{Gv3UrzR3SqmPQIgfG4qJMA}{127.0.0.1}{127.0.0.1:9300}
-   Hot threads at 2016-01-13T16:55:49.988Z, interval=500ms, busiestThreads=3, ignoreIdleThreads=true:
-
-    0.0% (216micros out of 500ms) cpu usage by thread 'elasticsearch[Ringo Kid][transport_client_timer][T#1]{Hashed wheel timer #1}'
-     10/10 snapshots sharing following 5 elements
-       java.lang.Thread.sleep(Native Method)
-       org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
-       org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
-       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
-       java.lang.Thread.run(Thread.java:745)
-
-    0.0% (216micros out of 500ms) cpu usage by thread 'elasticsearch[Ringo Kid][transport_client_timer][T#1]{Hashed wheel timer #1}'
-     10/10 snapshots sharing following 5 elements
-       java.lang.Thread.sleep(Native Method)
-       org.jboss.netty.util.HashedWheelTimer$Worker.waitForNextTick(HashedWheelTimer.java:445)
-       org.jboss.netty.util.HashedWheelTimer$Worker.run(HashedWheelTimer.java:364)
-       org.jboss.netty.util.ThreadRenamingRunnable.run(ThreadRenamingRunnable.java:108)
-       java.lang.Thread.run(Thread.java:745)
+::: {}
+Hot threads at 2016-07-26T18:46:18-07:00, busiestThreads=3:
+================================================================================
+ 0.15 % of cpu usage by timed_waiting thread named 'LogStash::Runner'
+	java.lang.Object.wait(Native Method)
+	java.lang.Thread.join(Thread.java:1253)
+	org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)
+	org.jruby.RubyThread.join(RubyThread.java:697)
+	org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)
+	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)
+	org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)
+	org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:306)
+	org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:136)
+	org.jruby.ast.CallNoArgNode.interpret(CallNoArgNode.java:60)
+ --------------------------------------------------------------------------------
+ 0.11 % of cpu usage by timed_waiting thread named 'Ruby-0-Thread-17'
+ /Users/username/BuildTesting/logstash-5.0.0logstash-core/lib/logstash/pipeline.rb:471
+	java.lang.Object.wait(Native Method)
+	org.jruby.RubyThread.sleep(RubyThread.java:1002)
+	org.jruby.RubyKernel.sleep(RubyKernel.java:803)
+	org.jruby.RubyKernel$INVOKER$s$0$1$sleep.call(RubyKernel$INVOKER$s$0$1$sleep.gen)
+	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:667)
+	org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:206)
+	org.jruby.runtime.callsite.CachingCallSite.call(CachingCallSite.java:168)
+	rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/BuildTesting/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb:84)
+	rubyjit.Module$$stoppable_sleep_c19c1639527ca7d373b5093f339d26538f1c21ef1028566121.__file__(/Users/username/BuildTesting/logstash-5.0.0/vendor/bundle/jruby/1.9/gems/stud-0.0.22/lib/stud/interval.rb)
+	org.jruby.ast.executable.AbstractScript.__file__(AbstractScript.java:46)
+ --------------------------------------------------------------------------------
+ 0.04 % of cpu usage by timed_waiting thread named '[main]-pipeline-manager'
+	java.lang.Object.wait(Native Method)
+	java.lang.Thread.join(Thread.java:1253)
+	org.jruby.internal.runtime.NativeThread.join(NativeThread.java:75)
+	org.jruby.RubyThread.join(RubyThread.java:697)
+	org.jruby.RubyThread$INVOKER$i$0$1$join.call(RubyThread$INVOKER$i$0$1$join.gen)
+	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:663)
+	org.jruby.internal.runtime.methods.DynamicMethod.call(DynamicMethod.java:198)
+	org.jruby.internal.runtime.methods.JavaMethod$JavaMethodN.call(JavaMethod.java:683)
+	org.jruby.runtime.callsite.CachingCallSite.cacheAndCall(CachingCallSite.java:286)
+	org.jruby.runtime.callsite.CachingCallSite.callBlock(CachingCallSite.java:81)
+
 --------------------------------------------------
 
 See <<monitoring-common-options, Common Options>> for a list of options that can be applied to all
diff --git a/docs/static/offline-plugins.asciidoc b/docs/static/offline-plugins.asciidoc
index e099faec8a8..38d1364a1bc 100644
--- a/docs/static/offline-plugins.asciidoc
+++ b/docs/static/offline-plugins.asciidoc
@@ -10,10 +10,6 @@ server. This staging machine downloads and packages the files used for offline i
 See the <<private-rubygem,Private Gem Repositories>> section for information on setting up your own private
 Rubygems server.
 
-Users who can work with a larger Logstash artifact size can use the *Logstash (All Plugins)* download link from the
-https://www.elastic.co/downloads/logstash[Logstash product page] to download Logstash bundled with the latest version of
-all available plugins. You can distribute this bundle to all nodes without further plugin staging.
-
 [float]
 === Building the Offline Package
 
diff --git a/docs/static/performance-checklist.asciidoc b/docs/static/performance-checklist.asciidoc
new file mode 100644
index 00000000000..48e8a4d1bc7
--- /dev/null
+++ b/docs/static/performance-checklist.asciidoc
@@ -0,0 +1,43 @@
+[[performance-troubleshooting]]
+
+== Performance Troubleshooting Guide
+
+You can use this troubleshooting guide to quickly diagnose and resolve Logstash performance problems. Advanced knowledge of pipeline internals is not required to understand this guide. However, the https://www.elastic.co/guide/en/logstash/current/pipeline.html[pipeline documentation] is recommended reading if you want to go beyond this guide.
+
+You may be tempted to jump ahead and change settings like `-w` as a first attempt to improve performance. In our experience, changing this setting makes it more difficult to troubleshoot performance problems because you increase the number of variables in play. Instead, make one change at a time and measure the results. Starting at the end of this list is a sure-fire way to create a confusing situation.
+
+[float]
+=== Performance Checklist
+
+. *Check the performance of input sources and output destinations:*
++
+* Logstash is only as fast as the services it connects to. Logstash can only consume and produce data as fast as its input and output destinations can!
+
+. *Check system statistics:*
++
+* CPU
+** Note whether the CPU is being heavily used. On Linux/Unix, you can run `top -H` to see process statistics broken out by thread, as well as total CPU statistics.
+** If CPU usage is high, skip forward to the section about checking the JVM heap and then read the section about tuning Logstash worker settings.
+* Memory
+** Be aware of the fact that Logstash runs on the Java VM. This means that Logstash will always use the maximum amount of memory you allocate to it. 
+** Look for other applications that use large amounts of memory and may be causing Logstash to swap to disk. This can happen if the total memory used by applications exceeds physical memory.
+* I/O Utilization
+** Monitor disk I/O to check for disk saturation. 
+*** Disk saturation can happen if you’re using Logstash plugins (such as the file output) that may saturate your storage. 
+*** Disk saturation can also happen if you're encountering a lot of errors that force Logstash to generate large error logs.
+*** On Linux, you can use iostat, dstat, or something similar to monitor disk I/O.
+** Monitor network I/O for network saturation.
+*** Network saturation can happen if you’re using inputs/outputs that perform a lot of network operations. 
+*** On Linux, you can use a tool like dstat or iftop to monitor your network.
+
+. *Check the JVM heap:*
++
+* Often times CPU utilization can go through the roof if the heap size is too low, resulting in the JVM constantly garbage collecting.
+* A quick way to check for this issue is to double the heap size and see if performance improves. Do not increase the heap size past the amount of physical memory. Leave at least 1GB free for the OS and other processes.
+* You can make more accurate measurements of the JVM heap by using either the `jmap` command line utility distributed with Java or by using VisualVM.
+
+. *Tune Logstash worker settings:*
++
+* Begin by scaling up the number of pipeline workers by using the `-w` flag. This will increase the number of threads available for filters and outputs. It is safe to scale this up to a multiple of CPU cores, if need be, as the threads can become idle on I/O.
+* Each output can only be active in a single pipeline worker thread by default. You can increase this by changing the `workers` setting in the configuration block for each output. Never make this value larger than the number of pipeline workers.
+* You may also tune the output batch size. For many outputs, such as the Elasticsearch output, this setting will correspond to the size of I/O operations. In the case of the Elasticsearch output, this setting corresponds to the batch size.
diff --git a/docs/static/plugin-generator.asciidoc b/docs/static/plugin-generator.asciidoc
new file mode 100644
index 00000000000..cd18d1d6713
--- /dev/null
+++ b/docs/static/plugin-generator.asciidoc
@@ -0,0 +1,19 @@
+[[plugin-generator]]
+=== Generating Plugins
+
+You can now create your own Logstash plugin in seconds! The generate subcommand of `bin/logstash-plugin` creates the foundation 
+for a new Logstash plugin with templatized files. It creates the correct directory structure, gemspec files, and dependencies so you 
+can start adding custom code to process data with Logstash.
+
+**Example Usage**
+
+[source,sh]
+--------------------------------------------
+bin/logstash-plugin generate --type input --name xkcd --path ~/ws/elastic/plugins
+-------------------------------------------
+
+* `--type`: Type of plugin - input, filter, output, or codec
+* `--name`: Name for the new plugin
+* `--path`: Directory path where the new plugin structure will be created. If not specified, it will be
+created in the current directory.
+
diff --git a/docs/static/plugin-manager.asciidoc b/docs/static/plugin-manager.asciidoc
index 75d297f2a52..f59626ccfb3 100644
--- a/docs/static/plugin-manager.asciidoc
+++ b/docs/static/plugin-manager.asciidoc
@@ -2,9 +2,9 @@
 == Working with plugins
 
 Logstash has a rich collection of input, filter, codec and output plugins. Plugins are available as self-contained
-packages called gems and hosted on RubyGems.org. The plugin manager accesed via `bin/logstash-plugin` script is used to manage the
-lifecycle of plugins in your Logstash deployment. You can install, uninstall and upgrade plugins using these Command Line
-Interface (CLI) described below.
+packages called gems and hosted on RubyGems.org. The plugin manager accessed via `bin/logstash-plugin` script is used to manage the
+lifecycle of plugins in your Logstash deployment. You can install, remove and upgrade plugins using the Command Line
+Interface (CLI) invocations described below.
 
 [float]
 [[listing-plugins]]
@@ -16,8 +16,8 @@ available in your deployment:
 [source,shell]
 ----------------------------------
 bin/logstash-plugin list <1>
-bin/logstash-plugin list --verbose <2>
-bin/logstash-plugin list *namefragment* <3>
+bin/logstash-plugin list --info <2>
+bin/logstash-plugin list '*namefragment*' <3>
 bin/logstash-plugin list --group output <4>
 ----------------------------------
 <1> Will list all installed plugins
@@ -57,14 +57,17 @@ bin/logstash-plugin install /path/to/logstash-output-kafka-1.0.0.gem
 
 [[installing-local-plugins-path]]
 [float]
-==== Advanced: Using `--pluginpath`
+==== Advanced: Using `--path.plugins`
 
-Using the `--pluginpath` flag, you can load a plugin source code located on your file system. Typically this is used by
+Using the Logstash `--path.plugins` flag, you can load a plugin source code located on your file system. Typically this is used by
 developers who are iterating on a custom plugin and want to test it before creating a ruby gem.
 
+The path needs to be in a  specific directory hierarchy: `PATH/logstash/TYPE/NAME.rb`, where TYPE is 'inputs' 'filters', 'outputs' or 'codecs' and NAME is the name of the plugin.
+
 [source,shell]
 ----------------------------------
-bin/logstash --pluginpath /opt/shared/lib/logstash/input/my-custom-plugin-code.rb
+# supposing the code is in /opt/shared/lib/logstash/inputs/my-custom-plugin-code.rb
+bin/logstash --path.plugins /opt/shared/lib
 ----------------------------------
 
 [[updating-plugins]]
@@ -91,7 +94,7 @@ If you need to remove plugins from your Logstash installation:
 
 [source,shell]
 ----------------------------------
-bin/logstash-plugin uninstall logstash-output-kafka
+bin/logstash-plugin remove logstash-output-kafka
 ----------------------------------
 
 [[proxy-plugins]]
@@ -111,6 +114,10 @@ bin/logstash-plugin install logstash-output-kafka
 
 Once set, plugin commands install, update can be used through this proxy.
 
+include::plugin-generator.asciidoc[]
+
 include::offline-plugins.asciidoc[]
 
-include::private-gem-repo.asciidoc[]
\ No newline at end of file
+include::private-gem-repo.asciidoc[]
+
+include::event-api.asciidoc[]
diff --git a/docs/static/releasenotes.asciidoc b/docs/static/releasenotes.asciidoc
index 899b9cc9a75..d28fabbe56c 100644
--- a/docs/static/releasenotes.asciidoc
+++ b/docs/static/releasenotes.asciidoc
@@ -1,61 +1,339 @@
 [[releasenotes]]
-== Logstash 2.1 Release Notes
+== Release Notes
+
+This section summarizes the changes in each release.
+
+* <<beta1, Logstash 5.0-beta1>>
+* <<alpha5,Logstash 5.0-alpha5>>
+* <<alpha4,Logstash 5.0-alpha4>>
+* <<alpha3,Logstash 5.0-alpha3>>
+* <<alpha2,Logstash 5.0-alpha2>>
+* <<alpha1,Logstash 5.0-alpha1>>
+
+[[beta1]]
+=== Logstash 5.0-beta1 Release Notes
+
+* Migrated Logstash's internal logging framework to Log4j2. This enhancement provides the following features:
+** Support for changing the <<logging>> level dynamically at runtime through REST endpoints. New APIs have been exposed
+under `_node/logging` to update log levels. Also a new endpoint `_node/logging` was added to return all existing loggers.
+** Configurable file rotation policy for logs. The default is per-day.
+** Support for component-level or plugin-level log settings.
+** Unified logging across Logstash's Java and Ruby code.
+** Logs are now placed in the `LS_HOME/logs` directory, which is configurable via the `path.logs` setting.
+* Breaking Change: Changed the default log severity level to `INFO` instead of `WARN` to match Elasticsearch.
+* Show meaningful error messages for unknown CLI commands ({lsissue}5748[Issue 5748]).
+* Added monitoring API enhancements:
+** Added `duration_in_millis` metric under `/_node/stats/pipeline/events`.
+** Added JVM GC stats under `/_node/stats/jvm`.
+** Removed the `/_node/mem` resource because it's been properly moved under `/_node/jvm/mem`.
+** Added config reload stats under new resource type `_node/stats/pipeline/reloads`.
+** Added config reload enabled/disabled info to `/_node/pipeline`.
+** Added the JVM GC strategy info under `/_node/jvm`.
+** Fixed the `?human` option to work correctly for the `hot_threads` API.
+** Made sure a non-existing API endpoint correctly returns 404 and a structured error message.
+* Plugin Developers: Improved nomenclature and methods for 'threadsafe' outputs. Removed the `workers_not_supported` method ({lsissue}5662[Issue 5662]).
 
 [float]
-== General
+==== Output Plugins
+
+*`Elasticsearch`*:
+
+* Breaking Change: The index template for 5.0 has been changed to reflect Elasticsearch's mapping changes. Most importantly,
+the subfield for string multi-fields has changed from `.raw` to `.keyword` to match Elasticsearch's default behavior
+(https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/386[Issue 386]). See <<breaking-changes>> for details about how this change affects new and existing users.
+* Added `check_connection_timeout` parameter, which has a default of 10m.
 
-* {lsissue}2376[Issue 2376]: Added ability to install and upgrade Logstash plugins without requiring internet
-connectivity.
-* {lsissue}3576[Issue 3576]: Support alternate or private Ruby gems server to install and update plugins.
-* {lsissue}3451[Issue 3451]: Added ability to reliably shutdown Logstash when there is a stall in event processing. This
-option can be enabled by passing `--allow-unsafe-shutdown` flag while starting Logstash. Please be aware that any in-
-flight events will be lost when shutdown happens.
-* {lsissue}4222[Issue 4222]: Fixed a memory leak which could be triggered when events having a date were serialized to
-string.
-* Added JDBC input to default package.
-* {lsissue}3243[Issue 3243]: Adding `--debug` to `--configtest` now shows the configuration in blocks annotated by source
-config file. Very useful when using multiple config files in a directory.
-* {lsissue}4130[Issue 4130]: Reset default worker threads to 1 when using non thread-safe filters like multiline.
-* Fixed file permissions for the `logrotate` configuration file.
-* {lsissue}3861[Issue 3861]: Changed the default heap size from 500MB to 1GB.
-* {lsissue}3645[Issue 3645]: Fixed config check option when starting Logstash through init scripts.
+[[alpha5]]
+=== Logstash 5.0-alpha5 Release Notes
+
+* Introduced a performance optimization called bi-values to store both JRuby and Java object types. This optimization
+benefits plugins written in Ruby.
+* Added support for specifying a comma-separated list of resources when calling the monitoring APIs. This can be used
+to filter the API response ({lsissue}5609[Issue 5609]).
+* Fixed the `/_node/hot_threads?human=true` human option so that it now returns a human-readable format, not JSON.
+* Added the pipeline stats from `/_node/stats/pipeline` to the parent `/_node/stats` resource for completeness.
 
 [float]
-== Input Plugins
+==== Input Plugins
+
+*`Beats`*:
+
+* Improved throughput performance by reimplementing the beats input plugin in Java and using Netty, an asynchronous I/O
+library. These changes resulted in up to 50% gains in throughput performance while preserving the original plugin
+functionality (https://github.com/logstash-plugins/logstash-input-beats/issues/92[Issue 92]).
+
+*`Kafka`*:
+
+* Added feature to allow regex patterns in topics so you can subscribe to multiple topics.
+
+*`JDBC`*:
+
+* Added the `charset` config option to support setting the character encoding for strings that are not in UTF-8 format.
+You can use the `columns_charset` option to override this encoding setting for individual columns 
+(https://github.com/logstash-plugins/logstash-input-jdbc/issues/143[Issue 143]).
+
+*`HTTP Poller`*:
+
+* Added meaningful error messages for missing trust store/keystore passwords. Also documented the creation of a custom keystore.
 
 [float]
-=== Twitter
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/21[Issue 21]: Added an option to fetch data from the
-sample Twitter streaming endpoint.
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/22[Issue 22]: Added hashtags, symbols and
-user_mentions as data for the non extended tweet event.
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/20[Issue 20]: Added an option to filter per location
-and language.
-* https://github.com/logstash-plugins/logstash-input-twitter/issues/11[Issue 11]: Added an option to stream data from a
-list of users.
+==== Filter Plugins
+
+*`CSV`*:
+
+* Added the `autodetect_column_names` option to read column names from the header.
+
+*`Grok`*:
+
+* Added support to cancel long-running execution. Many times users write runaway regular expressions that lead to a
+stalled Logstash. You can configure `timeout_millis` to cancel the current execution and continue processing the event
+downstream (https://github.com/logstash-plugins/logstash-filter-grok/issues/82[Issue 82]).
+
+*`Throttle`*:
+
+* Reimplemented the plugin to work with multiple threads, support asynchronous input, and properly track past events (https://github.com/logstash-plugins/logstash-filter-throttle/issues/4[Issue 4]).
 
 [float]
-=== Beats
-* https://github.com/logstash-plugins/logstash-input-beats/issues/10[Issue 10]: Properly handle multiline events from
-multiple sources, originating from Filebeat.
+==== Output Plugins
+
+*`Elasticsearch`*:
+
+* Added the ability for the plugin to choose which default template to use based on the Elasticsearch version (https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/401[Issue 401]).
+
+*`Kafka`*:
+
+* Made this output a shareable instance across multiple pipeline workers. This ensures efficient use of resources like broker
+TCP connections, internal producer buffers, and so on.
+
+*`Tcp`*:
+
+* Added SSL/TLS support for certificate-based encryption.
+
+
+[[alpha4]]
+=== Logstash 5.0-alpha4 Release Notes
+
+* Created a new `LS_HOME/data` directory to store plugin states, Logstash instance UUID, and more. This directory 
+location is configurable via the `path.data` setting in the `logstash.yml` <<logstash-settings-file,settings file>> ({lsissue}5404[Issue 5404]).
+* Made `bin/logstash -V/--version` run faster on Unix platforms.
+* Ehanced the <<monitoring,monitoring APIs>> by adding `hostname`, `http_address`, and `version` as static fields for all APIs ({lsissue}5450[Issue 5450]).
+* Added time tracking (wall-clock) to all individual filter and output instances. The goal is to help identify 
+which plugin configurations are consuming the most time. These statics are exposed by the `/_node/stats/pipeline` endpoint. See the <<pipeline-stats>> section under the <<node-stats-api>>.
+* Added the `/_node` endpoint, which provides static information for OS, JVM, and pipeline settings. See the <<node-info-api,node info API>>.
+* Moved the <<plugins-api,plugins API>> to the `_node/plugins` endpoint.
+* Moved the <<hot-threads-api,hot threads API>> to the `_node/hot_threads` endpoint.
+* Added a new `:list` property to the configuration parameters. This will allow the user to specify one or more values. 
+* Added a new URI config validator/type. This type allows plugins like the Elasticsearch output to safely log URIs for configuration. Any password information in the URI will be masked when the URI is logged.
 
 [float]
-=== File
-* https://github.com/logstash-plugins/logstash-input-file/issues/44[Issue 44]: Properly handle multiline events from
-multiple sources.
+==== Input Plugins
+
+*`Kafka`*:
+
+* Added support for Kafka broker 0.10.
+
+*`HTTP`*:
+
+* Fixed a bug where the HTTP input plugin blocked the node stats API (https://github.com/logstash-plugins/logstash-input-http/issues/51[Issue 51]). 
 
 [float]
-=== Eventlog
-* https://github.com/logstash-plugins/logstash-input-eventlog/issues/11[Issue 11]: Change the underlying library to
-capture Event Logs from Windows more reliably.
+==== Output Plugins
+
+*`Elasticsearch`*:
+
+* Elasticserach output is now fully threadsafe. This means internal resources can be shared among multiple
+`output { elasticsearch {} }` instances.
+* Added sniffing improvements so any current connections don't have to be closed/reopened after a sniff round.
+* Introduced a connection pool to efficiently reuse connections to Elasticsearch backends.
+* Added exponential backoff to connection retries with a ceiling of `retry_max_interval`, which is the most time to 
+wait between retries, and `retry_initial_interval`,  which is the initial amount of time to wait. The value of
+`retry_initial_interval` increases exponentially between retries until a request succeeds.
+     
+*`Kafka`*:
 
+* Added support for Kafka broker 0.10.
+   
 [float]
-== Output
+==== Filter Plugins
+
+*`Grok`*:
+
+* Added a stats counter on grok matches and failures. This is exposed in the `_node/stats/pipeline` endpoint.
+
+*`Date`*:
+
+* Added a stats counter on grok matches and failures. This is exposed in the `_node/stats/pipeline` endpoint.
+
+[[alpha3]]
+=== Logstash 5.0-alpha3 Release Notes
+
+* Breaking Change: Introduced a new way to configure application settings for Logstash through a `settings.yml` file. This
+file is typically located in `LS_HOME/config` or `/etc/logstash` when installed via packages. Logstash will
+not be able to start without this file, so please make sure to pass in `path.settings` if you are starting
+Logstash manually after installing it via a package (RPM or DEB) ({lsissue}4401[Issue 4401]).
+* Breaking Change: Most of the long form options (https://www.elastic.co/guide/en/logstash/5.0/command-line-flags.html) have
+been renamed to adhere to the YAML dot notation used in the settings file. Short form options have not been
+changed ({lsissue}4401[Issue 4401]).
+* Breaking Change: When Logstash is installed via DEB or RPM packages, it uses `/usr/share` and `/var` to install binaries
+and config files respectively. Previously it used the `/opt` directory. This change was done to
+make the user experience consistent with other Elastic products ({lsissue}5101[Issue 5101]).
+* Breaking Change: For plugin developers, the Event class has a new API to access its data. You will no longer be able to
+directly use the Event class through the Ruby hash paradigm. All the plugins packaged with Logstash have
+been updated to use the new API, and their versions have been bumped to the next major. ({lsissue}5141[Issue 5141])
+* Added support for systemd so you can now manage Logstash as a service on most Linux distributions ({lsissue}5012[Issue 5012]).
+* Added a new subcommand called `generate` to the `logstash-plugins` script that bootstraps a new plugin with
+the correct directory structure and all the required files.
+* Logstash can now emit its log in structured, JSON format. Specify `--log.format=json` in the settings file
+or via the command line ({lsissue}1569[Issue 1569]).
+* Added more operational information to help run Logstash in production. The `_node/stats` endpoint now 
+shows file descriptors and CPU information.
+* Fixed a bug where Logstash would not shutdown if CTRL-C was used while using stdin input in the 
+configuration ({lsissue}1769[Issue 1769]).
 
 [float]
-=== Elasticsearch
-* Improved the default template to use doc_values wherever possible.
-* Improved the default template to disable fielddata on analyzed string fields.
-* https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/260[Issue 260]: Added New setting: timeout.
-This lets you control the behavior of a slow/stuck request to Elasticsearch that could be, for example, caused by network,
-firewall, or load balancer issues.
+==== Input Plugins
+
+*`RabbitMQ`*:
+
+* Removed `verify_ssl` option, which was never used previously. To validate SSL certs, use the `ssl_certificate_path` and `ssl_certificate_password` config options (https://github.com/logstash-plugins/logstash-input-rabbitmq/issues/82[Issue 82]).
+
+*`Stdin`*: 
+
+* This plugin is now non-blocking, so you can use CTRL-C to stop Logstash.
+
+*`JDBC`*: 
+
+* Added the `jdbc_password_filepath` parameter for reading passwords from an external file
+(https://github.com/logstash-plugins/logstash-input-jdbc/issues/120[Issue 120]).
+
+[float]
+==== Filter Plugins
+
+*`XML`*:
+
+* Breaking Change: Added a new configuration called `suppress_empty`, which defaults to true. This changes the
+default behaviour of the plugin in favor of avoiding mapping conflicts when reaching Elasticsearch (https://github.com/logstash-plugins/logstash-filter-xml/issues/24[Issue 24]).
+* Added a new configuration called `force_content`. By default, the filter expands attributes differently
+for content in XML elements. This option allows you to force text content and attributes to always parse to
+a hash value (https://github.com/logstash-plugins/logstash-filter-xml/issues/14[Issue 14]).
+* Fixed a bug that ensures that a `target` is set when storing XML content in the event (`store_xml => true`).
+
+[[alpha2]]
+=== Logstash 5.0-alpha2 Release Notes
+
+* Added the `--preserve` option to the `bin/logstash-plugin` install command. This option allows you to preserve gem options that are already specified in the `Gemfile`. Previously, these options were overwritten.
+* Added support for `DEBUG=1` when running any plugin-related commands. This option gives you a bit more information about what the bundler is doing.
+* Added reload support to the init script so you can do `service logstash reload`.
+* Fixed use of the `KILL_ON_STOP_TIMEOUT` variable in the init script to allow Logstash to force stop ({lsissue}4991[Issue 4991]).
+* Upgraded to JRuby 1.7.25.
+* Renamed filenames for Debian and RPM artifacts to match Elasticsearch's naming scheme. The metadata is still the same, so upgrades will not be affected. If you have automated downloads for Logstash, please make sure you use the updated URLs ({lsissue}5100[Issue 5100]). 
+
+[float]
+==== Input Plugins
+
+*`Kafka`*:
+
+* Fixed an issue where Snappy and LZ4 compression were not working.
+
+[float]
+==== Filter Plugins
+
+*`GeoIP`*:
+
+* Added support for the GeoIP2 city database and support for IPv6 lookups (https://github.com/logstash-plugins/logstash-filter-geoip/issues/23[Issue 23]).
+
+[float]
+==== Output Plugins
+
+*`Elasticsearch`*:
+
+* Added support for specifying ingest pipelines (https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/410[Issue 410]).
+
+*`Kafka`*:
+
+* Fixed an issue where Snappy and LZ4 compression were not working (https://github.com/logstash-plugins/logstash-output-kafka/issues/50[Issue 50]).  
+
+[[alpha1]]
+=== Logstash 5.0-alpha1 Release Notes
+
+* Added APIs to monitor the Logstash pipeline. You can now query information/stats about event flow, JVM, 
+  and hot_threads.
+* Added dynamic config, a new feature to track config file for changes and restart the 
+  pipeline (same process) with updated config changes. This feature can be enabled in two 
+  ways: Passing a CLI long-form option `--auto-reload` or with short-form `-r`. Another 
+  option, `--reload-interval <seconds>` controls how often LS should check the config files 
+  for changes. Alternatively, if you don't start with the CLI option, you can send SIGHUP 
+  or `kill -1` signal to LS to reload the config file, and restart the pipeline ({lsissue}4513[Issue 4513]).
+* Added support to evaluate environment variables inside the Logstash config. You can also specify a 
+  default if the variable is not defined. The syntax is `${myVar:default}` ({lsissue}3944[Issue 3944]).
+* Improved throughput performance across the board (up by 2x in some configs) by implementing Event 
+  representation in Java. Event is the main object that encapsulates data as it flows through 
+  Logstash and provides APIs for the plugins to perform processing. This change also enables 
+  faster serialization for future persistence work ({lsissue}4191[Issue 4191]).
+* Added ability to configure custom garbage collection log file using `$LS_LOG_DIR`.
+* Deprecated `bin/plugin` in favor of `bin/logstash-plugin`. In the next major version `bin/plugin` will 
+  be removed to prevent `PATH` being polluted when other components of the Elastic stack are installed on 
+  the same instance ({lsissue}4891[Issue 4891]).
+* Fixed a bug where new pipeline might break plugins by calling the `register` method twice causing 
+  undesired behavior ({lsissue}4851[Issue 4851])).
+* Made `JAVA_OPTS` and `LS_JAVA_OPTS` work consistently on Windows ({lsissue}4758[Issue 4758]).
+* Fixed a bug where specifying JMX parameters in `LS_JAVA_OPTS` caused Logstash not to restart properly
+  ({lsissue}4319[Issue 4319]).
+* Fixed a bug where upgrading plugins with Manticore threw an error and sometimes corrupted installation ({lsissue}4818[Issue 4818]).
+* Removed milestone warning that was displayed when the `--pluginpath` option was used to load plugins ({lsissue}4562[Issue 4562]).
+* Upgraded to JRuby 1.7.24.
+* Reverted default output workers to 1. Previously we had made output workers the same as number of pipeline workers (#4877). 
+
+[float]
+==== Input Plugins
+
+*`Kafka`*:
+
+* Breaking Change: Added support for 0.9 consumer API. This plugin now supports SSL based encryption. 
+  This release changed a lot of configuration, so it is not backward compatible. Also, this version will not 
+  work with Kafka 0.8 broker
+
+*`Beats`*:
+
+* Enhanced to verify client certificates against CA (https://github.com/logstash-plugins/logstash-input-beats/issues/8[Issue 8]).
+
+*`RabbitMQ`*:
+
+* Breaking Change: Metadata is now disabled by default because it was regressing performance.
+* Improved performance by using an internal queue and bulk ACKs.
+
+*`Redis`*:
+
+* Increased the batch_size to 100 by default. This provides a big jump in throughput and 
+  reduction in CPU utilization (https://github.com/logstash-plugins/logstash-input-redis/issues/25[Issue 25])
+
+*`JDBC`*:
+
+* Added retry connection feature (https://github.com/logstash-plugins/logstash-input-http/issues/33[Issue 33])
+
+[float]
+==== Filter Plugins
+
+*`DNS`*:
+
+* Improved performance by adding caches to both successful and failed requests.
+* Added support for retrying with the `:max_retries` setting.
+* Lowered the default value of timeout from 2 to 0.5 seconds.
+
+[float]
+==== Output Plugins
+
+*`Elasticsearch`*:
+
+* Bumped minimum manticore version to 0.5.4 which fixes a memory leak when sniffing 
+  is used (https://github.com/logstash-plugins/logstash-output-elasticsearch/issues/392[Issue 392]).
+* Fixed bug when updating documents with doc_as_upsert and scripting.   
+* Made error messages more verbose and easier to parse by humans.
+* Retryable failures are now logged at the info level instead of warning.
+
+*`Kafka`*:
+
+Breaking Change: Added support for 0.9 API. This plugin now supports SSL based encryption. This release 
+changed a lot of configuration, so it is not backward compatible. Also, this version will not work 
+with Kafka 0.8 broker
+
diff --git a/docs/static/reloading-config.asciidoc b/docs/static/reloading-config.asciidoc
index 22f3118aac6..22cc383d7dd 100644
--- a/docs/static/reloading-config.asciidoc
+++ b/docs/static/reloading-config.asciidoc
@@ -4,19 +4,19 @@
 Starting with Logstash 2.3, you can set Logstash to detect and reload configuration
 changes automatically.
 
-To enable automatic config reloading, start Logstash with the `--auto-reload` (or `-r`)
+To enable automatic config reloading, start Logstash with the `--config.reload.automatic` (or `-r`)
 command-line option specified. For example:
 
 [source,shell]
 ----------------------------------
-bin/logstash –f apache.config --auto-reload
+bin/logstash –f apache.config --config.reload.automatic
 ----------------------------------
 
-NOTE: The `--auto-reload` option is not available when you specify the `-e` flag to pass
+NOTE: The `--config.reload.automatic` option is not available when you specify the `-e` flag to pass
 in  configuration settings from the command-line.
 
 By default, Logstash checks for configuration changes every 3 seconds. To change this interval,
-use the `--reload-interval <seconds>` option,  where `seconds` specifies how often Logstash
+use the `----config.reload.interval <seconds>` option,  where `seconds` specifies how often Logstash
 checks the config files for changes. 
 
 If Logstash is already running without auto-reload enabled, you can force Logstash to
diff --git a/docs/static/repositories.asciidoc b/docs/static/repositories.asciidoc
deleted file mode 100644
index 8a3abf61d97..00000000000
--- a/docs/static/repositories.asciidoc
+++ /dev/null
@@ -1,86 +0,0 @@
-[[package-repositories]]
-== Package Repositories
-
-We also have repositories available for APT and YUM based distributions. Note
-that we only provide binary packages, but no source packages, as the packages
-are created as part of the Logstash build.
-
-We have split the Logstash package repositories by version into separate urls
-to avoid accidental upgrades across major or minor versions. For all 1.5.x
-releases use 1.5 as version number, for 1.4.x use 1.4, etc.
-
-We use the PGP key
-http://pgp.mit.edu/pks/lookup?op=vindex&search=0xD27D666CD88E42B4[D88E42B4],
-Elastic's Signing Key, with fingerprint
-
-    4609 5ACC 8548 582C 1A26 99A9 D27D 666C D88E 42B4
-
-to sign all our packages. It is available from http://pgp.mit.edu.
-
-[float]
-=== APT
-
-Download and install the Public Signing Key:
-
-[source,sh]
---------------------------------------------------
-wget -qO - https://packages.elastic.co/GPG-KEY-elasticsearch | sudo apt-key add -
---------------------------------------------------
-
-Add the repository definition to your `/etc/apt/sources.list` file:
-
-["source","sh",subs="attributes,callouts"]
---------------------------------------------------
-echo "deb http://packages.elastic.co/logstash/{branch}/debian stable main" | sudo tee -a /etc/apt/sources.list
---------------------------------------------------
-
-[WARNING]
-==================================================
-Use the `echo` method described above to add the Logstash repository.  Do not
-use `add-apt-repository` as it will add a `deb-src` entry as well, but we do not
-provide a source package. If you have added the `deb-src` entry, you will see an
-error like the following:
-
-    Unable to find expected entry 'main/source/Sources' in Release file (Wrong sources.list entry or malformed file)
-
-Just delete the `deb-src` entry from the `/etc/apt/sources.list` file and the
-installation should work as expected.
-==================================================
-
-Run `sudo apt-get update` and the repository is ready for use. You can install
-it with:
-
-[source,sh]
---------------------------------------------------
-sudo apt-get update && sudo apt-get install logstash
---------------------------------------------------
-
-[float]
-=== YUM
-
-Download and install the public signing key:
-
-[source,sh]
---------------------------------------------------
-rpm --import https://packages.elastic.co/GPG-KEY-elasticsearch
---------------------------------------------------
-
-Add the following in your `/etc/yum.repos.d/` directory
-in a file with a `.repo` suffix, for example `logstash.repo`
-
-["source","sh",subs="attributes,callouts"]
---------------------------------------------------
-[logstash-{branch}]
-name=Logstash repository for {branch}.x packages
-baseurl=http://packages.elastic.co/logstash/{branch}/centos
-gpgcheck=1
-gpgkey=http://packages.elastic.co/GPG-KEY-elasticsearch
-enabled=1
---------------------------------------------------
-
-And your repository is ready for use. You can install it with:
-
-[source,sh]
---------------------------------------------------
-yum install logstash
---------------------------------------------------
diff --git a/docs/static/setting-up-logstash.asciidoc b/docs/static/setting-up-logstash.asciidoc
new file mode 100644
index 00000000000..c251171b849
--- /dev/null
+++ b/docs/static/setting-up-logstash.asciidoc
@@ -0,0 +1,183 @@
+[[setup-logstash]]
+== Setting Up and Running Logstash
+
+Before reading this section, see <<installing-logstash>> for basic installation instructions to get you started.
+
+This section includes additional information on how to set up and run Logstash, including:
+
+* <<dir-layout>>
+* <<config-setting-files>>
+* <<running-logstash>>
+* <<command-line-flags>>
+* <<logstash-settings-file>>
+
+[[dir-layout]]
+=== Logstash Directory Layout
+
+This section describes the default directory structure that is created when you unpack the Logstash installation packages.
+
+[[zip-targz-layout]]
+==== Directory Layout of `.zip` and `.tar.gz` Archives
+
+The `.zip` and `.tar.gz` packages are entirely self-contained. All files and
+directories are, by default, contained within the home directory -- the directory
+created when unpacking the archive.
+
+This is very convenient because you don't have to create any directories to start using Logstash, and uninstalling
+Lostash is as easy as removing the home directory.  However, it is advisable to change the default locations of the
+config and the logs directories so that you do not delete important data later on.
+
+[cols="<h,<,<m,<m",options="header",]
+|=======================================================================
+| Type | Description | Default Location | Setting
+| home
+  | Home directory of the Logstash installation.
+  | `{extract.path}`- Directory created by unpacking the archive
+ d|
+
+| bin
+  | Binary scripts, including `logstash` to start Logstash
+    and `logstash-plugin` to install plugins
+  | `{extract.path}/bin`
+ d|
+
+| settings
+  | Configuration files, including `logstash.yml` and `jvm.options`
+  | `{extract.path}/config`
+  | `path.settings`
+
+| plugins
+  | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.
+  | `{extract.path}/plugins`
+  | `path.plugins`
+
+|=======================================================================
+
+[[deb-layout]]
+==== Directory Layout of Debian and RPM Packages
+
+The Debian package and the RPM package each place config files, logs, and the settings files in the appropriate
+locations for the system:
+
+[cols="<h,<,<m,<m",options="header",]
+|=======================================================================
+| Type | Description | Default Location | Setting
+| home
+  | Home directory of the Logstash installation.
+  | `/usr/share/logstash`
+ d|
+
+| bin
+  | Binary scripts including `logstash` to start Logstash
+    and `logstash-plugin` to install plugins
+  | `/usr/share/logstash/bin`
+ d|
+
+| settings
+  | Configuration files, including `logstash.yml`, `jvm.options`, and `startup.options`
+  | `/etc/logstash`
+  | `path.settings`
+
+| conf
+  | Logstash pipeline configuration files
+  | `/etc/logstash/conf.d`
+  | `path.config`
+
+| logs
+  | Log files
+  | `/var/log/logstash`
+  | `path.logs`
+
+| plugins
+  | Local, non Ruby-Gem plugin files. Each plugin is contained in a subdirectory. Recommended for development only.
+  | `/usr/share/logstash/plugins`
+  | `path.plugins`
+
+|=======================================================================
+
+[[config-setting-files]]
+=== Logstash Configuration Files
+
+Logstash has two types of configuration files: _pipeline configuration files_, which define the Logstash processing
+pipeline, and _settings files_, which specify options that control Logstash startup and execution.
+
+==== Pipeline Configuration Files
+
+You create pipeline configuration files when you define the stages of your Logstash processing pipeline. On deb and
+rpm, you place the pipeline configuration files in the `/etc/logstash/conf.d` directory. Logstash tries to load all
+files in the `/etc/logstash/conf.d directory`, so don't store any non-config files or backup files in this directory.
+
+See <<configuration>> for more info.
+
+==== Settings Files
+
+The settings files are already defined in the Logstash installation. Logstash includes the following settings files:
+
+*`logstash.yml`*::
+  Contains Logstash configuration flags. You can set flags in this file instead of passing the flags at the command
+  line. Any flags that you set at the command line override the corresponding settings in the `logstash.yml` file. See <<logstash-settings-file>> for more info.
+*`jvm.options`*::
+  Contains JVM configuration flags. Specify each flag on a separate line. You can also use this file to set the locale
+  for Logstash.
+*`startup.options` (Linux)*::
+  Contains options used by the `system-install` script in `/usr/share/logstash/bin` to build the appropriate startup
+  script for your system. When you install the Logstash package, the `system-install` script executes at the end of the
+  installation process and uses the settings specified in `startup.options` to set options such as the user, group,
+  service name, and service description. By default, Logstash services are installed under the user `logstash`. The `startup.options` file makes it easier for you to install multiple instances of the Logstash service. You can copy
+  the file and change the values for specific settings. Note that the `startup.options` file is not read at startup. If
+  you want to change the Logstash startup script (for example, to change the Logstash user or read from a different
+  configuration path), you must re-run the `system-install` script (as root) to pass in the new settings.
+
+[[running-logstash]]
+=== Running Logstash as a Service on Debian or RPM
+
+Logstash is not started automatically after installation. How to start and stop Logstash depends on whether your system
+uses systemd, upstart, or SysV.
+
+Here are some common operating systems and versions, and the corresponding
+startup styles they use.  This list is intended to be informative, not exhaustive.
+
+|=======================================================================
+| Distribution | Service System |
+| Ubuntu 16.04 and newer | <<running-logstash-systemd,systemd>> |
+| Ubuntu 12.04 through 15.10 | <<running-logstash-upstart,upstart>> |
+| Debian 8 "jessie" and newer | <<running-logstash-systemd,systemd>> |
+| Debian 7 "wheezy" and older | <<running-logstash-sysv,sysv>> |
+| CentOS (and RHEL) 7 and newer | <<running-logstash-systemd,systemd>> |
+| CentOS (and RHEL) 6 | <<running-logstash-upstart,upstart>> |
+|=======================================================================
+
+[[running-logstash-systemd]]
+==== Running Logstash by Using Systemd
+
+Distributions like Debian Jessie, Ubuntu 15.10+, and many of the SUSE derivatives use systemd and the
+`systemctl` command to start and stop services. Logstash places the systemd unit files in `/etc/systemd/system` for both deb and rpm. After installing the package, you can start up Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo systemctl start logstash.service
+-------------------------------------------
+
+[[running-logstash-upstart]]
+==== Running Logstash by Using Upstart
+
+For systems that use upstart, you can start Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo initctl start logstash
+-------------------------------------------
+
+The auto-generated configuration file for upstart systems is `/etc/init/logstash.conf`.
+
+[[running-logstash-sysv]]
+==== Running Logstash by Using SysV
+
+For systems that use SysV, you can start Logstash with:
+
+[source,sh]
+--------------------------------------------
+sudo /etc/init.d/logstash start
+-------------------------------------------
+
+The auto-generated configuration file for SysV systems is `/etc/init.d/logstash`.
diff --git a/docs/static/settings-file.asciidoc b/docs/static/settings-file.asciidoc
new file mode 100644
index 00000000000..a646b5852ab
--- /dev/null
+++ b/docs/static/settings-file.asciidoc
@@ -0,0 +1,142 @@
+[[logstash-settings-file]]
+=== Settings File
+
+You can set options in the Logstash settings file, `logstash.yml`, to control Logstash execution. For example,
+you can specify pipeline settings, the location of configuration files, logging options, and other settings.
+Most of the settings in the `logstash.yml` file are also available as <<command-line-flags,command-line flags>>
+when you run Logstash. Any flags that you set at the command line override the corresponding settings in the
+`logstash.yml` file.
+
+The `logstash.yml` file, which is written in http://http://yaml.org/[YAML], is located in `LOGSTASH_HOME/config`. You can
+specify settings in hierarchical form or use flat keys. For example, to use hierarchical form to set the pipeline batch
+size and batch delay, you specify:
+
+[source,yaml]
+-------------------------------------------------------------------------------------
+pipeline:
+  batch:
+    size: 125
+    delay: 5
+-------------------------------------------------------------------------------------
+
+To express the same values as flat keys, you specify:
+
+[source,yaml]
+-------------------------------------------------------------------------------------
+pipeline.batch.size: 125
+pipeline.batch.delay: 5
+-------------------------------------------------------------------------------------
+
+The `logstash.yml` file includes the following settings:
+
+[options="header"]
+|=======================================================================
+| Setting | Description | Default value
+
+| `node.name`
+| A descriptive name for the node.
+| Machine's hostname
+
+| `path.data`
+| The directory that Logstash and its plugins use for any persistent needs.
+|`LOGSTASH_HOME/data`
+
+| `pipeline.workers`
+| The number of workers that will, in parallel, execute the filter and output stages of the pipeline.
+  If you find that events are backing up, or that the
+  CPU is not saturated, consider increasing this number to better utilize machine processing power.
+| Number of the host's CPU cores
+
+| `pipeline.output.workers`
+| The number of workers to use per output plugin instance.
+| `1`
+
+| `pipeline.batch.size`
+| The maximum number of events an individual worker thread will collect from inputs
+  before attempting to execute its filters and outputs.
+  Larger batch sizes are generally more efficient, but come at the cost of increased memory
+  overhead. You may have to increase the JVM heap size by setting the `LS_HEAP_SIZE`
+  variable to effectively use the option.
+| `125`
+
+| `pipeline.batch.delay`
+| When creating pipeline event batches, how long in milliseconds to wait before dispatching an undersized
+  batch to filters and workers.
+| `5`
+
+| `pipeline.unsafe_shutdown`
+| When set to `true`, forces Logstash to exit during shutdown even if there are still inflight events
+  in memory. By default, Logstash will refuse to quit until all received events
+  have been pushed to the outputs. Enabling this option can lead to data loss during shutdown.
+| `false`
+
+| `path.config`
+| The path to the Logstash config for the main pipeline. If you specify a directory or wildcard,
+  config files are read from the directory in alphabetical order.
+| Platform-specific. See <<dir-layout>>.
+
+| `config.string`
+| A string that contains the pipeline configuration to use for the main pipeline. Use the same syntax as
+  the config file.
+| None
+
+| `config.test_and_exit`
+| When set to `true`, checks that the configuration is valid and then exits. Note that grok patterns are not checked for
+  correctness with this setting. Logstash can read multiple config files from a directory. If you combine this
+  setting with `log.level: debug`, Logstash will log the combined config file, annotating
+  each config block with the source file it came from.
+| `false`
+
+| `config.reload.automatic`
+| When set to `true`, periodically checks if the configuration has changed and reloads the configuration whenever it is changed.
+  This can also be triggered manually through the SIGHUP signal.
+| `false`
+
+| `config.reload.interval`
+| How often in seconds Logstash checks the config files for changes.
+| `3`
+
+| `config.debug`
+| When set to `true`, shows the fully compiled configuration as a debug log message. You must also set `log.level: debug`.
+  WARNING: The log message will include any 'password' options passed to plugin configs as plaintext, and may result
+  in plaintext passwords appearing in your logs!
+| `false`
+
+| `http.host`
+| The bind address for the metrics REST endpoint.
+| `"127.0.0.1"`
+
+| `http.port`
+| The bind port for the metrics REST endpoint.
+| `9600`
+
+| `log.level`
+a|
+The log level. Valid options are:
+
+* `fatal` 
+* `error`
+* `warn`
+* `info`
+* `debug`
+* `trace`
+
+| `info`
+
+| `log.format`
+| The log format. Set to `json` to log in JSON format, or `plain` to use `Object#.inspect`.
+| `plain`
+
+| `path.logs`
+| The directory where Logstash will write its log to.
+| `LOGSTASH_HOME/logs
+
+| `path.plugins`
+| Where to find custom plugins. You can specify this setting multiple times to include
+  multiple paths. Plugins are expected to be in a specific directory hierarchy:
+  `PATH/logstash/TYPE/NAME.rb` where `TYPE` is `inputs`, `filters`, `outputs`, or `codecs`,
+  and `NAME` is the name of the plugin.
+| Platform-specific. See <<dir-layout>>.
+
+|=======================================================================
+
diff --git a/docs/static/stalled-shutdown.asciidoc b/docs/static/stalled-shutdown.asciidoc
new file mode 100644
index 00000000000..73a67baa1f1
--- /dev/null
+++ b/docs/static/stalled-shutdown.asciidoc
@@ -0,0 +1,56 @@
+[[stalled-shutdown]]
+=== Stalled Shutdown Detection
+
+When you attempt to shut down a running Logstash instance, Logstash performs several steps before it can safely shut down. It must:
+
+* Stop all input, filter and output plugins
+* Process all in-flight events
+* Terminate the Logstash process
+
+The following conditions affect the shutdown process:
+
+* An input plugin receiving data at a slow pace.
+* A slow filter, like a Ruby filter executing `sleep(10000)` or an Elasticsearch filter that is executing a very heavy
+query.
+* A disconnected output plugin that is waiting to reconnect to flush in-flight events.
+
+These situations make the duration and success of the shutdown process unpredictable.
+
+Logstash has a stall detection mechanism that analyzes the behavior of the pipeline and plugins during shutdown.
+This mechanism produces periodic information about the count of inflight events in internal queues and a list of busy
+worker threads.
+
+To enable Logstash to forcibly terminate in the case of a stalled shutdown, use the `--pipeline.unsafe_shutdown` flag when
+you start Logstash.
+
+[[shutdown-stall-example]]
+==== Stall Detection Example
+
+In this example, slow filter execution prevents the pipeline from clean shutdown. By starting Logstash with the
+`--pipeline.unsafe_shutdown` flag, quitting with *Ctrl+C* results in an eventual shutdown that loses 20 events.
+
+========
+[source,shell]
+bin/logstash -e 'input { generator { } } filter { ruby { code => "sleep 10000" } } 
+  output { stdout { codec => dots } }' -w 1 --pipeline.unsafe_shutdown
+Pipeline main started
+^CSIGINT received. Shutting down the agent. {:level=>:warn}
+stopping pipeline {:id=>"main", :level=>:warn}
+Received shutdown signal, but pipeline is still waiting for in-flight events
+to be processed. Sending another ^C will force quit Logstash, but this may cause
+data loss. {:level=>:warn}
+{"inflight_count"=>125, "stalling_thread_info"=>{["LogStash::Filters::Ruby", 
+{"code"=>"sleep 10000"}]=>[{"thread_id"=>19, "name"=>"[main]>worker0", 
+"current_call"=>"(ruby filter code):1:in `sleep'"}]}} {:level=>:warn}
+The shutdown process appears to be stalled due to busy or blocked plugins. 
+Check the logs for more information. {:level=>:error}
+{"inflight_count"=>125, "stalling_thread_info"=>{["LogStash::Filters::Ruby", 
+{"code"=>"sleep 10000"}]=>[{"thread_id"=>19, "name"=>"[main]>worker0", 
+"current_call"=>"(ruby filter code):1:in `sleep'"}]}} {:level=>:warn}
+{"inflight_count"=>125, "stalling_thread_info"=>{["LogStash::Filters::Ruby", 
+{"code"=>"sleep 10000"}]=>[{"thread_id"=>19, "name"=>"[main]>worker0", 
+"current_call"=>"(ruby filter code):1:in `sleep'"}]}} {:level=>:warn}
+Forcefully quitting logstash.. {:level=>:fatal}
+========
+
+When `--pipeline.unsafe_shutdown` isn't enabled, Logstash continues to run and produce these reports periodically.
\ No newline at end of file
diff --git a/docs/static/submitting-a-plugin.asciidoc b/docs/static/submitting-a-plugin.asciidoc
new file mode 100644
index 00000000000..d85db91a8ff
--- /dev/null
+++ b/docs/static/submitting-a-plugin.asciidoc
@@ -0,0 +1,107 @@
+[[submitting-plugin]]
+=== Submitting your plugin to RubyGems.org and the logstash-plugins repository
+
+Logstash uses http://rubygems.org[RubyGems.org] as its repository for all plugin
+artifacts. Once you have developed your new plugin, you can make it available to
+Logstash users by simply publishing it to RubyGems.org.
+
+==== Licensing
+Logstash and all its plugins are licensed under
+https://github.com/elasticsearch/logstash/blob/master/LICENSE[Apache License, version 2 ("ALv2")].
+If you make your plugin publicly available via http://rubygems.org[RubyGems.org],
+please make sure to have this line in your gemspec:
+
+* `s.licenses = ['Apache License (2.0)']`
+
+==== Publishing to http://rubygems.org[RubyGems.org]
+
+To begin, you’ll need an account on RubyGems.org
+
+* https://rubygems.org/sign_up[Sign-up for a RubyGems account].
+
+After creating an account,
+http://guides.rubygems.org/rubygems-org-api/#api-authorization[obtain] an API
+key from RubyGems.org. By default, RubyGems uses the file `~/.gem/credentials`
+to store your API key. These credentials will be used to publish the gem.
+Replace `username` and `password` with the credentials you created at
+RubyGems.org:
+
+[source,sh]
+----------------------------------
+curl -u username:password https://rubygems.org/api/v1/api_key.yaml > ~/.gem/credentials
+chmod 0600 ~/.gem/credentials
+----------------------------------
+
+Before proceeding, make sure you have the right version in your gemspec file
+and commit your changes.
+
+* `s.version = '0.1.0'`
+
+To publish version 0.1.0 of your new logstash gem:
+
+[source,sh]
+----------------------------------
+bundle install
+bundle exec rake vendor
+bundle exec rspec
+bundle exec rake publish_gem
+----------------------------------
+
+[NOTE]
+========
+Executing `rake publish_gem`:
+
+. Reads the version from the gemspec file (`s.version = '0.1.0'`)
+. Checks in your local repository if a tag exists for that version. If the tag
+already exists, it aborts the process. Otherwise, it creates a new version tag
+in your local repository.
+. Builds the gem
+. Publishes the gem to RubyGems.org
+========
+
+That's it! Your plugin is published! Logstash users can now install your plugin
+by running:
+
+[source,sh]
+[subs="attributes"]
+----------------------------------
+bin/plugin install logstash-{plugintype}-mypluginname
+----------------------------------
+
+==== Contributing your source code to https://github.com/logstash-plugins[logstash-plugins]
+
+It is not required to contribute your source code to
+https://github.com/logstash-plugins[logstash-plugins] github organization, but
+we always welcome new plugins!
+
+==== Benefits
+
+Some of the many benefits of having your plugin in the logstash-plugins
+repository are:
+
+* **Discovery** Your plugin will appear in the http://www.elasticsearch.org/guide/en/logstash/current/index.html[Logstash Reference],
+where Logstash users look first for plugins and documentation.
+* **Documentation** Your plugin documentation will automatically be added to the
+ http://www.elasticsearch.org/guide/en/logstash/current/index.html[Logstash Reference].
+* **Testing** With our testing infrastructure, your plugin will be continuously
+tested against current and future releases of Logstash.  As a result, users will
+have the assurance that if incompatibilities arise, they will be quickly
+discovered and corrected.
+
+==== Acceptance Guidelines
+
+* **Code Review** Your plugin must be reviewed by members of the community for
+coherence, quality, readability, stability and security.
+* **Tests** Your plugin must contain tests to be accepted.  These tests are also
+subject to code review for scope and completeness.  It's ok if you don't know
+how to write tests -- we will guide you. We are working on publishing a guide to
+creating tests for Logstash which will make it easier.  In the meantime, you can
+refer to http://betterspecs.org/ for examples.
+
+To begin migrating your plugin to logstash-plugins, simply create a new
+https://github.com/elasticsearch/logstash/issues[issue] in
+the Logstash repository. When the acceptance guidelines are completed, we will
+facilitate the move to the logstash-plugins organization using the recommended
+https://help.github.com/articles/transferring-a-repository/#transferring-from-a-user-to-an-organization[github process].
+
+pass::[<?edit_url?>]
diff --git a/docs/static/upgrading.asciidoc b/docs/static/upgrading.asciidoc
index 6e2d8337347..b4e8ca968ab 100644
--- a/docs/static/upgrading.asciidoc
+++ b/docs/static/upgrading.asciidoc
@@ -9,18 +9,29 @@ Before upgrading Logstash:
 * Test upgrades in a development environment before upgrading your production cluster.
 ===========================================
 
+If you are installing Logstash with other components in the Elastic Stack, also see the
+{stack}index.html[Elastic Stack installation and upgrade documentation].
+
+See the following topics for information about upgrading Logstash:
+
+* <<upgrading-using-package-managers>>
+* <<upgrading-using-direct-download>>
+* <<upgrading-logstash-5.0>>
+
+[[upgrading-using-package-managers]]
 === Upgrading Using Package Managers
 
 This procedure uses <<package-repositories,package managers>> to upgrade Logstash.
 
 1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
-2. Using the directions in the _Package Repositories_ section, update your repository links to point to the 2.0 repositories
+2. Using the directions in the _Package Repositories_ section, update your repository links to point to the 5.x repositories
 instead of the previous version.
 3. Run the `apt-get upgrade logstash` or `yum update logstash` command as appropriate for your operating system.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
-some Logstash plugins have changed in the 2.0 release.
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
+some Logstash plugins have changed in the 5.x release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
+[[upgrading-using-direct-download]]
 === Upgrading Using a Direct Download
 
 This procedure downloads the relevant Logstash binaries directly from Elastic.
@@ -28,82 +39,44 @@ This procedure downloads the relevant Logstash binaries directly from Elastic.
 1. Shut down your Logstash pipeline, including any inputs that send events to Logstash.
 2. Download the https://www.elastic.co/downloads/logstash[Logstash installation file] that matches your host environment.
 3. Unpack the installation file into your Logstash directory.
-4. Test your configuration file with the `logstash --configtest -f <configuration-file>` command. Configuration options for
-some Logstash plugins have changed in the 2.0 release.
+4. Test your configuration file with the `logstash --config.test_and_exit -f <configuration-file>` command. Configuration options for
+some Logstash plugins have changed in the 5.x release.
 5. Restart your Logstash pipeline after updating your configuration file.
 
-=== Upgrading Logstash and Elasticsearch to 2.0
-
-If you are using Elasticsearch as an output, and wish to upgrade to Elasticsearch 2.0, please be
-aware of https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking-changes-2.0.html[breaking changes]
-before you upgrade. In addition, the following steps needs to be performed after upgrading to Elasticsearch 2.0:
-
-**Mapping changes:** Users may have custom template changes, so by default a Logstash upgrade will
-leave the template as is. Even if you don't have a custom template, Logstash will not overwrite an existing
-template by default.
-
-There is one known issue (removal of https://www.elastic.co/guide/en/elasticsearch/reference/1.4/mapping-object-type.html#_path_3[path]) with using GeoIP filter that needs a manual update to the template.
-
-Note: If you have custom template changes, please make sure to save it and merge any changes. You can
-get the existing template by running:
-
-[source,shell]
-curl -XGET localhost:9200/_template/logstash
-
-
-Add the following option to your Logstash config:
+[[upgrading-logstash-5.0]]
+=== Upgrading Logstash to 5.0
 
-[source,json]
-output {
-	elasticsearch {
-		template_overwrite => true
-	}
-}
+Before upgrading Logstash, remember to read the <<breaking-changes,breaking changes>>.
 
-Restart Logstash.
+If you are installing Logstash with other components in the Elastic Stack, also see the
+{stack}index.html[Elastic Stack installation and upgrade documentation].
 
-**Dots in fields:** Elasticsearch 2.0 does not allow field names to contain the `.` character.
-Further details about this change https://www.elastic.co/guide/en/elasticsearch/reference/2.0/breaking_20_mapping_changes.html#_field_names_may_not_contain_dots[here]. Some plugins already have been updated to compensate
-for this breaking change, including logstash-filter-metrics and logstash-filter-elapsed.
-These plugin updates are available for Logstash 2.0. To upgrade to the latest version of these
-plugins, the command is:
+==== When to Upgrade
 
-[source,shell]
-bin/logstash-plugin update <plugin_name>
+Fresh installations can and should start with the same version across the Elastic Stack. 
 
-**Multiline Filter:** If you are using the Multiline Filter in your configuration and upgrade to Logstash 2.0,
-you will get an error. Make sure to explicitly set the number of filter workers (`-w`) to `1`. You can set the number
-of workers by passing a command line flag such as:
+Elasticsearch 5.0 does not require Logstash 5.0. An Elasticsearch 5.0 cluster will happily receive data from a
+Logstash 2.x instance via the default HTTP communication layer. This provides some flexibility to decide when to upgrade
+Logstash relative to an Elasticsearch upgrade. It may or may not be convenient for you to upgrade them together, and it
+is
+not required to be done at the same time as long as Elasticsearch is upgraded first.
 
-[source,shell]
-bin/logstash `-w 1`
+You should upgrade in a timely manner to get the performance improvements that come with Logstash 5.0, but do so in
+the way that makes the most sense for your environment.
 
-=== Upgrading Logstash to 2.2
+==== When Not to Upgrade
 
-Logstash 2.2 re-architected the pipeline stages to provide more performance and help future enhancements in resiliency.
-The new pipeline introduced micro-batches, processing groups of events at a time. The default batch size is
-125 per worker. Also, the filter and output stages are executed in the same thread, but still, as different stages.
-The CLI flag `--pipeline-workers` or `-w` control the number of execution threads, which is set by default to number of cores.
+If any Logstash plugin that you require is not compatible with Logstash 5.0, then you should wait until it is ready
+before upgrading.
 
-**Considerations for Elasticsearch Output**
-The default batch size of the pipeline is 125 events per worker. This will by default also be the bulk size
-used for the elasticsearch output. The Elasticsearch output's `flush_size` now acts only as a maximum bulk
-size (still defaulting to 500). For example, if your pipeline batch size is 3000 events, Elasticsearch
-Output will send 500 events at a time, in 6 separate bulk requests. In other words, for Elasticsearch output,
-bulk request size is chunked based on `flush_size` and `--pipeline-batch-size`. If `flush_size` is set greater
-than `--pipeline-batch-size`, it is ignored and `--pipeline-batch-size` will be used.
+Although we make great efforts to ensure compatibility, Logstash 5.0 is not completely backwards compatible. As noted
+in the Elastic Stack upgrade guide, Logstash 5.0 should not be upgraded before Elasticsearch 5.0. This is both
+practical and because some Logstash 5.0 plugins may attempt to use features of Elasticsearch 5.0 that did not exist
+in earlier versions. For example, if you attempt to send the 5.x template to a cluster before Elasticsearch 5.0, then it
+will not be able to use it and all indexing will fail likely fail. If you use your own, custom template with Logstash,
+then this issue can be ignored.
 
-The default number of output workers in Logstash 2.2 is now equal to the number of pipeline workers (`-w`)
-unless overridden in the Logstash config file. This can be problematic for some users as the
-extra workers may consume extra resources like file handles, especially in the case of the Elasticsearch
-output. Users with more than one Elasticsearch host may want to override the `workers` setting
-for the Elasticsearch output in their Logstash config to constrain that number to a low value, between 1 to 4.
+Note the Elasticsearch Output Index Template change in the <<breaking-changes>> documentation for further insight into
+this change and how it impacts operations.
 
-**Performance Tuning in 2.2**
-Since both filters and output workers are on the same thread, this could lead to threads being idle in I/O wait state.
-Thus, in 2.2, you can safely set `-w` to a number which is a multiple of the number of cores on your machine.
-A common way to tune performance is keep increasing the `-w` beyond the # of cores until performance no longer
-improves. A note of caution - make sure you also keep heapsize in mind, because the number of in-flight events
-are `#workers * batch_size * average_event size`. More in-flight events could add to memory pressure, eventually
-leading to Out of Memory errors. You can change the heapsize in Logstash by setting `LS_HEAP_SIZE`
 
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar b/gradle/wrapper/gradle-wrapper.jar
similarity index 74%
rename from logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar
rename to gradle/wrapper/gradle-wrapper.jar
index 13372aef5e2..ca78035ef05 100644
Binary files a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.jar and b/gradle/wrapper/gradle-wrapper.jar differ
diff --git a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties b/gradle/wrapper/gradle-wrapper.properties
similarity index 80%
rename from logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
rename to gradle/wrapper/gradle-wrapper.properties
index 25611753f15..36078a84d4d 100644
--- a/logstash-core-event-java/gradle/wrapper/gradle-wrapper.properties
+++ b/gradle/wrapper/gradle-wrapper.properties
@@ -1,6 +1,6 @@
-#Fri Jan 22 14:29:02 EST 2016
+#Wed Jun 29 13:06:17 PDT 2016
 distributionBase=GRADLE_USER_HOME
 distributionPath=wrapper/dists
 zipStoreBase=GRADLE_USER_HOME
 zipStorePath=wrapper/dists
-distributionUrl=https\://services.gradle.org/distributions/gradle-2.8-bin.zip
+distributionUrl=https\://services.gradle.org/distributions/gradle-2.13-bin.zip
diff --git a/logstash-core-event-java/gradlew b/gradlew
similarity index 97%
rename from logstash-core-event-java/gradlew
rename to gradlew
index 9d82f789151..27309d92314 100755
--- a/logstash-core-event-java/gradlew
+++ b/gradlew
@@ -6,12 +6,30 @@
 ##
 ##############################################################################
 
-# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
-DEFAULT_JVM_OPTS=""
+# Attempt to set APP_HOME
+# Resolve links: $0 may be a link
+PRG="$0"
+# Need this for relative symlinks.
+while [ -h "$PRG" ] ; do
+    ls=`ls -ld "$PRG"`
+    link=`expr "$ls" : '.*-> \(.*\)$'`
+    if expr "$link" : '/.*' > /dev/null; then
+        PRG="$link"
+    else
+        PRG=`dirname "$PRG"`"/$link"
+    fi
+done
+SAVED="`pwd`"
+cd "`dirname \"$PRG\"`/" >/dev/null
+APP_HOME="`pwd -P`"
+cd "$SAVED" >/dev/null
 
 APP_NAME="Gradle"
 APP_BASE_NAME=`basename "$0"`
 
+# Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
+DEFAULT_JVM_OPTS=""
+
 # Use the maximum available, or set MAX_FD != -1 to use that value.
 MAX_FD="maximum"
 
@@ -30,6 +48,7 @@ die ( ) {
 cygwin=false
 msys=false
 darwin=false
+nonstop=false
 case "`uname`" in
   CYGWIN* )
     cygwin=true
@@ -40,26 +59,11 @@ case "`uname`" in
   MINGW* )
     msys=true
     ;;
+  NONSTOP* )
+    nonstop=true
+    ;;
 esac
 
-# Attempt to set APP_HOME
-# Resolve links: $0 may be a link
-PRG="$0"
-# Need this for relative symlinks.
-while [ -h "$PRG" ] ; do
-    ls=`ls -ld "$PRG"`
-    link=`expr "$ls" : '.*-> \(.*\)$'`
-    if expr "$link" : '/.*' > /dev/null; then
-        PRG="$link"
-    else
-        PRG=`dirname "$PRG"`"/$link"
-    fi
-done
-SAVED="`pwd`"
-cd "`dirname \"$PRG\"`/" >/dev/null
-APP_HOME="`pwd -P`"
-cd "$SAVED" >/dev/null
-
 CLASSPATH=$APP_HOME/gradle/wrapper/gradle-wrapper.jar
 
 # Determine the Java command to use to start the JVM.
@@ -85,7 +89,7 @@ location of your Java installation."
 fi
 
 # Increase the maximum file descriptors if we can.
-if [ "$cygwin" = "false" -a "$darwin" = "false" ] ; then
+if [ "$cygwin" = "false" -a "$darwin" = "false" -a "$nonstop" = "false" ] ; then
     MAX_FD_LIMIT=`ulimit -H -n`
     if [ $? -eq 0 ] ; then
         if [ "$MAX_FD" = "maximum" -o "$MAX_FD" = "max" ] ; then
diff --git a/logstash-core-event-java/gradlew.bat b/gradlew.bat
similarity index 93%
rename from logstash-core-event-java/gradlew.bat
rename to gradlew.bat
index aec99730b4e..f6d5974e72f 100644
--- a/logstash-core-event-java/gradlew.bat
+++ b/gradlew.bat
@@ -8,14 +8,14 @@
 @rem Set local scope for the variables with windows NT shell
 if "%OS%"=="Windows_NT" setlocal
 
-@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
-set DEFAULT_JVM_OPTS=
-
 set DIRNAME=%~dp0
 if "%DIRNAME%" == "" set DIRNAME=.
 set APP_BASE_NAME=%~n0
 set APP_HOME=%DIRNAME%
 
+@rem Add default JVM options here. You can also use JAVA_OPTS and GRADLE_OPTS to pass JVM options to this script.
+set DEFAULT_JVM_OPTS=
+
 @rem Find java.exe
 if defined JAVA_HOME goto findJavaFromJavaHome
 
@@ -46,7 +46,7 @@ echo location of your Java installation.
 goto fail
 
 :init
-@rem Get command-line arguments, handling Windowz variants
+@rem Get command-line arguments, handling Windows variants
 
 if not "%OS%" == "Windows_NT" goto win9xME_args
 if "%@eval[2+2]" == "4" goto 4NT_args
diff --git a/integration/logstash_config/file_input_to_file_output_spec.rb b/integration/logstash_config/file_input_to_file_output_spec.rb
deleted file mode 100644
index b9a4fcfd5c9..00000000000
--- a/integration/logstash_config/file_input_to_file_output_spec.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-require "stud/temporary"
-
-describe "File input to File output" do
-  let(:number_of_events) { IO.readlines(sample_log).size }
-  let(:sample_log) { File.expand_path(File.join(File.dirname(__FILE__), "..", "support", "sample.log")) }
-  let(:output_file) { Stud::Temporary.file.path }
-  let(:config) { 
-<<EOS
-    input {
-       file {
-         path => \"#{sample_log}\"
-         stat_interval => 0
-         start_position => \"beginning\"
-         sincedb_path => \"/dev/null\"
-       }
-      }
-    output {
-      file {
-        path => \"#{output_file}\"
-      }
-    }
-EOS
-  }
-
-  before :all do
-    command("bin/logstash-plugin install logstash-input-file logstash-output-file")
-  end
-
-  it "writes events to file" do
-    cmd = "bin/logstash -e '#{config}'"
-    launch_logstash(cmd)
-
-    expect(File.exist?(output_file)).to eq(true)
-
-    # on shutdown the events arent flushed to disk correctly
-    # Known issue https://github.com/logstash-plugins/logstash-output-file/issues/12
-    expect(IO.readlines(output_file).size).to be_between(number_of_events - 10, number_of_events).inclusive
-  end
-end
diff --git a/integration/plugin_manager/logstash_spec.rb b/integration/plugin_manager/logstash_spec.rb
deleted file mode 100644
index f7047e986a2..00000000000
--- a/integration/plugin_manager/logstash_spec.rb
+++ /dev/null
@@ -1,11 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-require_relative "../../logstash-core/lib/logstash/version"
-
-describe "bin/logstash" do
-  it "returns the logstash version" do
-    result = command("bin/logstash --version")
-    expect(result.exit_status).to eq(0)
-    expect(result.stdout).to match(/^logstash\s#{LOGSTASH_VERSION}/)
-  end
-end
diff --git a/integration/plugin_manager/plugin_install_spec.rb b/integration/plugin_manager/plugin_install_spec.rb
deleted file mode 100644
index db31bc95740..00000000000
--- a/integration/plugin_manager/plugin_install_spec.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-require "fileutils"
-
-context "bin/logstash-plugin install" do
-  context "with a local gem" do
-    let(:gem_name) { "logstash-input-wmi" }
-    let(:local_gem) { gem_fetch(gem_name) }
-
-    it "install the gem succesfully" do
-      result = command("bin/logstash-plugin install --no-verify #{local_gem}")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout).to match(/^Installing\s#{gem_name}\nInstallation\ssuccessful$/)
-    end
-  end
-
-  context "when the plugin exist" do
-    let(:plugin_name) { "logstash-input-drupal_dblog" }
-
-    it "sucessfully install" do
-      result = command("bin/logstash-plugin install #{plugin_name}")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout).to match(/^Validating\s#{plugin_name}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
-    end
-
-    it "allow to install a specific version" do
-      version = "2.0.2"
-      result = command("bin/logstash-plugin install --version 2.0.2 #{plugin_name}")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout).to match(/^Validating\s#{plugin_name}-#{version}\nInstalling\s#{plugin_name}\nInstallation\ssuccessful$/)
-    end
-  end
-
-  context "when the plugin doesn't exist" do
-    it "fails to install" do
-      result = command("bin/logstash-plugin install --no-verify logstash-output-impossible-plugin")
-      expect(result.exit_status).to eq(1)
-      expect(result.stderr).to match(/Installation Aborted, message: Could not find gem/)
-    end
-  end
-end
diff --git a/integration/plugin_manager/plugin_list_spec.rb b/integration/plugin_manager/plugin_list_spec.rb
deleted file mode 100644
index d2ae7807f1c..00000000000
--- a/integration/plugin_manager/plugin_list_spec.rb
+++ /dev/null
@@ -1,41 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-
-describe "bin/logstash-plugin list" do
-  context "without a specific plugin" do
-    it "display a list of plugins" do
-      result = command("bin/logstash-plugin list")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout.split("\n").size).to be > 1
-    end
-
-    it "display a list of installed plugins" do
-      result = command("bin/logstash-plugin list --installed")
-      expect(result.exit_status).to eq(0)
-      expect(result.stdout.split("\n").size).to be > 1
-    end
-
-    it "list the plugins with their versions" do
-      result = command("bin/logstash-plugin list --verbose")
-      result.stdout.split("\n").each do |plugin|
-        expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+\)/)
-      end
-      expect(result.exit_status).to eq(0)
-    end
-  end
-
-  context "with a specific plugin" do
-    let(:plugin_name) { "logstash-input-stdin" }
-    it "list the plugin and display the plugin name" do
-      result = command("bin/logstash-plugin list #{plugin_name}")
-      expect(result.stdout).to match(/^#{plugin_name}$/)
-      expect(result.exit_status).to eq(0)
-    end
-
-    it "list the plugin with his version" do
-      result = command("bin/logstash-plugin list --verbose #{plugin_name}")
-      expect(result.stdout).to match(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
-      expect(result.exit_status).to eq(0)
-    end
-  end
-end
diff --git a/integration/plugin_manager/plugin_uninstall_spec.rb b/integration/plugin_manager/plugin_uninstall_spec.rb
deleted file mode 100644
index df3c6e4396e..00000000000
--- a/integration/plugin_manager/plugin_uninstall_spec.rb
+++ /dev/null
@@ -1,24 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-
-describe "bin/logstash-plugin uninstall" do
-  context "when the plugin isn't installed" do
-    it "fails to uninstall it" do
-      result = command("bin/logstash-plugin uninstall logstash-filter-cidr")
-      expect(result.stderr).to match(/ERROR: Uninstall Aborted, message: This plugin has not been previously installed, aborting/)
-      expect(result.exit_status).to eq(1)
-    end
-  end
-
-  context "when the plugin is installed" do
-      it "succesfully uninstall it" do
-      # make sure we have the plugin installed.
-      command("bin/logstash-plugin install logstash-filter-ruby")
-
-      result = command("bin/logstash-plugin uninstall logstash-filter-ruby")
-
-      expect(result.stdout).to match(/^Uninstalling logstash-filter-ruby/)
-      expect(result.exit_status).to eq(0)
-    end
-  end
-end
diff --git a/integration/plugin_manager/plugin_update_spec.rb b/integration/plugin_manager/plugin_update_spec.rb
deleted file mode 100644
index 549a9babc80..00000000000
--- a/integration/plugin_manager/plugin_update_spec.rb
+++ /dev/null
@@ -1,32 +0,0 @@
-# Encoding: utf-8
-require_relative "../spec_helper"
-
-describe "update" do
-  let(:plugin_name) { "logstash-input-stdin" }
-  let(:previous_version) { "2.0.1" }
-
-  before do
-    command("bin/logstash-plugin install --version #{previous_version} #{plugin_name}")
-    cmd = command("bin/logstash-plugin list --verbose #{plugin_name}")
-    expect(cmd.stdout).to match(/#{plugin_name} \(#{previous_version}\)/)
-  end
-
-  context "update a specific plugin" do
-    subject { command("bin/logstash-plugin update #{plugin_name}") }
-
-    it "has executed succesfully" do
-      expect(subject.exit_status).to eq(0)
-      expect(subject.stdout).to match(/Updating #{plugin_name}/)
-    end
-  end
-
-  context "update all the plugins" do
-    subject { command("bin/logstash-plugin update") }
-
-    it "has executed succesfully" do
-      expect(subject.exit_status).to eq(0)
-      cmd = command("bin/logstash-plugin list --verbose #{plugin_name}").stdout
-      expect(cmd).to match(/logstash-input-stdin \(#{LogStashTestHelpers.latest_version(plugin_name)}\)/)
-    end
-  end
-end
diff --git a/integration/spec_helper.rb b/integration/spec_helper.rb
deleted file mode 100644
index f4cddfa713d..00000000000
--- a/integration/spec_helper.rb
+++ /dev/null
@@ -1,37 +0,0 @@
-# encoding: utf-8
-require_relative "support/integration_test_helpers"
-require_relative "../logstash-core/lib/logstash/environment"
-require "fileutils"
-
-if LogStash::Environment.windows?
-  puts "[integration] Theses integration test are specifically made to be run on under linux/unix"
-  puts "[integration] Please see our windows version of the tests https://github.com/elastic/logstash/tree/master/test/windows"
-end
-
-# Configure the test environment
-source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-integration_path = File.join(source, "integration_run")
-
-puts "[integration_spec] configure environment"
-
-if Dir.exists?(integration_path)
-  # We copy the current logstash into a temporary directory
-  # since the tests are a bit destructive
-  FileUtils.mkdir_p(integration_path)
-  rsync_cmd = "rsync -a --delete --exclude 'rspec' --exclude '#{File.basename(integration_path)}' --exclude 'integration_spec' --exclude '.git' #{source} #{integration_path}"
-
-  puts "[integration_spec] Rsync source code into: #{integration_path}"
-  system(rsync_cmd)
-  puts "[integration_spec] Finish rsync"
-
-  LOGSTASH_TEST_PATH = File.join(integration_path, "logstash")
-else
-  LOGSTASH_TEST_PATH = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-end
-
-puts "[integration_spec] Running the test in #{LOGSTASH_TEST_PATH}"
-puts "[integration_spec] Running specs"
-
-RSpec.configure do |config|
-  config.order = "random"
-end
diff --git a/integration/support/integration_test_helpers.rb b/integration/support/integration_test_helpers.rb
deleted file mode 100644
index aad90f8f07a..00000000000
--- a/integration/support/integration_test_helpers.rb
+++ /dev/null
@@ -1,89 +0,0 @@
-# encoding: utf-8
-require "json"
-require "open3"
-require "open-uri"
-require "stud/temporary"
-require "fileutils"
-require "bundler"
-require "gems"
-
-class CommandResponse
-  attr_reader :stdin, :stdout, :stderr, :exit_status
-
-  def initialize(cmd, stdin, stdout, stderr, exit_status)
-    @stdin = stdin
-    @stdout = stdout
-    @stderr = stderr
-    @exit_status = exit_status
-    @cmd = cmd
-  end
-
-  def to_debug
-    "DEBUG: stdout: #{stdout}, stderr: #{stderr}, exit_status: #{exit_status}"
-  end
-
-  def to_s
-    @cmd
-  end
-end
-
-def command(cmd, path = nil)
-  # http://bundler.io/v1.3/man/bundle-exec.1.html
-  # see shelling out.
-  #
-  # Since most of the integration test are environment destructive
-  # its better to run them in a cloned directory.
-  path = LOGSTASH_TEST_PATH if path == nil
-
-  Bundler.with_clean_env do
-    Dir.chdir(path) do
-      Open3.popen3(cmd) do |stdin, stdout, stderr, wait_thr|
-          CommandResponse.new(cmd,
-            stdin,
-            stdout.read.chomp,
-            stderr.read.chomp,
-            wait_thr.value.exitstatus)
-      end
-    end
-  end
-end
-
-def gem_fetch(name)
-  tmp = Stud::Temporary.directory
-  FileUtils.mkdir_p(tmp)
-
-  c = command("gem fetch #{name}", tmp)
-
-  if c.exit_status == 1
-    raise RuntimeError, "Can't fetch gem #{name}"
-  end
-
-  return Dir.glob(File.join(tmp, "#{name}*.gem")).first
-end
-
-# This is a bit hacky since JRuby doesn't support fork,
-# we use popen4 which return the pid of the process and make sure we kill it
-# after letting it run for a few seconds.
-def launch_logstash(cmd, path = nil)
-  path = LOGSTASH_TEST_PATH if path == nil
-  pid = 0
-
-  Thread.new do
-    Bundler.with_clean_env do
-      Dir.chdir(path) do
-        pid, input, output, error = IO.popen4(cmd) #jruby only
-      end
-    end
-  end
-  sleep(30)
-  begin
-    Process.kill("INT", pid)
-  rescue
-  end
-end
-
-module LogStashTestHelpers
-  def self.latest_version(name)
-    Gems.versions(name).first["number"] 
-  end
-end
diff --git a/integration/support/sample.log b/integration/support/sample.log
deleted file mode 100644
index 8f304b59c45..00000000000
--- a/integration/support/sample.log
+++ /dev/null
@@ -1,50 +0,0 @@
-83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-search.png HTTP/1.1" 200 203023 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard3.png HTTP/1.1" 200 171717 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/highlight/highlight.js HTTP/1.1" 200 26185 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:44 +0000] "GET /presentations/logstash-monitorama-2013/plugin/zoom-js/zoom.js HTTP/1.1" 200 7697 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/plugin/notes/notes.js HTTP/1.1" 200 2892 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:42 +0000] "GET /presentations/logstash-monitorama-2013/images/sad-medic.png HTTP/1.1" 200 430406 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Bold.ttf HTTP/1.1" 200 38720 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/css/fonts/Roboto-Regular.ttf HTTP/1.1" 200 41820 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:45 +0000] "GET /presentations/logstash-monitorama-2013/images/frontend-response-codes.png HTTP/1.1" 200 52878 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard.png HTTP/1.1" 200 321631 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/Dreamhost_logo.svg HTTP/1.1" 200 2126 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:43 +0000] "GET /presentations/logstash-monitorama-2013/images/kibana-dashboard2.png HTTP/1.1" 200 394967 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/apache-icon.gif HTTP/1.1" 200 8095 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/nagios-sms5.png HTTP/1.1" 200 78075 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/redis.png HTTP/1.1" 200 25230 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/elasticsearch.png HTTP/1.1" 200 8026 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/logstashbook.png HTTP/1.1" 200 54662 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/github-contributions.png HTTP/1.1" 200 34245 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/css/print/paper.css HTTP/1.1" 200 4254 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:47 +0000] "GET /presentations/logstash-monitorama-2013/images/1983_delorean_dmc-12-pic-38289.jpeg HTTP/1.1" 200 220562 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/simple-inputs-filters-outputs.jpg HTTP/1.1" 200 1168622 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:46 +0000] "GET /presentations/logstash-monitorama-2013/images/tiered-outputs-to-inputs.jpg HTTP/1.1" 200 1079983 "http://semicomplete.com/presentations/logstash-monitorama-2013/" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-83.149.9.216 - - [26/Aug/2014:21:13:53 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Macintosh; Intel Mac OS X 10_9_1) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/32.0.1700.77 Safari/537.36"
-24.236.252.67 - - [26/Aug/2014:21:14:10 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Ubuntu; Linux x86_64; rv:26.0) Gecko/20100101 Firefox/26.0"
-93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1" 200 18848 "http://www.google.ro/url?sa=t&rct=j&q=&esrc=s&source=web&cd=2&ved=0CCwQFjAB&url=http%3A%2F%2Fwww.semicomplete.com%2Farticles%2Fdynamic-dns-with-dhcp%2F&ei=W88AU4n9HOq60QXbv4GwBg&usg=AFQjCNEF1X4Rs52UYQyLiySTQxa97ozM4g&bvm=bv.61535280,d.d2k" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:32 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/articles/dynamic-dns-with-dhcp/" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-93.114.45.13 - - [26/Aug/2014:21:14:33 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (X11; Linux x86_64; rv:25.0) Gecko/20100101 Firefox/25.0"
-66.249.73.135 - - [26/Aug/2014:21:15:03 +0000] "GET /blog/tags/ipv6 HTTP/1.1" 200 12251 "-" "Mozilla/5.0 (iPhone; CPU iPhone OS 6_0 like Mac OS X) AppleWebKit/536.26 (KHTML, like Gecko) Version/6.0 Mobile/10A5376e Safari/8536.25 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-50.16.19.13 - - [26/Aug/2014:21:15:15 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "http://www.semicomplete.com/blog/tags/puppet?flav=rss20" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
-66.249.73.185 - - [26/Aug/2014:21:15:23 +0000] "GET / HTTP/1.1" 200 37932 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-110.136.166.128 - - [26/Aug/2014:21:16:11 +0000] "GET /projects/xdotool/ HTTP/1.1" 200 12292 "http://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=5&cad=rja&sqi=2&ved=0CFYQFjAE&url=http%3A%2F%2Fwww.semicomplete.com%2Fprojects%2Fxdotool%2F&ei=6cwAU_bRHo6urAeI0YD4Ag&usg=AFQjCNE3V_aCf3-gfNcbS924S6jZ6FqffA&bvm=bv.61535280,d.bmk" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-46.105.14.53 - - [26/Aug/2014:21:16:17 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/projects/xdotool/" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-123.125.71.35 - - [26/Aug/2014:21:16:31 +0000] "GET /blog/tags/release HTTP/1.1" 200 40693 "-" "Mozilla/5.0 (compatible; Baiduspider/2.0; +http://www.baidu.com/search/spider.html)"
-110.136.166.128 - - [26/Aug/2014:21:16:22 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (Windows NT 6.2; WOW64; rv:28.0) Gecko/20100101 Firefox/28.0"
-50.150.204.184 - - [26/Aug/2014:21:17:06 +0000] "GET /images/googledotcom.png HTTP/1.1" 200 65748 "http://www.google.com/search?q=https//:google.com&source=lnms&tbm=isch&sa=X&ei=4-r8UvDrKZOgkQe7x4CICw&ved=0CAkQ_AUoAA&biw=320&bih=441" "Mozilla/5.0 (Linux; U; Android 4.0.4; en-us; LG-MS770 Build/IMM76I) AppleWebKit/534.30 (KHTML, like Gecko) Version/4.0 Mobile Safari/534.30"
-207.241.237.225 - - [26/Aug/2014:21:17:35 +0000] "GET /blog/tags/examples HTTP/1.0" 200 9208 "http://www.semicomplete.com/blog/tags/C" "Mozilla/5.0 (compatible; archive.org_bot +http://www.archive.org/details/archive.org_bot)"
-200.49.190.101 - - [26/Aug/2014:21:17:39 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
-200.49.190.100 - - [26/Aug/2014:21:17:37 +0000] "GET /blog/tags/web HTTP/1.1" 200 44019 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
-200.49.190.101 - - [26/Aug/2014:21:17:41 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
-200.49.190.101 - - [26/Aug/2014:21:17:48 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "QS304 Profile/MIDP-2.0 Configuration/CLDC-1.1"
-66.249.73.185 - - [26/Aug/2014:21:18:48 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-66.249.73.135 - - [26/Aug/2014:21:18:55 +0000] "GET /blog/tags/munin HTTP/1.1" 200 9746 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
-66.249.73.135 - - [26/Aug/2014:21:19:16 +0000] "GET /blog/tags/firefox?flav=rss20 HTTP/1.1" 200 16021 "-" "Mozilla/5.0 (compatible; Googlebot/2.1; +http://www.google.com/bot.html)"
diff --git a/lib/bootstrap/bundler.rb b/lib/bootstrap/bundler.rb
index 2948fe8aa29..13c04b58ab9 100644
--- a/lib/bootstrap/bundler.rb
+++ b/lib/bootstrap/bundler.rb
@@ -78,7 +78,7 @@ def setup!(options = {})
     # @return [String, Exception] the installation captured output and any raised exception or nil if none
     def invoke!(options = {})
       options = {:max_tries => 10, :clean => false, :install => false, :update => false, :local => false,
-                 :all => false, :package => false, :without => [:development]}.merge(options)
+                 :jobs => 12, :all => false, :package => false, :without => [:development]}.merge(options)
       options[:without] = Array(options[:without])
       options[:update] = Array(options[:update]) if options[:update]
 
@@ -104,13 +104,22 @@ def invoke!(options = {})
       ::Bundler.settings[:gemfile] = LogStash::Environment::GEMFILE_PATH
       ::Bundler.settings[:without] = options[:without].join(":")
 
+      if !debug?
+        # Will deal with transient network errors
+        execute_bundler_with_retry(options)
+      else
+        options[:verbose] = true
+        execute_bundler(options)
+      end
+    end
+
+    def execute_bundler_with_retry(options)
       try = 0
       # capture_stdout also traps any raised exception and pass them back as the function return [output, exception]
       output, exception = capture_stdout do
         loop do
           begin
-            ::Bundler.reset!
-            ::Bundler::CLI.start(bundler_arguments(options))
+            execute_bundler(options)
             break
           rescue ::Bundler::VersionConflict => e
             $stderr.puts("Plugin version conflict, aborting")
@@ -132,12 +141,20 @@ def invoke!(options = {})
           end
         end
       end
-
       raise exception if exception
 
       return output
     end
 
+    def execute_bundler(options)
+      ::Bundler.reset!
+      ::Bundler::CLI.start(bundler_arguments(options))
+    end
+
+    def debug?
+      ENV["DEBUG"]
+    end
+
     # build Bundler::CLI.start arguments array from the given options hash
     # @param option [Hash] the invoke! options hash
     # @return [Array<String>] Bundler::CLI.start string arguments array
@@ -162,6 +179,8 @@ def bundler_arguments(options = {})
         arguments << "--all" if options[:all]
       end
 
+      arguments << "--verbose" if options[:verbose]
+
       arguments.flatten
     end
 
diff --git a/lib/bootstrap/environment.rb b/lib/bootstrap/environment.rb
index 66ed16093f0..0411685f19a 100644
--- a/lib/bootstrap/environment.rb
+++ b/lib/bootstrap/environment.rb
@@ -56,20 +56,15 @@ def pattern_path(path)
   end
 end
 
-
 # when launched as a script, not require'd, (currently from bin/logstash and bin/logstash-plugin) the first
 # argument is the path of a Ruby file to require and a LogStash::Runner class is expected to be
 # defined and exposing the LogStash::Runner#main instance method which will be called with the current ARGV
 # currently lib/logstash/runner.rb and lib/pluginmanager/main.rb are called using this.
 if $0 == __FILE__
   LogStash::Bundler.setup!({:without => [:build, :development]})
+  require_relative "patches/jar_dependencies"
+
   require ARGV.shift
-  # TODO deprecate these arguments in the next major version. use -i only
-  if ARGV == ["irb"] || ARGV == ["pry"]
-    puts "Warn: option \"#{ARGV.first}\" is deprecated, use \"-i #{ARGV.first}\" or \"--interactive=#{ARGV.first}\" instead"
-    exit_status = LogStash::Runner.run("bin/logstash", ["--interactive", ARGV.first])
-  else
-    exit_status = LogStash::Runner.run("bin/logstash", ARGV)
-  end
+  exit_status = LogStash::Runner.run("bin/logstash", ARGV)
   exit(exit_status || 0)
 end
diff --git a/lib/bootstrap/patches/gems.rb b/lib/bootstrap/patches/gems.rb
new file mode 100644
index 00000000000..94154a51345
--- /dev/null
+++ b/lib/bootstrap/patches/gems.rb
@@ -0,0 +1,16 @@
+# encoding: utf-8
+require "gems"
+
+# This patch is necessary to avoid encoding problems when Net:HTTP return stuff in ASCII format, but
+# consumer libraries, like the YAML parsers expect them to be in UTF-8. As we're using UTF-8 everywhere
+# and the usage of versions is minimal in our codebase, the patch is done here. If extended usage of this
+# is done in the feature, more proper fix should be implemented, including the creation of our own lib for
+# this tasks.
+module Gems
+  module Request
+    def get(path, data = {}, content_type = 'application/x-www-form-urlencoded', request_host = host)
+      request(:get, path, data, content_type, request_host).force_encoding("UTF-8")
+    end
+  end
+end
+
diff --git a/lib/bootstrap/patches/jar_dependencies.rb b/lib/bootstrap/patches/jar_dependencies.rb
new file mode 100644
index 00000000000..2908ab73a82
--- /dev/null
+++ b/lib/bootstrap/patches/jar_dependencies.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require "jar_dependencies"
+
+def require_jar( *args )
+  return nil unless Jars.require?
+  result = Jars.require_jar( *args )
+  if result.is_a? String
+    # JAR_DEBUG=1 will now show theses
+    Jars.debug { "--- jar coordinate #{args[0..-2].join( ':' )} already loaded with version #{result} - omit version #{args[-1]}" }
+    Jars.debug { "    try to load from #{caller.join("\n\t")}" }
+    return false
+  end
+  Jars.debug { "    register #{args.inspect} - #{result == true}" }
+  result
+end
diff --git a/lib/bootstrap/rspec.rb b/lib/bootstrap/rspec.rb
index f32057c7f9c..4c95f3bfc76 100755
--- a/lib/bootstrap/rspec.rb
+++ b/lib/bootstrap/rspec.rb
@@ -7,6 +7,7 @@
 
 require "rspec/core"
 require "rspec"
+require 'ci/reporter/rake/rspec_loader'
 
 status = RSpec::Core::Runner.run(ARGV.empty? ? ["spec"] : ARGV).to_i
 exit status if status != 0
diff --git a/lib/bootstrap/rubygems.rb b/lib/bootstrap/rubygems.rb
index 06e1775c380..f11b792e7e3 100644
--- a/lib/bootstrap/rubygems.rb
+++ b/lib/bootstrap/rubygems.rb
@@ -43,6 +43,16 @@ def self.reset
       end
     end
 
+    ##
+    # Take a plugin name and get the latest versions available in the gem repository.
+    # @param [String] The plugin name
+    # @return [Hash] The collection of registered versions
+    ##
+    def versions(plugin)
+      require "gems"
+      require_relative "patches/gems"
+      Gems.versions(plugin)
+    end
     # Take a gem package and extract it to a specific target
     # @param [String] Gem file, this must be a path
     # @param [String, String] Return a Gem::Package and the installed path
diff --git a/lib/pluginmanager/gemfile.rb b/lib/pluginmanager/gemfile.rb
index b1648187764..68bf88334f1 100644
--- a/lib/pluginmanager/gemfile.rb
+++ b/lib/pluginmanager/gemfile.rb
@@ -41,13 +41,20 @@ def add(name, *requirements)
       @gemset.add_gem(Gem.parse(name, *requirements))
     end
 
-    # update existing or add new
+    # update existing or add new and merge passed options with current gem options if it exists
     # @param name [String] gem name
     # @param *requirements params following name use the same notation as the Gemfile gem DSL statement
     def update(name, *requirements)
       @gemset.update_gem(Gem.parse(name, *requirements))
     end
 
+    # overwrite existing or add new
+    # @param name [String] gem name
+    # @param *requirements params following name use the same notation as the Gemfile gem DSL statement
+    def overwrite(name, *requirements)
+      @gemset.overwrite_gem(Gem.parse(name, *requirements))
+    end
+
     # @return [Gem] removed gem or nil if not found
     def remove(name)
       @gemset.remove_gem(name)
@@ -99,6 +106,19 @@ def add_gem(_gem)
 
     # update existing or add new
     def update_gem(_gem)
+      if old = find_gem(_gem.name)
+        # always overwrite requirements if specified
+        old.requirements = _gem.requirements unless no_constrains?(_gem.requirements)
+        # but merge options
+        old.options = old.options.merge(_gem.options)
+      else
+        @gems << _gem
+        @gems_by_name[_gem.name.downcase] = _gem
+      end
+    end
+
+    # update existing or add new
+    def overwrite_gem(_gem)
       if old = find_gem(_gem.name)
         @gems[@gems.index(old)] = _gem
       else
@@ -119,8 +139,19 @@ def remove_gem(name)
     def copy
       Marshal.load(Marshal.dump(self))
     end
+
     private
 
+    def no_constrains?(requirements)
+      return true if requirements.nil? || requirements.empty?
+
+      # check for the dummy ">= 0" version constrain or any variations thereof
+      # which is in fact a "no constrain" constrain which we should discard
+      return true if requirements.size == 1 && requirements.first.to_s.gsub(/\s+/, "") == ">=0"
+
+      false
+    end
+
     def sources_to_s
       return "" if @sources.empty?
       @sources.map{|source| "source #{source.inspect}"}.join("\n")
diff --git a/lib/pluginmanager/generate.rb b/lib/pluginmanager/generate.rb
new file mode 100644
index 00000000000..6717682e021
--- /dev/null
+++ b/lib/pluginmanager/generate.rb
@@ -0,0 +1,92 @@
+# encoding: utf-8
+require "pluginmanager/command"
+require "pluginmanager/templates/render_context"
+require "erb"
+require "ostruct"
+require "fileutils"
+require "pathname"
+
+class LogStash::PluginManager::Generate < LogStash::PluginManager::Command
+
+  TYPES = [ "input", "filter", "output", "codec" ]
+
+  option "--type", "TYPE", "Type of the plugin {input, filter, codec, output}s", :required => true
+  option "--name", "PLUGIN", "Name of the new plugin", :required => true
+  option "--path", "PATH", "Location where the plugin skeleton will be created", :default => Dir.pwd
+
+  def execute
+    validate_params
+    source = File.join(File.dirname(__FILE__), "templates", "#{type}-plugin")
+    @target_path = File.join(path, full_plugin_name)
+    FileUtils.mkdir(@target_path)
+    puts " Creating #{@target_path}"
+
+    begin
+      create_scaffold(source, @target_path)
+    rescue Errno::EACCES => exception
+      report_exception("Permission denied when executing the plugin manager", exception)
+    rescue => exception
+      report_exception("Plugin creation Aborted", exception)
+    end
+  end
+
+  private
+
+  def validate_params
+    raise(ArgumentError, "should be one of: input, filter, codec or output") unless TYPES.include?(type)
+  end
+
+  def create_scaffold(source, target)
+    transform_r(source, target)
+  end
+
+  def transform_r(source, target)
+    Dir.entries(source).each do |entry|
+      next if [ ".", ".." ].include?(entry)
+      source_entry = File.join(source, entry)
+      target_entry = File.join(target, entry)
+
+      if File.directory?(source_entry)
+        FileUtils.mkdir(target_entry) unless File.exists?(target_entry)
+        transform_r(source_entry, target_entry)
+      else
+        # copy the new file, in case of being an .erb file should render first
+        if source_entry.end_with?("erb")
+          target_entry = target_entry.gsub(/.erb$/,"").gsub("example", name)
+          File.open(target_entry, "w") { |f| f.write(render(source_entry)) }
+        else
+          FileUtils.cp(source_entry, target_entry)
+        end
+        puts "\t create #{File.join(full_plugin_name, Pathname.new(target_entry).relative_path_from(Pathname.new(@target_path)))}"
+      end
+    end
+  end
+
+  def render(source)
+    template = File.read(source)
+    renderer = ERB.new(template)
+    context  = LogStash::PluginManager::RenderContext.new(options)
+    renderer.result(context.get_binding)
+  end
+
+  def options
+    git_data = get_git_info
+    @options ||= {
+      :plugin_name => name,
+      :author => git_data.author,
+      :email  => git_data.email,
+      :min_version => "2.0",
+    }
+  end
+
+  def get_git_info
+    git = OpenStruct.new
+    git.author = %x{ git config --get user.name  }.strip rescue "your_username"
+    git.email  = %x{ git config --get user.email }.strip rescue "your_username@example.com"
+    git
+  end
+
+  def full_plugin_name
+    @full_plugin_name ||= "logstash-#{type}-#{name.downcase}"
+  end
+end
diff --git a/lib/pluginmanager/install.rb b/lib/pluginmanager/install.rb
index dae85e3dfc5..0e6383d725b 100644
--- a/lib/pluginmanager/install.rb
+++ b/lib/pluginmanager/install.rb
@@ -9,6 +9,7 @@ class LogStash::PluginManager::Install < LogStash::PluginManager::Command
   parameter "[PLUGIN] ...", "plugin name(s) or file", :attribute_name => :plugins_arg
   option "--version", "VERSION", "version of the plugin to install"
   option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
+  option "--preserve", :flag, "preserve current gem options", :default => false
   option "--development", :flag, "install all development dependencies of currently installed plugins", :default => false
   option "--local", :flag, "force local-only plugin installation. see bin/logstash-plugin package|unpack", :default => false
 
@@ -90,7 +91,15 @@ def install_gems_list!(install_list)
 
     # Add plugins/gems to the current gemfile
     puts("Installing" + (install_list.empty? ? "..." : " " + install_list.collect(&:first).join(", ")))
-    install_list.each { |plugin, version, options| gemfile.update(plugin, version, options) }
+    install_list.each do |plugin, version, options|
+      if preserve?
+        plugin_gem = gemfile.find(plugin)
+        puts("Preserving Gemfile gem options for plugin #{plugin}") if plugin_gem && !plugin_gem.options.empty?
+        gemfile.update(plugin, version, options)
+      else
+        gemfile.overwrite(plugin, version, options)
+      end
+    end
 
     # Sync gemfiles changes to disk to make them available to the `bundler install`'s API
     gemfile.save
diff --git a/lib/pluginmanager/main.rb b/lib/pluginmanager/main.rb
index 15841b107e3..a7f1e50a129 100644
--- a/lib/pluginmanager/main.rb
+++ b/lib/pluginmanager/main.rb
@@ -16,22 +16,27 @@ module PluginManager
 require "pluginmanager/gemfile"
 require "pluginmanager/install"
 require "pluginmanager/uninstall"
+require "pluginmanager/remove"
 require "pluginmanager/list"
 require "pluginmanager/update"
 require "pluginmanager/pack"
 require "pluginmanager/unpack"
+require "pluginmanager/generate"
 
 module LogStash
   module PluginManager
     class Error < StandardError; end
 
     class Main < Clamp::Command
-      subcommand "install", "Install a plugin", LogStash::PluginManager::Install
-      subcommand "uninstall", "Uninstall a plugin", LogStash::PluginManager::Uninstall
+      subcommand "list", "List all installed Logstash plugins", LogStash::PluginManager::List
+      subcommand "install", "Install a Logstash plugin", LogStash::PluginManager::Install
+      subcommand "remove", "Remove a Logstash plugin", LogStash::PluginManager::Remove
       subcommand "update", "Update a plugin", LogStash::PluginManager::Update
       subcommand "pack", "Package currently installed plugins", LogStash::PluginManager::Pack
       subcommand "unpack", "Unpack packaged plugins", LogStash::PluginManager::Unpack
       subcommand "list", "List all installed plugins", LogStash::PluginManager::List
+      subcommand "generate", "Create the foundation for a new plugin", LogStash::PluginManager::Generate
+      subcommand "uninstall", "Uninstall a plugin. Deprecated: Please use remove instead", LogStash::PluginManager::Uninstall
     end
   end
 end
diff --git a/lib/pluginmanager/remove.rb b/lib/pluginmanager/remove.rb
new file mode 100644
index 00000000000..530463f64b1
--- /dev/null
+++ b/lib/pluginmanager/remove.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+require "pluginmanager/command"
+
+class LogStash::PluginManager::Remove < LogStash::PluginManager::Command
+
+  parameter "PLUGIN", "plugin name"
+
+  def execute
+    signal_error("File #{LogStash::Environment::GEMFILE_PATH} does not exist or is not writable, aborting") unless File.writable?(LogStash::Environment::GEMFILE_PATH)
+
+    ##
+    # Need to setup the bundler status to enable uninstall of plugins
+    # installed as local_gems, otherwise gem:specification is not
+    # finding the plugins
+    ##
+    LogStash::Bundler.setup!({:without => [:build, :development]})
+
+    # make sure this is an installed plugin and present in Gemfile.
+    # it is not possible to uninstall a dependency not listed in the Gemfile, for example a dependent codec
+    signal_error("This plugin has not been previously installed, aborting") unless LogStash::PluginManager.installed_plugin?(plugin, gemfile)
+
+    # since we previously did a gemfile.find(plugin) there is no reason why
+    # remove would not work (return nil) here
+    if gemfile.remove(plugin)
+      gemfile.save
+
+      puts("Removing #{plugin}")
+
+      # any errors will be logged to $stderr by invoke!
+      # output, exception = LogStash::Bundler.invoke!(:install => true, :clean => true)
+      output = LogStash::Bundler.invoke!(:install => true, :clean => true)
+
+      remove_unused_locally_installed_gems!
+    end
+  rescue => exception
+    gemfile.restore!
+    report_exception("Remove Aborted", exception)
+  ensure
+    display_bundler_output(output)
+  end
+end
diff --git a/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..654a05b6614
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-codec-<%= plugin_name %>
+Example codec plugin. This should help bootstrap your effort to write your own codec plugin!
diff --git a/lib/pluginmanager/templates/codec-plugin/Gemfile b/lib/pluginmanager/templates/codec-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/codec-plugin/LICENSE b/lib/pluginmanager/templates/codec-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/codec-plugin/README.md b/lib/pluginmanager/templates/codec-plugin/README.md
new file mode 100644
index 00000000000..a75e88df936
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-codec-awesome", :path => "/your/local/logstash-codec-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'codec {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-codec-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-codec-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/codec-plugin/Rakefile b/lib/pluginmanager/templates/codec-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
new file mode 100644
index 00000000000..91ed93785b8
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/lib/logstash/codecs/example.rb.erb
@@ -0,0 +1,45 @@
+# encoding: utf-8
+require "logstash/codecs/base"
+require "logstash/namespace"
+
+# This <%= @plugin_name %> codec will append a string to the message field
+# of an event, either in the decoding or encoding methods
+#
+# This is only intended to be used as an example.
+#
+# input {
+#   stdin { codec => <%= @plugin_name %> }
+# }
+#
+# or
+#
+# output {
+#   stdout { codec => <%= @plugin_name %> }
+# }
+#
+class LogStash::Codecs::<%= classify(plugin_name) %> < LogStash::Codecs::Base
+
+  # The codec name
+  config_name "<%= plugin_name %>"
+
+  # Append a string to the message
+  config :append, :validate => :string, :default => ', Hello World!'
+
+  def register
+    @lines = LogStash::Codecs::Line.new
+    @lines.charset = "UTF-8"
+  end # def register
+
+  def decode(data)
+    @lines.decode(data) do |line|
+      replace = { "message" => line.get("message").to_s + @append }
+      yield LogStash::Event.new(replace)
+    end
+  end # def decode
+
+  # Encode a single event, this returns the raw data to be returned as a String
+  def encode_sync(event)
+    event.get("message").to_s + @append + NL
+  end # def encode_sync
+
+end # class LogStash::Codecs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
new file mode 100644
index 00000000000..91e1b0600f1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/logstash-codec-example.gemspec.erb
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-codec-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "codec" }
+
+  # Gem dependencies
+  s.add_runtime_dependency 'logstash-core-plugin-api', "~> <%= min_version %>"
+  s.add_runtime_dependency 'logstash-codec-line'
+  s.add_development_dependency 'logstash-devutils'
+end
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
new file mode 100644
index 00000000000..48cca741ab2
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/codecs/example_spec.rb.erb
@@ -0,0 +1,3 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require "logstash/codecs/<%= plugin_name %>"
diff --git a/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
new file mode 100644
index 00000000000..dc64aba12c1
--- /dev/null
+++ b/lib/pluginmanager/templates/codec-plugin/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
diff --git a/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md b/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..6b18c6221de
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-filter-<%= plugin_name %>
+Example filter plugin. This should help bootstrap your effort to write your own filter plugin!
diff --git a/lib/pluginmanager/templates/filter-plugin/Gemfile b/lib/pluginmanager/templates/filter-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/filter-plugin/LICENSE b/lib/pluginmanager/templates/filter-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/filter-plugin/README.md b/lib/pluginmanager/templates/filter-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/filter-plugin/Rakefile b/lib/pluginmanager/templates/filter-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb b/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb
new file mode 100644
index 00000000000..ca5d9f7ca3b
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/lib/logstash/filters/example.rb.erb
@@ -0,0 +1,43 @@
+# encoding: utf-8
+require "logstash/filters/base"
+require "logstash/namespace"
+
+# This <%= @plugin_name %> filter will replace the contents of the default 
+# message field with whatever you specify in the configuration.
+#
+# It is only intended to be used as an <%= @plugin_name %>.
+class LogStash::Filters::<%= classify(plugin_name) %> < LogStash::Filters::Base
+
+  # Setting the config_name here is required. This is how you
+  # configure this filter from your Logstash config.
+  #
+  # filter {
+  #   <%= @plugin_name %> {
+  #     message => "My message..."
+  #   }
+  # }
+  #
+  config_name "<%= plugin_name %>"
+  
+  # Replace the message with this value.
+  config :message, :validate => :string, :default => "Hello World!"
+  
+
+  public
+  def register
+    # Add instance variables 
+  end # def register
+
+  public
+  def filter(event)
+
+    if @message
+      # Replace the event message with our message as configured in the
+      # config file.
+      event.set("message", @message)
+    end
+
+    # filter_matched should go in the last line of our successful code
+    filter_matched(event)
+  end # def filter
+end # class LogStash::Filters::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb b/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb
new file mode 100644
index 00000000000..5f910dc40fb
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/logstash-filter-example.gemspec.erb
@@ -0,0 +1,23 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-filter-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_development_dependency 'logstash-devutils'
+end
diff --git a/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb b/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb
new file mode 100644
index 00000000000..5dceafe7fc4
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/spec/filters/example_spec.rb.erb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require "logstash/filters/<%= plugin_name %>"
+
+describe LogStash::Filters::<%= classify(plugin_name) %> do
+  describe "Set to Hello World" do
+    let(:config) do <<-CONFIG
+      filter {
+        <%= plugin_name %> {
+          message => "Hello World"
+        }
+      }
+    CONFIG
+    end
+
+    sample("message" => "some text") do
+      expect(subject).to include("message")
+      expect(subject.get('message')).to eq('Hello World')
+    end
+  end
+end
diff --git a/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb b/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb
new file mode 100644
index 00000000000..dc64aba12c1
--- /dev/null
+++ b/lib/pluginmanager/templates/filter-plugin/spec/spec_helper.rb
@@ -0,0 +1,2 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
diff --git a/lib/pluginmanager/templates/input-plugin/CHANGELOG.md b/lib/pluginmanager/templates/input-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..eca3db404e8
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-input-<%= plugin_name %>
+Example input plugin. This should help bootstrap your effort to write your own input plugin!
diff --git a/lib/pluginmanager/templates/input-plugin/Gemfile b/lib/pluginmanager/templates/input-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/input-plugin/LICENSE b/lib/pluginmanager/templates/input-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/input-plugin/README.md b/lib/pluginmanager/templates/input-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/input-plugin/Rakefile b/lib/pluginmanager/templates/input-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb b/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb
new file mode 100644
index 00000000000..176467ccb5c
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/lib/logstash/inputs/example.rb.erb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+require "logstash/inputs/base"
+require "logstash/namespace"
+require "stud/interval"
+require "socket" # for Socket.gethostname
+
+# Generate a repeating message.
+#
+# This plugin is intented only as an example.
+
+class LogStash::Inputs::<%= classify(plugin_name) %> < LogStash::Inputs::Base
+  config_name "<%= @plugin_name %>"
+
+  # If undefined, Logstash will complain, even if codec is unused.
+  default :codec, "plain"
+
+  # The message string to use in the event.
+  config :message, :validate => :string, :default => "Hello World!"
+
+  # Set how frequently messages should be sent.
+  #
+  # The default, `1`, means send a message every second.
+  config :interval, :validate => :number, :default => 1
+
+  public
+  def register
+    @host = Socket.gethostname
+  end # def register
+
+  def run(queue)
+    # we can abort the loop if stop? becomes true
+    while !stop?
+      event = LogStash::Event.new("message" => @message, "host" => @host)
+      decorate(event)
+      queue << event
+      # because the sleep interval can be big, when shutdown happens
+      # we want to be able to abort the sleep
+      # Stud.stoppable_sleep will frequently evaluate the given block
+      # and abort the sleep(@interval) if the return value is true
+      Stud.stoppable_sleep(@interval) { stop? }
+    end # loop
+  end # def run
+
+  def stop
+    # nothing to do in this case so it is not necessary to define stop
+    # examples of common "stop" tasks:
+    #  * close sockets (unblocking blocking reads/accepts)
+    #  * cleanup temporary files
+    #  * terminate spawned threads
+  end
+end # class LogStash::Inputs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb b/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb
new file mode 100644
index 00000000000..9f8543887b2
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/logstash-input-example.gemspec.erb
@@ -0,0 +1,25 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-input-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = '{TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "input" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_runtime_dependency 'logstash-codec-plain'
+  s.add_runtime_dependency 'stud', '>= 0.0.22'
+  s.add_development_dependency 'logstash-devutils', '>= 0.0.16'
+end
diff --git a/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb b/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb
new file mode 100644
index 00000000000..7b8bfde8ea3
--- /dev/null
+++ b/lib/pluginmanager/templates/input-plugin/spec/inputs/example_spec.rb.erb
@@ -0,0 +1,11 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/inputs/<%= plugin_name %>"
+
+describe LogStash::Inputs::<%= classify(plugin_name) %> do
+
+  it_behaves_like "an interruptible input plugin" do
+    let(:config) { { "interval" => 100 } }
+  end
+
+end
diff --git a/lib/pluginmanager/templates/output-plugin/CHANGELOG.md b/lib/pluginmanager/templates/output-plugin/CHANGELOG.md
new file mode 100644
index 00000000000..9bb3255f54d
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/CHANGELOG.md
@@ -0,0 +1,2 @@
+## 0.1.0
+  - Plugin created with the logstash plugin generator
diff --git a/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb b/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb
new file mode 100644
index 00000000000..8074a9e9725
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/CONTRIBUTORS.erb
@@ -0,0 +1,10 @@
+The following is a list of people who have contributed ideas, code, bug
+reports, or in general have helped logstash along its way.
+
+Contributors:
+* <%= author %> - <%= email %>
+
+Note: If you've sent us patches, bug reports, or otherwise contributed to
+Logstash, and you aren't on the list above and want to be, please let us know
+and we'll make sure you're here. Contributions from folks like you are what make
+open source awesome.
diff --git a/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb b/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb
new file mode 100644
index 00000000000..2593de38fc7
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/DEVELOPER.md.erb
@@ -0,0 +1,2 @@
+# logstash-output-<%= plugin_name %>
+Example output plugin. This should help bootstrap your effort to write your own output plugin!
diff --git a/lib/pluginmanager/templates/output-plugin/Gemfile b/lib/pluginmanager/templates/output-plugin/Gemfile
new file mode 100644
index 00000000000..06618cefa69
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/Gemfile
@@ -0,0 +1,3 @@
+source 'https://rubygems.org'
+gemspec
+
diff --git a/lib/pluginmanager/templates/output-plugin/LICENSE b/lib/pluginmanager/templates/output-plugin/LICENSE
new file mode 100644
index 00000000000..51fca54c2a0
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/LICENSE
@@ -0,0 +1,11 @@
+Licensed under the Apache License, Version 2.0 (the "License");
+you may not use this file except in compliance with the License.
+You may obtain a copy of the License at
+
+    http://www.apache.org/licenses/LICENSE-2.0
+
+Unless required by applicable law or agreed to in writing, software
+distributed under the License is distributed on an "AS IS" BASIS,
+WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+See the License for the specific language governing permissions and
+limitations under the License.
diff --git a/lib/pluginmanager/templates/output-plugin/README.md b/lib/pluginmanager/templates/output-plugin/README.md
new file mode 100644
index 00000000000..f5301aca0f9
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/README.md
@@ -0,0 +1,86 @@
+# Logstash Plugin
+
+This is a plugin for [Logstash](https://github.com/elastic/logstash).
+
+It is fully free and fully open source. The license is Apache 2.0, meaning you are pretty much free to use it however you want in whatever way.
+
+## Documentation
+
+Logstash provides infrastructure to automatically generate documentation for this plugin. We use the asciidoc format to write documentation so any comments in the source code will be first converted into asciidoc and then into html. All plugin documentation are placed under one [central location](http://www.elastic.co/guide/en/logstash/current/).
+
+- For formatting code or config example, you can use the asciidoc `[source,ruby]` directive
+- For more asciidoc formatting tips, see the excellent reference here https://github.com/elastic/docs#asciidoc-guide
+
+## Need Help?
+
+Need help? Try #logstash on freenode IRC or the https://discuss.elastic.co/c/logstash discussion forum.
+
+## Developing
+
+### 1. Plugin Developement and Testing
+
+#### Code
+- To get started, you'll need JRuby with the Bundler gem installed.
+
+- Create a new plugin or clone and existing from the GitHub [logstash-plugins](https://github.com/logstash-plugins) organization. We also provide [example plugins](https://github.com/logstash-plugins?query=example).
+
+- Install dependencies
+```sh
+bundle install
+```
+
+#### Test
+
+- Update your dependencies
+
+```sh
+bundle install
+```
+
+- Run tests
+
+```sh
+bundle exec rspec
+```
+
+### 2. Running your unpublished Plugin in Logstash
+
+#### 2.1 Run in a local Logstash clone
+
+- Edit Logstash `Gemfile` and add the local plugin path, for example:
+```ruby
+gem "logstash-filter-awesome", :path => "/your/local/logstash-filter-awesome"
+```
+- Install plugin
+```sh
+bin/logstash-plugin install --no-verify
+```
+- Run Logstash with your plugin
+```sh
+bin/logstash -e 'filter {awesome {}}'
+```
+At this point any modifications to the plugin code will be applied to this local Logstash setup. After modifying the plugin, simply rerun Logstash.
+
+#### 2.2 Run in an installed Logstash
+
+You can use the same **2.1** method to run your plugin in an installed Logstash by editing its `Gemfile` and pointing the `:path` to your local plugin development directory or you can build the gem and install it using:
+
+- Build your plugin gem
+```sh
+gem build logstash-filter-awesome.gemspec
+```
+- Install the plugin from the Logstash home
+```sh
+bin/logstash-plugin install /your/local/plugin/logstash-filter-awesome.gem
+```
+- Start Logstash and proceed to test the plugin
+
+## Contributing
+
+All contributions are welcome: ideas, patches, documentation, bug reports, complaints, and even something you drew up on a napkin.
+
+Programming is not a required skill. Whatever you've seen about open source and maintainers or community members  saying "send patches or die" - you will not see that here.
+
+It is more important to the community that you are able to contribute.
+
+For more information about contributing, see the [CONTRIBUTING](https://github.com/elastic/logstash/blob/master/CONTRIBUTING.md) file.
diff --git a/lib/pluginmanager/templates/output-plugin/Rakefile b/lib/pluginmanager/templates/output-plugin/Rakefile
new file mode 100644
index 00000000000..d50e796f1c1
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/Rakefile
@@ -0,0 +1 @@
+require "logstash/devutils/rake"
diff --git a/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb b/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb
new file mode 100644
index 00000000000..eadd499bf98
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/lib/logstash/outputs/example.rb.erb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "logstash/namespace"
+
+# An <%= plugin_name %> output that does nothing.
+class LogStash::Outputs::<%= classify(plugin_name) %> < LogStash::Outputs::Base
+  config_name "<%= plugin_name %>"
+
+  public
+  def register
+  end # def register
+
+  public
+  def receive(event)
+    return "Event received"
+  end # def event
+end # class LogStash::Outputs::<%= classify(plugin_name) %>
diff --git a/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb b/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb
new file mode 100644
index 00000000000..db396e1ff1c
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/logstash-output-example.gemspec.erb
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name          = 'logstash-output-<%= plugin_name %>'
+  s.version       = '0.1.0'
+  s.licenses      = ['Apache License (2.0)']
+  s.summary       = 'TODO: Write a short summary, because Rubygems requires one.'
+  s.description   = 'TODO: Write a longer description or delete this line.'
+  s.homepage      = 'TODO: Put your plugin''s website or public repo URL here.'
+  s.authors       = ['<%= author %>']
+  s.email         = '<%= email %>'
+  s.require_paths = ['lib']
+
+  # Files
+  s.files = Dir['lib/**/*','spec/**/*','vendor/**/*','*.gemspec','*.md','CONTRIBUTORS','Gemfile','LICENSE','NOTICE.TXT']
+   # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "output" }
+
+  # Gem dependencies
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> <%= min_version %>"
+  s.add_runtime_dependency "logstash-codec-plain"
+  s.add_development_dependency "logstash-devutils"
+end
diff --git a/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb b/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb
new file mode 100644
index 00000000000..220d967bd63
--- /dev/null
+++ b/lib/pluginmanager/templates/output-plugin/spec/outputs/example_spec.rb.erb
@@ -0,0 +1,22 @@
+# encoding: utf-8
+require "logstash/devutils/rspec/spec_helper"
+require "logstash/outputs/<%= plugin_name %>"
+require "logstash/codecs/plain"
+require "logstash/event"
+
+describe LogStash::Outputs::<%= classify(plugin_name) %> do
+  let(:sample_event) { LogStash::Event.new }
+  let(:output) { LogStash::Outputs::<%= classify(plugin_name) %>.new }
+
+  before do
+    output.register
+  end
+
+  describe "receive message" do
+    subject { output.receive(sample_event) }
+
+    it "returns a string" do
+      expect(subject).to eq("Event received")
+    end
+  end
+end
diff --git a/lib/pluginmanager/templates/render_context.rb b/lib/pluginmanager/templates/render_context.rb
new file mode 100644
index 00000000000..583c6a9ee07
--- /dev/null
+++ b/lib/pluginmanager/templates/render_context.rb
@@ -0,0 +1,20 @@
+require "erb"
+
+module LogStash::PluginManager
+  class RenderContext
+    def initialize(options = {})
+      options.each do |name, value|
+        define_singleton_method(name) { value }
+      end
+    end
+
+    def get_binding
+      binding()
+    end
+
+    def classify(klass_name)
+      klass_name.split(/-|_/).map { |e| e.capitalize }.join("")
+    end
+
+  end
+end
diff --git a/lib/pluginmanager/uninstall.rb b/lib/pluginmanager/uninstall.rb
index e7598a4ebc9..2a954e4ea6f 100644
--- a/lib/pluginmanager/uninstall.rb
+++ b/lib/pluginmanager/uninstall.rb
@@ -1,13 +1,16 @@
 # encoding: utf-8
 require "pluginmanager/command"
 
+# TODO: SR: Delete this file in 6.0, as we deprecated uninstall in favar of remove to be consistent with the stack
+
 class LogStash::PluginManager::Uninstall < LogStash::PluginManager::Command
 
   parameter "PLUGIN", "plugin name"
 
   def execute
-    signal_error("File #{LogStash::Environment::GEMFILE_PATH} does not exist or is not writable, aborting") unless File.writable?(LogStash::Environment::GEMFILE_PATH)
+    puts "uninstall subcommand is deprecated and will be removed in the next major version. Please use logstash-plugin remove instead."
 
+    signal_error("File #{LogStash::Environment::GEMFILE_PATH} does not exist or is not writable, aborting") unless File.writable?(LogStash::Environment::GEMFILE_PATH)
     ##
     # Need to setup the bundler status to enable uninstall of plugins
     # installed as local_gems, otherwise gem:specification is not
diff --git a/lib/pluginmanager/update.rb b/lib/pluginmanager/update.rb
index fd840e91183..c33e25d610f 100644
--- a/lib/pluginmanager/update.rb
+++ b/lib/pluginmanager/update.rb
@@ -6,22 +6,25 @@
 
 class LogStash::PluginManager::Update < LogStash::PluginManager::Command
   REJECTED_OPTIONS = [:path, :git, :github]
+  # These are local gems used by LS and needs to be filtered out of other plugin gems
+  NON_PLUGIN_LOCAL_GEMS = ["logstash-core", "logstash-core-event-java", "logstash-core-plugin-api"]
 
   parameter "[PLUGIN] ...", "Plugin name(s) to upgrade to latest version", :attribute_name => :plugins_arg
   option "--[no-]verify", :flag, "verify plugin validity before installation", :default => true
   option "--local", :flag, "force local-only plugin update. see bin/logstash-plugin package|unpack", :default => false
 
   def execute
-    local_gems = gemfile.locally_installed_gems
+    # remove "system" local gems used by LS
+    local_gems = gemfile.locally_installed_gems.map(&:name) - NON_PLUGIN_LOCAL_GEMS
 
     if local_gems.size > 0
       if update_all?
-        plugins_with_path = local_gems.map(&:name)
+        plugins_with_path = local_gems
       else
-        plugins_with_path = plugins_arg & local_gems.map(&:name)
+        plugins_with_path = plugins_arg & local_gems
       end
 
-      warn_local_gems(plugins_with_path)
+      warn_local_gems(plugins_with_path) if plugins_with_path.size > 0
     end
     update_gems!
   end
@@ -47,7 +50,6 @@ def update_gems!
     filtered_plugins = plugins.map { |plugin| gemfile.find(plugin) }
       .compact
       .reject { |plugin| REJECTED_OPTIONS.any? { |key| plugin.options.has_key?(key) } }
-      .select { |plugin| local? || (verify? ? validates_version(plugin.name) : true) }
       .each   { |plugin| gemfile.update(plugin.name) }
 
     # force a disk sync before running bundler
@@ -69,12 +71,6 @@ def update_gems!
     display_bundler_output(output)
   end
 
-  # validate if there is any major version update so then we can ask the user if he is
-  # sure to update or not.
-  def validates_version(plugin)
-    LogStash::PluginManager.update_to_major_version?(plugin)
-  end
-
   # create list of plugins to update
   def plugins_to_update(previous_gem_specs_map)
     if update_all?
diff --git a/lib/pluginmanager/util.rb b/lib/pluginmanager/util.rb
index 149ff6256d4..debd709d256 100644
--- a/lib/pluginmanager/util.rb
+++ b/lib/pluginmanager/util.rb
@@ -50,29 +50,13 @@ def self.logstash_plugin?(plugin, version = nil, options={})
   # @option options [Boolean] :pre Include pre release versions in the search (default: false)
   # @return [Hash] The plugin version information as returned by rubygems
   def self.fetch_latest_version_info(plugin, options={})
-    require "gems"
     exclude_prereleases =  options.fetch(:pre, false)
-    versions = Gems.versions(plugin)
+    versions = LogStash::Rubygems.versions(plugin)
     raise ValidationError.new("Something went wrong with the validation. You can skip the validation with the --no-verify option") if !versions.is_a?(Array) || versions.empty?
     versions = versions.select { |version| !version["prerelease"] } if !exclude_prereleases
     versions.first
   end
 
-  # Let's you decide to update to the last version of a plugin if this is a major version
-  # @param [String] A plugin name
-  # @return [Boolean] True in case the update is moving forward, false otherwise
-  def self.update_to_major_version?(plugin_name)
-    plugin_version  = fetch_latest_version_info(plugin_name)
-    latest_version  = plugin_version['number'].split(".")
-    current_version = Gem::Specification.find_by_name(plugin_name).version.version.split(".")
-    if (latest_version[0].to_i > current_version[0].to_i)
-      ## warn if users want to continue
-      puts("You are updating #{plugin_name} to a new version #{latest_version.join('.')}, which may not be compatible with #{current_version.join('.')}. are you sure you want to proceed (Y/N)?")
-      return ( "y" == STDIN.gets.strip.downcase ? true : false)
-    end
-    true
-  end
-
   # @param spec [Gem::Specification] plugin gem specification
   # @return [Boolean] true if this spec is for an installable logstash plugin
   def self.logstash_plugin_gem_spec?(spec)
diff --git a/lib/systeminstall/pleasewrap.rb b/lib/systeminstall/pleasewrap.rb
new file mode 100755
index 00000000000..f7ee00bd447
--- /dev/null
+++ b/lib/systeminstall/pleasewrap.rb
@@ -0,0 +1,12 @@
+# encoding: utf-8
+$LOAD_PATH.unshift(File.expand_path(File.join(__FILE__, "..", "..")))
+
+require "bootstrap/environment"
+
+ENV["GEM_HOME"] = ENV["GEM_PATH"] = LogStash::Environment.logstash_gem_home
+Gem.use_paths(LogStash::Environment.logstash_gem_home)
+
+#libdir = File.expand_path("../lib", File.dirname(__FILE__))
+#$LOAD_PATH << libdir if File.exist?(File.join(libdir, "pleaserun", "cli.rb"))
+require "pleaserun/cli"
+exit(PleaseRun::CLI.run || 0)
diff --git a/logstash-core-event-java/.gitignore b/logstash-core-event-java/.gitignore
index a453cb95034..0224a02d2d8 100644
--- a/logstash-core-event-java/.gitignore
+++ b/logstash-core-event-java/.gitignore
@@ -3,7 +3,3 @@
 # build dirs
 build
 .gradle
-
-# Intellij
-.idea
-*.iml
diff --git a/logstash-core-event-java/build.gradle b/logstash-core-event-java/build.gradle
index b2a4a55ec43..d99d30476b4 100644
--- a/logstash-core-event-java/build.gradle
+++ b/logstash-core-event-java/build.gradle
@@ -27,7 +27,7 @@ apply plugin: 'idea'
 
 group = 'org.logstash'
 
-project.sourceCompatibility = 1.7
+project.sourceCompatibility = 1.8
 
 task sourcesJar(type: Jar, dependsOn: classes) {
     from sourceSets.main.allSource
@@ -92,9 +92,10 @@ idea {
 }
 
 dependencies {
-    compile 'com.fasterxml.jackson.core:jackson-core:2.7.1'
-    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.1-1'
-    provided 'org.jruby:jruby-core:1.7.22'
+    compile 'com.fasterxml.jackson.core:jackson-core:2.7.3'
+    compile 'com.fasterxml.jackson.core:jackson-databind:2.7.3'
+    compile 'org.apache.logging.log4j:log4j-api:2.6.2'
+    provided 'org.jruby:jruby-core:1.7.25'
     testCompile 'junit:junit:4.12'
     testCompile 'net.javacrumbs.json-unit:json-unit:1.9.0'
 }
diff --git a/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb b/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb
index cf86fec4d16..caf90a828ac 100644
--- a/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb
+++ b/logstash-core-event-java/lib/logstash-core-event-java/logstash-core-event-java.rb
@@ -25,4 +25,4 @@ module LogStash
 require "jruby_event_ext"
 require "jruby_timestamp_ext"
 require "logstash/event"
-require "logstash/timestamp"
\ No newline at end of file
+require "logstash/timestamp"
diff --git a/logstash-core-event-java/lib/logstash-core-event-java/version.rb b/logstash-core-event-java/lib/logstash-core-event-java/version.rb
index 6c297b7c2fd..2cb4dc2c9b5 100644
--- a/logstash-core-event-java/lib/logstash-core-event-java/version.rb
+++ b/logstash-core-event-java/lib/logstash-core-event-java/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_EVENT_JAVA_VERSION = "3.0.0.dev"
+LOGSTASH_CORE_EVENT_JAVA_VERSION = "5.1.0"
diff --git a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
index 143d7a3e068..eda40d431f0 100644
--- a/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
+++ b/logstash-core-event-java/lib/logstash-core-event-java_jars.rb
@@ -1,6 +1,14 @@
 # this is a generated file, to avoid over-writing it just delete this comment
-require 'jar_dependencies'
+begin
+  require 'jar_dependencies'
+rescue LoadError
+  require 'com/fasterxml/jackson/core/jackson-databind/2.7.3/jackson-databind-2.7.3.jar'
+  require 'com/fasterxml/jackson/core/jackson-annotations/2.7.0/jackson-annotations-2.7.0.jar'
+  require 'com/fasterxml/jackson/core/jackson-core/2.7.3/jackson-core-2.7.3.jar'
+end
 
-require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.1' )
-require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
-require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.1-1' )
+if defined? Jars
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-databind', '2.7.3' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-annotations', '2.7.0' )
+  require_jar( 'com.fasterxml.jackson.core', 'jackson-core', '2.7.3' )
+end
diff --git a/logstash-core-event-java/lib/logstash/event.rb b/logstash-core-event-java/lib/logstash/event.rb
index 8f6a1908901..38c5d206938 100644
--- a/logstash-core-event-java/lib/logstash/event.rb
+++ b/logstash-core-event-java/lib/logstash/event.rb
@@ -3,24 +3,52 @@
 require "logstash/namespace"
 require "logstash/json"
 require "logstash/string_interpolation"
-require "cabin"
 
 # transcient pipeline events for normal in-flow signaling as opposed to
 # flow altering exceptions. for now having base classes is adequate and
 # in the future it might be necessary to refactor using like a BaseEvent
 # class to have a common interface for all pileline events to support
 # eventual queueing persistence for example, TBD.
-class LogStash::ShutdownEvent; end
-class LogStash::FlushEvent; end
-
 module LogStash
-  FLUSH = LogStash::FlushEvent.new
+  class SignalEvent
+    def flush?; raise "abstract method"; end;
+    def shutdown?; raise "abstract method"; end;
+  end
+
+  class ShutdownEvent < SignalEvent
+    def flush?; false; end;
+    def shutdown?; true; end;
+  end
+
+  class FlushEvent < SignalEvent
+    def flush?; true; end;
+    def shutdown?; false; end;
+  end
+
+  FLUSH = FlushEvent.new
 
   # LogStash::SHUTDOWN is used by plugins
-  SHUTDOWN = LogStash::ShutdownEvent.new
+  SHUTDOWN = ShutdownEvent.new
+
+  class Event
+    MSG_BRACKETS_METHOD_MISSING = "Direct event field references (i.e. event['field']) have been disabled in favor of using event get and set methods (e.g. event.get('field')). Please consult the Logstash 5.0 breaking changes documentation for more details.".freeze
+    MSG_BRACKETS_EQUALS_METHOD_MISSING = "Direct event field references (i.e. event['field'] = 'value') have been disabled in favor of using event get and set methods (e.g. event.set('field', 'value')). Please consult the Logstash 5.0 breaking changes documentation for more details.".freeze
+    RE_BRACKETS_METHOD = /^\[\]$/.freeze
+    RE_BRACKETS_EQUALS_METHOD = /^\[\]=$/.freeze
+
+    def method_missing(method_name, *arguments, &block)
+      if RE_BRACKETS_METHOD.match(method_name.to_s)
+        raise NoMethodError.new(MSG_BRACKETS_METHOD_MISSING)
+      end
+      if RE_BRACKETS_EQUALS_METHOD.match(method_name.to_s)
+        raise NoMethodError.new(MSG_BRACKETS_EQUALS_METHOD_MISSING)
+      end
+      super
+    end
+  end
 end
 
 # for backward compatibility, require "logstash/event" is used a lots of places so let's bootstrap the
 # Java code loading from here.
 # TODO: (colin) I think we should mass replace require "logstash/event" with require "logstash-core-event"
-require "logstash-core-event"
\ No newline at end of file
+require "logstash-core-event"
diff --git a/logstash-core-event-java/lib/logstash/string_interpolation.rb b/logstash-core-event-java/lib/logstash/string_interpolation.rb
index 7baf091f304..2eef6dfdff8 100644
--- a/logstash-core-event-java/lib/logstash/string_interpolation.rb
+++ b/logstash-core-event-java/lib/logstash/string_interpolation.rb
@@ -6,12 +6,12 @@ module StringInterpolation
 
     # clear the global compiled templates cache
     def clear_cache
-      Java::ComLogstash::StringInterpolation.get_instance.clear_cache;
+      Java::OrgLogstash::StringInterpolation.get_instance.clear_cache;
     end
 
     # @return [Fixnum] the compiled templates cache size
     def cache_size
-      Java::ComLogstash::StringInterpolation.get_instance.cache_size;
+      Java::OrgLogstash::StringInterpolation.get_instance.cache_size;
     end
   end
 end
diff --git a/logstash-core-event-java/logstash-core-event-java.gemspec b/logstash-core-event-java/logstash-core-event-java.gemspec
index ef8d2a2bad5..42c190b5f28 100644
--- a/logstash-core-event-java/logstash-core-event-java.gemspec
+++ b/logstash-core-event-java/logstash-core-event-java.gemspec
@@ -26,6 +26,6 @@ Gem::Specification.new do |gem|
   # which does not have this problem.
   gem.add_runtime_dependency "ruby-maven", "~> 3.3.9"
 
-  gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.1"
-  gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.1-1"
+  gem.requirements << "jar com.fasterxml.jackson.core:jackson-core, 2.7.3"
+  gem.requirements << "jar com.fasterxml.jackson.core:jackson-databind, 2.7.3"
 end
diff --git a/logstash-core-event-java/spec/event_spec.rb b/logstash-core-event-java/spec/event_spec.rb
index 9df705418f3..0b7d174e782 100644
--- a/logstash-core-event-java/spec/event_spec.rb
+++ b/logstash-core-event-java/spec/event_spec.rb
@@ -4,6 +4,7 @@
 require "logstash/util"
 require "logstash/event"
 require "json"
+require "java"
 
 TIMESTAMP = "@timestamp"
 
@@ -26,83 +27,124 @@
 
     it "should serialize deep hash from field reference assignments" do
       e = LogStash::Event.new({TIMESTAMP => "2015-05-28T23:02:05.350Z"})
-      e["foo"] = "bar"
-      e["bar"] = 1
-      e["baz"] = 1.0
-      e["[fancy][pants][socks]"] = "shoes"
+      e.set("foo", "bar")
+      e.set("bar", 1)
+      e.set("baz", 1.0)
+      e.set("[fancy][pants][socks]", "shoes")
       expect(JSON.parse(e.to_json)).to eq(JSON.parse("{\"@timestamp\":\"2015-05-28T23:02:05.350Z\",\"@version\":\"1\",\"foo\":\"bar\",\"bar\":1,\"baz\":1.0,\"fancy\":{\"pants\":{\"socks\":\"shoes\"}}}"))
     end
   end
 
-  context "[]" do
+  context "#get" do
     it "should get simple values" do
       e = LogStash::Event.new({"foo" => "bar", "bar" => 1, "baz" => 1.0, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
-      expect(e["foo"]).to eq("bar")
-      expect(e["[foo]"]).to eq("bar")
-      expect(e["bar"]).to eq(1)
-      expect(e["[bar]"]).to eq(1)
-      expect(e["baz"]).to eq(1.0)
-      expect(e["[baz]"]).to eq(1.0)
-      expect(e[TIMESTAMP].to_s).to eq("2015-05-28T23:02:05.350Z")
-      expect(e["[#{TIMESTAMP}]"].to_s).to eq("2015-05-28T23:02:05.350Z")
+      expect(e.get("foo")).to eq("bar")
+      expect(e.get("[foo]")).to eq("bar")
+      expect(e.get("bar")).to eq(1)
+      expect(e.get("[bar]")).to eq(1)
+      expect(e.get("baz")).to eq(1.0)
+      expect(e.get("[baz]")).to eq(1.0)
+      expect(e.get(TIMESTAMP).to_s).to eq("2015-05-28T23:02:05.350Z")
+      expect(e.get("[#{TIMESTAMP}]").to_s).to eq("2015-05-28T23:02:05.350Z")
     end
 
     it "should get deep hash values" do
       e = LogStash::Event.new({"foo" => {"bar" => 1, "baz" => 1.0}})
-      expect(e["[foo][bar]"]).to eq(1)
-      expect(e["[foo][baz]"]).to eq(1.0)
+      expect(e.get("[foo][bar]")).to eq(1)
+      expect(e.get("[foo][baz]")).to eq(1.0)
     end
 
     it "should get deep array values" do
       e = LogStash::Event.new({"foo" => ["bar", 1, 1.0]})
-      expect(e["[foo][0]"]).to eq("bar")
-      expect(e["[foo][1]"]).to eq(1)
-      expect(e["[foo][2]"]).to eq(1.0)
-      expect(e["[foo][3]"]).to be_nil
+      expect(e.get("[foo][0]")).to eq("bar")
+      expect(e.get("[foo][1]")).to eq(1)
+      expect(e.get("[foo][2]")).to eq(1.0)
+      expect(e.get("[foo][3]")).to be_nil
     end
   end
 
-  context "[]=" do
+  context "#set" do
     it "should set simple values" do
       e = LogStash::Event.new()
-      expect(e["foo"] = "bar").to eq("bar")
-      expect(e["foo"]).to eq("bar")
+      expect(e.set("foo", "bar")).to eq("bar")
+      expect(e.get("foo")).to eq("bar")
 
       e = LogStash::Event.new({"foo" => "test"})
-      expect(e["foo"] = "bar").to eq("bar")
-      expect(e["foo"]).to eq("bar")
+      expect(e.set("foo", "bar")).to eq("bar")
+      expect(e.get("foo")).to eq("bar")
     end
 
     it "should set deep hash values" do
       e = LogStash::Event.new()
-      expect(e["[foo][bar]"] = "baz").to eq("baz")
-      expect(e["[foo][bar]"]).to eq("baz")
-      expect(e["[foo][baz]"]).to be_nil
+      expect(e.set("[foo][bar]", "baz")).to eq("baz")
+      expect(e.get("[foo][bar]")).to eq("baz")
+      expect(e.get("[foo][baz]")).to be_nil
     end
 
     it "should set deep array values" do
       e = LogStash::Event.new()
-      expect(e["[foo][0]"] = "bar").to eq("bar")
-      expect(e["[foo][0]"]).to eq("bar")
-      expect(e["[foo][1]"] = 1).to eq(1)
-      expect(e["[foo][1]"]).to eq(1)
-      expect(e["[foo][2]"] = 1.0 ).to eq(1.0)
-      expect(e["[foo][2]"]).to eq(1.0)
-      expect(e["[foo][3]"]).to be_nil
+      expect(e.set("[foo][0]", "bar")).to eq("bar")
+      expect(e.get("[foo][0]")).to eq("bar")
+      expect(e.set("[foo][1]", 1)).to eq(1)
+      expect(e.get("[foo][1]")).to eq(1)
+      expect(e.set("[foo][2]", 1.0)).to eq(1.0)
+      expect(e.get("[foo][2]")).to eq(1.0)
+      expect(e.get("[foo][3]")).to be_nil
     end
 
     it "should add key when setting nil value" do
       e = LogStash::Event.new()
-      e["[foo]"] = nil
+      e.set("[foo]", nil)
       expect(e.to_hash).to include("foo" => nil)
     end
 
     # BigDecinal is now natively converted by JRuby, see https://github.com/elastic/logstash/pull/4838
     it "should set BigDecimal" do
       e = LogStash::Event.new()
-      e["[foo]"] = BigDecimal.new(1)
-      expect(e["foo"]).to be_kind_of(BigDecimal)
-      expect(e["foo"]).to eq(BigDecimal.new(1))
+      e.set("[foo]", BigDecimal.new(1))
+      expect(e.get("foo")).to be_kind_of(BigDecimal)
+      expect(e.get("foo")).to eq(BigDecimal.new(1))
+    end
+
+    it "should set RubyBignum" do
+      e = LogStash::Event.new()
+      e.set("[foo]", -9223372036854776000)
+      expect(e.get("foo")).to be_kind_of(Bignum)
+      expect(e.get("foo")).to eq(-9223372036854776000)
+    end
+
+    it "should convert Time to Timestamp" do
+      e = LogStash::Event.new()
+      time = Time.now
+      e.set("[foo]", Time.at(time.to_f))
+      expect(e.get("foo")).to be_kind_of(LogStash::Timestamp)
+      expect(e.get("foo").to_f).to be_within(0.1).of(time.to_f)
+    end
+
+    it "should set XXJavaProxy Jackson crafted" do
+      proxy = org.logstash.Util.getMapFixtureJackson()
+      # proxy is {"string": "foo", "int": 42, "float": 42.42, "array": ["bar","baz"], "hash": {"string":"quux"} }
+      e = LogStash::Event.new()
+      e.set("[proxy]", proxy)
+      expect(e.get("[proxy][string]")).to eql("foo")
+      expect(e.get("[proxy][int]")).to eql(42)
+      expect(e.get("[proxy][float]")).to eql(42.42)
+      expect(e.get("[proxy][array][0]")).to eql("bar")
+      expect(e.get("[proxy][array][1]")).to eql("baz")
+      expect(e.get("[proxy][hash][string]")).to eql("quux")
+    end
+
+    it "should set XXJavaProxy hand crafted" do
+      proxy = org.logstash.Util.getMapFixtureHandcrafted()
+      # proxy is {"string": "foo", "int": 42, "float": 42.42, "array": ["bar","baz"], "hash": {"string":"quux"} }
+      e = LogStash::Event.new()
+      e.set("[proxy]", proxy)
+      expect(e.get("[proxy][string]")).to eql("foo")
+      expect(e.get("[proxy][int]")).to eql(42)
+      expect(e.get("[proxy][float]")).to eql(42.42)
+      expect(e.get("[proxy][array][0]")).to eql("bar")
+      expect(e.get("[proxy][array][1]")).to eql("baz")
+      expect(e.get("[proxy][hash][string]")).to eql("quux")
     end
   end
 
@@ -110,26 +152,26 @@
     it "getters should present a Ruby LogStash::Timestamp" do
       e = LogStash::Event.new()
       expect(e.timestamp.class).to eq(LogStash::Timestamp)
-      expect(e[TIMESTAMP].class).to eq(LogStash::Timestamp)
+      expect(e.get(TIMESTAMP).class).to eq(LogStash::Timestamp)
     end
 
     it "to_hash should inject a Ruby LogStash::Timestamp" do
       e = LogStash::Event.new()
 
-      expect(e.to_java).to be_kind_of(Java::ComLogstash::Event)
-      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::ComLogstash::Timestamp)
+      expect(e.to_java).to be_kind_of(Java::OrgLogstash::Event)
+      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::OrgLogstash::Timestamp)
 
       expect(e.to_hash[TIMESTAMP]).to be_kind_of(LogStash::Timestamp)
       # now make sure the original map was not touched
-      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::ComLogstash::Timestamp)
+      expect(e.to_java.get_field(TIMESTAMP)).to be_kind_of(Java::OrgLogstash::Timestamp)
     end
 
     it "should set timestamp" do
       e = LogStash::Event.new
       now = Time.now
-      e["@timestamp"] = LogStash::Timestamp.at(now.to_i)
+      e.set("@timestamp", LogStash::Timestamp.at(now.to_i))
       expect(e.timestamp.to_i).to eq(now.to_i)
-      expect(e["@timestamp"].to_i).to eq(now.to_i)
+      expect(e.get("@timestamp").to_i).to eq(now.to_i)
     end
   end
 
@@ -137,76 +179,41 @@
     it "should append" do
       event = LogStash::Event.new("message" => "hello world")
       event.append(LogStash::Event.new("message" => "another thing"))
-      expect(event["message"]).to eq(["hello world", "another thing"])
+      expect(event.get("message")).to eq(["hello world", "another thing"])
     end
   end
 
   context "tags" do
     it "should tag" do
       event = LogStash::Event.new("message" => "hello world")
-      expect(event["tags"]).to be_nil
-      event["tags"] = ["foo"]
-      expect(event["tags"]).to eq(["foo"])
+      expect(event.get("tags")).to be_nil
+      event.tag("foo")
+      expect(event.get("tags")).to eq(["foo"])
     end
   end
 
 
-  # noop logger used to test the injectable logger in Event
-  # this implementation is not complete because only the warn
-  # method is used in Event.
-  module DummyLogger
-    def self.warn(message)
-      # do nothing
-    end
-  end
-
-  context "logger" do
-
-    let(:logger) { double("Logger") }
-    after(:each) {  LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER }
-
-    # the following 2 specs are using both a real module (DummyLogger)
-    # and a mock. both tests are needed to make sure the implementation
-    # supports both types of objects.
+  # TODO(talevy): migrate tests to Java. no reason to test logging logic in ruby when it is being
+  #               done in java land.
 
-    it "should set logger using a module" do
-      LogStash::Event.logger = DummyLogger
-      expect(DummyLogger).to receive(:warn).once
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
-    end
+  # context "logger" do
 
-    it "should set logger using a mock" do
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
-    end
+  #   let(:logger) { double("Logger") }
 
-    it "should unset logger" do
-      # first set
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
+  #   before(:each) do
+  #     allow(LogStash::Event).to receive(:logger).and_return(logger)
+  #   end
 
-      # then unset
-      LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER
-      expect(logger).to receive(:warn).never
-      # this will produce a log line in stdout by the Java Event
-      LogStash::Event.new(TIMESTAMP => "ignore this log")
-    end
+  #   it "should set logger using a module" do
+  #     expect(logger).to receive(:warn).once
+  #     LogStash::Event.new(TIMESTAMP => "invalid timestamp")
+  #   end
 
-
-    it "should warn on parsing error" do
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once.with(/^Error parsing/)
-      LogStash::Event.new(TIMESTAMP => "invalid timestamp")
-    end
-
-    it "should warn on invalid timestamp object" do
-      LogStash::Event.logger = logger
-      expect(logger).to receive(:warn).once.with(/^Unrecognized/)
-      LogStash::Event.new(TIMESTAMP => Array.new)
-    end
-  end
+  #   it "should warn on invalid timestamp object" do
+  #     expect(logger).to receive(:warn).once.with(/^Unrecognized/)
+  #     LogStash::Event.new(TIMESTAMP => Array.new)
+  #   end
+  # end
 
   context "to_hash" do
     let (:source_hash) {  {"a" => 1, "b" => [1, 2, 3, {"h" => 1, "i" => "baz"}], "c" => {"d" => "foo", "e" => "bar", "f" => [4, 5, "six"]}} }
@@ -251,8 +258,8 @@ def self.warn(message)
       expect(LogStash::Event.from_json(source_json).size).to eq(1)
 
       event = LogStash::Event.from_json(source_json)[0]
-      expect(event["[foo]"]).to eq(1)
-      expect(event["[bar]"]).to eq("baz")
+      expect(event.get("[foo]")).to eq(1)
+      expect(event.get("[bar]")).to eq("baz")
     end
 
     it "should ignore blank strings" do
@@ -279,4 +286,40 @@ def self.warn(message)
        end
     end
   end
+
+  context "initialize" do
+
+    it "should accept Ruby Hash" do
+      e = LogStash::Event.new({"foo" => 1, TIMESTAMP => "2015-05-28T23:02:05.350Z"})
+      expect(e.get("foo")).to eq(1)
+      expect(e.timestamp.to_iso8601).to eq("2015-05-28T23:02:05.350Z")
+    end
+
+    it "should accept Java Map" do
+      h = Java::JavaUtil::HashMap.new
+      h.put("foo", 2);
+      h.put(TIMESTAMP, "2016-05-28T23:02:05.350Z");
+      e = LogStash::Event.new(h)
+
+      expect(e.get("foo")).to eq(2)
+      expect(e.timestamp.to_iso8601).to eq("2016-05-28T23:02:05.350Z")
+    end
+
+  end
+
+  context "method missing exception messages" do
+    subject { LogStash::Event.new({"foo" => "bar"}) }
+
+    it "#[] method raises a better exception message" do
+      expect { subject["foo"] }.to raise_error(NoMethodError, /Direct event field references \(i\.e\. event\['field'\]\)/)
+    end
+
+    it "#[]= method raises a better exception message" do
+      expect { subject["foo"] = "baz" }.to raise_error(NoMethodError, /Direct event field references \(i\.e\. event\['field'\] = 'value'\)/)
+    end
+
+    it "other missing method raises normal exception message" do
+      expect { subject.baz() }.to raise_error(NoMethodError, /undefined method `baz' for/)
+    end
+  end
 end
diff --git a/logstash-core-event-java/src/main/java/JrubyEventExtService.java b/logstash-core-event-java/src/main/java/JrubyEventExtService.java
index 306a45f3971..46d54f13c8a 100644
--- a/logstash-core-event-java/src/main/java/JrubyEventExtService.java
+++ b/logstash-core-event-java/src/main/java/JrubyEventExtService.java
@@ -1,4 +1,4 @@
-import com.logstash.ext.JrubyEventExtLibrary;
+import org.logstash.ext.JrubyEventExtLibrary;
 import org.jruby.Ruby;
 import org.jruby.runtime.load.BasicLibraryService;
 
diff --git a/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java b/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java
index 32d8eb2bf98..f11e38783e0 100644
--- a/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java
+++ b/logstash-core-event-java/src/main/java/JrubyTimestampExtService.java
@@ -1,5 +1,4 @@
-import com.logstash.ext.JrubyEventExtLibrary;
-import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.logstash.ext.JrubyTimestampExtLibrary;
 import org.jruby.Ruby;
 import org.jruby.runtime.load.BasicLibraryService;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Javafier.java b/logstash-core-event-java/src/main/java/com/logstash/Javafier.java
deleted file mode 100644
index f4f16266570..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/Javafier.java
+++ /dev/null
@@ -1,152 +0,0 @@
-package com.logstash;
-
-import org.jruby.RubyArray;
-import org.jruby.RubyHash;
-import org.jruby.RubyString;
-import org.jruby.RubyObject;
-import org.jruby.RubyBoolean;
-import org.jruby.RubyArray;
-import org.jruby.RubyFloat;
-import org.jruby.RubyInteger;
-import org.jruby.RubyNil;
-import org.jruby.RubyBoolean;
-import org.jruby.RubyFixnum;
-import org.jruby.RubyTime;
-import org.jruby.RubySymbol;
-import org.jruby.ext.bigdecimal.RubyBigDecimal;
-import com.logstash.ext.JrubyTimestampExtLibrary;
-import org.jruby.runtime.builtin.IRubyObject;
-import java.math.BigDecimal;
-import org.joda.time.DateTime;
-import java.util.*;
-
-public class Javafier {
-
-    private Javafier(){}
-
-    public static List<Object> deep(RubyArray a) {
-        final ArrayList<Object> result = new ArrayList();
-
-        // TODO: (colin) investagate why .toJavaArrayUnsafe() which should be faster by avoiding copying produces nil values spec errors in arrays
-        for (IRubyObject o : a.toJavaArray()) {
-            result.add(deep(o));
-        }
-        return result;
-    }
-
-    public static HashMap<String, Object> deep(RubyHash h) {
-        final HashMap result = new HashMap();
-
-        h.visitAll(new RubyHash.Visitor() {
-            @Override
-            public void visit(IRubyObject key, IRubyObject value) {
-                result.put(deep(key).toString(), deep(value));
-            }
-        });
-        return result;
-    }
-
-    public static String deep(RubyString s) {
-        return s.asJavaString();
-    }
-
-    public static long deep(RubyInteger i) {
-        return i.getLongValue();
-    }
-
-    public static long deep(RubyFixnum n) {
-        return n.getLongValue();
-    }
-
-    public static double deep(RubyFloat f) {
-        return f.getDoubleValue();
-    }
-
-    public static BigDecimal deep(RubyBigDecimal bd) {
-        return bd.getBigDecimalValue();
-    }
-
-    public static Timestamp deep(JrubyTimestampExtLibrary.RubyTimestamp t) {
-        return t.getTimestamp();
-    }
-
-    public static boolean deep(RubyBoolean b) {
-        return b.isTrue();
-    }
-
-    public static Object deep(RubyNil n) {
-        return null;
-    }
-
-    public static DateTime deep(RubyTime t) {
-        return t.getDateTime();
-    }
-
-    public static String deep(RubySymbol s) {
-        return s.asJavaString();
-    }
-
-    public static Object deep(RubyBoolean.True b) {
-        return true;
-    }
-
-    public static Object deep(RubyBoolean.False b) {
-        return false;
-    }
-
-    public static Object deep(IRubyObject o) {
-        // TODO: (colin) this enum strategy is cleaner but I am hoping that is not slower than using a instanceof cascade
-
-        RUBYCLASS clazz;
-        try {
-            clazz = RUBYCLASS.valueOf(o.getClass().getSimpleName());
-        } catch (IllegalArgumentException e) {
-            throw new IllegalArgumentException("Missing Ruby class handling for full class name=" + o.getClass().getName() + ", simple name=" + o.getClass().getSimpleName());
-        }
-
-        switch(clazz) {
-            case RubyArray: return deep((RubyArray)o);
-            case RubyHash: return deep((RubyHash)o);
-            case RubyString: return deep((RubyString)o);
-            case RubyInteger: return deep((RubyInteger)o);
-            case RubyFloat: return deep((RubyFloat)o);
-            case RubyBigDecimal: return deep((RubyBigDecimal)o);
-            case RubyTimestamp: return deep((JrubyTimestampExtLibrary.RubyTimestamp)o);
-            case RubyBoolean: return deep((RubyBoolean)o);
-            case RubyFixnum: return deep((RubyFixnum)o);
-            case RubyTime: return deep((RubyTime)o);
-            case RubySymbol: return deep((RubySymbol)o);
-            case RubyNil: return deep((RubyNil)o);
-            case True: return deep((RubyBoolean.True)o);
-            case False: return deep((RubyBoolean.False)o);
-        }
-
-        if (o.isNil()) {
-            return null;
-        }
-
-        // TODO: (colin) temporary trace to spot any unhandled types
-        System.out.println("***** WARN: UNHANDLED IRubyObject full class name=" + o.getMetaClass().getRealClass().getName() + ", simple name=" + o.getClass().getSimpleName() + " java class=" + o.getJavaClass().toString() + " toString=" + o.toString());
-
-        return o.toJava(o.getJavaClass());
-    }
-
-    enum RUBYCLASS {
-        RubyString,
-        RubyInteger,
-        RubyFloat,
-        RubyBigDecimal,
-        RubyTimestamp,
-        RubyArray,
-        RubyHash,
-        RubyBoolean,
-        RubyFixnum,
-        RubyObject,
-        RubyNil,
-        RubyTime,
-        RubySymbol,
-        True,
-        False;
-    }
-}
-
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Logger.java b/logstash-core-event-java/src/main/java/com/logstash/Logger.java
deleted file mode 100644
index fc425542715..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/Logger.java
+++ /dev/null
@@ -1,13 +0,0 @@
-package com.logstash;
-
-// minimalist Logger interface to wire a logger callback in the Event class
-// for now only warn is defined because this is the only method that's required
-// in the Event class.
-// TODO: (colin) generalize this
-
-public interface Logger {
-
-    // TODO: (colin) complete interface beyond warn when needed
-
-    void warn(String message);
-}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java b/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java
deleted file mode 100644
index 0bafab8c9da..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/Rubyfier.java
+++ /dev/null
@@ -1,65 +0,0 @@
-package com.logstash;
-
-import com.logstash.ext.JrubyTimestampExtLibrary;
-import org.jruby.Ruby;
-import org.jruby.RubyArray;
-import org.jruby.RubyHash;
-import org.jruby.ext.bigdecimal.RubyBigDecimal;
-import org.jruby.javasupport.JavaUtil;
-import org.jruby.runtime.builtin.IRubyObject;
-
-import java.math.BigDecimal;
-import java.util.*;
-
-public final class Rubyfier {
-
-    private Rubyfier(){}
-
-    public static IRubyObject deep(Ruby runtime, final Object input) {
-        if (input instanceof IRubyObject) return (IRubyObject)input;
-        if (input instanceof Map) return deepMap(runtime, (Map) input);
-        if (input instanceof List) return deepList(runtime, (List) input);
-        if (input instanceof Timestamp) return JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(runtime, (Timestamp)input);
-        if (input instanceof Collection) throw new ClassCastException("unexpected Collection type " + input.getClass());
-
-        // BigDecimal is not currenly handled by JRuby and this is the type Jackson uses for floats
-        if (input instanceof BigDecimal) return new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), (BigDecimal)input);
-
-        return JavaUtil.convertJavaToUsableRubyObject(runtime, input);
-    }
-
-    public static Object deepOnly(Ruby runtime, final Object input) {
-        if (input instanceof Map) return deepMap(runtime, (Map) input);
-        if (input instanceof List) return deepList(runtime, (List) input);
-        if (input instanceof Timestamp) return JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(runtime, (Timestamp)input);
-        if (input instanceof Collection) throw new ClassCastException("unexpected Collection type " + input.getClass());
-
-        // BigDecimal is not currenly handled by JRuby and this is the type Jackson uses for floats
-        if (input instanceof BigDecimal) return new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), (BigDecimal)input);
-
-        return input;
-    }
-
-    private static RubyArray deepList(Ruby runtime, final List list) {
-        final int length = list.size();
-        final RubyArray array = runtime.newArray(length);
-
-        for (Object item : list) {
-            // use deepOnly because RubyArray.add already calls JavaUtil.convertJavaToUsableRubyObject on item
-            array.add(deepOnly(runtime, item));
-        }
-
-        return array;
-    }
-
-    private static RubyHash deepMap(Ruby runtime, final Map<?, ?> map) {
-        RubyHash hash = RubyHash.newHash(runtime);
-
-        for (Map.Entry<?, ?> entry : map.entrySet()) {
-            // use deepOnly on value because RubyHash.put already calls JavaUtil.convertJavaToUsableRubyObject on items
-            hash.put(entry.getKey(), deepOnly(runtime, entry.getValue()));
-        }
-
-        return hash;
-    }
-}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java b/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java
deleted file mode 100644
index c12bb3e0573..00000000000
--- a/logstash-core-event-java/src/main/java/com/logstash/StdioLogger.java
+++ /dev/null
@@ -1,10 +0,0 @@
-package com.logstash;
-
-public class StdioLogger implements Logger {
-
-    // TODO: (colin) complete implementation beyond warn when needed
-
-    public void warn(String message) {
-        System.out.println(message);
-    }
-}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Accessors.java b/logstash-core-event-java/src/main/java/org/logstash/Accessors.java
similarity index 80%
rename from logstash-core-event-java/src/main/java/com/logstash/Accessors.java
rename to logstash-core-event-java/src/main/java/org/logstash/Accessors.java
index 4c1e597ecdc..bdf6c622b83 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Accessors.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Accessors.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.HashMap;
 import java.util.Map;
@@ -39,7 +39,7 @@ public Object del(String reference) {
                 }
                 return ((List<Object>) target).remove(i);
             } else {
-                throw new ClassCastException("expecting List or Map");
+                throw newCollectionException(target);
             }
         }
         return null;
@@ -67,7 +67,7 @@ private Object findTarget(FieldReference field) {
         target = this.data;
         for (String key : field.getPath()) {
             target = fetch(target, key);
-            if (target == null) {
+            if (! isCollection(target)) {
                 return null;
             }
         }
@@ -80,9 +80,13 @@ private Object findTarget(FieldReference field) {
     private Object findCreateTarget(FieldReference field) {
         Object target;
 
-        if ((target = this.lut.get(field.getReference())) != null) {
-            return target;
-        }
+        // flush the @lut to prevent stale cached fieldref which may point to an old target
+        // which was overwritten with a new value. for example, if "[a][b]" is cached and we
+        // set a new value for "[a]" then reading again "[a][b]" would point in a stale target.
+        // flushing the complete @lut is suboptimal, but a hierarchical lut would be required
+        // to be able to invalidate fieldrefs from a common root.
+        // see https://github.com/elastic/logstash/pull/5132
+        this.lut.clear();
 
         target = this.data;
         for (String key : field.getPath()) {
@@ -95,10 +99,8 @@ private Object findCreateTarget(FieldReference field) {
                     int i = Integer.parseInt(key);
                     // TODO: what about index out of bound?
                     ((List<Object>)target).set(i, result);
-                } else if (target == null) {
-                    // do nothing
-                } else {
-                    throw new ClassCastException("expecting List or Map");
+                } else if (target != null) {
+                    throw newCollectionException(target);
                 }
             }
             target = result;
@@ -133,8 +135,8 @@ private Object fetch(Object target, String key) {
             return result;
         } else if (target == null) {
             return null;
-        } {
-            throw new ClassCastException("expecting List or Map");
+        } else {
+            throw newCollectionException(target);
         }
     }
 
@@ -157,8 +159,19 @@ private Object store(Object target, String key, Object value) {
                 ((List<Object>) target).set(i, value);
             }
         } else {
-            throw new ClassCastException("expecting List or Map");
+            throw newCollectionException(target);
         }
         return value;
     }
+
+    private boolean isCollection(Object target) {
+        if (target == null) {
+            return false;
+        }
+        return (target instanceof Map || target instanceof List);
+    }
+
+    private ClassCastException newCollectionException(Object target) {
+        return new ClassCastException("expecting List or Map, found "  + target.getClass());
+    }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Cloner.java b/logstash-core-event-java/src/main/java/org/logstash/Cloner.java
similarity index 88%
rename from logstash-core-event-java/src/main/java/com/logstash/Cloner.java
rename to logstash-core-event-java/src/main/java/org/logstash/Cloner.java
index 4823f10726a..d2588064d5c 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Cloner.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Cloner.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.*;
 
@@ -24,6 +24,8 @@ private static <E> List<E> deepList(final List<E> list) {
             clone = new LinkedList<E>();
         } else if (list instanceof ArrayList<?>) {
             clone = new ArrayList<E>();
+        } else if (list instanceof ConvertedList<?>) {
+            clone = new ArrayList<E>();
         } else {
             throw new ClassCastException("unexpected List type " + list.getClass());
         }
@@ -43,6 +45,8 @@ private static <K, V> Map<K, V> deepMap(final Map<K, V> map) {
             clone = new TreeMap<K, V>();
         } else if (map instanceof HashMap<?, ?>) {
             clone = new HashMap<K, V>();
+        } else if (map instanceof ConvertedMap<?, ?>) {
+            clone = new HashMap<K, V>();
         } else {
             throw new ClassCastException("unexpected Map type " + map.getClass());
         }
diff --git a/logstash-core-event-java/src/main/java/org/logstash/ConvertedList.java b/logstash-core-event-java/src/main/java/org/logstash/ConvertedList.java
new file mode 100644
index 00000000000..d4cde257288
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/ConvertedList.java
@@ -0,0 +1,224 @@
+package org.logstash;
+
+import org.jruby.RubyArray;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.ArrayList;
+import java.util.Collection;
+import java.util.Comparator;
+import java.util.Iterator;
+import java.util.List;
+import java.util.ListIterator;
+import java.util.Spliterator;
+import java.util.function.Consumer;
+import java.util.function.Predicate;
+import java.util.function.UnaryOperator;
+import java.util.stream.Stream;
+
+import static org.logstash.Valuefier.convert;
+
+public class ConvertedList<T> implements List<T>, Collection<T>, Iterable<T> {
+    private final List<T> delegate;
+
+    public ConvertedList(List<T> delegate) {
+        this.delegate = delegate;
+    }
+    public ConvertedList() {
+        this.delegate = new ArrayList<>();
+    }
+
+    public static ConvertedList<Object> newFromList(List<Object> list) {
+        ConvertedList<Object> array = new ConvertedList<>();
+
+        for (Object item : list) {
+            array.add(convert(item));
+        }
+        return array;
+    }
+
+    public static ConvertedList<Object> newFromRubyArray(RubyArray a) {
+        final ConvertedList<Object> result = new ConvertedList<>();
+
+        for (IRubyObject o : a.toJavaArray()) {
+            result.add(convert(o));
+        }
+        return result;
+    }
+
+    public Object unconvert() {
+        final ArrayList<Object> result = new ArrayList<>();
+        for (Object obj : delegate) {
+            result.add(Javafier.deep(obj));
+        }
+        return result;
+    }
+
+    // delegate methods
+    @Override
+    public int size() {
+        return delegate.size();
+    }
+
+    @Override
+    public boolean isEmpty() {
+        return delegate.isEmpty();
+    }
+
+    @Override
+    public boolean contains(Object o) {
+        return delegate.contains(o);
+    }
+
+    @Override
+    public Iterator<T> iterator() {
+        return delegate.iterator();
+    }
+
+    @Override
+    public Object[] toArray() {
+        return delegate.toArray();
+    }
+
+    @Override
+    public <T1> T1[] toArray(T1[] a) {
+        return delegate.toArray(a);
+    }
+
+    @Override
+    public boolean add(T t) {
+        return delegate.add(t);
+    }
+
+    @Override
+    public boolean remove(Object o) {
+        return delegate.remove(o);
+    }
+
+    @Override
+    public boolean containsAll(Collection<?> c) {
+        return delegate.containsAll(c);
+    }
+
+    @Override
+    public boolean addAll(Collection<? extends T> c) {
+        return delegate.addAll(c);
+    }
+
+    @Override
+    public boolean addAll(int index, Collection<? extends T> c) {
+        return delegate.addAll(index, c);
+    }
+
+    @Override
+    public boolean removeAll(Collection<?> c) {
+        return delegate.removeAll(c);
+    }
+
+    @Override
+    public boolean retainAll(Collection<?> c) {
+        return delegate.retainAll(c);
+    }
+
+    @Override
+    public void replaceAll(UnaryOperator<T> operator) {
+        delegate.replaceAll(operator);
+    }
+
+    @Override
+    public void sort(Comparator<? super T> c) {
+        delegate.sort(c);
+    }
+
+    @Override
+    public void clear() {
+        delegate.clear();
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        return delegate.equals(o);
+    }
+
+    @Override
+    public int hashCode() {
+        return delegate.hashCode();
+    }
+
+    @Override
+    public T get(int index) {
+        return delegate.get(index);
+    }
+
+    @Override
+    public T set(int index, T element) {
+        return delegate.set(index, element);
+    }
+
+    @Override
+    public void add(int index, T element) {
+        delegate.add(index, element);
+    }
+
+    @Override
+    public T remove(int index) {
+        return delegate.remove(index);
+    }
+
+    @Override
+    public int indexOf(Object o) {
+        return delegate.indexOf(o);
+    }
+
+    @Override
+    public int lastIndexOf(Object o) {
+        return delegate.lastIndexOf(o);
+    }
+
+    @Override
+    public ListIterator<T> listIterator() {
+        return delegate.listIterator();
+    }
+
+    @Override
+    public ListIterator<T> listIterator(int index) {
+        return delegate.listIterator(index);
+    }
+
+    @Override
+    public List<T> subList(int fromIndex, int toIndex) {
+        return delegate.subList(fromIndex, toIndex);
+    }
+
+    @Override
+    public Spliterator<T> spliterator() {
+        return delegate.spliterator();
+    }
+
+    @Override
+    public String toString() {
+        final StringBuffer sb = new StringBuffer("ConvertedList{");
+        sb.append("delegate=").append(delegate.toString());
+        sb.append('}');
+        return sb.toString();
+    }
+
+    @Override
+    public boolean removeIf(Predicate<? super T> filter) {
+        return delegate.removeIf(filter);
+    }
+
+    @Override
+    public Stream<T> stream() {
+        return delegate.stream();
+    }
+
+    @Override
+    public Stream<T> parallelStream() {
+        return delegate.parallelStream();
+    }
+
+    @Override
+    public void forEach(Consumer<? super T> action) {
+        delegate.forEach(action);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/ConvertedMap.java b/logstash-core-event-java/src/main/java/org/logstash/ConvertedMap.java
new file mode 100644
index 00000000000..ea25ba6aba3
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/ConvertedMap.java
@@ -0,0 +1,182 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValues;
+import org.jruby.RubyHash;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.Collection;
+import java.util.HashMap;
+import java.util.Map;
+import java.util.Set;
+import java.util.function.BiConsumer;
+import java.util.function.BiFunction;
+import java.util.function.Function;
+
+public class ConvertedMap<K, V> implements Map<K, V> {
+
+    private final Map<K, V> delegate;
+
+    public ConvertedMap(Map<K, V> delegate) {
+        this.delegate = delegate;
+    }
+
+    public ConvertedMap() {
+        this.delegate = new HashMap<>();
+    }
+
+    public static ConvertedMap<String, Object> newFromMap(Map<String, Object> o) {
+        ConvertedMap<String, Object> cm = new ConvertedMap<>();
+        for (Map.Entry<String, Object> entry : o.entrySet()) {
+            String k = String.valueOf(BiValues.newBiValue(entry.getKey()).javaValue());
+            cm.put(k, Valuefier.convert(entry.getValue()));
+        }
+        return cm;
+    }
+
+    public static ConvertedMap<String, Object> newFromRubyHash(RubyHash o) {
+        final ConvertedMap<String, Object> result = new ConvertedMap<>();
+
+        o.visitAll(new RubyHash.Visitor() {
+            @Override
+            public void visit(IRubyObject key, IRubyObject value) {
+                String k = String.valueOf(BiValues.newBiValue(key).javaValue()) ;
+                result.put(k, Valuefier.convert(value));
+            }
+        });
+        return result;
+    }
+
+    public Object unconvert() {
+        final HashMap<K, V> result = new HashMap<>();
+        for (Map.Entry<K, V> entry : entrySet()) {
+            result.put(entry.getKey(), (V) Javafier.deep(entry.getValue()));
+        }
+        return result;
+    }
+
+    // Delegate methods
+    @Override
+    public int size() {
+        return delegate.size();
+    }
+
+    @Override
+    public boolean isEmpty() {
+        return delegate.isEmpty();
+    }
+
+    @Override
+    public boolean containsKey(Object key) {
+        return delegate.containsKey(key);
+    }
+
+    @Override
+    public boolean containsValue(Object value) {
+        return delegate.containsValue(value);
+    }
+
+    @Override
+    public V get(Object key) {
+        return delegate.get(key);
+    }
+
+    @Override
+    public V put(K key, V value) {
+        return delegate.put(key, value);
+    }
+
+    @Override
+    public V remove(Object key) {
+        return delegate.remove(key);
+    }
+
+    @Override
+    public void putAll(Map<? extends K, ? extends V> m) {
+        delegate.putAll(m);
+    }
+
+    @Override
+    public void clear() {
+        delegate.clear();
+    }
+
+    @Override
+    public Set<K> keySet() {
+        return delegate.keySet();
+    }
+
+    @Override
+    public Collection<V> values() {
+        return delegate.values();
+    }
+
+    @Override
+    public Set<Entry<K, V>> entrySet() {
+        return delegate.entrySet();
+    }
+
+    @Override
+    public V getOrDefault(Object key, V defaultValue) {
+        return delegate.getOrDefault(key, defaultValue);
+    }
+
+    @Override
+    public void forEach(BiConsumer<? super K, ? super V> action) {
+        delegate.forEach(action);
+    }
+
+    @Override
+    public void replaceAll(BiFunction<? super K, ? super V, ? extends V> function) {
+        delegate.replaceAll(function);
+    }
+
+    @Override
+    public V putIfAbsent(K key, V value) {
+        return delegate.putIfAbsent(key, value);
+    }
+
+    @Override
+    public boolean remove(Object key, Object value) {
+        return delegate.remove(key, value);
+    }
+
+    @Override
+    public boolean replace(K key, V oldValue, V newValue) {
+        return delegate.replace(key, oldValue, newValue);
+    }
+
+    @Override
+    public V replace(K key, V value) {
+        return delegate.replace(key, value);
+    }
+
+    @Override
+    public V computeIfAbsent(K key, Function<? super K, ? extends V> mappingFunction) {
+        return delegate.computeIfAbsent(key, mappingFunction);
+    }
+
+    @Override
+    public V computeIfPresent(K key, BiFunction<? super K, ? super V, ? extends V> remappingFunction) {
+        return delegate.computeIfPresent(key, remappingFunction);
+    }
+
+    @Override
+    public V compute(K key, BiFunction<? super K, ? super V, ? extends V> remappingFunction) {
+        return delegate.compute(key, remappingFunction);
+    }
+
+    @Override
+    public V merge(K key, V value, BiFunction<? super V, ? super V, ? extends V> remappingFunction) {
+        return delegate.merge(key, value, remappingFunction);
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        return delegate.equals(o);
+    }
+
+    @Override
+    public int hashCode() {
+        return delegate.hashCode();
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/DateNode.java b/logstash-core-event-java/src/main/java/org/logstash/DateNode.java
similarity index 92%
rename from logstash-core-event-java/src/main/java/com/logstash/DateNode.java
rename to logstash-core-event-java/src/main/java/org/logstash/DateNode.java
index 560d9f53d3c..423cb8ecb70 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/DateNode.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/DateNode.java
@@ -1,10 +1,9 @@
-package com.logstash;
+package org.logstash;
 
 import org.joda.time.DateTimeZone;
 import org.joda.time.format.DateTimeFormat;
 import org.joda.time.format.DateTimeFormatter;
 
-import java.io.IOError;
 import java.io.IOException;
 
 /**
diff --git a/logstash-core-event-java/src/main/java/com/logstash/EpochNode.java b/logstash-core-event-java/src/main/java/org/logstash/EpochNode.java
similarity index 93%
rename from logstash-core-event-java/src/main/java/com/logstash/EpochNode.java
rename to logstash-core-event-java/src/main/java/org/logstash/EpochNode.java
index 4451ffa73c4..cc228315d74 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/EpochNode.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/EpochNode.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Event.java b/logstash-core-event-java/src/main/java/org/logstash/Event.java
similarity index 83%
rename from logstash-core-event-java/src/main/java/com/logstash/Event.java
rename to logstash-core-event-java/src/main/java/org/logstash/Event.java
index bf62eb4ea3b..b54806db3dc 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Event.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Event.java
@@ -1,13 +1,23 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.databind.ObjectMapper;
-import com.logstash.ext.JrubyTimestampExtLibrary;
+import org.logstash.bivalues.NullBiValue;
+import org.logstash.bivalues.StringBiValue;
+import org.logstash.bivalues.TimeBiValue;
+import org.logstash.bivalues.TimestampBiValue;
+import org.logstash.ext.JrubyTimestampExtLibrary;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
 import org.joda.time.DateTime;
 import org.jruby.RubySymbol;
 
 import java.io.IOException;
 import java.io.Serializable;
-import java.util.*;
+import java.util.ArrayList;
+import java.util.Date;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
 
 
 public class Event implements Cloneable, Serializable {
@@ -27,13 +37,9 @@ public class Event implements Cloneable, Serializable {
     public static final String VERSION = "@version";
     public static final String VERSION_ONE = "1";
 
-    private static final Logger DEFAULT_LOGGER = new StdioLogger();
+    private static final Logger logger = LogManager.getLogger(Event.class);
     private static final ObjectMapper mapper = new ObjectMapper();
 
-    // logger is static since once set there is no point in changing it at runtime
-    // for other reasons than in tests/specs.
-    private transient static Logger logger = DEFAULT_LOGGER;
-
     public Event()
     {
         this.metadata = new HashMap<String, Object>();
@@ -48,13 +54,14 @@ public Event()
 
     public Event(Map data)
     {
-        this.data = data;
+        this.data = (Map<String, Object>)Valuefier.convert(data);
+
         if (!this.data.containsKey(VERSION)) {
             this.data.put(VERSION, VERSION_ONE);
         }
 
         if (this.data.containsKey(METADATA)) {
-            this.metadata = (HashMap<String, Object>) this.data.remove(METADATA);
+            this.metadata = (Map<String, Object>) this.data.remove(METADATA);
         } else {
             this.metadata = new HashMap<String, Object>();
         }
@@ -75,7 +82,7 @@ public Map<String, Object> getMetadata() {
     }
 
     public void setData(Map<String, Object> data) {
-        this.data = data;
+        this.data = ConvertedMap.newFromMap(data);
     }
 
     public Accessors getAccessors() {
@@ -120,6 +127,11 @@ public void setTimestamp(Timestamp t) {
     }
 
     public Object getField(String reference) {
+        Object val = getUnconvertedField(reference);
+        return Javafier.deep(val);
+    }
+
+    public Object getUnconvertedField(String reference) {
         if (reference.equals(METADATA)) {
             return this.metadata;
         } else if (reference.startsWith(METADATA_BRACKETS)) {
@@ -134,12 +146,12 @@ public void setField(String reference, Object value) {
             // TODO(talevy): check type of timestamp
             this.accessors.set(reference, value);
         } else if (reference.equals(METADATA_BRACKETS) || reference.equals(METADATA)) {
-            this.metadata = (HashMap<String, Object>) value;
+            this.metadata = (Map<String, Object>) value;
             this.metadata_accessors = new Accessors(this.metadata);
         } else if (reference.startsWith(METADATA_BRACKETS)) {
             this.metadata_accessors.set(reference.substring(METADATA_BRACKETS.length()), value);
         } else {
-            this.accessors.set(reference, value);
+            this.accessors.set(reference, Valuefier.convert(value));
         }
     }
 
@@ -156,7 +168,7 @@ public boolean includes(String reference) {
     public String toJson()
             throws IOException
     {
-        return mapper.writeValueAsString((Map<String, Object>)this.data);
+        return mapper.writeValueAsString(this.data);
     }
 
     public static Event[] fromJson(String json)
@@ -189,7 +201,7 @@ public static Event[] fromJson(String json)
     }
 
     public Map toMap() {
-        return this.data;
+        return Cloner.deep(this.data);
     }
 
     public Event overwrite(Event e) {
@@ -205,7 +217,6 @@ public Event overwrite(Event e) {
         return this;
     }
 
-
     public Event append(Event e) {
         Util.mapMerge(this.data, e.data);
 
@@ -247,16 +258,22 @@ public String toString() {
 
     private Timestamp initTimestamp(Object o) {
         try {
-            if (o == null) {
+            if (o == null || o instanceof NullBiValue) {
                 // most frequent
                 return new Timestamp();
             } else if (o instanceof String) {
                 // second most frequent
                 return new Timestamp((String) o);
+            } else if (o instanceof StringBiValue) {
+                return new Timestamp(((StringBiValue) o).javaValue());
+            } else if (o instanceof TimeBiValue) {
+                return new Timestamp(((TimeBiValue) o).javaValue());
             } else if (o instanceof JrubyTimestampExtLibrary.RubyTimestamp) {
                 return new Timestamp(((JrubyTimestampExtLibrary.RubyTimestamp) o).getTimestamp());
             } else if (o instanceof Timestamp) {
                 return new Timestamp((Timestamp) o);
+            } else if (o instanceof TimestampBiValue) {
+                return new Timestamp(((TimestampBiValue) o).javaValue());
             } else if (o instanceof DateTime) {
                 return new Timestamp((DateTime) o);
             } else if (o instanceof Date) {
@@ -264,10 +281,10 @@ private Timestamp initTimestamp(Object o) {
             } else if (o instanceof RubySymbol) {
                 return new Timestamp(((RubySymbol) o).asJavaString());
             } else {
-                Event.logger.warn("Unrecognized " + TIMESTAMP + " value type=" + o.getClass().toString());
+                logger.warn("Unrecognized " + TIMESTAMP + " value type=" + o.getClass().toString());
             }
         } catch (IllegalArgumentException e) {
-            Event.logger.warn("Error parsing " + TIMESTAMP + " string value=" + o.toString());
+            logger.warn("Error parsing " + TIMESTAMP + " string value=" + o.toString());
         }
 
         tag(TIMESTAMP_FAILURE_TAG);
@@ -287,10 +304,4 @@ public void tag(String tag) {
             tags.add(tag);
         }
     }
-
-    // Event.logger is static since once set there is no point in changing it at runtime
-    // for other reasons than in tests/specs.
-    public static void setLogger(Logger logger) {
-        Event.logger = logger;
-    }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/FieldReference.java b/logstash-core-event-java/src/main/java/org/logstash/FieldReference.java
similarity index 97%
rename from logstash-core-event-java/src/main/java/com/logstash/FieldReference.java
rename to logstash-core-event-java/src/main/java/org/logstash/FieldReference.java
index e0d7e3ee969..5e468743e39 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/FieldReference.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/FieldReference.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.ArrayList;
 import java.util.Arrays;
diff --git a/logstash-core-event-java/src/main/java/org/logstash/Javafier.java b/logstash-core-event-java/src/main/java/org/logstash/Javafier.java
new file mode 100644
index 00000000000..b12e7ec6bac
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/Javafier.java
@@ -0,0 +1,34 @@
+package org.logstash;
+
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+
+public class Javafier {
+    private static final String ERR_TEMPLATE = "Missing Ruby class handling for full class name=%s, simple name=%s";
+    /*
+    Javafier.deep() is called by getField.
+    When any value is added to the Event it should pass through Valuefier.convert.
+    deep(Object o) is the mechanism to pluck the Java value from a BiValue or convert a
+    ConvertedList and ConvertedMap back to ArrayList or HashMap.
+     */
+    private Javafier(){}
+
+    public static Object deep(Object o) {
+        if (o instanceof BiValue) {
+            return ((BiValue)o).javaValue();
+        } else if(o instanceof ConvertedMap) {
+            return ((ConvertedMap) o).unconvert();
+        }  else if(o instanceof ConvertedList) {
+            return ((ConvertedList) o).unconvert();
+        } else {
+            try {
+                return BiValues.newBiValue(o).javaValue();
+            } catch (IllegalArgumentException e) {
+                Class cls = o.getClass();
+                throw new IllegalArgumentException(String.format(ERR_TEMPLATE, cls.getName(), cls.getSimpleName()));
+            }
+        }
+    }
+}
+
diff --git a/logstash-core-event-java/src/main/java/com/logstash/KeyNode.java b/logstash-core-event-java/src/main/java/org/logstash/KeyNode.java
similarity index 91%
rename from logstash-core-event-java/src/main/java/com/logstash/KeyNode.java
rename to logstash-core-event-java/src/main/java/org/logstash/KeyNode.java
index cfc46861f69..e6a5c0bea3d 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/KeyNode.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/KeyNode.java
@@ -1,6 +1,7 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.databind.ObjectMapper;
+import org.logstash.bivalues.BiValue;
 
 import java.io.IOException;
 import java.util.List;
@@ -58,6 +59,9 @@ public static String join(List<?> list, String delim) {
     private static String toString(Object value, String delim) {
         if (value == null) return "";
         if (value instanceof List) return join((List)value, delim);
+        if (value instanceof BiValue) {
+            return ((BiValue) value).toString();
+        }
         return value.toString();
     }
 }
\ No newline at end of file
diff --git a/logstash-core-event-java/src/main/java/com/logstash/PathCache.java b/logstash-core-event-java/src/main/java/org/logstash/PathCache.java
similarity index 98%
rename from logstash-core-event-java/src/main/java/com/logstash/PathCache.java
rename to logstash-core-event-java/src/main/java/org/logstash/PathCache.java
index b7beff95b89..2e884470850 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/PathCache.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/PathCache.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.util.concurrent.ConcurrentHashMap;
 
diff --git a/logstash-core-event-java/src/main/java/org/logstash/Rubyfier.java b/logstash-core-event-java/src/main/java/org/logstash/Rubyfier.java
new file mode 100644
index 00000000000..a23b65c314b
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/Rubyfier.java
@@ -0,0 +1,59 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+import org.jruby.Ruby;
+import org.jruby.RubyArray;
+import org.jruby.RubyHash;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.Collection;
+import java.util.List;
+import java.util.Map;
+
+public final class Rubyfier {
+    private static final String ERR_TEMPLATE = "Missing Java class handling for full class name=%s, simple name=%s";
+    /*
+    Rubyfier.deep() is called by JrubyEventExtLibrary RubyEvent ruby_get_field,
+    ruby_remove, ruby_to_hash and ruby_to_hash_with_metadata.
+    When any value is added to the Event it should pass through Valuefier.convert.
+    Rubyfier.deep is the mechanism to pluck the Ruby value from a BiValue or convert a
+    ConvertedList and ConvertedMap back to RubyArray or RubyHash.
+    However, IRubyObjects and the RUby runtime do not belong in ConvertedMap or ConvertedList
+    so they are unconverted here.
+    */
+    private Rubyfier() {
+    }
+
+    public static IRubyObject deep(Ruby runtime, final Object input) {
+        if (input instanceof BiValue) return ((BiValue) input).rubyValue(runtime);
+        if (input instanceof Map) return deepMap(runtime, (Map) input);
+        if (input instanceof List) return deepList(runtime, (List) input);
+        if (input instanceof Collection) throw new ClassCastException("Unexpected Collection type " + input.getClass());
+
+        try {
+            return BiValues.newBiValue(input).rubyValue(runtime);
+        } catch (IllegalArgumentException e) {
+            Class cls = input.getClass();
+            throw new IllegalArgumentException(String.format(ERR_TEMPLATE, cls.getName(), cls.getSimpleName()));
+        }
+    }
+
+    private static RubyArray deepList(Ruby runtime, final List list) {
+        final int length = list.size();
+        final RubyArray array = runtime.newArray(length);
+        for (Object item : list) {
+            array.add(deep(runtime, item));
+        }
+        return array;
+    }
+
+    private static RubyHash deepMap(Ruby runtime, final Map<?, ?> map) {
+        RubyHash hash = RubyHash.newHash(runtime);
+        for (Map.Entry entry : map.entrySet()) {
+            // Note: RubyHash.put calls JavaUtil.convertJavaToUsableRubyObject on keys and values
+            hash.put(entry.getKey(), deep(runtime, entry.getValue()));
+        }
+        return hash;
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StaticNode.java b/logstash-core-event-java/src/main/java/org/logstash/StaticNode.java
similarity index 93%
rename from logstash-core-event-java/src/main/java/com/logstash/StaticNode.java
rename to logstash-core-event-java/src/main/java/org/logstash/StaticNode.java
index 73b5c160440..36c2ef11123 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/StaticNode.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/StaticNode.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java b/logstash-core-event-java/src/main/java/org/logstash/StringInterpolation.java
similarity index 98%
rename from logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java
rename to logstash-core-event-java/src/main/java/org/logstash/StringInterpolation.java
index 5830cc89672..2bf93561095 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/StringInterpolation.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/StringInterpolation.java
@@ -1,9 +1,8 @@
-package com.logstash;
+package org.logstash;
 
 
 import java.io.IOException;
 import java.util.Map;
-import java.util.Objects;
 import java.util.concurrent.ConcurrentHashMap;
 import java.util.regex.Matcher;
 import java.util.regex.Pattern;
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Template.java b/logstash-core-event-java/src/main/java/org/logstash/Template.java
similarity index 96%
rename from logstash-core-event-java/src/main/java/com/logstash/Template.java
rename to logstash-core-event-java/src/main/java/org/logstash/Template.java
index a17e69b3946..418e1690824 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Template.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Template.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 import java.util.ArrayList;
diff --git a/logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java b/logstash-core-event-java/src/main/java/org/logstash/TemplateNode.java
similarity index 87%
rename from logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java
rename to logstash-core-event-java/src/main/java/org/logstash/TemplateNode.java
index 942bbc1ee03..1f7d9fbcf56 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/TemplateNode.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/TemplateNode.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import java.io.IOException;
 
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java b/logstash-core-event-java/src/main/java/org/logstash/Timestamp.java
similarity index 99%
rename from logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
rename to logstash-core-event-java/src/main/java/org/logstash/Timestamp.java
index 434dc93a13c..f3cea5b560b 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Timestamp.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Timestamp.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.databind.annotation.JsonSerialize;
 import org.joda.time.DateTime;
diff --git a/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java b/logstash-core-event-java/src/main/java/org/logstash/TimestampSerializer.java
similarity index 95%
rename from logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
rename to logstash-core-event-java/src/main/java/org/logstash/TimestampSerializer.java
index c90afdd9227..3c854c3d8b5 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/TimestampSerializer.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/TimestampSerializer.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import com.fasterxml.jackson.core.JsonGenerator;
 import com.fasterxml.jackson.databind.JsonSerializer;
diff --git a/logstash-core-event-java/src/main/java/com/logstash/Util.java b/logstash-core-event-java/src/main/java/org/logstash/Util.java
similarity index 66%
rename from logstash-core-event-java/src/main/java/com/logstash/Util.java
rename to logstash-core-event-java/src/main/java/org/logstash/Util.java
index 907fd5489b1..77a6565b7de 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/Util.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/Util.java
@@ -1,13 +1,44 @@
-package com.logstash;
+package org.logstash;
 
+import com.fasterxml.jackson.databind.ObjectMapper;
+
+import java.io.IOException;
 import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
 import java.util.LinkedHashSet;
 import java.util.List;
 import java.util.Map;
 
+
 public class Util {
     private Util() {}
 
+    public static Object getMapFixtureJackson() throws IOException {
+        StringBuilder json = new StringBuilder();
+        json.append("{");
+        json.append("\"string\": \"foo\", ");
+        json.append("\"int\": 42, ");
+        json.append("\"float\": 42.42, ");
+        json.append("\"array\": [\"bar\",\"baz\"], ");
+        json.append("\"hash\": {\"string\":\"quux\"} }");
+
+        ObjectMapper mapper = new ObjectMapper();
+        return mapper.readValue(json.toString(), Object.class);
+    }
+
+    public static Map<String, Object> getMapFixtureHandcrafted() {
+        HashMap<String, Object> inner = new HashMap<>();
+        inner.put("string", "quux");
+        HashMap<String, Object> map = new HashMap<>();
+        map.put("string", "foo");
+        map.put("int", 42);
+        map.put("float", 42.42);
+        map.put("array", Arrays.asList("bar", "baz"));
+        map.put("hash", inner);
+        return map;
+    }
+
     public static void mapMerge(Map<String, Object> target, Map<String, Object> add) {
         for (Map.Entry<String, Object> e : add.entrySet()) {
             if (target.containsKey(e.getKey())) {
diff --git a/logstash-core-event-java/src/main/java/org/logstash/Valuefier.java b/logstash-core-event-java/src/main/java/org/logstash/Valuefier.java
new file mode 100644
index 00000000000..1739c11c312
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/Valuefier.java
@@ -0,0 +1,92 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+import org.logstash.ext.JrubyTimestampExtLibrary;
+import org.joda.time.DateTime;
+import org.jruby.RubyArray;
+import org.jruby.RubyHash;
+import org.jruby.RubyTime;
+import org.jruby.java.proxies.ArrayJavaProxy;
+import org.jruby.java.proxies.ConcreteJavaProxy;
+import org.jruby.java.proxies.JavaProxy;
+import org.jruby.java.proxies.MapJavaProxy;
+import org.jruby.javasupport.JavaUtil;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.util.List;
+import java.util.Map;
+
+public class Valuefier {
+    private static final String PROXY_ERR_TEMPLATE = "Missing Valuefier handling for full class name=%s, simple name=%s, wrapped object=%s";
+    private static final String ERR_TEMPLATE = "Missing Valuefier handling for full class name=%s, simple name=%s";
+
+    private Valuefier(){}
+
+    private static Object convertJavaProxy(JavaProxy jp) {
+        Object obj = JavaUtil.unwrapJavaObject(jp);
+        if (obj instanceof IRubyObject[]) {
+            ConvertedList<Object> list = new ConvertedList<>();
+            for (IRubyObject ro : ((IRubyObject[]) obj)) {
+                list.add(convert(ro));
+            }
+            return list;
+        }
+        if (obj instanceof List) {
+            return ConvertedList.newFromList((List<Object>) obj);
+        }
+        try {
+            return BiValues.newBiValue(jp);
+        } catch (IllegalArgumentException e) {
+            Class cls = obj.getClass();
+            throw new IllegalArgumentException(String.format(PROXY_ERR_TEMPLATE, cls.getName(), cls.getSimpleName(), obj.getClass().getName()), e);
+        }
+    }
+
+    public static Object convertNonCollection(Object o) {
+        try {
+            return BiValues.newBiValue(o);
+        } catch (IllegalArgumentException e) {
+            Class cls = o.getClass();
+            throw new IllegalArgumentException(String.format(ERR_TEMPLATE, cls.getName(), cls.getSimpleName()), e);
+        }
+    }
+
+    public static Object convert(Object o) throws IllegalArgumentException {
+        if (o instanceof ConvertedMap || o instanceof ConvertedList) {
+            return o;
+        }
+        if (o instanceof BiValue) {
+            return o;
+        }
+        if (o instanceof RubyHash) {
+            return ConvertedMap.newFromRubyHash((RubyHash) o);
+        }
+        if (o instanceof RubyArray) {
+            return ConvertedList.newFromRubyArray((RubyArray) o);
+        }
+        if (o instanceof Map) {
+            return ConvertedMap.newFromMap((Map<String, Object>) o);
+        }
+        if (o instanceof List) {
+            return ConvertedList.newFromList((List<Object>) o);
+        }
+        if (o instanceof MapJavaProxy){
+            return ConvertedMap.newFromMap((Map)((MapJavaProxy) o).getObject());
+        }
+        if (o instanceof ArrayJavaProxy || o instanceof ConcreteJavaProxy){
+            return convertJavaProxy((JavaProxy) o);
+        }
+        if (o instanceof RubyTime) {
+            RubyTime time = (RubyTime) o;
+            Timestamp ts = new Timestamp(time.getDateTime());
+            JrubyTimestampExtLibrary.RubyTimestamp rts = JrubyTimestampExtLibrary.RubyTimestamp.newRubyTimestamp(time.getRuntime(), ts);
+            return convertNonCollection(rts);
+        }
+        if (o instanceof DateTime) {
+            Timestamp ts = new Timestamp((DateTime) o);
+            return convertNonCollection(ts);
+        }
+        return convertNonCollection(o);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValue.java
new file mode 100644
index 00000000000..7270f8e5e8e
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValue.java
@@ -0,0 +1,18 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.runtime.builtin.IRubyObject;
+
+public interface BiValue<R extends IRubyObject, J> {
+    IRubyObject rubyValue(Ruby runtime);
+
+    J javaValue();
+
+    R rubyValueUnconverted();
+
+    boolean hasRubyValue();
+
+    boolean hasJavaValue();
+}
+
+
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValueCommon.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValueCommon.java
new file mode 100644
index 00000000000..0e8748f0be4
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValueCommon.java
@@ -0,0 +1,110 @@
+package org.logstash.bivalues;
+
+import com.fasterxml.jackson.annotation.JsonValue;
+import org.jruby.Ruby;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.io.InvalidObjectException;
+import java.io.ObjectInputStream;
+import java.io.ObjectStreamException;
+import java.io.Serializable;
+
+public abstract class BiValueCommon<R extends IRubyObject, J> implements Serializable {
+    protected transient R rubyValue;
+    protected J javaValue;
+
+    public R rubyValue(Ruby runtime) {
+        if (hasRubyValue()) {
+            return rubyValue;
+        }
+        addRuby(runtime);
+        return rubyValue;
+    }
+
+    @JsonValue
+    public J javaValue() {
+        if (javaValue == null) {
+            addJava();
+        }
+        return javaValue;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+
+        if (hasJavaValue() && javaValue.getClass().isAssignableFrom(o.getClass())){
+            return javaValue.equals(o);
+        }
+
+        if(!(o instanceof BiValue)) {
+            return false;
+        }
+
+        BiValueCommon<?, ?> other = (BiValueCommon<?, ?>) o;
+
+        return (other.hasJavaValue() && other.javaValue().equals(javaValue)) ||
+                (other.hasRubyValue() && other.rubyValueUnconverted().equals(rubyValue));
+
+    }
+
+    @Override
+    public int hashCode() {
+        if (hasRubyValue()) {
+            return rubyValue.hashCode();
+        }
+        if (hasJavaValue()) {
+            return javaValue.hashCode();
+        }
+        return 0;
+    }
+
+    public R rubyValueUnconverted() {
+        return rubyValue;
+    }
+
+    public boolean hasRubyValue() {
+        return null != rubyValue;
+    }
+
+    public boolean hasJavaValue() {
+        return null != javaValue;
+    }
+
+    protected abstract void addRuby(Ruby runtime);
+
+    protected abstract void addJava();
+
+    @Override
+    public String toString() {
+        if (hasRubyValue()) {
+            javaValue();
+        }
+        if (javaValue == null) {
+            return "";
+        }
+        return String.valueOf(javaValue);
+    }
+
+    protected static Object newProxy(BiValue instance) {
+        return new SerializationProxy(instance);
+    }
+
+    private static class SerializationProxy implements Serializable {
+        private static final long serialVersionUID = -1749700725129586973L;
+
+        private final Object javaValue;
+
+        public SerializationProxy(BiValue o) {
+            javaValue = o.javaValue(); // ensure the javaValue is converted from a ruby one if it exists
+        }
+
+        private Object readResolve() throws ObjectStreamException {
+            return BiValues.newBiValue(javaValue);
+        }
+    }
+
+    private void readObject(ObjectInputStream stream) throws InvalidObjectException {
+        throw new InvalidObjectException("Proxy required");
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValues.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValues.java
new file mode 100644
index 00000000000..8529189fa06
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BiValues.java
@@ -0,0 +1,203 @@
+package org.logstash.bivalues;
+
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.jruby.RubyBignum;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.RubyNil;
+import org.jruby.RubyString;
+import org.jruby.RubySymbol;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.jruby.java.proxies.JavaProxy;
+import org.jruby.runtime.builtin.IRubyObject;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+import java.util.HashMap;
+
+public enum BiValues {
+    ORG_LOGSTASH_EXT_JRUBYTIMESTAMPEXTLIBRARY$RUBYTIMESTAMP(BiValueType.TIMESTAMP),
+    ORG_LOGSTASH_TIMESTAMP(BiValueType.TIMESTAMP),
+    JAVA_LANG_BOOLEAN(BiValueType.BOOLEAN),
+    JAVA_LANG_DOUBLE(BiValueType.DOUBLE),
+    JAVA_LANG_FLOAT(BiValueType.FLOAT),
+    JAVA_LANG_INTEGER(BiValueType.INT),
+    JAVA_LANG_LONG(BiValueType.LONG),
+    JAVA_LANG_STRING(BiValueType.STRING),
+    JAVA_MATH_BIGDECIMAL(BiValueType.DECIMAL),
+    JAVA_MATH_BIGINTEGER(BiValueType.BIGINT),
+    ORG_JRUBY_EXT_BIGDECIMAL_RUBYBIGDECIMAL(BiValueType.DECIMAL),
+    ORG_JRUBY_JAVA_PROXIES_CONCRETEJAVAPROXY(BiValueType.JAVAPROXY),
+    ORG_JRUBY_RUBYBIGNUM(BiValueType.BIGINT),
+    ORG_JRUBY_RUBYBOOLEAN$FALSE(BiValueType.BOOLEAN),
+    ORG_JRUBY_RUBYBOOLEAN$TRUE(BiValueType.BOOLEAN),
+    ORG_JRUBY_RUBYBOOLEAN(BiValueType.BOOLEAN),
+    ORG_JRUBY_RUBYFIXNUM(BiValueType.LONG),
+    ORG_JRUBY_RUBYFLOAT(BiValueType.DOUBLE),
+    ORG_JRUBY_RUBYINTEGER(BiValueType.LONG),
+    ORG_JRUBY_RUBYNIL(BiValueType.NULL),
+    ORG_JRUBY_RUBYSTRING(BiValueType.STRING),
+    ORG_JRUBY_RUBYSYMBOL(BiValueType.SYMBOL), // one way conversion, a Java string will use STRING
+    NULL(BiValueType.NULL);
+
+    private static HashMap<String, String> initCache() {
+        HashMap<String, String> hm = new HashMap<>();
+        hm.put("org.logstash.Timestamp", "ORG_LOGSTASH_TIMESTAMP");
+        hm.put("org.logstash.ext.JrubyTimestampExtLibrary$RubyTimestamp", "ORG_LOGSTASH_EXT_JRUBYTIMESTAMPEXTLIBRARY$RUBYTIMESTAMP");
+        hm.put("java.lang.Boolean", "JAVA_LANG_BOOLEAN");
+        hm.put("java.lang.Double", "JAVA_LANG_DOUBLE");
+        hm.put("java.lang.Float", "JAVA_LANG_FLOAT");
+        hm.put("java.lang.Integer", "JAVA_LANG_INTEGER");
+        hm.put("java.lang.Long", "JAVA_LANG_LONG");
+        hm.put("java.lang.String", "JAVA_LANG_STRING");
+        hm.put("java.math.BigDecimal", "JAVA_MATH_BIGDECIMAL");
+        hm.put("java.math.BigInteger", "JAVA_MATH_BIGINTEGER");
+        hm.put("org.jruby.RubyBignum", "ORG_JRUBY_RUBYBIGNUM");
+        hm.put("org.jruby.RubyBoolean", "ORG_JRUBY_RUBYBOOLEAN");
+        hm.put("org.jruby.RubyBoolean$False", "ORG_JRUBY_RUBYBOOLEAN$FALSE");
+        hm.put("org.jruby.RubyBoolean$True", "ORG_JRUBY_RUBYBOOLEAN$TRUE");
+        hm.put("org.jruby.RubyFixnum", "ORG_JRUBY_RUBYFIXNUM");
+        hm.put("org.jruby.RubyFloat", "ORG_JRUBY_RUBYFLOAT");
+        hm.put("org.jruby.RubyInteger", "ORG_JRUBY_RUBYINTEGER");
+        hm.put("org.jruby.RubyNil", "ORG_JRUBY_RUBYNIL");
+        hm.put("org.jruby.RubyString", "ORG_JRUBY_RUBYSTRING");
+        hm.put("org.jruby.RubySymbol", "ORG_JRUBY_RUBYSYMBOL");
+        hm.put("org.jruby.ext.bigdecimal.RubyBigDecimal", "ORG_JRUBY_EXT_BIGDECIMAL_RUBYBIGDECIMAL");
+        hm.put("org.jruby.java.proxies.ConcreteJavaProxy", "ORG_JRUBY_JAVA_PROXIES_CONCRETEJAVAPROXY");
+        return hm;
+    }
+
+    private final BiValueType biValueType;
+
+    BiValues(BiValueType biValueType) {
+        this.biValueType = biValueType;
+    }
+
+    private static final HashMap<String, String> nameCache = initCache();
+
+    private BiValue build(Object value) {
+        return biValueType.build(value);
+    }
+
+    public static BiValue newBiValue(Object o) {
+        if (o == null){
+            return NULL.build(null);
+        }
+        BiValues bvs = valueOf(fetchName(o));
+        return bvs.build(o);
+    }
+
+    private static String fetchName(Object o) {
+        String cls = o.getClass().getName();
+        if (nameCache.containsKey(cls)) {
+            return nameCache.get(cls);
+        }
+        String toCache = cls.toUpperCase().replace('.', '_');
+        // TODO[Guy] log warn that we are seeing a uncached value
+        nameCache.put(cls, toCache);
+        return toCache;
+    }
+
+    private enum BiValueType {
+        STRING {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new StringBiValue((RubyString) value);
+                }
+                return new StringBiValue((String) value);
+            }
+        },
+        SYMBOL {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new SymbolBiValue((RubySymbol) value);
+                }
+                return new SymbolBiValue((String) value);
+            }
+        },
+        LONG {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new LongBiValue((RubyInteger) value);
+                }
+                return new LongBiValue((Long) value);
+            }
+        },
+        INT {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new IntegerBiValue((RubyInteger) value);
+                }
+                return new IntegerBiValue((Integer) value);
+            }
+        },
+        DOUBLE {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new DoubleBiValue((RubyFloat) value);
+                }
+                return new DoubleBiValue((Double) value);
+            }
+        },
+        FLOAT {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new DoubleBiValue((RubyFloat) value);
+                }
+                return new FloatBiValue((Float) value);
+            }
+        },
+        DECIMAL {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new BigDecimalBiValue((RubyBigDecimal) value);
+                }
+                return new BigDecimalBiValue((BigDecimal) value);
+            }
+        },
+        BOOLEAN {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new BooleanBiValue((RubyBoolean) value);
+                }
+                return new BooleanBiValue((Boolean) value);
+            }
+        },
+        TIMESTAMP {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new TimestampBiValue((RubyTimestamp) value);
+                }
+                return new TimestampBiValue((Timestamp) value);
+            }
+        },
+        NULL {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new NullBiValue((RubyNil) value);
+                }
+                return NullBiValue.newNullBiValue();
+            }
+        },
+        BIGINT {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new BigIntegerBiValue((RubyBignum) value);
+                }
+                return new BigIntegerBiValue((BigInteger) value);
+            }
+        },
+        JAVAPROXY {
+            BiValue build(Object value) {
+                if (value instanceof IRubyObject) {
+                    return new JavaProxyBiValue((JavaProxy) value);
+                }
+                return new JavaProxyBiValue(value);
+            }
+        };
+        abstract BiValue build(Object value);
+    }
+
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/BigDecimalBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BigDecimalBiValue.java
new file mode 100644
index 00000000000..a4226a9dcd4
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BigDecimalBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+
+import java.io.ObjectStreamException;
+import java.math.BigDecimal;
+
+public class BigDecimalBiValue extends BiValueCommon<RubyBigDecimal, BigDecimal> implements BiValue<RubyBigDecimal, BigDecimal> {
+
+    public BigDecimalBiValue(RubyBigDecimal rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public BigDecimalBiValue(BigDecimal javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private BigDecimalBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = new RubyBigDecimal(runtime, runtime.getClass("BigDecimal"), javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getBigDecimalValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/BigIntegerBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BigIntegerBiValue.java
new file mode 100644
index 00000000000..e2d04822448
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BigIntegerBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyBignum;
+
+import java.io.ObjectStreamException;
+import java.math.BigInteger;
+
+public class BigIntegerBiValue extends BiValueCommon<RubyBignum, BigInteger> implements BiValue<RubyBignum, BigInteger> {
+
+    public BigIntegerBiValue(RubyBignum rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public BigIntegerBiValue(BigInteger javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private BigIntegerBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = new RubyBignum(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/BooleanBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BooleanBiValue.java
new file mode 100644
index 00000000000..e50cee9cbd3
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/BooleanBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyBoolean;
+
+import java.io.ObjectStreamException;
+
+
+public class BooleanBiValue extends BiValueCommon<RubyBoolean, Boolean> implements BiValue<RubyBoolean, Boolean> {
+
+    public BooleanBiValue(RubyBoolean rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public BooleanBiValue(Boolean javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private BooleanBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyBoolean.newBoolean(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.isTrue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/DoubleBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/DoubleBiValue.java
new file mode 100644
index 00000000000..9185875f691
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/DoubleBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyFloat;
+
+import java.io.ObjectStreamException;
+
+
+public class DoubleBiValue extends BiValueCommon<RubyFloat, Double> implements BiValue<RubyFloat, Double> {
+
+    public DoubleBiValue(RubyFloat rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public DoubleBiValue(Double javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private DoubleBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyFloat.newFloat(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getDoubleValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/FloatBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/FloatBiValue.java
new file mode 100644
index 00000000000..95771b8b9ce
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/FloatBiValue.java
@@ -0,0 +1,40 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyFloat;
+
+import java.io.ObjectStreamException;
+
+
+public class FloatBiValue extends BiValueCommon<RubyFloat, Float> implements BiValue<RubyFloat, Float> {
+
+    public FloatBiValue(RubyFloat rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public FloatBiValue(Float javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private FloatBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyFloat.newFloat(runtime, (double)javaValue);
+    }
+
+    protected void addJava() {
+        double value = rubyValue.getDoubleValue();
+        if ((float) value != value) {
+            throw new ArithmeticException("Float overflow - Incorrect FloatBiValue usage: BiValues should pick DoubleBiValue for RubyFloat");
+        }
+        javaValue = (float) value;
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/IntegerBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/IntegerBiValue.java
new file mode 100644
index 00000000000..92e6da9d28a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/IntegerBiValue.java
@@ -0,0 +1,40 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyInteger;
+import org.jruby.javasupport.JavaUtil;
+
+import java.io.ObjectStreamException;
+
+public class IntegerBiValue extends BiValueCommon<RubyInteger, Integer> implements BiValue<RubyInteger, Integer> {
+
+    public IntegerBiValue(RubyInteger rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public IntegerBiValue(int javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private IntegerBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        long value = rubyValue.getLongValue();
+        if ((int) value != value) {
+            throw new ArithmeticException("Integer overflow - Incorrect IntegerBiValue usage: BiValues should pick LongBiValue for RubyInteger");
+        }
+        javaValue = (int) value;
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/JavaProxyBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/JavaProxyBiValue.java
new file mode 100644
index 00000000000..f8f3e8771ff
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/JavaProxyBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.java.proxies.JavaProxy;
+import org.jruby.javasupport.JavaUtil;
+
+import java.io.ObjectStreamException;
+
+public class JavaProxyBiValue extends BiValueCommon<JavaProxy, Object> implements BiValue<JavaProxy, Object> {
+
+    public JavaProxyBiValue(JavaProxy rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public JavaProxyBiValue(Object javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private JavaProxyBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (JavaProxy) JavaUtil.convertJavaToUsableRubyObject(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getObject();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/LongBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/LongBiValue.java
new file mode 100644
index 00000000000..2b7c1cb8ef4
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/LongBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyInteger;
+import org.jruby.javasupport.JavaUtil;
+
+import java.io.ObjectStreamException;
+
+public class LongBiValue extends BiValueCommon<RubyInteger, Long> implements BiValue<RubyInteger, Long> {
+
+    public LongBiValue(RubyInteger rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public LongBiValue(long javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private LongBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getLongValue();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/NullBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/NullBiValue.java
new file mode 100644
index 00000000000..4a5f4e53b77
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/NullBiValue.java
@@ -0,0 +1,45 @@
+package org.logstash.bivalues;
+
+import com.fasterxml.jackson.annotation.JsonValue;
+import org.jruby.Ruby;
+import org.jruby.RubyNil;
+
+import java.io.ObjectStreamException;
+
+public class NullBiValue extends BiValueCommon<RubyNil, Object> implements BiValue<RubyNil, Object> {
+    public static NullBiValue newNullBiValue() {
+        return new NullBiValue();
+    }
+
+    public NullBiValue(RubyNil rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    private NullBiValue() {
+        rubyValue = null;
+        javaValue = null;
+    }
+
+    @JsonValue
+    @Override
+    public Object javaValue() {
+        return null;
+    }
+
+    @Override
+    public boolean hasJavaValue() {
+        return true;
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = (RubyNil) runtime.getNil();
+    }
+
+    protected void addJava() {}
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/StringBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/StringBiValue.java
new file mode 100644
index 00000000000..5369e23e7bf
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/StringBiValue.java
@@ -0,0 +1,35 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubyString;
+
+import java.io.ObjectStreamException;
+
+public class StringBiValue extends BiValueCommon<RubyString, String> implements BiValue<RubyString, String> {
+
+    public StringBiValue(RubyString rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public StringBiValue(String javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private StringBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyString.newUnicodeString(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.asJavaString();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/SymbolBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/SymbolBiValue.java
new file mode 100644
index 00000000000..3cbcde3871f
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/SymbolBiValue.java
@@ -0,0 +1,35 @@
+package org.logstash.bivalues;
+
+import org.jruby.Ruby;
+import org.jruby.RubySymbol;
+
+import java.io.ObjectStreamException;
+
+public class SymbolBiValue extends BiValueCommon<RubySymbol, String> implements BiValue<RubySymbol, String> {
+
+    public SymbolBiValue(RubySymbol rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public SymbolBiValue(String javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private SymbolBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubySymbol.newSymbol(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.asJavaString();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/TimeBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/TimeBiValue.java
new file mode 100644
index 00000000000..bcced16503a
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/TimeBiValue.java
@@ -0,0 +1,37 @@
+package org.logstash.bivalues;
+
+import org.joda.time.DateTime;
+import org.jruby.Ruby;
+import org.jruby.RubyTime;
+
+import java.io.ObjectStreamException;
+
+
+public class TimeBiValue extends BiValueCommon<RubyTime, DateTime> implements BiValue<RubyTime, DateTime> {
+
+    public TimeBiValue(RubyTime rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public TimeBiValue(DateTime javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private TimeBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyTime.newTime(runtime, javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getDateTime();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/org/logstash/bivalues/TimestampBiValue.java b/logstash-core-event-java/src/main/java/org/logstash/bivalues/TimestampBiValue.java
new file mode 100644
index 00000000000..b1d98ca9eee
--- /dev/null
+++ b/logstash-core-event-java/src/main/java/org/logstash/bivalues/TimestampBiValue.java
@@ -0,0 +1,36 @@
+package org.logstash.bivalues;
+
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.jruby.Ruby;
+
+import java.io.ObjectStreamException;
+
+public class TimestampBiValue extends BiValueCommon<RubyTimestamp, Timestamp> implements BiValue<RubyTimestamp, Timestamp> {
+
+    public TimestampBiValue(RubyTimestamp rubyValue) {
+        this.rubyValue = rubyValue;
+        javaValue = null;
+    }
+
+    public TimestampBiValue(Timestamp javaValue) {
+        this.javaValue = javaValue;
+        rubyValue = null;
+    }
+
+    private TimestampBiValue() {
+    }
+
+    protected void addRuby(Ruby runtime) {
+        rubyValue = RubyTimestamp.newRubyTimestamp(runtime, (Timestamp) javaValue);
+    }
+
+    protected void addJava() {
+        javaValue = rubyValue.getTimestamp();
+    }
+
+    // Called when object is to be serialized on a stream to allow the object to substitute a proxy for itself.
+    private Object writeReplace() throws ObjectStreamException {
+        return newProxy(this);
+    }
+}
diff --git a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java b/logstash-core-event-java/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
similarity index 82%
rename from logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
rename to logstash-core-event-java/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
index 1cb630d5a75..2edd25e6645 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyEventExtLibrary.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/ext/JrubyEventExtLibrary.java
@@ -1,37 +1,20 @@
-package com.logstash.ext;
-
-import com.logstash.Logger;
-import com.logstash.Event;
-import com.logstash.PathCache;
-import com.logstash.Javafier;
-import com.logstash.Timestamp;
-import com.logstash.Rubyfier;
-import com.logstash.Javafier;
-import org.jruby.Ruby;
-import org.jruby.RubyObject;
-import org.jruby.RubyClass;
-import org.jruby.RubyModule;
-import org.jruby.RubyString;
-import org.jruby.RubyHash;
-import org.jruby.RubyBoolean;
-import org.jruby.RubyArray;
-import org.jruby.RubyFloat;
-import org.jruby.RubyInteger;
+package org.logstash.ext;
+
+import org.logstash.*;
+import org.jruby.*;
 import org.jruby.anno.JRubyClass;
 import org.jruby.anno.JRubyMethod;
 import org.jruby.exceptions.RaiseException;
+import org.jruby.java.proxies.MapJavaProxy;
 import org.jruby.javasupport.JavaUtil;
 import org.jruby.runtime.Arity;
 import org.jruby.runtime.ObjectAllocator;
 import org.jruby.runtime.ThreadContext;
 import org.jruby.runtime.builtin.IRubyObject;
 import org.jruby.runtime.load.Library;
-import org.jruby.ext.bigdecimal.RubyBigDecimal;
+
 import java.io.IOException;
 import java.util.Map;
-import java.util.HashMap;
-import java.util.List;
-
 
 public class JrubyEventExtLibrary implements Library {
 
@@ -39,10 +22,12 @@ public class JrubyEventExtLibrary implements Library {
     private static RubyClass GENERATOR_ERROR = null;
     private static RubyClass LOGSTASH_ERROR = null;
 
+    @Override
     public void load(Ruby runtime, boolean wrap) throws IOException {
         RubyModule module = runtime.defineModule("LogStash");
 
         RubyClass clazz = runtime.defineClassUnder("Event", runtime.getObject(), new ObjectAllocator() {
+            @Override
             public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
                 return new RubyEvent(runtime, rubyClass);
             }
@@ -53,7 +38,6 @@ public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
         clazz.setConstant("TIMESTAMP", runtime.newString(Event.TIMESTAMP));
         clazz.setConstant("TIMESTAMP_FAILURE_TAG", runtime.newString(Event.TIMESTAMP_FAILURE_TAG));
         clazz.setConstant("TIMESTAMP_FAILURE_FIELD", runtime.newString(Event.TIMESTAMP_FAILURE_FIELD));
-        clazz.setConstant("DEFAULT_LOGGER", runtime.getModule("Cabin").getClass("Channel").callMethod("get", runtime.getModule("LogStash")));
         clazz.setConstant("VERSION", runtime.newString(Event.VERSION));
         clazz.setConstant("VERSION_ONE", runtime.newString(Event.VERSION_ONE));
         clazz.defineAnnotatedMethods(RubyEvent.class);
@@ -73,24 +57,9 @@ public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
         }
     }
 
-    public static class ProxyLogger implements Logger {
-        private RubyObject logger;
-
-        public ProxyLogger(RubyObject logger) {
-             this.logger = logger;
-        }
-
-        // TODO: (colin) complete implementation beyond warn when needed
-
-        public void warn(String message) {
-            logger.callMethod("warn", RubyString.newString(logger.getRuntime(), message));
-        }
-    }
-
     @JRubyClass(name = "Event", parent = "Object")
     public static class RubyEvent extends RubyObject {
         private Event event;
-        private static RubyObject logger;
 
         public RubyEvent(Ruby runtime, RubyClass klass) {
             super(runtime, klass);
@@ -127,12 +96,10 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
             if (data == null || data.isNil()) {
                 this.event = new Event();
             } else if (data instanceof RubyHash) {
-                HashMap<String, Object>  newObj = Javafier.deep((RubyHash) data);
-                this.event = new Event(newObj);
-            } else if (data instanceof Map) {
-                this.event = new Event((Map) data);
-            } else if (Map.class.isAssignableFrom(data.getJavaClass())) {
-                this.event = new Event((Map)data.toJava(Map.class));
+                this.event = new Event(ConvertedMap.newFromRubyHash((RubyHash) data));
+            } else if (data instanceof MapJavaProxy) {
+                Map<String, Object> m = (Map)((MapJavaProxy)data).getObject();
+                this.event = new Event(ConvertedMap.newFromMap(m));
             } else {
                 throw context.runtime.newTypeError("wrong argument type " + data.getMetaClass() + " (expected Hash)");
             }
@@ -140,14 +107,14 @@ public IRubyObject ruby_initialize(ThreadContext context, IRubyObject[] args)
             return context.nil;
         }
 
-        @JRubyMethod(name = "[]", required = 1)
+        @JRubyMethod(name = "get", required = 1)
         public IRubyObject ruby_get_field(ThreadContext context, RubyString reference)
         {
-            Object value = this.event.getField(reference.asJavaString());
+            Object value = this.event.getUnconvertedField(reference.asJavaString());
             return Rubyfier.deep(context.runtime, value);
         }
 
-        @JRubyMethod(name = "[]=", required = 2)
+        @JRubyMethod(name = "set", required = 2)
         public IRubyObject ruby_set_field(ThreadContext context, RubyString reference, IRubyObject value)
         {
             String r = reference.asJavaString();
@@ -158,7 +125,7 @@ public IRubyObject ruby_set_field(ThreadContext context, RubyString reference, I
                 }
                 this.event.setTimestamp(((JrubyTimestampExtLibrary.RubyTimestamp)value).getTimestamp());
             } else {
-                this.event.setField(r, Javafier.deep(value));
+                this.event.setField(r, Valuefier.convert(value));
             }
             return value;
         }
@@ -245,7 +212,7 @@ public IRubyObject ruby_to_s(ThreadContext context)
         @JRubyMethod(name = "to_hash")
         public IRubyObject ruby_to_hash(ThreadContext context) throws IOException
         {
-            return Rubyfier.deep(context.runtime, this.event.toMap());
+            return Rubyfier.deep(context.runtime, this.event.getData());
         }
 
         @JRubyMethod(name = "to_hash_with_metadata")
@@ -276,8 +243,8 @@ public IRubyObject ruby_to_json(ThreadContext context, IRubyObject[] args)
             }
         }
 
-        // @param value [String] the json string. A json object/map will convert to an array containing a single Event.
-        // and a json array will convert each element into individual Event
+        // @param value [String] the json string. A json object/map will newFromRubyArray to an array containing a single Event.
+        // and a json array will newFromRubyArray each element into individual Event
         // @return Array<Event> array of events
         @JRubyMethod(name = "from_json", required = 1, meta = true)
         public static IRubyObject ruby_from_json(ThreadContext context, IRubyObject recv, RubyString value)
@@ -312,8 +279,9 @@ public static IRubyObject ruby_validate_value(ThreadContext context, IRubyObject
         @JRubyMethod(name = "tag", required = 1)
         public IRubyObject ruby_tag(ThreadContext context, RubyString value)
         {
+            //TODO(guy) should these tags be BiValues?
             this.event.tag(((RubyString) value).asJavaString());
-            return context.runtime.getNil();
+            return context.nil;
         }
 
         @JRubyMethod(name = "timestamp")
@@ -330,14 +298,5 @@ public IRubyObject ruby_set_timestamp(ThreadContext context, IRubyObject value)
             this.event.setTimestamp(((JrubyTimestampExtLibrary.RubyTimestamp)value).getTimestamp());
             return value;
         }
-
-        // set a new logger for all Event instances
-        // there is no point in changing it at runtime for other reasons than in tests/specs.
-        @JRubyMethod(name = "logger=", required = 1, meta = true)
-        public static IRubyObject ruby_set_logger(ThreadContext context, IRubyObject recv, IRubyObject value)
-        {
-            Event.setLogger(new ProxyLogger((RubyObject)value));
-            return value;
-        }
     }
 }
diff --git a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java b/logstash-core-event-java/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
similarity index 95%
rename from logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java
rename to logstash-core-event-java/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
index 9748a815ccb..76fe2b582c3 100644
--- a/logstash-core-event-java/src/main/java/com/logstash/ext/JrubyTimestampExtLibrary.java
+++ b/logstash-core-event-java/src/main/java/org/logstash/ext/JrubyTimestampExtLibrary.java
@@ -1,6 +1,5 @@
-package com.logstash.ext;
+package org.logstash.ext;
 
-import com.logstash.*;
 import org.jruby.*;
 import org.jruby.anno.JRubyClass;
 import org.jruby.anno.JRubyMethod;
@@ -12,18 +11,27 @@
 import org.jruby.runtime.ThreadContext;
 import org.jruby.runtime.builtin.IRubyObject;
 import org.jruby.runtime.load.Library;
+import org.logstash.Timestamp;
 
 import java.io.IOException;
 
 public class JrubyTimestampExtLibrary implements Library {
+
+    private static final ObjectAllocator ALLOCATOR = new ObjectAllocator() {
+        public RubyTimestamp allocate(Ruby runtime, RubyClass rubyClass) {
+            return new RubyTimestamp(runtime, rubyClass);
+        }
+    };
+
     public void load(Ruby runtime, boolean wrap) throws IOException {
+        createTimestamp(runtime);
+    }
+
+    public static RubyClass createTimestamp(Ruby runtime) {
         RubyModule module = runtime.defineModule("LogStash");
-        RubyClass clazz = runtime.defineClassUnder("Timestamp", runtime.getObject(), new ObjectAllocator() {
-            public IRubyObject allocate(Ruby runtime, RubyClass rubyClass) {
-                return new RubyTimestamp(runtime, rubyClass);
-            }
-        }, module);
+        RubyClass clazz = runtime.defineClassUnder("Timestamp", runtime.getObject(), ALLOCATOR, module);
         clazz.defineAnnotatedMethods(RubyTimestamp.class);
+        return clazz;
     }
 
     @JRubyClass(name = "Timestamp", parent = "Object")
diff --git a/logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java b/logstash-core-event-java/src/test/java/org/logstash/AccessorsTest.java
similarity index 87%
rename from logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java
rename to logstash-core-event-java/src/test/java/org/logstash/AccessorsTest.java
index 61855abc34b..df0a56c5f09 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/AccessorsTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/AccessorsTest.java
@@ -1,6 +1,7 @@
-package com.logstash;
+package org.logstash;
 
 import org.junit.Test;
+
 import static org.junit.Assert.*;
 
 import java.util.ArrayList;
@@ -182,4 +183,28 @@ public void testNilInclude() throws Exception {
 
         assertEquals(accessors.includes("nilfield"), true);
     }
+
+    @Test
+    public void testInvalidPath() throws Exception {
+        Map data = new HashMap();
+        Accessors accessors = new Accessors(data);
+
+        assertEquals(accessors.set("[foo]", 1), 1);
+        assertEquals(accessors.get("[foo][bar]"), null);
+    }
+
+    @Test
+    public void testStaleTargetCache() throws Exception {
+        Map data = new HashMap();
+
+        Accessors accessors = new Accessors(data);
+
+        assertEquals(accessors.get("[foo][bar]"), null);
+        assertEquals(accessors.set("[foo][bar]", "baz"), "baz");
+        assertEquals(accessors.get("[foo][bar]"), "baz");
+
+        assertEquals(accessors.set("[foo]", "boom"), "boom");
+        assertEquals(accessors.get("[foo][bar]"), null);
+        assertEquals(accessors.get("[foo]"), "boom");
+    }
 }
diff --git a/logstash-core-event-java/src/test/java/com/logstash/EventTest.java b/logstash-core-event-java/src/test/java/org/logstash/EventTest.java
similarity index 90%
rename from logstash-core-event-java/src/test/java/com/logstash/EventTest.java
rename to logstash-core-event-java/src/test/java/org/logstash/EventTest.java
index 505a0593535..a2714761623 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/EventTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/EventTest.java
@@ -1,11 +1,17 @@
-package com.logstash;
+package org.logstash;
 
 import org.junit.Test;
 
 import java.io.IOException;
-import java.util.*;
-import static org.junit.Assert.*;
+import java.util.ArrayList;
+import java.util.Arrays;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
 import static net.javacrumbs.jsonunit.JsonAssert.assertJsonEquals;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
 
 public class EventTest {
     @Test
@@ -104,6 +110,15 @@ public void testClone() throws Exception {
         assertJsonEquals(f.toJson(), e.toJson());
     }
 
+    @Test
+    public void testToMap() throws Exception {
+        Event e = new Event();
+        Map original = e.getData();
+        Map clone = e.toMap();
+        assertFalse(original == clone);
+        assertEquals(original, clone);
+    }
+
     @Test
     public void testAppend() throws Exception {
         Map data1 = new HashMap();
@@ -116,7 +131,9 @@ public void testAppend() throws Exception {
         Event e2 = new Event(data2);
         e.append(e2);
 
-        assertEquals(Arrays.asList("original1", "original2"), e.getField("field1"));
+        assertEquals(2, ((List) e.getField("[field1]")).size());
+        assertEquals("original1", e.getField("[field1][0]"));
+        assertEquals("original2", e.getField("[field1][1]"));
     }
 
     @Test
diff --git a/logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java b/logstash-core-event-java/src/test/java/org/logstash/FieldReferenceTest.java
similarity index 91%
rename from logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java
rename to logstash-core-event-java/src/test/java/org/logstash/FieldReferenceTest.java
index 73f04e3a7c4..280975f3230 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/FieldReferenceTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/FieldReferenceTest.java
@@ -1,11 +1,7 @@
-package com.logstash;
+package org.logstash;
 
 import org.junit.Test;
 
-import java.util.ArrayList;
-import java.util.Arrays;
-import java.util.List;
-
 import static org.junit.Assert.*;
 
 public class FieldReferenceTest {
diff --git a/logstash-core-event-java/src/test/java/org/logstash/JavafierTest.java b/logstash-core-event-java/src/test/java/org/logstash/JavafierTest.java
new file mode 100644
index 00000000000..e198bf507c5
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/org/logstash/JavafierTest.java
@@ -0,0 +1,19 @@
+package org.logstash;
+
+import org.jruby.RubyBignum;
+import org.junit.Test;
+
+import java.math.BigInteger;
+import static org.junit.Assert.assertEquals;
+
+public class JavafierTest extends TestBase {
+
+    @Test
+    public void testRubyBignum() {
+        RubyBignum v = RubyBignum.newBignum(ruby, "-9223372036854776000");
+
+        Object result = Javafier.deep(v);
+        assertEquals(BigInteger.class, result.getClass());
+        assertEquals( "-9223372036854776000", result.toString());
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java b/logstash-core-event-java/src/test/java/org/logstash/KeyNodeTest.java
similarity index 98%
rename from logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java
rename to logstash-core-event-java/src/test/java/org/logstash/KeyNodeTest.java
index 23cb27ac997..ea5b69c8920 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/KeyNodeTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/KeyNodeTest.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 import org.junit.Test;
 
diff --git a/logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java b/logstash-core-event-java/src/test/java/org/logstash/RubyfierTest.java
similarity index 75%
rename from logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java
rename to logstash-core-event-java/src/test/java/org/logstash/RubyfierTest.java
index af8ecbf0c28..e5bba62148c 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/RubyfierTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/RubyfierTest.java
@@ -1,6 +1,11 @@
-package com.logstash;
-
-import org.jruby.*;
+package org.logstash;
+
+import org.jruby.RubyArray;
+import org.jruby.RubyBignum;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyFloat;
+import org.jruby.RubyHash;
+import org.jruby.RubyString;
 import org.jruby.ext.bigdecimal.RubyBigDecimal;
 import org.jruby.javasupport.JavaUtil;
 import org.jruby.runtime.builtin.IRubyObject;
@@ -8,18 +13,19 @@
 
 import java.lang.reflect.Method;
 import java.math.BigDecimal;
+import java.math.BigInteger;
 import java.util.ArrayList;
 import java.util.HashMap;
 import java.util.List;
 import java.util.Map;
 
-import static org.junit.Assert.*;
+import static org.junit.Assert.assertEquals;
 
-public class RubyfierTest {
+public class RubyfierTest extends TestBase {
 
     @Test
     public void testDeepWithString() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), "foo");
+        Object result = Rubyfier.deep(ruby, "foo");
         assertEquals(RubyString.class, result.getClass());
         assertEquals("foo", result.toString());
     }
@@ -30,14 +36,14 @@ public void testDeepMapWithString()
     {
         Map data = new HashMap();
         data.put("foo", "bar");
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash) Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyString.class, result.getClass());
         assertEquals("bar", result.toString());
@@ -50,16 +56,16 @@ public void testDeepListWithString()
         List data = new ArrayList();
         data.add("foo");
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
         assertEquals(RubyString.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals("foo", rubyArray.toJavaArray()[0].toString());
     }
 
     @Test
     public void testDeepWithInteger() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1);
+        Object result = Rubyfier.deep(ruby, 1);
         assertEquals(RubyFixnum.class, result.getClass());
         assertEquals(1L, ((RubyFixnum)result).getLongValue());
     }
@@ -70,14 +76,14 @@ public void testDeepMapWithInteger()
     {
         Map data = new HashMap();
         data.put("foo", 1);
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyFixnum.class, result.getClass());
         assertEquals(1L, ((RubyFixnum)result).getLongValue());
@@ -90,16 +96,16 @@ public void testDeepListWithInteger()
         List data = new ArrayList();
         data.add(1);
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
         assertEquals(RubyFixnum.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1L, ((RubyFixnum)rubyArray.toJavaArray()[0]).getLongValue());
     }
 
     @Test
     public void testDeepWithFloat() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1.0F);
+        Object result = Rubyfier.deep(ruby, 1.0F);
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
     }
@@ -110,14 +116,14 @@ public void testDeepMapWithFloat()
     {
         Map data = new HashMap();
         data.put("foo", 1.0F);
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
@@ -130,16 +136,16 @@ public void testDeepListWithFloat()
         List data = new ArrayList();
         data.add(1.0F);
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
         assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
 
     @Test
     public void testDeepWithDouble() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), 1.0D);
+        Object result = Rubyfier.deep(ruby, 1.0D);
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
     }
@@ -150,14 +156,14 @@ public void testDeepMapWithDouble()
     {
         Map data = new HashMap();
         data.put("foo", 1.0D);
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyFloat.class, result.getClass());
         assertEquals(1.0D, ((RubyFloat)result).getDoubleValue(), 0);
@@ -170,16 +176,16 @@ public void testDeepListWithDouble()
         List data = new ArrayList();
         data.add(1.0D);
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
         assertEquals(RubyFloat.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyFloat)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
 
     @Test
     public void testDeepWithBigDecimal() {
-        Object result = Rubyfier.deep(Ruby.getGlobalRuntime(), new BigDecimal(1));
+        Object result = Rubyfier.deep(ruby, new BigDecimal(1));
         assertEquals(RubyBigDecimal.class, result.getClass());
         assertEquals(1.0D, ((RubyBigDecimal)result).getDoubleValue(), 0);
     }
@@ -191,14 +197,14 @@ public void testDeepMapWithBigDecimal()
         Map data = new HashMap();
         data.put("foo", new BigDecimal(1));
 
-        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyHash rubyHash = ((RubyHash)Rubyfier.deep(ruby, data));
 
         // Hack to be able to retrieve the original, unconverted Ruby object from Map
         // it seems the only method providing this is internalGet but it is declared protected.
         // I know this is bad practice but I think this is practically acceptable.
         Method internalGet = RubyHash.class.getDeclaredMethod("internalGet", IRubyObject.class);
         internalGet.setAccessible(true);
-        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(Ruby.getGlobalRuntime(), "foo"));
+        Object result = internalGet.invoke(rubyHash, JavaUtil.convertJavaToUsableRubyObject(ruby, "foo"));
 
         assertEquals(RubyBigDecimal.class, result.getClass());
         assertEquals(1.0D, ((RubyBigDecimal)result).getDoubleValue(), 0);
@@ -211,10 +217,19 @@ public void testDeepListWithBigDecimal()
         List data = new ArrayList();
         data.add(new BigDecimal(1));
 
-        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(Ruby.getGlobalRuntime(), data));
+        RubyArray rubyArray = ((RubyArray)Rubyfier.deep(ruby, data));
 
-        // toJavaArray does not convert inner elemenst to Java types \o/
+        // toJavaArray does not newFromRubyArray inner elemenst to Java types \o/
         assertEquals(RubyBigDecimal.class, rubyArray.toJavaArray()[0].getClass());
         assertEquals(1.0D, ((RubyBigDecimal)rubyArray.toJavaArray()[0]).getDoubleValue(), 0);
     }
+
+
+    @Test
+    public void testDeepWithBigInteger() {
+        Object result = Rubyfier.deep(ruby, new BigInteger("1"));
+        assertEquals(RubyBignum.class, result.getClass());
+        assertEquals(1L, ((RubyBignum)result).getLongValue());
+    }
+
 }
diff --git a/logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java b/logstash-core-event-java/src/test/java/org/logstash/StringInterpolationTest.java
similarity index 99%
rename from logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java
rename to logstash-core-event-java/src/test/java/org/logstash/StringInterpolationTest.java
index 52d4563db4b..8a28ee64939 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/StringInterpolationTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/StringInterpolationTest.java
@@ -1,4 +1,4 @@
-package com.logstash;
+package org.logstash;
 
 
 import org.joda.time.DateTime;
diff --git a/logstash-core-event-java/src/test/java/org/logstash/TestBase.java b/logstash-core-event-java/src/test/java/org/logstash/TestBase.java
new file mode 100644
index 00000000000..9bbc5ff08d0
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/org/logstash/TestBase.java
@@ -0,0 +1,25 @@
+package org.logstash;
+
+import org.logstash.ext.JrubyTimestampExtLibrary;
+import org.jruby.CompatVersion;
+import org.jruby.Ruby;
+import org.jruby.RubyInstanceConfig;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.junit.Before;
+
+public abstract class TestBase {
+    private static boolean setupDone = false;
+    public static Ruby ruby;
+
+    @Before
+    public void setUp() throws Exception {
+        if (setupDone) return;
+
+        RubyInstanceConfig config_19 = new RubyInstanceConfig();
+        config_19.setCompatVersion(CompatVersion.RUBY1_9);
+        ruby = Ruby.newInstance(config_19);
+        RubyBigDecimal.createBigDecimal(ruby); // we need to do 'require "bigdecimal"'
+        JrubyTimestampExtLibrary.createTimestamp(ruby);
+        setupDone = true;
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java b/logstash-core-event-java/src/test/java/org/logstash/TimestampTest.java
similarity index 98%
rename from logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java
rename to logstash-core-event-java/src/test/java/org/logstash/TimestampTest.java
index 539fbe227cb..db698d43cee 100644
--- a/logstash-core-event-java/src/test/java/com/logstash/TimestampTest.java
+++ b/logstash-core-event-java/src/test/java/org/logstash/TimestampTest.java
@@ -1,8 +1,9 @@
-package com.logstash;
+package org.logstash;
 
 import org.joda.time.DateTime;
 import org.joda.time.DateTimeZone;
 import org.junit.Test;
+
 import static org.junit.Assert.*;
 
 public class TimestampTest {
diff --git a/logstash-core-event-java/src/test/java/org/logstash/ValuefierTest.java b/logstash-core-event-java/src/test/java/org/logstash/ValuefierTest.java
new file mode 100644
index 00000000000..c80460cd454
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/org/logstash/ValuefierTest.java
@@ -0,0 +1,113 @@
+package org.logstash;
+
+import org.logstash.bivalues.BiValue;
+import org.logstash.bivalues.BiValues;
+import org.logstash.bivalues.TimestampBiValue;
+import org.joda.time.DateTime;
+import org.jruby.RubyClass;
+import org.jruby.RubyMatchData;
+import org.jruby.RubyString;
+import org.jruby.RubyTime;
+import org.jruby.java.proxies.ArrayJavaProxy;
+import org.jruby.java.proxies.ConcreteJavaProxy;
+import org.jruby.java.proxies.MapJavaProxy;
+import org.jruby.javasupport.Java;
+import org.jruby.runtime.builtin.IRubyObject;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.ExpectedException;
+
+import java.util.ArrayList;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.List;
+import java.util.Map;
+
+import static junit.framework.TestCase.assertEquals;
+
+public class ValuefierTest extends TestBase {
+    @Test
+    public void testMapJavaProxy() {
+        Map<IRubyObject, IRubyObject> map = new HashMap<>();
+        map.put(RubyString.newString(ruby, "foo"), RubyString.newString(ruby, "bar"));
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, HashMap.class);
+        MapJavaProxy mjp = new MapJavaProxy(ruby, proxyClass);
+        mjp.setObject(map);
+
+        Object result = Valuefier.convert(mjp);
+        assertEquals(ConvertedMap.class, result.getClass());
+        ConvertedMap<String, Object> m = (ConvertedMap) result;
+        BiValue bv = BiValues.newBiValue("bar");
+        assertEquals(bv.javaValue(), ((BiValue) m.get("foo")).javaValue());
+    }
+
+    @Test
+    public void testArrayJavaProxy() {
+        IRubyObject[] array = new IRubyObject[]{RubyString.newString(ruby, "foo")};
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, String[].class);
+        ArrayJavaProxy ajp = new ArrayJavaProxy(ruby, proxyClass, array);
+
+        Object result = Valuefier.convert(ajp);
+        assertEquals(ConvertedList.class, result.getClass());
+        List<Object> a = (ConvertedList) result;
+        BiValue bv = BiValues.newBiValue("foo");
+        assertEquals(bv.javaValue(), ((BiValue) a.get(0)).javaValue());
+    }
+
+    @Test
+    public void testConcreteJavaProxy() {
+        List<IRubyObject> array = new ArrayList<>();
+        array.add(RubyString.newString(ruby, "foo"));
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, ArrayList.class);
+        ConcreteJavaProxy cjp = new ConcreteJavaProxy(ruby, proxyClass, array);
+        Object result = Valuefier.convert(cjp);
+        assertEquals(ConvertedList.class, result.getClass());
+        List<Object> a = (ConvertedList) result;
+        BiValue bv = BiValues.newBiValue("foo");
+        assertEquals(bv.javaValue(), ((BiValue) a.get(0)).javaValue());
+    }
+
+    @Test
+    public void testRubyTime() {
+        RubyTime ro = RubyTime.newTime(ruby, DateTime.now());
+        Object result = Valuefier.convert(ro);
+
+        assertEquals(TimestampBiValue.class, result.getClass());
+    }
+
+    @Test
+    public void testJodaDateTIme() {
+        DateTime jo = DateTime.now();
+        Object result = Valuefier.convert(jo);
+
+        assertEquals(TimestampBiValue.class, result.getClass());
+    }
+
+    @Rule
+    public ExpectedException exception = ExpectedException.none();
+
+    @Test
+    public void testUnhandledObject() {
+        RubyMatchData md = new RubyMatchData(ruby);
+        exception.expect(IllegalArgumentException.class);
+        exception.expectMessage("Missing Valuefier handling for full class name=org.jruby.RubyMatchData, simple name=RubyMatchData");
+        Valuefier.convert(md);
+    }
+
+    @Test
+    public void testUnhandledProxyObject() {
+        HashSet<Integer> hs = new HashSet<>();
+        hs.add(42);
+        RubyClass proxyClass = (RubyClass) Java.getProxyClass(ruby, HashSet.class);
+        ConcreteJavaProxy cjp = new ConcreteJavaProxy(ruby, proxyClass, hs);
+        BiValue result = (BiValue) Valuefier.convert(cjp);
+        assertEquals(hs, result.javaValue());
+    }
+
+    @Test
+    public void scratch() {
+        String[] parts = "foo/1_4".split("\\W|_");
+        int ord = Integer.valueOf(parts[1]);
+        assertEquals(ord, 1);
+    }
+}
diff --git a/logstash-core-event-java/src/test/java/org/logstash/bivalues/BiValueTest.java b/logstash-core-event-java/src/test/java/org/logstash/bivalues/BiValueTest.java
new file mode 100644
index 00000000000..c4a96e2905d
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/org/logstash/bivalues/BiValueTest.java
@@ -0,0 +1,215 @@
+package org.logstash.bivalues;
+
+import org.logstash.TestBase;
+import org.joda.time.DateTime;
+import org.jruby.RubyBignum;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.RubyNil;
+import org.jruby.RubyString;
+import org.jruby.RubySymbol;
+import org.jruby.RubyTime;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.junit.Test;
+
+import java.io.ByteArrayInputStream;
+import java.io.ByteArrayOutputStream;
+import java.io.ObjectInputStream;
+import java.io.ObjectOutputStream;
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertTrue;
+
+public class BiValueTest extends TestBase {
+    @Test
+    public void testStringBiValueFromRuby() {
+        String s = "foo bar baz";
+        StringBiValue subject = new StringBiValue(RubyString.newString(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testStringBiValueFromJava() {
+        RubyString v = RubyString.newString(ruby, "foo bar baz");
+        StringBiValue subject = new StringBiValue("foo bar baz");
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testSymbolBiValueFromRuby() {
+        String s = "foo";
+        SymbolBiValue subject = new SymbolBiValue(RubySymbol.newSymbol(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testLongBiValueFromRuby() {
+        Long s = 123456789L;
+        LongBiValue subject = new LongBiValue(RubyFixnum.newFixnum(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testLongBiValueFromJava() {
+        RubyInteger v = RubyFixnum.newFixnum(ruby, 123456789L);
+        LongBiValue subject = new LongBiValue(123456789L);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+
+    @Test
+    public void testIntegerBiValueFromRuby() {
+        int j = 123456789;
+        IntegerBiValue subject = new IntegerBiValue(RubyFixnum.newFixnum(ruby, j));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertTrue(j - subject.javaValue() == 0);
+    }
+
+    @Test
+    public void testIntegerBiValueFromJava() {
+        RubyInteger v = RubyFixnum.newFixnum(ruby, 123456789);
+        IntegerBiValue subject = new IntegerBiValue(123456789);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testBigDecimalBiValueFromRuby() {
+        BigDecimal s = BigDecimal.valueOf(12345.678D);
+        BigDecimalBiValue subject = new BigDecimalBiValue(new RubyBigDecimal(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testBigDecimalBiValueFromJava() {
+        RubyBigDecimal v = new RubyBigDecimal(ruby, new BigDecimal(12345.678D));
+        BigDecimalBiValue subject = new BigDecimalBiValue(new BigDecimal(12345.678D));
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testDoubleBiValueFromRuby() {
+        Double s = 12345.678D;
+        DoubleBiValue subject = new DoubleBiValue(RubyFloat.newFloat(ruby, 12345.678D));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testDoubleBiValueFromJava() {
+        RubyFloat v = RubyFloat.newFloat(ruby, 12345.678D);
+        DoubleBiValue subject = new DoubleBiValue(12345.678D);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testBooleanBiValueFromRuby() {
+        BooleanBiValue subject = new BooleanBiValue(RubyBoolean.newBoolean(ruby, true));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertTrue(subject.javaValue());
+    }
+
+    @Test
+    public void testBooleanBiValueFromJava() {
+        RubyBoolean v = RubyBoolean.newBoolean(ruby, true);
+        BooleanBiValue subject = new BooleanBiValue(true);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testNullBiValueFromRuby() {
+        NullBiValue subject = new NullBiValue((RubyNil) ruby.getNil());
+        assertTrue(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(null, subject.javaValue());
+    }
+
+    @Test
+    public void testNullBiValueFromJava() {
+        NullBiValue subject = NullBiValue.newNullBiValue();
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(ruby.getNil(), subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testTimeBiValueFromRuby() {
+        DateTime t = DateTime.now();
+        RubyTime now = RubyTime.newTime(ruby, t);
+        TimeBiValue subject = new TimeBiValue(now);
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(t, subject.javaValue());
+    }
+
+    @Test
+    public void testTimeBiValueFromJava() {
+        DateTime t = DateTime.now();
+        TimeBiValue subject = new TimeBiValue(t);
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(RubyTime.newTime(ruby, t), subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testBigIntegerBiValueFromRuby() {
+        BigInteger s = BigInteger.valueOf(12345678L);
+        BigIntegerBiValue subject = new BigIntegerBiValue(new RubyBignum(ruby, s));
+        assertTrue(subject.hasRubyValue());
+        assertFalse(subject.hasJavaValue());
+        assertEquals(s, subject.javaValue());
+    }
+
+    @Test
+    public void testBigIntegerBiValueFromJava() {
+        RubyBignum v = new RubyBignum(ruby, BigInteger.valueOf(12345678L));
+        BigIntegerBiValue subject = new BigIntegerBiValue(BigInteger.valueOf(12345678L));
+        assertFalse(subject.hasRubyValue());
+        assertTrue(subject.hasJavaValue());
+        assertEquals(v, subject.rubyValue(ruby));
+    }
+
+    @Test
+    public void testSerialization() throws Exception {
+        RubyBignum v = RubyBignum.newBignum(ruby, "-9223372036854776000");
+        BiValue original = BiValues.newBiValue(v);
+
+        ByteArrayOutputStream baos = new ByteArrayOutputStream();
+        ObjectOutputStream oos = new ObjectOutputStream(baos);
+        oos.writeObject(original);
+        oos.close();
+
+        ByteArrayInputStream bais = new ByteArrayInputStream(baos.toByteArray());
+        ObjectInputStream ois = new ObjectInputStream(bais);
+        BiValue copy = (BiValue) ois.readObject();
+        assertEquals(original, copy);
+        assertFalse(copy.hasRubyValue());
+    }
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/test/java/org/logstash/bivalues/BiValuesTest.java b/logstash-core-event-java/src/test/java/org/logstash/bivalues/BiValuesTest.java
new file mode 100644
index 00000000000..267da0567a9
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/org/logstash/bivalues/BiValuesTest.java
@@ -0,0 +1,280 @@
+package org.logstash.bivalues;
+
+import org.logstash.TestBase;
+import org.logstash.Timestamp;
+import org.logstash.ext.JrubyTimestampExtLibrary.RubyTimestamp;
+import org.jruby.RubyBignum;
+import org.jruby.RubyBoolean;
+import org.jruby.RubyFixnum;
+import org.jruby.RubyFloat;
+import org.jruby.RubyInteger;
+import org.jruby.RubyNil;
+import org.jruby.RubyString;
+import org.jruby.RubySymbol;
+import org.jruby.ext.bigdecimal.RubyBigDecimal;
+import org.jruby.javasupport.JavaUtil;
+import org.junit.Test;
+
+import java.math.BigDecimal;
+import java.math.BigInteger;
+
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertFalse;
+import static org.junit.Assert.assertNull;
+import static org.junit.Assert.assertTrue;
+
+public class BiValuesTest extends TestBase {
+
+    @Test
+    public void testBiValuesStringRuby() {
+        String js = "double";
+        RubyString rs = RubyString.newUnicodeString(ruby, js);
+        BiValue subject = BiValues.newBiValue(rs);
+
+        assertEquals(rs, subject.rubyValueUnconverted());
+        assertEquals(rs.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(js, subject.javaValue());
+        assertEquals(String.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesStringJava() {
+        String js = "double";
+        RubyString rs = RubyString.newUnicodeString(ruby, js);
+        BiValue subject = BiValues.newBiValue(js);
+
+        assertEquals(js, subject.javaValue());
+        assertEquals(String.class, subject.javaValue().getClass());
+        assertEquals(rs, subject.rubyValue(ruby));
+        assertEquals(rs.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesSymbolRuby() {
+        String js = "double";
+        RubySymbol rs = RubySymbol.newSymbol(ruby, js);
+        BiValue subject = BiValues.newBiValue(rs);
+
+        assertEquals(rs, subject.rubyValueUnconverted());
+        assertEquals(rs.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(js, subject.javaValue());
+        assertEquals(String.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesLongRuby() {
+        long jo = 1234567L;
+        RubyInteger ro = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Long.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesLongJava() {
+        long jo = 1234567L;
+        RubyInteger ro = (RubyInteger) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Long.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesFloatRuby() {
+        double jo = 1234.567D;
+        RubyFloat ro = (RubyFloat) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Double.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesFloatJava() {
+        double jo = 1234.567D;
+        RubyFloat ro = (RubyFloat) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Double.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBigDecimalRuby() {
+        BigDecimal jo = BigDecimal.valueOf(12345.678D);
+        RubyBigDecimal ro = new RubyBigDecimal(ruby, ruby.getClass("BigDecimal"), jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigDecimal.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBigDecimalJava() {
+        BigDecimal jo = BigDecimal.valueOf(12345.678D);
+        RubyBigDecimal ro = new RubyBigDecimal(ruby, ruby.getClass("BigDecimal"), jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigDecimal.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanRubyTrue() {
+        boolean jo = true;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertTrue(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanRubyFalse() {
+        boolean jo = false;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertFalse(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanJavaTrue() {
+        boolean jo = true;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(jo);
+
+        assertTrue(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBooleanJavaFalse() {
+        boolean jo = false;
+        RubyBoolean ro = (RubyBoolean) JavaUtil.convertJavaToUsableRubyObject(ruby, jo);
+        BiValue<RubyBoolean, Boolean> subject = BiValues.newBiValue(jo);
+
+        assertFalse(subject.javaValue());
+        assertEquals(Boolean.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesTimestampRuby() {
+        Timestamp jo = new Timestamp("2014-09-23T00:00:00-0800");
+        RubyTimestamp ro = RubyTimestamp.newRubyTimestamp(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Timestamp.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesTimestampJava() {
+        Timestamp jo = new Timestamp("2014-09-23T00:00:00-0800");
+        RubyTimestamp ro = RubyTimestamp.newRubyTimestamp(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Timestamp.class, subject.javaValue().getClass());
+        assertEquals(ro.toString(), subject.rubyValue(ruby).toString());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesNilRuby() {
+        RubyNil ro = (RubyNil) ruby.getNil();
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertNull(subject.javaValue());
+    }
+
+    @Test
+    public void testBiValuesNullJava() {
+        RubyNil ro = (RubyNil) ruby.getNil();
+        BiValue subject = BiValues.newBiValue(null);
+
+        assertNull(subject.javaValue());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    @Test
+    public void testBiValuesBigIntegerRuby() {
+        BigInteger jo = BigInteger.valueOf(12345678L);
+        RubyBignum ro = new RubyBignum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(BigIntegerBiValue.class, subject.getClass());
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigInteger.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesBigIntegerJava() {
+        BigInteger jo = BigInteger.valueOf(12345678L);
+        RubyBignum ro = new RubyBignum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(jo, subject.javaValue());
+        assertEquals(BigInteger.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+    // NOTE: testBiValuesIntegerRuby will map to LongBiValue
+    @Test
+    public void testBiValuesIntegerRuby() {
+        int jo = 12345678;
+        RubyInteger ro = RubyFixnum.newFixnum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(ro);
+
+        assertEquals(LongBiValue.class, subject.getClass());
+        assertEquals(ro, subject.rubyValueUnconverted());
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+        assertEquals(12345678L, subject.javaValue());
+        assertEquals(Long.class, subject.javaValue().getClass());
+    }
+
+    @Test
+    public void testBiValuesIntegerJava() {
+        int jo = 12345678;
+        RubyInteger ro = RubyFixnum.newFixnum(ruby, jo);
+        BiValue subject = BiValues.newBiValue(jo);
+
+        assertEquals(IntegerBiValue.class, subject.getClass());
+        assertEquals(jo, subject.javaValue());
+        assertEquals(Integer.class, subject.javaValue().getClass());
+        assertEquals(ro, subject.rubyValue(ruby));
+        assertEquals(ro.getClass(), subject.rubyValue(ruby).getClass());
+    }
+
+}
\ No newline at end of file
diff --git a/logstash-core-event-java/src/test/java/org/logstash/bivalues/SomeJavaObject.java b/logstash-core-event-java/src/test/java/org/logstash/bivalues/SomeJavaObject.java
new file mode 100644
index 00000000000..1ebe27c4818
--- /dev/null
+++ b/logstash-core-event-java/src/test/java/org/logstash/bivalues/SomeJavaObject.java
@@ -0,0 +1,29 @@
+package org.logstash.bivalues;
+
+public class SomeJavaObject<T> {
+    private T value;
+
+    public T getValue() {
+        return value;
+    }
+
+    public SomeJavaObject(T value) {
+        this.value = value;
+    }
+
+    @Override
+    public boolean equals(Object o) {
+        if (this == o) return true;
+        if (!(o instanceof SomeJavaObject)) return false;
+
+        SomeJavaObject<?> that = (SomeJavaObject<?>) o;
+
+        return value != null ? value.equals(that.getValue()) : that.value == null;
+
+    }
+
+    @Override
+    public int hashCode() {
+        return value != null ? value.hashCode() : 0;
+    }
+}
diff --git a/logstash-core-event/lib/logstash-core-event/version.rb b/logstash-core-event/lib/logstash-core-event/version.rb
index 18e991d6b0c..28a996eec69 100644
--- a/logstash-core-event/lib/logstash-core-event/version.rb
+++ b/logstash-core-event/lib/logstash-core-event/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_EVENT_VERSION = "3.0.0.dev"
+LOGSTASH_CORE_EVENT_VERSION = "5.1.0"
diff --git a/logstash-core-event/lib/logstash/event.rb b/logstash-core-event/lib/logstash/event.rb
index b1eb9d46cdb..de0cf5fc00d 100644
--- a/logstash-core-event/lib/logstash/event.rb
+++ b/logstash-core-event/lib/logstash/event.rb
@@ -13,14 +13,27 @@
 # in the future it might be necessary to refactor using like a BaseEvent
 # class to have a common interface for all pileline events to support
 # eventual queueing persistence for example, TBD.
-class LogStash::ShutdownEvent; end
-class LogStash::FlushEvent; end
 
 module LogStash
-  FLUSH = LogStash::FlushEvent.new
+  class SignalEvent
+    def flush?; raise "abstract method"; end;
+    def shutdown?; raise "abstract method"; end;
+  end
+
+  class ShutdownEvent < SignalEvent
+    def flush?; false; end;
+    def shutdown?; true; end;
+  end
+
+  class FlushEvent < SignalEvent
+    def flush?; true; end;
+    def shutdown?; false; end;
+  end
+
+  FLUSH = FlushEvent.new
 
   # LogStash::SHUTDOWN is used by plugins
-  SHUTDOWN = LogStash::ShutdownEvent.new
+  SHUTDOWN = ShutdownEvent.new
 end
 
 # the logstash event object.
@@ -54,6 +67,7 @@ class DeprecatedMethod < StandardError; end
   VERSION_ONE = "1"
   TIMESTAMP_FAILURE_TAG = "_timestampparsefailure"
   TIMESTAMP_FAILURE_FIELD = "_@timestamp"
+  TAGS = "tags".freeze
 
   METADATA = "@metadata".freeze
   METADATA_BRACKETS = "[#{METADATA}]".freeze
@@ -113,7 +127,7 @@ def timestamp=(val)
     @data[TIMESTAMP] = val
   end
 
-  def [](fieldref)
+  def get(fieldref)
     if fieldref.start_with?(METADATA_BRACKETS)
       @metadata_accessors.get(fieldref[METADATA_BRACKETS.length .. -1])
     elsif fieldref == METADATA
@@ -123,7 +137,7 @@ def [](fieldref)
     end
   end
 
-  def []=(fieldref, value)
+  def set(fieldref, value)
     if fieldref == TIMESTAMP && !value.is_a?(LogStash::Timestamp)
       raise TypeError, "The field '@timestamp' must be a (LogStash::Timestamp, not a #{value.class} (#{value})"
     end
@@ -201,8 +215,9 @@ def sprintf(format)
 
   def tag(value)
     # Generalize this method for more usability
-    self["tags"] ||= []
-    self["tags"] << value unless self["tags"].include?(value)
+    tags = @accessors.get(TAGS) || []
+    tags << value unless tags.include?(value)
+    @accessors.set(TAGS, tags)
   end
 
   def to_hash_with_metadata
@@ -269,9 +284,8 @@ def init_timestamp(o)
       logger.warn("Error parsing #{TIMESTAMP} string, setting current time to #{TIMESTAMP}, original in #{TIMESTAMP_FAILURE_FIELD} field", :value => o.inspect, :exception => e.message)
     end
 
-    @data["tags"] ||= []
-    @data["tags"] << TIMESTAMP_FAILURE_TAG unless @data["tags"].include?(TIMESTAMP_FAILURE_TAG)
-    @data[TIMESTAMP_FAILURE_FIELD] = o
+    tag(TIMESTAMP_FAILURE_TAG)
+    @accessors.set(TIMESTAMP_FAILURE_FIELD, o)
 
     LogStash::Timestamp.now
   end
diff --git a/logstash-core-event/lib/logstash/string_interpolation.rb b/logstash-core-event/lib/logstash/string_interpolation.rb
index 13044e4c005..aaa54981165 100644
--- a/logstash-core-event/lib/logstash/string_interpolation.rb
+++ b/logstash-core-event/lib/logstash/string_interpolation.rb
@@ -115,7 +115,7 @@ def initialize(key)
     end
 
     def evaluate(event)
-      value = event[@key]
+      value = event.get(@key)
 
       case value
       when nil
diff --git a/logstash-core-event/lib/logstash/timestamp.rb b/logstash-core-event/lib/logstash/timestamp.rb
index fb75c5f2538..ab6b6edb3bc 100644
--- a/logstash-core-event/lib/logstash/timestamp.rb
+++ b/logstash-core-event/lib/logstash/timestamp.rb
@@ -24,7 +24,13 @@ def initialize(time = Time.new)
     end
 
     def self.at(*args)
-      Timestamp.new(::Time.at(*args))
+      epoch = args.first
+      if epoch.is_a?(BigDecimal)
+        # bug in JRuby prevents correcly parsing a BigDecimal fractional part, see https://github.com/elastic/logstash/issues/4565
+        Timestamp.new(::Time.at(epoch.to_i, epoch.frac.to_f * 1000000))
+      else
+        Timestamp.new(::Time.at(*args))
+      end
     end
 
     def self.parse(*args)
diff --git a/logstash-core-event/lib/logstash/util/accessors.rb b/logstash-core-event/lib/logstash/util/accessors.rb
index 01c16910855..23248f2c3ea 100644
--- a/logstash-core-event/lib/logstash/util/accessors.rb
+++ b/logstash-core-event/lib/logstash/util/accessors.rb
@@ -95,7 +95,14 @@ def lookup(field_reference)
     # @param field_reference [String] the field referece
     # @return [[Object, String]] the  [target, key] tuple associated with this field reference
     def lookup_or_create(field_reference)
-      @lut[field_reference] ||= find_or_create_target(field_reference)
+      # flush the @lut to prevent stale cached fieldref which may point to an old target
+      # which was overwritten with a new value. for example, if "[a][b]" is cached and we
+      # set a new value for "[a]" then reading again "[a][b]" would point in a stale target.
+      # flushing the complete @lut is suboptimal, but a hierarchical lut would be required
+      # to be able to invalidate fieldrefs from a common root.
+      # see https://github.com/elastic/logstash/pull/5132
+      @lut.clear
+      @lut[field_reference] = find_or_create_target(field_reference)
     end
 
     # find the target container object in store for this field reference
diff --git a/logstash-core-event/spec/logstash/event_spec.rb b/logstash-core-event/spec/logstash/event_spec.rb
index b0e4985bc07..3cdd33d12d8 100644
--- a/logstash-core-event/spec/logstash/event_spec.rb
+++ b/logstash-core-event/spec/logstash/event_spec.rb
@@ -8,68 +8,68 @@
   shared_examples "all event tests" do
     context "[]=" do
       it "should raise an exception if you attempt to set @timestamp to a value type other than a Time object" do
-        expect{subject["@timestamp"] = "crash!"}.to raise_error(TypeError)
+        expect{subject.set("@timestamp", "crash!")}.to raise_error(TypeError)
       end
 
       it "should assign simple fields" do
-        expect(subject["foo"]).to be_nil
-        expect(subject["foo"] = "bar").to eq("bar")
-        expect(subject["foo"]).to eq("bar")
+        expect(subject.get("foo")).to be_nil
+        expect(subject.set("foo", "bar")).to eq("bar")
+        expect(subject.get("foo")).to eq("bar")
       end
 
       it "should overwrite simple fields" do
-        expect(subject["foo"]).to be_nil
-        expect(subject["foo"] = "bar").to eq("bar")
-        expect(subject["foo"]).to eq("bar")
+        expect(subject.get("foo")).to be_nil
+        expect(subject.set("foo", "bar")).to eq("bar")
+        expect(subject.get("foo")).to eq("bar")
 
-        expect(subject["foo"] = "baz").to eq("baz")
-        expect(subject["foo"]).to eq("baz")
+        expect(subject.set("foo", "baz")).to eq("baz")
+        expect(subject.get("foo")).to eq("baz")
       end
 
       it "should assign deep fields" do
-        expect(subject["[foo][bar]"]).to be_nil
-        expect(subject["[foo][bar]"] = "baz").to eq("baz")
-        expect(subject["[foo][bar]"]).to eq("baz")
+        expect(subject.get("[foo][bar]")).to be_nil
+        expect(subject.set("[foo][bar]", "baz")).to eq("baz")
+        expect(subject.get("[foo][bar]")).to eq("baz")
       end
 
       it "should overwrite deep fields" do
-        expect(subject["[foo][bar]"]).to be_nil
-        expect(subject["[foo][bar]"] = "baz").to eq("baz")
-        expect(subject["[foo][bar]"]).to eq("baz")
+        expect(subject.get("[foo][bar]")).to be_nil
+        expect(subject.set("[foo][bar]", "baz")).to eq("baz")
+        expect(subject.get("[foo][bar]")).to eq("baz")
 
-        expect(subject["[foo][bar]"] = "zab").to eq("zab")
-        expect(subject["[foo][bar]"]).to eq("zab")
+        expect(subject.set("[foo][bar]", "zab")).to eq("zab")
+        expect(subject.get("[foo][bar]")).to eq("zab")
       end
 
       it "allow to set the @metadata key to a hash" do
-        subject["@metadata"] = { "action" => "index" }
-        expect(subject["[@metadata][action]"]).to eq("index")
+        subject.set("@metadata", { "action" => "index" })
+        expect(subject.get("[@metadata][action]")).to eq("index")
       end
 
       it "should add key when setting nil value" do
-        subject["[baz]"] = nil
+        subject.set("[baz]", nil)
         expect(subject.to_hash).to include("baz" => nil)
       end
 
       it "should set nil element within existing array value" do
-        subject["[foo]"] = ["bar", "baz"]
+        subject.set("[foo]", ["bar", "baz"])
 
-        expect(subject["[foo][0]"] = nil).to eq(nil)
-        expect(subject["[foo]"]).to eq([nil, "baz"])
+        expect(subject.set("[foo][0]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil, "baz"])
       end
 
       it "should set nil in first element within empty array" do
-        subject["[foo]"] = []
+        subject.set("[foo]", [])
 
-        expect(subject["[foo][0]"] = nil).to eq(nil)
-        expect(subject["[foo]"]).to eq([nil])
+        expect(subject.set("[foo][0]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil])
       end
 
       it "should set nil in second element within empty array" do
-        subject["[foo]"] = []
+        subject.set("[foo]", [])
 
-        expect(subject["[foo][1]"] = nil).to eq(nil)
-        expect(subject["[foo]"]).to eq([nil, nil])
+        expect(subject.set("[foo][1]", nil)).to eq(nil)
+        expect(subject.get("[foo]")).to eq([nil, nil])
       end
     end
 
@@ -79,7 +79,7 @@
         event = LogStash::Event.new({ "reference" => data })
         LogStash::Util::Decorators.add_fields({"reference_test" => "%{reference}"}, event, "dummy-plugin")
         data.downcase!
-        expect(event["reference_test"]).not_to eq(data)
+        expect(event.get("reference_test")).not_to eq(data)
       end
 
       it "should not return a Fixnum reference" do
@@ -87,7 +87,7 @@
         event = LogStash::Event.new({ "reference" => data })
         LogStash::Util::Decorators.add_fields({"reference_test" => "%{reference}"}, event, "dummy-plugin")
         data += 41
-        expect(event["reference_test"]).to eq("1")
+        expect(event.get("reference_test")).to eq("1")
       end
 
       it "should report a unix timestamp for %{+%s}" do
@@ -124,7 +124,7 @@
 
       it "should report fields with %{field} syntax" do
         expect(subject.sprintf("%{type}")).to eq("sprintf")
-        expect(subject.sprintf("%{message}")).to eq(subject["message"])
+        expect(subject.sprintf("%{message}")).to eq(subject.get("message"))
       end
 
       it "should print deep fields" do
@@ -153,35 +153,35 @@
       end
 
       it "should render nil array values as leading empty string" do
-        expect(subject["foo"] = [nil, "baz"]).to eq([nil, "baz"])
+        expect(subject.set("foo", [nil, "baz"])).to eq([nil, "baz"])
 
-        expect(subject["[foo][0]"]).to be_nil
-        expect(subject["[foo][1]"]).to eq("baz")
+        expect(subject.get("[foo][0]")).to be_nil
+        expect(subject.get("[foo][1]")).to eq("baz")
 
         expect(subject.sprintf("%{[foo]}")).to eq(",baz")
       end
 
       it "should render nil array values as middle empty string" do
-        expect(subject["foo"] = ["bar", nil, "baz"]).to eq(["bar", nil, "baz"])
+        expect(subject.set("foo", ["bar", nil, "baz"])).to eq(["bar", nil, "baz"])
 
-        expect(subject["[foo][0]"]).to eq("bar")
-        expect(subject["[foo][1]"]).to be_nil
-        expect(subject["[foo][2]"]).to eq("baz")
+        expect(subject.get("[foo][0]")).to eq("bar")
+        expect(subject.get("[foo][1]")).to be_nil
+        expect(subject.get("[foo][2]")).to eq("baz")
 
         expect(subject.sprintf("%{[foo]}")).to eq("bar,,baz")
       end
 
      it "should render nil array values as trailing empty string" do
-        expect(subject["foo"] = ["bar", nil]).to eq(["bar", nil])
+        expect(subject.set("foo", ["bar", nil])).to eq(["bar", nil])
 
-        expect(subject["[foo][0]"]).to eq("bar")
-        expect(subject["[foo][1]"]).to be_nil
+        expect(subject.get("[foo][0]")).to eq("bar")
+        expect(subject.get("[foo][1]")).to be_nil
 
         expect(subject.sprintf("%{[foo]}")).to eq("bar,")
      end
 
       it "should render deep arrays with nil value" do
-        subject["[foo]"] = [[12, nil], 56]
+        subject.set("[foo]", [[12, nil], 56])
         expect(subject.sprintf("%{[foo]}")).to eq("12,,56")
       end
 
@@ -198,18 +198,18 @@
 
     context "#[]" do
       it "should fetch data" do
-        expect(subject["type"]).to eq("sprintf")
+        expect(subject.get("type")).to eq("sprintf")
       end
       it "should fetch fields" do
-        expect(subject["a"]).to eq("b")
-        expect(subject['c']['d']).to eq("f")
+        expect(subject.get("a")).to eq("b")
+        expect(subject.get('c')['d']).to eq("f")
       end
       it "should fetch deep fields" do
-        expect(subject["[j][k1]"]).to eq("v")
-        expect(subject["[c][d]"]).to eq("f")
-        expect(subject['[f][g][h]']).to eq("i")
-        expect(subject['[j][k3][4]']).to eq("m")
-        expect(subject['[j][5]']).to eq(7)
+        expect(subject.get("[j][k1]")).to eq("v")
+        expect(subject.get("[c][d]")).to eq("f")
+        expect(subject.get('[f][g][h]')).to eq("i")
+        expect(subject.get('[j][k3][4]')).to eq("m")
+        expect(subject.get('[j][5]')).to eq(7)
 
       end
 
@@ -217,7 +217,7 @@
         count = 1000000
         2.times do
           start = Time.now
-          count.times { subject["[j][k1]"] }
+          count.times { subject.get("[j][k1]") }
           duration = Time.now - start
           puts "event #[] rate: #{"%02.0f/sec" % (count / duration)}, elapsed: #{duration}s"
         end
@@ -263,11 +263,11 @@
         )
         subject.overwrite(new_event)
 
-        expect(subject["message"]).to eq("foo bar")
-        expect(subject["type"]).to eq("new")
+        expect(subject.get("message")).to eq("foo bar")
+        expect(subject.get("type")).to eq("new")
 
         ["tags", "source", "a", "c", "f", "j"].each do |field|
-          expect(subject[field]).to be_nil
+          expect(subject.get(field)).to be_nil
         end
       end
     end
@@ -275,7 +275,7 @@
     context "#append" do
       it "should append strings to an array" do
         subject.append(LogStash::Event.new("message" => "another thing"))
-        expect(subject["message"]).to eq([ "hello world", "another thing" ])
+        expect(subject.get("message")).to eq([ "hello world", "another thing" ])
       end
 
       it "should concatenate tags" do
@@ -283,54 +283,54 @@
         # added to_a for when array is a Java Collection when produced from json input
         # TODO: we have to find a better way to handle this in tests. maybe override
         # rspec eq or == to do an explicit to_a when comparing arrays?
-        expect(subject["tags"].to_a).to eq([ "tag1", "tag2" ])
+        expect(subject.get("tags").to_a).to eq([ "tag1", "tag2" ])
       end
 
       context "when event field is nil" do
         it "should add single value as string" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq("append1")
+          expect(subject.get("field1")).to eq("append1")
         end
         it "should add multi values as array" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","append2" ]}))
-          expect(subject[ "field1" ]).to eq([ "append1","append2" ])
+          expect(subject.get("field1")).to eq([ "append1","append2" ])
         end
       end
 
       context "when event field is a string" do
-        before { subject[ "field1" ] = "original1" }
+        before { subject.set("field1", "original1") }
 
         it "should append string to values, if different from current" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
         it "should not change value, if appended value is equal current" do
           subject.append(LogStash::Event.new({"field1" => "original1"}))
-          expect(subject[ "field1" ]).to eq("original1")
+          expect(subject.get("field1")).to eq("original1")
         end
         it "should concatenate values in an array" do
           subject.append(LogStash::Event.new({"field1" => [ "append1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
         it "should join array, removing duplicates" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "append1" ])
         end
       end
       context "when event field is an array" do
-        before { subject[ "field1" ] = [ "original1", "original2" ] }
+        before { subject.set("field1", [ "original1", "original2" ] )}
 
         it "should append string values to array, if not present in array" do
           subject.append(LogStash::Event.new({"field1" => "append1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2", "append1" ])
         end
         it "should not append string values, if the array already contains it" do
           subject.append(LogStash::Event.new({"field1" => "original1"}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2" ])
         end
         it "should join array, removing duplicates" do
           subject.append(LogStash::Event.new({"field1" => [ "append1","original1" ]}))
-          expect(subject[ "field1" ]).to eq([ "original1", "original2", "append1" ])
+          expect(subject.get("field1")).to eq([ "original1", "original2", "append1" ])
         end
       end
 
@@ -342,7 +342,7 @@
 
       data = { "@timestamp" => "2013-12-21T07:25:06.605Z" }
       event = LogStash::Event.new(data)
-      expect(event["@timestamp"]).to be_a(LogStash::Timestamp)
+      expect(event.get("@timestamp")).to be_a(LogStash::Timestamp)
 
       duration = 0
       [warmup, count].each do |i|
@@ -391,12 +391,6 @@
     end
 
     context "timestamp initialization" do
-      let(:logger_mock) { double("logger") }
-
-      after(:each) do
-        LogStash::Event.logger = LogStash::Event::DEFAULT_LOGGER
-      end
-
       it "should coerce timestamp" do
         t = Time.iso8601("2014-06-12T00:12:17.114Z")
         expect(LogStash::Event.new("@timestamp" => t).timestamp.to_i).to eq(t.to_i)
@@ -411,20 +405,16 @@
       it "should tag for invalid value" do
         event = LogStash::Event.new("@timestamp" => "foo")
         expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq("foo")
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
 
         event = LogStash::Event.new("@timestamp" => 666)
         expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq(666)
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq(666)
       end
 
       it "should warn for invalid value" do
-        LogStash::Event.logger = logger_mock
-
-        expect(logger_mock).to receive(:warn).twice
-
         LogStash::Event.new("@timestamp" => :foo)
         LogStash::Event.new("@timestamp" => 666)
       end
@@ -432,14 +422,11 @@
       it "should tag for invalid string format" do
         event = LogStash::Event.new("@timestamp" => "foo")
         expect(event.timestamp.to_i).to be_within(1).of Time.now.to_i
-        expect(event["tags"]).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
-        expect(event[LogStash::Event::TIMESTAMP_FAILURE_FIELD]).to eq("foo")
+        expect(event.get("tags")).to eq([LogStash::Event::TIMESTAMP_FAILURE_TAG])
+        expect(event.get(LogStash::Event::TIMESTAMP_FAILURE_FIELD)).to eq("foo")
       end
 
       it "should warn for invalid string format" do
-        LogStash::Event.logger = logger_mock
-
-        expect(logger_mock).to receive(:warn)
         LogStash::Event.new("@timestamp" => "foo")
       end
     end
@@ -478,7 +465,7 @@
         end
 
         it "should still allow normal field access" do
-          expect(subject["hello"]).to eq("world")
+          expect(subject.get("hello")).to eq("world")
         end
       end
 
@@ -491,15 +478,15 @@
           expect(fieldref).to start_with("[@metadata]")
 
           # Set it.
-          subject[fieldref] = value
+          subject.set(fieldref, value)
         end
 
         it "should still allow normal field access" do
-          expect(subject["normal"]).to eq("normal")
+          expect(subject.get("normal")).to eq("normal")
         end
 
         it "should allow getting" do
-          expect(subject[fieldref]).to eq(value)
+          expect(subject.get(fieldref)).to eq(value)
         end
 
         it "should be hidden from .to_json" do
@@ -522,10 +509,10 @@
       context "with no metadata" do
         subject { LogStash::Event.new("foo" => "bar") }
         it "should have no metadata" do
-          expect(subject["@metadata"]).to be_empty
+          expect(subject.get("@metadata")).to be_empty
         end
         it "should still allow normal field access" do
-          expect(subject["foo"]).to eq("bar")
+          expect(subject.get("foo")).to eq("bar")
         end
 
         it "should not include the @metadata key" do
@@ -535,12 +522,27 @@
     end
 
     context "signal events" do
-      it "should define the shutdown event" do
+      it "should define the shutdown and flush event constants" do
         # the SHUTDOWN and FLUSH constants are part of the plugin API contract
         # if they are changed, all plugins must be updated
         expect(LogStash::SHUTDOWN).to be_a(LogStash::ShutdownEvent)
         expect(LogStash::FLUSH).to be_a(LogStash::FlushEvent)
       end
+
+      it "should define the shutdown event with SignalEvent as parent class" do
+        expect(LogStash::SHUTDOWN).to be_a(LogStash::SignalEvent)
+        expect(LogStash::FLUSH).to be_a(LogStash::SignalEvent)
+      end
+
+      it "should define the flush? method" do
+        expect(LogStash::SHUTDOWN.flush?).to be_falsey
+        expect(LogStash::FLUSH.flush?).to be_truthy
+      end
+
+      it "should define the shutdown? method" do
+        expect(LogStash::SHUTDOWN.shutdown?).to be_truthy
+        expect(LogStash::FLUSH.shutdown?).to be_falsey
+      end
     end
   end
 
@@ -599,7 +601,36 @@
     end
 
     it "return the string containing the timestamp, the host and the message" do
-      expect(event1.to_s).to eq("#{timestamp.to_iso8601} #{event1["host"]} #{event1["message"]}")
+      expect(event1.to_s).to eq("#{timestamp.to_iso8601} #{event1.get("host")} #{event1.get("message")}")
+    end
+  end
+
+  describe "Event accessors" do
+    let(:event) { LogStash::Event.new({ "message" => "foo" }) }
+
+    it "should invalidate target caching" do
+      expect(event.get("[a][0]")).to be_nil
+
+      expect(event.set("[a][0]", 42)).to eq(42)
+      expect(event.get("[a][0]")).to eq(42)
+      expect(event.get("[a]")).to eq({"0" => 42})
+
+      expect(event.set("[a]", [42, 24])).to eq([42, 24])
+      expect(event.get("[a]")).to eq([42, 24])
+
+      expect(event.get("[a][0]")).to eq(42)
+
+      expect(event.set("[a]", [24, 42])).to eq([24, 42])
+      expect(event.get("[a][0]")).to eq(24)
+
+      expect(event.set("[a][0]", {"a "=> 99, "b" => 98})).to eq({"a "=> 99, "b" => 98})
+      expect(event.get("[a][0]")).to eq({"a "=> 99, "b" => 98})
+
+      expect(event.get("[a]")).to eq([{"a "=> 99, "b" => 98}, 42])
+      expect(event.get("[a][0]")).to eq({"a "=> 99, "b" => 98})
+      expect(event.get("[a][1]")).to eq(42)
+      expect(event.get("[a][0][b]")).to eq(98)
     end
   end
 end
+
diff --git a/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
index e83d1586c2e..38e2052b874 100644
--- a/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
+++ b/logstash-core-plugin-api/lib/logstash-core-plugin-api/version.rb
@@ -1,2 +1 @@
-# encoding: utf-8
-LOGSTASH_CORE_PLUGIN_API = "1.0.0"
+LOGSTASH_CORE_PLUGIN_API = "2.1.12"
diff --git a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
index 08efcb63abf..99bb768d336 100644
--- a/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
+++ b/logstash-core-plugin-api/logstash-core-plugin-api.gemspec
@@ -11,13 +11,13 @@ Gem::Specification.new do |gem|
   gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
   gem.license       = "Apache License (2.0)"
 
-  gem.files         = Dir.glob(["logstash-core-event.gemspec", "lib/**/*.rb", "spec/**/*.rb"])
+  gem.files         = Dir.glob(["logstash-core-plugin-api.gemspec", "lib/**/*.rb", "spec/**/*.rb"])
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core-plugin-api"
   gem.require_paths = ["lib"]
   gem.version       = LOGSTASH_CORE_PLUGIN_API
 
-  gem.add_runtime_dependency "logstash-core", ">= 2.0.0", "<= 3.0.0.dev"
+  gem.add_runtime_dependency "logstash-core", "5.1.0"
 
   # Make sure we dont build this gem from a non jruby
   # environment.
diff --git a/logstash-core/build.gradle b/logstash-core/build.gradle
new file mode 100644
index 00000000000..c45ad34920c
--- /dev/null
+++ b/logstash-core/build.gradle
@@ -0,0 +1,88 @@
+import java.nio.file.Files
+import org.yaml.snakeyaml.Yaml
+import static java.nio.file.StandardCopyOption.REPLACE_EXISTING
+
+apply plugin: 'java'
+apply plugin: 'idea'
+
+group = 'org.logstash'
+description = """Logstash Core Java"""
+
+sourceCompatibility = 1.8
+targetCompatibility = 1.8
+
+// fetch version from Logstash's master versions.yml file
+def versionMap = (Map) (new Yaml()).load(new File("$projectDir/../versions.yml").text)
+version = versionMap['logstash-core']
+
+buildscript {
+    repositories {
+        mavenCentral()
+    }
+
+    dependencies {
+        classpath 'org.yaml:snakeyaml:1.17'
+    }
+}
+
+configurations {
+  provided
+}
+
+project.sourceSets {
+    main.compileClasspath += project.configurations.provided
+    main.runtimeClasspath += project.configurations.provided
+    test.compileClasspath += project.configurations.provided
+    test.runtimeClasspath += project.configurations.provided
+}
+project.javadoc.classpath += project.configurations.provided
+
+repositories {
+  mavenCentral()
+}
+
+dependencies {
+  runtime 'org.apache.logging.log4j:log4j-1.2-api:2.6.2'
+  compile 'org.apache.logging.log4j:log4j-api:2.6.2'
+  compile 'org.apache.logging.log4j:log4j-core:2.6.2'
+  compile 'com.fasterxml.jackson.core:jackson-core:2.7.4'
+  compile 'com.fasterxml.jackson.core:jackson-databind:2.7.4'
+  testCompile 'org.apache.logging.log4j:log4j-core:2.6.2:tests'
+  testCompile 'org.apache.logging.log4j:log4j-api:2.6.2:tests'
+  testCompile 'junit:junit:4.12'
+  provided 'org.jruby:jruby-core:1.7.25'
+}
+
+idea {
+    module {
+        scopes.PROVIDED.plus += [project.configurations.provided]
+    }
+}
+
+task generateGemJarRequiresFile << {
+  File jars_file = file('lib/jars.rb')
+  jars_file.newWriter().withWriter { w ->
+    w << "require \'jar_dependencies\'\n"
+    configurations.runtime.allDependencies.each {
+      w << "require_jar(\'${it.group}\', \'${it.name}\', \'${it.version}\')\n"
+    }
+    w << "require_jar(\'${project.group}\', \'${project.name}\', \'${project.version}\')\n"
+  }
+}
+
+task vendor << {
+    String vendorPathPrefix = "vendor/jars"
+    configurations.runtime.allDependencies.each { dep ->
+      File f = configurations.runtime.filter { it.absolutePath.contains("${dep.group}/${dep.name}/${dep.version}") }.singleFile
+      String groupPath = dep.group.replaceAll('\\.', '/')
+      File newJarFile = file("${vendorPathPrefix}/${groupPath}/${dep.name}/${dep.version}/${dep.name}-${dep.version}.jar")
+      newJarFile.mkdirs()
+      Files.copy(f.toPath(), newJarFile.toPath(), REPLACE_EXISTING)
+    }
+    String projectGroupPath = project.group.replaceAll('\\.', '/')
+    File projectJarFile = file("${vendorPathPrefix}/${projectGroupPath}/${project.name}/${project.version}/${project.name}-${project.version}.jar")
+    projectJarFile.mkdirs()
+    Files.copy(file("$buildDir/libs/${project.name}-${project.version}.jar").toPath(), projectJarFile.toPath(), REPLACE_EXISTING)
+}
+
+vendor.dependsOn(jar, generateGemJarRequiresFile)
diff --git a/logstash-core/lib/jars.rb b/logstash-core/lib/jars.rb
new file mode 100644
index 00000000000..8c615f64146
--- /dev/null
+++ b/logstash-core/lib/jars.rb
@@ -0,0 +1,7 @@
+require 'jar_dependencies'
+require_jar('org.apache.logging.log4j', 'log4j-1.2-api', '2.6.2')
+require_jar('org.apache.logging.log4j', 'log4j-api', '2.6.2')
+require_jar('org.apache.logging.log4j', 'log4j-core', '2.6.2')
+require_jar('com.fasterxml.jackson.core', 'jackson-core', '2.7.4')
+require_jar('com.fasterxml.jackson.core', 'jackson-databind', '2.7.4')
+require_jar('org.logstash', 'logstash-core', '5.1.0')
diff --git a/logstash-core/lib/logstash-core/version.rb b/logstash-core/lib/logstash-core/version.rb
index fdc9d13f1a4..44e0303f54c 100644
--- a/logstash-core/lib/logstash-core/version.rb
+++ b/logstash-core/lib/logstash-core/version.rb
@@ -5,4 +5,4 @@
 # Note to authors: this should not include dashes because 'gem' barfs if
 # you include a dash in the version string.
 
-LOGSTASH_CORE_VERSION = "3.0.0.dev"
+LOGSTASH_CORE_VERSION = "5.1.0"
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index 5d2fde3201d..e6114cf1d5d 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -18,38 +18,44 @@
 LogStash::Environment.load_locale!
 
 class LogStash::Agent
+  include LogStash::Util::Loggable
   STARTED_AT = Time.now.freeze
 
-  attr_reader :metric, :node_name, :pipelines, :logger
+  attr_reader :metric, :node_name, :pipelines, :settings, :webserver
+  attr_accessor :logger
 
   # initialize method for LogStash::Agent
   # @param params [Hash] potential parameters are:
   #   :node_name [String] - identifier for the agent
   #   :auto_reload [Boolean] - enable reloading of pipelines
   #   :reload_interval [Integer] - reload pipelines every X seconds
-  #   :logger [Cabin::Channel] - logger instance
-  def initialize(params)
-    @logger = params[:logger]
-    @auto_reload = params[:auto_reload]
+  def initialize(settings = LogStash::SETTINGS)
+    @logger = self.class.logger
+    @settings = settings
+    @auto_reload = setting("config.reload.automatic")
 
     @pipelines = {}
-    @node_name = params[:node_name] || Socket.gethostname
-    @web_api_http_host = params[:web_api_http_host]
-    @web_api_http_port = params[:web_api_http_port]
+    @node_name = setting("node.name")
+    @http_host = setting("http.host")
+    @http_port = setting("http.port")
+    @http_environment = setting("http.environment")
 
     @config_loader = LogStash::Config::Loader.new(@logger)
-    @reload_interval = params[:reload_interval] || 3 # seconds
+    @reload_interval = setting("config.reload.interval")
     @upgrade_mutex = Mutex.new
 
-    @collect_metric = params.fetch(:collect_metric, false)
-    setup_metric_collection
+    @collect_metric = setting("metric.collect")
+
+    # Create the collectors and configured it with the library
+    configure_metrics_collectors
+
+    @reload_metric = metric.namespace([:stats, :pipelines])
   end
 
   def execute
     @thread = Thread.current # this var is implicilty used by Stud.stop?
-    @logger.info("starting agent")
+    @logger.debug("starting agent")
 
-    start_background_services
     start_pipelines
     start_webserver
 
@@ -74,18 +80,33 @@ def execute
   # @param pipeline_id [String] pipeline string identifier
   # @param settings [Hash] settings that will be passed when creating the pipeline.
   #   keys should be symbols such as :pipeline_workers and :pipeline_batch_delay
-  def register_pipeline(pipeline_id, settings)
-    pipeline = create_pipeline(settings.merge(:pipeline_id => pipeline_id, :metric => metric))
+  def register_pipeline(pipeline_id, settings = @settings)
+    pipeline_settings = settings.clone
+    pipeline_settings.set("pipeline.id", pipeline_id)
+
+    pipeline = create_pipeline(pipeline_settings)
     return unless pipeline.is_a?(LogStash::Pipeline)
+    if @auto_reload && pipeline.non_reloadable_plugins.any?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_register"),
+                    :pipeline_id => pipeline_id,
+                    :plugins => pipeline.non_reloadable_plugins.map(&:class))
+      return
+    end
     @pipelines[pipeline_id] = pipeline
   end
 
   def reload_state!
     @upgrade_mutex.synchronize do
-      @pipelines.each do |pipeline_id, _|
+      @pipelines.each do |pipeline_id, pipeline|
+        next if pipeline.settings.get("config.reload.automatic") == false
         begin
           reload_pipeline!(pipeline_id)
         rescue => e
+          @reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
+            n.increment(:failures)
+            n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
+            n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+          end
           @logger.error(I18n.t("oops"), :message => e.message, :class => e.class.name, :backtrace => e.backtrace)
         end
       end
@@ -99,8 +120,13 @@ def uptime
     ((Time.now.to_f - STARTED_AT.to_f) * 1000.0).to_i
   end
 
+  def stop_collecting_metrics
+    @collector.stop
+    @periodic_pollers.stop
+  end
+
   def shutdown
-    stop_background_services
+    stop_collecting_metrics
     stop_webserver
     shutdown_pipelines
   end
@@ -109,10 +135,16 @@ def node_uuid
     @node_uuid ||= SecureRandom.uuid
   end
 
+  def running_pipelines?
+    @upgrade_mutex.synchronize do
+      @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }.any?
+    end
+  end
+
   private
   def start_webserver
-    options = {:http_host => @web_api_http_host, :http_port => @web_api_http_port }
-    @webserver = LogStash::WebServer.new(@logger, options)
+    options = {:http_host => @http_host, :http_ports => @http_port, :http_environment => @http_environment }
+    @webserver = LogStash::WebServer.new(@logger, self, options)
     Thread.new(@webserver) do |webserver|
       LogStash::Util.set_thread_name("Api Webserver")
       webserver.run
@@ -123,66 +155,84 @@ def stop_webserver
     @webserver.stop if @webserver
   end
 
-  def start_background_services
-    if collect_metrics?
-      @logger.debug("Agent: Starting metric periodic pollers")
-      @periodic_pollers.start
-    end
-  end
+  def configure_metrics_collectors
+    @collector = LogStash::Instrument::Collector.new
 
-  def stop_background_services
-    if collect_metrics?
-      @logger.debug("Agent: Stopping metric periodic pollers")
-      @periodic_pollers.stop
-    end
-  end
+    @metric = if collect_metrics?
+                @logger.debug("Agent: Configuring metric collection")
+                LogStash::Instrument::Metric.new(@collector)
+              else
+                LogStash::Instrument::NullMetric.new(@collector)
+              end
 
-  def setup_metric_collection
-    if collect_metrics?
-      @logger.debug("Agent: Configuring metric collection")
-      LogStash::Instrument::Collector.instance.agent = self
-      @metric = LogStash::Instrument::Metric.new
-    else
-      @metric = LogStash::Instrument::NullMetric.new
-    end
 
-    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(metric)
+    @periodic_pollers = LogStash::Instrument::PeriodicPollers.new(@metric)
+    @periodic_pollers.start
+  end
+
+  def reset_pipeline_metrics(id)
+    # selectively reset metrics we don't wish to keep after reloading
+    # these include metrics about the plugins and number of processed events
+    # we want to keep other metrics like reload counts and error messages
+    @collector.clear("stats/pipelines/#{id}/plugins")
+    @collector.clear("stats/pipelines/#{id}/events")
   end
 
   def collect_metrics?
     @collect_metric
   end
 
-  def create_pipeline(settings)
-    begin
-      config = fetch_config(settings)
-    rescue => e
-      @logger.error("failed to fetch pipeline configuration", :message => e.message)
-      return
+  def create_pipeline(settings, config=nil)
+    if config.nil?
+      begin
+        config = fetch_config(settings)
+      rescue => e
+        @logger.error("failed to fetch pipeline configuration", :message => e.message)
+        return
+      end
     end
 
     begin
-      LogStash::Pipeline.new(config, settings)
+      LogStash::Pipeline.new(config, settings, metric)
     rescue => e
-      @logger.error("fetched an invalid config", :config => config, :reason => e.message)
+      @reload_metric.namespace([settings.get("pipeline.id").to_sym, :reloads]).tap do |n|
+        n.increment(:failures)
+        n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
+        n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+      end
+      if @logger.debug?
+        @logger.error("fetched an invalid config", :config => config, :reason => e.message, :backtrace => e.backtrace)
+      else
+        @logger.error("fetched an invalid config", :config => config, :reason => e.message)
+      end
       return
     end
   end
 
   def fetch_config(settings)
-    @config_loader.format_config(settings[:config_path], settings[:config_string])
+    @config_loader.format_config(settings.get("path.config"), settings.get("config.string"))
   end
 
   # since this method modifies the @pipelines hash it is
   # wrapped in @upgrade_mutex in the parent call `reload_state!`
   def reload_pipeline!(id)
     old_pipeline = @pipelines[id]
-    new_pipeline = create_pipeline(old_pipeline.original_settings)
+    new_config = fetch_config(old_pipeline.settings)
+    if old_pipeline.config_str == new_config
+      @logger.debug("no configuration change for pipeline",
+                    :pipeline => id, :config => new_config)
+      return
+    end
+
+    new_pipeline = create_pipeline(old_pipeline.settings, new_config)
+
     return if new_pipeline.nil?
 
-    if old_pipeline.config_str == new_pipeline.config_str
-      @logger.debug("no configuration change for pipeline",
-                    :pipeline => id, :config => old_pipeline.config_str)
+    if new_pipeline.non_reloadable_plugins.any?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"),
+                    :pipeline_id => id,
+                    :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
+      return
     else
       @logger.warn("fetched new config for pipeline. upgrading..",
                    :pipeline => id, :config => new_pipeline.config_str)
@@ -194,12 +244,17 @@ def start_pipeline(id)
     pipeline = @pipelines[id]
     return unless pipeline.is_a?(LogStash::Pipeline)
     return if pipeline.ready?
-    @logger.info("starting pipeline", :id => id)
+    @logger.debug("starting pipeline", :id => id)
     Thread.new do
       LogStash::Util.set_thread_name("pipeline.#{id}")
       begin
         pipeline.run
       rescue => e
+        @reload_metric.namespace([id.to_sym, :reloads]) do |n|
+          n.increment(:failures)
+          n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
+          n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+        end
         @logger.error("Pipeline aborted due to error", :exception => e, :backtrace => e.backtrace)
       end
     end
@@ -215,19 +270,17 @@ def stop_pipeline(id)
   end
 
   def start_pipelines
-    @pipelines.each { |id, _| start_pipeline(id) }
+    @pipelines.each do |id, _|
+      start_pipeline(id)
+      # no reloads yet, initalize all the reload metrics
+      init_pipeline_reload_metrics(id)
+    end
   end
 
   def shutdown_pipelines
     @pipelines.each { |id, _| stop_pipeline(id) }
   end
 
-  def running_pipelines?
-    @upgrade_mutex.synchronize do
-      @pipelines.select {|pipeline_id, _| running_pipeline?(pipeline_id) }.any?
-    end
-  end
-
   def running_pipeline?(pipeline_id)
     thread = @pipelines[pipeline_id].thread
     thread.is_a?(Thread) && thread.alive?
@@ -235,11 +288,30 @@ def running_pipeline?(pipeline_id)
 
   def upgrade_pipeline(pipeline_id, new_pipeline)
     stop_pipeline(pipeline_id)
+    reset_pipeline_metrics(pipeline_id)
     @pipelines[pipeline_id] = new_pipeline
     start_pipeline(pipeline_id)
+    @reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
+      n.increment(:successes)
+      n.gauge(:last_success_timestamp, LogStash::Timestamp.now)
+    end
   end
 
   def clean_state?
     @pipelines.empty?
   end
+
+  def setting(key)
+    @settings.get(key)
+  end
+
+  def init_pipeline_reload_metrics(id)
+    @reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+      n.increment(:successes, 0)
+      n.increment(:failures, 0)
+      n.gauge(:last_error, nil)
+      n.gauge(:last_success_timestamp, nil)
+      n.gauge(:last_failure_timestamp, nil)
+    end
+  end
 end # class LogStash::Agent
diff --git a/logstash-core/lib/logstash/api/app_helpers.rb b/logstash-core/lib/logstash/api/app_helpers.rb
new file mode 100644
index 00000000000..98e6c377576
--- /dev/null
+++ b/logstash-core/lib/logstash/api/app_helpers.rb
@@ -0,0 +1,74 @@
+# encoding: utf-8
+require "logstash/json"
+require "logstash/api/errors"
+
+module LogStash::Api::AppHelpers
+  # This method handle both of the normal flow *happy path*
+  # and the display or errors, if more custom logic is added here
+  # it will make sense to separate them.
+  #
+  # See `#error` method in the `LogStash::Api::Module::Base`
+  def respond_with(data, options={})
+    as     = options.fetch(:as, :json)
+    filter = options.fetch(:filter, "")
+
+    status data.respond_to?(:status_code) ? data.status_code : 200
+
+    if as == :json
+      if api_error?(data)
+        data = generate_error_hash(data)
+      else
+        selected_fields = extract_fields(filter.to_s.strip)
+        data.select! { |k,v| selected_fields.include?(k) } unless selected_fields.empty?
+        unless options.include?(:exclude_default_metadata)
+          data = data.to_hash
+          if data.values.size == 0 && selected_fields.size > 0
+            raise LogStash::Api::NotFoundError
+          end
+          data = default_metadata.merge(data)
+        end
+      end
+
+      content_type "application/json"
+      LogStash::Json.dump(data, {:pretty => pretty?})
+    else
+      content_type "text/plain"
+      data.to_s
+    end
+  end
+
+  protected
+  def extract_fields(filter_string)
+    (filter_string.empty? ? [] : filter_string.split(",").map { |s| s.strip.to_sym })
+  end
+
+  def as_boolean(string)
+    return true   if string == true   || string =~ (/(true|t|yes|y|1)$/i)
+    return false  if string == false  || string.blank? || string =~ (/(false|f|no|n|0)$/i)
+    raise ArgumentError.new("invalid value for Boolean: \"#{string}\"")
+  end
+
+  def default_metadata
+    @factory.build(:default_metadata).all
+  end
+
+  def api_error?(error)
+    error.is_a?(LogStash::Api::ApiError)
+  end
+
+  def pretty?
+    params.has_key?("pretty")
+  end
+
+  def generate_error_hash(error)
+    {
+      :path => request.path,
+      :status => error.status_code,
+      :error => error.to_hash
+    }
+  end
+
+  def human?
+    params.has_key?("human") && (params["human"].nil? || as_boolean(params["human"]) == true)
+  end
+end
diff --git a/logstash-core/lib/logstash/api/command_factory.rb b/logstash-core/lib/logstash/api/command_factory.rb
new file mode 100644
index 00000000000..2d790b5ee1c
--- /dev/null
+++ b/logstash-core/lib/logstash/api/command_factory.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require "logstash/api/service"
+require "logstash/api/commands/system/basicinfo_command"
+require "logstash/api/commands/system/plugins_command"
+require "logstash/api/commands/stats"
+require "logstash/api/commands/node"
+require "logstash/api/commands/default_metadata"
+
+
+module LogStash
+  module Api
+    class CommandFactory
+      attr_reader :factory, :service
+
+      def initialize(service)
+        @service = service
+        @factory = {
+          :system_basic_info => ::LogStash::Api::Commands::System::BasicInfo,
+          :plugins_command => ::LogStash::Api::Commands::System::Plugins,
+          :stats => ::LogStash::Api::Commands::Stats,
+          :node => ::LogStash::Api::Commands::Node,
+          :default_metadata => ::LogStash::Api::Commands::DefaultMetadata
+        }
+      end
+
+      def build(*klass_path)
+        # Get a nested path with args like (:parent, :child)
+        klass = klass_path.reduce(factory) {|acc,v| acc[v]}
+
+        if klass
+          klass.new(service)
+        else
+          raise ArgumentError, "Class path '#{klass_path}' does not map to command!"
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/base.rb b/logstash-core/lib/logstash/api/commands/base.rb
new file mode 100644
index 00000000000..d2bef44e6fb
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/base.rb
@@ -0,0 +1,27 @@
+# encoding: utf-8
+
+module LogStash
+  module Api
+    module Commands
+      class Base
+        attr_reader :service
+        
+        def initialize(service = LogStash::Api::Service.instance)
+          @service = service
+        end
+
+        def uptime
+          service.agent.uptime
+        end
+        
+        def started_at
+          (LogStash::Agent::STARTED_AT.to_f * 1000.0).to_i
+        end
+
+        def extract_metrics(path, *keys)
+          service.extract_metrics(path, *keys)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/default_metadata.rb b/logstash-core/lib/logstash/api/commands/default_metadata.rb
new file mode 100644
index 00000000000..a4703072990
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/default_metadata.rb
@@ -0,0 +1,27 @@
+# encoding: utf-8
+
+require "logstash/api/commands/base"
+
+module LogStash
+  module Api
+    module Commands
+      class DefaultMetadata < Commands::Base
+        def all
+          {:host => host, :version => version, :http_address => http_address}
+        end
+
+        def host
+          Socket.gethostname
+        end
+
+        def version
+          LOGSTASH_CORE_VERSION
+        end
+
+        def http_address
+          service.agent.webserver.address
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/hot_threads_reporter.rb b/logstash-core/lib/logstash/api/commands/hot_threads_reporter.rb
new file mode 100644
index 00000000000..69a417cdb73
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/hot_threads_reporter.rb
@@ -0,0 +1,58 @@
+# encoding: utf-8
+
+class HotThreadsReport
+  STRING_SEPARATOR_LENGTH = 80.freeze
+  HOT_THREADS_STACK_TRACES_SIZE_DEFAULT = 10.freeze
+
+  def initialize(cmd, options)
+    @cmd = cmd
+    filter = { :stacktrace_size => options.fetch(:stacktrace_size, HOT_THREADS_STACK_TRACES_SIZE_DEFAULT) }
+    jr_dump = JRMonitor.threads.generate(filter)
+    @thread_dump = ::LogStash::Util::ThreadDump.new(options.merge(:dump => jr_dump))
+  end
+
+  def to_s
+    hash = to_hash[:hot_threads]
+    report =  "#{I18n.t("logstash.web_api.hot_threads.title", :hostname => hash[:hostname], :time => hash[:time], :top_count => @thread_dump.top_count )} \n"
+    report << '=' * STRING_SEPARATOR_LENGTH
+    report << "\n"
+    hash[:threads].each do |thread|
+      thread_report = "#{I18n.t("logstash.web_api.hot_threads.thread_title", :percent_of_cpu_time => thread[:percent_of_cpu_time], :thread_state => thread[:state], :thread_name => thread[:name])} \n"
+      thread_report << "#{thread[:path]}\n" if thread[:path]
+      thread[:traces].each do |trace|
+        thread_report << "\t#{trace}\n"
+      end
+      report << thread_report
+      report << '-' * STRING_SEPARATOR_LENGTH
+      report << "\n"
+    end
+    report
+  end
+
+  def to_hash
+    hash = { :time => Time.now.iso8601, :busiest_threads => @thread_dump.top_count, :threads => [] }
+    @thread_dump.each do |thread_name, _hash|
+      thread_name, thread_path = _hash["thread.name"].split(": ")
+      thread = { :name => thread_name,
+                 :percent_of_cpu_time => cpu_time_as_percent(_hash),
+                 :state => _hash["thread.state"]
+      }
+      thread[:path] = thread_path if thread_path
+      traces = []
+      _hash["thread.stacktrace"].each do |trace|
+        traces << trace
+      end
+      thread[:traces] = traces unless traces.empty?
+      hash[:threads] << thread
+    end
+    { :hot_threads => hash }
+  end
+
+  def cpu_time_as_percent(hash)
+    (((cpu_time(hash) / @cmd.uptime * 1.0)*10000).to_i)/100.0
+  end
+
+  def cpu_time(hash)
+    hash["cpu.time"] / 1000000.0
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/node.rb b/logstash-core/lib/logstash/api/commands/node.rb
new file mode 100644
index 00000000000..816d6be8f8b
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/node.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require "logstash/api/commands/base"
+require_relative "hot_threads_reporter"
+
+module LogStash
+  module Api
+    module Commands
+      class Node < Commands::Base
+
+        def all(selected_fields=[])
+          payload = {
+            :pipeline => pipeline,
+            :os => os,
+            :jvm => jvm
+          }
+          payload.select! { |k,v| selected_fields.include?(k) } unless selected_fields.empty?
+          payload
+        end
+
+        def pipeline
+          extract_metrics(
+            [:stats, :pipelines, :main, :config],
+            :workers, :batch_size, :batch_delay, :config_reload_automatic, :config_reload_interval
+          )
+        end
+
+        def os
+          {
+            :name => java.lang.System.getProperty("os.name"),
+            :arch => java.lang.System.getProperty("os.arch"),
+            :version => java.lang.System.getProperty("os.version"),
+            :available_processors => java.lang.Runtime.getRuntime().availableProcessors()
+          }
+        end
+
+        def jvm
+          memory_bean = ManagementFactory.getMemoryMXBean()
+
+          {
+            :pid =>  ManagementFactory.getRuntimeMXBean().getName().split("@").first.to_i,
+            :version => java.lang.System.getProperty("java.version"),
+            :vm_name => java.lang.System.getProperty("java.vm.name"),
+            :vm_version => java.lang.System.getProperty("java.version"),
+            :vm_vendor => java.lang.System.getProperty("java.vendor"),
+            :vm_name => java.lang.System.getProperty("java.vm.name"),
+            :start_time_in_millis => started_at,
+            :mem => {
+              :heap_init_in_bytes => (memory_bean.getHeapMemoryUsage().getInit() < 0 ? 0 : memory_bean.getHeapMemoryUsage().getInit()),
+              :heap_max_in_bytes => (memory_bean.getHeapMemoryUsage().getMax() < 0 ? 0 : memory_bean.getHeapMemoryUsage().getMax()),
+              :non_heap_init_in_bytes => (memory_bean.getNonHeapMemoryUsage().getInit() < 0 ? 0 : memory_bean.getNonHeapMemoryUsage().getInit()),
+              :non_heap_max_in_bytes => (memory_bean.getNonHeapMemoryUsage().getMax() < 0 ? 0 : memory_bean.getNonHeapMemoryUsage().getMax())
+            },
+            :gc_collectors => ManagementFactory.getGarbageCollectorMXBeans().collect(&:getName)
+          }
+        end
+
+        def hot_threads(options={})
+          HotThreadsReport.new(self, options)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/stats.rb b/logstash-core/lib/logstash/api/commands/stats.rb
new file mode 100644
index 00000000000..502c57fac03
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/stats.rb
@@ -0,0 +1,102 @@
+# encoding: utf-8
+require "logstash/api/commands/base"
+require 'logstash/util/thread_dump'
+require_relative "hot_threads_reporter"
+
+module LogStash
+  module Api
+    module Commands
+      class Stats < Commands::Base
+        def jvm
+          {
+            :threads => extract_metrics(
+              [:jvm, :threads],
+              :count,
+              :peak_count
+            ),
+            :mem => memory,
+            :gc => gc
+          }
+        end
+
+        def process
+          extract_metrics(
+            [:jvm, :process],
+            :open_file_descriptors,
+            :peak_open_file_descriptors,
+            :max_file_descriptors,
+            [:mem, [:total_virtual_in_bytes]],
+            [:cpu, [:total_in_millis, :percent]]
+          )
+        end
+
+        def events
+          extract_metrics(
+            [:stats, :events],
+            :in, :filtered, :out, :duration_in_millis
+          )
+        end
+
+        def pipeline
+          stats = service.get_shallow(:stats, :pipelines)
+          PluginsStats.report(stats)
+        end
+
+        def memory
+          memory = service.get_shallow(:jvm, :memory)
+          {
+            :heap_used_in_bytes => memory[:heap][:used_in_bytes],
+            :heap_used_percent => memory[:heap][:used_percent],
+            :heap_committed_in_bytes => memory[:heap][:committed_in_bytes],
+            :heap_max_in_bytes => memory[:heap][:max_in_bytes],
+            :heap_used_in_bytes => memory[:heap][:used_in_bytes],
+            :non_heap_used_in_bytes => memory[:non_heap][:used_in_bytes],
+            :non_heap_committed_in_bytes => memory[:non_heap][:committed_in_bytes],
+            :pools => memory[:pools].inject({}) do |acc, (type, hash)|
+              hash.delete("committed_in_bytes")
+              acc[type] = hash
+              acc
+            end
+          }
+        end
+
+        def gc
+          service.get_shallow(:jvm, :gc)
+        end
+
+        def hot_threads(options={})
+          HotThreadsReport.new(self, options)
+        end
+
+        module PluginsStats
+          module_function
+
+          def plugin_stats(stats, plugin_type)
+            # Turn the `plugins` stats hash into an array of [ {}, {}, ... ]
+            # This is to produce an array of data points, one point for each
+            # plugin instance.
+            return [] unless stats[:plugins] && stats[:plugins].include?(plugin_type)
+            stats[:plugins][plugin_type].collect do |id, data|
+              { :id => id }.merge(data)
+            end
+          end
+
+          def report(stats)
+            # Only one pipeline right now.
+            stats = stats[:main]
+
+            {
+              :events => stats[:events],
+              :plugins => {
+                :inputs => plugin_stats(stats, :inputs),
+                :filters => plugin_stats(stats, :filters),
+                :outputs => plugin_stats(stats, :outputs)
+              },
+              :reloads => stats[:reloads],
+            }
+          end
+        end # module PluginsStats
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb b/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb
new file mode 100644
index 00000000000..6eacdbd5b4b
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/system/basicinfo_command.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require 'logstash/api/commands/base'
+require "logstash/util/duration_formatter"
+require 'logstash/build'
+
+module LogStash
+  module Api
+    module Commands
+      module System
+        class BasicInfo < Commands::Base
+
+          def run
+            # Just merge this stuff with the defaults
+            BUILD_INFO
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/commands/system/plugins_command.rb b/logstash-core/lib/logstash/api/commands/system/plugins_command.rb
new file mode 100644
index 00000000000..378f65e8598
--- /dev/null
+++ b/logstash-core/lib/logstash/api/commands/system/plugins_command.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require "logstash/api/commands/base"
+
+module LogStash
+  module Api
+    module Commands
+      module System
+        class Plugins < Commands::Base
+          def run
+            { :total => plugins.count, :plugins => plugins }
+          end
+
+          private
+
+          def plugins
+            @plugins ||= find_plugins_gem_specs.map do |spec|
+              { :name => spec.name, :version => spec.version.to_s }
+            end.sort_by do |spec|
+              spec[:name]
+            end
+          end
+
+          def find_plugins_gem_specs
+            @specs ||= ::Gem::Specification.find_all.select{|spec| logstash_plugin_gem_spec?(spec)}
+          end
+
+          def logstash_plugin_gem_spec?(spec)
+            spec.metadata && spec.metadata["logstash_plugin"] == "true"
+          end
+
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/errors.rb b/logstash-core/lib/logstash/api/errors.rb
new file mode 100644
index 00000000000..e080305d9d1
--- /dev/null
+++ b/logstash-core/lib/logstash/api/errors.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    class ApiError < StandardError;
+      def initialize(message = nil)
+        super(message || "Api Error")
+      end
+
+      def status_code
+        500
+      end
+
+      def to_hash
+        { :message => to_s }
+      end
+    end
+
+    class NotFoundError < ApiError
+      def initialize
+        super("Not Found")
+      end
+
+      def status_code
+        404
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/init.ru b/logstash-core/lib/logstash/api/init.ru
deleted file mode 100644
index 7fc0c93e9b9..00000000000
--- a/logstash-core/lib/logstash/api/init.ru
+++ /dev/null
@@ -1,31 +0,0 @@
-ROOT = File.expand_path(File.dirname(__FILE__))
-$LOAD_PATH.unshift File.join(ROOT, 'lib')
-Dir.glob('lib/**').each{ |d| $LOAD_PATH.unshift(File.join(ROOT, d)) }
-
-require 'sinatra'
-require 'app/root'
-require 'app/modules/stats'
-require 'app/modules/node'
-require 'app/modules/node_stats'
-require 'app/modules/plugins'
-
-env = ENV["RACK_ENV"].to_sym
-set :environment, env
-
-set :service, LogStash::Api::Service.instance
-
-configure do
-  enable :logging
-end
-run LogStash::Api::Root
-
-namespaces = { "/_node" => LogStash::Api::Node,
-               "/_node/stats" => LogStash::Api::NodeStats,
-               "/_stats" => LogStash::Api::Stats,
-               "/_plugins" => LogStash::Api::Plugins }
-
-namespaces.each_pair do |namespace, app|
-  map(namespace) do
-    run app
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app.rb b/logstash-core/lib/logstash/api/lib/app.rb
deleted file mode 100644
index 72946ec6707..00000000000
--- a/logstash-core/lib/logstash/api/lib/app.rb
+++ /dev/null
@@ -1,40 +0,0 @@
-# encoding: utf-8
-require "cabin"
-require "logstash/json"
-require "helpers/app_helpers"
-require "app/service"
-require "app/command_factory"
-require "logstash/util/loggable"
-
-module LogStash::Api
-  class BaseApp < ::Sinatra::Application
-
-    attr_reader :factory
-
-    if settings.environment != :production
-      set :raise_errors, true
-      set :show_exceptions, :after_handler
-    end
-
-    include LogStash::Util::Loggable
-
-    helpers AppHelpers
-
-    def initialize(app=nil)
-      super(app)
-      @factory = CommandFactory.new(settings.service)
-    end
-
-    not_found do
-      status 404
-      as   = params.has_key?("human") ? :string : :json
-      text = as == :string ? "" : {}
-      respond_with(text, :as => as)
-    end
-
-    error do
-      logger.error(env['sinatra.error'].message, :url => request.url, :ip => request.ip, :params => request.params)
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/command.rb b/logstash-core/lib/logstash/api/lib/app/command.rb
deleted file mode 100644
index 75d8f958c6b..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/command.rb
+++ /dev/null
@@ -1,29 +0,0 @@
-# encoding: utf-8
-require "app/service"
-
-module LogStash::Api
-  class Command
-
-    attr_reader :service
-
-    def initialize(service = LogStash::Api::Service.instance)
-      @service = service
-    end
-
-    def run
-      raise "Not implemented"
-    end
-
-    def hostname
-      service.agent.node_name
-    end
-
-    def uptime
-      service.agent.uptime
-    end
-
-    def started_at
-      (LogStash::Agent::STARTED_AT.to_f * 1000.0).to_i
-    end
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/command_factory.rb b/logstash-core/lib/logstash/api/lib/app/command_factory.rb
deleted file mode 100644
index 29e71e6c4f7..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/command_factory.rb
+++ /dev/null
@@ -1,29 +0,0 @@
-# encoding: utf-8
-require "app/service"
-require "app/commands/system/basicinfo_command"
-require "app/commands/stats/events_command"
-require "app/commands/stats/hotthreads_command"
-require "app/commands/stats/memory_command"
-require "app/commands/system/plugins_command"
-
-module LogStash::Api
-  class CommandFactory
-
-    attr_reader :factory, :service
-
-    def initialize(service)
-      @service = service
-      @factory = {}.merge(
-        :system_basic_info => SystemBasicInfoCommand,
-        :events_command => StatsEventsCommand,
-        :hot_threads_command => HotThreadsCommand,
-        :memory_command => JvmMemoryCommand,
-        :plugins_command => PluginsCommand
-      )
-    end
-
-    def build(klass)
-      factory[klass].new(service)
-    end
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/stats/events_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/stats/events_command.rb
deleted file mode 100644
index 78337364548..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/stats/events_command.rb
+++ /dev/null
@@ -1,13 +0,0 @@
-# encoding: utf-8
-require "app/command"
-
-class LogStash::Api::StatsEventsCommand < LogStash::Api::Command
-
-  def run
-    #return whatever is comming out of the snapshot event, this obvoiusly
-    #need to be tailored to the right metrics for this command.
-    stats =  LogStash::Json.load(service.get(:events_stats))
-    stats["stats"]["events"]
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/stats/hotthreads_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/stats/hotthreads_command.rb
deleted file mode 100644
index 0c3f4ee2ef7..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/stats/hotthreads_command.rb
+++ /dev/null
@@ -1,120 +0,0 @@
-# encoding: utf-8
-require "app/command"
-require 'monitoring'
-require "socket"
-
-class LogStash::Api::HotThreadsCommand < LogStash::Api::Command
-
-  STACK_TRACES_SIZE_DEFAULT = 10.freeze
-
-  def run(options={})
-    filter = { :stacktrace_size => options.fetch(:stacktrace_size, STACK_TRACES_SIZE_DEFAULT) }
-    hash   = JRMonitor.threads.generate(filter)
-    ThreadDump.new(hash, self, options)
-  end
-
-  private
-
-  class ThreadDump
-
-    SKIPPED_THREADS             = [ "Finalizer", "Reference Handler", "Signal Dispatcher" ].freeze
-    THREADS_COUNT_DEFAULT       = 3.freeze
-    IGNORE_IDLE_THREADS_DEFAULT = true.freeze
-
-    attr_reader :top_count, :ignore, :dump
-
-    def initialize(dump, cmd, options={})
-      @dump      = dump
-      @options   = options
-      @top_count = options.fetch(:threads, THREADS_COUNT_DEFAULT)
-      @ignore    = options.fetch(:ignore_idle_threads, IGNORE_IDLE_THREADS_DEFAULT)
-      @cmd       = cmd
-    end
-
-    def to_s
-      hash = to_hash
-      report =  "#{I18n.t("logstash.web_api.hot_threads.title", :hostname => hash[:hostname], :time => hash[:time], :top_count => top_count )} \n"
-      hash[:threads].each do |thread|
-        thread_report = ""
-        thread_report = "\t #{I18n.t("logstash.web_api.hot_threads.thread_title", :percent_of_cpu_time => thread[:percent_of_cpu_time], :thread_state => thread[:state], :thread_name => thread[:name])} \n"
-        thread_report = "\t #{thread[:percent_of_cpu_time]} % of of cpu usage by #{thread[:state]} thread named '#{thread[:name]}'\n"
-        thread_report << "\t\t #{thread[:path]}\n" if thread[:path]
-        thread[:traces].split("\n").each do |trace|
-          thread_report << "#{trace}\n"
-        end
-        report << thread_report
-      end
-      report
-    end
-
-    def to_hash
-      hash = { :hostname => hostname, :time => Time.now.iso8601, :busiest_threads => top_count, :threads => [] }
-      each do |thread_name, _hash|
-        thread_name, thread_path = _hash["thread.name"].split(": ")
-        thread = { :name => thread_name,
-                   :percent_of_cpu_time => cpu_time_as_percent(_hash),
-                   :state => _hash["thread.state"]
-        }
-        thread[:path] = thread_path if thread_path
-        traces = ""
-        _hash["thread.stacktrace"].each do |trace|
-          traces << "\t\t#{trace}\n"
-        end
-        thread[:traces] = traces unless traces.empty?
-        hash[:threads] << thread
-      end
-      hash
-    end
-
-    private
-
-    def each(&block)
-      i=0
-      dump.each_pair do |thread_name, _hash|
-        break if i >= top_count
-        if ignore
-          next if idle_thread?(thread_name, _hash)
-        end
-        block.call(thread_name, _hash)
-        i += 1
-      end
-    end
-
-    def idle_thread?(thread_name, data)
-      idle = false
-      if SKIPPED_THREADS.include?(thread_name)
-        # these are likely JVM dependent
-        idle = true
-      elsif thread_name.match(/Ruby-\d+-JIT-\d+/)
-        # This are internal JRuby JIT threads, 
-        # see java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor for details.
-        idle = true
-      elsif thread_name.match(/pool-\d+-thread-\d+/)
-        # This are threads used by the internal JRuby implementation to dispatch
-        # calls and tasks, see prg.jruby.internal.runtime.methods.DynamicMethod.call
-        idle = true
-      else
-        data["thread.stacktrace"].each do |trace|
-          if trace.start_with?("java.util.concurrent.ThreadPoolExecutor.getTask")
-            idle = true
-            break
-          end
-        end
-      end
-      idle
-    end
-
-    def hostname
-      @cmd.hostname
-    end
-
-    def cpu_time_as_percent(hash)
-      (((cpu_time(hash) / @cmd.uptime * 1.0)*10000).to_i)/100.0
-    end
-
-    def cpu_time(hash)
-      hash["cpu.time"] / 1000000.0
-    end
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/stats/memory_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/stats/memory_command.rb
deleted file mode 100644
index b6aa34f5d42..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/stats/memory_command.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "app/command"
-require 'monitoring'
-
-class LogStash::Api::JvmMemoryCommand < LogStash::Api::Command
-
-  def run
-    memory = LogStash::Json.load(service.get(:jvm_memory_stats))
-    {
-      :heap_used_in_bytes => memory["heap"]["used_in_bytes"],
-      :heap_used_percent => memory["heap"]["used_percent"],
-      :heap_committed_in_bytes => memory["heap"]["committed_in_bytes"],
-      :heap_max_in_bytes => memory["heap"]["max_in_bytes"],
-      :heap_used_in_bytes => memory["heap"]["used_in_bytes"],
-      :non_heap_used_in_bytes => memory["non_heap"]["used_in_bytes"],
-      :non_heap_committed_in_bytes => memory["non_heap"]["committed_in_bytes"],
-      :pools => memory["pools"].inject({}) do |acc, (type, hash)|
-          hash.delete("committed_in_bytes")
-          acc[type] = hash
-          acc
-    end
-    }
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/system/basicinfo_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/system/basicinfo_command.rb
deleted file mode 100644
index 0822f54fb6a..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/system/basicinfo_command.rb
+++ /dev/null
@@ -1,15 +0,0 @@
-# encoding: utf-8
-require "app/command"
-require "logstash/util/duration_formatter"
-
-class LogStash::Api::SystemBasicInfoCommand < LogStash::Api::Command
-
-  def run
-    {
-      "hostname" => hostname,
-      "version" => {
-        "number" => LOGSTASH_VERSION
-      }
-    }
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/commands/system/plugins_command.rb b/logstash-core/lib/logstash/api/lib/app/commands/system/plugins_command.rb
deleted file mode 100644
index 07623283ecc..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/commands/system/plugins_command.rb
+++ /dev/null
@@ -1,28 +0,0 @@
-# encoding: utf-8
-require "app/command"
-
-class LogStash::Api::PluginsCommand < LogStash::Api::Command
-
-  def run
-    { :total => plugins.count, :plugins => plugins }
-  end
-
-  private
-
-  def plugins
-    @plugins ||= find_plugins_gem_specs.map do |spec|
-      { :name => spec.name, :version => spec.version.to_s }
-    end.sort_by do |spec|
-      spec[:name]
-    end
-  end
-
-  def find_plugins_gem_specs
-    @specs ||= Gem::Specification.find_all.select{|spec| logstash_plugin_gem_spec?(spec)}
-  end
-
-  def logstash_plugin_gem_spec?(spec)
-    spec.metadata && spec.metadata["logstash_plugin"] == "true"
-  end
-
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/node.rb b/logstash-core/lib/logstash/api/lib/app/modules/node.rb
deleted file mode 100644
index 3edfb0de5a1..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/node.rb
+++ /dev/null
@@ -1,25 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Node < BaseApp
-
-    helpers AppHelpers
-
-    # return hot threads information
-    get "/hot_threads" do
-      ignore_idle_threads = params["ignore_idle_threads"] || true
-
-      options = {
-        :ignore_idle_threads => as_boolean(ignore_idle_threads),
-        :human => params.has_key?("human")
-      }
-      options[:threads] = params["threads"].to_i if params.has_key?("threads")
-
-      command = factory.build(:hot_threads_command)
-      as    = options[:human] ? :string : :json
-      respond_with(command.run(options), {:as => as})
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/node_stats.rb b/logstash-core/lib/logstash/api/lib/app/modules/node_stats.rb
deleted file mode 100644
index 8317cad3369..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/node_stats.rb
+++ /dev/null
@@ -1,51 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class NodeStats < BaseApp
-
-    helpers AppHelpers
-
-
-    # Global _stats resource where all information is 
-    # retrieved and show
-    get "/" do
-      events_command = factory.build(:events_command)
-      payload = {
-        :events => events_command.run,
-        :jvm => jvm_payload
-      }
-
-      respond_with payload
-    end
-
-    # Show all events stats information
-    # (for ingested, emitted, dropped)
-    # - #events since startup
-    # - #data (bytes) since startup
-    # - events/s
-    # - bytes/s
-    # - dropped events/s
-    # - events in the pipeline
-    get "/events" do
-      command = factory.build(:events_command)
-      respond_with({ :events => command.run })
-    end
-
-    # return hot threads information
-    get "/jvm" do
-      respond_with jvm_payload
-    end
-
-    private
-
-    def jvm_payload
-      command = factory.build(:memory_command)
-      {
-        :timestamp => command.started_at,
-        :uptime_in_millis => command.uptime,
-        :mem => command.run
-      }
-    end
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/plugins.rb b/logstash-core/lib/logstash/api/lib/app/modules/plugins.rb
deleted file mode 100644
index 93a94bf76c3..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/plugins.rb
+++ /dev/null
@@ -1,15 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Plugins < BaseApp
-
-    helpers AppHelpers
-
-    get "/" do
-      command = factory.build(:plugins_command)
-      respond_with(command.run())
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/modules/stats.rb b/logstash-core/lib/logstash/api/lib/app/modules/stats.rb
deleted file mode 100644
index ed3aa54f789..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/modules/stats.rb
+++ /dev/null
@@ -1,21 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Stats < BaseApp
-
-    helpers AppHelpers
-
-    # return hot threads information
-    get "/jvm" do
-      command = factory.build(:memory_command)
-      jvm_payload = {
-        :timestamp => command.started_at,
-        :uptime_in_millis => command.uptime,
-        :mem => command.run
-      }
-      respond_with({:jvm => jvm_payload})
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/root.rb b/logstash-core/lib/logstash/api/lib/app/root.rb
deleted file mode 100644
index 75a0ba6be67..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/root.rb
+++ /dev/null
@@ -1,13 +0,0 @@
-# encoding: utf-8
-require "app"
-
-module LogStash::Api
-  class Root < BaseApp
-
-    get "/" do
-      command = factory.build(:system_basic_info)
-      respond_with command.run
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/service.rb b/logstash-core/lib/logstash/api/lib/app/service.rb
deleted file mode 100644
index 4b63593c18a..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/service.rb
+++ /dev/null
@@ -1,61 +0,0 @@
-# encoding: utf-8
-require "logstash/instrument/collector"
-require "logstash/util/loggable"
-
-class LogStash::Api::Service
-
-  include Singleton
-  include LogStash::Util::Loggable
-
-  def initialize
-    @snapshot_rotation_mutex = Mutex.new
-    @snapshot = nil
-    logger.debug("[api-service] start") if logger.debug?
-    LogStash::Instrument::Collector.instance.add_observer(self)
-  end
-
-  def stop
-    logger.debug("[api-service] stop") if logger.debug?
-    LogStash::Instrument::Collector.instance.delete_observer(self)
-  end
-
-  def agent
-    LogStash::Instrument::Collector.instance.agent
-  end
-
-  def started?
-    !@snapshot.nil? && has_counters?
-  end
-
-  def update(snapshot)
-    logger.debug("[api-service] snapshot received", :snapshot => snapshot) if logger.debug?
-    if @snapshot_rotation_mutex.try_lock
-      @snapshot = snapshot
-      @snapshot_rotation_mutex.unlock
-    end
-  end
-
-  def get(key)
-    metric_store = @snapshot.metric_store
-    if key == :jvm_memory_stats
-      data = metric_store.get_with_path("jvm/memory")[:jvm][:memory]
-    else
-      data = metric_store.get_with_path("stats/events")
-    end
-    LogStash::Json.dump(data)
-  end
-
-  private
-
-  def has_counters?
-    (["LogStash::Instrument::MetricType::Counter", "LogStash::Instrument::MetricType::Gauge"] - metric_types).empty?
-  end
-
-  def metric_types
-    types = []
-    @snapshot_rotation_mutex.synchronize do
-      types = @snapshot.metric_store.all.map { |t| t.class.to_s }
-    end
-    return types
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/app/stats.rb b/logstash-core/lib/logstash/api/lib/app/stats.rb
deleted file mode 100644
index 2d3f9a4f08b..00000000000
--- a/logstash-core/lib/logstash/api/lib/app/stats.rb
+++ /dev/null
@@ -1,56 +0,0 @@
-# encoding: utf-8
-require "app"
-require "app/stats/events_command"
-require "app/stats/hotthreads_command"
-
-module LogStash::Api
-  class Stats < BaseApp
-
-    helpers AppHelpers
-
-
-    # Global _stats resource where all information is 
-    # retrieved and show
-    get "/" do
-      events_command = factory.build(:events_command)
-      memory_command = factory.build(:memory_command)
-      payload = {
-        :events => events_command.run,
-        :jvm => { :memory => memory_command.run }
-      }
-      respond_with payload
-    end
-
-    # Show all events stats information
-    # (for ingested, emitted, dropped)
-    # - #events since startup
-    # - #data (bytes) since startup
-    # - events/s
-    # - bytes/s
-    # - dropped events/s
-    # - events in the pipeline
-    get "/events" do
-      command = factory.build(:events_command)
-      respond_with({ :events => command.run })
-    end
-
-    # return hot threads information
-    get "/jvm/hot_threads" do
-      top_threads_count = params["threads"] || 3
-      ignore_idle_threads = params["ignore_idle_threads"] || true
-      options = {
-        :threads => top_threads_count.to_i,
-        :ignore_idle_threads => as_boolean(ignore_idle_threads)
-      }
-      command = factory.build(:hot_threads_command)
-      respond_with(command.run(options), :string)
-    end
-
-    # return hot threads information
-    get "/jvm/memory" do
-      command = factory.build(:memory_command)
-      respond_with({ :memory => command.run })
-    end
-
-  end
-end
diff --git a/logstash-core/lib/logstash/api/lib/helpers/app_helpers.rb b/logstash-core/lib/logstash/api/lib/helpers/app_helpers.rb
deleted file mode 100644
index cd872edc51d..00000000000
--- a/logstash-core/lib/logstash/api/lib/helpers/app_helpers.rb
+++ /dev/null
@@ -1,23 +0,0 @@
-# encoding: utf-8
-require "logstash/json"
-
-module LogStash::Api::AppHelpers
-
-  def respond_with(data, options={})
-    as     = options.fetch(:as, :json)
-    pretty = params.has_key?("pretty")
-    if as == :json
-      content_type "application/json"
-      LogStash::Json.dump(data, {:pretty => pretty})
-    else
-      content_type "text/plain"
-      data.to_s
-    end
-  end
-
-  def as_boolean(string)
-    return true   if string == true   || string =~ (/(true|t|yes|y|1)$/i)
-    return false  if string == false  || string.blank? || string =~ (/(false|f|no|n|0)$/i)
-    raise ArgumentError.new("invalid value for Boolean: \"#{string}\"")
-  end
-end
diff --git a/logstash-core/lib/logstash/api/modules/base.rb b/logstash-core/lib/logstash/api/modules/base.rb
new file mode 100644
index 00000000000..9e84dc91811
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/base.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+require "logstash/api/app_helpers"
+require "logstash/api/command_factory"
+require "logstash/api/errors"
+
+module LogStash
+  module Api
+    module Modules
+      class Base < ::Sinatra::Base
+
+        helpers AppHelpers
+
+        # These options never change
+        # Sinatra isn't good at letting you change internal settings at runtime
+        # which is a requirement. We always propagate errors up and catch them
+        # in a custom rack handler in the RackApp class
+        set :environment, :production
+        set :raise_errors, true
+        set :show_exceptions, false
+
+        attr_reader :factory, :agent
+
+        include LogStash::Util::Loggable
+
+        helpers AppHelpers
+
+        def initialize(app=nil, agent)
+          super(app)
+          @agent = agent
+          @factory = ::LogStash::Api::CommandFactory.new(LogStash::Api::Service.new(agent))
+        end
+
+        not_found do
+          # We cannot raise here because it wont be catched by the `error` handler.
+          # So we manually create a new instance of NotFound and just pass it down.
+          respond_with(NotFoundError.new)
+        end
+
+        # This allow to have custom exception but keep a consistent
+        # format to report them.
+        error ApiError do |error|
+          respond_with(error)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/logging.rb b/logstash-core/lib/logstash/api/modules/logging.rb
new file mode 100644
index 00000000000..d18edd4e8d5
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/logging.rb
@@ -0,0 +1,52 @@
+# encoding: utf-8
+#
+java_import org.apache.logging.log4j.core.LoggerContext
+
+module LogStash
+  module Api
+    module Modules
+      class Logging < ::LogStash::Api::Modules::Base
+        # retrieve logging specific parameters from the provided settings
+        #
+        # return any unused configurations
+        def handle_logging(settings)
+          Hash[settings.map do |key, level|
+            if key.start_with?("logger.")
+              _, path = key.split("logger.")
+              LogStash::Logging::Logger::configure_logging(level, path)
+              nil
+            else
+              [key, level]
+            end
+          end]
+        end
+
+        put "/" do
+          begin
+            request.body.rewind
+            req_body = LogStash::Json.load(request.body.read)
+            remaining = handle_logging(req_body)
+            unless remaining.empty?
+              raise ArgumentError, I18n.t("logstash.web_api.logging.unrecognized_option", :option => remaining.keys.first)
+            end
+            respond_with({"acknowledged" => true})
+          rescue ArgumentError => e
+            status 400
+            respond_with({"error" => e.message})
+          end
+        end
+
+        get "/" do
+          context = LogStash::Logging::Logger::get_logging_context
+          if context.nil?
+            status 500
+            respond_with({"error" => "Logstash loggers were not initialized properly"})
+          else
+            loggers = context.getLoggers.map { |lgr| [lgr.getName, lgr.getLevel.name] }.sort
+            respond_with({"loggers" => Hash[loggers]})
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/node.rb b/logstash-core/lib/logstash/api/modules/node.rb
new file mode 100644
index 00000000000..32bf09149fa
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/node.rb
@@ -0,0 +1,36 @@
+# encoding: utf-8
+require "logstash/api/modules/base"
+require "logstash/api/errors"
+
+module LogStash
+  module Api
+    module Modules
+      class Node < ::LogStash::Api::Modules::Base
+        def node
+          factory.build(:node)
+        end
+
+        get "/hot_threads" do
+          ignore_idle_threads = params["ignore_idle_threads"] || true
+
+          options = { :ignore_idle_threads => as_boolean(ignore_idle_threads) }
+          options[:threads] = params["threads"].to_i if params.has_key?("threads")
+
+          as = human? ? :string : :json
+          respond_with(node.hot_threads(options), {:as => as})
+        end
+
+         get "/?:filter?" do
+           selected_fields = extract_fields(params["filter"].to_s.strip)
+           values = node.all(selected_fields)
+
+           if values.size == 0
+             raise NotFoundError
+           else
+             respond_with(values)
+           end
+         end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/node_stats.rb b/logstash-core/lib/logstash/api/modules/node_stats.rb
new file mode 100644
index 00000000000..239aaa87cd1
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/node_stats.rb
@@ -0,0 +1,44 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class NodeStats < ::LogStash::Api::Modules::Base
+
+        before do
+          @stats = factory.build(:stats)
+        end
+
+        get "/?:filter?" do
+          payload = {
+            :jvm => jvm_payload,
+            :process => process_payload,
+            :pipeline => pipeline_payload,
+          }
+          respond_with(payload, {:filter => params["filter"]})
+        end
+
+        private
+
+        def events_payload
+          @stats.events
+        end
+
+        def jvm_payload
+          @stats.jvm
+        end
+
+        def process_payload
+          @stats.process
+        end
+
+        def mem_payload
+          @stats.memory
+        end
+
+        def pipeline_payload
+          @stats.pipeline
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/plugins.rb b/logstash-core/lib/logstash/api/modules/plugins.rb
new file mode 100644
index 00000000000..7edd3da154a
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/plugins.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Plugins < ::LogStash::Api::Modules::Base
+
+        get "/" do
+          command = factory.build(:plugins_command)
+          respond_with(command.run())
+        end
+
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/root.rb b/logstash-core/lib/logstash/api/modules/root.rb
new file mode 100644
index 00000000000..10a414187ac
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/root.rb
@@ -0,0 +1,13 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Root < ::LogStash::Api::Modules::Base
+        get "/" do
+          command = factory.build(:system_basic_info)
+          respond_with command.run
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/modules/stats.rb b/logstash-core/lib/logstash/api/modules/stats.rb
new file mode 100644
index 00000000000..eee3d0b8b65
--- /dev/null
+++ b/logstash-core/lib/logstash/api/modules/stats.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+module LogStash
+  module Api
+    module Modules
+      class Stats < ::LogStash::Api::Modules::Base
+        def stats_command
+          factory.build(:stats)
+        end
+
+        # return hot threads information
+        get "/jvm/hot_threads" do
+          top_threads_count = params["threads"] || 3
+          ignore_idle_threads = params["ignore_idle_threads"] || true
+          options = {
+            :threads => top_threads_count.to_i,
+            :ignore_idle_threads => as_boolean(ignore_idle_threads)
+          }
+
+          respond_with(stats_command.hot_threads(options))
+        end
+
+        # return hot threads information
+        get "/jvm/memory" do
+          respond_with({ :memory => stats_command.memory })
+        end
+
+        get "/?:filter?" do
+          payload = {
+            :events => stats_command.events,
+            :jvm => {
+              :timestamp => stats_command.started_at,
+              :uptime_in_millis => stats_command.uptime,
+              :memory => stats_command.memory
+            }
+          }
+          respond_with(payload, {:filter => params["filter"]})
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/rack_app.rb b/logstash-core/lib/logstash/api/rack_app.rb
new file mode 100644
index 00000000000..79965e431ed
--- /dev/null
+++ b/logstash-core/lib/logstash/api/rack_app.rb
@@ -0,0 +1,113 @@
+require "sinatra"
+require "rack"
+require "logstash/api/modules/base"
+require "logstash/api/modules/node"
+require "logstash/api/modules/node_stats"
+require "logstash/api/modules/plugins"
+require "logstash/api/modules/root"
+require "logstash/api/modules/logging"
+require "logstash/api/modules/stats"
+
+module LogStash
+  module Api
+    module RackApp
+      METADATA_FIELDS = [:request_method, :path_info, :query_string, :http_version, :http_accept].freeze
+      def self.log_metadata(status, env)
+        METADATA_FIELDS.reduce({:status => status}) do |acc, field|
+          acc[field] = env[field.to_s.upcase]
+          acc
+        end
+      end
+
+      class ApiLogger
+        LOG_MESSAGE = "API HTTP Request".freeze
+
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          res = @app.call(env)
+          status, headers, body = res
+
+          if fatal_error?(status)
+            @logger.error? && @logger.error(LOG_MESSAGE, RackApp.log_metadata(status, env))
+          else
+            @logger.debug? && @logger.debug(LOG_MESSAGE, RackApp.log_metadata(status, env))
+          end
+
+          res
+        end
+
+        def fatal_error?(status)
+          status >= 500 && status < 600
+        end
+      end
+
+      class ApiErrorHandler
+        LOG_MESSAGE = "Internal API server error".freeze
+
+        def initialize(app, logger)
+          @app = app
+          @logger = logger
+        end
+
+        def call(env)
+          @app.call(env)
+        rescue => e
+          body = RackApp.log_metadata(500, env).
+                   merge({
+                           :error => "Unexpected Internal Error",
+                           :class => e.class.name,
+                           :message => e.message,
+                           :backtrace => e.backtrace
+                         })
+
+          @logger.error(LOG_MESSAGE, body)
+
+          [500,
+           {'Content-Type' => 'application/json'},
+           [LogStash::Json.dump(body)]
+          ]
+        end
+      end
+
+      def self.app(logger, agent, environment)
+        namespaces = rack_namespaces(agent)
+
+        Rack::Builder.new do
+          # Custom logger object. Rack CommonLogger does not work with cabin
+          use ApiLogger, logger
+
+          # In test env we want errors to propogate up the chain
+          # so we get easy to understand test failures.
+          # In production / dev we don't want a bad API endpoint
+          # to crash the process
+          if environment != "test"
+            use ApiErrorHandler, logger
+          end
+
+          run LogStash::Api::Modules::Root.new(nil, agent)
+          namespaces.each_pair do |namespace, app|
+            map(namespace) do
+              # Pass down a reference to the current agent
+              # This allow the API to have direct access to the collector
+              run app.new(nil, agent)
+            end
+          end
+        end
+      end
+
+      def self.rack_namespaces(agent)
+        {
+          "/_node" => LogStash::Api::Modules::Node,
+          "/_stats" => LogStash::Api::Modules::Stats,
+          "/_node/stats" => LogStash::Api::Modules::NodeStats,
+          "/_node/plugins" => LogStash::Api::Modules::Plugins,
+          "/_node/logging" => LogStash::Api::Modules::Logging
+        }
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/api/service.rb b/logstash-core/lib/logstash/api/service.rb
new file mode 100644
index 00000000000..32563fc994e
--- /dev/null
+++ b/logstash-core/lib/logstash/api/service.rb
@@ -0,0 +1,34 @@
+# encoding: utf-8
+require "logstash/instrument/collector"
+require "logstash/util/loggable"
+
+module LogStash
+  module Api
+    class Service
+      include LogStash::Util::Loggable
+
+      attr_reader :agent
+
+      def initialize(agent)
+        @agent = agent
+        logger.debug("[api-service] start") if logger.debug?
+      end
+
+      def started?
+        true
+      end
+
+      def snapshot
+        agent.metric.collector.snapshot_metric
+      end
+
+      def get_shallow(*path)
+        snapshot.metric_store.get_shallow(*path)
+      end
+
+      def extract_metrics(path, *keys)
+        snapshot.metric_store.extract_metrics(path, *keys)
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/build.rb b/logstash-core/lib/logstash/build.rb
new file mode 100644
index 00000000000..fb2132c22c4
--- /dev/null
+++ b/logstash-core/lib/logstash/build.rb
@@ -0,0 +1,6 @@
+# encoding: utf-8
+
+# DO NOT EDIT
+# this file acts as a placeholder for build information when executing
+# logstash in dev mode (outside of a package build)
+BUILD_INFO = {}
diff --git a/logstash-core/lib/logstash/codecs/base.rb b/logstash-core/lib/logstash/codecs/base.rb
index 4d10950f534..cf8b582d9d5 100644
--- a/logstash-core/lib/logstash/codecs/base.rb
+++ b/logstash-core/lib/logstash/codecs/base.rb
@@ -7,12 +7,18 @@
 # This is the base class for logstash codecs.
 module LogStash::Codecs; class Base < LogStash::Plugin
   include LogStash::Config::Mixin
+
   config_name "codec"
 
+  def self.plugin_type
+    "codec"
+  end
+
   def initialize(params={})
     super
     config_init(@params)
     register if respond_to?(:register)
+    setup_multi_encode!
   end
 
   public
@@ -23,10 +29,37 @@ def decode(data)
   alias_method :<<, :decode
 
   public
+  # DEPRECATED: Prefer defining encode_sync or multi_encode
   def encode(event)
-    raise "#{self.class}#encode must be overidden"
+    encoded = multi_encode([event])
+    encoded.each {|event,data| @on_event.call(event,data) }
   end # def encode
 
+  public
+  # Relies on the codec being synchronous (which they all are!)
+  # We need a better long term design here, but this is an improvement
+  # over the current API for shared plugins
+  # It is best if the codec implements this directly
+  def multi_encode(events)
+    if @has_encode_sync              
+      events.map {|event| [event, self.encode_sync(event)]}
+    else
+      batch = Thread.current[:logstash_output_codec_batch] ||= []
+      batch.clear
+      
+      events.each {|event| self.encode(event) }
+      batch
+    end
+  end
+
+  def setup_multi_encode!
+    @has_encode_sync = self.methods.include?(:encode_sync)
+
+    on_event do |event, data|
+      Thread.current[:logstash_output_codec_batch] << [event, data]
+    end
+  end
+
   public
   def close; end;
 
diff --git a/logstash-core/lib/logstash/config/config_ast.rb b/logstash-core/lib/logstash/config/config_ast.rb
index 41ec1c599a1..ce6295a6933 100644
--- a/logstash-core/lib/logstash/config/config_ast.rb
+++ b/logstash-core/lib/logstash/config/config_ast.rb
@@ -76,6 +76,14 @@ def self.defered_conditionals_index=(val)
     @defered_conditionals_index = val
   end
 
+  def self.plugin_instance_index
+    @plugin_instance_index
+  end
+
+  def self.plugin_instance_index=(val)
+    @plugin_instance_index = val
+  end
+
   class Node < Treetop::Runtime::SyntaxNode
     def text_value_for_comments
       text_value.gsub(/[\r\n]/, " ")
@@ -86,6 +94,7 @@ class Config < Node
     def compile
       LogStash::Config::AST.defered_conditionals = []
       LogStash::Config::AST.defered_conditionals_index = 0
+      LogStash::Config::AST.plugin_instance_index = 0
       code = []
 
       code << <<-CODE
@@ -94,6 +103,7 @@ def compile
         @outputs = []
         @periodic_flushers = []
         @shutdown_flushers = []
+        @generated_objects = {}
       CODE
 
       sections = recursive_select(LogStash::Config::AST::PluginSection)
@@ -113,7 +123,7 @@ def compile
         definitions << "define_singleton_method :#{type}_func do |event|"
         definitions << "  targeted_outputs = []" if type == "output"
         definitions << "  events = [event]" if type == "filter"
-        definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", :event => event.to_hash)"
+        definitions << "  @logger.debug? && @logger.debug(\"#{type} received\", \"event\" => event.to_hash)"
 
         sections.select { |s| s.plugin_type.text_value == type }.each do |s|
           definitions << s.compile.split("\n", -1).map { |e| "  #{e}" }
@@ -137,7 +147,9 @@ class Whitespace < Node; end
   class PluginSection < Node
     # Global plugin numbering for the janky instance variable naming we use
     # like @filter_<name>_1
-    @@i = 0
+    def initialize(*args)
+      super(*args)
+    end
 
     # Generate ruby code to initialize all the plugins.
     def compile_initializer
@@ -147,31 +159,31 @@ def compile_initializer
 
 
         code << <<-CODE
-          #{name} = #{plugin.compile_initializer}
-          @#{plugin.plugin_type}s << #{name}
+          @generated_objects[:#{name}] = #{plugin.compile_initializer}
+          @#{plugin.plugin_type}s << @generated_objects[:#{name}]
         CODE
 
         # The flush method for this filter.
         if plugin.plugin_type == "filter"
 
           code << <<-CODE
-            #{name}_flush = lambda do |options, &block|
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name})
+            @generated_objects[:#{name}_flush] = lambda do |options, &block|
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}])
 
-              events = #{name}.flush(options)
+              events = @generated_objects[:#{name}].flush(options)
 
               return if events.nil? || events.empty?
 
-              @logger.debug? && @logger.debug(\"Flushing\", :plugin => #{name}, :events => events)
+              @logger.debug? && @logger.debug(\"Flushing\", :plugin => @generated_objects[:#{name}], :events => events.map { |x| x.to_hash  })
 
               #{plugin.compile_starting_here.gsub(/^/, "  ")}
 
               events.each{|e| block.call(e)}
             end
 
-            if #{name}.respond_to?(:flush)
-              @periodic_flushers << #{name}_flush if #{name}.periodic_flush
-              @shutdown_flushers << #{name}_flush
+            if @generated_objects[:#{name}].respond_to?(:flush)
+              @periodic_flushers << @generated_objects[:#{name}_flush] if @generated_objects[:#{name}].periodic_flush
+              @shutdown_flushers << @generated_objects[:#{name}_flush]
             end
           CODE
 
@@ -192,9 +204,10 @@ def generate_variables
 
       plugins.each do |plugin|
         # Unique number for every plugin.
-        @@i += 1
+        LogStash::Config::AST.plugin_instance_index += 1
         # store things as ivars, like @filter_grok_3
-        var = "@#{plugin.plugin_type}_#{plugin.plugin_name}_#{@@i}"
+        var = :"#{plugin.plugin_type}_#{plugin.plugin_name}_#{LogStash::Config::AST.plugin_instance_index}"
+        # puts("var=#{var.inspect}")
         @variables[plugin] = var
       end
       return @variables
@@ -236,13 +249,13 @@ def compile_initializer
     def compile
       case plugin_type
       when "input"
-        return "start_input(#{variable_name})"
+        return "start_input(@generated_objects[:#{variable_name}])"
       when "filter"
         return <<-CODE
-          events = #{variable_name}.multi_filter(events)
+          events = @generated_objects[:#{variable_name}].multi_filter(events)
         CODE
       when "output"
-        return "targeted_outputs << #{variable_name}\n"
+        return "targeted_outputs << @generated_objects[:#{variable_name}]\n"
       when "codec"
         settings = attributes.recursive_select(Attribute).collect(&:compile).reject(&:empty?)
         attributes_code = "LogStash::Util.hash_merge_many(#{settings.map { |c| "{ #{c} }" }.join(", ")})"
@@ -391,7 +404,7 @@ def compile
       if type == "filter"
         i = LogStash::Config::AST.defered_conditionals_index += 1
         source = <<-CODE
-          def cond_func_#{i}(input_events)
+          @generated_objects[:cond_func_#{i}] = lambda do |input_events|
             result = []
             input_events.each do |event|
               events = [event]
@@ -405,7 +418,7 @@ def cond_func_#{i}(input_events)
         LogStash::Config::AST.defered_conditionals << source
 
         <<-CODE
-          events = cond_func_#{i}(events)
+          events = @generated_objects[:cond_func_#{i}].call(events)
         CODE
       else # Output
         <<-CODE
@@ -512,7 +525,7 @@ def compile
   end
   class Selector < RValue
     def compile
-      return "event[#{text_value.inspect}]"
+      return "event.get(#{text_value.inspect})"
     end
   end
   class SelectorElement < Node; end
diff --git a/logstash-core/lib/logstash/config/file.rb b/logstash-core/lib/logstash/config/file.rb
index fb0d939dfde..b0be8d0e81c 100644
--- a/logstash-core/lib/logstash/config/file.rb
+++ b/logstash-core/lib/logstash/config/file.rb
@@ -8,11 +8,10 @@
 
 class LogStash::Config::File
   include Enumerable
-  attr_accessor :logger
+  include LogStash::Util::Loggable
 
   public
   def initialize(text)
-    @logger = Cabin::Channel.get(LogStash)
     @text = text
     @config = parse(text)
   end # def initialize
diff --git a/logstash-core/lib/logstash/config/loader.rb b/logstash-core/lib/logstash/config/loader.rb
index 37179518ed5..b260830d741 100644
--- a/logstash-core/lib/logstash/config/loader.rb
+++ b/logstash-core/lib/logstash/config/loader.rb
@@ -3,6 +3,7 @@
 module LogStash; module Config; class Loader
   def initialize(logger)
     @logger = logger
+    @config_debug = LogStash::SETTINGS.get_value("config.debug")
   end
 
   def format_config(config_path, config_string)
@@ -69,14 +70,18 @@ def local_config(path)
         encoding_issue_files << file
       end
       config << cfg + "\n"
-      @logger.debug? && @logger.debug("\nThe following is the content of a file", :config_file => file.to_s)
-      @logger.debug? && @logger.debug("\n" + cfg + "\n\n")
+      if @config_debug
+        @logger.debug? && @logger.debug("\nThe following is the content of a file", :config_file => file.to_s)
+        @logger.debug? && @logger.debug("\n" + cfg + "\n\n")
+      end
     end
     if encoding_issue_files.any?
       fail("The following config files contains non-ascii characters but are not UTF-8 encoded #{encoding_issue_files}")
     end
-    @logger.debug? && @logger.debug("\nThe following is the merged configuration")
-    @logger.debug? && @logger.debug("\n" + config + "\n\n")
+    if @config_debug
+      @logger.debug? && @logger.debug("\nThe following is the merged configuration")
+      @logger.debug? && @logger.debug("\n" + config + "\n\n")
+    end
     return config
   end # def load_config
 
diff --git a/logstash-core/lib/logstash/config/mixin.rb b/logstash-core/lib/logstash/config/mixin.rb
index 1d152062222..70ad888c74c 100644
--- a/logstash-core/lib/logstash/config/mixin.rb
+++ b/logstash-core/lib/logstash/config/mixin.rb
@@ -1,8 +1,10 @@
 # encoding: utf-8
 require "logstash/namespace"
 require "logstash/config/registry"
+require "logstash/plugins/registry"
 require "logstash/logging"
 require "logstash/util/password"
+require "logstash/util/safe_uri"
 require "logstash/version"
 require "logstash/environment"
 require "logstash/util/plugin_version"
@@ -38,7 +40,7 @@ module LogStash::Config::Mixin
   PLUGIN_VERSION_1_0_0 = LogStash::Util::PluginVersion.new(1, 0, 0)
   PLUGIN_VERSION_0_9_0 = LogStash::Util::PluginVersion.new(0, 9, 0)
 
-  ENV_PLACEHOLDER_REGEX = /\$(?<name>\w+)|\$\{(?<name>\w+)(\:(?<default>[^}]*))?\}/
+  ENV_PLACEHOLDER_REGEX = /\$\{(?<name>\w+)(\:(?<default>[^}]*))?\}/
 
   # This method is called when someone does 'include LogStash::Config'
   def self.included(base)
@@ -53,7 +55,7 @@ def config_init(params)
     # Keep a copy of the original config params so that we can later
     # differentiate between explicit configuration and implicit (default)
     # configuration.
-    @original_params = params.clone
+    original_params = params.clone
     
     # store the plugin type, turns LogStash::Inputs::Base into 'input'
     @plugin_type = self.class.ancestors.find { |a| a.name =~ /::Base$/ }.config_name
@@ -64,7 +66,7 @@ def config_init(params)
       if opts && opts[:deprecated]
         extra = opts[:deprecated].is_a?(String) ? opts[:deprecated] : ""
         extra.gsub!("%PLUGIN%", self.class.config_name)
-        @logger.warn("You are using a deprecated config setting " +
+        self.logger.warn("You are using a deprecated config setting " +
                      "#{name.inspect} set in #{self.class.config_name}. " +
                      "Deprecated settings will continue to work, " +
                      "but are scheduled for removal from logstash " +
@@ -76,7 +78,7 @@ def config_init(params)
         extra = opts[:obsolete].is_a?(String) ? opts[:obsolete] : ""
         extra.gsub!("%PLUGIN%", self.class.config_name)
         raise LogStash::ConfigurationError,
-          I18n.t("logstash.agent.configuration.obsolete", :name => name,
+          I18n.t("logstash.runner.configuration.obsolete", :name => name,
                  :plugin => self.class.config_name, :extra => extra)
       end
     end
@@ -118,6 +120,7 @@ def config_init(params)
       end
     end
 
+
     if !self.class.validate(params)
       raise LogStash::ConfigurationError,
         I18n.t("logstash.runner.configuration.invalid_plugin_settings")
@@ -138,10 +141,15 @@ def config_init(params)
       next if key[0, 1] == "@"
 
       # Set this key as an instance variable only if it doesn't start with an '@'
-      @logger.debug("config #{self.class.name}/@#{key} = #{value.inspect}")
+      self.logger.debug("config #{self.class.name}/@#{key} = #{value.inspect}")
       instance_variable_set("@#{key}", value)
     end
 
+    # now that we know the parameters are valid, we can obfuscate the original copy
+    # of the parameters before storing them as an instance variable
+    self.class.secure_params!(original_params)
+    @original_params = original_params
+
     @config = params
   end # def config_init
 
@@ -149,7 +157,6 @@ def config_init(params)
   # Process following patterns : $VAR, ${VAR}, ${VAR:defaultValue}
   def replace_env_placeholders(value)
     return value unless value.is_a?(String)
-    #raise ArgumentError, "Cannot replace ENV placeholders on non-strings. Got #{value.class}" if !value.is_a?(String)
 
     value.gsub(ENV_PLACEHOLDER_REGEX) do |placeholder|
       # Note: Ruby docs claim[1] Regexp.last_match is thread-local and scoped to
@@ -163,7 +170,6 @@ def replace_env_placeholders(value)
       if replacement.nil?
         raise LogStash::ConfigurationError, "Cannot evaluate `#{placeholder}`. Environment variable `#{name}` is not set and there is no default value given."
       end
-      @logger.info? && @logger.info("Evaluating environment variable placeholder", :placeholder => placeholder, :replacement => replacement)
       replacement
     end
   end # def replace_env_placeholders
@@ -176,8 +182,12 @@ module DSL
     def config_name(name = nil)
       @config_name = name if !name.nil?
       LogStash::Config::Registry.registry[@config_name] = self
+      if self.respond_to?("plugin_type")
+        declare_plugin(self.plugin_type, @config_name)
+      end
       return @config_name
     end
+    alias_method :config_plugin, :config_name
 
     # Deprecated: Declare the version of the plugin
     # inside the gemspec.
@@ -188,8 +198,7 @@ def plugin_status(status = nil)
     # Deprecated: Declare the version of the plugin
     # inside the gemspec.
     def milestone(m = nil)
-      @logger = Cabin::Channel.get(LogStash)
-      @logger.debug(I18n.t('logstash.plugin.deprecated_milestone', :plugin => config_name))
+      self.logger.debug(I18n.t('logstash.plugin.deprecated_milestone', :plugin => config_name))
     end
 
     # Define a new configuration setting
@@ -199,7 +208,7 @@ def config(name, opts={})
 
       name = name.to_s if name.is_a?(Symbol)
       @config[name] = opts  # ok if this is empty
-
+      
       if name.is_a?(String)
         define_method(name) { instance_variable_get("@#{name}") }
         define_method("#{name}=") { |v| instance_variable_set("@#{name}", v) }
@@ -252,7 +261,6 @@ def inherited(subclass)
     def validate(params)
       @plugin_name = config_name
       @plugin_type = ancestors.find { |a| a.name =~ /::Base$/ }.config_name
-      @logger = Cabin::Channel.get(LogStash)
       is_valid = true
 
       print_version_notice
@@ -272,12 +280,12 @@ def print_version_notice
 
         if plugin_version < PLUGIN_VERSION_1_0_0
           if plugin_version < PLUGIN_VERSION_0_9_0
-            @logger.info(I18n.t("logstash.plugin.version.0-1-x", 
+            self.logger.info(I18n.t("logstash.plugin.version.0-1-x",
                                 :type => @plugin_type,
                                 :name => @config_name,
                                 :LOGSTASH_VERSION => LOGSTASH_VERSION))
           else
-            @logger.info(I18n.t("logstash.plugin.version.0-9-x", 
+            self.logger.info(I18n.t("logstash.plugin.version.0-9-x",
                                 :type => @plugin_type,
                                 :name => @config_name,
                                 :LOGSTASH_VERSION => LOGSTASH_VERSION))
@@ -287,7 +295,7 @@ def print_version_notice
         # If we cannot find a version in the currently installed gems we
         # will display this message. This could happen in the test, if you 
         # create an anonymous class to test a plugin.
-        @logger.warn(I18n.t("logstash.plugin.no_version",
+        self.logger.warn(I18n.t("logstash.plugin.no_version",
                                 :type => @plugin_type,
                                 :name => @config_name,
                                 :LOGSTASH_VERSION => LOGSTASH_VERSION))
@@ -311,65 +319,95 @@ def validate_check_invalid_parameter_names(params)
 
       if invalid_params.size > 0
         invalid_params.each do |name|
-          @logger.error("Unknown setting '#{name}' for #{@plugin_name}")
+          self.logger.error("Unknown setting '#{name}' for #{@plugin_name}")
         end
         return false
       end # if invalid_params.size > 0
       return true
     end # def validate_check_invalid_parameter_names
 
+    def validate_check_required_parameter(config_key, config_opts, k, v)
+      if config_key.is_a?(Regexp)
+        (k =~ config_key && v)
+      elsif config_key.is_a?(String)
+        k && v
+      end
+    end
+
     def validate_check_required_parameter_names(params)
       is_valid = true
 
       @config.each do |config_key, config|
         next unless config[:required]
 
-        if config_key.is_a?(Regexp)
-          next if params.keys.select { |k| k =~ config_key }.length > 0
-        elsif config_key.is_a?(String)
-          next if params.keys.member?(config_key)
+        if config_key.is_a?(Regexp) && !params.keys.any? { |k| k =~ config_key }
+          is_valid = false
         end
-        @logger.error(I18n.t("logstash.runner.configuration.setting_missing",
-                             :setting => config_key, :plugin => @plugin_name,
-                             :type => @plugin_type))
-        is_valid = false
+
+        value = params[config_key]
+        if value.nil? || (config[:list] && Array(value).empty?)
+          self.logger.error(I18n.t("logstash.runner.configuration.setting_missing",
+                               :setting => config_key, :plugin => @plugin_name,
+                               :type => @plugin_type))
+          is_valid = false
+        end        
       end
 
       return is_valid
     end
 
+    def process_parameter_value(value, config_settings)
+      config_val = config_settings[:validate]
+      
+      if config_settings[:list]
+        value = Array(value) # coerce scalars to lists
+        # Empty lists are converted to nils
+        return true, nil if value.empty?
+          
+        validated_items = value.map {|v| validate_value(v, config_val)}
+        is_valid = validated_items.all? {|sr| sr[0] }
+        processed_value = validated_items.map {|sr| sr[1]}
+      else
+        is_valid, processed_value = validate_value(value, config_val)
+      end
+      
+      return [is_valid, processed_value]
+    end
+
     def validate_check_parameter_values(params)
       # Filter out parametrs that match regexp keys.
       # These are defined in plugins like this:
       #   config /foo.*/ => ... 
-      is_valid = true
+      all_params_valid = true
 
       params.each do |key, value|
         @config.keys.each do |config_key|
           next unless (config_key.is_a?(Regexp) && key =~ config_key) \
                       || (config_key.is_a?(String) && key == config_key)
-          config_val = @config[config_key][:validate]
-          #puts "  Key matches."
-          success, result = validate_value(value, config_val)
-          if success 
-            # Accept coerced value if success
+
+          config_settings = @config[config_key]          
+
+          is_valid, processed_value = process_parameter_value(value, config_settings)
+          
+          if is_valid
+            # Accept coerced value if valid
             # Used for converting values in the config to proper objects.
-            params[key] = result if !result.nil?
+            params[key] = processed_value
           else
-            @logger.error(I18n.t("logstash.runner.configuration.setting_invalid",
+            self.logger.error(I18n.t("logstash.runner.configuration.setting_invalid",
                                  :plugin => @plugin_name, :type => @plugin_type,
                                  :setting => key, :value => value.inspect,
-                                 :value_type => config_val,
-                                 :note => result))
+                                 :value_type => config_settings[:validate],
+                                 :note => processed_value))
           end
-          #puts "Result: #{key} / #{result.inspect} / #{success}"
-          is_valid &&= success
+          
+          all_params_valid &&= is_valid
 
           break # done with this param key
         end # config.each
       end # params.each
 
-      return is_valid
+      return all_params_valid
     end # def validate_check_parameter_values
 
     def validator_find(key)
@@ -390,7 +428,7 @@ def validate_value(value, validator)
       result = nil
 
       if validator.nil?
-        return true
+        return true, value
       elsif validator.is_a?(Array)
         value = [*value]
         if value.size > 1
@@ -504,6 +542,12 @@ def validate_value(value, validator)
             end
 
             result = value.first.is_a?(::LogStash::Util::Password) ? value.first : ::LogStash::Util::Password.new(value.first)
+          when :uri
+            if value.size > 1
+              return false, "Expected uri (one value), got #{value.size} values?"
+            end
+            
+            result = value.first.is_a?(::LogStash::Util::SafeURI) ? value.first : ::LogStash::Util::SafeURI.new(value.first)
           when :path
             if value.size > 1 # Only 1 value wanted
               return false, "Expected path (one value), got #{value.size} values?"
@@ -537,6 +581,15 @@ def validate_value(value, validator)
       return true, result
     end # def validate_value
 
+    def secure_params!(params)
+      params.each do |key, value|
+        if [:uri, :password].include? @config[key][:validate]
+          is_valid, processed_value = process_parameter_value(value, @config[key])
+          params[key] = processed_value
+        end
+      end
+    end
+
     def hash_or_array(value)
       if !value.is_a?(Hash)
         value = [*value] # coerce scalar to array if necessary
diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index 79e7f24d86c..a918a40414b 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -1,7 +1,47 @@
 # encoding: utf-8
 require "logstash/errors"
+require "logstash/java_integration"
+require "logstash/config/cpu_core_strategy"
+require "logstash/settings"
+require "socket"
+require "stud/temporary"
 
 module LogStash
+  # In the event that we're requiring this file without bootstrap/environment.rb
+  if !defined?(LogStash::Environment::LOGSTASH_HOME)
+    module Environment
+      LOGSTASH_HOME = Stud::Temporary.directory("logstash-home")
+      Dir.mkdir(::File.join(LOGSTASH_HOME, "data"))
+    end
+  end
+
+  [
+            Setting::String.new("node.name", Socket.gethostname),
+    Setting::NullableString.new("path.config", nil, false),
+ Setting::WritableDirectory.new("path.data", ::File.join(LogStash::Environment::LOGSTASH_HOME, "data")),
+    Setting::NullableString.new("config.string", nil, false),
+           Setting::Boolean.new("config.test_and_exit", false),
+           Setting::Boolean.new("config.reload.automatic", false),
+           Setting::Numeric.new("config.reload.interval", 3), # in seconds
+           Setting::Boolean.new("metric.collect", true),
+            Setting::String.new("pipeline.id", "main"),
+   Setting::PositiveInteger.new("pipeline.workers", LogStash::Config::CpuCoreStrategy.maximum),
+   Setting::PositiveInteger.new("pipeline.output.workers", 1),
+   Setting::PositiveInteger.new("pipeline.batch.size", 125),
+           Setting::Numeric.new("pipeline.batch.delay", 5), # in milliseconds
+           Setting::Boolean.new("pipeline.unsafe_shutdown", false),
+                    Setting.new("path.plugins", Array, []),
+    Setting::NullableString.new("interactive", nil, false),
+           Setting::Boolean.new("config.debug", false),
+            Setting::String.new("log.level", "info", true, ["fatal", "error", "warn", "debug", "info", "trace"]),
+           Setting::Boolean.new("version", false),
+           Setting::Boolean.new("help", false),
+            Setting::String.new("log.format", "plain", true, ["json", "plain"]),
+            Setting::String.new("http.host", "127.0.0.1"),
+            Setting::PortRange.new("http.port", 9600..9700),
+            Setting::String.new("http.environment", "production"),
+  ].each {|setting| SETTINGS.register(setting) }
+
   module Environment
     extend self
 
diff --git a/logstash-core/lib/logstash/filter_delegator.rb b/logstash-core/lib/logstash/filter_delegator.rb
index 132d03f933e..02a4f3599ea 100644
--- a/logstash-core/lib/logstash/filter_delegator.rb
+++ b/logstash-core/lib/logstash/filter_delegator.rb
@@ -13,18 +13,18 @@ class FilterDelegator
     ]
     def_delegators :@filter, *DELEGATED_METHODS
 
-    def initialize(logger, klass, metric, *args)
-      options = args.reduce({}, :merge)
-
+    def initialize(logger, klass, metric, plugin_args)
       @logger = logger
       @klass = klass
-      @filter = klass.new(options)
+      @id = plugin_args["id"]
+      @filter = klass.new(plugin_args)
 
       # Scope the metrics to the plugin
-      namespaced_metric = metric.namespace(@filter.plugin_unique_name.to_sym)
-      @filter.metric = metric
+      namespaced_metric = metric.namespace("#{@klass.config_name}_#{@id}".to_sym)
+      @filter.metric = namespaced_metric
 
       @metric_events = namespaced_metric.namespace(:events)
+      namespaced_metric.gauge(:name, config_name)
 
       # Not all the filters will do bufferings
       define_flush_method if @filter.respond_to?(:flush)
@@ -37,7 +37,9 @@ def config_name
     def multi_filter(events)
       @metric_events.increment(:in, events.size)
 
+      clock = @metric_events.time(:duration_in_millis)
       new_events = @filter.multi_filter(events)
+      clock.stop
 
       # There is no garantee in the context of filter
       # that EVENTS_INT == EVENTS_OUT, see the aggregates and
diff --git a/logstash-core/lib/logstash/filters/base.rb b/logstash-core/lib/logstash/filters/base.rb
index ae6616ddf01..35bf49e46ab 100644
--- a/logstash-core/lib/logstash/filters/base.rb
+++ b/logstash-core/lib/logstash/filters/base.rb
@@ -7,6 +7,7 @@
 require "logstash/util/decorators"
 
 class LogStash::Filters::Base < LogStash::Plugin
+  include LogStash::Util::Loggable
   include LogStash::Config::Mixin
 
   config_name "filter"
@@ -117,6 +118,10 @@ class LogStash::Filters::Base < LogStash::Plugin
   # Optional.
   config :periodic_flush, :validate => :boolean, :default => false
 
+  def self.plugin_type
+    "filter"
+  end
+
   public
   def initialize(params)
     super
@@ -183,12 +188,12 @@ def filter_matched(event)
     # this is important because a construct like event["tags"].delete(tag) will not work
     # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
     @remove_tag.each do |tag|
-      tags = event["tags"]
+      tags = event.get("tags")
       break if tags.nil? || tags.empty?
       tag = event.sprintf(tag)
       @logger.debug? and @logger.debug("filters/#{self.class.name}: removing tag", :tag => tag)
       tags.delete(tag)
-      event["tags"] = tags
+      event.set("tags", tags)
     end
   end # def filter_matched
 
diff --git a/logstash-core/lib/logstash/inputs/base.rb b/logstash-core/lib/logstash/inputs/base.rb
index 414cd714784..ca769278893 100644
--- a/logstash-core/lib/logstash/inputs/base.rb
+++ b/logstash-core/lib/logstash/inputs/base.rb
@@ -9,7 +9,9 @@
 
 # This is the base class for Logstash inputs.
 class LogStash::Inputs::Base < LogStash::Plugin
+  include LogStash::Util::Loggable
   include LogStash::Config::Mixin
+
   config_name "input"
 
   # Add a `type` field to all events handled by this input.
@@ -48,6 +50,10 @@ class LogStash::Inputs::Base < LogStash::Plugin
   attr_accessor :params
   attr_accessor :threadable
 
+  def self.plugin_type
+    "input"
+  end
+
   public
   def initialize(params={})
     super
@@ -78,7 +84,7 @@ def stop
 
   public
   def do_stop
-    @logger.debug("stopping", :plugin => self)
+    @logger.debug("stopping", :plugin => self.class.name)
     @stop_called.make_true
     stop
   end
@@ -92,7 +98,7 @@ def stop?
   protected
   def decorate(event)
     # Only set 'type' if not already set. This is backwards-compatible behavior
-    event["type"] = @type if @type && !event.include?("type")
+    event.set("type", @type) if @type && !event.include?("type")
 
     LogStash::Util::Decorators.add_fields(@add_field,event,"inputs/#{self.class.name}")
     LogStash::Util::Decorators.add_tags(@tags,event,"inputs/#{self.class.name}")
diff --git a/logstash-core/lib/logstash/inputs/metrics.rb b/logstash-core/lib/logstash/inputs/metrics.rb
index 8a8ce92dcf0..d85bcc4e53c 100644
--- a/logstash-core/lib/logstash/inputs/metrics.rb
+++ b/logstash-core/lib/logstash/inputs/metrics.rb
@@ -21,7 +21,7 @@ def run(queue)
       @queue = queue
 
       # we register to the collector after receiving the pipeline queue
-      LogStash::Instrument::Collector.instance.add_observer(self)
+      metric.collector.add_observer(self)
 
       # Keep this plugin thread alive,
       # until we shutdown the metric pipeline
@@ -30,7 +30,7 @@ def run(queue)
 
     def stop
       @logger.debug("Metrics input: stopped")
-      LogStash::Instrument::Collector.instance.delete_observer(self)
+      metric.collector.delete_observer(self)
     end
 
     def update(snapshot)
diff --git a/logstash-core/lib/logstash/instrument/collector.rb b/logstash-core/lib/logstash/instrument/collector.rb
index 614ba372a40..c6946781fb5 100644
--- a/logstash-core/lib/logstash/instrument/collector.rb
+++ b/logstash-core/lib/logstash/instrument/collector.rb
@@ -8,7 +8,7 @@
 require "thread"
 
 module LogStash module Instrument
-  # The Collector singleton is the single point of reference for all
+  # The Collector is the single point of reference for all
   # the metrics collection inside logstash, the metrics library will make
   # direct calls to this class.
   #
@@ -17,7 +17,6 @@ module LogStash module Instrument
   class Collector
     include LogStash::Util::Loggable
     include Observable
-    include Singleton
 
     SNAPSHOT_ROTATION_TIME_SECS = 1 # seconds
     SNAPSHOT_ROTATION_TIMEOUT_INTERVAL_SECS = 10 * 60 # seconds
@@ -59,10 +58,6 @@ def push(namespaces_path, key, type, *metric_type_params)
       end
     end
 
-    def clear
-      @metric_store = MetricStore.new
-    end
-
     # Monitor the `Concurrent::TimerTask` this update is triggered on every successful or not
     # run of the task, TimerTask implement Observable and the collector acts as
     # the observer and will keep track if something went wrong in the execution.
@@ -75,7 +70,7 @@ def update(time_of_execution, result, exception)
       logger.error("Collector: Something went wrong went sending data to the observers",
                    :execution_time => time_of_execution,
                    :result => result,
-                   :exception => exception)
+                   :exception => exception.class.name)
     end
 
     # Snapshot the current Metric Store and return it immediately,
@@ -96,6 +91,10 @@ def start_periodic_snapshotting
       @snapshot_task.execute
     end
 
+    def stop
+      @snapshot_task.shutdown
+    end
+
     # Create a snapshot of the MetricStore and send it to to the registered observers
     # The observer will receive the following signature in the update methode.
     #
@@ -105,5 +104,9 @@ def publish_snapshot
       logger.debug("Collector: Sending snapshot to observers", :created_at => created_at) if logger.debug?
       notify_observers(snapshot_metric)
     end
+
+    def clear(keypath)
+      @metric_store.prune(keypath)
+    end
   end
 end; end
diff --git a/logstash-core/lib/logstash/instrument/metric.rb b/logstash-core/lib/logstash/instrument/metric.rb
index 601c7b0ed4b..6f071917a17 100644
--- a/logstash-core/lib/logstash/instrument/metric.rb
+++ b/logstash-core/lib/logstash/instrument/metric.rb
@@ -13,27 +13,27 @@ class MetricNoNamespaceProvided < MetricException; end
   class Metric
     attr_reader :collector
 
-    def initialize(collector = LogStash::Instrument::Collector.instance)
+    def initialize(collector)
       @collector = collector
     end
 
     def increment(namespace, key, value = 1)
-      validate_key!(key)
+      self.class.validate_key!(key)
       collector.push(namespace, key, :counter, :increment, value)
     end
 
     def decrement(namespace, key, value = 1)
-      validate_key!(key)
+      self.class.validate_key!(key)
       collector.push(namespace, key, :counter, :decrement, value)
     end
 
     def gauge(namespace, key, value)
-      validate_key!(key)
+      self.class.validate_key!(key)
       collector.push(namespace, key, :gauge, :set, value)
     end
 
     def time(namespace, key)
-      validate_key!(key)
+      self.class.validate_key!(key)
 
       if block_given?
         timer = TimedExecution.new(self, namespace, key)
@@ -46,7 +46,8 @@ def time(namespace, key)
     end
 
     def report_time(namespace, key, duration)
-      collector.push(namespace, key, :mean, :increment, duration)
+      self.class.validate_key!(key)
+      collector.push(namespace, key, :counter, :increment, duration)
     end
 
     # This method return a metric instance tied to a specific namespace
@@ -69,11 +70,11 @@ def namespace(name)
       NamespacedMetric.new(self, name)
     end
 
-    private
-    def validate_key!(key)
+    def self.validate_key!(key)
       raise MetricNoKeyProvided if key.nil? || key.empty?
     end
 
+    private
     # Allow to calculate the execution of a block of code.
     # This class support 2 differents syntax a block or the return of
     # the object itself, but in the later case the metric wont be recorded
@@ -81,7 +82,7 @@ def validate_key!(key)
     #
     # @see LogStash::Instrument::Metric#time
     class TimedExecution
-      MILLISECONDS = 1_000_000.0.freeze
+      MILLISECONDS = 1_000.0.freeze
 
       def initialize(metric, namespace, key)
         @metric = metric
diff --git a/logstash-core/lib/logstash/instrument/metric_store.rb b/logstash-core/lib/logstash/instrument/metric_store.rb
index 53ff0cd6668..c440e2524d2 100644
--- a/logstash-core/lib/logstash/instrument/metric_store.rb
+++ b/logstash-core/lib/logstash/instrument/metric_store.rb
@@ -2,6 +2,7 @@
 require "concurrent"
 require "logstash/event"
 require "logstash/instrument/metric_type"
+require "thread"
 
 module LogStash module Instrument
   # The Metric store the data structure that make sure the data is
@@ -25,6 +26,12 @@ def initialize
       # This hash has only one dimension
       # and allow fast retrieval of the metrics
       @fast_lookup = Concurrent::Map.new
+
+      # This Mutex block the critical section for the
+      # structured hash, it block the zone when we first insert a metric
+      # in the structured hash or when we query it for search or to make
+      # the result available in the API.
+      @structured_lookup_mutex = Mutex.new
     end
 
     # This method use the namespace and key to search the corresponding value of
@@ -46,16 +53,13 @@ def fetch_or_store(namespaces, key, default_value = nil)
       # BUT. If the value is not present in the `@fast_lookup` the value will be inserted and
       # `#puf_if_absent` will return nil. With this returned value of nil we assume that we don't
       # have it in the `@metric_store` for structured search so we add it there too.
-      #
-      # The problem with only using the `@metric_store` directly all the time would require us
-      # to use the mutex around the structure since its a multi-level hash, without that it wont
-      # return a consistent value of the metric and this would slow down the code and would
-      # complixity the code.
       if found_value = @fast_lookup.put_if_absent([namespaces, key], provided_value)
         return found_value
       else
-        # If we cannot find the value this mean we need to save it in the store.
-        fetch_or_store_namespaces(namespaces).fetch_or_store(key, provided_value)
+        @structured_lookup_mutex.synchronize do
+          # If we cannot find the value this mean we need to save it in the store.
+          fetch_or_store_namespaces(namespaces).fetch_or_store(key, provided_value)
+        end
         return provided_value
       end
     end
@@ -76,24 +80,89 @@ def fetch_or_store(namespaces, key, default_value = nil)
     # @param [Array] The path where values should be located
     # @return [Hash]
     def get_with_path(path)
-      key_paths = path.gsub(/^#{KEY_PATH_SEPARATOR}+/, "").split(KEY_PATH_SEPARATOR)
-      get(*key_paths)
+      get(*key_paths(path))
     end
 
     # Similar to `get_with_path` but use symbols instead of string
     #
-    # @param [Array<Symbol>
+    # @param [Array<Symbol>]
     # @return [Hash]
     def get(*key_paths)
       # Normalize the symbols access
       key_paths.map(&:to_sym)
       new_hash = Hash.new
 
-      get_recursively(key_paths, @store, new_hash)
+      @structured_lookup_mutex.synchronize do
+        get_recursively(key_paths, @store, new_hash)
+      end
 
       new_hash
     end
 
+    # Retrieve values like `get`, but don't return them fully nested.
+    # This means that if you call `get_shallow(:foo, :bar)` the result will not
+    # be nested inside of `{:foo {:bar => values}`.
+    #
+    # @param [Array<Symbol>]
+    # @return [Hash]
+    def get_shallow(*key_paths)
+      key_paths.reduce(get(*key_paths)) {|acc, p| acc[p]}
+    end
+
+
+    # Return a hash including the values of the keys given at the path given
+    # 
+    # Example Usage:
+    # extract_metrics(
+    #   [:jvm, :process],
+    #   :open_file_descriptors,
+    #   [:cpu, [:total_in_millis, :percent]]
+    #   [:pipelines, [:one, :two], :size]
+    # )
+    # 
+    # Returns:
+    # # From the jvm.process metrics namespace
+    # {
+    #   :open_file_descriptors => 123
+    #   :cpu => { :total_in_millis => 456, :percent => 789 }
+    #   :pipelines => {
+    #                   :one => {:size => 90210},
+    #                   :two => {:size => 8675309}
+    #                 }
+    # }
+    def extract_metrics(path, *keys)
+      keys.reduce({}) do |acc,k|
+        # Simplifiy 1-length keys
+        k = k.first if k.is_a?(Array) && k.size == 1
+
+        # If we have array values here we need to recurse
+        # There are two levels of looping here, one for the paths we might pass in
+        # one for the upcoming keys we might pass in
+        if k.is_a?(Array)
+          # We need to build up future executions to extract_metrics
+          # which means building up the path and keys arguments.
+          # We need a nested loop her to execute all permutations of these in case we hit
+          # something like [[:a,:b],[:c,:d]] which produces 4 different metrics
+          next_paths = Array(k.first)
+          next_keys = Array(k[1])
+          rest = k[2..-1]
+          next_paths.each do |next_path|
+            # If there already is a hash at this location use that so we don't overwrite it
+            np_hash = acc[next_path] || {}
+            
+            acc[next_path] = next_keys.reduce(np_hash) do |a,next_key|
+              a.merge! extract_metrics(path + [next_path], [next_key, *rest])
+            end
+          end
+        else # Scalar value
+          res = get_shallow(*path)[k]
+          acc[k] = res ? res.value : nil
+        end
+        
+        acc
+      end
+    end    
+
     # Return all the individuals Metric,
     # This call mimic a Enum's each if a block is provided
     #
@@ -110,11 +179,28 @@ def each(path = nil, &block)
     end
     alias_method :all, :each
 
+    def prune(path)
+      key_paths = key_paths(path).map {|k| k.to_sym }
+      @structured_lookup_mutex.synchronize do
+        keys_to_delete = @fast_lookup.keys.select {|namespace, _| (key_paths - namespace).empty? }
+        keys_to_delete.each {|k| @fast_lookup.delete(k) }
+        delete_from_map(@store, key_paths)
+      end
+    end
+
+    def size
+      @fast_lookup.size
+    end
+
     private
     def get_all
       @fast_lookup.values
     end
 
+    def key_paths(path)
+      path.gsub(/^#{KEY_PATH_SEPARATOR}+/, "").split(KEY_PATH_SEPARATOR)
+    end
+
     # This method take an array of keys and recursively search the metric store structure
     # and return a filtered hash of the structure. This method also take into consideration
     # getting two different branchs.
@@ -224,5 +310,14 @@ def fetch_or_store_namespace_recursively(map, namespaces_path, idx = 0)
       new_map = map.fetch_or_store(current) { Concurrent::Map.new }
       return fetch_or_store_namespace_recursively(new_map, namespaces_path, idx + 1)
     end
+
+    def delete_from_map(map, keys)
+      key = keys.first
+      if keys.size == 1
+        map.delete(key)
+      else
+        delete_from_map(map[key], keys[1..-1]) unless map[key].nil?
+      end
+    end
   end
 end; end
diff --git a/logstash-core/lib/logstash/instrument/metric_type/base.rb b/logstash-core/lib/logstash/instrument/metric_type/base.rb
index 5711c3f83b6..206f175c753 100644
--- a/logstash-core/lib/logstash/instrument/metric_type/base.rb
+++ b/logstash-core/lib/logstash/instrument/metric_type/base.rb
@@ -17,10 +17,7 @@ def inspect
 
     def to_hash
       {
-        "namespaces" => namespaces,
-        "key" => key,
-        "type" => type,
-        "value" => value
+        key => value
       }
     end
 
diff --git a/logstash-core/lib/logstash/instrument/namespaced_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
index 6b0ad020e60..1f056bd0735 100644
--- a/logstash-core/lib/logstash/instrument/namespaced_metric.rb
+++ b/logstash-core/lib/logstash/instrument/namespaced_metric.rb
@@ -24,7 +24,7 @@ def increment(key, value = 1)
       @metric.increment(namespace_name, key, value)
     end
 
-    def decrement(namespace, key, value = 1)
+    def decrement(key, value = 1)
       @metric.decrement(namespace_name, key, value)
     end
 
@@ -45,7 +45,7 @@ def collector
     end
 
     def namespace(name)
-      NamespacedMetric.new(metric, namespace_name.concat(Array(name)))
+      NamespacedMetric.new(metric, namespace_name + Array(name))
     end
 
     private
diff --git a/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb b/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
new file mode 100644
index 00000000000..c4e8e762c23
--- /dev/null
+++ b/logstash-core/lib/logstash/instrument/namespaced_null_metric.rb
@@ -0,0 +1,54 @@
+# encoding: utf-8
+require "logstash/instrument/null_metric"
+
+module LogStash module Instrument
+  # This class acts a a proxy between the metric library and the user calls.
+  #
+  # This is the class that plugins authors will use to interact with the `MetricStore`
+  # It has the same public interface as `Metric` class but doesnt require to send
+  # the namespace on every call.
+  #
+  # @see Logstash::Instrument::Metric
+  class NamespacedNullMetric
+    attr_reader :namespace_name
+    # Create metric with a specific namespace
+    #
+    # @param metric [LogStash::Instrument::Metric] The metric instance to proxy
+    # @param namespace [Array] The namespace to use
+    def initialize(metric = nil, namespace_name = :null)
+      @metric = metric
+      @namespace_name = Array(namespace_name)
+    end
+
+    def increment(key, value = 1)
+    end
+
+    def decrement(key, value = 1)
+    end
+
+    def gauge(key, value)
+    end
+
+    def report_time(key, duration)
+    end
+
+    def time(key, &block)
+      if block_given?
+        yield
+      else
+        ::LogStash::Instrument::NullMetric::NullTimedExecution
+      end
+    end
+
+    def collector
+      @metric.collector
+    end
+
+    def namespace(name)
+      NamespacedNullMetric.new(metric, namespace_name + Array(name))
+    end
+
+    private
+    attr_reader :metric
+  end
+end; end
diff --git a/logstash-core/lib/logstash/instrument/null_metric.rb b/logstash-core/lib/logstash/instrument/null_metric.rb
index b8054b766dc..afdb345ef79 100644
--- a/logstash-core/lib/logstash/instrument/null_metric.rb
+++ b/logstash-core/lib/logstash/instrument/null_metric.rb
@@ -2,45 +2,59 @@
 require "logstash/instrument/metric"
 
 module LogStash module Instrument
- # This class is used in the context when we disable the metric collection
- # for specific plugin to replace the `NamespacedMetric` class with this one
- # which doesn't produce any metric to the collector.
- class NullMetric
-   attr_reader :namespace_name, :collector
-
-   def increment(key, value = 1)
-   end
-
-   def decrement(namespace, key, value = 1)
-   end
-
-   def gauge(key, value)
-   end
-
-   def report_time(key, duration)
-   end
-
-   # We have to manually redefine this method since it can return an
-   # object this object also has to be implemented as a NullObject
-   def time(key)
-     if block_given?
-       yield
-     else
-       NullTimedExecution
-     end
-   end
-
-   def namespace(key)
-     self.class.new
-   end
-
-   private
-   # Null implementation of the internal timer class
-   #
-   # @see LogStash::Instrument::TimedExecution`
-   class NullTimedExecution
-     def self.stop
-     end
-   end
- end
+  # This class is used in the context when we disable the metric collection
+  # for specific plugin to replace the `NamespacedMetric` class with this one
+  # which doesn't produce any metric to the collector.
+  class NullMetric
+    attr_reader :namespace_name, :collector
+
+    def initialize(collector = nil)
+      @collector = collector
+    end
+
+    def increment(namespace, key, value = 1)
+      Metric.validate_key!(key)
+    end
+
+    def decrement(namespace, key, value = 1)
+      Metric.validate_key!(key)
+    end
+
+    def gauge(namespace, key, value)
+      Metric.validate_key!(key)
+    end
+
+    def report_time(namespace, key, duration)
+      Metric.validate_key!(key)
+    end
+
+    # We have to manually redefine this method since it can return an
+    # object this object also has to be implemented as a NullObject
+    def time(namespace, key)
+      Metric.validate_key!(key)
+      if block_given?
+        yield
+      else
+        NullTimedExecution
+      end
+    end
+
+    def namespace(name)
+      raise MetricNoNamespaceProvided if name.nil? || name.empty?
+      NamespacedNullMetric.new(self, name)
+    end
+
+    def self.validate_key!(key)
+      raise MetricNoKeyProvided if key.nil? || key.empty?
+    end
+
+    private
+    # Null implementation of the internal timer class
+    #
+    # @see LogStash::Instrument::TimedExecution`
+    class NullTimedExecution
+      def self.stop
+      end
+    end
+  end
 end; end
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/base.rb b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
index 32bfd931a9a..313f52b2504 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/base.rb
@@ -34,14 +34,16 @@ def collect
     end
 
     def start
-      logger.debug("PeriodicPoller: Starting", :poller => self,
+      logger.debug("PeriodicPoller: Starting",
                    :polling_interval => @options[:polling_interval],
                    :polling_timeout => @options[:polling_timeout]) if logger.debug?
+      
+      collect # Collect data right away if possible
       @task.execute
     end
 
     def stop
-      logger.debug("PeriodicPoller: Stopping", :poller => self)
+      logger.debug("PeriodicPoller: Stopping")
       @task.shutdown
     end
 
diff --git a/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
index 3b85d92efa6..744bfda8cc3 100644
--- a/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
+++ b/logstash-core/lib/logstash/instrument/periodic_poller/jvm.rb
@@ -1,9 +1,36 @@
+
 # encoding: utf-8
 require "logstash/instrument/periodic_poller/base"
-require 'monitoring'
+require "jrmonitor"
+require "set"
+
+java_import 'java.lang.management.ManagementFactory'
+java_import 'java.lang.management.OperatingSystemMXBean'
+java_import 'java.lang.management.GarbageCollectorMXBean'
+java_import 'com.sun.management.UnixOperatingSystemMXBean'
+java_import 'javax.management.MBeanServer'
+java_import 'javax.management.ObjectName'
+java_import 'javax.management.AttributeList'
+java_import 'javax.naming.directory.Attribute'
+
 
 module LogStash module Instrument module PeriodicPoller
   class JVM < Base
+    class GarbageCollectorName
+      YOUNG_GC_NAMES = Set.new(["Copy", "PS Scavenge", "ParNew", "G1 Young Generation"])
+      OLD_GC_NAMES = Set.new(["MarkSweepCompact", "PS MarkSweep", "ConcurrentMarkSweep", "G1 Old Generation"])
+
+      YOUNG = :young
+      OLD = :old
+
+      def self.get(gc_name)
+        if YOUNG_GC_NAMES.include?(gc_name)
+          YOUNG
+        elsif(OLD_GC_NAMES.include?(gc_name))
+          OLD
+        end
+      end
+    end
 
     attr_reader :metric
 
@@ -17,10 +44,61 @@ def collect
       collect_heap_metrics(raw)
       collect_non_heap_metrics(raw)
       collect_pools_metrics(raw)
+      collect_threads_metrics
+      collect_process_metrics
+      collect_gc_stats
     end
 
     private
 
+    def collect_gc_stats
+      garbage_collectors = ManagementFactory.getGarbageCollectorMXBeans()
+
+      garbage_collectors.each do |collector|
+        name = GarbageCollectorName.get(collector.getName())
+        if name.nil?
+          logger.error("Unknown garbage collector name", :name => name)
+        else
+          metric.gauge([:jvm, :gc, :collectors, name], :collection_count, collector.getCollectionCount())
+          metric.gauge([:jvm, :gc, :collectors, name], :collection_time_in_millis, collector.getCollectionTime())
+        end
+      end
+    end
+
+    def collect_threads_metrics
+      threads = JRMonitor.threads.generate
+
+      current = threads.count
+      if @peak_threads.nil? || @peak_threads < current
+        @peak_threads = current
+      end
+
+      metric.gauge([:jvm, :threads], :count, threads.count)
+      metric.gauge([:jvm, :threads], :peak_count, @peak_threads)
+    end
+
+    def collect_process_metrics
+      process_metrics = JRMonitor.process.generate
+
+      path = [:jvm, :process]
+
+
+      open_fds = process_metrics["open_file_descriptors"]
+      if @peak_open_fds.nil? || open_fds > @peak_open_fds
+        @peak_open_fds = open_fds
+      end
+      metric.gauge(path, :open_file_descriptors, open_fds)
+      metric.gauge(path, :peak_open_file_descriptors, @peak_open_fds)
+      metric.gauge(path, :max_file_descriptors, process_metrics["max_file_descriptors"])
+
+      cpu_path = path + [:cpu]
+      cpu_metrics = process_metrics["cpu"]
+      metric.gauge(cpu_path, :percent, cpu_metrics["process_percent"])
+      metric.gauge(cpu_path, :total_in_millis, cpu_metrics["total_in_millis"])
+
+      metric.gauge(path + [:mem], :total_virtual_in_bytes, process_metrics["mem"]["total_virtual_in_bytes"])
+    end
+
     def collect_heap_metrics(data)
       heap = aggregate_information_for(data["heap"].values)
       heap[:used_percent] = (heap[:used_in_bytes] / heap[:max_in_bytes].to_f)*100.0
@@ -46,6 +124,7 @@ def collect_pools_metrics(data)
       end
     end
 
+
     def build_pools_metrics(data)
       heap = data["heap"]
       old  = {}
@@ -84,9 +163,8 @@ def default_information_accumulator
         :committed_in_bytes => 0,
         :max_in_bytes => 0,
         :peak_used_in_bytes => 0,
-        :peak_max_in_bytes  => 0
+        :peak_max_in_bytes => 0
       }
     end
-
   end
 end; end; end
diff --git a/logstash-core/lib/logstash/java_integration.rb b/logstash-core/lib/logstash/java_integration.rb
index 670ceaae650..26f9eb546e0 100644
--- a/logstash-core/lib/logstash/java_integration.rb
+++ b/logstash-core/lib/logstash/java_integration.rb
@@ -1,5 +1,6 @@
 # encoding: utf-8
 require "java"
+require "jars"
 
 # this is mainly for usage with JrJackson json parsing in :raw mode which genenerates
 # Java::JavaUtil::ArrayList and Java::JavaUtil::LinkedHashMap native objects for speed.
diff --git a/logstash-core/lib/logstash/logging.rb b/logstash-core/lib/logstash/logging.rb
index 1dbaa0aa932..201d706bc75 100644
--- a/logstash-core/lib/logstash/logging.rb
+++ b/logstash-core/lib/logstash/logging.rb
@@ -1,91 +1,3 @@
 # encoding: utf-8
+require "logstash/logging/logger"
 require "logstash/namespace"
-require "cabin"
-require "logger"
-
-class LogStash::Logger
-  attr_accessor :target
-
-  public
-  def initialize(*args)
-    super()
-
-    #self[:program] = File.basename($0)
-    #subscribe(::Logger.new(*args))
-    @target = args[0]
-    @channel = Cabin::Channel.get(LogStash)
-
-    # lame hack until cabin's smart enough not to doubley-subscribe something.
-    # without this subscription count check, running the test suite
-    # causes Cabin to subscribe to STDOUT maaaaaany times.
-    subscriptions = @channel.instance_eval { @subscribers.count }
-    @channel.subscribe(@target) unless subscriptions > 0
-
-    # Set default loglevel to WARN unless $DEBUG is set (run with 'ruby -d')
-    @level = $DEBUG ? :debug : :warn
-    if ENV["LOGSTASH_DEBUG"]
-      @level = :debug
-    end
-
-    # Direct metrics elsewhere.
-    @channel.metrics.channel = Cabin::Channel.new
-  end # def initialize
-
-  # Delegation
-  def level=(value) @channel.level = value; end
-  def debug(*args); @channel.debug(*args); end
-  def debug?(*args); @channel.debug?(*args); end
-  def info(*args); @channel.info(*args); end
-  def info?(*args); @channel.info?(*args); end
-  def warn(*args); @channel.warn(*args); end
-  def warn?(*args); @channel.warn?(*args); end
-  def error(*args); @channel.error(*args); end
-  def error?(*args); @channel.error?(*args); end
-  def fatal(*args); @channel.fatal(*args); end
-  def fatal?(*args); @channel.fatal?(*args); end
-
-  def self.setup_log4j(logger)
-    require "java"
-
-    properties = java.util.Properties.new
-    log4j_level = "WARN"
-    case logger.level
-      when :debug
-        log4j_level = "DEBUG"
-      when :info
-        log4j_level = "INFO"
-      when :warn
-        log4j_level = "WARN"
-    end # case level
-    properties.setProperty("log4j.rootLogger", "#{log4j_level},logstash")
-
-    # TODO(sissel): This is a shitty hack to work around the fact that
-    # LogStash::Logger isn't used anymore. We should fix that.
-    target = logger.instance_eval { @subscribers }.values.first.instance_eval { @io }
-    case target
-      when STDOUT
-        properties.setProperty("log4j.appender.logstash",
-                      "org.apache.log4j.ConsoleAppender")
-        properties.setProperty("log4j.appender.logstash.Target", "System.out")
-      when STDERR
-        properties.setProperty("log4j.appender.logstash",
-                      "org.apache.log4j.ConsoleAppender")
-        properties.setProperty("log4j.appender.logstash.Target", "System.err")
-      when target.is_a?(File)
-        properties.setProperty("log4j.appender.logstash",
-                      "org.apache.log4j.FileAppender")
-        properties.setProperty("log4j.appender.logstash.File", target.path)
-      else
-        properties.setProperty("log4j.appender.logstash", "org.apache.log4j.varia.NullAppender")
-    end # case target
-
-    properties.setProperty("log4j.appender.logstash.layout",
-                  "org.apache.log4j.PatternLayout")
-    properties.setProperty("log4j.appender.logstash.layout.conversionPattern",
-                  "log4j, [%d{yyyy-MM-dd}T%d{HH:mm:ss.SSS}] %5p: %c: %m%n")
-
-    org.apache.log4j.LogManager.resetConfiguration
-    org.apache.log4j.PropertyConfigurator.configure(properties)
-    logger.debug("log4j java properties setup", :log4j_level => log4j_level)
-  end
-end # class LogStash::Logger
diff --git a/logstash-core/lib/logstash/logging/json.rb b/logstash-core/lib/logstash/logging/json.rb
new file mode 100644
index 00000000000..f51f8051d58
--- /dev/null
+++ b/logstash-core/lib/logstash/logging/json.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/logging"
+require "logstash/json"
+
+module LogStash; module Logging; class JSON
+  def initialize(io)
+    raise ArgumentError, "Expected IO, got #{io.class.name}" unless io.is_a?(IO)
+
+    @io = io
+    @lock = Mutex.new
+  end
+
+  def <<(obj)
+    serialized = LogStash::Json.dump(obj)
+    @lock.synchronize do
+      @io.puts(serialized)
+      @io.flush
+    end
+  end
+end; end; end
diff --git a/logstash-core/lib/logstash/logging/logger.rb b/logstash-core/lib/logstash/logging/logger.rb
new file mode 100644
index 00000000000..8acac62ff1a
--- /dev/null
+++ b/logstash-core/lib/logstash/logging/logger.rb
@@ -0,0 +1,83 @@
+require "logstash/java_integration"
+
+module LogStash
+  module Logging
+    class Logger
+      java_import org.apache.logging.log4j.Level
+      java_import org.apache.logging.log4j.LogManager
+      java_import org.apache.logging.log4j.core.config.Configurator
+      @@config_mutex = Mutex.new
+      @@logging_context = nil
+
+      def initialize(name)
+        @logger = LogManager.getLogger(name)
+      end
+
+      def debug?
+        @logger.is_debug_enabled
+      end
+
+      def info?
+        @logger.is_info_enabled
+      end
+
+      def error?
+        @logger.is_error_enabled
+      end
+
+      def warn?
+        @logger.is_warn_enabled
+      end
+
+      def fatal?
+        @logger.is_fatal_enabled
+      end
+
+      def trace?
+        @logger.is_trace_enabled
+      end
+
+      def debug(message, data = {})
+        @logger.debug(message, data)
+      end
+
+      def warn(message, data = {})
+        @logger.warn(message, data)
+      end
+
+      def info(message, data = {})
+        @logger.info(message, data)
+      end
+
+      def error(message, data = {})
+        @logger.error(message, data)
+      end
+
+      def fatal(message, data = {})
+        @logger.fatal(message, data)
+      end
+
+      def trace(message, data = {})
+        @logger.trace(message, data)
+      end
+
+      def self.configure_logging(level, path = LogManager::ROOT_LOGGER_NAME)
+        @@config_mutex.synchronize { Configurator.setLevel(path, Level.valueOf(level)) }
+      rescue Exception => e
+        raise ArgumentError, "invalid level[#{level}] for logger[#{path}]"
+      end
+
+      def self.initialize(config_location)
+        @@config_mutex.synchronize do
+          if @@logging_context.nil?
+            @@logging_context = Configurator.initialize(nil, config_location)
+          end
+        end
+      end
+
+      def self.get_logging_context
+        return @@logging_context
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/output_delegator.rb b/logstash-core/lib/logstash/output_delegator.rb
index 50a5a9d49c7..39a7fdb1f0a 100644
--- a/logstash-core/lib/logstash/output_delegator.rb
+++ b/logstash-core/lib/logstash/output_delegator.rb
@@ -1,163 +1,49 @@
-# encoding: utf-8
-require "concurrent/atomic/atomic_fixnum"
+require "logstash/output_delegator_strategy_registry"
+
+require "logstash/output_delegator_strategies/shared"
+require "logstash/output_delegator_strategies/single"
+require "logstash/output_delegator_strategies/legacy"
 
-# This class goes hand in hand with the pipeline to provide a pool of
-# free workers to be used by pipeline worker threads. The pool is
-# internally represented with a SizedQueue set the the size of the number
-# of 'workers' the output plugin is configured with.
-#
-# This plugin also records some basic statistics
 module LogStash class OutputDelegator
-  attr_reader :workers, :config, :worker_count, :threadsafe
+  attr_reader :metric, :metric_events, :strategy, :namespaced_metric, :metric_events, :id
 
-  # The *args this takes are the same format that a Outputs::Base takes. A list of hashes with parameters in them
-  # Internally these just get merged together into a single hash
-  def initialize(logger, klass, default_worker_count, metric, *args)
+  def initialize(logger, output_class, metric, strategy_registry, plugin_args)
     @logger = logger
-    @threadsafe = klass.threadsafe?
-    @config = args.reduce({}, :merge)
-    @klass = klass
-
-    # Create an instance of the input so we can fetch the identifier
-    output = @klass.new(*args)
-
-    # Scope the metrics to the plugin
-    namespaced_metric = metric.namespace(output.plugin_unique_name.to_sym)
-    @metric_events = namespaced_metric.namespace(:events)
-
-
-    # We define this as an array regardless of threadsafety
-    # to make reporting simpler, even though a threadsafe plugin will just have
-    # a single instance
-    #
-    # Older plugins invoke the instance method Outputs::Base#workers_not_supported
-    # To detect these we need an instance to be created first :()
-    # TODO: In the next major version after 2.x remove support for this
-    @workers = [@klass.new(*args)]
-    @workers.first.register # Needed in case register calls `workers_not_supported`
-
-    # DO NOT move this statement before the instantiation of the first single instance
-    # Read the note above to understand why
-    @worker_count = calculate_worker_count(default_worker_count)
-    @logger.debug("Will start workers for output", :worker_count => @worker_count, :class => klass)
-
-    warn_on_worker_override!
-    # This queue is used to manage sharing across threads
-    @worker_queue = SizedQueue.new(@worker_count)
-
-    @workers += (@worker_count - 1).times.map do
-      inst = @klass.new(*args)
-      inst.metric = @metric
-      inst.register
-      inst
-    end
-
-    @workers.each { |w| @worker_queue << w }
-
-    @events_received = Concurrent::AtomicFixnum.new(0)
-
-
-    # One might wonder why we don't use something like
-    # define_singleton_method(:multi_receive, method(:threadsafe_multi_receive)
-    # and the answer is this is buggy on Jruby 1.7.x . It works 98% of the time!
-    # The other 2% you get weird errors about rebinding to the same object
-    # Until we switch to Jruby 9.x keep the define_singleton_method parts
-    # the way they are, with a block
-    # See https://github.com/jruby/jruby/issues/3582
-    if threadsafe?
-      @threadsafe_worker = @workers.first
-      define_singleton_method(:multi_receive) do |events|
-        threadsafe_multi_receive(events)
-      end
-    else
-      define_singleton_method(:multi_receive) do |events|
-        worker_multi_receive(events)
-      end
-    end
-  end
-
-  def threadsafe?
-    !!@threadsafe
-  end
-
-  def warn_on_worker_override!
-    # The user has configured extra workers, but this plugin doesn't support it :(
-    if worker_limits_overriden?
-      message = @klass.workers_not_supported_message
-      warning_meta = {:plugin => @klass.config_name, :worker_count => @config["workers"]}
-      if message
-        warning_meta[:message] = message
-        @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported-with-message", warning_meta))
-      else
-        @logger.warn(I18n.t("logstash.pipeline.output-worker-unsupported", warning_meta))
-      end
-    end
+    @output_class = output_class
+    @metric = metric
+    @id = plugin_args["id"]
+
+    raise ArgumentError, "No strategy registry specified" unless strategy_registry
+    raise ArgumentError, "No ID specified! Got args #{plugin_args}" unless id
+    
+    @strategy = strategy_registry.
+                  class_for(self.concurrency).
+                  new(@logger, @output_class, @metric, plugin_args)
+    
+    @namespaced_metric = metric.namespace(id.to_sym)
+    @namespaced_metric.gauge(:name, config_name)
+    @metric_events = @namespaced_metric.namespace(:events)
   end
 
-  def worker_limits_overriden?
-    @config["workers"] && @config["workers"] > 1 && @klass.workers_not_supported?
-  end
-
-  def calculate_worker_count(default_worker_count)
-    if @threadsafe || @klass.workers_not_supported?
-      1
-    else
-      @config["workers"] || default_worker_count
-    end
+  def config_name
+    @output_class.config_name
   end
 
-  def config_name
-    @klass.config_name
+  def concurrency
+    @output_class.concurrency
   end
 
   def register
-    @workers.each {|w| w.register}
+    @strategy.register
   end
 
-  def threadsafe_multi_receive(events)
-    @events_received.increment(events.length)
+  def multi_receive(events)
     @metric_events.increment(:in, events.length)
-
-    @threadsafe_worker.multi_receive(events)
+    @strategy.multi_receive(events)
     @metric_events.increment(:out, events.length)
   end
 
-  def worker_multi_receive(events)
-    @events_received.increment(events.length)
-    @metric_events.increment(:in, events.length)
-
-    worker = @worker_queue.pop
-    begin
-      worker.multi_receive(events)
-      @metric_events.increment(:out, events.length)
-    ensure
-      @worker_queue.push(worker)
-    end
-  end
-
   def do_close
-    @logger.debug("closing output delegator", :klass => self)
-
-    @worker_count.times do
-      worker = @worker_queue.pop
-      worker.do_close
-    end
-  end
-
-  def events_received
-    @events_received.value
+    @strategy.do_close
   end
-
-  # There's no concept of 'busy' workers for a threadsafe plugin!
-  def busy_workers
-    if @threadsafe
-      0
-    else
-      @workers.size - @worker_queue.size
-    end
-  end
-
-  private
-  # Needed for testing, so private
-  attr_reader :threadsafe_worker, :worker_queue
-end end
+end; end
diff --git a/logstash-core/lib/logstash/output_delegator_strategies/legacy.rb b/logstash-core/lib/logstash/output_delegator_strategies/legacy.rb
new file mode 100644
index 00000000000..81f695afc9d
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategies/legacy.rb
@@ -0,0 +1,30 @@
+# Remove this in Logstash 6.0
+module LogStash module OutputDelegatorStrategies class Legacy
+  attr_reader :worker_count, :workers
+  
+  def initialize(logger, klass, metric, plugin_args)
+    @worker_count = (plugin_args["workers"] || 1).to_i
+    @workers = @worker_count.times.map { klass.new(plugin_args) }
+    @workers.each {|w| w.metric = metric }
+    @worker_queue = SizedQueue.new(@worker_count)
+    @workers.each {|w| @worker_queue << w}
+  end
+  
+  def register
+    @workers.each(&:register)
+  end
+  
+  def multi_receive(events)
+    worker = @worker_queue.pop
+    worker.multi_receive(events)
+  ensure
+    @worker_queue << worker if worker
+  end
+
+  def do_close
+    # No mutex needed since this is only called when the pipeline is clear
+    @workers.each(&:do_close)
+  end
+
+  ::LogStash::OutputDelegatorStrategyRegistry.instance.register(:legacy, self)
+end; end; end
diff --git a/logstash-core/lib/logstash/output_delegator_strategies/shared.rb b/logstash-core/lib/logstash/output_delegator_strategies/shared.rb
new file mode 100644
index 00000000000..9650cf7ee22
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategies/shared.rb
@@ -0,0 +1,21 @@
+module LogStash module OutputDelegatorStrategies class Shared
+  def initialize(logger, klass, metric, plugin_args)
+    @output = klass.new(plugin_args)
+    @output.metric = metric
+  end
+  
+  def register
+    @output.register
+  end
+
+  def multi_receive(events)
+    @output.multi_receive(events)
+  end
+
+  def do_close    
+    @output.do_close
+  end
+
+  ::LogStash::OutputDelegatorStrategyRegistry.instance.register(:shared, self)  
+end; end; end
+
diff --git a/logstash-core/lib/logstash/output_delegator_strategies/single.rb b/logstash-core/lib/logstash/output_delegator_strategies/single.rb
new file mode 100644
index 00000000000..d576a22df6f
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategies/single.rb
@@ -0,0 +1,24 @@
+module LogStash module OutputDelegatorStrategies class Single
+  def initialize(logger, klass, metric, plugin_args)
+    @output = klass.new(plugin_args)
+    @output.metric = metric
+    @mutex = Mutex.new
+  end
+
+  def register
+    @output.register
+  end
+  
+  def multi_receive(events)
+    @mutex.synchronize do
+      @output.multi_receive(events)
+    end
+  end
+
+  def do_close
+    # No mutex needed since this is only called when the pipeline is clear
+    @output.do_close
+  end
+
+  ::LogStash::OutputDelegatorStrategyRegistry.instance.register(:single, self)
+end; end; end
diff --git a/logstash-core/lib/logstash/output_delegator_strategy_registry.rb b/logstash-core/lib/logstash/output_delegator_strategy_registry.rb
new file mode 100644
index 00000000000..dc4e0a02000
--- /dev/null
+++ b/logstash-core/lib/logstash/output_delegator_strategy_registry.rb
@@ -0,0 +1,36 @@
+module LogStash; class OutputDelegatorStrategyRegistry
+  class InvalidStrategyError < StandardError; end
+                   
+  # This is generally used as a singleton
+  # Except perhaps during testing
+  def self.instance
+    @instance ||= self.new
+  end
+
+  def initialize()
+    @map = {}
+  end
+
+  def classes
+    @map.values
+  end
+
+  def types
+    @map.keys
+  end
+  
+  def class_for(type)
+    klass = @map[type]
+
+    if !klass
+      raise InvalidStrategyError, "Could not find output delegator strategy of type '#{type}'. Valid strategies: #{@strategy_registry.types}"
+    end
+
+    klass
+  end
+
+  def register(type, klass)
+    @map[type] = klass
+  end
+
+end; end
diff --git a/logstash-core/lib/logstash/outputs/base.rb b/logstash-core/lib/logstash/outputs/base.rb
index 3f59cc0e715..453d0cbdf98 100644
--- a/logstash-core/lib/logstash/outputs/base.rb
+++ b/logstash-core/lib/logstash/outputs/base.rb
@@ -8,6 +8,7 @@
 require "concurrent/atomic/atomic_fixnum"
 
 class LogStash::Outputs::Base < LogStash::Plugin
+  include LogStash::Util::Loggable
   include LogStash::Config::Mixin
 
   config_name "output"
@@ -20,41 +21,40 @@ class LogStash::Outputs::Base < LogStash::Plugin
 
   # The codec used for output data. Output codecs are a convenient method for encoding your data before it leaves the output, without needing a separate filter in your Logstash pipeline.
   config :codec, :validate => :codec, :default => "plain"
+  # TODO remove this in Logstash 6.0
+  # when we no longer support the :legacy type
+  # This is hacky, but it can only be herne
+  config :workers, :type => :number, :default => 1
+  
+  # Set or return concurrency type
+  def self.concurrency(type=nil)
+    if type
+      @concurrency = type
+    else
+      @concurrency || :legacy # default is :legacyo
+    end
+  end
 
-  # The number of workers to use for this output.
-  # Note that this setting may not be useful for all outputs.
-  config :workers, :validate => :number, :default => 1
-
-  attr_reader :worker_plugins, :available_workers, :workers, :worker_plugins, :workers_not_supported
-
+  # Deprecated: Favor `concurrency :shared`
   def self.declare_threadsafe!
-    declare_workers_not_supported!
-    @threadsafe = true
+    concurrency :shared
   end
 
+  # Deprecated: Favor `#concurrency`
   def self.threadsafe?
-    @threadsafe == true
+    concurrency == :shared
   end
 
+  # Deprecated: Favor `concurrency :single`
+  # Remove in Logstash 6.0.0
   def self.declare_workers_not_supported!(message=nil)
-    @workers_not_supported_message = message
-    @workers_not_supported = true
-  end
-
-  def self.workers_not_supported_message
-    @workers_not_supported_message
-  end
-
-  def self.workers_not_supported?
-    !!@workers_not_supported
+    concurrency :single
   end
 
   public
-  # TODO: Remove this in the next major version after Logstash 2.x
-  # Post 2.x it should raise an error and tell people to use the class level
-  # declaration
-  def workers_not_supported(message=nil)
-    self.class.declare_workers_not_supported!(message)
+
+  def self.plugin_type
+    "output"
   end
 
   public
@@ -62,9 +62,15 @@ def initialize(params={})
     super
     config_init(@params)
 
+    if self.workers != 1
+      raise LogStash::ConfigurationError, "You are using a plugin that doesn't support workers but have set the workers value explicitly! This plugin uses the #{concurrency} and doesn't need this option"
+    end
+
     # If we're running with a single thread we must enforce single-threaded concurrency by default
     # Maybe in a future version we'll assume output plugins are threadsafe
     @single_worker_mutex = Mutex.new
+    
+    @receives_encoded = self.methods.include?(:multi_receive_encoded)
   end
 
   public
@@ -80,7 +86,23 @@ def receive(event)
   public
   # To be overriden in implementations
   def multi_receive(events)
-    events.each {|event| receive(event) }
+    if @receives_encoded
+      self.multi_receive_encoded(codec.multi_encode(events))
+    else
+      events.each {|event| receive(event) }
+    end
+  end
+
+  def workers_not_supported(message=nil)
+    raise "This plugin (#{self.class.name}) is using the obsolete '#workers_not_supported' method. If you installed this plugin specifically on this Logstash version, it is not compatible. If you are a plugin author, please see https://www.elastic.co/guide/en/logstash/current/_how_to_write_a_logstash_output_plugin.html for more info"
+  end
+
+  def codec
+    params["codec"]
+  end
+
+  def concurrency
+    self.class.concurrency
   end
 
   private
diff --git a/logstash-core/lib/logstash/patches/clamp.rb b/logstash-core/lib/logstash/patches/clamp.rb
new file mode 100644
index 00000000000..bdebfeb6f6e
--- /dev/null
+++ b/logstash-core/lib/logstash/patches/clamp.rb
@@ -0,0 +1,96 @@
+require 'clamp'
+require 'logstash/environment'
+
+module Clamp
+  module Attribute
+    class Instance
+      def default_from_environment
+        # we don't want uncontrolled var injection from the environment
+        # since we're establishing that settings can be pulled from only three places:
+        # 1. default settings
+        # 2. yaml file
+        # 3. cli arguments
+      end
+    end
+  end
+
+  module Option
+
+    module Declaration
+      def deprecated_option(switches, type, description, opts = {})
+        Option::Definition.new(switches, type, description, opts).tap do |option|
+          declared_options << option
+          block ||= option.default_conversion_block
+          define_deprecated_accessors_for(option, opts, &block)
+        end
+      end
+    end
+
+    module StrictDeclaration
+
+      include Clamp::Attribute::Declaration
+      include LogStash::Util::Loggable
+
+      # Instead of letting Clamp set up accessors for the options
+      # weŕe going to tightly controlling them through
+      # LogStash::SETTINGS
+      def define_simple_writer_for(option, &block)
+        LogStash::SETTINGS.get(option.attribute_name)
+        define_method(option.write_method) do |value|
+          value = instance_exec(value, &block) if block
+          LogStash::SETTINGS.set_value(option.attribute_name, value)
+        end
+      end
+
+      def define_reader_for(option)
+        define_method(option.read_method) do
+          LogStash::SETTINGS.get_value(option.attribute_name)
+        end
+      end
+
+      def define_deprecated_accessors_for(option, opts, &block)
+        define_deprecated_writer_for(option, opts, &block)
+      end
+
+      def define_deprecated_writer_for(option, opts, &block)
+        define_method(option.write_method) do |value|
+          self.class.logger.warn "DEPRECATION WARNING: The flag #{option.switches} has been deprecated, please use \"--#{opts[:new_flag]}=#{opts[:new_value]}\" instead."
+          LogStash::SETTINGS.set(opts[:new_flag], opts[:new_value])
+        end
+      end
+    end
+
+    class Definition
+      # Allow boolean flags to optionally receive a true/false argument
+      # to explicitly set them, i.e.
+      # --long.flag.name       => sets flag to true
+      # --long.flag.name true  => sets flag to true
+      # --long.flag.name false => sets flag to false
+      # --long.flag.name=true  => sets flag to true
+      # --long.flag.name=false => sets flag to false
+      def extract_value(switch, arguments)
+        if flag? && (arguments.first.nil? || arguments.first.match("^-"))
+          flag_value(switch)
+        else
+          arguments.shift
+        end
+      end
+    end
+  end
+
+  # Create a subclass of Clamp::Command that enforces the use of
+  # LogStash::SETTINGS for setting validation
+  class StrictCommand < Command
+    class << self
+      include ::Clamp::Option::StrictDeclaration
+    end
+
+    def handle_remaining_arguments
+      unless remaining_arguments.empty?
+        signal_usage_error "Unknown command '#{remaining_arguments.first}'"
+      end
+    end
+  end
+end
+
+
diff --git a/logstash-core/lib/logstash/patches/puma.rb b/logstash-core/lib/logstash/patches/puma.rb
new file mode 100644
index 00000000000..b8507db1875
--- /dev/null
+++ b/logstash-core/lib/logstash/patches/puma.rb
@@ -0,0 +1,44 @@
+# encoding: utf-8
+#
+# Patch to replace the usage of STDERR and STDOUT
+# see: https://github.com/elastic/logstash/issues/5912
+module LogStash
+  class NullLogger
+    def self.debug(message)
+    end
+  end
+
+  # Puma uses by default the STDERR an the STDOUT for all his error
+  # handling, the server class accept custom a events object that can accept custom io object,
+  # so I just wrap the logger into an IO like object.
+  class IOWrappedLogger
+    def initialize(new_logger)
+      @logger_lock = Mutex.new
+      @logger = new_logger
+    end
+
+    def sync=(v)
+      # noop
+    end
+
+    def logger=(logger)
+      @logger_lock.synchronize { @logger = logger }
+    end
+
+    def puts(str)
+      # The logger only accept a str as the first argument
+      @logger_lock.synchronize { @logger.debug(str.to_s) }
+    end
+    alias_method :write, :puts
+    alias_method :<<, :puts
+  end
+
+end
+
+# Reopen the puma class to create a scoped STDERR and STDOUT
+# This operation is thread safe since its done at the class level
+# and force JRUBY to flush his classes cache.
+module Puma
+  STDERR = LogStash::IOWrappedLogger.new(LogStash::NullLogger)
+  STDOUT = LogStash::IOWrappedLogger.new(LogStash::NullLogger)
+end
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 9fcb77e9101..6b4c3b96622 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -9,20 +9,21 @@
 require "logstash/filters/base"
 require "logstash/inputs/base"
 require "logstash/outputs/base"
-require "logstash/config/cpu_core_strategy"
-require "logstash/util/defaults_printer"
 require "logstash/shutdown_watcher"
 require "logstash/util/wrapped_synchronous_queue"
 require "logstash/pipeline_reporter"
 require "logstash/instrument/metric"
 require "logstash/instrument/namespaced_metric"
 require "logstash/instrument/null_metric"
+require "logstash/instrument/namespaced_null_metric"
 require "logstash/instrument/collector"
 require "logstash/output_delegator"
 require "logstash/filter_delegator"
 
 module LogStash; class Pipeline
- attr_reader :inputs,
+  include LogStash::Util::Loggable
+
+  attr_reader :inputs,
     :filters,
     :outputs,
     :worker_threads,
@@ -30,60 +31,48 @@ module LogStash; class Pipeline
     :events_filtered,
     :reporter,
     :pipeline_id,
-    :metric,
-    :logger,
     :started_at,
     :thread,
     :config_str,
-    :original_settings
-
-  DEFAULT_SETTINGS = {
-    :default_pipeline_workers => LogStash::Config::CpuCoreStrategy.maximum,
-    :pipeline_batch_size => 125,
-    :pipeline_batch_delay => 5, # in milliseconds
-    :flush_interval => 5, # in seconds
-    :flush_timeout_interval => 60 # in seconds
-  }
+    :config_hash,
+    :settings,
+    :metric,
+    :filter_queue_client,
+    :input_queue_client
+
   MAX_INFLIGHT_WARN_THRESHOLD = 10_000
 
-  def self.validate_config(config_str, settings = {})
-    begin
-      # There should be a better way to test this
-      self.new(config_str, settings)
-    rescue => e
-      e.message
-    end
-  end
+  RELOAD_INCOMPATIBLE_PLUGINS = [
+    "LogStash::Inputs::Stdin"
+  ]
 
-  def initialize(config_str, settings = {})
+  def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
+    @logger = self.logger
     @config_str = config_str
-    @original_settings = settings
-    @logger = Cabin::Channel.get(LogStash)
-    @pipeline_id = settings[:pipeline_id] || self.object_id
-    @settings = DEFAULT_SETTINGS.clone
-    settings.each {|setting, value| configure(setting, value) }
-    @reporter = LogStash::PipelineReporter.new(@logger, self)
-
+    @config_hash = Digest::SHA1.hexdigest(@config_str)
+    # Every time #plugin is invoked this is incremented to give each plugin
+    # a unique id when auto-generating plugin ids
+    @plugin_counter ||= 0
+    @settings = settings
+    @pipeline_id = @settings.get_value("pipeline.id") || self.object_id
+    @reporter = PipelineReporter.new(@logger, self)
+
+    # A list of plugins indexed by id
+    @plugins_by_id = {}
     @inputs = nil
     @filters = nil
     @outputs = nil
 
     @worker_threads = []
 
-    # Metric object should be passed upstream, multiple pipeline share the same metric
-    # and collector only the namespace will changes.
-    # If no metric is given, we use a `NullMetric` for all internal calls.
-    # We also do this to make the changes backward compatible with previous testing of the
-    # pipeline.
-    #
-    # This need to be configured before we evaluate the code to make
-    # sure the metric instance is correctly send to the plugin.
-    @metric = settings.fetch(:metric, Instrument::NullMetric.new)
+    # This needs to be configured before we evaluate the code to make
+    # sure the metric instance is correctly send to the plugins to make the namespace scoping work
+    @metric = namespaced_metric.nil? ? Instrument::NullMetric.new : namespaced_metric
 
     grammar = LogStashConfigParser.new
     @config = grammar.parse(config_str)
     if @config.nil?
-      raise LogStash::ConfigurationError, grammar.failure_reason
+      raise ConfigurationError, grammar.failure_reason
     end
     # This will compile the config to ruby and evaluate the resulting code.
     # The code will initialize all the plugins and define the
@@ -93,7 +82,10 @@ def initialize(config_str, settings = {})
 
     # The config code is hard to represent as a log message...
     # So just print it.
-    @logger.debug? && @logger.debug("Compiled pipeline code:\n#{code}")
+
+    if @settings.get_value("config.debug") && @logger.debug?
+      @logger.debug("Compiled pipeline code", :code => code)
+    end
 
     begin
       eval(code)
@@ -101,14 +93,18 @@ def initialize(config_str, settings = {})
       raise
     end
 
-    @input_queue = LogStash::Util::WrappedSynchronousQueue.new
+    queue = Util::WrappedSynchronousQueue.new
+    @input_queue_client = queue.write_client
+    @filter_queue_client = queue.read_client
+    # Note that @inflight_batches as a central mechanism for tracking inflight
+    # batches will fail if we have multiple read clients here.
+    @filter_queue_client.set_events_metric(metric.namespace([:stats, :events]))
+    @filter_queue_client.set_pipeline_metric(
+        metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
+    )
     @events_filtered = Concurrent::AtomicFixnum.new(0)
     @events_consumed = Concurrent::AtomicFixnum.new(0)
 
-    # We generally only want one thread at a time able to access pop/take/poll operations
-    # from this queue. We also depend on this to be able to block consumers while we snapshot
-    # in-flight buffers
-    @input_queue_pop_mutex = Mutex.new
     @input_threads = []
     # @ready requires thread safety since it is typically polled from outside the pipeline thread
     @ready = Concurrent::AtomicBoolean.new(false)
@@ -120,38 +116,29 @@ def ready?
     @ready.value
   end
 
-  def configure(setting, value)
-    @settings[setting] = value
-  end
-
   def safe_pipeline_worker_count
-    default = DEFAULT_SETTINGS[:default_pipeline_workers]
-    thread_count = @settings[:pipeline_workers] #override from args "-w 8" or config
+    default = @settings.get_default("pipeline.workers")
+    pipeline_workers = @settings.get("pipeline.workers") #override from args "-w 8" or config
     safe_filters, unsafe_filters = @filters.partition(&:threadsafe?)
+    plugins = unsafe_filters.collect { |f| f.config_name }
 
-    if unsafe_filters.any?
-      plugins = unsafe_filters.collect { |f| f.config_name }
-      case thread_count
-      when nil
-        # user did not specify a worker thread count
-        # warn if the default is multiple
+    return pipeline_workers if unsafe_filters.empty?
 
-        if default > 1
-          @logger.warn("Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads",
-                       :count_was => default, :filters => plugins)
-        end
-
-        1 # can't allow the default value to propagate if there are unsafe filters
-      when 0, 1
-        1
-      else
+    if @settings.set?("pipeline.workers")
+      if pipeline_workers > 1
         @logger.warn("Warning: Manual override - there are filters that might not work with multiple worker threads",
-                     :worker_threads => thread_count, :filters => plugins)
-        thread_count # allow user to force this even if there are unsafe filters
+                     :worker_threads => pipeline_workers, :filters => plugins)
       end
     else
-      thread_count || default
+      # user did not specify a worker thread count
+      # warn if the default is multiple
+      if default > 1
+        @logger.warn("Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads",
+                     :count_was => default, :filters => plugins)
+        return 1 # can't allow the default value to propagate if there are unsafe filters
+      end
     end
+    pipeline_workers
   end
 
   def filters?
@@ -161,13 +148,12 @@ def filters?
   def run
     @started_at = Time.now
 
-    LogStash::Util.set_thread_name("[#{pipeline_id}]-pipeline-manager")
-    @logger.terminal(LogStash::Util::DefaultsPrinter.print(@settings))
     @thread = Thread.current
+    Util.set_thread_name("[#{pipeline_id}]-pipeline-manager")
 
     start_workers
 
-    @logger.log("Pipeline #{@pipeline_id} started")
+    @logger.info("Pipeline #{@pipeline_id} started")
 
     # Block until all inputs have stopped
     # Generally this happens if SIGINT is sent and `shutdown` is called from an external thread
@@ -177,12 +163,12 @@ def run
     wait_inputs
     transition_to_stopped
 
-    @logger.info("Input plugins stopped! Will shutdown filter/output workers.")
+    @logger.debug("Input plugins stopped! Will shutdown filter/output workers.")
 
     shutdown_flusher
     shutdown_workers
 
-    @logger.log("Pipeline #{@pipeline_id} has been shutdown")
+    @logger.debug("Pipeline #{@pipeline_id} has been shutdown")
 
     # exit code
     return 0
@@ -205,8 +191,6 @@ def stopped?
   end
 
   def start_workers
-    @inflight_batches = {}
-
     @worker_threads.clear # In case we're restarting the pipeline
     begin
       start_inputs
@@ -214,27 +198,36 @@ def start_workers
       @filters.each {|f| f.register }
 
       pipeline_workers = safe_pipeline_worker_count
-      batch_size = @settings[:pipeline_batch_size]
-      batch_delay = @settings[:pipeline_batch_delay]
+      batch_size = @settings.get("pipeline.batch.size")
+      batch_delay = @settings.get("pipeline.batch.delay")
+
       max_inflight = batch_size * pipeline_workers
+
+      config_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :config])
+      config_metric.gauge(:workers, pipeline_workers)
+      config_metric.gauge(:batch_size, batch_size)
+      config_metric.gauge(:batch_delay, batch_delay)
+      config_metric.gauge(:config_reload_automatic, @settings.get("config.reload.automatic"))
+      config_metric.gauge(:config_reload_interval, @settings.get("config.reload.interval"))
+
       @logger.info("Starting pipeline",
-                   :id => self.pipeline_id,
-                   :pipeline_workers => pipeline_workers,
-                   :batch_size => batch_size,
-                   :batch_delay => batch_delay,
-                   :max_inflight => max_inflight)
+                   "id" => self.pipeline_id,
+                   "pipeline.workers" => pipeline_workers,
+                   "pipeline.batch.size" => batch_size,
+                   "pipeline.batch.delay" => batch_delay,
+                   "pipeline.max_inflight" => max_inflight)
       if max_inflight > MAX_INFLIGHT_WARN_THRESHOLD
         @logger.warn "CAUTION: Recommended inflight events max exceeded! Logstash will run with up to #{max_inflight} events in memory in your current configuration. If your message sizes are large this may cause instability with the default heap size. Please consider setting a non-standard heap size, changing the batch size (currently #{batch_size}), or changing the number of pipeline workers (currently #{pipeline_workers})"
       end
 
       pipeline_workers.times do |t|
         @worker_threads << Thread.new do
-          LogStash::Util.set_thread_name("[#{pipeline_id}]>worker#{t}")
+          Util.set_thread_name("[#{pipeline_id}]>worker#{t}")
           worker_loop(batch_size, batch_delay)
         end
       end
     ensure
-      # it is important to garantee @ready to be true after the startup sequence has been completed
+      # it is important to guarantee @ready to be true after the startup sequence has been completed
       # to potentially unblock the shutdown method which may be waiting on @ready to proceed
       @ready.make_true
     end
@@ -245,73 +238,34 @@ def start_workers
   def worker_loop(batch_size, batch_delay)
     running = true
 
-    namespace_events = metric.namespace([:stats, :events])
-    namespace_pipeline = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :events])
+    @filter_queue_client.set_batch_dimensions(batch_size, batch_delay)
 
     while running
-      # To understand the purpose behind this synchronize please read the body of take_batch
-      input_batch, signal = @input_queue_pop_mutex.synchronize { take_batch(batch_size, batch_delay) }
-      running = false if signal == LogStash::SHUTDOWN
-
-      @events_consumed.increment(input_batch.size)
-      namespace_events.increment(:in, input_batch.size)
-      namespace_pipeline.increment(:in, input_batch.size)
+      batch = @filter_queue_client.take_batch
+      @events_consumed.increment(batch.size)
+      running = false if batch.shutdown_signal_received?
+      filter_batch(batch)
 
-      filtered_batch = filter_batch(input_batch)
-
-      if signal # Flush on SHUTDOWN or FLUSH
-        flush_options = (signal == LogStash::SHUTDOWN) ? {:final => true} : {}
-        flush_filters_to_batch(filtered_batch, flush_options)
+      if batch.shutdown_signal_received? || batch.flush_signal_received?
+        flush_filters_to_batch(batch)
       end
 
-      @events_filtered.increment(filtered_batch.size)
-
-      namespace_events.increment(:filtered, filtered_batch.size)
-      namespace_pipeline.increment(:filtered, filtered_batch.size)
-
-      output_batch(filtered_batch)
-
-      namespace_events.increment(:out, filtered_batch.size)
-      namespace_pipeline.increment(:out, filtered_batch.size)
-
-      inflight_batches_synchronize { set_current_thread_inflight_batch(nil) }
+      output_batch(batch)
+      @filter_queue_client.close_batch(batch)
     end
   end
 
-  def take_batch(batch_size, batch_delay)
-    batch = []
-    # Since this is externally synchronized in `worker_look` wec can guarantee that the visibility of an insight batch
-    # guaranteed to be a full batch not a partial batch
-    set_current_thread_inflight_batch(batch)
-
-    signal = false
-    batch_size.times do |t|
-      event = (t == 0) ? @input_queue.take : @input_queue.poll(batch_delay)
-
-      if event.nil?
-        next
-      elsif event == LogStash::SHUTDOWN || event == LogStash::FLUSH
-        # We MUST break here. If a batch consumes two SHUTDOWN events
-        # then another worker may have its SHUTDOWN 'stolen', thus blocking
-        # the pipeline. We should stop doing work after flush as well.
-        signal = event
-        break
-      else
-        batch << event
-      end
-    end
-
-    [batch, signal]
-  end
-
   def filter_batch(batch)
-    batch.reduce([]) do |acc,e|
-      if e.is_a?(LogStash::Event)
-        filtered = filter_func(e)
-        filtered.each {|fe| acc << fe unless fe.cancelled?}
+    batch.each do |event|
+      if event.is_a?(Event)
+        filter_func(event).each do |e|
+          # these are both original and generated events
+          batch.merge(e) unless e.cancelled?
+        end
       end
-      acc
     end
+    @filter_queue_client.add_filtered_metrics(batch)
+    @events_filtered.increment(batch.size)
   rescue Exception => e
     # Plugins authors should manage their own exceptions in the plugin code
     # but if an exception is raised up to the worker thread they are considered
@@ -327,31 +281,24 @@ def filter_batch(batch)
   # Take an array of events and send them to the correct output
   def output_batch(batch)
     # Build a mapping of { output_plugin => [events...]}
-    outputs_events = batch.reduce(Hash.new { |h, k| h[k] = [] }) do |acc, event|
+    output_events_map = Hash.new { |h, k| h[k] = [] }
+    batch.each do |event|
       # We ask the AST to tell us which outputs to send each event to
       # Then, we stick it in the correct bin
 
       # output_func should never return anything other than an Array but we have lots of legacy specs
       # that monkeypatch it and return nil. We can deprecate  "|| []" after fixing these specs
-      outputs_for_event = output_func(event) || []
-
-      outputs_for_event.each { |output| acc[output] << event }
-      acc
+      (output_func(event) || []).each do |output|
+        output_events_map[output].push(event)
+      end
     end
-
     # Now that we have our output to event mapping we can just invoke each output
     # once with its list of events
-    outputs_events.each { |output, events| output.multi_receive(events) }
-  end
-
-  def set_current_thread_inflight_batch(batch)
-    @inflight_batches[Thread.current] = batch
-  end
-
-  def inflight_batches_synchronize
-    @input_queue_pop_mutex.synchronize do
-      yield(@inflight_batches)
+    output_events_map.each do |output, events|
+      output.multi_receive(events)
     end
+    
+    @filter_queue_client.add_output_metrics(batch)
   end
 
   def wait_inputs
@@ -380,9 +327,9 @@ def start_input(plugin)
   end
 
   def inputworker(plugin)
-    LogStash::Util::set_thread_name("[#{pipeline_id}]<#{plugin.class.config_name}")
+    Util::set_thread_name("[#{pipeline_id}]<#{plugin.class.config_name}")
     begin
-      plugin.run(@input_queue)
+      plugin.run(@input_queue_client)
     rescue => e
       if plugin.stop?
         @logger.debug("Input plugin raised exception during shutdown, ignoring it.",
@@ -424,9 +371,9 @@ def shutdown(&before_stop)
 
     before_stop.call if block_given?
 
-    @logger.info "Closing inputs"
+    @logger.debug "Closing inputs"
     @inputs.each(&:do_stop)
-    @logger.info "Closed inputs"
+    @logger.debug "Closed inputs"
   end # def shutdown
 
   # After `shutdown` is called from an external thread this is called from the main thread to
@@ -435,8 +382,8 @@ def shutdown(&before_stop)
   def shutdown_workers
     # Each worker thread will receive this exactly once!
     @worker_threads.each do |t|
-      @logger.debug("Pushing shutdown", :thread => t)
-      @input_queue.push(LogStash::SHUTDOWN)
+      @logger.debug("Pushing shutdown", :thread => t.inspect)
+      @input_queue_client.push(SHUTDOWN)
     end
 
     @worker_threads.each do |t|
@@ -449,23 +396,38 @@ def shutdown_workers
   end
 
   def plugin(plugin_type, name, *args)
-    args << {} if args.empty?
+    @plugin_counter += 1
 
-    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
+    # Collapse the array of arguments into a single merged hash
+    args = args.reduce({}, &:merge)
 
-    klass = LogStash::Plugin.lookup(plugin_type, name)
+    id = if args["id"].nil? || args["id"].empty?
+           args["id"] = "#{@config_hash}-#{@plugin_counter}"
+         else
+           args["id"]
+         end
 
-    if plugin_type == "output"
-      LogStash::OutputDelegator.new(@logger, klass, default_output_workers, pipeline_scoped_metric.namespace(:outputs), *args)
-    elsif plugin_type == "filter"
-      LogStash::FilterDelegator.new(@logger, klass, pipeline_scoped_metric.namespace(:filters), *args)
-    else
-      klass.new(*args)
-    end
-  end
+    raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
+    
+    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
 
-  def default_output_workers
-    @settings[:pipeline_workers] || @settings[:default_pipeline_workers]
+    klass = Plugin.lookup(plugin_type, name)
+
+    # Scope plugins of type 'input' to 'inputs'
+    type_scoped_metric = pipeline_scoped_metric.namespace("#{plugin_type}s".to_sym)
+    plugin = if plugin_type == "output"
+               OutputDelegator.new(@logger, klass, type_scoped_metric,
+                                   OutputDelegatorStrategyRegistry.instance,
+                                   args)
+             elsif plugin_type == "filter"
+               FilterDelegator.new(@logger, klass, type_scoped_metric, args)
+             else # input
+               input_plugin = klass.new(args)
+               input_plugin.metric = type_scoped_metric.namespace(id)
+               input_plugin
+             end
+    
+    @plugins_by_id[id] = plugin
   end
 
   # for backward compatibility in devutils for the rspec helpers, this method is not used
@@ -476,7 +438,7 @@ def filter(event, &block)
   end
 
 
-  # perform filters flush and yeild flushed event to the passed block
+  # perform filters flush and yield flushed event to the passed block
   # @param options [Hash]
   # @option options [Boolean] :final => true to signal a final shutdown flush
   def flush_filters(options = {}, &block)
@@ -506,7 +468,7 @@ def shutdown_flusher
   def flush
     if @flushing.compare_and_set(false, true)
       @logger.debug? && @logger.debug("Pushing flush onto pipeline")
-      @input_queue.push(LogStash::FLUSH)
+      @input_queue_client.push(FLUSH)
     end
   end
 
@@ -520,23 +482,25 @@ def uptime
   end
 
   # perform filters flush into the output queue
+  #
+  # @param batch [ReadClient::ReadBatch]
   # @param options [Hash]
-  # @option options [Boolean] :final => true to signal a final shutdown flush
   def flush_filters_to_batch(batch, options = {})
+    options[:final] = batch.shutdown_signal_received?
     flush_filters(options) do |event|
       unless event.cancelled?
         @logger.debug? and @logger.debug("Pushing flushed events", :event => event)
-        batch << event
+        batch.merge(event)
       end
     end
 
     @flushing.set(false)
-  end # flush_filters_to_output!
+  end # flush_filters_to_batch
 
   def plugin_threads_info
     input_threads = @input_threads.select {|t| t.alive? }
     worker_threads = @worker_threads.select {|t| t.alive? }
-    (input_threads + worker_threads).map {|t| LogStash::Util.thread_info(t) }
+    (input_threads + worker_threads).map {|t| Util.thread_info(t) }
   end
 
   def stalling_threads_info
@@ -546,4 +510,24 @@ def stalling_threads_info
       .each {|t| t.delete("blocked_on") }
       .each {|t| t.delete("status") }
   end
+
+  def non_reloadable_plugins
+    (inputs + filters + outputs).select do |plugin|
+      RELOAD_INCOMPATIBLE_PLUGINS.include?(plugin.class.name)
+    end
+  end
+
+  # Sometimes we log stuff that will dump the pipeline which may contain
+  # sensitive information (like the raw syntax tree which can contain passwords)
+  # We want to hide most of what's in here
+  def inspect
+    {
+      :pipeline_id => @pipeline_id,
+      :settings => @settings.inspect,
+      :ready => @ready,
+      :running => @running,
+      :flushing => @flushing
+    }
+  end
+
 end end
diff --git a/logstash-core/lib/logstash/pipeline_reporter.rb b/logstash-core/lib/logstash/pipeline_reporter.rb
index c7ae6ca847c..ed73144c86c 100644
--- a/logstash-core/lib/logstash/pipeline_reporter.rb
+++ b/logstash-core/lib/logstash/pipeline_reporter.rb
@@ -39,7 +39,7 @@ def format_threads_by_plugin
     end
   end
 
-  def initialize(logger,pipeline)
+  def initialize(logger, pipeline)
     @logger = logger
     @pipeline = pipeline
   end
@@ -52,14 +52,14 @@ def snapshot
   end
 
   def to_hash
-    pipeline.inflight_batches_synchronize do |batch_map|
+    # pipeline.filter_queue_client.inflight_batches is synchronized
+    pipeline.filter_queue_client.inflight_batches do |batch_map|
       worker_states_snap = worker_states(batch_map) # We only want to run this once
       inflight_count = worker_states_snap.map {|s| s[:inflight_count] }.reduce(0, :+)
 
       {
         :events_filtered => events_filtered,
         :events_consumed => events_consumed,
-        :worker_count => pipeline.worker_threads.size,
         :inflight_count => inflight_count,
         :worker_states => worker_states_snap,
         :output_info => output_info,
@@ -83,32 +83,27 @@ def plugin_threads
     pipeline.plugin_threads
   end
 
-  # Not threadsafe! must be called within an `inflight_batches_synchronize` block
+  # Not threadsafe! ensure synchronization
   def worker_states(batch_map)
-      pipeline.worker_threads.map.with_index do |thread,idx|
-        status = thread.status || "dead"
-        inflight_count = batch_map[thread] ? batch_map[thread].size : 0
-        {
-          :status => status,
-          :alive => thread.alive?,
-          :index => idx,
-          :inflight_count => inflight_count
-        }
+    pipeline.worker_threads.map.with_index do |thread, idx|
+      status = thread.status || "dead"
+      inflight_count = batch_map[thread] ? batch_map[thread].size : 0
+      {
+        :status => status,
+        :alive => thread.alive?,
+        :index => idx,
+        :inflight_count => inflight_count
+      }
     end
   end
 
   def output_info
     pipeline.outputs.map do |output_delegator|
-      is_multi_worker = output_delegator.worker_count > 1
-
       {
         :type => output_delegator.config_name,
-        :config => output_delegator.config,
-        :is_multi_worker => is_multi_worker,
-        :events_received => output_delegator.events_received,
-        :workers => output_delegator.workers,
-        :busy_workers => output_delegator.busy_workers
+        :id => output_delegator.id,
+        :concurrency => output_delegator.concurrency,        
       }
     end
   end
-end end
\ No newline at end of file
+end end
diff --git a/logstash-core/lib/logstash/plugin.rb b/logstash-core/lib/logstash/plugin.rb
index d6c335e7279..c3ebd1c9abe 100644
--- a/logstash-core/lib/logstash/plugin.rb
+++ b/logstash-core/lib/logstash/plugin.rb
@@ -3,13 +3,13 @@
 require "logstash/logging"
 require "logstash/config/mixin"
 require "logstash/instrument/null_metric"
-require "cabin"
 require "concurrent"
 require "securerandom"
+require "logstash/plugins/registry"
 
 class LogStash::Plugin
+  include LogStash::Util::Loggable
   attr_accessor :params
-  attr_accessor :logger
 
   NL = "\n"
 
@@ -45,8 +45,11 @@ def eql?(other)
   end
 
   def initialize(params=nil)
+    @logger = self.logger
     @params = LogStash::Util.deep_clone(params)
-    @logger = Cabin::Channel.get(LogStash)
+    # The id should always be defined normally, but in tests that might not be the case
+    # In the future we may make this more strict in the Plugin API
+    @params["id"] ||= "#{self.class.config_name}_#{SecureRandom.uuid}"
   end
 
   # Return a uniq ID for this plugin configuration, by default
@@ -56,21 +59,13 @@ def initialize(params=nil)
   #
   # @return [String] A plugin ID
   def id
-    (@params["id"].nil? || @params["id"].empty?) ? SecureRandom.uuid : @params["id"]
-  end
-
-  # Return a unique_name, This is composed by the name of
-  # the plugin and the generated ID (of the configured one)
-  #
-  # @return [String] a unique name
-  def plugin_unique_name
-    "#{config_name}_#{id}"
+    @params["id"]
   end
 
   # close is called during shutdown, after the plugin worker
   # main task terminates
   def do_close
-    @logger.debug("closing", :plugin => self)
+    @logger.debug("closing", :plugin => self.class.name)
     close
   end
 
@@ -104,9 +99,15 @@ def metric=(new_metric)
   end
 
   def metric
-    @metric_plugin ||= enable_metric ? @metric : LogStash::Instrument::NullMetric.new
+    # We can disable metric per plugin if we want in the configuration
+    # we will use the NullMetric in this case.
+    @metric_plugin ||= if @enable_metric
+                         # Fallback when testing plugin and no metric collector are correctly configured.
+                         @metric.nil? ? LogStash::Instrument::NamespacedNullMetric.new : @metric
+                       else
+                         LogStash::Instrument::NamespacedNullMetric.new(@metric, :null)
+                       end
   end
-
   # return the configured name of this plugin
   # @return [String] The name of the plugin defined by `config_name`
   def config_name
@@ -117,23 +118,22 @@ def config_name
   # Look up a plugin by type and name.
   def self.lookup(type, name)
     path = "logstash/#{type}s/#{name}"
-
-    # first check if plugin already exists in namespace and continue to next step if not
-    begin
-      return namespace_lookup(type, name)
-    rescue NameError
-      logger.debug("Plugin not defined in namespace, checking for plugin file", :type => type, :name => name, :path => path)
+    LogStash::Registry.instance.lookup(type ,name) do |plugin_klass, plugin_name|
+      is_a_plugin?(plugin_klass, plugin_name)
     end
-
-    # try to load the plugin file. ex.: lookup("filter", "grok") will require logstash/filters/grok
-    require(path)
-
-    # check again if plugin is now defined in namespace after the require
-    namespace_lookup(type, name)
+    
   rescue LoadError, NameError => e
+    logger.debug("Problems loading the plugin with", :type => type, :name => name, :path => path)
     raise(LogStash::PluginLoadingError, I18n.t("logstash.pipeline.plugin-loading-error", :type => type, :name => name, :path => path, :error => e.to_s))
   end
 
+  public
+  def self.declare_plugin(type, name)
+    path = "logstash/#{type}s/#{name}"
+    registry = LogStash::Registry.instance
+    registry.register(path, self)
+  end
+
   private
   # lookup a plugin by type and name in the existing LogStash module namespace
   # ex.: namespace_lookup("filter", "grok") looks for LogStash::Filters::Grok
@@ -160,9 +160,4 @@ def self.namespace_lookup(type, name)
   def self.is_a_plugin?(klass, name)
     klass.ancestors.include?(LogStash::Plugin) && klass.respond_to?(:config_name) && klass.config_name == name
   end
-
-  # @return [Cabin::Channel] logger channel for class methods
-  def self.logger
-    @logger ||= Cabin::Channel.get(LogStash)
-  end
 end # class LogStash::Plugin
diff --git a/logstash-core/lib/logstash/plugins/registry.rb b/logstash-core/lib/logstash/plugins/registry.rb
new file mode 100644
index 00000000000..cab0181f764
--- /dev/null
+++ b/logstash-core/lib/logstash/plugins/registry.rb
@@ -0,0 +1,86 @@
+# encoding: utf-8
+require 'singleton'
+require "rubygems/package"
+require "logstash/util/loggable"
+
+module LogStash
+  class Registry
+    include LogStash::Util::Loggable
+
+    ##
+    # Placeholder class for registered plugins
+    ##
+    class Plugin
+      attr_reader :type, :name
+
+      def initialize(type, name)
+        @type  = type
+        @name  = name
+      end
+
+      def path
+        "logstash/#{type}s/#{name}"
+      end
+
+      def cannonic_gem_name
+        "logstash-#{type}-#{name}"
+      end
+
+      def installed?
+        find_plugin_spec(cannonic_gem_name).any?
+      end
+
+      private
+
+      def find_plugin_spec(name)
+        specs = ::Gem::Specification.find_all_by_name(name)
+        specs.select{|spec| logstash_plugin_spec?(spec)}
+      end
+
+      def logstash_plugin_spec?(spec)
+        spec.metadata && spec.metadata["logstash_plugin"] == "true"
+      end
+
+    end
+
+    include Singleton
+
+    def initialize
+      @registry = {}
+      @logger = self.logger
+    end
+
+    def lookup(type, plugin_name, &block)
+
+      plugin = Plugin.new(type, plugin_name)
+
+      if plugin.installed?
+        return @registry[plugin.path] if registered?(plugin.path)
+        require plugin.path
+        klass = @registry[plugin.path]
+        if block_given? # if provided pass a block to do validation
+          raise LoadError unless block.call(klass, plugin_name)
+        end
+        return klass
+      else
+        # The plugin was defined directly in the code, so there is no need to use the
+        # require way of loading classes
+        return @registry[plugin.path] if registered?(plugin.path)
+        raise LoadError
+      end
+    rescue => e
+      @logger.warn("Problems loading a plugin with", :type => type, :name => plugin, :path => plugin.path,
+                   :error_message => e.message, :error_class => e.class, :error_backtrace => e.backtrace)
+      raise LoadError, "Problems loading the requested plugin named #{plugin_name} of type #{type}. Error: #{e.class} #{e.message}"
+    end
+
+    def register(path, klass)
+      @registry[path] = klass
+    end
+
+    def registered?(path)
+      @registry.has_key?(path)
+    end
+
+  end
+end
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index a8984244a46..4d9ca67dbcd 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -3,7 +3,7 @@
 Encoding.default_external = Encoding::UTF_8
 $DEBUGLIST = (ENV["DEBUG"] || "").split(",")
 
-require "clamp" # gem 'clamp'
+require "clamp"
 require "net/http"
 require "logstash/environment"
 
@@ -12,185 +12,254 @@
 require "logstash/namespace"
 require "logstash/agent"
 require "logstash/config/defaults"
+require "logstash/shutdown_watcher"
+require "logstash/patches/clamp"
+require "logstash/settings"
+require "logstash/version"
+
+class LogStash::Runner < Clamp::StrictCommand
+  include LogStash::Util::Loggable
+  # The `path.settings` and `path.logs` need to be defined in the runner instead of the `logstash-core/lib/logstash/environment.rb`
+  # because the `Environment::LOGSTASH_HOME` doesn't exist in the context of the `logstash-core` gem.
+  #
+  # See issue https://github.com/elastic/logstash/issues/5361
+  LogStash::SETTINGS.register(LogStash::Setting::String.new("path.settings", ::File.join(LogStash::Environment::LOGSTASH_HOME, "config")))
+  LogStash::SETTINGS.register(LogStash::Setting::String.new("path.logs", ::File.join(LogStash::Environment::LOGSTASH_HOME, "logs")))
 
-class LogStash::Runner < Clamp::Command
+  # Node Settings
+  option ["-n", "--node.name"], "NAME",
+    I18n.t("logstash.runner.flag.node_name"),
+    :attribute_name => "node.name",
+    :default => LogStash::SETTINGS.get_default("node.name")
 
-  option ["-f", "--config"], "CONFIG_PATH",
+  # Config Settings
+  option ["-f", "--path.config"], "CONFIG_PATH",
     I18n.t("logstash.runner.flag.config"),
-    :attribute_name => :config_path
+    :attribute_name => "path.config"
 
-  option "-e", "CONFIG_STRING",
+  option ["-e", "--config.string"], "CONFIG_STRING",
     I18n.t("logstash.runner.flag.config-string",
-           :default_input => LogStash::Config::Defaults.input,
-           :default_output => LogStash::Config::Defaults.output),
-    :default => nil, :attribute_name => :config_string
+      :default_input => LogStash::Config::Defaults.input,
+      :default_output => LogStash::Config::Defaults.output),
+    :default => LogStash::SETTINGS.get_default("config.string"),
+    :attribute_name => "config.string"
 
-  option ["-w", "--pipeline-workers"], "COUNT",
+  # Pipeline settings
+  option ["-w", "--pipeline.workers"], "COUNT",
     I18n.t("logstash.runner.flag.pipeline-workers"),
-    :attribute_name => :pipeline_workers,
-    :default => LogStash::Pipeline::DEFAULT_SETTINGS[:default_pipeline_workers]
-
-  option ["-b", "--pipeline-batch-size"], "SIZE",
-         I18n.t("logstash.runner.flag.pipeline-batch-size"),
-         :attribute_name => :pipeline_batch_size,
-         :default => LogStash::Pipeline::DEFAULT_SETTINGS[:pipeline_batch_size]
-
-  option ["-u", "--pipeline-batch-delay"], "DELAY_IN_MS",
-         I18n.t("logstash.runner.flag.pipeline-batch-delay"),
-         :attribute_name => :pipeline_batch_delay,
-         :default => LogStash::Pipeline::DEFAULT_SETTINGS[:pipeline_batch_delay]
+    :attribute_name => "pipeline.workers",
+    :default => LogStash::SETTINGS.get_default("pipeline.workers")
 
-  option ["-l", "--log"], "FILE",
-    I18n.t("logstash.runner.flag.log"),
-    :attribute_name => :log_file
+  option ["-b", "--pipeline.batch.size"], "SIZE",
+    I18n.t("logstash.runner.flag.pipeline-batch-size"),
+    :attribute_name => "pipeline.batch.size",
+    :default => LogStash::SETTINGS.get_default("pipeline.batch.size")
 
-  # Old support for the '-v' flag'
-  option "-v", :flag,
-    I18n.t("logstash.runner.flag.verbosity"),
-    :attribute_name => :verbosity, :multivalued => true
+  option ["-u", "--pipeline.batch.delay"], "DELAY_IN_MS",
+    I18n.t("logstash.runner.flag.pipeline-batch-delay"),
+    :attribute_name => "pipeline.batch.delay",
+    :default => LogStash::SETTINGS.get_default("pipeline.batch.delay")
 
-  option "--quiet", :flag, I18n.t("logstash.runner.flag.quiet")
-  option "--verbose", :flag, I18n.t("logstash.runner.flag.verbose")
-  option "--debug", :flag, I18n.t("logstash.runner.flag.debug")
+  option ["--pipeline.unsafe_shutdown"], :flag,
+    I18n.t("logstash.runner.flag.unsafe_shutdown"),
+    :attribute_name => "pipeline.unsafe_shutdown",
+    :default => LogStash::SETTINGS.get_default("pipeline.unsafe_shutdown")
 
-  option ["-V", "--version"], :flag,
-    I18n.t("logstash.runner.flag.version")
+  # Data Path Setting
+  option ["--path.data"] , "PATH",
+    I18n.t("logstash.runner.flag.datapath"),
+    :attribute_name => "path.data",
+    :default => LogStash::SETTINGS.get_default("path.data")
 
-  option ["-p", "--pluginpath"] , "PATH",
+  # Plugins Settings
+  option ["-p", "--path.plugins"] , "PATH",
     I18n.t("logstash.runner.flag.pluginpath"),
-    :multivalued => true,
-    :attribute_name => :plugin_paths
+    :multivalued => true, :attribute_name => "path.plugins",
+    :default => LogStash::SETTINGS.get_default("path.plugins")
 
-  option ["-t", "--configtest"], :flag,
-    I18n.t("logstash.runner.flag.configtest"),
-    :attribute_name => :config_test
+  # Logging Settings
+  option ["-l", "--path.logs"], "PATH",
+    I18n.t("logstash.runner.flag.log"),
+    :attribute_name => "path.logs",
+    :default => LogStash::SETTINGS.get_default("path.logs")
 
-  option "--[no-]allow-unsafe-shutdown", :flag,
-    I18n.t("logstash.runner.flag.unsafe_shutdown"),
-    :attribute_name => :unsafe_shutdown,
-    :default => false
+  option "--log.level", "LEVEL", I18n.t("logstash.runner.flag.log_level"),
+    :default => LogStash::SETTINGS.get_default("log.level"),
+    :attribute_name => "log.level"
+
+  option "--config.debug", :flag,
+    I18n.t("logstash.runner.flag.config_debug"),
+    :default => LogStash::SETTINGS.get_default("config.debug"),
+    :attribute_name => "config.debug"
 
+  # Other settings
   option ["-i", "--interactive"], "SHELL",
     I18n.t("logstash.runner.flag.rubyshell"),
-    :attribute_name => :ruby_shell
+    :attribute_name => "interactive"
 
-  option ["-n", "--node-name"], "NAME",
-    I18n.t("logstash.runner.flag.node_name"),
-    :attribute_name => :node_name
+  option ["-V", "--version"], :flag,
+    I18n.t("logstash.runner.flag.version")
 
-  option ["-r", "--[no-]auto-reload"], :flag,
+  option ["-t", "--config.test_and_exit"], :flag,
+    I18n.t("logstash.runner.flag.configtest"),
+    :attribute_name => "config.test_and_exit",
+    :default => LogStash::SETTINGS.get_default("config.test_and_exit")
+
+  option ["-r", "--config.reload.automatic"], :flag,
     I18n.t("logstash.runner.flag.auto_reload"),
-    :attribute_name => :auto_reload, :default => false
+    :attribute_name => "config.reload.automatic",
+    :default => LogStash::SETTINGS.get_default("config.reload.automatic")
 
-  option ["--reload-interval"], "RELOAD_INTERVAL",
+  option ["--config.reload.interval"], "RELOAD_INTERVAL",
     I18n.t("logstash.runner.flag.reload_interval"),
-    :attribute_name => :reload_interval, :default => 3, &:to_i
-
-  option ["--http-host"], "WEB_API_HTTP_HOST",
-    I18n.t("logstash.web_api.flag.http_host"),
-    :attribute_name => :web_api_http_host, :default => "127.0.0.1"
+    :attribute_name => "config.reload.interval",
+    :default => LogStash::SETTINGS.get_default("config.reload.interval")
+
+  option ["--http.host"], "HTTP_HOST",
+    I18n.t("logstash.runner.flag.http_host"),
+    :attribute_name => "http.host",
+    :default => LogStash::SETTINGS.get_default("http.host")
+
+  option ["--http.port"], "HTTP_PORT",
+    I18n.t("logstash.runner.flag.http_port"),
+    :attribute_name => "http.port",
+    :default => LogStash::SETTINGS.get_default("http.port")
+
+  option ["--log.format"], "FORMAT",
+    I18n.t("logstash.runner.flag.log_format"),
+    :attribute_name => "log.format",
+    :default => LogStash::SETTINGS.get_default("log.format")
+
+  option ["--path.settings"], "SETTINGS_DIR",
+    I18n.t("logstash.runner.flag.path_settings"),
+    :attribute_name => "path.settings",
+    :default => LogStash::SETTINGS.get_default("path.settings")
+
+  ### DEPRECATED FLAGS ###
+  deprecated_option ["--verbose"], :flag,
+    I18n.t("logstash.runner.flag.verbose"),
+    :new_flag => "log.level", :new_value => "info"
+
+  deprecated_option ["--debug"], :flag,
+    I18n.t("logstash.runner.flag.debug"),
+    :new_flag => "log.level", :new_value => "debug"
+
+  deprecated_option ["--quiet"], :flag,
+    I18n.t("logstash.runner.flag.quiet"),
+    :new_flag => "log.level", :new_value => "error"
 
-  option ["--http-port"], "WEB_API_HTTP_PORT",
-    I18n.t("logstash.web_api.flag.http_port"),
-    :attribute_name => :web_api_http_port, :default => 9600
+  attr_reader :agent
 
-  def pipeline_workers=(pipeline_workers_value)
-    @pipeline_settings[:pipeline_workers] = validate_positive_integer(pipeline_workers_value)
+  def initialize(*args)
+    @settings = LogStash::SETTINGS
+    super(*args)
   end
 
-  def pipeline_batch_size=(pipeline_batch_size_value)
-    @pipeline_settings[:pipeline_batch_size] = validate_positive_integer(pipeline_batch_size_value)
-  end
+  def run(args)
+    settings_path = fetch_settings_path(args)
 
-  def pipeline_batch_delay=(pipeline_batch_delay_value)
-    @pipeline_settings[:pipeline_batch_delay] = validate_positive_integer(pipeline_batch_delay_value)
-  end
+    @settings.set("path.settings", settings_path) if settings_path
 
-  def validate_positive_integer(str_arg)
-    int_arg = str_arg.to_i
-    if str_arg !~ /^\d+$/ || int_arg < 1
-      raise ArgumentError, "Expected a positive integer, got '#{str_arg}'"
+    begin
+      LogStash::SETTINGS.from_yaml(LogStash::SETTINGS.get("path.settings"))
+    rescue Errno::ENOENT 
+      unless cli_help?(args)
+        $stderr.puts "ERROR: Logstash requires a setting file which is typically located in $LS_HOME/config or /etc/logstash. If you installed Logstash through a package and are starting it manually, please specify the location to this settings file by passing --path.settings /etc/logstash"
+        return 1
+      end   
+    rescue => e
+      # abort unless we're just looking for the help
+      unless cli_help?(args)
+        $stderr.puts "ERROR: Failed to load settings file from \"path.settings\". Aborting... path.setting=#{LogStash::SETTINGS.get("path.settings")}, exception=#{e.class}, message=>#{e.message}"
+        return 1
+      end
     end
 
-    int_arg
-  end
-
-  attr_reader :agent
-
-  def initialize(*args)
-    @logger = Cabin::Channel.get(LogStash)
-    @pipeline_settings ||= { :pipeline_id => "main" }
-    super(*args)
+    super(*[args])
   end
 
   def execute
     require "logstash/util"
     require "logstash/util/java_version"
     require "stud/task"
-    require "cabin" # gem 'cabin'
-
 
     # Configure Logstash logging facility, this need to be done before everything else to
     # make sure the logger has the correct settings and the log level is correctly defined.
-    configure_logging(log_file)
+    java.lang.System.setProperty("ls.logs", setting("path.logs"))
+    java.lang.System.setProperty("ls.log.format", setting("log.format"))
+    java.lang.System.setProperty("ls.log.level", setting("log.level"))
+    unless java.lang.System.getProperty("log4j.configurationFile")
+      log4j_config_location = ::File.join(setting("path.settings"), "log4j2.properties")
+      LogStash::Logging::Logger::initialize("file://" + log4j_config_location)
+    end
+    # override log level that may have been introduced from a custom log4j config file
+    LogStash::Logging::Logger::configure_logging(setting("log.level"))
+    
+    # Adding this here because a ton of LS users install LS via packages and try to manually 
+    # start Logstash using bin/logstash. See #5986. I think not logging to console is good for 
+    # services, but until LS users re-learn that logs end up in path.logs, we should keep this 
+    # message. Otherwise we'll be answering the same question again and again.
+    puts "Sending Logstash logs to #{setting("path.logs")} which is now configured via log4j2.properties."
+
+    if setting("config.debug") && logger.debug?
+      logger.warn("--config.debug was specified, but log.level was not set to \'debug\'! No config info will be logged.")
+    end
 
     LogStash::Util::set_thread_name(self.class.name)
 
     if RUBY_VERSION < "1.9.2"
-      $stderr.puts "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
+      logger.fatal "Ruby 1.9.2 or later is required. (You are running: " + RUBY_VERSION + ")"
       return 1
     end
 
-    # Print a warning to STDERR for bad java versions
-    LogStash::Util::JavaVersion.warn_on_bad_java_version
+    # Exit on bad java versions
+    java_version = LogStash::Util::JavaVersion.version
+    if LogStash::Util::JavaVersion.bad_java_version?(java_version)
+      logger.fatal "Java version 1.8.0 or later is required. (You are running: #{java_version})"
+      return 1
+    end
 
-    LogStash::ShutdownWatcher.unsafe_shutdown = unsafe_shutdown?
-    LogStash::ShutdownWatcher.logger = @logger
+    LogStash::ShutdownWatcher.unsafe_shutdown = setting("pipeline.unsafe_shutdown")
 
-    configure
+    configure_plugin_paths(setting("path.plugins"))
 
     if version?
       show_version
       return 0
     end
 
-    return start_shell(@ruby_shell, binding) if @ruby_shell
+    return start_shell(setting("interactive"), binding) if setting("interactive")
+
+    @settings.validate_all
+
+    @settings.format_settings.each {|line| logger.debug(line) }
 
-    if config_string.nil? && config_path.nil?
+    if setting("config.string").nil? && setting("path.config").nil?
       fail(I18n.t("logstash.runner.missing-configuration"))
     end
 
-    if @auto_reload && config_path.nil?
+    if setting("config.reload.automatic") && setting("path.config").nil?
       # there's nothing to reload
       signal_usage_error(I18n.t("logstash.runner.reload-without-config-path"))
     end
 
-    if config_test?
-      config_loader = LogStash::Config::Loader.new(@logger, config_test?)
-      config_str = config_loader.format_config(config_path, config_string)
-      config_error = LogStash::Pipeline.config_valid?(config_str)
-      if config_error == true
-        @logger.terminal "Configuration OK"
+    if setting("config.test_and_exit")
+      config_loader = LogStash::Config::Loader.new(logger)
+      config_str = config_loader.format_config(setting("path.config"), setting("config.string"))
+      begin
+        LogStash::Pipeline.new(config_str)
+        puts "Configuration OK"
+        logger.info "Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash"
         return 0
-      else
-        @logger.fatal I18n.t("logstash.error", :error => config_error)
+      rescue => e
+        logger.fatal I18n.t("logstash.runner.invalid-configuration", :error => e.message)
         return 1
       end
     end
 
-    @agent = create_agent(:logger => @logger,
-                          :auto_reload => @auto_reload,
-                          :reload_interval => @reload_interval,
-                          :collect_metric => true,
-                          :debug => debug?,
-                          :node_name => node_name,
-                          :web_api_http_host => @web_api_http_host,
-                          :web_api_http_port => @web_api_http_port)
+    @agent = create_agent(@settings)
 
-    @agent.register_pipeline("main", @pipeline_settings.merge({
-                          :config_string => config_string,
-                          :config_path => config_path
-                          }))
+    @agent.register_pipeline("main", @settings)
 
     # enable sigint/sigterm before starting the agent
     # to properly handle a stalled agent
@@ -200,12 +269,16 @@ def execute
     @agent_task = Stud::Task.new { @agent.execute }
 
     # no point in enabling config reloading before the agent starts
-    sighup_id = trap_sighup()
+    # also windows doesn't have SIGHUP. we can skip it
+    sighup_id = LogStash::Environment.windows? ? nil : trap_sighup()
 
     agent_return = @agent_task.wait
 
     @agent.shutdown
 
+    # flush any outstanding log messages during shutdown
+    org.apache.logging.log4j.LogManager.shutdown
+
     agent_return
 
   rescue Clamp::UsageError => e
@@ -213,7 +286,7 @@ def execute
     show_short_help
     return 1
   rescue => e
-    @logger.fatal I18n.t("oops", :error => e, :backtrace => e.backtrace)
+    logger.fatal(I18n.t("oops"), :error => e, :backtrace => e.backtrace)
     return 1
   ensure
     Stud::untrap("INT", sigint_id) unless sigint_id.nil?
@@ -225,15 +298,14 @@ def execute
   def show_version
     show_version_logstash
 
-    if [:info, :debug].include?(verbosity?) || debug? || verbose?
+    if logger.info?
       show_version_ruby
       show_version_java if LogStash::Environment.jruby?
-      show_gems if [:debug].include?(verbosity?) || debug?
+      show_gems if logger.debug?
     end
   end # def show_version
 
   def show_version_logstash
-    require "logstash/version"
     puts "logstash #{LOGSTASH_VERSION}"
   end # def show_version_logstash
 
@@ -254,13 +326,6 @@ def show_gems
     end
   end # def show_gems
 
-  # Do any start-time configuration.
-  #
-  # Log file stuff, plugin path checking, etc.
-  def configure
-    configure_plugin_paths(plugin_paths)
-  end # def configure
-
   # add the given paths for ungemified/bare plugins lookups
   # @param paths [String, Array<String>] plugins path string or list of path strings to add
   def configure_plugin_paths(paths)
@@ -274,53 +339,6 @@ def create_agent(*args)
     LogStash::Agent.new(*args)
   end
 
-  # Point logging at a specific path.
-  def configure_logging(path)
-    @logger = Cabin::Channel.get(LogStash)
-    # Set with the -v (or -vv...) flag
-    if quiet?
-      @logger.level = :error
-    elsif verbose?
-      @logger.level = :info
-    elsif debug?
-      @logger.level = :debug
-    else
-      # Old support for the -v and -vv stuff.
-      if verbosity? && verbosity?.any?
-        # this is an array with length of how many times the flag is given
-        if verbosity?.length == 1
-          @logger.warn("The -v flag is deprecated and will be removed in a future release. You should use --verbose instead.")
-          @logger.level = :info
-        else
-          @logger.warn("The -vv flag is deprecated and will be removed in a future release. You should use --debug instead.")
-          @logger.level = :debug
-        end
-      else
-        @logger.level = :warn
-      end
-    end
-
-    if log_file
-      # TODO(sissel): Implement file output/rotation in Cabin.
-      # TODO(sissel): Catch exceptions, report sane errors.
-      begin
-        @log_fd.close if @log_fd
-        @log_fd = File.new(path, "a")
-      rescue => e
-        fail(I18n.t("logstash.runner.configuration.log_file_failed",
-                    :path => path, :error => e))
-      end
-
-      @logger.subscribe(STDOUT, :level => :fatal)
-      @logger.subscribe(@log_fd)
-      @logger.terminal "Sending logstash logs to #{path}."
-    else
-      @logger.subscribe(STDOUT)
-    end
-
-    # TODO(sissel): redirect stdout/stderr to the log as well
-    # http://jira.codehaus.org/browse/JRUBY-7003
-  end # def configure_logging
 
   # Emit a failure message and abort.
   def fail(message)
@@ -349,14 +367,14 @@ def start_shell(shell, start_binding)
 
   def trap_sighup
     Stud::trap("HUP") do
-      @logger.warn(I18n.t("logstash.agent.sighup"))
+      logger.warn(I18n.t("logstash.agent.sighup"))
       @agent.reload_state!
     end
   end
 
   def trap_sigterm
     Stud::trap("TERM") do
-      @logger.warn(I18n.t("logstash.agent.sigterm"))
+      logger.warn(I18n.t("logstash.agent.sigterm"))
       @agent_task.stop!
     end
   end
@@ -364,15 +382,43 @@ def trap_sigterm
   def trap_sigint
     Stud::trap("INT") do
       if @interrupted_once
-        @logger.fatal(I18n.t("logstash.agent.forced_sigint"))
+        logger.fatal(I18n.t("logstash.agent.forced_sigint"))
         exit
       else
-        @logger.warn(I18n.t("logstash.agent.sigint"))
-        Thread.new(@logger) {|logger| sleep 5; logger.warn(I18n.t("logstash.agent.slow_shutdown")) }
+        logger.warn(I18n.t("logstash.agent.sigint"))
+        Thread.new(logger) {|lg| sleep 5; lg.warn(I18n.t("logstash.agent.slow_shutdown")) }
         @interrupted_once = true
         @agent_task.stop!
       end
     end
   end
 
-end # class LogStash::Runner
+  def setting(key)
+    @settings.get_value(key)
+  end
+
+  # where can I find the logstash.yml file?
+  # 1. look for a "--path.settings path"
+  # 2. look for a "--path.settings=path"
+  # 3. check if the LS_SETTINGS_DIR environment variable is set
+  # 4. return nil if not found
+  def fetch_settings_path(cli_args)
+    if i=cli_args.find_index("--path.settings")
+      cli_args[i+1]
+    elsif settings_arg = cli_args.find {|v| v.match(/--path.settings=/) }
+      match = settings_arg.match(/--path.settings=(.*)/)
+      match[1]
+    elsif ENV['LS_SETTINGS_DIR']
+      ENV['LS_SETTINGS_DIR']
+    else
+      nil
+    end
+  end
+  
+  # is the user asking for CLI help subcommand?
+  def cli_help?(args)
+    # I know, double negative
+    !(["--help", "-h"] & args).empty?
+  end  
+
+end
diff --git a/logstash-core/lib/logstash/settings.rb b/logstash-core/lib/logstash/settings.rb
new file mode 100644
index 00000000000..10ec1f5f7d3
--- /dev/null
+++ b/logstash-core/lib/logstash/settings.rb
@@ -0,0 +1,441 @@
+# encoding: utf-8
+require "logstash/util/loggable"
+
+module LogStash
+  class Settings
+
+    def initialize
+      @settings = {}
+    end
+
+    def register(setting)
+      if @settings.key?(setting.name)
+        raise ArgumentError.new("Setting \"#{setting.name}\" has already been registered as #{setting.inspect}")
+      else
+        @settings[setting.name] = setting
+      end
+    end
+
+    def get_setting(setting_name)
+      setting = @settings[setting_name]
+      raise ArgumentError.new("Setting \"#{setting_name}\" hasn't been registered") if setting.nil?
+      setting
+    end
+
+    def get_subset(setting_regexp)
+      regexp = setting_regexp.is_a?(Regexp) ? setting_regexp : Regexp.new(setting_regexp)
+      settings = self.class.new
+      @settings.each do |setting_name, setting|
+        next unless setting_name.match(regexp)
+        settings.register(setting.clone)
+      end
+      settings
+    end
+
+    def set?(setting_name)
+      get_setting(setting_name).set?
+    end
+
+    def clone
+      get_subset(".*")
+    end
+
+    def get_default(setting_name)
+      get_setting(setting_name).default
+    end
+
+    def get_value(setting_name)
+      get_setting(setting_name).value
+    end
+    alias_method :get, :get_value
+
+    def set_value(setting_name, value)
+      get_setting(setting_name).set(value)
+    end
+    alias_method :set, :set_value
+
+    def to_hash
+      hash = {}
+      @settings.each do |name, setting|
+        hash[name] = setting.value
+      end
+      hash
+    end
+
+    def merge(hash)
+      hash.each {|key, value| set_value(key, value) }
+      self
+    end
+
+    def format_settings
+      output = []
+      output << "-------- Logstash Settings (* means modified) ---------"
+      @settings.each do |setting_name, setting|
+        value = setting.value
+        default_value = setting.default
+        if default_value == value # print setting and its default value
+          output << "#{setting_name}: #{value.inspect}" unless value.nil?
+        elsif default_value.nil? # print setting and warn it has been set
+          output << "*#{setting_name}: #{value.inspect}"
+        elsif value.nil? # default setting not set by user
+          output << "#{setting_name}: #{default_value.inspect}"
+        else # print setting, warn it has been set, and show default value
+          output << "*#{setting_name}: #{value.inspect} (default: #{default_value.inspect})"
+        end
+      end
+      output << "--------------- Logstash Settings -------------------"
+      output
+    end
+
+    def reset
+      @settings.values.each(&:reset)
+    end
+
+    def from_yaml(yaml_path)
+      settings = read_yaml(::File.join(yaml_path, "logstash.yml"))
+      self.merge(flatten_hash(settings))
+    end
+
+    def validate_all
+      @settings.each do |name, setting|
+        setting.validate_value
+      end
+    end
+
+    private
+    def read_yaml(path)
+      YAML.safe_load(IO.read(path)) || {}
+    end
+
+    def flatten_hash(h,f="",g={})
+      return g.update({ f => h }) unless h.is_a? Hash
+      if f.empty?
+        h.each { |k,r| flatten_hash(r,k,g) }
+      else
+        h.each { |k,r| flatten_hash(r,"#{f}.#{k}",g) }
+      end
+      g
+    end
+  end
+
+  class Setting
+    include LogStash::Util::Loggable
+
+    attr_reader :name, :default
+
+    def initialize(name, klass, default=nil, strict=true, &validator_proc)
+      @name = name
+      unless klass.is_a?(Class)
+        raise ArgumentError.new("Setting \"#{@name}\" must be initialized with a class (received #{klass})")
+      end
+      @klass = klass
+      @validator_proc = validator_proc
+      @value = nil
+      @value_is_set = false
+      @strict = strict
+
+      validate(default) if @strict
+      @default = default
+    end
+
+    def value
+      @value_is_set ? @value : default
+    end
+
+    def set?
+      @value_is_set
+    end
+
+    def strict?
+      @strict
+    end
+
+    def set(value)
+      validate(value) if @strict
+      @value = value
+      @value_is_set = true
+      @value
+    end
+
+    def reset
+      @value = nil
+      @value_is_set = false
+    end
+
+    def to_hash
+      {
+        "name" => @name,
+        "klass" => @klass,
+        "value" => @value,
+        "value_is_set" => @value_is_set,
+        "default" => @default,
+        # Proc#== will only return true if it's the same obj
+        # so no there's no point in comparing it
+        # also thereś no use case atm to return the proc
+        # so let's not expose it
+        #"validator_proc" => @validator_proc
+      }
+    end
+
+    def ==(other)
+      self.to_hash == other.to_hash
+    end
+
+    def validate_value
+      validate(value)
+    end
+
+    protected
+    def validate(input)
+      if !input.is_a?(@klass)
+        raise ArgumentError.new("Setting \"#{@name}\" must be a #{@klass}. Received: #{input} (#{input.class})")
+      end
+
+      if @validator_proc && !@validator_proc.call(input)
+        raise ArgumentError.new("Failed to validate setting \"#{@name}\" with value: #{input}")
+      end
+    end
+
+    class Coercible < Setting
+      def initialize(name, klass, default=nil, strict=true, &validator_proc)
+        @name = name
+        unless klass.is_a?(Class)
+          raise ArgumentError.new("Setting \"#{@name}\" must be initialized with a class (received #{klass})")
+        end
+        @klass = klass
+        @validator_proc = validator_proc
+        @value = nil
+        @value_is_set = false
+
+        if strict
+          coerced_default = coerce(default)
+          validate(coerced_default)
+          @default = coerced_default
+        else
+          @default = default
+        end
+      end
+
+      def set(value)
+        coerced_value = coerce(value)
+        validate(coerced_value)
+        @value = coerce(coerced_value)
+        @value_is_set = true
+        @value
+      end
+
+      def coerce(value)
+        raise NotImplementedError.new("Please implement #coerce for #{self.class}")
+      end
+    end
+    ### Specific settings #####
+
+    class Boolean < Coercible
+      def initialize(name, default, strict=true, &validator_proc)
+        super(name, Object, default, strict, &validator_proc)
+      end
+
+      def coerce(value)
+        case value
+        when TrueClass, "true"
+          true
+        when FalseClass, "false"
+          false
+        else
+          raise ArgumentError.new("could not coerce #{value} into a boolean")
+        end
+      end
+    end
+
+    class Numeric < Coercible
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Numeric, default, strict)
+      end
+
+      def coerce(v)
+        return v if v.is_a?(::Numeric)
+
+        # I hate these "exceptions as control flow" idioms
+        # but Ruby's `"a".to_i => 0` makes it hard to do anything else.
+        coerced_value = (Integer(v) rescue nil) || (Float(v) rescue nil)
+
+        if coerced_value.nil?
+          raise ArgumentError.new("Failed to coerce value to Numeric. Received #{v} (#{v.class})")
+        else
+          coerced_value
+        end
+      end
+    end
+
+    class Integer < Coercible
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Integer, default, strict)
+      end
+
+      def coerce(value)
+        return value unless value.is_a?(::String)
+
+        coerced_value = Integer(value) rescue nil
+
+        if coerced_value.nil?
+          raise ArgumentError.new("Failed to coerce value to Integer. Received #{value} (#{value.class})")
+        else
+          coerced_value
+        end
+      end
+    end
+
+    class PositiveInteger < Integer
+      def initialize(name, default=nil, strict=true)
+        super(name, default, strict) do |v|
+          if v > 0
+            true
+          else
+            raise ArgumentError.new("Number must be bigger than 0. Received: #{v}")
+          end
+        end
+      end
+    end
+
+    class Port < Integer
+      VALID_PORT_RANGE = 1..65535
+
+      def initialize(name, default=nil, strict=true)
+        super(name, default, strict) { |value| valid?(value) }
+      end
+
+      def valid?(port)
+        VALID_PORT_RANGE.cover?(port)
+      end
+    end
+
+    class PortRange < Coercible
+      PORT_SEPARATOR = "-"
+
+      def initialize(name, default=nil, strict=true)
+        super(name, ::Range, default, strict=true) { |value| valid?(value) }
+      end
+
+      def valid?(range)
+        Port::VALID_PORT_RANGE.first <= range.first && Port::VALID_PORT_RANGE.last >= range.last
+      end
+
+      def coerce(value)
+        case value
+        when ::Range
+          value
+        when ::Fixnum
+          value..value
+        when ::String
+          first, last = value.split(PORT_SEPARATOR)
+          last = first if last.nil?
+          begin
+            (Integer(first))..(Integer(last))
+          rescue ArgumentError # Trap and reraise a more human error
+            raise ArgumentError.new("Could not coerce #{value} into a port range")
+          end
+        else
+          raise ArgumentError.new("Could not coerce #{value} into a port range")
+        end
+      end
+
+      def validate(value)
+        unless valid?(value)
+          raise ArgumentError.new("Invalid value \"#{value}, valid options are within the range of #{Port::VALID_PORT_RANGE.first}-#{Port::VALID_PORT_RANGE.last}")
+        end
+      end
+    end
+
+    class Validator < Setting
+      def initialize(name, default=nil, strict=true, validator_class=nil)
+        @validator_class = validator_class
+        super(name, ::Object, default, strict)
+      end
+
+      def validate(value)
+        @validator_class.validate(value)
+      end
+    end
+
+    class String < Setting
+      def initialize(name, default=nil, strict=true, possible_strings=[])
+        @possible_strings = possible_strings
+        super(name, ::String, default, strict)
+      end
+
+      def validate(value)
+        super(value)
+        unless @possible_strings.empty? || @possible_strings.include?(value)
+          raise ArgumentError.new("Invalid value \"#{value}\". Options are: #{@possible_strings.inspect}")
+        end
+      end
+    end
+
+    class NullableString < String
+      def validate(value)
+        return if value.nil?
+        super(value)
+      end
+    end
+
+    class ExistingFilePath < Setting
+      def initialize(name, default=nil, strict=true)
+        super(name, ::String, default, strict) do |file_path|
+          if !::File.exists?(file_path)
+            raise ::ArgumentError.new("File \"#{file_path}\" must exist but was not found.")
+          else
+            true
+          end
+        end
+      end
+    end
+
+    class WritableDirectory < Setting
+      def initialize(name, default=nil, strict=false)
+        super(name, ::String, default, strict)
+      end
+      
+      def validate(path)
+        super(path)
+
+        if ::File.directory?(path)
+          if !::File.writable?(path)
+            raise ::ArgumentError.new("Path \"#{path}\" must be a writable directory. It is not writable.")
+          end
+        elsif ::File.symlink?(path)
+          # TODO(sissel): I'm OK if we relax this restriction. My experience
+          # is that it's usually easier and safer to just reject symlinks.
+          raise ::ArgumentError.new("Path \"#{path}\" must be a writable directory. It cannot be a symlink.")
+        elsif ::File.exist?(path)
+          raise ::ArgumentError.new("Path \"#{path}\" must be a writable directory. It is not a directory.")
+        else
+          parent = ::File.dirname(path)
+          if !::File.writable?(parent)
+            raise ::ArgumentError.new("Path \"#{path}\" does not exist and I cannot create it because the parent path \"#{parent}\" is not writable.")
+          end
+        end
+
+        # If we get here, the directory exists and is writable.
+        true
+      end
+
+      def value
+        super.tap do |path|
+          if !::File.directory?(path)
+            # Create the directory if it doesn't exist.
+            begin
+              logger.info("Creating directory", setting: name, path: path)
+              ::FileUtils.mkdir_p(path)
+            rescue => e
+              # TODO(sissel): Catch only specific exceptions?
+              raise ::ArgumentError.new("Path \"#{path}\" does not exist, and I failed trying to create it: #{e.class.name} - #{e}")
+            end
+          end
+        end
+      end
+    end
+  end
+
+  SETTINGS = Settings.new
+end
+
diff --git a/logstash-core/lib/logstash/shutdown_watcher.rb b/logstash-core/lib/logstash/shutdown_watcher.rb
index fa0d1f01fd4..6fffc270f27 100644
--- a/logstash-core/lib/logstash/shutdown_watcher.rb
+++ b/logstash-core/lib/logstash/shutdown_watcher.rb
@@ -2,6 +2,7 @@
 
 module LogStash
   class ShutdownWatcher
+    include LogStash::Util::Loggable
 
     CHECK_EVERY = 1 # second
     REPORT_EVERY = 5 # checks
@@ -25,14 +26,6 @@ def self.unsafe_shutdown?
       @unsafe_shutdown
     end
 
-    def self.logger=(logger)
-      @logger = logger
-    end
-
-    def self.logger
-      @logger ||= Cabin::Channel.get(LogStash)
-    end
-
     def self.start(pipeline, cycle_period=CHECK_EVERY, report_every=REPORT_EVERY, abort_threshold=ABORT_AFTER)
       controller = self.new(pipeline, cycle_period, report_every, abort_threshold)
       Thread.new(controller) { |controller| controller.start }
diff --git a/logstash-core/lib/logstash/util/decorators.rb b/logstash-core/lib/logstash/util/decorators.rb
index 265656e5ce9..a04c2d4bded 100644
--- a/logstash-core/lib/logstash/util/decorators.rb
+++ b/logstash-core/lib/logstash/util/decorators.rb
@@ -6,9 +6,8 @@ module LogStash::Util
 
   # Decorators provides common manipulation on the event data.
   module Decorators
+    include LogStash::Util::Loggable
     extend self
-    
-    @logger = Cabin::Channel.get(LogStash)
 
     # fields is a hash of field => value
     # where both `field` and `value` can use sprintf syntax.
@@ -22,13 +21,13 @@ def add_fields(fields,event, pluginname)
             # note below that the array field needs to be updated then reassigned to the event.
             # this is important because a construct like event[field] << v will not work
             # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
-            a = Array(event[field])
+            a = Array(event.get(field))
             a << v
-            event[field] = a
+            event.set(field, a)
           else
-            event[field] = v
+            event.set(field, v)
           end
-          @logger.debug? and @logger.debug("#{pluginname}: adding value to field", :field => field, :value => value)
+          self.logger.debug? and self.logger.debug("#{pluginname}: adding value to field", "field" => field, "value" => value)
         end
       end
     end
@@ -37,13 +36,13 @@ def add_fields(fields,event, pluginname)
     def add_tags(tags, event, pluginname)
       tags.each do |tag|
         tag = event.sprintf(tag)
-        @logger.debug? and @logger.debug("#{pluginname}: adding tag", :tag => tag)
+        self.logger.debug? and self.logger.debug("#{pluginname}: adding tag", "tag" => tag)
         # note below that the tags array field needs to be updated then reassigned to the event.
         # this is important because a construct like event["tags"] << tag will not work
         # in the current Java event implementation. see https://github.com/elastic/logstash/issues/4140
-        tags = event["tags"] || []
+        tags = event.get("tags") || []
         tags << tag
-        event["tags"] = tags
+        event.set("tags", tags)
       end
     end
 
diff --git a/logstash-core/lib/logstash/util/defaults_printer.rb b/logstash-core/lib/logstash/util/defaults_printer.rb
deleted file mode 100644
index 6dd850e1d50..00000000000
--- a/logstash-core/lib/logstash/util/defaults_printer.rb
+++ /dev/null
@@ -1,31 +0,0 @@
-# encoding: utf-8
-require "logstash/namespace"
-require "logstash/util"
-require "logstash/util/worker_threads_default_printer"
-
-
-# This class exists to format the settings for defaults used
-module LogStash module Util class DefaultsPrinter
-  def self.print(settings)
-    new(settings).print
-  end
-
-  def initialize(settings)
-    @settings = settings
-    @printers = [workers]
-  end
-
-  def print
-    collector = []
-    @printers.each do |printer|
-      printer.visit(collector)
-    end
-    "Settings: " + collector.join(', ')
-  end
-
-  private
-
-  def workers
-    WorkerThreadsDefaultPrinter.new(@settings)
-  end
-end end end
diff --git a/logstash-core/lib/logstash/util/java_version.rb b/logstash-core/lib/logstash/util/java_version.rb
index 71225b30a0a..f73d09de10d 100644
--- a/logstash-core/lib/logstash/util/java_version.rb
+++ b/logstash-core/lib/logstash/util/java_version.rb
@@ -1,20 +1,6 @@
 # encoding: utf-8
-require 'cabin'
 
 module LogStash::Util::JavaVersion
-  def self.logger
-    @logger ||= Cabin::Channel.get(LogStash)
-  end
-
-  # Print a warning if we're on a bad version of java
-  def self.warn_on_bad_java_version
-    if self.bad_java_version?(self.version)
-      msg = "!!! Please upgrade your java version, the current version '#{self.version}' is not supported. We recommend a minimum version of Java 8"
-      STDERR.puts(msg)
-      logger.warn(msg)
-    end
-  end
-
   # Return the current java version string. Returns nil if this is a non-java platform (e.g. MRI).
   def self.version
     return nil unless LogStash::Environment.jruby?
diff --git a/logstash-core/lib/logstash/util/loggable.rb b/logstash-core/lib/logstash/util/loggable.rb
index 0add9f3e2b0..e7fc5cd73c5 100644
--- a/logstash-core/lib/logstash/util/loggable.rb
+++ b/logstash-core/lib/logstash/util/loggable.rb
@@ -1,29 +1,19 @@
 # encoding: utf-8
+require "logstash/logging/logger"
 require "logstash/namespace"
-require "cabin"
 
 module LogStash module Util
   module Loggable
-    class << self
-      def logger=(new_logger)
-        @logger = new_logger
+    def self.included(klass)
+      def klass.logger
+        ruby_name = self.name || self.class.name || self.class.to_s
+        log4j_name = ruby_name.gsub('::', '.').downcase
+        @logger ||= LogStash::Logging::Logger.new(log4j_name)
       end
 
       def logger
-        @logger ||= Cabin::Channel.get(LogStash)
+        self.class.logger
       end
     end
-
-    def self.included(base)
-      class << base
-        def logger
-          Loggable.logger
-        end
-      end
-    end
-
-    def logger
-      Loggable.logger
-    end
   end
 end; end
diff --git a/logstash-core/lib/logstash/util/safe_uri.rb b/logstash-core/lib/logstash/util/safe_uri.rb
new file mode 100644
index 00000000000..add29b6ee67
--- /dev/null
+++ b/logstash-core/lib/logstash/util/safe_uri.rb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+require "logstash/namespace"
+require "logstash/util"
+require "forwardable"
+
+# This class exists to quietly wrap a password string so that, when printed or
+# logged, you don't accidentally print the password itself.
+class LogStash::Util::SafeURI
+  PASS_PLACEHOLDER = "xxxxxx".freeze
+  HOSTNAME_PORT_REGEX=/\A(?<hostname>([A-Za-z0-9\.\-]+)|\[[0-9A-Fa-f\:]+\])(:(?<port>\d+))?\Z/
+  
+  extend Forwardable
+  
+  def_delegators :@uri, :coerce, :query=, :route_from, :port=, :default_port, :select, :normalize!, :absolute?, :registry=, :path, :password, :hostname, :merge, :normalize, :host, :component_ary, :userinfo=, :query, :set_opaque, :+, :merge!, :-, :password=, :parser, :port, :set_host, :set_path, :opaque=, :scheme, :fragment=, :set_query, :set_fragment, :userinfo, :hostname=, :set_port, :path=, :registry, :opaque, :route_to, :set_password, :hierarchical?, :set_user, :set_registry, :set_userinfo, :fragment, :component, :user=, :set_scheme, :absolute, :host=, :relative?, :scheme=, :user
+  
+  attr_reader :uri
+  
+  public
+  def initialize(arg)    
+    @uri = case arg
+           when String
+             arg = "//#{arg}" if HOSTNAME_PORT_REGEX.match(arg)
+             URI.parse(arg)
+           when URI
+             arg
+           else
+             raise ArgumentError, "Expected a string or URI, got a #{arg.class} creating a URL"
+           end
+  end
+
+  def to_s
+    sanitized.to_s
+  end
+
+  def inspect
+    sanitized.to_s
+  end
+
+  def sanitized
+    return uri unless uri.password # nothing to sanitize here!
+    
+    safe = uri.clone
+    safe.password = PASS_PLACEHOLDER
+    safe
+  end
+
+  def ==(other)
+    other.is_a?(::LogStash::Util::SafeURI) ? @uri == other.uri : false
+  end
+end
+
diff --git a/logstash-core/lib/logstash/util/thread_dump.rb b/logstash-core/lib/logstash/util/thread_dump.rb
new file mode 100644
index 00000000000..11d1a8da066
--- /dev/null
+++ b/logstash-core/lib/logstash/util/thread_dump.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+module LogStash
+  module Util
+    class ThreadDump
+      SKIPPED_THREADS             = [ "Finalizer", "Reference Handler", "Signal Dispatcher" ].freeze
+      THREADS_COUNT_DEFAULT       = 3.freeze
+      IGNORE_IDLE_THREADS_DEFAULT = true.freeze
+
+      attr_reader :top_count, :ignore, :dump
+
+      def initialize(options={})
+        @options   = options
+        @dump = options.fetch(:dump, JRMonitor.threads.generate({}))
+        @top_count = options.fetch(:threads, THREADS_COUNT_DEFAULT)
+        @ignore    = options.fetch(:ignore_idle_threads, IGNORE_IDLE_THREADS_DEFAULT)
+      end
+
+      def each(&block)
+        i=0
+        dump.each_pair do |thread_name, _hash|
+          break if i >= top_count
+          if ignore
+            next if idle_thread?(thread_name, _hash)
+          end
+          block.call(thread_name, _hash)
+          i += 1
+        end
+      end
+
+      def idle_thread?(thread_name, data)
+        idle = false
+        if SKIPPED_THREADS.include?(thread_name)
+          # these are likely JVM dependent
+          idle = true
+        elsif thread_name.match(/Ruby-\d+-JIT-\d+/)
+          # This are internal JRuby JIT threads, 
+          # see java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor for details.
+          idle = true
+        elsif thread_name.match(/pool-\d+-thread-\d+/)
+          # This are threads used by the internal JRuby implementation to dispatch
+          # calls and tasks, see prg.jruby.internal.runtime.methods.DynamicMethod.call
+          idle = true
+        else
+          data["thread.stacktrace"].each do |trace|
+            if trace.start_with?("java.util.concurrent.ThreadPoolExecutor.getTask")
+              idle = true
+              break
+            end
+          end
+        end
+        idle
+      end
+    end
+  end
+end
diff --git a/logstash-core/lib/logstash/util/worker_threads_default_printer.rb b/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
index 43869162865..b35058ac24e 100644
--- a/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
+++ b/logstash-core/lib/logstash/util/worker_threads_default_printer.rb
@@ -6,8 +6,8 @@
 module LogStash module Util class WorkerThreadsDefaultPrinter
 
   def initialize(settings)
-    @setting = settings.fetch(:pipeline_workers, 0)
-    @default = settings.fetch(:default_pipeline_workers, 0)
+    @setting = settings.fetch('pipeline.workers', 0)
+    @default = settings.fetch('default-pipeline-workers', 0)
   end
 
   def visit(collector)
diff --git a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
index a8822ca0af5..55bc66c237e 100644
--- a/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
+++ b/logstash-core/lib/logstash/util/wrapped_synchronous_queue.rb
@@ -37,5 +37,258 @@ def take
     def poll(millis)
       @queue.poll(millis, TimeUnit::MILLISECONDS)
     end
+
+    def write_client
+      WriteClient.new(self)
+    end
+
+    def read_client()
+      ReadClient.new(self)
+    end
+
+    class ReadClient
+      # We generally only want one thread at a time able to access pop/take/poll operations
+      # from this queue. We also depend on this to be able to block consumers while we snapshot
+      # in-flight buffers
+
+      def initialize(queue, batch_size = 125, wait_for = 5)
+        @queue = queue
+        @mutex = Mutex.new
+        # Note that @infilght_batches as a central mechanism for tracking inflight
+        # batches will fail if we have multiple read clients in the pipeline.
+        @inflight_batches = {}
+
+        # allow the worker thread to report the execution time of the filter + output
+        @inflight_clocks = {}
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def set_batch_dimensions(batch_size, wait_for)
+        @batch_size = batch_size
+        @wait_for = wait_for
+      end
+
+      def set_events_metric(metric)
+        @event_metric = metric
+      end
+
+      def set_pipeline_metric(metric)
+        @pipeline_metric = metric
+      end
+
+      def inflight_batches
+        @mutex.synchronize do
+          yield(@inflight_batches)
+        end
+      end
+
+      def current_inflight_batch
+        @inflight_batches.fetch(Thread.current, [])
+      end
+
+      def take_batch
+        @mutex.synchronize do
+          batch = ReadBatch.new(@queue, @batch_size, @wait_for)
+          set_current_thread_inflight_batch(batch)
+
+          # We dont actually have any events to work on so lets
+          # not bother with recording metrics for them
+          if batch.size > 0
+            add_starting_metrics(batch)
+            start_clock
+          end
+          batch
+        end
+      end
+
+      def set_current_thread_inflight_batch(batch)
+        @inflight_batches[Thread.current] = batch
+      end
+
+      def close_batch(batch)
+        @mutex.synchronize do
+          @inflight_batches.delete(Thread.current)
+          stop_clock
+        end
+      end
+
+      def start_clock
+        @inflight_clocks[Thread.current] = [
+          @event_metric.time(:duration_in_millis),
+          @pipeline_metric.time(:duration_in_millis)
+        ]
+      end
+
+      def stop_clock
+        unless @inflight_clocks[Thread.current].nil?
+          @inflight_clocks[Thread.current].each(&:stop)
+          @inflight_clocks.delete(Thread.current)
+        end
+      end
+
+      def add_starting_metrics(batch)
+        @event_metric.increment(:in, batch.starting_size)
+        @pipeline_metric.increment(:in, batch.starting_size)
+      end
+
+      def add_filtered_metrics(batch)
+        @event_metric.increment(:filtered, batch.filtered_size)
+        @pipeline_metric.increment(:filtered, batch.filtered_size)
+      end
+
+      def add_output_metrics(batch)
+        @event_metric.increment(:out, batch.filtered_size)
+        @pipeline_metric.increment(:out, batch.filtered_size)
+      end
+    end
+
+    class ReadBatch
+      def initialize(queue, size, wait)
+        @shutdown_signal_received = false
+        @flush_signal_received = false
+        @originals = Hash.new
+
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        # @cancelled = Hash.new
+
+        @generated = Hash.new
+        @iterating_temp = Hash.new
+        @iterating = false # Atomic Boolean maybe? Although batches are not shared across threads
+        take_originals_from_queue(queue, size, wait)
+      end
+
+      def merge(event)
+        return if event.nil? || @originals.key?(event)
+        # take care not to cause @generated to change during iteration
+        # @iterating_temp is merged after the iteration
+        if iterating?
+          @iterating_temp[event] = true
+        else
+          # the periodic flush could generate events outside of an each iteration
+          @generated[event] = true
+        end
+      end
+
+      def cancel(event)
+        # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+        raise("cancel is unsupported")
+        # @cancelled[event] = true
+      end
+
+      def each(&blk)
+        # take care not to cause @originals or @generated to change during iteration
+        @iterating = true
+
+        # below the checks for @cancelled.include?(e) have been replaced by e.cancelled?
+        # TODO: for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+        @originals.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @generated.each do |e, _|
+          blk.call(e) unless e.cancelled?
+        end
+        @iterating = false
+        update_generated
+      end
+
+      def size
+        filtered_size
+      end
+
+      def starting_size
+        @originals.size
+      end
+
+      def filtered_size
+        @originals.size + @generated.size
+      end
+
+      def cancelled_size
+      # TODO: disabled for https://github.com/elastic/logstash/issues/6055 = will have to properly refactor
+      raise("cancelled_size is unsupported ")
+        # @cancelled.size
+      end
+
+      def shutdown_signal_received?
+        @shutdown_signal_received
+      end
+
+      def flush_signal_received?
+        @flush_signal_received
+      end
+
+      private
+
+      def iterating?
+        @iterating
+      end
+
+      def update_generated
+        @generated.update(@iterating_temp)
+        @iterating_temp.clear
+      end
+
+      def take_originals_from_queue(queue, size, wait)
+        size.times do |t|
+          event = (t == 0) ? queue.take : queue.poll(wait)
+          if event.nil?
+            # queue poll timed out
+            next
+          elsif event.is_a?(LogStash::SignalEvent)
+            # We MUST break here. If a batch consumes two SHUTDOWN events
+            # then another worker may have its SHUTDOWN 'stolen', thus blocking
+            # the pipeline.
+            @shutdown_signal_received = event.shutdown?
+
+            # See comment above
+            # We should stop doing work after flush as well.
+            @flush_signal_received = event.flush?
+
+            break
+          else
+            @originals[event] = true
+          end
+        end
+      end
+    end
+
+    class WriteClient
+      def initialize(queue)
+        @queue = queue
+      end
+
+      def get_new_batch
+        WriteBatch.new
+      end
+
+      def push(event)
+        @queue.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def push_batch(batch)
+        batch.each do |event|
+          push(event)
+        end
+      end
+    end
+
+    class WriteBatch
+      def initialize
+        @events = []
+      end
+
+      def push(event)
+        @events.push(event)
+      end
+      alias_method(:<<, :push)
+
+      def each(&blk)
+        @events.each do |e|
+          blk.call(e)
+        end
+      end
+    end
   end
 end end
diff --git a/logstash-core/lib/logstash/version.rb b/logstash-core/lib/logstash/version.rb
index 70715b097cb..370d109a600 100644
--- a/logstash-core/lib/logstash/version.rb
+++ b/logstash-core/lib/logstash/version.rb
@@ -11,4 +11,4 @@
 #       eventually this file should be in the root logstash lib fir and dependencies in logstash-core should be
 #       fixed.
 
-LOGSTASH_VERSION = "3.0.0.dev"
+LOGSTASH_VERSION = "5.1.0"
diff --git a/logstash-core/lib/logstash/webserver.rb b/logstash-core/lib/logstash/webserver.rb
index 23bcaf0b576..b7b99b602a0 100644
--- a/logstash-core/lib/logstash/webserver.rb
+++ b/logstash-core/lib/logstash/webserver.rb
@@ -1,98 +1,95 @@
 # encoding: utf-8
+require "logstash/api/rack_app"
 require "puma"
-require "puma/single"
-require "puma/binder"
-require "puma/configuration"
-require "puma/commonlogger"
+require "puma/server"
+require "logstash/patches/puma"
+require "concurrent"
+require "thread"
 
-module LogStash 
+module LogStash
   class WebServer
-
     extend Forwardable
 
-    attr_reader :logger, :status, :config, :options, :cli_options, :runner, :binder, :events
+    attr_reader :logger, :status, :config, :options, :runner, :binder, :events, :http_host, :http_ports, :http_environment, :agent
 
     def_delegator :@runner, :stats
 
     DEFAULT_HOST = "127.0.0.1".freeze
-    DEFAULT_PORT = 9600.freeze
-
-    def initialize(logger, options={})
-      @logger      = logger
-      http_host    = options[:http_host] || DEFAULT_HOST
-      http_port    = options[:http_port] || DEFAULT_PORT
-      @options     = {}
-      @cli_options = options.merge({ :rackup => ::File.join(::File.dirname(__FILE__), "api", "init.ru"),
-                                     :binds => ["tcp://#{http_host}:#{http_port}"],
-                                     :debug => logger.debug?,
-                                     # Prevent puma from queueing request when not able to properly handling them,
-                                     # fixed https://github.com/elastic/logstash/issues/4674. See
-                                     # https://github.com/puma/puma/pull/640 for mode internal details in PUMA.
-                                     :queue_requests => false
-      })
-      @status      = nil
-
-      parse_options
-
-      @runner  = nil
-      @events  = ::Puma::Events.strings
-      @binder  = ::Puma::Binder.new(@events)
-      @binder.import_from_env
-
-      set_environment
+    DEFAULT_PORTS = (9600..9700).freeze
+    DEFAULT_ENVIRONMENT = 'production'.freeze
+
+    def initialize(logger, agent, options={})
+      @logger = logger
+      @agent = agent
+      @http_host = options[:http_host] || DEFAULT_HOST
+      @http_ports = options[:http_ports] || DEFAULT_PORTS
+      @http_environment = options[:http_environment] || DEFAULT_ENVIRONMENT
+      @options = {}
+      @status = nil
+      @running = Concurrent::AtomicBoolean.new(false)
     end
 
     def run
-      log "=== puma start: #{Time.now} ==="
-
-      @runner = Puma::Single.new(self)
-      @status = :run
-      @runner.run
-      stop(:graceful => true)
+      logger.debug("Starting puma")
+
+      stop # Just in case
+
+      running!
+
+      http_ports.each_with_index do |port, idx|
+        begin
+          if running?
+            @port = port
+            logger.debug("Trying to start WebServer", :port => @port)
+            start_webserver(@port)
+          else
+            break # we are closing down the server so just get out of the loop
+          end
+        rescue Errno::EADDRINUSE
+          if http_ports.count == 1
+            raise Errno::EADDRINUSE.new(I18n.t("logstash.web_api.cant_bind_to_port", :port => http_ports.first))
+          elsif idx == http_ports.count-1
+            raise Errno::EADDRINUSE.new(I18n.t("logstash.web_api.cant_bind_to_port_in_range", :http_ports => http_ports))
+          end
+        end
+      end
     end
 
-    def log(str)
-      logger.debug(str)
+    def running!
+      @running.make_true
     end
 
-    def error(str)
-      logger.error(str)
+    def running?
+      @running.value
     end
 
-    # Empty method, this method is required because of the puma usage we make through
-    # the Single interface, https://github.com/puma/puma/blob/master/lib/puma/single.rb#L82
-    # for more details. This can always be implemented when we want to keep track of this
-    # bit of data.
-    def write_state; end
+    def address
+      "#{http_host}:#{@port}"
+    end
 
     def stop(options={})
-      graceful = options.fetch(:graceful, true)
+      @running.make_false
+      @server.stop(true) if @server
+    end
 
-      if graceful
-        @runner.stop_blocked
-      else
-        @runner.stop
-      end rescue nil
+    def start_webserver(port)
+      # wrap any output that puma could generate into a wrapped logger
+      # use the puma namespace to override STDERR, STDOUT in that scope.
+      Puma::STDERR.logger = logger
+      Puma::STDOUT.logger = logger
 
-      @status = :stop
-      log "=== puma shutdown: #{Time.now} ==="
-    end
+      io_wrapped_logger = LogStash::IOWrappedLogger.new(logger)
 
-    private
+      app = LogStash::Api::RackApp.app(logger, agent, http_environment)
 
-    def env
-      @options[:debug] ? "development" : "production"
-    end
+      events = ::Puma::Events.new(io_wrapped_logger, io_wrapped_logger)
 
-    def set_environment
-      @options[:environment] = env
-      ENV['RACK_ENV']        = env
-    end
+      @server = ::Puma::Server.new(app, events)
+      @server.add_tcp_listener(http_host, port)
+
+      logger.info("Successfully started Logstash API endpoint", :port => @port)
 
-    def parse_options
-      @config  = ::Puma::Configuration.new(cli_options)
-      @config.load
-      @options = @config.options
+      @server.run.join
     end
   end
 end
diff --git a/logstash-core/locales/en.yml b/logstash-core/locales/en.yml
index 7797fee5730..29f307bfdf4 100644
--- a/logstash-core/locales/en.yml
+++ b/logstash-core/locales/en.yml
@@ -1,6 +1,6 @@
 # YAML notes
 #   |- means 'scalar block' useful for formatted text
-#   > means 'scalar block' but it chomps all newlines. Useful 
+#   > means 'scalar block' but it chomps all newlines. Useful
 #     for unformatted text.
 en:
   oops: |-
@@ -68,24 +68,34 @@ en:
         data loss.
       forced_sigint: >-
         SIGINT received. Terminating immediately..
+      non_reloadable_config_reload: >-
+        Unable to reload configuration because it does not support dynamic reloading
+      non_reloadable_config_register: |-
+        Logstash is not able to start since configuration auto reloading was enabled but the configuration contains plugins that don't support it. Quitting...
     web_api:
-      flag:
-        http_host: Web API binding host
-        http_port: Web API http port
+      cant_bind_to_port: |-
+        Logstash tried to bind to port %{port}, but the port is already in use. You can specify a new port by launching logtash with the --http.port option."
+      cant_bind_to_port_in_range: |-
+        Logstash tried to bind to port range %{http_ports}, but all the ports are already in use. You can specify a new port by launching logtash with the --http.port option."
       hot_threads:
         title: |-
           ::: {%{hostname}}
-            Hot threads at %{time}, busiestThreads=%{top_count}:
+          Hot threads at %{time}, busiestThreads=%{top_count}:
         thread_title: |-
-            %{percent_of_cpu_time} % of cpu usage by %{thread_state} thread named '%{thread_name}'
+          %{percent_of_cpu_time} % of cpu usage, state: %{thread_state}, thread name: '%{thread_name}'
+      logging:
+        unrecognized_option: |-
+          unrecognized option [%{option}]
     runner:
       short-help: |-
         usage:
-          bin/logstash -f CONFIG_PATH [-t] [-r] [--quiet|verbose|debug] [-w COUNT] [-l LOG]
-          bin/logstash -e CONFIG_STR [-t] [--quiet|verbose|debug] [-w COUNT] [-l LOG]
-          bin/logstash -i SHELL [--quiet|verbose|debug]
-          bin/logstash -V [--verbose|debug]
+          bin/logstash -f CONFIG_PATH [-t] [-r] [] [-w COUNT] [-l LOG]
+          bin/logstash -e CONFIG_STR [-t] [--log.level fatal|error|warn|info|debug|trace] [-w COUNT] [-l LOG]
+          bin/logstash -i SHELL [--log.level fatal|error|warn|info|debug|trace]
+          bin/logstash -V [--log.level fatal|error|warn|info|debug|trace]
           bin/logstash --help
+      invalid-configuration: >-
+        The given configuration is invalid. Reason: %{error}
       missing-configuration: >-
         No configuration file was specified. Perhaps you forgot to provide
         the '-f yourlogstash.conf' flag?
@@ -102,8 +112,7 @@ en:
           longer available. %{extra} If you have any questions about this, you
           are invited to visit https://discuss.elastic.co/c/logstash and ask.
         file-not-found: |-
-          No config files found: %{path}
-          Can you make sure this path is a logstash config file?
+          No config files found: %{path}. Can you make sure this path is a logstash config file?
         scheme-not-supported: |-
           URI scheme not supported: %{path}
           Either pass a local file path or "file|http://" URI
@@ -138,18 +147,18 @@ en:
           after %{after}
         invalid_plugin_register: >-
           Cannot register %{plugin} %{type} plugin.
-          The error reported is: 
+          The error reported is:
             %{error}
         plugin_path_missing: >-
           You specified a plugin path that does not exist: %{path}
         no_plugins_found: |-
           Could not find any plugins in "%{path}"
-          I tried to find files matching the following, but found none: 
+          I tried to find files matching the following, but found none:
             %{plugin_glob}
         log_file_failed: |-
           Failed to open %{path} for writing: %{error}
 
-          This is often a permissions issue, or the wrong 
+          This is often a permissions issue, or the wrong
           path was specified?
       flag:
         # Note: Wrap these at 55 chars so they display nicely when clamp emits
@@ -175,6 +184,8 @@ en:
           the empty string for the '-e' flag.
         configtest: |+
           Check configuration for valid syntax and then exit.
+        http_host: Web API binding host
+        http_port: Web API http port
         pipeline-workers: |+
           Sets the number of pipeline workers to run.
         pipeline-batch-size: |+
@@ -182,6 +193,11 @@ en:
         pipeline-batch-delay: |+
           When creating pipeline batches, how long to wait while polling
           for the next event.
+        path_settings: |+
+          Directory containing logstash.yml file. This can also be
+          set through the LS_SETTINGS_DIR environment variable.
+        path_logs: |+
+            Directory to Write Logstash internal logs to.
         auto_reload: |+
           Monitor configuration changes and reload
           whenever it is changed.
@@ -198,10 +214,14 @@ en:
           Specifying once will show 'informational'
           logs. Specifying twice will show 'debug'
           logs. This flag is deprecated. You should use
-          --verbose or --debug instead.
+          --log-level=info or --log-level=debug instead.
         version: |+
           Emit the version of logstash and its friends,
           then exit.
+        datapath: |+
+          This should point to a writable directory. Logstash
+          will use this directory whenever it needs to store
+          data. Plugins will also have access to this path.
         pluginpath: |+
           A path of where to find plugins. This flag
           can be given multiple times to include
@@ -210,15 +230,14 @@ en:
           'PATH/logstash/TYPE/NAME.rb' where TYPE is
           'inputs' 'filters', 'outputs' or 'codecs'
           and NAME is the name of the plugin.
-        quiet: |+
-          Quieter logstash logging. This causes only 
-          errors to be emitted.
-        verbose: |+
-          More verbose logging. This causes 'info' 
-          level logs to be emitted.
-        debug: |+
-          Most verbose logging. This causes 'debug'
-          level logs to be emitted.
+        log_level: |+
+          Set the log level for logstash. Possible values are:
+            - fatal
+            - error
+            - warn
+            - info
+            - debug
+            - trace
         unsafe_shutdown: |+
           Force logstash to exit during shutdown even
           if there are still inflight events in memory.
@@ -232,3 +251,19 @@ en:
           it will default to the current hostname.
         agent: |+
           Specify an alternate agent plugin name.
+        config_debug: |+
+          Print the compiled config ruby code out as a debug log (you must also have --log.level=debug enabled).
+          WARNING: This will include any 'password' options passed to plugin configs as plaintext, and may result
+          in plaintext passwords appearing in your logs!
+        log_format: |+
+          Specify if Logstash should write its own logs in JSON form (one
+          event per line) or in plain text (using Ruby's Object#inspect)
+        debug: |+
+          Set the log level to debug.
+          DEPRECATED: use --log.level=debug instead.
+        verbose: |+
+          Set the log level to info.
+          DEPRECATED: use --log.level=info instead.
+        quiet: |+
+          Set the log level to info.
+          DEPRECATED: use --log.level=quiet instead.
diff --git a/logstash-core/logstash-core.gemspec b/logstash-core/logstash-core.gemspec
index c830291c8f7..725cfc4be2e 100644
--- a/logstash-core/logstash-core.gemspec
+++ b/logstash-core/logstash-core.gemspec
@@ -11,15 +11,14 @@ Gem::Specification.new do |gem|
   gem.homepage      = "http://www.elastic.co/guide/en/logstash/current/index.html"
   gem.license       = "Apache License (2.0)"
 
-  gem.files         = Dir.glob(["logstash-core.gemspec", "lib/**/*.rb", "spec/**/*.rb", "locales/*", "lib/logstash/api/init.ru"])
+  gem.files         = Dir.glob(["logstash-core.gemspec", "lib/**/*.rb", "spec/**/*.rb", "locales/*", "lib/logstash/api/init.ru", "vendor/jars/**/*.jar"])
   gem.test_files    = gem.files.grep(%r{^(test|spec|features)/})
   gem.name          = "logstash-core"
-  gem.require_paths = ["lib"]
+  gem.require_paths = ["lib", "vendor/jars"]
   gem.version       = LOGSTASH_CORE_VERSION
 
-  gem.add_runtime_dependency "logstash-core-event-java", "~> 3.0.0.dev"
+  gem.add_runtime_dependency "logstash-core-event-java", "5.1.0"
 
-  gem.add_runtime_dependency "cabin", "~> 0.8.0" #(Apache 2.0 license)
   gem.add_runtime_dependency "pry", "~> 0.10.1"  #(Ruby license)
   gem.add_runtime_dependency "stud", "~> 0.0.19" #(Apache 2.0 license)
   gem.add_runtime_dependency "clamp", "~> 0.6.5" #(MIT license) for command line args/flags
@@ -27,10 +26,10 @@ Gem::Specification.new do |gem|
   gem.add_runtime_dependency "gems", "~> 0.8.3"  #(MIT license)
   gem.add_runtime_dependency "concurrent-ruby", "1.0.0"
   gem.add_runtime_dependency "sinatra", '~> 1.4', '>= 1.4.6'
-  gem.add_runtime_dependency 'puma', '~> 2.16', '>= 2.16.0'
-  gem.add_runtime_dependency "jruby-openssl", "0.9.13" # Required to support TLSv1.2
+  gem.add_runtime_dependency 'puma', '~> 2.16'
+  gem.add_runtime_dependency "jruby-openssl", "0.9.16" # >= 0.9.13 Required to support TLSv1.2
   gem.add_runtime_dependency "chronic_duration", "0.10.6"
-  gem.add_runtime_dependency "jruby-monitoring", '~> 0.1'
+  gem.add_runtime_dependency "jrmonitor", '~> 0.4.2'
 
   # TODO(sissel): Treetop 1.5.x doesn't seem to work well, but I haven't
   # investigated what the cause might be. -Jordan
@@ -46,7 +45,7 @@ Gem::Specification.new do |gem|
 
   if RUBY_PLATFORM == 'java'
     gem.platform = RUBY_PLATFORM
-    gem.add_runtime_dependency "jrjackson", "~> 0.3.7" #(Apache 2.0 license)
+    gem.add_runtime_dependency "jrjackson", "~> 0.4.0" #(Apache 2.0 license)
   else
     gem.add_runtime_dependency "oj" #(MIT-style license)
   end
@@ -59,4 +58,6 @@ Gem::Specification.new do |gem|
     # https://github.com/rubinius/rubinius/issues/2632#issuecomment-26954565
     gem.add_runtime_dependency "racc"
   end
+
+  gem.add_runtime_dependency 'jar-dependencies', '~> 0.3.4'
 end
diff --git a/logstash-core/spec/api/lib/api/logging_spec.rb b/logstash-core/spec/api/lib/api/logging_spec.rb
new file mode 100644
index 00000000000..214a2ad69f2
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/logging_spec.rb
@@ -0,0 +1,41 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/logging"
+require "logstash/json"
+
+describe LogStash::Api::Modules::Logging do
+  include_context "api setup"
+
+  describe "#logging" do
+
+    context "when setting a logger's log level" do
+      before(:all) do
+        @runner = LogStashRunner.new
+        @runner.start
+      end
+
+      after(:all) do
+        @runner.stop
+      end
+
+      it "should return a positive acknowledgement on success" do
+        put '/', '{"logger.logstash": "ERROR"}'
+        payload = LogStash::Json.load(last_response.body)
+        expect(payload['acknowledged']).to eq(true)
+      end
+
+      it "should throw error when level is invalid" do
+        put '/', '{"logger.logstash": "invalid"}'
+        payload = LogStash::Json.load(last_response.body)
+        expect(payload['error']).to eq("invalid level[invalid] for logger[logstash]")
+      end
+
+      it "should throw error when key logger is invalid" do
+        put '/', '{"invalid" : "ERROR"}'
+        payload = LogStash::Json.load(last_response.body)
+        expect(payload['error']).to eq("unrecognized option [invalid]")
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/api/lib/api/node_plugins_spec.rb b/logstash-core/spec/api/lib/api/node_plugins_spec.rb
new file mode 100644
index 00000000000..5389e10c418
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/node_plugins_spec.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require_relative "../../../support/shared_examples"
+require_relative "../../spec_helper"
+require "sinatra"
+require "logstash/api/modules/plugins"
+require "logstash/json"
+
+describe LogStash::Api::Modules::Plugins do
+  include_context "api setup"
+  include_examples "not found"
+
+  extend ResourceDSLMethods
+
+  before(:each) do
+    do_request { get "/" }
+  end
+
+  let(:payload) { LogStash::Json.load(last_response.body) }
+
+  describe "retrieving plugins" do
+    it "should return OK" do
+      expect(last_response).to be_ok
+    end
+
+    it "should return a list of plugins" do
+      expect(payload["plugins"]).to be_a(Array)
+    end
+
+    it "should return the total number of plugins" do
+      expect(payload["total"]).to be_a(Numeric)
+    end
+  end
+end
diff --git a/logstash-core/spec/api/lib/api/node_spec.rb b/logstash-core/spec/api/lib/api/node_spec.rb
index 119fca23ed2..a8f8b009f5b 100644
--- a/logstash-core/spec/api/lib/api/node_spec.rb
+++ b/logstash-core/spec/api/lib/api/node_spec.rb
@@ -1,16 +1,13 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/modules/node"
+require "logstash/api/modules/node"
 require "logstash/json"
 
-describe LogStash::Api::Node do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
+describe LogStash::Api::Modules::Node do
+  include_context "api setup"
+  include_examples "not found"
 
   describe "#hot threads" do
 
@@ -39,26 +36,116 @@ def app()
       end
 
       it "should return information for <= # requested threads" do
-        expect(payload["threads"].count).to be <= 5
+        expect(payload["hot_threads"]["threads"].count).to be <= 5
       end
     end
 
     context "when asking for human output" do
+      [
+        "/hot_threads?human",
+        "/hot_threads?human=true",
+        "/hot_threads?human=1",
+        "/hot_threads?human=t",
+      ].each do |path|
+
+        before(:all) do
+          do_request { get path }
+        end
+
+        let(:payload) { last_response.body }
+
+        it "should return a text/plain content type" do
+          expect(last_response.content_type).to eq("text/plain;charset=utf-8")
+        end
+
+        it "should return a plain text payload" do
+          expect{ JSON.parse(payload) }.to raise_error
+        end
+      end
+    end
 
+    context "When asking for human output and threads count" do
       before(:all) do
-        do_request { get "/hot_threads?human" }
+        # Make sure we have enough threads for this to work.
+        @threads = []
+        5.times { @threads << Thread.new { loop {} } }
+
+        do_request { get "/hot_threads?human=t&threads=2"}
+      end
+
+      after(:all) do
+        @threads.each { |t| t.kill } rescue nil
       end
 
       let(:payload) { last_response.body }
 
-      it "should return a text/plain content type" do
-        expect(last_response.content_type).to eq("text/plain;charset=utf-8")
+      it "should return information for <= # requested threads" do
+        expect(payload.scan(/thread name/).size).to eq(2)
       end
+    end
 
-      it "should return a plain text payload" do
-        expect{ JSON.parse(payload) }.to raise_error
+    context "when not asking for human output" do
+      [
+        "/hot_threads?human=false",
+        "/hot_threads?human=0",
+        "/hot_threads?human=f",
+      ].each do |path|
+        before(:all) do
+          do_request { get path }
+        end
+
+        it "should return a json payload content type" do
+          expect(last_response.content_type).to eq("application/json")
+        end
+
+        let(:payload) { last_response.body }
+
+        it "should return a json payload" do
+          expect{ JSON.parse(payload) }.not_to raise_error
+        end
       end
     end
 
+    describe "Generic JSON testing" do
+      extend ResourceDSLMethods
+
+      root_structure = {
+        "pipeline" => {
+          "workers" => Numeric,
+          "batch_size" => Numeric,
+          "batch_delay" => Numeric,
+          "config_reload_automatic" => Boolean,
+          "config_reload_interval" => Numeric
+        },
+        "os" => {
+          "name" => String,
+          "arch" => String,
+          "version" => String,
+          "available_processors" => Numeric
+        },
+        "jvm" => {
+          "pid" => Numeric,
+          "version" => String,
+          "vm_name" => String,
+          "vm_version" => String,
+          "vm_vendor" => String,
+          "start_time_in_millis" => Numeric,
+          "mem" => {
+            "heap_init_in_bytes" => Numeric,
+            "heap_max_in_bytes" => Numeric,
+            "non_heap_init_in_bytes" => Numeric,
+            "non_heap_max_in_bytes" => Numeric
+        },
+        "gc_collectors" => Array
+        },
+        "hot_threads"=> {
+          "time" => String,
+          "busiest_threads" => Numeric,
+          "threads" => Array
+        }
+      }
+
+      test_api_and_resources(root_structure, :exclude_from_root => ["hot_threads"])
+    end
   end
 end
diff --git a/logstash-core/spec/api/lib/api/node_stats_spec.rb b/logstash-core/spec/api/lib/api/node_stats_spec.rb
index c90d167e3a7..448543d250b 100644
--- a/logstash-core/spec/api/lib/api/node_stats_spec.rb
+++ b/logstash-core/spec/api/lib/api/node_stats_spec.rb
@@ -1,68 +1,85 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/modules/node_stats"
+require "logstash/api/modules/node_stats"
 require "logstash/json"
 
-describe LogStash::Api::NodeStats do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
-
-  let(:payload) { LogStash::Json.load(last_response.body) }
-
-  context "#root" do
-
-    before(:all) do
-      do_request { get "/" }
-    end
-
-    it "respond OK" do
-      expect(last_response).to be_ok
-    end
-
-    ["events", "jvm"].each do |key|
-      it "contains #{key} information" do
-        expect(payload).to include(key)
-      end
-    end
-  end
-
-  context "#events" do
-
-    let(:payload) { LogStash::Json.load(last_response.body) }
-
-    before(:all) do
-      do_request { get "/events" }
-    end
-
-    it "respond OK" do
-      expect(last_response).to be_ok
-    end
-
-    it "contains events information" do
-      expect(payload).to include("events")
-    end
-  end
-
-  context "#jvm" do
-
-    let(:payload) { LogStash::Json.load(last_response.body) }
-
-    before(:all) do
-      do_request { get "/jvm" }
-    end
-
-    it "respond OK" do
-      expect(last_response).to be_ok
-    end
-
-    it "contains memory information" do
-      expect(payload).to include("mem")
-    end
-  end
-
+describe LogStash::Api::Modules::NodeStats do
+  include_context "api setup"
+  include_examples "not found"
+
+  extend ResourceDSLMethods
+
+  # DSL describing response structure
+  root_structure = {
+    "jvm"=>{
+      "threads"=>{
+        "count"=>Numeric,
+        "peak_count"=>Numeric
+      },
+      "gc" => {
+        "collectors" => {
+          "young" => {
+            "collection_count" => Numeric,
+            "collection_time_in_millis" => Numeric
+          },
+          "old" => {
+            "collection_count" => Numeric,
+            "collection_time_in_millis" => Numeric
+          }
+        }
+      },
+      "mem" => {
+        "heap_used_in_bytes" => Numeric,
+        "heap_used_percent" => Numeric,
+        "heap_committed_in_bytes" => Numeric,
+        "heap_max_in_bytes" => Numeric,
+        "non_heap_used_in_bytes" => Numeric,
+        "non_heap_committed_in_bytes" => Numeric,
+        "pools" => {
+          "survivor" => {
+            "peak_used_in_bytes" => Numeric,
+            "used_in_bytes" => Numeric,
+            "peak_max_in_bytes" => Numeric,
+            "max_in_bytes" => Numeric
+          },
+          "old" => {
+            "peak_used_in_bytes" => Numeric,
+            "used_in_bytes" => Numeric,
+            "peak_max_in_bytes" => Numeric,
+            "max_in_bytes" => Numeric
+          },
+          "young" => {
+            "peak_used_in_bytes" => Numeric,
+            "used_in_bytes" => Numeric,
+            "peak_max_in_bytes" => Numeric,
+            "max_in_bytes" => Numeric
+          }
+        }
+      }
+    },
+    "process"=>{
+      "peak_open_file_descriptors"=>Numeric,
+      "max_file_descriptors"=>Numeric,
+      "open_file_descriptors"=>Numeric,
+      "mem"=>{
+        "total_virtual_in_bytes"=>Numeric
+      },
+      "cpu"=>{
+        "total_in_millis"=>Numeric,
+        "percent"=>Numeric
+      }
+    },
+   "pipeline" => {
+     "events" => {
+        "duration_in_millis" => Numeric,
+        "in" => Numeric,
+        "filtered" => Numeric,
+        "out" => Numeric
+     }
+    }
+  }
+
+  test_api_and_resources(root_structure)
 end
diff --git a/logstash-core/spec/api/lib/api/plugins_spec.rb b/logstash-core/spec/api/lib/api/plugins_spec.rb
index 4e0aa66b48b..ee554dae22f 100644
--- a/logstash-core/spec/api/lib/api/plugins_spec.rb
+++ b/logstash-core/spec/api/lib/api/plugins_spec.rb
@@ -1,18 +1,15 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/modules/plugins"
+require "logstash/api/modules/plugins"
 require "logstash/json"
 
-describe LogStash::Api::Plugins do
+describe LogStash::Api::Modules::Plugins do
+  include_context "api setup"
+  include_examples "not found"
 
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
-
-  before(:all) do
+  before(:each) do
     get "/"
   end
 
@@ -23,7 +20,7 @@ def app()
   end
 
   it "return valid json content type" do
-    expect(last_response.content_type).to eq("application/json")
+    expect(last_response.content_type).to eq("application/json"), "Did not get json, got #{last_response.content_type} / #{last_response.body}"
   end
 
   context "#schema" do
@@ -52,6 +49,5 @@ def app()
         expect(plugin["version"]).not_to be_empty
       end
     end
-
   end
 end
diff --git a/logstash-core/spec/api/lib/api/root_spec.rb b/logstash-core/spec/api/lib/api/root_spec.rb
index 6bc8a4937b6..ad9dc08381a 100644
--- a/logstash-core/spec/api/lib/api/root_spec.rb
+++ b/logstash-core/spec/api/lib/api/root_spec.rb
@@ -1,20 +1,18 @@
 # encoding: utf-8
 require_relative "../../spec_helper"
+require_relative "../../../support/shared_examples"
 require "sinatra"
-require "app/root"
+require "logstash/api/modules/root"
 require "logstash/json"
 
-describe LogStash::Api::Root do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
+describe LogStash::Api::Modules::Root do
+  include_context "api setup"
 
   it "should respond to root resource" do
     do_request { get "/" }
     expect(last_response).to be_ok
   end
 
+  include_examples "not found"
 end
+
diff --git a/logstash-core/spec/api/lib/api/stats_spec.rb b/logstash-core/spec/api/lib/api/stats_spec.rb
deleted file mode 100644
index 8dfd2617b42..00000000000
--- a/logstash-core/spec/api/lib/api/stats_spec.rb
+++ /dev/null
@@ -1,19 +0,0 @@
-# encoding: utf-8
-require_relative "../../spec_helper"
-require "sinatra"
-require "app/modules/stats"
-
-describe LogStash::Api::Stats do
-
-  include Rack::Test::Methods
-
-  def app()
-    described_class
-  end
-
-  it "respond to the jvm resource" do
-    do_request { get "/jvm" }
-    expect(last_response).to be_ok
-  end
-
-end
diff --git a/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
new file mode 100644
index 00000000000..f014f974059
--- /dev/null
+++ b/logstash-core/spec/api/lib/api/support/resource_dsl_methods.rb
@@ -0,0 +1,79 @@
+# Ruby doesn't have common class for boolean,
+# And to simplify the ResourceDSLMethods check it make sense to have it.
+module Boolean; end
+class TrueClass
+  include Boolean
+end
+class FalseClass
+  include Boolean
+end
+
+module ResourceDSLMethods
+  # Convert a nested hash to a mapping of key paths to expected classes
+  def hash_to_mapping(h, path=[], mapping={})
+    h.each do |k,v|
+      if v.is_a?(Hash)
+        hash_to_mapping(v, path + [k], mapping)
+      else
+        full_path = path + [k]
+        mapping[full_path] = v
+      end
+    end
+    mapping
+  end
+
+  def test_api(expected, path)
+    context "GET #{path}" do
+      let(:payload) { LogStash::Json.load(last_response.body) }
+      
+      before(:all) do
+        do_request { get path }
+      end      
+      
+      it "should respond OK" do
+        expect(last_response).to be_ok
+      end
+
+      
+      describe "the default metadata" do
+        it "should include the host" do
+          expect(payload["host"]).to eql(Socket.gethostname)
+        end
+
+        it "should include the version" do
+          expect(payload["version"]).to eql(LOGSTASH_CORE_VERSION)
+        end
+
+        it "should include the http address" do
+          expect(payload["http_address"]).to eql("#{Socket.gethostname}:#{::LogStash::WebServer::DEFAULT_PORTS.first}")
+        end
+      end
+      
+      hash_to_mapping(expected).each do |resource_path,klass|
+        dotted = resource_path.join(".")
+        
+        it "should set '#{dotted}' at '#{path}' to be a '#{klass}'" do
+          expect(last_response).to be_ok # fail early if need be
+          resource_path_value = resource_path.reduce(payload) do |acc,v|
+            expect(acc.has_key?(v)).to eql(true), "Expected to find value '#{v}' in structure '#{acc}', but could not. Payload was '#{payload}'"
+            acc[v]
+          end
+          expect(resource_path_value).to be_a(klass), "could not find '#{dotted}' in #{payload}"
+        end
+      end
+    end
+
+    yield if block_given? # Add custom expectations
+  end
+
+  def test_api_and_resources(expected, xopts={})
+    xopts[:exclude_from_root] ||= []
+    root_expectation = expected.clone
+    xopts[:exclude_from_root].each {|k| root_expectation.delete(k)}
+    test_api(root_expectation, "/")
+
+    expected.keys.each do |key|
+      test_api({key => expected[key]}, "/#{key}")
+    end
+  end
+end
diff --git a/logstash-core/spec/api/lib/commands/events_spec.rb b/logstash-core/spec/api/lib/commands/events_spec.rb
deleted file mode 100644
index 9bbcc3e7aa8..00000000000
--- a/logstash-core/spec/api/lib/commands/events_spec.rb
+++ /dev/null
@@ -1,17 +0,0 @@
-# encoding: utf-8
-require_relative "../../spec_helper"
-require "app/commands/stats/events_command"
-
-describe LogStash::Api::StatsEventsCommand do
-
-  context "#schema" do
-
-    let(:report) do
-      do_request { subject.run }
-    end
-
-    it "return events information" do
-      expect(report).to include("in", "filtered", "out")
-    end
-  end
-end
diff --git a/logstash-core/spec/api/lib/commands/jvm_spec.rb b/logstash-core/spec/api/lib/commands/jvm_spec.rb
deleted file mode 100644
index e3f01d00aaf..00000000000
--- a/logstash-core/spec/api/lib/commands/jvm_spec.rb
+++ /dev/null
@@ -1,45 +0,0 @@
-# encoding: utf-8
-require_relative "../../spec_helper"
-require "app/commands/stats/hotthreads_command"
-require "app/commands/stats/memory_command"
-
-describe "JVM stats" do
-
-  describe LogStash::Api::HotThreadsCommand do
-
-    let(:report) do
-      do_request { subject.run }
-    end
-
-    context "#schema" do
-      it "return hot threads information" do
-        report = do_request { subject.run }
-        expect(report.to_s).not_to be_empty
-      end
-
-    end
-  end
-
-  describe LogStash::Api::JvmMemoryCommand do
-
-    context "#schema" do
-
-      let(:report) do
-        do_request { subject.run }
-      end
-
-      it "return hot threads information" do
-        expect(report).not_to be_empty
-      end
-
-      it "return heap information" do
-        expect(report.keys).to include(:heap_used_in_bytes)
-      end
-
-      it "return non heap information" do
-        expect(report.keys).to include(:non_heap_used_in_bytes)
-      end
-
-    end
-  end
-end
diff --git a/logstash-core/spec/api/lib/commands/stats.rb b/logstash-core/spec/api/lib/commands/stats.rb
new file mode 100644
index 00000000000..3059e1460f3
--- /dev/null
+++ b/logstash-core/spec/api/lib/commands/stats.rb
@@ -0,0 +1,47 @@
+# encoding: utf-8
+require_relative "../../spec_helper"
+
+describe LogStash::Api::Commands::Stats do
+
+  let(:report_method) { :run }
+  subject(:report) { do_request { report_class.new.send(report_method) } }
+
+  let(:report_class) { described_class }
+
+  describe "#events" do
+    let(:report_method) { :events }
+
+    it "return events information" do
+      expect(report.keys).to include(:in, :filtered, :out)
+    end
+  end
+  
+  describe "#hot_threads" do
+    let(:report_method) { :hot_threads }
+    
+    it "should return hot threads information as a string" do
+      expect(report.to_s).to be_a(String)
+    end
+
+    it "should return hot threads info as a hash" do
+      expect(report.to_hash).to be_a(Hash)
+    end
+  end
+
+  describe "memory stats" do
+    let(:report_method) { :memory }
+      
+    it "return hot threads information" do
+      expect(report).not_to be_empty
+    end
+
+    it "return heap information" do
+      expect(report.keys).to include(:heap_used_in_bytes)
+    end
+
+    it "return non heap information" do
+      expect(report.keys).to include(:non_heap_used_in_bytes)
+    end
+
+  end
+end
diff --git a/logstash-core/spec/api/lib/errors_spec.rb b/logstash-core/spec/api/lib/errors_spec.rb
new file mode 100644
index 00000000000..430671402d0
--- /dev/null
+++ b/logstash-core/spec/api/lib/errors_spec.rb
@@ -0,0 +1,27 @@
+# encoding: utf-8
+require_relative "../spec_helper"
+require "logstash/api/errors"
+
+describe LogStash::Api::ApiError do
+  subject { described_class.new }
+
+  it "#status_code returns 500" do
+    expect(subject.status_code).to eq(500)
+  end
+
+  it "#to_hash return the message of the exception" do
+    expect(subject.to_hash).to include(:message => "Api Error")
+  end
+end
+
+describe LogStash::Api::NotFoundError do
+  subject { described_class.new }
+
+  it "#status_code returns 404" do
+    expect(subject.status_code).to eq(404)
+  end
+
+  it "#to_hash return the message of the exception" do
+    expect(subject.to_hash).to include(:message => "Not Found")
+  end
+end
diff --git a/logstash-core/spec/api/lib/rack_app_spec.rb b/logstash-core/spec/api/lib/rack_app_spec.rb
new file mode 100644
index 00000000000..9c6af8679e9
--- /dev/null
+++ b/logstash-core/spec/api/lib/rack_app_spec.rb
@@ -0,0 +1,90 @@
+require "logstash/api/rack_app"
+require "rack/test"
+
+describe LogStash::Api::RackApp do
+  include Rack::Test::Methods
+
+  class DummyApp
+    class RaisedError < StandardError; end
+
+    def call(env)
+      case env["PATH_INFO"]
+      when "/good-page"
+        [200, {}, ["200 OK"]]
+      when "/service-unavailable"
+        [503, {}, ["503 service unavailable"]]
+      when "/raise-error"
+        raise RaisedError, "Error raised"
+      else
+        [404, {}, ["404 Page not found"]]
+      end
+    end
+  end
+
+  let(:logger) { double("logger") }
+
+  describe LogStash::Api::RackApp::ApiErrorHandler do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+
+    it "should let good requests through as normal" do
+      get "/good-page"
+      expect(last_response).to be_ok
+    end
+
+    it "should let through 5xx codes" do
+      get "/service-unavailable"
+      expect(last_response.status).to eql(503)
+    end
+
+    describe "raised exceptions" do
+      before do
+        allow(logger).to receive(:error).with(any_args)
+        get "/raise-error"
+      end
+
+      it "should return a 500 error" do
+        expect(last_response.status).to eql(500)
+      end
+
+      it "should return valid JSON" do
+        expect { LogStash::Json.load(last_response.body) }.not_to raise_error
+      end
+
+      it "should log the error" do
+        expect(logger).to have_received(:error).with(LogStash::Api::RackApp::ApiErrorHandler::LOG_MESSAGE, anything).once
+      end
+    end
+  end
+
+  describe LogStash::Api::RackApp::ApiLogger do
+    let(:app) do
+      # Scoping in rack builder is weird, these need to be locals
+      rack_class = described_class
+      rack_logger = logger
+      Rack::Builder.new do
+        use rack_class, rack_logger
+        run DummyApp.new
+      end
+    end
+
+    it "should log good requests as info" do
+      expect(logger).to receive(:debug?).and_return(true)
+      expect(logger).to receive(:debug).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/good-page"
+    end
+
+    it "should log 5xx requests as warnings" do
+      expect(logger).to receive(:error?).and_return(true)
+      expect(logger).to receive(:error).with(LogStash::Api::RackApp::ApiLogger::LOG_MESSAGE, anything).once
+      get "/service-unavailable"
+    end
+  end
+end
diff --git a/logstash-core/spec/api/spec_helper.rb b/logstash-core/spec/api/spec_helper.rb
index 90a1bb1e378..8e4912ddf84 100644
--- a/logstash-core/spec/api/spec_helper.rb
+++ b/logstash-core/spec/api/spec_helper.rb
@@ -1,16 +1,16 @@
 # encoding: utf-8
 API_ROOT = File.expand_path(File.join(File.dirname(__FILE__), "..", "..", "lib", "logstash", "api"))
 
+require "stud/task"
 require "logstash/devutils/rspec/spec_helper"
-
+$LOAD_PATH.unshift(File.expand_path(File.dirname(__FILE__)))
+require "lib/api/support/resource_dsl_methods"
+require 'rspec/expectations'
+require "logstash/settings"
 require 'rack/test'
 require 'rspec'
 require "json"
 
-ENV['RACK_ENV'] = 'test'
-
-Rack::Builder.parse_file(File.join(API_ROOT, 'init.ru'))
-
 def read_fixture(name)
   path = File.join(File.dirname(__FILE__), "fixtures", name)
   File.read(path)
@@ -18,11 +18,9 @@ def read_fixture(name)
 
 module LogStash
   class DummyAgent < Agent
-    def fetch_config(settings)
-      "input { generator {count => 0} } output { }"
+    def start_webserver
+      @webserver = Struct.new(:address).new("#{Socket.gethostname}:#{::LogStash::WebServer::DEFAULT_PORTS.first}")
     end
-
-    def start_webserver; end
     def stop_webserver; end
   end
 end
@@ -37,51 +35,40 @@ class LogStashRunner
   attr_reader :config_str, :agent, :pipeline_settings
 
   def initialize
-    args = [
-      :logger => Cabin::Channel.get(LogStash),
-      :auto_reload => false,
-      :collect_metric => true,
-      :debug => false,
-      :node_name => "test_agent",
-      :web_api_http_port => rand(9600..9700)
-    ]
-
-    @config_str   = "input { generator {count => 0} } output { }"
-    @agent = LogStash::DummyAgent.new(*args)
-    @pipeline_settings ||= { :pipeline_id => "main",
-                             :config_str => config_str,
-                            :pipeline_batch_size => 1,
-                            :flush_interval => 1,
-                            :pipeline_workers => 1 }
+    @config_str   = "input { generator {count => 100 } } output { dummyoutput {} }"
+
+    args = {
+      "config.reload.automatic" => false,
+      "metric.collect" => true,
+      "log.level" => "debug",
+      "node.name" => "test_agent",
+      "http.port" => rand(9600..9700),
+      "http.environment" => "test",      
+      "config.string" => @config_str,
+      "pipeline.batch.size" => 1,
+      "pipeline.workers" => 1
+    }
+    @settings = ::LogStash::SETTINGS.clone.merge(args)
+
+    @agent = LogStash::DummyAgent.new(@settings)
   end
 
   def start
-    agent.register_pipeline("main", pipeline_settings)
-    @runner = Thread.new(agent) do |_agent|
-      _agent.execute
-    end
-    wait_until_snapshot_received
+    # We start a pipeline that will generate a finite number of events
+    # before starting the expectations
+    agent.register_pipeline("main", @settings)
+    @agent_task = Stud::Task.new { agent.execute }
+    @agent_task.wait
   end
 
   def stop
     agent.shutdown
-    Thread.kill(@runner)
-    sleep 0.1 while !@runner.stop?
-  end
-
-  private
-
-  def wait_until_snapshot_received
-    while !LogStash::Api::Service.instance.started? do
-      sleep 0.5
-    end
   end
 end
 
-
 ##
 # Method used to wrap up a request in between of a running
-# pipeline, this makes the hole execution model easier and
+# pipeline, this makes the whole execution model easier and
 # more contained as some threads might go wild.
 ##
 def do_request(&block)
@@ -92,30 +79,6 @@ def do_request(&block)
   ret_val
 end
 
-##
-# Helper module that setups necessary mocks when doing the requests,
-# this could be just included in the test and the runner will be
-# started managed for all tests.
-##
-module LogStash; module RSpec; module RunnerConfig
-  def self.included(klass)
-    klass.before(:all) do
-      LogStashRunner.instance.start
-    end
-
-    klass.before(:each) do
-      runner = LogStashRunner.instance
-      allow(LogStash::Instrument::Collector.instance).to receive(:agent).and_return(runner.agent)
-    end
-
-    klass.after(:all) do
-      LogStashRunner.instance.stop
-    end
-  end
-end; end; end
-
-require 'rspec/expectations'
-
 RSpec::Matchers.define :be_available? do
   match do |plugin|
     begin
@@ -126,3 +89,20 @@ def self.included(klass)
     end
   end
 end
+
+shared_context "api setup" do
+  before :all do
+    @runner = LogStashRunner.new
+    @runner.start
+  end
+  
+  after :all do
+    @runner.stop
+  end
+
+  include Rack::Test::Methods
+
+  def app()
+    described_class.new(nil, @runner.agent)
+  end
+end
diff --git a/logstash-core/spec/conditionals_spec.rb b/logstash-core/spec/conditionals_spec.rb
index dab6fc901e3..d831513fbfb 100644
--- a/logstash-core/spec/conditionals_spec.rb
+++ b/logstash-core/spec/conditionals_spec.rb
@@ -25,6 +25,17 @@ def conditional(expression, &block)
 describe "conditionals in output" do
   extend ConditionalFanciness
 
+  class DummyNullOutput < LogStash::Outputs::Base
+    def register
+    end
+    def multi_receive(events)
+    end
+  end
+
+  before do
+    LogStash::Registry.instance.register("logstash/outputs/dummynull", DummyNullOutput)
+  end
+
   describe "simple" do
     config <<-CONFIG
       input {
@@ -35,7 +46,7 @@ def conditional(expression, &block)
       }
       output {
         if [foo] == "bar" {
-          stdout { }
+          dummynull { }
         }
       }
     CONFIG
@@ -64,24 +75,24 @@ def conditional(expression, &block)
     CONFIG
 
     sample({"foo" => "bar"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to eq("world")
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to eq("world")
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample({"notfoo" => "bar"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to eq("hugs")
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to eq("hugs")
     end
 
     sample({"bar" => "baz"}) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to eq("pants")
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to eq("pants")
+      expect(subject.get("free")).to be_nil
     end
   end
 
@@ -102,31 +113,31 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => "bar", "nest" => 124) do
-      expect(subject["always"]).to be_nil
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to be_nil
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample("foo" => "bar", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to eq("world")
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to eq("world")
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to be_nil
     end
 
     sample("notfoo" => "bar", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to be_nil
-      expect(subject["free"]).to eq("hugs")
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to be_nil
+      expect(subject.get("free")).to eq("hugs")
     end
 
     sample("bar" => "baz", "nest" => 123) do
-      expect(subject["always"]).to eq("awesome")
-      expect(subject["hello"]).to be_nil
-      expect(subject["fancy"]).to eq("pants")
-      expect(subject["free"]).to be_nil
+      expect(subject.get("always")).to eq("awesome")
+      expect(subject.get("hello")).to be_nil
+      expect(subject.get("fancy")).to eq("pants")
+      expect(subject.get("free")).to be_nil
     end
   end
 
@@ -140,7 +151,7 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => 123, "bar" => 123) do
-      expect(subject["tags"] ).to include("woot")
+      expect(subject.get("tags") ).to include("woot")
     end
   end
 
@@ -169,12 +180,12 @@ def conditional(expression, &block)
     CONFIG
 
     sample("foo" => "foo", "foobar" => "foobar", "greeting" => "hello world") do
-      expect(subject["tags"]).to include("field in field")
-      expect(subject["tags"]).to include("field in string")
-      expect(subject["tags"]).to include("string in field")
-      expect(subject["tags"]).to include("field in list")
-      expect(subject["tags"]).not_to include("shouldnotexist")
-      expect(subject["tags"]).to include("shouldexist")
+      expect(subject.get("tags")).to include("field in field")
+      expect(subject.get("tags")).to include("field in string")
+      expect(subject.get("tags")).to include("string in field")
+      expect(subject.get("tags")).to include("field in list")
+      expect(subject.get("tags")).not_to include("shouldnotexist")
+      expect(subject.get("tags")).to include("shouldexist")
     end
   end
 
@@ -192,107 +203,107 @@ def conditional(expression, &block)
 
     sample("foo" => "foo", "somelist" => [ "one", "two" ], "foobar" => "foobar", "greeting" => "hello world", "tags" => [ "fancypantsy" ]) do
       # verify the original exists
-      expect(subject["tags"]).to include("fancypantsy")
+      expect(subject.get("tags")).to include("fancypantsy")
 
-      expect(subject["tags"]).to include("baz")
-      expect(subject["tags"]).not_to include("foo")
-      expect(subject["tags"]).to include("notfoo")
-      expect(subject["tags"]).to include("notsomelist")
-      expect(subject["tags"]).not_to include("somelist")
-      expect(subject["tags"]).to include("no string in missing field")
+      expect(subject.get("tags")).to include("baz")
+      expect(subject.get("tags")).not_to include("foo")
+      expect(subject.get("tags")).to include("notfoo")
+      expect(subject.get("tags")).to include("notsomelist")
+      expect(subject.get("tags")).not_to include("somelist")
+      expect(subject.get("tags")).to include("no string in missing field")
     end
   end
 
   describe "operators" do
     conditional "[message] == 'sample'" do
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("different") { expect(subject["tags"] ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("different") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] != 'sample'" do
-      sample("sample") { expect(subject["tags"] ).to include("failure") }
-      sample("different") { expect(subject["tags"] ).to include("success") }
+      sample("sample") { expect(subject.get("tags") ).to include("failure") }
+      sample("different") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] < 'sample'" do
-      sample("apple") { expect(subject["tags"] ).to include("success") }
-      sample("zebra") { expect(subject["tags"] ).to include("failure") }
+      sample("apple") { expect(subject.get("tags") ).to include("success") }
+      sample("zebra") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] > 'sample'" do
-      sample("zebra") { expect(subject["tags"] ).to include("success") }
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
+      sample("zebra") { expect(subject.get("tags") ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] <= 'sample'" do
-      sample("apple") { expect(subject["tags"] ).to include("success") }
-      sample("zebra") { expect(subject["tags"] ).to include("failure") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("success") }
+      sample("zebra") { expect(subject.get("tags") ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] >= 'sample'" do
-      sample("zebra") { expect(subject["tags"] ).to include("success") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
+      sample("zebra") { expect(subject.get("tags") ).to include("success") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
     end
 
     conditional "[message] =~ /sample/" do
-      sample("apple") { expect(subject["tags"] ).to include("failure") }
-      sample("sample") { expect(subject["tags"] ).to include("success") }
-      sample("some sample") { expect(subject["tags"] ).to include("success") }
+      sample("apple") { expect(subject.get("tags") ).to include("failure") }
+      sample("sample") { expect(subject.get("tags") ).to include("success") }
+      sample("some sample") { expect(subject.get("tags") ).to include("success") }
     end
 
     conditional "[message] !~ /sample/" do
-      sample("apple") { expect(subject["tags"]).to include("success") }
-      sample("sample") { expect(subject["tags"]).to include("failure") }
-      sample("some sample") { expect(subject["tags"]).to include("failure") }
+      sample("apple") { expect(subject.get("tags")).to include("success") }
+      sample("sample") { expect(subject.get("tags")).to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).to include("failure") }
     end
 
   end
 
   describe "negated expressions" do
     conditional "!([message] == 'sample')" do
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("different") { expect(subject["tags"]).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("different") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] != 'sample')" do
-      sample("sample") { expect(subject["tags"]).not_to include("failure") }
-      sample("different") { expect(subject["tags"]).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("failure") }
+      sample("different") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] < 'sample')" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("zebra") { expect(subject["tags"]).not_to include("failure") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] > 'sample')" do
-      sample("zebra") { expect(subject["tags"]).not_to include("success") }
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] <= 'sample')" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("zebra") { expect(subject["tags"]).not_to include("failure") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] >= 'sample')" do
-      sample("zebra") { expect(subject["tags"]).not_to include("success") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
+      sample("zebra") { expect(subject.get("tags")).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
     end
 
     conditional "!([message] =~ /sample/)" do
-      sample("apple") { expect(subject["tags"]).not_to include("failure") }
-      sample("sample") { expect(subject["tags"]).not_to include("success") }
-      sample("some sample") { expect(subject["tags"]).not_to include("success") }
+      sample("apple") { expect(subject.get("tags")).not_to include("failure") }
+      sample("sample") { expect(subject.get("tags")).not_to include("success") }
+      sample("some sample") { expect(subject.get("tags")).not_to include("success") }
     end
 
     conditional "!([message] !~ /sample/)" do
-      sample("apple") { expect(subject["tags"]).not_to include("success") }
-      sample("sample") { expect(subject["tags"]).not_to include("failure") }
-      sample("some sample") { expect(subject["tags"]).not_to include("failure") }
+      sample("apple") { expect(subject.get("tags")).not_to include("success") }
+      sample("sample") { expect(subject.get("tags")).not_to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).not_to include("failure") }
     end
 
   end
@@ -300,47 +311,47 @@ def conditional(expression, &block)
   describe "value as an expression" do
     # testing that a field has a value should be true.
     conditional "[message]" do
-      sample("apple") { expect(subject["tags"]).to include("success") }
-      sample("sample") { expect(subject["tags"]).to include("success") }
-      sample("some sample") { expect(subject["tags"]).to include("success") }
+      sample("apple") { expect(subject.get("tags")).to include("success") }
+      sample("sample") { expect(subject.get("tags")).to include("success") }
+      sample("some sample") { expect(subject.get("tags")).to include("success") }
     end
 
     # testing that a missing field has a value should be false.
     conditional "[missing]" do
-      sample("apple") { expect(subject["tags"]).to include("failure") }
-      sample("sample") { expect(subject["tags"]).to include("failure") }
-      sample("some sample") { expect(subject["tags"]).to include("failure") }
+      sample("apple") { expect(subject.get("tags")).to include("failure") }
+      sample("sample") { expect(subject.get("tags")).to include("failure") }
+      sample("some sample") { expect(subject.get("tags")).to include("failure") }
     end
   end
 
   describe "logic operators" do
     describe "and" do
       conditional "[message] and [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "[message] and ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
       conditional "![message] and [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
       conditional "![message] and ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
     end
 
     describe "or" do
       conditional "[message] or [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "[message] or ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "![message] or [message]" do
-        sample("whatever") { expect(subject["tags"]).to include("success") }
+        sample("whatever") { expect(subject.get("tags")).to include("success") }
       end
       conditional "![message] or ![message]" do
-        sample("whatever") { expect(subject["tags"]).to include("failure") }
+        sample("whatever") { expect(subject.get("tags")).to include("failure") }
       end
     end
   end
@@ -348,19 +359,19 @@ def conditional(expression, &block)
   describe "field references" do
     conditional "[field with space]" do
       sample("field with space" => "hurray") do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
 
     conditional "[field with space] == 'hurray'" do
       sample("field with space" => "hurray") do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
 
     conditional "[nested field][reference with][some spaces] == 'hurray'" do
       sample({"nested field" => { "reference with" => { "some spaces" => "hurray" } } }) do
-        expect(subject["tags"]).to include("success")
+        expect(subject.get("tags")).to include("success")
       end
     end
   end
@@ -385,13 +396,13 @@ def conditional(expression, &block)
       expect(subject).to be_an(Array)
       expect(subject.length).to eq(2)
 
-      expect(subject[0]["type"]).to eq("original")
-      expect(subject[0]["cond1"]).to eq("true")
-      expect(subject[0]["cond2"]).to eq(nil)
+      expect(subject[0].get("type")).to eq("original")
+      expect(subject[0].get("cond1")).to eq("true")
+      expect(subject[0].get("cond2")).to eq(nil)
 
-      expect(subject[1]["type"]).to eq("clone")
-      # expect(subject[1]["cond1"]).to eq(nil)
-      # expect(subject[1]["cond2"]).to eq("true")
+      expect(subject[1].get("type")).to eq("clone")
+      # expect(subject[1].get("cond1")).to eq(nil)
+      # expect(subject[1].get("cond2")).to eq("true")
     end
   end
 
@@ -413,16 +424,16 @@ def conditional(expression, &block)
 
     sample({"type" => "original"}) do
       # puts subject.inspect
-      expect(subject[0]["cond1"]).to eq(nil)
-      expect(subject[0]["cond2"]).to eq(nil)
+      expect(subject[0].get("cond1")).to eq(nil)
+      expect(subject[0].get("cond2")).to eq(nil)
 
-      expect(subject[1]["type"]).to eq("clone1")
-      expect(subject[1]["cond1"]).to eq("true")
-      expect(subject[1]["cond2"]).to eq(nil)
+      expect(subject[1].get("type")).to eq("clone1")
+      expect(subject[1].get("cond1")).to eq("true")
+      expect(subject[1].get("cond2")).to eq(nil)
 
-      expect(subject[2]["type"]).to eq("clone2")
-      expect(subject[2]["cond1"]).to eq(nil)
-      expect(subject[2]["cond2"]).to eq("true")
+      expect(subject[2].get("type")).to eq("clone2")
+      expect(subject[2].get("cond1")).to eq(nil)
+      expect(subject[2].get("cond2")).to eq("true")
     end
   end
 
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index b7ad9065e04..a9ec4cde430 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -1,85 +1,229 @@
 # encoding: utf-8
-require 'spec_helper'
-require 'stud/temporary'
+require "spec_helper"
+require "stud/temporary"
+require "logstash/inputs/generator"
+require_relative "../support/mocks_classes"
 
 describe LogStash::Agent do
 
-  let(:logger) { double("logger") }
-  let(:agent_args) { { :logger => logger } }
-  subject { LogStash::Agent.new(agent_args) }
+  let(:agent_settings) { LogStash::SETTINGS }
+  let(:agent_args) { {} }
+  let(:pipeline_settings) { agent_settings.clone }
+  let(:pipeline_args) { {} }
+  let(:config_file) { Stud::Temporary.pathname }
+  let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
+
+  subject { LogStash::Agent.new(agent_settings) }
 
   before :each do
-    [:info, :warn, :error, :fatal, :debug].each do |level|
-      allow(logger).to receive(level)
+    File.open(config_file, "w") { |f| f.puts config_file_txt }
+    agent_args.each do |key, value|
+      agent_settings.set(key, value)
+      pipeline_settings.set(key, value)
     end
-    [:info?, :warn?, :error?, :fatal?, :debug?].each do |level|
-      allow(logger).to receive(level)
+    pipeline_args.each do |key, value|
+      pipeline_settings.set(key, value)
     end
   end
 
+  after :each do
+    LogStash::SETTINGS.reset
+    File.unlink(config_file)
+  end
+
+  it "fallback to hostname when no name is provided" do
+    expect(LogStash::Agent.new.node_name).to eq(Socket.gethostname)
+  end
+
   describe "register_pipeline" do
     let(:pipeline_id) { "main" }
-    let(:settings) { {
-      :config_string => "input { } filter { } output { }",
-      :pipeline_workers => 4
-    } }
-
-    let(:agent_args) { {
-      :logger => logger,
-      :auto_reload => false,
-      :reload_interval => 0.01
-    } }
+    let(:config_string) { "input { } filter { } output { }" }
+    let(:agent_args) do
+      {
+        "config.string" => config_string,
+        "config.reload.automatic" => true,
+        "config.reload.interval" => 0.01,
+        "pipeline.workers" => 4,
+      }
+    end
 
     it "should delegate settings to new pipeline" do
-      expect(LogStash::Pipeline).to receive(:new).with(settings[:config_string], hash_including(settings))
-      subject.register_pipeline(pipeline_id, settings)
+      expect(LogStash::Pipeline).to receive(:new) do |arg1, arg2|
+        expect(arg1).to eq(config_string)
+        expect(arg2.to_hash).to include(agent_args)
+      end
+      subject.register_pipeline(pipeline_id, agent_settings)
     end
   end
 
   describe "#execute" do
-    let(:sample_config) { "input { generator { count => 100000 } } output { stdout { } }" }
-    let(:config_file) { Stud::Temporary.pathname }
+    let(:config_file_txt) { "input { generator { count => 100000 } } output { }" }
 
     before :each do
       allow(subject).to receive(:start_webserver).and_return(false)
       allow(subject).to receive(:stop_webserver).and_return(false)
-      File.open(config_file, "w") { |f| f.puts sample_config }
-    end
-
-    after :each do
-      File.unlink(config_file)
     end
 
     context "when auto_reload is false" do
-      let(:agent_args) { { :logger => logger, :auto_reload => false, :reload_interval => 0.01, :config_path => config_file } }
+      let(:agent_args) do
+        {
+          "config.reload.automatic" => false,
+          "path.config" => config_file
+        }
+      end
+      let(:pipeline_id) { "main" }
 
-      before :each do
-        allow(subject).to receive(:sleep)
-        allow(subject).to receive(:clean_state?).and_return(false)
-        allow(subject).to receive(:running_pipelines?).and_return(true)
+      before(:each) do
+        subject.register_pipeline(pipeline_id, pipeline_settings)
       end
 
       context "if state is clean" do
+        before :each do
+          allow(subject).to receive(:running_pipelines?).and_return(true)
+          allow(subject).to receive(:sleep)
+          allow(subject).to receive(:clean_state?).and_return(false)
+        end
+
         it "should not reload_state!" do
           expect(subject).to_not receive(:reload_state!)
           t = Thread.new { subject.execute }
           sleep 0.1
           Stud.stop!(t)
           t.join
+          subject.shutdown
+        end
+      end
+
+      context "when calling reload_pipeline!" do
+        context "with a config that contains reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { stdin {} } filter { } output { }" }
+
+          it "does not upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to_not receive(:upgrade_pipeline)
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.send(:"reload_pipeline!", "main")
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
+        end
+
+        context "with a config that does not contain reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+
+          it "does upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to receive(:upgrade_pipeline).once.and_call_original
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.send(:"reload_pipeline!", "main")
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+
+            subject.shutdown
+          end
+        end
+
+      end
+      context "when calling reload_state!" do
+        context "with a pipeline with auto reloading turned off" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+          let(:pipeline_args) { { "config.reload.automatic" => false } }
+
+          it "does not try to reload the pipeline" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to_not receive(:reload_pipeline!)
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.reload_state!
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+
+            subject.shutdown
+          end
+        end
+
+        context "with a pipeline with auto reloading turned on" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+          let(:pipeline_args) { { "config.reload.automatic" => true } }
+
+          it "tries to reload the pipeline" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to receive(:reload_pipeline!).once.and_call_original
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            subject.reload_state!
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+
+            subject.shutdown
+          end
         end
       end
     end
 
     context "when auto_reload is true" do
-      let(:agent_args) { { :logger => logger, :auto_reload => true, :reload_interval => 0.01 } }
+      let(:agent_args) do
+        {
+          "config.reload.automatic" => true,
+          "config.reload.interval" => 0.01,
+          "path.config" => config_file,
+        }
+      end
+      let(:pipeline_id) { "main" }
+
+      before(:each) do
+        subject.register_pipeline(pipeline_id, pipeline_settings)
+      end
+
       context "if state is clean" do
         it "should periodically reload_state" do
           allow(subject).to receive(:clean_state?).and_return(false)
-          expect(subject).to receive(:reload_state!).at_least(3).times
           t = Thread.new { subject.execute }
+          sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+          expect(subject).to receive(:reload_state!).at_least(2).times
           sleep 0.1
           Stud.stop!(t)
           t.join
+          subject.shutdown
+        end
+      end
+
+      context "when calling reload_state!" do
+        context "with a config that contains reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { stdin {} } filter { } output { }" }
+
+          it "does not upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to_not receive(:upgrade_pipeline)
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
+        end
+
+        context "with a config that does not contain reload incompatible plugins" do
+          let(:second_pipeline_config) { "input { generator { } } filter { } output { }" }
+
+          it "does upgrade the new config" do
+            t = Thread.new { subject.execute }
+            sleep 0.01 until subject.running_pipelines? && subject.pipelines.values.first.ready?
+            expect(subject).to receive(:upgrade_pipeline).once.and_call_original
+            File.open(config_file, "w") { |f| f.puts second_pipeline_config }
+            sleep 0.1
+            Stud.stop!(t)
+            t.join
+            subject.shutdown
+          end
         end
       end
     end
@@ -89,9 +233,10 @@
     let(:pipeline_id) { "main" }
     let(:first_pipeline_config) { "input { } filter { } output { }" }
     let(:second_pipeline_config) { "input { generator {} } filter { } output { }" }
-    let(:pipeline_settings) { {
-      :config_string => first_pipeline_config,
-      :pipeline_workers => 4
+    let(:pipeline_args) { {
+      "config.string" => first_pipeline_config,
+      "pipeline.workers" => 4,
+      "config.reload.automatic" => true
     } }
 
     before(:each) do
@@ -102,14 +247,41 @@
       it "upgrades the state" do
         expect(subject).to receive(:fetch_config).and_return(second_pipeline_config)
         expect(subject).to receive(:upgrade_pipeline).with(pipeline_id, kind_of(LogStash::Pipeline))
-        subject.send(:reload_state!)
+        subject.reload_state!
       end
     end
     context "when fetching the same state" do
       it "doesn't upgrade the state" do
         expect(subject).to receive(:fetch_config).and_return(first_pipeline_config)
         expect(subject).to_not receive(:upgrade_pipeline)
-        subject.send(:reload_state!)
+        subject.reload_state!
+      end
+    end
+  end
+
+  describe "Environment Variables In Configs" do
+    let(:pipeline_config) { "input { generator { message => '${FOO}-bar' } } filter { } output { }" }
+    let(:agent_args) { {
+      "config.reload.automatic" => false,
+      "config.reload.interval" => 0.01,
+      "config.string" => pipeline_config
+    } }
+    let(:pipeline_id) { "main" }
+
+    context "environment variable templating" do
+      before :each do
+        @foo_content = ENV["FOO"]
+        ENV["FOO"] = "foo"
+      end
+
+      after :each do
+        ENV["FOO"] = @foo_content
+      end
+
+      it "doesn't upgrade the state" do
+        allow(subject).to receive(:fetch_config).and_return(pipeline_config)
+        subject.register_pipeline(pipeline_id, pipeline_settings)
+        expect(subject.pipelines[pipeline_id].inputs.first.message).to eq("foo-bar")
       end
     end
   end
@@ -117,9 +289,9 @@
   describe "#upgrade_pipeline" do
     let(:pipeline_id) { "main" }
     let(:pipeline_config) { "input { } filter { } output { }" }
-    let(:pipeline_settings) { {
-      :config_string => pipeline_config,
-      :pipeline_workers => 4
+    let(:pipeline_args) { {
+      "config.string" => pipeline_config,
+      "pipeline.workers" => 4
     } }
     let(:new_pipeline_config) { "input { generator {} } output { }" }
 
@@ -127,6 +299,10 @@
       subject.register_pipeline(pipeline_id, pipeline_settings)
     end
 
+    after(:each) do
+      subject.shutdown
+    end
+
     context "when the upgrade fails" do
       before :each do
         allow(subject).to receive(:fetch_config).and_return(new_pipeline_config)
@@ -135,14 +311,14 @@
       end
 
       it "leaves the state untouched" do
-        subject.send(:reload_state!)
+        subject.send(:"reload_pipeline!", pipeline_id)
         expect(subject.pipelines[pipeline_id].config_str).to eq(pipeline_config)
       end
 
       context "and current state is empty" do
         it "should not start a pipeline" do
           expect(subject).to_not receive(:start_pipeline)
-          subject.send(:reload_state!)
+          subject.send(:"reload_pipeline!", pipeline_id)
         end
       end
     end
@@ -152,37 +328,27 @@
       before :each do
         allow(subject).to receive(:fetch_config).and_return(new_config)
         allow(subject).to receive(:stop_pipeline)
+        allow(subject).to receive(:start_pipeline)
       end
       it "updates the state" do
-        subject.send(:reload_state!)
+        subject.send(:"reload_pipeline!", pipeline_id)
         expect(subject.pipelines[pipeline_id].config_str).to eq(new_config)
       end
       it "starts the pipeline" do
         expect(subject).to receive(:stop_pipeline)
         expect(subject).to receive(:start_pipeline)
-        subject.send(:reload_state!)
+        subject.send(:"reload_pipeline!", pipeline_id)
       end
     end
   end
 
   describe "#fetch_config" do
-    let(:file_config) { "input { generator { count => 100 } } output { stdout { } }" }
     let(:cli_config) { "filter { drop { } } " }
-    let(:tmp_config_path) { Stud::Temporary.pathname }
-    let(:agent_args) { { :logger => logger, :config_string => "filter { drop { } } ", :config_path => tmp_config_path } }
-
-    before :each do
-      IO.write(tmp_config_path, file_config)
-    end
-
-    after :each do
-      File.unlink(tmp_config_path)
-    end
+    let(:agent_args) { { "config.string" => cli_config, "path.config" => config_file } }
 
     it "should join the config string and config path content" do
-      settings = { :config_path => tmp_config_path, :config_string => cli_config }
-      fetched_config = subject.send(:fetch_config, settings)
-      expect(fetched_config.strip).to eq(cli_config + IO.read(tmp_config_path))
+      fetched_config = subject.send(:fetch_config, agent_settings)
+      expect(fetched_config.strip).to eq(cli_config + IO.read(config_file).strip)
     end
   end
 
@@ -197,4 +363,186 @@
       expect(subject.uptime).to be >= 0
     end
   end
+
+
+  context "metrics after config reloading" do
+    let!(:config) { "input { generator { } } output { dummyoutput { } }" }
+    let!(:config_path) do
+      f = Stud::Temporary.file
+      f.write(config)
+      f.fsync
+      f.close
+      f.path
+    end
+    let(:pipeline_args) do
+      {
+        "pipeline.workers" => 2,
+        "path.config" => config_path
+      }
+    end
+
+    let(:agent_args) do
+      {
+        "config.reload.automatic" => false,
+        "pipeline.batch.size" => 1,
+        "metric.collect" => true
+      }
+    end
+
+    # We need to create theses dummy classes to know how many
+    # events where actually generated by the pipeline and successfully send to the output.
+    # Theses values are compared with what we store in the metric store.
+    class DummyOutput2 < DummyOutput; end
+
+    let!(:dummy_output) { DummyOutput.new }
+    let!(:dummy_output2) { DummyOutput2.new }
+
+    before :each do
+      allow(DummyOutput).to receive(:new).at_least(:once).with(anything).and_return(dummy_output)
+      allow(DummyOutput2).to receive(:new).at_least(:once).with(anything).and_return(dummy_output2)
+
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(LogStash::Codecs::Plain)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput2").and_return(DummyOutput2)
+
+      @abort_on_exception = Thread.abort_on_exception
+      Thread.abort_on_exception = true
+
+      @t = Thread.new do
+        subject.register_pipeline("main",  pipeline_settings)
+        subject.execute
+      end
+
+      sleep(0.01) until dummy_output.events.size > 1
+    end
+
+    after :each do
+      begin
+        subject.shutdown
+        Stud.stop!(@t)
+        @t.join
+      ensure
+        Thread.abort_on_exception = @abort_on_exception
+      end
+    end
+
+    context "when reloading a good config" do
+      let(:new_config_generator_counter) { 500 }
+      let(:new_config) { "input { generator { count => #{new_config_generator_counter} } } output { dummyoutput2 {} }" }
+      before :each do
+        # We know that the store has more events coming in.
+        i = 0
+        while dummy_output.events.size <= new_config_generator_counter
+          i += 1
+          raise "Waiting too long!" if i > 20
+          sleep(0.1)
+        end
+
+        # Also force a flush to disk to make sure ruby reload it.
+        File.open(config_path, "w") do |f|
+          f.write(new_config)
+          f.fsync
+        end
+
+        subject.send(:"reload_pipeline!", "main")
+
+        # wait until pipeline restarts
+        sleep(0.01) until dummy_output2.events.size > 0
+
+        # be eventually consistent.
+        sleep(0.01) while dummy_output2.events.size < new_config_generator_counter
+      end
+
+      it "resets the pipeline metric collector" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:events][:in].value
+        expect(value).to eq(new_config_generator_counter)
+      end
+
+      it "does not reset the global event count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/events")[:stats][:events][:in].value
+        expect(value).to be > new_config_generator_counter
+      end
+
+      it "increases the successful reload count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
+        expect(value).to eq(1)
+      end
+
+      it "does not set the failure reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_failure_timestamp].value
+        expect(value).to be(nil)
+      end
+
+      it "sets the success reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_success_timestamp].value
+        expect(value).to be_a(LogStash::Timestamp)
+      end
+
+      it "does not set the last reload error" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_error].value
+        expect(value).to be(nil)
+      end
+
+    end
+
+    context "when reloading a bad config" do
+      let(:new_config) { "input { generator { count => " }
+      let(:new_config_generator_counter) { 500 }
+      before :each do
+        # We know that the store has more events coming in.
+        i = 0
+        while dummy_output.events.size <= new_config_generator_counter
+          i += 1
+          raise "Waiting too long!" if i > 20
+          sleep(0.1)
+        end
+
+        # Also force a flush to disk to make sure ruby reload it.
+        File.open(config_path, "w") do |f|
+          f.write(new_config)
+          f.fsync
+        end
+
+        subject.send(:"reload_pipeline!", "main")
+      end
+
+      it "does not increase the successful reload count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:successes].value
+        expect(value).to eq(0)
+      end
+
+      it "does not set the successful reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_success_timestamp].value
+        expect(value).to be(nil)
+      end
+
+      it "sets the failure reload timestamp" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_failure_timestamp].value
+        expect(value).to be_a(LogStash::Timestamp)
+      end
+
+      it "sets the last reload error" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:last_error].value
+        expect(value).to be_a(Hash)
+        expect(value).to include(:message, :backtrace)
+      end
+
+      it "increases the failed reload count" do
+        snapshot = subject.metric.collector.snapshot_metric
+        value = snapshot.metric_store.get_with_path("/stats/pipelines")[:stats][:pipelines][:main][:reloads][:failures].value
+        expect(value).to be > 0
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/codecs/base_spec.rb b/logstash-core/spec/logstash/codecs/base_spec.rb
new file mode 100644
index 00000000000..42b07b5d4ed
--- /dev/null
+++ b/logstash-core/spec/logstash/codecs/base_spec.rb
@@ -0,0 +1,74 @@
+# encoding: utf-8
+require "spec_helper"
+
+DATA_DOUBLE = "data".freeze
+
+# use a dummy NOOP output to test Outputs::Base
+class LogStash::Codecs::NOOPAsync < LogStash::Codecs::Base
+  attr_reader :last_result
+  config_name "noop_async"
+
+  def encode(event)
+    @last_result = @on_event.call(event, DATA_DOUBLE)
+  end
+end
+
+class LogStash::Codecs::NOOPSync < LogStash::Codecs::Base
+  attr_reader :last_result
+  config_name "noop_sync"
+
+  def encode_sync(event)
+    DATA_DOUBLE
+  end
+end
+
+class LogStash::Codecs::NOOPMulti < LogStash::Codecs::Base
+  attr_reader :last_result
+  config_name "noop_multi"
+
+  def encode_sync(event)
+    DATA_DOUBLE
+  end
+end
+
+describe LogStash::Codecs::Base do
+  let(:params) { {} }
+  subject(:instance) { klass.new(params.dup) }
+  let(:event) { double("event") }
+  let(:encoded_data) { DATA_DOUBLE }
+  let(:encoded_tuple) { [event, encoded_data] }
+
+  describe "encoding" do
+    shared_examples "encoder types" do |codec_class|
+      let(:klass) { codec_class }
+      
+      describe "#{codec_class}" do
+        describe "multi_encode" do
+          it "should return an array of [event,data] tuples" do
+            expect(instance.multi_encode([event,event])).to eq([encoded_tuple, encoded_tuple])
+          end
+        end
+        
+        describe "#encode" do
+          before do
+            @result = nil
+            instance.on_event do |event, data|
+              @result = [event, data]
+            end
+            instance.encode(event)
+          end
+          
+          it "should yield the correct result" do
+            expect(@result).to eq(encoded_tuple)
+          end
+        end
+      end
+    end
+
+    include_examples("encoder types", LogStash::Codecs::NOOPAsync)
+    include_examples("encoder types", LogStash::Codecs::NOOPSync)
+    include_examples("encoder types", LogStash::Codecs::NOOPMulti)
+  end
+end
+
+                          
diff --git a/logstash-core/spec/logstash/config/config_ast_spec.rb b/logstash-core/spec/logstash/config/config_ast_spec.rb
index 917e0575916..657b00523c4 100644
--- a/logstash-core/spec/logstash/config/config_ast_spec.rb
+++ b/logstash-core/spec/logstash/config/config_ast_spec.rb
@@ -143,4 +143,82 @@
       end
     end
   end
+
+  context "when using two plugin sections of the same type" do
+    let(:pipeline_klass) do
+      Class.new do
+        def initialize(config)
+          grammar = LogStashConfigParser.new
+          @config = grammar.parse(config)
+          @code = @config.compile
+          eval(@code)
+        end
+        def plugin(*args);end
+      end
+    end
+    context "(filters)" do
+      let(:config_string) {
+        "input { generator { } }
+         filter { filter1 { } }
+         filter { filter1 { } }
+         output { output1 { } }"
+      }
+
+
+      it "should create a pipeline with both sections" do
+        generated_objects = pipeline_klass.new(config_string).instance_variable_get("@generated_objects")
+        filters = generated_objects.keys.map(&:to_s).select {|obj_name| obj_name.match(/^filter.+?_\d+$/) }
+        expect(filters.size).to eq(2)
+      end
+    end
+
+    context "(filters)" do
+      let(:config_string) {
+        "input { generator { } }
+         output { output1 { } }
+         output { output1 { } }"
+      }
+
+
+      it "should create a pipeline with both sections" do
+        generated_objects = pipeline_klass.new(config_string).instance_variable_get("@generated_objects")
+        outputs = generated_objects.keys.map(&:to_s).select {|obj_name| obj_name.match(/^output.+?_\d+$/) }
+        expect(outputs.size).to eq(2)
+      end
+    end
+  end
+  context "when creating two instances of the same configuration" do
+
+    let(:config_string) {
+      "input { generator { } }
+       filter {
+         if [type] == 'test' { filter1 { } }
+       }
+       output {
+         output1 { }
+       }"
+    }
+
+    let(:pipeline_klass) do
+      Class.new do
+        def initialize(config)
+          grammar = LogStashConfigParser.new
+          @config = grammar.parse(config)
+          @code = @config.compile
+          eval(@code)
+        end
+        def plugin(*args);end
+      end
+    end
+
+    describe "generated conditional functionals" do
+      it "should be created per instance" do
+        instance_1 = pipeline_klass.new(config_string)
+        instance_2 = pipeline_klass.new(config_string)
+        generated_method_1 = instance_1.instance_variable_get("@generated_objects")[:cond_func_1]
+        generated_method_2 = instance_2.instance_variable_get("@generated_objects")[:cond_func_1]
+        expect(generated_method_1).to_not be(generated_method_2)
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/config/loader_spec.rb b/logstash-core/spec/logstash/config/loader_spec.rb
index b51272ee13a..955feb2f615 100644
--- a/logstash-core/spec/logstash/config/loader_spec.rb
+++ b/logstash-core/spec/logstash/config/loader_spec.rb
@@ -3,7 +3,9 @@
 require "logstash/config/loader"
 
 describe LogStash::Config::Loader do
-  subject { described_class.new(Cabin::Channel.get) }
+  let(:logger) { double("logger") }
+  subject { described_class.new(logger) }
+
   context "when local" do
     before { expect(subject).to receive(:local_config).with(path) }
 
diff --git a/logstash-core/spec/logstash/config/mixin_spec.rb b/logstash-core/spec/logstash/config/mixin_spec.rb
index 2a9bb8ac3d5..a5b74cf5726 100644
--- a/logstash-core/spec/logstash/config/mixin_spec.rb
+++ b/logstash-core/spec/logstash/config/mixin_spec.rb
@@ -68,6 +68,74 @@
     end
   end
 
+  context "when validating lists of items" do
+    let(:klass) do
+      Class.new(LogStash::Filters::Base)  do
+        config_name "multiuri"
+        config :uris, :validate => :uri, :list => true
+        config :strings, :validate => :string, :list => true
+        config :required_strings, :validate => :string, :list => true, :required => true
+      end
+    end
+
+    let(:uris) { ["http://example.net/1", "http://example.net/2"] }
+    let(:safe_uris) { uris.map {|str| ::LogStash::Util::SafeURI.new(str) } }
+    let(:strings) { ["I am a", "modern major general"] }
+    let(:required_strings) { ["required", "strings"] }
+
+    subject { klass.new("uris" => uris, "strings" => strings, "required_strings" => required_strings) }
+
+    it "a URI list should return an array of URIs" do
+      expect(subject.uris).to match_array(safe_uris)
+    end
+
+    it "a string list should return an array of strings" do
+      expect(subject.strings).to match_array(strings)
+    end
+
+    context "with a scalar value" do
+      let(:strings) { "foo" }
+
+      it "should return the scalar value as a single element array" do
+        expect(subject.strings).to match_array([strings])
+      end
+    end
+
+    context "with an empty list" do
+      let(:strings) { [] }
+
+      it "should return nil" do
+        expect(subject.strings).to be_nil
+      end
+    end
+
+    describe "with required => true" do
+      context "and a single element" do
+        let(:required_strings) { ["foo"] }
+
+        it "should return the single value" do
+          expect(subject.required_strings).to eql(required_strings)
+        end
+      end
+
+      context "with an empty list" do
+        let (:required_strings) { [] }
+
+        it "should raise a configuration error" do
+          expect { subject.required_strings }.to raise_error(LogStash::ConfigurationError)
+        end        
+      end
+
+      context "with no value specified" do
+        let (:required_strings) { nil }
+
+        it "should raise a configuration error" do
+          expect { subject.required_strings }.to raise_error(LogStash::ConfigurationError)
+        end
+      end          
+    end
+  end
+
   context "when validating :password" do
     let(:klass) do
       Class.new(LogStash::Filters::Base)  do
@@ -96,6 +164,99 @@
       clone = subject.class.new(subject.params)
       expect(clone.password.value).to(be == secret)
     end
+
+    it "should obfuscate original_params" do
+      expect(subject.original_params['password']).to(be_a(LogStash::Util::Password))
+    end
+  end
+
+  context "when validating :uri" do
+    let(:klass) do
+      Class.new(LogStash::Filters::Base)  do
+        config_name "fakeuri"
+        config :uri, :validate => :uri
+      end
+    end
+
+    shared_examples("safe URI") do |options|
+      options ||= {}
+      
+      subject { klass.new("uri" => uri_str) }
+
+      it "should be a SafeURI object" do
+        expect(subject.uri).to(be_a(LogStash::Util::SafeURI))
+      end
+
+      it "should correctly copy URI types" do
+        clone = subject.class.new(subject.params)
+        expect(clone.uri.to_s).to eql(uri_hidden)
+      end
+
+      it "should make the real URI object availale under #uri" do
+        expect(subject.uri.uri).to be_a(::URI)
+      end
+
+      it "should obfuscate original_params" do
+        expect(subject.original_params['uri']).to(be_a(LogStash::Util::SafeURI))
+      end
+
+      if !options[:exclude_password_specs]
+        describe "passwords" do
+          it "should make password values hidden with #to_s" do
+            expect(subject.uri.to_s).to eql(uri_hidden)
+          end
+
+          it "should make password values hidden with #inspect" do
+            expect(subject.uri.inspect).to eql(uri_hidden)
+          end
+        end
+      end
+
+      context "attributes" do
+        [:scheme, :user, :password, :hostname, :path].each do |attr|
+          it "should make #{attr} available" do
+            expect(subject.uri.send(attr)).to eql(self.send(attr))
+          end
+        end
+      end
+    end
+
+    context "with a host:port combination" do
+      let(:scheme) { nil }
+      let(:user) { nil }
+      let(:password) { nil }
+      let(:hostname) { "myhostname" }
+      let(:port) { 1234 }
+      let(:path) { "" }
+      let(:uri_str) { "#{hostname}:#{port}" }
+      let(:uri_hidden) { "//#{hostname}:#{port}" }
+
+      include_examples("safe URI", :exclude_password_specs => true)
+    end
+
+    context "with a username / password" do
+      let(:scheme) { "myscheme" }
+      let(:user) { "myuser" }
+      let(:password) { "fancypants" }
+      let(:hostname) { "myhostname" }
+      let(:path) { "/my/path" }
+      let(:uri_str) { "#{scheme}://#{user}:#{password}@#{hostname}#{path}" }
+      let(:uri_hidden) { "#{scheme}://#{user}:#{LogStash::Util::SafeURI::PASS_PLACEHOLDER}@#{hostname}#{path}" }
+
+      include_examples("safe URI")
+    end
+
+    context "without a username / password" do
+      let(:scheme) { "myscheme" }
+      let(:user) { nil }
+      let(:password) { nil }
+      let(:hostname) { "myhostname" }
+      let(:path) { "/my/path" }
+      let(:uri_str) { "#{scheme}://#{hostname}#{path}" }
+      let(:uri_hidden) { "#{scheme}://#{hostname}#{path}" }
+
+      include_examples("safe URI")
+    end
   end
 
   describe "obsolete settings" do
@@ -156,11 +317,15 @@
     let(:plugin_class) do
       Class.new(LogStash::Filters::Base)  do
         config_name "one_plugin"
-        config :oneString, :validate => :string
-        config :oneBoolean, :validate => :boolean
-        config :oneNumber, :validate => :number
-        config :oneArray, :validate => :array
-        config :oneHash, :validate => :hash
+        config :oneString, :validate => :string, :required => false
+        config :oneBoolean, :validate => :boolean, :required => false
+        config :oneNumber, :validate => :number, :required => false
+        config :oneArray, :validate => :array, :required => false
+        config :oneHash, :validate => :hash, :required => false
+
+        def initialize(params)
+          super(params)
+        end
       end
     end
 
@@ -213,7 +378,7 @@
           "oneString" => "${FunString:foo}",
           "oneBoolean" => "${FunBool:false}",
           "oneArray" => [ "first array value", "${FunString:foo}" ],
-          "oneHash" => { "key1" => "${FunString:foo}", "key2" => "$FunString is ${FunBool}", "key3" => "${FunBool:false} or ${funbool:false}" }
+          "oneHash" => { "key1" => "${FunString:foo}", "key2" => "${FunString} is ${FunBool}", "key3" => "${FunBool:false} or ${funbool:false}" }
         )
       end
 
@@ -223,8 +388,34 @@
         expect(subject.oneArray).to(be == [ "first array value", "fancy" ])
         expect(subject.oneHash).to(be == { "key1" => "fancy", "key2" => "fancy is true", "key3" => "true or false" })
       end
+    end
+
+    context "should support $ in values" do
+      before do
+        ENV["bar"] = "foo"
+        ENV["f$$"] = "bar"
+      end
+
+      after do
+        ENV.delete("bar")
+        ENV.delete("f$$")
+      end
+
+      subject do
+        plugin_class.new(
+          "oneString" => "${f$$:val}",
+          "oneArray" => ["foo$bar", "${bar:my$val}"]
+          # "dollar_in_env" => "${f$$:final}"
+        )
+      end
+
+      it "should support $ in values" do
+        expect(subject.oneArray).to(be == ["foo$bar", "foo"])
+      end
 
+      it "should not support $ in environment variable name" do
+        expect(subject.oneString).to(be == "${f$$:val}")
+      end
     end
   end
-
 end
diff --git a/logstash-core/spec/logstash/filter_delegator_spec.rb b/logstash-core/spec/logstash/filter_delegator_spec.rb
index 595fbfedc36..951c72f69d3 100644
--- a/logstash-core/spec/logstash/filter_delegator_spec.rb
+++ b/logstash-core/spec/logstash/filter_delegator_spec.rb
@@ -10,7 +10,8 @@
   let(:config) do
     { "host" => "127.0.0.1", "id" => filter_id }
   end
-  let(:metric) { LogStash::Instrument::NullMetric.new }
+  let(:collector) { [] }
+  let(:metric) { LogStash::Instrument::NamespacedNullMetric.new(collector, :null) }
   let(:events) { [LogStash::Event.new, LogStash::Event.new] }
 
   before :each do
@@ -67,15 +68,22 @@ def filter(event)
     end
 
     context "when the filter buffer events" do
-      it "doesn't increment out" do
+      before do
+        allow(metric).to receive(:increment).with(anything, anything)
+      end
+
+      it "has incremented :in" do
         expect(metric).to receive(:increment).with(:in, events.size)
-        expect(metric).not_to receive(:increment)
+        subject.multi_filter(events)
+      end
 
+      it "has not incremented :out" do
+        expect(metric).not_to receive(:increment).with(:out, anything)
         subject.multi_filter(events)
       end
     end
 
-    context "when the fitler create more events" do
+    context "when the filter create more events" do
       let(:plugin_klass) do
         Class.new(LogStash::Filters::Base) do
           config_name "super_plugin"
@@ -91,6 +99,10 @@ def filter(event)
         end
       end
 
+      before do
+        allow(metric).to receive(:increment).with(anything, anything)
+      end
+
       it "increments the in/out of the metric" do
         expect(metric).to receive(:increment).with(:in, events.size)
         expect(metric).to receive(:increment).with(:out, events.size * 2)
@@ -112,6 +124,10 @@ def filter(event)
       end
     end
 
+    before do
+      allow(metric).to receive(:increment).with(anything, anything)
+    end
+
     it "doesnt define a flush method" do
       expect(subject.respond_to?(:flush)).to be_falsey
     end
diff --git a/logstash-core/spec/logstash/filters/base_spec.rb b/logstash-core/spec/logstash/filters/base_spec.rb
index 177c44dcb8c..26c5d6c5438 100644
--- a/logstash-core/spec/logstash/filters/base_spec.rb
+++ b/logstash-core/spec/logstash/filters/base_spec.rb
@@ -62,7 +62,7 @@ def filter(event)
     CONFIG
 
     sample "example" do
-      insist { subject["new_field"] } == ["new_value", "new_value_2"]
+      insist { subject.get("new_field") } == ["new_value", "new_value_2"]
     end
   end
 
@@ -76,7 +76,7 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
   end
 
@@ -90,11 +90,11 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "test"]
     end
   end
 
@@ -108,19 +108,19 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop") do
-      insist { subject["tags"] } == ["test"]
+      insist { subject.get("tags") } == ["test"]
     end
 
     sample("type" => "noop", "tags" => ["t1"]) do
-      insist { subject["tags"] } == ["t1", "test"]
+      insist { subject.get("tags") } == ["t1", "test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1", "t2", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "test"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1", "t2", "t3", "test"]
+      insist { subject.get("tags") } == ["t1", "t2", "t3", "test"]
     end
   end
 
@@ -134,27 +134,27 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop", "tags" => ["t4"]) do
-      insist { subject["tags"] } == ["t4"]
+      insist { subject.get("tags") } == ["t4"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2", "t3"]) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"t2\", \"t3\"]}")) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     sample("type" => "noop", "tags" => ["t1", "t2"]) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"t2\"]}")) do
-      insist { subject["tags"] } == ["t1"]
+      insist { subject.get("tags") } == ["t1"]
     end
   end
 
@@ -168,13 +168,13 @@ def filter(event)
     CONFIG
 
     sample("type" => "noop", "tags" => ["t1", "goaway", "t3"], "blackhole" => "goaway") do
-      insist { subject["tags"] } == ["t1", "t3"]
+      insist { subject.get("tags") } == ["t1", "t3"]
     end
 
     # also test from Json deserialized data to test the handling of native Java collections by JrJackson
     # see https://github.com/elastic/logstash/issues/2261
     sample(LogStash::Json.load("{\"type\":\"noop\", \"tags\":[\"t1\", \"goaway\", \"t3\"], \"blackhole\":\"goaway\"}")) do
-      insist { subject["tags"] } == ["t1", "t3"]
+      insist { subject.get("tags") } == ["t1", "t3"]
     end
   end
 
@@ -230,7 +230,7 @@ def filter(event)
 
     sample("type" => "noop", "t1" => ["t2", "t3"]) do
       insist { subject }.include?("t1")
-      insist { subject["[t1][0]"] } == "t3"
+      insist { subject.get("[t1][0]") } == "t3"
     end
   end
 
diff --git a/logstash-core/spec/logstash/inputs/base_spec.rb b/logstash-core/spec/logstash/inputs/base_spec.rb
index d87f07b49f6..a3f01fa89e1 100644
--- a/logstash-core/spec/logstash/inputs/base_spec.rb
+++ b/logstash-core/spec/logstash/inputs/base_spec.rb
@@ -15,50 +15,50 @@ def register; end
     input = LogStash::Inputs::NOOP.new("tags" => "value")
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value"])
+    expect(evt.get("tags")).to eq(["value"])
   end
 
   it "should add multiple tag" do
     input = LogStash::Inputs::NOOP.new("tags" => ["value1","value2"])
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value1","value2"])
+    expect(evt.get("tags")).to eq(["value1","value2"])
   end
 
   it "should allow duplicates  tag" do
     input = LogStash::Inputs::NOOP.new("tags" => ["value","value"])
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["value","value"])
+    expect(evt.get("tags")).to eq(["value","value"])
   end
 
   it "should add tag with sprintf" do
     input = LogStash::Inputs::NOOP.new("tags" => "%{type}")
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["tags"]).to eq(["noop"])
+    expect(evt.get("tags")).to eq(["noop"])
   end
 
   it "should add single field" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"field" => "value"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["field"]).to eq("value")
+    expect(evt.get("field")).to eq("value")
   end
 
   it "should add single field with sprintf" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"%{type}" => "%{type}"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["noop"]).to eq("noop")
+    expect(evt.get("noop")).to eq("noop")
   end
 
   it "should add multiple field" do
     input = LogStash::Inputs::NOOP.new("add_field" => {"field" => ["value1", "value2"], "field2" => "value"})
     evt = LogStash::Event.new({"type" => "noop"})
     input.instance_eval {decorate(evt)}
-    expect(evt["field"]).to eq(["value1","value2"])
-    expect(evt["field2"]).to eq("value")
+    expect(evt.get("field")).to eq(["value1","value2"])
+    expect(evt.get("field2")).to eq("value")
   end
 end
 
diff --git a/logstash-core/spec/logstash/inputs/metrics_spec.rb b/logstash-core/spec/logstash/inputs/metrics_spec.rb
index 97a89facda3..842e8b48bfe 100644
--- a/logstash-core/spec/logstash/inputs/metrics_spec.rb
+++ b/logstash-core/spec/logstash/inputs/metrics_spec.rb
@@ -3,15 +3,17 @@
 require "spec_helper"
 
 describe LogStash::Inputs::Metrics do
+  let(:collector) { LogStash::Instrument::Collector.new }
+  let(:metric) { LogStash::Instrument::Metric.new(collector) }
+  let(:queue) { [] }
+
   before :each do
-    LogStash::Instrument::Collector.instance.clear
+    subject.metric = metric
   end
 
-  let(:queue) { [] }
-
   describe "#run" do
     it "should register itself to the collector observer" do
-      expect(LogStash::Instrument::Collector.instance).to receive(:add_observer).with(subject)
+      expect(collector).to receive(:add_observer).with(subject)
       t = Thread.new { subject.run(queue) }
       sleep(0.1) # give a bit of time to the thread to start
       subject.stop
@@ -19,24 +21,21 @@
   end
 
   describe "#update" do
-    let(:namespaces)  { [:root, :base] }
-    let(:key)        { :foo }
-    let(:metric_store) { LogStash::Instrument::MetricStore.new }
-
     it "should fill up the queue with received events" do
       Thread.new { subject.run(queue) }
       sleep(0.1)
       subject.stop
 
-      metric_store.fetch_or_store(namespaces, key, LogStash::Instrument::MetricType::Counter.new(namespaces, key))
-      subject.update(LogStash::Instrument::Snapshot.new(metric_store))
+      metric.increment([:root, :test], :plugin)
+
+      subject.update(collector.snapshot_metric)
       expect(queue.count).to eq(1)
     end
   end
 
   describe "#stop" do
     it "should remove itself from the the collector observer" do
-      expect(LogStash::Instrument::Collector.instance).to receive(:delete_observer).with(subject)
+      expect(collector).to receive(:delete_observer).with(subject)
       t = Thread.new { subject.run(queue) }
       sleep(0.1) # give a bit of time to the thread to start
       subject.stop
diff --git a/logstash-core/spec/logstash/instrument/collector_spec.rb b/logstash-core/spec/logstash/instrument/collector_spec.rb
index b96be4a5ede..2a9979d0caa 100644
--- a/logstash-core/spec/logstash/instrument/collector_spec.rb
+++ b/logstash-core/spec/logstash/instrument/collector_spec.rb
@@ -3,7 +3,7 @@
 require "spec_helper"
 
 describe LogStash::Instrument::Collector do
-  subject { LogStash::Instrument::Collector.instance }
+  subject { LogStash::Instrument::Collector.new }
   describe "#push" do
     let(:namespaces_path) { [:root, :pipelines, :pipelines01] }
     let(:key) { :my_key }
diff --git a/logstash-core/spec/logstash/instrument/metric_spec.rb b/logstash-core/spec/logstash/instrument/metric_spec.rb
index 0a8a65d4338..4e2eae81e92 100644
--- a/logstash-core/spec/logstash/instrument/metric_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_spec.rb
@@ -67,15 +67,15 @@
 
   context "#time" do
     let(:sleep_time) { 2 }
-    let(:sleep_time_ms) { sleep_time * 1_000_000 }
+    let(:sleep_time_ms) { sleep_time * 1_000 }
 
     it "records the duration" do
       subject.time(:root, :duration_ms) { sleep(sleep_time) }
 
-      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5000)
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5)
       expect(collector[0]).to match(:root)
       expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:mean)
+      expect(collector[2]).to be(:counter)
     end
 
     it "returns the value of the executed block" do
@@ -90,7 +90,7 @@
       expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 0.1)
       expect(collector[0]).to match(:root)
       expect(collector[1]).to be(:duration_ms)
-      expect(collector[2]).to be(:mean)
+      expect(collector[2]).to be(:counter)
     end
   end
 
diff --git a/logstash-core/spec/logstash/instrument/metric_store_spec.rb b/logstash-core/spec/logstash/instrument/metric_store_spec.rb
index 4371977355b..993be7fd4f3 100644
--- a/logstash-core/spec/logstash/instrument/metric_store_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_store_spec.rb
@@ -142,6 +142,73 @@
       end
     end
 
+    describe "get_shallow" do
+      it "should retrieve a path as a single value" do
+        r = subject.get_shallow(:node, :sashimi, :pipelines, :pipeline01, :processed_events_in)
+        expect(r.value).to eql(1)
+      end
+    end
+
+    describe "extract_metrics" do
+      it "should retrieve non-nested values correctly" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines, :pipeline01],
+          :processed_events_in,
+          :processed_events_out,
+        )
+        expect(r[:processed_events_in]).to eql(1)
+        expect(r[:processed_events_out]).to eql(1)
+      end
+
+      it "should retrieve nested values correctly alongside non-nested ones" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines, :pipeline01],
+          :processed_events_in,
+          [:plugins, :"logstash-output-elasticsearch", :event_in]
+        )
+       expect(r[:processed_events_in]).to eql(1)
+        expect(r[:plugins][:"logstash-output-elasticsearch"][:event_in]).to eql(1)
+      end
+
+      it "should retrieve multiple nested keys at a given location" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines],
+          [:pipeline01, [:processed_events_in, :processed_events_out]]
+        )
+
+        expect(r[:pipeline01][:processed_events_in]).to eql(1)
+        expect(r[:pipeline01][:processed_events_out]).to eql(1)
+      end
+
+      it "should retrieve a single key nested in multiple places" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines],
+          [[:pipeline01, :pipeline02], :processed_events_out]
+        )
+
+        expect(r[:pipeline01][:processed_events_out]).to eql(1)
+        expect(r[:pipeline02][:processed_events_out]).to eql(1)
+      end
+
+      it "handle overlaps of paths" do
+        r = subject.extract_metrics(
+          [:node, :sashimi, :pipelines],
+          [:pipeline01, :processed_events_in],
+          [[:pipeline01, :pipeline02], :processed_events_out]
+        )
+
+        expect(r[:pipeline01][:processed_events_in]).to eql(1)
+        expect(r[:pipeline01][:processed_events_out]).to eql(1)
+        expect(r[:pipeline02][:processed_events_out]).to eql(1)
+      end
+    end
+
+    describe "#size" do
+      it "returns the number of unique metrics" do
+        expect(subject.size).to eq(metric_events.size)
+      end
+    end
+
     describe "#each" do
       it "retrieves all the metric" do
         expect(subject.each.size).to eq(metric_events.size)
@@ -160,4 +227,35 @@
       end
     end
   end
+
+  describe "#prune" do
+    let(:metric_events) {
+      [
+        [[:node, :sashimi, :pipelines, :pipeline01, :plugins, :"logstash-output-elasticsearch"], :event_in, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_in, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline01], :processed_events_out, :increment],
+        [[:node, :sashimi, :pipelines, :pipeline02], :processed_events_out, :increment],
+      ]
+    }
+
+    before :each do
+      # Lets add a few metrics in the store before trying to find them
+      metric_events.each do |namespaces, metric_key, action|
+        metric = subject.fetch_or_store(namespaces, metric_key, LogStash::Instrument::MetricType::Counter.new(namespaces, metric_key))
+        metric.execute(action)
+      end
+    end
+
+    it "should remove all keys with the same starting path as the argument" do
+      expect(subject.get(:node, :sashimi, :pipelines, :pipeline01)).to be_a(Hash)
+      subject.prune("/node/sashimi/pipelines/pipeline01")
+      expect { subject.get(:node, :sashimi, :pipelines, :pipeline01) }.to raise_error LogStash::Instrument::MetricStore::MetricNotFound
+    end
+
+    it "should keep other metrics on different path branches" do
+      expect(subject.get(:node, :sashimi, :pipelines, :pipeline02)).to be_a(Hash)
+      subject.prune("/node/sashimi/pipelines/pipeline01")
+      expect { subject.get(:node, :sashimi, :pipelines, :pipeline02) }.to_not raise_error
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
index b51aebc792d..05d9054069d 100644
--- a/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_type/counter_spec.rb
@@ -28,12 +28,7 @@
 
   context "When creating a hash " do
     it "creates the hash from all the values" do
-      metric_hash = {
-        "key" => key,
-        "namespaces" => namespaces,
-        "value" => 0,
-        "type" => "counter"
-      }
+      metric_hash = { key => 0 }
       expect(subject.to_hash).to match(metric_hash)
     end
   end
diff --git a/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb b/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
index 0481f6d283b..e285a8eb5cf 100644
--- a/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
+++ b/logstash-core/spec/logstash/instrument/metric_type/gauge_spec.rb
@@ -29,10 +29,7 @@
   context "When creating a hash " do
     it "creates the hash from all the values" do
       metric_hash = {
-        "key" => key,
-        "namespaces" => namespaces,
-        "value" => value,
-        "type" => "gauge"
+        key => value
       }
       expect(subject.to_hash).to match(metric_hash)
     end
diff --git a/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb b/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
index 6ba84168df9..1f7f1d8ab9f 100644
--- a/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
+++ b/logstash-core/spec/logstash/instrument/namespaced_metric_spec.rb
@@ -2,10 +2,11 @@
 require "logstash/instrument/namespaced_metric"
 require "logstash/instrument/metric"
 require_relative "../../support/matchers"
+require_relative "../../support/shared_examples"
 require "spec_helper"
 
 describe LogStash::Instrument::NamespacedMetric do
-  let(:namespace) { :stats }
+  let(:namespace) { :root }
   let(:collector) { [] }
   let(:metric) { LogStash::Instrument::Metric.new(collector) }
 
@@ -22,4 +23,69 @@
   it "returns the value of the block" do
     expect(subject.time(:duration_ms) { "hello" }).to eq("hello")
   end
+
+  it "its doesnt change the original `namespace` when creating a subnamespace" do
+    new_namespace = subject.namespace(:wally)
+
+    expect(subject.namespace_name).to eq([namespace])
+    expect(new_namespace.namespace_name).to eq([:root, :wally])
+  end
+
+  context "#increment" do
+    it "a counter by 1" do
+      metric = subject.increment(:error_rate)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 1)
+    end
+
+    it "a counter by a provided value" do
+      metric = subject.increment(:error_rate, 20)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :increment, 20)
+    end
+  end
+
+  context "#decrement" do
+    it "a counter by 1" do
+      metric = subject.decrement(:error_rate)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 1)
+    end
+
+    it "a counter by a provided value" do
+      metric = subject.decrement(:error_rate, 20)
+      expect(collector).to be_a_metric_event([:root, :error_rate], :counter, :decrement, 20)
+    end
+  end
+
+  context "#gauge" do
+    it "set the value of a key" do
+      metric = subject.gauge(:size_queue, 20)
+      expect(collector).to be_a_metric_event([:root, :size_queue], :gauge, :set, 20)
+    end
+  end
+
+  context "#time" do
+    let(:sleep_time) { 2 }
+    let(:sleep_time_ms) { sleep_time * 1_000 }
+
+    it "records the duration" do
+      subject.time(:duration_ms) { sleep(sleep_time) }
+
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 5)
+      expect(collector[0]).to match([:root])
+      expect(collector[1]).to be(:duration_ms)
+      expect(collector[2]).to be(:counter)
+    end
+
+    it "return a TimedExecution" do
+      execution = subject.time(:duration_ms)
+      sleep(sleep_time)
+      execution.stop
+
+      expect(collector.last).to be_within(sleep_time_ms).of(sleep_time_ms + 0.1)
+      expect(collector[0]).to match([:root])
+      expect(collector[1]).to be(:duration_ms)
+      expect(collector[2]).to be(:counter)
+    end
+  end
+
+  include_examples "metrics commons operations"
 end
diff --git a/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb b/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb
new file mode 100644
index 00000000000..fdd831dfbfc
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/namespaced_null_metric_spec.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require "logstash/instrument/namespaced_null_metric"
+require "logstash/instrument/null_metric"
+require_relative "../../support/matchers"
+require "spec_helper"
+
+describe LogStash::Instrument::NamespacedNullMetric do
+  let(:namespace) { :root }
+  let(:collector) { [] }
+  let(:metric) { LogStash::Instrument::NullMetric.new(collector) }
+
+  subject { described_class.new(metric, namespace) }
+
+  it "defines the same interface as `Metric`" do
+    expect(described_class).to implement_interface_of(LogStash::Instrument::NamespacedMetric)
+  end
+
+  it "returns a TimedException when we call without a block" do
+    expect(subject.time(:duration_ms)).to be(LogStash::Instrument::NullMetric::NullTimedExecution)
+  end
+
+  it "returns the value of the block" do
+    expect(subject.time(:duration_ms) { "hello" }).to eq("hello")
+  end
+
+  it "its doesnt change the original `namespace` when creating a subnamespace" do
+    new_namespace = subject.namespace(:wally)
+
+    expect(subject.namespace_name).to eq([namespace])
+    expect(new_namespace.namespace_name).to eq([:root, :wally])
+  end
+
+end
diff --git a/logstash-core/spec/logstash/instrument/null_metric_spec.rb b/logstash-core/spec/logstash/instrument/null_metric_spec.rb
index ec55d341be4..27a861eae69 100644
--- a/logstash-core/spec/logstash/instrument/null_metric_spec.rb
+++ b/logstash-core/spec/logstash/instrument/null_metric_spec.rb
@@ -1,21 +1,23 @@
 # encoding: utf-8
 require "logstash/instrument/null_metric"
 require "logstash/instrument/namespaced_metric"
+require_relative "../../support/shared_examples"
 require_relative "../../support/matchers"
+require "spec_helper"
 
 describe LogStash::Instrument::NullMetric do
+
+  let(:key) { "test" }
+  let(:collector) { [] }
+  subject { LogStash::Instrument::NullMetric.new(collector) }
+
   it "defines the same interface as `Metric`" do
-    expect(described_class).to implement_interface_of(LogStash::Instrument::NamespacedMetric)
+    expect(described_class).to implement_interface_of(LogStash::Instrument::Metric)
   end
 
-  describe "#time" do
-    it "returns the value of the block without recording any metrics" do
-      expect(subject.time(:execution_time) { "hello" }).to eq("hello")
-    end
-
-    it "return a TimedExecution" do
-      execution = subject.time(:do_something)
-      expect { execution.stop }.not_to raise_error
+  describe "#namespace" do
+    it "return a NamespacedNullMetric" do
+      expect(subject.namespace(key)).to be_kind_of LogStash::Instrument::NamespacedNullMetric
     end
   end
 end
diff --git a/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb b/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb
new file mode 100644
index 00000000000..e3e113ca117
--- /dev/null
+++ b/logstash-core/spec/logstash/instrument/periodic_poller/jvm_spec.rb
@@ -0,0 +1,75 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/instrument/periodic_poller/jvm"
+require "logstash/instrument/collector"
+
+describe LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName do
+  subject { LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName }
+
+  context "when the gc is of young type" do
+    LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName::YOUNG_GC_NAMES.each do |name|
+      it "returns young for #{name}" do
+        expect(subject.get(name)).to eq(:young)
+      end
+    end
+  end
+
+  context "when the gc is of old type" do
+    LogStash::Instrument::PeriodicPoller::JVM::GarbageCollectorName::OLD_GC_NAMES.each do |name|
+      it "returns old for #{name}" do
+        expect(subject.get(name)).to eq(:old)
+      end
+    end
+  end
+
+  it "returns `nil` when we dont know the gc name" do
+      expect(subject.get("UNKNOWN GC")).to be_nil
+  end
+end
+
+describe LogStash::Instrument::PeriodicPoller::JVM do
+  let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+  let(:options) { {} }
+  subject(:jvm) { described_class.new(metric, options) }
+
+  it "should initialize cleanly" do
+    expect { jvm }.not_to raise_error
+  end
+
+  describe "collections" do
+    subject(:collection) { jvm.collect }
+    it "should run cleanly" do
+      expect { collection }.not_to raise_error
+    end
+
+    describe "metrics" do
+      before(:each) { jvm.collect }
+      let(:snapshot_store) { metric.collector.snapshot_metric.metric_store }
+      subject(:jvm_metrics) { snapshot_store.get_shallow(:jvm) }
+
+      # Make looking up metric paths easy when given varargs of keys
+      # e.g. mval(:parent, :child)
+      def mval(*metric_path)
+        metric_path.reduce(jvm_metrics) {|acc,k| acc[k]}.value
+      end
+
+      [
+        [:process, :max_file_descriptors],
+        [:process, :open_file_descriptors],
+        [:process, :peak_open_file_descriptors],
+        [:process, :mem, :total_virtual_in_bytes],
+        [:process, :cpu, :total_in_millis],
+        [:process, :cpu, :percent],
+        [:gc, :collectors, :young, :collection_count],
+        [:gc, :collectors, :young, :collection_time_in_millis],
+        [:gc, :collectors, :old, :collection_count],
+        [:gc, :collectors, :old, :collection_time_in_millis]
+      ].each do |path|
+        path = Array(path)
+        it "should have a value for #{path} that is Numeric" do
+          expect(mval(*path)).to be_a(Numeric)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/output_delegator_spec.rb b/logstash-core/spec/logstash/output_delegator_spec.rb
index 524ad779ec9..56dbd5168f7 100644
--- a/logstash-core/spec/logstash/output_delegator_spec.rb
+++ b/logstash-core/spec/logstash/output_delegator_spec.rb
@@ -5,23 +5,31 @@
 describe LogStash::OutputDelegator do
   let(:logger) { double("logger") }
   let(:events) { 7.times.map { LogStash::Event.new }}
-  let(:default_worker_count) { 1 }
+  let(:plugin_args) { {"id" => "foo", "arg1" => "val1"} }
+  let(:collector) { [] }
+  let(:metric) { LogStash::Instrument::NamespacedNullMetric.new(collector, :null) }
 
-  subject { described_class.new(logger, out_klass, default_worker_count, LogStash::Instrument::NullMetric.new) }
+  subject { described_class.new(logger, out_klass, metric, ::LogStash::OutputDelegatorStrategyRegistry.instance, plugin_args) }
 
   context "with a plain output plugin" do
     let(:out_klass) { double("output klass") }
     let(:out_inst) { double("output instance") }
+    let(:concurrency) { :single }
 
     before(:each) do
+      # use the same metric instance
+      allow(metric).to receive(:namespace).with(any_args).and_return(metric)
+
       allow(out_klass).to receive(:new).with(any_args).and_return(out_inst)
-      allow(out_klass).to receive(:threadsafe?).and_return(false)
-      allow(out_klass).to receive(:workers_not_supported?).and_return(false)
+      allow(out_klass).to receive(:name).and_return("example")
+      allow(out_klass).to receive(:concurrency).with(any_args).and_return concurrency
+      allow(out_klass).to receive(:config_name).and_return("dummy_plugin")
       allow(out_inst).to receive(:register)
       allow(out_inst).to receive(:multi_receive)
       allow(out_inst).to receive(:metric=).with(any_args)
       allow(out_inst).to receive(:id).and_return("a-simple-plugin")
-      allow(out_inst).to receive(:plugin_unique_name).and_return("hello-123")
+
+      allow(subject.metric_events).to receive(:increment).with(any_args)
       allow(logger).to receive(:debug).with(any_args)
     end
 
@@ -29,8 +37,14 @@
       expect { subject }.not_to raise_error
     end
 
+    it "should push the name of the plugin to the metric" do
+      expect(metric).to receive(:gauge).with(:name, out_klass.config_name)
+      described_class.new(logger, out_klass, metric, ::LogStash::OutputDelegatorStrategyRegistry.instance, plugin_args)
+    end
+
     context "after having received a batch of events" do
       before do
+        subject.register
         subject.multi_receive(events)
       end
 
@@ -39,96 +53,88 @@
       end
 
       it "should increment the number of events received" do
-        expect(subject.events_received).to eql(events.length)
+        expect(subject.metric_events).to have_received(:increment).with(:in, events.length)
+        expect(subject.metric_events).to have_received(:increment).with(:out, events.length)
       end
     end
 
-    it "should register all workers on register" do
-      expect(out_inst).to receive(:register)
-      subject.register
-    end
-
-    it "should close all workers when closing" do
-      expect(out_inst).to receive(:do_close)
-      subject.do_close
-    end
-
-    describe "concurrency and worker support" do
-      describe "non-threadsafe outputs that allow workers" do
-        let(:default_worker_count) { 3 }
-
-        before do
-          allow(out_klass).to receive(:threadsafe?).and_return(false)
-          allow(out_klass).to receive(:workers_not_supported?).and_return(false)
-          allow(out_inst).to receive(:metric=).with(any_args)
-          allow(out_inst).to receive(:id).and_return("a-simple-plugin")
-        end
-
-        it "should instantiate multiple workers" do
-          expect(subject.workers.length).to eql(default_worker_count)
-        end
-
-        it "should send received events to the worker" do
-          expect(out_inst).to receive(:multi_receive).with(events)
-          subject.multi_receive(events)
-        end
+    describe "closing" do
+      before do
+        subject.register
       end
 
-      describe "threadsafe outputs" do
-        before do
-          allow(out_klass).to receive(:threadsafe?).and_return(true)
-          allow(out_inst).to receive(:metric=).with(any_args)
-          allow(out_inst).to receive(:id).and_return("a-simple-plugin")
-          allow(out_klass).to receive(:workers_not_supported?).and_return(false)
-        end
-
-        it "should return true when threadsafe? is invoked" do
-          expect(subject.threadsafe?).to eql(true)
-        end
-
-        it "should define a threadsafe_worker" do
-          expect(subject.send(:threadsafe_worker)).to eql(out_inst)
-        end
-
-        it "should utilize threadsafe_multi_receive" do
-          expect(subject.send(:threadsafe_worker)).to receive(:multi_receive).with(events)
-          subject.multi_receive(events)
-        end
-
-        it "should not utilize the worker queue" do
-          expect(subject.send(:worker_queue)).not_to receive(:pop)
-          subject.multi_receive(events)
-        end
-
-        it "should send received events to the worker" do
-          expect(out_inst).to receive(:multi_receive).with(events)
-          subject.multi_receive(events)
-        end
+      it "should register the output plugin instance on register" do
+        expect(out_inst).to have_received(:register)
       end
-    end
-  end
 
-  # This may seem suspiciously similar to the class in outputs/base_spec
-  # but, in fact, we need a whole new class because using this even once
-  # will immutably modify the base class
-  class LogStash::Outputs::NOOPDelLegacyNoWorkers < ::LogStash::Outputs::Base
-    LEGACY_WORKERS_NOT_SUPPORTED_REASON = "legacy reason"
-
-    def register
-      workers_not_supported(LEGACY_WORKERS_NOT_SUPPORTED_REASON)
+      it "should close the output plugin instance when closing" do
+        expect(out_inst).to receive(:do_close)
+        subject.do_close
+      end
     end
-  end
 
-  describe "legacy output workers_not_supported" do
-    let(:default_worker_count) { 2 }
-    let(:out_klass) { LogStash::Outputs::NOOPDelLegacyNoWorkers }
-
-    before do
-      allow(logger).to receive(:debug).with(any_args)
-    end
+    describe "concurrency strategies" do
+      it "should have :single as the default" do
+        expect(subject.concurrency).to eq :single
+      end
 
-    it "should only setup one worker" do
-      expect(subject.worker_count).to eql(1)
+      [
+        [:shared, ::LogStash::OutputDelegatorStrategies::Shared],
+        [:single, ::LogStash::OutputDelegatorStrategies::Single],
+        [:legacy, ::LogStash::OutputDelegatorStrategies::Legacy],
+      ].each do |strategy_concurrency,klass|
+        context "with strategy #{strategy_concurrency}" do
+          let(:concurrency) { strategy_concurrency }
+
+          it "should find the correct concurrency type for the output" do
+            expect(subject.concurrency).to eq(strategy_concurrency)
+          end
+          
+          it "should find the correct Strategy class for the worker" do
+            expect(subject.strategy).to be_a(klass)
+          end
+
+          it "should set the correct parameters on the instance" do
+            expect(out_klass).to have_received(:new).with(plugin_args)
+          end
+
+          it "should set the metric on the instance" do
+            expect(out_inst).to have_received(:metric=).with(metric)
+          end
+
+          [[:register], [:do_close], [:multi_receive, [[]] ] ].each do |method, args|
+            context "strategy objects" do
+              before do
+                allow(subject.strategy).to receive(method)
+              end
+              
+              it "should delegate #{method} to the strategy" do
+                subject.send(method, *args)
+                if args
+                  expect(subject.strategy).to have_received(method).with(*args)
+                else
+                  expect(subject.strategy).to have_received(method).with(no_args)
+                end
+              end
+            end
+
+            context "strategy output instances" do
+              before do
+                allow(out_inst).to receive(method)
+              end
+              
+              it "should delegate #{method} to the strategy" do
+                subject.send(method, *args)
+                if args
+                  expect(out_inst).to have_received(method).with(*args)
+                else
+                  expect(out_inst).to have_received(method).with(no_args)
+                end
+              end
+            end
+          end
+        end
+      end
     end
   end
 end
diff --git a/logstash-core/spec/logstash/outputs/base_spec.rb b/logstash-core/spec/logstash/outputs/base_spec.rb
index 44d49a60b99..fe03883fae6 100644
--- a/logstash-core/spec/logstash/outputs/base_spec.rb
+++ b/logstash-core/spec/logstash/outputs/base_spec.rb
@@ -2,9 +2,9 @@
 require "spec_helper"
 
 # use a dummy NOOP output to test Outputs::Base
-class LogStash::Outputs::NOOP < LogStash::Outputs::Base
+class LogStash::Outputs::NOOPSingle < LogStash::Outputs::Base
   config_name "noop"
-  milestone 2
+  concurrency :single
 
   config :dummy_option, :validate => :string
 
@@ -15,26 +15,101 @@ def receive(event)
   end
 end
 
-class LogStash::Outputs::NOOPLegacyNoWorkers < ::LogStash::Outputs::Base
-  LEGACY_WORKERS_NOT_SUPPORTED_REASON = "legacy reason"
+class LogStash::Outputs::NOOPShared < ::LogStash::Outputs::Base
+  concurrency :shared
+  
+  def register; end
+end
+
+class LogStash::Outputs::NOOPLegacy < ::LogStash::Outputs::Base
+  def register; end
+end
 
-  def register
-    workers_not_supported(LEGACY_WORKERS_NOT_SUPPORTED_REASON)
+class LogStash::Outputs::NOOPMultiReceiveEncoded < ::LogStash::Outputs::Base
+  concurrency :single
+  
+  def register; end
+
+  def multi_receive_encoded(events_and_encoded)
   end
 end
 
 describe "LogStash::Outputs::Base#new" do
-  it "should instantiate cleanly" do
-    params = { "dummy_option" => "potatoes", "codec" => "json", "workers" => 2 }
-    worker_params = params.dup; worker_params["workers"] = 1
+  let(:params) { {} }  
+  subject(:instance) { klass.new(params.dup) }
+
+  context "single" do
+    let(:klass) { LogStash::Outputs::NOOPSingle }
+    
+    it "should instantiate cleanly" do
+      params = { "dummy_option" => "potatoes", "codec" => "json", "workers" => 2 }
+      worker_params = params.dup; worker_params["workers"] = 1
 
-    expect do
-      LogStash::Outputs::NOOP.new(params.dup)
-    end.not_to raise_error
+      expect{ subject }.not_to raise_error
+    end
+
+    it "should set concurrency correctly" do
+      expect(subject.concurrency).to eq(:single)
+    end
   end
 
-  it "should move workers_not_supported declarations up to the class level" do
-    LogStash::Outputs::NOOPLegacyNoWorkers.new.register
-    expect(LogStash::Outputs::NOOPLegacyNoWorkers.workers_not_supported?).to eql(true)
+  context "shared" do
+    let(:klass) { LogStash::Outputs::NOOPShared }
+    
+    it "should set concurrency correctly" do
+      expect(subject.concurrency).to eq(:shared)
+    end
+  end
+
+  context "legacy" do
+    let(:klass) { LogStash::Outputs::NOOPLegacy }
+    
+    it "should set concurrency correctly" do
+      expect(subject.concurrency).to eq(:legacy)
+    end
+
+    it "should default the # of workers to 1" do
+      expect(subject.workers).to eq(1)
+    end
+
+    it "should default concurrency to :legacy" do
+      expect(subject.concurrency).to eq(:legacy)
+    end
+  end
+
+  describe "dispatching multi_receive" do
+    let(:event) { double("event") }
+    let(:events) { [event] }
+    
+    context "with multi_receive_encoded" do
+      let(:klass) { LogStash::Outputs::NOOPMultiReceiveEncoded }
+      let(:codec) { double("codec") }
+      let(:encoded) { double("encoded") }
+      
+      before do
+        allow(codec).to receive(:multi_encode).with(events).and_return(encoded)
+        allow(instance).to receive(:codec).and_return(codec)
+        allow(instance).to receive(:multi_receive_encoded)        
+        instance.multi_receive(events)
+      end
+
+      it "should invoke multi_receive_encoded if it exists" do
+        expect(instance).to have_received(:multi_receive_encoded).with(encoded)
+      end
+    end
+
+    context "with plain #receive" do
+      let(:klass) { LogStash::Outputs::NOOPSingle }
+
+      before do
+        allow(instance).to receive(:multi_receive).and_call_original
+        allow(instance).to receive(:receive).with(event)
+        instance.multi_receive(events)
+      end
+
+      it "should receive the event by itself" do
+        expect(instance).to have_received(:receive).with(event)
+      end
+    end
   end
 end
diff --git a/logstash-core/spec/logstash/pipeline_reporter_spec.rb b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
index bdd83d4ff24..40e821d6941 100644
--- a/logstash-core/spec/logstash/pipeline_reporter_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_reporter_spec.rb
@@ -4,6 +4,7 @@
 require "logstash/pipeline_reporter"
 
 class DummyOutput < LogStash::Outputs::Base
+
   config_name "dummyoutput"
   milestone 2
 
@@ -43,10 +44,15 @@ def close
     allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
 
     @pre_snapshot = reporter.snapshot
+    
     pipeline.run
     @post_snapshot = reporter.snapshot
   end
 
+  after do
+    pipeline.shutdown
+  end
+
   describe "events filtered" do
     it "should start at zero" do
       expect(@pre_snapshot.events_filtered).to eql(0)
@@ -76,10 +82,4 @@ def close
       expect(@post_snapshot.inflight_count).to eql(0)
     end
   end
-
-  describe "output states" do
-    it "should include the count of received events" do
-      expect(@post_snapshot.output_info.first[:events_received]).to eql(generator_count)
-    end
-  end
-end
\ No newline at end of file
+end
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 3753ccaae00..197f0631bd8 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -2,6 +2,7 @@
 require "spec_helper"
 require "logstash/inputs/generator"
 require "logstash/filters/multiline"
+require_relative "../support/mocks_classes"
 
 class DummyInput < LogStash::Inputs::Base
   config_name "dummyinput"
@@ -48,30 +49,6 @@ def close
   end
 end
 
-class DummyOutput < LogStash::Outputs::Base
-  config_name "dummyoutput"
-  milestone 2
-
-  attr_reader :num_closes, :events
-
-  def initialize(params={})
-    super
-    @num_closes = 0
-    @events = []
-  end
-
-  def register
-  end
-
-  def receive(event)
-    @events << event
-  end
-
-  def close
-    @num_closes = 1
-  end
-end
-
 class DummyOutputMore < DummyOutput
   config_name "dummyoutputmore"
 end
@@ -103,13 +80,79 @@ def close() end
 end
 
 class TestPipeline < LogStash::Pipeline
-  attr_reader :outputs, :settings, :logger
+  attr_reader :outputs, :settings
 end
 
 describe LogStash::Pipeline do
-  let(:worker_thread_count)     { LogStash::Pipeline::DEFAULT_SETTINGS[:default_pipeline_workers] }
+  let(:worker_thread_count)     { 5 }
   let(:safe_thread_count)       { 1 }
   let(:override_thread_count)   { 42 }
+  let(:pipeline_settings_obj) { LogStash::SETTINGS }
+  let(:pipeline_settings) { {} }
+
+  before :each do
+    pipeline_workers_setting = LogStash::SETTINGS.get_setting("pipeline.workers")
+    allow(pipeline_workers_setting).to receive(:default).and_return(worker_thread_count)
+    pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+  end
+
+  after :each do
+    pipeline_settings_obj.reset
+  end
+
+
+  describe "event cancellation" do
+    # test harness for https://github.com/elastic/logstash/issues/6055
+
+    let(:output) { DummyOutputWithEventsArray.new }
+
+    before do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputwitheventsarray").and_return(DummyOutputWithEventsArray)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "drop").and_call_original
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "mutate").and_call_original
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_call_original
+      allow(DummyOutputWithEventsArray).to receive(:new).with(any_args).and_return(output)
+    end
+
+    let(:config) do
+      <<-CONFIG
+        input {
+          generator {
+            lines => ["1", "2", "END"]
+            count => 1
+          }
+        }
+        filter {
+          if [message] == "1" {
+            drop {}
+          }
+          mutate { add_tag => ["notdropped"] }
+        }
+        output { dummyoutputwitheventsarray {} }
+      CONFIG
+    end
+
+    it "should not propage cancelled events from filter to output" do
+      abort_on_exception_state = Thread.abort_on_exception
+      Thread.abort_on_exception = true
+
+      pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
+      Thread.new { pipeline.run }
+      sleep 0.1 while !pipeline.ready?
+      wait(3).for do
+        # give us a bit of time to flush the events
+        # puts("*****" + output.events.map{|e| e.message}.to_s)
+        output.events.map{|e| e.get("message")}.include?("END")
+      end.to be_truthy
+      expect(output.events.size).to eq(2)
+      expect(output.events[0].get("tags")).to eq(["notdropped"])
+      expect(output.events[1].get("tags")).to eq(["notdropped"])
+      pipeline.shutdown
+
+      Thread.abort_on_exception = abort_on_exception_state
+    end
+  end
 
   describe "defaulting the pipeline workers based on thread safety" do
     before(:each) do
@@ -137,6 +180,27 @@ class TestPipeline < LogStash::Pipeline
         eos
       }
 
+      describe "debug compiled" do
+        let(:logger) { double("pipeline logger").as_null_object }
+
+        before do
+          expect(TestPipeline).to receive(:logger).and_return(logger)
+          allow(logger).to receive(:debug?).and_return(true)
+        end
+
+        it "should not receive a debug message with the compiled code" do
+          pipeline_settings_obj.set("config.debug", false)
+          expect(logger).not_to receive(:debug).with(/Compiled pipeline/, anything)
+          pipeline = TestPipeline.new(test_config_with_filters)
+        end
+
+        it "should print the compiled code if config.debug is set to true" do
+          pipeline_settings_obj.set("config.debug", true)
+          expect(logger).to receive(:debug).with(/Compiled pipeline/, anything)
+          pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
+        end
+      end
+
       context "when there is no command line -w N set" do
         it "starts one filter thread" do
           msg = "Defaulting pipeline worker threads to 1 because there are some filters that might not work with multiple worker threads"
@@ -145,18 +209,21 @@ class TestPipeline < LogStash::Pipeline
             {:count_was=>worker_thread_count, :filters=>["dummyfilter"]})
           pipeline.run
           expect(pipeline.worker_threads.size).to eq(safe_thread_count)
+          pipeline.shutdown
         end
       end
 
       context "when there is command line -w N set" do
+        let(:pipeline_settings) { {"pipeline.workers" => override_thread_count } }
         it "starts multiple filter thread" do
-          msg = "Warning: Manual override - there are filters that might not work with multiple worker threads"
-          pipeline = TestPipeline.new(test_config_with_filters)
+          msg = "Warning: Manual override - there are filters that might" +
+                " not work with multiple worker threads"
+          pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
           expect(pipeline.logger).to receive(:warn).with(msg,
             {:worker_threads=> override_thread_count, :filters=>["dummyfilter"]})
-          pipeline.configure(:pipeline_workers, override_thread_count)
           pipeline.run
           expect(pipeline.worker_threads.size).to eq(override_thread_count)
+          pipeline.shutdown
         end
       end
     end
@@ -182,6 +249,7 @@ class TestPipeline < LogStash::Pipeline
         pipeline = TestPipeline.new(test_config_with_filters)
         pipeline.run
         expect(pipeline.worker_threads.size).to eq(worker_thread_count)
+        pipeline.shutdown
       end
     end
   end
@@ -221,29 +289,21 @@ class TestPipeline < LogStash::Pipeline
     }
 
     context "output close" do
-      it "should call close of output without output-workers" do
-        pipeline = TestPipeline.new(test_config_without_output_workers)
-        pipeline.run
+      let(:pipeline) { TestPipeline.new(test_config_without_output_workers) }
+      let(:output) { pipeline.outputs.first }
 
-        expect(pipeline.outputs.size ).to eq(1)
-        expect(pipeline.outputs.first.workers.size ).to eq(pipeline.default_output_workers)
-        expect(pipeline.outputs.first.workers.first.num_closes ).to eq(1)
+      before do
+        allow(output).to receive(:do_close)
       end
 
-      it "should call output close correctly with output workers" do
-        pipeline = TestPipeline.new(test_config_with_output_workers)
+      after do
+        pipeline.shutdown
+      end
+      
+      it "should call close of output without output-workers" do
         pipeline.run
 
-        expect(pipeline.outputs.size ).to eq(1)
-        # We even close the parent output worker, even though it doesn't receive messages
-
-        output_delegator = pipeline.outputs.first
-        output = output_delegator.workers.first
-
-        expect(output.num_closes).to eq(1)
-        output_delegator.workers.each do |plugin|
-          expect(plugin.num_closes ).to eq(1)
-        end
+        expect(output).to have_received(:do_close).once
       end
     end
   end
@@ -265,6 +325,7 @@ class TestPipeline < LogStash::Pipeline
         expect(pipeline).to receive(:start_flusher).ordered.and_call_original
 
         pipeline.run
+        pipeline.shutdown
       end
     end
 
@@ -283,7 +344,7 @@ class TestPipeline < LogStash::Pipeline
       CONFIG
 
       sample("hello") do
-        expect(subject["message"]).to eq("hello")
+        expect(subject.get("message")).to eq("hello")
       end
     end
 
@@ -303,10 +364,10 @@ class TestPipeline < LogStash::Pipeline
       sample(["foo", "bar"]) do
         expect(subject.size).to eq(2)
 
-        expect(subject[0]["message"]).to eq("foo\nbar")
-        expect(subject[0]["type"]).to be_nil
-        expect(subject[1]["message"]).to eq("foo\nbar")
-        expect(subject[1]["type"]).to eq("clone1")
+        expect(subject[0].get("message")).to eq("foo\nbar")
+        expect(subject[0].get("type")).to be_nil
+        expect(subject[1].get("message")).to eq("foo\nbar")
+        expect(subject[1].get("type")).to eq("clone1")
       end
     end
   end
@@ -314,7 +375,8 @@ class TestPipeline < LogStash::Pipeline
   describe "max inflight warning" do
     let(:config) { "input { dummyinput {} } output { dummyoutput {} }" }
     let(:batch_size) { 1 }
-    let(:pipeline) { LogStash::Pipeline.new(config, :pipeline_batch_size => batch_size, :pipeline_workers => 1) }
+    let(:pipeline_settings) { { "pipeline.batch.size" => batch_size, "pipeline.workers" => 1 } }
+    let(:pipeline) { LogStash::Pipeline.new(config, pipeline_settings_obj) }
     let(:logger) { pipeline.logger }
     let(:warning_prefix) { /CAUTION: Recommended inflight events max exceeded!/ }
 
@@ -342,7 +404,6 @@ class TestPipeline < LogStash::Pipeline
   end
 
   context "compiled filter funtions" do
-
     context "new events should propagate down the filters" do
       config <<-CONFIG
         filter {
@@ -358,17 +419,17 @@ class TestPipeline < LogStash::Pipeline
       sample("hello") do
         expect(subject.size).to eq(3)
 
-        expect(subject[0]["message"]).to eq("hello")
-        expect(subject[0]["type"]).to be_nil
-        expect(subject[0]["foo"]).to eq("bar")
+        expect(subject[0].get("message")).to eq("hello")
+        expect(subject[0].get("type")).to be_nil
+        expect(subject[0].get("foo")).to eq("bar")
 
-        expect(subject[1]["message"]).to eq("hello")
-        expect(subject[1]["type"]).to eq("clone1")
-        expect(subject[1]["foo"]).to eq("bar")
+        expect(subject[1].get("message")).to eq("hello")
+        expect(subject[1].get("type")).to eq("clone1")
+        expect(subject[1].get("foo")).to eq("bar")
 
-        expect(subject[2]["message"]).to eq("hello")
-        expect(subject[2]["type"]).to eq("clone2")
-        expect(subject[2]["foo"]).to eq("bar")
+        expect(subject[2].get("message")).to eq("hello")
+        expect(subject[2].get("type")).to eq("clone2")
+        expect(subject[2].get("foo")).to eq("bar")
       end
     end
   end
@@ -380,8 +441,14 @@ class TestPipeline < LogStash::Pipeline
     output { }
     CONFIG
 
-    it "uses a `NullMetric` object if no metric is given" do
-      pipeline = LogStash::Pipeline.new(config)
+    it "uses a `NullMetric` object if `metric.collect` is set to false" do
+      settings = double("LogStash::SETTINGS")
+
+      allow(settings).to receive(:get_value).with("pipeline.id").and_return("main")
+      allow(settings).to receive(:get_value).with("metric.collect").and_return(false)
+      allow(settings).to receive(:get_value).with("config.debug").and_return(false)
+
+      pipeline = LogStash::Pipeline.new(config, settings)
       expect(pipeline.metric).to be_kind_of(LogStash::Instrument::NullMetric)
     end
   end
@@ -439,14 +506,15 @@ class TestPipeline < LogStash::Pipeline
 
     it "flushes the buffered contents of the filter" do
       Thread.abort_on_exception = true
-      pipeline = LogStash::Pipeline.new(config, { :flush_interval => 1 })
+      pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
       Thread.new { pipeline.run }
       sleep 0.1 while !pipeline.ready?
-      # give us a bit of time to flush the events
       wait(5).for do
-        next unless output && output.events && output.events.first
-        output.events.first["message"].split("\n").count
-      end.to eq(number_of_events)
+        # give us a bit of time to flush the events
+        output.events.empty?
+      end.to be_falsey
+      event = output.events.pop
+      expect(event.get("message").count("\n")).to eq(99)
       pipeline.shutdown
     end
   end
@@ -499,7 +567,7 @@ class TestPipeline < LogStash::Pipeline
       t = Thread.new { subject.run }
       sleep(0.1)
       expect(subject.started_at).to be < Time.now
-      t.kill rescue nil
+      subject.shutdown
     end
   end
 
@@ -524,15 +592,18 @@ class TestPipeline < LogStash::Pipeline
         t = Thread.new { subject.run }
         sleep(0.1)
         expect(subject.uptime).to be > 0
-        t.kill rescue nil
+        subject.shutdown
       end
     end
   end
 
   context "when collecting metrics in the pipeline" do
-    subject { described_class.new(config, { :metric => metric, :pipeline_id => pipeline_id }) }
-    let(:pipeline_id) { :main }
-    let(:metric) { LogStash::Instrument::Metric.new }
+    let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new) }
+
+    subject { described_class.new(config, pipeline_settings_obj, metric) }
+
+    let(:pipeline_settings) { { "pipeline.id" => pipeline_id } }
+    let(:pipeline_id) { "main" }
     let(:number_of_events) { 1000 }
     let(:multiline_id) { "my-multiline" }
     let(:multiline_id_other) { "my-multiline_other" }
@@ -566,6 +637,7 @@ class TestPipeline < LogStash::Pipeline
       EOS
     end
     let(:dummyoutput) { DummyOutput.new({ "id" => dummy_output_id }) }
+    let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
 
     before :each do
       allow(DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
@@ -574,12 +646,15 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
 
-      # Reset the metric store
-      LogStash::Instrument::Collector.instance.clear
-
       Thread.new { subject.run }
       # make sure we have received all the generated events
-      sleep 1 while dummyoutput.events.size < number_of_events
+
+      times = 0
+      while dummyoutput.events.size < number_of_events
+        times += 1
+        sleep 0.25
+        raise "Waited too long" if times > 4
+      end
     end
 
     after :each do
@@ -587,9 +662,10 @@ class TestPipeline < LogStash::Pipeline
     end
 
     context "global metric" do
-      let(:collected_metric) { LogStash::Instrument::Collector.instance.snapshot_metric.metric_store.get_with_path("stats/events") }
+      let(:collected_metric) { metric_store.get_with_path("stats/events") }
 
-      it "populates the differents" do
+      it "populates the different metrics" do
+        expect(collected_metric[:stats][:events][:duration_in_millis].value).not_to be_nil
         expect(collected_metric[:stats][:events][:in].value).to eq(number_of_events)
         expect(collected_metric[:stats][:events][:filtered].value).to eq(number_of_events)
         expect(collected_metric[:stats][:events][:out].value).to eq(number_of_events)
@@ -597,9 +673,10 @@ class TestPipeline < LogStash::Pipeline
     end
 
     context "pipelines" do
-      let(:collected_metric) { LogStash::Instrument::Collector.instance.snapshot_metric.metric_store.get_with_path("stats/pipelines/") }
+      let(:collected_metric) { metric_store.get_with_path("stats/pipelines/") }
 
       it "populates the pipelines core metrics" do
+        expect(collected_metric[:stats][:pipelines][:main][:events][:duration_in_millis].value).not_to be_nil
         expect(collected_metric[:stats][:pipelines][:main][:events][:in].value).to eq(number_of_events)
         expect(collected_metric[:stats][:pipelines][:main][:events][:filtered].value).to eq(number_of_events)
         expect(collected_metric[:stats][:pipelines][:main][:events][:out].value).to eq(number_of_events)
@@ -615,9 +692,37 @@ class TestPipeline < LogStash::Pipeline
       end
 
       it "populates the output metrics" do
-        plugin_name = "dummyoutput_#{dummy_output_id}".to_sym
+        plugin_name = dummy_output_id.to_sym
         expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:events][:out].value).to eq(number_of_events)
       end
+
+      it "populates the name of the output plugin" do
+        plugin_name = dummy_output_id.to_sym
+        expect(collected_metric[:stats][:pipelines][:main][:plugins][:outputs][plugin_name][:name].value).to eq(DummyOutput.config_name)
+      end
+
+      it "populates the name of the filter plugin" do
+        [multiline_id, multiline_id_other].map(&:to_sym).each do |id|
+          plugin_name = "multiline_#{id}".to_sym
+          expect(collected_metric[:stats][:pipelines][:main][:plugins][:filters][plugin_name][:name].value).to eq(LogStash::Filters::Multiline.config_name)
+        end
+      end
+    end
+  end
+
+  context "Pipeline object" do
+    before do
+      allow(LogStash::Plugin).to receive(:lookup).with("input", "generator").and_return(LogStash::Inputs::Generator)
+      allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
+      allow(LogStash::Plugin).to receive(:lookup).with("filter", "dummyfilter").and_return(DummyFilter)
+      allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(DummyOutput)
+    end
+
+    let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+    let(:pipeline2) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
+
+    it "should not add ivars" do
+       expect(pipeline1.instance_variables).to eq(pipeline2.instance_variables)
     end
   end
 end
diff --git a/logstash-core/spec/logstash/plugin_spec.rb b/logstash-core/spec/logstash/plugin_spec.rb
index 3950fbcc6e3..6ef3398b879 100644
--- a/logstash-core/spec/logstash/plugin_spec.rb
+++ b/logstash-core/spec/logstash/plugin_spec.rb
@@ -8,12 +8,12 @@
 
 describe LogStash::Plugin do
   it "should fail lookup on inexisting type" do
-    expect_any_instance_of(Cabin::Channel).to receive(:debug).once
+    #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
     expect { LogStash::Plugin.lookup("badbadtype", "badname") }.to raise_error(LogStash::PluginLoadingError)
   end
 
   it "should fail lookup on inexisting name" do
-    expect_any_instance_of(Cabin::Channel).to receive(:debug).once
+    #expect_any_instance_of(Cabin::Channel).to receive(:debug).once
     expect { LogStash::Plugin.lookup("filter", "badname") }.to raise_error(LogStash::PluginLoadingError)
   end
 
@@ -34,6 +34,19 @@ class LogStash::Filters::LadyGaga < LogStash::Filters::Base
     expect(LogStash::Plugin.lookup("filter", "lady_gaga")).to eq(LogStash::Filters::LadyGaga)
   end
 
+  describe "plugin signup in the registry" do
+
+    let(:registry) { LogStash::Registry.instance }
+
+    it "should be present in the registry" do
+      class LogStash::Filters::MyPlugin < LogStash::Filters::Base
+        config_name "my_plugin"
+      end
+      path     = "logstash/filters/my_plugin"
+      expect(registry.registered?(path)).to eq(true)
+    end
+  end
+
   describe "#inspect" do
     class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
       config_name "param1"
@@ -63,7 +76,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('1.0.0')))
 
-      expect_any_instance_of(Cabin::Channel).not_to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).not_to receive(:info)
       subject.validate({})
     end
 
@@ -72,7 +85,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('0.9.1')))
 
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:info)
         .with(/Using version 0.9.x/)
 
       subject.validate({})
@@ -83,7 +96,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('0.1.1')))
 
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:info)
         .with(/Using version 0.1.x/)
       subject.validate({})
     end
@@ -97,7 +110,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
         .with(plugin_name)
         .and_return(double(:version => Gem::Version.new('0.1.1')))
 
-      expect_any_instance_of(Cabin::Channel).to receive(:info)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:info)
         .once
         .with(/Using version 0.1.x/)
 
@@ -106,7 +119,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
     end
 
     it "warns the user if we can't find a defined version" do
-      expect_any_instance_of(Cabin::Channel).to receive(:warn)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:warn)
         .once
         .with(/plugin doesn't have a version/)
 
@@ -115,7 +128,7 @@ class LogStash::Filters::MyTestFilter < LogStash::Filters::Base
 
 
     it 'logs a warning if the plugin use the milestone option' do
-      expect_any_instance_of(Cabin::Channel).to receive(:debug)
+      expect_any_instance_of(LogStash::Logging::Logger).to receive(:debug)
         .with(/stromae plugin is using the 'milestone' method/)
 
       class LogStash::Filters::Stromae < LogStash::Filters::Base
@@ -223,7 +236,7 @@ def register; end
     end
   end
 
-  describe "#plugin_unique_name" do
+  describe "#id" do
     let(:plugin) do
       Class.new(LogStash::Filters::Base,) do
         config_name "simple_plugin"
@@ -245,7 +258,7 @@ def register; end
       subject { plugin.new(config) }
 
       it "return a human readable ID" do
-        expect(subject.plugin_unique_name).to eq("simple_plugin_#{my_id}")
+        expect(subject.id).to eq(my_id)
       end
     end
 
@@ -253,7 +266,81 @@ def register; end
       subject { plugin.new(config) }
 
       it "return a human readable ID" do
-        expect(subject.plugin_unique_name).to match(/^simple_plugin_/)
+        expect(subject.id).to match(/^simple_plugin_/)
+      end
+    end
+  end
+
+
+  context "When the plugin record a metric" do
+    let(:config) { {} }
+
+    [LogStash::Inputs::Base, LogStash::Filters::Base, LogStash::Outputs::Base].each do |base|
+      let(:plugin) do
+        Class.new(base) do
+          #include LogStash::Util::Loggable
+          config_name "testing"
+
+          def register
+            metric.gauge("power_level", 9000)
+          end
+        end
+      end
+
+      subject { plugin.new(config) } 
+
+      context "when no metric is set to the plugin" do
+        context "when `enable_metric` is TRUE" do
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use a `NullMetric`" do
+            expect(subject.metric).to be_kind_of(LogStash::Instrument::NamespacedNullMetric)
+          end
+        end
+
+        context "when `enable_metric` is FALSE" do
+          let(:config) { { "enable_metric" => false } }
+
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use a `NullMetric`" do
+            expect(subject.metric).to be_kind_of(LogStash::Instrument::NamespacedNullMetric)
+          end
+        end
+      end
+
+      context "When a specific metric collector is configured" do
+        context "when `enable_metric` is TRUE" do
+          let(:metric) { LogStash::Instrument::Metric.new(LogStash::Instrument::Collector.new).namespace("dbz") }
+
+          before :each do
+            subject.metric = metric
+          end
+
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use the configured metric" do
+            expect(subject.metric).to eq(metric)
+          end
+        end
+
+        context "when `enable_metric` is FALSE" do
+          let(:config) { { "enable_metric" => false } }
+
+          it "recording metric should not raise an exception" do
+            expect { subject.register }.not_to raise_error
+          end
+
+          it "should use a `NullMetric`" do
+            expect(subject.metric).to be_kind_of(LogStash::Instrument::NamespacedNullMetric)
+          end
+        end
       end
     end
   end
diff --git a/logstash-core/spec/logstash/plugins/registry_spec.rb b/logstash-core/spec/logstash/plugins/registry_spec.rb
new file mode 100644
index 00000000000..f109f68653a
--- /dev/null
+++ b/logstash-core/spec/logstash/plugins/registry_spec.rb
@@ -0,0 +1,57 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/plugins/registry"
+require "logstash/inputs/base"
+
+# use a dummy NOOP input to test plugin registry
+class LogStash::Inputs::Dummy < LogStash::Inputs::Base
+  config_name "dummy"
+
+  def register; end
+
+end
+
+describe LogStash::Registry do
+
+  let(:registry) { described_class.instance }
+
+  context "when loading installed plugins" do
+
+    let(:plugin) { double("plugin") }
+
+    it "should return the expected class" do
+      klass = registry.lookup("input", "stdin")
+      expect(klass).to eq(LogStash::Inputs::Stdin)
+    end
+
+    it "should raise an error if can not find the plugin class" do
+      expect(LogStash::Registry::Plugin).to receive(:new).with("input", "elastic").and_return(plugin)
+      expect(plugin).to receive(:path).and_return("logstash/input/elastic").twice
+      expect(plugin).to receive(:installed?).and_return(true)
+      expect { registry.lookup("input", "elastic") }.to raise_error(LoadError)
+    end
+
+    it "should load from registry is already load" do
+      registry.lookup("input", "stdin")
+      expect(registry).to receive(:registered?).and_return(true).once
+      registry.lookup("input", "stdin")
+      internal_registry = registry.instance_variable_get("@registry")
+      expect(internal_registry).to include("logstash/inputs/stdin" => LogStash::Inputs::Stdin)
+    end
+  end
+
+  context "when loading code defined plugins" do
+    it "should return the expected class" do
+      klass = registry.lookup("input", "dummy")
+      expect(klass).to eq(LogStash::Inputs::Dummy)
+    end
+  end
+
+  context "when plugin is not installed and not defined" do
+    it "should raise an error" do
+      expect { registry.lookup("input", "elastic") }.to raise_error(LoadError)
+    end
+  end
+
+end
+
diff --git a/logstash-core/spec/logstash/runner_spec.rb b/logstash-core/spec/logstash/runner_spec.rb
index f8bcd9a6f35..00ff679e6d8 100644
--- a/logstash-core/spec/logstash/runner_spec.rb
+++ b/logstash-core/spec/logstash/runner_spec.rb
@@ -3,7 +3,10 @@
 require "logstash/runner"
 require "stud/task"
 require "stud/trap"
+require "stud/temporary"
 require "logstash/util/java_version"
+require "logstash/logging/json"
+require "json"
 
 class NullRunner
   def run(args); end
@@ -12,22 +15,61 @@ def run(args); end
 describe LogStash::Runner do
 
   subject { LogStash::Runner }
-  let(:channel) { Cabin::Channel.new }
+  let(:logger) { double("logger") }
 
   before :each do
-    allow(Cabin::Channel).to receive(:get).with(LogStash).and_return(channel)
+    allow(LogStash::Runner).to receive(:logger).and_return(logger)
+    allow(logger).to receive(:debug?).and_return(true)
+    allow(logger).to receive(:subscribe).with(any_args)
+    allow(logger).to receive(:debug) {}
+    allow(logger).to receive(:log) {}
+    allow(logger).to receive(:info) {}
+    allow(logger).to receive(:fatal) {}
+    allow(logger).to receive(:warn) {}
+    allow(LogStash::ShutdownWatcher).to receive(:logger).and_return(logger)
+    allow(LogStash::Logging::Logger).to receive(:configure_logging) do |level, path|
+      allow(logger).to receive(:level).and_return(level.to_sym)
+    end
+  end
+
+  after :each do
+    LogStash::SETTINGS.reset
+  end
+
+  after :all do
+  end
+
+  describe "argument precedence" do
+    let(:config) { "input {} output {}" }
+    let(:cli_args) { ["-e", config, "-w", "20"] }
+    let(:settings_yml_hash) { { "pipeline.workers" => 2 } }
+
+    before :each do
+      allow(LogStash::SETTINGS).to receive(:read_yaml).and_return(settings_yml_hash)
+    end
+
+    after :each do
+      LogStash::SETTINGS.reset
+    end
+
+    it "favors the last occurence of an option" do
+      expect(LogStash::Agent).to receive(:new) do |settings|
+        expect(settings.get("config.string")).to eq(config)
+        expect(settings.get("pipeline.workers")).to eq(20)
+      end
+      subject.run("bin/logstash", cli_args)
+    end
   end
 
   describe "argument parsing" do
     subject { LogStash::Runner.new("") }
+
     context "when -e is given" do
 
       let(:args) { ["-e", "input {} output {}"] }
       let(:agent) { double("agent") }
-      let(:agent_logger) { double("agent logger") }
 
       before do
-        allow(agent).to receive(:logger=).with(anything)
         allow(agent).to receive(:shutdown)
         allow(agent).to receive(:register_pipeline)
       end
@@ -41,10 +83,8 @@ def run(args); end
 
     context "with no arguments" do
       let(:args) { [] }
-      let(:agent) { double("agent") }
 
       before(:each) do
-        allow(LogStash::Agent).to receive(:new).and_return(agent)
         allow(LogStash::Util::JavaVersion).to receive(:warn_on_bad_java_version)
       end
 
@@ -95,6 +135,25 @@ def run(args); end
     end
   end
 
+  describe "--config.test_and_exit" do
+    subject { LogStash::Runner.new("") }
+    let(:args) { ["-t", "-e", pipeline_string] }
+
+    context "with a good configuration" do
+      let(:pipeline_string) { "input { } filter { } output { }" }
+      it "should exit successfuly" do
+        expect(subject.run(args)).to eq(0)
+      end
+    end
+
+    context "with a bad configuration" do
+      let(:pipeline_string) { "rlwekjhrewlqrkjh" }
+      it "should fail by returning a bad exit code" do
+        expect(logger).to receive(:fatal)
+        expect(subject.run(args)).to eq(1)
+      end
+    end
+  end
   describe "pipeline settings" do
     let(:pipeline_string) { "input { stdin {} } output { stdout {} }" }
     let(:main_pipeline_settings) { { :pipeline_id => "main" } }
@@ -107,23 +166,204 @@ def run(args); end
       allow(pipeline).to receive(:shutdown)
     end
 
+    context "when :http.host is defined by the user" do
+      it "should pass the value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.host")).to be(true)
+          expect(settings.get("http.host")).to eq("localhost")
+        end
+
+        args = ["--http.host", "localhost", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    context "when :http.host is not defined by the user" do
+      it "should pass the value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.host")).to be_falsey
+          expect(settings.get("http.host")).to eq("127.0.0.1")
+        end
+
+        args = ["-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    context "when :http.port is defined by the user" do
+      it "should pass a single value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.port")).to be(true)
+          expect(settings.get("http.port")).to eq(10000..10000)
+        end
+
+        args = ["--http.port", "10000", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+
+      it "should pass a range value to the webserver" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.port")).to be(true)
+          expect(settings.get("http.port")).to eq(10000..20000)
+        end
+
+        args = ["--http.port", "10000-20000", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
+    context "when no :http.port is not defined by the user" do
+      it "should use the default settings" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("http.port")).to be_falsey
+          expect(settings.get("http.port")).to eq(9600..9700)
+        end
+
+        args = ["-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+
     context "when :pipeline_workers is not defined by the user" do
       it "should not pass the value to the pipeline" do
-        expect(LogStash::Pipeline).to receive(:new).once.with(pipeline_string, hash_excluding(:pipeline_workers)).and_return(pipeline)
-
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("pipeline.workers")).to be(false)
+        end
         args = ["-e", pipeline_string]
         subject.run("bin/logstash", args)
       end
     end
 
+    context "when :pipeline_workers flag is passed without a value" do
+      it "should raise an error" do
+        args = ["-e", pipeline_string, "-w"]
+        expect { subject.run("bin/logstash", args) }.to raise_error
+      end
+    end
+
     context "when :pipeline_workers is defined by the user" do
       it "should pass the value to the pipeline" do
-        main_pipeline_settings[:pipeline_workers] = 2
-        expect(LogStash::Pipeline).to receive(:new).with(pipeline_string, hash_including(main_pipeline_settings)).and_return(pipeline)
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.set?("pipeline.workers")).to be(true)
+          expect(settings.get("pipeline.workers")).to be(2)
+        end
 
         args = ["-w", "2", "-e", pipeline_string]
         subject.run("bin/logstash", args)
       end
     end
+
+    describe "config.debug" do
+      it "should set 'config.debug' to false by default" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.get("config.debug")).to eq(false)
+        end
+        args = ["--log.level", "debug", "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+
+      it "should allow overriding config.debug" do
+        expect(LogStash::Agent).to receive(:new) do |settings|
+          expect(settings.get("config.debug")).to eq(true)
+        end
+        args = ["--log.level", "debug", "--config.debug",  "-e", pipeline_string]
+        subject.run("bin/logstash", args)
+      end
+    end
+  end
+
+  describe "--log.level" do
+    before :each do
+      allow_any_instance_of(subject).to receive(:show_version)
+    end
+    context "when not set" do
+      it "should set log level to warn" do
+        args = ["--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:info)
+      end
+    end
+    context "when setting to debug" do
+      it "should set log level to debug" do
+        args = ["--log.level", "debug",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:debug)
+      end
+    end
+    context "when setting to verbose" do
+      it "should set log level to info" do
+        args = ["--log.level", "info",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:info)
+      end
+    end
+    context "when setting to quiet" do
+      it "should set log level to error" do
+        args = ["--log.level", "error",  "--version"]
+        subject.run("bin/logstash", args)
+        expect(logger.level).to eq(:error)
+      end
+    end
+
+    context "deprecated flags" do
+      context "when using --quiet" do
+        it "should warn about the deprecated flag" do
+          expect(logger).to receive(:warn).with(/DEPRECATION WARNING/)
+          args = ["--quiet", "--version"]
+          subject.run("bin/logstash", args)
+        end
+
+        it "should still set the log level accordingly" do
+          args = ["--quiet", "--version"]
+          subject.run("bin/logstash", args)
+          expect(logger.level).to eq(:error)
+        end
+      end
+      context "when using --debug" do
+        it "should warn about the deprecated flag" do
+          expect(logger).to receive(:warn).with(/DEPRECATION WARNING/)
+          args = ["--debug", "--version"]
+          subject.run("bin/logstash", args)
+        end
+
+        it "should still set the log level accordingly" do
+          args = ["--debug", "--version"]
+          subject.run("bin/logstash", args)
+          expect(logger.level).to eq(:debug)
+        end
+      end
+      context "when using --verbose" do
+        it "should warn about the deprecated flag" do
+          expect(logger).to receive(:warn).with(/DEPRECATION WARNING/)
+          args = ["--verbose", "--version"]
+          subject.run("bin/logstash", args)
+        end
+
+        it "should still set the log level accordingly" do
+          args = ["--verbose", "--version"]
+          subject.run("bin/logstash", args)
+          expect(logger.level).to eq(:info)
+        end
+      end
+    end
+  end
+
+  describe "path.settings" do
+    subject { LogStash::Runner.new("") }
+    context "if does not exist" do
+      let(:args) { ["--path.settings", "/tmp/a/a/a/a", "-e", "input {} output {}"] }
+
+      it "should terminate logstash" do
+        expect(subject.run(args)).to eq(1)
+      end
+
+      context "but if --help is passed" do
+        let(:args) { ["--path.settings", "/tmp/a/a/a/a", "--help"] }
+
+        it "should show help" do
+          expect { subject.run(args) }.to raise_error(Clamp::HelpWanted)
+        end
+      end
+    end
   end
 end
diff --git a/logstash-core/spec/logstash/setting_spec.rb b/logstash-core/spec/logstash/setting_spec.rb
new file mode 100644
index 00000000000..e16c1ed1353
--- /dev/null
+++ b/logstash-core/spec/logstash/setting_spec.rb
@@ -0,0 +1,152 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting do
+  let(:logger) { double("logger") }
+  describe "#value" do
+    context "when using a default value" do
+      context "when no value is set" do
+        subject { described_class.new("number", Numeric, 1) }
+        it "should return the default value" do
+          expect(subject.value).to eq(1)
+        end
+      end
+
+      context "when a value is set" do
+        subject { described_class.new("number", Numeric, 1) }
+        let(:new_value) { 2 }
+        before :each do
+          subject.set(new_value)
+        end
+        it "should return the set value" do
+          expect(subject.value).to eq(new_value)
+        end
+      end
+    end
+
+    context "when not using a default value" do
+      context "when no value is set" do
+        subject { described_class.new("number", Numeric, nil, false) }
+        it "should return the default value" do
+          expect(subject.value).to eq(nil)
+        end
+      end
+
+      context "when a value is set" do
+        subject { described_class.new("number", Numeric, nil, false) }
+        let(:new_value) { 2 }
+        before :each do
+          subject.set(new_value)
+        end
+        it "should return the set value" do
+          expect(subject.value).to eq(new_value)
+        end
+      end
+    end
+  end
+
+  describe "#set?" do
+    context "when there is not value set" do
+      subject { described_class.new("number", Numeric, 1) }
+      it "should return false" do
+        expect(subject.set?).to be(false)
+      end
+    end
+    context "when there is a value set" do
+      subject { described_class.new("number", Numeric, 1) }
+      before :each do
+        subject.set(2)
+      end
+      it "should return false" do
+        expect(subject.set?).to be(true)
+      end
+    end
+  end
+
+  describe "#set" do
+    subject { described_class.new("number", Numeric, 1) }
+    it "should change the value of a setting" do
+      expect(subject.value).to eq(1)
+      subject.set(4)
+      expect(subject.value).to eq(4)
+    end
+    context "when executed for the first time" do
+      it "should change the result of set?" do
+        expect(subject.set?).to eq(false)
+        subject.set(4)
+        expect(subject.set?).to eq(true)
+      end
+    end
+    context "when the argument's class does not match @klass" do
+      it "should throw an exception" do
+        expect { subject.set("not a number") }.to raise_error
+      end
+    end
+    context "when strict=false" do
+      let(:strict) { false }
+      subject { described_class.new("number", Numeric, 1, strict) }
+      before do
+        expect(subject).not_to receive(:validate)
+      end
+
+      it "should not call #validate" do
+        subject.set(123)
+      end
+    end
+    context "when strict=true" do
+      let(:strict) { true }
+      subject { described_class.new("number", Numeric, 1, strict) }
+      before do
+        expect(subject).to receive(:validate)
+      end
+
+      it "should call #validate" do
+        subject.set(123)
+      end
+    end
+  end
+
+  describe "#reset" do
+    subject { described_class.new("number", Numeric, 1) }
+    context "if value is already set" do
+      before :each do
+        subject.set(2)
+      end
+      it "should reset value to default" do
+        subject.reset
+        expect(subject.value).to eq(1)
+      end
+      it "should reset set? to false" do
+        expect(subject.set?).to eq(true)
+        subject.reset
+        expect(subject.set?).to eq(false)
+      end
+    end
+  end
+
+  describe "validator_proc" do
+    let(:default_value) { "small text" }
+    subject { described_class.new("mytext", String, default_value) {|v| v.size < 20 } }
+    context "when validation fails" do
+      let(:new_value) { "very very very very very big text" }
+      it "should raise an exception" do
+        expect { subject.set(new_value) }.to raise_error
+      end
+      it "should not change the value" do
+        subject.set(new_value) rescue nil
+        expect(subject.value).to eq(default_value)
+      end
+    end
+    context "when validation is successful" do
+      let(:new_value) { "smaller text" }
+      it "should not raise an exception" do
+        expect { subject.set(new_value) }.to_not raise_error
+      end
+      it "should change the value" do
+        subject.set(new_value)
+        expect(subject.value).to eq(new_value)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/integer_spec.rb b/logstash-core/spec/logstash/settings/integer_spec.rb
new file mode 100644
index 00000000000..f7097d96153
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/integer_spec.rb
@@ -0,0 +1,20 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::Integer do
+  subject { described_class.new("a number", nil, false) }
+  describe "#set" do
+    context "when giving a number which is not an integer" do
+      it "should raise an exception" do
+        expect { subject.set(1.1) }.to raise_error(ArgumentError)
+      end
+    end
+    context "when giving a number which is an integer" do
+      it "should set the number" do
+        expect { subject.set(100) }.to_not raise_error
+        expect(subject.value).to eq(100)
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/numeric_spec.rb b/logstash-core/spec/logstash/settings/numeric_spec.rb
new file mode 100644
index 00000000000..cab162fce33
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/numeric_spec.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::Numeric do
+  subject { described_class.new("a number", nil, false) }
+  describe "#set" do
+    context "when giving a string which doesn't represent a string" do
+      it "should raise an exception" do
+        expect { subject.set("not-a-number") }.to raise_error(ArgumentError)
+      end
+    end
+    context "when giving a string which represents a " do
+      context "float" do
+        it "should coerce that string to the number" do
+          subject.set("1.1")
+          expect(subject.value).to eq(1.1)
+        end
+      end
+      context "int" do
+        it "should coerce that string to the number" do
+          subject.set("1")
+          expect(subject.value).to eq(1)
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/port_range_spec.rb b/logstash-core/spec/logstash/settings/port_range_spec.rb
new file mode 100644
index 00000000000..05afd21edbb
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/port_range_spec.rb
@@ -0,0 +1,93 @@
+# encoding: utf-8
+#
+require "logstash/settings"
+require "spec_helper"
+
+describe LogStash::Setting::PortRange do
+
+  context "When the value is a Fixnum" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", 9000) }
+
+    it "coerces the value in a range" do
+      expect { subject }.not_to raise_error
+    end
+
+    it "returns a range" do
+      expect(subject.value).to eq(9000..9000)
+    end
+
+    it "can update the range" do
+      subject.set(10000)
+      expect(subject.value).to eq(10000..10000)
+    end
+  end
+
+  context "When the value is a string" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", "9000-10000") }
+
+    it "coerces a string range with the format (9000-10000)" do
+      expect { subject }.not_to raise_error
+    end
+
+    it "refuses when then upper port is out of range" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", "1000-95000") }.to raise_error
+    end
+
+    it "returns a range" do
+      expect(subject.value).to eq(9000..10000)
+    end
+
+    it "can update the range" do
+      subject.set("500-1000")
+      expect(subject.value).to eq(500..1000)
+    end
+  end
+
+  context "when the value is a garbage string" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", "fsdfnsdkjnfjs") }
+
+    it "raises an argument error" do
+      expect { subject }.to raise_error
+    end
+
+
+    it "raises an exception on update" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", 10000).set("dsfnsdknfksdnfjksdnfjns") }.to raise_error
+    end
+  end
+
+  context "when the value is an unkown type" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", 0.1) }
+
+
+    it "raises an argument error" do
+      expect { subject }.to raise_error
+    end
+
+
+    it "raises an exception on update" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", 10000).set(0.1) }.to raise_error
+    end
+  end
+
+  context "When value is a range" do
+    subject { LogStash::Setting::PortRange.new("mynewtest", 9000..10000) }
+
+    it "accepts a ruby range as the default value" do
+      expect { subject }.not_to raise_error
+    end
+
+    it "can update the range" do
+      subject.set(500..1000)
+      expect(subject.value).to eq(500..1000)
+    end
+
+    it "refuses when then upper port is out of range" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", 9000..1000000) }.to raise_error
+    end
+
+    it "raise an exception on when port are out of range" do
+      expect { LogStash::Setting::PortRange.new("mynewtest", -1000..1000) }.to raise_error
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/string_spec.rb b/logstash-core/spec/logstash/settings/string_spec.rb
new file mode 100644
index 00000000000..69d835649ee
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/string_spec.rb
@@ -0,0 +1,21 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Setting::String do
+  let(:possible_values) { ["a", "b", "c"] }
+  subject { described_class.new("mytext", possible_values.first, true, possible_values) }
+  describe "#set" do
+    context "when a value is given outside of possible_values" do
+      it "should raise an ArgumentError" do
+        expect { subject.set("d") }.to raise_error(ArgumentError)
+      end
+    end
+    context "when a value is given within possible_values" do
+      it "should set the value" do
+        expect { subject.set("a") }.to_not raise_error
+        expect(subject.value).to eq("a")
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings/writable_directory_spec.rb b/logstash-core/spec/logstash/settings/writable_directory_spec.rb
new file mode 100644
index 00000000000..be46b28a05e
--- /dev/null
+++ b/logstash-core/spec/logstash/settings/writable_directory_spec.rb
@@ -0,0 +1,124 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+require "tmpdir"
+require "socket" # for UNIXSocket
+
+describe LogStash::Setting::WritableDirectory do
+  let(:mode_rx) { 0555 }
+  # linux is 108, Macos is 104, so use a safe value
+  # Stud::Temporary.pathname, will exceed that size without adding anything
+  let(:parent) { File.join(Dir.tmpdir, Time.now.to_f.to_s) }
+  let(:path) { File.join(parent, "fancy") }
+
+  before { Dir.mkdir(parent) }
+  after { Dir.exist?(path) && Dir.unlink(path) rescue nil }
+  after { Dir.unlink(parent) }
+
+  shared_examples "failure" do
+    before { subject.set(path) }
+    it "should fail" do
+      expect { subject.validate_value }.to raise_error
+    end
+  end
+
+  subject do
+    # Create a new WritableDirectory setting with no default value strict
+    # disabled.
+    described_class.new("fancy.path", "", false)
+  end
+
+  describe "#value" do
+    before { subject.set(path) }
+
+    context "when the directory is missing" do
+
+      context "and the parent is writable" do
+        after { 
+          Dir.unlink(path) 
+        }
+        it "creates the directory" do
+          subject.value # need to invoke `#value` to make it do the work.
+          expect(::File.directory?(path)).to be_truthy
+        end
+      end
+
+      context "and the directory cannot be created" do
+        before { File.chmod(mode_rx, parent) }
+        it "should fail" do
+          expect { subject.value }.to raise_error
+        end
+      end
+    end
+  end
+
+  describe "#set and #validate_value" do
+    context "when the directory exists" do
+      before { Dir.mkdir(path) }
+      after { Dir.unlink(path) }
+
+      context "and is writable" do
+        before { subject.set(path) }
+        # assume this spec already created a directory that's writable... fair? :)
+        it "should return true" do
+          expect(subject.validate_value).to be_truthy
+        end
+      end
+
+      context "but is not writable" do
+        before { File.chmod(0, path) }
+        it_behaves_like "failure"
+      end
+    end
+
+    context "when the path exists" do
+      after { File.unlink(path) }
+
+      context "but is a file" do
+        before { File.new(path, "w").close }
+        it_behaves_like "failure"
+      end
+
+      context "but is a socket" do
+        let(:socket) { UNIXServer.new(path) }
+        before { socket } # realize `socket` value
+        after { socket.close }
+        it_behaves_like "failure"
+      end
+      context "but is a symlink" do
+        before { File::symlink("whatever", path) }
+        it_behaves_like "failure"
+      end
+    end
+
+    context "when the directory is missing" do
+      # Create a path with at least one subdirectory we can try to fiddle with permissions
+
+      context "but can be created" do
+        before do
+          # If the path doesn't exist, we want to try creating it, so let's be
+          # extra careful and make sure the path doesn't exist yet.
+          expect(File.directory?(path)).to be_falsey
+          subject.set(path)
+        end
+
+        after do
+          Dir.unlink(path)
+        end
+
+        it "should return true" do
+          expect(subject.validate_value).to be_truthy
+        end
+      end
+
+      context "and cannot be created" do
+        before do
+          # Remove write permission on the parent
+          File.chmod(mode_rx, parent)
+        end
+
+        it_behaves_like "failure"
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/settings_spec.rb b/logstash-core/spec/logstash/settings_spec.rb
new file mode 100644
index 00000000000..138040fa62f
--- /dev/null
+++ b/logstash-core/spec/logstash/settings_spec.rb
@@ -0,0 +1,89 @@
+# encoding: utf-8
+require "spec_helper"
+require "logstash/settings"
+
+describe LogStash::Settings do
+  let(:numeric_setting_name) { "number" }
+  let(:numeric_setting) { LogStash::Setting.new(numeric_setting_name, Numeric, 1) }
+  describe "#register" do
+    context "if setting has already been registered" do
+      before :each do
+        subject.register(numeric_setting)
+      end
+      it "should raise an exception" do
+        expect { subject.register(numeric_setting) }.to raise_error
+      end
+    end
+    context "if setting hasn't been registered" do
+      it "should not raise an exception" do
+        expect { subject.register(numeric_setting) }.to_not raise_error
+      end
+    end
+  end
+  describe "#get_setting" do
+    context "if setting has been registered" do
+      before :each do
+        subject.register(numeric_setting)
+      end
+      it "should return the setting" do
+        expect(subject.get_setting(numeric_setting_name)).to eq(numeric_setting)
+      end
+    end
+    context "if setting hasn't been registered" do
+      it "should raise an exception" do
+        expect { subject.get_setting(numeric_setting_name) }.to raise_error
+      end
+    end
+  end
+  describe "#get_subset" do
+    let(:numeric_setting_1) { LogStash::Setting.new("num.1", Numeric, 1) }
+    let(:numeric_setting_2) { LogStash::Setting.new("num.2", Numeric, 2) }
+    let(:numeric_setting_3) { LogStash::Setting.new("num.3", Numeric, 3) }
+    let(:string_setting_1) { LogStash::Setting.new("string.1", String, "hello") }
+    before :each do
+      subject.register(numeric_setting_1)
+      subject.register(numeric_setting_2)
+      subject.register(numeric_setting_3)
+      subject.register(string_setting_1)
+    end
+
+    it "supports regex" do
+      expect(subject.get_subset(/num/).get_setting("num.3")).to eq(numeric_setting_3)
+      expect { subject.get_subset(/num/).get_setting("string.1") }.to raise_error
+    end
+
+    it "returns a copy of settings" do
+      subset = subject.get_subset(/num/)
+      subset.set("num.2", 1000)
+      expect(subject.get("num.2")).to eq(2)
+      expect(subset.get("num.2")).to eq(1000)
+    end
+  end
+
+  describe "#validate_all" do
+    subject { described_class.new }
+    let(:numeric_setting_name) { "example" }
+    let(:numeric_setting) { LogStash::Setting.new(numeric_setting_name, Numeric, 1, false) }
+
+    before do
+      subject.register(numeric_setting)
+      subject.set_value(numeric_setting_name, value)
+    end
+
+    context "when any setting is invalid" do
+      let(:value) { "some string" }
+
+      it "should fail" do
+        expect { subject.validate_all }.to raise_error
+      end
+    end
+
+    context "when all settings are valid" do
+      let(:value) { 123 }
+
+      it "should succeed" do
+        expect { subject.validate_all }.not_to raise_error
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/logstash/shutdown_watcher_spec.rb b/logstash-core/spec/logstash/shutdown_watcher_spec.rb
index 118e126ea5d..2f934f59b77 100644
--- a/logstash-core/spec/logstash/shutdown_watcher_spec.rb
+++ b/logstash-core/spec/logstash/shutdown_watcher_spec.rb
@@ -3,8 +3,6 @@
 require "logstash/shutdown_watcher"
 
 describe LogStash::ShutdownWatcher do
-  let(:channel) { Cabin::Channel.new }
-
   let(:check_every) { 0.01 }
   let(:check_threshold) { 100 }
   subject { LogStash::ShutdownWatcher.new(pipeline, check_every) }
@@ -14,8 +12,6 @@
   report_count = 0
 
   before :each do
-    LogStash::ShutdownWatcher.logger = channel
-
     allow(pipeline).to receive(:reporter).and_return(reporter)
     allow(pipeline).to receive(:thread).and_return(Thread.current)
     allow(reporter).to receive(:snapshot).and_return(reporter_snapshot)
diff --git a/logstash-core/spec/logstash/util/defaults_printer_spec.rb b/logstash-core/spec/logstash/util/defaults_printer_spec.rb
deleted file mode 100644
index b3f0576a3a9..00000000000
--- a/logstash-core/spec/logstash/util/defaults_printer_spec.rb
+++ /dev/null
@@ -1,50 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/util/defaults_printer"
-
-describe LogStash::Util::DefaultsPrinter do
-  shared_examples "a defaults printer" do
-    it 'the .print method returns a defaults description' do
-      expect(actual_block.call).to eq(expected)
-    end
-  end
-
-  let(:workers)  { 1 }
-  let(:expected) { "Settings: User set pipeline workers: #{workers}" }
-  let(:settings) { {} }
-
-  describe 'class methods API' do
-    let(:actual_block) do
-      -> {described_class.print(settings)}
-    end
-
-    context 'when the settings hash is empty' do
-      let(:expected) { "Settings: " }
-      it_behaves_like "a defaults printer"
-    end
-
-    context 'when the settings hash has content' do
-      let(:worker_queue) { 42 }
-      let(:settings) { {:pipeline_workers => workers} }
-      it_behaves_like "a defaults printer"
-    end
-  end
-
-  describe 'instance method API' do
-    let(:actual_block) do
-      -> {described_class.new(settings).print}
-    end
-
-    context 'when the settings hash is empty' do
-      let(:expected) { "Settings: " }
-      it_behaves_like "a defaults printer"
-    end
-
-    context 'when the settings hash has content' do
-      let(:workers) { 13 }
-      let(:settings) { {:pipeline_workers => workers} }
-
-      it_behaves_like "a defaults printer"
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/util/worker_threads_default_printer_spec.rb b/logstash-core/spec/logstash/util/worker_threads_default_printer_spec.rb
deleted file mode 100644
index 1842b4373ad..00000000000
--- a/logstash-core/spec/logstash/util/worker_threads_default_printer_spec.rb
+++ /dev/null
@@ -1,45 +0,0 @@
-# encoding: utf-8
-require "spec_helper"
-require "logstash/util/worker_threads_default_printer"
-
-describe LogStash::Util::WorkerThreadsDefaultPrinter do
-  let(:settings)  { {} }
-  let(:collector) { [] }
-
-  subject { described_class.new(settings) }
-
-  before { subject.visit(collector) }
-
-  describe "the #visit method" do
-    context 'when the settings hash is empty' do
-      it 'adds nothing to the collector' do
-        subject.visit(collector)
-        expect(collector).to eq([])
-      end
-    end
-
-    context 'when the settings hash has both user and default content' do
-      let(:settings) { {:pipeline_workers => 42, :default_pipeline_workers => 5} }
-
-      it 'adds two strings' do
-        expect(collector).to eq(["User set pipeline workers: 42", "Default pipeline workers: 5"])
-      end
-    end
-
-    context 'when the settings hash has only user content' do
-      let(:settings) { {:pipeline_workers => 42} }
-
-      it 'adds a string with user set pipeline workers' do
-        expect(collector.first).to eq("User set pipeline workers: 42")
-      end
-    end
-
-    context 'when the settings hash has only default content' do
-      let(:settings) { {:default_pipeline_workers => 5} }
-
-      it 'adds a string with default pipeline workers' do
-        expect(collector.first).to eq("Default pipeline workers: 5")
-      end
-    end
-  end
-end
diff --git a/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
index 871952482aa..d7b403ed0b0 100644
--- a/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
+++ b/logstash-core/spec/logstash/util/wrapped_synchronous_queue_spec.rb
@@ -1,28 +1,120 @@
 # encoding: utf-8
 require "spec_helper"
 require "logstash/util/wrapped_synchronous_queue"
+require "logstash/instrument/collector"
 
 describe LogStash::Util::WrappedSynchronousQueue do
- context "#offer" do
-   context "queue is blocked" do
-     it "fails and give feedback" do
-       expect(subject.offer("Bonjour", 2)).to be_falsey
-     end
-   end
-
-   context "queue is not blocked" do
-     before do
-       @consumer = Thread.new { loop { subject.take } }
-       sleep(0.1)
-     end
-
-     after do
-       @consumer.kill
-     end
-     
-     it "inserts successfully" do
-       expect(subject.offer("Bonjour", 20)).to be_truthy
-     end
-   end
- end
+  context "#offer" do
+    context "queue is blocked" do
+      it "fails and give feedback" do
+        expect(subject.offer("Bonjour", 2)).to be_falsey
+      end
+    end
+
+    context "queue is not blocked" do
+      before do
+        @consumer = Thread.new { loop { subject.take } }
+        sleep(0.1)
+      end
+
+      after do
+        @consumer.kill
+      end
+
+      it "inserts successfully" do
+        expect(subject.offer("Bonjour", 20)).to be_truthy
+      end
+    end
+  end
+
+  describe "queue clients" do
+    context "when requesting a write client" do
+      it "returns a client" do
+        expect(subject.write_client).to be_a(LogStash::Util::WrappedSynchronousQueue::WriteClient)
+      end
+    end
+
+    context "when requesting a read client" do
+      it "returns a client" do
+        expect(subject.read_client).to be_a(LogStash::Util::WrappedSynchronousQueue::ReadClient)
+      end
+    end
+
+    class DummyQueue < Array
+      def take() shift(); end
+      def poll(*) shift(); end
+    end
+
+    describe "WriteClient | ReadClient" do
+      let(:queue) { DummyQueue.new }
+      let(:write_client) { LogStash::Util::WrappedSynchronousQueue::WriteClient.new(queue)}
+      let(:read_client)  { LogStash::Util::WrappedSynchronousQueue::ReadClient.new(queue)}
+
+      context "when reading from the queue" do
+        let(:collector) { LogStash::Instrument::Collector.new }
+
+        before do
+          read_client.set_events_metric(LogStash::Instrument::Metric.new(collector).namespace(:events))
+          read_client.set_pipeline_metric(LogStash::Instrument::Metric.new(collector).namespace(:pipeline))
+        end
+
+        context "when the queue is empty" do
+          it "doesnt record the `duration_in_millis`" do
+            batch = read_client.take_batch
+            read_client.close_batch(batch)
+            store = collector.snapshot_metric.metric_store
+            expect(store.size).to eq(0)
+          end
+        end
+
+        context "when we have item in the queue" do
+          it "records the `duration_in_millis`" do
+            batch = write_client.get_new_batch
+            5.times {|i| batch.push("value-#{i}")}
+            write_client.push_batch(batch)
+            read_batch = read_client.take_batch
+            sleep(0.1) # simulate some work?
+            read_client.close_batch(batch)
+            store = collector.snapshot_metric.metric_store
+
+            expect(store.size).to eq(4)
+            expect(store.get_shallow(:events, :in).value).to eq(5)
+            expect(store.get_shallow(:events, :duration_in_millis).value).to be > 0
+            expect(store.get_shallow(:pipeline, :in).value).to eq(5)
+            expect(store.get_shallow(:pipeline, :duration_in_millis).value).to be > 0
+          end
+        end
+      end
+
+      context "when writing to the queue" do
+        before :each do
+          read_client.set_events_metric(LogStash::Instrument::NamespacedNullMetric.new([], :null))
+          read_client.set_pipeline_metric(LogStash::Instrument::NamespacedNullMetric.new([], :null))
+        end
+
+        it "appends batches to the queue" do
+          batch = write_client.get_new_batch
+          5.times {|i| batch.push(LogStash::Event.new({"message" => "value-#{i}"}))}
+          write_client.push_batch(batch)
+          read_batch = read_client.take_batch
+          expect(read_batch.size).to eq(5)
+          i = 0
+          read_batch.each do |data|
+            expect(data.get("message")).to eq("value-#{i}")
+            # read_batch.cancel("value-#{i}") if i > 2     # TODO: disabled for https://github.com/elastic/logstash/issues/6055 - will have to properly refactor
+            data.cancel if i > 2
+            read_batch.merge(LogStash::Event.new({"message" => "generated-#{i}"})) if i > 2
+            i += 1
+          end
+          # expect(read_batch.cancelled_size).to eq(2) # disabled for https://github.com/elastic/logstash/issues/6055
+          i = 0
+          read_batch.each do |data|
+            expect(data.get("message")).to eq("value-#{i}") if i < 3
+            expect(data.get("message")).to eq("generated-#{i}") if i > 2
+            i += 1
+          end
+        end
+      end
+    end
+  end
 end
diff --git a/logstash-core/spec/logstash/webserver_spec.rb b/logstash-core/spec/logstash/webserver_spec.rb
new file mode 100644
index 00000000000..c5dba3fbe57
--- /dev/null
+++ b/logstash-core/spec/logstash/webserver_spec.rb
@@ -0,0 +1,153 @@
+# encoding: utf-8
+# require "logstash/json"
+require "logstash/webserver"
+require_relative "../support/helpers"
+require "socket"
+require "spec_helper"
+require "open-uri"
+
+def block_ports(range)
+  servers = []
+
+  range.each do |port|
+    begin
+      server = TCPServer.new("localhost", port)
+      Thread.new do
+        client = server.accept rescue nil
+      end
+      servers << server
+    rescue => e
+      # The port can already be taken
+    end
+  end
+
+  sleep(1)
+  servers
+end
+
+def free_ports(servers)
+  servers.each do |t|
+    t.close rescue nil # the threads are blocked just kill
+  end
+end
+
+describe LogStash::WebServer do
+  before :all do
+    @abort = Thread.abort_on_exception
+    Thread.abort_on_exception = true
+  end
+
+  after :all do
+    Thread.abort_on_exception = @abort
+  end
+
+  let(:logger) { LogStash::Logging::Logger.new("testing") }
+  let(:agent) { double("agent") }
+  let(:webserver) { double("webserver") }
+
+  before :each do
+    allow(webserver).to receive(:address).and_return("127.0.0.1")
+    allow(agent).to receive(:webserver).and_return(webserver)
+  end
+
+  subject { LogStash::WebServer.new(logger,
+                                    agent,
+                                    { :http_host => "localhost", :http_ports => port_range })}
+
+  let(:port_range) { 10000..10010 }
+
+  context "when an exception occur in the server thread" do
+    let(:spy_output) { spy("stderr").as_null_object }
+
+    it "should not log to STDERR" do
+      backup_stderr = STDERR
+      backup_stdout = STDOUT
+
+      # We are redefining constants, so lets silence the warning
+      silence_warnings do
+        STDOUT = STDERR = spy_output
+      end
+
+      expect(spy_output).not_to receive(:puts).with(any_args)
+      expect(spy_output).not_to receive(:write).with(any_args)
+
+      # This trigger an infinite loop in the reactor
+      expect(IO).to receive(:select).and_raise(IOError).at_least(:once)
+
+      t = Thread.new do
+        subject.run
+      end
+
+      sleep(1)
+
+      # We cannot use stop here, since the code is stuck in an infinite loop
+      t.kill rescue nil
+
+      silence_warnings do
+        STDERR = backup_stderr
+        STDOUT = backup_stdout
+      end
+    end
+  end
+
+  context "when the port is already in use and a range is provided" do
+    after(:each) { free_ports(@servers) }
+
+    context "when we have available ports" do
+      before(:each) do
+        @servers = block_ports(10000..10005)
+      end
+
+      it "successfully find an available port" do
+        t = Thread.new do
+          subject.run
+        end
+
+        sleep(1)
+
+        response = open("http://localhost:10006").read
+        expect { LogStash::Json.load(response) }.not_to raise_error
+        expect(subject.address).to eq("localhost:10006")
+
+        subject.stop
+        t.kill rescue nil
+      end
+    end
+
+    context "when all the ports are taken" do
+      before(:each) do
+        @servers = block_ports(port_range)
+      end
+
+      it "raise an exception" do
+        expect { subject.run }.to raise_error(Errno::EADDRINUSE, /Logstash tried to bind to port range/)
+      end
+    end
+  end
+end
+
+describe LogStash::IOWrappedLogger do
+  let(:logger)  { spy("logger") }
+  let(:message) { "foobar" }
+
+  subject { described_class.new(logger) }
+
+  it "responds to puts" do
+    subject.puts(message)
+    expect(logger).to have_received(:debug).with(message)
+  end
+
+  it "responds to write" do
+    subject.write(message)
+    expect(logger).to have_received(:debug).with(message)
+  end
+
+  it "reponds to <<" do
+    subject << message
+    expect(logger).to have_received(:debug).with(message)
+  end
+
+  it "responds to sync=(v)" do
+    expect{ subject.sync = true }.not_to raise_error
+  end
+end
diff --git a/logstash-core/spec/static/i18n_spec.rb b/logstash-core/spec/static/i18n_spec.rb
new file mode 100644
index 00000000000..b2cd76377d2
--- /dev/null
+++ b/logstash-core/spec/static/i18n_spec.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require "spec_helper"
+require "i18n"
+
+I18N_T_REGEX = Regexp.new('I18n.t.+?"(.+?)"')
+
+describe I18n do
+  context "when using en.yml" do
+    glob_path = File.join(LogStash::Environment::LOGSTASH_HOME, "logstash-*", "lib", "**", "*.rb")
+
+    Dir.glob(glob_path).each do |file_name|
+
+      context "in file \"#{file_name}\"" do
+        File.foreach(file_name) do |line|
+          next unless (match = line.match(I18N_T_REGEX))
+          line = $INPUT_LINE_NUMBER
+          key = match[1]
+          it "in line #{line} the \"#{key}\" key should exist" do
+            expect(I18n.exists?(key)).to be_truthy
+          end
+        end
+      end
+    end
+  end
+end
diff --git a/logstash-core/spec/support/helpers.rb b/logstash-core/spec/support/helpers.rb
new file mode 100644
index 00000000000..8051743b7ae
--- /dev/null
+++ b/logstash-core/spec/support/helpers.rb
@@ -0,0 +1,8 @@
+# encoding: utf-8
+def silence_warnings
+  warn_level = $VERBOSE
+  $VERBOSE = nil
+  yield
+ensure
+  $VERBOSE = warn_level
+end
diff --git a/logstash-core/spec/support/mocks_classes.rb b/logstash-core/spec/support/mocks_classes.rb
new file mode 100644
index 00000000000..d90318d5330
--- /dev/null
+++ b/logstash-core/spec/support/mocks_classes.rb
@@ -0,0 +1,49 @@
+# encoding: utf-8
+require "logstash/outputs/base"
+require "thread"
+
+class DummyOutput < LogStash::Outputs::Base
+  config_name "dummyoutput"
+  milestone 2
+
+  attr_reader :num_closes, :events
+
+  def initialize(params={})
+    super
+    @num_closes = 0
+    @events = Queue.new
+  end
+
+  def register
+  end
+
+  def receive(event)
+    @events << event
+  end
+
+  def close
+    @num_closes = 1
+  end
+end
+
+class DummyOutputWithEventsArray < LogStash::Outputs::Base
+  config_name "dummyoutput2"
+  milestone 2
+
+  attr_reader :events
+
+  def initialize(params={})
+    super
+    @events = []
+  end
+
+  def register
+  end
+
+  def receive(event)
+    @events << event
+  end
+
+  def close
+  end
+end
diff --git a/logstash-core/spec/support/shared_examples.rb b/logstash-core/spec/support/shared_examples.rb
new file mode 100644
index 00000000000..0218bebb53c
--- /dev/null
+++ b/logstash-core/spec/support/shared_examples.rb
@@ -0,0 +1,108 @@
+# encoding: utf-8
+# Define the common operation that both the `NullMetric` class and the Namespaced class should answer.
+shared_examples "metrics commons operations" do
+  let(:key) { "galaxy" }
+
+  describe "#increment" do
+    it "allows to increment a key with no amount" do
+      expect { subject.increment(key, 100) }.not_to raise_error
+    end
+
+    it "allow to increment a key" do
+      expect { subject.increment(key) }.not_to raise_error
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.increment("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.increment(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#decrement" do
+    it "allows to decrement a key with no amount" do
+      expect { subject.decrement(key, 100) }.not_to raise_error
+    end
+
+    it "allow to decrement a key" do
+      expect { subject.decrement(key) }.not_to raise_error
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.decrement("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.decrement(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#gauge" do
+    it "allows to set a value" do
+      expect { subject.gauge(key, "pluto") }.not_to raise_error
+    end
+
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.gauge("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.gauge(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#report_time" do
+    it "allow to record time" do
+      expect { subject.report_time(key, 1000) }.not_to raise_error
+    end
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.report_time("", 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.report_time(nil, 20) }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+
+  describe "#time" do
+    it "allow to record time with a block given" do
+      expect do
+        subject.time(key) { 1+1 }
+      end.not_to raise_error
+    end
+
+    it "returns the value of the block without recording any metrics" do
+      expect(subject.time(:execution_time) { "hello" }).to eq("hello")
+    end
+
+    it "return a TimedExecution" do
+      execution = subject.time(:do_something)
+      expect { execution.stop }.not_to raise_error
+    end
+
+
+    it "raises an exception if the key is an empty string" do
+      expect { subject.time("") {} }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+
+    it "raise an exception if the key is nil" do
+      expect { subject.time(nil) {} }.to raise_error(LogStash::Instrument::MetricNoKeyProvided)
+    end
+  end
+end
+
+shared_examples "not found" do
+  it "should return a 404 to unknown request" do
+    do_request { get "/i_want_to_believe-#{Time.now.to_i}" }
+    expect(last_response.content_type).to eq("application/json")
+    expect(last_response).not_to be_ok
+    expect(last_response.status).to eq(404)
+    expect(LogStash::Json.load(last_response.body)).to include("status" => 404)
+    expect(LogStash::Json.load(last_response.body)["path"]).not_to be_nil
+  end
+end
+
diff --git a/logstash-core/src/main/java/org/logstash/log/CustomLogEvent.java b/logstash-core/src/main/java/org/logstash/log/CustomLogEvent.java
new file mode 100644
index 00000000000..f382bce75a1
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/CustomLogEvent.java
@@ -0,0 +1,37 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.log;
+
+import com.fasterxml.jackson.databind.annotation.JsonSerialize;
+import org.apache.logging.log4j.Level;
+import org.apache.logging.log4j.Marker;
+import org.apache.logging.log4j.core.config.Property;
+import org.apache.logging.log4j.core.impl.Log4jLogEvent;
+import org.apache.logging.log4j.message.Message;
+
+import java.util.List;
+
+@JsonSerialize(using = CustomLogEventSerializer.class)
+public class CustomLogEvent extends Log4jLogEvent {
+    public CustomLogEvent(final String loggerName, final Marker marker, final String loggerFQCN, final Level level,
+                          final Message message, final List<Property> properties, final Throwable t) {
+        super(loggerName, marker, loggerFQCN, level, message, properties, t);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/CustomLogEventSerializer.java b/logstash-core/src/main/java/org/logstash/log/CustomLogEventSerializer.java
new file mode 100644
index 00000000000..6432394d91f
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/CustomLogEventSerializer.java
@@ -0,0 +1,48 @@
+package org.logstash.log;
+
+import com.fasterxml.jackson.core.JsonGenerator;
+import com.fasterxml.jackson.databind.JsonMappingException;
+import com.fasterxml.jackson.databind.JsonSerializer;
+import com.fasterxml.jackson.databind.SerializerProvider;
+
+import java.io.IOException;
+import java.util.Map;
+
+public class CustomLogEventSerializer extends JsonSerializer<CustomLogEvent> {
+    @Override
+    public void serialize(CustomLogEvent event, JsonGenerator generator, SerializerProvider provider) throws IOException {
+        generator.writeStartObject();
+        generator.writeObjectField("level", event.getLevel());
+        generator.writeObjectField("loggerName", event.getLoggerName());
+        generator.writeObjectField("timeMillis", event.getTimeMillis());
+        generator.writeObjectField("thread", event.getThreadName());
+        generator.writeFieldName("logEvent");
+        generator.writeStartObject();
+        if (event.getMessage() instanceof StructuredMessage) {
+            StructuredMessage message = (StructuredMessage) event.getMessage();
+            generator.writeStringField("message", message.getMessage());
+            if (message.getParams() != null) {
+                for (Map.Entry<Object, Object> entry : message.getParams().entrySet()) {
+                    Object value = entry.getValue();
+                    try {
+                        generator.writeObjectField(entry.getKey().toString(), value);
+                    } catch (JsonMappingException e) {
+                        generator.writeObjectField(entry.getKey().toString(), value.toString());
+                    }
+                }
+            }
+
+        } else {
+            generator.writeStringField("message", event.getMessage().getFormattedMessage());
+        }
+
+        generator.writeEndObject();
+        generator.writeEndObject();
+    }
+
+    @Override
+    public Class<CustomLogEvent> handledType() {
+
+        return CustomLogEvent.class;
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/LogstashLogEventFactory.java b/logstash-core/src/main/java/org/logstash/log/LogstashLogEventFactory.java
new file mode 100644
index 00000000000..f13c264dd21
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/LogstashLogEventFactory.java
@@ -0,0 +1,17 @@
+package org.logstash.log;
+
+import org.apache.logging.log4j.Level;
+import org.apache.logging.log4j.Marker;
+import org.apache.logging.log4j.core.LogEvent;
+import org.apache.logging.log4j.core.config.Property;
+import org.apache.logging.log4j.core.impl.LogEventFactory;
+import org.apache.logging.log4j.message.Message;
+
+import java.util.List;
+
+public class LogstashLogEventFactory implements LogEventFactory {
+    @Override
+    public LogEvent createEvent(String loggerName, Marker marker, String fqcn, Level level, Message data, List<Property> properties, Throwable t) {
+        return new CustomLogEvent(loggerName, marker, fqcn, level, data, properties, t);
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/LogstashMessageFactory.java b/logstash-core/src/main/java/org/logstash/log/LogstashMessageFactory.java
new file mode 100644
index 00000000000..7cadbbf0385
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/LogstashMessageFactory.java
@@ -0,0 +1,33 @@
+package org.logstash.log;
+
+import org.apache.logging.log4j.message.Message;
+import org.apache.logging.log4j.message.MessageFactory;
+import org.apache.logging.log4j.message.ObjectMessage;
+import org.apache.logging.log4j.message.ParameterizedMessage;
+import org.apache.logging.log4j.message.SimpleMessage;
+
+import java.util.Map;
+
+public final class LogstashMessageFactory implements MessageFactory {
+
+    public static final LogstashMessageFactory INSTANCE = new LogstashMessageFactory();
+
+    @Override
+    public Message newMessage(Object message) {
+        return new ObjectMessage(message);
+    }
+
+    @Override
+    public Message newMessage(String message) {
+        return new SimpleMessage(message);
+    }
+
+    @Override
+    public Message newMessage(String message, Object... params) {
+        if (params.length == 1 && params[0] instanceof Map) {
+            return new StructuredMessage(message, params);
+        } else {
+            return new ParameterizedMessage(message, params);
+        }
+    }
+}
diff --git a/logstash-core/src/main/java/org/logstash/log/StructuredMessage.java b/logstash-core/src/main/java/org/logstash/log/StructuredMessage.java
new file mode 100644
index 00000000000..2145acea746
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/log/StructuredMessage.java
@@ -0,0 +1,76 @@
+package org.logstash.log;
+
+import com.fasterxml.jackson.databind.annotation.JsonSerialize;
+import org.apache.logging.log4j.message.Message;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.Map;
+
+@JsonSerialize(using = CustomLogEventSerializer.class)
+public class StructuredMessage implements Message {
+    private final String message;
+    private final Map<Object, Object> params;
+
+    @SuppressWarnings("unchecked")
+    public StructuredMessage(String message) {
+        this(message, (Map) null);
+    }
+
+    @SuppressWarnings("unchecked")
+    public StructuredMessage(String message, Object[] params) {
+        final Map<Object, Object> paramsMap;
+        if (params.length == 1 && params[0] instanceof Map) {
+            paramsMap = (Map) params[0];
+        } else {
+            paramsMap = new HashMap<>();
+            try {
+                for (int i = 0; i < params.length; i += 2) {
+                    paramsMap.put(params[i].toString(), params[i + 1]);
+                }
+            } catch (IndexOutOfBoundsException e) {
+                throw new IllegalArgumentException("must log key-value pairs");
+            }
+        }
+        this.message = message;
+        this.params = paramsMap;
+    }
+
+    public StructuredMessage(String message, Map<Object, Object> params) {
+        this.message = message;
+        this.params = params;
+    }
+
+    public String getMessage() {
+        return message;
+    }
+
+    public Map<Object, Object> getParams() {
+        return params;
+    }
+
+    @Override
+    public Object[] getParameters() {
+        return params.values().toArray();
+    }
+
+    @Override
+    public String getFormattedMessage() {
+        String formatted = message;
+        if (params != null && !params.isEmpty()) {
+            formatted += " " + params;
+        }
+        return formatted;
+    }
+
+    @Override
+    public String getFormat() {
+        return null;
+    }
+
+
+    @Override
+    public Throwable getThrowable() {
+        return null;
+    }
+}
diff --git a/logstash-core/src/main/resources/log4j2.component.properties b/logstash-core/src/main/resources/log4j2.component.properties
new file mode 100644
index 00000000000..18b737986ae
--- /dev/null
+++ b/logstash-core/src/main/resources/log4j2.component.properties
@@ -0,0 +1,2 @@
+Log4jLogEventFactory=org.logstash.log.LogstashLogEventFactory
+log4j2.messageFactory=org.logstash.log.LogstashMessageFactory
diff --git a/logstash-core/src/test/java/org/logstash/log/CustomLogEventTests.java b/logstash-core/src/test/java/org/logstash/log/CustomLogEventTests.java
new file mode 100644
index 00000000000..9604c457894
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/log/CustomLogEventTests.java
@@ -0,0 +1,113 @@
+/*
+ * Licensed to Elasticsearch under one or more contributor
+ * license agreements. See the NOTICE file distributed with
+ * this work for additional information regarding copyright
+ * ownership. Elasticsearch licenses this file to you under
+ * the Apache License, Version 2.0 (the "License"); you may
+ * not use this file except in compliance with the License.
+ * You may obtain a copy of the License at
+ *
+ *    http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing,
+ * software distributed under the License is distributed on an
+ * "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
+ * KIND, either express or implied.  See the License for the
+ * specific language governing permissions and limitations
+ * under the License.
+ */
+
+package org.logstash.log;
+
+import com.fasterxml.jackson.databind.ObjectMapper;
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.apache.logging.log4j.junit.LoggerContextRule;
+import org.apache.logging.log4j.test.appender.ListAppender;
+import org.junit.ClassRule;
+import org.junit.Test;
+
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.List;
+import java.util.Map;
+
+import static junit.framework.TestCase.assertEquals;
+import static junit.framework.TestCase.assertNotNull;
+
+public class CustomLogEventTests {
+    private static final ObjectMapper mapper = new ObjectMapper();
+    private static final String CONFIG = "log4j2-test1.xml";
+    private ListAppender appender;
+
+    @ClassRule
+    public static LoggerContextRule CTX = new LoggerContextRule(CONFIG);
+
+    @Test
+    public void testPatternLayout() {
+        appender = CTX.getListAppender("EventLogger").clear();
+        Logger logger = LogManager.getLogger("EventLogger");
+        logger.info("simple message");
+        logger.warn("complex message", Collections.singletonMap("foo", "bar"));
+        logger.error("my name is: {}", "foo");
+        logger.error("here is a map: {}. ok?", Collections.singletonMap(2, 5));
+        logger.warn("ignored params {}", 4, 6);
+        List<String> messages = appender.getMessages();
+        assertEquals(5, messages.size());
+        assertEquals("[INFO][EventLogger] simple message", messages.get(0));
+        assertEquals("[WARN][EventLogger] complex message {foo=bar}", messages.get(1));
+        assertEquals("[ERROR][EventLogger] my name is: foo", messages.get(2));
+        assertEquals("[ERROR][EventLogger] here is a map: {}. ok? {2=5}", messages.get(3));
+        assertEquals("[WARN][EventLogger] ignored params 4", messages.get(4));
+    }
+
+    @Test
+    @SuppressWarnings("unchecked")
+    public void testJSONLayout() throws Exception {
+        appender = CTX.getListAppender("JSONEventLogger").clear();
+        Logger logger = LogManager.getLogger("JSONEventLogger");
+        logger.info("simple message");
+        logger.warn("complex message", Collections.singletonMap("foo", "bar"));
+        logger.error("my name is: {}", "foo");
+        logger.error("here is a map: {}", Collections.singletonMap(2, 5));
+        logger.warn("ignored params {}", 4, 6, 8);
+
+        List<String> messages = appender.getMessages();
+
+        Map<String, Object> firstMessage = mapper.readValue(messages.get(0), Map.class);
+
+        assertEquals(5, firstMessage.size());
+        assertEquals("INFO", firstMessage.get("level"));
+        assertEquals("JSONEventLogger", firstMessage.get("loggerName"));
+        assertNotNull(firstMessage.get("thread"));
+        assertEquals(Collections.singletonMap("message", "simple message"), firstMessage.get("logEvent"));
+
+        Map<String, Object> secondMessage = mapper.readValue(messages.get(1), Map.class);
+
+        assertEquals(5, secondMessage.size());
+        assertEquals("WARN", secondMessage.get("level"));
+        assertEquals("JSONEventLogger", secondMessage.get("loggerName"));
+        assertNotNull(secondMessage.get("thread"));
+        Map<String, Object> logEvent = new HashMap<>();
+        logEvent.put("message", "complex message");
+        logEvent.put("foo", "bar");
+        assertEquals(logEvent, secondMessage.get("logEvent"));
+
+        Map<String, Object> thirdMessage = mapper.readValue(messages.get(2), Map.class);
+        assertEquals(5, thirdMessage.size());
+        logEvent = Collections.singletonMap("message", "my name is: foo");
+        assertEquals(logEvent, thirdMessage.get("logEvent"));
+
+        Map<String, Object> fourthMessage = mapper.readValue(messages.get(3), Map.class);
+        assertEquals(5, fourthMessage.size());
+        logEvent = new HashMap<>();
+        logEvent.put("message", "here is a map: {}");
+        logEvent.put("2", 5);
+        assertEquals(logEvent, fourthMessage.get("logEvent"));
+
+        Map<String, Object> fifthMessage = mapper.readValue(messages.get(4), Map.class);
+        assertEquals(5, fifthMessage.size());
+        logEvent = Collections.singletonMap("message", "ignored params 4");
+        assertEquals(logEvent, fifthMessage.get("logEvent"));
+    }
+}
diff --git a/logstash-core/src/test/resources/log4j-list.properties b/logstash-core/src/test/resources/log4j-list.properties
new file mode 100644
index 00000000000..435d6042ea0
--- /dev/null
+++ b/logstash-core/src/test/resources/log4j-list.properties
@@ -0,0 +1,11 @@
+status = error
+name = LogstashPropertiesConfig
+
+appender.list.type = List
+appender.list.name = List
+#appender.list.layout.type = JSONLayout
+#appender.list.layout.compact = true
+#appender.list.layout.eventEol = true
+
+rootLogger.level = info
+rootLogger.appenderRef.stdout.ref = List
diff --git a/logstash-core/src/test/resources/log4j2-test1.xml b/logstash-core/src/test/resources/log4j2-test1.xml
new file mode 100644
index 00000000000..ad81ddde21e
--- /dev/null
+++ b/logstash-core/src/test/resources/log4j2-test1.xml
@@ -0,0 +1,31 @@
+<?xml version="1.0" encoding="UTF-8"?>
+<Configuration status="OFF" name="LoggerTest">
+    <properties>
+        <property name="filename">target/test.log</property>
+    </properties>
+    <ThresholdFilter level="trace"/>
+
+    <Appenders>
+        <List name="EventLogger">
+            <PatternLayout pattern="[%p][%c] %m"/>
+        </List>
+        <List name="JSONEventLogger">
+            <JSONLayout compact="true" eventEol="true" />
+        </List>
+    </Appenders>
+
+    <Loggers>
+        <Logger name="EventLogger" level="debug" additivity="false">
+            <AppenderRef ref="EventLogger"/>
+        </Logger>>
+
+        <Logger name="JSONEventLogger" level="debug" additivity="false">
+            <AppenderRef ref="JSONEventLogger"/>
+        </Logger>>
+
+        <Root level="trace">
+            <AppenderRef ref="EventLogger"/>
+        </Root>
+    </Loggers>
+
+</Configuration>
\ No newline at end of file
diff --git a/pkg/centos/after-install.sh b/pkg/centos/after-install.sh
index 224904e4b32..ac226fd84b2 100644
--- a/pkg/centos/after-install.sh
+++ b/pkg/centos/after-install.sh
@@ -1,6 +1,9 @@
-/sbin/chkconfig --add logstash
-
-chown -R logstash:logstash /opt/logstash
-chown logstash /var/log/logstash
+chown -R logstash:logstash /usr/share/logstash
+chown -R logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
-chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.logs:|path.logs: /var/log/logstash|' \
+  -e 's|# path.data:|path.data: /var/lib/logstash|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/centos/before-install.sh b/pkg/centos/before-install.sh
index 5a852488ff3..78fc0b77d49 100644
--- a/pkg/centos/before-install.sh
+++ b/pkg/centos/before-install.sh
@@ -5,6 +5,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -r -g logstash -d /opt/logstash \
+  useradd -r -g logstash -d /usr/share/logstash \
     -s /sbin/nologin -c "logstash" logstash
 fi
diff --git a/pkg/centos/before-remove.sh b/pkg/centos/before-remove.sh
index 5109888475f..6687ee896e5 100644
--- a/pkg/centos/before-remove.sh
+++ b/pkg/centos/before-remove.sh
@@ -1,6 +1,32 @@
+# CentOS/RHEL and SuSE
 if [ $1 -eq 0 ]; then
-  /sbin/service logstash stop >/dev/null 2>&1 || true
-  /sbin/chkconfig --del logstash
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
+
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
diff --git a/pkg/debian/after-install.sh b/pkg/debian/after-install.sh
index 18b4ea8c32c..8a2f0767997 100644
--- a/pkg/debian/after-install.sh
+++ b/pkg/debian/after-install.sh
@@ -1,7 +1,12 @@
 #!/bin/sh
 
-chown -R logstash:logstash /opt/logstash
-chown logstash /var/log/logstash
+chown -R logstash:logstash /usr/share/logstash
+chown -R logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
 chmod 755 /etc/logstash
-chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.logs:|path.logs: /var/log/logstash|' \
+  -e 's|# path.data:|path.data: /var/lib/logstash|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/debian/before-install.sh b/pkg/debian/before-install.sh
index 45ef4e40f1f..03cf86125a9 100644
--- a/pkg/debian/before-install.sh
+++ b/pkg/debian/before-install.sh
@@ -7,6 +7,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -M -r -g logstash -d /var/lib/logstash \
+  useradd -M -r -g logstash -d /usr/share/logstash \
     -s /usr/sbin/nologin -c "LogStash Service User" logstash
 fi
diff --git a/pkg/debian/before-remove.sh b/pkg/debian/before-remove.sh
index a3f911e60ea..16347f266fc 100644
--- a/pkg/debian/before-remove.sh
+++ b/pkg/debian/before-remove.sh
@@ -1,13 +1,38 @@
 #!/bin/sh
-
+# Debian
 if [ $1 = "remove" ]; then
-  service logstash stop >/dev/null 2>&1 || true
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /usr/sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
 
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
 
-  if getent group logstash >/dev/null ; then
+  if getent group logstash > /dev/null ; then
     groupdel logstash
   fi
 fi
diff --git a/pkg/jvm.options b/pkg/jvm.options
new file mode 100644
index 00000000000..2568d6d4f5a
--- /dev/null
+++ b/pkg/jvm.options
@@ -0,0 +1,74 @@
+## JVM configuration
+
+# Xms represents the initial size of total heap space
+# Xmx represents the maximum size of total heap space
+
+-Xms256m
+-Xmx1g
+
+################################################################
+## Expert settings
+################################################################
+##
+## All settings below this section are considered
+## expert settings. Don't tamper with them unless
+## you understand what you are doing
+##
+################################################################
+
+## GC configuration
+-XX:+UseParNewGC
+-XX:+UseConcMarkSweepGC
+-XX:CMSInitiatingOccupancyFraction=75
+-XX:+UseCMSInitiatingOccupancyOnly
+
+## optimizations
+
+# disable calls to System#gc
+-XX:+DisableExplicitGC
+
+## locale
+# Set the locale language
+#-Duser.language=en
+
+# Set the locale country
+#-Duser.country=US
+
+# Set the locale variant, if any
+#-Duser.variant=
+
+## basic
+
+# set the I/O temp directory
+#-Djava.io.tmpdir=$HOME
+
+# set to headless, just in case
+-Djava.awt.headless=true
+
+# ensure UTF-8 encoding by default (e.g. filenames)
+-Dfile.encoding=UTF-8
+
+# use our provided JNA always versus the system one
+#-Djna.nosys=true
+
+## heap dumps
+
+# generate a heap dump when an allocation from the Java heap fails
+# heap dumps are created in the working directory of the JVM
+-XX:+HeapDumpOnOutOfMemoryError
+
+# specify an alternative path for heap dumps
+# ensure the directory exists and has sufficient space
+#-XX:HeapDumpPath=${LOGSTASH_HOME}/heapdump.hprof
+
+## GC logging
+#-XX:+PrintGCDetails
+#-XX:+PrintGCTimeStamps
+#-XX:+PrintGCDateStamps
+#-XX:+PrintClassHistogram
+#-XX:+PrintTenuringDistribution
+#-XX:+PrintGCApplicationStoppedTime
+
+# log GC status to a file with time stamps
+# ensure the directory exists
+#-Xloggc:${LS_GC_LOG_FILE}
diff --git a/pkg/log4j2.properties b/pkg/log4j2.properties
new file mode 100644
index 00000000000..4471045c6e7
--- /dev/null
+++ b/pkg/log4j2.properties
@@ -0,0 +1,28 @@
+status = error
+name = LogstashPropertiesConfig
+
+appender.rolling.type = RollingFile
+appender.rolling.name = plain_rolling
+appender.rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.rolling.policies.type = Policies
+appender.rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.rolling.policies.time.interval = 1
+appender.rolling.policies.time.modulate = true
+appender.rolling.layout.type = PatternLayout
+appender.rolling.layout.pattern = [%d{ISO8601}][%-5p][%-25c] %.10000m%n
+
+appender.json_rolling.type = RollingFile
+appender.json_rolling.name = json_rolling
+appender.json_rolling.fileName = ${sys:ls.logs}/logstash-${sys:ls.log.format}.log
+appender.json_rolling.filePattern = ${sys:ls.logs}/logstash-${sys:ls.log.format}-%d{yyyy-MM-dd}.log
+appender.json_rolling.policies.type = Policies
+appender.json_rolling.policies.time.type = TimeBasedTriggeringPolicy
+appender.json_rolling.policies.time.interval = 1
+appender.json_rolling.policies.time.modulate = true
+appender.json_rolling.layout.type = JSONLayout
+appender.json_rolling.layout.compact = true
+appender.json_rolling.layout.eventEol = true
+
+rootLogger.level = ${sys:ls.log.level}
+rootLogger.appenderRef.rolling.ref = ${sys:ls.log.format}_rolling
diff --git a/pkg/logstash.default b/pkg/logstash.default
deleted file mode 100644
index c3415146761..00000000000
--- a/pkg/logstash.default
+++ /dev/null
@@ -1,41 +0,0 @@
-###############################
-# Default settings for logstash
-###############################
-
-# Override Java location
-#JAVACMD=/usr/bin/java
-
-# Set a home directory
-#LS_HOME=/var/lib/logstash
-
-# Arguments to pass to logstash agent
-#LS_OPTS=""
-
-# Arguments to pass to java
-#LS_HEAP_SIZE="1g"
-#LS_JAVA_OPTS="-Djava.io.tmpdir=$HOME"
-
-# pidfiles aren't used for upstart; this is for sysv users.
-#LS_PIDFILE=/var/run/logstash.pid
-
-# user id to be invoked as; for upstart: edit /etc/init/logstash.conf
-#LS_USER=logstash
-
-# logstash logging
-#LS_LOG_FILE=/var/log/logstash/logstash.log
-#LS_USE_GC_LOGGING="true"
-#LS_GC_LOG_FILE=/var/log/logstash/gc.log
-
-# logstash configuration directory
-#LS_CONF_DIR=/etc/logstash/conf.d
-
-# Open file limit; cannot be overridden in upstart
-#LS_OPEN_FILES=16384
-
-# Nice level
-#LS_NICE=19
-
-# If this is set to 1, then when `stop` is called, if the process has
-# not exited within a reasonable time, SIGKILL will be sent next.
-# The default behavior is to simply log a message "program stop failed; still running"
-KILL_ON_STOP_TIMEOUT=0
diff --git a/pkg/logstash.sysv b/pkg/logstash.sysv
deleted file mode 100755
index d971b4a405c..00000000000
--- a/pkg/logstash.sysv
+++ /dev/null
@@ -1,198 +0,0 @@
-#!/bin/sh
-# Init script for logstash
-# Maintained by Elasticsearch
-# Generated by pleaserun.
-# Implemented based on LSB Core 3.1:
-#   * Sections: 20.2, 20.3
-#
-### BEGIN INIT INFO
-# Provides:          logstash
-# Required-Start:    $remote_fs $syslog
-# Required-Stop:     $remote_fs $syslog
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description:
-# Description:        Starts Logstash as a daemon.
-### END INIT INFO
-
-PATH=/sbin:/usr/sbin:/bin:/usr/bin
-export PATH
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-name=logstash
-pidfile="/var/run/$name.pid"
-
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="1g"
-LS_LOG_DIR=/var/log/logstash
-LS_LOG_FILE="${LS_LOG_DIR}/$name.log"
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-
-
-[ -r /etc/default/$name ] && . /etc/default/$name
-[ -r /etc/sysconfig/$name ] && . /etc/sysconfig/$name
-
-program=/opt/logstash/bin/logstash
-args="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-quiet() {
-  "$@" > /dev/null 2>&1
-  return $?
-}
-
-start() {
-
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  HOME=${LS_HOME}
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-
-  # chown doesn't grab the suplimental groups when setting the user:group - so we have to do it for it.
-  # Boy, I hope we're root here.
-  SGROUPS=$(id -Gn "$LS_USER" | tr " " "," | sed 's/,$//'; echo '')
-
-  if [ ! -z $SGROUPS ]
-  then
-	EXTRA_GROUPS="--groups $SGROUPS"
-  fi
-
-  # set ulimit as (root, presumably) first, before we drop privileges
-  ulimit -n ${LS_OPEN_FILES}
-
-  # Run the program!
-  nice -n ${LS_NICE} chroot --userspec $LS_USER:$LS_GROUP $EXTRA_GROUPS / sh -c "
-    cd $LS_HOME
-    ulimit -n ${LS_OPEN_FILES}
-    exec \"$program\" $args
-  " > "${LS_LOG_DIR}/$name.stdout" 2> "${LS_LOG_DIR}/$name.err" &
-
-  # Generate the pidfile from here. If we instead made the forked process
-  # generate it there will be a race condition between the pidfile writing
-  # and a process possibly asking for status.
-  echo $! > $pidfile
-
-  echo "$name started."
-  return 0
-}
-
-stop() {
-  # Try a few times to kill TERM the program
-  if status ; then
-    pid=`cat "$pidfile"`
-    echo "Killing $name (pid $pid) with SIGTERM"
-    kill -TERM $pid
-    # Wait for it to exit.
-    for i in 1 2 3 4 5 6 7 8 9 ; do
-      echo "Waiting $name (pid $pid) to die..."
-      status || break
-      sleep 1
-    done
-    if status ; then
-      if [[ $KILL_ON_STOP_TIMEOUT -eq 1 ]] ; then
-        echo "Timeout reached. Killing $name (pid $pid) with SIGKILL. This may result in data loss."
-        kill -KILL $pid
-        echo "$name killed with SIGKILL."
-      else
-        echo "$name stop failed; still running."
-      fi
-    else
-      echo "$name stopped."
-    fi
-  fi
-}
-
-status() {
-  if [ -f "$pidfile" ] ; then
-    pid=`cat "$pidfile"`
-    if kill -0 $pid > /dev/null 2> /dev/null ; then
-      # process by this pid is running.
-      # It may not be our pid, but that's what you get with just pidfiles.
-      # TODO(sissel): Check if this process seems to be the same as the one we
-      # expect. It'd be nice to use flock here, but flock uses fork, not exec,
-      # so it makes it quite awkward to use in this case.
-      return 0
-    else
-      return 2 # program is dead but pid file exists
-    fi
-  else
-    return 3 # program is not running
-  fi
-}
-
-force_stop() {
-  if status ; then
-    stop
-    status && kill -KILL `cat "$pidfile"`
-  fi
-}
-
-configtest() {
-  # Check if a config file exists
-  if [ ! "$(ls -A ${LS_CONF_DIR}/* 2> /dev/null)" ]; then
-    echo "There aren't any configuration files in ${LS_CONF_DIR}"
-    return 1
-  fi
-
-  HOME=${LS_HOME}
-  export PATH HOME
-
-  test_args="--configtest -f ${LS_CONF_DIR} ${LS_OPTS}"
-  $program ${test_args}
-  [ $? -eq 0 ] && return 0
-  # Program not configured
-  return 6
-}
-
-case "$1" in
-  start)
-    status
-    code=$?
-    if [ $code -eq 0 ]; then
-      echo "$name is already running"
-    else
-      start
-      code=$?
-    fi
-    exit $code
-    ;;
-  stop) stop ;;
-  force-stop) force_stop ;;
-  status)
-    status
-    code=$?
-    if [ $code -eq 0 ] ; then
-      echo "$name is running"
-    else
-      echo "$name is not running"
-    fi
-    exit $code
-    ;;
-  restart)
-
-    quiet configtest
-    RET=$?
-    if [ ${RET} -ne 0 ]; then
-      echo "Configuration error. Not restarting. Re-run with configtest parameter for details"
-      exit ${RET}
-    fi
-    stop && start
-    ;;
-  configtest)
-    configtest
-    exit $?
-    ;;
-  *)
-    echo "Usage: $SCRIPTNAME {start|stop|force-stop|status|restart|configtest}" >&2
-    exit 3
-  ;;
-esac
-
-exit $?
diff --git a/pkg/logstash.sysv.debian b/pkg/logstash.sysv.debian
deleted file mode 100644
index f83c468d81a..00000000000
--- a/pkg/logstash.sysv.debian
+++ /dev/null
@@ -1,181 +0,0 @@
-#!/bin/bash
-#
-# /etc/init.d/logstash -- startup script for LogStash.
-#
-### BEGIN INIT INFO
-# Provides:          logstash
-# Required-Start:    $all
-# Required-Stop:     $all
-# Default-Start:     2 3 4 5
-# Default-Stop:      0 1 6
-# Short-Description: Starts logstash
-# Description:       Starts logstash using start-stop-daemon
-### END INIT INFO
-
-set -e
-
-NAME=logstash
-DESC="Logstash Daemon"
-DEFAULT=/etc/default/$NAME
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-. /lib/lsb/init-functions
-
-if [ -r /etc/default/rcS ]; then
-   . /etc/default/rcS
-fi
-
-# The following variables can be overwritten in $DEFAULT
-PATH=/bin:/usr/bin:/sbin:/usr/sbin
-
-# See contents of file named in $DEFAULT for comments
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="1g"
-LS_LOG_FILE=/var/log/logstash/$NAME.log
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-LS_PIDFILE=/var/run/$NAME.pid
-
-# End of variables that can be overwritten in $DEFAULT
-
-# overwrite settings from default file
-if [ -f "$DEFAULT" ]; then
-   . "$DEFAULT"
-fi
-
-# Define other required variables
-PID_FILE=${LS_PIDFILE}
-DAEMON=/opt/logstash/bin/logstash
-DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-# Check DAEMON exists
-if ! test -e $DAEMON; then
-   log_failure_msg "Script $DAEMON doesn't exist"
-   exit 1
-fi
-
-case "$1" in
-   start)
-      if [ -z "$DAEMON" ]; then
-         log_failure_msg "no logstash script found - $DAEMON"
-         exit 1
-      fi
-
-      # Check if a config file exists
-      if [ ! "$(ls -A $LS_CONF_DIR/*.conf 2> /dev/null)" ]; then
-         log_failure_msg "There aren't any configuration files in $LS_CONF_DIR"
-         exit 1
-      fi
-
-      log_daemon_msg "Starting $DESC"
-
-      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-      if start-stop-daemon --test --start --pidfile "$PID_FILE" \
-         --user "$LS_USER" --exec "$JAVA" \
-      >/dev/null; then
-         # Prepare environment
-         HOME="${HOME:-$LS_HOME}"
-         LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-         ulimit -n ${LS_OPEN_FILES}
-	 cd "${LS_HOME}"
-         export PATH HOME JAVACMD LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-
-         # Start Daemon
-         start-stop-daemon --start -b --user "$LS_USER" -c "$LS_USER":"$LS_GROUP" \
-           -d "$LS_HOME" --nicelevel "$LS_NICE" --pidfile "$PID_FILE" --make-pidfile \
-           --exec $DAEMON -- $DAEMON_OPTS
-
-         sleep 1
-
-         # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-         JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-         if start-stop-daemon --test --start --pidfile "$PID_FILE" \
-             --user "$LS_USER" --exec "$JAVA" \
-         >/dev/null; then
-
-            if [ -f "$PID_FILE" ]; then
-               rm -f "$PID_FILE"
-            fi
-
-            log_end_msg 1
-         else
-            log_end_msg 0
-         fi
-      else
-         log_progress_msg "(already running)"
-         log_end_msg 0
-      fi
-   ;;
-   stop)
-      log_daemon_msg "Stopping $DESC"
-
-      set +e
-
-      if [ -f "$PID_FILE" ]; then
-         start-stop-daemon --stop --pidfile "$PID_FILE" \
-            --user "$LS_USER" \
-            --retry=TERM/20/KILL/5 >/dev/null
-
-         if [ $? -eq 1 ]; then
-            log_progress_msg "$DESC is not running but pid file exists, cleaning up"
-         elif [ $? -eq 3 ]; then
-            PID="`cat $PID_FILE`"
-            log_failure_msg "Failed to stop $DESC (pid $PID)"
-            exit 1
-         fi
-
-         rm -f "$PID_FILE"
-      else
-         log_progress_msg "(not running)"
-      fi
-
-      log_end_msg 0
-      set -e
-   ;;
-   status)
-      set +e
-
-      # Parse the actual JAVACMD from the process' environment, we don't care about errors.
-      JAVA=$(cat /proc/$(cat "${PID_FILE}" 2>/dev/null)/environ 2>/dev/null | grep -z ^JAVACMD= | cut -d= -f2)
-      start-stop-daemon --test --start --pidfile "$PID_FILE" \
-         --user "$LS_USER" --exec "$JAVA" \
-      >/dev/null 2>&1
-
-      if [ "$?" = "0" ]; then
-         if [ -f "$PID_FILE" ]; then
-            log_success_msg "$DESC is not running, but pid file exists."
-            exit 1
-         else
-            log_success_msg "$DESC is not running."
-            exit 3
-         fi
-      else
-         log_success_msg "$DESC is running with pid `cat $PID_FILE`"
-      fi
-
-      set -e
-   ;;
-   restart|force-reload)
-      if [ -f "$PID_FILE" ]; then
-         $0 stop
-         sleep 1
-      fi
-
-      $0 start
-   ;;
-   *)
-      log_success_msg "Usage: $0 {start|stop|restart|force-reload|status}"
-      exit 1
-   ;;
-esac
-
-exit 0
diff --git a/pkg/logstash.sysv.redhat b/pkg/logstash.sysv.redhat
deleted file mode 100755
index 07f606e8d41..00000000000
--- a/pkg/logstash.sysv.redhat
+++ /dev/null
@@ -1,132 +0,0 @@
-#! /bin/sh
-#
-#       /etc/rc.d/init.d/logstash
-#
-#       Starts Logstash as a daemon
-#
-# chkconfig: 2345 90 10
-# description: Starts Logstash as a daemon.
-
-### BEGIN INIT INFO
-# Provides: logstash
-# Required-Start: $local_fs $remote_fs
-# Required-Stop: $local_fs $remote_fs
-# Default-Start: 2 3 4 5
-# Default-Stop: S 0 1 6
-# Short-Description: Logstash
-# Description: Starts Logstash as a daemon.
-### END INIT INFO
-
-. /etc/rc.d/init.d/functions
-
-NAME=logstash
-DESC="Logstash Daemon"
-DEFAULT=/etc/sysconfig/$NAME
-
-if [ `id -u` -ne 0 ]; then
-   echo "You need root privileges to run this script"
-   exit 1
-fi
-
-# The following variables can be overwritten in $DEFAULT
-PATH=/bin:/usr/bin:/sbin:/usr/sbin
-
-# See contents of file named in $DEFAULT for comments
-LS_USER=logstash
-LS_GROUP=logstash
-LS_HOME=/var/lib/logstash
-LS_HEAP_SIZE="1g"
-LS_LOG_FILE=/var/log/logstash/$NAME.log
-LS_CONF_DIR=/etc/logstash/conf.d
-LS_OPEN_FILES=16384
-LS_NICE=19
-LS_OPTS=""
-LS_PIDFILE=/var/run/$NAME/$NAME.pid
-
-# End of variables that can be overwritten in $DEFAULT
-
-if [ -f "$DEFAULT" ]; then
-  . "$DEFAULT"
-fi
-
-# Define other required variables
-PID_FILE=${LS_PIDFILE}
-
-DAEMON="/opt/logstash/bin/logstash"
-DAEMON_OPTS="agent -f ${LS_CONF_DIR} -l ${LS_LOG_FILE} ${LS_OPTS}"
-
-#
-# Function that starts the daemon/service
-#
-do_start()
-{
-
-  if [ -z "$DAEMON" ]; then
-    echo "not found - $DAEMON"
-    exit 1
-  fi
-
-  if pidofproc -p "$PID_FILE" >/dev/null; then
-    exit 0
-  fi
-
-  # Prepare environment
-  HOME="${HOME:-$LS_HOME}"
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  ulimit -n ${LS_OPEN_FILES}
-  cd "${LS_HOME}"
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-  test -n "${JAVACMD}" && export JAVACMD
-
-  nice -n ${LS_NICE} runuser -s /bin/sh -c "exec $DAEMON $DAEMON_OPTS" ${LS_USER} >> $LS_LOG_FILE 2>&1 < /dev/null &
-
-  RETVAL=$?
-  local PID=$!
-  # runuser forks rather than execing our process.
-  usleep 500000
-  JAVA_PID=$(ps axo ppid,pid | awk -v "ppid=$PID" '$1==ppid {print $2}')
-  PID=${JAVA_PID:-$PID}
-  echo $PID > $PID_FILE
-  [ "$PID" = "$JAVA_PID" ] && success
-}
-
-#
-# Function that stops the daemon/service
-#
-do_stop()
-{
-    killproc -p $PID_FILE $DAEMON
-    RETVAL=$?
-    echo
-    [ $RETVAL = 0 ] && rm -f ${PID_FILE}
-}
-
-case "$1" in
-  start)
-    echo -n "Starting $DESC: "
-    do_start
-    touch /var/run/logstash/$NAME
-    ;;
-  stop)
-    echo -n "Stopping $DESC: "
-    do_stop
-    rm /var/run/logstash/$NAME
-    ;;
-  restart|reload)
-    echo -n "Restarting $DESC: "
-    do_stop
-    do_start
-    ;;
-  status)
-    echo -n "$DESC"
-    status -p $PID_FILE
-    exit $?
-    ;;
-  *)
-    echo "Usage: $SCRIPTNAME {start|stop|status|restart}" >&2
-    exit 3
-    ;;
-esac
-
-echo
-exit 0
diff --git a/pkg/logstash.upstart.ubuntu b/pkg/logstash.upstart.ubuntu
deleted file mode 100644
index 482c53d7bf3..00000000000
--- a/pkg/logstash.upstart.ubuntu
+++ /dev/null
@@ -1,48 +0,0 @@
-# logstash - agent instance
-#
-
-description     "logstash agent"
-
-start on virtual-filesystems
-stop on runlevel [06]
-
-# Respawn it if the process exits
-respawn
-
-# We're setting high here, we'll re-limit below.
-limit nofile 65550 65550
-
-setuid logstash
-setgid logstash
-
-# You need to chdir somewhere writable because logstash needs to unpack a few
-# temporary files on startup.
-console log
-script
-  # Defaults
-  PATH=/bin:/usr/bin
-  LS_HOME=/var/lib/logstash
-  LS_HEAP_SIZE="1g"
-  LS_LOG_FILE=/var/log/logstash/logstash.log
-  LS_USE_GC_LOGGING=""
-  LS_GC_LOG_FILE=""
-  LS_CONF_DIR=/etc/logstash/conf.d
-  LS_OPEN_FILES=16384
-  LS_NICE=19
-  LS_OPTS=""
-
-  # Override our defaults with user defaults:
-  [ -f /etc/default/logstash ] && . /etc/default/logstash
-
-  HOME="${HOME:-$LS_HOME}"
-  LS_JAVA_OPTS="${LS_JAVA_OPTS} -Djava.io.tmpdir=${LS_HOME}"
-  # Reset filehandle limit
-  ulimit -n ${LS_OPEN_FILES}
-  cd "${LS_HOME}"
-
-  # Export variables
-  export PATH HOME LS_HEAP_SIZE LS_JAVA_OPTS LS_USE_GC_LOGGING LS_GC_LOG_FILE
-  test -n "${JAVACMD}" && export JAVACMD
-
-  exec nice -n ${LS_NICE} /opt/logstash/bin/logstash agent -f "${LS_CONF_DIR}" -l "${LS_LOG_FILE}" ${LS_OPTS}
-end script
diff --git a/pkg/startup.options b/pkg/startup.options
new file mode 100644
index 00000000000..dcb850e66df
--- /dev/null
+++ b/pkg/startup.options
@@ -0,0 +1,52 @@
+################################################################################
+# These settings are ONLY used by $LS_HOME/bin/system-install to create a custom
+# startup script for Logstash.  It should automagically use the init system
+# (systemd, upstart, sysv, etc.) that your Linux distribution uses.
+#
+# After changing anything here, you need to re-run $LS_HOME/bin/system-install
+# as root to push the changes to the init script.
+################################################################################
+
+# Override Java location
+JAVACMD=/usr/bin/java
+
+# Set a home directory
+LS_HOME=/usr/share/logstash
+
+# logstash settings directory, the path which contains logstash.yml
+LS_SETTINGS_DIR=/etc/logstash
+
+# Arguments to pass to logstash
+LS_OPTS="--path.settings ${LS_SETTINGS_DIR}"
+
+# Arguments to pass to java
+LS_JAVA_OPTS=""
+
+# pidfiles aren't used the same way for upstart and systemd; this is for sysv users.
+LS_PIDFILE=/var/run/logstash.pid
+
+# user and group id to be invoked as
+LS_USER=logstash
+LS_GROUP=logstash
+
+# Enable GC logging by uncommenting the appropriate lines in the GC logging
+# section in jvm.options
+LS_GC_LOG_FILE=/var/log/logstash/gc.log
+
+# Open file limit
+LS_OPEN_FILES=16384
+
+# Nice level
+LS_NICE=19
+
+# Change these to have the init script named and described differently
+# This is useful when running multiple instances of Logstash on the same
+# physical box or vm
+SERVICE_NAME="logstash"
+SERVICE_DESCRIPTION="logstash"
+
+# If you need to run a command or script before launching Logstash, put it
+# between the lines beginning with `read` and `EOM`, and uncomment those lines.
+###
+## read -r -d '' PRESTART << EOM
+## EOM
diff --git a/pkg/ubuntu/after-install.sh b/pkg/ubuntu/after-install.sh
index bcecadf8af7..8c521d50a59 100644
--- a/pkg/ubuntu/after-install.sh
+++ b/pkg/ubuntu/after-install.sh
@@ -1,6 +1,11 @@
 #!/bin/sh
 
-chown -R logstash:logstash /opt/logstash
-chown logstash /var/log/logstash
+chown -R logstash:logstash /usr/share/logstash
+chown -R logstash /var/log/logstash
 chown logstash:logstash /var/lib/logstash
-chmod 0644 /etc/logrotate.d/logstash
+sed -i \
+  -e 's|# path.config:|path.config: /etc/logstash/conf.d|' \
+  -e 's|# path.logs:|path.logs: /var/log/logstash|' \
+  -e 's|# path.data:|path.data: /var/lib/logstash|' \
+  /etc/logstash/logstash.yml
+/usr/share/logstash/bin/system-install /etc/logstash/startup.options
diff --git a/pkg/ubuntu/before-install.sh b/pkg/ubuntu/before-install.sh
index 45ef4e40f1f..03cf86125a9 100644
--- a/pkg/ubuntu/before-install.sh
+++ b/pkg/ubuntu/before-install.sh
@@ -7,6 +7,6 @@ fi
 
 # create logstash user
 if ! getent passwd logstash >/dev/null; then
-  useradd -M -r -g logstash -d /var/lib/logstash \
+  useradd -M -r -g logstash -d /usr/share/logstash \
     -s /usr/sbin/nologin -c "LogStash Service User" logstash
 fi
diff --git a/pkg/ubuntu/before-remove.sh b/pkg/ubuntu/before-remove.sh
index a3f911e60ea..0384e74ffca 100644
--- a/pkg/ubuntu/before-remove.sh
+++ b/pkg/ubuntu/before-remove.sh
@@ -1,13 +1,38 @@
 #!/bin/sh
-
+# Ubuntu
 if [ $1 = "remove" ]; then
-  service logstash stop >/dev/null 2>&1 || true
+  # Upstart
+  if [ -r "/etc/init/logstash.conf" ]; then
+    if [ -f "/sbin/stop" ]; then
+      /sbin/stop logstash >/dev/null 2>&1 || true
+    else
+      /usr/sbin/service logstash stop >/dev/null 2>&1 || true
+    fi
+    if [ -f "/etc/init/logstash.conf" ]; then
+      rm /etc/init/logstash.conf
+    fi
+  # SYSV
+  elif [ -r "/etc/init.d/logstash" ]; then
+    /sbin/chkconfig --del logstash
+    if [ -f "/etc/init.d/logstash" ]; then
+      rm /etc/init.d/logstash
+    fi
+  # systemd
+  else
+    systemctl stop logstash >/dev/null 2>&1 || true
+    if [ -f "/etc/systemd/system/logstash-prestart.sh" ]; then
+      rm /etc/systemd/system/logstash-prestart.sh
+    fi
 
+    if [ -f "/etc/systemd/system/logstash.service" ]; then
+      rm /etc/systemd/system/logstash.service
+    fi
+  fi
   if getent passwd logstash >/dev/null ; then
     userdel logstash
   fi
 
-  if getent group logstash >/dev/null ; then
+  if getent group logstash > /dev/null ; then
     groupdel logstash
   fi
 fi
diff --git a/qa/.gitignore b/qa/.gitignore
new file mode 100644
index 00000000000..2fc713beaf3
--- /dev/null
+++ b/qa/.gitignore
@@ -0,0 +1,4 @@
+Gemfile.lock
+acceptance/.vagrant
+.vagrant
+.vm_ssh_config
diff --git a/qa/.rspec b/qa/.rspec
new file mode 100644
index 00000000000..23c67a21149
--- /dev/null
+++ b/qa/.rspec
@@ -0,0 +1,2 @@
+--format
+documentation
diff --git a/qa/Gemfile b/qa/Gemfile
new file mode 100644
index 00000000000..1919a50f272
--- /dev/null
+++ b/qa/Gemfile
@@ -0,0 +1,5 @@
+source "https://rubygems.org"
+gem "runner-tool", :git => "https://github.com/purbon/runner-tool.git"
+gem "rspec", "~> 3.1.0"
+gem "rake"
+gem "pry", :group => :test
diff --git a/qa/README.md b/qa/README.md
new file mode 100644
index 00000000000..27b37721f8e
--- /dev/null
+++ b/qa/README.md
@@ -0,0 +1,201 @@
+## Acceptance test Framework
+
+Welcome to the acceptance test framework for logstash, in this small
+README we're going to describe it's features and the necessary steps you will need to
+follow to setup your environment.
+
+### Setup your environment
+
+In summary this test framework is composed of:
+
+* A collection of rspec helpers and matchers that make creating tests
+  easy.
+* This rspecs helpers execute commands over SSH to a set of machines.
+* The tests are run, for now, as vagrant (virtualbox provided) machines.
+
+As of this, you need to have installed:
+
+* The latest version vagrant (=> 1.8.1)
+* Virtualbox as VM provider (=> 5.0)
+
+Is important to notice that the first time you set everything up, or when a
+new VM is added, there is the need to download the box (this will
+take a while depending on your internet speed).
+
+### Running Tests
+
+It is possible to run the full suite of the acceptance test with the codebase by 
+running the command `ci/ci_acceptance.sh`, this command will generate the artifacts, bootstrap
+the VM and run the tests.
+
+This test are based on a collection of Vagrant defined VM's where the
+different test are going to be executed, so first setup necessary is to
+have vagrant properly available, see https://www.vagrantup.com/ for
+details on how to install it.
+
+_Inside the `qa` directory_
+
+First of all execute the command `bundle` this will pull the necessary
+dependencies in your environment, after this is done, this is the collection of task available for you:
+
+```
+skywalker% rake -T
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+rake qa:vm:halt[platform]           # Halt all VM's involved in the acceptance test round
+rake qa:vm:setup[platform]          # Bootstrap all the VM's used for this tests
+rake qa:vm:ssh_config               # Generate a valid ssh-config
+```
+
+Important to be aware that using any of this commands:
+
+```
+rake qa:acceptance:all              # Run all acceptance
+rake qa:acceptance:debian           # Run acceptance test in debian machines
+rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+rake qa:acceptance:suse             # Run acceptance test in suse machines
+```
+
+before you *will have to bootstrap* all selected machines, you can do
+that using the `rake qa:vm:setup[platform]` task. This is done like this
+as bootstrap imply setting up the VM'S and this might take some time and
+you might only want to this once.
+
+In the feature we might add new rake tasks to do all at once, but for now you can use the script under
+`ci/ci_acceptance.sh` to do all at once.
+
+For local testing purposes, is recommended to not run all together, pick your target and run with the single machine command, If you're willing to run on single one, you should use:
+
+```
+rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+```
+
+### How to run tests
+
+If you are *running this test for first time*, you will need to setup
+your VM's first, you can do that using either `vagrant up` or `rake qa:vm:setup[platform]`. 
+
+In this framework we're using ssh to connect to a collection of Vagrant
+machines, so first and most important is to generate a valid ssh config
+file, this could be done running `rake qa:vm:ssh_config`. When this task
+is finished a file named `.vm_ssh_config` will be generated with all the
+necessary information to connect with the different machines.
+
+Now is time to run your test and to do that we have different options:
+
+* rake qa:acceptance:all              # Run all acceptance
+* rake qa:acceptance:debian           # Run acceptance test in debian machines
+* rake qa:acceptance:redhat           # Run acceptance test in redhat machines
+* rake qa:acceptance:suse             # Run acceptance test in suse machines
+* rake qa:acceptance:single[machine]  # Run one single machine acceptance test
+
+Generally speaking this are complex tests so they take a long time to
+finish completely, if you look for faster feedback see at the end of this
+README how to run fewer tests.
+
+## Architecture of the Framework
+
+If you wanna know more about how this framework works, here is your
+section of information.
+
+### Directory structure
+
+* ```acceptance/``` here it goes all the specs definitions.
+* ```config```  inside you can find all config files, for now only the
+  platform definition.
+* ```rspec``` here stay all framework parts necessary to get the test
+  running, you will find the commands, the rspec matchers and a
+collection of useful helpers for your test.
+* ```sys``` a collection of bash scripts used to bootstrap the machines.
+* ```vagrant``` classes and modules used to help us running vagrant.
+
+### The platform configuration file
+
+Located inside the config directory there is the platforms.json which is used to define the different platforms we test with.
+Important bits here are:
+
+* `latest` key defines the latest published version of LS release which is used to test the package upgrade scenario.
+* inside the `platforms` key you will find the list of current available
+  OS we tests with, this include the box name, their type and if they
+have to go under specific bootstrap scripts (see ```specific: true ```
+in the platform definition).
+
+This file is the one that you will use to know about differnt OS's
+testes, add new ones, etc..
+
+### I want to add a test, what should I do?
+
+To add a test you basically should start by the acceptance directory,
+here you will find an already created tests, most important locations
+here are:
+
+* ```lib``` here is where the tests are living. If a test is not going
+  to be reused it should be created here.
+* ```shared_examples``` inside that directory should be living all tests
+  that could be reused in different scenarios, like you can see the CLI
+ones.
+
+but we want to write tests, here is an example of how do they look like,
+including the different moving parts we encounter in the framework.
+
+
+```
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    ##
+    # ServiceTester::Artifact is the component used to interact with the
+    # destination machineri and the one that keep the necessary logic
+    # for it.
+    ##
+
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+
+    ## your test code goes here.
+  end
+```
+
+this is important because as you know we test with different machines,
+so the build out artifact will be the component necessary to run the
+actions with the destination machine.
+
+but this is the main parts, to run your test you need the framework
+located inside the ```rspec``` directory. Here you will find a
+collection of commands, properly organized per operating system, that
+will let you operate and get your tests done. But don't freak out, we
+got all logic necessary to select the right one for your test.
+
+You'll probably find enough supporting classes for different platforms, but if not, feel free to add it.
+
+FYI, this is how a command looks like:
+
+```
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+  end
+  ```
+this is how we run operations and wrap them as ruby code.
+
+### Running a test (detailed level)
+
+There is also the possibility to run your tests with more granularity by
+using the `rspec` command, this will let you for example run a single
+tests, a collection of them using filtering, etc.
+
+Check https://relishapp.com/rspec/rspec-core/v/3-4/docs/command-line for more details, but here is a quick cheat sheet to run them:
+
+# Run the examples that get "is installed" in their description
+
+*  bundle exec rspec acceptance/spec -e "is installed" 
+
+# Run the example defined at line 11
+
+*  bundle exec rspec acceptance/spec/lib/artifact_operation_spec.rb:11
diff --git a/qa/Rakefile b/qa/Rakefile
new file mode 100644
index 00000000000..fe52f107d9c
--- /dev/null
+++ b/qa/Rakefile
@@ -0,0 +1,76 @@
+require "rspec"
+require "rspec/core/runner"
+require "rspec/core/rake_task"
+require_relative "vagrant/helpers"
+require_relative "platform_config"
+
+platforms = PlatformConfig.new
+
+task :spec    => 'spec:all'
+task :default => :spec
+
+namespace :qa do
+
+  namespace :vm do
+
+    def user_feedback_string_for(action, platform, machines, options={})
+      experimental_string = options["experimental"] ? "experimental" : "non experimental"
+      message  = "#{action} all #{experimental_string} VM's defined in acceptance/Vagrantfile"
+      "#{message} for #{platform}: #{machines}" if !platform.nil?
+    end
+
+    desc "Generate a valid ssh-config"
+    task :ssh_config do
+      require "json"
+      raw_ssh_config    = LogStash::VagrantHelpers.fetch_config.stdout.split("\n");
+      parsed_ssh_config = LogStash::VagrantHelpers.parse(raw_ssh_config)
+      File.write(".vm_ssh_config", parsed_ssh_config.to_json)
+    end
+
+    desc "Bootstrap all the VM's used for this tests"
+    task :setup, :platform do |t, args|
+      config   = PlatformConfig.new
+      experimental = (ENV['LS_QA_EXPERIMENTAL_OS'].to_s.downcase || "false") == "true"
+      machines = config.select_names_for(args[:platform], {"experimental" => experimental})
+
+      puts user_feedback_string_for("bootstraping", args[:platform], machines, {"experimental" => experimental})
+
+      options = {:debug => ENV['LS_QA_DEBUG']}
+      LogStash::VagrantHelpers.destroy(machines, options)
+      LogStash::VagrantHelpers.bootstrap(machines, options)
+    end
+
+    desc "Halt all VM's involved in the acceptance test round"
+    task :halt, :platform do |t, args|
+      config   = PlatformConfig.new
+      experimental = (ENV['LS_QA_EXPERIMENTAL_OS'].to_s.downcase || "false") == "true"
+      machines = config.select_names_for(args[:platform], {"experimental" => experimental})
+
+      puts user_feedback_string_for("halting", args[:platform], machines, {"experimental" => experimental})
+      options = {:debug => ENV['LS_QA_DEBUG']}
+
+      LogStash::VagrantHelpers.halt(machines, options)
+    end
+  end
+
+  namespace :acceptance do
+    desc "Run all acceptance"
+    task :all do
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/*_spec.rb"]]))
+    end
+
+    platforms.types.each do |type|
+      desc "Run acceptance test in #{type} machines"
+      task type do
+        ENV['LS_TEST_PLATFORM']=type
+        exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/*_spec.rb"]]))
+      end
+    end
+
+    desc "Run one single machine acceptance test"
+    task :single, :machine do |t, args|
+      ENV['LS_VAGRANT_HOST']  = args[:machine]
+      exit(RSpec::Core::Runner.run([Rake::FileList["acceptance/spec/lib/**/**/*_spec.rb"]]))
+    end
+  end
+end
diff --git a/qa/Vagrantfile b/qa/Vagrantfile
new file mode 100644
index 00000000000..b7da73064f6
--- /dev/null
+++ b/qa/Vagrantfile
@@ -0,0 +1,28 @@
+# -*- mode: ruby -*-
+# vi: set ft=ruby :
+require_relative "./platform_config.rb"
+
+Vagrant.configure(2) do |config|
+  platforms = PlatformConfig.new
+
+  platforms.each do |platform|
+    config.vm.define platform.name do |machine|
+      machine.vm.box = platform.box
+      machine.vm.provider "virtualbox" do |v|
+        v.memory = 2096
+        v.cpus = 4
+      end
+      machine.vm.synced_folder "../build", "/logstash-build", create: true
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.privileged
+        sh.privileged = true
+      end
+
+      machine.vm.provision :shell do |sh|
+        sh.path = platform.bootstrap.non_privileged
+        sh.privileged = false
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/config_helper.rb b/qa/acceptance/spec/config_helper.rb
new file mode 100644
index 00000000000..3d9730c2d2d
--- /dev/null
+++ b/qa/acceptance/spec/config_helper.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require "json"
+
+module SpecsHelper
+
+  def self.configure(vagrant_boxes)
+    setup_config = JSON.parse(File.read(File.join(File.dirname(__FILE__), "..", "..", ".vm_ssh_config")))
+    boxes        = vagrant_boxes.inject({}) do |acc, v|
+      acc[v.name] = v.type
+      acc
+    end
+    ServiceTester.configure do |config|
+      config.servers = []
+      config.lookup  = {}
+      setup_config.each do |host_info|
+        next unless boxes.keys.include?(host_info["host"])
+        url = "#{host_info["hostname"]}:#{host_info["port"]}"
+        config.servers << url
+        config.lookup[url] = {"host" => host_info["host"], "type" => boxes[host_info["host"]] }
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/lib/artifact_operation_spec.rb b/qa/acceptance/spec/lib/artifact_operation_spec.rb
new file mode 100644
index 00000000000..fdebf23de31
--- /dev/null
+++ b/qa/acceptance/spec/lib/artifact_operation_spec.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require_relative '../spec_helper'
+require_relative '../shared_examples/installed'
+require_relative '../shared_examples/running'
+require_relative '../shared_examples/updated'
+
+# This tests verify that the generated artifacts could be used properly in a relase, implements https://github.com/elastic/logstash/issues/5070
+describe "artifacts operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "installable", logstash
+    it_behaves_like "updated", logstash
+  end
+end
diff --git a/qa/acceptance/spec/lib/cli_operation_spec.rb b/qa/acceptance/spec/lib/cli_operation_spec.rb
new file mode 100644
index 00000000000..43718dc488c
--- /dev/null
+++ b/qa/acceptance/spec/lib/cli_operation_spec.rb
@@ -0,0 +1,23 @@
+# encoding: utf-8
+require_relative "../spec_helper"
+require_relative "../shared_examples/cli/logstash/version"
+require_relative "../shared_examples/cli/logstash-plugin/install"
+require_relative "../shared_examples/cli/logstash-plugin/list"
+require_relative "../shared_examples/cli/logstash-plugin/uninstall"
+require_relative "../shared_examples/cli/logstash-plugin/remove"
+require_relative "../shared_examples/cli/logstash-plugin/update"
+
+# This is the collection of test for the CLI interface, this include the plugin manager behaviour, 
+# it also include the checks for other CLI options.
+describe "CLI operation" do
+  config = ServiceTester.configuration
+  config.servers.each do |address|
+    logstash = ServiceTester::Artifact.new(address, config.lookup[address])
+    it_behaves_like "logstash version", logstash
+    it_behaves_like "logstash install", logstash
+    it_behaves_like "logstash list", logstash
+    it_behaves_like "logstash uninstall", logstash
+    it_behaves_like "logstash remove", logstash
+    it_behaves_like "logstash update", logstash
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
new file mode 100644
index 00000000000..1ef1e8aa90f
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/install.rb
@@ -0,0 +1,69 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash install" do |logstash|
+  before(:each) do
+    logstash.install({:version => LOGSTASH_VERSION})
+  end
+
+  after(:each) do
+    logstash.uninstall
+  end
+
+  describe "on #{logstash.hostname}" do
+    context "with a direct internet connection" do
+      context "when the plugin exist" do
+        context "from a local `.GEM` file" do
+          let(:gem_name) { "logstash-filter-qatest-0.1.1.gem" }
+          let(:gem_path_on_vagrant) { "/tmp/#{gem_name}" }
+          before(:each) do
+            logstash.download("https://rubygems.org/gems/#{gem_name}", gem_path_on_vagrant)
+          end
+
+          after(:each) { logstash.delete_file(gem_path_on_vagrant) }
+
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install #{gem_path_on_vagrant}")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-dns")
+          end
+        end
+
+        context "when fetching a gem from rubygems" do
+
+          it "successfully install the plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest")
+          end
+
+          it "successfully install the plugin when verification is disabled" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest")
+          end
+
+          it "fails when installing a non logstash plugin" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install  bundler")
+            expect(command).not_to install_successfully
+          end
+
+          it "allow to install a specific version" do
+            command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify --version 0.1.0 logstash-filter-qatest")
+            expect(command).to install_successfully
+            expect(logstash).to have_installed?("logstash-filter-qatest", "0.1.0")
+          end
+        end
+      end
+
+      context "when the plugin doesnt exist" do
+        it "fails to install and report an error" do
+          command = logstash.run_command_in_path("bin/logstash-plugin install --no-verify logstash-output-impossible-plugin")
+          expect(command.stderr).to match(/Plugin not found, aborting/)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
new file mode 100644
index 00000000000..5f1e00fd2df
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/list.rb
@@ -0,0 +1,48 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash list" do |logstash|
+  describe "logstash-plugin list on #{logstash.hostname}" do
+    before(:all) do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after(:all) do
+      logstash.uninstall
+    end
+
+    context "without a specific plugin" do
+      it "display a list of plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "display a list of installed plugins" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --installed")
+        expect(result.stdout.split("\n").size).to be > 1
+      end
+
+      it "list the plugins with their versions" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose")
+        result.stdout.split("\n").each do |plugin|
+          expect(plugin).to match(/^logstash-\w+-\w+\s\(\d+\.\d+.\d+(.\w+)?\)/)
+        end
+      end
+    end
+
+    context "with a specific plugin" do
+      let(:plugin_name) { "logstash-input-stdin" }
+      it "list the plugin and display the plugin name" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name}$/)
+      end
+
+      it "list the plugin with his version" do
+        result = logstash.run_command_in_path("bin/logstash-plugin list --verbose #{plugin_name}")
+        expect(result).to run_successfully_and_output(/^#{plugin_name} \(\d+\.\d+.\d+\)/)
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb
new file mode 100644
index 00000000000..08b6aad4cbe
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/remove.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash remove" do |logstash|
+  describe "logstash-plugin remove on #{logstash.hostname}" do
+    before :each do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    context "when the plugin isn't installed" do
+      it "fails to remove it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin remove logstash-filter-qatest")
+        expect(result.stderr).to match(/ERROR: Remove Aborted, message: This plugin has not been previously installed, aborting/)
+      end
+    end
+
+    # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
+    xcontext "when the plugin is installed" do
+      it "succesfully removes it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+        expect(logstash).to have_installed?("logstash-filter-qatest")
+
+        result = logstash.run_command_in_path("bin/logstash-plugin remove logstash-filter-qatest")
+        expect(result.stdout).to match(/^Removing logstash-filter-qatest/)
+        expect(logstash).not_to have_installed?("logstash-filter-qatest")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
new file mode 100644
index 00000000000..2b14b7f09c1
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/uninstall.rb
@@ -0,0 +1,35 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+require "fileutils"
+
+shared_examples "logstash uninstall" do |logstash|
+  describe "logstash-plugin uninstall on #{logstash.hostname}" do
+    before :each do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    context "when the plugin isn't installed" do
+      it "fails to uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stderr).to match(/ERROR: Uninstall Aborted, message: This plugin has not been previously installed, aborting/)
+      end
+    end
+
+    # Disabled because of this bug https://github.com/elastic/logstash/issues/5286
+    xcontext "when the plugin is installed" do
+      it "succesfully uninstall it" do
+        result = logstash.run_command_in_path("bin/logstash-plugin install logstash-filter-qatest")
+        expect(logstash).to have_installed?("logstash-filter-qatest")
+
+        result = logstash.run_command_in_path("bin/logstash-plugin uninstall logstash-filter-qatest")
+        expect(result.stdout).to match(/^Uninstalling logstash-filter-qatest/)
+        expect(logstash).not_to have_installed?("logstash-filter-qatest")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
new file mode 100644
index 00000000000..12d317db325
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash-plugin/update.rb
@@ -0,0 +1,40 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash update" do |logstash|
+  describe "logstash-plugin update on #{logstash.hostname}" do
+    before :each do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :each do
+      logstash.uninstall
+    end
+
+    let(:plugin_name) { "logstash-filter-qatest" }
+    let(:previous_version) { "0.1.0" }
+
+    before do
+      logstash.run_command_in_path("bin/logstash-plugin install --no-verify --version #{previous_version} #{plugin_name}")
+      # Logstash wont update when we have a pinned versionin the gemfile so we remove them
+      logstash.replace_in_gemfile(',[[:space:]]"0.1.0"', "")
+      expect(logstash).to have_installed?(plugin_name, previous_version)
+    end
+
+    context "update a specific plugin" do
+      it "has executed succesfully" do
+        cmd = logstash.run_command_in_path("bin/logstash-plugin update --no-verify #{plugin_name}")
+        expect(cmd.stdout).to match(/Updating #{plugin_name}/)
+        expect(logstash).not_to have_installed?(plugin_name, previous_version)
+      end
+    end
+
+    context "update all the plugins" do
+      it "has executed succesfully" do
+        logstash.run_command_in_path("bin/logstash-plugin update --no-verify")
+        expect(logstash).to have_installed?(plugin_name, "0.1.1")
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/cli/logstash/version.rb b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
new file mode 100644
index 00000000000..ae6843313d7
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/cli/logstash/version.rb
@@ -0,0 +1,28 @@
+# encoding: utf-8
+require_relative "../../../spec_helper"
+require "logstash/version"
+
+shared_examples "logstash version" do |logstash|
+  describe "logstash --version" do
+    before :all do
+      logstash.install({:version => LOGSTASH_VERSION})
+    end
+
+    after :all do
+      logstash.uninstall
+    end
+
+    context "on #{logstash.hostname}" do
+      it "returns the right logstash version" do
+        result = logstash.run_command_in_path("bin/logstash --version")
+        expect(result).to run_successfully_and_output(/#{LOGSTASH_VERSION}/)
+      end
+      context "when also using the --path.settings argument" do
+        it "returns the right logstash version" do
+          result = logstash.run_command_in_path("bin/logstash --path.settings=/etc/logstash --version")
+          expect(result).to run_successfully_and_output(/#{LOGSTASH_VERSION}/)
+        end
+      end
+    end
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/installed.rb b/qa/acceptance/spec/shared_examples/installed.rb
new file mode 100644
index 00000000000..3d058134fc3
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/installed.rb
@@ -0,0 +1,26 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if a package is possible to be installed without errors.
+RSpec.shared_examples "installable" do |logstash|
+
+  before(:each) do
+    logstash.uninstall
+    logstash.install({:version => LOGSTASH_VERSION})
+  end
+
+  it "is installed on #{logstash.hostname}" do
+    expect(logstash).to be_installed
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+  it "is removable on #{logstash.hostname}" do
+    logstash.uninstall
+    expect(logstash).to be_removed
+  end
+end
diff --git a/qa/acceptance/spec/shared_examples/running.rb b/qa/acceptance/spec/shared_examples/running.rb
new file mode 100644
index 00000000000..55d18507cba
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/running.rb
@@ -0,0 +1,17 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# Test if an installed package can actually be started and runs OK.
+RSpec.shared_examples "runnable" do |logstash|
+
+  before(:each) do
+    logstash.install({:version => LOGSTASH_VERSION})
+  end
+
+  it "is running on #{logstash.hostname}" do
+    logstash.start_service
+    expect(logstash).to be_running
+    logstash.stop_service
+  end
+
+end
diff --git a/qa/acceptance/spec/shared_examples/updated.rb b/qa/acceptance/spec/shared_examples/updated.rb
new file mode 100644
index 00000000000..f6c9e81319a
--- /dev/null
+++ b/qa/acceptance/spec/shared_examples/updated.rb
@@ -0,0 +1,27 @@
+require_relative '../spec_helper'
+require          'logstash/version'
+
+# This test checks if the current package could used to update from the latest version released.
+RSpec.shared_examples "updated" do |logstash|
+
+  before(:all) { logstash.uninstall }
+  after(:all)  do
+    logstash.stop_service # make sure the service is stopped
+    logstash.uninstall #remove the package to keep uniform state
+  end
+
+  before(:each) do
+    options={:version => LOGSTASH_LATEST_VERSION, :snapshot => false, :base => "./" }
+    logstash.install(options) # make sure latest version is installed
+  end
+
+  it "can be updated an run on #{logstash.hostname}" do
+    expect(logstash).to be_installed
+    # Performing the update
+    logstash.install({:version => LOGSTASH_VERSION})
+    expect(logstash).to be_installed
+    # starts the service to be sure it runs after the upgrade
+    logstash.start_service
+    expect(logstash).to be_running
+  end
+end
diff --git a/qa/acceptance/spec/spec_helper.rb b/qa/acceptance/spec/spec_helper.rb
new file mode 100644
index 00000000000..99d6bb9a141
--- /dev/null
+++ b/qa/acceptance/spec/spec_helper.rb
@@ -0,0 +1,33 @@
+# encoding: utf-8
+require 'runner-tool'
+require_relative '../../rspec/helpers'
+require_relative '../../rspec/matchers'
+require_relative 'config_helper'
+require_relative "../../platform_config"
+
+ROOT = File.expand_path(File.join(File.dirname(__FILE__), '..', '..', '..'))
+$LOAD_PATH.unshift File.join(ROOT, 'logstash-core/lib')
+
+RunnerTool.configure
+
+RSpec.configure do |c|
+  c.include ServiceTester
+end
+
+platform = ENV['LS_TEST_PLATFORM'] || 'all'
+experimental = (ENV['LS_QA_EXPERIMENTAL_OS'].to_s.downcase || "false") == "true"
+
+config                  = PlatformConfig.new
+LOGSTASH_LATEST_VERSION = config.latest
+
+default_vagrant_boxes = ( platform == 'all' ? config.platforms : config.filter_type(platform, {"experimental" => experimental}) )
+
+selected_boxes = if ENV.include?('LS_VAGRANT_HOST') then
+                   config.platforms.select { |p| p.name  == ENV['LS_VAGRANT_HOST'] }
+                 else
+                   default_vagrant_boxes
+                 end
+
+SpecsHelper.configure(selected_boxes)
+
+puts "[Acceptance specs] running on #{ServiceTester.configuration.hosts}" if !selected_boxes.empty?
diff --git a/qa/config/platforms.json b/qa/config/platforms.json
new file mode 100644
index 00000000000..52f8223644e
--- /dev/null
+++ b/qa/config/platforms.json
@@ -0,0 +1,18 @@
+{ 
+  "latest": "5.0.0-alpha3",
+  "platforms" : {
+    "ubuntu-1204": { "box": "elastic/ubuntu-12.04-x86_64", "type": "debian" },
+    "ubuntu-1404": { "box": "elastic/ubuntu-14.04-x86_64", "type": "debian", "specific": true },
+    "ubuntu-1604": { "box": "elastic/ubuntu-16.04-x86_64", "type": "debian", "experimental": true },
+    "centos-6": { "box": "elastic/centos-6-x86_64", "type": "redhat" },
+    "centos-7": { "box": "elastic/centos-7-x86_64", "type": "redhat" },
+    "oel-6": { "box": "elastic/oraclelinux-6-x86_64", "type": "redhat" },
+    "oel-7": { "box": "elastic/oraclelinux-7-x86_64", "type": "redhat" },
+    "fedora-22": { "box": "elastic/fedora-22-x86_64", "type": "redhat", "experimental": true },
+    "fedora-23": { "box": "elastic/fedora-23-x86_64", "type": "redhat", "experimental": true },
+    "debian-8": { "box": "elastic/debian-8-x86_64", "type": "debian", "specific":  true },
+    "opensuse-13": { "box": "elastic/opensuse-13-x86_64", "type": "suse" },
+    "sles-11": { "box": "elastic/sles-11-x86_64", "type": "suse", "specific": true },
+    "sles-12": { "box": "elastic/sles-12-x86_64", "type": "suse", "specific": true }
+  }
+}
diff --git a/qa/integration/.gitignore b/qa/integration/.gitignore
new file mode 100644
index 00000000000..e7775797e47
--- /dev/null
+++ b/qa/integration/.gitignore
@@ -0,0 +1,2 @@
+/services/installed
+/fixtures/certificates
diff --git a/qa/integration/.rspec b/qa/integration/.rspec
new file mode 100644
index 00000000000..83a4f920292
--- /dev/null
+++ b/qa/integration/.rspec
@@ -0,0 +1,2 @@
+--default-path specs/
+--format documentation
\ No newline at end of file
diff --git a/qa/integration/Gemfile b/qa/integration/Gemfile
new file mode 100644
index 00000000000..3be9c3cd812
--- /dev/null
+++ b/qa/integration/Gemfile
@@ -0,0 +1,2 @@
+source "https://rubygems.org"
+gemspec
diff --git a/qa/integration/README.md b/qa/integration/README.md
new file mode 100644
index 00000000000..2b411cf5156
--- /dev/null
+++ b/qa/integration/README.md
@@ -0,0 +1,41 @@
+## Logstash Integration Tests aka RATS
+
+These set of tests are full integration tests as in: they can start LS from a binary, run configs using `-e` and can use any external services like Kafka, ES and S3. This framework is hybrid -- a combination of bash scripts (to mainly setup services), Ruby service files, and RSpec. All test assertions are done in RSpec.
+
+No VMs, all tests run locally.
+
+## Dependencies
+* An existing Logstash binary, defaults to `LS_HOME/build/logstash-<version>`
+* `rspec`
+
+## Preparing a test run
+
+1. If you already have a LS binary in `LS_HOME/build/logstash-<version>`, skip to step 5
+2. From Logstash git source directory or `LS_HOME` run `rake artifact:tar` to build a package
+2. Untar the newly built package
+3. `cd build`
+4. `tar xvf logstash-<version>.tar.gz`
+5. `cd LS_HOME/qa/integration`
+6. `bundle install`: This will install test dependency gems.
+
+You are now ready to run any tests from `qa/integration`.
+* Run all tests: `rspec specs/*`
+* Run single test: `rspec specs/es_output_how_spec.rb`
+
+### Directory Layout
+
+* `fixtures`: In this dir you will test settings in form of `test_name.yml`. Here you specify services to run, LS config, test specific scripts ala `.travis.yml`
+* `services`: This dir has bash scripts that download and bootstrap binaries for services. This is where services like ES will be downloaded and run from. Service can have 3 files: `<service>_setup.sh`, `<service>_teardown.sh` and `<service>`.rb. The bash scripts deal with downloading and bootstrapping, but the ruby source will trigger them from the test as a shell out (using backticks). The tests are blocked until the setup/teardown completes. For example, Elasticsearch service has `elasticsearch_setup.sh`, `elasticsearch_teardown.sh` and `elasticsearch.rb`. The service name in yml is "elasticsearch".
+* `framework`: Test framework source code.
+* `specs`: Rspec tests that use services and validates stuff
+
+### Adding a new test
+
+1. Creating a new test -- lets use as example. Call it "test_file_input" which brings up LS to read from a file and assert file contents (file output) were as expected.
+2. You'll have to create a yml file in `fixtures` called `test_file_input_spec.yml`. Here you define any external services you need and any LS config.
+3. Create a corresponding `test_file_input_spec.rb` in `specs` folder and use the `fixtures` object to get all services, config etc. The `.yml` and rspec file has to be the same name for the settings to be picked up. You can start LS inside the tests and assume all external services have already been started.
+4. Write rspec code to validate.
+
+## Future Improvements
+
+1. Perform setup and teardown from Ruby and get rid of bash files altogether.
diff --git a/qa/integration/fixtures/01_logstash_bin_smoke_spec.yml b/qa/integration/fixtures/01_logstash_bin_smoke_spec.yml
new file mode 100644
index 00000000000..2ea4dc401a5
--- /dev/null
+++ b/qa/integration/fixtures/01_logstash_bin_smoke_spec.yml
@@ -0,0 +1,9 @@
+---
+services:
+  - logstash
+config: |-
+ input {
+    tcp {
+      port => '<%=options[:port]%>'
+    }
+  } 
\ No newline at end of file
diff --git a/qa/integration/fixtures/beats_input_spec.yml b/qa/integration/fixtures/beats_input_spec.yml
new file mode 100644
index 00000000000..4c452f30647
--- /dev/null
+++ b/qa/integration/fixtures/beats_input_spec.yml
@@ -0,0 +1,35 @@
+---
+services:
+  - filebeat
+  - logstash
+config:
+  without_tls: |-
+    input {
+      beats {
+        port => 5044
+      }
+    }
+    output {}
+  tls_server_auth: |-
+    input {
+      beats {
+        ssl => true
+        port => 5044
+        ssl_certificate => '<%=options[:ssl_certificate]%>'
+        ssl_key => '<%=options[:ssl_key]%>'
+      }
+    }
+    output {}
+  tls_mutual_auth: |-
+    input {
+      beats {
+        ssl => true
+        port => 5044
+        ssl_certificate => '<%=options[:ssl_certificate]%>'
+        ssl_key => '<%=options[:ssl_key]%>'
+        ssl_verify_mode => "peer"
+      }
+    }
+    output {}
+input: how_sample.input
+teardown_script:
diff --git a/qa/integration/fixtures/env_variables_config_spec.yml b/qa/integration/fixtures/env_variables_config_spec.yml
new file mode 100644
index 00000000000..008e2c95f2f
--- /dev/null
+++ b/qa/integration/fixtures/env_variables_config_spec.yml
@@ -0,0 +1,21 @@
+---
+services:
+  - logstash
+config: |-
+ input {
+    tcp {
+      port => "${TEST_ENV_TCP_PORT}"
+    }
+  }
+  filter {
+    mutate {
+      add_tag => [ "blah", "${TEST_ENV_TAG}" ]
+    }
+  }
+  output {
+    file {
+      path => "${TEST_ENV_PATH}/logstash_env_test.log"
+      flush_interval => 0
+      codec => line { format => "%{message} %{tags}" }
+    }
+  }
\ No newline at end of file
diff --git a/qa/integration/fixtures/es_output_how_spec.yml b/qa/integration/fixtures/es_output_how_spec.yml
new file mode 100644
index 00000000000..cf2c436b6be
--- /dev/null
+++ b/qa/integration/fixtures/es_output_how_spec.yml
@@ -0,0 +1,34 @@
+---
+services:
+  - logstash
+  - elasticsearch
+config: |-
+  input {
+    stdin { }
+  }
+
+  filter {
+    grok {
+      match => {
+        "message" => "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}"
+      }
+    }
+
+    date {
+      match => [ "timestamp", "dd/MMM/YYYY:HH:mm:ss Z" ]
+      locale => en
+    }
+    geoip {
+      source => "clientip"
+    }
+    useragent {
+      source => "agent"
+      target => "useragent"
+    }
+  }
+  output {
+    elasticsearch {}
+  }
+
+input: how_sample.input
+teardown_script:
diff --git a/qa/integration/fixtures/how_sample.input b/qa/integration/fixtures/how_sample.input
new file mode 100644
index 00000000000..bef7ff54b4e
--- /dev/null
+++ b/qa/integration/fixtures/how_sample.input
@@ -0,0 +1,37 @@
+74.125.176.147 - - [11/Sep/2014:21:50:37 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "FeedBurner/1.0 (http://www.FeedBurner.com)"
+66.249.84.55 - - [11/Sep/2014:21:50:54 +0000] "GET / HTTP/1.1" 200 37932 "-" "Mozilla/5.0 (Windows NT 6.1; rv:6.0) Gecko/20110814 Firefox/6.0 Google favicon"
+66.249.84.55 - - [11/Sep/2014:21:50:54 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 6.1; rv:6.0) Gecko/20110814 Firefox/6.0 Google favicon"
+46.105.14.53 - - [11/Sep/2014:21:51:35 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
+194.153.217.248 - - [11/Sep/2014:21:51:56 +0000] "GET /blog/geekery/debugging-java-performance.html HTTP/1.1" 200 15796 "http://logstash.net/docs/1.3.3/life-of-an-event" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:57 +0000] "GET /style2.css HTTP/1.1" 200 4877 "http://www.semicomplete.com/blog/geekery/debugging-java-performance.html" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:57 +0000] "GET /reset.css HTTP/1.1" 200 1015 "http://www.semicomplete.com/blog/geekery/debugging-java-performance.html" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:58 +0000] "GET /favicon.ico HTTP/1.1" 200 3638 "-" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:58 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "http://www.semicomplete.com/blog/geekery/debugging-java-performance.html" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+194.153.217.248 - - [11/Sep/2014:21:51:58 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "http://www.semicomplete.com/style2.css" "Mozilla/5.0 (Windows NT 5.1; rv:27.0) Gecko/20100101 Firefox/27.0"
+159.253.145.222 - - [11/Sep/2014:21:51:53 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+213.113.233.227 - - [11/Sep/2014:21:53:14 +0000] "GET /articles/dynamic-dns-with-dhcp/ HTTP/1.1" 200 18848 "https://www.google.se/" "Mozilla/5.0 (Windows NT 6.1; WOW64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/33.0.1750.154 Safari/537.36"
+173.192.221.211 - - [11/Sep/2014:21:53:31 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+5.10.83.95 - - [11/Sep/2014:21:54:16 +0000] "GET /files/xdotool/docs/man/?C=N;O=D HTTP/1.1" 200 955 "-" "Mozilla/5.0 (compatible; AhrefsBot/5.0; +http://ahrefs.com/robot/)"
+194.103.63.154 - - [11/Sep/2014:21:54:26 +0000] "GET /favicon.ico HTTP/1.1" 304 - "-" "Mozilla/4.0 (compatible;)"
+95.30.144.170 - - [11/Sep/2014:21:56:20 +0000] "GET /presentations/vim/ HTTP/1.0" 200 20572 "http://semicomplete.com/presentations/vim/" "Opera/9.80 (Windows NT 6.1; WOW64; U; MRA 8.0 (build 5880); ru) Presto/2.10.289 Version/12.02"
+159.253.145.222 - - [11/Sep/2014:21:56:27 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+46.105.14.53 - - [11/Sep/2014:21:56:36 +0000] "GET /blog/tags/puppet?flav=rss20 HTTP/1.1" 200 14872 "-" "UniversalFeedParser/4.2-pre-314-svn +http://feedparser.org/"
+202.69.11.26 - - [11/Sep/2014:21:56:40 +0000] "GET /blog/tags/X11 HTTP/1.1" 200 32742 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+5.10.83.96 - - [11/Sep/2014:21:56:47 +0000] "GET /blog/geekery/insist-on-better-asserts.html HTTP/1.1" 200 9514 "-" "Mozilla/5.0 (compatible; AhrefsBot/5.0; +http://ahrefs.com/robot/)"
+202.69.11.26 - - [11/Sep/2014:21:56:48 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
+202.69.11.26 - - [11/Sep/2014:21:56:54 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
+202.69.11.26 - - [11/Sep/2014:21:56:57 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:57:06 +0000] "GET /images/web/2009/banner.png HTTP/1.1" 200 52315 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+174.36.228.156 - - [11/Sep/2014:21:57:28 +0000] "GET /blog HTTP/1.1" 200 37936 "-" "Mozilla/5.0 (X11; U; Linux x86_64; en-US; rv:1.9.0.19; aggregator:Spinn3r (Spinn3r 3.1); http://spinn3r.com/robot) Gecko/2010040121 Firefox/3.0.19"
+108.174.55.234 - - [11/Sep/2014:21:57:42 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "-"
+173.192.221.212 - - [11/Sep/2014:21:58:16 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+217.69.133.237 - - [11/Sep/2014:21:58:29 +0000] "GET /scripts/netcat-webserver HTTP/1.1" 304 - "-" "Mozilla/5.0 (compatible; Linux x86_64; Mail.RU_Bot/2.0; +http://go.mail.ru/help/robots)"
+202.69.11.26 - - [11/Sep/2014:21:58:52 +0000] "GET /images/googledotcom.png HTTP/1.1" 200 65748 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:59:10 +0000] "GET /blog/tags/X11 HTTP/1.1" 200 32742 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:59:15 +0000] "GET /reset.css HTTP/1.1" 200 1015 "-" "-"
+202.69.11.26 - - [11/Sep/2014:21:59:17 +0000] "GET /images/jordan-80.png HTTP/1.1" 200 6146 "-" "HUAQIN60A_6464_11B_HW|0816 (MRE/3.1.00(1536);MAUI/PA_089A_V8_0_9;BDATE/2013/08/16 14:02;LCD/240320;CHIP/MT6260;KEY/Normal;TOUCH/0;CAMERA/1;SENSOR/0;DEV/HUAQIN60A_6464_11B_HW|0816;WAP Browser/MAUI (HTTP PGDL;HTTPS);GMOBI/001;MBOUNCE/002;MOMAGIC/003;INDEX/004;SPICEI2I/005;GAMELOFT/006;) ZS628PA_089A_V8_0_9 Release/2013.08.16 WAP Browser/MAUI (HTTP PGDL; HTTPS) Profile/  Q03C1-2.40 en-US"
+202.69.11.26 - - [11/Sep/2014:21:59:19 +0000] "GET /style2.css HTTP/1.1" 200 4877 "-" "-"
+5.153.24.131 - - [11/Sep/2014:21:59:18 +0000] "GET /files/logstash/logstash-1.1.0-monolithic.jar HTTP/1.1" 200 40923996 "-" "Chef Client/10.18.2 (ruby-1.8.7-p302; ohai-6.14.0; x86_64-linux; +http://opscode.com)"
+198.46.149.143 - - [11/Sep/2014:21:59:29 +0000] "GET /blog/geekery/disabling-battery-in-ubuntu-vms.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1" 200 9316 "-" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
+198.46.149.143 - - [11/Sep/2014:21:59:29 +0000] "GET /blog/geekery/solving-good-or-bad-problems.html?utm_source=feedburner&utm_medium=feed&utm_campaign=Feed%3A+semicomplete%2Fmain+%28semicomplete.com+-+Jordan+Sissel%29 HTTP/1.1" 200 10756 "-" "Tiny Tiny RSS/1.11 (http://tt-rss.org/)"
+183.60.215.50 - - [11/Sep/2014:22:00:00 +0000] "GET /scripts/netcat-webserver HTTP/1.1" 200 182 "-" "Mozilla/5.0 (compatible; EasouSpider; +http://www.easou.com/search/spider.html)"
diff --git a/qa/integration/fixtures/kafka_input_spec.yml b/qa/integration/fixtures/kafka_input_spec.yml
new file mode 100644
index 00000000000..a7df6098eab
--- /dev/null
+++ b/qa/integration/fixtures/kafka_input_spec.yml
@@ -0,0 +1,24 @@
+---
+services:
+  - logstash
+  - kafka
+config: |-
+  input {
+    kafka {
+      topics => "logstash_topic_plain"
+      auto_offset_reset => "earliest"
+      codec => "plain"
+      group_id => "ls10"
+    }
+  }
+  output {
+    file {
+       path => "kafka_input.output"
+       flush_interval => 0
+       codec => line { format => "%{message}" }
+    }
+  }
+
+input: how_sample.input
+actual_output: kafka_input.output 
+teardown_script:
diff --git a/qa/integration/fixtures/monitoring_api_spec.yml b/qa/integration/fixtures/monitoring_api_spec.yml
new file mode 100644
index 00000000000..52bff4e9e95
--- /dev/null
+++ b/qa/integration/fixtures/monitoring_api_spec.yml
@@ -0,0 +1,4 @@
+---
+name: Metrics test
+services:
+  - logstash
diff --git a/qa/integration/fixtures/reload_config_spec.yml b/qa/integration/fixtures/reload_config_spec.yml
new file mode 100644
index 00000000000..bc01103f544
--- /dev/null
+++ b/qa/integration/fixtures/reload_config_spec.yml
@@ -0,0 +1,37 @@
+---
+services:
+  - logstash
+config:
+  initial: |-
+    input {
+      tcp {
+        port => '<%=options[:port]%>'
+      }
+    }
+    output {
+      file {
+         path => '<%=options[:file]%>'
+         flush_interval => 0
+         codec => line { format => "%{message}" }
+      }
+    }
+  reload: |-
+    input {
+      tcp {
+        port => '<%=options[:port]%>'
+      }
+    }
+    filter {
+      grok {
+        match => {
+          "message" => "%{IPORHOST:clientip} %{USER:ident} %{USER:auth} \[%{HTTPDATE:timestamp}\] \"%{WORD:verb} %{DATA:request} HTTP/%{NUMBER:httpversion}\" %{NUMBER:response:int} (?:-|%{NUMBER:bytes:int}) %{QS:referrer} %{QS:agent}"
+        }
+      }
+    }
+    output {
+      file {
+         path => '<%=options[:file]%>'
+         flush_interval => 0
+         codec => json
+      }
+    }
\ No newline at end of file
diff --git a/qa/integration/fixtures/settings_spec.yml b/qa/integration/fixtures/settings_spec.yml
new file mode 100644
index 00000000000..53be4a7f335
--- /dev/null
+++ b/qa/integration/fixtures/settings_spec.yml
@@ -0,0 +1,9 @@
+---
+services:
+  - logstash
+config: |-
+ input {
+    tcp {
+      port => '<%=options[:port]%>'
+    }
+  }  
\ No newline at end of file
diff --git a/qa/integration/framework/fixture.rb b/qa/integration/framework/fixture.rb
new file mode 100644
index 00000000000..ba5120e3ac1
--- /dev/null
+++ b/qa/integration/framework/fixture.rb
@@ -0,0 +1,81 @@
+require_relative "../services/service_locator"
+
+# A class that holds all fixtures for a given test file. This deals with
+# bootstrapping services, dealing with config files, inputs etc
+class Fixture
+  FIXTURES_DIR = File.expand_path(File.join("..", "..", "fixtures"), __FILE__)
+
+  attr_reader :input
+  attr_reader :actual_output
+  attr_reader :test_dir
+  attr_reader :settings
+
+  class TemplateContext
+    attr_reader :options
+
+    def initialize(options)
+      @options = options
+    end
+
+    def get_binding
+      binding
+    end
+  end
+
+  def initialize(test_file_location)
+    @test_file_location = test_file_location
+    @settings = TestSettings.new(@test_file_location)
+    @service_locator = ServiceLocator.new(@settings)
+    setup_services
+    @input = File.join(FIXTURES_DIR, @settings.get("input")) if @settings.is_set?("input")
+    @actual_output = @settings.get("actual_output")
+  end
+
+  def config(node = "root", options = nil)
+    if node == "root"
+      config = @settings.get("config")
+    else
+      config = @settings.get("config")[node]
+    end
+
+    if options != nil
+       ERB.new(config, nil, "-").result(TemplateContext.new(options).get_binding)
+    else
+      config
+    end
+  end
+
+  def get_service(name)
+    @service_locator.get_service(name)
+  end
+
+  def output_equals_expected?
+    FileUtils.identical?(@actual_output, @input)
+  end
+
+  def output_exists?
+    File.exists?(@actual_output)
+  end
+
+  def teardown
+    File.delete(@actual_output) if @settings.is_set?("actual_output") && output_exists?
+    puts "Tearing down services"
+    services = @settings.get("services")
+    services.each do |name|
+      @service_locator.get_service(name).teardown
+    end
+  end
+
+  def setup_services
+    puts "Setting up services"
+    services = @settings.get("services")
+    services.each do |name|
+     @service_locator.get_service(name).setup
+    end
+    if @settings.is_set?("setup_script")
+      puts "Setting up test specific fixtures"
+      script = File.join(FIXTURES_DIR, @settings.get("setup_script"))
+      `#{script}`
+    end
+  end
+end
diff --git a/qa/integration/framework/helpers.rb b/qa/integration/framework/helpers.rb
new file mode 100644
index 00000000000..23a911e091e
--- /dev/null
+++ b/qa/integration/framework/helpers.rb
@@ -0,0 +1,45 @@
+# encoding: utf-8
+# Helper module for all tests
+
+require "flores/random"
+
+def wait_for_port(port, retry_attempts)
+  tries = retry_attempts
+  while tries > 0
+    if is_port_open?(port)
+      break
+    else
+      sleep 1
+    end
+    tries -= 1
+  end
+end
+
+def is_port_open?(port)
+  begin
+    s = TCPSocket.open("localhost", port)
+    s.close
+    return true
+  rescue Errno::ECONNREFUSED, Errno::EHOSTUNREACH
+    return false
+  end
+end
+
+def send_data(port, data)
+  socket = TCPSocket.new("127.0.0.1", port)
+  socket.puts(data)
+  socket.flush
+  socket.close
+end
+
+def config_to_temp_file(config)
+  f = Stud::Temporary.file
+  f.write(config)
+  f.close
+  f.path
+end
+
+def random_port
+  # 9600-9700 is reserved in Logstash HTTP server, so we don't want that
+  Flores::Random.integer(9701..15000)
+end  
\ No newline at end of file
diff --git a/qa/integration/framework/settings.rb b/qa/integration/framework/settings.rb
new file mode 100644
index 00000000000..d0a25ad3a6a
--- /dev/null
+++ b/qa/integration/framework/settings.rb
@@ -0,0 +1,53 @@
+require 'yaml'
+
+# All settings for a test, global and per test
+class TestSettings
+  # Setting for the entire test suite
+  INTEG_TESTS_DIR = File.expand_path(File.join("..", ".."), __FILE__)
+  # Test specific settings
+  SUITE_SETTINGS_FILE = File.join(INTEG_TESTS_DIR, "suite.yml")
+  FIXTURES_DIR = File.join(INTEG_TESTS_DIR, "fixtures")
+
+  def initialize(test_file_path)
+    test_name = File.basename(test_file_path, ".*" )
+    @tests_settings_file = File.join(FIXTURES_DIR, "#{test_name}.yml")
+    # Global suite settings
+    @suite_settings = YAML.load_file(SUITE_SETTINGS_FILE)
+    # Per test settings, where one can override stuff and define test specific config
+    @test_settings = YAML.load_file(@tests_settings_file)
+    
+    if verbose_mode?
+      puts "Test settings file: #{@tests_settings_file}"
+      puts "Suite settings file: #{SUITE_SETTINGS_FILE}"
+    end  
+
+    if is_set?("config")
+      if get("config").is_a?(Hash)
+        tmp = {}
+        get("config").each do |k, v|
+          tmp[k] = get("config")[k].gsub('\n','').split.join(" ")
+        end
+        @test_settings["config"] = tmp
+      else
+        config_string = get("config").gsub('\n','').split.join(" ")
+        @test_settings["config"] = config_string
+      end
+    end
+  end
+
+  def get(key)
+    if @test_settings.key?(key)
+      @test_settings[key]
+    else
+      @suite_settings[key]
+    end
+  end
+  
+  def verbose_mode?
+    @suite_settings["verbose_mode"]
+  end  
+
+  def is_set?(key)
+    @suite_settings.key?(key) || @test_settings.key?(key)
+  end
+end
diff --git a/qa/integration/integration_tests.gemspec b/qa/integration/integration_tests.gemspec
new file mode 100644
index 00000000000..5a44f769538
--- /dev/null
+++ b/qa/integration/integration_tests.gemspec
@@ -0,0 +1,23 @@
+Gem::Specification.new do |s|
+  s.name        = 'logstash-integration-tests'
+  s.version     = '0.1.0'
+  s.licenses    = ['Apache License (2.0)']
+  s.summary     = "Tests LS binary"
+  s.description = "This is a Logstash integration test helper gem"
+  s.authors     = [ "Elastic"]
+  s.email       = 'info@elastic.co'
+  s.homepage    = "http://www.elastic.co/guide/en/logstash/current/index.html"
+
+  # Files
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Gem dependencies
+  s.add_development_dependency 'elasticsearch'
+  s.add_development_dependency 'childprocess'
+  s.add_development_dependency 'rspec-wait'
+  s.add_development_dependency 'manticore'
+  s.add_development_dependency 'stud'
+  s.add_development_dependency 'pry'
+  s.add_development_dependency 'logstash-devutils'
+  s.add_development_dependency 'flores'
+end
diff --git a/qa/integration/services/elasticsearch_service.rb b/qa/integration/services/elasticsearch_service.rb
new file mode 100644
index 00000000000..66963aca5de
--- /dev/null
+++ b/qa/integration/services/elasticsearch_service.rb
@@ -0,0 +1,12 @@
+require 'elasticsearch'
+
+class ElasticsearchService < Service
+  def initialize(settings)
+    super("elasticsearch", settings)
+  end
+
+  def get_client
+    Elasticsearch::Client.new(:hosts => "localhost:9200")
+  end
+
+end
\ No newline at end of file
diff --git a/qa/integration/services/elasticsearch_setup.sh b/qa/integration/services/elasticsearch_setup.sh
new file mode 100755
index 00000000000..72942af985b
--- /dev/null
+++ b/qa/integration/services/elasticsearch_setup.sh
@@ -0,0 +1,43 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${ES_VERSION+1}" ]; then
+  echo "Elasticsearch version is $ES_VERSION"
+  version=$ES_VERSION
+else
+   version=5.0.0-beta1
+fi
+
+ES_HOME=$INSTALL_DIR/elasticsearch
+
+setup_es() {
+  if [ ! -d $ES_HOME ]; then
+      local version=$1
+      download_url=https://artifacts.elastic.co/downloads/elasticsearch/elasticsearch-$version.tar.gz
+      curl -sL $download_url > $INSTALL_DIR/elasticsearch.tar.gz
+      mkdir $ES_HOME
+      tar -xzf $INSTALL_DIR/elasticsearch.tar.gz --strip-components=1 -C $ES_HOME/.
+      rm $INSTALL_DIR/elasticsearch.tar.gz
+  fi
+}
+
+start_es() {
+  es_args=$@
+  $ES_HOME/bin/elasticsearch $es_args -p $ES_HOME/elasticsearch.pid > /tmp/elasticsearch.log 2>/dev/null &
+  count=120
+  echo "Waiting for elasticsearch to respond..."
+  while ! curl --silent localhost:9200 && [[ $count -ne 0 ]]; do
+      count=$(( $count - 1 ))
+      [[ $count -eq 0 ]] && return 1
+      sleep 1
+  done
+  echo "Elasticsearch is Up !"
+  return 0
+}
+
+setup_install_dir
+setup_es $version
+start_es
diff --git a/qa/integration/services/elasticsearch_teardown.sh b/qa/integration/services/elasticsearch_teardown.sh
new file mode 100755
index 00000000000..f8e4dd51139
--- /dev/null
+++ b/qa/integration/services/elasticsearch_teardown.sh
@@ -0,0 +1,15 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+ES_HOME=$INSTALL_DIR/elasticsearch
+
+stop_es() {
+    pid=$(cat $ES_HOME/elasticsearch.pid)
+    [ "x$pid" != "x" ] && [ "$pid" -gt 0 ]
+    kill -SIGTERM $pid
+}
+
+stop_es
\ No newline at end of file
diff --git a/qa/integration/services/filebeat_service.rb b/qa/integration/services/filebeat_service.rb
new file mode 100644
index 00000000000..9904499dd16
--- /dev/null
+++ b/qa/integration/services/filebeat_service.rb
@@ -0,0 +1,51 @@
+# encoding: utf-8
+class FilebeatService < Service
+  FILEBEAT_CMD = [File.join(File.dirname(__FILE__), "installed", "filebeat", "filebeat"), "-c"]
+
+  class BackgroundProcess
+    def initialize(cmd)
+      @client_out = Stud::Temporary.file
+      @client_out.sync
+
+      @process = ChildProcess.build(*cmd)
+      @process.duplex = true
+      @process.io.stdout = @process.io.stderr = @client_out
+      ChildProcess.posix_spawn = true
+    end
+
+    def start
+      @process.start
+      sleep(0.1)
+      self
+    end
+
+    def execution_output
+      @client_out.rewind
+
+      # can be used to helper debugging when a test fails
+      @execution_output = @client_out.read
+    end
+
+    def stop
+      begin
+        @process.poll_for_exit(5)
+      rescue ChildProcess::TimeoutError
+        Process.kill("KILL", @process.pid)
+      end
+    end
+  end
+
+  def initialize(settings)
+    super("filebeat", settings)
+  end
+
+  def run(config_path)
+    cmd = FILEBEAT_CMD.dup << config_path
+    puts "Starting Filebeat with #{cmd.join(" ")}"
+    @process = BackgroundProcess.new(cmd).start
+  end
+
+  def stop
+    @process.stop
+  end
+end
diff --git a/qa/integration/services/filebeat_setup.sh b/qa/integration/services/filebeat_setup.sh
new file mode 100755
index 00000000000..a1061667007
--- /dev/null
+++ b/qa/integration/services/filebeat_setup.sh
@@ -0,0 +1,38 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${FILEBEAT_VERSION}" ]; then
+  echo "Filebeat version is $FILEBEAT_VERSION"
+  version=$FILEBEAT_VERSION
+else
+  version=5.0.0-beta1
+fi
+
+FB_HOME=$INSTALL_DIR/filebeat
+
+setup_fb() {
+    local version=$1
+    platform=`uname -s | tr '[:upper:]' '[:lower:]'`
+    architecture=`uname -m | tr '[:upper:]' '[:lower:]'`
+    download_url=https://artifacts.elastic.co/downloads/beats/filebeat/filebeat-$version-$platform-$architecture.tar.gz
+    curl -sL $download_url > $INSTALL_DIR/filebeat.tar.gz
+    mkdir $FB_HOME
+    tar -xzf $INSTALL_DIR/filebeat.tar.gz --strip-components=1 -C $FB_HOME/.
+    rm $INSTALL_DIR/filebeat.tar.gz
+}
+
+generate_certificate() {
+    target_directory=$current_dir/../fixtures/certificates
+    mkdir -p $target_directory
+    openssl req -subj '/CN=localhost/' -x509 -days $((100 * 365)) -batch -nodes -newkey rsa:2048 -keyout $target_directory/certificate.key -out $target_directory/certificate.crt
+}
+
+setup_install_dir
+
+if [ ! -d $FB_HOME ]; then
+    generate_certificate
+    setup_fb $version
+fi
diff --git a/qa/integration/services/helpers.sh b/qa/integration/services/helpers.sh
new file mode 100644
index 00000000000..dbf970a3eba
--- /dev/null
+++ b/qa/integration/services/helpers.sh
@@ -0,0 +1,30 @@
+#!/bin/bash
+set -e
+current_dir="$(dirname "$0")"
+
+INSTALL_DIR=$current_dir/installed
+PORT_WAIT_COUNT=20
+
+setup_install_dir() {
+    if [[ ! -d "$INSTALL_DIR" ]]; then
+        mkdir $INSTALL_DIR
+    fi
+}
+
+wait_for_port() {
+    count=$PORT_WAIT_COUNT
+    port=$1
+    while ! nc -z localhost $port && [[ $count -ne 0 ]]; do
+        count=$(( $count - 1 ))
+        [[ $count -eq 0 ]] && return 1
+        sleep 0.5
+    done
+    # just in case, one more time
+    nc -z localhost $port
+}
+
+clean_install_dir() {
+    if [[ -d "$INSTALL_DIR" ]]; then
+        rm -rf $INSTALL_DIR
+    fi
+}
diff --git a/qa/integration/services/kafka_service.rb b/qa/integration/services/kafka_service.rb
new file mode 100644
index 00000000000..71908f9acbb
--- /dev/null
+++ b/qa/integration/services/kafka_service.rb
@@ -0,0 +1,7 @@
+require_relative "service"
+
+class KafkaService < Service
+  def initialize(settings)
+    super("kafka", settings)
+  end
+end
diff --git a/qa/integration/services/kafka_setup.sh b/qa/integration/services/kafka_setup.sh
new file mode 100755
index 00000000000..3a3b283300d
--- /dev/null
+++ b/qa/integration/services/kafka_setup.sh
@@ -0,0 +1,68 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+if [ -n "${KAFKA_VERSION+1}" ]; then
+    echo "KAFKA_VERSION is $KAFKA_VERSION"
+    version=$KAFKA_VERSION
+else
+    version=0.10.0.1
+fi
+
+KAFKA_HOME=$INSTALL_DIR/kafka
+KAFKA_TOPIC=logstash_topic_plain
+KAFKA_MESSAGES=37
+KAFKA_LOGS_DIR=/tmp/ls_integration/kafka-logs
+
+setup_kafka() {
+    local version=$1
+    if [ ! -d $KAFKA_HOME ]; then
+        echo "Downloading Kafka version $version"
+        curl -s -o $INSTALL_DIR/kafka.tgz "http://ftp.wayne.edu/apache/kafka/$version/kafka_2.11-$version.tgz"
+        mkdir $KAFKA_HOME && tar xzf $INSTALL_DIR/kafka.tgz -C $KAFKA_HOME --strip-components 1
+        rm $INSTALL_DIR/kafka.tgz
+    fi
+}
+
+start_kafka() {
+    echo "Starting ZooKeeper"
+    $KAFKA_HOME/bin/zookeeper-server-start.sh -daemon $KAFKA_HOME/config/zookeeper.properties
+    wait_for_port 2181
+    echo "Starting Kafka broker"
+    $KAFKA_HOME/bin/kafka-server-start.sh -daemon $KAFKA_HOME/config/server.properties --override delete.topic.enable=true --override log.dirs=$KAFKA_LOGS_DIR --override log.flush.interval.ms=200
+    wait_for_port 9092
+}
+
+wait_for_messages() {
+    local count=10
+    local read_lines=0
+    
+    echo "Checking if Kafka topic has been populated with data"
+    while [[ $read_lines -ne $KAFKA_MESSAGES ]] && [[ $count -ne 0 ]]; do
+        read_lines=`$KAFKA_HOME/bin/kafka-console-consumer.sh --topic $KAFKA_TOPIC --new-consumer --bootstrap-server localhost:9092 --from-beginning --max-messages $KAFKA_MESSAGES --timeout-ms 10000 | wc -l`
+        count=$(( $count - 1 ))
+        [[ $count -eq 0 ]] && return 1
+        sleep 0.5
+        ls -lrt $KAFKA_LOGS_DIR/$KAFKA_TOPIC-0/
+    done
+    echo "Kafka topic has been populated with test data"
+}
+
+setup_install_dir
+setup_kafka $version
+start_kafka
+# Set up topics
+$KAFKA_HOME/bin/kafka-topics.sh --create --partitions 1 --replication-factor 1 --topic $KAFKA_TOPIC --zookeeper localhost:2181
+# check topic got created
+num_topic=`$KAFKA_HOME/bin/kafka-topics.sh --list --zookeeper localhost:2181 | grep $KAFKA_TOPIC | wc -l`
+[[ $num_topic -eq 1 ]]
+# Add test messages to the newly created topic
+cp $current_dir/../fixtures/how_sample.input $KAFKA_HOME
+[[ ! -s  how_sample.input ]]
+$KAFKA_HOME/bin/kafka-console-producer.sh --topic $KAFKA_TOPIC --broker-list localhost:9092 < $KAFKA_HOME/how_sample.input
+echo "Kafka load status code $?"
+# Wait until broker has all messages
+wait_for_messages
+echo "Kafka Setup complete"
diff --git a/qa/integration/services/kafka_teardown.sh b/qa/integration/services/kafka_teardown.sh
new file mode 100755
index 00000000000..0d10cffe844
--- /dev/null
+++ b/qa/integration/services/kafka_teardown.sh
@@ -0,0 +1,22 @@
+#!/bin/bash
+set -ex
+current_dir="$(dirname "$0")"
+
+source "$current_dir/helpers.sh"
+
+KAFKA_HOME=$INSTALL_DIR/kafka
+
+stop_kafka() {
+    echo "Stopping Kafka broker"
+    $KAFKA_HOME/bin/kafka-server-stop.sh
+    echo "Stopping zookeeper"
+    $KAFKA_HOME/bin/zookeeper-server-stop.sh
+}
+
+# delete test topic
+echo "Deleting test topic in Kafka"
+$KAFKA_HOME/bin/kafka-topics.sh --delete --topic logstash_topic_plain --zookeeper localhost:2181 --if-exists
+stop_kafka
+rm -rf /tmp/ls_integration/kafka-logs
+rm -rf /tmp/zookeeper
+
diff --git a/qa/integration/services/logstash_service.rb b/qa/integration/services/logstash_service.rb
new file mode 100644
index 00000000000..4ae85afdbcb
--- /dev/null
+++ b/qa/integration/services/logstash_service.rb
@@ -0,0 +1,174 @@
+require_relative "monitoring_api"
+
+require "childprocess"
+require "bundler"
+require "tempfile"
+require 'yaml'
+
+# A locally started Logstash service
+class LogstashService < Service
+
+  LS_ROOT_DIR = File.join("..", "..", "..", "..")
+  LS_VERSION_FILE = File.expand_path(File.join(LS_ROOT_DIR, "versions.yml"), __FILE__)
+  LS_BUILD_DIR = File.join(LS_ROOT_DIR, "build")
+  LS_BIN = File.join("bin", "logstash")
+  LS_CONFIG_FILE = File.join("config", "logstash.yml")
+
+  STDIN_CONFIG = "input {stdin {}} output { }"
+  RETRY_ATTEMPTS = 10
+
+  @process = nil
+  
+  attr_reader :logstash_home
+  attr_reader :application_settings_file
+  attr_writer :env_variables
+
+  def initialize(settings)
+    super("logstash", settings)
+
+    # if you need to point to a LS in different path
+    if @settings.is_set?("ls_home_abs_path")
+      @logstash_home = @settings.get("ls_home_abs_path")
+    else
+      # use the LS which was just built in source repo
+      ls_version_file = YAML.load_file(LS_VERSION_FILE)
+      ls_file = "logstash-" + ls_version_file["logstash"]
+      # First try without the snapshot if it's there
+      @logstash_home = File.expand_path(File.join(LS_BUILD_DIR, ls_file), __FILE__)
+      @logstash_home += "-SNAPSHOT" unless Dir.exists?(@logstash_home)
+
+      puts "Using #{@logstash_home} as LS_HOME"
+      @logstash_bin = File.join("#{@logstash_home}", LS_BIN)
+      raise "Logstash binary not found in path #{@logstash_home}" unless File.file? @logstash_bin
+    end
+    
+    @application_settings_file = File.join(@logstash_home, LS_CONFIG_FILE)
+    @monitoring_api = MonitoringAPI.new
+  end
+
+  def alive?
+    if @process.nil? || @process.exited?
+      raise "Logstash process is not up because of an errot, or it stopped"
+    else
+      @process.alive?
+    end
+  end
+  
+  def exited?
+    @process.exited?
+  end
+  
+  def exit_code
+    @process.exit_code
+  end  
+
+  # Starts a LS process in background with a given config file
+  # and shuts it down after input is completely processed
+  def start_background(config_file)
+    spawn_logstash("-e", config_file)
+  end
+
+  # Given an input this pipes it to LS. Expects a stdin input in LS
+  def start_with_input(config, input)
+    Bundler.with_clean_env do
+      `cat #{input} | #{@logstash_bin} -e \'#{config}\'`
+    end
+  end
+
+  def start_with_config_string(config)
+    spawn_logstash("-e", "#{config} ")
+  end
+
+  # Can start LS in stdin and can send messages to stdin
+  # Useful to test metrics and such
+  def start_with_stdin
+    puts "Starting Logstash #{@logstash_bin} -e #{STDIN_CONFIG}"
+    Bundler.with_clean_env do
+      out = Tempfile.new("duplex")
+      out.sync = true
+      @process = ChildProcess.build(@logstash_bin, "-e", STDIN_CONFIG)
+      # pipe STDOUT and STDERR to a file
+      @process.io.stdout = @process.io.stderr = out
+      @process.duplex = true
+      @process.start
+      wait_for_logstash
+      puts "Logstash started with PID #{@process.pid}" if alive?
+    end
+  end
+
+  def write_to_stdin(input)
+    if alive?
+      @process.io.stdin.puts(input)
+    end
+  end
+
+  # Spawn LS as a child process
+  def spawn_logstash(*args)
+    puts "Starting Logstash #{@logstash_bin} #{args}" 
+    Bundler.with_clean_env do
+      @process = ChildProcess.build(@logstash_bin, *args)
+      @env_variables.map { |k, v|  @process.environment[k] = v} unless @env_variables.nil?
+      @process.io.inherit!
+      @process.start
+      wait_for_logstash
+      puts "Logstash started with PID #{@process.pid}" if @process.alive?
+    end
+  end
+
+  def teardown
+    if !@process.nil?
+      # todo: put this in a sleep-wait loop to kill it force kill
+      @process.io.stdin.close rescue nil
+      @process.stop
+      @process = nil
+    end
+  end
+
+  # check if LS HTTP port is open
+  def is_port_open?
+    begin
+      s = TCPSocket.open("localhost", 9600)
+      s.close
+      return true
+    rescue Errno::ECONNREFUSED, Errno::EHOSTUNREACH
+      return false
+    end
+  end
+
+  def monitoring_api
+    raise "Logstash is not up, but you asked for monitoring API" unless alive?
+    @monitoring_api
+  end
+
+  # Wait until LS is started by repeatedly doing a socket connection to HTTP port
+  def wait_for_logstash
+    tries = RETRY_ATTEMPTS
+    while tries > 0
+      if is_port_open?
+        break
+      else
+        sleep 1
+      end
+      tries -= 1
+    end
+  end
+  
+  # this method only overwrites existing config with new config
+  # it does not assume that LS pipeline is fully reloaded after a 
+  # config change. It is up to the caller to validate that.
+  def reload_config(initial_config_file, reload_config_file)
+    FileUtils.cp(reload_config_file, initial_config_file)
+  end  
+  
+  def get_version
+    `#{@logstash_bin} --version`
+  end
+  
+  def get_version_yml
+    LS_VERSION_FILE
+  end   
+  
+  def process_id
+    @process.pid
+  end
+end
diff --git a/qa/integration/services/monitoring_api.rb b/qa/integration/services/monitoring_api.rb
new file mode 100644
index 00000000000..27ef78b1611
--- /dev/null
+++ b/qa/integration/services/monitoring_api.rb
@@ -0,0 +1,30 @@
+require "manticore"
+require "json"
+
+# Convenience class to interact with the HTTP monitoring APIs
+class MonitoringAPI
+
+  def pipeline_stats
+    resp = Manticore.get("http://localhost:9600/_node/stats/pipeline").body
+    stats_response = JSON.parse(resp)
+    stats_response["pipeline"]
+  end
+
+  def event_stats
+    stats = pipeline_stats
+    stats["events"]
+  end
+
+  def version
+    request = @agent.get("http://localhost:9600/")
+    response = request.execute
+    r = JSON.parse(response.body.read)
+    r["version"]
+  end
+  
+  def node_info
+    resp = Manticore.get("http://localhost:9600/_node").body
+    JSON.parse(resp)
+  end
+
+end
diff --git a/qa/integration/services/service.rb b/qa/integration/services/service.rb
new file mode 100644
index 00000000000..f2c3525d2f6
--- /dev/null
+++ b/qa/integration/services/service.rb
@@ -0,0 +1,30 @@
+# Base class for a service like Kafka, ES, Logstash
+class Service
+
+  def initialize(name, settings)
+    @name = name
+    @settings = settings
+    @setup_script = File.expand_path("../#{name}_setup.sh", __FILE__)
+    @teardown_script = File.expand_path("../#{name}_teardown.sh", __FILE__)
+  end
+
+  def setup
+    puts "Setting up #{@name} service"
+    if File.exists?(@setup_script)
+      `#{@setup_script}`
+    else
+      puts "Setup script not found for #{@name}"
+    end
+    puts "#{@name} service setup complete"
+  end
+
+  def teardown
+    puts "Tearing down #{@name} service"
+    if File.exists?(@setup_script)
+      `#{@teardown_script}`
+    else
+      puts "Teardown script not found for #{@name}"
+    end
+    puts "#{@name} service teardown complete"
+  end
+end
diff --git a/qa/integration/services/service_locator.rb b/qa/integration/services/service_locator.rb
new file mode 100644
index 00000000000..ff537ebcd2a
--- /dev/null
+++ b/qa/integration/services/service_locator.rb
@@ -0,0 +1,30 @@
+# encoding: utf-8
+require_relative "service"
+
+# This is a registry used in Fixtures so a test can get back any service class
+# at runtime
+# All new services should register here
+class ServiceLocator
+  FILE_PATTERN = "_service.rb"
+
+  def initialize(settings)
+    @services = {}
+    available_services do |name, klass|
+      @services[name] = klass.new(settings)
+    end
+  end
+
+  def get_service(name)
+    @services.fetch(name)
+  end
+
+  def available_services
+    Dir.glob(File.join(File.dirname(__FILE__), "*#{FILE_PATTERN}")).each do |f|
+      require f
+      basename = File.basename(f).gsub(/#{FILE_PATTERN}$/, "")
+      service_name = basename.downcase
+      klass = Object.const_get("#{service_name.capitalize}Service")
+      yield service_name, klass
+    end
+  end
+end
diff --git a/qa/integration/specs/01_logstash_bin_smoke_spec.rb b/qa/integration/specs/01_logstash_bin_smoke_spec.rb
new file mode 100644
index 00000000000..3fa9e3faf02
--- /dev/null
+++ b/qa/integration/specs/01_logstash_bin_smoke_spec.rb
@@ -0,0 +1,57 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+require "yaml"
+
+describe "Test Logstash instance" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+    # used in multiple LS tests
+    @ls1 = @fixture.get_service("logstash")
+    @ls2 = LogstashService.new(@fixture.settings)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  after(:each) {
+    @ls1.teardown
+    @ls2.teardown
+  }
+  
+  let(:num_retries) { 10 }
+  let(:config1) { config_to_temp_file(@fixture.config("root", { :port => random_port })) }
+  let(:config2) { config_to_temp_file(@fixture.config("root", { :port => random_port })) }
+
+  it "can start the embedded http server on default port 9600" do
+    @ls1.start_with_stdin
+    try(num_retries) do
+      expect(is_port_open?(9600)).to be true
+    end
+  end
+  
+  it "multiple of them can be started on the same box with automatically trying different ports for HTTP server" do
+    @ls1.spawn_logstash("-f", config1)
+    try(num_retries) do
+      expect(is_port_open?(9600)).to be true
+    end
+
+    puts "will try to start the second LS instance on 9601"
+
+    # bring up new LS instance
+    @ls2.spawn_logstash("-f", config2)
+    try(20) do
+      expect(is_port_open?(9601)).to be true
+    end
+
+    expect(@ls1.process_id).not_to eq(@ls2.process_id)
+  end
+  
+  it "gets the right version when asked" do
+    expected = YAML.load_file(LogstashService::LS_VERSION_FILE)
+    expect(@ls1.get_version.strip).to eq("logstash #{expected['logstash']}")
+  end
+end    
\ No newline at end of file
diff --git a/qa/integration/specs/beats_input_spec.rb b/qa/integration/specs/beats_input_spec.rb
new file mode 100644
index 00000000000..1dc31b5e10a
--- /dev/null
+++ b/qa/integration/specs/beats_input_spec.rb
@@ -0,0 +1,144 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require "stud/temporary"
+require "stud/try"
+require "rspec/wait"
+require "yaml"
+require "fileutils"
+
+describe "Beat Input" do
+  before(:all) do
+    @fixture = Fixture.new(__FILE__)
+  end
+
+  after :each do
+    logstash_service.teardown
+    filebeat_service.stop
+  end
+
+  let(:max_retry) { 120 }
+  let(:registry_file) { Stud::Temporary.file.path }
+  let(:logstash_service) { @fixture.get_service("logstash") }
+  let(:filebeat_service) { @fixture.get_service("filebeat") }
+  let(:log_path) do
+    tmp_path = Stud::Temporary.file.path #get around ignore older completely
+    source = File.expand_path(@fixture.input)
+    FileUtils.cp(source, tmp_path)
+    tmp_path
+  end
+  let(:number_of_events) do
+    File.open(log_path, "r").readlines.size
+  end
+
+  shared_examples "send events" do
+    let(:filebeat_config_path) do
+      file = Stud::Temporary.file
+      file.write(YAML.dump(filebeat_config))
+      file.close
+      file.path
+    end
+
+
+    it "sucessfully send events" do
+      logstash_service.start_background(logstash_config)
+      process = filebeat_service.run(filebeat_config_path)
+
+      # It can take some delay for filebeat to connect to logstash and start sending data.
+      # Its possible that logstash isn't completely initialized here, we can get "Connection Refused"
+      begin
+        sleep(1) while (result = logstash_service.monitoring_api.event_stats).nil?
+      rescue
+        retry
+      end
+
+      Stud.try(max_retry.times, RSpec::Expectations::ExpectationNotMetError) do
+         result = logstash_service.monitoring_api.event_stats
+         expect(result["in"]).to eq(number_of_events)
+      end
+    end
+  end
+
+  context "Without TLS" do
+    let(:logstash_config) { @fixture.config("without_tls") }
+    let(:filebeat_config) do
+      {
+        "filebeat" => {
+          "prospectors" => [{ "paths" => [log_path], "input_type" => "log" }],
+          "registry_file" => registry_file,
+          "scan_frequency" => "1s"
+        },
+        "output" => {
+          "logstash" => { "hosts" => ["localhost:5044"] },
+          "logging" => { "level" => "debug" }
+        }
+      }
+    end
+
+    include_examples "send events"
+  end
+
+  context "With TLS" do
+    let(:certificate_directory) { File.expand_path(File.join(File.dirname(__FILE__), "..", "fixtures", "certificates")) }
+    let(:certificate) { File.join(certificate_directory, "certificate.crt") }
+    let(:ssl_key) { File.join(certificate_directory, "certificate.key") }
+    let(:certificate_authorities) { [certificate] }
+
+    context "Server auth" do
+      let(:logstash_config) { @fixture.config("tls_server_auth", { :ssl_certificate => certificate, :ssl_key => ssl_key}) }
+      let(:filebeat_config) do
+        {
+          "filebeat" => {
+            "prospectors" => [{ "paths" => [log_path], "input_type" => "log" }],
+            "registry_file" => registry_file,
+            "scan_frequency" => "1s"
+          },
+          "output" => {
+            "logstash" => {
+              "hosts" => ["localhost:5044"],
+              "tls" => {
+                "certificate_authorities" => certificate_authorities
+              },
+              "ssl" => {
+                "certificate_authorities" => certificate_authorities
+              }
+            },
+            "logging" => { "level" => "debug" }
+          }
+        }
+      end
+
+      include_examples "send events"
+    end
+
+    context "Mutual auth" do
+      let(:logstash_config) { @fixture.config("tls_mutual_auth", { :ssl_certificate => certificate, :ssl_key => ssl_key}) }
+      let(:filebeat_config) do
+        {
+          "filebeat" => {
+            "prospectors" => [{ "paths" => [log_path], "input_type" => "log" }],
+            "registry_file" => registry_file,
+            "scan_frequency" => "1s"
+          },
+          "output" => {
+            "logstash" => {
+              "hosts" => ["localhost:5044"],
+              "tls" => {
+                "certificate_authorities" => certificate_authorities,
+                "certificate" => certificate,
+                "certificate_key" => ssl_key
+              },
+              "ssl" => {
+                "certificate_authorities" => certificate_authorities,
+                "certificate" => certificate,
+                "key" => ssl_key
+              }
+            },
+            "logging" => { "level" => "debug" }
+          }
+        }
+      end
+
+      include_examples "send events"
+    end
+  end
+end
diff --git a/qa/integration/specs/env_variables_config_spec.rb b/qa/integration/specs/env_variables_config_spec.rb
new file mode 100644
index 00000000000..801ad5179b6
--- /dev/null
+++ b/qa/integration/specs/env_variables_config_spec.rb
@@ -0,0 +1,48 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+
+describe "Test Logstash configuration" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  let(:num_retries) { 10 }
+  let(:test_tcp_port) { random_port }
+  let(:test_tag) { "environment_variables_are_evil" }
+  let(:test_path) { Stud::Temporary.directory }
+  let(:sample_data) { '74.125.176.147 - - [11/Sep/2014:21:50:37 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "FeedBurner/1.0 (http://www.FeedBurner.com)"' }
+
+  it "expands environment variables in all plugin blocks" do
+    # set ENV variables before starting the service
+    test_env = {}
+    test_env["TEST_ENV_TCP_PORT"] = "#{test_tcp_port}"
+    test_env["TEST_ENV_TAG"] = test_tag
+    test_env["TEST_ENV_PATH"] = test_path
+    
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.env_variables = test_env
+    logstash_service.start_background(@fixture.config)
+    # check if TCP port env variable was resolved
+    try(num_retries) do
+      expect(is_port_open?(test_tcp_port)).to be true
+    end
+    
+    #send data and make sure all env variables are expanded by checking each stage
+    send_data(test_tcp_port, sample_data)
+    output_file = File.join(test_path, "logstash_env_test.log")
+    try(num_retries) do
+      expect(File.exists?(output_file)).to be true
+    end
+    # should have created the file using env variable with filters adding a tag based on env variable
+    try(num_retries) do
+      expect(IO.read(output_file).gsub("\n", "")).to eq("#{sample_data} blah,environment_variables_are_evil")
+    end
+  end
+end  
\ No newline at end of file
diff --git a/qa/integration/specs/es_output_how_spec.rb b/qa/integration/specs/es_output_how_spec.rb
new file mode 100644
index 00000000000..da1916f52bb
--- /dev/null
+++ b/qa/integration/specs/es_output_how_spec.rb
@@ -0,0 +1,40 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+
+describe "Test Elasticsearch output" do
+
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    es_client = @fixture.get_service("elasticsearch").get_client
+    es_client.indices.delete(index: 'logstash-*')
+    @fixture.teardown
+  }
+
+  it "can ingest 37K log lines of sample apache logs" do
+    logstash_service = @fixture.get_service("logstash")
+    es_service = @fixture.get_service("elasticsearch")
+    logstash_service.start_with_input(@fixture.config, @fixture.input)
+    es_client = es_service.get_client
+    # now we test if all data was indexed by ES, but first refresh manually
+    es_client.indices.refresh
+    result = es_client.search(index: 'logstash-*', size: 0, q: '*')
+    expect(result["hits"]["total"]).to eq(37)
+    
+    # randomly checked for results and structured fields
+    result = es_client.search(index: 'logstash-*', size: 1, q: 'dynamic')
+    expect(result["hits"]["total"]).to eq(1)
+    s = result["hits"]["hits"][0]["_source"]
+    expect(s["bytes"]).to eq(18848)
+    expect(s["response"]).to eq(200)
+    expect(s["clientip"]).to eq("213.113.233.227")
+    expect(s["geoip"]["longitude"]).to eq(12.9443)
+    expect(s["geoip"]["latitude"]).to eq(56.1357)
+    expect(s["verb"]).to eq("GET")
+    expect(s["useragent"]["os"]).to eq("Windows 7")
+  end
+
+end
diff --git a/qa/integration/specs/kafka_input_spec.rb b/qa/integration/specs/kafka_input_spec.rb
new file mode 100644
index 00000000000..dc2d371ae8d
--- /dev/null
+++ b/qa/integration/specs/kafka_input_spec.rb
@@ -0,0 +1,25 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require "rspec/wait"
+
+describe "Test Kafka Input" do
+  let(:timeout_seconds) { 30 }
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+
+  it "can ingest 37 apache log lines from Kafka broker" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_background(@fixture.config)
+
+    wait(timeout_seconds).for { @fixture.output_exists? }.to be true
+    expect(@fixture.output_equals_expected?).to be true
+      lambda { "Expected File output to match what was ingested into Kafka." }
+  end
+
+end
diff --git a/qa/integration/specs/monitoring_api_spec.rb b/qa/integration/specs/monitoring_api_spec.rb
new file mode 100644
index 00000000000..90d16cbfeaf
--- /dev/null
+++ b/qa/integration/specs/monitoring_api_spec.rb
@@ -0,0 +1,35 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require "logstash/devutils/rspec/spec_helper"
+
+describe "Test Monitoring API" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  let(:number_of_events) { 5 }
+  let(:max_retry) { 120 }
+
+  it "can retrieve event stats" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.start_with_stdin
+    number_of_events.times { logstash_service.write_to_stdin("Hello world") }
+    
+    begin
+      sleep(1) while (result = logstash_service.monitoring_api.event_stats).nil?
+    rescue
+      retry
+    end
+
+    Stud.try(max_retry.times, RSpec::Expectations::ExpectationNotMetError) do
+       result = logstash_service.monitoring_api.event_stats
+       expect(result["in"]).to eq(number_of_events)
+    end
+  end
+
+end
diff --git a/qa/integration/specs/reload_config_spec.rb b/qa/integration/specs/reload_config_spec.rb
new file mode 100644
index 00000000000..530e75a2a7f
--- /dev/null
+++ b/qa/integration/specs/reload_config_spec.rb
@@ -0,0 +1,77 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+require "socket"
+require "json"
+
+describe "Test Logstash service when config reload is enabled" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  let(:timeout_seconds) { 5 }
+  let(:initial_port) { random_port }
+  let(:reload_port) { random_port }
+  let(:retry_attempts) { 10 }
+  let(:output_file1) { Stud::Temporary.file.path }
+  let(:output_file2) { Stud::Temporary.file.path }
+  let(:sample_data) { '74.125.176.147 - - [11/Sep/2014:21:50:37 +0000] "GET /?flav=rss20 HTTP/1.1" 200 29941 "-" "FeedBurner/1.0 (http://www.FeedBurner.com)"' }
+  
+  let(:initial_config_file) { config_to_temp_file(@fixture.config("initial", { :port => initial_port, :file => output_file1 })) }
+  let(:reload_config_file) { config_to_temp_file(@fixture.config("reload", { :port => reload_port, :file => output_file2 })) }
+
+  it "can reload when changes are made to TCP port and grok pattern" do
+    logstash_service = @fixture.get_service("logstash")
+    logstash_service.spawn_logstash("-f", "#{initial_config_file}", "--config.reload.automatic", "true")
+    logstash_service.wait_for_logstash
+    wait_for_port(initial_port, retry_attempts)
+    
+    # try sending events with this
+    send_data(initial_port, sample_data)
+    Stud.try(retry_attempts.times, RSpec::Expectations::ExpectationNotMetError) do
+      expect(IO.read(output_file1).gsub("\n", "")).to eq(sample_data)
+    end
+    
+    # check metrics
+    result = logstash_service.monitoring_api.event_stats
+    expect(result["in"]).to eq(1)
+    expect(result["out"]).to eq(1)
+    
+    # do a reload
+    logstash_service.reload_config(initial_config_file, reload_config_file)
+
+    logstash_service.wait_for_logstash
+    wait_for_port(reload_port, retry_attempts)
+    
+    # make sure old socket is closed
+    expect(is_port_open?(initial_port)).to be false
+    
+    send_data(reload_port, sample_data)
+    Stud.try(retry_attempts.times, RSpec::Expectations::ExpectationNotMetError) do
+      expect(IO.read(output_file2).blank?).to be false
+    end
+    
+    # check metrics. It should be reset
+    result = logstash_service.monitoring_api.event_stats
+    expect(result["in"]).to eq(1)
+    expect(result["out"]).to eq(1)
+    
+    # check reload stats
+    reload_stats = logstash_service.monitoring_api.pipeline_stats["reloads"]
+    expect(reload_stats["successes"]).to eq(1)
+    expect(reload_stats["failures"]).to eq(0)
+    expect(reload_stats["last_success_timestamp"].blank?).to be false
+    expect(reload_stats["last_error"]).to eq(nil)
+    
+    # parse the results and validate
+    re = JSON.load(File.new(output_file2))
+    expect(re["clientip"]).to eq("74.125.176.147")
+    expect(re["response"]).to eq(200)
+  end
+end
\ No newline at end of file
diff --git a/qa/integration/specs/settings_spec.rb b/qa/integration/specs/settings_spec.rb
new file mode 100644
index 00000000000..e817c1a5d03
--- /dev/null
+++ b/qa/integration/specs/settings_spec.rb
@@ -0,0 +1,139 @@
+require_relative '../framework/fixture'
+require_relative '../framework/settings'
+require_relative '../services/logstash_service'
+require_relative '../framework/helpers'
+require "logstash/devutils/rspec/spec_helper"
+require "yaml"
+
+describe "Test Logstash instance whose default settings are overridden" do
+  before(:all) {
+    @fixture = Fixture.new(__FILE__)
+    @logstash_service = @fixture.get_service("logstash")
+  }
+
+  after(:all) {
+    @fixture.teardown
+  }
+  
+  before(:each) {
+    # backup the application settings file -- logstash.yml
+    FileUtils.cp(@logstash_service.application_settings_file, "#{@logstash_service.application_settings_file}.original")
+  }
+  
+  after(:each) {
+    @logstash_service.teardown
+    # restore the application settings file -- logstash.yml
+    FileUtils.mv("#{@logstash_service.application_settings_file}.original", @logstash_service.application_settings_file)
+  }
+
+  let(:num_retries) { 15 }
+  let(:test_port) { random_port }
+  let(:temp_dir) { Stud::Temporary.directory("logstash-settings-test") }
+  let(:tcp_config) { @fixture.config("root", { :port => test_port }) }
+  
+  def change_setting(name, value)
+    settings = {}
+    settings[name] = value
+    overwrite_settings(settings)
+  end
+  
+  def overwrite_settings(settings)
+    IO.write(@logstash_service.application_settings_file, settings.to_yaml)
+  end  
+  
+  it "should start with a new data dir" do
+    change_setting("path.data", temp_dir)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+  end
+  
+  it "should write logs to a new dir" do
+    change_setting("path.logs", temp_dir)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+    expect(File.exists?("#{temp_dir}/logstash-plain.log")).to be true
+  end
+  
+  it "should read config from the specified dir in logstash.yml" do
+    change_setting("path.config", temp_dir)
+    test_config_path = File.join(temp_dir, "test.config")
+    IO.write(test_config_path, tcp_config)
+    expect(File.exists?(test_config_path)).to be true
+    @logstash_service.spawn_logstash
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+  end
+  
+  it "should exit when config test_and_exit is set" do
+    s = {}
+    s["path.config"] = temp_dir
+    s["config.test_and_exit"] = true
+    s["path.logs"] = temp_dir
+    overwrite_settings(s)
+    test_config_path = File.join(temp_dir, "test.config")
+    IO.write(test_config_path, "#{tcp_config}")
+    expect(File.exists?(test_config_path)).to be true
+    @logstash_service.spawn_logstash
+    try(num_retries) do
+      expect(@logstash_service.exited?).to be true
+    end
+    expect(@logstash_service.exit_code).to eq(0)
+    
+    # now with bad config
+    IO.write(test_config_path, "#{tcp_config} filters {} ")
+    expect(File.exists?(test_config_path)).to be true
+    @logstash_service.spawn_logstash
+    try(num_retries) do
+      expect(@logstash_service.exited?).to be true
+    end
+    expect(@logstash_service.exit_code).to eq(1)
+  end
+  
+  it "change pipeline settings" do
+    s = {}
+    workers = 31
+    batch_size = 1250
+    s["pipeline.workers"] = workers
+    s["pipeline.batch.size"] = batch_size
+    overwrite_settings(s)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    @logstash_service.wait_for_logstash
+    # check LS is up and running with new data path
+    try do
+      expect(is_port_open?(test_port)).to be true
+    end
+    
+    # now check monitoring API to validate
+    node_info = @logstash_service.monitoring_api.node_info
+    expect(node_info["pipeline"]["workers"]).to eq(workers)
+    expect(node_info["pipeline"]["batch_size"]).to eq(batch_size)
+  end
+  
+  it "start on a different HTTP port" do
+    # default in 9600
+    http_port = random_port
+    change_setting("http.port", http_port)
+    @logstash_service.spawn_logstash("-e", tcp_config)
+    
+    try(num_retries) do
+      expect(is_port_open?(http_port)).to be true
+    end
+    # check LS is up and running with new data path
+    try(num_retries) do
+      expect(is_port_open?(test_port)).to be true
+    end
+    
+    resp = Manticore.get("http://localhost:#{http_port}/_node").body
+    node_info = JSON.parse(resp)
+    # should be default
+    expect(node_info["http_address"]).to eq("127.0.0.1:#{http_port}")
+  end
+end  
\ No newline at end of file
diff --git a/qa/integration/suite.yml b/qa/integration/suite.yml
new file mode 100644
index 00000000000..8ccdff47bbc
--- /dev/null
+++ b/qa/integration/suite.yml
@@ -0,0 +1,6 @@
+---
+# Use this to output more debug-level information  
+verbose_mode: false  
+# Typically we use the binaries in LS_HOME/build. If you want to QA a LS in different location, 
+# use the absolute path below  
+#ls_home_abs_path: /tmp/logstash-5.0.0-alpha6
\ No newline at end of file
diff --git a/qa/platform_config.rb b/qa/platform_config.rb
new file mode 100644
index 00000000000..37073b2bb50
--- /dev/null
+++ b/qa/platform_config.rb
@@ -0,0 +1,80 @@
+# encoding: utf-8
+require "json"
+require "ostruct"
+
+# This is a wrapper to encapsulate the logic behind the different platforms we test with, 
+# this is done here in order to simplify the necessary configuration for bootstrap and interactions
+# necessary later on in the tests phases.
+#
+class PlatformConfig
+
+  # Abstract the idea of a platform, aka an OS
+  class Platform
+
+    attr_reader :name, :box, :type, :bootstrap, :experimental
+
+    def initialize(name, data)
+      @name = name
+      @box  = data["box"]
+      @type = data["type"]
+      @experimental = data["experimental"] || false
+      configure_bootstrap_scripts(data)
+    end
+
+    private
+
+    def configure_bootstrap_scripts(data)
+      @bootstrap = OpenStruct.new(:privileged     => "sys/#{type}/bootstrap.sh",
+                                  :non_privileged => "sys/#{type}/user_bootstrap.sh")
+      ##
+      # for now the only specific boostrap scripts are ones need
+      # with privileged access level, whenever others are also
+      # required we can update this section as well with the same pattern.
+      ##
+      @bootstrap.privileged = "sys/#{type}/#{name}/bootstrap.sh" if data["specific"]
+    end
+  end
+
+  DEFAULT_CONFIG_LOCATION = File.join(File.dirname(__FILE__), "config", "platforms.json").freeze
+
+  attr_reader :platforms, :latest
+
+  def initialize(config_path = DEFAULT_CONFIG_LOCATION)
+    @config_path = config_path
+    @platforms = []
+
+    data = JSON.parse(File.read(@config_path))
+    data["platforms"].each do |k, v|
+      @platforms << Platform.new(k, v)
+    end
+    @platforms.sort! { |a, b| a.name <=> b.name }
+    @latest = data["latest"]
+  end
+
+  def find!(platform_name)
+    result = @platforms.find { |platform| platform.name == platform_name }.first
+    if result.nil?
+      raise "Cannot find platform named: #{platform_name} in @config_path"
+    else
+      return result
+    end
+  end
+
+  def each(&block)
+    @platforms.each(&block)
+  end
+
+  def filter_type(type_name, options={})
+    experimental = options.fetch("experimental", false)
+    @platforms.select { |platform| platform.type == type_name && platform.experimental == experimental }
+  end
+
+  def select_names_for(platform, options={})
+    filter_options = { "experimental" => options.fetch("experimental", false) }
+    !platform.nil? ? filter_type(platform, filter_options).map{ |p| p.name } : ""
+  end
+
+  def types
+    @platforms.collect(&:type).uniq.sort
+  end
+end
diff --git a/qa/rspec/commands.rb b/qa/rspec/commands.rb
new file mode 100644
index 00000000000..f90bc74cc97
--- /dev/null
+++ b/qa/rspec/commands.rb
@@ -0,0 +1,140 @@
+# encoding: utf-8
+require_relative "./commands/debian"
+require_relative "./commands/ubuntu"
+require_relative "./commands/redhat"
+require_relative "./commands/suse"
+require_relative "./commands/centos/centos-6"
+require_relative "./commands/oel/oel-6"
+require_relative "./commands/ubuntu/ubuntu-1604"
+require_relative "./commands/suse/sles-11"
+
+require "forwardable"
+
+module ServiceTester
+
+  # An artifact is the component being tested, it's able to interact with
+  # a destination machine by holding a client and is basically provides all 
+  # necessary abstractions to make the test simple.
+  class Artifact
+
+    extend Forwardable
+    def_delegators :@client, :installed?, :removed?, :running?
+
+    attr_reader :host, :client
+
+    def initialize(host, options={})
+      @host    = host
+      @options = options
+      @client  = CommandsFactory.fetch(options["type"], options["host"])
+    end
+
+    def hostname
+      @options["host"]
+    end
+
+    def name
+      "logstash"
+    end
+
+    def hosts
+      [@host]
+    end
+
+    def snapshot
+      client.snapshot(@options["host"])
+    end
+
+    def restore
+      client.restore(@options["host"])
+    end
+
+    def start_service
+      client.start_service(name, host)
+    end
+
+    def stop_service
+      client.stop_service(name, host)
+    end
+
+    def install(options={})
+      base      = options.fetch(:base, ServiceTester::Base::LOCATION)
+      package   = client.package_for(filename(options), base)
+      client.install(package, host)
+    end
+
+    def uninstall
+      client.uninstall(name, host)
+    end
+
+    def run_command_in_path(cmd)
+      client.run_command_in_path(cmd, host)
+    end
+
+    def run_command(cmd)
+      client.run_command(cmd, host)
+    end
+
+    def plugin_installed?(name, version = nil)
+      client.plugin_installed?(host, name, version)
+    end
+
+    def download(from, to)
+      client.download(from, to , host)
+    end
+    
+    def replace_in_gemfile(pattern, replace)
+      client.replace_in_gemfile(pattern, replace, host)
+    end
+
+    def delete_file(path)
+      client.delete_file(path, host)
+    end
+
+    def to_s
+      "Artifact #{name}@#{host}"
+    end
+
+    private
+
+    def filename(options={})
+      snapshot  = options.fetch(:snapshot, true)
+      "logstash-#{options[:version]}#{(snapshot ?  "-SNAPSHOT" : "")}"
+    end
+  end
+
+  # Factory of commands used to select the right clients for a given type of OS and host name,
+  # this give you as much granularity as required.
+  class CommandsFactory
+
+    def self.fetch(type, host)
+      case type
+      when "debian"
+        if host.start_with?("ubuntu")
+          if host == "ubuntu-1604"
+            return Ubuntu1604Commands.new
+          else
+            return UbuntuCommands.new
+          end
+        else
+          return DebianCommands.new
+        end
+      when "suse"
+        if host == "sles-11"
+          return Sles11Commands.new
+        else
+          return SuseCommands.new
+        end
+      when "redhat"
+        if host == "centos-6"
+          return Centos6Commands.new
+        elsif host == "oel-6"
+          return Oel6Commands.new
+        else
+          return RedhatCommands.new
+        end
+      else
+        return
+      end
+    end
+  end
+end
diff --git a/qa/rspec/commands/base.rb b/qa/rspec/commands/base.rb
new file mode 100644
index 00000000000..3f06c2a02b1
--- /dev/null
+++ b/qa/rspec/commands/base.rb
@@ -0,0 +1,70 @@
+# encoding: utf-8
+require_relative "../../vagrant/helpers"
+require_relative "system_helpers"
+
+module ServiceTester
+
+  class InstallException < Exception; end
+
+  class Base
+    LOCATION="/logstash-build".freeze
+    LOGSTASH_PATH="/usr/share/logstash/".freeze
+
+    def snapshot(host)
+      LogStash::VagrantHelpers.save_snapshot(host)
+    end
+
+    def restore(host)
+      LogStash::VagrantHelpers.restore_snapshot(host)
+    end
+
+    def start_service(service, host=nil)
+      service_manager(service, "start", host)
+    end
+
+    def stop_service(service, host=nil)
+      service_manager(service, "stop", host)
+    end
+
+    def run_command(cmd, host)
+      hosts = (host.nil? ? servers : Array(host))
+
+      response = nil
+      at(hosts, {in: :serial}) do |_host|
+        response = sudo_exec!("JARS_SKIP='true' #{cmd}")
+      end
+      response
+    end
+
+    def replace_in_gemfile(pattern, replace, host)
+      gemfile = File.join(LOGSTASH_PATH, "Gemfile")
+      cmd = "sed -i.sedbak 's/#{pattern}/#{replace}/' #{gemfile}"
+      run_command(cmd, host)
+    end
+
+    def run_command_in_path(cmd, host)
+      run_command("#{File.join(LOGSTASH_PATH, cmd)}", host)
+    end
+
+    def plugin_installed?(host, plugin_name, version = nil)
+      if version.nil?
+        cmd = run_command_in_path("bin/logstash-plugin list", host)
+        search_token = plugin_name
+      else
+        cmd = run_command_in_path("bin/logstash-plugin list --verbose", host)
+        search_token ="#{plugin_name} (#{version})"
+      end
+
+      plugins_list = cmd.stdout.split("\n")
+      plugins_list.include?(search_token)
+    end
+
+    def download(from, to, host)
+      run_command("wget #{from} -O #{to}", host)
+    end
+
+    def delete_file(path, host)
+      run_command("rm -rf #{path}", host)
+    end
+  end
+end
diff --git a/qa/rspec/commands/centos/centos-6.rb b/qa/rspec/commands/centos/centos-6.rb
new file mode 100644
index 00000000000..371590490e6
--- /dev/null
+++ b/qa/rspec/commands/centos/centos-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Centos6Commands < RedhatCommands
+      include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/debian.rb b/qa/rspec/commands/debian.rb
new file mode 100644
index 00000000000..30fa8c8daf6
--- /dev/null
+++ b/qa/rspec/commands/debian.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class DebianCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Package: #{package}$/)
+      stdout.match(/^Status: install ok installed$/)
+    end
+
+    def package_for(filename, base=ServiceTester::Base::LOCATION)
+      File.join(base, "#{filename}.deb")
+    end
+
+    def install(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      errors = []
+      at(hosts, {in: :serial}) do |_|
+        cmd = sudo_exec!("dpkg -i --force-confnew #{package}")
+        if cmd.exit_status != 0
+          errors << cmd.stderr.to_s
+        end
+      end
+      raise InstallException.new(errors.join("\n")) unless errors.empty?
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("dpkg -r #{package}")
+        sudo_exec!("dpkg --purge #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("dpkg -s #{package}")
+        stdout = cmd.stderr
+      end
+      (
+        stdout.match(/^Package `#{package}' is not installed and no info is available.$/) ||
+        stdout.match(/^dpkg-query: package '#{package}' is not installed and no information is available$/)
+      )
+    end
+  end
+end
diff --git a/qa/rspec/commands/oel/oel-6.rb b/qa/rspec/commands/oel/oel-6.rb
new file mode 100644
index 00000000000..ed92a8ce11d
--- /dev/null
+++ b/qa/rspec/commands/oel/oel-6.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../redhat"
+
+module ServiceTester
+  class Oel6Commands < RedhatCommands
+    include ::ServiceTester::InitD
+  end
+end
diff --git a/qa/rspec/commands/redhat.rb b/qa/rspec/commands/redhat.rb
new file mode 100644
index 00000000000..b7dce804869
--- /dev/null
+++ b/qa/rspec/commands/redhat.rb
@@ -0,0 +1,53 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class RedhatCommands < Base
+
+    include ::ServiceTester::SystemD
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("yum list installed  #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^Installed Packages$/)
+      stdout.match(/^logstash.noarch/)
+    end
+
+    def package_for(filename, base=ServiceTester::Base::LOCATION)
+      File.join(base, "#{filename}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = []
+      exit_status = 0
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("yum install -y  #{package}")
+        exit_status += cmd.exit_status
+        errors << cmd.stderr unless cmd.stderr.empty?
+      end
+      if exit_status > 0 
+        raise InstallException.new(errors.join("\n"))
+      end
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("yum remove -y #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("yum list installed #{package}")
+        stdout = cmd.stderr
+      end
+      stdout.match(/^Error: No matching Packages to list$/)
+    end
+  end
+end
diff --git a/qa/rspec/commands/suse.rb b/qa/rspec/commands/suse.rb
new file mode 100644
index 00000000000..9b1e6eee902
--- /dev/null
+++ b/qa/rspec/commands/suse.rb
@@ -0,0 +1,63 @@
+# encoding: utf-8
+require_relative "base"
+
+module ServiceTester
+  class SuseCommands < Base
+
+    def installed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^i | logstash | An extensible logging pipeline | package$/)
+    end
+
+    def package_for(filename, base=ServiceTester::Base::LOCATION)
+      File.join(base, "#{filename}.rpm")
+    end
+
+    def install(package, host=nil)
+      hosts  = (host.nil? ? servers : Array(host))
+      errors = []
+      at(hosts, {in: :serial}) do |_host|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive install  #{package}")
+        errors << cmd.stderr unless cmd.stderr.empty?
+      end
+      raise InstallException.new(errors.join("\n")) unless errors.empty?
+    end
+
+    def uninstall(package, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        cmd = sudo_exec!("zypper --no-gpg-checks --non-interactive remove #{package}")
+      end
+    end
+
+    def removed?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd    = exec!("zypper search #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/No packages found/)
+    end
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/Active: active \(running\)/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/suse/sles-11.rb b/qa/rspec/commands/suse/sles-11.rb
new file mode 100644
index 00000000000..80dd94dd719
--- /dev/null
+++ b/qa/rspec/commands/suse/sles-11.rb
@@ -0,0 +1,25 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../suse"
+
+module ServiceTester
+  class Sles11Commands < SuseCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("/etc/init.d/#{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} is running$/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("/etc/init.d/#{service} #{action}")
+      end
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/system_helpers.rb b/qa/rspec/commands/system_helpers.rb
new file mode 100644
index 00000000000..8cb8922946b
--- /dev/null
+++ b/qa/rspec/commands/system_helpers.rb
@@ -0,0 +1,42 @@
+require_relative "base"
+
+module ServiceTester
+  module SystemD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      (
+        stdout.match(/Active: active \(running\)/) &&
+        stdout.match(/#{package}.service - #{package}/)
+      )
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("service #{service} #{action}")
+      end
+    end
+  end
+
+  module InitD
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("initctl status #{package}")
+        stdout = cmd.stdout
+      end
+      stdout.match(/#{package} start\/running/)
+    end
+
+    def service_manager(service, action, host=nil)
+      hosts = (host.nil? ? servers : Array(host))
+      at(hosts, {in: :serial}) do |_|
+        sudo_exec!("initctl #{action} #{service}")
+      end
+    end 
+  end
+end
diff --git a/qa/rspec/commands/ubuntu.rb b/qa/rspec/commands/ubuntu.rb
new file mode 100644
index 00000000000..1d1ae75f96d
--- /dev/null
+++ b/qa/rspec/commands/ubuntu.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require_relative "debian"
+
+module ServiceTester
+  class UbuntuCommands < DebianCommands
+
+    def running?(hosts, package)
+      stdout = ""
+      at(hosts, {in: :serial}) do |host|
+        cmd = sudo_exec!("service #{package} status")
+        stdout = cmd.stdout
+      end
+      stdout.match(/^#{package} start\/running/)
+    end
+
+  end
+end
diff --git a/qa/rspec/commands/ubuntu/ubuntu-1604.rb b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
new file mode 100644
index 00000000000..ae26bc09f28
--- /dev/null
+++ b/qa/rspec/commands/ubuntu/ubuntu-1604.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require_relative "../base"
+require_relative "../ubuntu"
+
+module ServiceTester
+  class Ubuntu1604Commands < UbuntuCommands
+      include ::ServiceTester::SystemD
+  end
+end
diff --git a/qa/rspec/helpers.rb b/qa/rspec/helpers.rb
new file mode 100644
index 00000000000..a939fa7dfca
--- /dev/null
+++ b/qa/rspec/helpers.rb
@@ -0,0 +1,38 @@
+# encoding: utf-8
+require_relative "commands"
+
+module ServiceTester
+
+  class Configuration
+    attr_accessor :servers, :lookup
+    def initialize
+      @servers  = []
+      @lookup   = {}
+    end
+
+    def hosts
+      lookup.values.map { |val| val["host"] }
+    end
+  end
+
+  class << self
+    attr_accessor :configuration
+  end
+
+  def self.configure
+    self.configuration ||= Configuration.new
+    yield(configuration) if block_given?
+  end
+
+  def servers
+    ServiceTester.configuration.servers
+  end
+
+  def select_client
+    CommandsFactory.fetch(current_example.metadata[:platform])
+  end
+
+  def current_example
+    RSpec.respond_to?(:current_example) ? RSpec.current_example : self.example
+  end
+end
diff --git a/qa/rspec/matchers.rb b/qa/rspec/matchers.rb
new file mode 100644
index 00000000000..4da583262f2
--- /dev/null
+++ b/qa/rspec/matchers.rb
@@ -0,0 +1,4 @@
+# encoding: utf-8
+require_relative "./matchers/be_installed"
+require_relative "./matchers/be_running"
+require_relative "./matchers/cli_matchers"
diff --git a/qa/rspec/matchers/be_installed.rb b/qa/rspec/matchers/be_installed.rb
new file mode 100644
index 00000000000..4de70ae21ed
--- /dev/null
+++ b/qa/rspec/matchers/be_installed.rb
@@ -0,0 +1,15 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_installed do
+  match do |subject|
+    subject.installed?(subject.hosts, subject.name)
+  end
+end
+
+RSpec::Matchers.define :be_removed do
+  match do |subject|
+    subject.removed?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/be_running.rb b/qa/rspec/matchers/be_running.rb
new file mode 100644
index 00000000000..dc687e1b11d
--- /dev/null
+++ b/qa/rspec/matchers/be_running.rb
@@ -0,0 +1,9 @@
+# encoding: utf-8
+require 'rspec/expectations'
+require_relative '../helpers'
+
+RSpec::Matchers.define :be_running do
+  match do |subject|
+    subject.running?(subject.hosts, subject.name)
+  end
+end
diff --git a/qa/rspec/matchers/cli_matchers.rb b/qa/rspec/matchers/cli_matchers.rb
new file mode 100644
index 00000000000..e31aa050af3
--- /dev/null
+++ b/qa/rspec/matchers/cli_matchers.rb
@@ -0,0 +1,31 @@
+# encoding: utf-8
+RSpec::Matchers.define :be_successful do
+  match do |actual|
+    actual.exit_status == 0
+  end
+end
+
+RSpec::Matchers.define :fail_and_output do |expected_output|
+  match do |actual|
+    actual.exit_status == 1 && actual.stderr =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :run_successfully_and_output do |expected_output|
+  match do |actual|
+    (actual.exit_status == 0 || actual.exit_status.nil?) && actual.stdout =~ expected_output
+  end
+end
+
+RSpec::Matchers.define :have_installed? do |name,*args|
+  match do |actual|
+    version = args.first
+    actual.plugin_installed?(name, version)
+  end
+end
+
+RSpec::Matchers.define :install_successfully do
+  match do |cmd|
+    expect(cmd).to run_successfully_and_output(/Installation successful/)
+  end
+end
diff --git a/test/windows/acceptance/logstash_release_acceptance.ps1 b/qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
similarity index 96%
rename from test/windows/acceptance/logstash_release_acceptance.ps1
rename to qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
index da812277543..397b490781d 100644
--- a/test/windows/acceptance/logstash_release_acceptance.ps1
+++ b/qa/scripts/windows/acceptance/logstash_release_acceptance.ps1
@@ -6,7 +6,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 $LS_CONFIG="test.conf"
 $LS_BRANCH=$env:LS_BRANCH
diff --git a/test/windows/acceptance/logstash_release_default_plugins.ps1 b/qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
similarity index 96%
rename from test/windows/acceptance/logstash_release_default_plugins.ps1
rename to qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
index f4cf177e718..77b121fc6d3 100644
--- a/test/windows/acceptance/logstash_release_default_plugins.ps1
+++ b/qa/scripts/windows/acceptance/logstash_release_default_plugins.ps1
@@ -6,7 +6,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 # - Ruby 7 or newer
 
 $ruby = $env:RUBY_HOME  + "\jruby.exe"
diff --git a/test/windows/event_log/logstash_event_log_plugin_integration.ps1 b/qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
similarity index 98%
rename from test/windows/event_log/logstash_event_log_plugin_integration.ps1
rename to qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
index a267c05e72d..710daf8573a 100644
--- a/test/windows/event_log/logstash_event_log_plugin_integration.ps1
+++ b/qa/scripts/windows/event_log/logstash_event_log_plugin_integration.ps1
@@ -8,7 +8,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 Add-Type -assembly "system.io.compression.filesystem"
 
diff --git a/test/windows/integration/logstash_simple_integration.ps1 b/qa/scripts/windows/integration/logstash_simple_integration.ps1
similarity index 98%
rename from test/windows/integration/logstash_simple_integration.ps1
rename to qa/scripts/windows/integration/logstash_simple_integration.ps1
index 73fe2fcff7e..fb861cd2e98 100644
--- a/test/windows/integration/logstash_simple_integration.ps1
+++ b/qa/scripts/windows/integration/logstash_simple_integration.ps1
@@ -8,7 +8,7 @@
 #
 # - Powershell 4
 # - Windows 7 or newer
-# - Java 7 or newer
+# - Java 8 or newer
 
 Add-Type -assembly "system.io.compression.filesystem"
 
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
new file mode 100644
index 00000000000..1e9fe168abe
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-newer.gemspec
@@ -0,0 +1,25 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.1'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
+
diff --git a/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
new file mode 100644
index 00000000000..82e3be79baf
--- /dev/null
+++ b/qa/support/logstash-filter-qatest/logstash-filter-qatest-old.gemspec
@@ -0,0 +1,24 @@
+Gem::Specification.new do |s|
+  s.name            = 'logstash-filter-qatest'
+  s.version         = '0.1.0'
+  s.licenses        = ['Apache License (2.0)']
+  s.summary         = "This plugin is only used in the acceptance test"
+  s.description     = "This plugin is only used in the acceptance test"
+  s.authors         = ["Elasticsearch"]
+  s.email           = 'info@elasticsearch.com'
+  s.homepage        = "http://www.elasticsearch.org/guide/en/logstash/current/index.html"
+  s.require_paths = ["lib"]
+
+  # Files
+  s.files = `git ls-files`.split($\)+::Dir.glob('vendor/*')
+
+  # Tests
+  s.test_files = s.files.grep(%r{^(test|spec|features)/})
+
+  # Special flag to let us know this is actually a logstash plugin
+  s.metadata = { "logstash_plugin" => "true", "logstash_group" => "filter" }
+
+  # Gem dependencies
+  s.add_development_dependency 'logstash-devutils'
+  s.add_runtime_dependency "logstash-core-plugin-api", "~> 2.0"
+end
diff --git a/qa/sys/debian/bootstrap.sh b/qa/sys/debian/bootstrap.sh
new file mode 100644
index 00000000000..29c2c840346
--- /dev/null
+++ b/qa/sys/debian/bootstrap.sh
@@ -0,0 +1,7 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
diff --git a/qa/sys/debian/debian-8/bootstrap.sh b/qa/sys/debian/debian-8/bootstrap.sh
new file mode 100644
index 00000000000..d1a23d54430
--- /dev/null
+++ b/qa/sys/debian/debian-8/bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+echo "deb http://http.debian.net/debian jessie-backports main" >> /etc/apt/sources.list
+apt-get update
+apt-get install -y openjdk-8-jdk
diff --git a/qa/sys/debian/ubuntu-1404/bootstrap.sh b/qa/sys/debian/ubuntu-1404/bootstrap.sh
new file mode 100644
index 00000000000..728b0c3d13f
--- /dev/null
+++ b/qa/sys/debian/ubuntu-1404/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+add-apt-repository ppa:openjdk-r/ppa
+apt-get update
+apt-get install -y openjdk-8-jdk
+update-alternatives --config java
+update-alternatives --config javac
+update-ca-certificates -f
diff --git a/qa/sys/debian/user_bootstrap.sh b/qa/sys/debian/user_bootstrap.sh
new file mode 100644
index 00000000000..a99d4ff056f
--- /dev/null
+++ b/qa/sys/debian/user_bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.deb"
+wget -q https://download.elastic.co/logstash/logstash/packages/debian/$LOGSTASH_FILENAME
diff --git a/qa/sys/redhat/bootstrap.sh b/qa/sys/redhat/bootstrap.sh
new file mode 100644
index 00000000000..5976f47722f
--- /dev/null
+++ b/qa/sys/redhat/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+yum update
+yum install -y java-1.8.0-openjdk-devel.x86_64
diff --git a/qa/sys/redhat/user_bootstrap.sh b/qa/sys/redhat/user_bootstrap.sh
new file mode 100644
index 00000000000..4713b3f5c1b
--- /dev/null
+++ b/qa/sys/redhat/user_bootstrap.sh
@@ -0,0 +1,5 @@
+#!/usr/bin/env bash
+
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
diff --git a/qa/sys/suse/bootstrap.sh b/qa/sys/suse/bootstrap.sh
new file mode 100644
index 00000000000..4dba83eb9ea
--- /dev/null
+++ b/qa/sys/suse/bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+
+zypper --non-interactive list-updates
+zypper --non-interactive --no-gpg-checks --quiet install --no-recommends java-1_8_0-openjdk-devel
diff --git a/qa/sys/suse/sles-11/bootstrap.sh b/qa/sys/suse/sles-11/bootstrap.sh
new file mode 100644
index 00000000000..654be5d7ec0
--- /dev/null
+++ b/qa/sys/suse/sles-11/bootstrap.sh
@@ -0,0 +1,8 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+ln -s /usr/sbin/update-alternatives /usr/sbin/alternatives
+curl -L 'https://edelivery.oracle.com/otn-pub/java/jdk/8u77-b03/jdk-8u77-linux-x64.rpm' -H 'Accept-Encoding: gzip, deflate, sdch' -H 'Accept-Language: en-US,en;q=0.8' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0' -H 'Cookie: oraclelicense=accept-securebackup-cookie;' -H 'Connection: keep-alive' --compressed -o oracle_jdk_1.8.rpm
+zypper -q -n --non-interactive install oracle_jdk_1.8.rpm
diff --git a/qa/sys/suse/sles-12/bootstrap.sh b/qa/sys/suse/sles-12/bootstrap.sh
new file mode 100644
index 00000000000..56b4d0fd7d6
--- /dev/null
+++ b/qa/sys/suse/sles-12/bootstrap.sh
@@ -0,0 +1,11 @@
+#!/usr/bin/env bash
+
+zypper rr systemsmanagement_puppet
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD1/ dvd1 || true
+zypper addrepo -t yast2 http://demeter.uni-regensburg.de/SLES12-x64/DVD2/ dvd2 || true
+zypper addrepo http://download.opensuse.org/repositories/Java:Factory/SLE_12/Java:Factory.repo || true
+zypper --no-gpg-checks --non-interactive refresh
+zypper --non-interactive list-updates
+ln -s /usr/sbin/update-alternatives /usr/sbin/alternatives
+curl -L 'https://edelivery.oracle.com/otn-pub/java/jdk/8u77-b03/jdk-8u77-linux-x64.rpm' -H 'Accept-Encoding: gzip, deflate, sdch' -H 'Accept-Language: en-US,en;q=0.8' -H 'Upgrade-Insecure-Requests: 1' -H 'User-Agent: Mozilla/5.0' -H 'Cookie: oraclelicense=accept-securebackup-cookie;' -H 'Connection: keep-alive' --compressed -o oracle_jdk_1.8.rpm
+zypper -q -n --non-interactive install oracle_jdk_1.8.rpm
diff --git a/qa/sys/suse/user_bootstrap.sh b/qa/sys/suse/user_bootstrap.sh
new file mode 100644
index 00000000000..be22af8b0c1
--- /dev/null
+++ b/qa/sys/suse/user_bootstrap.sh
@@ -0,0 +1,4 @@
+#!/usr/bin/env bash
+VERSION=`cat /vagrant/config/platforms.json | grep  latest | cut -d":" -f2 | sed 's/["\|,| ]//g'`
+LOGSTASH_FILENAME="logstash-${VERSION}.rpm"
+wget -q https://download.elastic.co/logstash/logstash/packages/centos/$LOGSTASH_FILENAME
diff --git a/qa/vagrant/command.rb b/qa/vagrant/command.rb
new file mode 100644
index 00000000000..a07b71f817a
--- /dev/null
+++ b/qa/vagrant/command.rb
@@ -0,0 +1,71 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+
+module LogStash
+  class CommandExecutor
+    class CommandError < StandardError; end
+
+    class CommandResponse
+      attr_reader :stdin, :stdout, :stderr, :exitstatus
+
+      def initialize(stdin, stdout, stderr, exitstatus)
+        @stdin = stdin
+        @stdout = stdout
+        @stderr = stderr
+        @exitstatus = exitstatus
+      end
+
+      def success?
+        exitstatus == 0
+      end
+    end
+
+    def self.run(cmd, debug=false)
+      # This block is require to be able to launch a ruby subprocess
+      # that use bundler.
+      Bundler.with_clean_env do
+        stdin, stdout, stderr, wait_thr = Open3.popen3(cmd)
+        stdout_acc, stderr_acc = "", ""
+        stdout_reporter = reporter(stdout, wait_thr) do |c|
+          stdout_acc << c
+          print c if debug
+        end
+        reporter(stderr, wait_thr) do |c|
+          stderr_acc << c;
+          print c if debug
+        end
+        stdout_reporter.join
+        CommandResponse.new(stdin, stdout_acc, stderr_acc, wait_thr.value.exitstatus)
+      end
+    end
+
+    # This method will raise an exception if the `CMD`
+    # was not run successfully and will display the content of STDERR
+    def self.run!(cmd, debug=false)
+      response = run(cmd, debug)
+
+      unless response.success?
+        raise CommandError, "CMD: #{cmd} STDERR: #{response.stderr}"
+      end
+      response
+    end
+
+    private
+
+    def self.reporter(io, wait_thr, &block)
+      Thread.new(io, wait_thr) do |_io, _wait_thr|
+        while (_wait_thr.status == "run")
+          begin
+            c = _io.read(1)
+            block.call(c) if c
+          rescue IO::WaitReadable
+            IO.select([_io])
+            retry
+          end
+        end
+      end
+    end
+
+  end
+end
diff --git a/qa/vagrant/helpers.rb b/qa/vagrant/helpers.rb
new file mode 100644
index 00000000000..96ea1992a02
--- /dev/null
+++ b/qa/vagrant/helpers.rb
@@ -0,0 +1,55 @@
+# encoding: utf-8
+require "open3"
+require "bundler"
+require_relative "command"
+
+module LogStash
+  class VagrantHelpers
+
+    def self.halt(machines="", options={})
+      debug = options.fetch(:debug, false)
+      CommandExecutor.run!("vagrant halt #{machines.join(' ')}", debug)
+    end
+
+    def self.destroy(machines="", options={})
+      debug = options.fetch(:debug, false)
+      CommandExecutor.run!("vagrant destroy --force #{machines.join(' ')}", debug) 
+    end
+
+    def self.bootstrap(machines="", options={})
+      debug = options.fetch(:debug, false)
+      CommandExecutor.run!("vagrant up #{machines.join(' ')}", debug)
+    end
+
+    def self.save_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot save #{machine} #{machine}-snapshot")
+    end
+
+    def self.restore_snapshot(machine="")
+      CommandExecutor.run!("vagrant snapshot restore #{machine} #{machine}-snapshot")
+    end
+
+    def self.fetch_config
+      machines = CommandExecutor.run!("vagrant status").stdout.split("\n").select { |l| l.include?("running") }.map { |r| r.split(' ')[0]}
+      CommandExecutor.run!("vagrant ssh-config #{machines.join(' ')}")
+    end
+
+    def self.parse(lines)
+      hosts, host = [], {}
+      lines.each do |line|
+        if line.match(/Host\s(.*)$/)
+          host = { :host => line.gsub("Host","").strip }
+        elsif line.match(/HostName\s(.*)$/)
+          host[:hostname] = line.gsub("HostName","").strip
+        elsif line.match(/Port\s(.*)$/)
+          host[:port]     = line.gsub("Port","").strip
+        elsif line.empty?
+          hosts << host
+          host = {}
+        end
+      end
+      hosts << host
+      hosts
+    end
+  end
+end
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index bc3d086e105..c00202a4d1e 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -2,6 +2,9 @@ require "logstash/version"
 
 namespace "artifact" do
 
+  SNAPSHOT_BUILD = ENV["RELEASE"] != "1"
+  PACKAGE_SUFFIX = SNAPSHOT_BUILD ? "-SNAPSHOT" : ""
+
   def package_files
     [
       "LICENSE",
@@ -9,8 +12,19 @@ namespace "artifact" do
       "NOTICE.TXT",
       "CONTRIBUTORS",
       "bin/**/*",
+      "config/**/*",
+      "data",
       "lib/bootstrap/**/*",
       "lib/pluginmanager/**/*",
+      "lib/systeminstall/**/*",
+      "logstash-core/lib/**/*",
+      "logstash-core/locales/**/*",
+      "logstash-core/vendor/**/*",
+      "logstash-core/*.gemspec",
+      "logstash-core-event-java/lib/**/*",
+      "logstash-core-event-java/*.gemspec",
+      "logstash-core-plugin-api/lib/**/*",
+      "logstash-core-plugin-api/*.gemspec",
       "patterns/**/*",
       "vendor/??*/**/*",
       # To include ruby-maven's hidden ".mvn" directory, we need to
@@ -33,6 +47,8 @@ namespace "artifact" do
     @exclude_paths << "**/test/files/slow-xpath.xml"
     @exclude_paths << "**/logstash-*/spec"
     @exclude_paths << "bin/bundle"
+    @exclude_paths << "bin/rspec"
+    @exclude_paths << "bin/rspec.bat"
 
     @exclude_paths
   end
@@ -53,6 +69,77 @@ namespace "artifact" do
     end.flatten.uniq
   end
 
+  desc "Generate rpm, deb, tar and zip artifacts"
+  task "all" => ["prepare", "build"]
+
+  desc "Build a tar.gz of default logstash plugins with all dependencies"
+  task "tar" => ["prepare", "generate_build_metadata"] do
+    puts("[artifact:tar] Building tar.gz of default plugins")
+    build_tar
+  end
+
+  desc "Build a zip of default logstash plugins with all dependencies"
+  task "zip" => ["prepare", "generate_build_metadata"] do
+    puts("[artifact:zip] Building zip of default plugins")
+    build_zip
+  end
+
+  desc "Build an RPM of logstash with all dependencies"
+  task "rpm" => ["prepare", "generate_build_metadata"] do
+    puts("[artifact:rpm] building rpm package")
+    package("centos", "5")
+  end
+
+  desc "Build a DEB of logstash with all dependencies"
+  task "deb" => ["prepare", "generate_build_metadata"] do
+    puts("[artifact:deb] building deb package")
+    package("ubuntu", "12.04")
+  end
+
+  desc "Generate logstash core gems"
+  task "gems" => ["prepare"] do
+    Rake::Task["artifact:build-logstash-core"].invoke
+    Rake::Task["artifact:build-logstash-core-event"].invoke
+    Rake::Task["artifact:build-logstash-core-plugin-api"].invoke
+  end
+
+  # "all-plugins" version of tasks
+  desc "Generate rpm, deb, tar and zip artifacts (\"all-plugins\" version)"
+  task "all-all-plugins" => ["prepare-all", "build"]
+
+  desc "Build a zip of all logstash plugins from logstash-plugins github repo"
+  task "zip-all-plugins" => ["prepare-all", "generate_build_metadata"] do
+    puts("[artifact:zip] Building zip of all plugins")
+    build_zip "-all-plugins"
+  end
+
+  desc "Build a tar.gz of all logstash plugins from logstash-plugins github repo"
+  task "tar-all-plugins" => ["prepare-all", "generate_build_metadata"] do
+    puts("[artifact:tar] Building tar.gz of all plugins")
+    build_tar "-all-plugins"
+  end
+
+  # Auxiliary tasks
+  task "build" => [:generate_build_metadata] do
+    Rake::Task["artifact:gems"].invoke unless SNAPSHOT_BUILD
+    Rake::Task["artifact:deb"].invoke
+    Rake::Task["artifact:rpm"].invoke
+    Rake::Task["artifact:zip"].invoke
+    Rake::Task["artifact:tar"].invoke
+  end
+
+  task "generate_build_metadata" do
+    return if defined?(BUILD_METADATA_FILE)
+    BUILD_METADATA_FILE = Tempfile.new('build.rb')
+    build_info = {
+      "build_date" => Time.now.iso8601,
+      "build_sha" => `git rev-parse HEAD`.chomp,
+      "build_snapshot" => SNAPSHOT_BUILD
+    }
+    metadata = [ "# encoding: utf-8", "BUILD_INFO = #{build_info}" ]
+    IO.write(BUILD_METADATA_FILE.path, metadata.join("\n"))
+  end
+
   # We create an empty bundle config file
   # This will allow the deb and rpm to create a file
   # with the correct user group and permission.
@@ -62,8 +149,8 @@ namespace "artifact" do
   end
 
   # locate the "gem "logstash-core" ..." line in Gemfile, and if the :path => "..." option if specified
-  # build and install the local logstash-core gem otherwise just do nothing, bundler will deal with it.
-  task "install-logstash-core" do
+  # build the local logstash-core gem otherwise just do nothing, bundler will deal with it.
+  task "build-logstash-core" do
     # regex which matches a Gemfile gem definition for the logstash-core gem and captures the :path option
     gem_line_regex = /^\s*gem\s+["']logstash-core["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
 
@@ -74,15 +161,16 @@ namespace "artifact" do
     path = matches.first[gem_line_regex, 1]
 
     if path
-      Rake::Task["plugin:install-local-core-gem"].invoke("logstash-core", path)
+      Rake::Task["plugin:build-local-core-gem"].invoke("logstash-core", path)
     else
-      puts("[artifact:install-logstash-core] using logstash-core from Rubygems")
+      puts "The Gemfile should reference \"logstash-core\" gem locally through :path, but found instead: #{matches}"
+      exit(1)
     end
   end
 
   # # locate the "gem "logstash-core-event*" ..." line in Gemfile, and if the :path => "." option if specified
-  # # build and install the local logstash-core-event* gem otherwise just do nothing, bundler will deal with it.
-  task "install-logstash-core-event" do
+  # # build the local logstash-core-event* gem otherwise just do nothing, bundler will deal with it.
+  task "build-logstash-core-event" do
     # regex which matches a Gemfile gem definition for the logstash-core-event* gem and captures the gem name and :path option
     gem_line_regex = /^\s*gem\s+["'](logstash-core-event[^"^']*)["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
 
@@ -94,85 +182,107 @@ namespace "artifact" do
     path = matches.first[gem_line_regex, 2]
 
     if path
-      Rake::Task["plugin:install-local-core-gem"].invoke(name, path)
+      Rake::Task["plugin:build-local-core-gem"].invoke(name, path)
     else
-      puts("[artifact:install-logstash-core] using #{name} from Rubygems")
+      puts "The Gemfile should reference \"logstash-core-event\" gem locally through :path, but found instead: #{matches}"
+      exit(1)
     end
   end
 
-  task "prepare" => ["bootstrap", "plugin:install-default", "install-logstash-core", "install-logstash-core-event", "clean-bundle-config"]
-  task "prepare-all" => ["bootstrap", "plugin:install-all", "install-logstash-core", "install-logstash-core-event", "clean-bundle-config"]
+  # locate the "gem "logstash-core-plugin-api" ..." line in Gemfile, and if the :path => "..." option if specified
+  # build the local logstash-core-plugin-api gem otherwise just do nothing, bundler will deal with it.
+  task "build-logstash-core-plugin-api" do
+    # regex which matches a Gemfile gem definition for the logstash-core gem and captures the :path option
+    gem_line_regex = /^\s*gem\s+["']logstash-core-plugin-api["'](?:\s*,\s*["'][^"^']+["'])?(?:\s*,\s*:path\s*=>\s*["']([^"^']+)["'])?/i
 
-  desc "Build a tar.gz of default logstash plugins with all dependencies"
-  task "tar" => ["prepare"] do
-    puts("[artifact:tar] Building tar.gz of default plugins")
-    build_tar
+    lines = File.readlines("Gemfile")
+    matches = lines.select{|line| line[gem_line_regex]}
+    abort("ERROR: Gemfile format error, need a single logstash-core-plugin-api gem specification") if matches.size != 1
+
+    path = matches.first[gem_line_regex, 1]
+
+    if path
+      Rake::Task["plugin:build-local-core-gem"].invoke("logstash-core-plugin-api", path)
+    else
+      puts "The Gemfile should reference \"logstash-core-plugin-api\" gem locally through :path, but found instead: #{matches}"
+      exit(1)
+    end
   end
 
-  desc "Build a tar.gz of all logstash plugins from logstash-plugins github repo"
-  task "tar-all-plugins" => ["prepare-all"] do
-    puts("[artifact:tar] Building tar.gz of all plugins")
-    build_tar "-all-plugins"
+  task "prepare" do
+    if ENV['SKIP_PREPARE'] != "1"
+      ["bootstrap", "plugin:install-default", "artifact:clean-bundle-config"].each {|task| Rake::Task[task].invoke }
+    end
+  end
+
+  task "prepare-all" do
+    if ENV['SKIP_PREPARE'] != "1"
+      ["bootstrap", "plugin:install-all", "artifact:clean-bundle-config"].each {|task| Rake::Task[task].invoke }
+    end
   end
 
   def build_tar(tar_suffix = nil)
     require "zlib"
     require "archive/tar/minitar"
     require "logstash/version"
-    tarpath = "build/logstash#{tar_suffix}-#{LOGSTASH_VERSION}.tar.gz"
+    tarpath = "build/logstash#{tar_suffix}-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}.tar.gz"
     puts("[artifact:tar] building #{tarpath}")
     gz = Zlib::GzipWriter.new(File.new(tarpath, "wb"), Zlib::BEST_COMPRESSION)
     tar = Archive::Tar::Minitar::Output.new(gz)
     files.each do |path|
-      stat = File.lstat(path)
-      path_in_tar = "logstash-#{LOGSTASH_VERSION}/#{path}"
-      opts = {
-        :size => stat.size,
-        :mode => stat.mode,
-        :mtime => stat.mtime
-      }
-      if stat.directory?
-        tar.tar.mkdir(path_in_tar, opts)
-      else
-        tar.tar.add_file_simple(path_in_tar, opts) do |io|
-          File.open(path,'rb') do |fd|
-            chunk = nil
-            size = 0
-            size += io.write(chunk) while chunk = fd.read(16384)
-            if stat.size != size
-              raise "Failure to write the entire file (#{path}) to the tarball. Expected to write #{stat.size} bytes; actually write #{size}"
-            end
-          end
-        end
-      end
+      write_to_tar(tar, path, "logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}/#{path}")
     end
+
+    # add build.rb to tar
+    metadata_file_path_in_tar = File.join("logstash-core", "lib", "logstash", "build.rb")
+    path_in_tar = File.join("logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}", metadata_file_path_in_tar)
+    write_to_tar(tar, BUILD_METADATA_FILE.path, path_in_tar)
+
     tar.close
     gz.close
     puts "Complete: #{tarpath}"
   end
 
-  desc "Build a zip of default logstash plugins with all dependencies"
-  task "zip" => ["prepare"] do
-    puts("[artifact:zip] Building zip of default plugins")
-    build_zip
-  end
-
-  desc "Build a zip of all logstash plugins from logstash-plugins github repo"
-  task "zip-all-plugins" => ["prepare-all"] do
-    puts("[artifact:zip] Building zip of all plugins")
-    build_zip "-all-plugins"
+  def write_to_tar(tar, path, path_in_tar)
+    stat = File.lstat(path)
+    opts = {
+      :size => stat.size,
+      :mode => stat.mode,
+      :mtime => stat.mtime
+    }
+    if stat.directory?
+      tar.tar.mkdir(path_in_tar, opts)
+    else
+      tar.tar.add_file_simple(path_in_tar, opts) do |io|
+        File.open(path,'rb') do |fd|
+          chunk = nil
+          size = 0
+          size += io.write(chunk) while chunk = fd.read(16384)
+          if stat.size != size
+            raise "Failure to write the entire file (#{path}) to the tarball. Expected to write #{stat.size} bytes; actually write #{size}"
+          end
+        end
+      end
+    end
   end
 
   def build_zip(zip_suffix = "")
     require 'zip'
-    zippath = "build/logstash#{zip_suffix}-#{LOGSTASH_VERSION}.zip"
+    zippath = "build/logstash#{zip_suffix}-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}.zip"
     puts("[artifact:zip] building #{zippath}")
     File.unlink(zippath) if File.exists?(zippath)
     Zip::File.open(zippath, Zip::File::CREATE) do |zipfile|
       files.each do |path|
-        path_in_zip = "logstash-#{LOGSTASH_VERSION}/#{path}"
+        path_in_zip = "logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}/#{path}"
         zipfile.add(path_in_zip, path)
       end
+
+      # add build.rb to zip
+      metadata_file_path_in_zip = File.join("logstash-core", "lib", "logstash", "build.rb")
+      path_in_zip = File.join("logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}", metadata_file_path_in_zip)
+      path = BUILD_METADATA_FILE.path
+      Zip.continue_on_exists_proc = true
+      zipfile.add(path_in_zip, path)
     end
     puts "Complete: #{zippath}"
   end
@@ -185,36 +295,46 @@ namespace "artifact" do
 
     dir = FPM::Package::Dir.new
 
+    metadata_file_path = File.join("logstash-core", "lib", "logstash", "build.rb")
+    metadata_source_file_path = BUILD_METADATA_FILE.path
+    dir.input("#{metadata_source_file_path}=/usr/share/logstash/#{metadata_file_path}")
+
     files.each do |path|
       next if File.directory?(path)
-      dir.input("#{path}=/opt/logstash/#{path}")
+      # Omit any config dir from /usr/share/logstash for packages, since we're
+      # using /etc/logstash below
+      next if path.start_with?("config/")
+      dir.input("#{path}=/usr/share/logstash/#{path}")
     end
 
     basedir = File.join(File.dirname(__FILE__), "..")
 
-    File.join(basedir, "pkg", "logrotate.conf").tap do |path|
-      dir.input("#{path}=/etc/logrotate.d/logstash")
-    end
-
     # Create an empty /var/log/logstash/ directory in the package
     # This is a bit obtuse, I suppose, but it is necessary until
     # we find a better way to do this with fpm.
     Stud::Temporary.directory do |empty|
+      dir.input("#{empty}/=/usr/share/logstash/data")
       dir.input("#{empty}/=/var/log/logstash")
       dir.input("#{empty}/=/var/lib/logstash")
       dir.input("#{empty}/=/etc/logstash/conf.d")
     end
 
+    File.join(basedir, "pkg", "log4j2.properties").tap do |path|
+      dir.input("#{path}=/etc/logstash")
+    end
+    
+    package_filename = "logstash-#{LOGSTASH_VERSION}#{PACKAGE_SUFFIX}.TYPE"
+
     case platform
       when "redhat", "centos"
-        File.join(basedir, "pkg", "logrotate.conf").tap do |path|
-          dir.input("#{path}=/etc/logrotate.d/logstash")
+        File.join(basedir, "pkg", "startup.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.default").tap do |path|
-          dir.input("#{path}=/etc/sysconfig/logstash")
+        File.join(basedir, "pkg", "jvm.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
-          dir.input("#{path}=/etc/init.d/logstash")
+        File.join(basedir, "config", "logstash.yml").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
         require "fpm/package/rpm"
         out = dir.convert(FPM::Package::RPM)
@@ -222,25 +342,29 @@ namespace "artifact" do
         out.attributes[:rpm_use_file_permissions] = true
         out.attributes[:rpm_user] = "root"
         out.attributes[:rpm_group] = "root"
-        out.config_files << "etc/sysconfig/logstash"
-        out.config_files << "etc/logrotate.d/logstash"
-        out.config_files << "/etc/init.d/logstash"
+        out.attributes[:rpm_os] = "linux"
+        out.config_files << "/etc/logstash/startup.options"
+        out.config_files << "/etc/logstash/jvm.options"
+        out.config_files << "/etc/logstash/logstash.yml"
       when "debian", "ubuntu"
-        File.join(basedir, "pkg", "logstash.default").tap do |path|
-          dir.input("#{path}=/etc/default/logstash")
+        File.join(basedir, "pkg", "startup.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
+        end
+        File.join(basedir, "pkg", "jvm.options").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
-        File.join(basedir, "pkg", "logstash.sysv").tap do |path|
-          dir.input("#{path}=/etc/init.d/logstash")
+        File.join(basedir, "config", "logstash.yml").tap do |path|
+          dir.input("#{path}=/etc/logstash")
         end
         require "fpm/package/deb"
         out = dir.convert(FPM::Package::Deb)
         out.license = "Apache 2.0"
         out.attributes[:deb_user] = "root"
         out.attributes[:deb_group] = "root"
-        out.attributes[:deb_suggests] = "java7-runtime-headless"
-        out.config_files << "/etc/default/logstash"
-        out.config_files << "/etc/logrotate.d/logstash"
-        out.config_files << "/etc/init.d/logstash"
+        out.attributes[:deb_suggests] = "java8-runtime-headless"
+        out.config_files << "/etc/logstash/startup.options"
+        out.config_files << "/etc/logstash/jvm.options"
+        out.config_files << "/etc/logstash/logstash.yml"
     end
 
     # Packaging install/removal scripts
@@ -265,7 +389,6 @@ namespace "artifact" do
     out.url = "http://www.elasticsearch.org/overview/logstash/"
     out.description = "An extensible logging pipeline"
     out.vendor = "Elasticsearch"
-    out.dependencies << "logrotate"
 
     # Because we made a mistake in naming the RC version numbers, both rpm and deb view
     # "1.5.0.rc1" higher than "1.5.0". Setting the epoch to 1 ensures that we get a kind
@@ -290,7 +413,7 @@ namespace "artifact" do
 
     out.attributes[:force?] = true # overwrite the rpm/deb/etc being created
     begin
-      path = File.join(basedir, "build", out.to_s)
+      path = File.join(basedir, "build", out.to_s(package_filename))
       x = out.output(path)
       puts "Completed: #{path}"
     ensure
@@ -298,15 +421,4 @@ namespace "artifact" do
     end
   end # def package
 
-  desc "Build an RPM of logstash with all dependencies"
-  task "rpm" => ["prepare"] do
-    puts("[artifact:rpm] building rpm package")
-    package("centos", "5")
-  end
-
-  desc "Build a DEB of logstash with all dependencies"
-  task "deb" => ["prepare"] do
-    puts("[artifact:deb] building deb package")
-    package("ubuntu", "12.04")
-  end
 end
diff --git a/rakelib/benchmark.rake b/rakelib/benchmark.rake
index 148922f6531..29bfd2c2844 100644
--- a/rakelib/benchmark.rake
+++ b/rakelib/benchmark.rake
@@ -1,7 +1,7 @@
 namespace :benchmark do
   desc "Run benchmark code in benchmark/*.rb"
   task :run => ["test:setup"] do
-    path = File.join(LogStash::Environment::LOGSTASH_HOME, "benchmark", "*.rb")
+    path = File.join(LogStash::Environment::LOGSTASH_HOME, "tools/benchmark", "*.rb")
     Dir.glob(path).each { |f| require f }
   end
 end
diff --git a/rakelib/build.rake b/rakelib/build.rake
index 2b443add8b7..45aec1f9b5f 100644
--- a/rakelib/build.rake
+++ b/rakelib/build.rake
@@ -4,3 +4,6 @@ end
 directory "build/bootstrap" => "build" do |task, args|
   mkdir_p task.name unless File.directory?(task.name)
 end
+directory "build/gems" => "build" do |task, args|
+  mkdir_p task.name unless File.directory?(task.name)
+end
diff --git a/rakelib/compile.rake b/rakelib/compile.rake
index aa20eb7091b..e5730125885 100644
--- a/rakelib/compile.rake
+++ b/rakelib/compile.rake
@@ -11,11 +11,16 @@ namespace "compile" do
 
   task "grammar" => "logstash-core/lib/logstash/config/grammar.rb"
 
+  task "logstash-core-java" do
+    puts("Building logstash-core using gradle")
+    system("./gradlew", "vendor", "-p", "./logstash-core")
+  end
+
   task "logstash-core-event-java" do
     puts("Building logstash-core-event-java using gradle")
-    system("logstash-core-event-java/gradlew", "jar", "-p", "./logstash-core-event-java")
+    system("./gradlew", "jar", "-p", "./logstash-core-event-java")
   end
 
   desc "Build everything"
-  task "all" => ["grammar", "logstash-core-event-java"]
+  task "all" => ["grammar", "logstash-core-java", "logstash-core-event-java"]
 end
diff --git a/rakelib/default_plugins.rb b/rakelib/default_plugins.rb
index 04e342d6199..73c1062fb74 100644
--- a/rakelib/default_plugins.rb
+++ b/rakelib/default_plugins.rb
@@ -4,7 +4,6 @@ module RakeLib
     # plugins included by default in the logstash distribution
     DEFAULT_PLUGINS = %w(
       logstash-input-heartbeat
-      logstash-output-zeromq
       logstash-codec-collectd
       logstash-output-xmpp
       logstash-codec-dots
@@ -19,11 +18,8 @@ module RakeLib
       logstash-codec-msgpack
       logstash-codec-multiline
       logstash-codec-netflow
-      logstash-codec-oldlogstashjson
       logstash-codec-plain
       logstash-codec-rubydebug
-      logstash-filter-anonymize
-      logstash-filter-checksum
       logstash-filter-clone
       logstash-filter-csv
       logstash-filter-date
@@ -35,7 +31,6 @@ module RakeLib
       logstash-filter-json
       logstash-filter-kv
       logstash-filter-metrics
-      logstash-filter-multiline
       logstash-filter-mutate
       logstash-filter-ruby
       logstash-filter-sleep
@@ -48,7 +43,6 @@ module RakeLib
       logstash-filter-xml
       logstash-input-couchdb_changes
       logstash-input-elasticsearch
-      logstash-input-eventlog
       logstash-input-exec
       logstash-input-file
       logstash-input-ganglia
@@ -75,27 +69,18 @@ module RakeLib
       logstash-input-udp
       logstash-input-unix
       logstash-input-xmpp
-      logstash-input-zeromq
       logstash-input-kafka
       logstash-input-beats
       logstash-output-cloudwatch
       logstash-output-csv
       logstash-output-elasticsearch
-      logstash-output-email
-      logstash-output-exec
       logstash-output-file
-      logstash-output-ganglia
-      logstash-output-gelf
       logstash-output-graphite
-      logstash-output-hipchat
       logstash-output-http
       logstash-output-irc
-      logstash-output-juggernaut
-      logstash-output-lumberjack
+      logstash-output-kafka
       logstash-output-nagios
-      logstash-output-nagios_nsca
       logstash-output-null
-      logstash-output-opentsdb
       logstash-output-pagerduty
       logstash-output-pipe
       logstash-output-rabbitmq
@@ -107,7 +92,7 @@ module RakeLib
       logstash-output-stdout
       logstash-output-tcp
       logstash-output-udp
-      logstash-output-kafka
+      logstash-output-webhdfs
     )
 
     # plugins required to run the logstash core specs
@@ -131,7 +116,6 @@ module RakeLib
 
     ALL_PLUGINS_SKIP_LIST = Regexp.union([
       /^logstash-filter-yaml$/,
-      /jms$/,
       /example$/,
       /drupal/i,
       /^logstash-output-logentries$/,
@@ -144,7 +128,13 @@ module RakeLib
       /^logstash-output-webhdfs$/,
       /^logstash-input-rackspace$/,
       /^logstash-output-rackspace$/,
-      /^logstash-input-dynamodb$/
+      /^logstash-input-dynamodb$/,
+      /^logstash-filter-language$/,
+      /^logstash-input-heroku$/,
+      /^logstash-output-google_cloud_storage$/,
+      /^logstash-input-journald$/,
+      /^logstash-input-log4j2$/,
+      /^logstash-codec-cloudtrail$/
     ])
 
 
diff --git a/rakelib/package.rake b/rakelib/package.rake
index 56606c93136..73885a013d2 100644
--- a/rakelib/package.rake
+++ b/rakelib/package.rake
@@ -1,7 +1,7 @@
 namespace "package" do
 
   task "bundle" do
-    system("bin/logstash-plugin", "package")
+    system("bin/logstash-plugin", "pack")
     raise(RuntimeError, $!.to_s) unless $?.success?
   end
 
diff --git a/rakelib/plugin.rake b/rakelib/plugin.rake
index 5620def5315..79be84172b7 100644
--- a/rakelib/plugin.rake
+++ b/rakelib/plugin.rake
@@ -1,4 +1,5 @@
 require_relative "default_plugins"
+require 'rubygems'
 
 namespace "plugin" do
 
@@ -9,7 +10,7 @@ namespace "plugin" do
 
   task "install-development-dependencies" do
     puts("[plugin:install-development-dependencies] Installing development dependencies of all installed plugins")
-    install_plugins("--development")
+    install_plugins("--development",  "--preserve")
 
     task.reenable # Allow this task to be run again
   end
@@ -17,35 +18,35 @@ namespace "plugin" do
   task "install", :name do |task, args|
     name = args[:name]
     puts("[plugin:install] Installing plugin: #{name}")
-    install_plugins("--no-verify", name)
+    install_plugins("--no-verify", "--preserve", name)
 
     task.reenable # Allow this task to be run again
   end # task "install"
 
   task "install-default" do
     puts("[plugin:install-default] Installing default plugins")
-    install_plugins("--no-verify", *LogStash::RakeLib::DEFAULT_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::DEFAULT_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-core" do
     puts("[plugin:install-core] Installing core plugins")
-    install_plugins("--no-verify", *LogStash::RakeLib::CORE_SPECS_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::CORE_SPECS_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-jar-dependencies" do
     puts("[plugin:install-jar-dependencies] Installing jar_dependencies plugins for testing")
-    install_plugins("--no-verify", *LogStash::RakeLib::TEST_JAR_DEPENDENCIES_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::TEST_JAR_DEPENDENCIES_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
 
   task "install-vendor" do
     puts("[plugin:install-jar-dependencies] Installing vendor plugins for testing")
-    install_plugins("--no-verify", *LogStash::RakeLib::TEST_VENDOR_PLUGINS)
+    install_plugins("--no-verify", "--preserve", *LogStash::RakeLib::TEST_VENDOR_PLUGINS)
 
     task.reenable # Allow this task to be run again
   end
@@ -58,7 +59,7 @@ namespace "plugin" do
     # TODO Push this downstream to #install_plugins
     p.each do |plugin|
       begin
-        install_plugins("--no-verify", plugin)
+        install_plugins("--no-verify", "--preserve", plugin)
       rescue
         puts "Unable to install #{plugin}. Skipping"
         next
@@ -80,7 +81,7 @@ namespace "plugin" do
     task.reenable # Allow this task to be run again
   end
 
-  task "build-local-core-gem", [:name, :path]  do |task, args|
+  task "build-local-core-gem", [:name, :path] => ["build/gems"]  do |task, args|
     name = args[:name]
     path = args[:path]
 
@@ -88,7 +89,12 @@ namespace "plugin" do
 
     puts("[plugin:build-local-core-gem] Building #{File.join(path, name)}.gemspec")
 
-    system("cd #{path}; gem build #{name}.gemspec")
+    gem_path = nil
+    Dir.chdir(path) do
+      spec = Gem::Specification.load("#{name}.gemspec")
+      gem_path = Gem::Package.build(spec)
+    end
+    FileUtils.cp(File.join(path, gem_path), "build/gems/")
 
     task.reenable # Allow this task to be run again
   end
diff --git a/rakelib/test.rake b/rakelib/test.rake
index 8c0d16ff4ef..a02a5c5d65a 100644
--- a/rakelib/test.rake
+++ b/rakelib/test.rake
@@ -24,7 +24,7 @@ namespace "test" do
     # logstash-core-event specs since currently this is the most complete Event and Timestamp specs
     # which actually defines the Event contract and should pass regardless of the actuall underlying
     # implementation.
-    specs = ["spec/**/*_spec.rb", "logstash-core/spec/**/*_spec.rb", "logstash-core-event/spec/**/*_spec.rb"]
+    specs = ["spec/unit/**/*_spec.rb", "logstash-core/spec/**/*_spec.rb", "logstash-core-event/spec/**/*_spec.rb"]
 
     # figure if the logstash-core-event-java gem is loaded and if so add its specific specs in the core specs to run
     begin
@@ -104,29 +104,6 @@ namespace "test" do
     end
     task.reenable
   end
-
-  task "integration" => ["setup"] do
-    require "fileutils" 
-
-    source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-    integration_path = File.join(source, "integration_run")
-    FileUtils.rm_rf(integration_path)
-
-    exit(RSpec::Core::Runner.run([Rake::FileList["integration/**/*_spec.rb"]]))
-  end
-
-  namespace "integration" do
-    task "local" => ["setup"] do
-      require "fileutils"
-
-      source = File.expand_path(File.join(File.dirname(__FILE__), ".."))
-      integration_path = File.join(source, "integration_run")
-      FileUtils.mkdir_p(integration_path)
-
-      puts "[integration_spec] configuring local environment for running test in #{integration_path}, if you want to change this behavior delete the directory."
-      exit(RSpec::Core::Runner.run([Rake::FileList["integration/**/*_spec.rb"]]))
-    end
-  end
 end
 
 task "test" => [ "test:core" ]
diff --git a/rakelib/vendor.rake b/rakelib/vendor.rake
index df29654e68e..b5cac5a353c 100644
--- a/rakelib/vendor.rake
+++ b/rakelib/vendor.rake
@@ -1,6 +1,6 @@
 namespace "vendor" do
   VERSIONS = {
-    "jruby" => { "version" => "1.7.24", "sha1" => "0c321d2192768dfec419bee6b44c7190f4db32e1" },
+    "jruby" => { "version" => "1.7.25", "sha1" => "cd15aef419f97cff274491e53fcfb8b88ec36785" },
   }
 
   def vendor(*args)
diff --git a/rakelib/version.rake b/rakelib/version.rake
new file mode 100644
index 00000000000..45655cd330d
--- /dev/null
+++ b/rakelib/version.rake
@@ -0,0 +1,132 @@
+require 'yaml'
+
+def get_versions
+  yaml_versions = YAML.safe_load(IO.read("versions.yml"))
+  {
+    "logstash" => {
+      "location" => File.join("logstash-core", "lib", "logstash", "version.rb"),
+      "yaml_version" => yaml_versions["logstash"],
+      "current_version" => get_version(File.join("logstash-core", "lib", "logstash", "version.rb")),
+    },
+    "logstash-core" => {
+      "location" => File.join("logstash-core", "lib", "logstash-core", "version.rb"),
+      "yaml_version" => yaml_versions["logstash-core"],
+      "current_version" => get_version(File.join("logstash-core", "lib", "logstash-core", "version.rb")),
+    },
+    "logstash-core-event" => {
+      "location" => File.join("logstash-core-event", "lib", "logstash-core-event", "version.rb"),
+      "yaml_version" => yaml_versions["logstash-core-event"],
+      "current_version" => get_version(File.join("logstash-core-event", "lib", "logstash-core-event", "version.rb")),
+    },
+    "logstash-core-event-java" => {
+      "location" => File.join("logstash-core-event-java", "lib", "logstash-core-event-java", "version.rb"),
+      "yaml_version" => yaml_versions["logstash-core-event-java"],
+      "current_version" => get_version(File.join("logstash-core-event-java", "lib", "logstash-core-event-java", "version.rb")),
+    },
+    "logstash-core-plugin-api" => {
+      "location" => File.join("logstash-core-plugin-api", "lib", "logstash-core-plugin-api", "version.rb"),
+      "yaml_version" => yaml_versions["logstash-core-plugin-api"],
+      "current_version" => get_version(File.join("logstash-core-plugin-api", "lib", "logstash-core-plugin-api", "version.rb")),
+    }
+  }
+end
+
+def get_version(file)
+  text = IO.read(file)
+  version = text.match(/^[A-Z_]+ = "(.+?)"/)
+  version[1]
+end
+
+namespace :version do
+
+  desc "check if the versions.yml is out of sync with .gemspecs and other references"
+  task :check do
+    out_of_sync = get_versions.select do |component, metadata|
+      metadata["yaml_version"] != metadata["current_version"]
+    end
+    if out_of_sync.any?
+      out_of_sync.each do |component, metadata|
+        puts "#{component} is out of sync. CURRENT: #{metadata['current_version']} | YAML: #{metadata['yaml_version']}"
+      end
+      exit(1)
+    end
+  end
+
+  desc "push versions found in versions.yml to all component version locations"
+  task :sync do
+    versions = get_versions
+    # update version.rb files
+    versions.select do |component, metadata|
+      next if metadata["yaml_version"] == metadata["current_version"]
+      puts "Updating \"#{component}\" from \"#{metadata['current_version']}\" to \"#{metadata['yaml_version']}\""
+      text = IO.read(metadata["location"])
+      IO.write(metadata["location"], text.gsub(metadata["current_version"], metadata["yaml_version"]))
+    end
+
+    # update dependencies
+    #
+    # logstash-core depends on logstash-core-event-java
+    # ./logstash-core/logstash-core.gemspec:  gem.add_runtime_dependency "logstash-core-event-java", "~> 5.0.0.dev"
+    logstash_core_gemspec = File.join("logstash-core", "logstash-core.gemspec")
+    logstash_core_event_java_version = versions['logstash-core-event-java']['yaml_version']
+    text = IO.read(logstash_core_gemspec)
+    IO.write(logstash_core_gemspec, text.sub(
+      /  gem.add_runtime_dependency \"logstash-core-event-java\", \".+?\"/,
+      "  gem.add_runtime_dependency \"logstash-core-event-java\", \"#{logstash_core_event_java_version}\""))
+
+    # logstash-core-event-java depends on logstash-code
+    # ./logstash-core-plugin-api/logstash-core-plugin-api.gemspec:  gem.add_runtime_dependency "logstash-core", "5.0.0.dev"
+    logstash_core_plugin_api_gemspec = File.join("logstash-core-plugin-api", "logstash-core-plugin-api.gemspec")
+    logstash_core_version = versions['logstash-core']['yaml_version']
+    text = IO.read(logstash_core_plugin_api_gemspec)
+    IO.write(logstash_core_plugin_api_gemspec, text.sub(
+      /  gem.add_runtime_dependency \"logstash-core\", \".+?\"/,
+      "  gem.add_runtime_dependency \"logstash-core\", \"#{logstash_core_version}\""))
+  end
+
+  desc "show version of core components"
+  task :show do
+    Rake::Task["version:sync"].invoke; Rake::Task["version:sync"].reenable
+    get_versions.each do |component, metadata|
+      puts "#{component}: #{metadata['yaml_version']}"
+    end
+  end
+
+  desc "set version of logstash, logstash-core, logstash-core-event, logstash-core-event-java"
+  task :set, [:version] => [:validate] do |t, args|
+    hash = {}
+    get_versions.each do |component, metadata|
+      # we just assume that, usually, all components except
+      # "logstash-core-plugin-api" will be versioned together
+      # so let's skip this one and have a separate task for it
+      if component == "logstash-core-plugin-api"
+        hash[component] = metadata["yaml_version"]
+      else
+        hash[component] = args[:version]
+      end
+    end
+    IO.write("versions.yml", hash.to_yaml)
+    Rake::Task["version:sync"].invoke; Rake::Task["version:sync"].reenable
+  end
+
+  desc "set version of logstash-core-plugin-api"
+  task :set_plugin_api, [:version] => [:validate] do |t, args|
+    hash = {}
+    get_versions.each do |component, metadata|
+      if component == "logstash-core-plugin-api"
+        hash[component] = args[:version]
+      else
+        hash[component] = metadata["yaml_version"]
+      end
+    end
+    IO.write("versions.yml", hash.to_yaml)
+    Rake::Task["version:sync"].invoke; Rake::Task["version:sync"].reenable
+  end
+
+  task :validate, :version do |t, args|
+    unless Regexp.new('^\d+\.\d+\.\d+(?:-\w+\d+)?$').match(args[:version])
+      abort("Invalid version argument: \"#{args[:version]}\". Aborting...")
+    end
+  end
+
+end
diff --git a/rakelib/z_rubycheck.rake b/rakelib/z_rubycheck.rake
index ed22ed016c7..bf077b8700b 100644
--- a/rakelib/z_rubycheck.rake
+++ b/rakelib/z_rubycheck.rake
@@ -32,7 +32,7 @@ if ENV['USE_RUBY'] != '1'
     # if required at this point system gems can be installed using the system_gem task, for example:
     # Rake::Task["vendor:system_gem"].invoke(jruby, "ffi", "1.9.6")
 
-    exec(jruby, "-S", rake, *ARGV)
+    exec(jruby, "-J-Xmx1g", "-S", rake, *ARGV)
   end
 end
 
diff --git a/spec/bootstrap/environment_spec.rb b/spec/bootstrap/environment_spec.rb
new file mode 100644
index 00000000000..f31cfcd38e8
--- /dev/null
+++ b/spec/bootstrap/environment_spec.rb
@@ -0,0 +1,6 @@
+# encoding: utf-8
+require "spec_helper"
+require "bootstrap/environment"
+
+describe LogStash::Environment do
+end
diff --git a/spec/spec_helper.rb b/spec/spec_helper.rb
index 5428fd8fd90..9bec90f640c 100644
--- a/spec/spec_helper.rb
+++ b/spec/spec_helper.rb
@@ -6,6 +6,23 @@
 
 require "logstash/devutils/rspec/spec_helper"
 
+require "flores/rspec"
+require "flores/random"
+
+class JSONIOThingy < IO
+  def initialize; end
+  def flush; end
+
+  def puts(payload)
+    # Ensure that all log payloads are valid json.
+    LogStash::Json.load(payload)
+  end
+end
+
+RSpec.configure do |c|
+  Flores::RSpec.configure(c)
+end
+
 def installed_plugins
   Gem::Specification.find_all.select { |spec| spec.metadata["logstash_plugin"] }.map { |plugin| plugin.name }
 end
diff --git a/spec/bootstrap/bundler_spec.rb b/spec/unit/bootstrap/bundler_spec.rb
similarity index 100%
rename from spec/bootstrap/bundler_spec.rb
rename to spec/unit/bootstrap/bundler_spec.rb
diff --git a/spec/license_spec.rb b/spec/unit/license_spec.rb
similarity index 78%
rename from spec/license_spec.rb
rename to spec/unit/license_spec.rb
index f37f29d0431..9425cfc9111 100644
--- a/spec/license_spec.rb
+++ b/spec/unit/license_spec.rb
@@ -25,7 +25,11 @@
     [
       # Skipped because of already included and bundled within JRuby so checking here is redundant.
       # Need to take action about jruby licenses to enable again or keep skeeping.
-      "jruby-openssl"
+      "jruby-openssl",
+      # Skipped because version 2.6.2 which we use has multiple licenses: MIT, ARTISTIC 2.0, GPL-2
+      # See https://rubygems.org/gems/mime-types/versions/2.6.2
+      # version 3.0 of mime-types (which is only compatible with Ruby 2.0) is MIT licensed
+      "mime-types"
     ]
   end
 
@@ -48,7 +52,8 @@
         next unless runtime_spec
         next if skipped_dependencies.include?(runtime_spec.name)
         runtime_spec.licenses.each do |license|
-          expect(license.downcase).to match(expected_licenses)
+          expect(license.downcase).to match(expected_licenses), 
+            lambda { "Runtime license check failed for gem #{runtime_spec.name} with version #{runtime_spec.version}" }
         end
       end
     end
diff --git a/spec/pluginmanager/gemfile_spec.rb b/spec/unit/plugin_manager/gemfile_spec.rb
similarity index 100%
rename from spec/pluginmanager/gemfile_spec.rb
rename to spec/unit/plugin_manager/gemfile_spec.rb
diff --git a/spec/plugin_manager/install_spec.rb b/spec/unit/plugin_manager/install_spec.rb
similarity index 100%
rename from spec/plugin_manager/install_spec.rb
rename to spec/unit/plugin_manager/install_spec.rb
diff --git a/spec/plugin_manager/update_spec.rb b/spec/unit/plugin_manager/update_spec.rb
similarity index 100%
rename from spec/plugin_manager/update_spec.rb
rename to spec/unit/plugin_manager/update_spec.rb
diff --git a/spec/plugin_manager/util_spec.rb b/spec/unit/plugin_manager/util_spec.rb
similarity index 100%
rename from spec/plugin_manager/util_spec.rb
rename to spec/unit/plugin_manager/util_spec.rb
diff --git a/spec/util/compress_spec.rb b/spec/unit/util/compress_spec.rb
similarity index 100%
rename from spec/util/compress_spec.rb
rename to spec/unit/util/compress_spec.rb
diff --git a/spec/util/retryable_spec.rb b/spec/unit/util/retryable_spec.rb
similarity index 100%
rename from spec/util/retryable_spec.rb
rename to spec/unit/util/retryable_spec.rb
diff --git a/tools/Gemfile.beaker b/tools/Gemfile.beaker
deleted file mode 100644
index 97a67a20ade..00000000000
--- a/tools/Gemfile.beaker
+++ /dev/null
@@ -1,11 +0,0 @@
-source 'https://rubygems.org'
-
-gem 'beaker', '2.27.0'
-gem 'beaker-rspec'
-gem 'pry'
-gem 'docker-api', '~> 1.0'
-gem 'rubysl-securerandom'
-gem 'rspec_junit_formatter'
-gem 'rspec', '~> 3.1'
-gem 'rake'
-gem 'fog-google', '~> 0.0.9'
diff --git a/benchmark/collector.rb b/tools/benchmark/collector.rb
similarity index 100%
rename from benchmark/collector.rb
rename to tools/benchmark/collector.rb
diff --git a/tools/benchmark/event_accessor.rb b/tools/benchmark/event_accessor.rb
new file mode 100644
index 00000000000..018785b51ae
--- /dev/null
+++ b/tools/benchmark/event_accessor.rb
@@ -0,0 +1,17 @@
+# encoding: utf-8
+require "benchmark/ips"
+require "logstash/event"
+
+options = { :time => 10, :warmup => 60 }
+puts "Same Event instance"
+
+event = LogStash::Event.new("foo" => {"bar" => {"foobar" => "morebar"} })
+STDERR.puts ""
+STDERR.puts " ----------> event.get(\"[foo][bar][foobar]\") => #{event.get("[foo][bar][foobar]")}"
+STDERR.puts ""
+
+Benchmark.ips do |x|
+  x.config(options)
+
+  x.report("Deep fetch") { event.get("[foo][bar][foobar]") }
+end
diff --git a/benchmark/event_sprintf.rb b/tools/benchmark/event_sprintf.rb
similarity index 100%
rename from benchmark/event_sprintf.rb
rename to tools/benchmark/event_sprintf.rb
diff --git a/tools/release.sh b/tools/release.sh
deleted file mode 100644
index 435196a95cb..00000000000
--- a/tools/release.sh
+++ /dev/null
@@ -1,67 +0,0 @@
-#!/bin/bash
-
-logstash=$PWD
-contrib=$PWD/../logstash-contrib/
-
-workdir="$PWD/build/release/"
-mkdir -p $workdir
-
-# circuit breaker to fail if there's something silly wrong.
-if [ -z "$workdir" ] ; then
-  echo "workdir is empty?!"
-  exit 1
-fi
-
-if [ ! -d "$contrib" ] ; then
-  echo "Missing: $contrib"
-  echo "Maybe git clone it?"
-  exit 1
-fi
-
-set -e
-
-prepare() {
-  rsync -a --delete $logstash/{bin,docs,lib,spec,Makefile,gembag.rb,logstash.gemspec,tools,locales,patterns,LICENSE,README.md} $contrib/{lib,spec} $workdir
-  rm -f $logstash/.VERSION.mk
-  make -C $logstash .VERSION.mk
-  make -C $logstash tarball package
-  make -C $contrib tarball package
-  cp $logstash/.VERSION.mk $workdir
-  rm -f $workdir/build/pkg
-  rm -f $workdir/build/*.{zip,rpm,gz,deb} || true
-}
-
-docs() {
-  make -C $workdir build
-  (cd $contrib; find lib/logstash -type f -name '*.rb') > $workdir/build/contrib_plugins
-  make -C $workdir -j 4 docs
-}
-
-tests() {
-  make -C $logstash test QUIET=
-  make -C $logstash tarball test QUIET=
-}
-
-packages() {
-  for path in $logstash $contrib ; do
-    rm -f $path/build/*.tar.gz
-    rm -f $path/build/*.zip
-    echo "Building packages: $path"
-    make -C $path tarball
-    for dir in build pkg . ; do
-      [ ! -d "$path/$dir" ] && continue
-      (cd $path/$dir;
-        for i in *.gz *.rpm *.deb *.zip *.jar ; do
-          [ ! -f "$i" ] && continue
-          echo "Copying $path/$dir/$i"
-          cp $i $workdir/build
-        done
-      )
-    done
-  done
-}
-
-prepare
-tests
-docs
-packages
diff --git a/tools/upload.sh b/tools/upload.sh
deleted file mode 100644
index 72684486c8c..00000000000
--- a/tools/upload.sh
+++ /dev/null
@@ -1,8 +0,0 @@
-
-basedir=$(dirname $0)/../
-bucket=download.elasticsearch.org
-
-s3cmd put -P $basedir/build/release/build/*.gz s3://${bucket}/logstash/logstash/
-s3cmd put -P $basedir/build/release/build/*.rpm s3://${bucket}/logstash/logstash/packages/centos/
-s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/debian
-s3cmd put -P $basedir/build/release/build/*.deb s3://${bucket}/logstash/logstash/packages/ubuntu
diff --git a/versions.yml b/versions.yml
new file mode 100644
index 00000000000..2f8ebfc6811
--- /dev/null
+++ b/versions.yml
@@ -0,0 +1,6 @@
+---
+logstash: 5.1.0
+logstash-core: 5.1.0
+logstash-core-event: 5.1.0
+logstash-core-event-java: 5.1.0
+logstash-core-plugin-api: 2.1.12
