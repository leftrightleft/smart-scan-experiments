diff --git a/logstash-core/lib/logstash/environment.rb b/logstash-core/lib/logstash/environment.rb
index cb973084160..c4e28fcea96 100644
--- a/logstash-core/lib/logstash/environment.rb
+++ b/logstash-core/lib/logstash/environment.rb
@@ -91,6 +91,8 @@ module Environment
             Setting::Boolean.new("dead_letter_queue.enable", false),
             Setting::Bytes.new("dead_letter_queue.max_bytes", "1024mb"),
             Setting::Numeric.new("dead_letter_queue.flush_interval", 5000),
+    Setting::NullableString.new("dead_letter_queue.retain.size"), # example 1024mb
+    Setting::NullableString.new("dead_letter_queue.retain.age"), # example 5d
             Setting::TimeValue.new("slowlog.threshold.warn", "-1"),
             Setting::TimeValue.new("slowlog.threshold.info", "-1"),
             Setting::TimeValue.new("slowlog.threshold.debug", "-1"),
diff --git a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
index 88e8175936c..c76be303c8c 100644
--- a/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
+++ b/logstash-core/src/main/java/org/logstash/common/DeadLetterQueueFactory.java
@@ -45,6 +45,7 @@
 import java.io.IOException;
 import java.nio.file.Paths;
 import java.time.Duration;
+import java.time.Instant;
 import java.util.concurrent.ConcurrentHashMap;
 
 /**
@@ -75,19 +76,23 @@ private DeadLetterQueueFactory() {
      * @param maxQueueSize Maximum size of the dead letter queue (in bytes). No entries will be written
      *                     that would make the size of this dlq greater than this value
      * @param flushInterval Maximum duration between flushes of dead letter queue files if no data is sent.
+     * @param agePolicy retention policy based on the age of events in the queue.
+     * @param sizePolicy retention policy based on the size of the queue.
      * @return The write manager for the specific id's dead-letter-queue context
      */
-    public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval) {
-        return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize, flushInterval));
+    public static DeadLetterQueueWriter getWriter(String id, String dlqPath, long maxQueueSize, Duration flushInterval,
+                                                  DeadLetterQueueWriter.DLQRetentionPolicy<Instant> agePolicy, DeadLetterQueueWriter.DLQRetentionPolicy<Long> sizePolicy) {
+        return REGISTRY.computeIfAbsent(id, key -> newWriter(key, dlqPath, maxQueueSize, flushInterval, agePolicy, sizePolicy));
     }
 
     public static DeadLetterQueueWriter release(String id) {
         return REGISTRY.remove(id);
     }
 
-    private static DeadLetterQueueWriter newWriter(final String id, final String dlqPath, final long maxQueueSize, final Duration flushInterval) {
+    private static DeadLetterQueueWriter newWriter(final String id, final String dlqPath, final long maxQueueSize, final Duration flushInterval,
+                                                   DeadLetterQueueWriter.DLQRetentionPolicy<Instant> agePolicy, DeadLetterQueueWriter.DLQRetentionPolicy<Long> sizePolicy) {
         try {
-            return new DeadLetterQueueWriter(Paths.get(dlqPath, id), MAX_SEGMENT_SIZE_BYTES, maxQueueSize, flushInterval);
+            return new DeadLetterQueueWriter(Paths.get(dlqPath, id), MAX_SEGMENT_SIZE_BYTES, maxQueueSize, flushInterval, agePolicy, sizePolicy);
         } catch (IOException e) {
             logger.error("unable to create dead letter queue writer", e);
         }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java
index 75110249a36..bba182a0d86 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueReader.java
@@ -52,9 +52,10 @@
 import java.nio.file.WatchEvent;
 import java.nio.file.WatchKey;
 import java.nio.file.WatchService;
+import java.util.Comparator;
+import java.util.NoSuchElementException;
 import java.util.concurrent.ConcurrentSkipListSet;
 import java.util.concurrent.TimeUnit;
-import java.util.function.Function;
 import java.util.stream.Collectors;
 
 import static java.nio.file.StandardWatchEventKinds.ENTRY_CREATE;
@@ -73,29 +74,40 @@ public DeadLetterQueueReader(Path queuePath) throws IOException {
         this.queuePath = queuePath;
         this.watchService = FileSystems.getDefault().newWatchService();
         this.queuePath.register(watchService, ENTRY_CREATE, ENTRY_DELETE);
-        this.segments = new ConcurrentSkipListSet<>((p1, p2) -> {
-            Function<Path, Integer> id = (p) -> Integer.parseInt(p.getFileName().toString().split("\\.")[0]);
-            return id.apply(p1).compareTo(id.apply(p2));
-        });
+        this.segments = new ConcurrentSkipListSet<>(
+                Comparator.comparingInt(DeadLetterQueueReader::extractSegmentId)
+        );
 
         segments.addAll(getSegmentPaths(queuePath).collect(Collectors.toList()));
     }
 
+    private static int extractSegmentId(Path p) {
+        return Integer.parseInt(p.getFileName().toString().split("\\.")[0]);
+    }
+
+    private static Timestamp extractEntryTimestamp(byte[] serialized) {
+        try {
+            return DLQEntry.deserialize(serialized).getEntryTime();
+        } catch (IOException e) {
+            throw new IllegalStateException(e);
+        }
+    }
+
     public void seekToNextEvent(Timestamp timestamp) throws IOException {
         for (Path segment : segments) {
+            if (!Files.exists(segment)) {
+                segments.remove(segment);
+                continue;
+            }
             currentReader = new RecordIOReader(segment);
-            byte[] event = currentReader.seekToNextEventPosition(timestamp, (b) -> {
-                try {
-                    return DLQEntry.deserialize(b).getEntryTime();
-                } catch (IOException e) {
-                    throw new IllegalStateException(e);
-                }
-            }, Timestamp::compareTo);
+            byte[] event = currentReader.seekToNextEventPosition(timestamp, DeadLetterQueueReader::extractEntryTimestamp, Timestamp::compareTo);
             if (event != null) {
                 return;
             }
         }
-        currentReader.close();
+        if (currentReader != null) {
+            currentReader.close();
+        }
         currentReader = null;
     }
 
@@ -103,16 +115,32 @@ private long pollNewSegments(long timeout) throws IOException, InterruptedExcept
         long startTime = System.currentTimeMillis();
         WatchKey key = watchService.poll(timeout, TimeUnit.MILLISECONDS);
         if (key != null) {
-            for (WatchEvent<?> watchEvent : key.pollEvents()) {
-                if (watchEvent.kind() == StandardWatchEventKinds.ENTRY_CREATE) {
-                    segments.addAll(getSegmentPaths(queuePath).collect(Collectors.toList()));
-                }
-                key.reset();
-            }
+            pollEventsOnWatch(key);
         }
         return System.currentTimeMillis() - startTime;
     }
 
+    private void pollNewSegments() throws IOException {
+        WatchKey key = watchService.poll();
+        if (key != null) {
+            pollEventsOnWatch(key);
+        }
+    }
+
+    private void pollEventsOnWatch(WatchKey key) throws IOException {
+        for (WatchEvent<?> watchEvent : key.pollEvents()) {
+            if (watchEvent.kind() == StandardWatchEventKinds.ENTRY_CREATE) {
+                segments.addAll(getSegmentPaths(queuePath).collect(Collectors.toList()));
+            } else if (watchEvent.kind() == StandardWatchEventKinds.ENTRY_DELETE) {
+                final int oldSize = segments.size();
+                segments.clear();
+                segments.addAll(getSegmentPaths(queuePath).collect(Collectors.toList()));
+                logger.debug("Notified of segment removal, switched from {} to {} segments", oldSize, segments.size());
+            }
+            key.reset();
+        }
+    }
+
     public DLQEntry pollEntry(long timeout) throws IOException, InterruptedException {
         byte[] bytes = pollEntryBytes(timeout);
         if (bytes == null) {
@@ -134,34 +162,77 @@ byte[] pollEntryBytes(long timeout) throws IOException, InterruptedException {
                 logger.debug("No entries found: no segment files found in dead-letter-queue directory");
                 return null;
             }
-            currentReader = new RecordIOReader(segments.first());
+            try {
+                final Path firstSegment = segments.first();
+                currentReader = new RecordIOReader(firstSegment);
+            } catch (NoSuchElementException ex) {
+                // all the elements were removed after the empty check
+                logger.debug("No entries found: no segment files found in dead-letter-queue directory");
+                return null;
+            }
         }
 
         byte[] event = currentReader.readEvent();
         if (event == null && currentReader.isEndOfStream()) {
-            if (currentReader.getPath().equals(segments.last())) {
+            Path lastSegment;
+            try {
+                lastSegment = segments.last();
+            } catch (NoSuchElementException ex) {
+                // the last segment was removed while processing
+                logger.debug("No last segment found, poll for new segments");
+                lastSegment = null;
+            }
+            if (lastSegment == null || currentReader.getPath().equals(lastSegment)) {
                 pollNewSegments(timeoutRemaining);
             } else {
                 currentReader.close();
-                currentReader = new RecordIOReader(segments.higher(currentReader.getPath()));
-                return pollEntryBytes(timeoutRemaining);
+                final Path nextSegment = nextExistingSegmentFile(currentReader.getPath());
+                if (nextSegment == null) {
+                    // segments were all already deleted files, do a poll
+                    pollNewSegments(timeoutRemaining);
+                } else {
+                    currentReader = new RecordIOReader(nextSegment);
+                    return pollEntryBytes(timeoutRemaining);
+                }
             }
         }
 
         return event;
     }
 
+    private Path nextExistingSegmentFile(Path currentSegmentPath) {
+        Path nextExpectedSegment;
+        boolean skip;
+        do {
+            nextExpectedSegment = segments.higher(currentSegmentPath);
+            if (nextExpectedSegment != null && !Files.exists(nextExpectedSegment)) {
+                segments.remove(nextExpectedSegment);
+                skip = true;
+            } else {
+                skip = false;
+            }
+        } while (skip);
+        return nextExpectedSegment;
+    }
+
     public void setCurrentReaderAndPosition(Path segmentPath, long position) throws IOException {
         // If the provided segment Path exist, then set the reader to start from the supplied position
         if (Files.exists(segmentPath)) {
             currentReader = new RecordIOReader(segmentPath);
             currentReader.seekToOffset(position);
-        }else{
+        } else {
             // Otherwise, set the current reader to be at the beginning of the next
             // segment.
-            Path next = segments.higher(segmentPath);
-            if (next != null){
+            Path next = nextExistingSegmentFile(segmentPath);
+            if (next != null) {
                 currentReader = new RecordIOReader(next);
+            } else {
+                pollNewSegments();
+                // give a second try after a reload of segments from filesystem
+                next = nextExistingSegmentFile(segmentPath);
+                if (next != null) {
+                    currentReader = new RecordIOReader(next);
+                }
             }
         }
     }
diff --git a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
index b1d2e2cea62..2ef1096a787 100644
--- a/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
+++ b/logstash-core/src/main/java/org/logstash/common/io/DeadLetterQueueWriter.java
@@ -44,8 +44,11 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardCopyOption;
+import java.time.Clock;
 import java.time.Duration;
 import java.time.Instant;
+import java.time.temporal.TemporalAmount;
+import java.util.Optional;
 import java.util.concurrent.Executors;
 import java.util.concurrent.ScheduledExecutorService;
 import java.util.concurrent.TimeUnit;
@@ -64,11 +67,138 @@
 import org.logstash.FileLockFactory;
 import org.logstash.Timestamp;
 
+import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
 import static org.logstash.common.io.RecordIOReader.SegmentStatus;
+import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
 
 public final class DeadLetterQueueWriter implements Closeable {
 
+    public interface DLQRetentionPolicy<T> {
+        boolean verify(T dimension);
+
+        void updateStatus(DeadLetterQueueWriter queueWriter) throws IOException;
+
+        void apply(DeadLetterQueueWriter queueWriter) throws IOException;
+    }
+
+    static class NoopRetentionPolicy<T> implements DLQRetentionPolicy<T> {
+
+        @Override
+        public boolean verify(T dimension) {
+            return false;
+        }
+
+        @Override
+        public void updateStatus(DeadLetterQueueWriter queueWriter) throws IOException {
+        }
+
+        @Override
+        public void apply(DeadLetterQueueWriter queueWriter) throws IOException {
+        }
+    }
+
+    public static final DeadLetterQueueWriter.NoopRetentionPolicy<Long> NOOP_SIZE_POLICY = new DeadLetterQueueWriter.NoopRetentionPolicy<>();
+
+    public static final DeadLetterQueueWriter.NoopRetentionPolicy<Instant> NOOP_AGE_POLICY = new DeadLetterQueueWriter.NoopRetentionPolicy<>();
+
+    public static class SizeRetentionPolicy implements DLQRetentionPolicy<Long> {
+        private static final Logger logger = LogManager.getLogger(SizeRetentionPolicy.class);
+
+        private long maxRetentionSize;
+
+        public SizeRetentionPolicy(long maxRetentionSize) {
+            this.maxRetentionSize = maxRetentionSize;
+        }
+
+        @Override
+        public boolean verify(Long size) {
+            return size > maxRetentionSize;
+        }
+
+        @Override
+        public void updateStatus(DeadLetterQueueWriter queueWriter) {}
+
+        @Override
+        public void apply(DeadLetterQueueWriter queueWriter) throws IOException {
+            // remove oldest segment
+            final Optional<Path> oldestSegment = getSegmentPaths(queueWriter.queuePath).sorted().findFirst();
+            if (!oldestSegment.isPresent()) {
+                throw new IllegalStateException("Listing of DLQ segments was empty during retain size(" + maxRetentionSize + ") check");
+            }
+            final Path beheadedSegment = oldestSegment.get();
+            final long segmentSize = Files.size(beheadedSegment);
+            queueWriter.currentQueueSize.add(-segmentSize);
+            Files.delete(beheadedSegment);
+            logger.debug("Deleted exceeded retained size segment file {}", beheadedSegment);
+        }
+    }
+
+    public static class AgeRetentionPolicy implements DLQRetentionPolicy<Instant> {
+        private static final Logger logger = LogManager.getLogger(AgeRetentionPolicy.class);
+
+        private Optional<Timestamp> oldestSegmentTimestamp;
+        private Optional<Path> oldestSegmentPath;
+        private final TemporalAmount retentionTime;
+
+        public AgeRetentionPolicy(TemporalAmount retentionTime) {
+            this.retentionTime = retentionTime;
+        }
+
+        @Override
+        public boolean verify(final Instant currentTime) {
+            return oldestSegmentTimestamp
+                    .map(t -> t.toInstant().isBefore(currentTime.minus(retentionTime)))
+                    .orElse(false);
+        }
+
+        @Override
+        public void updateStatus(DeadLetterQueueWriter queueWriter) throws IOException {
+            oldestSegmentPath = getSegmentPaths(queueWriter.queuePath).sorted().findFirst();
+            if (!oldestSegmentPath.isPresent()) {
+                oldestSegmentTimestamp = Optional.empty();
+                return;
+            }
+            // extract the newest timestamp from the oldest segment
+            oldestSegmentTimestamp = Optional.of(readTimestampOfLastEventInSegment(oldestSegmentPath.get()));
+        }
+
+        private Timestamp readTimestampOfLastEventInSegment(Path segmentPath) throws IOException {
+            final int lastBlockId = (int) Math.ceil(((Files.size(segmentPath) - VERSION_SIZE) / (double) BLOCK_SIZE)) - 1;
+            byte[] eventBytes;
+            try (RecordIOReader recordReader = new RecordIOReader(segmentPath)) {
+                int blockId = lastBlockId;
+                do {
+                    recordReader.seekToBlock(blockId);
+                    eventBytes = recordReader.readEvent();
+                    blockId--;
+                } while (eventBytes == null && blockId >= 0); // no event present in last block, try with the one before
+            }
+            if (eventBytes == null) {
+                throw new IllegalStateException("Cannot find a complete event into the segment file: " + segmentPath);
+            }
+            return DLQEntry.deserialize(eventBytes).getEntryTime();
+        }
+
+        @Override
+        public void apply(DeadLetterQueueWriter queueWriter) throws IOException {
+            // remove all the old segments that verifies the age retention condition
+            boolean cleanNextSegment;
+            do {
+                Path beheadedSegment = oldestSegmentPath.orElseThrow(() -> new IllegalStateException("DLQ writer can't find the oldest segment to drop on path: " + queueWriter.queuePath));
+                final long segmentSize = Files.size(beheadedSegment);
+                queueWriter.currentQueueSize.add(-segmentSize);
+                Files.delete(beheadedSegment);
+                logger.debug("Deleted exceeded retained age segment file {}", beheadedSegment);
+
+                updateStatus(queueWriter);
+                cleanNextSegment = oldestSegmentTimestamp
+                        .map(t -> t.toInstant().isBefore(Instant.now().minus(retentionTime)))
+                        .orElse(false);
+            } while(cleanNextSegment);
+        }
+    }
+
     @VisibleForTesting
     static final String SEGMENT_FILE_PATTERN = "%d.log";
     private static final Logger logger = LogManager.getLogger(DeadLetterQueueWriter.class);
@@ -90,8 +220,23 @@ private enum FinalizeWhen {ALWAYS, ONLY_IF_STALE};
     private Instant lastWrite;
     private final AtomicBoolean open = new AtomicBoolean(true);
     private ScheduledExecutorService flushScheduler;
+    private final DLQRetentionPolicy<Long> sizePolicy;
+    private final DLQRetentionPolicy<Instant> agePolicy;
+    private final Clock clock;
 
     public DeadLetterQueueWriter(final Path queuePath, final long maxSegmentSize, final long maxQueueSize, final Duration flushInterval) throws IOException {
+        this(queuePath, maxSegmentSize, maxQueueSize, flushInterval, Clock.systemDefaultZone(), NOOP_AGE_POLICY, NOOP_SIZE_POLICY);
+    }
+
+    public DeadLetterQueueWriter(final Path queuePath, final long maxSegmentSize, final long maxQueueSize,
+                                 final Duration flushInterval,
+                                 DLQRetentionPolicy<Instant> ageRetentionPolicy, DLQRetentionPolicy<Long> sizePolicy) throws IOException {
+        this(queuePath, maxSegmentSize, maxQueueSize, flushInterval, Clock.systemDefaultZone(), ageRetentionPolicy, sizePolicy);
+    }
+
+    DeadLetterQueueWriter(final Path queuePath, final long maxSegmentSize, final long maxQueueSize,
+                          final Duration flushInterval, final Clock clock,
+                          DLQRetentionPolicy<Instant> ageRetentionPolicy, DLQRetentionPolicy<Long> sizePolicy) throws IOException {
         this.fileLock = FileLockFactory.obtainLock(queuePath, LOCK_FILE);
         this.queuePath = queuePath;
         this.maxSegmentSize = maxSegmentSize;
@@ -99,8 +244,13 @@ public DeadLetterQueueWriter(final Path queuePath, final long maxSegmentSize, fi
         this.flushInterval = flushInterval;
         this.currentQueueSize = new LongAdder();
         this.currentQueueSize.add(getStartupQueueSize());
+        this.clock = clock;
+
+        this.sizePolicy = sizePolicy;
+        this.agePolicy = ageRetentionPolicy;
 
         cleanupTempFiles();
+        agePolicy.updateStatus(this);
         currentSegmentIndex = getSegmentPaths(queuePath)
                 .map(s -> s.getFileName().toString().split("\\.")[0])
                 .mapToInt(Integer::parseInt)
@@ -176,8 +326,14 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {
         }
         byte[] record = entry.serialize();
         int eventPayloadSize = RECORD_HEADER_SIZE + record.length;
+        if (sizePolicy.verify(currentQueueSize.longValue() + eventPayloadSize)) {
+            sizePolicy.apply(this);
+        }
+
+        satisfyAgeRetentionPolicy();
+
         if (currentQueueSize.longValue() + eventPayloadSize > maxQueueSize) {
-            logger.error("cannot write event to DLQ(path: " + this.queuePath + "): reached maxQueueSize of " + maxQueueSize);
+            logger.error("cannot write event to DLQ(path: {}): reached maxQueueSize of {}", this.queuePath, maxQueueSize);
             return;
         } else if (currentWriter.getPosition() + eventPayloadSize > maxSegmentSize) {
             finalizeSegment(FinalizeWhen.ALWAYS);
@@ -186,6 +342,13 @@ private void innerWriteEntry(DLQEntry entry) throws IOException {
         lastWrite = Instant.now();
     }
 
+    private void satisfyAgeRetentionPolicy() throws IOException {
+        final Instant now = clock.instant();
+        if(agePolicy.verify(now)) {
+            agePolicy.apply(this);
+        }
+    }
+
     /**
      * Method to determine whether the event has already been processed by the DLQ - currently this
      * just checks the metadata to see if metadata has been added to the event that indicates that
@@ -226,6 +389,8 @@ private void finalizeSegment(final FinalizeWhen finalizeWhen) throws IOException
                 Files.move(queuePath.resolve(String.format(TEMP_FILE_PATTERN, currentSegmentIndex)),
                         queuePath.resolve(String.format(SEGMENT_FILE_PATTERN, currentSegmentIndex)),
                         StandardCopyOption.ATOMIC_MOVE);
+                agePolicy.updateStatus(this);
+                satisfyAgeRetentionPolicy();
                 if (isOpen()) {
                     nextWriter();
                 }
@@ -342,7 +507,7 @@ private void deleteTemporaryFile(Path tempFile, String segmentName) throws IOExc
         Files.delete(deleteTarget);
     }
 
-    private static boolean isWindows(){
+    private static boolean isWindows() {
         return System.getProperty("os.name").startsWith("Windows");
     }
 }
diff --git a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
index 139648b72ff..11dca8e5368 100644
--- a/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
+++ b/logstash-core/src/main/java/org/logstash/execution/AbstractPipelineExt.java
@@ -26,11 +26,18 @@
 import java.nio.file.Paths;
 import java.security.MessageDigest;
 import java.security.NoSuchAlgorithmException;
+import java.text.ParseException;
+import java.time.Instant;
+import java.time.temporal.ChronoUnit;
+import java.time.temporal.TemporalUnit;
 import java.util.ArrayList;
 import java.time.Duration;
 import java.util.Arrays;
 import java.util.List;
 import java.util.UUID;
+import java.util.regex.Matcher;
+import java.util.regex.Pattern;
+
 import org.apache.commons.codec.binary.Hex;
 import org.apache.logging.log4j.LogManager;
 import org.apache.logging.log4j.Logger;
@@ -53,6 +60,7 @@
 import org.logstash.common.DeadLetterQueueFactory;
 import org.logstash.common.EnvironmentVariableProvider;
 import org.logstash.common.SourceWithMetadata;
+import org.logstash.common.io.DeadLetterQueueWriter;
 import org.logstash.config.ir.ConfigCompiler;
 import org.logstash.config.ir.InvalidIRException;
 import org.logstash.config.ir.PipelineConfig;
@@ -67,6 +75,9 @@
 import org.logstash.secret.store.SecretStore;
 import org.logstash.secret.store.SecretStoreExt;
 
+import static org.logstash.common.io.DeadLetterQueueWriter.NOOP_AGE_POLICY;
+import static org.logstash.common.io.DeadLetterQueueWriter.NOOP_SIZE_POLICY;
+
 /**
  * JRuby extension to provide ancestor class for Ruby's Pipeline and JavaPipeline classes.
  * */
@@ -278,13 +289,28 @@ public final IRubyObject lir(final ThreadContext context) {
     public final IRubyObject dlqWriter(final ThreadContext context) {
         if (dlqWriter == null) {
             if (dlqEnabled(context).isTrue()) {
+                final DeadLetterQueueWriter.DLQRetentionPolicy<Instant> ageRetentionPolicy;
+                if (hasSetting(context, "dead_letter_queue.retain.age") && !getSetting(context, "dead_letter_queue.retain.age").isNil()) {
+                    // convert to Duration
+                    final Duration age = parseToDuration(getSetting(context, "dead_letter_queue.retain.age").convertToString().toString());
+                    ageRetentionPolicy = new DeadLetterQueueWriter.AgeRetentionPolicy(age);
+                } else {
+                    ageRetentionPolicy = NOOP_AGE_POLICY;
+                }
+                final DeadLetterQueueWriter.DLQRetentionPolicy<Long> sizeRetentionPolicy;
+                if (hasSetting(context, "dead_letter_queue.retain.size") && !getSetting(context, "dead_letter_queue.retain.size").isNil()) {
+                    final long retainedSize = parseToBytes(getSetting(context, "dead_letter_queue.retain.size").convertToString().toString());
+                    sizeRetentionPolicy = new DeadLetterQueueWriter.SizeRetentionPolicy(retainedSize);
+                } else {
+                    sizeRetentionPolicy = NOOP_SIZE_POLICY;
+                }
                 dlqWriter = JavaUtil.convertJavaToUsableRubyObject(
                     context.runtime,
                     DeadLetterQueueFactory.getWriter(
                         pipelineId.asJavaString(),
                         getSetting(context, "path.dead_letter_queue").asJavaString(),
                         getSetting(context, "dead_letter_queue.max_bytes").convertToInteger().getLongValue(),
-                        Duration.ofMillis(getSetting(context, "dead_letter_queue.flush_interval").convertToInteger().getLongValue()))
+                        Duration.ofMillis(getSetting(context, "dead_letter_queue.flush_interval").convertToInteger().getLongValue()), ageRetentionPolicy, sizeRetentionPolicy)
                     );
             } else {
                 dlqWriter = RubyUtil.DUMMY_DLQ_WRITER_CLASS.callMethod(context, "new");
@@ -293,6 +319,39 @@ public final IRubyObject dlqWriter(final ThreadContext context) {
         return dlqWriter;
     }
 
+    /**
+     * Convert values like 512mb to the corresponding value in bytes.
+     * */
+    private long parseToBytes(String bytesSize) {
+        final Matcher matcher = Pattern.compile("(?<value>\\d+)\\s*mb").matcher(bytesSize);
+        if (!matcher.matches()) {
+            throw new IllegalArgumentException("Cannot parse space specification [" + bytesSize + "], expected format: <number>mb");
+        }
+        final long value = Long.parseLong(matcher.group("value"));
+        return value * 1024 * 1024;
+    }
+
+    /**
+     * Convert time strings like 3d or 4h or 5m to a duration
+     * */
+    private Duration parseToDuration(String timeStr) {
+        final Matcher matcher = Pattern.compile("(?<value>\\d+)\\s*(?<time>[dhms])").matcher(timeStr);
+        if (!matcher.matches()) {
+            throw new IllegalArgumentException("Cannot parse time specification [" + timeStr + "]");
+        }
+        final int value = Integer.parseInt(matcher.group("value"));
+        final String timeSpecifier = matcher.group("time");
+        final TemporalUnit unit;
+        switch(timeSpecifier) {
+            case "d": unit = ChronoUnit.DAYS; break;
+            case "h": unit = ChronoUnit.HOURS; break;
+            case "m": unit = ChronoUnit.MINUTES; break;
+            case "s": unit = ChronoUnit.SECONDS; break;
+            default: unit = ChronoUnit.SECONDS;
+        }
+        return Duration.of(value, unit);
+    }
+
     @JRubyMethod(name = "dlq_enabled?")
     public final IRubyObject dlqEnabled(final ThreadContext context) {
         return getSetting(context, "dead_letter_queue.enable");
diff --git a/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java b/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java
index 12df8049b41..8ed69fa3271 100644
--- a/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/DeadLetterQueueFactoryTest.java
@@ -51,6 +51,8 @@
 
 import static junit.framework.TestCase.assertSame;
 import static org.junit.Assert.assertTrue;
+import static org.logstash.common.io.DeadLetterQueueWriter.NOOP_AGE_POLICY;
+import static org.logstash.common.io.DeadLetterQueueWriter.NOOP_SIZE_POLICY;
 
 public class DeadLetterQueueFactoryTest {
     public static final String PIPELINE_NAME = "pipelineA";
@@ -68,9 +70,9 @@ public void setUp() throws Exception {
     public void testSameBeforeRelease() throws IOException {
         try {
             Path pipelineA = dir.resolve(PIPELINE_NAME);
-            DeadLetterQueueWriter writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1));
+            DeadLetterQueueWriter writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1), NOOP_AGE_POLICY, NOOP_SIZE_POLICY);
             assertTrue(writer.isOpen());
-            DeadLetterQueueWriter writer2 = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1));
+            DeadLetterQueueWriter writer2 = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1), NOOP_AGE_POLICY, NOOP_SIZE_POLICY);
             assertSame(writer, writer2);
             writer.close();
         } finally {
@@ -82,11 +84,11 @@ public void testSameBeforeRelease() throws IOException {
     public void testOpenableAfterRelease() throws IOException {
         try {
             Path pipelineA = dir.resolve(PIPELINE_NAME);
-            DeadLetterQueueWriter writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1));
+            DeadLetterQueueWriter writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1), NOOP_AGE_POLICY, NOOP_SIZE_POLICY);
             assertTrue(writer.isOpen());
             writer.close();
             DeadLetterQueueFactory.release(PIPELINE_NAME);
-            writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1));
+            writer = DeadLetterQueueFactory.getWriter(PIPELINE_NAME, pipelineA.toString(), 10000, Duration.ofSeconds(1), NOOP_AGE_POLICY, NOOP_SIZE_POLICY);
             assertTrue(writer.isOpen());
             writer.close();
         }finally{
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
index f1a0d1af747..00e392a1122 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueReaderTest.java
@@ -31,6 +31,7 @@
 import org.logstash.ackedqueue.StringElement;
 
 import java.io.IOException;
+import java.nio.file.Files;
 import java.nio.file.Path;
 import java.time.Duration;
 import java.util.*;
@@ -40,6 +41,9 @@
 
 import static org.hamcrest.MatcherAssert.assertThat;
 import static org.hamcrest.Matchers.*;
+import static org.junit.Assert.assertEquals;
+import static org.junit.Assert.assertNull;
+import static org.logstash.common.io.DeadLetterQueueWriterTest.MB;
 import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
 
@@ -56,6 +60,14 @@ private static String segmentFileName(int i) {
         return String.format(DeadLetterQueueWriter.SEGMENT_FILE_PATTERN, i);
     }
 
+    private static void deleteSegment(Path file) {
+        try {
+            Files.delete(file);
+        } catch (IOException e) {
+            e.printStackTrace();
+        }
+    }
+
     @Before
     public void setUp() throws Exception {
         dir = temporaryFolder.newFolder().toPath();
@@ -124,16 +136,16 @@ public void testReadFromTwoSegments() throws Exception {
     // This test checks that polling after a block has been mostly filled with an event is handled correctly.
     @Test
     public void testRereadFinalBlock() throws Exception {
-        Event event = createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        Event event = DeadLetterQueueTestUtils.createEventWithConstantSerializationOverhead(Collections.emptyMap());
 
         // Fill event with not quite enough characters to fill block. Fill event with valid RecordType characters - this
         // was the cause of https://github.com/elastic/logstash/issues/7868
-        event.setField("message", generateMessageContent(32495));
+        event.setField("message", DeadLetterQueueTestUtils.generateMessageContent(32495));
         long startTime = System.currentTimeMillis();
         int messageSize = 0;
         try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
             for (int i = 0; i < 2; i++) {
-                DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), constantSerializationLengthTimestamp(startTime++));
+                DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime++));
                 final int serializationLength = entry.serialize().length;
                 assertThat("setup: serialized entry size...", serializationLength, is(lessThan(BLOCK_SIZE)));
                 messageSize += serializationLength;
@@ -181,16 +193,31 @@ public void testSeekToMiddleOfRemovedLog() throws Exception {
         validateEntries(startLog, 2, 3, 32);
     }
 
+    @Test
+    public void testSeekToMiddleWhileTheLogIsRemoved() throws IOException, InterruptedException {
+        writeSegmentSizeEntries(3);
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
+
+            // removes 2 segments simulating a retention policy action
+            Files.delete(dir.resolve("1.log"));
+            Files.delete(dir.resolve("2.log"));
+
+            readManager.setCurrentReaderAndPosition(dir.resolve("1.log"), 1);
+            DLQEntry readEntry = readManager.pollEntry(100);
+            assertThat(readEntry.getReason(), equalTo(String.valueOf(3)));
+        }
+    }
+
     private void writeSegmentSizeEntries(int count) throws IOException {
         final Event event = createEventWithConstantSerializationOverhead();
         long startTime = System.currentTimeMillis();
-        DLQEntry templateEntry = new DLQEntry(event, "1", "1", String.valueOf(0), constantSerializationLengthTimestamp(startTime));
+        DLQEntry templateEntry = new DLQEntry(event, "1", "1", String.valueOf(0), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime));
         int size = templateEntry.serialize().length + RecordIOWriter.RECORD_HEADER_SIZE + VERSION_SIZE;
         DeadLetterQueueWriter writeManager = null;
         try {
             writeManager = new DeadLetterQueueWriter(dir, size, defaultDlqSize, Duration.ofSeconds(1));
             for (int i = 1; i <= count; i++) {
-                writeManager.writeEntry(new DLQEntry(event, "1", "1", String.valueOf(i), constantSerializationLengthTimestamp(startTime++)));
+                writeManager.writeEntry(new DLQEntry(event, "1", "1", String.valueOf(i), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime++)));
             }
         } finally {
             writeManager.close();
@@ -199,16 +226,12 @@ private void writeSegmentSizeEntries(int count) throws IOException {
 
 
     private void validateEntries(Path firstLog, int startEntry, int endEntry, int startPosition) throws IOException, InterruptedException {
-        DeadLetterQueueReader readManager = null;
-        try {
-            readManager = new DeadLetterQueueReader(dir);
+        try (DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
             readManager.setCurrentReaderAndPosition(firstLog, startPosition);
             for (int i = startEntry; i <= endEntry; i++) {
                 DLQEntry readEntry = readManager.pollEntry(100);
                 assertThat(readEntry.getReason(), equalTo(String.valueOf(i)));
             }
-        } finally {
-            readManager.close();
         }
     }
 
@@ -252,7 +275,7 @@ public void testBlockBoundaryMultiple() throws Exception {
         int messageSize = 0;
         try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
             for (int i = 1; i <= 5; i++) {
-                DLQEntry entry = new DLQEntry(event, "", "", "", constantSerializationLengthTimestamp(startTime++));
+                DLQEntry entry = new DLQEntry(event, "", "", "", DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime++));
                 messageSize += entry.serialize().length;
                 writeManager.writeEntry(entry);
                 if (i == 4){
@@ -270,7 +293,7 @@ public void testBlockBoundaryMultiple() throws Exception {
     @Test
     public void testFlushAfterWriterClose() throws Exception {
         Event event = new Event();
-        event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT/8));
+        event.setField("T", DeadLetterQueueTestUtils.generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT/8));
         Timestamp timestamp = new Timestamp();
 
         try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))) {
@@ -291,7 +314,7 @@ public void testFlushAfterWriterClose() throws Exception {
     public void testFlushAfterSegmentComplete() throws Exception {
         Event event = new Event();
         final int EVENTS_BEFORE_FLUSH = randomBetween(1, 32);
-        event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
+        event.setField("T", DeadLetterQueueTestUtils.generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = new Timestamp();
 
         try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * EVENTS_BEFORE_FLUSH, defaultDlqSize, Duration.ofHours(1))) {
@@ -324,7 +347,7 @@ public void testMultiFlushAfterSegmentComplete() throws Exception {
         final int eventsInSegment = randomBetween(1, 32);
         // Write enough events to not quite complete a second segment.
         final int totalEventsToWrite = (2 * eventsInSegment) - 1;
-        event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
+        event.setField("T", DeadLetterQueueTestUtils.generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = new Timestamp();
 
         try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE * eventsInSegment, defaultDlqSize, Duration.ofHours(1))) {
@@ -363,7 +386,7 @@ public void testFlushAfterDelay() throws Exception {
         Event event = new Event();
         int eventsPerBlock = randomBetween(1,16);
         int eventsToWrite = eventsPerBlock - 1;
-        event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT/eventsPerBlock));
+        event.setField("T", DeadLetterQueueTestUtils.generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT/eventsPerBlock));
         Timestamp timestamp = new Timestamp();
 
         System.out.println("events per block= " + eventsPerBlock);
@@ -397,7 +420,7 @@ public void testFlushAfterDelay() throws Exception {
     @Test
     public void testBlockAndSegmentBoundary() throws Exception {
         Event event = createEventWithConstantSerializationOverhead();
-        event.setField("T", generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
+        event.setField("T", DeadLetterQueueTestUtils.generateMessageContent(PAD_FOR_BLOCK_SIZE_EVENT));
         Timestamp timestamp = constantSerializationLengthTimestamp();
 
         try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, BLOCK_SIZE, defaultDlqSize, Duration.ofSeconds(1))) {
@@ -423,7 +446,7 @@ public void testWriteReadRandomEventSize() throws Exception {
 
         try(DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * 1024 * 1024, defaultDlqSize, Duration.ofSeconds(1))) {
             for (int i = 0; i < eventCount; i++) {
-                event.setField("message", generateMessageContent((int)(Math.random() * (maxEventSize))));
+                event.setField("message", DeadLetterQueueTestUtils.generateMessageContent((int)(Math.random() * (maxEventSize))));
                 DLQEntry entry = new DLQEntry(event, "", "", String.valueOf(i), new Timestamp(startTime++));
                 writeManager.writeEntry(entry);
             }
@@ -468,6 +491,47 @@ public void testWriteStopBigWriteSeekByTimestamp() throws Exception {
                           String.valueOf(FIRST_WRITE_EVENT_COUNT));
     }
 
+    @Test
+    public void testSeekByTimestampMoveAfterDeletedSegment() throws IOException, InterruptedException {
+        final long startTime = 1646296760000L;
+        final int eventsPerSegment = prepareFilledSegmentFiles(2, startTime);
+
+        try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
+            // remove the first segment
+            Files.delete(dir.resolve(segmentFileName(1)));
+
+            //Exercise, seek in the middle of first segment
+            final Timestamp seekTarget = new Timestamp(startTime + (eventsPerSegment / 2));
+            reader.seekToNextEvent(seekTarget);
+
+            // Verify, hit the first event of the second segment
+            DLQEntry readEntry = reader.pollEntry(100);
+            assertEquals("Must load first event of next available segment", readEntry.getReason(), String.format("%05d", eventsPerSegment));
+            final Timestamp firstEventSecondSegmentTimestamp = new Timestamp(startTime + eventsPerSegment);
+            assertEquals(firstEventSecondSegmentTimestamp, readEntry.getEntryTime());
+        }
+    }
+
+    @Test
+    public void testSeekByTimestampWhenAllSegmentsAreDeleted() throws IOException, InterruptedException {
+        final long startTime = System.currentTimeMillis();
+        final int eventsPerSegment = prepareFilledSegmentFiles(2, startTime);
+
+        try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
+            // remove the first segment
+            Files.delete(dir.resolve(segmentFileName(1)));
+            Files.delete(dir.resolve(segmentFileName(2)));
+
+            //Exercise, seek in the middle of first segment
+            final Timestamp seekTarget = new Timestamp(startTime + (eventsPerSegment / 2));
+            reader.seekToNextEvent(seekTarget);
+
+            // Verify, hit the first event of the second segment
+            DLQEntry readEntry = reader.pollEntry(100);
+            assertNull("No entry is available after all segments are deleted", readEntry);
+        }
+    }
+
     /**
      * Tests concurrently reading and writing from the DLQ.
      * @throws Exception On Failure
@@ -485,7 +549,7 @@ public void testConcurrentWriteReadRandomEventSize() throws Exception {
                     for (int i = 0; i < eventCount; i++) {
                         event.setField(
                                 "message",
-                                generateMessageContent((int) (Math.random() * (maxEventSize)))
+                                DeadLetterQueueTestUtils.generateMessageContent((int) (Math.random() * (maxEventSize)))
                         );
                         writeManager.writeEntry(
                                 new DLQEntry(
@@ -519,59 +583,12 @@ public void testConcurrentWriteReadRandomEventSize() throws Exception {
         }
     }
 
-    /**
-     * Produces a {@link Timestamp} whose epoch milliseconds is _near_ the provided value
-     * such that the result will have a constant serialization length of 24 bytes.
-     *
-     * If the provided epoch millis is exactly a whole second with no remainder, one millisecond
-     * is added to the value to ensure that there are remainder millis.
-     *
-     * @param millis
-     * @return
-     */
-    private Timestamp constantSerializationLengthTimestamp(long millis) {
-        if ( millis % 1000 == 0) { millis += 1; }
-
-        final Timestamp timestamp = new Timestamp(millis);
-        assertThat(String.format("pre-validation: expected timestamp to serialize to exactly 24 bytes, got `%s`", timestamp),
-                   timestamp.serialize().length, is(24));
-        return new Timestamp(millis);
-    }
-
     private Timestamp constantSerializationLengthTimestamp() {
-        return constantSerializationLengthTimestamp(System.currentTimeMillis());
-    }
-
-    private Timestamp constantSerializationLengthTimestamp(final Timestamp basis) {
-        return constantSerializationLengthTimestamp(basis.toEpochMilli());
-    }
-
-    /**
-     * Because many of the tests here rely on _exact_ alignment of serialization byte size,
-     * and the {@link Timestamp} has a variable-sized serialization length, we need a way to
-     * generated events whose serialization length will not vary depending on the millisecond
-     * in which the test was run.
-     *
-     * This method uses the normal method of creating an event, and ensures that the value of
-     * the timestamp field will serialize to a constant length, truncating precision and
-     * possibly shifting the value to ensure that there is sub-second remainder millis.
-     *
-     * @param data
-     * @return
-     */
-    private Event createEventWithConstantSerializationOverhead(final Map<String, Object> data) {
-        final Event event = new Event(data);
-
-        final Timestamp existingTimestamp = event.getTimestamp();
-        if (existingTimestamp != null) {
-            event.setTimestamp(constantSerializationLengthTimestamp(existingTimestamp));
-        }
-
-        return event;
+        return DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(System.currentTimeMillis());
     }
 
     private Event createEventWithConstantSerializationOverhead() {
-        return createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        return DeadLetterQueueTestUtils.createEventWithConstantSerializationOverhead(Collections.emptyMap());
     }
 
     private int randomBetween(int from, int to){
@@ -579,21 +596,6 @@ private int randomBetween(int from, int to){
         return r.nextInt((to - from) + 1) + from;
     }
 
-    private String generateMessageContent(int size) {
-        char[] valid = new char[RecordType.values().length + 1];
-        int j = 0;
-        valid[j] = 'x';
-        for (RecordType type : RecordType.values()){
-            valid[j++] = (char)type.toByte();
-        }
-        Random random = new Random();
-        char fillWith = valid[random.nextInt(valid.length)];
-
-        char[] fillArray = new char[size];
-        Arrays.fill(fillArray, fillWith);
-        return new String(fillArray);
-    }
-
     private void seekReadAndVerify(final Timestamp seekTarget, final String expectedValue) throws Exception {
         try(DeadLetterQueueReader readManager = new DeadLetterQueueReader(dir)) {
             readManager.seekToNextEvent(seekTarget);
@@ -611,4 +613,90 @@ private void writeEntries(final Event event, int offset, final int numberOfEvent
             }
         }
     }
+
+    @Test
+    public void testReaderFindSegmentHoleAfterSimulatingRetentionPolicyClean() throws IOException, InterruptedException {
+        final int eventsPerSegment = prepareFilledSegmentFiles(3);
+        int remainingEventsInSegment = eventsPerSegment;
+
+        try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
+            // read the first event to initialize reader structures
+            final DLQEntry dlqEntry = reader.pollEntry(1_000);
+            assertEquals("00000", dlqEntry.getReason());
+            remainingEventsInSegment--;
+
+            // simulate a retention policy clean, drop the middle segment file
+            final Path segmentToDrop = Files.list(dir)
+                    .sorted()
+                    .skip(1)
+                    .findFirst()
+                    .orElseThrow(() -> new IllegalStateException("Expected to grab the second segment"));
+            Files.delete(segmentToDrop);
+
+            // consume the first segment
+            for (int i = 0; i < remainingEventsInSegment; i++) {
+                reader.pollEntry(1_000);
+            }
+
+            // Exercise
+            // consume the first event after the hole
+            final DLQEntry entryAfterHole = reader.pollEntry(1_000);
+            assertEquals(319, eventsPerSegment);
+            assertEquals(String.format("%05d", eventsPerSegment * 2), entryAfterHole.getReason());
+        }
+    }
+
+    @Test
+    public void testReaderHitNoMoreSegmentsAfterSimulatingRetentionPolicyClean() throws IOException, InterruptedException {
+        int remainingEventsInSegment = prepareFilledSegmentFiles(3);
+
+        try (DeadLetterQueueReader reader = new DeadLetterQueueReader(dir)) {
+            // read the first event to initialize reader structures
+            final DLQEntry dlqEntry = reader.pollEntry(1_000);
+            assertEquals("00000", dlqEntry.getReason());
+            remainingEventsInSegment--;
+
+            // simulate a retention policy clean, that drops the remaining segments
+            Files.list(dir)
+                    .sorted()
+                    .skip(1)
+                    .forEach(DeadLetterQueueReaderTest::deleteSegment);
+
+            // consume the first segment
+            for (int i = 0; i < remainingEventsInSegment; i++) {
+                reader.pollEntry(1_000);
+            }
+
+            // Exercise
+            // consume the first event after the hole
+            final DLQEntry entryAfterHole = reader.pollEntry(1_000);
+            assertNull(entryAfterHole);
+        }
+    }
+
+    private int prepareFilledSegmentFiles(int segments) throws IOException {
+        return prepareFilledSegmentFiles(segments, System.currentTimeMillis());
+    }
+
+    private int prepareFilledSegmentFiles(int segments, long start) throws IOException {
+        final Event event = DeadLetterQueueTestUtils.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueTestUtils.generateMessageContent(32479));
+
+        DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", 1), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(start));
+        assertEquals("Serialized dlq entry + header MUST be 32Kb (size of a block)", BLOCK_SIZE, entry.serialize().length + 13);
+
+        final int maxSegmentSize = 10 * MB;
+        final int loopPerSegment = (int) Math.floor((maxSegmentSize - 1.0) / BLOCK_SIZE);
+        try (DeadLetterQueueWriter writer = new DeadLetterQueueWriter(dir, maxSegmentSize, defaultDlqSize, Duration.ofSeconds(1))) {
+            final int loops = loopPerSegment * segments;
+            for (int i = 0; i < loops; i++) {
+                entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(start++));
+                writer.writeEntry(entry);
+            }
+            System.out.println("end at millis: " + start + ", timestamp: " + DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(start));
+        }
+
+        assertEquals(segments, Files.list(dir).count());
+        return loopPerSegment;
+    }
 }
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
new file mode 100644
index 00000000000..587779fa024
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueTestUtils.java
@@ -0,0 +1,75 @@
+package org.logstash.common.io;
+
+import org.logstash.Event;
+import org.logstash.Timestamp;
+
+import java.util.Arrays;
+import java.util.Map;
+import java.util.Random;
+
+import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.is;
+
+final class DeadLetterQueueTestUtils {
+    /**
+     * Produces a {@link Timestamp} whose epoch milliseconds is _near_ the provided value
+     * such that the result will have a constant serialization length of 24 bytes.
+     *
+     * If the provided epoch millis is exactly a whole second with no remainder, one millisecond
+     * is added to the value to ensure that there are remainder millis.
+     *
+     * @param millis
+     * @return
+     */
+    static Timestamp constantSerializationLengthTimestamp(long millis) {
+        if ( millis % 1000 == 0) { millis += 1; }
+
+        final Timestamp timestamp = new Timestamp(millis);
+        assertThat(String.format("pre-validation: expected timestamp to serialize to exactly 24 bytes, got `%s`", timestamp),
+                   timestamp.serialize().length, is(24));
+        return new Timestamp(millis);
+    }
+
+    static String generateMessageContent(int size) {
+        char[] valid = new char[RecordType.values().length + 1];
+        int j = 0;
+        valid[j] = 'x';
+        for (RecordType type : RecordType.values()){
+            valid[j++] = (char)type.toByte();
+        }
+        Random random = new Random();
+        char fillWith = valid[random.nextInt(valid.length)];
+
+        char[] fillArray = new char[size];
+        Arrays.fill(fillArray, fillWith);
+        return new String(fillArray);
+    }
+
+    /**
+     * Because many of the tests here rely on _exact_ alignment of serialization byte size,
+     * and the {@link Timestamp} has a variable-sized serialization length, we need a way to
+     * generated events whose serialization length will not vary depending on the millisecond
+     * in which the test was run.
+     *
+     * This method uses the normal method of creating an event, and ensures that the value of
+     * the timestamp field will serialize to a constant length, truncating precision and
+     * possibly shifting the value to ensure that there is sub-second remainder millis.
+     *
+     * @param data
+     * @return
+     */
+    static Event createEventWithConstantSerializationOverhead(final Map<String, Object> data) {
+        final Event event = new Event(data);
+
+        final Timestamp existingTimestamp = event.getTimestamp();
+        if (existingTimestamp != null) {
+            event.setTimestamp(constantSerializationLengthTimestamp(existingTimestamp));
+        }
+
+        return event;
+    }
+
+    static Timestamp constantSerializationLengthTimestamp(final Timestamp basis) {
+        return constantSerializationLengthTimestamp(basis.toEpochMilli());
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
index ad35a9ef46a..dd26f86a336 100644
--- a/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
+++ b/logstash-core/src/test/java/org/logstash/common/io/DeadLetterQueueWriterTest.java
@@ -25,10 +25,18 @@
 import java.nio.file.Files;
 import java.nio.file.Path;
 import java.nio.file.StandardOpenOption;
+import java.time.Clock;
 import java.time.Duration;
+import java.time.Instant;
+import java.time.ZoneId;
+import java.util.Collections;
+import java.util.List;
+import java.util.Set;
+import java.util.stream.Collectors;
 import java.util.stream.Stream;
 
 import org.hamcrest.CoreMatchers;
+import org.hamcrest.Matchers;
 import org.junit.Before;
 import org.junit.Rule;
 import org.junit.Test;
@@ -41,13 +49,50 @@
 import static org.hamcrest.CoreMatchers.is;
 import static org.hamcrest.CoreMatchers.not;
 import static org.hamcrest.MatcherAssert.assertThat;
+import static org.hamcrest.Matchers.greaterThan;
+import static org.hamcrest.Matchers.lessThan;
+import static org.junit.Assert.assertEquals;
 import static org.junit.Assert.assertTrue;
 import static org.junit.Assert.fail;
+import static org.logstash.common.io.DeadLetterQueueWriter.NOOP_AGE_POLICY;
+import static org.logstash.common.io.DeadLetterQueueWriter.NOOP_SIZE_POLICY;
 import static org.logstash.common.io.RecordIOWriter.BLOCK_SIZE;
 import static org.logstash.common.io.RecordIOWriter.RECORD_HEADER_SIZE;
 import static org.logstash.common.io.RecordIOWriter.VERSION_SIZE;
 
 public class DeadLetterQueueWriterTest {
+
+    static class ForwardableClock extends Clock {
+
+        private Clock currentClock;
+
+        ForwardableClock(Clock clock) {
+            this.currentClock = clock;
+        }
+
+        void forward(Duration period) {
+            currentClock = Clock.offset(currentClock, period);
+        }
+
+        @Override
+        public ZoneId getZone() {
+            return currentClock.getZone();
+        }
+
+        @Override
+        public Clock withZone(ZoneId zone) {
+            return currentClock.withZone(zone);
+        }
+
+        @Override
+        public Instant instant() {
+            return currentClock.instant();
+        }
+    }
+
+    public static final int MB = 1024 * 1024;
+    public static final int GB = 1024 * 1024 * 1024;
+    public static final int FULL_SEGMENT_FILE_SIZE = 319 * BLOCK_SIZE + 1; // 319 records that fills completely a block plus the 1 byte header of the segment file
     private Path dir;
 
     @Rule
@@ -209,4 +254,204 @@ private long dlqLength() throws IOException {
                 .mapToLong(p -> p.toFile().length()).sum();
         }
     }
-}
+
+    @Test
+    public void testRemoveOldestSegmentWhenRetainedSizeIsExceeded() throws IOException {
+        Event event = DeadLetterQueueTestUtils.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueTestUtils.generateMessageContent(32479));
+        long startTime = System.currentTimeMillis();
+
+        int messageSize = 0;
+        final DeadLetterQueueWriter.SizeRetentionPolicy sizeRetentionPolicy = new DeadLetterQueueWriter.SizeRetentionPolicy(20 * MB);
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 1 * GB,
+                Duration.ofSeconds(1), NOOP_AGE_POLICY, sizeRetentionPolicy)) {
+
+            // 320 generates 10 Mb of data
+            for (int i = 0; i < (320 * 2) - 1; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime++));
+                final int serializationLength = entry.serialize().length;
+                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                messageSize += serializationLength;
+                writeManager.writeEntry(entry);
+            }
+            assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
+        }
+
+        // but every segment file has 1 byte header, 639 messages of 32Kb generates 3 files
+        // 0.log with 319
+        // 1.log with 319
+        // 2.log with 1
+        List<String> segmentFileNames = Files.list(dir)
+                .map(Path::getFileName)
+                .map(Path::toString)
+                .sorted()
+                .collect(Collectors.toList());
+        assertEquals(3, segmentFileNames.size());
+        final String fileToBeRemoved = segmentFileNames.get(0);
+
+        // Exercise
+        // with another 32Kb message write we go to write the third file and trigger the 20Mb limit of retained store
+        final long prevQueueSize;
+        final long beheadedQueueSize;
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 1 * GB,
+                Duration.ofSeconds(1), NOOP_AGE_POLICY, sizeRetentionPolicy)) {
+            prevQueueSize = writeManager.getCurrentQueueSize();
+            final int expectedQueueSize = 2 * // number of full segment files
+                    FULL_SEGMENT_FILE_SIZE  + // size of a segment file
+                    VERSION_SIZE + BLOCK_SIZE + // the third segment file with just one message
+                    VERSION_SIZE; // the header of the head tmp file created in opening
+            assertEquals("Queue size is composed of 2 full segment files plus one with an event plus another with just the header byte",
+                    expectedQueueSize, prevQueueSize);
+            DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", (320 * 2) - 1), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime));
+            writeManager.writeEntry(entry);
+            beheadedQueueSize = writeManager.getCurrentQueueSize();
+        }
+
+        // 1.log with 319
+        // 2.log with 1
+        // 3.log with 1, created because the close flushes and beheaded the tail file.
+        Set<String> afterBeheadSegmentFileNames = Files.list(dir)
+                .map(Path::getFileName)
+                .map(Path::toString)
+                .collect(Collectors.toSet());
+        assertEquals(3, afterBeheadSegmentFileNames.size());
+        assertThat(afterBeheadSegmentFileNames, Matchers.not(Matchers.contains(fileToBeRemoved)));
+        final long expectedQueueSize = prevQueueSize +
+                BLOCK_SIZE - // the space of the newly inserted message
+                FULL_SEGMENT_FILE_SIZE; //the size of the removed segment file
+        assertEquals("Total queue size must be decremented by the size of the first segment file",
+                expectedQueueSize, beheadedQueueSize);
+    }
+
+    @Test
+    public void testRemoveOldestSegmentsWhenRetainedAgeIsExceeded_reopeningWriterUseCase() throws IOException {
+        final Event event = DeadLetterQueueTestUtils.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueTestUtils.generateMessageContent(32479));
+
+        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
+        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
+        prepareDLQWithFirstSegmentOlderThanRetainPeriod(event, fakeClock);
+
+        // Exercise
+        final long prevQueueSize;
+        final long beheadedQueueSize;
+
+        final DeadLetterQueueWriter.AgeRetentionPolicy ageRetentionPolicy = new DeadLetterQueueWriter.AgeRetentionPolicy(Duration.ofDays(2));
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 1 * GB,
+                Duration.ofSeconds(1), fakeClock, ageRetentionPolicy, NOOP_SIZE_POLICY)) {
+            prevQueueSize = writeManager.getCurrentQueueSize();
+            assertEquals("Queue size is composed of one just one empty file with version byte", VERSION_SIZE, prevQueueSize);
+
+            // write new entry that trigger clean of age retained segment
+            DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", 320), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(System.currentTimeMillis()));
+            writeManager.writeEntry(entry);
+            beheadedQueueSize = writeManager.getCurrentQueueSize();
+        }
+
+        assertEquals(VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+    }
+
+    private void prepareDLQWithFirstSegmentOlderThanRetainPeriod(Event event, ForwardableClock fakeClock) throws IOException {
+        final Duration littleMore2Days = Duration.ofDays(2).plusMinutes(1);
+        long startTime = fakeClock.instant().minus(littleMore2Days).toEpochMilli();
+        int messageSize = 0;
+
+        final DeadLetterQueueWriter.AgeRetentionPolicy ageRetentionPolicy = new DeadLetterQueueWriter.AgeRetentionPolicy(Duration.ofDays(2));
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 1 * GB,
+                Duration.ofSeconds(1), fakeClock, ageRetentionPolicy, NOOP_SIZE_POLICY)) {
+
+            // 320 generates 10 Mb of data
+            for (int i = 0; i < 320 - 1; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime++));
+                final int serializationLength = entry.serialize().length;
+                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                messageSize += serializationLength;
+                writeManager.writeEntry(entry);
+            }
+            assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
+        }
+    }
+
+    @Test
+    public void testRemoveOldestSegmentsWhenRetainedAgeIsExceeded_withContinuousWriterUseCase() throws IOException {
+        final Event event = DeadLetterQueueTestUtils.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueTestUtils.generateMessageContent(32479));
+        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
+        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
+
+        long startTime = fakeClock.instant().toEpochMilli();
+        int messageSize = 0;
+
+        final Duration retention = Duration.ofDays(2);
+        final DeadLetterQueueWriter.AgeRetentionPolicy ageRetentionPolicy = new DeadLetterQueueWriter.AgeRetentionPolicy(retention);
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 1 * GB,
+                Duration.ofSeconds(1), fakeClock, ageRetentionPolicy, NOOP_SIZE_POLICY)) {
+
+            // 320 generates 10 Mb of data
+            for (int i = 0; i < 320 - 1; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime++));
+                final int serializationLength = entry.serialize().length;
+                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                messageSize += serializationLength;
+                writeManager.writeEntry(entry);
+            }
+            assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
+
+            // Exercise
+            // write an event that goes in second segment
+            fakeClock.forward(retention.plusSeconds(1));
+            final long prevQueueSize = writeManager.getCurrentQueueSize();
+            assertEquals("Queue size is composed of one full segment files", FULL_SEGMENT_FILE_SIZE, prevQueueSize);
+
+            // write new entry that trigger clean of age retained segment
+            DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", 320), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(System.currentTimeMillis()));
+            writeManager.writeEntry(entry);
+            final long beheadedQueueSize = writeManager.getCurrentQueueSize();
+
+            assertEquals(VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+        }
+    }
+
+    @Test
+    public void testRemoveMultipleOldestSegmentsWhenRetainedAgeIsExceeded() throws IOException {
+        final Event event = DeadLetterQueueTestUtils.createEventWithConstantSerializationOverhead(Collections.emptyMap());
+        event.setField("message", DeadLetterQueueTestUtils.generateMessageContent(32479));
+
+        final Clock pointInTimeFixedClock = Clock.fixed(Instant.parse("2022-02-22T10:20:30.00Z"), ZoneId.of("Europe/Rome"));
+        final ForwardableClock fakeClock = new ForwardableClock(pointInTimeFixedClock);
+
+        long startTime = fakeClock.instant().toEpochMilli();
+        int messageSize = 0;
+
+        final Duration retention = Duration.ofDays(2);
+        final DeadLetterQueueWriter.AgeRetentionPolicy ageRetentionPolicy = new DeadLetterQueueWriter.AgeRetentionPolicy(retention);
+        try (DeadLetterQueueWriter writeManager = new DeadLetterQueueWriter(dir, 10 * MB, 1 * GB,
+                Duration.ofSeconds(1), fakeClock, ageRetentionPolicy, NOOP_SIZE_POLICY)) {
+
+            // 320 generates 10 Mb of data
+            for (int i = 0; i < 319 * 2; i++) {
+                DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", i), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(startTime++));
+                final int serializationLength = entry.serialize().length;
+                assertThat("setup: serialized entry size...", serializationLength, Matchers.is(lessThan(BLOCK_SIZE)));
+                messageSize += serializationLength;
+                writeManager.writeEntry(entry);
+            }
+            assertThat(messageSize, Matchers.is(greaterThan(BLOCK_SIZE)));
+
+            // make the retention age to pass for the first 2 full segments
+            fakeClock.forward(retention.plusSeconds(1));
+
+            // Exercise
+            // write an event that goes in second segment
+            final long prevQueueSize = writeManager.getCurrentQueueSize();
+            assertEquals("Queue size is composed of 2 full segment files", 2 * FULL_SEGMENT_FILE_SIZE, prevQueueSize);
+
+            // write new entry that trigger clean of age retained segment
+            DLQEntry entry = new DLQEntry(event, "", "", String.format("%05d", 320), DeadLetterQueueTestUtils.constantSerializationLengthTimestamp(System.currentTimeMillis()));
+            writeManager.writeEntry(entry);
+            final long beheadedQueueSize = writeManager.getCurrentQueueSize();
+
+            assertEquals(VERSION_SIZE + BLOCK_SIZE, beheadedQueueSize);
+        }
+    }
+}
\ No newline at end of file
