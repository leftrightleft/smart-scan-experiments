diff --git a/bin/lock b/bin/lock
new file mode 100755
index 00000000000..a8a0529a943
--- /dev/null
+++ b/bin/lock
@@ -0,0 +1,9 @@
+#!/usr/bin/env bin/ruby
+
+require_relative "../lib/bootstrap/environment"
+LogStash::Bundler.setup!({:without => [:build, :development]})
+require "logstash-core"
+
+lock = Java::OrgLogstash::FileLockFactory.getDefault.obtainLock(ARGV[0], ".lock")
+puts("locking " + File.join(ARGV[0], ".lock"))
+sleep
diff --git a/logstash-core/lib/logstash/agent.rb b/logstash-core/lib/logstash/agent.rb
index eb8995ec49d..2eb8eb0f730 100644
--- a/logstash-core/lib/logstash/agent.rb
+++ b/logstash-core/lib/logstash/agent.rb
@@ -189,7 +189,22 @@ def running_pipelines?
     end
   end
 
+  def close_pipeline(id)
+    pipeline = @pipelines[id]
+    if pipeline
+      @logger.warn("closing pipeline", :id => id)
+      pipeline.close
+    end
+  end
+
+  def close_pipelines
+    @pipelines.each  do |id, _|
+      close_pipeline(id)
+    end
+  end
+
   private
+
   def start_webserver
     options = {:http_host => @http_host, :http_ports => @http_port, :http_environment => @http_environment }
     @webserver = LogStash::WebServer.new(@logger, self, options)
@@ -232,7 +247,21 @@ def collect_metrics?
     @collect_metric
   end
 
-  def create_pipeline(settings, config=nil)
+  def increment_reload_failures_metrics(id, message, backtrace = nil)
+    @instance_reload_metric.increment(:failures)
+    @pipeline_reload_metric.namespace([id.to_sym, :reloads]).tap do |n|
+      n.increment(:failures)
+      n.gauge(:last_error, { :message => message, :backtrace =>backtrace})
+      n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
+    end
+    if @logger.debug?
+      @logger.error("Cannot load an invalid configuration", :reason => message, :backtrace => backtrace)
+    else
+      @logger.error("Cannot load an invalid configuration", :reason => message)
+    end
+  end
+
+  def create_pipeline(settings, config = nil)
     if config.nil?
       begin
         config = fetch_config(settings)
@@ -245,17 +274,7 @@ def create_pipeline(settings, config=nil)
     begin
       LogStash::Pipeline.new(config, settings, metric)
     rescue => e
-      @instance_reload_metric.increment(:failures)
-      @pipeline_reload_metric.namespace([settings.get("pipeline.id").to_sym, :reloads]).tap do |n|
-        n.increment(:failures)
-        n.gauge(:last_error, { :message => e.message, :backtrace => e.backtrace})
-        n.gauge(:last_failure_timestamp, LogStash::Timestamp.now)
-      end
-      if @logger.debug?
-        @logger.error("fetched an invalid config", :config => config, :reason => e.message, :backtrace => e.backtrace)
-      else
-        @logger.error("fetched an invalid config", :config => config, :reason => e.message)
-      end
+      increment_reload_failures_metrics(settings.get("pipeline.id"), e.message, e.backtrace)
       return
     end
   end
@@ -264,29 +283,96 @@ def fetch_config(settings)
     @config_loader.format_config(settings.get("path.config"), settings.get("config.string"))
   end
 
-  # since this method modifies the @pipelines hash it is
-  # wrapped in @upgrade_mutex in the parent call `reload_state!`
+  # reload_pipeline trys to reloads the pipeline with id using a potential new configuration if it changed
+  # since this method modifies the @pipelines hash it is wrapped in @upgrade_mutex in the parent call `reload_state!`
+  # @param id [String] the pipeline id to reload
   def reload_pipeline!(id)
     old_pipeline = @pipelines[id]
     new_config = fetch_config(old_pipeline.settings)
+
     if old_pipeline.config_str == new_config
       @logger.debug("no configuration change for pipeline", :pipeline => id)
       return
     end
 
-    new_pipeline = create_pipeline(old_pipeline.settings, new_config)
+    # check if this pipeline is not reloadable. it should not happen as per the check below
+    # but keep it here as a safety net if a reloadable pipeline was releoaded with a non reloadable pipeline
+    if old_pipeline.non_reloadable_plugins.any?
+      @logger.error("pipeline is not reloadable", :pipeline => id)
+      return
+    end
+
+    # BasePipeline#initialize will compile the config, and load all plugins and raise an exception
+    # on an invalid configuration
+    begin
+      pipeline_validator = LogStash::BasePipeline.new(new_config, old_pipeline.settings)
+    rescue => e
+      increment_reload_failures_metrics(id, e.message, e.backtrace)
+      return
+    end
+
+    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort
+    if pipeline_validator.non_reloadable_plugins.any?
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => id, :plugins => pipeline_validator.non_reloadable_plugins.map(&:class))
+      increment_reload_failures_metrics(id, "non reloadable pipeline")
+      return
+    end
+
+    # we know configis valid so we are fairly comfortable to first stop old pipeline and then start new one
+    upgrade_pipeline(id, old_pipeline.settings, new_config)
+  end
+
+  # upgrade_pipeline first stops the old pipeline and starts the new one
+  # this method exists only for specs to be able to expects this to be executed
+  # @params pipeline_id [String] the pipeline id to upgrade
+  # @params settings [Settings] the settings for the new pipeline
+  # @params new_config [String] the new pipeline config
+  def upgrade_pipeline(pipeline_id, settings, new_config)
+    @logger.warn("fetched new config for pipeline. upgrading..", :pipeline => pipeline_id, :config => new_config)
+
+    # first step: stop the old pipeline.
+    # IMPORTANT: a new pipeline with same settings should not be instantiated before the previous one is shutdown
+
+    stop_pipeline(pipeline_id)
+    reset_pipeline_metrics(pipeline_id)
+
+    # second step create and start a new pipeline now that the old one is shutdown
 
-    return if new_pipeline.nil?
+    new_pipeline = create_pipeline(settings, new_config)
+    if new_pipeline.nil?
+      # this is a scenario where the configuration is valid (compilable) but the new pipeline refused to start
+      # and at this point NO pipeline is running
+      @logger.error("failed to create the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
+      increment_reload_failures_metrics(pipeline_id, "failed to create the reloaded pipeline")
+      return
+    end
 
+    ### at this point pipeline#close must be called if upgrade_pipeline does not succeed
+
+    # check if the new pipeline will be reloadable in which case we want to log that as an error and abort. this should normally not
+    # happen since the check should be done in reload_pipeline! prior to get here.
     if new_pipeline.non_reloadable_plugins.any?
-      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"),
-                    :pipeline_id => id,
-                    :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
+      @logger.error(I18n.t("logstash.agent.non_reloadable_config_reload"), :pipeline_id => pipeline_id, :plugins => new_pipeline.non_reloadable_plugins.map(&:class))
+      increment_reload_failures_metrics(pipeline_id, "non reloadable pipeline")
+      new_pipeline.close
       return
-    else
-      @logger.warn("fetched new config for pipeline. upgrading..",
-                   :pipeline => id, :config => new_pipeline.config_str)
-      upgrade_pipeline(id, new_pipeline)
+    end
+
+    # @pipelines[pipeline_id] must be initialized before #start_pipeline below which uses it
+    @pipelines[pipeline_id] = new_pipeline
+
+    if !start_pipeline(pipeline_id)
+      @logger.error("failed to start the reloaded pipeline and no pipeline is currently running", :pipeline => pipeline_id)
+      # do not call increment_reload_failures_metrics here since #start_pipeline already does it on failure
+      new_pipeline.close
+      return
+    end
+
+    # pipeline started successfuly, update reload success metrics
+    @instance_reload_metric.increment(:successes)
+    @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
+      n.increment(:successes)
+      n.gauge(:last_success_timestamp, LogStash::Timestamp.now)
     end
   end
 
@@ -348,20 +434,6 @@ def running_pipeline?(pipeline_id)
     thread.is_a?(Thread) && thread.alive?
   end
 
-  def upgrade_pipeline(pipeline_id, new_pipeline)
-    stop_pipeline(pipeline_id)
-    reset_pipeline_metrics(pipeline_id)
-    @pipelines[pipeline_id] = new_pipeline
-    if start_pipeline(pipeline_id) # pipeline started successfuly
-      @instance_reload_metric.increment(:successes)
-      @pipeline_reload_metric.namespace([pipeline_id.to_sym, :reloads]).tap do |n|
-        n.increment(:successes)
-        n.gauge(:last_success_timestamp, LogStash::Timestamp.now)
-      end
-      
-    end
-  end
-
   def clean_state?
     @pipelines.empty?
   end
diff --git a/logstash-core/lib/logstash/pipeline.rb b/logstash-core/lib/logstash/pipeline.rb
index 2b75172d6b0..5424d4db0e0 100644
--- a/logstash-core/lib/logstash/pipeline.rb
+++ b/logstash-core/lib/logstash/pipeline.rb
@@ -22,43 +22,24 @@
 require "logstash/output_delegator"
 require "logstash/filter_delegator"
 
-module LogStash; class Pipeline
+module LogStash; class BasePipeline
   include LogStash::Util::Loggable
 
-  attr_reader :inputs,
-    :filters,
-    :outputs,
-    :worker_threads,
-    :events_consumed,
-    :events_filtered,
-    :reporter,
-    :pipeline_id,
-    :started_at,
-    :thread,
-    :config_str,
-    :config_hash,
-    :settings,
-    :metric,
-    :filter_queue_client,
-    :input_queue_client,
-    :queue
-
-  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
+  attr_reader :config_str, :config_hash, :inputs, :filters, :outputs, :pipeline_id
 
   RELOAD_INCOMPATIBLE_PLUGINS = [
-    "LogStash::Inputs::Stdin"
+      "LogStash::Inputs::Stdin"
   ]
 
-  def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
+  def initialize(config_str, settings = SETTINGS)
     @logger = self.logger
     @config_str = config_str
     @config_hash = Digest::SHA1.hexdigest(@config_str)
     # Every time #plugin is invoked this is incremented to give each plugin
     # a unique id when auto-generating plugin ids
     @plugin_counter ||= 0
-    @settings = settings
-    @pipeline_id = @settings.get_value("pipeline.id") || self.object_id
-    @reporter = PipelineReporter.new(@logger, self)
+
+    @pipeline_id = settings.get_value("pipeline.id") || self.object_id
 
     # A list of plugins indexed by id
     @plugins_by_id = {}
@@ -66,8 +47,88 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     @filters = nil
     @outputs = nil
 
-    @worker_threads = []
+    grammar = LogStashConfigParser.new
+    parsed_config = grammar.parse(config_str)
+    raise(ConfigurationError, grammar.failure_reason) if parsed_config.nil?
+
+    config_code = parsed_config.compile
+
+    # config_code = BasePipeline.compileConfig(config_str)
+
+    if settings.get_value("config.debug") && @logger.debug?
+      @logger.debug("Compiled pipeline code", :code => config_code)
+    end
+
+    # Evaluate the config compiled code that will initialize all the plugins and define the
+    # filter and output methods.
+    begin
+      eval(config_code)
+    rescue => e
+      raise e
+    end
+  end
+
+  def plugin(plugin_type, name, *args)
+    @plugin_counter += 1
+
+    # Collapse the array of arguments into a single merged hash
+    args = args.reduce({}, &:merge)
+
+    id = if args["id"].nil? || args["id"].empty?
+           args["id"] = "#{@config_hash}-#{@plugin_counter}"
+         else
+           args["id"]
+         end
+
+    raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
+
+    @plugins_by_id[id] = true
+
+    # use NullMetric if called in the BasePipeline context otherwise use the @metric value
+    metric = @metric || Instrument::NullMetric.new
+
+    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
+
+    # Scope plugins of type 'input' to 'inputs'
+    type_scoped_metric = pipeline_scoped_metric.namespace("#{plugin_type}s".to_sym)
+
+    klass = Plugin.lookup(plugin_type, name)
+
+    if plugin_type == "output"
+      OutputDelegator.new(@logger, klass, type_scoped_metric, OutputDelegatorStrategyRegistry.instance,  args)
+    elsif plugin_type == "filter"
+      FilterDelegator.new(@logger, klass, type_scoped_metric, args)
+    else # input
+      input_plugin = klass.new(args)
+      input_plugin.metric = type_scoped_metric.namespace(id)
+      input_plugin
+    end
+  end
+
+  def non_reloadable_plugins
+    (inputs + filters + outputs).select do |plugin|
+      RELOAD_INCOMPATIBLE_PLUGINS.include?(plugin.class.name)
+    end
+  end
+end; end
+
+module LogStash; class Pipeline < BasePipeline
+  attr_reader \
+    :worker_threads,
+    :events_consumed,
+    :events_filtered,
+    :reporter,
+    :started_at,
+    :thread,
+    :settings,
+    :metric,
+    :filter_queue_client,
+    :input_queue_client,
+    :queue
+
+  MAX_INFLIGHT_WARN_THRESHOLD = 10_000
 
+  def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
     # This needs to be configured before we evaluate the code to make
     # sure the metric instance is correctly send to the plugins to make the namespace scoping work
     @metric = if namespaced_metric
@@ -76,29 +137,12 @@ def initialize(config_str, settings = SETTINGS, namespaced_metric = nil)
                 Instrument::NullMetric.new
               end
 
-    grammar = LogStashConfigParser.new
-    @config = grammar.parse(config_str)
-    if @config.nil?
-      raise ConfigurationError, grammar.failure_reason
-    end
-    # This will compile the config to ruby and evaluate the resulting code.
-    # The code will initialize all the plugins and define the
-    # filter and output methods.
-    code = @config.compile
-    @code = code
+    @settings = settings
+    @reporter = PipelineReporter.new(@logger, self)
+    @worker_threads = []
 
-    # The config code is hard to represent as a log message...
-    # So just print it.
+    super(config_str, settings)
 
-    if @settings.get_value("config.debug") && @logger.debug?
-      @logger.debug("Compiled pipeline code", :code => code)
-    end
-
-    begin
-      eval(code)
-    rescue => e
-      raise
-    end
     @queue = build_queue_from_settings
     @input_queue_client = @queue.write_client
     @filter_queue_client = @queue.read_client
@@ -202,8 +246,7 @@ def run
     shutdown_flusher
     shutdown_workers
 
-    @filter_queue_client.close
-    @queue.close
+    close
 
     @logger.debug("Pipeline #{@pipeline_id} has been shutdown")
 
@@ -211,6 +254,11 @@ def run
     return 0
   end # def run
 
+  def close
+    @filter_queue_client.close
+    @queue.close
+  end
+
   def transition_to_running
     @running.make_true
   end
@@ -227,12 +275,32 @@ def stopped?
     @running.false?
   end
 
+  # register_plugin simply calls the plugin #register method and catches & logs any error
+  # @param plugin [Plugin] the plugin to register
+  # @return [Plugin] the registered plugin
+  def register_plugin(plugin)
+    plugin.register
+    plugin
+  rescue => e
+    @logger.error("Error registering plugin", :plugin => plugin.inspect, :error => e.message)
+    raise e
+  end
+
+  # register_plugins calls #register_plugin on the plugins list and upon exception will call Plugin#do_close on all registered plugins
+  # @param plugins [Array[Plugin]] the list of plugins to register
+  def register_plugins(plugins)
+    registered = []
+    plugins.each { |plugin| registered << register_plugin(plugin) }
+  rescue => e
+    registered.each(&:do_close)
+    raise e
+  end
+
   def start_workers
     @worker_threads.clear # In case we're restarting the pipeline
     begin
-      start_inputs
-      @outputs.each {|o| o.register }
-      @filters.each {|f| f.register }
+      register_plugins(@outputs)
+      register_plugins(@filters)
 
       pipeline_workers = safe_pipeline_worker_count
       batch_size = @settings.get("pipeline.batch.size")
@@ -263,6 +331,16 @@ def start_workers
           worker_loop(batch_size, batch_delay)
         end
       end
+
+      # inputs should be started last, after all workers
+      begin
+        start_inputs
+      rescue => e
+        # if there is any exception in starting inputs, make sure we shutdown workers.
+        # exception will already by logged in start_inputs
+        shutdown_workers
+        raise e
+      end
     ensure
       # it is important to guarantee @ready to be true after the startup sequence has been completed
       # to potentially unblock the shutdown method which may be waiting on @ready to proceed
@@ -354,10 +432,11 @@ def start_inputs
     end
     @inputs += moreinputs
 
-    @inputs.each do |input|
-      input.register
-      start_input(input)
-    end
+    # first make sure we can register all input plugins
+    register_plugins(@inputs)
+
+    # then after all input plugins are sucessfully registered, start them
+    @inputs.each { |input| start_input(input) }
   end
 
   def start_input(plugin)
@@ -433,41 +512,6 @@ def shutdown_workers
     @outputs.each(&:do_close)
   end
 
-  def plugin(plugin_type, name, *args)
-    @plugin_counter += 1
-
-    # Collapse the array of arguments into a single merged hash
-    args = args.reduce({}, &:merge)
-
-    id = if args["id"].nil? || args["id"].empty?
-           args["id"] = "#{@config_hash}-#{@plugin_counter}"
-         else
-           args["id"]
-         end
-
-    raise ConfigurationError, "Two plugins have the id '#{id}', please fix this conflict" if @plugins_by_id[id]
-    
-    pipeline_scoped_metric = metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :plugins])
-
-    klass = Plugin.lookup(plugin_type, name)
-
-    # Scope plugins of type 'input' to 'inputs'
-    type_scoped_metric = pipeline_scoped_metric.namespace("#{plugin_type}s".to_sym)
-    plugin = if plugin_type == "output"
-               OutputDelegator.new(@logger, klass, type_scoped_metric,
-                                   OutputDelegatorStrategyRegistry.instance,
-                                   args)
-             elsif plugin_type == "filter"
-               FilterDelegator.new(@logger, klass, type_scoped_metric, args)
-             else # input
-               input_plugin = klass.new(args)
-               input_plugin.metric = type_scoped_metric.namespace(id)
-               input_plugin
-             end
-    
-    @plugins_by_id[id] = plugin
-  end
-
   # for backward compatibility in devutils for the rspec helpers, this method is not used
   # in the pipeline anymore.
   def filter(event, &block)
@@ -548,12 +592,6 @@ def stalling_threads_info
       .each {|t| t.delete("status") }
   end
 
-  def non_reloadable_plugins
-    (inputs + filters + outputs).select do |plugin|
-      RELOAD_INCOMPATIBLE_PLUGINS.include?(plugin.class.name)
-    end
-  end
-
   def collect_stats
     pipeline_metric = @metric.namespace([:stats, :pipelines, pipeline_id.to_s.to_sym, :queue])
     pipeline_metric.gauge(:type, settings.get("queue.type"))
@@ -590,5 +628,4 @@ def inspect
       :flushing => @flushing
     }
   end
-
-end end
+end; end
diff --git a/logstash-core/lib/logstash/runner.rb b/logstash-core/lib/logstash/runner.rb
index 78d291829c8..548ab4caac0 100644
--- a/logstash-core/lib/logstash/runner.rb
+++ b/logstash-core/lib/logstash/runner.rb
@@ -249,7 +249,7 @@ def execute
       config_loader = LogStash::Config::Loader.new(logger)
       config_str = config_loader.format_config(setting("path.config"), setting("config.string"))
       begin
-        LogStash::Pipeline.new(config_str)
+        LogStash::BasePipeline.new(config_str)
         puts "Configuration OK"
         logger.info "Using config.test_and_exit mode. Config Validation Result: OK. Exiting Logstash"
         return 0
diff --git a/logstash-core/spec/logstash/agent_spec.rb b/logstash-core/spec/logstash/agent_spec.rb
index b3d558e1fd6..3f5181cbe1d 100644
--- a/logstash-core/spec/logstash/agent_spec.rb
+++ b/logstash-core/spec/logstash/agent_spec.rb
@@ -52,6 +52,10 @@
       }
     end
 
+    after(:each) do
+      subject.close_pipelines
+    end
+
     it "should delegate settings to new pipeline" do
       expect(LogStash::Pipeline).to receive(:new) do |arg1, arg2|
         expect(arg1).to eq(config_string)
@@ -262,10 +266,14 @@
       subject.register_pipeline(pipeline_id, pipeline_settings)
     end
 
+    after(:each) do
+      subject.close_pipelines
+    end
+
     context "when fetching a new state" do
       it "upgrades the state" do
         expect(subject).to receive(:fetch_config).and_return(second_pipeline_config)
-        expect(subject).to receive(:upgrade_pipeline).with(pipeline_id, kind_of(LogStash::Pipeline))
+        expect(subject).to receive(:upgrade_pipeline).with(pipeline_id, kind_of(LogStash::Settings), second_pipeline_config)
         subject.reload_state!
       end
     end
@@ -295,6 +303,7 @@
 
       after :each do
         ENV["FOO"] = @foo_content
+        subject.close_pipelines
       end
 
       it "doesn't upgrade the state" do
@@ -319,14 +328,16 @@
     end
 
     after(:each) do
-      subject.shutdown
+      subject.close_pipelines
     end
 
     context "when the upgrade fails" do
       before :each do
         allow(subject).to receive(:fetch_config).and_return(new_pipeline_config)
         allow(subject).to receive(:create_pipeline).and_return(nil)
-        allow(subject).to receive(:stop_pipeline)
+        allow(subject).to receive(:stop_pipeline) do |id|
+          subject.close_pipeline(id)
+        end
       end
 
       it "leaves the state untouched" do
@@ -346,16 +357,20 @@
       let(:new_config) { "input { generator { count => 1 } } output { }" }
       before :each do
         allow(subject).to receive(:fetch_config).and_return(new_config)
-        allow(subject).to receive(:stop_pipeline)
         allow(subject).to receive(:start_pipeline)
+        allow(subject).to receive(:stop_pipeline) do |id|
+           subject.close_pipeline(id)
+        end
       end
       it "updates the state" do
         subject.send(:"reload_pipeline!", pipeline_id)
         expect(subject.pipelines[pipeline_id].config_str).to eq(new_config)
       end
       it "starts the pipeline" do
-        expect(subject).to receive(:stop_pipeline)
         expect(subject).to receive(:start_pipeline)
+        expect(subject).to receive(:stop_pipeline) do |id|
+          subject.close_pipeline(id)
+        end
         subject.send(:"reload_pipeline!", pipeline_id)
       end
     end
@@ -416,6 +431,12 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
     let!(:dummy_output) { LogStash::Outputs::DroppingDummyOutput.new }
     let!(:dummy_output2) { DummyOutput2.new }
     let(:initial_generator_threshold) { 1000 }
+    let(:pipeline_thread) do
+      Thread.new do
+        subject.register_pipeline("main",  pipeline_settings)
+        subject.execute
+      end
+    end
 
     before :each do
       allow(LogStash::Outputs::DroppingDummyOutput).to receive(:new).at_least(:once).with(anything).and_return(dummy_output)
@@ -429,10 +450,11 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
       @abort_on_exception = Thread.abort_on_exception
       Thread.abort_on_exception = true
 
-      @t = Thread.new do
-        subject.register_pipeline("main",  pipeline_settings)
-        subject.execute
-      end
+      pipeline_thread
+      # @t = Thread.new do
+      #   subject.register_pipeline("main",  pipeline_settings)
+      #   subject.execute
+      # end
 
       # wait for some events to reach the dummy_output
       sleep(0.01) until dummy_output.events_received > initial_generator_threshold
@@ -441,8 +463,8 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
     after :each do
       begin
         subject.shutdown
-        Stud.stop!(@t)
-        @t.join
+        Stud.stop!(pipeline_thread)
+        pipeline_thread.join
       ensure
         Thread.abort_on_exception = @abort_on_exception
       end
@@ -451,8 +473,8 @@ class DummyOutput2 < LogStash::Outputs::DroppingDummyOutput; end
     context "when reloading a good config" do
       let(:new_config_generator_counter) { 500 }
       let(:new_config) { "input { generator { count => #{new_config_generator_counter} } } output { dummyoutput2 {} }" }
-      before :each do
 
+      before :each do
         File.open(config_path, "w") do |f|
           f.write(new_config)
           f.fsync
diff --git a/logstash-core/spec/logstash/pipeline_spec.rb b/logstash-core/spec/logstash/pipeline_spec.rb
index 791ca1f8e87..5913fe5c57c 100644
--- a/logstash-core/spec/logstash/pipeline_spec.rb
+++ b/logstash-core/spec/logstash/pipeline_spec.rb
@@ -138,7 +138,7 @@ class TestPipeline < LogStash::Pipeline
       Thread.abort_on_exception = true
 
       pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
-      Thread.new { pipeline.run }
+      t = Thread.new { pipeline.run }
       sleep 0.1 while !pipeline.ready?
       wait(3).for do
         # give us a bit of time to flush the events
@@ -149,6 +149,7 @@ class TestPipeline < LogStash::Pipeline
       expect(output.events[0].get("tags")).to eq(["notdropped"])
       expect(output.events[1].get("tags")).to eq(["notdropped"])
       pipeline.shutdown
+      t.join
 
       Thread.abort_on_exception = abort_on_exception_state
     end
@@ -192,12 +193,14 @@ class TestPipeline < LogStash::Pipeline
           pipeline_settings_obj.set("config.debug", false)
           expect(logger).not_to receive(:debug).with(/Compiled pipeline/, anything)
           pipeline = TestPipeline.new(test_config_with_filters)
+          pipeline.close
         end
 
         it "should print the compiled code if config.debug is set to true" do
           pipeline_settings_obj.set("config.debug", true)
           expect(logger).to receive(:debug).with(/Compiled pipeline/, anything)
           pipeline = TestPipeline.new(test_config_with_filters, pipeline_settings_obj)
+          pipeline.close
         end
       end
 
@@ -385,9 +388,12 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("codec", "plain").and_return(DummyCodec)
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
       allow(logger).to receive(:warn)
-      thread = Thread.new { pipeline.run }
+      # pipeline must be first called outside the thread context because it lazyly initialize
+      p = pipeline
+      t = Thread.new { p.run }
+      sleep(0.1) until pipeline.ready?
       pipeline.shutdown
-      thread.join
+      t.join
     end
 
     it "should not raise a max inflight warning if the max_inflight count isn't exceeded" do
@@ -440,6 +446,10 @@ class TestPipeline < LogStash::Pipeline
     let(:settings) { LogStash::SETTINGS.clone }
     subject { LogStash::Pipeline.new(config, settings, metric) }
 
+    after :each do
+      subject.close
+    end
+
     context "when metric.collect is disabled" do
       before :each do
         settings.set("metric.collect", false)
@@ -528,9 +538,21 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutputmore").and_return(DummyOutputMore)
     end
 
+    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
+    before :each do
+      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
+      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
+      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    end
+
     let(:pipeline1) { LogStash::Pipeline.new("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
     let(:pipeline2) { LogStash::Pipeline.new("input { dummyinputgenerator {} } filter { dummyfilter {} } output { dummyoutputmore {}}") }
 
+    after  do
+      pipeline1.close
+      pipeline2.close
+    end
+
     it "should handle evaluating different config" do
       expect(pipeline1.output_func(LogStash::Event.new)).not_to include(nil)
       expect(pipeline1.filter_func(LogStash::Event.new)).not_to include(nil)
@@ -573,7 +595,7 @@ class TestPipeline < LogStash::Pipeline
     it "flushes the buffered contents of the filter" do
       Thread.abort_on_exception = true
       pipeline = LogStash::Pipeline.new(config, pipeline_settings_obj)
-      Thread.new { pipeline.run }
+      t = Thread.new { pipeline.run }
       sleep 0.1 while !pipeline.ready?
       wait(3).for do
         # give us a bit of time to flush the events
@@ -582,6 +604,7 @@ class TestPipeline < LogStash::Pipeline
       event = output.events.pop
       expect(event.get("message").count("\n")).to eq(99)
       pipeline.shutdown
+      t.join
     end
   end
 
@@ -596,6 +619,13 @@ class TestPipeline < LogStash::Pipeline
     let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
     let(:pipeline2) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
 
+    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
+    before :each do
+      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
+      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
+      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    end
+
     it "should handle evaluating different config" do
       # When the functions are compiled from the AST it will generate instance
       # variables that are unique to the actual config, the intances are pointing
@@ -626,8 +656,14 @@ class TestPipeline < LogStash::Pipeline
 
     subject { described_class.new(config) }
 
-    it "returns nil when the pipeline isnt started" do
-      expect(subject.started_at).to be_nil
+    context "when the pipeline is not started" do
+      after :each do
+        subject.close
+      end
+
+      it "returns nil when the pipeline isnt started" do
+        expect(subject.started_at).to be_nil
+      end
     end
 
     it "return when the pipeline started working" do
@@ -648,6 +684,10 @@ class TestPipeline < LogStash::Pipeline
     subject { described_class.new(config) }
 
     context "when the pipeline is not started" do
+      after :each do
+        subject.close
+      end
+
       it "returns 0" do
         expect(subject.uptime).to eq(0)
       end
@@ -655,10 +695,14 @@ class TestPipeline < LogStash::Pipeline
 
     context "when the pipeline is started" do
       it "return the duration in milliseconds" do
-        t = Thread.new { subject.run }
+        # subject must be first call outside the thread context because of lazy initialization
+        s = subject
+        t = Thread.new { s.run }
+        sleep(0.1) until subject.ready?
         sleep(0.1)
         expect(subject.uptime).to be > 0
         subject.shutdown
+        t.join
       end
     end
   end
@@ -704,6 +748,12 @@ class TestPipeline < LogStash::Pipeline
     end
     let(:dummyoutput) { ::LogStash::Outputs::DummyOutput.new({ "id" => dummy_output_id }) }
     let(:metric_store) { subject.metric.collector.snapshot_metric.metric_store }
+    let(:pipeline_thread) do
+      # subject has to be called for the first time outside the thread because it will create a race condition
+      # with the subject.ready? call since subject is lazily initialized
+      s = subject
+      Thread.new { s.run }
+    end
 
     before :each do
       allow(::LogStash::Outputs::DummyOutput).to receive(:new).with(any_args).and_return(dummyoutput)
@@ -712,7 +762,9 @@ class TestPipeline < LogStash::Pipeline
       allow(LogStash::Plugin).to receive(:lookup).with("filter", "multiline").and_return(LogStash::Filters::Multiline)
       allow(LogStash::Plugin).to receive(:lookup).with("output", "dummyoutput").and_return(::LogStash::Outputs::DummyOutput)
 
-      Thread.new { subject.run }
+      pipeline_thread
+      sleep(0.1) until subject.ready?
+
       # make sure we have received all the generated events
       wait(3).for do
         # give us a bit of time to flush the events
@@ -722,6 +774,7 @@ class TestPipeline < LogStash::Pipeline
 
     after :each do
       subject.shutdown
+      pipeline_thread.join
     end
 
     context "global metric" do
@@ -787,6 +840,13 @@ class TestPipeline < LogStash::Pipeline
     let(:pipeline1) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
     let(:pipeline2) { LogStash::Pipeline.new("input { generator {} } filter { dummyfilter {} } output { dummyoutput {}}") }
 
+    # multiple pipelines cannot be instantiated using the same PQ settings, force memory queue
+    before :each do
+      pipeline_workers_setting = LogStash::SETTINGS.get_setting("queue.type")
+      allow(pipeline_workers_setting).to receive(:value).and_return("memory")
+      pipeline_settings.each {|k, v| pipeline_settings_obj.set(k, v) }
+    end
+
     it "should not add ivars" do
        expect(pipeline1.instance_variables).to eq(pipeline2.instance_variables)
     end
diff --git a/logstash-core/src/main/java/org/logstash/FileLockFactory.java b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
new file mode 100644
index 00000000000..b553ab9e6e7
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/FileLockFactory.java
@@ -0,0 +1,121 @@
+// this class is largely inspired by Lucene FSLockFactory and friends, below is the Lucene original Apache 2.0 license:
+
+/*
+ * Licensed to the Apache Software Foundation (ASF) under one or more
+ * contributor license agreements.  See the NOTICE file distributed with
+ * this work for additional information regarding copyright ownership.
+ * The ASF licenses this file to You under the Apache License, Version 2.0
+ * (the "License"); you may not use this file except in compliance with
+ * the License.  You may obtain a copy of the License at
+ *
+ *     http://www.apache.org/licenses/LICENSE-2.0
+ *
+ * Unless required by applicable law or agreed to in writing, software
+ * distributed under the License is distributed on an "AS IS" BASIS,
+ * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+ * See the License for the specific language governing permissions and
+ * limitations under the License.
+ */
+
+package org.logstash;
+
+import java.io.IOException;
+import java.nio.channels.FileChannel;
+import java.nio.channels.FileLock;
+import java.nio.file.FileSystems;
+import java.nio.file.Files;
+import java.nio.file.Path;
+import java.nio.file.StandardOpenOption;
+import java.util.Collections;
+import java.util.HashMap;
+import java.util.HashSet;
+import java.util.Map;
+import java.util.Set;
+
+/**
+ * FileLockFactory provides a way to obtain an exclusive file lock for a given dir path and lock name.
+ * The obtainLock() method will return a Filelock object which should be released using the releaseLock()
+ * method. Normally the returned FileLock object should not be manipulated directly. Only the obtainLock()
+ * and releaseLock() methods should be use to gain and release exclusive access.
+ * This is threadsafe and will work across threads on the same JVM and will also work across processes/JVM.
+ *
+ * TODO: because of the releaseLock() method, strictly speaking this class is not only a factory anymore,
+ * maybe we should rename it FileLockManager?
+ */
+public class FileLockFactory {
+
+    /**
+     * Singleton instance
+     */
+    public static final FileLockFactory INSTANCE = new FileLockFactory();
+
+    private FileLockFactory() {}
+
+    private static final Set<String> LOCK_HELD = Collections.synchronizedSet(new HashSet<>());
+    private static final Map<FileLock, String> LOCK_MAP =  Collections.synchronizedMap(new HashMap<>());
+
+    public static final FileLockFactory getDefault() {
+        return FileLockFactory.INSTANCE;
+    }
+
+    public FileLock obtainLock(String lockDir, String lockName) throws IOException {
+        Path dirPath = FileSystems.getDefault().getPath(lockDir);
+
+        // Ensure that lockDir exists and is a directory.
+        // note: this will fail if lockDir is a symlink
+        Files.createDirectories(dirPath);
+
+        Path lockPath = dirPath.resolve(lockName);
+
+        try {
+            Files.createFile(lockPath);
+        } catch (IOException ignore) {
+            // we must create the file to have a truly canonical path.
+            // if it's already created, we don't care. if it cant be created, it will fail below.
+        }
+
+        // fails if the lock file does not exist
+        final Path realLockPath = lockPath.toRealPath();
+
+        if (LOCK_HELD.add(realLockPath.toString())) {
+            FileChannel channel = null;
+            FileLock lock = null;
+            try {
+                channel = FileChannel.open(realLockPath, StandardOpenOption.CREATE, StandardOpenOption.WRITE);
+                lock = channel.tryLock();
+                if (lock != null) {
+                    LOCK_MAP.put(lock, realLockPath.toString());
+                    return lock;
+                } else {
+                    throw new LockException("Lock held by another program on lock path: " + realLockPath);
+                }
+            } finally {
+                if (lock == null) { // not successful - clear up and move out
+                    try {
+                        if (channel != null) {
+                            channel.close();
+                        }
+                    } catch (Throwable t) {
+                        // suppress any channel close exceptions
+                    }
+
+                    boolean removed = LOCK_HELD.remove(realLockPath.toString());
+                    if (removed == false) {
+                        throw new LockException("Lock path was cleared but never marked as held: " + realLockPath);
+                    }
+                }
+            }
+        } else {
+            throw new LockException("Lock held by this virtual machine on lock path: " + realLockPath);
+        }
+    }
+
+    public void releaseLock(FileLock lock) throws IOException {
+        String lockPath = LOCK_MAP.remove(lock);
+        if (lockPath == null) { throw new LockException("Cannot release unobtained lock"); }
+        lock.release();
+        Boolean removed = LOCK_HELD.remove(lockPath);
+        if (removed == false) { throw new LockException("Lock path was not marked as held: " + lockPath); }
+    }
+
+}
diff --git a/logstash-core/src/main/java/org/logstash/LockException.java b/logstash-core/src/main/java/org/logstash/LockException.java
new file mode 100644
index 00000000000..fad548440d4
--- /dev/null
+++ b/logstash-core/src/main/java/org/logstash/LockException.java
@@ -0,0 +1,13 @@
+package org.logstash;
+
+import java.io.IOException;
+
+public class LockException extends IOException {
+    public LockException(String message) {
+        super(message);
+    }
+
+    public LockException(String message, Throwable cause) {
+        super(message, cause);
+    }
+}
\ No newline at end of file
diff --git a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
index 9720b0c6ff4..2ae58d76025 100644
--- a/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
+++ b/logstash-core/src/main/java/org/logstash/ackedqueue/Queue.java
@@ -1,5 +1,9 @@
 package org.logstash.ackedqueue;
 
+import org.apache.logging.log4j.LogManager;
+import org.apache.logging.log4j.Logger;
+import org.logstash.FileLockFactory;
+import org.logstash.LockException;
 import org.logstash.common.io.CheckpointIO;
 import org.logstash.common.io.PageIO;
 import org.logstash.common.io.PageIOFactory;
@@ -8,6 +12,7 @@
 import java.io.IOException;
 import java.lang.reflect.InvocationTargetException;
 import java.lang.reflect.Method;
+import java.nio.channels.FileLock;
 import java.nio.file.NoSuchFileException;
 import java.util.ArrayList;
 import java.util.List;
@@ -64,6 +69,12 @@ public class Queue implements Closeable {
     final Condition notFull  = lock.newCondition();
     final Condition notEmpty = lock.newCondition();
 
+    // exclusive dir access
+    private FileLock dirLock;
+    private final static String LOCK_NAME = ".lock";
+
+    private static final Logger logger = LogManager.getLogger(Queue.class);
+
     public Queue(Settings settings) {
         this(
                 settings.getDirPath(),
@@ -138,73 +149,89 @@ public void open() throws IOException {
 
         if (!this.closed.get()) { throw new IOException("queue already opened"); }
 
-        Checkpoint headCheckpoint;
+        lock.lock();
         try {
-            headCheckpoint = this.checkpointIO.read(checkpointIO.headFileName());
-        } catch (NoSuchFileException e) {
-            headCheckpoint = null;
-        }
+            // verify exclusive access to the dirPath
+            this.dirLock = FileLockFactory.getDefault().obtainLock(this.dirPath, LOCK_NAME);
 
-        // if there is no head checkpoint, create a new headpage and checkpoint it and exit method
-        if (headCheckpoint == null) {
-            this.seqNum = 0;
-            headPageNum = 0;
+            Checkpoint headCheckpoint;
+            try {
+                headCheckpoint = this.checkpointIO.read(checkpointIO.headFileName());
+            } catch (NoSuchFileException e) {
+                headCheckpoint = null;
+            }
 
-            newCheckpointedHeadpage(headPageNum);
-            this.closed.set(false);
+            // if there is no head checkpoint, create a new headpage and checkpoint it and exit method
+            if (headCheckpoint == null) {
+                logger.debug("No head checkpoint found at: {}, creating new head page", checkpointIO.headFileName());
 
-            return;
-        }
+                this.seqNum = 0;
+                headPageNum = 0;
 
-        // at this point we have a head checkpoint to figure queue recovery
+                newCheckpointedHeadpage(headPageNum);
+                this.closed.set(false);
 
-        // reconstruct all tail pages state upto but excluding the head page
-        for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
+                return;
+            }
 
-            // all tail checkpoints in the sequence should exist, if not abort mission with a NoSuchFileException
-            Checkpoint tailCheckpoint = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));
+            // at this point we have a head checkpoint to figure queue recovery
 
-            PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
+            // reconstruct all tail pages state upto but excluding the head page
+            for (int pageNum = headCheckpoint.getFirstUnackedPageNum(); pageNum < headCheckpoint.getPageNum(); pageNum++) {
 
-            add(tailCheckpoint, pageIO);
-        }
+                // all tail checkpoints in the sequence should exist, if not abort mission with a NoSuchFileException
+                Checkpoint tailCheckpoint = this.checkpointIO.read(this.checkpointIO.tailFileName(pageNum));
 
-        // transform the head page into a tail page only if the headpage is non-empty
-        // in both cases it will be checkpointed to track any changes in the firstUnackedPageNum when reconstructing the tail pages
+                logger.debug("opening tail page: {}, in: {}, with checkpoint: {}", pageNum, this.dirPath, tailCheckpoint.toString());
 
-        if (headCheckpoint.getMinSeqNum() <= 0 && headCheckpoint.getElementCount() <= 0) {
-            // head page is empty, let's keep it as-is
+                PageIO pageIO = this.pageIOFactory.build(pageNum, this.pageCapacity, this.dirPath);
 
-            PageIO headPageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
-            this.headPage = new HeadPage(headCheckpoint, this, headPageIO);
-            this.currentByteSize += headPageIO.getCapacity();
+                add(tailCheckpoint, pageIO);
+            }
 
-            // but checkpoint it to update the firstUnackedPageNum if it changed
-            this.headPage.checkpoint();
-        } else {
-            // head page is non-empty, transform it into a tail page and create a new empty head page
+            // transform the head page into a tail page only if the headpage is non-empty
+            // in both cases it will be checkpointed to track any changes in the firstUnackedPageNum when reconstructing the tail pages
+
+            logger.debug("opening head page: {}, in: {}, with checkpoint: {}", headCheckpoint.getPageNum(), this.dirPath, headCheckpoint.toString());
 
-            PageIO pageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
+            if (headCheckpoint.getMinSeqNum() <= 0 && headCheckpoint.getElementCount() <= 0) {
+                // head page is empty, let's keep it as-is
 
-            TailPage p = new TailPage(headCheckpoint, this, pageIO);
-            p.checkpoint();
-            add(headCheckpoint, pageIO);
+                PageIO headPageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
+                this.headPage = new HeadPage(headCheckpoint, this, headPageIO);
+                this.currentByteSize += headPageIO.getCapacity();
 
-            headPageNum = headCheckpoint.getPageNum() + 1;
-            newCheckpointedHeadpage(headPageNum);
+                // but checkpoint it to update the firstUnackedPageNum if it changed
+                this.headPage.checkpoint();
+            } else {
+                // head page is non-empty, transform it into a tail page and create a new empty head page
 
-            // track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum
-            if (headCheckpoint.maxSeqNum() > this.seqNum) { this.seqNum = headCheckpoint.maxSeqNum(); }
-         }
+                PageIO pageIO = this.pageIOFactory.build(headCheckpoint.getPageNum(), this.pageCapacity, this.dirPath);
 
-        // only activate the first tail page
-        if (tailPages.size() > 0) {
-            this.tailPages.get(0).getPageIO().activate();
-        }
+                TailPage p = new TailPage(headCheckpoint, this, pageIO);
+                p.checkpoint();
+                add(headCheckpoint, pageIO);
+
+                headPageNum = headCheckpoint.getPageNum() + 1;
+                newCheckpointedHeadpage(headPageNum);
+
+                // track the seqNum as we add this new tail page, prevent empty tailPage with a minSeqNum of 0 to reset seqNum
+                if (headCheckpoint.maxSeqNum() > this.seqNum) { this.seqNum = headCheckpoint.maxSeqNum(); }
+             }
+
+            // only activate the first tail page
+            if (tailPages.size() > 0) {
+                this.tailPages.get(0).getPageIO().activate();
+            }
 
-        // TODO: here do directory traversal and cleanup lingering pages? could be a background operations to not delay queue start?
+            // TODO: here do directory traversal and cleanup lingering pages? could be a background operations to not delay queue start?
 
-        this.closed.set(false);
+            this.closed.set(false);
+        } catch (LockException e) {
+            throw new LockException("The queue failed to obtain exclusive access, cause: " + e.getMessage());
+        } finally {
+            lock.unlock();
+        }
     }
 
     private void add(Checkpoint checkpoint, PageIO pageIO) throws IOException {
@@ -583,6 +610,12 @@ public void close() throws IOException {
                 // unblocking is safe and will return from the write call
                 notFull.signalAll();
             } finally {
+                try {
+                    FileLockFactory.getDefault().releaseLock(this.dirLock);
+                } catch (IOException e) {
+                    // log error and ignore
+                    logger.error("Queue close releaseLock failed, error={}", e.getMessage());
+                }
                 lock.unlock();
             }
         }
diff --git a/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
new file mode 100644
index 00000000000..f11c97dd2f6
--- /dev/null
+++ b/logstash-core/src/test/java/org/logstash/FileLockFactoryTest.java
@@ -0,0 +1,91 @@
+package org.logstash;
+
+import org.junit.Before;
+import org.junit.Rule;
+import org.junit.Test;
+import org.junit.rules.TemporaryFolder;
+
+import static org.junit.Assert.fail;
+
+import java.io.IOException;
+import java.nio.channels.FileLock;
+import java.nio.file.FileSystems;
+import java.nio.file.Path;
+
+import static org.hamcrest.CoreMatchers.equalTo;
+import static org.hamcrest.CoreMatchers.is;
+import static org.hamcrest.MatcherAssert.assertThat;
+
+
+public class FileLockFactoryTest {
+    @Rule public TemporaryFolder temporaryFolder = new TemporaryFolder();
+    private String lockDir;
+    private final String LOCK_FILE = ".test";
+
+    private FileLock lock;
+
+    @Before
+    public void setUp() throws Exception {
+        lockDir = temporaryFolder.newFolder("lock").getPath();
+    }
+
+    @Before
+    public void lockFirst() throws Exception {
+        lock = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock.isValid(), is(equalTo(true)));
+        assertThat(lock.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void ObtainLockOnNonLocked() throws IOException {
+        // empty to just test the lone @Before lockFirst() test
+    }
+
+    @Test(expected = LockException.class)
+    public void ObtainLockOnLocked() throws IOException {
+        FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+    }
+
+    @Test
+    public void ObtainLockOnOtherLocked() throws IOException {
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, ".test2");
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void LockReleaseLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+    }
+
+    @Test
+    public void LockReleaseLockObtainLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+    }
+
+    @Test
+    public void LockReleaseLockObtainLockRelease() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+
+        FileLock lock2 = FileLockFactory.getDefault().obtainLock(lockDir, LOCK_FILE);
+        assertThat(lock2.isValid(), is(equalTo(true)));
+        assertThat(lock2.isShared(), is(equalTo(false)));
+
+        FileLockFactory.getDefault().releaseLock(lock2);
+    }
+
+    @Test(expected = LockException.class)
+    public void ReleaseNullLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(null);
+     }
+
+    @Test(expected = LockException.class)
+    public void ReleaseUnobtainedLock() throws IOException {
+        FileLockFactory.getDefault().releaseLock(lock);
+        FileLockFactory.getDefault().releaseLock(lock);
+    }
+}
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
index 5115c69a5d1..95c6cd67a87 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/HeadPageTest.java
@@ -29,6 +29,8 @@ public void newHeadPage() throws IOException {
         assertThat(p.isFullyAcked(), is(false));
         assertThat(p.hasSpace(10), is(true));
         assertThat(p.hasSpace(100), is(false));
+
+        q.close();
     }
 
     @Test
@@ -47,6 +49,8 @@ public void pageWrite() throws IOException {
         assertThat(p.hasSpace(element.serialize().length), is(false));
         assertThat(p.isFullyRead(), is(false));
         assertThat(p.isFullyAcked(), is(false));
+
+        q.close();
     }
 
     @Test
@@ -71,6 +75,8 @@ public void pageWriteAndReadSingle() throws IOException {
         assertThat(p.hasSpace(element.serialize().length), is(false));
         assertThat(p.isFullyRead(), is(true));
         assertThat(p.isFullyAcked(), is(false));
+
+        q.close();
     }
 
     @Test
@@ -95,6 +101,8 @@ public void pageWriteAndReadMulti() throws IOException {
         assertThat(p.hasSpace(element.serialize().length), is(false));
         assertThat(p.isFullyRead(), is(true));
         assertThat(p.isFullyAcked(), is(false));
+
+        q.close();
     }
 
     // disabled test until we figure what to do in this condition
diff --git a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
index 88aabb63c76..a79cba1c29a 100644
--- a/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
+++ b/logstash-core/src/test/java/org/logstash/ackedqueue/QueueTest.java
@@ -40,6 +40,8 @@ public void newQueue() throws IOException {
         q.open();
 
         assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));
+
+        q.close();
     }
 
     @Test
@@ -55,6 +57,8 @@ public void singleWriteRead() throws IOException {
         assertThat(b.getElements().size(), is(equalTo(1)));
         assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
         assertThat(q.nonBlockReadBatch(1), is(equalTo(null)));
+
+        q.close();
     }
 
     @Test
@@ -70,6 +74,8 @@ public void singleWriteMultiRead() throws IOException {
         assertThat(b.getElements().size(), is(equalTo(1)));
         assertThat(b.getElements().get(0).toString(), is(equalTo(element.toString())));
         assertThat(q.nonBlockReadBatch(2), is(equalTo(null)));
+
+        q.close();
     }
 
     @Test
@@ -93,6 +99,8 @@ public void multiWriteSamePage() throws IOException {
 
         assertThat(b.getElements().size(), is(equalTo(1)));
         assertThat(b.getElements().get(0).toString(), is(equalTo(elements.get(2).toString())));
+
+        q.close();
     }
 
     @Test
@@ -135,6 +143,8 @@ public void writeMultiPage() throws IOException {
 
         b = q.nonBlockReadBatch(10);
         assertThat(b, is(equalTo(null)));
+
+        q.close();
     }
 
 
@@ -176,6 +186,8 @@ public void writeMultiPageWithInOrderAcking() throws IOException {
         b.close();
 
         assertThat(q.getHeadPage().isFullyAcked(), is(equalTo(true)));
+
+        q.close();
     }
 
     @Test
@@ -256,6 +268,8 @@ public void writeMultiPageWithInOrderAckingCheckpoints() throws IOException {
         assertThat(c.getMinSeqNum(), is(equalTo(3L)));
         assertThat(c.getFirstUnackedSeqNum(), is(equalTo(5L)));
         assertThat(c.getFirstUnackedPageNum(), is(equalTo(1)));
+
+        q.close();
     }
 
     @Test
@@ -297,6 +311,8 @@ public void randomAcking() throws IOException {
             }
 
             assertThat(q.getTailPages().size(), is(equalTo(0)));
+
+            q.close();
         }
     }
 
@@ -346,6 +362,8 @@ public void reachMaxUnread() throws IOException, InterruptedException, Execution
 
         // since we did not ack and pages hold a single item
         assertThat(q.getTailPages().size(), is(equalTo(ELEMENT_COUNT)));
+
+        q.close();
     }
 
     @Test
@@ -399,6 +417,8 @@ public void reachMaxUnreadWithAcking() throws IOException, InterruptedException,
         assertThat(q.getHeadPage().getElementCount() > 0L, is(true));
         assertThat(q.getHeadPage().unreadCount(), is(equalTo(1L)));
         assertThat(q.unreadCount, is(equalTo(1L)));
+
+        q.close();
     }
 
     @Test(timeout = 5000)
@@ -433,6 +453,8 @@ public void reachMaxSizeTest() throws IOException, InterruptedException, Executi
         assertThat(q.isFull(), is(true));
 
         executor.shutdown();
+
+        q.close();
     }
 
     @Test(timeout = 5000)
@@ -476,6 +498,8 @@ public void resumeWriteOnNoLongerFullQueueTest() throws IOException, Interrupted
         assertThat(q.isFull(), is(false));
 
         executor.shutdown();
+
+        q.close();
     }
 
     @Test(timeout = 5000)
@@ -516,6 +540,8 @@ public void queueStillFullAfterPartialPageAckTest() throws IOException, Interrup
         assertThat(q.isFull(), is(true)); // queue should still be full
 
         executor.shutdown();
+
+        q.close();
     }
 
     @Test
diff --git a/logstash-core/src/test/java/org/logstash/stress/Concurent.java b/logstash-core/src/test/java/org/logstash/stress/Concurent.java
index 48a4a5385a6..6792bb9b646 100644
--- a/logstash-core/src/test/java/org/logstash/stress/Concurent.java
+++ b/logstash-core/src/test/java/org/logstash/stress/Concurent.java
@@ -162,6 +162,8 @@ public static void oneProducersOneMultipleConsumer() throws IOException, Interru
         } else {
             System.out.println("SUCCESS, result size=" + output.size() + ", elapsed=" + Duration.between(start, end) + ", rate=" + (new Float(ELEMENT_COUNT) / Duration.between(start, end).toMillis()) * 1000);
         }
+
+        q.close();
     }
 
 
diff --git a/rakelib/artifacts.rake b/rakelib/artifacts.rake
index 1215345d15b..1937bf4e262 100644
--- a/rakelib/artifacts.rake
+++ b/rakelib/artifacts.rake
@@ -60,6 +60,7 @@ namespace "artifact" do
     @exclude_paths << "bin/bundle"
     @exclude_paths << "bin/rspec"
     @exclude_paths << "bin/rspec.bat"
+    @exclude_paths << "bin/lock"
 
     @exclude_paths
   end
